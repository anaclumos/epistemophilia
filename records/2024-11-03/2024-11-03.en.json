[
  {
    "id": 42033241,
    "title": "Touchscreens are out, and tactile controls are back",
    "originLink": "https://spectrum.ieee.org/touchscreens",
    "originBody": "CONSUMER ELECTRONICS INTERVIEW Touchscreens Are Out, and Tactile Controls Are Back Rachel Plotnick's \"re-buttonization\" expertise is in demand GWENDOLYN RAK4H6 MIN READ Gwendolyn Rak is an assistant editor at IEEE Spectrum covering consumer electronics and careers. Touchscreen controls in some new car models are being replaced by conventional knobs and buttons. WINNIEVINZENCE/ISTOCK",
    "commentLink": "https://news.ycombinator.com/item?id=42033241",
    "commentBody": "Touchscreens are out, and tactile controls are back (ieee.org)384 points by pseudolus 4 hours agohidepastfavorite282 comments praptak 2 hours agoTouchscreens are anti-accessibility. Lack of tactile feedback for the sight-impaired is the obvious part but there is another thing: Touchscreens just stop registering your touch when you get old. The older you get the less moisture there's in your skin, which at some point makes touch screens ignore you. https://www.gabefender.com/writing/touch-screens-dont-work-f... reply ho_schi 30 minutes agoparentI can confirm the troubles with age. Another problem are cold and hot environments (sweat). Anyway. Tactile input is generally better where an efficient placement of physical input controls is possible. Garmin is a seldom example of a company doing it right with the Edge 840. They merged the tactile 530 and the touch 830 into one device. The best of both worlds. Guess what I prefer? It is the Edge 530. Better screen to body ratio :) The rise of the touchscreens are an accident. Because MBAs believe iPhone == touch == good. It isn’t. The iPhone is just small, physical switches expensive (remember the slider smartphones) and you can merge output with input (this a pro and a con). Nice when you want to zoom a map. Horrible if Okay changes the position, worse when the keyboard requires the half screen and interaction is generally ineffective. I recommend: https://en.wikipedia.org/wiki/The_Design_of_Everyday_Things 1. Layout your tactile interface in a way that it allows the user to create a mental model of it. 2. In best case this model exists already. 3. Make it hard to use it wrong. 4. Make it easy to use it right. 5. Also applies to the output. If the turn indicator is ON, make it ON. PS: Right now I struggle to hide my touch keyboard. No „DONE“ or „HIDE“ and I cannot access my bookmarks for my recommendation. reply phrenq 2 hours agoparentprevAt a former company, we were all issued YubiKey Nanos, which just never worked for me. None of my coworkers had a problem, but I couldn’t get the damn thing to register a touch no matter what I did, including swapping keys. Eventually I came across a thread on an internal list for employees over forty, with several other people who were all having the same problem. The solution? Lick your finger. Gross, but it did the trick. And I’m stuck licking my finger every time I need to make a YubiKey work. reply 01HNNWZ0MV43FF 2 hours agorootparentI wonder if that explains the stereotype of old people licking their fingers to turn pages reply MikeTheGreat 1 hour agorootparentYes, it does. Source: when I was 5 I saw my grandmother, RIP, doing that and asked her about it. She explained that as she got older her fingers got drier, and now it's just easier to flip pages that way. reply mrkstu 40 minutes agorootparentprevIts also the only way I can open a plastic bag in the meat section of Costco. reply gessha 30 minutes agorootparentI was shopping at a grocery store and a lady saw me visible distraught by not being able to open a clear bag and she told me to touch some of the produce I’m about to pick up or the moisture around them. Never had the problem again. Thank you, random lady! reply hn72774 12 minutes agorootparentprevRubbing the opening side of the bag between your palms generates static and opens it too. Learned that from a meat department employee who saw me struggling one day. reply phrenq 1 hour agorootparentprevThat’s funny, I hadn’t thought of that. It very well might be true that turning pages is easier for people with more moisture in their skin. reply zxexz 1 hour agorootparentYes this is very much the reason. It gets dry where I am in the winter, and it never occurred to me to do this. An older gent in a coffee shop once watched me try to turn a page, and enlightened me. I’ve met more than a few people who have a dedicated finger glove for turning pages :) reply buran77 56 minutes agorootparentWet sponges [0] for people counting money was a very common sight some decades ago before money counting machines and mostly electronic payments. Probably still being used just not so obvious anymore. Regardless of age fingertips will eventually get too dry as the paper you work with absorbs all the moisture and flipping pages or separating banknotes becomes hard. For touchscreens dry fingers are also called \"zombie finger\" [1]. The screen registers the too minute change in electrical field as noise and rejects the touch event. Some sweat (but not too much) on the fingers makes all the difference. [0] https://image.shutterstock.com/image-photo/sponge-finger-wet... [1] https://www.consumerreports.org/cro/news/2015/06/zombie-fing... reply freedomben 55 minutes agorootparentprevI'm also in a dry climate, and even as a teenager I often had to lick my fingers in order to get the plastic bags at the grocery store open (I was a bagger so had to do it all day). Eventually we got smart and started putting wet sponges by the bags, which is also an amazing life hack if you have trouble turning pages. reply arccy 1 hour agorootparentprevso books are not accessible: kindles are better reply kibwen 1 hour agorootparentI'm unclear if this was intended to be sarcastic, but it's certainly possible for e-readers to be more accessible than books, at least for models that actually have physical buttons (and especially considering that e-readers can have zoomable text). reply freedomben 53 minutes agorootparentprevMaybe not accessible in the a11y sense, but definitely accessible in the \"can read without DRM\" sense. I'll take a book I can keep and read whenever and however I want to one that has to phone home and ask a giant corporation if it's ok. reply pndy 1 hour agorootparentprevThat sounds more like a soft percussive maintenance reply kgwgk 1 hour agorootparentprevOld people? reply Projectiboga 1 hour agorootparentprevAh I'm 56, the touch screen on my phone has gotten finicky. I'll have to see if that would help in a pinch. I wouldn't want to rely on that all the time but under time pressure it is good to know about that. reply szszrk 38 minutes agorootparentI'm almost 40, some years ago I noticed that at winter I get more frustrated with my phones and start thinking of changing them. It turned out each winter I make screens much more dirty and my fingers are drier. Touch gets more random, fingerprint readers success rate drops from 100% to more like 50%. Nowadays I make sure I clean the screen with actual dedicated products often, and make sure I keep hands moisturized. It works well, even if the latter contributes to the former. Haven't changed phone in over 2 years and still don't feel the need for change :) reply DavideNL 16 minutes agorootparentprev> Lick your finger. Exactly, i do the same thing with my (new) Macbook Air, it makes the TouchID sensor work much more reliable (also, i use my middle finger by the way...) reply pipes 1 hour agorootparentprevGreat! I'm in my forties and my laptop finger print scanner seems really temperamental, I'll try licking my finger! reply BobAliceInATree 56 minutes agorootparentprevIf you’d rather not lick your finger, you can get one of those old-school sponge stamp/envelope moisteners. reply marcosdumay 1 hour agorootparentprevThere exist some \"artificial tongues\" that people use to turn pages. It's a rosin-like substance that comes in a small can. reply deepsun 1 hour agorootparentThere exist artificial fingers to tap smartphones, sold in cold climates, so you don't need to take off gloves. Regular sausages in their packaging work great. reply Reason077 32 minutes agorootparentI used to have some woollen winter gloves with built in touch-screen fingertips. They worked well, but also made things quite slippery when holding a phone. This once resulted in a shattered screen when the phone slipped out of my gloved hand and flew onto a cold, hard, London pavement… reply cbzbc 1 hour agorootparentprevYou can effectively get these surfaces built into the glove, a number of companies sell such things. reply mirekrusin 27 minutes agorootparentWhy they don't need to be wet? reply cbzbc 24 minutes agorootparentTo activate a touchscreen they simply need to have capacitive properties. reply Log_out_ 2 hours agorootparentprevUser accounting the with the sibling system.lick the device to prove its your cake or you are thoroughly protected from being thoroughly grossed out by growing up with siblings . reply willmadden 1 hour agorootparentprevDrink more water. reply miohtama 1 hour agorootparentCheck under your eyelid that you are not an android reply vdvsvwvwvwvwv 1 hour agorootparentprevMoisturizer shoud help too? reply xyst 1 hour agorootparentprevjust apply some “hawk tuah”, new tagline for yubikey nano reply dimal 2 hours agoparentprev> She started getting frustrated, “it’s my fault, I don’t know how to use this thing properly.” This is heartbreaking. The woman is being excluded through no fault of her own, and she blames herself. I find this to be a common for people who don’t think of themselves as disabled but are made disabled by bad interfaces. They think there must be something wrong with themselves because everyone else has such an easy time, when really it’s the technology. reply rubslopes 1 hour agorootparentI once saw an elderly woman trying to receive medical care at an urgent care clinic. She brought her documents and medical insurance card, but the receptionist told her she could only be checked in if she provided a two-factor authentication code from her insurance app. The woman was totally confused. It was heartbreaking to watch. reply MikeTheGreat 1 hour agorootparentWhere do you live? And how is this supposed to work? Like, at all? Does the urgent care place have 2FA set up for every insurance company? Just the insurance companies they accept? What about folks that don't have their phone on them (which is reasonable to forget if you need medical care urgently, even if it's not ambulance-grade urgently). Plus, you've got the fact that the elderly are both a major market for medical services and famously techno-phobic.... reply rubslopes 21 minutes agorootparentBrazil. In my country, technology is growing rapidly, but in an unregulated way. On one hand, we have a modern banking system that allows instant money transfers to anyone at any time, and the government is developing its own cryptocurrency. With our electronic voting machines, the country knows election results within two hours after polls close. On the other hand, each company, including those providing essential services, creates its own solution without any regulatory oversight. This fragmentation extends even to official government services. In the case I mentioned, each private health insurance company freely determines its own procedures for patient check-in at affiliated clinics. With my insurance plan, my ID card is sufficient--for now. reply jve 1 hour agorootparentprevIsn't a document (what you own) + showing up physically so you can be scanned by eyeballs already not 2FA? What better authentication you can get than that? reply electricwire 1 hour agorootparentprevThe cynical side of me wonders if its not intended to work well reply rubslopes 15 minutes agorootparentI don’t agree with the practice, but from what I understand, they’re trying to prevent clinics from scamming insurance companies by faking clinical visits. I've heard that this is a thing that happens here. reply the_other 1 hour agorootparentprev> I find this to be a common for people who don’t think of themselves as disabled but are made disabled by bad interfaces A lot of disabled people today subscribe to the \"social model of disability\" [0] rather than the \"medical model\". Under the social model, the obstacle is not some property of the individual experiencing an access issue, but are created by a system made by other people who didn't provide alternative access methods. Society and its inventions disable, rather than the individual's condition. Clearly, disabled people have mental or physiological conditions that produce non-mainstream access needs. None of them deny that... but the social model invites us to take a society-wide ownership of this, and to better support a wider range of access needs by default. In contrast, the medical model tends to situate the disability within the individual, based on their physiological condition. This tends to put the ownership on the individual (or their immediate carers), which in turn tends to perpetuate exclusion and access challenges. [0]: https://en.wikipedia.org/wiki/Social_model_of_disability [1]: https://en.wikipedia.org/wiki/Medical_model_of_disability reply carlosjobim 46 minutes agorootparentThat sounds like a lot of meaningless theories made up by somebody who got a high paying, cushy government job to \"help\" disabled people because he/she is related to a politician. Pretty typical of modern ideology: find somebody to blame - ideally \"society\", instead of helping your fellow human. reply tokai 2 hours agoparentprevHave seen laborers and blue collar workers, my father included, that have to use their knuckles because their finger tips are too callused and dry for touchsceens. Seems like many groups have these kind of issues. reply have_faith 16 minutes agorootparentI'm not a labourer in the slightest, but I'm a rock climber though. Sometimes for up to two days after a strong session the finger print reader on my keyboard doesn't work. It always eventually starts working again :) reply analog31 29 minutes agorootparentprevThat's me. I'm a double bassist, a kind of blue collar laborer. ;-) It's worst during the winter. It's a shame because I prefer to use a touch screen on my laptop. reply ainiriand 1 hour agorootparentprevExactly, my brother works on power lines and wears many different types of gloves, all his computer tools are button actionated. reply dan-0 30 minutes agoparentprevThe opposite side of this is unpredictable or unintended behavior from too much moisture, which in my experience has been an acknowledgement with touch screens for quite some time. As touch screens for applications started to become common, this naturally filtered into tactical and service work fields. There is an advantage in this as it allows a more compact interface that can change more easily based on what the user needs. However the down side is, in harsh fast paced environments where the user may be moving quickly and sweating, it's much harder to register intended user feedback to the interface. The problem is not just if touch screens should be used, but also how they should be implemented. Especially on the side of general consumer electronics, like mobile phones, iOS and Android have built in interfaces for accessibility. In some cases you can get built in accessibility out of the box with very little effort, but the reality is, it takes a decent effort in most cases to get it right and users who need this behavior are not a heavy majority. This results in a deprioritization of accessibility in many mobile applications. This gets much worse with more hardware centric devices like thermostats, ovens, refrigerators, etc which have a higher tendency to have user interfaces developed in house and lacking any accessibility. Compounding this problem, with the popularity of touch screen interfaces, and post COVID supply chain problems, many users who needed accessible functionally were (maybe still are) left without many options, likely either having to pay a heavy premium for something with usable accessibility features, but probably more realistically, just taking what they can get. Modern technology makes accessibility easier than ever now, and enables accessibility in places that didn't previously exist, but the lack of willingness to implement accessible features on the part of some corporations is not just providing terrible accessibility, it's taking accessibility away from places where it previously existed. reply sholladay 41 minutes agoparentprevI thought so too at first, but in hindsight this is a bad take. Pinch-to-zoom was revolutionary for people with low vision. VoiceOver was revolutionary for people with no vision. Blind people ended up being early adopters of iPhones because of how much better the UX is compared to phones with physical controls, where memorization of the controls and menus is much more necessary. The flexibility of UI enabled by touchscreens was revolutionary for people with dexterity and cognitive issues. See the Assistive Access feature, for example, which has made Jitterbug phones obsolete for many people. Touchscreens not responding to dry skin is a real problem, though I’ve only ever seen that on cheap hardware. Testing the device is obviously necessary. I still want physical controls for simple and common cases, such as the vents in my car. But I now think of them more in terms of convenience and safety rather than accessibility. reply joe_the_user 19 minutes agorootparentI'm a caregiver for a couple people with dexterity and cognitive issues and I'm pretty sure a physical button is the absolutely simplest thing for them as much as for anyone else. And sure, an Ipad definitely can solve some accessible issues for some things but my clients watch things on TVs and monitors rather than Ipads (even when they have them). But more to the point, I love my clients and friends with such issues but they don't drive and shouldn't drive. reply croes 28 minutes agorootparentprevGood for them, bad for everyone else. reply wnevets 46 minutes agoparentprev> Touchscreens just stop registering your touch when you get old. The older you get the less moisture there's in your skin, which at some point makes touch screens ignore you. I had no idea that was a thing but it makes sense now that you said it. I will now be a lot more understanding when older folks have trouble using their phones, self-checkout, etc. reply nuancebydefault 1 hour agoparentprevHowever, i thought touch screens in cars are pressure sensitive rather than measuring changes in capacity? If the press is not registered in my car, i press a little harder. You can also use it while wearing normal gloves. That said, i use the physical knobs a lot more often, since your finger position will easily follow any moving button and nudges in rotating or shifting knobs feel super satisfying. reply freedomben 51 minutes agorootparentThose are actually different technologies, and I'm guessing you got used to the pressure sensitivity on older cars. They use resistive touchscreens which are cheaper. Cheap android tablets used to routinely use those instead of the capacitive touchscreens as well. It's been a while since I had to use a resistive touchscreen, and I'm glad for it. reply BenjiWiebe 45 minutes agorootparentOur milking robot has a resistive touchscreen. In that case it's excellent - you can spray it clean, and it still works while it's wet. reply freedomben 37 minutes agorootparentGreat point, there are situations where resistive is a lot better. I've definitely cursed my phone screen when it's raining. reply ethagnawl 56 minutes agoparentprevI'm only 40 but have had this issue for years -- especially with Apple products -- and I think it's compounded by my fingers being pretty callused. Regardless, I'm just unable to reliably use my family's iPad, sign my kids out of daycare, etc. This is yet another example of accessibility being in everyone's interest. reply WesolyKubeczek 49 minutes agorootparentIf your iPad is plugged in, it’s extremely finicky with regard to the type of both the charger and the cable. Touch ID is extremely sensitive to electrical currents near it, it seems, and on home button/Touch ID it’s right near the charging socket. Oops. Unplug it, Touch ID works. reply galleywest200 1 hour agoparentprevI am the opposite, my hands get so sweaty that touchscreens register random inputs because of the residual salty moisture. reply rralian 1 hour agoparentprevInteresting. I’ve noticed this happening for me but I thought it was because my fingertips are calloused from playing guitar. But I’m also in my late forties. So it’s probably a double whammy for me. reply yazzku 2 hours agoparentprevEven for non-impaired people, the lag on a touch screen is utterly miserable. reply praptak 1 hour agorootparentTouchscreens are also extra bad in the car. The hands have mass and the motion of a car is shaky due to bumpy roads, curves, braking, etc. This makes it hard to hit the desired area on a vertical touchscreen at near full extension of the arm. reply bhauer 2 hours agorootparentprevSoftware lag isn't unique to touchscreens. Software lag is always a terrible thing, and developers who de-prioritize performance should be ashamed, but that is true regardless of what input is used. reply jabroni_salad 1 hour agorootparentIt's kinda bearable with buttons because you get feedback. The ATM I use isn't the speediest thing but the buttons have a very tactile feel and it beeps at you for every press. It might not be \"impressive\", but it does cause forty dollars to appear and that's really all I wanted from it. Now ask anyone with a touch screen in their car what their error rate on that thing is. Even the really good ones are pretty bad. reply yazzku 1 hour agorootparentprevA touch screen imposes additional lag, though. Detecting finger swipes for left/right, for example, requires more processing than spinning a fucking dial or pressing on a button. But, like you said, performance doesn't matter anymore to the companies that design these interfaces. We should have criminal laws for this type of thing along with the return to proper hardware interfaces. Lack of performance should be a criminal offense. reply ChumpGPT 1 hour agoparentprevThis is so interesting; thanks for sharing. I often see my father-in-law tapping his phone a dozen times to get a response (85), and then I was teaching my mother how to use an iPad and noticed it was not that responsive to her taps. I sometimes need to tap twice to get anything to happen (50+), this helps me understand why. reply jsheard 1 hour agorootparentYou should check if there's a system setting to adjust the touch sensitivity, it's usually billed as a way to compensate for screen protectors but it'll probably also help with dry fingers not being detected reliably. reply praptak 1 hour agorootparentprevSee also the linked article. You can mitigate that by getting a stylus. reply miki123211 1 hour agoparentprevIt's not touchscreens that are anti-accessibility, it's touch controls. That's a very important distinction. I can use a proper touchscreen phone just fine, as its OS is advanced enough to run a screen reader, and its touch screen can precisely locate where it was touched and supports flicks, swipes and multi-finger gestures. Proper touch screens have some very important advantages, notably being able to show different controls at different times. You want to have a different button layout when you're typing a text than when you're watching a movie or playing a game. Physical buttons make this impossible. Even blind people benefit from this, modern phones have a mode where you can use a touch screen to input characters in Braille, treating different parts of the screen as keys on a brailler (think piano with 6 keys). Each combination of these keys, pressed or touched at once, inputs a specific character[1]. Now touch controls, like those you can find on a washing machine / coffee maker, make no sense. There's no screen behind them, so they're not dynamic in any way, and the primitive software of such devices (as well as the need to seell them in multiple countries without providing specific support for any particular human language) make accessibility impossible to achieve. reply lozf 1 hour agorootparent> Physical buttons make this impossible. For many years now we've had various interfaces that use physical buttons whose function can change at different times during operation, the current function being indicated by the screen: Old fashioned ATMs with 4 buttons on each side of the screen, many business-class feature-phones had \"soft-keys\", even old DOS programs that used Function Keys are conceptually similar. There are differing degrees of compromise vs utility. reply aftbit 1 hour agorootparentprevI like the strategy used by multi-function displays in plane cockpits. They have physical buttons along the side that can trigger different actions, labelled by text and icons on the screen alongside the button. This allows you to find and press the button even if there is turbulence or engine vibrations making it hard to use a touchscreen. reply kragen 1 hour agorootparentprevTouchpads, you mean? Touchpads are waterproof and washable, like membrane keyboards, which is a real plus in the kitchen. Also they're cheap as fuck. reply patrickhogan1 1 minute agoprevThe Sony WH-1000XM4 headphones have both touch and voice controls, but they can be frustrating to use. The touch controls are meant to be easy, but they’re often too sensitive or don’t respond well. For instance, a small accidental swipe can pause or skip a song, which interrupts my music. The voice feature, \"Speak-to-Chat,\" stops the music if it hears you talking or even singing along, which can be annoying. I usually turn off these controls because they’re more hassle than help—it’s actually easier to adjust the volume on my iPhone when I’m on a run. reply Dwedit 2 hours agoprevThe worst of both worlds is Touch Buttons. No screen, just a touch-sensitive surface that's divided into areas that activate upon any kind of skin contact, whether intentional or not. I always see my dishwasher having some bizarre setting active because of accidental contact with a touch button. reply wlesieutre 2 hours agoparentThe worst variation I’ve ever seen, courtesy of r/CrappyDesign: My oven uses a touchscreen, so whenever I open it, steam gets on the touchscreen and messes with the settings. http://web.archive.org/web/20210509153031/https://www.reddit... reply f1shy 1 hour agorootparentOh yes… never buy a Miele oven with touch buttons reply vdvsvwvwvwvwv 1 hour agorootparentSame with Bosch. Two problems: Buttons stopped working after warranty expired so had to pay for a service call to have it fixed. Luckily no parts were needed. I don't recall the reason right now. It has a spinny disc, so like a potentiometer but not. It is a flat removable ring and behind it it uses a touch button of sorts You have to pull it off amd clean it before every use for it to work and when it does work it is very fiddly to use. reply qwertox 2 hours agorootparentprevWow, this is definitely the worst example. At least it's not in a security sensitive context. reply wlesieutre 1 hour agorootparentMakes you wonder if anyone at the company ever even tested it with food in the oven reply MaxikCZ 1 hour agorootparentI dont mind the fact that they havent tested it with food, but I cant understand how they never recalled every single unit after noticing it for the first time. Its like they see it, and be like \"Ah, everyone who bought it got screwed over, and it will hurt our brand, but its still cheaper to quietly ignore it\". Despisable reply arp242 54 minutes agorootparentprevUsually what happens is that it's tested under ideal \"lab conditions\", so this never happens. In real life ovens get a bit grimy and produce more smoke. Stuff like that. Still shoddy engineering of course. It's the same with designers doing their light-grey text on a white background with their 8K colour-perfect screen in optimal lighting conditions, and then when you point out this is difficult to read they go \"I don't see the problem!\" reply x3n0ph3n3 49 minutes agorootparentprevI have that same control panel, but on an oven/microwave combo. It's truly horrible. reply schmidtleonard 2 hours agoparentprevDon't forget to pair the Touch Button with a Minimalist design that gives no indication if a button has been pressed! Bonus points for a big long click buffer and strange multi-click semantics so that once the computer unfreezes your attempts at diagnostics are redirected into messing up the state in weird and wonderful ways that you will have to unpack over the next week. reply K0balt 2 hours agorootparentDon’t forget the Uber-minimalist aesthetic, where there are no markings or textures to designate the touch regions, but instead you just touch or swipe different parts of the object for different functionality. That’s my favourite, especially after you haven’t used something for a few months. Bonus points if a firmware update changes the invisible control layout. reply schmidtleonard 7 minutes agorootparentHell yeah! Let's change the active region to the upper left corner of the hamburger symbol and make sure that the hieroglyph itself doesn't reflect this in any way. Dear Satan, I believe now would be a good time to discuss the subject of a raise! reply corytheboyd 2 hours agoparentprevA tiny amount of water getting on these buttons can make them go nuts too… I absolutely hate the electric stove ranges with surface touch buttons… as if those never get water on them… reply lrasinen 2 hours agorootparentOr cats. Waking up in the middle of the night because the stove is beeping is not my favorite thing. reply rkuska 2 hours agoparentprevI spent (5y ago) so much time searching for induction stove with physical knobs. The touch interface at my previous place was driving me crazy, a slight misalidgment and the stove would beep like it’s end of the world. Luckily Miele produces some at the premium price (or was at the time) but I considered it an investment in my mental health. reply lolinder 2 hours agorootparentA touch interface on the stove seems like the canonical example of a straightforwardly bad idea. Sure, let's use a capacitive touch interface to control the most dangerous appliance in the kitchen, one which also happens to frequently be the most humid spot and also the most likely to feature splashed oil! What could possibly go wrong? reply scotty79 1 hour agorootparentI actually love that I can easily wipe everything when it's dirty. I'd hate cleaning knobs and most of the tactile buttons. Some touch controls are incredibly good at filtering false inputs. Unfortunately you can't tell which. reply rrix2 1 hour agorootparent> I actually love that I can easily wipe everything when it's dirty. I'd hate cleaning knobs and most of the tactile buttons. the knobs on my manually operated range pull right off their posts and go soak in the sink with some soap and hot water once a week while i spray the range's control surface with whatever spray cleaner and wipe it off with every other flat surface in my kitchen. after ten or fifteen minutes of soaking, anything left on the knobs fall off with a dry rag that goes in the cloth washer afterwards. reply ninkendo 26 minutes agorootparentI’m in full agreement with everyone here who hates touch screens, and I also spent a long time looking for induction ranges with physical knobs (IIRC there was only one model in the universe with them), and was so mad that I had to get one with touch buttons… But I gotta say, the ability to just simply wipe the whole stove surface with a towel and be done has more than made up for the touch buttons sucking. With physical knobs: Take knobs off and soak them, use a towel and wipe a circle around the nub that’s left, try not to leave a circular streak pattern, put knobs back. Or just wipe the knobs with the towel and get close enough on the surface. Touch buttons: wipe the whole thing in big strokes, you’re done. I clean the whole surface after every use now, because it’s just so damned easy. reply lolinder 29 minutes agorootparentprevYep, every knob I've ever had on a stove works this way and makes them trivial to clean. In the meantime, during regular use they're guaranteed to never stop functioning because they got wet or oily. reply Dwedit 33 minutes agorootparentprevYou can easily wipe a membrane keypad clean. Those require force to trigger the buttons, so they are not at all like touch buttons. reply KineticLensman 2 hours agorootparentprevTotally agree. The controllability of my Nef induction hobs was excellent, but the controls were horrendous. E.g. going from a level 9 rapid heat-up to a level 2 simmer is seven distinct touches. Each with an annoying beep. Related to this is the lack of a single-tap hob-off for an individual hob. For medical reasons [1] I had to transition from the induction hob to a ceramic hob, and had to choose the Nef equivalent because it had the same physical footprint. So now I have the same crap controls with much worse response time to the control inputs themselves. The ceramic hob also can't detect when a pan has been removed so will leave a hob dangerously hot but not glowing. I've got used to it now but it is very frustrating and still catches me out sometimes. [1] I have an implanted defibrillator whose sensor is nulled out by an inductions hob's magnetic fields. reply James_K 1 hour agorootparentA lot of people don't realise that you can push both the up and down button at the same time to set a hob ring to zero intensity. So level 9 to level 2 is actually just three presses. reply vdvsvwvwvwvwv 1 hour agorootparentOn mine 2 back to 9 is 7 presses. Use case: adding more water to rice. reply marcosdumay 1 hour agorootparentprevOf all things, it's a novel kind of stove with the distinctive feature that you can place a piece of plastic just next to the food and it will work fine... Why no designer wants to exploit that feature? reply m463 51 minutes agorootparentprevThe Breville Control Freak is pretty cool (but horrendously expensive) reply yurishimo 13 minutes agorootparentOwning 4 of these would be untenable! reply scotty79 1 hour agorootparentprevI'm currently using Miele with touch controls but it's really good at filtering out false inputs. I have no problem whatsoever even with my messy cooking. Too bad you have no way of telling how good controls are in a product before you start using them. reply mmooss 9 minutes agoparentprevEasier to clean - no nooks or crannies. reply s0rce 22 minutes agoparentprevMy induction cooktop has the worst touch buttons that constant beep and sense pots, tongs or other stuff as well. Very annoying. reply m463 53 minutes agoparentprevThe worst is the dishwasher buttons/lights on the edge of the door so you can't look at them without opening it. What is this stupid trend? reply pants2 30 minutes agorootparentMy dishwasher has buttons on the top like this, and during the heat dry cycle the steam will activate the buttons and I'll hear lots of random beeps from the kitchen. Ponce in a while it manages to cancel or restart itself, hilariously bad design. reply Dwedit 35 minutes agorootparentprevWhile my dishwasher has the \"buttons\" at the top of the door, it puts the light on the bottom, so it shines on the floor. Little red dot. Cats like that dot. reply drivers99 2 hours agoparentprevSame with my apartment’s smart lock. The deadbolt gets extended accidentally while the door is open when someone brushes against the panel from the outside and you have to reach around the door to retract it. reply jhickok 1 hour agoparentprevSee the Dell XPS https://imgur.com/KbOXGYa reply matsemann 2 hours agoparentprevA peugeot (e308?) I rented for a few weeks had that. Absolutely bonkers. When driving I normally feel my way (\"max heating to get rid of fog is the third button to the left\"), but with this I would also activate all other kinds of stuff all the time. Recently changed offices at work. The new one has the same kind of buttons for the keypad. Just a flat surface with 9 numbers. I accidentally double press all the time, as it's hard to feel with no tactile feedback what you're doing and it's a bit delayed in the \"beeps\". So then you have to wait a few seconds and try again. Drives me mad. reply josefrichter 2 hours agoprevInterestingly, almost all designers know that touch screens in cars are bad idea. They always knew it. Bit for some reason, the designers in automotive industry were the only ones who didn’t know. It’s a mystery. reply Laremere 51 minutes agoparentIn addition to what others are saying, US law requires new cars to have back up cameras and the related screen. So everything else immediately becomes \"so we add it to the screen we already have to have, or add a new physical control?\" On another note, I do like my (getting older) Mazda's screen. It has touch, but I honestly forget it does because the control knob is so much better for use while driving. Nice and tactile. Additionally all of the important controls have physical buttons. Only major problem I have with it is that if it can't connect to Bluetooth (which is stupidly often), it decides to switch back to radio, blasting that at me. Then I have to sit there going through multiple menus to get Bluetooth reconnected. reply seanmcdirmid 2 hours agoparentprevCost. They put them in to save money. It’s not a mystery at all. Plumbing wires for a bunch of analog switches is more expensive than one databus, and then there is the simplicity of turning your hardware problem into a software one. reply kragen 59 minutes agorootparentYou can connect a bunch of analog switches to one LINbus microcontroller; then you only need one databus. reply kemiller 2 hours agorootparentprevThere is truth to that, but it’s also true that cars simply have way more functionality than they did 20 years ago and it’s effectively impossible to assign a button to every thing. reply bluefirebrand 2 hours agorootparentDo cars really have that much more functionality that it requires everything to be thrown into a touch screen? I have a 2017 Chevy Sonic with a built in touchscreen and I basically never have to touch it other than to input an address into Android auto. I haven't found any pieces of the car functionality I cannot access through a button somewhere on the dash or steering wheel I doubt a 2024 car has that much more functionality than my 2017 reply vel0city 1 hour agorootparentBoth of my cars from different makers have a ton of things which don't have a physical button. Configuring the colors of the lights in the interior, setting restrictions on secondary keys, changing the doorpad settings, configuring navigation quick saves, configuring auto lock on walk away, whether the car moves the seats back for easier getting in and out, how much it moves the seats for that, toggling liftgate gestures, setting the default settings for ADAS systems, configuring if the mirrors automatically tuck in or not, configuring the puddle lights, configuring charging settings, configuring stereo equalizer and other deeper settings, rear occupant alert systems, configuring how long it waits to have the lights on, defaults for auto-high beam and its sensitivity, configuring remote start options, deeper setting options for drive modes, configuring cross traffic alerting, deeper route planning, etc. Probably still a hundred more options I haven't listed here. reply Crespyl 1 hour agorootparentSurely none of that requires a touchscreen though? Just basic generic navigation and selection buttons will work fine. reply vel0city 1 hour agorootparentIt doesn't require it to be a touchscreen, sure, but it practically requires it to be a screen. But I'd much rather just quickly tap a checkbox instead of press, press, press, press, press, press, press, press, press, press, press, press, press, press, press, press, press, press, press, press, press, press, press navigating the giant array of settings. And then on top of that people want AA/CarPlay which is designed around touch inputs first, so you're going to have that screen be touch anyways. None of that should really be changed by the driver when the car is in motion, and you'd have to manage the deep navigation of a bunch of button presses on a screen anyways so arguing you'd be less distracted is a moot point. reply miltonlost 1 hour agorootparentprevAnd so you can have physical buttons (left, right, up, down, enter) and a screen with a menu for all those options. reply robocat 16 minutes agorootparent> left, right, up, down, enter Every UI using \"simple\" menu button navigation has been horrific in my experience. Remote controls, handsets, yadda yadda. reply vel0city 1 hour agorootparentprevHow is that better? Press press press press press press press press press press press press press press press cool just set one setting. Versus tap settings, flick scroll, tap to set. reply eloisant 1 hour agorootparentprevI don't mind having the extra functionality on the touch screen, just let me use the basic ones that already existed before touchscreens (A/C control, volume, etc.) on physical buttons. reply xyst 1 hour agorootparentprev> more functionality The functionality you refer to is probably the creature comforts (ie, multi zone A/C, memory settings for front seats, …). But the essentials of a car (ie, transmission, wheels, structural integrity, windshield wipers) haven’t changed for decades. What has changed though is: - increasing size of vehicles due to increasing insecurity of American buyers - a large majority of class C holders largely unprepared for the size of these vehicles - this gives manufacturers the opportunity to stuff as much tech junk into these vehicles to give these less qualified drivers more assistance - coincidentally, all of this tech junk comes with a very high premium for manufacturers and dealerships Fear sells in this country. 9/11 changed the game. reply James_K 2 hours agorootparentprevI think the added complexity is in areas where it doesn't really matter. The stuff the driver actually cares about is still the same as it was then. You can just put the rest in a bluetooth phone app. If it is more complicated than a button press, people probably shouldn't be messing with it while driving anyway. reply tpmoney 1 hour agorootparentI definitely don't want my car controls tied to a phone app. No matter what I should be able to configure my car's functions long after the company stops distributing their app. But there's no reason why we can't have a \"best of both worlds\" sort of deal. I have a modern Mazda with a touch screen that comes with a center control knob and has physical controls for a good chunk of the settings you'd ever want to change while driving. So I don't have to go through menus to change my air conditioning from low to high, but I also don't have to use a tiny character led display and a \"push 3 times, then hold for 5 seconds then pull twice and rotate 37.8 degrees\" multi function button to find and access settings outside of those physical controls. In fact, the touch screen disables touch input at speed, so the control cluster MUST be able to access any functionality without relying on the touch display. It works pretty darn well. In fact the only thing I'd argue it could do better is be more responsive and have a decent set of distinct tones for navigating the screens without sight. It's not often I want a setting in the menus while driving, but it would be a lot nicer if each menu screen had a distinct set of sounds so that by ear I could know where I am and memorize those controls if I needed to. reply James_K 1 hour agorootparent> long after the company stops distributing their app There is a cool idea called open source, but I suppose something as radical as giving users ownership of software for their car isn't something companies would be willing to consider. Much better when you get to charge a subscription for heated seats. reply jabroni_salad 1 hour agorootparentprevHow many settings does a typical TV have these days? You can modify all of those with a d-pad. What is happening in your car that actually needs touch? reply appplication 1 hour agorootparentI see your point, but I wouldn’t exactly uphold TV menu navigation as a model of good user design. reply bob1029 2 hours agorootparentprevCouldn't you still run a digital bus all the way and then have some conversion to/from analog controls at the end? Keep the computer but lose the screen? The interface is the problem, not the underlying information representation or communication. reply karaterobot 2 hours agorootparentThat's a good idea, but I think at least part of the reason it's more complicated is that you have to design and fabricate a new face plate for the dashboard, and get a new set of controls every time you want to change something on it. Say you wanted to add a new button on a particular trim level only, because it has a feature that the other levels don't. You'd need to either redesign that whole part of the console for just that trim level, or else sell everyone at a lower trim level a console with an extra button that does nothing. Multiply that by N, for every tiny feature you want to sell on the higher trim levels. If you've got a digital display, of course, you can just go crazy and add all the UI elements (and features) you want. reply miki123211 1 hour agorootparentThis actually makes sense. If you want the car to be fully customer configurable, you basically need a custom dashboard for every single car. You also need to think about what happens when the customer does an upgrade. reply marcosdumay 36 minutes agorootparentSomebody could invent a device that creates plastic boards with custom-designed shapes. reply nuancebydefault 1 hour agorootparentprevCost and durability as well. Physical knobs wear out because of friction and dust. reply qwertox 2 hours agorootparentprevThese buttons are usually located so close to each other, that one PCB can hold many of them. Then you need just one set of wires which connects the ECU to the controller on the PCB. reply hyperadvanced 16 minutes agorootparentprevThat isn’t really true when you factor in the cost of engineering new parts/systems compared to just doing it like you’ve always done. I know a guy who worked at GM and apparently they got bit by the “digital transformation” bug and decided that the army of iPhone app developers and ex Silicon Valley folks was what they needed to stay relevant. Hence the omnipresent touch screen. reply jsd1982 2 hours agorootparentprevA touchscreen with an entire software engineering department behind its software is cheaper than buttons? reply vel0city 1 hour agorootparentYou make the software button once and it's there for the many millions of cars. You have to actually manufacture and stick in the many millions of buttons otherwise. Besides the actual action was going to be software on the bus anyways. Your window switch hasn't been directly connected to a motor in decades. It's sending a \"window down\" message to the bus that goes to the window actuator unit that then drives the motor. You're still paying someone to make it computerized anyways, you were going to pay a team of designers to draw it up and make the plans for the physical switch as well. The screen was going to be there anyways due to backup camera requirements and because consumers want AA/Carplay. reply MostlyStable 2 hours agorootparentprevEvidence suggests that their engineering teams are either not that big or not that good given how garbage most vehicle UI/software is, and it's a price you pay (mostly) once per touchscreen software design, which will span several models, where as the component + install cost needs to be paid for every vehicle in perpetuity. reply ahartmetz 30 minutes agorootparentIf you haven't been there, you cannot imagine how bad most car manufacturer's software departments are. They are big, expensive, and crawling with bad practices. Management usually doesn't have a clue about software, so there's a lot of maneuvering with goals being anything but producing good software quickly and cheaply. reply daniel_reetz 2 hours agorootparentprevIt's a little deeper than this, software for each module is typically provided by a tier 1 or tier 2 supplier according to a spec provided by the OEM. Sometimes the tier 1 or tier 2 supplier is also subbing out the software or stuck with some system on chip that sucks. So for a made-up example, GM wants to build a smart dash in the latest SUV, maybe Bosch or Continental has one with a SoC inside and their own software hell. OEM works with supplier to integrate, bugfix, skin, and customize. But they don't write it from scratch. reply caskstrength 1 hour agorootparentYes, and suppliers outsource the actual development and testing to cut costs even further. reply ahartmetz 25 minutes agorootparentAFAIK, car manufacturers want to bring more software in house as a core competency, which is probably good because the \"Tier 1\"s are generally even worse at software than them and have worse aligned incentives. reply BurningFrog 2 hours agorootparentprevIf there are enough buttons, yes. Toyota makes 10 million cars a year. Another angle is that you can add/remove/relabel software buttons later. Hardware decisions are much more final. reply dwelch91 1 hour agorootparentWhen I worked at Toyota (well, NUMMI) in the '90s, the engineers from Toyota Japan that told me: \"I'd kill my mother to save $1 on each car produced.\" Yes, at Toyota's scale, $1/car is a lot of money. reply dghlsakjg 1 hour agorootparentprevYes. The buttons still need to be programmed to do something so the cost savings isn't really on the software team. Having a standard touchscreen that you can slap into any of your cars, and update OTA is huge. reply Gare 2 hours agorootparentprevGiven economies of scale, yes reply izacus 1 hour agorootparentprevRemember that someone needs to manufacture those buttons, install them in the factory, stock them for replacement and keep them around several countries in the world in warehouses for when they break. Now replace all that with a single screen and suddenly costs savings everywhere \\o/ reply Triphibian 1 hour agorootparentprevLet's not forget you can charge a mint to replace the half-assed Ipad you have jammed into the dashboard when it goes bad. reply lomase 2 hours agorootparentprevThe hardware buttons need a system, microcontroller with software or whatever, to manage its state just like the screen. reply HideousKojima 2 hours agorootparentprevSoft keys don't require any significant wire plumbing, the keys are less than an inch from the screen. And they've been used for decades in ATMs and gas pumps: https://en.m.wikipedia.org/wiki/Soft_key reply kjkjadksj 2 hours agorootparentprevMy old head unit was all buttons and slipped into the dashboard in one piece with one plug too. In the custom stereo world having a touch screen interface always carried a premium over good old buttons. I’m not sure why that should change. Screens are much larger and full color on touch screen cars too compared to basic lcd alphanumeric screens. reply MostlyStable 2 hours agoparentprevI've been relatively convinced that it was a cost savings measure. Both in cost of components and, probably more importantly, cost in labor of install, since touchscreens are cheaper on both regards. Everyone knew it was worse, but it saved money, and, at least for a while, it could be marketed as \"premium\". reply layer8 2 hours agoparentprevThe designers are not the ones who decided on that. It’s cost reduction, feature flexibility (you can decide later what exactly to provide in the software), and the marketing semblance of a cool modern interface. reply amelius 2 hours agoparentprevOf course they knew it. But they __also__ knew that buyers wouldn't figure it out until after buying the car. Fast forward a decade, and now buyers want buttons. reply xyst 1 hour agoparentprevProbably cost and the rise of touch screen mobile phones (ie, og iPhone of 2007-2008) reply lwn 2 hours agoprevAs a synthesizer enthusiast, I'm excited to read about this. A well-designed button layout on a synth sparks my creativity. Tweaking knobs on a touchscreen doesn’t work for me because I constantly have to check the screen to make sure my fingers are on the right control. reply motohagiography 2 hours agoparentthe obvious consequence of electric vehicles is live configurable filters and patches for performance tuning. I want an ADSR for my accelerator in different modes. give me an EQ for acceleration and braking, along with a feedback cycle for cruising, and the era of performance personalization will be huge. I would buy a tesla instantly if you gave me a eurorack dashboard insert! eurorack module designers have moved hardware interface design to where they can create intuitive design languages as well. reply qwertox 2 hours agoparentprevIts so great when you know where the buttons are located, that you can touch them in the darkness without them suddenly selecting anything. When you need to make sure \"is this the second one from the left?\", then apply some force to actually change its value. reply cbzbc 1 hour agoparentprevSimilarly, I find mixing on a tablet slower than mixing on a console with tactile controls - because you can do things like change multiple things by different degrees at once (you don't have to look at both controls to ensure your fingers are tracking) and adjust a control while looking at the stafe. reply jncfhnb 1 hour agoparentprevAh but have you tried the conductive touch pads on the Strega that make your body’s conductive properties a human patch cable? reply TheRealPomax 2 hours agoparentprevA poorly designed synth doesn't generally cause a car accident though, far less of a legislative impetus to stop softwaring everything in synth-land =) reply wigster 1 hour agorootparenti'd argue the interface on the old yamaha dx synths with FM synthesis was a bit of a car crash I certainly never got my brain round them. ;-) reply HerbMcM 1 hour agoprevOnce upon a time I used Android Auto and things were good. Most controls were in the corners, you see, which allowed me to perform a couple of changes without looking at the touchscreen. One day, a GUI designer decided to put a horizontal bar going through the top of the display just to display a very tiny clock on the top right corner. The top left corner I used to bring up the menu and quickly select options no longer worked reliably as it was under that horizontal strip. I stopped using Android Auto after a couple of months. reply lolinder 1 minute agoparentThis is why I may never upgrade to a vehicle newer than ~2010. I've dealt with too many consumer electronics that auto-update in ways that make them useless to me, and I'm not willing to make a car-sized purchase in the vague hope that this consumer electronic device will be the exception and will keep working for 10+ years (assuming I maintain it) in the same way as it did when I bought it. I develop and rely on muscle memory when driving, and I'm not going to invest in muscle memory that can be changed out from under me on the whims of some product manager somewhere. reply andybak 1 hour agoparentprevThis was one of the first lessons I learned about good UX design and was the canonical example when discussing what Mac OS classic did right and Windows did wrong. I think it was Norman Nielson thing or one of those old school gurus. How are people allowed to work on UIs without learning the core syllabus? The basics of their trade? I grew up on this stuff and I'm not even a UX specialist or a UI designer. Or are they getting overridden by bad product managers and other shitty stakeholders? reply internet101010 1 hour agorootparentThey are being overridden by people trying to justify their jobs by changing things for the sake of changing things. reply syncsynchalt 24 minutes agorootparentprevSee also Fitt's Law (https://en.wikipedia.org/wiki/Fitts%27s_law) and with regards to Apple OS design, Bruce \"Tog\" Tognazzini (https://en.wikipedia.org/wiki/Bruce_Tognazzini), now at Norman Nielson Group. reply esskay 53 minutes agoprevTouchscreens are perfectly fine on phones, tablets etc. But for something like a car it takes a special kind of idiot to implement a touch only way of controlling things like heating/ac, volume, etc. Even for certain audio controls it makes no sense. My (fairly old now) Toyota's touch screen is needed for switch between radio and usb (no carplay/android auto), even thats annoying to use. reply m463 47 minutes agoparentTesla is the worst. why would you want to select your gear on the touchscreen? I wonder how many sales they lose on the new models because the turn signal stalks are gone? (all stalks) reply __turbobrew__ 38 minutes agorootparentA coworker told me a story where they drove a Tesla to Tahoe Lake and it started snowing. The Tesla sensors did not pick up the snow so the windshield wipers never came on. After nearly crashing the car because they couldn’t see, they pulled over and it took them a long time to find out how to turn on the wipers through the touch UI. I hate hate hate non-analog controls in cars. reply m463 21 minutes agorootparentI rented a model 3 from hertz a while back. First time in a model 3, and I couldn't figure out how to lock the car. I finally figured out how to lock it on the touchscreen, but then I would open the door and get out and it would unlock again. I finally figured out two ways to lock the car, but it took a bunch of web searches to get it. reply m348e912 2 hours agoprevOne thing that would really get me to consider buying a Tesla is to add a few high quality _assignable_ knobs and controls that I could configure to control radio volume, heat, or whatever function I'd like. (within reason) Oh and real indicator stalks, that would be nice too. reply m463 40 minutes agoparentPeople with older teslas don't want to get the new ones. They did away with all the stalks. The car guesses which direction you want to drive. Turn signals are buttons on the (rotating) steering wheel (or yoke). The worst is that the touchscreen has very tiny targets. There's nowhere to rest your hands, you have to stab at them from the driver's seat (in a moving car) sigh. reply modeless 2 hours agoparentprevThere are some third party buttons like that: https://www.google.com/search?q=tesla+buttons reply djaychela 57 minutes agoparentprev> Oh and real indicator stalks, that would be nice too. IMO that should be the law. reply electriclove 4 minutes agorootparentNah, let the market decide reply GenerWork 2 hours agoparentprevEnhance Auto has intriguing products that may be right up your alley[0]. That being said, they're obviously aftermarket and not OEM. Last I heard they were working on aftermarket stalks, but I'm not sure where they're at on that project. [0] https://enhauto.com/knob reply amluto 2 hours agorootparentThey lost me at: > The S3XY Knob comes with a Gen2 Commander, which adds unique automation to your Tesla, such as automatically restarting your Autopilot after a lane change and turning off the wipers during AP drives. [emphasis added] At what point should a company that builds products like that be liable for the damages they encourage? For that matter, reckless endangerment and involuntary manslaughter are crimes in many jurisdictions. reply hinkley 2 hours agoparentprevnext [19 more] [flagged] qwerpy 2 hours agorootparentThis attitude is a HN/reddit thing and the incredulity is a sign you spend a lot of time in that online bubble. The rest of us just buy what makes the most sense for us and our families. reply vouwfietsman 2 hours agorootparentIt really isn't though. Regardless of whether the above comment makes sense, Musk is just an incredibly public figure now, even in non-tech spaces. Also including the word family in your wording is weird, I guess this is part of American political & economical parlance. reply kibwen 2 hours agorootparentprevTrust me, none of my offline cohort use Reddit, and they're all aware that Elon Musk has spent $100 million+ backing the only candidate for president who's a felonious convicted rapist and buddy of Jeffrey Epstein. They're not opting for Teslas. reply hinkley 1 hour agorootparentMore directly: he actively contemptuous of the only demographic that buys electric cars. He thinks they’re losers, and dangerous. reply compootr 2 hours agorootparentprevif the product is the best fit for me, I don't really care that he's behind it reply miltonlost 1 hour agorootparentmust be nice to be able to ignore and not really care about fascists reply compootr 0 minutes agorootparentI get that you should vote with your wallet, but your $40k or so is pissing in the ocean for musk. he literally could not care less about your money if you want to hurt yourself more than you hurt him, go for it, but I like choosing what'll benefit me the most (and while emotions about CEOs have some weight, I don't base my entire decision on them) fwiw the product is okay (and they've got a pretty good network of charging gear so far) but I do enjoy my hybrid car with physical buttons! ta1243 1 hour agorootparentprevA car which doesn't have indicators? reply zwily 2 hours agorootparentprevGiven TSLA’s last earnings, lots of people are. reply MostlyStable 2 hours agorootparentprevAssuming that a Tesla is a vehicle that you would enjoy more than an alternative vehicle option or that would better fit your needs (which is presumably the situation if you are considering buying one), then the value you are giving up for yourself by going with an inferior-for-your-needs option far, far outweighs the money from an individual sale that goes to Musk. If you are willing to hurt yourself more than you are hurting Musk, then I guess sure, you do you. reply vouwfietsman 2 hours agorootparentAre you criticizing the entire concept of a boycott? reply hinkley 1 hour agorootparentThey always do. “If you don’t like it leave.”“Quitter! Voting with your wallet is so pointless.” reply MostlyStable 1 hour agorootparentprevI don't know why people feel this need to take the words that someone said and add an entirely new meaning to them. Notice that nowhere in my comment or the comment that I replied to does the word \"boycott\" appear (and the first comment doesn't even really suggest one). So much of the poor quality of discourse online is people doing exactly what you did: take a comment someone made and decide to interpret it in a way that makes the commenter seem bad and the person reading it feel good. Here is (in my opinion) the civil way to make your point: Yeah, it's probably true that the value to the individual is greater than the value that Musk gets (this is the entire concept of consumer surplus), but in some cases, an individual might feel it justified to take a hit for moral reasons, such as in a Boycott. You are replying to the thing I actually said and inviting further discourse rather than asking a rhetorical question that you believe (likely incorrectly) that you already know the answer to. reply vouwfietsman 23 minutes agorootparent> I don't know why people feel this need to take the words that someone said and add an entirely new meaning to them. I am genuinely curious to your definition of boycott and how that is different to what you and the original commentor are talking about. Note that this is not to say that I do or do not approve of a boycott in this case, rather I find it intriguing that you seem to argue that boycotts in general make no sense. Also note that my comment is a question, and though I understand it can be read sarcastically, was actually not meant as such and so the entire reply you made is mostly an adversarial situation playing out in your head. reply hinkley 57 minutes agorootparentprev> I don't know why people feel this need to take the words that someone said and add an entirely new meaning to them. This is playing games. Whether you intended it or not. I called for boycott. You dismissed it. GP called you on it and now you’re playing dumb. This worked in public discourse a long time ago but nobody buys it anymore. It’s a bad deflection. HN is better at it than Reddit but there’s still way too many people who pretend like their reply has nothing to do with the chain of conversation they attached it to. Some of that is how we display follow-ups, but I can’t fix that. reply lotsofpulp 1 hour agorootparentprevYes, because the alternatives all wanted $20k+ for nickel and diming on basic functionality that a Tesla has. It’s 2024, and somehow only Tesla or maybe Rivian has accessible video recording built in, even though all cars are covered with cameras. I went into car buying thinking I wouldn’t entertain a Tesla due to lack of Carplay, and came out never wanting to step into a dealership again. So many stupid games they want to play, I just want to pick my color and pay for the car on my couch on my phone and pick it up. reply Reason077 1 hour agorootparentAs much as I love CarPlay, I don’t miss it much when driving a Tesla because they’re one of the few car manufacturers where the built-in interface is just as good, and in some ways better, than what CarPlay gives you. The things I do miss are the ability to run 3rd-party apps, and a decent voice interface: Tesla’s voice interface sucks very bad compared to Siri. reply adventured 2 hours agorootparentprevYou give Musk money every time you pay taxes. Or if you're outside the US, anytime you touch anything economic that has ever been priced in US Dollars (since the US runs a huge budget deficit which it finances through printing dollars, aka debt monetization, which is partially paid for by foreigners who interact with the USD directly or indirectly). reply electriclove 5 minutes agoprevThe touchscreen in Tesla cars is amazing. And there are a few tactile controls on the steering wheel. reply mmooss 19 minutes agoprev> if we look at the 1800s, people were sending messages via telegraph about what the future would look like if we all had this dashboard of buttons at our command where we could communicate with anyone and shop for anything. I've read a bunch of history of computers and related technology, and I've never seen that. Where can I find it? (I don't doubt it; I want to read it!) It shouldn't surprise me: The telegraph made immediate, cost-effective wide-area communication possible, and of course people then weren't idiots (or we're not so smart) - some of them imagined future development and applications. reply TonyTrapp 2 hours agoprevJust in time. Yesterday I had to use a touchscreen-based card reader for the first time to pay for something. What a jarring interaction. Impossible to use muscle memory, so I actually had to think what my PIN was and had to look at the screen the whole time, being stressed about pressing just a bit too much to the left or the right so that the wrong digit would be entered. I very much prefer classic card terminals, thank you very much. reply lifestyleguru 1 hour agoparentDid it display an ad before displaying the keyboard? Because I encountered terminals which have physical keyboard but also display an ad on the screen. No physical keyboard? A perfect captive audience. reply liendolucas 43 minutes agoprevFinally, also note that an LCD screen is not needed at all in the driver's console. Analog indicators for speed, rpms and simple lights are just fine. What I would really really like to have on all vehicles is an error LCD screen that describes with full and clear details any type of malfunction. We're still stuck with error codes but hey we give owners all this fancy and unnecessary digital toys and when a problem araises we need to plug a scanner to decode what's going on with our vehicles. reply fasteddie31003 2 hours agoprevI'll be the contrarian and say I prefer touchscreens. To get some system into a touchscreen you need to digitize the whole system which allows you to control it through automation which creates a more versatile system. The system could be digitized and then have a physical control to change the state, but then it's not necessary at that point. reply purplethinking 21 minutes agoparentAlso, when it comes to cars, and probably other devices/vehicles in the future, they are increasingly operating themselves. You can buy FSD for Tesla and drive for hours in mixed highway and city streets without having to intervene. When you do intervene you can take control for 15 seconds and then give back control to the system. At that point, why put in buttons to optimize the experience for human drivers? This is true for other cars as well, but to a lesser extent, but the direction is clear. reply mmooss 3 minutes agorootparent> At that point, why put in buttons to optimize the experience for human drivers? Less optimization results in more accidents, injuries, and deaths. reply vel0city 1 hour agoparentprevI'm pretty pro touchscreen to a point. Any driving critical control should be physical. Lights, turn signals, horn, steering wheel controls, etc. Physical controls with physical feedback. Everything the driver should mess with should be either on the wheel or immediately around it and should be physical. Other than that, I really don't care. When I'm punching in the address on the navigation system, give me a massive screen. When I'm stopped and trying to look up something in my media collection, give me a massive touchscreen. When I'm trying to quickly glance at the map, make it a giant screen so I can see it all quickly. Or better yet a HUD or have it on the instrument cluster. reply TomK32 2 hours agoparentprevThere's a interesting middle ground, programmable button that is also a rotary button that gives feedback, the KeWheel by KEBA. I'm sure that are similar solutions from other manufacturers. reply pdimitar 2 hours agoparentprev\"Could\" being the keyword here. We're not there yet. Also the touchscreens break muscle memory habits and don't give any feedback. These things are actually extremely important f.ex. in a car. reply jajko 2 hours agoparentprevYou probably meant other industry but this is a terrible mindset for cars for example. Touchscreens are so terrible premium manufacturers ignored them for a long time since its obvious downgrade in comfort and safety, yet people kept buying teslas despite this, even bragging how cool some cheap ipad is. reply m463 24 minutes agoprevI kind of wonder if touchscreens and apps are a way of firing customers. Get compliant customers that won't complain and will rent autopilot or buy range upgrades their cars already had the hardware for. reply gigatexal 2 hours agoprevYes I do prefer analog controls. Dials for heat. Open close flaps for vents. On off switches. Tangentially: the Tesla single giant glass console is in dire need of a UX designer to take the clutter out and make it far more usable. It’s here I wish that Apple had bought Tesla many many years ago: CarPlay as they have it now where it takes over the whole screen would have been amazing. reply bhauer 2 hours agoparent> Yes I do prefer analog controls. Dials for heat. Open close flaps for vents. On off switches. Dials and switches can be fully digital (e.g., dials can be free-spinning, without locks at each end of a setting). So preferring dials and switches seems reasonable. But flaps for vents are very difficult to automate. Returning to manual flaps in cars would mean losing modern cars' ability to associate and restore HVAC vent preferences with driver profiles. It would mean returning to the time when it was actually necessary to adjust the HVAC vents every time you swapped drivers. While setting vent preferences on the screen may take a second or two longer than manually setting them, thanks to the setting being associated with my driver profile, it's a set-once-and-forget-forever setting. The net time and annoyance savings is large. reply finnh 1 hour agorootparentThanks for explaining something I've never understood. I still think it is silly, tho - it makes sense only if each driver always wants vents pointing at the same place. my preferences change by season, by day, by hour, so needing to go through a screen is a time-loss and annoyance generator, not vice-versa. reply allears 1 hour agorootparentprevFor me there's no set-and-forget-forever setting. Depending on the weather, how I'm dressed, how many other people in the car, whether there's a smelly diesel truck ahead, etc., that's a setting I need to change all the time. reply bhauer 1 hour agorootparentI guess everyone is different, but what you described absolutely doesn't resonate with me. I never have adjusted my HVAC vents after their initial configuration. Winter, summer, whatever. I always want the air to flow the same way. reply m463 32 minutes agoparentprev> Apple had bought Tesla lol. I think tesla was copying apple, relentlessly removing without knowing when to stop. Apple has lost its way too in this respect. reply jstummbillig 2 hours agoprevLet it be known that (good) designers are fully aware of how bad touchscreens are, with regards to UX and many other things. It's just that touchscreens have been the least bad option, when you really need/want (always arguable, of course) to iterate a lot on the software, that is inside an expensive and not cheaply/easily modifiable piece of hardware. reply brudgers 2 hours agoprevHardware is a useful abstraction. reply dfxm12 1 hour agoprevI've got a new car. I got the giant touchscreen because the model with the advanced safety features only came with the touchscreen. However, thanks to all the buttons on the steering wheel (which are the same on all models), I have to touch the screen approximately zero times while driving. It might as well just be a display. That said, I am appreciative of people coming to their senses over this. Maybe not every car maker thought this out as much. reply internet101010 1 hour agoprevThey've been back. One of the main reasons I went with the car I ended up buying was because it had buttons. And it's fast. And it has carplay. And I don't have to press the (A) button every time I turn on the car to disable the engine off at red light thing. reply jordanmorgan10 1 hour agoprevI seem to be in the minority. I love the whole screen approach in my model 3. I can customize the bottom shortcuts how I like, the screen adapts to the context and things don’t feel more than 1 tap away. I’d take that over plasticy looking car buttons for the most part. reply mlhpdx 2 hours agoprevPhysical buttons don’t require an ad blocker. reply UniverseHacker 1 hour agoprevI’m into classic European cars and am horrified by the people replacing high end vintage german head units that integrate with the rest of the car, e.g. speed sensitive volume for shitty alibaba touch screens. reply kemiller 2 hours agoprevThere’s a pre/trans fallacy at work in here. We are not returning to the buttons we had before, we are recreating the role of physical buttons in a world where the long tail of controls has somewhere to go. And I’m all for it. reply teatro 25 minutes agoprevOof, I’m about to buy our new family car which has not a single knob anywhere, they all were replaced by those sensitive switches and sliders. reply anonymous344 1 hour agoprevthese idiots does not understand that in the car your hand is moving up and down because road is uneven. Touch screen sucks in car if the car is moving reply voytec 1 hour agoprevI need to dry my hands before clicking \"no longer exercising\" on my Apple Watch after swimming. It adds my steps through the beach to the towel as the distance swum but allows using the physical crown/button to eject water... reply RobotToaster 2 hours agoprevTom Paris was right. reply taylodl 14 minutes agoprevSanity prevails! reply James_K 2 hours agoprevThese stupid touchscreen controls are one of the main things that convinced me modern designers simply don't both testing and using the products they produce. If you take a touchscreen stove top and use it for more than about 5 minutes, you quickly find yourself wishing for the knobs back. reply 2099miles 1 hour agoprevEspecially in cars, especially in simple controls, touch screens are great for low screen real estate but cars are one of the dumbest places for them since there is so much real estate and so little need for a screen reply rkagerer 2 hours agoprevFinally reply elwebmaster 1 hour agoprevTouchscreens in cars should have been illegal to begin with it. How can it be that operating a cellphone is not allowed but operating a “tablet” is a necessity? reply electriclove 1 minute agoparentNo, it shouldn’t be illegal. If it is inferior, they will lose sales and money. reply vel0city 1 hour agoparentprevI'm not playing Call of Duty mobile or watching YouTube on the screen on my head unit. I'm not scrolling TikTik or having a text message conversation on a head unit screen. If you think it's the same thing, you haven't actually driven a car with a screen before. reply grugagag 1 hour agoprevIm surprised touchscreens ever passed compliance for automobiles, in some cases they’re a downright danger. reply vdvsvwvwvwvwv 1 hour agoparentTo the point where if the touchscreen happens to be loose, and have it's own battery I could lose my license for touching it (unless maybe it is cradled). reply mrgoldenbrown 1 hour agoprevOh Thank God. reply andsoitis 2 hours agoprevConspiracy by button manufacturers! More seriously, there are tradeoffs either way. Physical knobs give great feedback, require less cognitive load, and remain fixed. The latter is also where touch screens shine - the UI can evolve over time. In some settings touch screens are superior to physical buttons and in other scenarios it is the reverse. Choose the right button for the job. reply LocalH 2 hours agoparent> The latter is also where touch screens shine - the UI can evolve over time. This is not necessarily a benefit. Such interfaces often break muscle memory when they change, often with no choice to the user. At least manufacturers can't come in when you have physical controls and suddenly replace your control panel without consent because they have a \"better\" one. reply frde_me 45 minutes agorootparentQuite honestly, as long as the UX is _actually_ improving, I'm completely fine with having to adapt. I don't want to live in a world where things stay the same just because it's comfortable. Having said that, at least 50% of the time that people change the experience, it makes it worst. So I agree that for companies that don't know how to design interfaces, this is maybe a benefit. reply pdimitar 2 hours agoparentprevUI can evolve over time -- for appliances that need it. Almost none of them need it, and always always the \"UI enhancements\" are stuff nobody asked for, like 24/7 telemetry to servers that are gods know where. No thanks. Another commenter beat me to it but I'll just join him to reinforce their point: UI changes also break muscle which is something extremely important to have in a car and in your home appliances. People just don't enjoy relearning their own machines when they expect the job to be done with minimal cognitive overhead. reply ctoth 2 hours agoparentprevWhy would I ever want my oven or stove to evolve over time? reply RobotToaster 2 hours agorootparentYou don't want your oven to play ads for the latest peppermint and pineapple flavoured chicken tenders? reply beezlewax 2 hours agorootparentprevHow can you get your clients to pay an oven subscription otherwise? reply superposeur 2 hours agorootparentTruly a vision of dystopia. reply alamortsubite 2 hours agorootparentprevNot you, your overlords. reply greenchair 2 hours agorootparentoven overlords reply drivers99 2 hours agorootparentovenshitification reply xxr 2 hours agorootparentprevCooks love the sense of pride and accomplishment they feel when they unlock new modes and temperatures, and they really go nuts over learning about exciting new products and services by the appliance’s partners in a way that is uniquely targeted to them /s reply snapcaster 1 hour agoparentprevCan you point to a single instance where the UI scheme for _an appliance_ was evolved over time in a way consumers like? I understand what you're saying is theoretically possible I just can't think of any instance in which it happened reply parasubvert 37 minutes agorootparentTVs evolved from knobs on the device to buttons on a remote (or touchscreen). Washing machines evolved from finicky one way turn relay knobs to tactile bidirectional digital knobs with buttons for options (like extra rinses, prewash, temperature, etc) VCRs used to be so unusable they'd blink 12:00 because no one knew how to set the time. BluRay players and PVRs put everything on screen accessible via remote or mobile app. Smart door locks make it very easy to lock/unlock a door via phone or watch vs futzing with keys that can be easily lost possibly requiring a new lock. Much better for guests or families. Old dial or even digital thermostats were nearly impossible to properly schedule, modern digital thermostats use phones or websites, much easier (and also visualizes all your HVAC stats!) Smart lights let you group lights together independent of power wiring, change colors, etc Japanese in-seat toilet bidets with dashboards or remote are masterful compared to traditional bidets with faucets. Single lever faucets vs separate dial faucets for hot/cold water reply wannacboatmovie 2 hours agoparentprevEvolve? Or let a faceless company disrupt my workflow when they bundle UI \"enhancements\" with security updates? reply Rygian 2 hours agorootparents/my workflow/me driving down a highway at deadly speeds/ reply meindnoch 2 hours agoparentprevTouchscreens are a viable alternative to buttons only if the system can react to touches within at most 500ms. We have enough evidence now to conclude that only Apple and Google engineers are capable of such an undertaking. Everyone else should stick to physical buttons. reply lomase 1 hour agorootparentFor context I did development with a Teensy board and the library I was using for physical buttons claims to have 20 nanoseconds latency using the CPU interrupts. reply croisillon 1 hour agoparentprevbuilding kitchen appliances has been an incredible journey but we will be sunsetting all your appliances within 30 days, thanks for believing in us! reply JadeNB 2 hours agoparentprev> Choose the right button for the job. I think that the problem comes with what the article mentions in the first paragraph—there are some places where UI might evolve with time, but my kitchen appliances, my washing machine, and much of my car are not places where I expect new UI paradigms, or want them if somebody dreams one up. Sure, the pendulum will eventually swing back again the other way to too much skeumorphism, but for now I'm going to push reflexively for physical buttons first, and ask questions later. reply BoingBoomTschak 2 hours agoparentprev>The latter is also where touch screens shine - the UI can evolve over time. I think that also serves as a perverse incentive: no need to make it as perfect as possible the first time, you can always fix it later! Tech debt, coming to the controls of your moving 1~2 tons of metal, f yeah! reply MiddleEndian 2 hours agoparentprev>The latter is also where touch screens shine - the UI can evolve over time. Yeah no thanks. reply __MatrixMan__ 1 hour agoprevWhile we're at it, let's come up with a tactile way to connect wireless things. I'm so tired of hunting down all of my devices and disabling Bluetooth just so that when I turn on my headphones they connect to the appropriate device. I'd love to just touch the two things together and hear a beep to know they're paired. reply Gasp0de 2 hours agoprevNo one can ever have believed that touchscreens are a good method of operating anything without looking at it. reply JadeNB 2 hours agoprev> home appliances like stoves and washing machines are returning to knobs It can't come a bit too soon. My oven has buttons that aren't actually raised from their surroundings, and presses are registered via some sort of presumably fancy processing that I guess sounded slick when it was being pitched, but in practice means that it's very, very difficult to be confident that a button press will do anything, especially when fingers are greasy from cooking. Oh, and sometimes whatever processor it's using gets frozen up, so I have to turn it off and back on again. But, since it's hardwired, this involves toggling a fuse. I'm sure that there are many ways that this is a better oven than the one in the many-decades-old apartment where I used to live, but I never had to re-boot that oven. reply jsheard 2 hours agoparent> presumably fancy processing that I guess sounded slick I'm pretty sure that capacitive touch sensing is just cheaper than physical interfaces, it's more to do with corner cutting than being slick. All you need to create a capsense \"button\" is some traces on a PCB, they're essentially free if you're making a PCB anyway. reply JadeNB 1 hour agorootparent> I'm pretty sure that capacitive touch sensing is just cheaper than physical interfaces, it's more to do with corner cutting than being slick. All you need to create a capsense \"button\" is some traces on a PCB, they're essentially free if you're making a PCB anyway. That makes sense. Thanks! reply Zanfa 2 hours agoparentprevI love how my stove’s capacitive buttons sometimes don’t register when I’m using one hand to stir with a conductive spatula while trying to turn down the temp with the other until I let go of the spatula. reply bitwize 2 hours agoparentprevDishwasher, same thing. Half the time it won't register a press when I need it to turn on. Yet the cat can start a cycle when he decides he wants to have a climb. reply EMCymatics 29 minutes agoprevI can't believe they didn't get immediately cancelled after they put a hole in a navy destroyer. reply Apocryphon 1 hour agoprevCould a pullback from flat design and a revival of skeuomorphism be far behind? reply ktosobcy 1 hour agoprevFinally! reply dbg31415 2 hours agoprevQuick, get me my BlackBerry! reply AlienRobot 1 hour agoprev>Plotnick is [...] the leading expert on buttons and how people interact with them. Something must be wrong with me. This sentence would sound so lame to the average person and yet it sounds fascinating to me. I wish I had the title of \"the leading expert on buttons.\" I really LOVE how the WHOLE article is about BUTTONS BUTTONS BUTTONS. It really clears any doubt about her expertise. It's not an exaggeration. It's an actual leading expert on buttons! >The blind community had to fight for years to make touchscreens more accessible. It’s always been funny to me that we call them touchscreens. We think about them as a touch modality, but a touchscreen prioritizes the visual. Really interesting observation. In order to press virtual buttons, you have to look at the screen to figure out whether the button is (unless it's a full-width button at the bottom). Physical buttons generally don't require this in order to be pushed. They may still require this if the action the button performs depends on a state that is indicated by a screen, e.g. a menu where you have directional buttons to change the selected item. reply scotty79 1 hour agoprevI think touchscreens could be fine, even in cars if they limited inputs to broad swipes. As for visuals it should rely on simple colors to encode functionality and provide feedback during operation. The problem is feature creep where they want user to have so many functions that they have no choice but to use buttons and detailed graphics. I think if the smallest buttons they used occupied at least quarter of the screen and if screen would have corners that you can physically grab onto when you are pressing they could be mostly fine-ish. UX designers that design console experiences for visually impaired people would be the best people to create UI for cars. Although still not perfect. reply gatane 2 hours agoprevHell yeah, buttons are back baby reply willmadden 1 hour agoprevGood! reply surgical_fire 2 hours agoprevI wish this was true for cellphones as well. reply frde_me 53 minutes agoparentI disagree with this. The touchscreen on my phone allows for so much versatile applications than is possible with physical buttons. I really don't miss the days where applications had to retrofit their controls onto a fixed physical setting. Sure, maybe for dialling a phone number or texting it was better. But for everything else I do on a phone, give me a touchscreen. reply Reason077 1 hour agoparentprevIf physical keys were the way to go in smartphones, we’d all still be using BlackBerrys. If it’s a dumbphone you want, there’s plenty of models available with physical keys. reply add-sub-mul-div 2 hours agoparentprevMy Treo with a physical keyboard was the last mobile device I had that typing wasn't a chore with. Touch screen primacy has turned mobile devices from content creation to content consumption devices. reply wslh 2 hours agoprevMy first reaction after buying my Garmin watch was to disable the touchscreen since it already has buttons. For tracking different sports, the touchscreen adds a potential risk of accidental touches, which could affect measurements and performance. Plus, I'm not certain, but it may consume more battery. I chose this watch for its impressive battery life (including solar charging), so minimizing unnecessary battery use is important to me. On the other hand, I find it unnatural to have physical buttons on a tablet. My brain takes a moment to adjust to the fact that the volume up and volume down buttons on the iPad reverse their behavior based on the device’s orientation. I would also prefer if fingerprint detection on the iPad were integrated into the display, as seen in some Samsung phones. reply m3kw9 2 hours agoprevFor cars reply doublerabbit 2 hours agoprevNow if only add physical keyboards yo phones again... reply alejohausner 49 minutes agoprevWhat I really would like to see is a car with a full command-line interface with a qwerty keyboard built into the steering wheel. Then you could type > setgear r ENTER To put the car in reverse. Of course people on hn could just abbreviate that to > r ENTER using a ksh macro! But for newbie users we could have a 3 button mouse instead. /s reply ned99 2 hours agoprev [2 more] [flagged] ianschmitz 2 hours agoparent [–] You haven’t read the article, but you deemed it pointless. Nice. reply Consider applying for YC's W25 batch! Applications are open till Nov 12. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Some new car models are reverting from touchscreens to traditional knobs and buttons, a trend known as \"re-buttonization.\"\"",
      "Rachel Plotnick, an expert in this field, is gaining recognition for her insights into this shift in car design.\""
    ],
    "commentSummary": [
      "Touchscreens are being replaced by tactile controls to address accessibility issues, particularly for sight-impaired and older individuals with dry skin.",
      "Physical controls, such as those on Garmin's Edge 840, are favored for their efficiency and reliability over touchscreens, which can lack tactile feedback and responsiveness.",
      "The shift back to tactile interfaces is viewed as a move towards improved usability and accessibility, countering the cost-driven rise of touchscreens influenced by devices like the iPhone."
    ],
    "points": 385,
    "commentCount": 282,
    "retryCount": 0,
    "time": 1730644197
  },
  {
    "id": 42032638,
    "title": "If you need the money, don't take the job",
    "originLink": "https://bitfieldconsulting.com/posts/need-money",
    "originBody": "May 2 May 2 If you need the money, don't take the job John Arundel From My Horrible Career Your lovin’ gives me a thrill, but your lovin' don't pay my bills This is the fourth of a five-part series about my horrible career: how it started, how it’s going, and what I learned from making every mistake imaginable. My horrible career Not a real developer Master of my domain If you need the money, don’t take the job Will write for food The full series is now available as a book: My Horrible Career It’s late, and the campfire is burning low. My fellow content creator Zack Proser and I are sitting on a log, chowing down on s’mores and enjoying a few spine-chilling tales of terror about my horrible career. Well, not that horrible: I now make a living from at least two of my favourite activities, writing and mentoring. No luck so far getting people to pay me to tinker with their vintage Land Rovers, or to sit in a comfy chair eating chocolates and watching classic Doctor Who, but hope springs eternal. Last time, we talked about how I made the leap from salaried server-monkey to starting my own company. Independence is great, but it turns out that when you quit your job, you also stop getting the regular paychecks (why does no one ever tell you these things?) Money, like oxygen, is one of those things that you don’t really appreciate until you suddenly don’t have enough of it. Some of the biggest challenges for the newly independent developer or creator, indeed, revolve around money: how to get people to give it to you, how much to ask for, and how to be worth what you’re asking. Let’s get into that. Over to Zack. In our last chat, John, you froze my blood with a horror story about what happened when you unwisely took on a fixed-price contract. From what you said, it doesn’t sound as though a deal like that is good for either you or the client? It’s not. A fixed-price deal sets up the wrong incentive structure. It’s in the client’s interest to agree the lowest price possible, and then to squeeze the consultant for every drop of juice they can get. So that sucks. On the other hand, it’s in the consultant’s interest to do the quickest and cheapest work necessary to get the client to sign off on the job. After that, they have zero incentive to fix any problems that arise. It’s just bad news all round. So what would you recommend instead? When someone asks for a fixed price, I’ll explain to them why that’s not in either of our interests, and suggest that instead they pay me a fair hourly rate for what I do. And, so they know what they’re in for, I’ll give them a careful and realistic estimate of how many hours it’ll take to deliver what they’ve asked for. That gives them the cost control they need, but doesn’t commit them to the whole sum right away. Once we’ve agreed the terms, I can get started, and I have a strong incentive to deliver value for them right away. If they like the work I’m doing, and they want more, I’ll happily provide it: it pays me to do so! On the other hand, if after the first day or two they think “This guy’s no good,” they can just pay me off and wave goodbye: they’re not locked in for the long term to a vendor choice they’re already regretting. At the end of the job, if they want further changes or enhancements, they can have them, but not for free. I’ll estimate them, and then they can decide whether or not it’s worth the extra cost. That prevents a nasty case of “just one more thing” syndrome. One reason some companies don’t like to pay for consulting work by the hour is that they think it’ll incentivise the consultant to goof off. You know, take it slowly, invent problems and delays, anything to rack up a few extra hours. Sure, unscrupulous consultants will do that, but then it’s on the client to pick that up and get rid of them. It’s not a successful long-term strategy. Why not? I’ve heard it wisely said that there are two kinds of consultants: poachers and farmers. The poacher wants to make a quick kill and get clean away. The farmer, on the other hand, is in the game for the long haul. She shears the sheep closely, but gently, and nurtures it. She feeds it, cares for it, keeps it warm in the winter, and tends it when it’s sick. That way, it’ll be around for her to shear again next year. Flattering analogy. Okay, but I’ve worked on farms: you’ll never find a deeper love than a farmer has for her sheep. They’re literally her livelihood: if they do well, she does well. Sure, she might see a quick profit if she slaughtered them all this year, but then there’s no next year. Instead, she looks after the sheep, and they’ll look after her. Similarly, as a consultant you could strike fast, extract as much money from the client for as little work as possible, and then ride like hell for the hills. There’ll always be more unsuspecting clients for you to predate upon, won’t there? It seems tempting, but actually, it doesn’t work. Hunting for new clients all the time is difficult and expensive, and often it doesn’t work out: you put time and effort into meetings, proposals, and estimates, and half the time it just doesn’t go anywhere. Much better not to lose the old client in the first place. Also, sooner or later, the word about you will get around. One way or the other. So you should be the consultant that you’d want to hire. Repeat business is the best business, because marketing is hard. After a while, you won’t need to market your services. People will start coming to you. At that point you know you’re doing something right. I see the value of charging by the hour, but one thing I struggle with personally is knowing what to charge. Do you have any advice? Yes, that’s difficult. The best advice I ever got on this was “Think of a number, then double it.” If you don’t feel a little embarrassed when you tell them the rate, that probably means you’re coming in too low. If you always get a yes, then you’re definitely coming in too low. The tendency is always to under-price yourself. Partly because you’re modest and self-effacing (that’s one of the reasons we all love you), but also because you want the business. That’s another big mistake I made starting out. I thought “Well, I don’t really know very much. I’ll charge a low rate to reflect that.” Here’s a weird fact: businesses actually like paying a lot for consultants. It’s called the Chivas Regal effect. You know, if people see a cheap bottle of whisky, they’ll assume (rightly) that it’s horrible. But if it costs sixty bucks a bottle, they’ll assume it must be a quality product. Otherwise, how on earth could the manufacturer justify charging that much for it? And the whisky actually tastes better because it costs a lot. It sure does. Cheers! Imagine a manager has some problem they can’t fix. What’s their response? Hire a consultant. A cheap consultant? Absolutely not. Only bad managers have cheaply-solved problems. The more they spend, the greater the kudos they’ll get from upstairs, and the bigger their budget will be next year. After all, the division with the most expensive problems must be the most important one in the company. And everyone can relax and feel good knowing that the problem’s being taken care of by the highest-priced consultancy firm available. Wait, what? I’m serious. When you’re on trial for murder, do you want to hire the cheapest lawyer in the phone book? I don’t think so. You go to the super expensive lawyer, the one that defends Tom Cruise against whatever the latest weird and concerning allegations might be. Okay, someone else, not Tom Cruise: I don’t want to hear from his lawyers. But that’s my point. So it’s about being reassuringly expensive. It really is. You have to actually be good at what you’re doing too, of course, or the whole scheme falls apart. What the client’s money really buys, though, is that delightful feeling of making the thing somebody else’s problem. You know, we’ve turned it all over to a top-tier expert, and now we just don’t have to think about it anymore. The more that person charges, the more reassured the client feels. It’s better for you, too, because when they’re paying you a lot of money, they put a lot of value on your advice. By contrast, the less they’re paying, the less they’re inclined to listen to you. You need them to listen to you, and that’s why you need to charge top dollar. In “Secrets of Consulting”, which is the first book everybody should read on this subject, the incredibly wise Jerry Weinberg says: Make sure they pay you enough so they’ll do what you say. That’s genius, but… how do you work out how much that should be? Here’s a good way to get to the right number: every time a client accepts your price, make it a little bit higher the next time around. When people start complaining, or turning you down, you know you’re getting into the ballpark. When your acceptance rate falls to about fifty percent, then you’re probably there. And don’t forget, the more clients you work with, the more you learn. The more you’ve learned, the better a consultant you are, and the more you can charge. So your price should steadily increase over time (but make sure it’s always in line with your value). What if they want to negotiate? You can negotiate on everything else, but don’t negotiate on price. Would you go to the cheapest dentist? Would you haggle with your brain surgeon from the operating table? I hope not. Some things just aren’t worth cheaping out on. Clients will still try, but you should politely decline. Clients who want a discount always turn out to be the worst clients, if you cave in to them. By offering you less, they’re really saying “We don’t think you’re worth what you’re asking.” You don’t want to work for someone who sees you that way. Plus, people who ask for discounts are habitual cheapskates. They think they’re smarter than everybody else: only suckers pay sticker price. They think you’re trying to take them for a sucker. If you accept their lowball offer, now they know they’ve got you on the hook. They’ll try to screw you in every other way, too: asking for extra things here and there, demanding shorter deadlines, out-of-hours support, every little deal-sweetener they can think of. Unless you really can’t survive without that client, don’t take them on. Jerry Weinberg also says “If you need the money, don’t take the job.” That’s great advice. I’m putting “Secrets of Consulting” on my reading list! What else? One common problem is that the client doesn’t know anything about the subject area: if they did, they wouldn’t need you. So they also don’t know what constitutes a fair price for the work. When they hear your rate, it may sound too high, and you won’t get the deal. But that’s okay. You just dodged a bullet, because you don’t actually want a client who doesn’t understand the value of what you do. That’ll never be a profitable engagement, in any sense of the word. Once someone got in touch with me and, when I quoted my rate, they said “Gosh, that’s so much more than we were expecting. We’d like to hire you, but there’s a guy in Eastern Europe who says he’ll do it for like 20% of that.” I said fine. Absolutely, go with the cheap guy. If he can do the same quality work as me for a fifth of what I charge, I told them, then it’s positively their fiduciary duty to hire him, which they did. A few months later they came back to me and said, “Now we understand why he was so cheap. Are you still available, by any chance?” And of course I was… at the rate I originally quoted. Business Jack does not play gentle. That ended up being a long and very rewarding engagement, and they were super happy with my work. Partly because it was good, if I say so myself, but also because they’d seen something of the cheaper alternative. They’d learned the value of what I do, and I learned that it’s okay not to let yourself be beat down on price. Things can still go squirrely even after you’ve landed the gig, too. You know, you give the client the benefit of your expertise, and they say something like “Thanks very much, but that won’t work for us. We’re going to do XYZ instead, because money / politics / reasons.” It’s easy to get upset in that situation, and start arguing with them or berating them. What kind of idiots pay for an expensive consultant and then ignore what they say? The commonest kind, it turns out. The fact is that they may just not be ready to hear what you’re saying. Sometimes they need a little time to think about it, and sometimes they need to learn what happens if they do it the other way. In fact, I’ve gone so far as to formulate what I call Bitfield’s First Law of Consulting: Sometimes the client needs to feel the pain of not taking your advice, before they’re ready to take your advice. It’s amazing how effective that can be. When they do come back to you, it’s often with a greatly increased respect for what you say next. “He said we’d crash and burn if we did X, and he was right! This guy really knows what he’s talking about. Let’s listen to him in a rapt, respectful silence.” I’m doing just that. Shall we take another natural break, and return for a final bite of your wisdom s’more later on? Sure thing. We’ll talk about how my consulting career ended up taking me somewhere quite unexpected, but that turned out to be exactly where I needed to be. See you next time. Next: Will write for food And you can read the whole series right now in this charming, fun-size ebook: My Horrible Career $9.95 Add To Cart John Arundel career, zack proser, my horrible career Twitter LinkedIn0 Reddit John Arundel John Arundel Cras mattis consectetur purus sit amet fermentum. Integer posuere erat a ante venenatis dapibus posuere velit aliquet. Aenean eu leo quam. Pellentesque ornare sem lacinia quam venenatis vestibulum.",
    "commentLink": "https://news.ycombinator.com/item?id=42032638",
    "commentBody": "If you need the money, don't take the job (bitfieldconsulting.com)220 points by gus_leonel 6 hours agohidepastfavorite134 comments neilv 3 minutes ago> Clients who want a discount always turn out to be the worst clients, if you cave in to them. By offering you less, they’re really saying “We don’t think you’re worth what you’re asking.” You don’t want to work for someone who sees you that way. Sometimes it's the job of someone, like in Procurement, to try to get a better price from a vendor, regardless of what value the company thinks they're getting. But unless they think you're a commodity that they know they can get from another vendor, they can't push back too much. Other times (IME as an independent technical consultant), the executive/manager hiring you, who has some sense of your value, has to make a case to someone else, about why your rate is worth it. This can be due to sticker shock, or can be something they have to justify on government contracts. So they might ask for help with this case, subtly or directly. Like putting together a bio, itemizing some credentials, or answering specific regulation-based questions to make sure you qualify for some grade or exemption. IME, that's never been a negotiating tactic, and always worked out, getting the work with no discount of rate. (Just anecdata; I don't claim to be an expert at the business of consulting. I generally delivered very high value, like few people could, and some clients got a great deal, while I also turned down a lot of work where they client thought they had commodity needs. I could've addressed this by growing a consulting firm, hiring \"associates\", who I supervised and mentored as a \"partner\". But, rather than sell difficult-to-value billable hours from a distance, I'd prefer to build tech startups, where I can be holistic on an entire product/business, and reap bigger rewards if successful.) reply ITB 4 hours agoprevAs someone that has been on the hiring side, I always prefer to build a full time team, even if that means paying for someone in my team to learn something new. The part that consultants don’t talk about is that you have to pay them to learn your code / company too. Nobody can just jump in and add value immediately. So you’re paying for onboarding hours, you’re paying for other employees to educate the consultant, and so on. Likewise as an engineer, I think the most interesting projects and companies err towards primarily full time people. So if my goal is to work on the most interesting projects, I’d want to work full time. A lot of consultants, for example someone helping with SOC2 compliance, is mostly copy pasting a large document with recommendations and moving some paragraphs around based on interviews with the team. Not different from your average lawyer helping with an estate plan or will. The reality is that for many things you do want the cheapest lawyer, because the project is simple and repetitive. It’s the uneducated that pay more. So yes for a murder trial I want to pay a lot for a lawyer, but for small things I don’t, and you shouldn’t either. I simply don’t buy the analogy. Finally, while consulting might be good for cash flow to hours-worked efficiency, it’s not clear it will make a person wealthier than other forms of income. I’ve met rich employees, founders, but not so much independent consultants. reply orzig 30 minutes agoparentI've been on both sides of the table for both full-time and consultants. The first rule of humans is that they vary widely within categories (even if there are statistically significant differences in population average), so I urge people to take all of the anecdotes on this page with many grains of salt. That said, here are some of my own, condensed across experiences: - An executive wanted to use ML, couldn't get enough political capital to build a full-time team (and didn't have the knowledge to interview candidates anyway). A very small consulting firm, sourced from high personal trust networks, got an MVP going. That led to a FT team getting built. Code was a mess but started the flywheel and after several years everyone agrees it was a great ROI - A large company had a tiny team using a technology that would be standard for FAANG but they had no support or training. Zero progress for about a year. Not important enough to throw huge money at but that link in the chain was becoming a bottleneck for the core product. A consultant came in explicitly as a multi-week trainer, and but the team just never got up to speed. Consultant's code examples grew until they were just direct bugfixes, and though nobody was happy, at least the crisis was resolved. - A product finally reaches product market fit. Executives decide to hire only after the team is drowning in tech debt. But it has to be full time, and it has to be junior. It takes months, then requires training. Tech debt is twice as bad as the original crisis level when the new hire becomes value positive. For some bizarre reason he leaves after exactly 2 years. Many consultants have a branding problem: Their biggest victories are fixing a disaster that their clients would rather not talk about publicly. Hard to put on a billboard, if you ever want work from that client again! reply aerhardt 3 hours agoparentprevRegarding the last point: maybe you're based in the US and you know some rich employees there, but I don't know a single one where I live. Consulting is very hard - for the reasons you mention, and others - but in some places it has a much higher ceiling than being an employee. reply bradlys 2 hours agorootparentFor those who don't know - aerhardt lives in Spain. The US seems pretty distinct in that you can be a laborer and have quite high incomes. It's not unheard of to see people in major cities within the US having 7-figure incomes and still being an individual contributor in their field. Most of the \"consultants\" I've heard of outside of the US are only \"rich\" in that their incomes are high for their region (charging HCOL US rates while living in a place that is LCOL) or they're not doing labor anymore because they own a consultancy firm. In the US, I think consulting isn't worth it unless you are extremely lucky. You're more likely to see high income by trying to join FAANG. I've been one of these people with the 7-fig incomes in the US. Unsurprisingly, you'll see a lot of data points like mine here because HN is primarily focused on the US audience since YC is a US (primarily silicon valley) company. reply tonyedgecombe 3 hours agoparentprevI nearly always got a job offer at the end of my contracts. I never accepted them because the terms were always poor, they couldn't match my rates. Not only that but being a contractor offers some distance from the internal politics. Usually the internal staff knew the business well but were weak on the technology they were using. No amount of training seemed to fix that. The only downside for me was the travel which was why I used the downtime between jobs to pivot to developing my own product. reply up2isomorphism 2 hours agoparentprevThe problem is that in today’s climate, FTE is not really what it means. A FTE can also vanish at anytime, except it actually will ask you to pay more. And when a core tech leader left, there are also very few people can actually do any nontrivial code change to the repo. reply creer 1 hour agorootparentIt depends on the country, even region. For the US west coast, FTE has been gone for a long time. More than a generation. reply alephnerd 13 minutes agorootparentFTEs get stock packages, ESPPs, 401ks, employee healthcare plans, and bonuses. Consultants/Contractors do not. This is why consultants/contractors have to charge higher hourly rates. reply chiefalchemist 1 hour agoparentprev> The part that consultants don’t talk about is that you have to pay them to learn your code / company too. Why should they? That's a given. Of course they don't know your code. But it's likely they've seen more code than a typical employee and therefore have more experience to contribute. A consultant is a serior employee sans the commitment. To your point about time, consultants don't need 1 v 1s, quarterly / annual reviews, etc. I understand why your cynical. I've seen some half-ass'ed consultants as well. But I've seen just as many stale, complacent, and complicit employees. reply Nursie 3 hours agoparentprev> Nobody can just jump in and add value immediately. That depends on the project and how much domain knowledge the consultant/contractor has. I've certainly been able to contribute on day 1 on several of my gigs. > I’ve met rich employees, founders, but not so much independent consultants. May depend on the market, and what you mean by 'rich'. Consultants rarely get stock, but they often make out far better than employees. I've met very few employees that got anywhere near 'rich' but consultants in London (for example) can certainly become quite wealthy. reply 2-3-7-43-1807 3 hours agoparentprev> So you’re paying for onboarding hours, you’re paying for other employees to educate the consultant, and so on. You also pay for the psychological friction and pressure that the consultant has to suffer from the fact that customer expects them to be geniuses with magical powers and the project managers planning the time line according to that. reply noitpmeder 3 hours agorootparentHard to fault the customers for expecting that when that's how most consultants sell themselves, especially the shit ones. reply rikthevik 2 hours agorootparentYeah, it's tough to know who can actually deliver. I know one guy (a consultant now) who I would trust to Navy SEALs parachute into almost any technical problem and come up with a real solution. Any part of the stack, from building a physical hardware test bench, debugging firmware compiler bugs, desktop software development, to a scalable web stack. These people exist, and they're not cheap. However, it's worth noting that no one with skills like that works at Deloitte. reply AdrianB1 34 minutes agorootparentDeloitte does not hire great people. They would have to pay them great and corporate managers would rather cut themselves an arm than paying well a good employee, it is a matter of principles. reply 2-3-7-43-1807 1 hour agorootparentprevso, where do they work? reply ganoushoreilly 1 hour agorootparentSmall consultancies, usually self funded. They tend to eschew flash for substance. They're usually experts in their niche and stick with it. reply rikthevik 22 minutes agorootparentExactly. The guy I'm thinking of has a small number of very happy clients and he can choose who he wants to work with. It's funny that at the high end, it seems like things get easier. You don't have to haggle over an hour or two. You name your price and deliver super high value to people who are happy to pay. It makes me think of \"The Wolf\" from Pulp Fiction. \"It's about thirty minutes away. I'll be there in ten.\" reply 2-3-7-43-1807 3 hours agorootparentprevtake it easy, pal. consultants at least those in corps don't sell themselves they are being sold. reply agent281 3 hours agorootparentprevIf you're lucky and the consultant is experienced and has narrow scope, they can provide a lot of structure. They will also push back where applicable. If they have a wide scope and/or they don't have much experience, you are 100% correct that they have to figure it out like anyone else. reply znpy 59 minutes agoparentprev> As someone that has been on the hiring side, I always prefer to build a full time team, even if that means paying for someone in my team to learn something new. > The part that consultants don’t talk about is that you have to pay them to learn your code / company too. Nobody can just jump in and add value immediately. So you’re paying for onboarding hours, you’re paying for other employees to educate the consultant, and so on. as somebody who's been on the team, i agree on this. in my previous job my skip-level management insisted on handing off some crucial infrastructure work to external consultants. results: 1. we on the (internal) devops/sre team did not know how parts of our infrastructure worked 2. when changes were needed, the first lenghty step was to figure out what was built by the consultant, and how (the manager did not like to pay the extra hours, of course) 3. when help was needed to troubleshoot something along with developers, we (devops/sre) ended up playing the role of messenger, and stuff that could have take anything from 2 hours to two days ended up taking up to two weeks It was a frustrating job, so of course some people (me included) ended up leaving the company. I still see the value of consultants as people that can bring the knowledge you need when you need it. However if they're going to work on something long lived, the manager must be enforcing some proper knowledge transfer during the consultant employment and at the end of it. reply munchler 5 hours agoprevI was a consultant for nearly a decade and far preferred fixed-price contracts, on the rare occasion we could get them. When you are paid hourly, you work at the customer’s whim, and are often treated like an employee, or worse. You are essentially just a body for rent. When you have a fixed price, you retain full autonomy. It’s up to you to decide the schedule, as long as you deliver on-time and within budget. The customer can’t change scope without your agreement. You feel like an actual human being. reply _heimdall 4 hours agoparentI've found that whether I prefer fixed price or hourly depends heavily on the client. When the scope is clear and I have a feeling that the client won't be the demanding type with frequent, unplanned changes I often prefer fixed price. If the scope isn't clear though, or if I get a feeling there will be a lot of incoming change requests, I'll go hourly. reply billy99k 3 hours agoparentprevThis was the exact opposite, in my experience. Clients that want fixed-price contracts always have ridiculous deadlines and expected me to work 24/7. When I charge hourly, clients think very carefully before I work on something. The 12 hour days to meet some crazy deadline also disappears pretty quickly when you are paying for all the hours. I won't even accept fixed-price contracts anymore. reply ganoushoreilly 57 minutes agorootparentI only do fix priced contracts with a lock solid list of tasks. Usually it's smaller in scope and clearly defined. I spell out that any deviations are costs outside of the original pricing. As another poster mentioned, you very much have to be selective. A lot of time our first contract will be a fixed price with a client to show our ability to deliver, so in that instance we lose a little to gain a client. It's a balance and it's not an easy life for anyone. Consultants can be beaten and battered on behalf of an internal team and those that are good realize that in many instances that's what you're hired for. reply vitaflo 2 hours agorootparentprevAlso my experience after 15 years of contracting. The fixed-bid contracts were the worst because they attempting to squeeze as much as they could for that cost. After a few of them I stopped even discussing them as an option. If someone loses their autonomy as a contractor that's all on them, not the contract. I was always very clear that it was a B2B relationship with my clients. Hourly rates also meant that if projects went long, or other work became avail, I would usually just continue to work on it, which can become quite lucrative. Fixed-bid contracts always gave the client a reason not to sign another contract. Time and Materials contracts meant a 3 month contract could easily turn into a 9 month contract, no new contract necessary. reply alsetmusic 4 hours agoparentprevI worked at a company where they'd switched from hourly billing to fixed costs not long before I'd joined. The incentive for the client was that there was no reason for the company to inflate job hours. The incentive for the company was to encourage longterm contracts for ongoing work. Seemed like a win for that particular environment. A bit different from an individual consultant, but an interesting case of comparing two models. reply spwa4 3 hours agorootparentI've found multiple times companies switch back from fixed price as soon as managers realize it is a very easy way to extract a lot of money from their employer via their cronies. Fixed price sounds like a great idea, but if you can't trust management, even middle management, it will rapidly turn into a disaster. reply ta1243 3 hours agoparentprevI'm not a consultant/contractor, I have a fixed salary and have been an employee here for over 20 years. But wouldn't it better for both consultant and customer to agree regular \"gates\". Agree on $100k of work for 10 units of work or whatever, but agree that it's paid in instalments of $10k every time each milestone is passed, ensuring that any misunderstandings are understood sooner. The customer would benefit in that they don't have to wait 3 months to get a waterfall delivery which perhaps meets the spec, but not their requirement The consultant benefits in that they don't do 3 months of work and don't get a penny until the customer is happy reply kmoser 2 hours agorootparentA smart consultant will include milestones like this in the contract: not just milestones for what they are expected to deliver, but milestones for when the client is expected to pay them, as well as milestones for when third parties (and/or the client) are expected to provide additional input/data/opinions. The contract should also describe what happens when those milestones are not met, e.g. for every day that milestone X is not met, then milestone Y will be pushed back accordingly, or the cost for Z will go up by some amount. A well written contract should make it almost irrelevant whether you're getting paid hourly or for a fixed cost, because you (and the client) are covered either way. reply tonyedgecombe 3 hours agoparentprevI was always extremely wary of clients that wanted a fixed price contract. It seemed the more they pushed for it the more hidden issues there were waiting to bite you. Having said that sometimes it can work in your favour. I did one project for a pharmaceutical company where I just happened to have several pieces of knowledge that they needed and they were losing a lot of money waiting for a solution. The first thing they said to me when I walked in the door was \"we are so glad you are here, we have been looking for a solution for ages and are losing a lot of money every day\". Eight days work kept me going for the whole year. reply rascul 4 hours agoparentprevMy experience is different. I have full autonomy whether I am paid hourly or a fixed rate for the job. Often I'll prefer hourly if it's hard for me to get an idea how long it will take. reply youssefabdelm 3 hours agoparentprevCan also do both (kinda). Hourly and specify budget range. \"I think it can get done with this many hours\", then charge 30% up front of budget estimate and the rest of the hours on completion. reply 2-3-7-43-1807 3 hours agoparentprevCustomers always prefer fixed-price and it doesn't protect you from being treated like an employed or worse ... reply matsemann 1 hour agoparentprevMy experience is funnily enough the opposite. When on fixed-price, it was always an argument about if something was a bug (thus I had to fix for free), ambiguously specified or a change from original requirements (they would have to pay to change for the last two). I felt we always were adversary to each other. With an hourly contract, we're on the same team and try to use me for the most valuable stuff as long as you have me. reply datavirtue 4 hours agoparentprevYou also have full autonomy by the hour. If some manager gets confused send them a copy of the contract. reply karaterobot 46 minutes agoprev> What the client’s money really buys, though, is that delightful feeling of making the thing somebody else’s problem. You know, we’ve turned it all over to a top-tier expert, and now we just don’t have to think about it anymore. The more that person charges, the more reassured the client feels. The way it was phrased to me was \"they're paying for your confidence\". Confidence that you can just come in and get the job done without too much trouble for them. That was pretty much the best advice I got as a consultant. His advice about doubling what you think your rate should be is also good, and when my company started doing that, we didn't lose clients, instead we moved up-market and actually ended up working with better clients on much more interesting projects. I think both nuggets of wisdom are of a piece: act like you're an expert that is paid commensurate with your success rate. Of course, you have to back it up, but isn't that an implementation detail? reply andai 5 hours agoprev>The more they spend, the greater the kudos they’ll get from upstairs, and the bigger their budget will be next year. After all, the division with the most expensive problems must be the most important one in the company. I heard this is true in government, and it was presented as an example of why government has the wrong incentives, and private companies are much better: in a private company you get a bonus for being efficient and saving the company money, whereas in a government you get punished and they shrink your department. So I was surprised to see it presented here as a thing that happens in big companies too (but I've only worked with very small companies). reply Hasu 5 hours agoparent> So I was surprised to see it presented here as a thing that happens in big companies too It happens at small companies and medium sized companies too. I've seen it as both a contractor and an employee. Like much common wisdom, the idea that companies are efficient (and specifically more efficient than government) is a myth. reply lolinder 5 hours agorootparent> Like much common wisdom, the idea that companies are efficient (and specifically more efficient than government) is a myth. The claim as I've understood it is more that a private company puts control over efficiency more directly into the hands of the people whose money is being wasted. A government entity is typically nominally accountable to the taxpayer, but that accountability is through so many layers of indirection that it doesn't actually do anything. A privately owned company with a small number of owners will in theory be more responsive to monetary pressures. I think where this breaks down is with ridiculously large publicly traded companies where stock price has more to do with shareholders betting on the irrational behavior of other shareholders than it does with the actual performance of the company. When the owners of the company make money when the company makes money, it will typically straighten out its inefficiencies. When the owners of the company make money by buying and selling abstract financial instruments, not so much. reply rainsford 3 hours agorootparentI think there's another explanation as well, and it's also the reason why \"running the government like a business\" makes no sense even if private industry can be more efficient. Companies are profit driven entities and at the end of the day can look at net profit and see if it's positive or negative. This doesn't guarantee efficiency, but it gives you a pretty simple success metric that you can then work to tie everything else back to. This is a major oversimplification, but government doesn't even have that basic starting framework. Government functions cost money, but generally produce no direct monetary profit that can be measured against their cost. Of course government produces other outcomes, but since those outcomes are not monetary, you run into a units problem if you compare inputs and outputs. You can say how many crimes the FBI solves per million dollars of budget, but there's no easy way to measure a \"break even\" point where you have a net benefit. You can measure relative efficiency (solving more crimes for a given budget), but you can't say what the right target value is. Even if you could measure government efficiency the same way, the other issue is that companies can improve their efficiency by just stopping inefficient activities to focus on the efficient ones, while government agencies generally do not have that choice. The FBI could probably increase their efficiency by ignoring hard to solve crimes and focusing exclusively on the easy to solve ones, but that's probably not the right thing to incentivize. reply cogman10 4 hours agorootparentprevThe worst of both worlds is where the US has landed. The public private cooperation is government bureaucracy and accountability coupled with a private company that knows there's basically no penalty to overshooting a government contract \"What are they going to do, leave the road unfinished?\". There are natural monopolies that exist, road building, water, sewer, power. All these things shouldn't be controlled by private entities. reply derektank 5 hours agorootparentprevCompanies can be just as inefficient as the government, sure. But unless they have a monopoly, either natural or government-enforced, inefficient companies will go out of business while inefficient government agencies can linger on for a long, long time reply spicyusername 5 hours agorootparentOnly inefficient small companies go out of business. Inefficient large companies can also linger on for a long, long time. reply derektank 5 hours agorootparentI don't agree, though I guess it depends on how long long is. There are certainly some examples I can think of but I would argue most of them are actually natural monopolies in disguise, usually benefitting from network effects I 100% grant that large organizations by their nature are less efficient than small organizations due to lossy communication. And some companies have a minimum size due to the nature of their work, which places inherent limits on their efficiency. But they're still subject to competitive pricing from other, similarly large companies. reply addaon 3 hours agorootparent> I don't agree, though I guess it depends on how long long is. HP started the Grand Experiment two decades ago to determine how long it takes to destroy a large company if every decision is either incompetent or malicious, with little assistance from network effects… and the experiment is still running. reply jl6 4 hours agorootparentprevSome large companies acquire the characteristics of government (spending becomes remote from the source of funds; political cover from being too big to fail; lack of meaningful competition). So when large corporations become malign or inefficient, it can be because of how government-like they have become, and some kind of competition & markets authority should step in. reply moffkalast 5 hours agorootparentprevOne might argue there is no such thing as an efficient large company. They're just somewhat structured chaos, the larger, the more chaotic. reply snapplebobapple 5 hours agorootparentI would argue the pruning function becomes way more important (and way less used) the bigger a company gets and so there are very few large companies that are efficient, not none. Twitter comes to mind since they just had 70% plus of their workforce pruned as a likely efficient company (at least at serving social media pages, not at making money so far). I also think this is why government is the most dangerous power structure (they almost never prune anything and they have theoretical claim to 100% of the country's GDP through taxation. It would be better for us all if they were heavily restricted or just figured out how to prune effectively instead of just raising taxes all the time to support inefficient program spending) reply randomdata 3 hours agorootparentGovernment is just a makeup of workers who serve the population at large. The trouble is that the population at large can never come to agree on what to prune. I want this, you want that. You want me to give up this in the name of efficiency, I want you to give up that in the name of efficiency, but neither of us want to give up what we want so in the end we agree that if I can keep this, you can keep that, thus nothing gets pruned. reply moffkalast 1 hour agorootparentThat reminds me why corporations have an easier time pruning, They're not democratic, they are basically feudal. Like, there's a king on the top, he has his board of nobility, VP dukes, knight middle managers and the peasants who do all the work and own nothing. Whatever the king and nobility say is law, they're accountable to nobody (except for the pope/national government). reply randomdata 1 hour agorootparent> They're not democratic They are democratic, but usually of the weighted variety. Typically, he who owns more shares has greater say – although occasionally you will see other weighting methods. Government is more likely to consider each individual an equal shareholder, although not always. Corporations likely also benefit here from the owners generally having more care for the organization and a greater desire to see it succeed. If there is something that needs to change they will work to ensure that it gets changed as soon as a problem is identified. Most government shareholders would rather sit back and just hope that things work out. reply moffkalast 55 minutes agorootparentRight, this applies more to private held firms and late game startups than public corporations, but I would still expect that the higher you typically go, the more shares one owns on average, so the weighted average of that won't be too far off compared to the actual structure, minus external shareholders. We can imagine those as foreign kingdoms that the king owes money to :P Plus there is upwards mobility, whereas in typical feudalism there is none, but it is still funny to think about the suspiciously odd similarities. reply datavirtue 4 hours agorootparentprevYou guys are talking about the operational efficiency you can see in the trenches. The important efficiency has bubbled up to a bigger domain. reply pfdietz 5 hours agorootparentprevShrinking all the while, until they go out of business. reply acdha 4 hours agorootparentprevInefficient companies _may_ go out of business. The problem is that this only happens if there’s more effective competition and clear market pressure across the board. Consider Google: they’re highly inefficient in many areas producing entire applications which are written off not long after release and failing to capitalize on areas they had substantial edges in (e.g. AI), (arguably their last successful product launch was in the 2000s) with the net result that they employed a ton of people doing things which were not really tied to satisfied customers. This didn’t matter because they had a few business areas where they had massive profits despite not having a government-enforced monopoly which more than made up for those losses. Most of the tech giants have variations on that theme where they have many people who can report absolutely absurd internal inefficiencies but until that’s broken out on a balance sheet it’ll probably never change. Government is unique in two ways: the first is that it’s more public (which is good, but e.g. you’d be shocked if Comcast was audited at the same level) but the other is that much of that inefficiency is mandated by the same people who complain about it in public. For example, benefits programs are often structured to require expensive validation processes which cost more than the savings, and there’s intense pressure to contract everything out even though that process requires significant overhead. reply Fricken 5 hours agorootparentprevGovernments have to do things business won't touch because those things are too inefficient. reply MattPalmer1086 4 hours agorootparentYep, a classic case is governments have to serve all their citizens, including all the really problematic and expensive ones. Businesses don't have to do that. reply lazide 4 hours agorootparentprevEspecially after they’re done with them. (/s, kinda) reply matwood 5 hours agorootparentprevScale and inertia matters. It can take a very long time for a large companies inefficiencies to be exploited by others. reply insane_dreamer 4 hours agorootparentprev> unless they have a monopoly this is far too often the case -- i.e., they are the only business providing X in Y region; or while technically not a monopoly their market share is so large it takes a very long time to be displaced (i.e., Google) reply wisemang 3 hours agorootparentAlso consider oligopolies, for example the shit show that is Canadian telecom. reply passing_by_and 1 hour agorootparentprevThis is a claim without evidence and simply not true. There is no argument that private organizations are perfectly efficient. Rather, that there are built in feed back loops that drive towards efficiency. This includes bankruptcy as ultimate conclusion in some cases. Government on the other hand, has no such feedback loops and misaligned incentives which produce enormous fraud, waste, and abuse. There is no example of government being more efficient at any activity. Pick one, and there is a counter example in private industry doing it better for less. reply Kon-Peki 4 hours agorootparentprev> Like much common wisdom, the idea that companies are efficient (and specifically more efficient than government) is a myth. There is a book - Recoding America - that gives a behind-the-scenes look into government IT and the more recent changes that improve efficiencies and outcomes. It was published in the last 2 or 3 years and your local library almost certainly has a copy. FYI - the first chapter or two seem to imply developers are to blame for things that are truly the fault of leadership; power through that section and the rest of the book shows developers in a different light: perhaps she was setting up a strawman to make the change look even better. reply ok_computer 3 hours agorootparentprevI believe the argument for privatization of services is that you can switch providers in a marketplace, that should provide efficient options. So you can switch payroll and billing systems from ado to new without laying off a whole division and software rewrites. However in practice it leads to vendor capture and more interface layers of project and program managers and auditors. reply irjustin 5 hours agorootparentprevIt happens when the guys above can't accurately judge the problem below. So Many times it's due to having never dealt with the problem themselves, but not always. reply Kuinox 5 hours agoparentprev> in a private company you get a bonus for being efficient and saving the company money Ah ah ah, good joke. A manager in a company get promoted because more peoples work under them, not because they managed to keep the numbers low. reply inetknght 5 hours agoparentprev> in a private company you get a bonus for being efficient and saving the company money In a company, the bonus you get for being efficient is \"thank you\" (if you're lucky) and the CEO buying a new lamborghini. reply throwaway106382 5 hours agoparentprev> in a private company you get a bonus for being efficient and saving the company money haha, good joke every time I've saved company money and made them more efficient my reward was: more work for the same salary. reply matwood 5 hours agoparentprevLarge companies start to become indistinguishable from government. At a certain size it’s bureaucracy all the way down. reply ecshafer 5 hours agoparentprevFor management in companies, the key resume drivers are team sizes and budgets to show how “experienced” they are. There are definitely adverse incentives. A lot of empire building goes on as managers try and grow how much they oversee to move up. reply 7thpower 3 hours agoparentprevOne of the reasons expensive consulting agreements, especially those backed by strong brands, are so prized in large companies is because they offer career insurance paid for by the employer that covers the employee. If you are an employee who has decision rights you have far less to gain from 20% less cost than you do 10% more certainty and a strong name to hide behind. In most large companies, they often talk about entrepreneurial spirit, efficiency, ruthlessness innovation, and all of the other behaviors that made the company great, but the reality is that the incentives are much different once the founders have left and career managers have moved in. What’s left is a lot of people who make enough to not be hungry but are not rich enough that they will be okay if things blow up. If you want to grow in middle management your best bet is lots of headcount, a large operating budget, and lots of promises that will take years to be validated. reply sebzim4500 5 hours agoparentprevA lot of the stereotypical mistakes that governments made are also made by large businesses, but it's definitely to a lesser extent. reply chomskyole 5 hours agorootparentHaving only worked at large businesses, I'd be keen to see some data on this. I would think that the incentives in large organisations are similar? reply lazide 4 hours agorootparentLarge organizations have to at least pretend to try to make money - the gov’t has no such restriction. Also, the gov’t has a large scale, legally enforced monopoly. reply stvltvs 4 hours agorootparentGovernment orgs have public financial transparency that businesses don't. For example, when I worked for a public university, my salary was available on a publicly accessible website. Government budgets get a lot more outside scrutiny and pressure to avoid waste. It's not perfect. There's plenty of wasteful spending, but I wouldn't be surprised if research showed government and large businesses are similarly wasteful. reply n_ary 2 hours agoparentprev> In a private company you get a bonus for being efficient and saving the company money Private companies get bonuses for wasting money and besides the money wasters don't care because it is not their money. The unspeakable money burning that goes on in my workplace is insane. Infact, I have seen first hand that, people who wasted most money(despite no useful outcome) get promoted quickly by several ranks, which means anyone wishing for a promotion now finds elaborate ways to waste more money. reply dahart 3 hours agoparentprevThis is human behavior. It doesn’t matter if it’s private or public, when people get together in groups to do business there tends to be similar behavioral patterns and outcomes. This is why almost none of the stories of differences between governments and corporations are true; people are people. Also there are plenty of government cases of being rewarded for efficiency, and cases of corporate employees being punished for saving money. I’ve heard bunches of stories here on HN of programmers complaining about being demoted or fired or flatly ignored when economizing business processes. reply BeFlatXIII 3 hours agoparentprev> So I was surprised to see it presented here as a thing that happens in big companies too (but I've only worked with very small companies). Many companies are internally every bit as centralized and dysfunctional as the Soviet Union. reply wouldbecouldbe 4 hours agoparentprevAt some size a corporate and government have a lot of similarities. Much more then small vs big companies. Only difference I've seen is that companies do a big clean up every now and then, which helps although doesn't solve everything. Governments just keep pestering on. reply shortrounddev2 5 hours agoparentprevMy dad worked for at&t and he got a budget for treating clients to lunch and such during meetings. He only got as much as he spent, so if he had $1000 left over, he would get $1000 less the next year. Each year, he would send someone out on his team to just buy a ton of gift certificates for the most common restaurants they take clients to and blow their budget in one day reply brudgers 5 hours agoparentprevIn the US, governments frequently use the lowest bidder. For professional work there is often a rate table. reply deburo 4 hours agorootparentGovernment contracts are a snake pit. It's the first thing that disillusioned completely from government efficiency. It has an insane degree of collusion and overbilling. reply badpun 4 hours agoparentprevDemocratic government has an incentive to serve its constituents, while a company has an incentive to make profits for its owners. Even if companies are more efficient than government, they are more efficient at the things that only benefit a few people (the owners), and sometimes at the harm of everybody else. At best, a company is indifferent about public good. reply miohtama 5 hours agoparentprevWorst is defense industry Cost Plus contracts with guaranteed profit https://www.smalltofeds.com/2014/09/fixed-price-versus-cost-... No matter how bad you are, you end up making profit. Combines worst of socialism and capitalism. reply stackskipton 3 hours agorootparentAs someone who used to work in the industry, whole reason for Cost Plus is with more traditional contracts, there was a ton of paperwork showing the government wasn't getting ripped off which is obviously crazy expensive as well. So whole point of this fix rate system is government puts out \"We are going to pay X for Y with profit of Z, take it or leave it\" reply rm_-rf_slash 5 hours agorootparentprevAt least with cost plus the producers have an incentive to dedicate enormous assets to build the thing. Government contracting is murderously complex, and factorially moreso when it involves the military. Being off a few percentage points of the initial cost estimate can mean the difference between a profit or an 8 figure loss or more. If the profit insurance wasn’t in place, fewer contractors would take the risk. reply Dalewyn 5 hours agoparentprevThe logic here is: If a budget wasn't spent in its entirety (or they didn't go overbudget), they didn't need the money and thus less money can be allocated next time around. If they keep requesting more money than they ultimately spend, less weight (if any) should be placed upon their beancounters in subsequent allocations. It certainly makes sense, but the behaviour that logic incentivizes leaves much to be desired. reply bena 5 hours agorootparentIt does not make sense. It never made sense. It is patently stupid and it should only take five minutes to realize why. If I go to the store today and buy groceries, it’ll cost X. If I go tomorrow as well, it’ll cost Y. The two numbers are barely related to each other. And only because I don’t need the things I bought yesterday. And yes, that’s day to day. What about year to year? Last year, I did not need any repairs to my house. This year I do. A few years ago, I needed a new roof. Three different year, three radically different home repair budgets. The entire exercise is a refusal to due diligence of any sort. If you say $X/yr, then you don’t have to ask why. You don’t have to scrutinize reasons. Then everything becomes about adjusting this number without ever asking what it is being used for. reply shagie 4 hours agorootparentYou're looking at everything as one budget that you control. Consider instead that you're part of a co-op and your responsibility is to buy the groceries always. Someone else does the home repairs. Someone else keeps all the cars fueled. Someone else pays the utility bill. There are fifty different people with responsibilities. Sometimes there will also be \"You must spend $100 on supplies for an event on July 17th.\" On January 1, you get all the money that you will get for the year to buy groceries. How much do you ask the co-op board for? Government doesn't see it as one big account, but rather \"this is how much we get, this is how much we allocate to the different accounts.\" Home budgets work differently. There's one pool of money and if you have an expense you can skimp on another part of the budget. Had to get a plumber? Hold back on going to the movies for a few weeks. You can't do \"we had a natural disaster so we're short on money and not going to pay the prison guards for a few weeks.\" reply Atreiden 4 hours agorootparentprevI don't disagree that it results in broken incentives and otherwise undesirable outcomes. But to take the example of your roof - in a business scenario, you would amortize that expense over the number of years you expect the roof to last. 15 years means you'd take 1/15 (and probably multiply by the inflation rate to account for rising costs) and put that into your annual budget request. reply hnlmorg 4 hours agorootparentprev> And yes, that’s day to day. What about year to year? Actually year of year it makes more sense because anomalies are averaged out over time. I also think you’re being unduly unfair about the system. Yes it’s a garbage system but a lot of what you described is wisdoms that’s learned in hindsight. However when you consider that most of budgets are relatively static (eg salaries, cost of goods at a smaller scale, etc) it’s easy to fall into the trap of thinking that the costs of running a apartment could be relatively static year on year. In fact the idea of dynamic pricing is a relatively new concept. This doesn’t mean that the idea of budgets being relative to spend is a good one. Just that it’s easy to see how it was once perceived as a viable methodology. reply dooglius 5 hours agorootparentprevI don't have a background in accounting, but my understanding is that assets, and the depreciation of assets, should be included in the budget. So, the way to frame you examples would be that your costs are the food you consume from your fridge and pantry, not the transition of money to groceries; and the cost for your roof was its depreciation due to age over time, not the transfer of money to a new roof. (I'm not making a claim as to whether this is actually how budgeting is done, in practice). reply lazide 4 hours agorootparentprevYou’re thinking of it from the perspective of someone who cares about reality. At the scale most governments work, reality is impossible to actually care about. It’s too much detail, and too confusing. Instead, they allocate money, and then punish people when ‘the wrong things happen’. Dealing with the reality is someone else’s problem, further down the hierarchy. Also known as ‘shit rolls down hill’. reply Dalewyn 5 hours agorootparentprevThat's fine for most budgeting. This isn't one of them. Why? Because the monies being spent are taxes. There is a moral imperative to not waste a single cent of taxpayer monies, which means you as the guy writing the budget must ask what every single cent is being spent towards and must claw back (and subsequently refuse to budget) any pennies that are either unaccounted for or are unwarranted. (Yes, tax dollars are wasted and deliberately so in reality. That is another subject of debate.) Hence why the incentives are the way they are. reply api 5 hours agoparentprevI heard this once from a finance guy: If you want to convert someone into a cut-all-government right winger or libertarian, send them to work in government for a few years. If you want to convert someone into a communist, send them to Wall Street for a few years. Any sane person exposed to any human institution will typically walk away thinking \"burn it all to the f'ing ground.\" All human institutions tend to be loaded with self dealing, perverse incentives, and incompetence. We are not good at this. The only thing that checks it is companies going out of business or the government analog, which is struggle between agencies for funding and accolades. But as others have said large companies can muddle on for a long time and large anything can amass a lot of political and cultural cache and become too big to fail. reply einpoklum 4 hours agoparentprev> in a private company you get a bonus for being efficient and saving the company money You're more likely to get not much more than a pat on the back, and then be expected to work with the lower budget for eternity, even if your savings were a one-time overhead hack. reply nojvek 4 hours agoparentprevThe government is usually the largest company in a country that has a monopoly. reply cushpush 6 hours agoprev>So it’s about being reassuringly expensive. > It really is. You have to actually be good at what you’re doing too, of course, or the whole scheme falls apart. What the client’s money really buys, though, is that delightful feeling of making the thing somebody else’s problem. You know, we’ve turned it all over to a top-tier expert, and now we just don’t have to think about it anymore. The more that person charges, the more reassured the client feels. Great advice. reply dgfitz 6 hours agoparentI don’t know why, but I wish the author had done an sed s/scheme/concept Edit: to be clear, it’s a fine word choice. It just jumped out at me. reply gausswho 4 hours agorootparentMy understanding is this preference is a little known schism in Britishisms and American English. In American English, a scheme is an unscrupulous, nefarious plan villains make. Often accompanied with riotous laughter. In British English, it's more general. Like a plan, but with superficial consensus and often spreadsheets. Trees die, but people don't. You see this neutral usage in government discussion regularly. It also reminds me: Europeans speaking English often use 'simple' before they start demonstrating things. Often painfully non-obvious things. Really boxes my ears. If it was simple I wouldn't be asking for an explanation and now you're insulting me. reply alphabetter 3 hours agorootparentExactly this. As a British English speaker that works a lot with the US it was an early learning. In British English a \"scheme\" has no negative connotations. It's commonly used in all kinds of legitimate places - for example the company you work at will have a \"pension scheme\". In U.S. English it has a connotation that it is nefarious in some way. reply codersfocus 3 hours agorootparentprevNeither concept nor plan are appropriate alternatives for scheme. Let's not argue with the inventors of the language on word choice. reply mstade 5 hours agorootparentprevI agree. Scheme sounds like its some kind of evil plot, but truly it’s just a matter of mutual respect between the consultant and client, and – crucially – the consultant’s self respect. Poor word choice aside, the advice really is solid. I’ve been a consultant for nearly 20 years, the last 10 as an independent, and it took many years of screwing this up before coming to mostly the same conclusions. reply dceddia 5 hours agorootparentprevI felt the same. I took that as a bit of the author’s impostor syndrome showing through (perhaps feeling, even after all this time, that charging what he’s worth feels dishonest somehow). reply hn_version_0023 5 hours agorootparentprevSchemes are inherently dishonest IMO. That’s the entire reason. reply mrob 5 hours agorootparentThe references to Land Rover and Doctor Who make me think the author is British. In British English, \"scheme\" does not usually have negative connotations. reply hiatus 5 hours agorootparentprevAt least in the UK people use it to describe things like pension plans. I don't think it's inherently unsavory. reply wouldbecouldbe 5 hours agoprevFixed pricing put lot of risk on the creator, but hourly pricing there is no risk. I prefer a proper balance of partially fixed, with a limit and out clauses, and then maintenance and further development on an hourly basis. reply mrbombastic 4 hours agoparentCould you give some examples of out clauses and limits? This sounds very appealing to me as someone who has gotten bit by the “one more thing” loop but has had a hard time selling hourly. reply Sytten 5 hours agoprevThe missing piece is retainers. They are IMO the cheat code of consultants because you can stack them and they are more stable. Being \"available\" has a price that way too many consultants don't charge for. reply dionys 4 hours agoparentit's an interesting point, could you please go into a bit more detail on how you structure the retainer (certain amount of hours per month)? How many you can stack and so on? reply em-bee 3 hours agorootparenti managed to change a multi year engagement that was billed hourly or by day into a fixed retainer. not knowing what to charge i simply asked the client to tell me what it was worth to him. he chose the amount that i earned with him in the past year. i accepted that, figuring that i would renegotiate if the work got more, or simply not be available for more work than what the retainer was worth to me. what happened in practice was that the work got less. this is circumstantial of course. it happened mainly because we both moved cities and were no longer able to work face-to-face. arguably that made my work more efficient as i no longer had to spend time to travel to the office and i could parallelize multiple things that i was working on. my effective hourly rate ended up being somewhere around $300-500. you can't charge that when billing hourly. i happily would have worked two or three times as much without asking for more. reply 8organicbits 3 hours agorootparentprevI've had concurrent retainers, but I'd avoid over-stacking them. Retainers are a liability, you could have multiple clients ask for hours simultaneously, and you'll suddenly be very busy. Weekly retainers are better than monthly because they smooth out the hours. Personally, I divide my availability into contracts and the more concurrent contracts I have, the more I pad for overhead. So one 40 hr/wk contract vs three 10 hr/wk contracts. It's possible to stack more, as often retainers aren't fully used, but I think it risks the relationship with clients, which is worth more than over scheduling yourself. reply mrbombastic 4 hours agoprevThis all sounds very appealing on the consultant side but I am not very convinced on the buyer side. The argument that with hourly you can get rid of consultants faster might work on people with medium to large budgets but fixed pricing has the guarantee that at least at the end I have X or I don’t have to pay. People with small budgets often don’t even have the money to burn testing out a consultant with nothing to show for it. reply mckn1ght 3 hours agoparentI don’t know how it’s typically done but if I’m taking a fixed cost project as a consultant I’d ask for some portion up front. If you can’t even put up a portion or pay for the first few hours then how are you gonna pay for the actual project? If (not when) it starts making money? Might as well look for a “cofounder engineer” willing to work for equity. reply adamtaylor_13 3 hours agoprevI own a software studio that exclusively does fixed-price engagements. In fact, it’s a selling point to most of our clients. Hourly-billing benefits no one. reply asah 4 hours agoprevFixed price works great if the client's expectations are easy to meet and you've done the work many times before in a fraction of the time that others would take by the hour. reply bryancoxwell 5 hours agoprevThe image at the top of this post led me to believe this would somehow be related to Golang? Editing to add: I did enjoy it regardless. Informative and well written. reply Winsaucerer 3 hours agoprev> A fixed-price deal sets up the wrong incentive structure. It’s in the client’s interest to agree the lowest price possible, and then to squeeze the consultant for every drop of juice they can get. So that sucks. > > On the other hand, it’s in the consultant’s interest to do the quickest and cheapest work necessary to get the client to sign off on the job. After that, they have zero incentive to fix any problems that arise. It’s just bad news all round. I don't agree with this at all. If you give a fixed price contract that includes bug fixes for a period, then your incentives are well aligned. - You want to release robust code (to avoid warranty bug fixes), and your client wants that too. - You want to finish quickly to get final payment sooner, and your client wants it finished quickly too. - You set the price to what it needs to be and you'd be happy with, and if it's too rich for the client, you just don't take the job (does require you to be in a situation where you can reject underpriced projects, and requires you to be good at quoting). - Client shouldn't care if you took half the hours you estimated, and they don't need to know. You both agreed to a price that works for you both, so ideally both sides have had a winning experience. I have had experience with a client that tried to squeeze every drop of juice, and that sucked. They had no choice though because their budget was completely unmovable (came from grants). My thinking is that it's probably best to prune such clients when you can, and stick with those whose expectations align well with your way of working. Not every client tries to squeeze every last feature out of you. Hourly is easy for the seller, but it's a sucky experience for the client. It builds distrust, and unlike fixed price, I think it's actually hourly pricing that misaligns incentives. I also don't like the social aspect. E.g., where people question how you're spending your hours, or you have a chat about the weekend don't know whether to make it clear that you're not billing for that time, etc. reply cudgy 3 hours agoprev“And, so they know what they’re in for, I’ll give them a careful and realistic estimate of how many hours it’ll take to deliver what they’ve asked for. That gives them the cost control they need, but doesn’t commit them to the whole sum right away.” Do you get paid for estimating? To properly estimate takes a good deal of time. reply punduk 2 hours agoprevThat's exactly how companies that sell consulting work. No bargaining, you have to pay millions of dollars for enterprise software nonsense. reply osigurdson 5 hours agoprev>> You know, take it slowly, invent problems and delays, anything to rack up a few extra hours I think this is a potential moral hazard with agile / scrum / mob programming, etc. It can have a tendency to minimize risk for the consultancy while maximizing billable hours. reply throwaway98797 6 hours agoprevmeh sometimes fixed pricing makes sense i’ve paid fixed pricing and i’ve taken fixed pricing it depends on the situation my favorite way to charge is on a per month basis if they start to abuse you, push back, if you can’t or they are good at manipulation leave also different people have different preferences for some the tracking of hours is painful, for others it’s comforting and fair meet the client or the consultant where they are at reply mstade 5 hours agoparentFixed pricing can be fine, but requires a lot of due diligence with both parties, and much more detailed contracts and crucially – scope. Expectation management is key. (Btw, what you’re describing sounds more like a retainer which isn’t quite the same.) My experience is that fixed price only works for very short (days and weeks, not months) projects that can be scoped very strictly. Even then, there’s a lot of risk of miscommunication and uncomfortable discussions. My rule of thumb is that if I’m doing a fixed price contract, it can’t be longer than two weeks. This usually amounts to very targeted fixes, upgrades, research projects. Things that are typically not so hard to time box. reply calvinmorrison 5 hours agorootparentWe do tons of fixed contracts. I find that we pretty much hit expectations and we can do change orders. I think its reasonable for a project that's 3 months. But we scope up front and build that into the fixed bid. reply throwaway98797 5 hours agorootparentprevtrue it’s retaineresq i’ll roughly give you ~20 hours a week for the next 3 months but if you don’t use it you lose it granted if their utilization drops off too much the contract gets re priced or paused the most money i’ve made is when i can be comfortable having an uncomfortable conversation up front with occasional nope thrown and all that matters is that you deliver *visible* value reply jermaustin1 5 hours agoparentprevI've done fixed pricing, but I also have an hours cap on that fixed pricing. I communicate how many hours are left in the project. Its less a fixed price deliverable, and more pre-buy some hours for a discounted rate. If the project is delivered and there are hours left, I came out ahead, if the hours run out, but I'm only a few hours from delivered, I will throw those in for free just to finish out the project. reply throwaway98797 5 hours agorootparentyep fixed with an implicit or explicit cap reply kjksf 5 hours agoparentprevIn the context of the article, fixed pricing means pricing per project. Charging per month of work is not that. reply Brian_K_White 5 hours agorootparentThey didn't suggest it was. They offered it as something that is neither fixed nor hourly. reply YetAnotherNick 5 hours agoparentprevAlso, if you really want to maximize what you earn, specially if you work part time, is to charge fixed price. Because pricing not only depends on how much work needs to be done, but also how much the client wants it/how fast they want it, and the ability for client to pay. Some client care too much about pricing, some clients doesn't care at all. Even in second bunch, its hard to inflate hourly price. But for fixed pricing, there is almost no limit on how much you can charge. reply anovikov 1 hour agoprevTL;DR: If money is an object for a client, look for another client. reply pathfindershiva 5 hours agoprev [–] So you wrote the horrible career as book and pivoted to writing career :P reply Consider applying for YC's W25 batch! Applications are open till Nov 12. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author discusses the drawbacks of fixed-price contracts, highlighting that they often create poor incentives for both clients and consultants.- Emphasizes the importance of charging a fair hourly rate, providing realistic estimates, and ensuring clients value the consultant's work.- Advises against negotiating on price to avoid difficult clients and suggests setting a high rate to ensure clients take the consultant's advice seriously."
    ],
    "commentSummary": [
      "Fixed-price contracts may result in misaligned incentives, with clients pushing for more work and consultants doing the bare minimum.",
      "Hourly billing is suggested as a more flexible option that better aligns the interests of both clients and consultants.",
      "Retainers are highlighted as a way to provide stability for consultants, offering a consistent income stream."
    ],
    "points": 220,
    "commentCount": 134,
    "retryCount": 0,
    "time": 1730636244
  },
  {
    "id": 42031169,
    "title": "Speed, scale and reliability: 25 years of Google datacenter networking evolution",
    "originLink": "https://cloud.google.com/blog/products/networking/speed-scale-reliability-25-years-of-data-center-networking",
    "originBody": "Networking Speed, scale and reliability: 25 years of Google data-center networking evolution October 30, 2024 Amin Vahdat VP/GM, Machine Learning, Systems, and Cloud AI, Google Cloud Google Cloud Summit Series Discover the latest in AI, Security, Workspace, App Dev, & more. Register Rome wasn’t built in a day, and neither was Google’s network. But 25 years in, we’ve built out network infrastructure with scale and technical sophistication that’s nothing short of remarkable. It’s all the more impressive because in the beginning, Google’s network infrastructure was relatively simple. But as our user base and the demand for our services grew exponentially, we realized that we needed a network that could handle an unprecedented scale of data and traffic, and that could adapt to dynamic traffic patterns as our workloads changed over time. This ignited a 25-year journey marked by numerous engineering innovations and milestones, ultimately leading to our current fifth-generation Jupiter data center network architecture, which now scales to 13 Petabits/sec of bisectional bandwidth. To put this data rate in perspective, this network could support a video call (@1.5 Mb/s) for all 8 billion people on Earth! Today, we have hundreds of Jupiter fabrics deployed around the world, simultaneously supporting hundreds of services, billions of active daily users, all of our Google Cloud customers, and some of the largest ML training and serving infrastructures in the world. I would like to share more about our journey as we look ahead to the next generation of data center network infrastructure. $300 to try Google Cloud networking Build and scale your applications with high availability and performance. New customers get $300 in free credit on signup and all customers get free usage of 20+ products. Start building for free Guiding principles Our network evolution has been guided by a few key principles: Anything, anywhere: Our data center networks support efficiency and simplicity by allowing large-scale jobs to be placed anywhere among 100k+ servers within the same network fabric, with high-speed access to needed storage and support services. This scale improves application performance for internal and external workloads and eliminates internal fragmentation. Predictable, low latency: We prioritize consistent performance and minimizing tail latency by provisioning bandwidth headroom, maintaining 99.999% network availability, and proactively managing congestion through end-host and fabric cooperation. Software-defined and systems-centric: Leveraging software-defined networking (SDN) for flexibility and agility, we qualify and globally release dozens of new features every two weeks across our global network. Incremental evolution and dynamic topology: Incremental evolution helps us to refresh the network granularly (rather than bringing it down wholesale), while dynamic topology helps us to continuously adapt to changing workload demands. The combination of optical circuit switching and SDN supports in-place physical upgrades and an ever-evolving, heterogeneous network that supports multiple hardware generations in a single fabric. Traffic engineering and application-centric QoS: Optimizing traffic flows and ensuring Quality of Service helps us tailor the network to each application's needs. Integrating across the above principles is the foundation for our work. The network is the foundation of reliability for all other compute services, from storage to AI. As such, the network must fail last and fail least. To support this foundational responsibility, we rigorously define and monitor every bad minute1 across hundreds of clusters and millions of ports across our global network. Our progress on reliability is such that our in-house, software-defined Jupiter networks deliver a factor of 50x more reliability than prior versions of our data center networks. 2015 - Jupiter, the first Petabit network In a seminal paper, we showed that Jupiter data center networks scaled to 1.3 Pb/s of aggregate bandwidth by leveraging merchant switch silicon, Clos topologies and Software Defined Networking (SDN). This generation of Jupiter was the culmination of five generations of data center networks developed in house by the Google networking team. At that time, this data rate — in one Google data center — was more than the estimated aggregate IP traffic data rate for the global internet. 2022 - Enabling 6 Petabit per second In 2022 we announced that our Jupiter networks scaled to over 6 Pb/s, with deep integration of optical circuit switching (OCS), wave division multiplexing (WDM), and a highly scalable Orion SDN controller. These technologies unlocked a range of advancements, including incremental network builds, enhanced performance, reduced costs, lower power consumption, dynamic traffic management, and seamless upgrades. 2023 - 13 Petabit per second network We have further enhanced Jupiter to support native 400 Gb/s link speeds in the network core. The fundamental building block of Jupiter networks (called the aggregation block) now consists of 512 ports of 400 Gb/s of connectivity both to end hosts and to the rest of the data center, for an aggregate of 204.8 Tb/s of bidirectional non-blocking bandwidth per block. We support 64 such blocks for a total bisection bandwidth of 64*204.8 Tb/s = 13.1 Pb/s. This technology has been powering Google's production data centers for over a year, fueling the rapid advancement of artificial intelligence, machine learning, web search, and other data-intensive applications. 2024 and beyond - Extreme networking in the age of AI While celebrating over two decades of innovation in data center networking, we’re already charting the course for the next generation of network infrastructure to support the age of AI. For example, our teams are busy working on networking infrastructure needs for our upcoming A3 Ultra VMs, that feature NVIDIA ConnectX-7 networking, supports non-blocking 3.2 Tbps per server of GPU-to-GPU traffic over RoCE (RDMA over converged ethernet) and our future offerings based on NVIDIA GB200 NVL72. Over the next few years, we will deliver significant advances in network scale and bandwidth, both per-port and network-wide. We will continue to push the boundaries of end-host integration, including the transport and congestion control stack, and streamline network stages to achieve even lower latency with tighter tails. Real-time topology engineering, deeper integration with the compute and storage stacks, and continued refinements to host-based load balancing techniques will further enhance network reliability and latency. With these innovations, our network will remain a cornerstone for the transformative applications and services that enrich the lives of our users throughout the world while simultaneously supporting the groundbreaking AI capabilities that power both our internal services and Google Cloud products. We are excited to take on these challenges and opportunities to see what the next 25 years hold for Google networking! Further resources Jupiter Rising: A Decade of Clos Topologies and Centralized Control in Google’s Datacenter Network, SIGCOMM ‘15 [paper] Journey of the first Jupiter datacenter network leveraging merchant switch silicon, Clos topologies and Software Defined Networking (SDN). First deployed in production in 2012. Mission Apollo: Landing Optical Circuit Switching at Datacenter Scale, arxiv.org, 2022 [paper] First deployed in production in 2013. Orion: Google's Software-Defined Networking Control Plane. NSDI ‘21 [paper] Google's high-performance, scalable, intent-based distributed SDN platform used in both datacenter and wide area networks. First deployed in production in 2016. Jupiter Evolving: Transforming Google's Datacenter Network via Optical Circuit Switches and Software-Defined Networking, SIGCOMM ’22 [paper] Enabling technologies: OCS (2013), Orion SDN (2016), 200Gbps networking (2020), direct-connect topology (2017), dynamic traffic engineering (2018), dynamic topology engineering (2021). Swift: Delay is Simple and Effective for Congestion Control in the Datacenter, SIGCOMM ‘20 [paper] Swift, a congestion control protocol using hardware timestamps and AIMD control with a delay target, delivers excellent performance in Google datacenters with low flow completion times for short RPCs and high throughput for long RPCs. First deployed in production in 2017 PLB: Congestion Signals are Simple and Effective for Network Load Balancing, SIGCOMM ‘22 [paper] Protective Load Balancing (PLB) is a simple, effective host-based load balancing design that reduces network congestion and improves performance by randomly changing paths for congested connections, preferring to repath after idle periods to minimize packet reordering. First deployed in production in 2020 1. Any minute where a statistically significant number of network flows in the data center network experience a total or partial outage above a defined threshold. Posted in Networking Systems Related articles Networking Introducing an industry first: application awareness on Cloud Interconnect By Muninder Sambi • 7-minute read Containers & Kubernetes GKE and the dreaded IP_SPACE_EXHAUSTED error: Understanding the culprit By Ghadeer Shaaya • 7-minute read Containers & Kubernetes Leveraging Class E IPv4 Address space to mitigate IPv4 exhaustion issues in GKE By Basant Amarkhed • 6-minute read Networking Routing in Google Cloud: Where can I send my IP packet from a VM? By Manjuram Perumalla • 9-minute read",
    "commentLink": "https://news.ycombinator.com/item?id=42031169",
    "commentBody": "Speed, scale and reliability: 25 years of Google datacenter networking evolution (cloud.google.com)209 points by sandwichsphinx 14 hours agohidepastfavorite43 comments cletus 8 hours agoThis mentions Jupiter generations, which I think is about 10-15 years old at this point. It doesn't really talk about what existed before so it's not really 25 years of history here. I want to say \"Watchtower\" was before Jupiter? but honestly it's been about a decade since I read anything about it. Google's DC networking is interesting because of how deeply integrated it is into the entire software stack. Click on some of the links and you'll see it mentions SDN (Software Defined Network). This is so Borg instances can talk to each other within the same service at high throughput and low latency. 8-10 years ago this was (IIRC) 40Gbps connections. It's probably 100Gbps now but that's just a guess. But the networking is also integrated into global services like traffic management to handle, say, DDoS attacks. Anyway, from reading this it doesn't sound like Google is abandoning their custom TPU silicon (ie it talks about the upcoming A3 Ultra and Trillium). So where does NVidia ConnectX fit in? AFAICT that's just the NIC they're plugging into Jupiter. That's probably what enables (or will enable) 100Gbps connections between servers. Yes, 100GbE optical NICs have existed for a long time. I would assume that NVidia produce better ones in terms of price, performance, size, power usage and/or heat produced. Disclaimer: Xoogler. I didn't work in networking though. reply neomantra 3 hours agoparentNvidia got ConnectX from their Mellanox acquisition -- they were experts in RMDA, particularly with Infiniband but eventually pushing Ethernet (RoCE). These NICs have hardware-acceleration of RDMA. Over the RDMA fabric, GPUs can communicate with each other without much CPU usage (the \"GPU-to-GPU\" mentioned in the article). [I know nothing about Jupiter, and little about RDMA in practice, but used ConnectX for VMA, its hardware-accelerated, kernel-bypass tech.] reply CBLT 51 minutes agoparentprevI would guess the Nvidia ConnectX is part of a secondary networking plane, not plugged into Jupiter. Current-gen Google NICs are custom hardware with a _lot_ of Google-specific functionality, such as running the borglet on the NIC to free up all CPU cores for guests. reply virtuallynathan 5 hours agoparentprevThis latest revision of Jupiter is apparently 400G, as is the ConnectX-7, A3 Ultra will have 8 of them! reply ceph_ 2 hours agoparentprevFrom memory: Firehose > Watchtower > WCC > SCC > Jupiter v1 reply DeathArrow 10 hours agoprevIt seems all cutting edge datacenters like x.ai Colossus are using Nvidia networking. Now Google is upgrading to Nvidia networking, too. Since Nvidia owns most of the Gpgpu products, they have top notch networking and interconnect, I wonder if they don't have a plan to own all datacenter hardware in the future. Maybe they plan to also release CPUs, motherboards, storage and whatever else is needed. reply danpalmer 9 hours agoparentI read this slightly differently, that specific machine types with Nvidia GPU hardware also have Nvidia networking for tying together those GPUs. Google has its own TPUs and don’t really use GPUs except to sell them to end customers on cloud I think. So using Nvidia networking for Nvidia GPUs across many machines on cloud is really just a reflection of what external customers want to buy. Disclaimer, I work at Google but have no non public info about this. reply dmacedo 1 hour agorootparentHaving just worked with some of the Thread folks at M&S, thought I'd reach out and say hello. Seems like it was an awesome team! (= reply adrian_b 9 hours agoparentprevNvidia networking is what used to be called Mellanox networking, which was already dominant in datacenters. reply immibis 4 hours agorootparentOnly within supercomputers (including the smaller GPU ones used to train AI). Normal data centers use Cisco or Juniper or similarly.well known Ethernet equipment, and they still do. The Mellanox/Nvidia Infiniband networks are specifically used for supercomputer-like clusters. reply wbl 4 hours agorootparentMellanox Ethernet NIC got used a bunch of places due to better programmability. reply Kab1r 10 hours agoparentprevGrace Hopper already includes Arm based CPUs (and reference motherboards) reply mikeyouse 4 hours agoparentprevYeah there’s a bit of industry worry about that very eventuality — hence the ultra Ethernet consortium trying to work on open source alternatives to the mellanox/nvidia lock-in. https://ultraethernet.org/ reply timenova 10 hours agoparentprevThat was their plan with trying to buy ARM... reply HDThoreaun 8 hours agoparentprevI have to wonder if Nvidia has reached a point where it hesitates to develop new products because it would hurt their margins. Sure they could probably release a profitable networking product but if they did their net margins would decrease even as profit increased. This may actually hurt their market cap as investors absolutely love high margins. reply eru 8 hours agorootparentThey can always release capital back to investors, and then those investors can put the money into different companies that eg produce networking equipment. reply thrw42A8N 7 hours agorootparentWhy would they release money if they can invest it and return much more? reply alex_young 11 hours agoprevLike most discussions of the last 25 years, this one starts 9 years ago. Good times. reply eru 8 hours agoparentThe Further Resources section goes a bit further back. reply maz1b 7 hours agoprevPretty crazy. Supporting 1.5mbps video calls for each human on earth? Did I read that right? Just goes to show how drastic and extraordinary levels of scale can be. reply ksec 4 hours agoprevThey managed to double from 6 Petabit per second in 2022 to 13 Pbps in 2023. I assume with ConnectX-8 this could be 26 Pbps in 2025/26. The ConnextX-8 is PCI-e 6 so I assume we could get 1.6Tbps ConnextX-9 with PCI-e 7.0 which is not far away. Cant wait to see the FreeBSD Netflix version of that post. This also goes back to how increasing throughput is relatively easy and has a very strong roadmap. While increasing storage is difficult. I notice YouTube has been serving higher bitrate video in recent years with H.264. Instead of storing yet another copy of video files in VP9 or AV1 unless they are 2K+. reply jerzmacow 11 hours agoprevWow and it doesn't open with a picture of their lego server? Wasn't that their first one, 25 years ago? reply teractiveodular 10 hours agoparentIt's a marketing piece, they don't particularly want to emphasize the hacky early days for an audience of Serious Enterprise Customers. reply dangoodmanUT 5 hours agoprevDoes gcp have the worst networking for gpu training though? reply dweekly 3 hours agoparentFor TPU pods they use 3D torus topology with multi-terabit cross connects. For GPU, A3 Ultra instances offer \"non-blocking 3.2 Tbps per server of GPU-to-GPU traffic over RoCE\". Is that the worst for training? Namely: do superior solutions exist? reply reaperducer 33 minutes agoprevSpeed, scale and reliability Choose any two. reply teractiveodular 20 minutes agoparentWhich of those is Google's network missing? reply huqedato 9 hours agoprevnext [3 more] [flagged] dewey 8 hours agoparentJust like Apple is doomed…for the past 20 years. reply dmd 7 hours agorootparentA friend of mine bet me $1000 in 2004 that Apple would be out of business in 5 years. Best bet i ever made, but if only I’d put my winnings in AAPL! reply 486sx33 7 hours agoprevThe most amazing surveillance machine ever … reply belter 7 hours agoprev [–] Awesome Google... Now learn what an availability zone is and stop creating them with firewalls across the same data center. Oh and make your data centers smaller. Not so big they can be seen in Google Maps. Because otherwise, you will be unable to move those whale sized workloads to an alternative. https://youtu.be/mDNHK-SzXEM?t=564 https://news.ycombinator.com/item?id=35713001 \"Unmasking Google Cloud: How to Determine if a Region Supports Physical Zone Separation\" - https://cagataygurturk.medium.com/unmasking-google-cloud-how... reply tecleandor 5 hours agoparentMaking a datacenter not visible from Google Maps, at least on most big cities where Google zones are deployed, would mean making them smaller than a car. Or even smaller than a dishwasher. If I check London (where europe-west2 is kinda located) on Google Maps right now, I can easily discern manhole covers or people. If I check Jakarta (Asia-southeast2) things smaller than a car get confusing, but you can definitely see them. reply belter 1 hour agorootparentYour comment does not address the essence of the point I was trying to make. If you have a monstrous data-center, instead of many smaller, in relative size, you are putting too many eggs on a giant basket. reply joshuamorton 12 minutes agorootparentWhat if you have dozens of big data centers? reply traceroute66 2 hours agoparentprev [–] > Oh and make your data centers smaller. Not so big they can be seen in Google Maps. I'll let you into a secret. Well, its not really a secret because everyone in the industry knows it.... All the cloud providers, big and small, they use the same third-party colocation sites as everybody else. Sure, the big-boys have a few of their own sites. But that is mostly in the US, and mostly because they got some tax break in some state for building a datacentre in the middle of nowhere. But in reality you can count the wholly-owned sites on one hand, one-an-a-half hands at a push. In most countries outside the US however, all the cloud providers are in the same colo sites as you and I. You will not see their kit because they rent whole floors rather than racks or cages. But trust me, they are there, all of them, in the same building as you. AWS, Microsoft, Google all in the same building. So that's why (some of) the sites you see on Google Maps are so big. Because they are colocation campuses used by many customers, cloud and corporate. reply anewplace 2 hours agorootparent [–] This isn't even close to true. You can just go on Google Maps and visually see the literally *hundreds* of wholly-owned and custom built data centers from AWS, MS, and Google. Edge locations (like Cloud CDN) are often in colos, but the main regions compute/storage are not. Most of them are even labeled on Google Maps. Here's a couple search terms you can just type into Google Maps and see a small fraction of what I mean: - \"Google Data Center Berkeley County\" - \"Microsoft Data Center Boydton\" - \"GXO council bluffs\" (two locations will appear, both are GCP data centers) - \"Google Data Center - Henderson\" - \"Microsoft - DB5 Datacentre\" (this one is in Dublin, and is huuuuuge) - \"Meta Datacenter Clonee\" - \"Google Data Center (New Albany)\" (just to the east of this one is a massive Meta data center campus, and to the immediate east of it is a Microsoft data center campus under construction) And that's just a small sample. There are hundreds of these sites across the US. You're somewhat right that a lot of international locations are colocated in places like Equinix data centers, but even then it's not all of them and varies by country (for example in Dublin they mostly all have their own buildings, not colo). If you know where to look and what the buildings look like, the custom-build and self-owned data centers from the big cloud providers are easy to spot since they all have their own custom design. reply traceroute66 2 hours agorootparent [–] > This isn't even close to true. Bullshit. Yes it is 100% true. Why ? Because I work at many of these sites in many countries in my region. I walk past the offices where the Google staff sit. I bump into Amazon staff in the elevators and the customer break rooms. I walk past the Microsoft suites with their very particular security setup. All in the same buildings as $myCo's cages and racks. I'm not going to go into further detail for obvious security reasons, but when I say \"trust me\", I mean it. reply anewplace 2 hours agorootparentYea, you're not the only \"insider\" here. And you're 100% wrong. Just because you completely misunderstand what those Amazon/MS employees are doing in those buildings doesn't mean that you know what you're talking about. The big cloud players have the vast majority of their compute and storage hosted out of their own custom built and self-owned data centers. The stuff you see in colos is just the edge locations like Cloudfront and Cloud CDN, or the new-ish offerings like AWS Local Zones (which are a mix between self-owned and colo, depending on how large the local zone is). Most of this is publicly available by just reading sites like datacenterdynamics.com regularly, btw. No insider knowledge needed. reply traceroute66 2 hours agorootparent> Yea, you're not the only \"insider\" here. And you're 100% wrong. Let's just say you don't know who I am and who I have spoken to about it. Those people are very senior, and work under positions where by definition of their job role they absolutely know what's going on, and so they are under NDA and so I'm not going to go into any further detail whatsoever. Those people have confirmed what I have observed. And no, before you try to say it .... I'm NOT talking about security staff or the on-site cloud-staff I mentioned before. I'm talking about pay-grades many many many layers higher. reply anewplace 2 hours agorootparentWell those people lied to you then, or more likely there was a misunderstanding, because you can literally just look up the sites I mentioned above and see that you're entirely incorrect. You don't need to be under NDA to see the hundreds of billions of dollars worth of custom built and self-owned data centers that the big players have. Hell, you can literally just look at their public websites: https://www.google.com/about/datacenters/locations/ I am one of those \"pay grades many layers higher\", and I can personally confirm that each of the locations above is wholly owned and used by Google, and only Google, which already invalidates your claim that \"you can count the wholly-owned sites on one hand\". Again, this isn't secret info, so I have no issue sharing it. reply traceroute66 1 hour agorootparent> Well those people lied to you then, or more likely there was a misunderstanding You are crossing into the personal insult side chum .... I suggest you stop it right there. I don't know you. You don't know me. But what I do know is I was not lied to and there was no misunderstanding. Because I'm not relying on what one person or one company told me, my facts have been diligently and discretely cross-checked. I've worked in the industry long enough, I wasn't born yesterday... To add to the list of people I was NOT talking to, you can add people like greasy sales reps who might have reason to lie. I'm not stupid. By implication you are also insinuating multiple very senior people would collude to lie to me when they don't even work for the same company. I think even you would agree that's a nuts allegation to make ? You have no clue about the who, what, where and when of my discussions. You are trying to make me divulge details publicly of who I spoke to etc. which I'm not going to go into. Especially with a random Joe I don't know and certainly not on a public space such as this. End of story. reply anewplace 48 minutes agorootparentI'm not trying to make you divulge anything. I don't particularly care who you talk to, or who you are, nor do I care if you take it as a \"personal insult\" that you might be wrong. You are right that it would be nuts that multiple senior people would collude to lie to you, which is why it's almost certainly more likely that you are just misunderstanding the information that was provided to you. It's possible to prove that you are incorrect based on publicly available data from multiple different sources. You can keep being stubborn if you want, but that won't make any of your statements correct. You didn't ask for my advice, but I'll give it anyway: try to be more open to the possibility that you're wrong, especially when evidence that you're wrong is right in front of you. End of story. reply joshuamorton 19 minutes agorootparentprev [–] It is true that every cloud provider uses some edge/colo infra, but it is also not true that most (or even really any relevant) processing happens in those colo/edge locations. Google lists their dc locations publicly: https://www.google.com/about/datacenters/locations/ Aws doesn't list the campuses as publicly, but https://aws.amazon.com/about-aws/global-infrastructure/regio... shows the AZ vs edge deployments and any situation with multiple AZs is going to have buildings, not floors operated by Amazon. And limiting to just outside the US, both aws and Google have more than ten wholly owned campuses each, and then on top of that, there is edge/colo space. reply Consider applying for YC's W25 batch! Applications are open till Nov 12. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Over 25 years, Google has developed its data-center networking to achieve high speed, scale, and reliability, culminating in the fifth-generation Jupiter network architecture with 13 Petabits per second (Pb/s) bandwidth.- Key principles in this evolution include efficiency, low latency, software-defined networking, and dynamic topology, with significant milestones in 2015, 2022, and 2023.- Google plans to continue advancing its network infrastructure to support artificial intelligence (AI) with further innovations in network scale, bandwidth, and reliability."
    ],
    "commentSummary": [
      "The discussion covers Google's 25-year evolution in data center networking, highlighting the shift from older systems like \"Watchtower\" to the advanced \"Jupiter\" system, which supports high-speed connections up to 100Gbps.",
      "Nvidia's contribution to networking hardware is noted, particularly through their ConnectX Network Interface Cards (NICs), which enable efficient GPU communication with minimal CPU involvement.",
      "There is speculation on Nvidia's future role in data center hardware and a debate on the industry's dependence on their technology, alongside discussions on the scale and visibility of data centers, advocating for smaller, less conspicuous facilities."
    ],
    "points": 209,
    "commentCount": 43,
    "retryCount": 0,
    "time": 1730608154
  },
  {
    "id": 42030463,
    "title": "Security flaws found in Nvidia GeForce GPUs",
    "originLink": "https://www.pcworld.com/article/2504035/security-flaws-found-in-all-nvidia-geforce-gpus-update-drivers-asap.html",
    "originBody": "News Security flaws found in all Nvidia GeForce GPUs. Update drivers ASAP! If you have an Nvidia GeForce graphics card, you need to download the latest driver updates now. By Laura Pippig and Joel Lee Oct 29, 2024 6:47 am PDT Image: Nvidia Graphics card manufacturer Nvidia is currently issuing a warning to all owners of GeForce GPUs. According to an Nvidia security bulletin, several security vulnerabilities requiring urgent attention have been discovered in the company’s own display drivers and other software. A total of eight vulnerabilities are listed, all of them with a “High” severity rating. If you have an Nvidia GeForce GPU, you need to act now. The scope of the security flaws According to Nvidia, it’s possible for attackers to gain access to your entire system by exploiting one of the vulnerabilities. With this kind of access, hackers can not only infiltrate and execute malicious code in your PC, but also read and steal personal data. The vulnerabilities affect GeForce software, Nvidia RTX, Quadro, NVS, and Tesla, both under Windows and Linux. Nvidia hasn’t explicitly said whether any of these vulnerabilities are already being exploited in the wild. However, as all GeForce graphics cards are affected, it’s probably a far-reaching problem. Urgent steps for GeForce users To address these security vulnerabilities, you’ll need the latest Nvidia GeForce drivers with version 566.03 for Windows and versions 565.57.01, 550.127.05, and 535.216.01 for Linux. Nvidia also points out that some distributors also supply the necessary security updates as versions 565.92, 561.03, 556.35, and 553.05. For Nvidia RTX, Quadro, and NVS, update versions 566.03, 553.24, and 538.95 will address the security issues. You can search for the appropriate updates using Nvidia’s Manual Driver Search tool and download them directly. The latest drivers are also offered in the Nvidia App and GeForce Experience app. Further reading: Important Nvidia App tweaks for GeForce gamers This article originally appeared on our sister publication PC-WELT and was translated and localized from German.",
    "commentLink": "https://news.ycombinator.com/item?id=42030463",
    "commentBody": "Security flaws found in Nvidia GeForce GPUs (pcworld.com)192 points by wumeow 17 hours agohidepastfavorite128 comments DowsingSpoon 12 hours agohttps://nvidia.custhelp.com/app/answers/detail/a_id/5586 >NVIDIA GPU Display Driver for Windows and Linux contains a vulnerability which could allow a privileged attacker to escalate permissions. A successful exploit of this vulnerability might lead to code execution, denial of service, escalation of privileges, information disclosure, and data tampering What does “privileged attacker” mean on Linux? In my mind, “privileged” would mean they already have root, but in that case there’s nothing to escalate, right? reply l33tman 8 hours agoparentWondering the same. The same column lists a bunch of Windows-only CVEs where an unprivileged user can do stuff, so there has to be some difference between those (CVE‑2024‑0117 - CVE‑2024‑0121) and the headliner CVE‑2024‑0126 They mention hypervisor breaches further below, so could the CVE 0126 imply that a local root user on a shared GPU machine of some sort can break out of the virtualization? reply mmsc 6 hours agoparentpreva member of the 'video' group reply helsinkiandrew 10 hours agoparentprevPresumably it needs to be run by someone who already has access to the computer and gives them (or a program executed by them) root/escalated privileges. Although that might include running code from a webpage etc. reply hiddencost 9 hours agorootparentIncluding an advertiser on a webpage. reply vbezhenar 4 hours agorootparentDo you have proof of concept? If it would be exploitable from the browser, it would be huge. reply bzzzt 2 hours agorootparentIt's called exploit chaining. Browsers have remote code vulnerabilities all the time. Didn't take me more than 3 seconds to google for one example: https://github.blog/security/vulnerability-research/getting-... If you can execute code as the user this Nvidia flaw is all it takes to install a persistent rootkit. reply formerly_proven 8 hours agoparentprevThat one is probably only an issue for folks who care about the root/kernel distinction, but there’s a bunch of buffer issues with the user mode component (this runs inside your process when you use graphics APIs). Not enough details, but that could be potentially exploitable from e.g. WebGL/WebGPU reply advisedwang 14 hours agoprevIf I'm reading the bulletin right, then all the issues can only be exploited from code already running on your machine. So if you have a single user machine and aren't already owned then this is a non-issue and the verbiage in the title and PC World article are not warranted. The people that actually need to update are: * Multi-user systems with some untrusted users. * Users with malware on their system already (which could privilege escalate) * virtualization hosts of untrusted guests. reply TazeTSchnitzel 13 hours agoparentIf you use a web browser or play multiplayer video games then there will be code running on your system that interacts with GPU drivers that you haven't explicitly chosen to download and which could potentially exploit certain vulnerabilities. reply smolder 12 hours agorootparentThis highlights why we shouldn't let browsers (google) keep expanding their reach outside of the historical sandbox. It's almost like all the in-browser Java and Flash problems being repeated. They're creating security problems more than helping legitimate developers. WebGL was fine. Websockets were fine. WebGPU and the recently proposed arbitrary socket API are jumping the shark. Raw GPU access and TCP/UDP access are simply bad ideas from inexperienced people and need to be shut down. If you truly need that stuff I think the solution is to step up your game and make native applications. reply kmeisthax 11 hours agorootparentI'm not sure why WebGPU is a step too far but WebGL isn't? Every other API for using a GPU went the same direction; why should HTML be stuck with a JS projection of OpenGL ES while native developers get Vulkan? The security properties of both are very similar, Vulkan/Metal/DX12 just lets you skip varying levels of compatibility nonsense inherent in old graphics APIs. reply smolder 10 hours agorootparent> why should HTML be stuck with a JS projection of OpenGL ES while native developers get Vulkan? Because web browsers are supposed to be locked down and able to run untrusted code, not an operating system that reinvents all the same failings of actual operating systems. They should be functionality impaired in favor of safety as much as possible. For the same reason you don't get access to high precision timing in browser (a lesson that took a while to learn!), you shouldn't have arbitrary capabilities piled onto it. reply imiric 9 hours agorootparentThose are all historical remnants. Modern web browsers serve a radically different purpose than they did in the 90s. It doesn't make sense to even keep calling them \"web browsers\" since most people don't know what a \"web\" is, let alone \"browse\" it. Modern browsers are application runtimes with a very flexible delivery mechanism. It's really up to web developers to decide what features this system should have to enable rich experiences for their users. Declaring that they should be functionally impaired or what they \"should be\" without taking into account the user experience we want to deliver is the wrong way of approaching this. To be clear: I do think we should take security very seriously, especially in the one program people use the most. I also think reinventing operating systems to run within other operating systems is silly. But the web browser has become the primary application runtime and is how most people experience computing, so enabling it to deliver rich user experiences is inevitable. Doing this without compromising security or privacy is a very difficult problem, which should be addressed. It's not like the web is not a security and privacy nightmare without this already. So the solution is not to restrict functionality in order to safeguard security, but to find a way to implement these features securely and safely. reply andyjohnson0 6 hours agorootparent> Modern browsers are application runtimes with a very flexible delivery mechanism. Clearly this is true. But as someone with an old-school preference for native applications over webapps (mostly for performance/ux/privacy reasons) it irritates me that I need to use an everything app just to browse HN or Wikipedia. I don't want to go all hairshirt and start using Lynx, I just want something with decent ux and a smaller vulnerability surface. reply crazygringo 5 hours agorootparent> it irritates me that I need to use an everything app just to browse HN or Wikipedia But why? That feels like saying it irritates someone they need to run Windows in order to run Notepad, when they don't need the capabilities of Photoshop at the moment. An everything app is for everything. Including the simple things. The last thing I'd want is to have to use one browser for simpler sites and another for more complex sites and webapps and constantly have to remember which one was for which. reply ndriscoll 3 hours agorootparentSome of us don't use the web for anything other than websites. I'm honestly not even sure what people are talking about with some proliferation of \"apps\". There's discord/slack, and...? And chat was on the road to being an open protocol until Google/Facebook saw the potential for lockin and both dropped XMPP. I already have an operating system. It's like saying I don't need notepad to be able to execute arbitrary programs with 3D capabilities and listen sockets because it's a text editor. You also wouldn't need to remember what your generic sandbox app runtime is. Use your browser, and if you click on a link to an app, you'll be prompted to open the link using your default handler for that mime type. reply mihaaly 8 hours agorootparentprev> Modern web browsers serve a radically different purpose than they did in the 90s And that is a bad thing it was pushed this far! Exactly this is the argument here! reply imiric 7 hours agorootparentI don't necessarily disagree. But there's no going back now. There's a demand for rich user experiences that are not as easy to implement or deliver via legacy operating systems. So there's no point in arguing to keep functionality out of web browsers, since there is no practical alternative for it. reply andyjohnson0 6 hours agorootparentIf rich ux can be delivered in a web browser then it can be delivered in a native app. I'd assert that the reason this is uncommon now (with the exception of games) is economic not technological. reply imiric 5 hours agorootparentIt is partly economic, but I would say that it's more of a matter of convenience. Developing a web application is more approachable than a native app, and the pool of web developers is larger. Users also don't want the burden of installing and upgrading apps, they just want them available. Even traditional app stores that mobile devices popularized are antiquated now. Requesting a specific app by its unique identifier, which is what web URLs are, is much more user friendly than navigating an app store, let alone downloading an app on a traditional operating system and dealing with a hundred different \"package managers\", and all the associated issues that come with that. Some app stores and package managers automate a lot of this complexity to simplify the UX, and all of them use the web in the background anyway, but the experience is far from just loading a web URL in a browser. And native apps on most platforms are also a security nightmare, which is why there is a lot of momentum to replicate the mobile and web sandboxing model on traditional OSs, which is something that web browsers have had for a long time. The answer is somewhere in the middle. We need better and more secure operating systems that replicate some of the web model, and we need more capable and featureful \"web browsers\" that deliver the same experience as native apps. There have been numerous attempts at both approaches over the past decade+ with varying degrees of success, but there is still a lot of work to be done. reply ndriscoll 3 hours agorootparentEvery package manager I know of lets you install a package directly without any kind of Internet connection (I haven't tried much, but I've run into CORS errors with file URIs that suggest browser authors don't want those to work). They also--critically--allow you to not update your software. The web today is mostly a media consumption platform. Applications for people who want to use their computer as a tool rather than a toy don't fit the model of \"connect to some URL and hope your tools are still there\". reply chrisjj 4 hours agorootparentprev> And native apps on most platforms are also a security nightmare You make it sound like a web browser is not a native app. reply Sophira 1 hour agorootparentprevThe difference is in the learning curve. On Windows, making a native app usually requires you to install a bunch of things - a compiler, a specific code editor, etc - in order to even be able to start learning. Meanwhile, while that's also true for web apps, you can get started with learning HTML and basic JavaScript in Notepad, with no extra software needed. (Of course, you might then progress to actually using compilers like TypeScript, frameworks like React, and so on, but you don't need them to start learning.) There's always been a much higher perceived barrier to be able to make native apps in Windows, whereas it's easier to get started with web development. reply mihaaly 8 hours agorootparentprevExactly. Putting everything, I mean everything into the browser, and arguing for it, is stupid. It stops becoming a browser then and becomes a native sytem, with the problems of the native systems accessing the open wild all over again. And then? Will be there a sandbox inside the browser/new-OS for the sake of security then? Sanbox into a not so sandbox anymore? reply vbezhenar 4 hours agorootparentModern operating systems are bad and they are not going to be fixed. So Browser is another attempt at creating better operating system. Why modern operating systems are bad: 1. Desktop OS allow installation of unrestricted applications. And actually most applications are unrestricted. While there are attempts at creating containerised applications, those attempts are weak and not popular. When I'm installing World of Warcraft, its installer silently adds trusted root certificate into my computer. 2. Mobile OS are walled gardens. You can't just run anything, you need to jump through many hoops at best or live in certain countries at worst. 3. There's no common ground for every operating system. Every operating system is different, has completely different APIs. While there are frameworks which try to abstract those things, those frameworks adds their own pile of issues. Browser just fixes everything. It provides secure sandbox which is trusted by billions of users. It does not restrict user in any way, there's no \"Website Store\" or something like that, you can open everything and you can bring your app online within few minutes. It provides an uniform API which is enough to create many kinds of applications and it'll run everywhere: iPhone, Pixel, Macbook, Surface, Thinkpad. reply junon 8 hours agorootparentprevThat's exactly what ChromeOS is/was. Users hated it. reply chrisjj 4 hours agorootparentprevThis guy gets it 100%. reply chrisjj 4 hours agorootparentprev> why should HTML be stuck with a JS projection of OpenGL ES while native developers get Vulkan? Same reason kids should be stuck with Nerf guns while grownups have firearms. reply ChadNauseam 11 hours agorootparentprevI agree. I'm not knowledgable enough to say for sure, but my intuition is that the total complexity of WebGPU (browser implementation + driver) is usually less than the total complexity of WebGL. reply smolder 10 hours agorootparentWebGL is like letting your browser use your GPU with a condom on, and WebGPU is doing the same without one. The indirection is useful for safety assuming people maintain the standard and think about it. Opening up capability in the browser needs to be a careful process. It has not been recently. reply macguillicuddy 9 hours agorootparentIt's my understanding that the browsers use a translation layer (such as ANGLE) between both WebGL and WebGPU and a preferred lower level native API (Vulkan or Metal). In this regard I don't believe WebGL has any more or less protection than WebGPU. It's not right to confuse abstraction with a layer of security. reply smolder 8 hours agorootparentMy analogy was bad and I'd probably be wrong as you (and your sibling post) say to expect WebGPU to have any lurking dangers as compared to WebGL. I was mainly trying to express concern with new APIs and capabilities being regularly added, and the danger inherent in growing these surfaces. reply immibis 9 hours agorootparentprevThe translation layer is the safety layer. In principle, it's like running Java bytecode instead of machine code. reply modeless 9 hours agorootparentprevIt's clear that you know nothing about how WebGL or WebGPU are implemented. WebGPU is not more \"raw\" than WebGL. You should stop speaking confidently on these topics and misleading people who don't realize that you are not an expert. reply smolder 8 hours agorootparentI'd dispute that I know nothing. I'm not an expert but have worked with both, mostly WebGL. Anyways, sorry, it was a bad analogy and you're right, I don't know enough, particularly to say that WebGPU has any unique flaws or exposes any problems not in WebGL. I'm merely suspicious that it could, and maybe that is just from ignorance in this case. reply flohofwoe 5 hours agorootparentprevThat's incorrect, WebGPU has the exact same security guarantees as WebGL, if anything the specification is even stricter to completely eliminate UB (which native 3D APIs are surprisingly full of). But no data or shader code makes it to the GPU driver without thorough validation both in WebGL and WebGPU (WebGPU *may* suffer from implementation bugs just the same as WebGL of course). > Opening up capability in the browser needs to be a careful process. It has not been recently. That's what about 95% of the WebGPU design process is about and why it takes so long (the design process started in 2017). Creating a cross-platform 3D API is trivial, doing this with web security requirements is not. reply codedokode 4 hours agorootparentprevBoth WebGL and WebGPU should be locked behind permission because they allow fingerprinting user's hardware (also they provide the name of user's graphic card). And because they expose your GPU drivers to the whole world. reply cjk 5 hours agorootparentprevAgree wholeheartedly (and I used to work on Safari/WebKit). Cross-platform app frameworks have never been a panacea, but I think there may be a middle ground to be found between the web and truly native apps. Something with a shallower learning curve, batteries-included updating and distribution, etc. that isn’t the web or Electron. That said, I worry that it’s too late. Even if such a framework were to magically appear, the momentum of the complex beast that is the web platform will probably not slow. reply whywhywhywhy 5 hours agorootparentprev> I think the solution is to step up your game and make native applications Say goodbye to anyone supporting Linux at all in that case. These rare security issues are a small price to pay for having software that works everywhere. reply chrisjj 4 hours agorootparentRephrasing that: malware that works everywhere is a small price for software that works everywhere. It isn't. And there is no basis for your assertion these security issues are rare. reply vbezhenar 4 hours agorootparentprevTCP/UDP access is behind explicit prompt and it's basically the same as executing downloaded application, so I don't think that it's anything bad. Basically you either install software to your local system which does not have any restrictions or you use web application which still is pretty restricted and contained. reply chrisjj 4 hours agorootparent> TCP/UDP access is behind explicit prompt ... that is satisfied by a single click from malware or social engineering. Insane. reply vbezhenar 3 hours agorootparentWould you be happy with two clicks? Three clicks? Like what's the principal difference? As I said, you can download and run arbitrary application with one click today. And may be second click to confirm to operating system (not sure if it's always necessary). Insane thing is that arbitrary application has instantly full access to your computer. And web application still heavily constrained and has to ask about almost every permission. reply chrisjj 3 hours agorootparentI would accept zero clicks on a browser that I've installed without this dangerous feature and /with a promise no autoupdate will sneak it in/. The reason your web page has to be imprisoned in permissions is that it is a web page from just about anyone using access that the browser has given it without telling the user. reply chrisjj 4 hours agorootparentprevClose your eyes before you see WebUSB. Reckless and irresponsible in the extreme. reply _flux 1 hour agorootparentWebMIDI is cool, though! I updated my Novation Launchpad firmware with that. reply godelski 9 hours agorootparentprevIsn't WebGPU supposed to be containerized? So that it only access its processes, which are the computations it is running for rendering? I honestly don't know much but I had heard it was sandboxed. reply a2128 7 hours agorootparentIt's not uncommon that I go to Shadertoy and see strange visual artifacts in some shaders including window contents of other applications running on my system, potentially including sensitive information. It's difficult to make GPU access secure because GPU vendors never really prioritized security, so there's countless ways to do something that's wonky and accidentally leaks memory from something the app isn't supposed to have access to. You can containerize CPU and have strict guarantees that there's no way host memory will map into the container, but AFAIK this isn't a thing on GPUs except in some enterprise cards. reply flohofwoe 5 hours agorootparent> ... including window contents of other applications running on my system, potentially including sensitive information. If this is actually the case (which I doubt very much - no offense) then please definitely write a ticket to your browser vendor, because that would be a massive security problem and would be more news-worthy than this NVIDIA CVE (leaking image data into WebGL textures was actually a bug I remember right around the time when WebGL was in development, but that was fixed quickly). reply 2OEH8eoCRo0 2 hours agorootparentprevThey just keep promising more access to my machine but don't worry it is all totally secure! We promise! Yeah sure, when has that ever worked out? reply exodust 11 hours agorootparentprev> step up your game and make native applications. Each to their own but I consider native applications a step down from web apps. \"Screw it, I'm giving up my web app and will now pay Apple/Google the protection money and margin they demand to shelter within their ad-ridden ecosystem lock-in.\" ... yeh that's definitely a step down. reply smolder 10 hours agorootparentYou're talking like android and ios are the only platforms. The downsides of those platforms don't justify a web browser (which should be safe to use) granting excessive capability to untrusted code. reply nkrisc 7 hours agorootparentIf you’re targeting a mass market audience they often are the only platforms. For many people their phone is their only computing device. reply lodovic 9 hours agorootparentprevNeither Apple or Google are in scope for this issue. This is about NVidia GPUs. reply Thorrez 4 hours agorootparentAre you saying that WebGPU should only be supported on Android and iOS, because Android and iOS have more secure GPUs? Desktop browsers shouldn't support WebGPU (but should continue supporting WebGL)? reply highwaylights 7 hours agorootparentprevThis to me is the big risk here. A worm hidden in a game mod or something. I can see it staying in the wild for a long time too. How many of the people that are playing on these cards, or crypto mining, or doing LLM work, are really going to even find out about these vulnerabilities and update the drivers? reply gruez 2 hours agorootparent>This to me is the big risk here. A worm hidden in a game mod or something. Game mods are already barely sandboxed to begin with. Unless proven otherwise (ie. by manually inspecting the mod package), you should treat game mods the same as random exes you got off the internet, not harmless apps you install on a whim. reply fsflover 8 hours agorootparentprevThis is why Qubes OS, which runs everything in isolated VMs, doesn't allow them to use the GPU. My daily driver, can't recommend it enough if you care about security. reply thot_experiment 4 hours agorootparentWhat is your threat model that you chose to daily something so restrictive? reply ThePowerOfFuet 8 hours agorootparentprevGPU support for Qubes is coming. reply HellsMaddy 14 hours agoparentprevI wonder if it could be exploited via WebGL/WebGPU? reply fsckboy 13 hours agoparentprev>if you have a single user machine and aren't already owned then this is a non-issue if you have a single user machine and ARE already owned then this is REALLY a non-issue for you. reply ryukoposting 4 hours agoparentprevI wonder if single-user systems running something like a1111 should be concerned. Could the plugin system be an attack vehicle? reply gruez 2 hours agorootparentA cursory search suggests such plugins aren't sandboxed and run with the same privileges as the main program itself, so I'd definitely be suspicious of any plugin. reply ummonk 14 hours agoparentprevDoes that mean it cannot be exploited through WebGL? reply solardev 14 hours agoparentprevI wonder if this affects Geforce Now reply bpye 16 hours agoprevThe actual security bulletin is here - https://nvidia.custhelp.com/app/answers/detail/a_id/5586 As it points out, this is an issue with the driver rather than the physical GPU. reply kmoser 15 hours agoparentI can't find anything on that page or https://nvidia.custhelp.com/app/answers/detail/a_id/5586#sec... that describes whether Windows Update will update the drivers automatically. I just did a Windows Update and it's still got an NVIDIA display driver dated 3/9/2023. reply Hawxy 15 hours agorootparentNo, Windows Update's Nvidia driver set is usually years out of date and rarely gets updated. It exists as an emergency fallback and doesn't push out regular updates. reply kmoser 2 hours agorootparentWhich begs the question: is Windows' NVIDIA driver even affected by this recent flaw? Because I don't use my PC to play games and thus don't need anything more than run-of-the-mill graphics acceleration, I'm loathe to download NVIDIA's enormous drivers, which I assume contain extraneous features and utilities that are useless to me. reply baq 10 hours agorootparentprevIt’s also likely nvidia keeps submitting the driver and it can’t pass the 'is this not crap' step of the process. Wouldn’t surprise me in the least. reply chrisjj 3 hours agorootparentAnd there's the nub. Perhaps nVideo should request access to a dpecial 'this is less crap' process and help MS understand how dangerous is the previous crap it approved. reply chrisjj 3 hours agorootparentprevHow is this latest not an emergency, I wonder. reply Toorkit 15 hours agorootparentprevOn Windows you have to either download the driver updates manually, or install that GeForce Experience thing that keeps them up to date for you. reply esperent 14 hours agorootparentThere's a third party utility program you can use, it notifies you of new versions and lets you skip installing a lot of the bloatware like GeForce. I think it's called NV Install. reply mrec 10 hours agorootparentNVCleanInstall, maybe? I couldn't find anything called \"NV Install\". Personally I'm still running with the drivers that came with the box when I bought it in 2020. GeForce Experience is an abomination; besides the mind-boggling bloat, demanding that I create an account just to download a driver update really made me determined never to buy NVidia ever again. reply esperent 5 hours agorootparentYes, apologies I was on my phone earlier and didn't find it from a quick search. But I just checked my laptop and that's the one I'm using. I allows stripping out some telemetry and a few other things beside GeForce experience. reply 0points 7 hours agorootparentprevGeForce Experience is optional. reply Dalewyn 12 hours agorootparentprevGeForce Experience is optional. I can't fathom why people want to abstract something as simple as downloading the drivers straight from Nvidia and installing it, but then again people (perhaps rightfully) don't understand WTF a computer is. reply lukan 11 hours agorootparent\"I can't fathom why people want to abstract something as simple as downloading the drivers straight from Nvidia and installing it, but then again people\" I think I do understand WTF a computer is, yet at some point I also had a tool on windows installed, that automatically downloaded ALL of the drivers for all devices. Convenient, but the main reason I installed the thing was, because it could install drivers I did not even find on official websites. But just out of curiosity, if you understand what a computer is, why do you prefer manual labour and look down on people who automate things? reply Dalewyn 4 hours agorootparent>why do you prefer manual labour Because driver updates I didn't strictly need have historically ruined my day more often than not. No, I'm not grabbing this driver update either. My Nvidia drivers are years old but they work fine, and I have better things to do than troubleshoot borkage stemming from drivers I didn't need to fix. Remember: If it ain't broke, don't fix it. >and look down on people who automate things? The specific audience here should know better than to delegate updates (let alone updates for system components) to some nebulous automated and/or all-in-one construct provided by third-parties to the hardware/driver vendor. reply oefrha 11 hours agorootparentprevI download drivers straight from nvidia.com and it takes many steps: go to drivers page, choose product series, choose product (choices don’t seem to matter but who knows if that’s gonna change at some point), click start search, click on the search result to go to driver page, click download, run installer, click click click click click. It’s a hassle compared to updating just about anything else on my machines. I only do it because (1) GeForce Experience requires logging in with Nvidia account and seems to log me out every time; (2) when GeForce Experience updates the driver it seems to pause forever doing god knows what between finishing the download and starting to install. reply Dalewyn 11 hours agorootparent>(choices don’t seem to matter but who knows if that’s gonna change at some point) They do matter, you're just lucky(?) always using hardware and software environments that are always covered by the first thing on the list. If you don't want to specify details and run an installer, that's squarely a You problem. reply oefrha 11 hours agorootparentThat you “can’t fathom” why people want to automate a multi-minute chore is a “You problem”, really. reply mulmen 10 hours agorootparentprevI read that as “no matter what I choose I get the same file but I still make the correct choice because Nvidia knows better than I do.” reply Dwedit 12 hours agorootparentprevIn the past, GeForce Experience had game streaming functionality. Similar to VNC, but using hardware-accelerated video codecs, and supports joysticks and sound. GeForce Experience removed the game streaming feature in 2023, but the protocol was reverse-engineered, and there's compatible third-party tools for game streaming. Sunshine is the server, and Moonlight is the client. It works a lot better than Miracast. reply KaoruAoiShiho 4 hours agorootparentUmm no it didn't. The game streaming is still there and I use it literally every day to access my windows PCs from my macs. reply richardfey 11 hours agorootparentprevIt was working also on devices like the Shield. reply chrisjj 3 hours agorootparentprevPerhaps people understand that having to accept so-called fixes from the source of such a serious problem is highly stressful. reply whywhywhywhy 5 hours agorootparentprevThere’s some weird desire for product managers working on the windows platform to build “Dashboards” in front of the most simple things. reply chrisjj 3 hours agorootparentIt is advertising. reply fulafel 11 hours agoparentprevA driver, to be even more correct. reply Kon-Peki 15 hours agoprevWow, if you go to the Nvidia drivers website, they are still pushing the vulnerable 565.90 version as the \"stable version\" for creatives. It's only the gamer version that has been updated to 566.03 with the fixes. Incredible. reply exmadscientist 9 hours agoparentIt is widely believed that the two versions are completely identical, just on a different release cadence. The \"Studio\" version is also supposed to pass a wider set of tests, but it's still the same driver. The \"Game\" driver is really only important for newly released games needing fixes/patches/optimizations that just weren't available yet for the previous \"Studio\" driver release. At least, that seems to be the consensus among people who've tried to figure this out. There's no official word. reply Melatonic 26 minutes agorootparentThe studio driver also opens up some functionality that the game driver does not. reply Kon-Peki 2 hours agorootparentprevThe fine print on the security notice page says that there is a 565.92 version that some OEMs are pushing out themselves. I get that there is a different release cadence, but it’s simply not acceptable to do business as normal surrounding security releases. The driver page should either have that 656.92 available or disable the download link for the stable channel with a note on when they expect it to be available again. At the very least, some product manager should be fired. This is a legal liability, and no amount of click-wrap disclaimers will protect them if someone gets owned because of this negligence. reply kbmr 14 hours agoparentprevLet's see if they recommend the fixed version by Monday or Tuesday at the latest reply weird-eye-issue 14 hours agoparentprevRelax buddy reply 20after4 9 hours agoprevNo update is currently available for Debian¹. This vulnerability is marked as \"low priority\"² for Bookworm. 1. https://security-tracker.debian.org/tracker/source-package/n... 2. https://tracker.debian.org/pkg/nvidia-graphics-drivers reply zamadatix 15 hours agoprevAs is always the case all throughout every year: https://www.nvidia.com/en-us/security/ (enter \"GPU Display\" in the search filter box) reply thn-gap 5 hours agoprevThis just reminded me that the GeForce Experience (which is recommended to use the recording functions, and keep updating the drivers), requires an account. And the software is unable to keep your account logged in. It's outrageous to pay so much for a graphics card to later have this experience. reply iamateapot 4 hours agoparentThe NVIDIA App (beta), which, as I understand it, combines the GeForce Experience and the Control Panel doesn't require the login (for now?): https://www.nvidia.com/en-us/software/nvidia-app/ https://arstechnica.com/gaming/2024/02/nvidias-new-app-doesn... reply rubicks 12 hours agoprevTimes like this really bolster my distrust and hatred for those horrible dkms installers. Thank you, nvidia, for living up to my worst expectations. reply pjmlp 9 hours agoprevMost of them are our out of bounds memory corruption friends, which everyone keeps telling it is not an issue on their C and C++ code, only happens to others. reply xyst 4 hours agoprevI already have kernel level root kits installed for multiplayer games. This privilege escalation exploit is child’s play. reply SoftTalker 14 hours agoprevGPU monoculture showed its downside. reply tedunangst 12 hours agoparentBecause gpu drivers were so much better when there were six vendors? reply amelius 6 hours agorootparentNo, because we'd get some competition. On features like security. reply tliltocatl 5 hours agorootparentUnlikely. Security isn't really a competitive feature for a customer product (security theater, on the other hand…). Ever if an average user had ever cared about security, they would never be able to evaluate the claims until it's too late to matter. And as JBiserkov points out in a sibling comment, GPUs really prioritize power above everything else, it's not like either OpenAI or a gamer would be happy about reducing their TFLOPs/FPS for the sake of security. reply amelius 4 hours agorootparentMaybe, but GPUs are also used in virtualized environments (datacenters), where you can't ignore these security issues. reply JBiserkov 6 hours agorootparentprevThe 'S' in FPS stands for \"security\". reply amelius 4 hours agorootparentAnd where is the competition? reply amlib 4 hours agorootparentprevFamously Poor Security? :) reply anonym29 14 hours agoparentprev\"Take that, normies!\" - Nouveau users tomorrow, after their framebuffer finishes loading reply geerlingguy 13 hours agorootparents/after/if ;) reply ClassyJacket 13 hours agoparentprevOther than the 2000$ graphics cards? reply Springtime 12 hours agoprevLooks like their manual driver search model dropdown only goes back to the Turing generation (20 series). Edit: found the Windows driver[1] directly via an online search which covers older models, too. [1] https://www.nvidia.com/download/driverResults.aspx/235774/ reply Dwedit 12 hours agoprevNVIDIA's website lists 566.03 as the newest \"Game Ready\" driver, and 565.90 as the newest \"Studio\" driver. Does the Studio driver contain the security fix as well? reply exmadscientist 9 hours agoparentNo. The hotfix is in v566 or later. The \"Game\" and \"Studio\" drivers are widely believed to be from one codebase, just released on different schedules and with a larger test suite for the \"Studio\" driver. So you have your choice of hotfix-without-testing or tested-without-hotfix! reply Muromec 6 hours agoparentprevWhy is their versioning scheme follow based on a stock price and not normie numbers? reply 2OEH8eoCRo0 2 hours agoprevHow's that webgpu or webassembly working out? reply jamesy0ung 16 hours agoprevTo save a click, the release date of the updated driver (566.03) is 22/10/2024. reply ekianjo 15 hours agoparentYou could have added that the fixes for Linux are in versions 565.57.01, 550.127.05, and 535.216.01. Not everyone runs Windows. reply givinguflac 5 hours agoprevOh good, historically Nvidia drivers on Linux are just… chef’s kiss. Hopefully this will improve things /s. reply Uptrenda 11 hours agoprev [–] That's going to effect a lot of computers. This family of GPUs is incredibly popular. Many of my own graphics cards are GeForce :/ reply echoangle 6 hours agoparent [–] Ok, so update the drivers. It sounds like you think you have to throw the GPU away. reply chrisjj 3 hours agorootparent [–] Update the drivers and enjoy all the app breakage that delivers. reply Consider applying for YC's W25 batch! Applications are open till Nov 12. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Nvidia has discovered eight high-severity security vulnerabilities in their GeForce GPU display drivers and software, potentially allowing attackers to access systems and steal data.- These vulnerabilities affect a range of Nvidia products, including GeForce, Nvidia RTX, Quadro, NVS, and Tesla, on both Windows and Linux operating systems.- Users are advised to update their drivers immediately to the latest versions: 566.03 for Windows and 565.57.01, 550.127.05, and 535.216.01 for Linux, available through Nvidia's Manual Driver Search tool, Nvidia App, and GeForce Experience app."
    ],
    "commentSummary": [
      "Nvidia GeForce GPUs have a security vulnerability in their Windows and Linux drivers, which could allow attackers to escalate permissions, leading to potential code execution and data tampering.- The flaw is particularly concerning for multi-user systems, systems with existing malware, and virtualization hosts, though it is not easily exploitable via browsers.- Nvidia has released updated drivers to mitigate the issue, and users are advised to update their drivers, especially on systems with untrusted users or existing malware."
    ],
    "points": 192,
    "commentCount": 128,
    "retryCount": 0,
    "time": 1730595997
  },
  {
    "id": 42028569,
    "title": "Eighty Years of the Finite Element Method (2022)",
    "originLink": "https://link.springer.com/article/10.1007/s11831-022-09740-9",
    "originBody": "Home Archives of Computational Methods in Engineering Article Eighty Years of the Finite Element Method: Birth, Evolution, and Future Review article Open access Published: 13 June 2022 Volume 29, pages 4431–4453, (2022) Cite this article Download PDF You have full access to this open access article Archives of Computational Methods in Engineering Aims and scope Submit manuscript Wing Kam Liu, Shaofan Li & Harold S. Park 75k Accesses 75 Altmetric Explore all metrics A Correction to this article was published on 28 June 2022 This article has been updated Abstract This document presents comprehensive historical accounts on the developments of finite element methods (FEM) since 1941, with a specific emphasis on developments related to solid mechanics. We present a historical overview beginning with the theoretical formulations and origins of the FEM, while discussing important developments that have enabled the FEM to become the numerical method of choice for so many problems rooted in solid mechanics. Similar content being viewed by others Smoothed Finite Element Methods (S-FEM): An Overview and Recent Developments Article 21 November 2016 A Review: Applications of the Spectral Finite Element Method Article Open access 28 March 2023 Finite element methods: Research in India over the last decade Article 20 August 2019 The year 2021 marks the eightieth anniversary of the invention of the finite element method (FEM), which has become the computational workhorse for engineering design analysis and scientific modeling of a wide range of physical processes, including material and structural mechanics, fluid flow and heat conduction, various biological processes for medical diagnosis and surgery planning, electromagnetics and semi-conductor circuit and chip design and analysis, additive manufacturing, and in general every conceivable problem that can be described by partial differential equations (PDEs). The FEM has fundamentally revolutionized the way we do scientific modeling and engineering design, ranging from automobiles, aircraft, marine structures, bridges, highways, and high-rise buildings. Associated with the development of FEMs has been the concurrent development of an engineering science discipline called computational mechanics, or computational science and engineering. In this paper, we present a historical perspective on the developments of finite element methods mainly focusing on its applications and related developments in solid and structural mechanics, with limited discussions to other fields in which it has made significant impact, such as fluid mechanics, heat transfer, and fluid–structure interaction. To have a complete storyline, we divide the development of the finite element method into four time periods: I. (1941–1965) Early years of FEM; II. (1966–1991) Golden age of FEM; III. (1992–2017) Large scale, industrial applications of FEM and development of material modeling, and IV (2018–) the state-of-the-art FEM technology for the current and future eras of FEM research. Note that this paper may not strictly follow the chronological order of FEM developments, because often time these developments were interwoven across different time periods. 1 (1941–1965) The Birth of the Finite Element Method The origin of the FEM as a numerical modeling paradigm may be traced back to the early 1940s. In 1941, A. Hrennikof, a Russian-Canadian structural engineer at the University of British Columbia, published a paper in the ASME Journal of Applied Mechanics on his membrane and plate model as a lattice framework [1]. This paper is now generally regarded as a turning point that led to the birth of the FEM. In the paper, he discretized the solution domain into a mesh of lattice structure, which was the earliest form of a mesh discretization. On May 3rd, 1941, the same year that Hrennikoff published his paper, R. Courant of New York University delivered an invited lecture at a meeting of the American Mathematical Society held in Washington D.C. on his numerical treatment using a variational method to solve a second order PDE, which arises from Saint–Venant’s torsion problem of a cylinder. In this work, Courant systematically used the Rayleigh–Ritz method with a trial function defined on finite triangle subdomains, which is a primitive form of of the finite element method [2]. Courant’s presentation was later published as a paper in 1943 [3]. Similar works of discretization and variational formulations were also reported in the literature, including McHenry [4], Prager and Synge [5], and Synge and Rheinboldt [6]. As Ray Clough commented in his 1980 paper, “One aspect of the FEM, mathematical modeling of continua by discrete elements, can be related to work done independently in the 1940s by McHenry and Hrennikoff-formulating bar element assemblages to simulate plane stress systems. Indeed, I spent the summer of 1952 at the Boeing Airplane Company trying to adapt this procedure to the analysis of a delta airplane wing, the problem which eventually led to the FEM”. By the early 1950s, several engineers and academics had further developed and extended these early approaches to solve real engineering problems in aeronautical and civil engineering. In parallel but with different emphases, Argyris at the Imperial College London, and M. J. Turner (1950–1956) at Boeing Company, who was later joined by R.W. Clough of UC Berkeley and H.C. Martin of Washington University, developed what we know today as the earliest form of the finite element method (1954), which was called the Matrix Stiffness Method at the time. In a paper published in 1960 [14], R.W. Clough coined the phrase Finite Element Method, and this unassuming and right-to-the-point phrase was an instant hit, bringing out the essence of the method. With his deep insights and profound vision into Courant’s variational approach, J.H. Argyris developed the energy method for engineering structures [6,7,9], a foundational development enabling FEM for solids. While importantly, Turner, Clough, Martin, and Topp successfully developed FEM interpolants for triangular elements [10], which is suitable for structural parts with arbitrary shape. In some senses, the invention of the triangle element was a “quantum leap”, and hence for a large spectrum of the engineering community, the inception of the FEM is the publication of the landmark paper by Turner et al. [10]. The following is an excerpt from a 2014 document that celebrates the 50th anniversary of the formation of National Academy of Engineering, which is an official account of that part of FEM history: To ensure safety and avoid costly modifications after planes entered production, engineers needed a reliable method for determine in advance whether their designs could withstand the stresses of fight. M. Jon Turner, head of Boeing’s Structural Dynamics Unit, addressed that problem in the early 1950s by bringing civil engineering professor Ray Clough of the University of California, Berkeley, and Harold Martin of the University of Washington to Boeing for summer ``faculty internships,’’ Collectively, they created a method of structural analysis that Turner applied at Boeing using computers to perform the myriad calculations need to predict real-world performance. That fruitful collaboration led to Clough’s development a few years later of what he named the finite element method (FEM). Clough formed a research group at UC Berkeley that used FEM in a host of analytical and experimental activities, from designing buildings and structures to withstand nuclear blasts or earthquakes to analyzing structural requirements for spacecraft and deep-water offshore drilling. By revolutionizing the application of computer technologies in engineering, FEM continues to help engineers design to this day all sorts of durable, cost-effective structures. Meanwhile, Turner’s efforts at Boeing contributed to the success of its renowned line of commercial jets, beginning in 1958 with the 707 and continuing in 1964 with the 727, which could land on shorter runways and serve more airports. Equipped with three fuel-efficient turbofan engines, the 727 became the workhorse of commercial aviation and helped achieve a threefold increase in U.S. passenger air traffic in the’60s. Independently and separately, in the early 1960s, Kang Feng of the Chinese Academy of Science also proposed a discretization-based numerical method for variational principles for solving elliptic partial differential equations [11]. As Peter Lax [11, 12] commented, “Independently of parallel developments in the West, he (Feng) created a theory of the finite element method. He was instrumental in both the implementation of the method and the creation of its theoretical foundation using estimates in Sobolev spaces….”, which was one of the first convergence studies of FEMs. During this period, several great engineering minds were focusing on developing FEMs. J.H. Argyris with his co-workers at the University of Stuttgart; R. Clough and colleagues such as E. L. Wilson and R.L. Taylor at the University of California, Berkeley; O.C. Zienkiewicz with his colleagues such as E. Hinton and B. Irons at Swansea University; P. G. Ciarlet at the University of Paris XI; R. Gallager and his group at Cornell University, R. Melosh at Philco Corporation, B. Fraeijs de Veubeke at the Université de Liège, and J. S. Przemieniecki at the Air Force Institute of Technology had made some important and significant contributions to early developments of finite element methods. To understand what happened sixty years ago, we quote an excerpt from a FEM history paper by Clough and Wilson [13], in which they recalled: When Clough presented the first paper using the finite element terminology in 1960 it attracted the attention of his friend, Professor O. C. Zienkiewicz, who was then on the faculty at Northwestern University. A few weeks after the presentation of the paper Zienkiewicz invited Clough to present a seminar on the finite element method to his students. Zienkiewicz was considered one of the world’s experts on the application of the finite difference method to the solution of continuum mechanics problems in Civil Engineering; therefore, Clough was prepared to debate the relative merits of the two methods. However, after a few penetrating questions about the finite element method, Zienkiewicz was almost an instant convert to the method. Z. Bazant, then a visiting associate research engineer at UC Berkeley, recalled: …… The founding of FEM was paper in the ASCE Conf. on Electronic Computation Clough [14], which was the first to derive, by virtual work, the finite element stiffness matrix of an element of a continuum (a triangular constant strain element). ……. I recall Ray Clough showing to me his 1962 report to the U.S. Engineer District, Little Rock, Corps of Engineers on his analysis of a crack observed in Norfolk Dam [15], during my stay in Berkeley in 1969. I was mesmerized by seeing that 1962 report. It presented a 2D stress analysis of large crack observed in Norfork dam. The dam was subdivided into about 200 triangular elements and provided stress contours for a number of loading cases. …… Clough was at that time way ahead of anybody else. To collaborate Z. Bazant’s recollection, we cite E.L. Wilson’s recounting of that part of FEM history: In 1956, Ray, Shirley, and three small children spent a year in Norway at the Ship Research Institute in Trondheim. The engineers at the institute were calculating stresses due to ship vibrations to predict fatigue failures at the stress concentrations. This is when Ray realized his element research should be called the Finite Element Method which could solve many different types of problems in continuum mechanics. Ray realized the FEM was a direct competitor to the Finite Difference Method. At that time FDM was being used to solve many problems in continuum mechanics. His previous work at Boeing, the Direct Stiffness Method, was only used to calculate displacements not stresses. In the fall semester of 1957, Ray returned from his sabbatical leave in Norway and immediately posted a page on the student bulletin board asking students to contact him if they were interested in conducting finite element research for the analysis of membrane, plate, shell, and solid structures. Although Ray did not have funding for finite element research, a few graduate students who had other sources of funds responded. At that time, the only digital computer in the College of Engineering was an IBM 701 that was produced in 1951 and was based on vacuum tube technology. The maximum number of linear equations that it could solve was 40. Consequently, when Ray presented his first FEM paper in September 1960, “The Finite Element Method in Plane Stress Analysis,” at the ASCE 2 nd Conference on Electronic Computation in Pittsburgh, Pennsylvania, the coarse-mesh stress-distribution obtained was not very accurate. Therefore, most of the attendees at the conference were not impressed. After the improvement of the speed and capacity of the computers on the Berkeley campus, Professor Clough’s paper was a very fine mesh analysis of an existing concrete dam. The paper was first presented in September 1962 at a NATO conference in Lisbon, Portugal. Within a few months, the paper was republished in an international Bulletin, which had a very large circulation, as “Stress Analysis of a Gravity Dam by the Finite Element Method”, (with E. Wilson), International Bulletin RILEM, No. 10, June 1963. The Lisbon paper reported on the finite element analysis of the 250-foot-high Norfork Dam in Arkansas, which had developed a vertical crack during construction in 1942. The FEM analysis correctly predicted the location and size of the crack due to the temperature changes and produced realistic displacements and stresses within the dam and foundation for both gravity and several hydrostatic load conditions. Because of this publication, many international students and visiting scholars came to Berkeley to work with Professor Clough. Also, he freely gave the FORTRAN listing of their finite element analysis computer program to be used to evaluated displacement and stresses in other two-dimensional plane structures with different geometry, materials, and loading. Therefore, professional engineers could easily use the powerful new FEM to solve for the stress distributions in their structural engineering problems in continuum mechanics. However, he did not capitalize on his success in the development of the FEM. He returned to the task of building the earthquake engineering program at Berkeley – the task he given when he was first hired in 1949. For their decisive contributions to the creation and developments of FEM, R. W. Clough was awarded the National Medal of Science in 1994 by the then vice-president of the United States Al Gore, while O. C. Zienkiewicz was honored as a Commander of the Order of the British Empire (CBE). Today, the consensus is that J.H. Argyris, R.W. Clough and O. C. Zienkiewicz made the most pivotal, critical, and significant contributions to the birth and early developments of finite element method following an early contribution to its mathematical foundation from R. Courant. It is worth noting that E.L. Wilson of UC Berkeley was the first person to develop finite element open-source software. An excerpt from Clough and Wilson’s paper in 1999 stated 13: In 1958 Wilson, under the direction of Clough, initiated the development of an automated finite element program based on the rectangular plane stress finite element developed at Boeing. After several months of learning to program the IBM 701, Wilson produced a limited capacity, semiautomated program which was based on the force method. An MS research report was produced, which has long since been misplaced, with the approximate title of Computer Analysis of Plane Stress Structures. …… In 1959 the IBM 704 computer was installed on the Berkeley Campus. It had 32K of 32-bit memory and a floating-point arithmetic unit which was approximately 100 times faster than the IBM 701. This made it possible to solve practical structures using fine meshes. It is also worth mentioning that, Oden [16], a senior structural engineer in the research and development division of General Dynamics Corporation at Fort Worth at the time, wrote a 163-page comprehensive technical report with G.C. Best, in which they developed a long list of solid and structural finite elements, including tetrahedral, hexahedral, thin plate, thick plate, plate element with stringers or stiffeners, composite sandwich plate elements, and shallow shell elements [17]. In fact, Oden and Best wrote one of the first general purpose FEM computer codes at the time. The Fortran FEM code developed by Oden and Best had an element library that includes elements for 3D elasticity, 2D plane elasticity, 3D beam and rod elements, composite layered plate and shell elements, and elements for general composite materials. Their work also included hybrid methods and stress based FEMs, which may be even earlier than those of Pian [18]. Moreover, Oden and Best’s FEM code was also able to handle FEM eigenvalue modal analysis, and numerical integration over triangle and tetrahedra elements, and it had linear system solvers for general FEM static analysis that were among the most effective at that time (see [19]). This FEM computer code was used for many years in aircraft analysis and design in the aerospace and defense industry. It appears that, I.C. Taig [20] of English Electric Aviation first introduced the concept of the isoparametric element when he used the matrix-displacement method to conduct stress analysis of aerospace structures, which was later formally dubbed ``isoparametric element’’ by Ergatoudis et al. [21]. It should be mentioned that there are some other pioneers who made some significant contributions in the early developments of FEM, such as Levy [22], Comer [23], Langefors [24], Denke [25], Wehle and Lansing [26], Hoff et al. [27], and Archer [28], among others. These individuals came together made remarkable and historic contributions to the creation of finite element method. Among them, some notable contributions were made by J. S. Przemieniecki (Janusz Stanisław Przemieniecki), who was a Polish engineer and a professor and then dean at the Air Force Institute of Technology in Ohio in the United States from 1961 to 1995. Przemieniecki conducted a series pioneering research works on using FEMs to perform stress and buckling analyses of aerospace structures such as plates, shells, and columns (see Przemieniecki [29], Przemieniecki and Denke [30], Przemieniecki [28,29,30,31,32,33]). In terms of worldwide research interest, by 1965, FEM research had become a highly active field, with the total number of papers published in the literature exceeding 1000. During this period, there were two seemingly unrelated events for FEM development, which significantly affected future FEM developments. These events were the discovery of mixed variational principles in elasticity. In 1950, E. Reissner [34] rediscovered E. Hellinger’s mixed variational principle from 1914 [35], in which both the displacement field and the stress field are the primary unknowns. This variational principle is called the Hellinger–Reissner variational principle. Shortly after, Hu [36] and Washizu [37] proposed a three-field mixed variational principle in elasticity, which was called the Hu-Washizu variational principle. As early as 1964, Pian [18] recognized the potential of using these variational principles to formulate Galerkin weak form-based FE formulations and proposed the assumed stress FEM. This began the use of mixed variational principles to formulate Galerkin FEMs, which was followed by the assumed strain FEM developed by J. Simo and his co-workers in the later period of FEM developments. 2 (1966–1991) The Golden Age of the Finite Element Method The mid 1960s saw rapid developments in finite element method research and applications. As T.J.R. Hughes recalled, “I first heard the words ‘the Finite Element Method” in 1967—which changed my life. I started to read everything that was available and convinced my boss to start the Finite Element Method Development Group, which he did. Dr. Henno Allik was Group Leader, and I was the Group, then we added programmers. In one year, we had a 57,000-line code, GENSAM (General Structural Analysis and Matrix Program, or something like that). That was 1969, and the code was continually developed thereafter and may still be in development and use at GD/Electric Boat and General Atomics, originally a division of General Dynamics.” Starting from the end of 1960s, the rigorous approximation theory that underpins the FEM started to be developed (e.g. Aubin [38], Zlamal [39], Birkhoff [40], Nitsche [41], Aziz [42], Bubuska [43], Bubuska and Aziz [44], Dupont [45], Douglas and Dupont [46]. Nitsche and Schatz [47], and Bubuska [48]). This movement was first highlighted by the proof of optimal and superconvergence of FEMs. This attracted the interest of some distinguished mathematicians all over the world, including G. Birkhoff, M.H. Schultz, R.S. Varaga, J. Bramble, M. Zlamal, J. Cea, J.P. Aubin, J. Douglas, T. Dupont, L.C. Goldstein, LR. Scott, J. Nitsche, A.H. Schatz, P.G. Cialet, G. Strang, G. Fix, JL. Lions, M. Crouzeix, P.A. Raviart, and I. Babuska, A.K. Aziz, and J.T. Oden (see Bramble, Notsche and Schatz [49], Bubuska, Oden, and Lee [50]) . Some notable results developed for the proof of FEM convergence are the Cea lemma and the Bramble-Hubert lemma. The fundamental work of Nitsche [41] on L∞ estimates for general classes of linear elliptic problems also stands out as one of the most important contributions for mathematical foundation of FEM in 1970s. It may be noted that unlike other mathematics movements, the convergence study of FEMs was an engineering-oriented movement. The mathematicians soon found that, in practice, engineers were using either non-conforming FEM interpolants or numerical quadrature that violates variational principles or the standard bilinear form in Hilbert space. G. Strang referred to these numerical techniques as “variational crimes”. To circumvent complicated convergence proofs, the early FEM patch tests were invented by B. Iron and R. Melosh, which were proven to be instrumental for ensuring convergence to the correct solution. Following T.H.H. Pian’s invention of the assumed stress element, attention shifted to the mixed variational principle based FEM [51, 52]). In 1965, L.R. Hermann [53] proposed a mixed variational principle for incompressible solids. However, most mixed variational principles are not extreme variational principles, and thus suffer from numerical instability. In early 1970s, I. Babuska and F. Brezzi discovered their groundbreaking results, known today as the Babuska-Brezzi condition, or the LBB condition, giving tribute to O. Ladyzhenskya—a Russian mathematician, who provided the early insight of this problem. The so-called LBB condition, or the inf–sup condition, provides a sufficient condition for a saddle point problem to have a unique solution that depends continuously on the input data; thus, it provides a guideline to construct shape functions for the mixed variational principle-based FEM (see: [54]). Entering the 1970s, FEM development began to focus on using FEM to simulate the dynamic behavior of structures, including crashworthiness in the automotive industry. Various time integration methods had been developed, including the Newmark-beta method, Wilson-theta method [55], the Hilbert-Hughes-Taylor alpha method [56], the Houbolt integration algorithm, and the explicit time integration algorithm [57]. Since the early 1970s, FEM explicit time integration methods had been used to solve various engineering problems, e.g., dynamic contact problems [58], dynamics problems of solid structure [59], fluid dynamics problems [60], and it was extensively used in large scale FEM Lagrangian hydrocodes developed in Lawrence Livermore National Laboratory [61]. However, an important development came in the late 1970s when T. Belytschko, K. C. Park, and later TJR Hughes proposed using explicit or implicit-explicit, or implicit time integration with damping control to solve nonlinear structural dynamics problems. Other notable contributors to this key development are A. Combescure, A. Gravouil among others (see e.g., [62]). It turned out that the explicit time integration was a game changer for the automotive industry, establishing FEM technology as the main tool of passenger vehicle design and crashworthiness analysis. By the end of 1980s, there were thousands of workstations running explicit time integration-based FEM codes in the three major automakers in the United States. One of the main FEM research topics in the 1980s was using FEM techniques to solve the Navier–Stokes equation as an alternative to finite difference and finite volume methods. Starting the early 1970s, J.T. Oden and his co-workers had begun working on FEM solutions for fluid dynamics [63]. The main challenge in using FEM solving fluid dynamics problems is that the Navier–Stokes equation is not an elliptical partial different equation, and the minimization or variational principle-based Petrov–Galerkin procedure may suffer both stability as well as convergence issues. To resolve these issues, T.J.R. Hughes, and his co-workers such as A.N. Brooks and T.E. Tezduyar developed the streamline upwind/Petrov Galerkin method and later Stabilized Galerkin FEM to solve Navier–Stokes equations under various initial and boundary conditions [64]. Furthermore, Hughes and co-workers later developed space–time FEMs and variational multiscale FEMs (see Hughes and Tezdyuar [65] Brooks and Hughes [66], Mizukami and Hughes [67], and Hughes [68]). For this work, from 1986 to 1991, Hughes and his co-workers such as L.P. Franca and others wrote a ten-part series on FEM formulations for computational fluid dynamics Hughes et al. [69,70,71,71] and Shakib et al. [72]. Several years later, Hughes developed a Green function-based subgrid model acting as a stabilized finite element method [68]. Towards the mid-1980s, advanced FEM mesh generation techniques have been developed, which incorporate various solid modeling techniques by using interactive computer graphics and adaptive mesh generator [73], Bennett and Botkin [74] as well as improved quadtree approach to generate FEM meshes for complex geometric shapes and objects [75]. Today, FEM mesh generation has become an integrated part of solid modeling and engineering design, which has the capabilities of automatic node insertion and refinement Wordenweber [76] and Ho-Le [77]. Among the many advances in FEM technologies in 1980s, the most notable may belong to J. Simo at the University of California, Berkeley and later at Stanford University. Simo and Taylor [78] developed the consistent tangent operator for computational plasticity, which was a milestone after the original concept of consistent linearization proposed by Hughes and Pister [79]. Moreover, after the Hughes-Liu 3D degenerated continuum shell and beam elements and the Belytchko-Tsay single-point element, Simo and his co-workers, such as L. Vu-Quoc and D.D. Fox, developed geometrically exact beam and shell theories and their FEM formulations (see Simo and Vu-Quoc [80], Simo and Fox [81], and Simo et al. [82]. Moreover, Simo and his co-workers such as MS Rifai and F. Armero also developed various assumed strain or enhanced strain methods for mixed variational formulations (see Simo and Rifai [83] and Simo and Armero [84]). It should be noted that E. Ramm and his colleagues at the University of Stuttgart have also made significant contributions on geometrically nonlinear shell element formulations over a span of more than thirty years e.g., Andelfinger and Ramm [85] and Bischoff and Ramm [86]. Another highlight of FEM technology is the development of FEM solvers for fluid–structure interaction. From the mid-1970s to early 1990s, there was an urgent need to develop techniques to solve large-scale fluid–structure interaction problems in the aerospace and civil engineering industries. A class of FEM fluid–structure interaction solvers were developed, and some early contributors include J. Donea, A. Huerta (see [87,88,89,89]), and the Hughes-Liu-Zimmermann Arbitrary Lagrangian–Eulerian (ALE) fluid–structure FEM formulation (see [90, 91]), which describes the moving boundary problem. ALE-based FEM simulations were used due to their ability to alleviate many of the drawbacks of traditional Lagrangian-based and Eulerian-based FEM formulations. When using the ALE technique in engineering modeling and simulations, the computational mesh inside the domains can move arbitrarily to optimize the shapes of elements, while the mesh on the boundaries and interfaces of the domains can move along with materials to precisely track the boundaries and interfaces of a multi-material system. The invention of the ALE FEM may be credited to Hirt, Amsden, and Cook [92]. C. Farhat was the first person to use a large-scale parallel ALE-FEM solver to compute fluid–structure interaction problems [93]. He and his group systematically applied FEM-based computational fluid dynamics (CFD) solvers for aircraft structure design and analysis. They developed the finite element tearing and interconnecting (FETI) method for the scalable solution of large-scale systems of equations on massively parallel processors. Today, some fundamental ALE concepts have also been applied to numerical modeling engineering fields other than FEM, such as meshfree modeling. For example, to alleviate tensile instability and distorted particle distributions in smoothed particle hydrodynamic (SPH) simulations, a so-called shifting technique has been adopted in many of today’s SPH simulations (e.g., Oger et al. [94]). The FEM fluid–structure interaction research had a major impact on many practical applications, such as providing the foundation for the patient specific modeling of vascular disease and the FEM-based predictive medicine later developed by C.A. Taylor and T.J.R. Hughes and their co-workers in the mid-1990s (see Taylor et al. [95]). Holzapfel, Eberlein, Wriggers, and Weizsäcker also developed large strain FEM formulations for soft biological membranes (see Holzapfel et al. [96]). Another major milestone in the development of FEMs was the invention and the development of nonlinear probabilistic or random field FEMs, which was first developed by W. K. Liu and T. Belytschko in the late 1980s. (e.g., Liu et al. [97]). By considering uncertainty in loading conditions, material behavior, geometric configuration, and support or boundary conditions, the probabilistic FEM provided a stochastic approach in computational mechanics to account for all these uncertain aspects, which could then be applied in structure reliability analysis. The random field FEM research has become crucial in civil and aerospace engineering and the field of uncertainty quantification. In the early 1980s, M.E. Botkin at General Motors research Lab [98], and N. Kikuchi and his group at the University of Michigan developed structural shape optimization FEM for the automotive industry (see [99], [100], [101]). Other contributors include M. H. Imam from Uman Al-Qura University [102]). This preceded the seminal 1988 paper of Bendsoe and Kikuchi, who developed a homogenization approach to finding the optimal shape of a structure under prescribed loading. Later developments in topology optimization were driven by Gengdong Cheng, Martin Bendsoe, and his student, Ole Sigmund. One of the driving forces in the FEM development during in the early decades was the safety analysis of big dams, at first, then of concrete nuclear reactor vessels for gas-cooled reactors, [103], nuclear containments, hypothetical nuclear reactor accidents [104, 105] and of tunnels and of foundations for reinforced concrete structures. To simulate concrete failure, the vertical stress drops in FEMs, and progressive softening technique were introduced already in 1968. However, the spurious mesh sensitivity and its impact on strain localization was generally overlooked until demonstrated mathematically by Bazant [105], because strain softening states of small enough test specimens in stiff enough testing frames are stable. Numerically this was demonstrated by crack band FEM calculations in Bazant and Cedolin [106], where it was also shown that spurious mesh sensitivity causing the sudden stress drop can be avoided by adjusting the material strength to ensure the correct energy release rate. Hillerborg et al. [107] avoided mesh sensitivity by using an interelement cohesive softening called the fictitious crack model. Despite the success of the early FEM calculations, the concept of progressive strain-softening damage was not generally accepted by mechanicians until its validity and limitations were demonstrated by Bazant and Belytschko [108] and Bazant and Chang [109]. They showed that the existence of elastic unloading stiffness (previously ignored) makes waves propagation in a strain-softening state possible. In 1989, Lubliner, Oliver, Oller, and Onate ([110] developed a plastic-damage theory-based FEM formulation to model concrete materials by introducing internal variables, which has the capacity of modeling concrete material degradation and cracking. Today, the multiscale based homogenization and damage analysis method is the state-of-the-art FEM modeling for concrete materials [111]. In the mid-1980s, the mesh sensitivity issue in calculating strain softening or strain localization problems in computational plasticity became a challenging topic. It was eventually accepted that the partial differential equations that are associated with the classical plasticity become ill-posed after the material passes the yield point and enters the softening stage. This especially became a dire situation when civil engineers applied FEM to solve complex structural and geotechnical engineering problems, which involved complex plastic deformations of concrete, rock, soil, clay, and granular material in general. Bazant and others realized that this is because the classical continuum plasticity theory lacks an internal length scale. To remedy this problem, starting from the middle 1980s, many efforts were devoted to establishing FEM formulations of nonlocal (Bazant et al. [112]), strain-gradient, strain-Laplacian media, micropolar or Cosserat continua, because they provided an internal length scale, allowing FEM simulations to capture, in a mesh-independent manner, the strain softening, strain localization or shear band formation. Pijaudier-Cabot and Bazant [113] developed an effective nonlocal FEM in which nonlocality is applied only to the damage strain. Other influential and representative works in this topic are from De Borst [114], Peerlings, De Borest and others [115]. Steinmann and Willam [116], Dietsche et al. [117], Steinmann [118], and Iordache and Willam [119]. In 1976, TJR Hughes, R.L. Taylor, J.L. Sackman, A. Curnier, and W. Kanoknukulchai published a paper entitled “A finite element method for a class of contact-impact problems.” [120]. This is one of the earliest FEM analyses in computational contact mechanics. It is the very first work on FEM modeling of dynamic contact and impact problems, and it plays an important role in the simulation accuracy for engineering problems involving interaction between different continuous objects. Examples include sheet metal forming, target impact and penetration, and interaction between pavement and tires. Developing accurate FEM contact algorithms has been a focal point since the 1980s. Various FEM contact algorithms have been developed, and some main contributors are N. Kikuchi, J.T. Oden, J. Simo, P, Wriggers, R.L. Taylor, P. Papadopoulos, and T.A. Laursen. FEM contact algorithm research remained an active research topic until late 1990s (see Simo et al. [121], Simo and Lausen [122], Kikuchi and Oden [123], and Papadopoulos and Taylor [124]). In this period, one interesting emerging area was the development of FEM exterior calculus by Arnold et al. [125]. The FEM exterior calculus uses tools from differential geometry, algebraic topology, and homological algebra to develop FEM discretizations that are compatible with the underlying geometric, topological, and algebraic structures of the problems that are under consideration. 3 (1992–2017) Broad Industrial Applications and Materials Modeling The first major event in FEM development this period was the formulation of the Zienkiewicz-Zhu error estimator [126], which was a major contribution to the mathematical approximation theory of FEMs in the 1990s. The Zienkiewicz-Zhu posteriori error estimators provide the quality control of a FEM solution with an optimal use of computational resources by refining the mesh adaptively. The idea and spirit of Zienkiewicz-Zhu was further carried out by Ainsworth and Oden (see [127, 128, 129]), and today using posteriori error estimation to improve the quality has been elevated to the height of Bayesian inference and Bayesian update. This research topic is now intimately related with what is now called validation and verification (V&V). Since the late 1970s, Szabo [130] and Babuska [131, 132] started to develop hp versions of FEMs based on piecewise-polynomial approximations that employ elements of variable size (h) and polynomial degree (p). They discovered that the FEM converges exponentially when the mesh is refined using a suitable combination of h-refinements (dividing elements into smaller ones) and p-refinements (increasing their polynomial degree). The exponential convergence makes the method a very attractive choice compared to most other FEMs, which only converge with an algebraic rate. This work continued until the late 1990s, spearheaded by M. Ainsworth, L. Demkowicz, J.T. Oden, C.A. Duarte, O.C. Zienkiewicz, and C.E. Baumann (see Demkowicz et al. [133], Demkowicz et al. [134], Demkowicz et al. [135], Oden et al. [136], Oden et al. [137], Baumann and Oden [138]. J. Fish. [139] also proposed a s-version FEM by superposing additional mesh(es) of higher-order hierarchical elements on top of the original mesh of C0 FEM discretization, so that it increases the resolution of the FEM solution. To solve material and structural failure problems, research work in the 1990s focused on variational principle based discretized methods to solve fracture mechanics problems or strain localization problems. In 1994, Xu and Needleman [140] developed a FEM cohesive zone model (CZM) that can simulate crack growth without remeshing, which was later further improved by M. Ortiz and his co-workers, e.g. Camacho and Ortiz [141] and Ortiz and Pandolfi [142], who later used CZM FEM to solve fragmentation and material fatigue problems [143]. It should be noted that long before the invention of cohesive zone model, Pietruszczak and Mroz [144] developed the first cohesive FEM for shear fracture in soil. Later, Bazant’s group at Northwestern University developed various interface FEMs, such as the microplane model, to study size effects of concrete and other brittle composite materials (Brocca and Bazant [145], Caner and Bazant [146]). These models became a standard tool for simulating missile impact and explosions at, e.g., ERDA Vicksburg. A microplane FEM model for fiber composites has been developed for Chrysler and Ford Co. to compare various designs of automobile crush-cans (Smilauer et al. [147]). An anisotropic poromechanical microplane model has been formulated and used for FEM analysis of hydraulic fracturing (Rahimi et al. [148]). To alleviate mesh bias issues in modeling material fracture and damage problems, T. Belytschko and WK. Liu developed meshfree particle methods, namely the element-free Galerkin (EFG) method [149] and the reproducing kernel particle method (RKPM) [150, 151], which are based on the moving least square method and the wavelet multiresolution analysis, respectively. RKPM provides consistency and thus convergence enhancements as compared to the popular smoothed particle hydrodynamics (SPH) method. Li et al. [152, 153] successfully employed meshfree Galerkin methods to accurately simulate adiabatic shear band formation and propagation with minimum mesh adaptation. At the same time, Duarte and Oden developed the so-called hp-Cloud method, Onate et al. developed a finite point method [154], and Atluri and Zhu [155] proposed a meshless local Petrov–Galerkin (MLPG method, among many other meshfree methods. Fleming and Belytschko also showed that singularity functions could be included in the approximation functions to greatly improve simulations involving fracture mechanics (see Fleming et al. [156]). One of the most challenging problems in the development of meshfree Galerkin methods is how to integrate the weak form, because the meshfree interpolants are highly irregular and it is difficult to make them variationally consistent. In 2001, J. S. Chen and his co-workers proposed a stabilized conforming nodal integration method for meshfree RKPM method, which is not only simple and stable, but also variationally consistent with the Galerkin weak formulation (see Chen et al. [157]). Shortly after the meshfree method developments, I. Babuska and his co-workers developed the partition of unity finite element method (PUFEM), which was later coined generalized finite element method (GFEM) (see Melenk and Babuska [158] and Babuska and Melenk [159]). PUFEM is a powerful method because it can be used to construct FE spaces of any given regularity, which is a generalization of the h, p, and hp versions of the FEM, as well as providing the ability to embed an analytic solution into the FEM discretization instead of relying upon a generic polynomial interpolant. A significant breakthrough in computational fracture mechanics and FEM refinement technology came in the late 1990s, when Belytschko and his co-workers, including Black, Moes, and Dolbow, developed the eXtended finite element (X-FEM) (see [160, 161], which uses various enriched discontinuous shape functions to accurately capture the morphology of a cracked body without remeshing. Because the adaptive enrichment process is governed by the crack tip energy release rate, X-FEM provides an accurate solution for linear elastic fracture mechanics (LEFM). In developing X-FEM, T. Belytschko brilliantly utilized the PUFEM concept to solve fracture mechanics problems without remeshing. Entering the new millennium, Bourdin, Francfort, Marigo developed a phase-field approach for modeling material fracture [162]. Almost simultaneously, Karma and his co-workers [163], [164]) also proposed and developed the phase-field method to solve crack growth and crack propagation problems, as the phase field method can accurately predict material damage for brittle fracture without remeshing. The main advantage of the phase field approach is that by using the Galerkin FEM to solve the continuum equations of motion as well as a phase equation, one can find the crack solution in continuum modeling without encountering stress singularity as well as remeshing, and the crack may be viewed as the sharp interface limit of the phase field solution. Some of the leading contributors for this research are Bourdin, Borden, Hughes, Kuhn, Muller, Miehe, Landis, among others (see Bourdin and Chambolle [165], Kuhn and Müller [166]. Miehe et al. [167], and Borden et al. [168], Wilson et al. [169], and Pham et al. [170]). As mentioned before, the main reason for the huge success of FEMs is their broad applicability to engineering analysis and design across scientific disciplines. On the other hand, most mechanical engineering designs are performed by using various computer-aided design (CAD) tools, such as solid modeling. To directly blend the FEM into CAD design tools, T.J.R. Hughes, and his co-workers such as J.A. Cottrell and Y. Bazilevs [171], [172] developed the isogeometric analysis (IGA) FEM, which established the Galerkin variational weak formulation in the control mesh and uses the non-uniform rational basis spline (NURBS) functions as the FEM shape function to solve the problem at design stage. IGA-FEM method successfully integrates FEM into conventional NURBS-based CAD tools, without converting data between CAD and FEA packages in analyzing new designs during development stage. Due to the emergence of nanotechnology, various multiscale methods have been developed to couple atomistic methods such as molecular dynamics and density functional theory (DFT) and other ab initio methods with continuum scale FEMs. The most notable contributions in this area are hand-shake method [173] quasi-continuum FEM [174], and the bridging scale method developed by Wagner and Liu [175]. In 2007, Gavini, Bhattacharya, and Ortiz developed quasi-continuum orbital-free density-functional theory (DFT) FEM for multi-million atom DFT calculations (see Gavini et al. [176]). Broadly speaking, there has been a rapid development in computational homogenization methods since the late 1990s, which is an alternate approach to obtaining continuum-scale properties based on smaller scale microstructures. The computational homogenization method or the FEM homogenization methods for composite materials may be divided into two main categories: (1) Computational asymptotic homogenization method or multiscale computational homogenization, which is aimed for modeling composite materials with periodic microstructure. The pioneer of multiscale homogenization FEM may be credited to T.Y. Hou and his co-workers [177], and other earlier contributors are N. Kikuchi [101], Ghosh et al. [178], Fish et al. [179, 180]) and M. S. Shephard, (2) Computational micromechanics method, which is mainly aimed for composite materials with random microstructure, even though it may also be applied to materials with periodic microstructure, where the main contributors are: P. Suquet et al. at the French National Centre for Scientific Research (CNRS), Dvorak et al.[181] and J. Fish at Rensselaer Polytechnic Institute and C. Miehe and his co-workers (e.g. [182]) at the University of Stuttgart. During the same period, many novel FEM fluid–structure interaction solvers have been developed, for instance, the immersed FEM developed by Zhang et al. [183], which was motivated by the immersed boundary method pioneered by C. Peskin of Courant Institute of Mathematical Sciences, New York University. An important FEM application area emerged with the developments of computational plasticity. The early finite element computational plasticity formulation is based on hypoelastic–plastic rate formulation. To satisfy the objectivity requirement, T.J, R. Hughes and J. Winget first proposed the so-called incremental objectivity (see Hughes and Winget [184]), which was probably in the first time the esoteric continuum mechanics theory was applied to practical FEM computational formulations and computations, and it in turn promoted the development of nonlinear continuum mechanics in 1980s and 1990s. Soon afterwards. Simo and Hughes then extended Hughes-Winget incremental objectivity algorithm to the finite deformation case in computational plasticity (see Simo and Hughes [185]). The notion of consistency between the tangent stiffness matrix and the integration algorithm employed in the solution of the incremental problem was introduced by Nagtegaal [186] and Simo and Taylor [78]. Consistent formulations have been subsequently developed for finite deformation plasticity by Simo and Oritz [187] (1988) within the framework of multiplicative decomposition of the deformation gradient and hyperelasticity. Also in 1980s, based on the Gurson model, Tvergaard and Needleman [188] developed the FEM based Gurson-Tvergaard-Needleman model, which is probably the most widely used FEM computational plasticity constitutive model used in material modeling, though recently attention has turned to developing machine-learning based or data-driven computational plasticity models e.g., F. Chinesta, et al. [189], and the unsupervised machine learning data-driven finite element methods, called as the Self-consistent Clustering Analysis (SCA) (see Z. Liu et al. [190, 191]). An important advance of the FEM is the development of the crystal plasticity finite element method (CPFEM), which was first introduced in a landmark paper by Pierce et al. [192]. In the past almost four decades, there are numerous researchers who have made significant contributions to the subject, for example, A. Arsenlis and DM. Parks from MIT and Lawrence Livermore National Laboratory [193], [194], [195], Dawson et al. at Cornell University (Quey et al. [196]; Mathur and Dawson [197]; Raabe et al. at Max-Planck-Institute fur Eisenforschung [196,197,198,201], among others. Based on crystal slip, CPFEM can calculate dislocation, crystal orientation and other texture information to consider crystal anisotropy during computations, and it has been applied to simulate crystal plasticity deformation, surface roughness, fractures and so on. Recently, S. Li and his co-workers developed a FEM-based multiscale dislocation pattern dynamics to model crystal plasticity in single crystal [202, 203]. Yu et al., [204] reformulated the self-consistent clustering analysis (SCA) for general elasto-viscoplastic materials under finite deformation. The accuracy and efficiency for predicting overall mechanical response of polycrystalline materials are demonstrated with a comparison to traditional full-field FEMs. In 2013, a group of Italian scientists and engineers led by L. Beirão da Veiga and F. Brezzi proposed a so-called virtual element method (VEM) (see Beirao et al. [205, 206]). The virtual element method is an extension of the conventional FEM for arbitrary element geometries. It allows the polytopal discretizations (polygons in 2-D or polyhedra in 3-D), which may be even highly irregular and non-convex element domains. The name virtual derives from the fact that knowledge of the local shape function basis is not required, and it is in fact never explicitly calculated. VEM possesses features that make it superior to the conventional FEM for some special problems such as the problems with complex geometries for which a good quality mesh is difficult to obtain, solutions that require very local refinements, and among others. In these special cases, VEM demonstrates robustness and accuracy in numerical calculations, when the mesh is distorted. As early as 1957, R. Clough introduced the first graduate FEM course at UC-Berkeley, and since then FEM courses at both graduate and undergraduate levels have been added into engineering higher education curriculums in all the major engineering schools and Universities all over the world. As J.T. Oden recalled in his 1963 paper, “I went on to return to academia in 1964 and among my first chores was to develop a graduate course on finite element methods. At the same time, I taught mathematics and continuum mechanics, and it became clear to me that finite elements and digital computing offered hope of transforming nonlinear continuum mechanics from a qualitative and academic subject into something useful in modern scientific computing and engineering.”. By the end of 2015, there have been more than several hundred monographs and textbooks on the FEM published in dozens of languages worldwide. An exposition of FEM mathematical theory by Strang and Fix [207] was among the earliest of those FEM books. J.T. Oden and his collaborators such as GF Carey and JN Reddy and others wrote a five-volume FEM monograph in 1980s (see [167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,213, 214]). Other influential finite element books or monographs are those by Zienkiewicz and Cheung [215], and Zienkiewicz and Taylor [216], ora later Zienkiewicz, Taylor and Zhu [217], TJR. Hughes [218], Cook et al. [219] or later Cook [220]), Bathe [221], and the Nonlinear FEM monographs by Belytschko et al. [222] and by De Borst et al. [223], among others, all have made major impacts on FEM educations and applications. Among all these FEM monographs and textbooks, the book by Zienkiewicz and Taylor [216] or Zienkiewicz et al. [217] probably have had most impacts on FEM technology popularization, which may be because Taylor wrote a FEM research computer program code named FEAP, which was placed in the appendix of that book, providing an immediate guidance and instruction on how to implement FEM in computers. The ready availability of FEM textbooks and tutorials, along with the large FEM software market, have made the FEM accessible to users across academia and industry. The development of FEM software technology started in the early1960s. In 1963, E.L. Wilson and R. Clough developed a structural mechanics computing code called Symbolic Matrix Interpretive System, SMIS, which was initially intended to fill the gap between the matrix method and hand calculating in structure mechanics. It turns out the development of SMIS led to the birth of FEM software. Then, based on SMIS, Wilson initiated and developed a general-purpose static and dynamic Structural Analysis Program, SAP. In late 1960s and the early 1970s, K, J. Bathe developed the nonlinear FEM code ADINA based on SAP IV and NONSAP. Today, the brand name SAP2000 has become synonymous with the state-of-the-art FEM structural analysis and design methods since its introduction over 55 years ago. At the same period, to compete with the Soviet Union’s space program, NASA developed its own FEM code called NASTRAN (NASA STRuctural ANalysis Program). The first version of NASTRAN was called COSMIC Nastran, which debuted in 1969, with a key figure in its development being R. H. MacNeal. As early as 1963, R. H. MacNeal founded the MacNeal-Schwendler Software Corporation (MSC) along with R. Schwendler. Under his leadership, MSC developed its first structural analysis software called SADSAM (Structural Analysis by Digital Simulation of Analog Methods), which showed the early form of FEM analysis software technology. In response to NASA’s request for proposals in 1965 for a general purpose structural analysis program, Dr. MacNeal contributed significantly to the early efforts of the aerospace industry by successfully simulating on-the-ground physical testing through computing to deliver the right answers and physics needed to take humans to the moon. In 1971, MSC Software released a commercial version of Nastran, named MSC Nastran. About the same time in 1960s, J. A. Swanson worked at Westinghouse Astronuclear Laboratory in Pittsburgh, and he was responsible for stress analysis of the components in NERVA nuclear reactor rockets. While there he then developed 3D FEM model and computer codes to analyze and predict transient stresses and displacements of the reactor system. To integrate different computer codes and streamline the processing, Swanson asked his employer Westinghouse to develop a general-purpose FEM computer code, but his suggestion was rejected, and then he left the company and developed the initial ANSYS FEM code. Today, ANSYS has become one of the major FEM commercial software worldwide. Several years later, J.O. Hallquist at Lawrence Livermore National Laboratory also developed a 3D nonlinear FEM code called DYNA3D, which was extensively used impact, dynamic contact, and failure analysis of structures, which later evolved to LS-DYNA. LS-DYNA is the major FEM software used in automobile design and crashworthiness analyses. In 2018, Livermore Software Technology was purchased by ANSYS, and LS-DYNA became a part of ANSYS as ANSYS LS-DYNA. In the early development of nonlinear FEM software, there were two early pioneers: P. V. Marcal and D. Hibbitt, who was Marcal’s student at Brown University. In the early 1970s, Marcal founded the MARC corporation to develop the first general purpose nonlinear FEM program, which is still widely used today in industry and academia for analysis of complex structures, such as nuclear reactors, car crashworthiness and manufacturing processes. While Hibbitt together with B. Karlsson and P. Sorenson formed a FEM company called HKS in 1978, and they released a large-scale commercial FEM software called ABAQUS. One of the major features of ABAQUS is that it allows the user defined subroutines, which greatly facilitates the researchers to conduct their researches by using the standard FEM solvers with reliability and efficiency. Together with ANSYS, ABAQUS is one of the two commercial FEM software that dominate the market. By the late 1990s to early 2000s, the FEM software industry had become a multi-billion-dollar business. There were several household FEM software company names such as ANSYS, ABAQUS, ADINA, LS-DYNA, NASTRAN, COMSOL Multiphysics, CSI, among others. Today, there are also a plethora of open-source FEM software available online, such as FreeFEM, OpenSees, Elmer, FEBio, FEniCS Project, DUNE, among some others. 4 Present and Future The modern form of the FEM can routinely solve many large and complex industrial problems. It enables developing a fundamental understanding and allows for the predictive analysis for product design. For new scientific discoveries and engineering innovations, the development of new scientific principles often trails the pace of new inventions with the sheer volume of data that are generated across multiple spatial, temporal, and design parameter (spatial–temporal-parameter) spaces. For this reason, FE researchers are studying various forms of machine and deep learning methods, of which this class of methods covers the largest class of interpolations. According to the universal approximation theorem, a neural network (NN) can be designed and trained to approximate any given continuous function with any desired accuracy [224, 225, 226, 227] which is believed to drive new discoveries and enable future computational discretization technologies. In this context, Mechanistic Data Science (MDS) FEMs, which combine known scientific principles with newly collected data, will provide the critically needed research that can be a boon for new inventions. Scientific and engineering problems typically fall under three categories: (1) problems with abundant data but undeveloped or unavailable scientific principles, (2) problems that have limited data and limited scientific knowledge, and (3) problems that have known scientific principles with uncertain parameters, with possible high computational load [228]. In essence, mechanistic data science (MDS) mimics the way human civilization has discovered solutions to difficult and unsolvable problems from the beginning of time. Instead of heuristics, MDS uses machine learning methods like active deep learning and hierarchical neural network(s) to process input data, extract mechanistic features, reduce dimensions, learn hidden relationships through regression and classification, and provide a knowledge database. The resulting reduced order form can be utilized for design and optimization of new scientific and engineering systems (see Liu et. al. [229]). Thus, the new focus of the FEM research has shifted towards the development of machine learning based FEMs and reduced order models. With the recent development of machine learning and deep learning methods, solving FEM by constructing a deep neural network has become a state-of-the-art technology. Earlier research focused on building up a shallow neural network following the FEM structure to solve boundary value problems. Takeuchi and Kosugi [230] proposed a neural network representation of the FEM to solve Poisson equation problems. Yagawa and Aoki [231] replaced the FEM functional with the network energy of interconnected neural networks (NNs) to solve a heat conduction problem. Due to the limitation of computationliual power and slow convergency rate in shallow neural networks, earlier applications could only solve simple PDE problems. After the 2010s, neural networks for solving computational mechanics problems have become increasingly popular with the rapid growth of deep learning techniques and the development of more sophisticated neural network structures, such as convolutional neural networks (CNN), Generative Adversarial Networks (GAN) and residual neural networks (ResNet). For its high dimensional regression ability, some researchers, for example, Ghavamiana and Simone [232] used deep neural networks as a regression model to learn the material behavior or microstructure response. Other works focus on solving PDEs using deep learning neural networks. G. Karniadakis and his coworkers (see Raissi et al. [233, 234], Karniadakis et al. [235]) proposed a Physics-Informed Neural Networks (PINNs) to solve high dimensional PDEs in the strong form with constraints to accommodate both natural and essential boundary conditions. The idea of constructing deep neural networks following the FEM structure is investigated again with advanced neural network methodologies. Weinan E and his co-workers [236] and B. Yu [237] proposed a Deep Ritz Method for solving variational problems. Sirignano and Spiliopoulos [238] proposed the so-called Deep Galerkin Method (DGM) to solve high-dimensional PDEs. Zabaras and his co-worders proposed a CNN-based physics-constrained deep learning framework for high-dimensional surrogate modeling and uncertainty quantification (see Zhu et al. [239]). Rabczuk [240] systematically explore the potential to use NNs for computational mechanics by solving energetic format of the PDE (see Samaniego et al. [241]). Lee [242] proposed a partition of unity network for deep hp approximation of PDEs and extensively the training and initialization strategy to accelerate the convergence of the solution process (see Lee et al. [242]). The constructing of element shape function by activation functions has been studied by J. Opschoor and his coworkers (See [243, 244]). Inspired by the universal approximation of deep neural networks (DNN), Zhang et al. [245] published the first paper on the construction of the FEM shape functions-based on the hierarchical nature of the DNN, called Hierarchical Deep-learning Neural Networks (HiDeNN). Specifically, the authors demonstrated the construction of a few classes of deep learning interpolation functions, such as the reproducing kernel particle method (RKPM), non-uniform rational B-spline (NURBS), and isogeometric analysis (IGA), among other approximation techniques. Saha et al. [228] generalized HiDeNN to a unified Artificial intelligence (AI)-framework, called HiDeNN-AI. HiDeNN-AI can assimilate many data-driven tools in an appropriate way, which provides a general approach to solve challenging science and engineering problems with little or no available physics as well as with extreme computational demand. To reduce the FE computational cost, the so-called two-stage data-driven methods have been proposed of which during the offline stage, where a database generated by the FEM is first developed, and the final solutions are computed during the online stage. C. Farhat and his group at Stanford University have developed several dimensional reductions of nonlinear FEM dynamic models, including mesh sampling and weighting for the hyper reduction of nonlinear Petrov‐Galerkin reduced‐order models (see Farhat et al. [246], and Grimberg et al. [247]). To further reduce the FEM computational burden in multiscale analysis, Liu et al. [190] applied the unsupervised machine learning techniques, such as the k-mean clustering method to group the material points during the offline stage and obtain the final solutions by solving the reduced-order Lippmann–Schwinger micromechanics equations. This class of data-driven approaches circumvents the computational burden of the well-established FE square method, the offline-online database approach to solve the concurrent FEM problems. They named the method the Finite Element-self-consistent clustering analysis (FE-SCA) of which the computational cost of the microscale analysis is reduced tremendously in multiple orders of magnitude speed up (see Li et al. [248]). Gao et al. proposed an alternative (FE-SCAxSCA…xSCA) clustering analysis, of which the continuum FEM scale is concurrently solved with the (n-1) coupled-scale Lippmann–Schwinger micromechanics equations [249, 250]. It has been recently extended to (FE-SCA**2) by He et al. [251, 252]. Ortiz and his co-workers at Caltech developed data driven FEMs for dynamics and noisy data (see [253, 254]). Chen and his co-workers have developed a physics-constrained data-driven RKPM method based on locally convex reconstruction for noisy databases (see He and Chen [255]). S. Li and his group at UC-Berkeley utilized FEM solution generated data to develop a machine learning based inverse solution to predict pre-crash data of car collision (see Chen et al. [256]). Bessa et al. [257] proposed a data-driven framework to address the longstanding challenge of a two-scale analysis and design of materials under uncertainty applicable to problems that involve unacceptable computational expense when solved by standard FEM analysis of representative volume elements. The paper defined a framework that incorporates the SCA method to build large databases suitable for machine learning. The authors believe that this will open new avenues to finding innovative materials with new capabilities in an era of high-throughput computing (“big-data”). Reduced order modeling has been an active research field over the last decades. Early research works focused on the proper orthogonal decomposition (POD) method (also known as Karhunen-Loève transform, or principal component analysis) with the purpose of reducing the degrees of freedom of the discretized equations. The POD based model reduction has shown great success in computational fluid dynamics, see e.g., the works of Berkooz et al. [258]. For further accelerating the simulations, K. Willcox and her coworkers (see [259, 260]) proposed a missing point estimation method, which is known later as a hyper reduction method. Other notable works related to POD and hyper reduction methods are the Gauss–Newton with approximated tensors (GNAT) method, Grassmann manifold based reduced basis adaptation, thanks to C. Farhat and his coworkers (see Carlberg and Farhat [261], Carlberg et al. [262], Grimberg et al. [247], Amsallem and Farhat [263], and Farhat et al. [246]). For solid mechanics, Ryckelynck et al. [264] proposed a hyper reduction method based on FEM for dealing with nonlinear problems. Lu et al. [265] proposed an adaptive hyper reduction for coupled thermal-fluid analysis. Another type of model reduction method, which is based on mathematics and has a rigorous error bound estimate, is called reduced basis method, as proposed by Maday and Rønquist [266]. The proper generalized decomposition (PGD) based model reduction, as an extension of POD, can be dated back to 1980s, and it was introduced by P. Ladevèze (See Ladeveze [267], Ladeveze and Rougee [268]) under the name of radial time–space approximation. F. Chinesta et al. [269, 270] developed a PGD method to account for the parameter space, aiming at building offline computational vademecum for fast online predictions. It is noted that PGD methods are based on the idea of separation of variables and in particular a canonical tensor decompostion (TD). Recent works have been conducted to combine deep machine learning methods with reduced order modeling methods. Zhang et al. [271] consolidated the various attributes of TD and PGD methods with HiDeNN and proposed the so-called HiDeNN-TD and HiDeNN-PGD methods. The comparison of FEM, TD/PGD, HiDeNN-TD/PGD, HiDeNN, and DNN has been conducted in terms of accuracy and speed. It is shown that the HiDeNN-TD/PGD outperforms other methods with a good balance between accuracy and speed. The proposed HiDeNN-TD/PGD method is expected to provide novel powful tools for solving large scale high dimensional problems while maintaining a high accuracy. Various applications, including multiphysics coupled additive manufacturing, multiscale composite modeling, and structural topology optimization, have been discussed in a generalized reduced order machine learning finite element framework [272]. In particular, this reduced order machine learning framework is expected to enable ultra large-scale high resolution topology design that is currently challenging for the FEM based topology optimization. In this regard, Lu et al. [273] recently developed a convolution HiDeNN-TD formulation with a built-in density filter for high resolution topology design. This convolution formulation incorporates the concept of meshfree approximation into the finite element function approximation and allows smoother solutions and automatic length-scale control in topology design. It is shown that the convolution HiDeNN-TD leads to better design with smoother and fine structures. This general convolution formulation opens new perspectives to resolve the length scale effect and can be applied to many orther problems, such as additive manufacturing and microstructure modeling. The extra-ordinary interpolating capability of neural network has resulted in numerous research in approximating and solving the ordinary and partial differential equations e.g. Chen et al. [274]. Currently, the researchers are looking into approximating the mathematical operators directly from universal approximation of operator theory (see Chen and Chen [275]). The goal of this endeavor is to solve the integral equations such as the Lippmann–Schwinger micromechanics equations (Z. Liu et. al. [190]). A major step forward towards this target is made by Li et al. [276] in their proposed Graph Kernel Network, in which they developed neural operators for solving partial differential equations. In this work, a kernel-based graph neural network is shown to be able to mimic the Green’s function method for solving partial differential equation. One drawback of this method is the associated computational cost and storage requirement increases with the size of the problem. A more general version of approximating the Green’s function operator is proposed by Lu et al. in their DeepONet [277] and by Gin et al. in their DeepGreen methods (see [278]). These approximation works are vital in computational mechanics as these can directly solve the micromechanics equation for multiscale analysis like FE-SCA. Moreover, these networks have suggested that the analytical calculus method such as differentiation and integration, and solution of differential and integration equations can be directly expressed as an approximation of neural network. In this regard, recurrent neural networks have shown promised to be identical in structure of wave equations [279]. These researches are directed towards a future when the research and education in the science, technology, engineering, and mathematics (STEM) sector with discrete calculus will be transformed with the aid of deep learning. We are envisioning this field as deep learning discrete calculus, a new perspective in teaching calculus by the integration of calculus definition, numerical analysis, and deep learning (see Liu et al. [280]). The development of model reduction methods meets the urgent demand in the industry for fast and nearly real time simulations of engineering problems, such as online dynamic system control, structural health monitoring, vehicle health monitoring, on-line advanced manufacturing feedback control, automated driving controls and decisions, etc. Such applications usually require an intensive interaction between sensors, control algorithms, and simulation tools. Practical optimal control may require a reliable prediction within the range of milli- or sub-milli-second. Reducing the computation cost of simulations has been one of the major motivations for developing model reduction methods. Other reduced order modeling related topics are the feature engineering and data analytics, which constitute an extensive literature in field of machine learning. Thus, the reduced order modeling and the machine learning have intrinsic connections. Developing reduced order machine learning methods may enable physics-data combined models that can overcome the current bottleneck in model reduction methods and purely data-driven machine learning approaches. Change history 28 June 2022 A Correction to this paper has been published: https://doi.org/10.1007/s11831-022-09784-x References Hrennikoff A (1941) Solution of problems of elasticity by the framework method. J Appl Mech 8(4):169–175 Article MathSciNet MATH Google Scholar Williamson F (1980) A historical note on the finite element method. Int J Num Meth’s Eng 15:930–934 Article MathSciNet MATH Google Scholar Courant R (1943) Variational methods for the solution of problems of equilibrium and vibrations. Bull Am Math Soc 49:1–23 Article MathSciNet MATH Google Scholar McHenry D (1943) A lattice analogy for the solution of stress problems. J Instit Civ Eng 21(2):59–82 Article Google Scholar Prager W, Synge JL (1947) Approximations in elasticity based on the concept of function space. Q Appl Math 5(3):241–269 Article MathSciNet MATH Google Scholar Synge JL, Rheinboldt WC (1957) The hyper circle in mathematical physics. Phys Today 10(10):45 Article Google Scholar Argyris JH (1954) Energy theorems and structural analysis. Aircraft Eng 26:347–356 (383-387, 394) Article Google Scholar Argyris JH (1955) Energy theorems and structural analysis. Aircraft Eng 27:42–58 (Feb.), 80–94 (March), 125–134 (April), 145–158 (May 1955) Argyris JH (1965) Continua and discontinua. In: Preziemiencki et al (eds) Proceedings conference on matrix methods in structural mechanics. AFFDL-TR-66-80, (Oct. 26–28, 1965), Wright-Patterson AFB, Ohio, pp 11–190 Turner MJ, Clough RW, Martin HC, Topp LJ (1956) Stiffness and deflection analysis of complex structures. J Aeronaut Sci 23(9):805–823 Article MATH Google Scholar Feng K (1965) Variational principle-based finite difference formulation. Appl Comput Math 2(4):238–262 (in Chinese) Google Scholar Lax P (1003) Kang Feng. SIAM News 26(11), 7-8. Clough RW, Wilson EL (1999 Aug) Early finite element research at Berkeley. In: Fifth US national conference on computational mechanics, pp 1–35 Clough RW (1960) The finite element method in plane stress analysis. In: Proceedings of 2nd ASCE conference on electronic computation. Pittsburgh PA, 8–9 Sept Clough RW (1962) The stress distribution of Norfork Dam. California Univ Berkeley Inst of Engineering Research, California Google Scholar Oden JT (1963) Analysis of thin plates supported at their corners: Prepared for Office of Engineering Research, College of Engineering. School of Civil Engineering, Oklahoma State University Best G, Oden JT (1963) Stiffness matrices for shell-type structures, Research & Engineering Department, Research Report No. 233, General Dynamics, Fort Worth Google Scholar Pian TH (1964) Derivation of element stiffness matrices by assumed stress distributions. AIAA J 2(7):1333–1336 Article Google Scholar Best, G. and Oden, J.T. (1963). Stiffness matrices for shell-type structures, Research & Engineering Department, Research Report No. 233, General Dynamics, Fort Worth Taig IC (1961) Structural analysis by the matrix-displacement method. Engl Elect. Auiat. Rep. No. SO17 Ergatoudis I, Irons BM, Zienkiewicz OC (1968) Curved, isoparametric, “quadrilateral” elements for finite element analysis. Int J Solids Struct 4(1):31–42 Article MATH Google Scholar Levy S (1953) Structural analysis and influence coefficients for delta wings. J Aeronaut Sci 20(7):449–454 Article MATH Google Scholar Comer RL, Levy S (1963) Deflections of an inflated circular-cylindrical cantilever beam. AIAA J 1(7):1652–1655 Article MATH Google Scholar Langefors B (1952) Analysis of elastic structures by matrix transformation with special regard to semimonocoque structures. J Aeronaut Sci 19(7):451–458 Article MATH Google Scholar Denke PH (1966) Engineering aspects and mathematical formulation of the problem of a computerized aircraft structural analysis system. SAE Transactions, pp 327–343 Wehle LB Jr, Lansing W (1952) A method for reducing the analysis of complex redundant structures to a routine procedure. J Aeronaut Sci 19(10):677–684 Article Google Scholar Hoff NJ, Klein B, Libby PA (1946) numerical procedures for the calculation of stresses in monocoques: influence coefficients of curved bars for distortions in their own plane. IV (vol 4). National Advisory Committee for Aeronautics Archer JS (1965) Consistent matrix formulations for structural analysis using finite-element techniques. AIAA J 3(10):1910–1918 Article MATH Google Scholar Przemieniecki JS (1963) Matrix structural analysis of substructures. AIAA J 1(1):138–147 Article Google Scholar Przemieniecki JS, Denke PH (1966) Joining of complex substructures by the matrix force method. J Aircr 3(3):236–243 Article Google Scholar Przemieniecki JS (1968) Discrete-element methods for stability analysis of complex structures. Aeronaut J 72(696):1077–1086 Article Google Scholar Przemieniecki JS (1972) Matrix analysis of local instability in plates, stiffened panels, and columns. Int J Numer Meth Eng 5(2):209–216 Article MATH Google Scholar Przemieniecki JS (1973) Finite element structural analysis of local instability. AIAA J 11(1):33–39 Article MATH Google Scholar Reissner E (1950) On a variational theorem in elasticity. J Math Phys 29(1–4):90–95 Article MathSciNet MATH Google Scholar Hellinger E, Klein F, Müller C (1914) Die allgemeinen Ansätzeder Mechanik der Kontinua. Enr Math Wti 4(602):694 Google Scholar Hu HC (1955) On some variational principles in the theory of elasticity and plasticity. Sci Sin 4:33–54 MATH Google Scholar Washizu K (1955) On the variational principles of elasticity and plasticity. Massachusetts Inst of Tech Cambridge Aeroelastic and Structures Research Lab Aubin JP (1967) Behavior of the error of the approximate solutions of boundary-value problems for linear elliptic operators by Galerkin’s Method and finite differences. Annali della Scuola Normale di Pisa, Series 3(21):599–637 MATH Google Scholar Zlamal M (1968) On the finite element method. Numerische Math 12:394–440 Article MathSciNet MATH Google Scholar Birkhoff G (1969) Piecewise bicubic interpolation and approximation in polygons. In: Shoenberg IJ (ed) approximations with special emphasis on spline functions. Academic Press, New York, pp 85–121 Google Scholar Nitsche J (1970) Lineare spline-funktionen und die methoden von Ritz für elliptische randwertprobleme. Arch Ration Mech Anal 36(5):348–355 Article MathSciNet MATH Google Scholar Aziz AK (ed) (1972) The mathematical foundations of the finite element method with applications to partial differential equations. Academic Press, New York Google Scholar Babuska I (1971) Error bounds for the finite element method. Numerische Math 16:322–333 Article MathSciNet MATH Google Scholar Babushka I, Aziz AK (1972) Survey lectures on the mathematical foundation of the finite element method. In: Aziz AK (ed) The mathematical foundation of the finite element method with applications to partial differential equations. Academic Press, New York, pp 5–359 Google Scholar Dupont T (1973) L^2-estimates for Galerkin methods for second order hyperbolic equations. SIAM J Numer Anal 10(5):880–889 Article MathSciNet MATH Google Scholar Douglas J, Dupont T (1973) Superconvergence for Galerkin methods for the two-point boundary problem via local projections. Numer Math 21(3):270–278 Article MathSciNet MATH Google Scholar Nitsche JA, Schatz AH (1974) Interior estimates for Ritz-Galerkin methods. Math Comput 28(128):937–958 Article MathSciNet MATH Google Scholar Babuska I (1976) Finite element methods for domains with corners. Computing 6:264–273 Article MathSciNet MATH Google Scholar Bramble JH, Nitsche JA, Schatz AH (1975) Maximum-norm interior estimates for Ritz-Galerkin methods. Math Comput 29(131):677–688 Article MathSciNet MATH Google Scholar Babuska I, Oden JT, Lee JK (1977) Mixed-hybrid finite element approximations of second-order elliptic boundary-value problems. Comput Methods Appl Mech Eng 11:175–206 Article MathSciNet MATH Google Scholar Pian THH (1965) Element stiffness matrices for boundary compatibility and for prescribed boundary stresses. In: Proceedings of the First Conference of Matrix Methods in Structural Mechanics, pp 457–477 Pian TH, Sumihara K (1984) Rational approach for assumed stress finite elements. Int J Numer Methods Eng 20(9):1685–1695 Article MATH Google Scholar Herrmann LR (1965) Elasticity equations for incompressible and nearly incompressible materials by a variational theorem. AIAA journal, 3(10):1896–1900 Boffi D, Brezzi F, Fortin M (2013) Mixed finite element methods and applications (vol 44). Springer, Berlin, pp xiv–685 Book MATH Google Scholar Wilson EL, Clough RW (1962) Dynamic analysis by step-by-step matrix analysis. In: Symposium on the Use of Computers in Civil Engineering, Lisbon, Portugal Hilber HM, Hughes TJR, Taylor RL (1977) Improved numerical dissipation for time integration algorithms in structural dynamics. Earthquake Eng Struct Dynam 5(3):283–292 Article Google Scholar Houbolt JC (1950) A recurrence matrix solution for the dynamic response of elastic aircraft. J Aeronaut Sci 17(9):540–550 Anderheggen E, Ekchian D, Heiduschke K, Bartelt P (1970) A contact algorithm for explicit dynamic FEM analysis. WIT Trans Eng Sci 1:272 Google Scholar Biffle JH, Key SW (1977) Finite element formulations for transient dynamic problems in solids using explicit time integration. Comput Methods Appl Mech Eng 12(3):323–336 Article MathSciNet MATH Google Scholar Gresho PM, Lee RL, Sani RL, Stullich TW (1978) Time-dependent FEM solution of the incompressible Navier--Stokes equations in two-and three-dimensions (No. UCRL--81323). California University, Lawrence Livermore Lab, Livermore, California (USA) Goudreau GL, Hallquist J (1982) Recent developments in large-scale finite element Lagrangian hydrocode technology. Comput Methods Appl Mech Eng 33(1–3):725–757 Article MathSciNet MATH Google Scholar Gravouil A, Combescure A (2001) Multi-time-step explicit–implicit method for non-linear structural dynamics. Int J Numer Meth Eng 50(1):199–225 Article MATH Google Scholar Oden JT, Wellford LC Jr (1972) Analysis of flow of viscous fluids by the finite-element method. AIAA J 10(12):1590–1599 Article MATH Google Scholar Hughes TJR, Tezduyar T (1981) Finite elements based upon Mindlin plate theory with particular reference to the four-node bilinear isoparametric element. J Appl Mech 48:587 Article MATH Google Scholar Hughes TJR, Liu WK (1981) Nonlinear finite element analysis of shells: Part I. Three-dimensional shells. Comput Methods Appl Mech Eng 26(3):331–362 Article MATH Google Scholar Brooks AN, Hughes TJR (1982) Streamline upwind/Petrov-Galerkin formulations for convection dominated flows with particular emphasis on the incompressible Navier-Stokes’s equations. Comput Methods Appl Mech Eng 32(1–3):199–259 Article MathSciNet MATH Google Scholar Mizukami A, Hughes TJR (1985) A Petrov-Galerkin finite element method for convection-dominated flows: an accurate upwinding technique for satisfying the maximum principle. Comput Methods Appl Mech Eng 50(2):181–193 Article MathSciNet MATH Google Scholar Hughes TJR (1995) Multiscale phenomena: Green’s functions, the Dirichlet-to-Neumann formulation, subgrid scale models, bubbles, and the origins of stabilized methods. Comput Methods Appl Mech Eng 127(1–4):387–401 Article MathSciNet MATH Google Scholar Hughes TJR, Mallet M, Akira M (1986) A new finite element formulation for computational fluid dynamics: II. Beyond SUPG. Comput Methods Appl Mech Eng 54(3):341–355 Article MathSciNet MATH Google Scholar Hughes TJR, Franca LP, Mallet M (1986) A new finite element formulation for computational fluid dynamics: I. Symmetric forms of the compressible Euler and Navier-Stokes equations and the second law of thermodynamics. Comput Methods Appl Mech Eng 54(2):223–234 Article MathSciNet MATH Google Scholar Hughes TJR, Franca LP, Balestra M (1986) A new finite element formulation for computational fluid dynamics: V. Circumventing the Babuška-Brezzi condition: a stable Petrov-Galerkin formulation of the Stokes problem accommodating equal-order interpolations. Comput Methods Appl Mech Eng 59(1):85–99 Article MATH Google Scholar Shakib F, Hughes TJR, Johan Z (1991) A new finite element formulation for computational fluid dynamics: X. The compressible Euler and Navier-Stokes equations. Compur Methods Appl Mech Eng 89(1–3):141–219 Article MathSciNet Google Scholar Caendish JC, Field DA, Frey WH (1985) An approach to automatic three-dimensional finite element mesh generation. Int J Numer Meth Eng 21(2):329–347 Article MATH Google Scholar Bennett JA, Botkin ME (1985) Structural shape optimization with geometric description and adaptive mesh refinement. AIAA J 23(3):458–464 Article Google Scholar Yerry M, Shephard M (1983) A modified quadtree approach to finite element mesh generation. IEEE Comput Graphics Appl 3(01):39–46 Article Google Scholar Wördenweber B (1984) Finite element mesh generation. Comput-Aided Des 16(5):285–291 Article Google Scholar Ho-Le K (1988) Finite element mesh generation methods: a review and classification. Comput Aided Des 20(1):27–38 Article MATH Google Scholar Simo JC, Taylor RL (1985) Consistent tangent operators for rate-independent elastoplasticity. Comput Methods Appl Mech Eng 48(1):101–118 Article MATH Google Scholar Hughes TJ, Pister KS (1978) Consistent linearization in mechanics of solids and structures. Comput Struct 8(3–4):391–397 Article MathSciNet MATH Google Scholar Simo JC, Vu-Quoc L (1986) A three-dimensional finite-strain rod model. Part II: Computational aspects. Comput Methods Appl Mech Eng 58(1):79–116 Article MATH Google Scholar Simo JC, Fox DD (1989) On a stress resultant geometrically exact shell model. Part I: formulation and optimal parametrization. Comput Methods Appl Mech Eng 72(3):267–304 Article MATH Google Scholar Simo JC, Fox DD, Rifai MS (1989) On a stress resultant geometrically exact shell model. Part II: The linear theory; computational aspects. Comput Methods Appl Mech Eng 73(1):53–92 Article MATH Google Scholar Simo JC, Rifai M (1990) A class of mixed assumed strain methods and the method of incompatible modes. Int J Numer Meth Eng 29(8):1595–1638 Article MathSciNet MATH Google Scholar Simo JC, Armero F (1992) Geometrically non-linear enhanced strain mixed methods and the method of incompatible modes. Int J Numer Meth Eng 33(7):1413–1449 Article MATH Google Scholar Andelfinger U, Ramm E (1993) EAS-elements for two-dimensional, three-dimensional, plate and shell structures and their equivalence to HR-elements. Int J Numer Meth Eng 36(8):1311–1337 Article MATH Google Scholar Bischoff M, Ramm E (1998) Shear deformable shell elements for large strains and rotations. Int J Numer Meth Eng 40(23):4427–4449 Article MATH Google Scholar Donea J, Giuliani S, Halleux JP (1982) An arbitrary Lagrangian-Eulerian finite element method for transient dynamic fluid-structure interactions. Comput Methods Appl Mech Eng 33(1–3):689–723 Article MATH Google Scholar Donea J, Huerta A (2003) Finite element methods for flow problems. John Wiley & Sons, Hoboken Book Google Scholar Donea J, Huerta A, Ponthot JP, Rodríguez‐Ferran A (2017) Arbitrary Lagrangian–E ulerian Methods.In: Encyclopedia of Computational Mechanics Second Edition, pp 1–23 Hughes TJR, Liu WK, Zimmerman TK (1981) Lagrangian-Eulerian finite element formulations for incompressible viscous flows. Comput Methods Appl Mech Eng 29:329–349 Article MathSciNet MATH Google Scholar Hughes TJR, Liu WK, Zimmermann TK (1981) Lagrangian-Eulerian finite element formulation for incompressible viscous flows. Comput Methods Appl Mech Eng 29(3):329–349 Article MathSciNet MATH Google Scholar Hirt CW, Amsden AA, Cook JL (1974) An arbitrary Lagrangian-Eulerian computing method for all flow speeds. J Comput Phys 14(3):227–253 Article MATH Google Scholar Farhat C (1994) Implicit parallel processing in structural mechanics. Comp Mech Adv 2:1–124 MathSciNet MATH Google Scholar Oger G, Marrone S, Le Touzé D, De Leffe M (2016) SPH accuracy improvement through the combination of a quasi-Lagrangian shifting transport velocity and consistent ALE formalisms. J Comput Phys 313:76–98 Article MathSciNet MATH Google Scholar Taylor CA, Hughes TJR, Zarins CK (1998) Finite element modeling of blood flow in arteries. Comput Methods Appl Mech Eng 158(12):155–196 Article MathSciNet MATH Google Scholar Holzapfel GA, Eberlein R, Wriggers P, Weizsäcker HW (1996) Large strain analysis of soft biological membranes: formulation and finite element analysis. Comput Methods Appl Mech Eng 132(1–2):45–61 Article MATH Google Scholar Liu WK, Belytschko T, Mani A (1986) Random field finite elements. Int J Numer Meth Eng 23(10):1831–1845 Article MathSciNet MATH Google Scholar Botkin ME (1982) Shape optimization of plate and shell structures. AIAA J 20(2):268–273 Article Google Scholar Kikuchi N, Chung KY, Torigaki T, Taylor JE (1986) Adaptive finite element methods for shape optimization of linearly elastic structures. Comput Methods Appl Mech Eng 57(1):67–89 Suzuki K, Kikuchi N (1991) A homogenization method for shape and topology optimization. Comput Methods Appl Mech Eng 93(3):291–318 Article MATH Google Scholar Bendsøe MP, Kikuchi N (1988) Generating optimal topologies in structural design using a homogenization method. Comput Methods Appl Mech Eng 71(2):197–224 Article MathSciNet MATH Google Scholar Imam MH (1982) Three-dimensional shape optimization. Int J Numer Meth Eng 18(5):661–673 Article MATH Google Scholar Rashid YR (1968) Ultimate strength analysis of prestressed concrete pressure vessels. Nucl Eng Des 7(4):334–344 Article Google Scholar Marchertas AH, Fistedis SH, Bažant ZP, Belytschko TB (1978) Analysis and application of prestressed concrete reactor vessels for LMFBR containment. Nucl Eng Des 49(1–2):155–173 Article Google Scholar Bažant ZP (1976) Instability, ductility, and size effect in strain-softening concrete. J Eng Mech Div 102(2):331–344 Article Google Scholar Bažant ZP, Cedolin L (1980) Fracture mechanics of reinforced concrete. J Eng Mech Div 106(6):1287–1306 Article Google Scholar Hillerborg A, Modéer M, Petersson PE (1976) Analysis of crack formation and crack growth in concrete by means of fracture mechanics and finite elements. Cem Concr Res 6(6):773–781 Article Google Scholar Bažant ZP, Belytschko TB (1985) Wave propagation in a strain-softening bar: exact solution. J Eng Mech 111(3):381–389 Google Scholar Bažant ZP, Chang TP (1987) Nonlocal finite element analysis of strain-softening solids. J Eng Mech 113(1):89–105 Google Scholar Lubliner J, Oliver J, Oller S, Oñate E (1989) A plastic-damage model for concrete. Int J Solids Struct 25(3):299–326 Article Google Scholar Wriggers P, Moftah SO (2006) Mesoscale models for concrete: Homogenization and damage behavior. Finite Elem Anal Des 42(7):623–636 Article Google Scholar Bazant ZP, Belytschko TB, Chang TP (1984) Continuum theory for strain-softening. J Eng Mech 110(12):1666–1692 Google Scholar Pijaudier-Cabot G, Bažant ZP (1987) Nonlocal damage theory. J Eng Mech 113(10):1512–1533 MATH Google Scholar De Borst R, Mühlhaus HB (1992) Gradient-dependent plasticity: formulation and algorithmic aspects. Int J Numer Meth Eng 35(3):521–539 Article MATH Google Scholar Peerlings RH, de Borst R, Brekelmans WM, de Vree J (1996) Gradient enhanced damage for quasi-brittle materials. Int J Numer Meth Eng 39(19):3391–3403 Article MATH Google Scholar Steinmann P, Willam K (1991) Localization within the framework of micropolar elasto-plasticity. In: Brüller OS, Mannl V, Najar J (eds) Advances in continuum mechanics. Springer, Berlin, Heidelberg, pp 296–313 Chapter Google Scholar Dietsche A, Steinmann P, Willam K (1993) Micropolar elastoplasticity and its role in localization. Int J Plast 9(7):813–831 Article MATH Google Scholar Steinmann P (1994) A micropolar theory of finite deformation and finite rotation multiplicative elastoplasticity. Int J Solids Struct 31(8):1063–1084 Article MathSciNet MATH Google Scholar Iordache MM, Willam K (1998) Localized failure analysis in elastoplastic Cosserat continua. Comput Methods Appl Mech Eng 151(3–4):559–586 Article MATH Google Scholar Hughes TJR, Taylor RL, Sackman JL, Curnier A, Kanoknukulchai W (1976) A finite element method for a class of contact-impact problems. Comput Methods Appl Mech Eng 8(3):249–276 Article MATH Google Scholar Simo JC, Wriggers P, Taylor RL (1985) A perturbed Lagrangian formulation for the finite element solution of contact problems. Comput Methods Appl Mech Eng 50(2):163–180 Article MathSciNet MATH Google Scholar Simo JC, Laursen TA (1992) An augmented Lagrangian treatment of contact problems involving friction. Comput Struct 42(1):97–116 Article MathSciNet MATH Google Scholar Kikuchi N, Oden JT (1988) Contact problems in elasticity: a study of variational inequalities and finite element methods. Society for Industrial and Applied Mathematics, Philadelphia Book MATH Google Scholar Papadopoulos P, Taylor RL (1992) A mixed formulation for the finite element solution of contact problems. Comput Methods Appl Mech Eng 94(3):373–389 Article MATH Google Scholar Arnold D, Falk R, Winther R (2010) Finite element exterior calculus: from Hodge theory to numerical stability. Bull Am Math Soc 47(2):281–354 Article MathSciNet MATH Google Scholar Zienkiewicz OC, Zhu JZ (1992) The superconvergent patch recovery and a posteriori error estimates. Part 1: The recovery technique. Int J Numer Methods Eng 33(7):1331–1364 Article MATH Google Scholar Ainsworth M, Senior B (1998) An adaptive refinement strategy for hp-finite element computations. Appl Numer Math 26(1–2):165–178 Article MathSciNet MATH Google Scholar Ainsworth M, Oden JT (2000) A posteriori error estimation in finite element analysis. John Wiley & Sons, Hoboken Book MATH Google Scholar Ainsworth M, Oden JT (1997) A posteriori error estimation in finite element analysis. Comput Methods Appl Mech Eng 142(1–2):1–88 Article MathSciNet MATH Google Scholar Szabo BA, Mehta AK (1978) p-convergent finite element approximations in fracture mechanics. Int J Num Meth Engng 12:551–560 Article MATH Google Scholar Babuska I, Szabo BA, Kat IN (1981) The p-version of the finite element method. SIAM J Numer Anl 18:515–544 Article MathSciNet MATH Google Scholar Babuska I, Szabo BA, Kat IN (1982) On the rates of convergence of the finite element method. Int J Numer Meth Eng 18:323–341 Article MathSciNet MATH Google Scholar Demkowicz L, Oden JT, Rachowicz W, Hardy O (1989) Toward a universal hp adaptive finite element strategy, Part 1. Constrained approximation and data structure. Comput Methods Appl Mech Eng 77(1–2):79–112 Article MATH Google Scholar Demkowicz L, Oden JT, Rachowicz W (1990) A new finite element method for solving compressible Navier-Stokes equations based on an operator splitting method and hp adaptivity. Comput Methods Appl Mech Eng 84(3):275–326 Article MATH Google Scholar Demkowicz L, Monk P, Vardapetyan L, Rachowicz W (2000) De Rham diagram for hp finite element spaces. Comput Math Appl 39(7–8):29–38 Article MathSciNet MATH Google Scholar Oden JT, Demkowicz L, Rachowicz W, Westermann TA (1989) Toward a universal hp adaptive finite element strategy, Part 2. A posteriori error estimation. Comput Methods Appl Mech Eng 77(12):113–180 Article MATH Google Scholar Oden JT, Duarte CAM, Zienkiewicz OC (1998) A new cloud-based hp finite element method. Comput Methods Appl Mech Eng 153(1–2):117–126 Article MathSciNet MATH Google Scholar Baumann CE, Oden JT (1999) A discontinuous hp finite element method for convection: diffusion problems. Comput Methods Appl Mech Eng 175(3–4):311–341 Article MathSciNet MATH Google Scholar Fish J (1992) The s-version of the finite element method. Comput Struct 43(3):539–547 Article MATH Google Scholar Xu XP, Needleman A (1994) Numerical simulations of fast crack growth in brittle solids. J Mech Phys Solids 42(9):1397–1434 Article MATH Google Scholar Camacho GT, Ortiz M (1996) Computational modelling of impact damage in brittle materials. Int J Solids Struct 33(20–22):2899–2938 Article MATH Google Scholar Ortiz M, Pandolfi A (1999) Finite-deformation irreversible cohesive elements for three-dimensional crack-propagation analysis. Int J Numer Meth Eng 44(9):1267–1282 Article MATH Google Scholar Nguyen O, Repetto EA, Ortiz M, Radovitzky RA (2001) A cohesive model of fatigue crack growth. Int J Fract 110(4):351–369 Article Google Scholar Pietruszczak ST, Mroz Z (1981) Finite element analysis of deformation of strain-softening materials. Int J Numer Meth Eng 17(3):327–334 Article MATH Google Scholar Brocca M, Bažant ZP (2001) Size effect in concrete columns: finite-element analysis with microplane model. J Struct Eng 127(12):1382–1390 Article Google Scholar Caner FC, Bažant ZP (2009) Size effect on strength of laminate-foam sandwich plates: finite element analysis with interface fracture. Compos B Eng 40(5):337–348 Article Google Scholar Šmilauer V, Hoover CG, Bažant ZP, Caner FC, Waas AM, Shahwan KW (2011) Multiscale simulation of fracture of braided composites via repetitive unit cells. Eng Fract Mech 78(6):901–918 Article Google Scholar Rahimi-Aghdam S, Chau VT, Lee H, Nguyen H, Li W, Karra S, Rougier E, Viswanathan H, Srinivasan G, Bažant ZP (2019) Branching of hydraulic cracks enabling permeability of gas or oil shale with closed natural fractures. Proc Natl Acad Sci 116(5):1532–1537 Article Google Scholar Belytschko T, Lu YY, Gu L (1994) Element-free Galerkin methods. Int J Numer Meth Eng 37(2):229–256 Article MathSciNet MATH Google Scholar Liu WK, Jun S, Zhang YF (1995) Reproducing kernel particle methods. Int J Numer Meth Fluids 20(8–9):1081–1106 Article MathSciNet MATH Google Scholar Liu WK, Jun S, Li S, Adee J, Belytschko T (1995) Reproducing kernel particle methods for structural dynamics. Int J Numer Meth Eng 38(10):655–1679 Article MathSciNet MATH Google Scholar Li S, Liu W-K, Qian D, Guduru DP, Rosakis AJ (2001) Dynamic shear band propagation and micro-structure of adiabatic shear band. Comput Methods Appl Mech Eng 191:73–92 Article MATH Google Scholar Li S, Liu WK, Rosakis AJ, Belytschko T, Hao W (2002) Mesh-free Galekin simulations of dynamic shear band propagation and failure mode transition. Int J Solids Struct 39(5):1213–1240 Article MATH Google Scholar Oñate E, Idelsohn S, Zienkiewicz OC, Taylor R (1996) A finite point method in computational mechanics. Applications to convective transport and fluid flow. Int J Numer Methods Eng 39(22):3839–3866 Atluri SN, Zhu T (1998) A new meshless local Petrov-Galerkin (MLPG) approach in computational mechanics. Comput Mech 22(2):117–127 Article MathSciNet MATH Google Scholar Fleming M, Chu YA, Moran B, Belytschko T (1997) Enriched element-free galerkin methods for crack tip fields. Int J Numer Meth Eng 40:1483–1504 Article MathSciNet Google Scholar Chen JS, Wu CT, Yoon S, You Y (2001) A stabilized conforming nodal integration for Galerkin mesh-free methods. Int J Numer Meth Eng 50(2):435–466 Article MATH Google Scholar Melenk JM, Babuška I (1996) The partition of unity finite element method: basic theory and applications. Comput Methods Appl Mech Eng 139(1–4):289–314 Art",
    "commentLink": "https://news.ycombinator.com/item?id=42028569",
    "commentBody": "Eighty Years of the Finite Element Method (2022) (springer.com)182 points by sandwichsphinx 23 hours agohidepastfavorite92 comments angry_moose 22 hours agoI've been a full-time FEM Analyst for 15 years now. It's generally a nice article, though in my opinion paints a far rosier picture of the last couple decades than is warranted. Actual, practical use of FEM has been stagnate for quite some time. There have been some nice stability improvements to the numerical algorithms that make highly nonlinear problems a little easier; solvers are more optimized; and hardware is of course dramatically more capable (flash storage has been a godsend). Basically every advanced/\"next generation\" thing the article touts has fallen flat on its face when applied to real problems. They have some nice results on the world's simplest \"laboratory\" problem, but accuracy is abysmal on most real-world problems - e.g. it might give good results on a cylinder in simple tension, but fails horribly when adding bending. There's still nothing better, but looking back I'm pretty surprised I'm still basically doing things the same way I was as an Engineer 1; and not for lack of trying. I've been on countless development projects that seem promising but just won't validate in the real world. Industry focus has been far more on Verification and Validation (ASME V&V 10/20/40) which has done a lot to point out the various pitfalls and limitations. Academic research and the software vendors haven't been particularly keen to revisit the supposedly \"solved\" problems we're finding. reply digdugdirk 17 hours agoparentI'm a mechanical engineer, and I've been wanting to better understand the computational side of the tools I use every day. Do you have any recommendations for learning resources if one wanted to \"relearn\" FEA from a computer science perspective? reply piuantiderp 12 hours agorootparentStart with FDM. Solve Bernoulli deflection of a beam reply physicsguy 8 hours agorootparentprevHave a look at FEniCs to start with. reply ccosm 20 hours agoparentprev>Basically every advanced/\"next generation\" thing the article touts has fallen flat on its face when applied to real problems Even Arnold's work? FEEC seemed quite promising last time I was reading about it, but never seemed to get much traction in the wider FEM world. reply catgary 21 hours agoparentprevI kind of thought Neural Operators were slotting into the some problem domains where FEM is used (based on recent work in weather modelling, cloth modelling, etc) and thought there was some sort of FEM -> NO lineage. Did I completely misunderstand that whole thing? reply angry_moose 21 hours agorootparentThose are definitely up next in the flashy-new-thing pipeline and I'm not that up to speed on them yet. Another group within my company is evaluating them right now and the early results seems to be \"not very accurate, but directionally correct and very fast\" so there may be some value in non-FEM experts using them to quickly tell if A or B is a better design; but will still need a more proper analysis in more accurate tools. It's still early though and we're just starting to see the first non-research solvers hitting the market. reply kk58 1 hour agorootparentVery curious, we are getting good results with PiNN and operators, what's your domain? reply amelius 20 hours agorootparentprevI was under the impression that the linear systems that come out of FEM methods are in some cases being solved by neural networks (or partially, e.g. as a preconditioner in an iterative scheme), but I don't know the details. reply akomtu 20 hours agoparentprevCould you write a blogpost-style article on how to model the shallow water wave equation on a sphere? The article would start with the simplest possible method, something that could be implemented in short C program, and would continue with a progressively more accurate and complex methods. reply sampo 38 minutes agorootparent> Could you write a blogpost-style article on how to model the shallow water wave equation on a sphere? Typically, Finite Volume Method is used for fluid flow problems. It is possible to use Finite Element Methods, but it is rare. reply neumann 20 hours agorootparentprevIf you are interested in this, I'd recommend following an openfoam tutorial, c++ though. You could do SWE with finite elements, but generally finite volumes would be your choice to handle any potential discontinuities and is more stable and accurate for practical problems. Here is a tutorial. https://www.tfd.chalmers.se/~hani/kurser/OS_CFD_2010/johanPi... reply fastasucan 18 hours agorootparentprevInteresting that this reads almost like an chatgpt prompt. reply gwern 17 hours agorootparentLazy people have been lazy forever. I stumbled across an example of this the other day from the 1990s, I think, and was shocked how much the student emails sounded like LLM prompts: https://www.chiark.greenend.org.uk/~martinh/poems/questions.... reply pixelpoet 10 hours agorootparentAt least those had some basic politeness. So often I'm blown away not only how people blithely write \"I NEED HELP, GIMME XYZ NOW NERDS\" but especially how everyone is just falling over themselves to actually help! WTF? Basic politeness is absolutely dead, nobody has any concept of acknowledging they are asking for a favour; we just blast Instagram/TikTok reels at top volume and smoke next to children and elderly in packed public spaces etc. I'm 100% sure it's not rose-tinted memories of the 90s making me think, it wasn't always like this... reply MichaelZuo 16 hours agorootparentprevIt reminds me of the old joke that half of the students are below average… reply wombatpm 14 hours agorootparentExpect in Lake Woebegone, all of the children are above average reply meindnoch 8 hours agorootparentprevBut that's not true, unless by \"average\" you mean the median. reply KeplerBoy 8 hours agorootparentNormally, it's all the same. reply meindnoch 7 hours agorootparentOnly if the distribution has zero skewness. Unless \"normally\" you mean the normal distribution, which indeed has zero skewness. reply KeplerBoy 13 minutes agorootparentYes, it was a admittedly bad pun. CamperBob2 1 hour agorootparentprev\"As an AI language model, I am happy to comply with your request ( https://chatgpt.com/share/6727b644-b2e0-800b-b613-322072d9d3... ), but good luck finding a data set to verify it, LOL.\" reply the5avage 10 hours agoparentprevHave you heard of physics informed neural nets? It seems like a hot candidate to potentially yield better results in the future reply weinzierl 23 hours agoprevI started my career doing FE modeling and analysis with ANSYS and NASTRAN. Sometimes I miss these days. Thinking about how to simplify a real world problem so far that it is solvable with the computational means available was always fun. Then pushing quads around for hours until the mesh was good had an almost meditative effect. But I don't feel overwhelmingly eager to learn a new software or language. Much to my surprise, it seems there hasn't been much movement there. ANSYS still seems to be the leader for general simulation and multi-physics. NASTRAN still popular. Still no viable open-source solution. The only new player seems to be COMSOL. Has anyone experience with it? Would it be worth a try for someone who knows ANSYS and NASTRAN well? reply navane 22 hours agoparentI've used ansys daily for over a decade, and the only movement is in how they name their license tiers. It's a slow muddy death march. Every year I'm fighting the software more and more, the sales men are clearly at the wheel. They buy \"vertical aligned\" software, integrate it, then slowly let it die. They just announced they're killing off one of these next year, that they bought ten years ago, because they want to push a competitive product with 20% of the features. I've been using nastran for half as long but it isn't much better. It's all sales. I dabbed a bit in abaqus, that seems nice. Probably cause I just dabbed in it. But here I'm just trying to do my work, and all these companies do is move capabilities around their license tiers and boil the frog as fast as they get away with. reply angry_moose 21 hours agorootparentI've gone Abaqus > Ansys > Abaqus/LS-DYNA over my career and hate Ansys with a fiery passion. It's the easiest one to run your first model in, but when you start applying it to real problems its a fully adversarial relationship. The fact you have to make a complete copy of the geometry/mesh to a new Workbench \"block\" to run a slightly different load case (and you can't read in an orphaned results files) is just horrible. Abaqus is more difficult to get up to speed in, but its really nice from an advanced usability standpoint. They struggle due to cost though, it is hugely expensive and we've had to fight hard to keep it time and time again. LS-Dyna is similar to Abaqus (though I'm not fully up in it yet), but we're all just waiting to see how Ansys ruins it, especially now that they got bought out by Synopsys. reply navane 18 hours agorootparentI don't know how long ago you used ansys, and i definitely don't want to sell it, but you can share geometry/mesh between those \"blocks\" (by dragging blocks on top of each other), and you can read in result orphaned result files. reply magicalhippo 22 hours agoparentprev> Still no viable open-source solution. For the more low-level stuff there's the FEniCS project[1], for solving PDEs using fairly straight forward Python code like this[2]. When I say fairly straight forward, I mean it follows the math pretty closely, it's not exactly high-school level stuff. [1]: https://fenicsproject.org/ [2]: https://jsdokken.com/dolfinx-tutorial/chapter2/linearelastic... reply lll-o-lll 21 hours agorootparentInteresting. Please bear with me as this is going off 25 year old memories, but my memory is that the workflow for using FEA tools was: Model in some 3D modelling engineering tool (e.g. SolidWorks), ansys to run FEA, iterate if needed, prototype, iterate. So to have anything useful, you need that entire pipeline? For hobbyists, I assume we need this stack. What are the popular modelling tools? reply bauta-steen 19 hours agorootparentTo get started with Fenics you can maybe use the FEATool GUI, which makes it easier to set up FEA models, and also export Python simulation scripts to learn or modify the Fenics syntax [1]. [1]: https://www.featool.com/tutorial/2017/06/16/Python-Multiphys... reply physicsguy 8 hours agorootparentprevFEniCs is mostly used by academic researchers, I used it for FEM modelling in magnetic for e.g. where the sorts of problems we wanted to solve you can’t do in a commercial package. reply magicalhippo 20 hours agorootparentprevYeah not my domain so wouldn't really know. For FEniCS I know Gmsh[1] was used. There's some work[2][3] been done to integrate FEniCS with FreeCAD. It seems FreeCAD also supports[4] other FEM solvers. But, I guess you get what you pay for in this space still. [1]: https://gmsh.info/ [2]: https://github.com/qingfengxia/Cfd [3]: https://github.com/qingfengxia/FenicsSolver [3]: https://wiki.freecad.org/FEM_Solver reply physicsguy 8 hours agorootparentYou can export other CAD meshes for use in it reply fn-mote 20 hours agorootparentprev> For hobbyists, I assume we need this stack. Just curious what kind of hobby leads to a finite element analysis? reply kragen 18 hours agorootparentElectronics (when you start to care about EMI or antenna design), model airplanes (for aerodynamics), rocketry, machining (especially if you want to get into SPIF), robotics, 3-D printing (especially for topology optimization), basically anything that deals with designing solid structures in the physical world. Also, computer graphics, including video games. Unfortunately the barrier to entry is too high for most hobbyists in these fields to use FEM right now. reply nativeit 16 hours agorootparentThere are some obvious downsides and exceptions to this sentiment, but on balance, I really appreciate how the expansive access to information via the internet has fostered this phenomenon: where an unremarkable fella with a dusty media studies degree, a well-equipped garage, and probably too much free time can engineer and construct robotic machines, implement/tweak machine vision mechanisms, microwave radio transceivers, nanometer-scale measurements using laser diodes and optical interferometry, deep-sky astrophotography, etc., etc.. Of course, with burgeoning curiosity and expanding access to surplus university science lab equipment, comes armchair experts and the potential for insufferability[0]. It’s crucial to maintain perspective and be mindful of just how little any one person (especially a person with a media studies degree) can possibly know. [0] I’m pretty sure “insufferability” isn’t a real word. [Edit: don’t use an asterisk for footnotes.] reply kragen 9 hours agorootparent> comes armchair experts and the potential for insufferability Hey, I resemble that remark! I'd be maybe a little less armchair with more surplus equipment access, but maybe no less insufferable. By all accounts, though, a degree of insufferability is no bar to doing worthwhile work; Socrates, Galileo, Newton, Babbage, and Heaviside were all apparently quite insufferable, perhaps as much so as that homeless guy who yells at you about adrenochrome when you walk by his park encampment. (Don't fall into the trap of thinking it's an advantage, though.) Getting sidetracked by trivialities and delusions is a greater risk. Most people spend their whole lives on it. As for how little any person can know, you can certainly know more than anyone who lived a century ago: more than Einstein, more than Edison, more than Noether, more than Tesla, more than Gauss. Any one of the hobbies you named will put you in contact with information they never had, and you can draw on a century or more of academic literature they didn't have, thanks to Libgen and Sci-Hub (and thus Bitcoin). And it's easy to know more than an average doctorate holder; all you have to do is study, but not forget everything you study the way university students do, and not fall into traps like ancient aliens and the like. I mean, you can still do good work if you believe in ancient aliens (Newton and Tesla certainly believed dumber things) but probably not good archeological work. Don't be discouraged by prejudice against autodidacts. Lagrange, Heaviside, and du Châtelet were autodidacts, and Ptolemy seems to have been as well. And they didn't even have Wikipedia or Debian! Nobody gets a Nobel for passing a lot of exams. reply jasomill 18 hours agorootparentprevIMO, the mathematics underlying finite element methods and related subjects — finite element exterior calculus comes immediately to mind — are interesting enough to constitute a hobby in their own right. reply angry_moose 22 hours agoparentprevCOMSOL's big advantage is it ties together a lot of different physics regimes together and makes it very easy to couple different physics together. Want to do coupled structures/fluid? Or coupled electromagnetism/mechanical? Its probably the easiest one to use. Each individual physics regime is not particularly good on its own - there are far better mechanical, CFD, electromagnetism, etc solvers out there - but they're all made by different vendors and don't play nicely with each other. reply goodtruck 2 hours agoparentprevI am hoping this open source FEM library will catch on : https://www.dealii.org/. The deal in deal.II stands for Differential Equation Analysis Library. It's written in C++, makes heavy use of templates and been in development since 2000. It's not meant for solid mechanics or fluid mechanics specifically, but for FEM solutions of general PDEs. The documentation is vast, the examples are numerous and the library interfaces with other libraries like Petsc, Trilinos etc. You can output results to a variety of formats. I believe support for triangle and tetrahedral elements has been added only recently. In spite of this, one quirk of the library is that meshes are called \"triangulations\". reply drpossum 8 hours agoparentprevI've worked with COMSOL (I have a smaller amount of ANSYS experience to compare to). For the most part I preferred COMSOL's UI and workflow and leveraged a lot of COMSOL's scripting capabilities which was handy for a big but procedural geometry I had (I don't know ANSYS's capabilities for that). They of course largely do the same stuff. If you have easy access to COMSOL to try it out I'd recommend it just for the experience. I've found sometimes working with other tools make me recognize some capabilities or technique that hadn't clicked for me yet. reply class3shock 2 hours agoparentprevAbaqus is up there with Ansys aswell as others have mentioned. reply karencarits 8 hours agoparentprevOpenFOAM seems like an opensource option but I have found it rather impenetrable - there are some youtube videos and pdf tutorials, but they are quite dense and specific and doens't seem to cover the entire pipeline Happy to hear if people have good resources! reply master_crab 20 hours agoparentprevStill no viable open-source solution. Wait? What? NASTRAN was originally developed by NASA and open sourced over two decades ago. Is this commercial software built on top that is closed source? I’m astonished ANSYS and NASTRAN are still the only players in town. I remember using NASTRAN 20 years ago for FE of structures while doing aero engineering. And even then NASTRAN was almost 40 years old and ancient. reply formerly_proven 18 hours agorootparentThere's a bunch of open source fem solvers e.g. Calculix, Code_Aster, OpenRadioss and probably a few unmaintained forks of (NASA) NASTRAN, but there's no multiphysics package I don't think. reply bobim 11 hours agorootparentThese are at least capable of thermomechanical with fluid-structure coupling. Not all-physics but still multi. True that things with multi species diffusion or electromagnetics are missing, but maybe Elmer can fill the gap. reply hhoorzad 22 hours agoparentprevAbaqus is pretty big too. I've worked with both Ansys and Abaqus and I generally prefer the latter. reply foxglacier 9 hours agoparentprevI work in this field and it really is stagnant and dominated by high-priced Ansys/etc. For some reason silicon valley's open sourceness hasn't touched it. For open source, there's CalculiX which is full of bugs and Code Aster which everybody I've heard about it from say it's too confusing to use. CalculiX has Prepomax as a fairly new and popular pre/post. reply littlestymaar 20 hours agoparentprev> The only new player seems to be COMSOL Ouch. I kind of know Comsol because it was already taught in my engineering school 15 years ago, so that it still counts as a “new entrant” really gives an idea of how slow the field evolves. reply petters 20 hours agorootparentThe COMSOL company was started in 1986.... reply kayo_20211030 23 hours agoparentprevAs a recovering fe modeler, I understand completely. reply MengerSponge 16 hours agoparentprevOnce you have a mesh that's \"good enough\", you can use any number of numeric solvers. COMSOL has a very good mesher, and a competent geometry editor. It's scriptable, and their solvers are also very good. There might be better programs for some problems, but COMSOL is quite nice. reply antononcube 18 hours agoprevDuring my industrial PhD, I created an Object-Oriented Programming (OOP) framework for Large Scale Air-Pollution (LSAP) simulations. The OOP framework I created was based on Petrov-Galerkin FEM. (Both proper 2D and \"layered\" 3D.) Before my PhD work, the people I worked with (worked for) used spectral methods and Alternate-direction FEM (i.e. using 1D to approximate 2D.) In some conferences and interviews certain scientists would tell me that programming FEM is easy (for LSAP.) I always kind of agree and ask how many times they have done it. (For LSAP or anything else.) I was not getting an answer from those scientists... Applying FEM to real-life problems can involve the resolving of quite a lot of \"little\" practical and theoretical gotchas, bugs, etc. reply chipdart 12 hours agoparent> Applying FEM to real-life problems can involve the resolving of quite a lot of \"little\" practical and theoretical gotchas, bugs, etc. FEM at it's core ends up being just a technique to find approximate solutions to problems expressed with partial differential equations. Finding solutions to practical problems that meet both boundary conditions and domain is practically impossible to have with analytical methods. FEM trades off correctness with an approximation that can be exact in prescribed boundary conditions but is an approximation in both how domains are expressed and the solution,and has nice properties such as the approximation errors converging to the exact solution by refining the approximation. This means exponentially larger computational budgets. reply kaonwarb 22 hours agoprevI also studied FEM in undergrad and grad school. There's something very satisfying about breaking an intractably difficult real-world problem up into finite chunks of simplified, simulated reality and getting a useful, albeit explicitly imperfect, answer out of the other end. I find myself thinking about this approach often. reply pvg 23 hours agoprevA 45 comment thread at the time https://news.ycombinator.com/item?id=33480799 reply niraj-agarwal 20 hours agoprevPredicting how things evolve in space-time is a fundamental need. Finite element methods deserve the glory of a place at the top of the HN list. I opted for \"orthogonal collocation\" as the method of choice for my model back in the day because it was faster and more fitting to the problem at hand. A couple of my fellow researchers did use FEM. It was all the rage in the 90s for sure. reply bgoated01 14 hours agoprevInteresting perspective. I just attended an academic conference on isogeometric analysis (IGA), which is briefly mentioned in this article. Tom Hughes, who is mentioned several times, is now the de facto leader of the IGA research community. IGA has a lot of potential to solve many of the pain points of FEM. It has better convergence rates in general, allows for better timesteps in explicit solvers, has better methods to ensure stability in, e.g., incompressible solids, and perhaps most exciting, enables an immersed approach, where the problem of meshing is all but gone as the geometry is just immersed in a background grid that is easy to mesh. There is still a lot to be done to drive adoption in industry, but this is likely the future of FEM. reply chipdart 12 hours agoparent> IGA has a lot of potential to solve many of the pain points of FEM. Isn't IGA's shtick just replacing classical shape functions with the splines used to specify the geometry? If I recall correctly convergence rates are exactly the same, but the whole approach fails to realize that, other than boundaries, geometry and the fields of quantities of interest do not have the same spatial distributions. IGA has been around for ages, and never materialized beyond the \"let's reuse the CAD functions\" trick, which ends up making the problem more complex without any tangible return when compared with plain old P-refinent. What is left in terms of potential? > Tom Hughes, who is mentioned several times, is now the de facto leader of the IGA research community. I recall the name Tom Hughes. I have his FEM book and he's been for years (decades) the only one pushing the concept. The reason being that the whole computational mechanics community looked at it,found it interesting, but ultimately wasn't worth the trouble. There are far more interesting and promising ideas in FEM than using splines to build elements. reply bgoated01 4 hours agorootparent> Isn't IGA's shtick just replacing classical shape functions with the splines used to specify the geometry? That's how it started, yes. The splines used to specify the geometry are trimmed surfaces, and IGA has expanded from there to the use of splines generally as the shape functions, as well as trimming of volumes, etc. This use of smooth splines as shape functions improves the accuracy per degree of freedom. > If I recall correctly convergence rates are exactly the same Okay, looks like I remembered wrong here. What we do definitely see is that in IGA you get the convergence rates of higher degrees without drastically increasing your degree of freedom, meaning that there is better accuracy per degree of freedom for any degree above 1. See for example Figures 16 and 18 in this paper: https://www.researchgate.net/profile/Laurens-Coox/publicatio... > geometry and the fields of quantities of interest do not have the same spatial distributions. Using the same shape functions doesn't automatically mean that they will have the same spatial distributions. In fact, with hierarchical refinement in splines you can refine the geometry and any single field of interest separately. > What is left in terms of potential? The biggest potential other than higher accuracy per degree of freedom is perhaps trimming. In FEM, trimming your shape functions makes the solution unusable. In IGA, you can immerse your model in a \"brick\" of smooth spline shape functions, trim off the region outside, and run the simulation while still getting optimal convergence properties. This effectively means little to no meshing required. For a company that is readying this for use in industry, take a look at https://coreform.com/ (disclosure, I used to be a software developer there). reply fngarrett 15 hours agoprevFor anyone interested in a contemporary implementation, SELF is a spectral element library in object-oriented fortran [1]. The devs here at Fluid Numerics have upcoming benchmarks on our MI300A system and other cool hardware. [1] https://github.com/FluidNumerics/SELF reply greesil 23 hours agoprevI took a course in undergrad, and was exposed to it in grad school again, and for the life of me I still don't understand the derivations either Galerkin or variational. reply faustlast 2 hours agoparentI learned from the structural engineering perspective. What are you struggling with? In my mind I have this logic flow: 1. strong form pde; 2. weak form; 3. discretized weak form; 4. compute integrals (numerically) over each element; 5. assemble the linear system; 6. solve the linear system. reply mlhpdx 22 hours agoprevI have such a fondness for FEA. ANSYS and COSMOS were the ones I used, and I’ve written toy modelers and solvers (one for my HP 48g) and even tinkered with using GPUs for getting answers faster (back in the early 2000s). Unfortunately my experience is that FEA is a blunt instrument with narrow practical applications. Where it’s needed, it is absolutely fantastic. Where it’s used when it isn’t needed, it’s quite the albatross. reply sashank_1509 22 hours agoprevMy hot take is that, FEM is best used as unit testing of Machine Design, not a guide towards design that it’s often used as. The greatest mechanical engineer I know, once designed an entire mechanical wrist arm with five fingers, actuations, lots of parts and flexible finger tendon. He never used FEM at any part of his design. He instead did it in the old fashioned, design and fab a simple prototype, get a feel for it, use the tolerances you discovered in the next prototype and just keep iterating quickly. If I went to him and told him to model the flexor of his fingers in FEM, and then gave him a book to tell him how to correctly use the FEM software so that you got non “non-sensical” results I would have slowed him down if anything. Just build and you learn the tolerances, and the skill is in building many cheap prototypes to get the best idea of what the final expensive build will look like. reply eru 22 hours agoparent> The greatest mechanical engineer I know, [...] And with that you wrote the best reply to your own comment. Great programmers of the past wrote amazing systems just in assembly. But you needed to be a great programmer just to get anything done at all. Nowadays dunces like me can write reasonable software in high level languages with plenty of libraries. That's progress. Similar for mechanical engineering. (Doing prototypes etc might still be a good idea, of course. My argument is mainly that what works for the best engineers doesn't necessarily work for the masses.) reply navane 22 hours agorootparentAlso, might work for a mechanical arm the size of an arm, but not for the size of the Eiffel tower. reply sashank_1509 18 hours agorootparentEiffel Tower was built before FEM existed. In fact I doubt they even did FEM like calculations reply mitthrowaway2 15 hours agorootparentThis is true, although it was notable as an early application of Euler-Bernoulli beam theory in structural engineering, which helped to prove the usefulness of that method. reply navane 18 hours agorootparentprevI ment a mechanical arm the size of the eifel tower. You don't want to iterate physical products at that size. reply kragen 18 hours agorootparentGoing by Boeing vs. SpaceX, iteration seems to be the most effective approach to building robotic physical products the size of the Eiffel Tower. reply eru 11 hours agorootparentI'm sure they are doing plenty of calculations beforehand, too. reply kragen 8 hours agorootparentUnquestionably! Using FEM. reply fluorinerocket 22 hours agoparentprevWould FEM be useful for that kind problem? It's more for figuring out if your structure will take the load, where stress concentrations are, what happens with thermal expansion. FEM won't do much for figuring out what the tolerance need to be on intricate mechanisms reply angry_moose 21 hours agoparentprevTo be fair, FEM is not the right tool for mechanical linkage design (if anything, you'd use rigid body dynamics). FEM is the tool you'd use to tell when and where the mechanical linkage assembly will break. reply NathanaelRea 21 hours agoparentprevGarbage in garbage out. If you don't fully understand the model, then small parameter changes can create wildly different results. It's always good to go back to fundamentals and hand check a simplification to get a feel for how it should behave. reply amelius 20 hours agoparentprevIf he were designing a bridge, however ... reply fastasucan 18 hours agoparentprevIts wrong to assume that everyone and every projects can use an iterative method with endless prototypes. Id you do I have a prototype bridge to sell you. reply antegamisou 21 hours agoparentprevGood luck designing crash resilient structures without simulating it on FEM based software though. reply somat 18 hours agorootparentThe FEM is just a model of the crash resistant structure. Hopefully it will behave like the actual structure, but that is not guaranteed. We use the FEM because it is faster and cheaper than doing the tests on the actual thing. However if you have the time and money to do your crash resiliency tests on the actual product during the development phase. I expect the results would be much better. reply formerly_proven 7 hours agorootparentYes, with infinite time and budget you'd get much better results. That does not sound like an interesting proposition, though. reply sashank_1509 19 hours agorootparentprevI’d guess most of the bridges in US were built before FEM existed reply meindnoch 8 hours agorootparentAnyone can design a bridge that holds up. Romans did it millenia ago. Engineering is designing a bridge that holds up to a certain load, with the least amount of material and/or cost. FEM gives you tighter bounds on that. reply drpossum 8 hours agorootparentprevThe average age of a bridge in the US is about 40-50 years old and the title of the article has \"80 years of FEM\". https://www.infrastructurereportcard.org/wp-content/uploads/... I'd posit a large fraction were designed with FEM. reply double0jimb0 15 hours agorootparentprevFEM runs on the same math and theories those bridges were designed on on paper. reply tightbookkeeper 20 hours agorootparentprevThey did this just fine until without such tools for the majority of innovation in the last century. reply drpossum 8 hours agorootparentHaving worked on the design of safety structures with mechanical engineers for a few projects, it is far, far cheaper to do a simulation and iterate over designs and situations than do that in a lab or work it out by hand. The type of stuff you can do on paper without FEM tends to be significantly oversimplified. It doesn't replace things like actual tests, but it makes designing and understanding testing more efficient and more effective. It is also much easier to convince reviewers you've done your job correctly with them. I'd argue computer simulation has been an important component a majority of mechanical engineering innovation in the last century. If you asked a mechanical engineer to ignore those tools in their job they'd (rightly) throw a fit. We did \"just fine\" without cars for the majority of humanity, but motorized vehicles significantly changed how we do things and changed the reach of what we can do. reply tightbookkeeper 4 hours agorootparent> It is also much easier to convince reviewers you've done your job correctly with them. In other words, the work that doesn't change the underlying reality of the product? > We did \"just fine\" without cars for the majority of humanity We went to the moon, invented aircraft, bridges, skyscrapers, etc, all without FEM. So that's why this is a bad comparison. > If you asked a mechanical engineer to ignore those tools in their job they'd (rightly) throw a fit. Of course. That's what they are accustomed to. 80/20 paper techniques that were replaced by SW were forgotten. When tests are cheap, you make a lot of them. When they are expensive, you do a few and maximize the information you learn from them. I'm not arguing FEM doesn't provide net benefit to the industry. reply antegamisou 16 hours agorootparentprevExcept that everything's gotten abysmally complex. Vehicle crash test experiments are a good example of validating the FEM simulation (yes that's the correct order, not vice versa) reply tightbookkeeper 15 hours agorootparentHow can you assert so confidently you know the cause and effect? Certainly computers allow more complexity, so there is interplay between what it enables and what’s driven by good engineering. reply cyberax 20 hours agoprevFEM - because we can't solve PDEs! reply westurner 18 hours agoprev [–] From \"Chaos researchers can now predict perilous points of no return\" (2022) https://news.ycombinator.com/item?id=32862414 : > FEM: Finite Element Method: https://en.wikipedia.org/wiki/Finite_element_method >> FEM: Finite Element Method (for ~solving coupled PDEs (Partial Differential Equations)) >> FEA: Finite Element Analysis (applied FEM) > awesome-mecheng > Finite Element Analysis: https://github.com/m2n037/awesome-mecheng#fea And also, \"Learning quantum Hamiltonians at any temperature in polynomial time\" (2024) https://arxiv.org/abs/2310.02243 re: the \"relaxation technique\" .. https://news.ycombinator.com/item?id=40396171 reply Consider applying for YC's W25 batch! Applications are open till Nov 12. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article reviews the evolution of the Finite Element Method (FEM) over 80 years, highlighting its significance in engineering and scientific modeling, particularly in solid mechanics.- FEM's development is categorized into four periods: early years (1941-1965), golden age (1966-1991), industrial applications and materials modeling (1992-2017), and the present and future.- It emphasizes FEM's integration with modern computational techniques like machine learning, its impact on industries, and its role in advancing engineering education and software development."
    ],
    "commentSummary": [
      "The Finite Element Method (FEM) remains a fundamental tool in engineering, but its practical application has seen little innovation, with many advancements not succeeding in real-world use.",
      "Industry attention has moved towards Verification and Validation, emphasizing FEM's limitations, while commercial software such as ANSYS and NASTRAN continue to lead the market.",
      "Emerging methods like Isogeometric Analysis (IGA) and Neural Operators offer potential but have not yet gained widespread adoption."
    ],
    "points": 182,
    "commentCount": 92,
    "retryCount": 0,
    "time": 1730575420
  },
  {
    "id": 42028833,
    "title": "Next Generation Out of Band Garbage Collection",
    "originLink": "https://railsatscale.com/2024-10-23-next-generation-oob-gc/",
    "originBody": "Next Generation Out of Band Garbage Collection 2024-10-23 • Jean Boussier In 2023, I wrote about how we’ve tuned Ruby’s garbage collector for Shopify’s monolith, including how we implemented out-of-band garbage collection to reduce the impact of major collection on latency. While the latency improvements were massive, we weren’t entirely satisfied with the heuristics used to trigger out-of-band garbage collection. It was purely based on averages, so we had to trade latency for capacity. More importantly, it didn’t fully eliminate major collection from request cycles, it only made it very rare. But in December 2023, while discussing with Koichi Sasada, we came up with a new idea. Disabling Major GC Entirely If we want major GC to never trigger during a request cycle, why not disable it entirely? In March 2024, during our annual Ruby Infrastructure team gathering, we fleshed out the details of the new feature we wanted, and Matthew Valentine-House started working on a proof of concept, which we then deployed to a small percentage of our production servers to see how effective it could be. First, we needed a way to entirely prevent the Garbage Collector from automatically performing a major collection, but also to stop promoting objects to the old generation. Ideally in a web application, aside from some in-memory caches, no object allocated as part of a request should survive longer than the request itself. Any object that does is probably something that should be eagerly loaded during boot, or some state that is leaking between requests. As such, any object promoted to the old generation during a request cycle is very unlikely to be immortal, so promoting it is wasteful. We also needed a way to ask the GC whether it would have run a major collection so that we could manually trigger it outside of the request cycle, and only exactly as much as needed. The initial proposal was for three new methods, GC.disable_major, GC.enable_major and GC.needs_major?. After some back and forth with other Ruby committers, it became a single new method: GC.config(rgengc_allow_full_mark: true/false). We also exposed a new key in GC.latest_gc_info, :needs_major_by, for use in checking whether a major GC needs to run: GC.latest_gc_info(:needs_major_by). This new feature was released as part of Ruby 3.4.0-preview2. Effectiveness Since Shopify monolith runs on Ruby’s master branch, we don’t have to wait for the December release to use these new features, so recently I went to work on enabling the new out-of-band GC implementation on 50% of production servers, and the results are amazing on all metrics. First, as we anticipated, the time spent in GC during request cycles at the very tail end (p95/p99/p99.99) dropped very significantly. However, more surprisingly, it also improved median latency: The overall impact on service latency is of course more modest, but still very nice with a 5% reduction of average latency and a 10% reduction of p99 latency: The impact on capacity, however, is less significant than we had hoped for. During the day, when there are frequent deploys, this doesn’t make much of a difference. However when deploys pause for a few hours, the new out-of-band collector runs much less often than the old implementation: Implementation In addition, to be more effective, this new implementation is also radically simple, thanks to the hooks provided by Pitchfork # pitchfork.conf.rb after_worker_fork do |_server, _worker| GC.config(rgengc_allow_full_mark: false) end after_request_complete do |_server, _worker, _rack_env| if GC.latest_gc_info(:need_major_by) GC.start end end Next Steps? Now that the major collection is out of the picture, the next step is to look at the minor collections. We can’t disable minor collection, as otherwise large requests that allocate a lot would run out of memory. However, we could try to additionally use heuristics from GC.stat to eagerly trigger minor garbage collection out-of-band, so that the majority of requests don’t have to spend any time at all in GC. But the potential gains are much smaller because minor collection is quite fast even on our monolith.",
    "commentLink": "https://news.ycombinator.com/item?id=42028833",
    "commentBody": "Next Generation Out of Band Garbage Collection (railsatscale.com)153 points by ksec 23 hours agohidepastfavorite75 comments cletus 6 hours ago> no object allocated as part of a request should survive longer than the request itself So I've spent a lot of time doing Hack (and PHP) as well as Java, Python and other languages. For me, as far as serving HTTP requests goes, Hack/PHP are almost the perfect language. Why? 1. A stateless functional core. There's no loading of large libraries, which is an issue with Python and Java in certain paradigms. The core API us just functions that mean startup costs for a non-stateful service are near zero; 2. The model, as alluded to the above quote, basically creates temporary objects and then tears everything down at the end of the request. It's so much more difficult to leak resources this way as opposed to, say, a stateful Java or C++ server. PHP got a lot of hate unjustly for its \"global\" scope when in fact it's not global at all. \"Global\" in PHP/Hack is simply request-scoped and pretty much every language offers request-scoping; 3. There's no threading. Hack, in particular, uses a cooperative async/await model. Where you'd normally create threads (eg making a network request), that's handled by the runtime to make an async/await call out of non-blocking I/O. You never have to deal with mutexes, thread starvation, thread pools, lock ups, etc. You never want to deal with that in \"application\" or \"product\" code. Never. So this article is specific to Ruby-on-Rails, which obviously still has persistent objects, hence the need for GC still. How Facebook deals with this is kinda interesting. Most FB product code uses an in-memory write-through graph database (called TAO, backed to MySQL). There is an entity model in Hack on top of this that does a whole bunch of stuff like enforcing privacy (ie you basically never talk to TAO directly and if you do, you're going to have to explain why that's necessary, and you absolutely never talk to MySQL directly). But the point is that persistent entities are request-scoped as well (unlike RoR I guess?). reply BoppreH 4 hours agoparentIf the core API is just functions, how do stateful applications handle connections to this persistent storage? Can you still have a connection pool, or does every request pay the extra latency to start a new connection and re-authenticate? reply spullara 1 hour agorootparenti think they do now but originally one of the reasons people use mysql with languages that are connection per request was that mysql connections were very cheap. reply derefr 47 minutes agorootparentAnd why pgbouncer used to be considered an essential part of a Postgres web-app-backend deployment — if your business layer didn’t pool and reuse connections, then having an external shim component that pools and reuses connections would solve a lot of the impedance mismatch. reply hinkley 20 hours agoprev> Ideally in a web application, aside from some in-memory caches, no object allocated as part of a request should survive longer than the request itself. This is one of those areas where out of process caching wins. In process caching has a nasty habit of putting freshly created objects into collections that have survived for days or hours, creating writes in the old generation and back references from old to new. Going out of process makes it someone else’s problem. And if it’s a compiled language with no or a better GC, all the better. reply the_mitsuhiko 8 hours agoparentOne of the challenges I see in general is that languages don't have enough capabilities to express intent of lifetimes / control flow. What I mean by that is that there is a significant difference between spawning a thread with the intention of joining, allocating memory with the intention of only lasting to the end of the request etc. vs spawning a permanent background thread or stashing away an object into a global cache. This is starting to really become a problem in the observability space and async locals. Node.js for instance currently will keep async locals around for too long of a time because they are propagated everywhere. For instance if you call `console.log` in a promise you will leak an async local forever. Next.js famously keeps around way too many async locals past the request boundary for caching related reasons. A solution would be to have a trampoline to call things through that make it explicit that everything happening past that point is supposed to \"detach\" from the current flow. An allocator or a context local system can then use that information to change behavior. reply dinosaurdynasty 3 hours agorootparentYou could do something like that in Rust with a request-scooped arena. But then you'd have to do Rust. In node you could use worker threads (which create a new V8 instance in a separate OS thread) but that's probably too heavy handed. reply byroot 20 hours agoparentprevAuthor here. Agreed. We have some facility for out of process caching (node local memcached), and I frequently have to argue with colleagues that it's generally preferable to in-process caching. reply foobazgt 16 hours agorootparentA similar strategy is to serialize the object and store it in-process but off heap. This is useful when the values are private to the process, and/or they don't need to survive a crash, and/or you need to avoid the network overhead. Access times are often 100x-1000x faster. reply kgeist 9 hours agorootparentThis is something I've been thinking about for a while. What if we create a language where all object references are explicitly specified to be either request-scoped or application-scoped. Don't allow application-scoped objects reference request-scoped objects. Allow to manually upgrade a reference from request-scoped to application-scoped if needed. That would allow us to have ephemeral per-request heaps which are torn down after every request at once. In-request garbage collections are super-fast. Application-scoped objects are never collected (i.e. no major collections). Wouldn't this simple model solve most problems? Basically, a very simple equivalent to Rust's lifetimes tailored to web services without all the complexity, and much less GC overhead than in traditional GC systems. reply jnwatson 7 hours agorootparent\"What if we create a language where all object references are explicitly specified to be either request-scoped or application-scoped.\" I've done this in both C and C++. The downside of automatic memory management is you have to accept the decisions the memory manager makes. Still, generational GC like in Ruby and Python essentially attempts to discern the lifetime of allocations, and it gets it right most of the time. reply JonChesterfield 9 hours agorootparentprevYou could call them \"arenas\" to be consistent with the prior art. Yes, if you can partition the heaps into ones with distinct lifetimes, good plan. reply ryukoposting 7 hours agorootparentprevGiven that Rust seems to be the generalized solution to this problem, would a viable prototype just be a Rust HTTP server with a Ruby interpreter embedded in it? Write the code in some kind of Ruby DSL which then feeds back into Rust? I ask because I have embedded Ruby in applications before, and I'm looking for an excuse to do it in Rust. reply hinkley 18 hours agorootparentprevI accept that it is true but I bristle at the fact of it. It shouldn’t be true. reply byroot 11 hours agorootparentDepends, it's not just about access time and GC pressure, it's also about sharing that cache with other processes on the node. reply tsimionescu 11 hours agoparentprevThis could also be a case for non-managed objects in the same process. APIs aren't typically very friendly, but I would expect they could be made so, especially if it was for a dedicated use-case like caching. reply echelon 17 hours agoparentprevRust is emerging as a major contender for HTTP/gRPC backend services. Actix, Axum, sqlx, diesel, and a whole host of other utilities and frameworks make writing Rust for HTTP just as easy and developer efficient as Golang or Java, but the code will never have to deal with GC. It's easy to pull request scoped objects into durable caches. reply jeeyoungk 22 hours agoprevThis is from several years ago (2017), but this has very similar vibe as Instagram disabling Python GC - https://instagram-engineering.com/dismissing-python-garbage-... reply IshKebab 20 hours agoparentA 10% performance improvement on Python code is laughable. You can get a 5000% performance improvement if you switch to a better language. reply ziddoap 19 hours agorootparentI bet the engineers at Instagram were unaware of pythons performance profile when they chose it, you should let them know that they should just switch to a different language. reply byroot 19 hours agorootparentMeta is just a small startup though, they probably don't have enough resources nor the skills to switch to a better language even after they've heard the gospel. reply IshKebab 9 hours agorootparentI don't know if you're joking or not but that is exactly true. Meta went as far as creating their own PHP engine and then a new PHO compatible language because they didn't have the resources to switch from PHP. Instagram is presumably in the same position. Switching language is basically impossible once you have a certain amount of code. I'm sure they were aware of the performance issues with Python but they probably said \"we'll worry about it later\" when they were a small startup and now it's too late. reply eru 8 hours agorootparentWell, Facebook also created their own hacked up version of PHP (called Hack) that's presumably easier to migrate PHP to. Hack is actually surprisingly pleasant, basically about the best language they could have made starting from PHP. (I know, that's damning with faint praise. But I actually mean this unironically. It has TypeScript vibes.) reply krapp 8 hours agorootparentI was excited about Hack when it came out. Unfortunately PHP took just enough from it to kill it. I gave up on it once Composer stopped supporting it, after backwards compatibility with PHP no longer became a goal. IMHO Hack's best feature was native support for XHP... which (also unfortunately) isn't something PHP decided to take. reply Twirrim 37 minutes agorootparentHack really gave PHP/Zend the kick up the arse that it seemed to need. reply eru 8 hours agorootparentprevI only used Hack when I was very briefly working for Facebook. (And I used PHP once before nearly 20 years ago by now for some web site I had 'inherited', back when PHP was truly an awful language) reply IshKebab 6 hours agorootparentprevYes I mentioned that. reply immibis 10 hours agorootparentprevThe joke is that Facebook literally did, right? reply byroot 10 hours agorootparentDid what? Rewrite Instagram into another language? Do you have any source on this? Last time I checked they're working on improving Python performance instead (yes I know they forked it into Cinder, but they're trying to upstream their optimizations [0]). Which is very similar to what we're doing at Shopify. Of course 100% of Instagram isn't in Python, I'm certain there's lots of supporting services in C++ etc, but AFAIK the Instagram \"frontend\" is still largely a Python/Django app. The joke is that if Meta thought that replacing all the Python code they have with something else was worth it, they'd have done it already. [0] https://github.com/facebookincubator/cinder reply lucianbr 9 hours agorootparent> The joke is that if Meta thought that replacing all the Python code they have with something else was worth it, they'd have done it already. \"Worth it\" depends on both how much performance improvement you get, and how hard it is to replace. Did you consider maybe the rewriting effort is so humongous that it is not worth doing despite large performance improvements? Thus making the joke not funny at all... reply byroot 8 hours agorootparentThat's exactly the joke though. Every time Ruby (or Python) is discussed on HN we get the same old tired question of \"why don't they just rewrite in Rust\". But that's some silly engineer tunnel vision, squeezing the very last bit of performance out of a system isn't a goal in itself. You just need it to be efficient enough that it cost you significantly less to run that the amount of revenue it brings you. I can bet you that moving off Python must have been pitched dozens and dozens of time by Meta engineers, but deemed not worth it, because execution speed isn't the only important characteristic. So yes, I find it hilarious when HN commenters suggests companies should rewrite all their software into whatever is seen as the most performant one. reply IshKebab 6 hours agorootparentIt's usually dismissed because companies think short term, and switching languages is a project with huge short term disadvantages and huge long term advantages. reply debugnik 9 hours agorootparentprevI think they meant Facebook switched their PHP code to Hack and HHVM, their own PHP-like language and implementation. reply worik 13 hours agorootparentprev> I bet the engineers at Instagram were unaware of pythons performance profile when they chose it, Is the Instagram stack Python? I doubt it, but stranger things have happened I suspect it is actually some derivative of Apache, or Nginx. Something sensible reply httgp 11 hours agorootparentInstagram is built with Django. reply electricwire 19 hours agorootparentprevThey may well have been initially, its a pretty puzzling choice reply rty32 18 hours agorootparentprevVery naive take. 1. 10% performance improvement at Instagram could lead to many millions of revenue \"instantly\". It is not laughable at any company. 2. It won't be a 5000% performance improvement. Facebook uses its own fork of Python that is heavily optimized. Probably still far from C++, but you should be thinking about languages like Java when talking about performance. \"Better\" is a very subjective term when discussing languages, and I hope such discussions can be more productive and meaningful. reply foobazgt 16 hours agorootparentCinder's benchmarks don't seem \"like Java\" performance, given they aren't that far off cython. https://github.com/facebookincubator/cinder/blob/cinder/3.8/... reply eru 8 hours agorootparentCPython itself has seen lots of performance improvements recently. Benchmarks on CPython 3.12 take about half the time they took on CPython 3.9. reply IshKebab 6 hours agorootparentYeah it's definitely welcome, but even if it is double the performance (doesn't seem to be quite there in my experience) fast languages are still 25-50x faster. It's like walking twice as fast when the alternative is driving. reply lucianbr 9 hours agorootparentprev> Facebook uses its own fork of Python that is heavily optimized. So likely the 5000% improvement is no longer possible because they already did multiple 10% improvements? I don't know how this counters the original point. All clues point to FB going this route because they had too much code already in PHP, and not because the performance improvement would be small. In any case, \"facebook does it\" is not a good argument that something is the right thing to do. Might be, might not be. FB isn't above wrong decisions. Else we should buy \"real estate\" in the metaverse. reply spullara 18 hours agoprevAll the other virtual machines that support GC need to look at the JVM's ZGC and Shenandoah. Sub-millisecond pause times with terabyte heaps. reply byroot 10 hours agoparentWe're actually hoping to get mmtK included in Ruby to be able to use more advanced GCs. Medium term we hope to use Immix: https://bugs.ruby-lang.org/issues/20860 And yes, we're aware of ZGC &co https://www.eightbitraptor.com/presentations/RubyKaigi2023-m... reply bob1029 15 hours agoparentprevI think we should be careful when correlating heap size with how long the collection should take. Also, I really want ZGC in .NET runtime, but I don't think I'll ever get support for it first party. There's some kind of principled ideologue holdout situation going on over at Microsoft. Every time I get into it with one of their engineers I'm sent to some impotent \"please may I have a temporary GC exemption\" API. All I want is it to do nothing. How hard is it to just not clean up the goddamn garbage? Give me a registry flag + env variable + cli arg all required at the same time if you're so worried someone might trip over it. reply reubenbond 13 hours agorootparentWould GC.TryStartNoGCRegion work for you? https://learn.microsoft.com/en-us/dotnet/api/system.gc.tryst... reply neonsunset 7 hours agorootparentprevZGC does not stand for zero. It stands for Z Garbage Collector. It's a next-generation GC implementation for OpenJDK that focuses on low pause time while supporting very large heap sizes. It does not \"not collect garbage\". You could try using https://github.com/kkokosa/UpsilonGC and seeing if it still works. At the end of the day for anything performance-related you can just write code with manual memory management with RAII patterns via IDisposable on structs and get code that performs closely to C++ or Rust. It's also necessary to understand if this is a good idea at all - most of the time you do want to just rely on GC. reply bob1029 6 hours agorootparent> ZGC does not stand for zero. It stands for Z Garbage Collector. Apologies - I was attempting to referring to \"absolutely no\" garbage collection path. I was thinking of Epsilon [0]. > It's also necessary to understand if this is a good idea at all - most of the time you do want to just rely on GC. Assume we are building a cruise missile flight computer. I have enough ram for ~100 hours of flight if we never clean up any allocations. I only have enough fuel for 8 hours of flight on a good day. Why do I still need a garbage collector? All I need is a garbage generator. The terminal ballistics and warhead are the \"out of band\" aspects in this arrangement. > You could try using https://github.com/kkokosa/UpsilonGC and seeing if it still works. I've spent weeks on this exact thing. I cannot get it to work. This gets me back to the first party support aspect. [0] https://openjdk.org/jeps/318 reply algorithmsRcool 15 hours agoparentprevHow much of a throughput penalty do those options incur on the application? reply spullara 14 hours agorootparentI think it only affects throughput at the limit at the 5% level. All the portfolio companies that implemented it got a net increase in performance as they avoid redlining their servers. reply oorza 16 hours agoparentprevC4 still smokes them both, doesn't it? reply spullara 9 hours agorootparenthard to smoke sub-millisecond pauses but there may be other axes where it is better. it used to be that people thought azul was better because it was generational but now zgc is as well. my guess is that c4 doesn't have enough of an edge at this point but happy to see benchmarks that prove otherwise. reply voiper1 10 hours agoprev> Ideally in a web application, aside from some in-memory caches, no object allocated as part of a request should survive longer than the request itself. Any object that does is probably something that should be eagerly loaded during boot, or some state that is leaking between requests. As such, any object promoted to the old generation during a request cycle is very unlikely to be immortal, so promoting it is wasteful. So could each request clean up it's own garbage when it finishes, so then they should never need any global garbage collection? reply byroot 10 hours agoparentThat's pretty much what I'm hinting at at the end when I mention minor GC. I don't think doing it after each request would be sensible, but counter intuitively, the time it takes to run GC isn't proportional to amount of garbage to collect, but to the number of live objects left (ignoring some minor things like finalizers). So on paper at least we could run a minor GC for very cheap after each request, but there's likely some better heuristics given currently the median request already spent less than 1ms in GC, so after every requests might be overdoing it. Also even if we were doing that, many requests would still have to run GC because they allocate more than there is memory available, so they need to clean their own garbage to continue, you can't delay GC indefinitely. But at least now, endpoints that spend too much time in GC are responsible for their own demise, so engineers responsible for a given endpoint performance have a clear signal that they should allocate less, whereas before it could easily discounted as being caused by lots of garbage left over by another collocated endpoint. reply hgo 7 hours agorootparentWould it be possible for the allocator/GC to know what allocations are made within a request and make a generation for specifically for it? Allocations too big to fit would be made like usual reply byroot 7 hours agorootparentThat's already what we effectively have. Since objects cannot be promoted to the old generation inside the request cycle, objects in the new gen are request allocated objects. So if we were to eagerly trigger a minor GC after a request, we'd have very little objects to scan, and only need to sweep garbage, which is only a small fraction of time spent in GC. reply sys64739 18 hours agoprevWhat dashboard software is that? reply mhh__ 17 hours agoparentLooks like grafana reply henning 20 hours agoprev [–] They built a large codebase on a language that doesn't let you control memory, because that makes you \"more productive\". So just having Rails allocate a per-request arena that is asynchronously freed which would force the programmer not to have any objects that outlive the request, or just pre-allocating memory for a fixed amount of request handling per server instance, or whatever allocation behavior you want to do that is generally possible in C/C++/Zig/Rust/Odin/etc, requires hacking on the language itself. Which means your changes have to go through the Ruby team first. Any additional changes would also need to go through them, which increases the cost of change. Then there is a permanent layer of indirection between your GC callbacks and the semantics of what those callbacks do. Instead of just writing out the custom allocators you want, because that's impossible. How depressing. reply xerxes901 20 hours agoparent> Which means your changes have to go through the Ruby team first. Any additional changes would also need to go through them … I do want to pick on this specifically - people can and should be patching open source projects they depend on and deploying them to production (exactly as described in the article). Something being in the language vs in “user” code should be no barrier to improving it. reply gmueckl 1 hour agorootparentPatching a dependency comes with significant downstream costs. You need to carry the patch forward to new upstream versions . This implies remembering that the dependency was patched, extracting a patch from the existing changed code, and reapplying the patch, fixing comflicts, recompiling the now special version of that dependency and running tests, checking/updating required license notices accordingly. This is in essence another form of technical dept. reply simonask 11 hours agorootparentprevThere's a pretty huge difference between implementing a performance optimization that works in your use case, and upstreaming that optimization to be generally usable. The latter is often orders of magnitude more work, and the existing solution is probably chosen to be well suited in general. reply jrockway 20 hours agoparentprev [–] I'm guessing that Zig, Rust, Oden, and \"etc.\" didn't exist when they started the codebase. Now they need to keep moving in their imperfect state. I don't think anyone would start a large company on Ruby today. (They would on Python, though, which is equally unfortunate.) reply byroot 19 hours agorootparentI don't see how it is imperfect. Per request arenas sound super cool on paper, and work very well on system with clear constraints. But if suddenly a request start allocating more than the arena can accommodate you're in a bit of a pickle. They're absolutely not a panacea. Setting aside the challenge of refactoring the Ruby VM to allow this sort of arenas, they'd be a terrible fit for Shopify's monolith. Ultimately, while it's a bit counter intuitive, GCs can perform extremely well in term of throughput. Ruby's GC isn't quite there yet, but still perform quite well and is improving every versions. reply samatman 17 hours agorootparent> allocating more than the arena can accommodate In Zig, at least, this isn't how arenas work. They're a wrapper around a backing allocator, so if the arena runs out of memory, then that means the process is out of memory, something no allocation strategy can fix (ignoring the fact that Zig returns a specific error when that happens, and maybe you can trigger some cache eviction or something like that). It's easy to set them to retain a 'reasonable' allocated capacity when they get reset, for whatever value of reasonable, so big allocation spikes get actually freed, but normal use just moves a pointer back and reuses that memory. I don't see Shopify harvesting a lot of value from a complete Zig rewrite, no. But arenas are basically ideal for the sort of memory use which web servers typically exhibit. reply byroot 10 hours agorootparent> something no allocation strategy can fix Well, yes, with a GC when your heap is full, you make space by getting rid of the garbage. Also, with a good GC, allocating is most of the time just bumping a pointer, exactly like an arena, and the collection time is proportional to the number of live objects, which when triggered out of band is basically 0. Hence why I think a well tuned GC really isn't that far off. reply ngrilly 9 hours agorootparentprevI really like Zig, and I’ve been thinking about using it for developing servers offering APIs over HTTP, using arenas bound to request lifetime. I think I would be comfortable developing like this myself. But all the devs in my org have only ever used managed-memory languages such as Java, C#, Python or JavaScript, which makes me hesitant, as I’m wondering about the learning curve, and of course the risk of use-after-free. Not something I would do anyway before Zig reaches 1.0. reply samatman 4 hours agorootparentI wrote substantial amounts of C, and Pascal/Delphi before that, before learning Zig, so you and I wouldn't see the same learning curve. That said, I found it straightforward to take up. Andrew Kelly places a great emphasis on simplicity in the sense Rich Hickey uses the term, so Zig has a small collection of complete solutions which compose well. Now is a great time to pick up the language, but I would say that production is not the right place to do that for a programmer learning memory management for the first time. Right now we're late in the release cycle, so I'd download a nightly rather than use 0.13, if you wanted to try it out. Advent of Code is coming up, so that's an option. Using a memory-managed language means you need to design a memory policy for the code. Zig's GeneralPurposeAllocator will catch use after free and double free in debug mode, but that can only create confidence in memory handling code if and when you can be sure that there aren't latent bugs waiting to trigger in production. Arenas help with that a lot, because they reduce N allocations and frees to 1, for any given set of allocations. But one still has to make sure that the lifetime of allocations within the arena doesn't outlast the round, and you can only get that by design in Zig, lifetimes and ownership aren't part of the type system like they are in Rust. In practice, or I should say with practice, this is readily achievable. At current levels of language maturity, small teams of experienced Zig developers can and do put servers into production with good results. But it's probably not time for larger teams to learn as they go and try the same thing. reply ngrilly 1 hour agorootparentI started programming in Pascal, C and C++, so personally I’m fine with manual memory management, especially with a language like Zig. I actually find it quite refreshing. I’m just wondering if it’s possible to “scale” this approach to a team of developers who may not have that past experience (having only worked with GCed languages) without ending in a code base littered with use-after-free errors. reply simonask 11 hours agorootparentprevAnd when the default arena size is often outgrown, you'll known from whatever diagnostics/logging/dashboard solution you are using. Which is incidentally also a great tool when optimizing per-request memory usage. Being explicit about memory has many advantages, and is a strict requirement when scaling. reply danmur 18 hours agorootparentprevI don't think using Zig over Python is gonna have the biggest impact in making your next big company successful. It's a drop in the ocean compared to the quality of people you have to actually design and build it. reply simonask 11 hours agorootparentIt doesn't matter for a startup trying to get acquired. It does matter for a company trying to scale its user base while keeping costs down. reply indulona 11 hours agorootparentprevThat is nonsense. If you can run your code on 10 servers instead of 1k servers, that is an insane time and money saver that could make or break a company. reply WJW 4 hours agorootparentFor most startups the difference between Rust and Ruby is not 10 servers vs 1000 but rather between using 0.1% of a CPU or 1% of a CPU. A single server running Rails will easily scale to hundreds of thousands of daily users. Most companies never get that many users in the first place, and those that do will have the funds to afford rewriting the hottest paths in a more performant language. reply bhaak 20 hours agorootparentprev [–] Startups are not large companies in the beginning. Although I'm not sure what the preferred language for quickly getting a startup up and running would be these days. reply floating-io 19 hours agorootparent [–] Whatever works and you can find enough developers for. The language (or the rest of the stack even) is rarely a barrier to success. What matters are a good idea, good motivation, and decent availability of competence. JMHO. reply Consider applying for YC's W25 batch! Applications are open till Nov 12. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "In 2023, Shopify optimized Ruby's garbage collector by implementing out-of-band garbage collection to reduce latency, though initial heuristics were not fully effective.- By March 2024, a proof of concept was developed to disable major garbage collections during request cycles, leading to the introduction of a new method, GC.config(rgengc_allow_full_mark: true/false), in Ruby 3.4.0-preview2.- Implementing this method on 50% of Shopify's servers resulted in significant latency improvements, with modest capacity gains, and future efforts will focus on optimizing minor collections."
    ],
    "commentSummary": [
      "The discussion highlights the advantages of using Hack/PHP for HTTP requests, focusing on its stateless functional core, request-scoped objects, and cooperative async/await model, which help avoid threading issues.",
      "It also explores garbage collection (GC) in Ruby-on-Rails and other languages, suggesting performance improvements through request-scoped memory management and advanced GC techniques, such as those in Java Virtual Machine's (JVM) Z Garbage Collector (ZGC).",
      "The challenges of switching programming languages for large codebases, like Instagram's use of Python, are noted, emphasizing the complexity of rewriting systems despite potential performance benefits."
    ],
    "points": 153,
    "commentCount": 75,
    "retryCount": 0,
    "time": 1730577450
  },
  {
    "id": 42032387,
    "title": "Matrix 2.0 Is Here",
    "originLink": "https://matrix.org/blog/2024/10/29/matrix-2.0-is-here/?resubmit",
    "originBody": "Matrix 2.0 Is Here! 29.10.2024 00:00 — General — Matthew Hodgson Hi all, Since the outset of Matrix, our aim has always been to provide a protocol that lets you build open, decentralised, secure communication apps which outperform the mainstream centralised alternatives. It’s been a twisty journey - first focusing on making Matrix work at all (back in 2014), and then getting it out of beta with Matrix 1.0 in 2019, and now focusing on making Matrix fast, usable and mainstream-ready with Matrix 2.0. Meanwhile, the pendulum of decentralisation continues to accelerate in our direction. Our friends at Bluesky have shown that it’s possible to build decentralised social apps which are mainstream friendly enough for Presidents to recommend them; Elon continues to destroy Twitter and showcase the importance of decentralisation to everyone, and even Meta is dabbling in decentralised social media (and decentralised communication!) So, where does Matrix sit in all this? Well, in order to make the transition to mainstream, we’ve been beavering away to implement four main pillars in Matrix 2.0: Instant login, instant launch, and instant sync (aka Simplified Sliding Sync, MSC4186) Next Generation Auth (aka Native OIDC, MSC3861) Native Matrix Encrypted Multiparty VoIP/Video (aka MatrixRTC, MSC4143) Invisible Encryption (MSC4153 & friends). Between these, we believe that Matrix can now be used to build apps which genuinely outperform the mainstream alternatives - and what’s more, these MSCs now all have implementations you can use today. The MSCs themselves are not all finalised and ready for merge (but are getting close), and when they pass FCP (Final Comment Period) to merge into the spec, we will formally bump the spec release to version 2.0. We actually declared Matrix 2.0 as ready for action back at The Matrix Conference last month, and now that the videos have been published you can watch the launch right here: Since the conference talk things have already moved on a bit, though, and we’ve landed a bunch of tweaks to address teething issues - and so here’s the current state of action: 1. Simplified Sliding Sync Simplified Sliding Sync is the final version of Sliding Sync - the API which provides instant login, launch & sync in Matrix 2.0. To say that the API has been through a lot of iterations is an understatement, but we’re finally there with MSC4186, which simplifies the original Sliding Sync API (MSC3575) by removing the concept of a server-determined room list ordering entirely, and instead lets the client sort the roomlist as needed (while letting the client paginate in the roomlist incrementally, to ensure instant responsivity, no matter how large your room list is). Simplified Sliding Sync is now implemented natively in Synapse as of 1.114, and so there is no longer any need to run a Sliding Sync Proxy in order to use the API. In fact, the Sliding Sync Proxy is being deprecated and the old Sliding Sync code will be removed from matrix-rust-sdk in the nearish future. Unfortunately we don’t have bandwidth to maintain both the native implementation Synapse as well as the proxy shim - and the proxy inevitably has suffered from a lot of limitations (e.g. having to do a full v2 initial sync for new logins, slowing them down to v2 performance - as well as duplicating storage between Synapse and the proxy). There’ll be a dedicated deprecation blog post for the proxy and pre-simplified Sliding Sync shortly. Meanwhile, work is well underway for native Sliding Sync support in conduit, conduwuit and grapevine - conduwuit guesses “next few weeks” for native Simplified Sync Support to land. Dendrite is unfortunately still not funded, so will happen on a best effort basis. In terms of performance, native Simplified Sliding Sync in Synapse is spectacular - outperforming the proxy throughout, and making the old sync v2 API look positively prehistoric. Gone are the days of waiting for your app to sync, even if you’ve been offline for weeks/months; gone are the days of waiting minutes to login, and gone are the days of staring at a spinner as your app launches. It really is a new era - and having been hyping it since the first demo at FOSDEM 2023, I promise we won’t bang on about sliding sync any more after this; it’s finally here and landed and we can move on and enjoy it! For more details, see Ivan’s talk from The Matrix Conference: 2. Next Generation Auth Next Generation Auth is what we’re calling the migration to using industry standard OpenID Connect as the authentication API in Matrix 2.0, moving on from the custom auth API that Matrix has historically used. We’re calling this Next Gen Auth because we were seeing a lot of folks incorrectly assuming that adopting OpenID Connect for auth meant we would somehow be encouraging 3rd party social login or single-sign-on - which is not the case. Next Gen Auth is simply swapping out Matrix’s old custom auth APIs with equivalents which are defined by the OpenID Foundation; after all, Matrix is a communication protocol, not an authentication protocol. In return, we get much more mature and secure authentication APIs - and access to the whole OpenID Identity Provider ecosystem, including support for Two Factor Auth and Multi-Factor Auth, hardware authentication tokens, passkeys, and device-based login flows (aka QR Login). We also stop both Matrix clients and servers having to implement the sprawling legacy Matrix authentication API surface - ensuring that users only ever hand their account password to their auth server, rather than having to trust their clients to handle it securely, which in turn plays much nicer with password managers (who only have to remember how to auth with your auth server, rather than a myriad different clients). It also lets you share authentication between apps if you want; gives us access_token refresh (at last!) to avoid long-lived access_tokens hanging around; and in future will also support OIDC scopes so you can limit the access particular clients get to your account. In short, Next Gen Auth is transformative, and the initial implementation at matrix-authentication-service (MAS) is ready for admins to deploy and use (and in future will be available embedded in Synapse too). Unfortunately it is not yet live on matrix.org (given we have to migrate tens of millions of accounts, complete with all the social login complexities) but we’re hoping to get it there in the coming months. Probably one of the clearest immediate benefits of Next Gen Auth is the ability to do a full login including setting up all your end-to-end-encryption simply by scanning a QR code on an existing client, courtesy of MSC4108 and OAuth 2.0 Device Authorization Grants. In other words, you don’t have to specify a server, or your username, or account password, or your recovery key - you just scan a QR code and you’re in. The very latest Element X releases on iOS & Android have this implemented and enabled by default, and so if you’re on a server which has deployed MAS, you can go to “link new device” in Element Web/Desktop to show a QR code to instantly log in. For more info on all things Next Gen Auth, see Quentin’s talk from The Matrix Conference: 3. Native Matrix Group VoIP/Video: MatrixRTC Next we have MatrixRTC: end-to-end-encrypted group voice and video conferencing over Matrix. Historically, group VoIP in Matrix has relied on third party conferencing systems (Jitsi, and before that FreeSWITCH) - providing no support for Matrix’s end-to-end encryption, or indeed Matrix’s user identities, decentralisation or decentralised access control. With MatrixRTC this changes: we now have a standard way to establish large-scale end-to-end-encrypted group video calls via Matrix, leveraging all the benefits of Matrix’s end-to-end-encryption infrastructure, user identity, room permissions, etc. It also supports different media stacks to actually handle the media conferencing - today, the main implementation uses the LiveKit SFU, but there’s also an experimental full-mesh WebRTC implementation. Element Call has been the driving app behind MatrixRTC, and as of today is now enabled in the release versions of both Element Web/Desktop and Element X to provide native MatrixRTC calling embedded in the apps: if you hit the video call button you will now have the option to spin up a MatrixRTC call via Element Call rather than via Jitsi or Legacy 1:1 calling, and if the room is end-to-end-encrypted, all the conference will be too. Meanwhile - MatrixRTC isn’t just Element Call: Famedly showed off experimental interop with FluffyChat at FOSDEM back in Feb, and Element showed off experimental interop with BigBlueButton in August. Given more and more conferencing tools are converging on LiveKit as a best-in-class SFU, it’s an amazing opportunity to use Matrix and MatrixRTC to power the end-to-end-encryption and decentralisation and get standardised voip/video interop from the outset. For more info, see Timo’s talk from The Matrix Conference: That said, there are a few caveats right now: We do not have interoperability between legacy Matrix 1:1 voice/video calling and MatrixRTC (and it’s not clear if/when we will get to it) - but Matrix 2.0 clients like Element X exclusively use MatrixRTC for VoIP/video, including for 1:1 calls. This is in order to only maintain one VoIP stack, and so that you get multidevice and multiuser support for free, even in 1:1s. As a result, we’re in the process of figuring out how to warn legacy callers that MatrixRTC-only clients won’t be able to answer their calls (e.g. MSC4220) - this hasn’t shipped yet. iOS 18 broke CallKit + WebRTC, so Element X iOS has had to disable fancy OS-natively-integrated MatrixRTC calling; and has a support issue open with Apple to try to solve this. We’ve had some fun teething issues thanks to the volume of signalling in MatrixRTC exposing some sync bugs - these are almost solved, but probably mean MatrixRTC should still be considered beta for a few more days until the fixes land. 4. Invisible Encryption The final pillar of Matrix 2.0 is Invisible Encryption - making Matrix’s end-to-end encryption as seamless and invisible as the centralised alternatives (Signal, WhatsApp, iMessage and friends). This does not mean reducing security in any way - just the opposite, in fact. It means: Ensuring that Unable To Decrypt (UTD) bugs never happen. Huge amounts of work has gone into this over the course of the year, especially via complement-crypto as a comprehensive end-to-end-test suite for both matrix-rust-sdk and matrix-js-sdk based Matrix clients. We are finally at the point where UTDs are so rare that most people simply never see them, and any reports get jumped on (and complement-crypto tests get written) whenever they emerge. Anecdotally, I now get way more “waiting for message…” errors on WhatsApp than I do on Matrix, these days! For more details on the Hunt For UTDs, see Kegan’s talk from The Matrix Conference: We are excluding non-cross-signed devices from Matrix (MSC4153). The fact that Matrix ever supported the idea of users enabling encryption on a device without proving that they are the valid owner by signing it (by verifying it with another device, or providing their recovery key/passphrase) is a nasty hangover from back before we introduced cross-signing. Nowadays, the fact that non-cross-signed devices exist acts to reduce security and complicate UX and implementations with big scary red warnings whenever an unverified device joins a conversation. Instead, once MSC4153 lands, we’re going to simply exclude unverified devices entirely - not encrypt to them, and not decrypt from them. We can then get rid of all the confusing warnings associated with them. We’re also solving the confusing “grey shield” warnings: “the authenticity of this message cannot be confirmed on this device” and similar. These are avoidable warnings caused by message keys which can no longer be tracked back to the original sender - e.g. if they’re restored from backup, or if the original sending device has been deleted. We’re fixing these with authenticated backup (MSC4048) and including device keys on Olm events (MSC4147) respectively. Finally, we’re moving to Trust On First Use (TOFU). This means that even if you didn’t explicitly verify another user, you still get warned if their identity changes (unlike previously, when it was ignored). An initial implementation just landed in matrix-rust-sdk a few weeks ago, and so appropriate warnings at the application level should arrive shortly - matching the equivalent Signal or WhatsApp “this user’s identity has changed” warnings, albeit not yet synced between devices. As you can probably tell, Invisible Encryption is something of a moving target; for instance, the scope could extend to cover all scenarios where users might expect messages to be decryptable (e.g. Dehydrated Devices: the ability to decrypt messages sent to you when you are not logged in anywhere - or Sharing Keys when new users join a room/space). However, the 4 points above are the main ones, and they are in the process of landing right now. The end result is already an immeasurable improvement in the reliability and robustness of Matrix’s encryption - and once the remaining pieces land, the user experience of a Matrix client should be that the encryption is almost entirely invisible to the user - unless something bad is happening; much like TLS. Valere & Patrick’s talk on Invisible Crypto from The Matrix Conference would get embedded here, but unfortunately the audio was mangled - we’ll rerecord it and publish it shortly. In the meantime, you can find their slides here. What’s next? Well, first of all we’re going to stop announcing Matrix 2.0 - it’s finally here (modulo the spec release)! There’s obviously a bit of follow-up still to be done though: Getting MAS live on matrix.org Gracefully handling the lack of interop between legacy Matrix calls and MatrixRTC Landing the various remaining components of Invisible Crypto Broadening the implementation base for these APIs, and rolling it out across the ecosystem Beyond Matrix 2.0, though, there’s a large pile of other areas which need attention: Improving state resolution. We’re currently investigating a set of issues where the merge resolution algorithm has misbehaved, and are planning a new room version to address it - watch this space for updates. Trust and Safety. Abuse is increasing concern, and moderation and trust & safety tooling work has ended up fragmented and balkanised. The Governing Board is putting together a cross-ecosystem working group to try to address this - again, watch this space for updates. All the business-as-usual MSCs which have stacked up - Custom Emoji, Extensible Profiles, Custom Presence, etc. It’d also be good to finally realise the full performance advantages of faster room joins… And then, in the longer term - what might Matrix 3.0 bring? Honestly, at this point, it’s an open question. Could it be landing major Trust & Safety changes in order to radically empower users to avoid and mitigate abuse? Could it be switching to MLS (or Decentralised MLS) for encryption? Could it be the glorious return of P2P Matrix (if it was funded)? Could it even be figuring out if/how to converge with (or layer on top of) MIMI or other federation protocols? Answers on a postcard to #sct-office:matrix.org please! Conclusion If ever there was a time to exhort your friends to give Matrix another go - this is it. Matrix 2.0 has been a long time coming, and we need to get the word out that the step change forwards has finally arrived, and that apps built on Matrix 2.0 can seriously outperform the mainstream alternatives. Ideally, this could be Matrix’s “Firefox moment” - when from the ashes of old open source code, a new generation appears which can punch its weight against the proprietary incumbents. Right now the only Matrix 2.0 client is Element X (and Element Web/Desktop if you enable the experimental Simplified Sliding Sync implementation in labs on develop/nightly builds) - see element.io/blog for full details - but we expect to see at least matrix-rust-sdk and matrix-js-sdk based clients using the new APIs as a matter of course in the coming months, and then hopefully everyone else too. So: if you run a Matrix server, please consider deploying MAS to enable next-gen auth (Sebastien Spaeth’s blog also has a good indie tutorial) - and add Element Call to give MatrixRTC a try. Over the coming months we should see more support in more Matrix distributions, until hopefully we will all be living in a Matrix 2.0 world! Huge thanks are due to everyone who has helped design, build, and iterate on Matrix 2.0, and to everyone who has kept the faith as we’ve put it all together. A special thanks also to BWI who helped fund much of Element’s work on this for the benefit of Matrix as a whole. Finally, if you want Matrix to prevail - please join the Foundation and support the project financially. We urgently need organisational members, especially after the costs of running The Matrix Conference, and particularly if your organisation is commercially dependent on Matrix, you simply must become a member. While it’s very flattering that Matrix gets treated as a commons these days, without financial support the underlying Matrix project will die and your project will fail. Whereas if everyone building on Matrix supported us, we would be moving way faster and with fewer constraints - so please get involved at matrix.org/support and help. Thanks for flying Matrix! Matthew The Foundation needs you The Matrix.org Foundation is a non-profit and only relies on donations to operate. Its core mission is to maintain the Matrix Specification, but it does much more than that. It maintains the matrix.org homeserver and hosts several bridges for free. It fights for our collective rights to digital privacy and dignity. Support us Post Contents 1. Simplified Sliding Sync 2. Next Generation Auth 3. Native Matrix Group VoIP/Video: MatrixRTC 4. Invisible Encryption What’s next? Conclusion",
    "commentLink": "https://news.ycombinator.com/item?id=42032387",
    "commentBody": "Matrix 2.0 Is Here (matrix.org)152 points by ptman 7 hours agohidepastfavorite47 comments IanCal 2 minutes agoDoes synapse support this at the moment? I can't quite tell from the writing here if/how you can run things that support this - my guess was no because it mentions improvements to the js SDK are required and there's no warning on the synapse repo. Or is that just that these features are opt-in? reply alwayslikethis 2 hours agoprevIf you want to host a homeserver but feel overwhelmed about the amount of services you'll have to host (especially if you want to have bridges to other services), check out matrix-docker-ansible-deploy [1]. It's pretty much a set and forget experience with reproducible deployment, and the documentation walks through any decision you need to make. 1. https://github.com/spantaleev/matrix-docker-ansible-deploy reply Arathorn 1 hour agoparentIronically i just spent all weekend writing a new \"quick start\" guide for Matrix 2.0 deployments using docker-compose (so you literally just set some env variables, `docker compose up` and that's it; no ansible involved). I just shifted to the debugging phase, but once it's ready, it might be even easier than matrix-docker-ansible-deploy as a super-fast way to get started :) reply alwayslikethis 1 hour agorootparentI'd say the primary benefit of ansible is it makes you document everything. It's easy to simply set and forget with tools like docker compose, but then when you need to change something again, you have to recall what you did originally and fix that. reply Arathorn 1 hour agorootparentyup, although i'm hoping that a self-contained docker-compose repo that you don't need to edit will help with that (rather than it being a homespun docker-compose setup, which i agree can rapidly become unmanageable). ansible just feels a bit slow & cumbersome for simpler setups, so i'm using envsubst for basic templating to see how it feels. It's perhaps telling that something like Rocket.Chat just has `docker-compose up` too: https://docs.rocket.chat/v1/docs/deploy-with-docker-docker-c... and it's annoying that Matrix doesn't have something that simple (especially given Matrix 2.0 has more moving parts serverside: auth + voip server). reply neets 1 hour agorootparentprevPlease share the code here reply Arathorn 1 hour agorootparentwell, yes - just let me finish writing it first ;P reply urda 11 minutes agoparentprevI haven't tried them, but I've seen https://etke.cc/ suggested for dealing with a group who will \"host\" the server. reply ranger_danger 18 minutes agoparentprev> If you want to host a homeserver but feel overwhelmed I feel overwhelmed at the number of options there are in this ansible thing. Now I definitely don't want to try it either way. reply _peeley 2 hours agoprevVery exciting! I'm particularly pleased to see the invisible encryption stuff mentioned. One of the biggest pain points I had when setting up a self-hosted Matrix instance and getting all my devices signed in was the crypto stuff. At least in the client I use, Element, I was bombarded with tons of popups with vague \"Upgrade your encryption!\" prompts upon logging in the first time. The copywriting on the \"Security & Privacy\" page was less than helpful in illuminating what I was actually \"upgrading\" or setting up, since specific technical terms (e.g. recovery key/security phrase/security key) were all used more or less interchangeably. If that kind of confusion can be reduced or swept under the rug for end-users, it'd be a huge improvement on user experience. reply Arathorn 1 hour agoparentYup. One of the biggest learnings of E2EE in Matrix is that the complexity is 95% user experience. However, in Element X, we've been determined to get it right - although there is still some temporary UX in there while full-blown Invisible Crypto is still rolling out (as it requires a breaking change to stop encrypting/decrypting with unsigned devices - the equivalent to a browser refusing to talk TLS to self-signed certs). If you haven't seen MSC4161 (https://github.com/matrix-org/matrix-spec-proposals/blob/and...) i highly recommend it as evidence of how we've made a serious effort to fix the terminology and copy - not just for Element X but across all Matrix clients. reply dimal 2 hours agoprevOff topic but I love the YouTube player interface. Instead of loading it by default (and adding invasive Google tracking to the page), you get the option to opt in. Very nice. reply birb07 1 hour agoparenthosting such videos on YouTube alternatives (even federated once) would be even better. I.e Blender has a peertube instance video.blender.org reply 01HNNWZ0MV43FF 2 hours agoprevDoes this change anything for notifications? I've had to pause using Matrix (self-hosted Synapse plus Schidichat for Android after Element had the same issues) to talk to my friends because we routinely get shit like: - Message is sent to the server but nobody else's phone gets notified about it for minutes / hours - Message just can't be sent to the server even though the sending phone has Internet and a desktop web client on the same account works great reply Arathorn 2 hours agoparentIf the problems were caused by issues in the old Element Android (or Schildichat) app then yes - the Element X rewrite will likely fix it. It supports UnifiedPush, so if you're self-hosting a push gateway it should nicely integrate (as does Schildichat Next). If the problem was in your push infrastructure, then the new app won't fix anything - however, UnifiedPush should be reliable these days (especially if you host your own instance; some of the shared ones are overloaded and/or deliberately throttle Matrix push). FCM obviously should be reliable too. reply udev4096 59 minutes agoprevI wrote an easy to follow guide for hosting matrix a while back: https://hcrypt.net/synapse.html reply robcohen 4 hours agoprevCongratulations to the Matrix team. I look forward to trying out everything this release offers and seeing how I can implement in the organizations I work with. reply stackskipton 2 hours agoprevIs it possible with Matrix to run your own server but let Matrix.org handle authentication? I always thought for many that's probably best of both worlds. reply alwayslikethis 2 hours agoparentWhat benefits does this have? You'll still have to have a server running and you lose control of your account, if I'm understanding what you mean (letting matrix.org handle accounts). reply stackskipton 1 hour agorootparentUser friction. My problem is this: I'm in a bunch of Discord Server/Guilds but watching Discord Inc., it's clear that they are ZIRP powered and it might come crashing down in massive fireball. However, if users have 5 separate logins, they are not going to convert over. I realize the implications of \"What happens if you get Matrix.org account banned?\" but that's next week problem. reply bhauer 2 hours agoprevPreviously, it seemed the sliding sync required a Postgres-backed Synapse installation. Does the Matrix 2.0 version of Synapse provide a seamless upgrade path for those using the default Sqlite installation? reply _flux 1 hour agoparentBy the way, Sqlite should only to be used when testing, not when actually deploying a system that interacts with other systems. https://element-hq.github.io/synapse/latest/setup/installati... says as much. reply Arathorn 2 hours agoparentprevYes. the sliding sync proxy shim is gone; Synapse now uses its native database for sliding sync, same as the old sync API - so it works with both postgres & sqlite. reply gnabgib 7 hours agoprevPreviously (42 points) https://news.ycombinator.com/item?id=41987113 reply _aavaa_ 45 minutes agoprevTheir main, and default client, still doesn't support multiple accounts. It's been years reply lousken 3 hours agoprevi hope after the transition is done, there will be a focus on multiplatform e2e search, attachment tab improvements (it's fine forCould it be the glorious return of P2P Matrix (if it was funded)? I forgot how it works right now, but I certainly would hope so in case of private messages (incl. P2P encrypted audio and video calls). reply Arathorn 1 hour agoparentP2P Matrix is a dialect of Matrix where the server runs inside the client itself - see arewep2pyet.com for details. In other words, there are no servers (other than optional store-and-forward relays). It's insanely cool - you can see a demo at https://www.youtube.com/watch?v=eUPJ9zFV5IE&t=2192s. It's also unfunded and on hiatus until someone provides some $ so we can work on it again. Right now, normal Matrix is a client-server model: you can't send messages from a client without it talking to a server (and then to another server, and then to another client). MatrixRTC VoIP and Video calls in Matrix 2.0 also go via server (for now), in order to support multiple participants and firewall traversal. Obviously, both messages and calls are end-to-end-encrypted (other than in public chatrooms), which makes it less important that they go via a server today. reply johnisgood 1 hour agorootparentThank you for your answer. > It's also unfunded and on hiatus until someone provides some $ so we can work on it again What are the available ways for someone to provide funding? reply Arathorn 1 hour agorootparenthttps://matrix.org/membership/ is how best to support - or buy stuff from organisations who support the Foundation. P2P Matrix requires a 3-4 person team to progress constructively (rather than limping along on a shoestring), which means $$$K/y over and above the current funding targets: https://matrix.org/blog/2024/01/2024-roadmap-and-fundraiser/ reply sroerick 2 hours agoprevDoes Matrix still sync all metadata with all connected federated instances? reply panick21_ 2 hours agoparentIts room based. Each room is synced between all servers used by participating members. But if you have a room with only people from 1 server, that metadata stays on that server. reply johnisgood 2 hours agorootparentIs it possible to configure this so that the metadata remains on the server by restricting access to only those users who are not members of any other servers? reply Ugzuzg 2 hours agorootparentWhile creating a room, you can choose to \"Block anyone not part of your server from ever joining this room\". reply alwayslikethis 2 hours agorootparentprevYou can disallow users from joining based on their homeserver or only allow local (on your homeserver) users, so the answer is yes. reply Arcuru 2 hours agorootparentprevI don't think there's a setting for \"only allow people from X server\", but you can make the room private and only invite people from that particular server. reply superkuh 1 hour agoprev>Native Matrix Encrypted Multiparty VoIP/Video (aka MatrixRTC, MSC4143) Excellent. This is basically my only use for Matrix because encrypted video chats, 1-to-1, and now many-to-many (YES!) are the only thing Matrix does better than IRC. I started using Matrix video chat to talk to family during the early pandemic and sort of got used to it. reply bhaney 42 minutes agoprev> now focusing on making Matrix fast, usable and mainstream-ready with Matrix 2.0 Feels like all of those things should have been the focus of 1.0, no? reply tucnak 2 hours agoprevI'm a big fan of Matrix, but I'm embarrassed to recommend it to my friends until Element X supports audio calls... It's been months since the leading-edge release, and I don't know what to think: there's Element Call, however it's neither supported by Element X nor real alternative to legacy peer-to-peer audio calls. reply Arcuru 2 hours agoparentElement X doesn't even support Threads or Spaces still, the two things that are absolutely required to organize discussions. reply Arathorn 1 hour agoparentprevElement X natively integrates Element Call for voip/video calls - this is one of the core things of this week's release. If you hit the video call button, it'll start off with video muted, and it should behave like a voice call (although there are a few bugs in the integration still, hence it being marked beta - especially on iOS, where CallKit + WebRTC stopped working in iOS 18. We're trying to work with Apple on it.) reply aniviacat 1 hour agoparentprevWhat do you mean by audio calls? I haven't used Element X yet, but judging from the store screenshots you can turn off the camera in video calls. reply evbogue 2 hours agoprevRelated: Matrix 2.0: The Future of Matrix --> https://news.ycombinator.com/item?id=37599510 reply MarvinYork 3 hours agoprevnext [4 more] [flagged] yjftsjthsd-h 3 hours agoparentOne is a JavaScript thing, one is a chat protocol. I don't know how you would compare them? reply keb_ 2 hours agorootparentOK fine, how does it compare to RetroArch? reply freedomben 39 minutes agorootparentVery favorably. It's like a hybrid between Postgres and Fedora reply fluorinerocket 46 minutes agoprev [–] Disappointed this ain't about linear algebra reply Consider applying for YC's W25 batch! Applications are open till Nov 12. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Matrix 2.0 has been launched to position Matrix as a mainstream-ready, open, decentralized, and secure communication protocol.- Key features include Simplified Sliding Sync for instant login, Next Generation Auth with OpenID Connect, and MatrixRTC for encrypted multiparty VoIP/Video.- The update focuses on improving encryption reliability and requires community financial support for ongoing development."
    ],
    "commentSummary": [
      "Matrix 2.0 has been released, featuring improvements to the chat protocol, including invisible encryption and native Matrix Encrypted Multiparty VoIP/Video.",
      "A new \"quick start\" guide using docker-compose is being developed to simplify the setup process, with matrix-docker-ansible-deploy recommended for ease of hosting.",
      "The release aims to enhance speed and user-friendliness, though some users have concerns about specific features like audio calls in Element X, while others are optimistic about the potential for peer-to-peer (P2P) Matrix."
    ],
    "points": 152,
    "commentCount": 48,
    "retryCount": 0,
    "time": 1730632148
  },
  {
    "id": 42030625,
    "title": "Ractor – a Rust Actor Framework",
    "originLink": "https://slawlor.github.io/ractor/quickstart/",
    "originBody": "Quickstart On this page Some notations to keep in mind Messaging actors Installation Your first actor All together now Adding State Running a stateful sample Some notations to keep in mindPermalink While working through this quickstart, a few notations we want to clarify for readers. Messaging actorsPermalink Since we’re trying to model as best we can around Erlang’s practices, message sends in Ractor can occur in 2 ways, first-and-forget and waiting on a reply. Their notations however follow the Erlang naming schemes of “cast” and “call” respectively. InstallationPermalink Install ractor by adding the following to your Cargo.toml dependencies [dependencies] ractor = \"0.9\" Your first actorPermalink We have to, of course, start with the iconic “Hello world” sample. We want to build an actor that’s going to print “Hello world” for every message sent to it. Let’s begin by defining our actor and filling in the necessary bits. We’ll start with out message definition pub enum MyFirstActorMessage { /// Print's hello world PrintHelloWorld, } Then we follow up with the most basic required actor definition use ractor::{Actor, ActorRef, ActorProcessingErr}; pub struct MyFirstActor; #[async_trait::async_trait] impl Actor for MyFirstActor { type State = (); type Msg = MyFirstActorMessage; type Arguments = (); async fn pre_start(&self, _myself: ActorRef, _arguments: Self::Arguments) -> Result { Ok(()) } } Let’s break down what we’re doing here, firstly we need our actor’s struct-type which we’re calling MyFirstActor. We are then defining our Actor behavior, which minimally needs to define three types State - The “state” of the actor, for stateless actors this can be simply () denoting that the actor has no mutable state Msg - The actor’s message type. Arguments - Startup arguments which are consumed by pre_start in order to construct initial state. This is helpful for say a TCP actor which is spawned from a TCP listener actor. The listener needs to pass the owned stream to the new actor, and Arguments is there to facilitate that so the other actor can properly build it’s state without clone()ing structs with potential side effects. Lastly we are defining the actor’s startup routine in pre_start which emits the initial state of the actor upon success. Once this is run, your actor is alive and healthy just waiting for messages to be received! Well that’s all fine and dandy, but how is this going to print hello world?! Well we haven’t defined that bit yet, we need to wire up a message handler. Let’s do that! #[async_trait::async_trait] impl Actor for MyFirstActor { type State = (); type Msg = MyFirstActorMessage; type Arguments = (); async fn pre_start(&self, _myself: ActorRef, _arguments: Self::Arguments) -> Result { Ok(()) } async fn handle(&self, _myself: ActorRef, message: Self::Msg, _state: &mut Self::State) -> Result { match message { MyFirstActorMessage::PrintHelloWorld => { println!(\"Hello world!\"); } } Ok(()) } } Ok now that looks better! Here we’ve added the message handler handle() method which will be executed for every message received in the queue. All together nowPermalink Let’s wire it all up into a proper program now. #[tokio::main] async fn main() { // Build an ActorRef along with a JoinHandle which lives for the life of the // actor. Most of the time we drop this handle, but it's handy in the // main function to wait for clean actor shut-downs (all stop handlers will // have completed) let (actor, actor_handle) = Actor::spawn(None, MyFirstActor, ()).await.expect(\"Actor failed to start\"); for _i in 0..10 { // Sends a message, with no reply actor.cast(MyFirstActorMessage::PrintHelloWorld).expect(\"Failed to send message to actor\"); } // give a little time to print out all the messages tokio::time::sleep(tokio::time::Duration::from_millis(100)).await; // Cleanup actor.stop(None); actor_handle.await.unwrap(); } Adding StatePermalink Now what if we wanted to ask the actor for some information? Like the number of hello-worlds that it has printed thus far in its lifecycle, let’s see what that might look like. use ractor::{Actor, ActorRef, ActorProcessingErr, RpcReplyPort}; pub enum MyFirstActorMessage { /// Print's hello world PrintHelloWorld, /// Replies with how many hello worlds have occurred HowManyHelloWorlds(RpcReplyPort), } pub struct MyFirstActor; #[async_trait::async_trait] impl Actor for MyFirstActor { type State = u16; type Msg = MyFirstActorMessage; type Arguments = (); async fn pre_start(&self, _myself: ActorRef, _arguments: Self::Arguments) -> Result { Ok(0) } async fn handle(&self, _myself: ActorRef, message: Self::Msg, state: &mut Self::State) -> Result { match message { MyFirstActorMessage::PrintHelloWorld => { println!(\"Hello world!\"); *state += 1; } MyFirstActorMessage::HowManyHelloWorlds(reply) => { if reply.send(*state).is_err() { println!(\"Listener dropped their port before we could reply\"); } } } Ok(()) } } There’s a bit to unpack here, so let’s start with the basics. We changed the type of the Actor::State to be a u16 so that the actor could maintain some internal state which is the count of the number of times it’s printed “Hello world” We changed the hello-world message handling to increment the state every time it prints We added a new message type MyFirstActorMessage::HowManyHelloWorlds which has an argument of type RpcReplyPort. This is one of the primary ways actors can inter-communicate, via remote procedure calls. This call is a message which provides the response channel (the “port”) as an argument, so the receiver doesn’t need to know who asked. We’ll look at how we construct this in a bit We added a hander match arm for this message type, which sends the reply back when requested. Running a stateful samplePermalink Very similar to the non-stateful example, we’ll wire it up as such! #[tokio::main] async fn main() { // Build an ActorRef along with a JoinHandle which lives for the life of the // actor. Most of the time we drop this handle, but it's handy in the // main function to wait for clean actor shut-downs (all stop handlers will // have completed) let (actor, actor_handle) = Actor::spawn(None, MyFirstActor, ()) .await .expect(\"Actor failed to start\"); for _i in 0..10 { // Sends a message, with no reply actor.cast(MyFirstActorMessage::PrintHelloWorld) .expect(\"Failed to send message to actor\"); } let hello_world_count = ractor::call_t!(actor, MyFirstActorMessage::HowManyHelloWorlds, 100) .expect(\"RPC failed\"); println!(\"Actor replied with {} hello worlds!\", hello_world_count); // Cleanup actor.stop(None); actor_handle.await.unwrap(); } WHOA what is call_t!?! That’s a handy macro which constructs our RPC call for us! There’s are three macro variants to ease development use for actor messaging cast! - alias of actor.cast(MESG), simply send a message to the actor non-blocking call! - alias of actor.call(|reply| MESG(reply)) which builds our message for us without having to provide a lambda function to take the reply port as an argument to construct the message type. We don’t need to actually build & wait on the port, the RPC functionality will do that for us. call_t! - Same as call! but with a timeout argument Checkout docs.rs on RPCs for more detailed information on these macros. In this brief example, we’re having our actor send our 10 messages, and then sending a final query message to read the current count and print it. We’re additionally giving it 100ms to execute (hence the use of call_t!) or return a timeout result.",
    "commentLink": "https://news.ycombinator.com/item?id=42030625",
    "commentBody": "Ractor – a Rust Actor Framework (slawlor.github.io)128 points by todsacerdoti 17 hours agohidepastfavorite60 comments tel 3 hours agoI'm always most curious with these frameworks how they're considering supervision. That's the real superpower of OTP, especially over Rust. To me, Rust has adequate concurrency tooling to make ad hoc actor designs roughly on par with more developed ones for many tasks, but supervision is both highly valuable and non-trivial. Briefly, I'd say Rust is top-notch for lower-level concurrency primitives but lacks architectural guidance. Supervisor trees are a great choice here for many applications. I've tried implementing supervision a few times and the design is both subtle and easy to get wrong. Even emulating OTP, if you go that route, requires exploring lots of quiet corner cases that they handle. Reifying this all into a typed language is an additional challenge. I've found myself tending toward one_for_all strategies. A reusable Fn that takes some kind of supervisor context and builds a set of children, potentially recursively building the supervision tree beneath, tends to be the best design for typed channels. It forces one_for_all however as it's a monolithic restart function. You can achieve limited (batch-y) rest_for_one by having the last thing you boot in your one_for_all be another one_for_all supervisor, but this feels a little hacky and painful and pushes back against more granular rest_for_one designs. You then probably want a specialized supervisor for one_for_one, similar to Elixir's DynamicSupervisor. reply 0x457 8 minutes agoparentHere is how it explained in the docs: https://docs.rs/ractor/latest/ractor/actor/index.html#actor-... reply pton_xd 1 hour agoparentprev> That's the real superpower of OTP, especially over Rust. That, and preemptive scheduling. And being able to inspect / debug / modify a live system. Man, these actor frameworks just make me appreciate how cool Erlang is. reply ninkendo 3 hours agoprevAt $dayjob I've taken to just implementing my own actor-like model when I need interior mutability across tasks. Something like: struct State { counter: usize, control: mpsc::Receiver } struct StateActor { addr: mpsc::Sender } enum Msg { Increment { reply: oneshot::Sender } } impl StateActor { pub async fn increment(&self) { let (tx, rx) = oneshot::channel(); let msg = Msg::Increment { reply: tx }; self.addr.send(msg).await.unwrap(); rx.await.unwrap(); } } impl State { fn start(self) { tokio::spawn(async move { /* ... tokio::select from self.control in a loop, handle messages, self is mutable */ /* e.g. self.counter +1 1; msg.reply.send(()) */ }) } } // in main some_state_actor.increment().await // doesn't return until the message is processed A StateActor can be cheaply cloned and used in multiple threads at once, and methods on it are sent as messages to the actual State object which loops waiting for messages. Shutdown can be sent as an in-band message or via a separate channel, etc. To me it's simpler than bringing in an entire actor framework, and it's especially useful if you already have control loops in your program (say, for periodic work), and want an easy system for sending messages to/from them. That is to say, if I used an existing actor framework, it solves the message sending/isolation part, but if I want to do my own explicit work inside the tokio::select loop that's not strictly actor message processing, I already have a natural place to do it. reply slau 1 hour agoprevI’m not saying that you can’t use another project’s name, but there already exists an actor framework named Ractor, but this one written in Ruby. https://docs.ruby-lang.org/en/master/Ractor.html reply samsartor 1 hour agoprevActors in Rust still kind of suck because there are no first-class resume arguments. In order to provide a `&mut State` you have to either: - handle messages sequentially per-actor (like Ractor, and like Bastion in practice) - create your own future trait without async/await (like Actix) I keep hoping the language team will get coroutines off the ground, but it hasn't happened yet. reply qwertox 7 hours agoprevRelated from one month ago: Kameo - https://github.com/tqwewe/kameo Kameo – Fault-tolerant async actors built on Tokio - https://news.ycombinator.com/item?id=41723569 - October 2024 (58 comments) reply whytevuhuni 2 hours agoprevWhat's the benefit of having a State type, instead of using handle(&mut self), pre_start() -> Self, and putting the state inside the type that implements the actor? reply derefr 1 hour agoparentFrom the README (https://github.com/slawlor/ractor?tab=readme-ov-file#differe...): > When designing ractor, we made the explicit decision to make a separate state type for an actor, rather than passing around a mutable self reference. The reason for this is that if we were to use a &mut self reference, creation + instantiation of the Self struct would be outside of the actor's specification (i.e. not in pre_start) and the safety it gives would be potentially lost, causing potential crashes in the caller when it maybe shouldn't. > Lastly is that we would need to change some of the ownership properties that ractor is currently based on to pass an owned self in each call, returning a Self reference which seems clunky in this context. reply Sytten 5 hours agoprevI personnaly don't like the single enum model for messages. I prefer the generic Handler trait model. Also another square that I have not circled with async actor other than actix is that all of them use mutable self and they dont have a great way to have long running tasks. Sometimes you would want to call a remote server without blocking the actor, in actix this is easy to do and you unamed child actors to do that. All those newer frameworks don't have the primitives. reply derefr 1 hour agoparent> all of them use mutable self As demonstrated in the linked tutorial, Ractor passes its handlers `&self` with `&mut State`. reply echelon 4 hours agoparentprevIt looks like the message type can be anything: type Msg = MyFirstActorMessage; reply jatins 13 hours agoprevThat looks interesting! What's the distributed story of Ractor? Would you need a central store like Redis to serve as Actor registry? One of the promises of Elixir/Erlang is that you can call a process/Actor on different machine just like you can one on same once you put together a bunch of machines in a cluster reply stefanka 9 hours agoparentSeems to be supported > Additionally ractor has a companion library, ractor_cluster which is needed for ractor to be deployed in a distributed (cluster-like) scenario. ractor_cluster shouldn’t be considered production ready, but it is relatively stable and we’d love your feedback! reply kemiller 2 hours agoparentprevElixir/erlang/gleam also have the advantage that many libraries are written as servers and so automatically gain the benefits and resilience. Something all these actor frameworks can’t give you. reply simonask 11 hours agoprevRactor is cool, but I've been wondering why it uses the async `async_trait` rather than native async traits. Is it just because it came out before async traits were stabilized, and now code relies on it and it would be a breaking change to migrate? For context, the `async_trait` crate makes futures from trait methods that are wrapped in `Pin>>`, which means that every async call to a trait method must make a heap allocation. This is currently a necessary thing to do if async trait methods were invoked with dynamic dispatch (through `dyn Actor`), but the `Actor` trait has associated methods, so that is already not generally possible. I realize that methods on the `Actor` trait return futures that are `Send`, but specifically for an actor framework that feels like a very specific design choice that isn't universally good or necessary. Another design would give let the spawned task that executes the actor's messages exclusive access to the actor (so `handle()` could take `&mut self`). I've ended up implementing a simple alternative design in my own project (it's not fundamentally very hard, but doesn't have all the features, like supervision) because the per-message heap allocations and internal locking became wasteful for my use case. reply SkiFire13 9 hours agoparent> I've been wondering why it uses the async `async_trait` rather than native async traits From their crate documentation[0]: > The minimum supported Rust version (MSRV) is 1.64. However if you disable the `async-trait` feature, then you need Rust >= 1.75 due to the native use of `async fn` in traits. [0]: https://docs.rs/ractor/0.13.0/ractor/index.html reply victorbjorklund 12 hours agoprevThis is nice. Any way to also make it an Erlang complient node so it can be called from a erlang cluster? reply jerf 3 hours agoparentMaking something Erlang-compliant is actually fairly heavy duty. It's more than just \"you've got actors, I've got actors, let's play together!\"... you have to support the Erlang term format specifically, so on the Rust side you would need to be able to convert to and from all the state you actually want into Erlang terms, including PID references, you need to support Erlang's linking capability, you need to support Erlang's name server lookups, you need to support Erlang's specific messaging semantics for out-of-order message handling and mailboxes, binary sharing, probably some other things I'm not remembering. It's not an impossible amount of work but it's not in the \"couple of weeks of spare time for a motivated developer\" range either. Most people most of the time are just better hooking up one of the many, many other event busses that both Rust and Erlang can speak to and working across that. reply sph 2 hours agorootparentNot only that, but you need a full BEAM-compatible VM, as you can send entire processes across nodes. reply derefr 54 minutes agorootparentNo you can't. The closest you could get to this would be: 1. assuming a running process pA on nodeA, registered in a cluster-level process registry by name N; 2. send the process pA's state to a function on node B, which will use it to start a process pB on node B that is a clone of pA; 3. update the cluster-level process registry to point N at pB. 4. if you want to be really clever — tail-call (or code-upgrade) pA out of its existing loop, into code that makes pA act as a proxy for pB, so that anything sending a message to pA ends up talking to pB through pA. But this isn't \"sending a process\"; pA itself remains \"stuck\" on nodeA (node IDs are actually embedded in PIDs!), and there's especially no native mechanism that would do things like rewrite the links/monitors on other processes that currently point to pA, to instead point to pB (so if pA was a supervisor, you couldn't \"re-parent\" the children of pA over to pB.) reply throwawaymaths 1 hour agorootparentprevNo, there are cnodes. reply hit8run 5 hours agoprevWell someone got inspired by Rubys Ractors hence the naming? https://docs.ruby-lang.org/en/master/ractor_md.html reply mtndew4brkfst 4 hours agoparentUnlikely, the contributors don't seem to have meaningful Ruby backgrounds. Isn't it a simpler supposition that the naming intent was merely (R)ust Actors? reply rubyfan 3 hours agoparentprevYeah bad choice of naming here. Oddly this project shows up first on DDG for “ractor” despite prevalent Ruby ractor documentation. reply k3vinw 9 hours agoprevThere’s also actix: https://github.com/actix/actix reply mtndew4brkfst 7 hours agoparentIt's been informally discouraged for use in net-new projects via a chat message from one of the maintainers during this year's RustConf. https://discord.com/channels/734893811884621927/127043967262... Actix-web remains healthy in this regard, it was specifically just about the actix crate. reply Capricorn2481 3 hours agorootparentThis link just goes to a blank discord page for me as if something is trying to load. I don't think these are shareable if you aren't in the server. reply lionkor 8 hours agoparentprevI use actix_web for all my web server stuff, its wonderful, super fast, really easy to use and not that easy to fuck up. reply hobofan 41 minutes agorootparentI know it seems unintuitive but actix-web is largely unrelated to actix nowadays: https://actix.rs/docs/whatis/ reply rapsey 11 hours agoprevSorry for being so negative but why do people keep building these Actor frameworks for Rust? None of them get any usage. reply riquito 1 hour agoparent> Sorry for being so negative but why do people keep building these Actor frameworks for Rust? None of them get any usage. There are good answers already on the actual \"why\", but I'd like to point out that \"none of them get any usage\" is a much better reason to build a new one than \"there's already a library dominating the ecosystem\". Clearly none of the existing libraries, for one reason or another, have been deemed interesting enough, let's see someone else try reply babo 10 hours agoparentprevThis seems to be used at Meta: \"Ractor had a session at RustConf’24 about how it’s used for distributed overload protection in Rust Thrift servers at Meta.\" reply JW_00000 9 hours agorootparentTheir presentation [1] contains a few slides with the motivation for this framework (they have existing Thrift services in C++ and Python, and want to start using Rust too) and why actors (answer: they scale better than a naive solution?). Also a slide \"why a new framework\" when so many already exist for Rust. (Answer: many are dead/unsupported, are too far from Erlang principles, not flexible enough, or use custom runtimes while ractor builds on Tokio.) [1] https://slawlor.github.io/ractor/assets/rustconf2024_present... reply mtndew4brkfst 7 hours agorootparentTiny bit of context I'd also call out is that one of the primary authors comes from WhatsApp IIRC, with an extensive Erlang background. reply seanhunter 11 hours agoparentprevFor reasonable reasons in my view. The actor model is imo a great way of doing concurrency in the absence of the data race guarantee that safe rust provides at compile time. If you know you have no data races, I don't think actors give you that much. That said, some people just really like actors as a mental model and/or they want to interoperate with actor-based systems written in other languages or provide an actor substrate written in rust that will be embedded into another language perhaps? It's definitely a niche usage. reply t-writescode 9 hours agorootparent> That said, some people just really like actors as a mental model This is me, for sure. \"little box does one thing, processes one thing at a time, maintains the state of one thing, talks to different pieces of the system through this predefined method\", etc, that actors give me is very easy for me to reason about and work with, so I use it and love it. reply dietr1ch 9 hours agoparentprevI guess it's a bit hard to change the way of thinking and structuring programs/applications. We mostly do either blocking procedural things, or web servers. Maybe actors can help remove the network from web servers where they add unnecessary complexity/overhead. At the same time Web servers seem a bit better as I don't really want to be figuring out how many actors of each \"class of actor\" to spawn and to maybe babysit each of them, and for debugging the web stack has more tooling and is better understood than any particular actor system. The advantages seem mostly out of experience and a lot of time. Maybe some iteration of an Actor framework will render Web servers too quirky/unsafe/slow/complex. reply the8472 6 hours agoparentprevI've used something in the general shape of actors (though mostly handrolled) when implementing websocket APIs. If I had to wire up lots of or several layers of those then using a library for that would start to make sense. reply eviks 5 hours agoparentprevBecause usage isn't the only motivator? Because they hope they'll get usage (can yuo see the future that non will get any usage?)? reply stefanka 9 hours agoparentprevCan it be used for distributed computation for ML or data science tasks (eg., like what dask does for Python)? reply jerf 4 hours agorootparentActors are better suited for highly heterogeneous task sets, where am actor or small set of them correspond to some task and you may have thousands or more. Homogenous tasks should use an approach that is aware of and takes advantage of the homogeneity, e.g., the sort of specific optimizations a framework might make to orchestrate data flows to keep everything busy with parallel tasks. You can use actors for orchestration, but you're really just using them because they're there, not because they bring any special advantages to the task. Any other solution that works is fine and there would never be a particular reason to switch to actors if you already had a working alternative. reply stefanka 9 minutes agorootparentAre you aware of any framework or could use for such tasks (ML and alike)? reply biorach 10 hours agoparentprev> None of them get any usage. Github stats show some uptake so clearly someone finds them useful reply dietr1ch 9 hours agorootparentI don't think it's a good metric, does it get stars because it's cool, or because people are building things with it while taking advantage of the Actor model? reply biorach 8 hours agorootparentwell it's more of a metric than you've provided reply dietr1ch 2 hours agorootparentWhy do you get so defensive? Also, didn't what I said about usage better map to number of dependent crates or downloads by cargo? Both are listed on https://crates.io/crates/ractor reply bowsamic 11 hours agoparentprevIt’s kind of like Java. It’s a way to feel like you’re doing work without actually doing anything reply pjmlp 9 hours agorootparentWell my phone, TV and blue ray player do lots of useful stuff. reply ahoka 7 hours agorootparentDisplaying ads? reply pjmlp 4 hours agorootparentPowered by Java, in an a market share Desktop Linux will never achieve. reply talldayo 1 hour agorootparentLinux has a license that protects it from the class of no-holds-barred codebase abuse Java suffers from, for one. reply pjmlp 57 minutes agorootparentApparently OEMs aren't aware of such marvel. reply rochak 10 hours agorootparentprevCouldn’t have said it any better myself reply vdvsvwvwvwvwv 8 hours agorootparentExcept it makes no sense. Java is a language that, while idiomatically a bit boilerplatey, is generally a workhorse for getting shit done. reply bowsamic 6 hours agorootparentNo I mean in Java it’s common to write extensive boilerplate and abstractions just bc it feels constructive. Java is extremely useful, but business use of it is not so nice reply ahoka 7 hours agorootparentprevIt’s everything but that. It’s the instrument of over engineering. Maybe not the language itself (it was the Rust if its time, everyone wanted to use it for everything, even if it made no sense), but the whole ecosystem just prefers complexity and breaking changes. Maybe it’s a coping mechanism for devs stuck with it, or I don’t know. reply Ygg2 5 hours agorootparent> it's the instrument of over engineering. True, but find me one language that someone, somewhere didn't over- something. Either over-engineer or over-simplify, or over-use. People get excited about technology and try to push the envelope on its usage. That's true of any widely used tech. Hitting the golden middle is notoriously hard, especially when it is not the same middle for everyone. > the whole ecosystem What precisely do you mean? Sure Spring is notorious for this, but not the wider ecosystem. Sure some libraries might not be using SemVer, but Maven itself predates it as well. reply helge9210 2 hours agoprevSeeing \"channel\" and \"actors can inter-communicate, via remote procedure calls\" do I understand this correctly this is not an actor model implementation? reply a-dub 12 hours agoprev [–] how does it compare with bastion? reply Consider applying for YC's W25 batch! Applications are open till Nov 12. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The guide introduces Ractor, a Rust library for actor-based programming, covering key concepts like messaging, installation, and creating actors.",
      "It explains the \"cast\" (fire-and-forget) and \"call\" (wait for reply) messaging schemes, similar to Erlang, and provides code examples for creating and running actors.",
      "The guide also details how to add state to actors and use `RpcReplyPort` for inter-actor communication, with examples of stateful actor implementation."
    ],
    "commentSummary": [
      "Ractor is a Rust actor framework that emphasizes supervision, a feature inspired by Erlang's OTP, to manage actor systems effectively.- It integrates with Tokio and has a companion library, ractor_cluster, for distributed scenarios, and is notably used at Meta for distributed overload protection in Rust Thrift servers.- The framework's design, including the use of async_trait, is shaped by Rust's evolving features, but integration with Erlang systems remains complex due to differing messaging semantics and VM requirements."
    ],
    "points": 128,
    "commentCount": 60,
    "retryCount": 0,
    "time": 1730598443
  }
]
