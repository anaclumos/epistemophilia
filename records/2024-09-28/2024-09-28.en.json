[
  {
    "id": 41678412,
    "title": "FFT-based ocean-wave rendering, implemented in Godot",
    "originLink": "https://github.com/2Retr0/GodotOceanWaves",
    "originBody": "GodotOceanWaves An open ocean rendering experiment in the Godot Engine utilizing the inverse Fourier transform of directional ocean-wave spectra for wave generation. A concise set of parameters is exposed, allowing for scriptable, real-time modification of wave properties to emulate a wide-variety of ocean-wave environments. ocean_demo.mp4 Introduction Why Fourier Transforms? A common approach for animating water in video games is by displacing vertices using Gerstner waves. While Gerstner waves work well for modeling the lower-frequency details in calmer waters, they fall short in accurately representing the choppy surfaces in an open ocean. To simulate the latter, a more complex approach simulates waves using the inverse Fourier transform of ocean-wave spectra modeled from empirical data gathered by oceanographers. A benefit of working in frequency space using ocean-wave spectra is the ease of modifying ocean properties (e.g., surface choppiness). When using Gerstner waves, it is unclear how waves (and their parameters) need to be changed to emulate a certain ocean state. In contrast, ocean-wave spectra expose parameters that change waves' properties directly. To compute the Fourier transform, a fast Fourier transform algorithm (FFT) is used specifically. On top of having a lower computational complexity than the classical discrete Fourier transform algorithm ($O(N \\log N)$ versus $O(N^2)$), the FFT is scalable as a parallel system. This means that it is perfect for running on the GPU. Using Gerstner waves requires each thread to perform $N$ computations, one for each wave. In contrast, FFT-based waves only require each thread to perform $\\log(N)$ equivalent computations. At scale, more waves can be added to the system (at the same performance cost), permitting more accurate surface simulation. Results Wave Shading Lighting Model The ocean lighting model largely follows the BSDF described in the 'Atlas' GDC talk. One deviation, however, is the use of the GGX distribution (rather than Beckmann distribution) for the microfacet distribution. This was due to the GGX distribution's 'flatter' and softer highlights providing a more uniform appearance in many of the ocean-wave environments tested. The normal/foam map is sampled with a mix between bicubic and bilinear filtering depending on the world-space pixel density (a value dependent on the normal map texture resolution and texture UV tiling size). This effectively reduces texture aliasing artifacts at lower surface resolutions while maintaining the detail at higher surface resolutions. Sea Foam Tessendorf notes a method for determining when to generate sea foam by checking where the waves' peaks curl into themselves (i.e., when the Jacobian of the displacement is negative). Foam accumulates linearly and dissipates exponentially on a texture over multiple wave updates, and are controlled by \"foam grow rate\" and \"foam decay rate\" parameters respectively. Sea Spray Sea spray is modeled using particles via Godot's GPUParticles3D node and makes heavy use of a custom particle shader. Particles are distributed evenly across the plane within the GPUParticles3D node's bounding box. Then, they are culled based on the foam amount present at their position. Un-culled particles begin their lifecycle at a random offset. Each sea spray particle uses a billboarded sprite with a single static texture. Over the course of their lifecycle, particles' scales and displacements are modified to emulate a splash's appearance. A dissolve effect in particles' mesh shader fades the sprite in a way that simulates how sea spray atomizes once in the air. One major drawback of this method is that a large increase in particle amount only results in a small increase in sea spray density. This is due to the equal distribution of particles along the bounding box, which results in a majority of the added particles being culled. Wave Simulation The method for generating surface waves closely follows Tessendorf. A directional ocean-wave spectrum function is multiplied with Gaussian-distributed random numbers to generate an initial spectral sea state. The initial state is then propagated in time through a \"dispersion relation\" (relating the frequency of waves and their propagation speed). An inverse Fourier transform can then be applied to the propagated state to generate displacement and normal maps. The methodology Tessendorf describes was implemented through a compute shader pipeline using Godot's RenderingDevice abstraction. The following sections detail more on major aspects of the wave generation system. Ocean-Wave Spectra The directional ocean-wave spectrum function, $S(\\omega, \\theta)$, returns the energy of a wave given its frequency ($\\omega$) and direction ($\\theta$). It is comprised of a non-directional spectrum function, $S(\\omega)$, and a directional spread function, $D(\\omega, \\theta)$; the choice of either is entirely independent. For the non-directional spectrum function, the Texel-Marsen-Arsloe (TMA) spectrum described in Horvath was chosen. Given the wind speed ($U$), depth ($D$), and fetch length (i.e., distance from shoreline) ($F$), the TMA spectrum combines its preceding JONSWAP spectrum with a depth attenuation function and is defined as $S_{\\text{TMA}}(\\omega) = S_{\\text{JONSWAP}}(\\omega)\\Phi(\\omega)$ where: $$\\begin{align*} S_{\\text{JONSWAP}}(\\omega) &= \\Big[0.076\\Big(\\tfrac{U^2}{F \\cdot 9.81}\\Big)^{0.22}\\Big]\\Big[\\tfrac{9.81^2}{\\omega^5}\\exp\\Big({-\\tfrac 5 4}\\big(\\tfrac{\\omega_p}{\\omega}\\big)^4\\Big)\\Big] \\Big[3.3^{\\exp\\Big(-\\tfrac{(\\omega - \\omega_p)^2}{2(0.07 + 0.02\\cdot\\mathrm{step}(\\omega - \\omega_p))^2\\omega_p^2}\\Big)}\\Big]\\\\\\ \\Phi(\\omega) &\\approx \\tfrac 1 2 \\omega_h^2 + ({-\\omega}_h^2+2\\omega_h-1)\\cdot\\mathrm{step}(\\omega_h - 1)\\\\\\ \\omega_p &= 22\\Big(\\tfrac{9.81^2}{U F}\\Big)\\\\\\ \\omega_h &= \\omega \\sqrt{\\tfrac D {9.81}} \\end{align*}$$ For the directional spread function, a combination of the flat and Hasselmann directional spreadings described in Horvath—mixed by a 'spread' parameter ($\\mu$)—was chosen. Horvath also proposes the addition of a 'swell' parameter ($\\xi$) to model ocean-wave elongation—this was also incorporated into the spread model. The mixed spread function is defined as ${D_{\\text{mixed}}(\\omega, \\theta) = \\mathrm{lerp}((2\\pi){^{-1}},\\ Q(s+s_\\xi)\\text{|}\\cos(\\theta \\text{/}2)\\text{|}^{2(s+s_\\xi)},\\ \\mu)}$ where: $$\\begin{align*}Q(\\sigma) &\\approx \\begin{cases} 0.09\\sigma^3 + \\big(\\tfrac{\\ln^2 2}{\\pi} - \\tfrac{\\pi}{12}\\big)\\sigma^2+\\big(\\tfrac{\\ln 2}{\\pi}\\big)\\sigma+\\tfrac{1}{2\\pi} & \\text{if } \\sigma \\leq 0.4\\\\\\ \\frac{\\sqrt \\sigma}{2\\sqrt \\pi} + \\frac{1}{16\\sqrt{\\pi \\sigma}} & \\text{otherwise.} \\end{cases}\\\\\\ s &= \\begin{cases} 6.97\\big(\\tfrac \\omega {\\omega_p}\\big){^{4.06}} & \\text{if } \\omega \\leq \\omega_p\\\\\\ 9.77\\big(\\tfrac \\omega {\\omega_p}\\big){^{-2.33 -1.45(\\omega_p U\\text{/}9.81-1.17)}} & \\text{otherwise.} \\end{cases}\\\\\\ s_\\xi &= 16 \\tanh\\big(\\tfrac{\\omega_p}{\\omega}\\big)\\xi^2 \\end{align*}$$ $Q(\\sigma)$ is a normalization factor used to satisfy the condition: $\\int_{-\\pi}^\\pi D(\\omega, \\theta)d \\theta = 1$. The Hasselmann directional spread was chosen due to its approximate analytical solution for $Q(\\sigma)$ (as opposed to e.g., the Donelan-Banner directional spread also described in Horvath). Following a suggestion in Tessendorf, the resultant spectrum function was also multiplied by a small-wave suppression term, $\\exp({-k}^2(1-\\delta)^2)$ (given the magnitude of the wave vector ($k$) and a 'detail' parameter ($\\delta$)). Combining the above, our final directional ocean-wave spectrum function used can be denoted as: $$S(\\omega, \\theta) = S_{\\text{TMA}}(\\omega)D_{\\text{mixed}}(\\omega, \\theta)\\exp({-k}^2(1-\\delta)^2)$$ Fast Fourier Transform A custom FFT implementation was written for the GPU using compute shaders. The Stockham FFT algorithm was used over the Cooley-Tukey algorithm to avoid the initial bit-reversal permutation. Following Flügge, a 'butterfly' texture is computed, once per spectrum texture resolution change, encoding the dataflow of the FFT. First, the FFT kernel is applied row-wise to perform the 2D FFT on the spectrum texture. The texture is then transposed using a compute shader, allowing the same row-wise FFT kernel to then be reused for—what is effectively—a column-wise FFT. This transposition also improves memory access patterns along with enabling pipeline reuse. Wave Cascades At large-enough distances—especially with sea foam present—tiling artifacts become very apparent. The wave generation system allows multiple wave cascades to be layered simultaneously to address this. Each cascade has its own tiling size and set of parameters. Cascades can be added/removed from the generation system dynamically in real-time. However, as all cascades use the same compute pipelines, they must have the same spectra texture resolution. Alternatively, blending wave displacements/normals with noise could also reduce tiling artifacts—at a lesser performance cost. Each wave cascades’ parameters and size must be carefully chosen to avoid wave interference when layered. Similarly, the cascades' wave phases should be offset to avoid interference with other cascades. The generation system automatically attempts this by offsetting each cascades’ start times differently (honestly, not sure if it works lol). Load Balancing Due to the erratic nature of wave motion, their movement can appear perceptually smooth even without updating their displacements every frame. Thus, an \"update rate\" parameter was introduced to control how often wave cascades are updated per second. While this reduces the amount of GPU-time spent working on FFT, frames during which the wave generation pipeline runs still stutter. An experiment to asynchronously compute cascade updates using Godot's local RenderingDevices, caused significant performance overhead due to transferring textures between the CPU and GPU. Instead, the wave generation system attempts to load-balance cascades. Whenever the frame time is shorter than the update rate, only one cascade is updated per frame. This reduces stuttering while still benefiting from the lower GPU workload of frame skipping. The displacement, normal, and foam maps generated after running FFT on our directional ocean-wave spectrum function (along with its associated parameters) yield realistic surface motion across a broad range of ocean-wave environments. ocean_param_demo.mp4 References Flügge, Fynn-Jorin. Realtime GPGPU FFT Ocean Water Simulation. Hamburg University of Technology. (2017). Gunnell, Garrett. I Tried Simulating The Entire Ocean. (2023). Horvath, Christopher J. Empirical Directional Wave Spectra for Computer Graphics. DigiPro. (2015). Tessendorf, Jerry. Simulating Ocean Water. SIGGRAPH. (2004). Matusiak, Robert. Implementing Fast Fourier Transform Algorithms of Real-Valued Sequences. Texas Instruments. (2001). Mihelich, Mark. Wakes, Explosions and Lighting: Interactive Water Simulation in 'Atlas'. GDC. (2019). Pensionerov, Ivan. FFT-Ocean. GitHub. (2020). Attribution Evening Road 01 (Pure Sky) by Jarod Guest is used under the CC0 1.0 license. OTFFT DIT Stockham Algorithm by Takuya Okahisa is used and modified under the MIT license.",
    "commentLink": "https://news.ycombinator.com/item?id=41678412",
    "commentBody": "FFT-based ocean-wave rendering, implemented in Godot (github.com/2retr0)528 points by RafelMri 12 hours agohidepastfavorite75 comments kamranjon 7 hours agoThe other two Godot repos by this person are very interesting as well. I love the level of detail they add to explaining their repos. This one is particularly interesting: https://github.com/2Retr0/GodotGaussianSplatting Wonder if they are a student, they seem to cite other work frequently and have a strong grasp on recently published materials. reply codetrotter 6 hours agoparent> Wonder if they are a student Seems like they might be. One of their repos has this title and description: > ENGR96A-coursework > Relevant coursework for ENGR 96A Introduction to Engineering Design F23 And F23, judging by the dates of the commits in that repo means Fall 2023. Of course, it could be that this and other UCLA courses referenced in the repos are open for everyone. So maybe you don’t have to be enrolled as a traditional student at UCLA to take them. reply kelseyfrog 26 minutes agoprevAny other resources on empirically derived rendering/animation methods? A lot of hobbyist gamedevs can think of tutorials where we \"slap noise\" on various things. While a good temporary use, there's an pedagogical gap between beginner and advanced methods. Another that comes to mind is vegetation animation. Like ocean waves, we often see animators throw a few octaves of sin/cos on plants to simulate wind, but because it doesn't spectrally match what we see in the real world, it looks off. reply jesperwe 9 hours agoprev20 years ago I could spend months tweaking ocean surface in renders and not get even close to that. Amazing how good this is!! Although the demo clip feels a bit exaggerated (saying this having over 50k Nm open water ocean sailing in my logbook). Waves that sharp and high would need the wind blowing a lot stronger. But I am sure that is just a parameter adjustment away! Since it is in Godot I assume the rendering is real time? Does it need a monster GPU? reply aithrowawaycomm 5 hours agoparentThis is not a criticism, just an observation: it looks like what I imagine an ocean of hot corn syrup would look like (after dyeing it blue). The viscosity seems right; possibly the surface tension is not what ocean water would have (a colloid of salty H2O and biomaterial, which is common in real-world experience but quite ugly for computational fluid dynamics). Also note that the ocean spray here is a post-hoc effect, but for a real ocean the spray dulls the sharpness of the waves in a way that will be (vaguely) apparent visually. Of course there's almost no \"physics\" in this elegant, simple, and highly effective model, so I want to emphasize that suggesting directions to poke around and try things should not be construed as an armchair criticism. reply hackable_sand 41 minutes agorootparentThis is literally a criticism. reply aithrowawaycomm 23 minutes agorootparentA) It would be a criticism if I thought these effects could be plausibly rendered with a similar FFT algorithm, but that seems unlikely to me. I think these results are \"highly effective\" given the toolset, which is not attempting to emulate the actual physics. B) This project is not an all-out attempt to make lifelike water, it is described as an experiment. I am making an observation about the result of the experiment, not criticizing the project for failing to meet standards it wasn't holding itself to. reply lloeki 4 hours agoparentprev> 20 years ago I could spend months tweaking ocean surface in renders and not get even close to that. There are tons of videos now about that making the whole thing (somewhat) more approachable, but there are still a lot of pitfalls! One of my favourites on the subject: https://m.youtube.com/watch?v=yPfagLeUa7k This one is nice too: https://m.youtube.com/watch?v=kGEqaX4Y4bQ reply pjmlp 7 hours agoparentprevBack when I graduated, doing particle engines, with marching cubes and stuff like that, was a graduation thesis project. Nowadays it is a check box on a game engine, one of many. People don't imagine how good they have it with modern engines. Not to take any value out of this work, this is a great achievement and kudos to the author, only making the point how good we have nowadays. reply kevindamm 6 hours agorootparentBack when I graduated I was still holding my breath for the patent expiration on marching cubes, GPUs were still being made for PCI and even ISA slots, you could find some game engines but you would have a better time writing one specialized for the type of game or graphics you were targeting. Things really have improved a lot. reply pjmlp 5 hours agorootparentI guess we might have a similar age, first computer Timex 2068. :) reply kevindamm 5 hours agorootparentsounds like it, my first computer was an Atari 800 XL that shared the family TV as its monitor (and fortunately there was an RF switch to toggle between it and the antenna so we worked out a kind of timesharing system) reply pjmlp 4 hours agorootparentSounds familiar. :) reply sails 5 hours agoparentprevAgree, based on the clips, it looks a bit random. I think it is looks to be very good, and probably the best I’ve seen having given it a cursory search recently to see what was possible. In terms of what I’d like to see, open ocean waves generally have more rhythm, I’d be very interested to see a simulation of 15 knots of wind blowing over 1km for a few hours and see if that matches what I observe, which would be relatively organised wave trains (sets) that build and disperse. reply ryanjshaw 8 hours agoparentprevThere's 2 demo clips, the second one shows quite a number of parameters you can adjust. reply pixelatedindex 5 hours agorootparentI think the point is that these parameters you adjust are being rendered near real-time, whereas back in their days you’ll have to enter these values, and add custom tweaking because the hardware just wasn’t powerful enough to do the things we can do now at many times the speed. Not to mention the vast improvement in mileage for your time. reply dylan604 4 hours agorootparentyeah, the \"in real time\" is what kills me. the old joke of blue bar races, rendering, buffering, and any of the other things us gray beards had to put up with is just unimaginable to the whippersnappers of today. reply CyberDildonics 1 hour agoparentprev20 years ago I could spend months tweaking ocean surface in renders and not get even close to that. I'm not sure what you mean here, because this is made directly from research that was done 20 years ago and it looks the same, it's just being done in real time. reply matsemann 6 hours agoparentprev> I could spend months tweaking ocean I have a game project. But I always get nerd sniped by cool game stuff, and want to implement them myself. My progress so far could probably have been achieved in a 48h gamejam if I just used/bought existing assets. Instead I have also spent weekends playing with water shaders and getting them to look how I want. But my game is a puzzle game. I don't need water, except that I now have a cool splash screen.. reply fcatalan 7 hours agoprevThings like this brought me into computers, but along the way I fell for the easy and boring life of glueing libraries, endpoints and corporate bullshit that leads to burnout. Perhaps some day... reply lukko 3 hours agoparentI think make the move gradually - find the stuff you were excited in originally, that you would love to learn more about and eventually do. Spend maybe a few hours a week diving into it - then gradually increase and move away from your current job. Go for it - we're all rooting for you! reply kevindamm 6 hours agoparentprevSomeday could be today... reply Trasmatta 4 hours agorootparentMy problem is I'm so burnt out from the aforementioned stuff I don't have the motivation or energy for the cool stuff anymore. I feel like I need a year long sabbatical first, but reality says otherwise. reply zackmorris 1 hour agorootparentSame here, this has been the central crisis of my working adult life for 25 years. Unfortunately it never gets better. And I've taken 6 months to 1 year off for severe burnout with physical symptoms like adrenal fatigue twice now. My feeling is that this problem is intractable alone. We need groups working towards liberation, and societal change to support healthy work/life balance. What that looks like in practice is that wealthy people, especially those who won the internet lottery, should start giving something back. At the most basic level, that's paying one's taxes. Beyond that, they should start setting aside ego-based goals and start accepting requests outside of their attention so that the most pressing problems facing humanity can finally get solved. Give a billionaire $1 billion and a year later they'll turn it into $2 billion. Give one of us $1 billion and a year later a form of cancer will be cured. That's why they have the money and we don't, and why it takes so long for things to get better, if they ever do. reply otteromkram 4 hours agorootparentprevSounds like you just need more coffee :-D reply Trasmatta 2 hours agorootparentMore coffee, maybe a pizza party. That'll fix me right up. reply maaaaattttt 6 hours agorootparentprevThere are only two good moments to plant a tree: 20 years ago and today. reply Aeolun 6 hours agorootparentIsn’t it like: the best time to plant a tree is 20 years ago. The next best time is today. reply epaga 6 hours agorootparentThough I never understood why 19 years ago wouldn’t be the second best… reply airstrike 6 hours agorootparentBecause you didn't think of it then. You thought of it 20 years ago and didn't do it, and you're considering it again now. Instead of telling yourself it is too late to do it now, just go for it, so that in 20 years more you will be happy you planted this tree. reply lukan 3 hours agorootparentAlso some people like their children and grandchildren to enjoy some fruit trees. (Even if they are just \"family\" in the broader sense and not their own) reply plasticchris 6 hours agorootparentprevBecause the statement is meant to motivate the reader. reply rbetts 6 hours agorootparentprevThat would make today the worst day to plant a tree? reply phn 6 hours agorootparentThe worst so far, and the best when compared to all of your remaining days. reply maaaaattttt 1 hour agorootparentprevYes that’s it, thank you. I’ll admit I was too lazy to look the exact one up. reply btbuildem 3 hours agoprevIt's interesting how hard this problem is. We've been trying for decades, and we're still in the uncanny valley with it. If you freeze-frame this, the peaking waves look like snow-capped mountains. It feels unrealistic because for water to have features this sharp, it would have to be quite windy -- and the wind would never be blowing straight up. Here, the sharp features would need to be directional. The simulation has the swell nature of the waves down pretty well though. There isn't as much horizontal movement, as more up-and-down, which is what you'd expect to see in open water. reply zamalek 3 hours agoparent> uncanny valley I've been playing Nightingale and, oh boy, is the ocean water something special there. To be clear, they don't tackle spray and such by keeping the waves calm, which has allowed them to focus on other things. Whatever it is they are doing to simulate light transmission is working. The light shining through the waves at sunrise or sunset looks great and really sells the effect for me. It could maybe be accused of being a bit painterly, but it doesn't look wrong. reply viraptor 9 hours agoprevDifferent approaches, but if someone's interested in waves/ocean simulation, Acerola published some awesome (as always) videos on this topic. https://youtu.be/PH9q0HNBjT4 and https://youtu.be/yPfagLeUa7k (edit: just realised one is linked in the references, just under the real name rather than nick) reply Eddy_Viscosity2 5 hours agoprevThis is great, but this approach will break down a bit for REALLY big waves as they become non-linear. Water waves are not sinusoidal, but are close enough for small and medium wave heights that these methods work really well. The big big waves are not only much farther from sinusoidal but the waves start interacting with each in a non-linear way that can't be captured by the linear superposition approach used here. So for most sea states, this is fantastic. But if you want to do the 'perfect storm' wave or something like that, you need to use a different approach for realism. reply tomcam 9 hours agoprevI spent years living on the beach. When you live on the beach, you watch the ocean for hours at a time because it’s mesmerizing and feels sensational. I wouldn’t guess for a second this was a render. reply tsurba 9 hours agoprevShadertoy also has nice ones https://www.shadertoy.com/view/Ms2SD1 reply julosflb 6 hours agoprevVery neat! I'm hydrodynamics by background and I wondered a long time ago why this kind of approach was not used as I always found ocean waves to look awful in movies. Once you describe ocean sea state in frequency domain, it is quite easy to give to floating objects like ships realistic motions using what we called RAO in this field (linear operator). You can also model sea disturbance (diffracted and radiated waves) caused by an object in a similar fashion. reply convivialdingo 6 hours agoprevFourier was measuring tidal waves when he came up with wave frequency transforms - so in a way this is almost a full circle. Very impressed! reply eps 10 hours agoprevDon't have any substantial comments, but I must say that the result is really impressive. Just wow. reply bnegreve 8 hours agoprevIt is very impressive, and better than anything I've seen before but think something is bit off with the swell. If I had to explain I would say that high frequency waves don't travel on top of low frequency waves the way they do in the video. reply lambdaone 5 hours agoprevThis is astoundingly good work; even though, as other commenters have said, it could still be improved on, the fact that it achieves such a level of realism with such a simple and elegant framework is truly impressive. reply lukko 7 hours agoprevOh my - this is great! Does it mention what the density of the mesh is, or is it a flat plane with no displacement? Also, I wonder if there's a way to optimise the foam particles in some way. It does seem very wasteful to generate them across the whole plane, when most are culled. I wonder if the particle emission / creation could be linked to foam density? reply nkrisc 5 hours agoparentI didn’t see a mention of the mesh density, but peeking at the main scene file (I assume the one seen in the video), this appears to be the mesh used: https://github.com/2Retr0/GodotOceanWaves/blob/main/assets/w... reply lukko 3 hours agorootparentah amazing, thanks - looks like 330k vertices reply bee_rider 3 hours agoprevIt looks really good. Out of curiosity, I’m sure everybody has heard of the FFT. They are quite general and used all over the place, and I imagine they’d be the first thing somebody would reach for if they had to describe waves. But I’d never heard of Gerstner waves. This leads me to believe that Gerstner waves are a more specialized thing. Since lots of work has already gone into rendering water, I tend to assume the method with a name I’ve never heard of was only reached for after very clever people rejected Fourier transforms for some reason. But, the results look better than most of what I see elsewhere. Has something changed to enable the more conventional solution? reply kroolik 10 hours agoprevThat's some National Geographic clip. Now, show us the demo! reply HL33tibCe7 7 hours agoprevThis is beautiful, I'd love to have this as a lockscreen or even a screen panel on a wall somewhere reply shireboy 4 hours agoprevNice, but may need to adjust the limits: https://news.ycombinator.com/item?id=41631177 reply cheschire 7 hours agoprevWhile I appreciate ever more realistic water bodies, the part that game makers really struggle with is where the water encounters an obstacle. I did not see any mention of this in the description. Conceivably though, this is not a huge conceptual leap right? A game maker would simply need to add logic to impact the frequencies near objects, no? reply magicalhippo 6 hours agoparentThe referenced paper \"Simulating Ocean Water\" talks about this in section 5. Indeed the FFT approach makes this difficult, where a different approach[1] for waves reflecting off obstacles. That approach uses convolution, however you can perform convolution using FFTs[2], so perhaps there's some nice way to combine the two approaches. edit: I just skimmed the papers, and it seems[1] does indeed mention combining the FFT approach with the convolution approach in the section on Ambient Waves. [1]: https://people.computing.clemson.edu/~jtessen/reports/papers... [2]: https://phys.uri.edu/nigh/NumRec/bookfpdf/f13-1.pdf reply thom 6 hours agoparentprevTo an extent you can get away with just sampling the output of the water shader to work out the water's effective height at any given point. Big changes in height, or buoyancy for non-static objects, indicate bigger angles hitting the waves and you can fake some splashes with particles while the wave itself just gets occluded. Apply forces at just three or four points and you can make a boat rock pretty believably on top of this kind of water. reply Archelaos 9 hours agoprevThe demo looks unrealistic. The waves never break. Increasinlgy steeper slops with pointy peaks travel from left to right until they just sink down towards the left. reply AlunAlun 8 hours agoparentThis is explicitly a simulation of ocean waves, and ocean waves do not break. reply Archelaos 7 hours agorootparentOkay. That makes sense. What about the steep elevation? Shouldn't it be smoother at the top? reply UniverseHacker 5 hours agorootparentAs someone who has done offshore sailing… the waves also look unnaturally steep/tall/pointy to me. It’s very cool, but needs some tweaking still. reply rkagerer 9 hours agoprevThe sample video is really impressive, it's worth a peek. reply aetherspawn 6 hours agoprevI think the downside of this approach is you can’t ie split the waves with a ship. reply coldcode 6 hours agoparentYeah I wondered how that could be done, otherwise this would only be good for backgrounds. reply chhs 9 hours agoprevLooks incredible! reply bobim 5 hours agoprevI would like a Silent Hunter III remake with this. reply arminiusreturns 3 hours agoprevThis is awesome to see! Much more progress than I have made on my gplv3 version for my system. (the original intention was to replicate Blackwake style ship combat) For what it's worth, the real difficulty in gameplay is getting physics objects to interact with the waves properly. reply jkmegtu 4 hours agoprevFree fire reply pmarreck 6 hours agoprevHow do I get this masterpiece running locally to play with? I don't even know what Godot is (but I've used makefiles and such) reply eaglefield 3 hours agoparentThe github repository is a godot project. Godot is a game engine. The quickest way is to download godot. Clone the repository and open the folder as a project in godot. reply jimmySixDOF 5 hours agoparentprevIf you have a Quest VR device Godot just released an editor with live scene updates I am bookmarking this to try see what it looks like in there. reply krapp 1 hour agoparentprevGodot is an open source game framework: https://godotengine.org/ You would presumably need to download the Godot framework and open it in the framework. reply wslh 6 hours agoprevGreat! I've shared this with every physicist I know who's not directly involved in animations. Quick question from my swimming class yesterday: We know that professional swimmers use a range of technologies, both old and new, in their training. Is there currently a model that fully simulates the physics of swimming across different styles? If not, this seems like a great project idea! reply ReptileMan 8 hours agoprevThat looks wet. And honestly it is the best compliment I can give. reply Mattish 9 hours agoprev [–] 2 of the linked references have full implementations of very similar things, with some shared references. Is there something here which godot is enabling which wasn't previously possible? It seems to be entirely GPU compute workload with particles which are available as part of all mature rendering engines reply arminiusreturns 3 hours agoparent [–] Godot itself is open source, which I think has brought a strong community of people to it. My mega-big-ultra project I've been working on since 2013 would be nowhere near as close to where it is now (close to launching crowdfunding/alpha) if I hadn't transitioned to Godot! (I was fighting the UE4 system from the days when it was $20/mo, we linux people really got dealt dirty by Tim Sweeny, with lots of empty promises for linux support) reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "**GodotOceanWaves** is an open ocean rendering experiment in the Godot Engine, utilizing the inverse Fourier transform for wave generation, allowing real-time modification of wave properties.",
      "The project employs the Fast Fourier Transform (FFT) algorithm for efficient computation on the GPU, and uses a BSDF lighting model with GGX distribution for realistic ocean shading.",
      "Features include sea foam and spray simulation, wave cascades to address tiling artifacts, and load balancing to minimize GPU workload, making it a comprehensive tool for simulating various ocean environments."
    ],
    "commentSummary": [
      "FFT-based ocean-wave rendering in Godot has gained attention for its realistic wave simulations, despite some noted limitations like the absence of breaking waves and steep peaks.",
      "The discussion includes the evolution of rendering technology and the challenges of simulating realistic water interactions, with users expressing admiration for the project's potential applications.",
      "Users also speculate on the author's academic background and share links to related resources and videos, highlighting the broader interest in the author's other Godot projects."
    ],
    "points": 528,
    "commentCount": 75,
    "retryCount": 0,
    "time": 1727506301
  },
  {
    "id": 41678208,
    "title": "Amusing Ourselves to Death (2014)",
    "originLink": "https://otpok.com/2014/01/03/amusing-ourselves-to-death/",
    "originBody": "Amusing ourselves to death Talha Ghannam / January 3, 2014 This is perhaps one of the most striking passages I have read for a while. It describes the modern world with startling accuracy. In our fear of an increasingly authoritarian rule, we have allowed a far more dangerous vision to come true: heedlessness Below is the foreward of Neil Postman’s book “Amusing Ourselves to Death: Public Discourse in the Age of Show Business“, accompanied by a comic illustration of the two ideas. It gives a concise comparison of the two authors views and what they foresaw society will become. But perhaps the remarkable part of this whole story passage lies beyond its lines with us: Most of us will read this and continue living our life exactly the same way as before …wake up ————————————————————– We were keeping our eye on 1984. When the year came and the prophecy didn’t, thoughtful Americans sang softly in praise of themselves. The roots of liberal democracy had held. Wherever else the terror had happened, we, at least, had not been visited by Orwellian nightmares. But we had forgotten that alongside Orwell’s dark vision, there was another – slightly older, slightly less well known, equally chilling: Aldous Huxley’s Brave New World. Contrary to common belief even among the educated, Huxley and Orwell did not prophesy the same thing. Orwell warns that we will be overcome by an externally imposed oppression. But in Huxley’s vision, no Big Brother is required to deprive people of their autonomy, maturity and history. As he saw it, people will come to love their oppression, to adore the technologies that undo their capacities to think. What Orwell feared were those who would ban books. What Huxley feared was that there would be no reason to ban a book, for there would be no one who wanted to read one. Orwell feared those who would deprive us of information. Huxley feared those who would give us so much that we would be reduced to passivity and egoism. Orwell feared that the truth would be concealed from us. Huxley feared the truth would be drowned in a sea of irrelevance. Orwell feared we would become a captive culture. Huxley feared we would become a trivial culture, preoccupied with some equivalent of the feelies, the orgy porgy, and the centrifugal bumblepuppy. As Huxley remarked in Brave New World Revisited, the civil libertarians and rationalists who are ever on the alert to oppose tyranny “failed to take into account man’s almost infinite appetite for distractions”. In 1984, Huxley added, people are controlled by inflicting pain. In Brave New World, they are controlled by inflicting pleasure. In short, Orwell feared that what we hate will ruin us. Huxley feared that what we love will ruin us. This book is about the possibility that Huxley, not Orwell, was right [Neil Postman – Amusing ourselves to death] [The comic is Stuart McMillen’s interpretation of media theorist Niel Postman’s book Amusing Ourselves to Death (1985), subtitled “Public Discourse in the Age of Show Business”] Share this: Twitter Facebook Like Loading... January 3, 2014 in Book Recommendations, Continuing the path, General Wisdoms, Main Posts. Tags: 1984, aldous, amusing, death, george, huxley, neil, orwell, ourselves, postman, to",
    "commentLink": "https://news.ycombinator.com/item?id=41678208",
    "commentBody": "Amusing Ourselves to Death (2014) (otpok.com)300 points by yamrzou 13 hours agohidepastfavorite224 comments richk449 1 hour agoFor those who haven’t read the book, Amusing Ourselves to Death is incredible, and absolutely worth reading. One of my mentors gave it to me years ago and it became one of those mind blowing reads. In the book, Postman analyzes how media affects humans and society. He basically gives a framework for predicting and understanding the effects of different types of media. The book was written before social media, so the examples are books, newspapers, tv, radio, etc. But so much of social media seems obvious once you read his analysis. Every time I see the typical discussion (person A: social media makes people dumb; person B: Plato said books make people dumb) I think that the discussion could use some Postman - not all media affects us in the same way - some media encourages behaviors that are good for society, and some media encourages behaviors that are bad for society. reply Liquix 1 hour agoparentPrecisely - the medium is the message. Books are slow, deliberate, require imagination and attention. Seven second tiktok videos are the opposite. The method by which we consume the information impacts us just as much if not more than its content. reply detourdog 46 minutes agorootparentThe issue is spending equal amounts of cumulative time in 7 second disparate chunks. The book could build to a complex insight. The other strange part of video media is that intimate parasocial relationships it builds. reply im3w1l 12 minutes agorootparentBooks also build intimate parasocial relationships. People feel a sense of loss when finishing a book. reply antisthenes 1 hour agoparentprev> I think that the discussion could use some Postman - not all media affects us in the same way - some media encourages behaviors that are good for society, and some media encourages behaviors that are bad for society. I think media simply mostly brings out the behaviors already inherent to people. If a shitty person sees their behavior validated in media, they are now more likely to act it out in real life. Likewise for good behaviors. Some people are definitely more susceptible to such influence than others. reply ccorcos 8 minutes agoprevThis comic misses some of my favorite details from the book. The book talks a lot about Marshall McLuhan's quote \"the medium is the message\" and about how discourse has turned more and more into entertainment. Nixon lost to Kennedy because he was more attractive on television, and people are judged by how they look or behave as opposed to what they say. More than anything, this book really made me appreciate written discourse. reply r721 11 hours agoprevOriginal comic source: https://www.recombinantrecords.net/docs/2009-05-Amusing-Ours... (archived: https://web.archive.org/web/20100111052839/http://www.recomb...) HN discussion from 2010: https://news.ycombinator.com/item?id=1554733 Wikipedia entry for the book: https://en.wikipedia.org/wiki/Amusing_Ourselves_to_Death reply seabombs 9 hours agoparentBit of an aside, it was fun to notice the Australian references in the comic. Surprises me still to see something Australian on the \"regular\" internet. reply throwaway2037 5 hours agorootparent> the Australian references I read the comic. Which Aus refs? reply everybodyknows 2 hours agorootparent\"orgy porgy, and the centrifugal bumblepuppy\"? These are unknown to most Americans. reply 0_____0 1 hour agorootparentIsn't that a reference to Brave New World? reply Jtsummers 58 minutes agorootparentYes. reply tehnub 11 hours agoprevPeople make too much of what Orwell supposedly feared may happen some day. He was writing about stuff the Soviet and British governments were doing in his time, and in particular, imagined what Soviet rule over Britain may look like. Assigning this philosophy to him and criticizing him for it seems unfair. reply UniverseHacker 5 hours agoparentAlso… maybe he achieved his purpose? Most people have read his books, and it helped generate a widespread aversion to authoritarianism… actually preventing it from happening in the USA and Britain. Many countries nowadays do have societies and governments that look a lot like 1984. reply bdowling 3 hours agorootparentNo, most people pretended to read Animal Farm and 1984 in middle school and haven’t thought about them since. They didn’t understand them at the time, and they don’t see the similar manipulations going on in today’s society. reply scarecrowbob 2 hours agorootparentI agree. _Animal Farm_ was taught to me by my capitalist and authoritarian teachers as an anti-communist screed. The problem with the farm is that the pigs end up being capitalists, though I have never seen or heard of the text being taught to children as an anti-capitalist text. This culture does a very good job of assuming that the field of possibilities can easily and quickly be reduced to two choices and, further, that we're forced to choose between the two and, finally, that anyone who thinks there are additional possible choices is \"being childish and unrealistic\". I find the US to be highly authoritarian, full of easy examples double-thought and duck speak. I don't, however think folks are being manipulated: I think that the fundamental authoritarian move is that folks here have manipulated themselves. To me, that's close to the point of 1984. Or Mark Fisher's books, or Ursula Le Guin. I'm in the middle of Butler's _The Parable of the Sower_ and it's feeling like that self-deception is core to what is happening in that book as well: folks reducing the world to some pretty horrible binaries and then becoming happily ensnared in the ensuing problems. reply johnchristopher 39 minutes agorootparent> I don't, however think folks are being manipulated: I think that the fundamental authoritarian move is that folks here have manipulated themselves. To me, that's close to the point of 1984. Yes. It's rare to find people online who understand this point of 1984, that everyone becomes big brother and that's how it (it being the oppressive system in place to keep people in check) sustains itself. reply takinola 2 hours agorootparentprev> I find the US to be highly authoritarian, full of easy examples double-thought and duck speak. I’m curious to know where else you have lived (I assume you live in the US). As someone that has lived under an authoritarian regime, I find this statement really hard to agree with. The US is far from perfect but it is far from authoritarian in my opinion. reply UniverseHacker 2 hours agorootparentprevIt's neither anti-capitalist nor anti-communist, just anti-authoritarian. reply skeeter2020 1 hour agorootparentThat's what I was thinking. First, if you don't remember the text go read it right now; it will take you an hour. It's not really pro- or alt- anything, as much as anti- reply 363874844 2 hours agorootparentprevDid it really though? Ideological censorship has been on the rise for awhile. Books might only be occasionally banned but that's because their relevance in the modern zeitgeist has waned. The privatization of the public square has nonetheless meant that moderation of communication has become widespread and politically charged. reply UniverseHacker 2 hours agorootparentI didn't claim Orwell had universally eliminated all traces authoritarianism... it is rising in popularity right now. Only that it may have shifted things enough to keep us from literally living in a 1984-esque society nowadays... enough so that people in this thread and the article above are saying Orwell was \"wrong.\" reply Mistletoe 4 hours agorootparentprevI think you are right. I wish people paid attention to Blade Runner, Alien, etc. and realized what an equal or greater danger unchecked corporations are. reply skeeter2020 1 hour agoparentprevDid he fear what would happen, or propose extrapolating the current experience as a cautionary tale? Meanwhile Huxley presented a world where we voluntarily pursue ignoble goals. I don't think we're criticizing him as much as mourning we appear to have palced the yoke upon ourselves. reply m463 23 minutes agoparentprevWhy can't it be both outcomes at once? reply helloplanets 10 hours agoprevA pernicious excitement to learn and play chess has spread all over the country, and numerous clubs for practicing this game have been formed in cities and villages…chess is a mere amusement of a very inferior character, which robs the mind of valuable time that might be devoted to nobler acquirements … they require out-door exercises–not this sort of mental gladiatorship. A game of chess does not add a single new fact to the mind; it does not excite a single beautiful thought; nor does it serve a single purpose for polishing and improving the nobler faculties. Scientific American, July, 1858 [0]: https://www.smithsonianmag.com/smart-news/19th-century-conce... reply Topfi 9 hours agoparentAnother more historic example in the same mold: >>If men learn [to read], it will implant forgetfulness in their souls. They will cease to exercise memory because they rely on that which is written, calling things to remembrance no longer from within themselves, but by means of external marks. Lenin eventually took the same view - that chess was distracting him from a higher purpose The world would have been a better place had he kept playing chess then. reply kubb 10 hours agoparentprevI kind of agree. Chess sucks big time, especially played online. Playing with your grandpa in the park is OK. reply ffsm8 10 hours agorootparentnext [4 more] [flagged] Aeolun 10 hours agorootparentAside from much more variables in Starcraft and co? reply mikub 10 hours agorootparentprevWhy does he have to be bad at chess if he doesn't like playing online? Also I think \"Playing with your grandpa.\" was just a metaphor for, playing with people you like. Some people just play games to have fun, you know, not everything has to be a competition. reply ffsm8 10 hours agorootparentBecause their argument wasn't \"competitive gameplay is terrible for society\", but \"chess is bad\". But even if it was the former argument: that's unquestionably false too. Just look at professional sports like soccer or Basketball. Surely you're not going to argue that this is bad...? Because that's fundamentally the same, just on a different topic reply badpun 8 hours agoparentprevThere was some chess prodigy who, in his teens, was already winning against most champions, and who in his early twenties abandoned chess altogether, citing it a \"waste of life\". reply nick0garvey 5 hours agorootparentMorphy, one of the greatest players of all time, is famous for this. \"The ability to play chess is the sign of a gentleman. The ability to play chess well is the sign of a wasted life.\" reply HellDunkel 10 hours agoprevA couple of years ago i was working for a design studio which produced an image movie for a big cooperation which somehow painted an utopian future for their upcoming product ideas. In that movie there was a woman reading in \"Brave new World\".[EDITED] It was clear none of the people involved read the book. My remarks were swept aside by claiming hardly anyone has read the book anyway... headlessness might be a real issue. reply UniverseHacker 5 hours agoparentThat’s hilariously ironic reply hoppyhoppy2 9 hours agoparentprevDo you mean \"Brave New World\"? reply willguest 10 hours agoprevGiven that the vast majority of people go to work to earn money for businesses that exist either to exploit natural resources or appreciate in value in the eyes of an economic system that prioritizes increasing capital valuations above all else, including human dignity, long-term survival and the life of other species, I would say we're already there. Talking about a dystopian future is a convenient way to assuade our sense of dissonance that the present is most certainly not that. Case in point, nobody wants to rid the Earth of insects, fill the oceans with plastic or plough microplastics into every orifice, but we are all complicit and can't seem to gather ourselves to fix it. reply yaky 4 hours agoparent> First of all, I know it's all people like you. And that's what's so scary. Individually you don't know what you're doing collectively. - The Circle by Dave Eggers > In the course of her job, Resaint had met people like Megrimson, executives who went into work and sat down at their desks and made decisions that ravaged the world. They didn't seem evil to her. They seemed more like fungal colonies or AI subroutines, mechanical components of a self-perpetuating super-organism, with no real subjectivity of their own. That said, she would have happily watched any of them die. - Venomous Lumpsucker by Ned Beauman I know it's still science/climate fiction, but very relevant to your point. reply TacticalCoder 8 hours agoparentprev> Given that the vast majority of people go to work to earn money for businesses that exist either to exploit natural resources ... Or for governments, doing government jobs that produce absolutely nothing of value and force people to waste a big chunk of their lives on administrative tasks... reply epicide 6 hours agorootparentOr for corporations that produce things of negative value and force people to waste a big chunk of their lives on administrative tasks. reply CraigJPerry 4 hours agorootparentprev\"a form of paid employment that is so completely pointless, unnecessary, or pernicious that even the employee cannot justify its existence even though, as part of the conditions of employment, the employee feels obliged to pretend that this is not the case\" — Nathan Heller Whats the public vs private split to this idea? Its not a new idea - “Half the money I spend on advertising is wasted; the trouble is I don't know which half.” — John Wanamaker reply randomdata 4 hours agorootparentHe doesn’t speak to any particular split. The government forces the private sector to do pointless work as much or more. reply specproc 5 hours agorootparentprevI come from a town where the biggest employer is the state in a few different forms. I think it's entirely valid for the government to keep them all busy 9-5, salaried and pensioned. Main function of the state IMO. I don't fear government, I fear the lack of it. reply willguest 2 hours agorootparentThis doesn't deserve to be downvoted, it is very legitimate point. I think there is a role for public organisation, with political groups being one type. I am, however, critical of the prevailing agenda, since they often exist in a system where money can play a big role in deciding which priorities rise to the top. I am not sure that politics plays such a central role as it used or as many people assume. Our society today seems to be divided functionally... we answer to many bosses, some economic, some political, some technological, and so on. For me the more/less gov't debate misses the important points. It is the incentives, mechanism and processes that determine their value. I think you are referring to maintain order and keeping the peace - valuable functions, but made more necessary when people are under such strain in their daily lives. reply verisimi 3 hours agoparentprevDo you really think the world has no insects, that the ocean is full of plastics and that there are micro-plastics everywhere? Have you personally checked any of these claims? Or are you depending upon (possibly fraudulent) science claims? Additionally, if you want to fix something, go ahead and do it. But don't start petitioning this out that authority to do it - they won't, you won't like the result, but you are sanctioning use of force against others. reply willguest 2 hours agorootparentDoubt is not a strategy and nay-saying rarely moves a conversation forward. My comment was not intended to illicit the abdication of responsibility - quite the reverse. Where was my petitioning? Your response is combative, seemingly intended to be personally offensive and built on a straw-man argument, likely the result of a multitude of others treating your opinions like crap. I'm not them... You are absolutely right, agency and the realisation of it is a key component to shifting us out of this mess. I put significant daily effort into building an alternative kind of personal existence, at significant personal cost. I attempt to carry as much of this as I can since I recognise my own complicity in this mess. reply verisimi 1 hour agorootparentMy more relevant point to the OP, with reference to your comment, is that no, we are not amusing ourselves to death. It's more that we are worrying ourselves to death, despite a lack of personal indicators about this or that. If you want to proceed meaningfully to self, this requires prioritising one's personal experience over the provided stats, which are as much a control/guidance mechanism as anything. reply cholantesh 2 hours agorootparentprevAre you aware of widespread fraud in the data associated with insect population decline? Or are you arguing that science is just inherently fraudulent or unreliable? These sound like pretty extreme responses of the type ancaps gave for years to the claim that global surface temperatures were rising. Those looked pretty quaint even without the benefit if hindsight. reply verisimi 1 hour agorootparentI am arguing that scientific data is unreliable in general. reply cholantesh 29 minutes agorootparentWhich is simultaneously broad and vacuous. Why? Compared to what? In this specific context, what method would provide a better intellectual foundation? reply verisimi 20 minutes agorootparentPersonal verification. At least some attempt towards that. And I don't mean reading an article. The scientific method applied personally. Pretty much no one is checking anything. reply zafka 2 hours agorootparentprevI have lived in South Florida, close to the Atlantic ocean for close to 40 years. In the time I have lived here, the insect populations have noticeably dropped. I have also queried relatives from the midwest. It used to be every summer your car would get covered with dead bugs, not so any more. As for plastics in the ocean, every time I go to the beach I see a lot of macro plastics. reply verisimi 1 hour agorootparentThank you for your personal testimony. It's good to hear that in your experience it seems that the insect populations really have dropped. Of course, there may be other reasons - eg if there are highways where it used to be small roads, you would expect insects to stay clear of the area. Also, litter you can see, is not microplastics - you can't see the microplastics.... But they are there! Apparently. reply willguest 1 hour agorootparentSo, objective science is untrustworthy and subjective testimony is easily explained away. Let me guess - only your opinion is the one that matters. You have a poorly-crafted answer for both sides of the coin, but also fail to read the details of the things you oppose. reply verisimi 22 minutes agorootparentOne has to ask oneself, is it better to have a comforting story, that is likely leveraged for someone, somewhere's benefit, or to start with the honest position, which is that \"I don't know\". One can of course become more certain of whatever-it-is, but not without attempting some personal research. Or, one can just defer all personal responsibility and parrot whatever the consensus view is. reply podviaznikov 12 hours agoprevLive Neil Postman. Discovered him around 2016 and read many of his books. And planning to regularly reread him. So many things changed since he died but his ideas hold up pretty good. reply doubleorseven 10 hours agoparentHe passed away before the first iPhone and now my only 2 wishes are: 1) a new book about how smartphones revolutionize the modern world and 2) a new Lauryn hill album reply tines 11 hours agoparentprevTechnopoly is also amazing, make that your next read. reply podviaznikov 4 hours agorootparentyes, I’ve read that and many of his books. they all a bit similar and he reuses tons of similar quotes and ideas. but that is even better, he just tries to drive the same points. reply j_maffe 8 hours agoprevContent from Amusing Ourselves to Death presented as a visual comic to facilitate/\"enhance\" its communication is deeply ironic. Can't wait for the TikTok video. reply tlb 10 hours agoprevThe sad thing is that none of it is very amusing. Current events twitter is more aggravating than amusing. We're aggravating ourselves to death. reply tempodox 8 hours agoparentThe point is that it's both distraction. Social media has told us that enragement sucks up even more attention than amusement. Mission accomplished. reply FrustratedMonky 5 hours agoparentprev\"aggravating than amusing.\" But it is distracting. Huxley didn't necessarily say everything was fun, just that it is distracting. Rage, anger, outrage, keeps people engaged more than amusing. reply Cthulhu_ 8 hours agoparentprevYeah, and then you do something fun to distract you from said current events. reply TacticalCoder 7 hours agoparentprev> Current events twitter is more aggravating than amusing. Because the situation is grave? I think that many things in the world recently are seriously fucked up and the mainstream media are completely manipulated (an example would the FCC fasttracking the acquisition by Soros of 200 radio stations in the US right before the elections). X/Twitter is showing things as they are: be it SpaceX launches or things actually happening in the world. If I read a post on X / Twitter about the US being now $35 trillion in debt, is it the fault of Twitter that the world's first economy is so much in debt that very serious shit may happen at some point? Don't shoot the last non-censored messenger and don't put your head in the sand. reply UniverseHacker 5 hours agorootparentI briefly spent some time on Twitter a few months ago, and it was almost entirely widespread hysteria over things that were easily verifiable as trivially false. For example, at the time people were enraged about a supposed trans woman winning a boxing match in the olympics- but she was not in fact a trans woman. My take is that X/Twitter is mostly sharing fake rage bait, to emotionally manipulate people into supporting various hate based political movements. reply wrs 4 hours agorootparentprev> X/Twitter is showing things as they are Wow. No, X/Twitter is showing things people (and quite a few bots) say. There’s no mechanism there to emphasize things that are. In fact you’re rewarded for saying things that aren’t, if they generate engagement, which is so much easier if you just make stuff up. reply ptero 6 hours agorootparentprevWhile the situation may be grave it was usually much worse for the vast majority of the population. People are aggravated and enraged by famine deaths precisely because they affect much smaller percentage of people than they used to, where death from hunger was common. The debt and likely coming financial repression is a weaker form of coin debasement practiced regularly by many kings. And so on... reply dave333 11 hours agoprevDoing things merely to stimulate pleasurable brain chemistry is fine unless all you do is play games or watch formulaic media that have no lasting effect or achievement. reply bee_rider 3 hours agoparentFormulaic or novel media doesn’t make a huge difference, it’s just passive consumption either way. Coincidentally I’ve been listening to Divers a bunch recently (which is a really great album, although I am just passively consuming just like everything else… I don’t know if it falls under formulaic for you, but it is a good formula if so). A lot of it is about death and time, and she ends one of the songs with a sort of gentle sing-songy “Look, and despair.” I always read the “Look on my works, ye Mighty, and despair!” in Ozymandias as a giant “fuck your hopes and achievements” from that ancient king. Anyway, Joanna Newsom’s delivery is a lot more gentle, so maybe the fact that all of our effects and achievements will ground to dust is not so bad. Mattering would be very stressful. reply mediumsmart 10 hours agoparentprevwhats wrong with headshots on the PC while doomscrolling on the phone in the age of monsters and idiots? reply jll29 7 hours agorootparentIf you, like me, would have had to watch people play Candy Crush on the Tube [= the London subway] on the way to work every morning for seven years, you would not ask. I felt terribly sorry for that loss of GDP and decline in mean global IQ. reply Trasmatta 6 hours agorootparentWhy would playing Candy Crush on the subway have any impact on GDP or IQ? Sounds like a non sequitur. reply Toorkit 5 hours agorootparentThe plebians should be doing business during their commute, never stop hustling! /s reply alecco 7 hours agoprevGiven the current pro-war propaganda all over the place combined with the creeping cost of living, I think Nineteen Eighty-Four is becoming more prescient than Brave New World. reply r721 5 hours agoparentThe topic of \"creeping cost of living\" reminded me of two other good dystopian novels: https://en.wikipedia.org/wiki/Make_Room!_Make_Room! https://en.wikipedia.org/wiki/334_(novel) reply BoingBoomTschak 2 hours agoparentprev1984 can't be more prescient than a book that has already become reality in most of the world that matters (culture, art, science, influence wise). If you don't see that hedonism and general moral decay has become the overwhelming norm, you're probably part of it. 1984's surveillance may be here, but the brute force is only reserved for the rare nails sticking out; if those haven't already gone crazy from surviving in the sane asylum. reply schmookeeg 1 hour agoprevI find it really strange that the 3 paragraphs of text at the top needed the comics summarizing them below. Like, did our short attention spans need those little footholds in order to progress through the point being made? :) Sounds like I need this book added to my reading list. I've not been able to get through Brave New World, but I might give it another try also. reply have_faith 10 hours agoprevPeople always reference 1984 but Orwell’s essay “Pleasure Spots” is probably more relevant to this subject: https://www.orwellfoundation.com/the-orwell-foundation/orwel... reply jumping_frog 8 hours agoparentI think there was a quote by a Nordic writer (possibly Hans Christian Andersen) in which he talked about how circuses and amusement parks keep people distracted and busy so we don't focus on important policy issues. https://www.bbc.com/culture/article/20220818-the-surprisingl... https://en.wikipedia.org/wiki/Bread_and_circuses reply yamrzou 5 hours agorootparentLa Boétie? Plays, farces, spectacles, gladiators, strange beasts, medals, pictures and other such opiates, these were for ancient peoples the bait toward slavery, the price of their liberty, the instruments of tyranny. By these practices and enticements the ancient dictators so successfully lulled their subjects … that the stupified peoples, fascinated by the pastimes and vain pleasures, … learned subservience as naively, bit not so creditably, as little children learn to read by looking at bright picture books. reply robocat 7 hours agorootparentprevThat references the Latin phrase: panem et circenses Paragraph from a page describing it: It refers to a concept prevalent in ancient Rome, where the government would provide its citizens with free food and entertainment in the form of lavish spectacles, such as gladiator fights, chariot races, and theatrical performances. The phrase highlights the strategy employed by the ruling class to keep the population content and distracted from important political issues and matters of governance. Orwell and Huxleys work are both centralised (authoritarian - you had to take your Soma) whereas our current risk is possibly more systematic and less conspirational. reply aklemm 4 hours agoprevI just listened to this a few weeks ago. It’s incredible and really helps frame how we got here and is still very relevant to social media even though it’s written about TV/Hollywood. You’ll be hard pressed to find deeper media analysis that remains very approachable. reply keybored 9 hours agoprevThe article just lifts the content from the book and doesn’t add anything original. Great. We’ve heard. reply jll29 7 hours agoparentI would say 1984 is rather more subtle than portrayed here. For instance, the masses are kept at bay by scaring them with perpetual wars on the one hand and by keeping them distracted with machine-generation filthy \"literature\" (we only reached the level of technical sophistication to do that courtesy GPT/Llama last year). That part is more similar to what the post portrays as the \"Huxley\" view, perhaps. The appendix of 1984 (\"Newspeak\") is a masterpiece on its own, redefining English words like \"freedom\" so that they can only mean \"free from lice\", with the notion of freedom forgotten. reply bsenftner 9 hours agoparentprevExactly, and the comments here are not about the point in the article, just like the article points out people missing the point of Huxley. reply mixtureoftakes 10 hours agoprevMost of us will read this and continue living our life exactly the same way as before …wake up reply jumping_frog 8 hours agoparentEven if people wake up and \"do something\", govts will just tire us out. Similar to how online protests against reddit, (or on ground protests like occupy [X], and so on) and others failed. We have no option but to accept what is handed out to us. reply r721 5 hours agoparentprevReminds me of my favorite copypasta: >If you're reading this, you've been in a coma for almost 20 years because of a car accident. We're trying a new technique. We don't know where this message will end up in your dream, but we hope we're getting through. Please wake up. reply epicide 6 hours agoparentprevWake up and... do what exactly? Tell others to \"wake up\" ad nauseum? The whole \"wake up, sheeple, you're being manipulated\" is both correct and amusingly self-terminating. Metacognition, for all its benefits, comes with the newfound sisyphean task of being unable to intentionally avoid thinking about a white elephant for an entire minute. \"Don't be influenced by the ads/media/propaganda\" works about as well. So perhaps the best way to reduce manipulation is to find a way back to sleep sometimes. A sort of meta-meta-cognition, if you will. It's self-awareness all the way down. reply imjonse 11 hours agoprevThe book's title is a nod to the Roger Waters album/song that deals with the same theme. reply kurtdev 11 hours agoparentThe book predates the song and album by about 7 years, so the album name references the book. Postman even mentioned the fact in his 1995 book \"The end of education\" reply m-i-l 8 hours agorootparent> \"Postman even mentioned the fact in his 1995 book \"The end of education\"\" Quote from Postman according to wikipedia[0]: \"the level of sensibility required to appreciate the music of Roger Waters is both different and lower than what is required to appreciate, let us say, a Chopin étude.\" Ouch. I actually got the album when it came out, and was roughly aware of the concept and the book from reviews in the music press. Had I known that it was comparing Orwell and Huxley I'd have definitely made the effort to read more. But this was before the internet so it wasn't easy (you had to do things like going to a public library), so technological progress is not all downside. [0] https://en.wikipedia.org/wiki/Amusing_Ourselves_to_Death reply imjonse 6 hours agorootparentprevThank you. I was mislead by the date on the post and did not know the book was older. TIL. reply LeonB 11 hours agoparentprevI think it’s the other way around — the book is from ~ 1985 while the Roger Waters albums is ~ 1992. reply imjonse 6 hours agorootparentThank you, my bad, you are right, as the other sibling comment. reply indy 10 hours agoprevDopamine is one hell of a drug. reply AStonesThrow 10 hours agoparentOutrage and fear are exhausting, let me tell ya. Somehow I cannot get away from nursing my PTSD online, with sick pleasure in picking fights and \"winning\" arguments. Sometimes I wake up with a thread racing through my mind and the perfect retort to my \"adversary\" I honestly don't hate you guys, but you give my life purpose and meaning... So thank you reply naming_the_user 12 hours agoprevLegendary comment from the old boy Terry Davis as the top post there. reply edm0nd 12 hours agoparentGods true OS reply rlt 12 hours agoparentprevRIP reply becquerel 12 hours agoparentprevThe only true seer of the modern age. reply photochemsyn 4 hours agoprevBrave New World supposes a world of plenty controlled by a few ruling oligarchs and aristocrats; 1984 supposes a world of scarcity also controlled by a few ruling oligarchs and aristocrats. One society is controlled by the carrot, and in the other society, given the shortage of carrots, the stick is brought out to maintain the social order. reply FrustratedMonky 5 hours agoprevThe book came out in 2005. Was there any follow up, I didn't see one on the wiki. It seems like we are accelerating to this. Even the changes between 2005 and 2024. Near 20 years, we've leaned into the Huxley vision. Really leaned into it. This is all getting really scary. I feel like we should do something. We should really band together and change course. I volunteer to go out and do something, except of course, I'm a bit distracted at the moment, so maybe can we put off the change for another week? I really need to see the end of this season of \"Industry\". Then we can do something, I'm sure I'll have some free time next week to get right on this. reply layer8 1 hour agoparentThe book came out in 1985. The author was already dead in 2005. reply FrustratedMonky 1 hour agorootparentMissed that. It was re-issued in 2005. For 20th anniversary. So nearly 40 years. reply dangus 5 hours agoprevHuxley’s fears presented in this particular way are immediately debunked by actual book sales and education statistics. Independent bookstores have been consistently growing since 2009: https://www.hks.harvard.edu/centers/mrcbg/programs/growthpol... The book industry is expanding with particularly strong growth in e-books and audiobooks: https://worldmetrics.org/book-industry-statistics/ Educational attainment is generally increasing as time goes on in the US: https://en.wikipedia.org/wiki/Educational_attainment_in_the_... Voter turnout has increased over time in the United States: https://en.wikipedia.org/wiki/Voter_turnout_in_United_States... If anything I think that the general population is becoming more aware and educated. A more diversified leisure industry with more options than the days of having three channels on television is not the same as drowning in amusement, or the average person spending more time on amusement than on “serious” and “thoughtful” activities. Instead, it means that the individual has more options for forms of amusement they enjoy. reply FrustratedMonky 4 hours agoparentI think you can argue that 'books' were deemed as intellectual in Huxley/Orwell's time, so banning them would be a sign of society decline ---> BUT, todays books can be seen as just part of the entertainment distraction. Books sales are up, but how many of them are YA, Manga , Pop-Fiction. They are as shallow and distracting as a TV Show. I tend to think even reading the worst trash book is still better than Video. But it is still playing into distraction. Note: I Like Manga, but those series that are 100 volumes long, that is distracting. reply IndrekR 9 hours agoprev(2014) reply moffkalast 11 hours agoprevIt is interesting that these two books essentially show the most extreme end result of the two major economic systems. Socialist authoritarian communist states gravitate towards 1984, capitalist liberal democracies turn into Brave New World. reply rramadass 10 hours agoparentExactly! Both Orwell and Huxley are right but in different contexts. Also note that both of their works are an exaggerated caricature of aspects of Society which they wished to highlight and show its insidiousness. Thus one has to look beyond the \"painted picture\" and understand what was being meant. However; Orwell had a better insight on the overall issues which can be found in his essays eg. \"Notes on Nationalism\", \"All Art is Propaganda\", \"Politics and the English Language\" (eg. Newspeak) etc. reply renanoliveira0 4 hours agoparentprevWell said. I think the issue stems from the same point. Both cases assume that individuals are being coerced out of their potential to transform the world for the better, whether by Big Brother or by TikTok. In my view, both stem from an assumption that I don’t see playing out in the real world: that all individuals have the desire or capacity to make a difference and be something \"more\". I think this idea came from the Enlightenment. That’s when we started to forget that, unfortunately, the overwhelming majority are just here to occupy space. reply alexashka 11 hours agoprevThinly veiled 'I despise stupid people', this one. They'd be boozing (more than they already are) if there wasn't such variety of cheap and available entertainment, the author doesn't seem to realize? It's not what stupid people do in their free time - it's what capable and smart people value and pursue that makes all the difference. Nietzsche laid this out quite beautifully in Thus Spoke Zarathustra. Huxley and Orwell are kindergarten philosophy by comparison. reply yldedly 11 hours agoparent>it's what capable and smart people value and pursue that makes all the difference. How do you know capable and smart people will keep having good values? Seems to me that it's true until it isn't - populism takes over politics, ideology takes over the humanities, science gets Goodharted to death, etc. Values are highly circular - we value what high-status people in our (sub)culture value, and you become high-status by getting what people value. This holds for smart people as well. reply alexashka 10 hours agorootparent> How do you know capable and smart people will keep having good values? 'Good' values don't exist, so we need not worry about that one :) reply yldedly 9 hours agorootparentThen what do you mean when you say \"make a difference\"? reply moffkalast 6 hours agorootparentI think they mean the literal opposite of things staying the same, not the \"helping people\" idiom. reply yldedly 4 hours agorootparentFair enough, but for the sake of this conversation, if we say 'good' values are those that keep things from staying the same, aren't the values of smart people just as likely to evolve towards 'bad' ones? For example, I'm sure most people know at least one smart person who only plays video games; it does seem that we'll keep inventing forms of entertainment that wirehead people more and more effectively, which seems in line with the Brave New World scenario. reply malthaus 11 hours agoparentprevare you saying smart people are immune to the temptations of attention-dopamine? because i'd consider myself above average in terms of intelligence and ambition but i still fall into the procrastination trap often. now you might say that this makes me in fact \"stupid\" per your defininition (or maybe arrogant as i overestimate myself) but i see this in other people as well. i also would not say that being \"productive\" as in moving humanity ahead must be the KPI by which everyone is measured. you only have one life, you can spend it how you want, even if that is watching tiktoks 24/7. reply willguest 9 hours agorootparentThis sentiment, that each is entitled to a life of choosing, resonates strongly with the spirit of individualism. Within it there is a disregard for obligation or belonging that, I think, is connected to the desire for mindless occupation and distraction. I suspect that, the more one is cut off from a sense of collective purpose, the more one finds solace in activities that reinforce a sense of \"alright\" in place of true wellness. Btw, I'm also a big procrastinator and I consider it a gift. Many wonderful things in my life have been helped by it. In this sense, I agree that there is something about an inner drive that should be listened and reacted to, but I am not sure that all activities are of equal value. reply judofyr 10 hours agoparentprev> Thinly veiled 'I despise stupid people', this one. Are you talking about this comic (i.e. a few sentences from the book) or the whole book? I read the book a few years back and it's entirely focused on culture as a whole and less about the individual choices. He's not making a point of \"television makes you dumb\" (or \"dumb people watches television\"), but rather he makes the distinction between an \"oral\"-, \"press\"- and \"television\"-based culture. He claims that it's bad when television becomes the main platform that a society centers its communication around. He's also honest that there's probably far more junk (in absolute terms) in printing than in television: \"Television is not old enough to have matched printing's output of junk.\" It's not about the amount of \"junk\" – it's about something more fundamental about the medium. I found the book quite interesting and would highly recommend reading it! > They'd be boozing (more than they already are) if there wasn't such variety of cheap and available entertainment, the author doesn't seem to realize? That's an extremely pessimistic view of the world: Categorizing a set of human beings as \"stupid\" and saying that it doesn't matter how society is structured? And \"smart people\" are also influenced by how our society is structured, no? reply quartesixte 10 hours agorootparent>He's not making a point of \"television makes you dumb\" (or \"dumb people watches television\"), but rather he makes the distinction between an \"oral\"-, \"press\"- and \"television\"-based culture. He claims that it's bad when television becomes the main platform that a society centers its communication around. Or as Postman himself put it, \"the medium is the metaphor\". And he strongly disliked the metaphor TV was bringing to bear on the Western World. And everyone should note this is the television of the 1980s. You still don't really have home recording, there are a limited number of channels, and the monoculture truly exists. reply judofyr 10 hours agorootparent> And everyone should note this is the television of the 1980s. You still don't really have home recording, there are a limited number of channels, and the monoculture truly exists. This is a good point as well! When reading it I was reflecting on how internet compares to 1980s television. Yes, it has much more dopamine-fueled content, but it's way less of a monoculture. It gives a lot of opportunity for people to seek out what they're interested in and there's hundreds (thousands?) of communities with very different set of thoughts. reply quartesixte 9 hours agorootparentYou also can't create actively yourself! TV was definitely a consumer only culture, with all creation heavily gatekept by an entire industry. Compare this to the print culture prior to that. The Internet definitely has changed this, and now we are back into a creation capable metaphor. reply yungporko 9 hours agoparentprevplenty of smart people wasting their lives scrolling through bullshit. you don't use your brain to solve problems if you're never bored and allowing your mind to wander. reply bschmidt1 4 hours agoprevIt's amazing that solving death and aging is not Goal #1 of every rich and poor person on the planet. Death is coming for you and you're trying to get rich? Engaging in politics? Fighting? What's that gonna do when you're falling apart in real time? We're all dying fast. Medical industry can't stop it either, they don't know how. Nobody does. Yet nobody seems to notice. Nobody seems to care. reply wrkronmiller 3 hours agoparentEven if you could solve aging, you could never solve death. Probability and entropy will catch up with you eventually. I think that most people over a certain age are quite aware of their own mortality, and are looking to bring meaning to the time that they have. reply bschmidt1 3 hours agorootparent> eventually I'll take millions of years instead of 75 thanks > looking to bring meaning to the time that they have Everybody says something like this in response to this kind of question about death/aging - or they go all religious on me talking about Jesus etc. I'm like \"what is 2 + 2\" and half or more of the people go \"I like cake\" reply teamspirit 3 hours agoparentprevThat’s what bugs me about gen ai. How is it that all these resources are being used on recreating things that humans already do and not entirely focused on aging, health, and the climate? We already have artists, we don’t have a cure for what we’ve done to the climate. It’s frustrating. reply gessha 3 hours agoparentprevSome[1] do invest, others don’t. Personally, I see myself on the poor side, rather than the rich side and what I care about is having a good life however short it is. Family, friends and adventure. I don’t believe in the afterlife in any form and I wish I could live forever with my loved ones but I’ve also accepted it’s natural to die. Maybe one day we will overcome death and we will live until the heat death of the universe. Meh. [1] https://en.wikipedia.org/wiki/Calico_(company) reply bschmidt3 3 hours agorootparentInvest? No that aint it. I'm talking about the poignant reality that you, me, and everyone on this website will be gone in a matter of decades (or less). Half or more believe that upon death they will be instantly transported to a GOLDEN CITY (unless you're bad, then you go to FIRE CITY!) forever. \"Gold good... fire bad...\" yeah totally not made up guys sounds real. For everyone else it's MasterCard - distractions. reply cen4 11 hours agoprevThink more about Attention. Not about Information. Information is exploding and global available Attention doesn't grow. People who pay attention to one thing, can't use the same time to pay attention to something else. So govts and corps fight over this common pool of Attention using the Media (TV/Movies/Radio/Social/News/Sports/Gaming etc etc), just like they fought over land and oil and other natural resources. Media is literally used like front line troops of colonial empires in Attention capture wars. But no one wins as long as Global Human Attention isn't given purpose. We await someone or some group to articulate that vision. Until then people working in Attention Capture fields will keep amusing us to death. reply onion2k 11 hours agoparentBut no one wins as long as Global Human Attention isn't given purpose. The problem is that \"people working in Attention Capture fields\" are the exact people who are winning, at least by the most common scoring mechanism of 'wealth'. reply chongli 10 hours agorootparentThey're enriching their bank accounts just as they're impoverishing their spirits. On their deathbeds, no one ever says \"gee, I wish I'd spent more time at the office.\" The same could be said for any other wealth-motivated exercise. If I have learned one thing in life it is this: money is, at best, a necessary evil. A means to an end. Pursuing it as an end in itself is an indication that we have strayed from the path and forgotten what we were doing. reply strken 7 hours agorootparentI swear I am going to, on my deathbed, say \"gee, I wish I'd spent more time at the office,\" just to stop this quote going around. In the last five years I've had a few conversations about regret with elderly relatives who have now passed away. None of them regretted going to work. My grandparents met at work. One regretted that she'd been a draftswoman rather than an engineer, but that's almost the exact opposite. I don't understand why people think doing good work that inspires pride and then getting paid for it is going to be some kind of horrible deathbed regret. It has literally never been a deathbed regret for anyone whose deathbed I have attended. reply psychoslave 6 hours agorootparent> I don't understand why people think doing good work that inspires pride and then getting paid for it is going to be some kind of horrible deathbed regret. Because there is nothing in the definition you give that remotely looks like the median job. Societies are not structured to maximize the number of jobs that fits this definition. If social structure happens to fit your Ikigai, congratulation you won the cosmic loto, enjoy. But maybe it’s not a relevant point to show surprise on this point. Consider how much people in the rest of humanity will have to go through major existential stressful abhorrent challenges, geopolitical struggles, being effectively reduced to dull task slaves by whoever happen to be their lord of the day. How then to be surprised that at the end of their life they can think \"work moments were so shitty, I wish I had spend more enjoyable ones like those I experienced while taking time with people I deeply sincerely love\". By the way, if you haven’t yet do that today, tell at least to at least three people around you how much you love them and care that they enjoy moment passed together. I promise you won’t regret it on your death bed. ;) reply strken 5 hours agorootparentI'm talking about a wide range of jobs -- draftswoman, ship's engineer and structural engineer, typist, teacher -- held by people who grew up during the major existential struggles that were the great depression and the second world war. Working as a typist in the 1940s and 1950s was not some kind of utopian magical job full of meaning, but my grandmother could still hit a higher WPM than I can and she was proud of what she'd done. Earning an income was a means of doing things that she would never have been able to do otherwise. She felt lucky to be able to do things like take a ship to Europe without a chaperone and using money she'd earned, given that her mother's generation of women would have found it much harder. My experience has been that older people often have a different outlook on life than what people my age, including me, would predict. Part of that is experience, part is coming from a different time where the baseline for quality of life was lower, and I suspect part of it is rose-tinted glasses. reply amelius 6 hours agorootparentprevThey should have said: \"gee, I wish I spent more time influencing people into buying things they do not need\". reply onion2k 7 hours agorootparentprevOn their deathbeds, no one ever says \"gee, I wish I'd spent more time at the office.\" I bet a lot of people have died regretting they didn't earn more. reply Nevermark 8 hours agorootparentprevMoney has made it far easier to barter work into what people need to survive, obtain stability & enjoy life, than its absence. It’s easy to be jaded by those that obviously value it more than others’ well being. But the mismatch of priorities is what is wrong, that doesn’t negate money’s positive practical value & impact. Most people have a more multifaceted relationship with money than as a dehumanizing god or drug. reply CalRobert 8 hours agorootparentprevSure, but they’re also outbidding me for a house. reply apwell23 7 hours agorootparentAre you trying to get in a \"good school district\" ? Is that really that strongly correlated to kids outcome in life . kids grow up and fight for a house in good school district :). I feel like having kids is root of all evil. Ppl justify all sorts of things (like wars and bidding for houses) and say that they are doing those things for sake of their kids. reply CalRobert 45 minutes agorootparentNo, but I want to live somewhere my kids have the freedom to bike and walk around town safely. Anyway, I just took a job helping get more houses off gas and on heat pumps, which is aligned with my goal to be part of addressing climate change, but it is hard to get a house near work (in Amsterdam in this case) when not optimising for pay. reply 10u152 7 hours agorootparentprev>…having kids is the root of all evil. Quite a take there. Kids are also the source of all joy and happiness, depending on how you look at it. reply apwell23 7 hours agorootparentYea I agree. I have kids too :) But they are also the source of global warming, wars. On a personal level they a source of anxiety, continuous striving, jealousy and fear. reply ossobuco 7 hours agorootparentprev> I feel like having kids is root of all evil. Ppl justify all sorts of things and say that they are doing those things for sake of their kids. If that's true, then your/our existence is rooted in evil. Accordingly, if your parents were good people, they would have spared you of your existence, but apparently their very nature pushed them to bring more evil to the world. reply apwell23 7 hours agorootparentyes exactly! reply AStonesThrow 10 hours agorootparentprevYeah but there's negative attention, too. Negative attention will eventually have consequences. Either they grow deaf, run away, become enraged, etc. I think of the millions of ads, singers, bullies, salesmen who've vied for my attention, and you wear down. You get sick of saying \"no\", pretending not to notice, brushing aside dialogs, feeling bad because you can't help. https://www.musixmatch.com/lyrics/Laurie-Anderson/Language-I... reply HPsquared 11 hours agoparentprevI don't know, it's possible for a person to pay no attention to anything. Therefore it isn't always maxed out. Also the \"quality\" of attention can vary. I think the time spent looking at things has increased, but the level of focus and \"deep attention\" paid to things has likely fallen over time. reply sph 10 hours agoparentprev> But no one wins as long as Global Human Attention isn't given purpose. We await someone or some group to articulate that vision. One of the best things I've done for myself is to stop reading the news. You will not believe how this ignorance has led me to a calmer life, to the gasp and concern of my peers, wondering how am I able to cope, to exist, without knowing what happens \"in the world?\" As you say, anyone has 1 unit of attention, and unlike many other things, it is fully in our control. The biggest lie modern generations have been told is that the more knowledge about things, the greater the happiness. That you need to know what happens half a world away from you, often in more detail than what happens at your doorstep. What saddens me the most about the future generations is seeing how politicised they has become, politics the game of rich old people; the powers that be have figured out that if they turn what happens in the palace into entertainment, people are distracted and don't get into them silly ideas like trying to change things. These days politics is slapstick comedy for \"grown ups\", and it's sad to see it infect the younger generations now. reply frereubu 10 hours agorootparentYou might enjoy this piece by Charles Simic, which is a touchstone of mine: \"I’m having trouble deciding whether I understand the world better now that I’m in my seventies than I did when I was younger, or whether I’m becoming more and more clueless every day. The truth is somewhere in between, I suspect, but that doesn’t make me rest any easier at night. Like others growing old, I had expected that after everything I had lived through and learned in my life, I would attain a state of Olympian calm and would regard the news of the day with amusement, like a clip from a bad old movie I had seen far too many times. It hasn’t happened to me yet. My late father, in the final year of his life, claimed that he finally found that long-sought serenity by no longer reading the papers and watching television. Even then, and I was thirty years younger than he, I knew what he meant. What devotees of sadomasochism do to their bodies is nothing compared to the torments that those addicted to the news and political commentary inflict on their minds almost every hour of the day.\" https://archive.is/0GZmW (I haven't managed to stop reading the news unfortunately...) reply checkyoursudo 8 hours agorootparentThank you for that. I really enjoyed it. It resonates with me. I have lived in three different countries, and in my two non-native countries, I have enjoyed my life much more. I think part of it is that there I have been somewhat oblivious to the news and current politics of the new places I have lived. I get some of news and politics from my friends, but I do not follow it like I used to in my country of origin. I have also dramatically reduced my news consumption in my native country, because I am not there very often, and it does not preoccupy me so much anymore. Though I have not given up the news entirely, either in my home country or in my adopted countries, but less is, I feel, much better. I understand, fully and deeply, why news and current events are important, but they are also a cancer. At least in the way that they are sold to us. I also get a sense that the negative effect that the news has our mental health is quite widespread around the world. As in, it is not unique to America or Britain or European countries, etc. reply _gmax0 10 hours agorootparentprevIt's my opinion that \"thinking locally and acting locally\" is a strategy better reserved for old age. reply soulofmischief 9 hours agoparentprevThere is a war going on for your mind. https://www.youtube.com/watch?v=4Ne0DmiuHeg reply fallous 9 hours agoparentprevArguably religion used to provide that purpose but most of the Western world has walked away from it without choosing something to replace that sense of purpose. If, as Marx asserted, that religion was the opiate of the masses then the current \"attention economy\" is the methamphetamine of the masses. reply detourdog 9 hours agorootparentWhat I have noticed about religion is that today's view of it is distorted. I see it as closer to psychology/sociology/civics. The description I see used today about ancient ideas of social cohesion is narrow minded with a hint of superiority. reply portaouflop 11 hours agoparentprev> as long as Global Human Attention isn't given purpose We tried out the grand visions to improve the human condition with one great push in the 20th century- didn’t work out so well reply BriggyDwiggs42 11 hours agorootparentThe failure of some grand visions doesn’t doom all future ones. That’s just silly. reply portaouflop 10 hours agorootparentGrand vision (or ideology as it’s also called) is a dead end of history - has been tried too many times, always failed spectacularly. Instead we need small incremental lasting change - thinking we can transform life within a generation without repercussions, that’s just silly reply vlovich123 10 hours agorootparentprevEveryone knows that if at first you don’t succeed, never try again. reply Nasrudith 9 hours agorootparentprevGrand visions are more in service of megalomaniacal egos than actual solutions. They all just paint over the very real complexities of the world and expect things to just work as they envisioned. Just get rid of the sparrows eating grains and it will just be fine! There are limits to what complexities can be contained within one human mind, and with a world already orders of magnitudes more complex than that we need the humility to admit that the vision of one human mind is not and cannot be all-encompassing. I think it is fair to say that the usefulness of grand visions is dead. reply CalRobert 8 hours agorootparentprevThe Green Revolution, vaccines, and space exploration have been pretty great. reply throwaway2037 6 hours agorootparentLiteracy too! reply smokel 11 hours agoparentprevIt's not all about attention. Most companies are still in it for the money, and attention is only a means to an end. For the idiotic narcissist leaders that pop up every now and then, attention might be interesting by itself. But luckily for us, there's just very few of those. Most of our government bodies are comprised of people who actually mean to do good, and just a bit of attention to some important matters suffices. reply tropicalfruit 10 hours agoparentprevi would add laziness too. attention usually takes the path of least effort. reply navjack27 8 hours agorootparentChange attention to intention reply moffkalast 11 hours agoparentprevAttention is all you need? reply ianpenney 10 hours agorootparentThis is a very deep thought that has crossed my mind quite a lot as I’ve used LLMs and other AI. Ironically, we are discovering the human condition by evaluating what we are “not”. … but, we are. reply detourdog 9 hours agorootparentprevAttention is how we see ourselves reflected in others. reply anthk 11 hours agoprevBrave New World and 1984 are books to avoid every extreme on politics, either left or right (put every Monopoly neocon fanboy, racist non-civic nationalist or burocratic socialist in there). 1984 looked scary, but BNW was hopeless. It exerced a much better control. The world of 1984 collapsed down itself. reply 082349872349872 10 hours agoparentWhat's wrong with BNW? Have you forgotten the islands? reply JKCalhoun 6 hours agorootparentYeah, I have to re-read Brave New World because over the time since I have read it I have come to believe it was actually Utopian. The artists and others that could not conform were in fact given an island where like-minded artists could flourish. Sometimes I think that's all we all want: to find a community of like-minded people we can live among. reply anthk 10 hours agorootparentprevOn \"The Island\", well, it's the book Huxley wrote as a counterpart against BNW. reply 082349872349872 9 hours agorootparentThe islands in BNW: \"[Bernard is] being sent to an island. That's to say, he's being sent to a place where he'll meet the most interesting set of men and women to be found anywhere in the world. All the people who, for one reason or another, have got too self-consciously individual to fit into community-life. All the people who aren't satisfied with orthodoxy, who've got independent ideas of their own. Every one, in a word, who's any one. I almost envy you, Mr. Watson.\" reply ilrwbwrkhv 12 hours agoprevA person running for president of this country comes from show business and there are venture capitalists like Mark Andreeson who seriously talk about him as somebody who knows policy all because they can get a seat at the table. reply zabzonk 12 hours agoparentA person ELECTED for president of the USA came from show business - Reagan. reply gomerspiles 12 hours agorootparentA demented figurehead with other people behind him directing the show. It's as if acting was the perfect training for the worst idea for a position in a system of checks and balances. reply tuatoru 11 hours agorootparentnext [5 more] [flagged] gomerspiles 10 hours agorootparentBiden was never an entertainer like the other 2 demented old men (entrusted with WMDs) under discussion. If we send you to space you'll have to pass a fitness test.. Because we aren't stupid? But sure, anyone in the middle of a psychotic break who can't tell fact from fiction should be fine for the entirety of national interests. Shoot down another plane for the old Gipper! reply mdp2021 10 hours agorootparent> If we send you to space you'll have to pass a fitness test.. Because we aren't stupid? The real problem societies face is reaching a good fitness test for decision makers. That includes voters - discriminating, promoting, managing (etc.) the best electorate. And we had more focus and success in the past (abbeys, Venice etc.) than in the present, where the matter of electoral systems is kept like a theoretical branch of political science. And in running reality, people get Gerrymandering - an _opposite_ effort. reply gomerspiles 10 hours agorootparentThat's a problem, but I think the problem is checks and balances for actual repairs to the checks and balances that would restrict a role are prohibitely hard to make while privilege escalations enlarging a role are at best temporarily denied. Mr Trump was supposed to be picked up by a military tribunal and probably executed. Whether that tribunal system is overreaching would have been an excellent discussion, after the execution. reply bsenftner 9 hours agorootparentprevThat \"fitness test\" would be measuring a person's maturity, or better said \"lack of immaturity\". How to measure maturity might just be the most important measure ever devised by the human race, because it would enable US, the humans being led by our democracies, to finally demote the power hungry smooth talking immature, short sighted leaders. reply robotresearcher 11 hours agorootparentprevAs did Volodymyr Zelenskyy. reply hackable_sand 7 minutes agorootparentI don't think he can run for office in the US so this is false. reply bamboozled 12 hours agorootparentprevIf I remember correctly, Trump was also elected once, as stupid as that is. reply samllmas 11 hours agorootparentHe dabbled in show biz though. reply hshshshsvsv 11 hours agoparentprevCongrats. You brainwashed yourself into believing only certain class of people can run the country. Founding fathers would be proud. reply onion2k 11 hours agorootparentI think it's fair to say the \"why not inject yourself with bleach!\" people shouldn't be running the show while there's a class of people who do what people tell them without questioning whether it's a good idea. People in power have a responsibility not to suggest things that would kill people. That isn't a high bar. reply robotresearcher 2 hours agorootparentYou make a case against stupid and irresponsible people that I agree with completely. Entertainers are not necessarily these things. Just like lawyers - presidents are often lawyers - are not necessarily brilliant paragons. reply hshshshsvsv 8 hours agorootparentprevThis assumes people are stupid and have no intelligence of their own and needs to be told what they should be doing. reply onion2k 7 hours agorootparentNot quite. It assumes that some people don't verify what they're told, and so follow what people in authority tell them. That means people in authority have a responsibility not to abuse their authority. If everyone was rational and didn't do what they were told, choosing to verify everything and only follow what was appropriate, then the entire marketing, ad, government, legal, prison, etc industries would all collapse because they wouldn't be necessary any more. It's fairly obvious that there are people who follow dangerous, stupid advice. reply hshshshsvsv 7 hours agorootparentOkay. That's fair. reply JKCalhoun 6 hours agorootparentprevI'd prefer that the people elected to lead are smarter than I am. reply mrkeen 11 hours agorootparentprevThe founding fathers, not the founding parents. Those slave-owners held it self-evident that all men had the unalienable right of liberty (among other rights.) I can't remember 1984 well enough to remember any specific examples of doublethink, but they can't be as good as this one. reply hshshshsvsv 9 hours agorootparent> The founding fathers, not the founding parents. Sorry. Who among the founding fathers was women to call it parents? Founding fathers seems to be more accurate than founder Mothers or Founding parents. reply valval 12 hours agoparentprevAnd that should disqualify a presidential candidate… why exactly? reply mdp2021 10 hours agorootparentThe OP did not express the idea properly: people are not prejudicially disqualified because of the industries they worked in, but intrinsic disqualification comes from twisted profiles. \"Guts exciters\", getting followers through seducing their lower instances, is one of them. I am not labelling individuals here - but there are very many around the world fitting that description. reply valval 6 hours agorootparentMy question was rhetorical, since I knew the previous poster’s position was indefensible. Whatever point you’re trying to make is also hilarious to watch. In democracy it does not matter what instincts were provoked to get the votes. Sometimes the person you didn’t like wins, and that’s part of the deal. reply Dalewyn 11 hours agorootparentprevIt ain't democracy if you ain't won. -Vocal Minority, Intellectual Minority, Minorities et al. reply becquerel 12 hours agoprev [–] Hey guys, what if good things were actually bad? Wow!! Instead of enjoying ourselves we should instead spend eight hours a day intently studying woodworking & tax policy. The fact that people enjoy talking to each other and looking at cat pictures on social media proves that people will accept fascism and that Western liberal democracy is fated for impotence. reply theobreuerweil 11 hours agoparent [–] There is a middle ground between woodworking and TikTok, no? People enjoyed talking to each other and had fun before we had technology. It’s easy to see social media as harmless, and maybe it is, but it also has the potential to act as a powerful tool for serving propaganda and brainwashing. I’m not suggesting an actual conspiracy theory here but it’s concerning that a few huge companies have the power to broadcast (and control) the flow of information to a majority of population, who will consume that information by and large without suspicion. If for some reason Facebook or TikTok really wanted to meaningfully shift public opinion, they probably could, and in any direction they might choose. reply nonrandomstring 10 hours agorootparent> before we had technology There wasn't a time \"before we had technology\". Best to avoid that line of thinking if you want to escape the determinist (Veblem) trap and end up like Kaczynski. Postman is an author we enjoy but seldom acknowledge the wider genre into which he fits. It's called \"tech critique\". You can study it through the ages, comparing the outlooks and influences of Einstein, Ellul, Freud, Fromm, Heidegger, Illich, Kaczynski, Marcuse, Mumford, Nietzsche, and Postman, as well as sci-fi writers like Wells, Forster, Clarke, Gibson, Le Guin, Dick...It makes a very good companion to a study of the philosophy of science. Some takeaways (at least ones that stick in my mind): Technology is inseparable from the human condition, There are no primitivist escapes, noble savages or gardens of Walden. By the same token there is not and won't ever be any golden age of Utopian technology. Technology most closely resembles a \"drug\" in all its manifest functions. Technology comes with an accumulative maintenance cost. It is monotonic/directional. There's no easy way back and we can't uninvent stuff. Minimising the _harms_ of technology while maximising the benefits and maintaining human dignity amidst it is the best we can do. Even if initially excited by new developments all people are ultimately ambivalent about technology. They fear it, use it begrudgingly and resent their dependency on it. Iron bridges and steam locomotives raised the same questions as GPS and iPhones do today. Many people romanticise and worship technology. It is a secular God. If we \"love\" it, it's the sick love of an addict or the sadomasochistic power glee (tech \"dealers\" like Ellison, Zuck, and Musk) A tiny few (that's us) enjoy a curious fascination that makes tech an \"end in itself\". Those people get used to create a supply for the dealers and addicts. Anyway you gotta love Postman, if only for exquisite use of \"centrifugal bumblepuppy\". What he describes in this passage is really the soporific control/domination effects of technology in the hands of tyrants/dealers who delight in the subjugation of attention - which I think is made best by Alexis de Tocqueville in Democracy in America. reply Nasrudith 9 hours agorootparentprev [–] I disagree that influence is really that malleable. Even if we take the power of selection algorithms for granted it is still constrained and must work with the 'winds' of the content posted. If they tried something 'simple' as promoting non-mammalian meat sources they would only succeed in creating memes mocking the concept. Besides, the most \"effective\" influencer does next to nothing because they were going to do that sort of thing already. There is a reason you see music stars doing promotionals for pleasurable to consume caloried drinks as a use for personal funds and not say deferred gratification products like investment banking. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Talha Ghannam's article reviews Neil Postman's book \"Amusing Ourselves to Death,\" which contrasts the dystopian visions of George Orwell and Aldous Huxley.",
      "Postman argues that Huxley's vision, where society is subjugated through pleasure and distraction, is more relevant today, highlighting the impact of trivial entertainment and irrelevant information.",
      "The article underscores the importance of recognizing and addressing the modern tendency to be overwhelmed by inconsequential content."
    ],
    "commentSummary": [
      "\"Amusing Ourselves to Death\" by Neil Postman analyzes the societal impact of various media types, including books, newspapers, TV, and radio.",
      "Despite being written before the advent of social media, Postman's insights remain relevant, emphasizing that different media influence behaviors and society in distinct ways.",
      "The book underscores the importance of understanding how the medium itself shapes our actions and societal norms."
    ],
    "points": 300,
    "commentCount": 224,
    "retryCount": 0,
    "time": 1727502997
  },
  {
    "id": 41677131,
    "title": "Everything you need to know about Python 3.13 – JIT and GIL went up the hill",
    "originLink": "https://drew.silcock.dev/blog/everything-you-need-to-know-about-python-3-13/",
    "originBody": "On 2nd October 2024, the Python core developers and community will release CPython v3.13.0 – and it’s a doozy. (Update: release has now been pushed back to 7th October.) So what makes this release different, and why should you care about it? In short, there are two big changes being made to how Python runs at a core level which have the potential to radically change the performance profile of Python code in the future. Those changes are: A “free-threaded” version of CPython which allows you to disable the Global Interpreter Lock (GIL), and Support for experimental Just-in-Time (JIT) compilation. So what are these new features and what impact will they have on you? # Global Interpreter Lock (GIL) # What is the GIL? From the inception of the Python programming language by Guido Van Rossum in a science park in East Amsterdam in the late ’80s, it was designed and implemented as a single-threaded interpreted language. What exactly does this mean? You’ll commonly hear that there are 2 types of programming languages – interpreted and compiled. So which is Python? The answer is: yes. You will very rarely find a programming language which is purely interpreted from source by an interpreter. For interpreted languages, the human-readable source code is almost always compiled into some kind of intermediary form, called bytecode. The interpreter then looks at the bytecode and executes the instructions one-by-one. The “interpreter” here is commonly called a “virtual machine”, especially in other languages like Java which does the same thing as Python re. Java bytecode and Java VMs. In Java and friends, it’s much more common to ship the compiled bytecode itself, whereas Python applications are usually distributed as source code (although, having said that, packages are often deployed as wheels as well as sdist nowadays). Virtual machines in this meaning of the word come up in all kinds of unexpected places, like in the PostScript format (PDF files are essentially compiled PostScript) and in font rendering1. If you’ve ever noticed a bunch of *.pyc files in your Python projects, this is the compiled bytecode for your application. You can decompile and explore pyc files in exactly the same way you can find Java class files. 💡 Python vs CPython I can already hear a chorus of pedantic Pythonistas shouting “Python isn’t the same as CPython!”, and they’re right. And this is an important distinction to make. Python is the programming language, which is essentially a specification saying what the language should do. CPython is the reference implementation of this language specification, and what we’re talking about here is mostly about the CPython implementation. There are other Python implementations, like PyPy which has always used JIT compilation, Jython which runs on the JVM and IronPython which runs on the .NET CLR. Having said that, pretty much everyone just uses CPython and so I think it’s reasonable to talk about “Python” when we’re really talking about CPython. If you disagree, go ahead and get in the comments or write me a strongly worded email with an aggressive font (maybe Impact; I’ve always thought Comic Sans has a subtly threatening aura). So when we run Python, the python executable will generate the bytecode which is a stream of instructions, then the interpreter will read and execute the instructions one-by-one. If you try to spin up multiple threads, what happens then? Well, the threads all share the same memory (apart from their local variables) and so they can all access and update the same objects. Each thread will be executing its own bytecode using its own stack and instruction pointer. What happens if multiple threads try to access / edit the same object at the same time? Imagine one thread is trying to add to a dict while another is trying to read from it. There are two options here: Make the implementation of dict (and all the other objects) thread-safe, which takes a lot of effort and will make it slower for a single-threaded application, or Create a global mutual exclusion lock (a.k.a. mutex) which allows only one thread to be executing bytecode at any one time. This latter option is the GIL. The former option is what the Python developers are calling “free-threading” mode. It’s also worth mentioning that the GIL makes garbage collection much simpler and faster. We don’t have time to go into the depths of garbage collection here as it’s a whole big topic in itself, but a simplified version is that Python keeps a count of how many references there are to a particular object, and when that count reaches zero, Python knows that it can safely delete that object. If there are multiple threads concurrently creating and dropping references to different objects, this can lead to race conditions and memory corruptions, so any free-threaded version needs to use atomically counted references for all objects. The GIL also makes it much easier to develop C extensions for Python (e.g. using the confusingly named Cython) as you can make assumptions about thread safety that make your life much easier, check out the py-free-threading guide for porting C extensions for more details on this. # Why does Python have a GIL? Despite having a surge in popularity over the last few years, it’s not a particularly new language – it was conceived in the late ’80s, with the first release on 20th February 1991 (making it slightly older than me). Back then, computers looked very different. Most programs were single-threaded and the performance of individual cores was increasing exponentially (see good old Moore’s Law). In this environment, it didn’t make much sense to compromise single-threaded performance for thread safety when most programs would not be utilising multiple cores. Also, implementing thread safety obviously takes a lot of work. This isn’t to say that you can’t utilise multiple cores in Python. It just means that instead of using threading, you have to utilise multiple processes (i.e. multiprocessing module). Multi-processing differs from multi-threading because each process is its own Python interpreter with its own separate memory space. This means that multiple processes can’t access the same objects in memory but instead you have to use special constructs and communication to share data (see “Sharing state between processes” and multiprocessing.Queue). It’s worth mentioning that there is a bit more overhead in using multiple processes as opposed to multiple threads, in addition to it being more difficult to share data. Using multiple threads is sometimes not as bad as people commonly assume, however. If Python is doing I/O like reading from files or making network calls, it will release the GIL so that other threads can run. This means that if you’re doing lots of I/O, multi-threading will often be as fast as multi-processing. It’s when you are CPU-bound that the GIL becomes a big issue. # Ok, so why are they removing the GIL now? The removal of the GIL has been something that certain people have been pushing for for several years now, but the main reason it’s not been done is not the amount of work it takes but instead is the corresponding performance degradation that would come with it for single-threaded programs. Nowadays, the incremental improvements in single-core performance of computers doesn’t change too much from year to year (although big advances are being made with custom processor architectures, e.g. Apple Silicon chips) while the number of cores in a computer continues to increase. This means it’s much more common for programs to utilise multiple cores and hence the inability of Python to properly utilise multi-threading is becoming more and more of an issue. Fast forward to 2021 and Sam Gross implemented a no-GIL Proof of Concept implementation that spurred the Python Steering Council to propose a vote on PEP 703 – Making the Global Interpreter Lock Optional in CPython. The outcome of the vote was positive, resulting in the Steering Council accepting the proposal as part of a gradual rollout in three phases: Phase 1: Free-threading mode is an experimental build-time option that is not the default. Phase 2: Free-threading mode is officially supported but still not the default. Phase 3: Free-threading mode is the default. From reading the discussions, there’s a strong desire to not “split” Python into two separate implementations – one with the GIL and one without – with the intention being that eventually after free-threading mode has been the default for a while, the GIL will be removed entirely and the free-threading mode will be the only mode. While all this GIL vs. no-GIL stuff has been going on the last few years, there has been a parallel effort called the “Faster CPython” project. This has been funded by Microsoft and led by Mark Shannon and Guido van Rossum himself, both of whom work at Microsoft. The effort this team have been making has produced some very impressive results, particularly for 3.11 which boasted significant performance boosts over 3.10. With the combination of community / council support, increasing ubiquity of multi-core processors and the Faster CPython effort, the time was ripe for the beginning of Phase 1 of the GIL adoption plan. # What does the performance look like? I’ve run a few benchmarks on both my machine – MacBook Pro with Apple M3 Pro (CPU has 6 performance cores and 6 efficiency cores) – and on a quiet EC2 instance – t3.2xlarge (8 vCPUs). The graphs below show a comparison of the runtime performance of a CPU-intensive task (converging Mandelbrot set) between Python 3.12 and Python 3.13 with and without the GIL. (These graphs aren’t the most readable, I know – I’ll improve on them when I get some time.) To explain what these runtimes mean: 3.12.6 – Python version 3.12.6. 3.13.0rc2 – the default build of Python 3.13.0 release candidate 2 (the latest version at the time of writing). 3.13.0rc2t – the Python 3.13.0 release candidate 2 with experimental free-threading enabled at build-time, run without additional arguments (i.e. GIL disabled). 3.13.0rc2t-g1 – the Python 3.13.0 release candidate 2 with experimental free-threading enabled at build-time, run with the -X gil=1 argument, thereby “re-enabling” the GIL at runtime. A few caveats to this: I didn’t use a proper well established benchmark, just a simple iterative algorithm. You can find the code for running the benchmarks and graphing the results at: github.com/drewsilcock/gil-perf. Try it out for yourself! I used hyperfine to run the benchmarks, which is a really good tool, but these aren’t proper scientific benchmarks running on dedicated hardware. My MacBook is running a whole bunch of things and even the EC2 instance will have stuff going on in the background, although not nearly as much. These benchmarks are really interesting and fun to talk about, but do bear in mind that in the real world, most libraries that do CPU-intensive work use Cython or similar – very few people use raw Python for very compute-intensive tasks. Cython has the ability to release the GIL temporarily during execution and has had for a while. These benchmarks aren’t representative of this use case. With that in mind, we can already make a few observations: The performance degradation when Python is built with free-threading support is significant – around 20%. It doesn’t matter whether you re-enable the GIL via the -X gil=1 argument, the performance degradation is the same. Multi-threading shows a significant boost with GIL disabled, as expected. Multi-threading with GIL enabled is slower than single-threading, as expected. Multi-threading with GIL disabled is about the same as multi-processing. Then again, this is a pretty noddy example where you don’t need to do much real work. Apple Silicon chips really are quite impressive. Single-threaded performance on my M3 Pro is about 4x faster than single-threaded performance on the t3.2xlarge. I mean, I know t3 are designed to be cheap and burstable, but even so! It’s even more impressive if you consider the insane battery life you get out of these things2. # How do I try out free-threaded Python? At the time of writing, Python 3.13 is still in release candidate and hasn’t been officially released. Having said that, today is Saturday 28th September and the release is scheduled for 2nd 7th October which is Wednesday a week on Thursday, so its not far away. (Update: updated release date to reflect pushed back release schedule.) If you want to try it out ahead of time, you’re out of luck with rye which only seems to ship deployed versions and uv which includes the 3.13.0rc2 build but not the 3.13.0rc2t build. Luckily, pyenv supports both 3.13.0rc2 and 3.13.0rc2t. To try it out for yourself: Terminal window 1 # If you're reading this from the future, rye may have it: 2 $ rye toolchain list --include-downloadablerg -F cpython@3.13 3 4 # uv may also have it 5 $ uv python listrg -F cpython-3.13 6 7 # pyenv should have it, though. 8 $ pyenv install --listrg '^\\s+3\\.13' 9 10 # Take 3.13.0rc2t for a spin 11 $ pyenv install 3.13.0rc2t 12 $ pyenv local 3.13.0rc2t 13 14 $ python -VV 15 Python 3.13.0rc2 experimental free-threading build (main, Sep 18 2024, 16:41:38) [Clang 15.0.0 (clang-1500.3.9.4)] 16 17 $ python -c 'import sys;print(\"GIL enabled 🔒\" if sys._is_gil_enabled() else \"GIL disabled 😎\")' 18 GIL disabled 😎 19 20 # GIL can be re-enabled at runtime 21 $ python -X gil=1 -c 'import sys;print(\"GIL enabled 🔒\" if sys._is_gil_enabled() else \"GIL disabled 😎\")' 22 GIL enabled 🔒 Just a heads up if you are trying free-threading Python – if you don’t specify either -X gil=0 or -X gil=1, the GIL will be disabled by default but simply importing a module which does not support running without the GIL will cause the GIL to be re-enabled. I found this when running the benchmarks because I imported matplotlib, which results in the GIL being re-enabled and all my benchmarks coming out rubbish. If you manually specify -X gil=0, the GIL will not be sneakily re-enabled, even if a package does not mark itself as supporting GIL-free running. # JIT (Just-in-Time) Compiler It’s not just the GIL that’s a big change in this Python release – there’s also the addition into the Python interpreter of an experimental JIT compiler. # What is a JIT? JIT stands for Just in Time and is a compilation technique where machine code is produced just in time to execute it, as opposed to ahead of time (AOT) like your traditional C compiler like gcc or clang. We already talked about bytecode and the interpreter earlier. The important thing is that, before Python 3.13, the interpreter would look at each bytecode instruction one at a time and turn each one into native machine code before executing it. With the introduction of the JIT compiler, it is now possible for bytecode to be “interpreted” into machine code once and updated as necessary, instead of being re-interpreted every time. It’s important to point out that this kind of JIT that has been introduced in 3.13 is what’s called “copy-and-patch” JIT. This is a very recent idea introduced in 2021 in an article called “Copy-and-patch compilation: a fast compilation algorithm for high-level languages and bytecode. The core idea behind copy-and-patch as opposed to more advanced JIT compilers is that there is a simple list of pre-generated templates – the JIT compiler will pattern match for bytecode that matches one of the pre-defined templates and if it does, it will patch in pre-generated native machine code. Traditional JIT compilers will be massively more advanced that this and also massively more memory intensive, especially if you compare it to heavily JIT-compiled languages like Java or C#. (That’s part of the reason Java programs take up so much memory.) What’s great about JIT compilers is that they can adapt to your code as its running. For instance, as your code runs, the JIT compiler will keep track of how “hot” each piece of code is. JIT compilers can perform incremental optimisations as the code get hotter and hotter and even use information about how the program is actually running to inform the optimisations it is making (like how Profile-Guided Optimisation does for AOT compilers). This means that JIT doesn’t waste time optimising some code which is only running once but the really hot sections of code can have heavy run-time informed optimisations done on them. Now, the JIT compiler in Python 3.13 is relatively simple and won’t be doing any crazy at this stage, but it’s a really exciting development for the future of Python performance. # What difference will the JIT make to me? In the short term, the introduction of the JIT is unlikely to make any difference to how you write or run your Python code. But it’s an exciting internal change to the way that the Python interpreter operates that could lead to much more significant performance improvements being made to Python performance in the future. In particular, it opens up the way for incremental performance improvements to be made over time which could gradually bump up Python’s performance to be more competitive with other languages. Having said that, this is still early stages and the copy-and-patch JIT technique is both new and lightweight, so there’s more big changes needed before we start seeing significant benefits from the JIT compiler. # How do I try out the JIT? The JIT compilers is “experimental” in 3.13 and isn’t built with support out of the box (at least not when I downloaded 3.13.0rc2 using pyenv). You can enable experimental JIT support by doing: Terminal window 1 $ PYTHON_CONFIGURE_OPTS=\"--enable-experimental-jit\" pyenv install 3.13-dev 2 python-build: use openssl@3 from homebrew 3 python-build: use readline from homebrew 4 Cloning https://github.com/python/cpython... 5 Installing Python-3.13-dev... 6 python-build: use tcl-tk from homebrew 7 python-build: use readline from homebrew 8 python-build: use ncurses from homebrew 9 python-build: use zlib from xcode sdk 10 Installed Python-3.13-dev to /Users/drew.silcock/.pyenv/versions/3.13-dev 11 $ python -c 'import sysconfig;print(\"JIT enabled 🚀\" if \"-D_Py_JIT\" in sysconfig.get_config_var(\"PY_CORE_CFLAGS\") else \"JIT disabled 😒\")' 12 JIT enabled 🚀 There are additional configure options which you can read about on the PEP 744 discussion page (like enabling the JIT but requiring it be enabled by running -X jit=1 at runtime, etc.). The test here only checks for whether the JIT was enabled at built-time, not whether it’s currently running (e.g. has been disabled at runtime). It is possible to check at runtime whether the JIT is enabled, but it’s a bit more tricky. Here’s a script you can use to figure it out (taken from the PEP 744 discussion page)3: 1 import _opcode 2 import types 3 4 5 def is_jitted(f: types.FunctionType) -> bool: 6 for i in range(0, len(f.__code__.co_code), 2): 7 try: 8 _opcode.get_executor(f.__code__, i) 9 except RuntimeError: 10 # This isn't a JIT build: 11 return False 12 except ValueError: 13 # No executor found: 14 continue 15 return True 16 return False 17 18 19 def fibonacci(n): 20 a, b = 0, 1 21 for _ in range(n): 22 a, b = b, a + b 23 return a 24 25 26 def main(): 27 fibonacci(100) 28 if is_jitted(fibonacci): 29 print(\"JIT enabled 🚀\") 30 else: 31 print(\"Doesn't look like the JIT is enabled 🥱\") 32 33 34 35 if __name__ == \"__main__\": 36 main() The PEP 744 discussion has mention of both PYTHON_JIT=0/1 and -X jit=0/1 – I did not find that the -X option did anything at all, but the environment variable seems to do the trick. Terminal window 1 $ python is-jit.py 2 JIT enabled 🚀 3 $ PYTHON_JIT=0 python is-jit.py 4 Doesn't look like the JIT is enabled 🥱 # Conclusion Python 3.13 is a big release in introducing some exciting new concepts and features to the runtime. It’s unlikely to make any immediate different to how you write and run your Python, but it’s likely that over the next few months and years as both free-threading and JIT become more mature and well established, they’ll begin to have more and more of an impact on the performance profile of Python code, particularly for CPU-bound tasks. # Further reading PEP 703 – Making the Global Interpreter Lock Optional in CPython py-free-threading Python 3.13 gets a JIT – Anthony Shaw PEP 744 – JIT Compilation Discuss – PEP 744: JIT Compilation # Updates 2024-09-28: Updated release date for v3.13.0 from 2nd October to 7th October. Thanks to nmstoker on HN for pointing this out. # Footnotes Font rendering is a fascinating topic and that immensely complex. Trust me, however complicated you think font rendering is, it’s more complicated that that. IIRC most of the complexity actually comes from nicely drawing text at small resolutions. For instance, in TrueType both a whole font and individual glyphs have instructions associated with them which are executed by the FontEngine virtual machine a.k.a. interpreter. If this is something you’re interested in learning more about, I highly recommend Sebastian Lague’s video – Coding Adventure: Rendering Text. He makes really great videos. The TrueType reference is also surprisingly readable. ↩ Apple aren’t even paying me to say this stuff, it’s just true. ↩ I also found a few people online talking about how you could use sysconfig.get_config_var(\"JIT_DEPS\") but I did not found that this worked at all for me. ↩ How Postgres stores oversized values – let's raise a TOAST",
    "commentLink": "https://news.ycombinator.com/item?id=41677131",
    "commentBody": "Everything you need to know about Python 3.13 – JIT and GIL went up the hill (silcock.dev)285 points by chmaynard 17 hours agohidepastfavorite148 comments pjmlp 12 hours agoI really find a bummer that there is such a resistance to make JIT enable versions available for download, alongside the other variants. Naturally I can easily compile my own Python 3.13 version, no biggie. However from my experience, this makes many people that could potentially try it out and give feedback, don't care and rather wait. reply Ralfp 9 hours agoparentJIT is in weird place now because according to a recent talk from PyCON US by it’s author, the tier 2 optimizer that prepares code for JIT reduces performance by 20% and JIT just recovers that performance loss. A lot of langugage is still not optimized by tier 2 [1] and even less has copy and patch templates for JIT. And JIT itself currently has some memory management issues to iron out. [1]: https://github.com/python/cpython/issues/118093 Talk by Brandt Butcher was there but it was made private: https://www.youtube.com/watch?v=wr0fVU3Ajwc reply pjmlp 9 hours agorootparentI have seen some stuff about that, however that is to be expected. Many of these kind of changes take time, and require multiple interactions. See Go or .NET tooling bootstraping, all the years that took MaximeVM to evolve into GraalVM, Swift evolution versus Objective-C, Java/Kotlin AOT evolution story on Android, and so on. If only people that really deeply care get to compile from source to try out the JIT and give feedback, it will have even less people trying it out than those that bother with PyPy. reply targafarian 5 hours agorootparentOn the other hand, if people who don't care enough to compile it for themselves try it out, the Python devs can be flooded with complaints and bug reports that effectively come down to it being in alpha. You get both sides (yes, you might limit some who would otherwise try it out). I think requiring people to compile to try out such a still-fraught, alpha-level feature isn't too onerous. (And that's only from official sources; third parties can offer compiled versions to their hearts' content!) reply sevensor 6 hours agorootparentprev> those that bother with PyPy Which itself needed to be compiled from source the first time I tried it. All the hours of Mandelbrot were worth the spectacular speedup. reply pjmlp 5 hours agorootparentWhich was a thing several years ago, not in 2024. As mentioned I don't have any issues compiling it myself, more of an adoption kind of remark. reply mattip 4 hours agorootparentprevThere are portable downloads of PyPy here https://pypy.org/download.html reply the__alchemist 2 hours agoparentprevRelated: It's a mystery to me why Python doesn't provide interpreter binaries for linux. (They do for Windows) reply woodruffw 2 hours agorootparentLinux isn’t one target, but a matrix of targets: providing binaries for Linux means picking architectures, libc versions and variants, OpenSSL versions and forks, etc. against which to canonicalize. This also has downstream implications, e.g. a CPython binary with a static build of OpenSSL might contain an OpenSSL vulnerability that CPython is now on the hook for remediating (rather than delegating that remediation responsibility to downstream distributions). Some of this complexity is also true for Windows, but Linux’s (good!) diversity makes it a bigger challenge. reply the__alchemist 54 minutes agorootparentValid, but you can make three binaries that will work for the most common use cases. (I've done this) For a project as big as this, doing so is trivial, vice offloading it onto the users. reply amelius 42 minutes agorootparentprevWhy libc can't converge to a long-term stable version is another mystery. reply kiney 2 hours agorootparentprevwhy would they? On Linux yoi typically use thr distribution provided binary or if you really need something newer a container reply Joeboy 30 minutes agorootparentOn Linux you typically clone a project that specifies a specific python version (or range of versions), so you install the right version somewhere and set up your virtualenv to use it. At least in my world. reply the__alchemist 52 minutes agorootparentprevUsing distro linux for non-OS code is IMO a pain. Risk to break your OS, and the distro Py may be old. reply sitkack 2 hours agoparentprevThey are saving the JIT on release for 3.14! Sssshhhhhh reply theandrewbailey 49 minutes agorootparentPi-thon reply pzo 11 hours agoparentprevnot sure if there will distributed in homebrew et al but at least \"Pre-built binaries marked as free-threaded can be installed as part of the official Windows and macOS installers\" [0] [0] https://docs.python.org/3.13/whatsnew/3.13.html#free-threade... reply pjmlp 11 hours agorootparentYes, that is my point, GIL free is available, but not wit JIT enabled. reply nerdponx 11 hours agorootparentprevHopefully Macports will decide to offer it. reply jononor 7 hours agoparentprevIt will happen in one of the later releases. They might not be ready for widespread testing from people who are not willing to build from source, yet. reply MrThoughtful 13 hours agoprevRemoving the GIL sounds like it will make typical Python programs slower and will introduce a lot of complexity? What is the real world benefit we will get in return? In the rare case where I need to max out more than one CPU core, I usually implement that by having the OS run multiple instances of my program and put a bit of parallelization logic into the program itself. Like in the mandelbrot example the author gives, I would simply tell each instance of the program which part of the image it will calculate. reply inoop 3 hours agoparentAs always, it depends a lot on what you're doing, and a lot of people are using Python for AI. One of the drawbacks of multi-processing versus multi-threading is that you cannot share memory (easily, cheaply) between processes. During model training, and even during inference, this becomes a problem. For example, imagine a high volume, low latency, synchronous computer vision inference service. If you're handling each request in a different process, then you're going to have to jump through a bunch of hoops to make this performant. For example, you'll need to use shared memory to move data around, because images are large, and sockets are slow. Another issue is that each process will need a different copy of the model in GPU memory, which is a problem in a world where GPU memory is at a premium. You could of course have a single process for the GPU processing part of your model, and then automatically batch inputs into this process, etc. etc. (and people do) but all this is just to work around the lack of proper threading support in Python. By the way, if anyone is struggling with these challenges today, I recommend taking a peek at nvidia's Triton inference server (https://github.com/triton-inference-server/server), which handles a lot of these details for you. It supports things like zero-copy sharing of tensors between parts of your model running in different processes/threads and does auto-batching between requests as well. Especially auto-batching gave us big throughput increase with a minor latency penalty! reply jgraettinger1 7 minutes agorootparent> For example, imagine a high volume, low latency, synchronous computer vision inference service. I'm not in this space and this is probably too simplistic, but I would think pairing asyncio to do all IO (reading / decoding requests and preparing them for inference) coupled with asyncio.to_thread'd calls to do_inference_in_C_with_the_GIL_released(my_prepared_request), would get you nearly all of the performance benefit using current Python. reply saagarjha 31 minutes agorootparentprevMachine learning people not call their thing Triton challenge (IMPOSSIBLE) reply tonygrue 13 hours agoparentprevThere is an argument that if you need in process multithreading you should use a different language. But a lot of people need to use python because everything else they’re doing is in python. There are quite a few common cases where in process multi threading is useful. The main ones are where you have large inputs or large outputs to the work units. In process is nice because you can move the input or output state to the work units instead of having to copy it. One very common case is almost all gui applications. Where you want to be able to do all work on background threads and just move data back and forth from the coordinating ui thread. JavaScript’s lack of support here, outside of a native language compiled into emscripten, is one reason web apps are so hard to make jankless. The copies of data across web workers or python processes are quite expensive as far as things go. Once a week or so, I run into a high compute python scenario where the existing forms of multiprocessing fail me. Large shared inputs and or don’t want the multiprocess overhead; but GIL slows everything down. reply vlovich123 12 hours agorootparent> Where you want to be able to do all work on background threads and just move data back and forth from the coordinating ui thread. JavaScript’s lack of support here, outside of a native language compiled into emscripten, is one reason web apps are so hard to make jankless I thought transferring array buffers through web workers didn’t involve any copies of you actually transferred ownership: worker.postMessage(view.buffer, [view.buffer]); I can understand that web workers might be more annoying to orchestrate than native threads and the like but I’m not sure that it lacks the primitives to make it possible. More likely it’s really hard to have a pauseless GC for JS (Python predominantly relies on reference counting and uses gc just to catch cycles). reply Etheryte 11 hours agorootparentThis is true, but when do you really work with array buffers in Javascript? The default choice for whatever it is that you're doing is almost always something else, save for a few edge cases, and then you're stuck trying to bend your business logic to a different data type. reply vlovich123 10 hours agorootparentThat’s a choice you get to make and probably depends on your problem domain and other things. For example when I was writing R2 it was all ArrayBuffers up and down the stack. And you could use something like capnproto or flat buffers for managing your object graph within an array buffer. But yeah, being able to transfer custom object graphs would be more powerful. reply tombl 7 hours agorootparentIs this some internal cloudflare feature flag or can everybody pass ArrayBuffers zero-copy via service bindings? (random question, totally understand if you're not the right person to ask) reply dwattttt 5 hours agorootparentIt's not a cloudflare thing: https://developer.mozilla.org/en-US/docs/Web/API/Worker/post... reply formerly_proven 9 hours agorootparentprevThere is this assumption in these discussions that anything consuming significant CPU must necessarily have a simple interface that’s easy to reduce to a C-level ABI, like calling an ML library on an image, a numpy function on two big matrices or some encryption function. Therefore it is trivial to just move these to native code with an easy, narrow interface. This assumption is incorrect. There are plenty of problems that consist entirely of business logic manipulating large and complex object graphs. “Just rewrite the hot function in rust, bro” and “just import multiprocessing, bro” are functionally identical to rewriting most of the application for these. The performance work of the last few years, free threading and JIT are very valuable for these. All the rest is already written in C. reply wruza 7 hours agorootparentIt's a good assumption though, because it keeps (in this case kept) closed the door to the absolutely nightmarish landscape of \"multithreading to the masses\". Those who made it open probably see it better, but, imo and ime, it should remain closed. Maybe they'll manage to handle it this time, but I'm 95% sure it's gonna be yet another round of ass pain for the world of python. reply pansa2 6 hours agorootparentprev> “Just rewrite the hot function in rust, bro” and “just import multiprocessing, bro” are functionally identical to rewriting most of the application for these. Isn't \"just use threads, bro\" likely to be equally difficult? reply bdd8f1df777b 6 hours agoparentprevThe biggest use case (that I am aware of) of GIL-less Python is for parallel feeding data into ML model training. * PyTorch currently uses `multiprocessing` for that, but it is fraught with bugs and with less than ideal performance, which is sorely needed for ML training (it can starve the GPU). * Tensorflow just discards Python for data loading. Its data loaders are actually in C++ so it has no performance problems. But it is so inflexible that it is always painful for me to load data in TF. Given how hot ML is, and how Python is currently the major language for ML, it makes sense for them to optimize for this. reply lifthrasiir 12 hours agoparentprev> Removing the GIL sounds like it will make typical Python programs slower and will introduce a lot of complexity? This was the original reason for CPython to retain GIL for very long time, and probably true for most of that time. That's why the eventual GIL removal had to be paired with other important performance improvements like JIT, which was only implemented after some feasible paths were found and explicitly funded by a big sponsor. reply klranh 6 hours agorootparentThat is the official story. None of it has materialized so far. reply simonw 13 hours agoparentprevMy hunch is that in just a few years time single core computers will be almost extinct. Removing the GIL now feels to me like good strategic preparation for the near future. reply naming_the_user 13 hours agorootparentIt depends what you mean by extinct. I can't think of any actual computer outside of embedded that has been single core for at least a decade. The Core Duo and Athlon X2 were released almost 20 years ago now and within a few years basically everything was multicore. (When did we get old?) If you mean that single core workloads will be extinct, well, that's a harder sell. reply simonw 13 hours agorootparentYeah, I just checked and even a RaspberryPi has four cores these days. So I guess they went extinct a long time ago! reply poincaredisk 9 hours agorootparentYes, but: * Most of the programs I write are not (trivially) parallelizable, and a the bottleneck is still a single core performance * There is more than one process at any time, especially on servers. Other cores are also busy and have their own work to do. reply deadbunny 25 minutes agorootparentYes, but: 1. Other people with different needs exist. 2. That's why we have schedulers. reply formerly_proven 9 hours agorootparentprevEven many microcontrollers have multiple cores nowadays. It’s not the norm just yet, though. reply masklinn 3 hours agorootparentprev> My hunch is that in just a few years time single core computers will be almost extinct. Single core computers are already functionally extinct, but single-threaded programs are not. reply im3w1l 13 hours agorootparentprevSingle core computers yes. Single core containers though.. reply seabrookmx 3 hours agorootparentIt's actually quite difficult to get a \"single core\" container (ie: a container with access to only one logical processor). When you set \"request: 1\" in Kubernetes or another container manager, you're saying \"give me 1 CPU worth of CPU time\" but if the underlying Linux host has 16 logical cores your container will still see them. Your container is free to use 1/16th of each of them, 100% of one of them, or anything in-between. You might think this doesn't matter in the end but it can if you have a lot of workloads on that node and those cores are busy. Your single threaded throughout can become quite compromised as a result. reply gomerspiles 12 hours agorootparentprevSingle core containers are also a terrible idea. Life got much less deadlocked as soon as there were 2+ processors everywhere. (Huh, people like hard OS design problems for marginal behavior? OSes had trouble adopting SMP but we also got to jettison a lot of deadlock discussions as soon as there was CPU 2. It only takes a few people not prioritizing 1 CPU testing at any layer to make your 1 CPU container much worse than a 2 VCPU container limited to a 1 CPU average.) reply pansa2 12 hours agoparentprev> What is the real world benefit we will get in return? If you have many CPU cores and an embarrassingly parallel algorithm, multi-threaded Python can now approach the performance of a single-threaded compiled language. reply Certhas 11 hours agorootparentThe question really is if one couldn't make multiprocess better instead of multithreaded. I did a ton of MPI work with python ten years ago already. What's more I am now seeing in Julia that multithreading doesn't scale to larger core counts (like 128) due to the garbage collector. I had to revert to multithreaded again. reply Sakos 10 hours agorootparentI assume you meant you had to revert to multiprocess? reply Certhas 4 hours agorootparentYes exactly. Thanks. reply 0x000xca0xfe 6 hours agorootparentprevYou could already easily parallelize with the multiprocessing module. The real difference is the lower communication overhead between threads vs. processes thanks to a shared address space. reply bmitc 4 hours agorootparentEasily is an overstatement. Multiprocessing is fraught with quirks. reply 0x000xca0xfe 3 hours agorootparentWell I once had an analytics/statistics tool that regularly chewed through a couple GBs of CSV files. After enough features had been added it took almost 5 minutes per run which got really annoying. It took me less than an hour to add multiprocessing to analyze each file in its own process and merge the results together at the end. The runtime dropped to a couple seconds on my 24 thread machine. It really was much easier than expected. Rewriting it in C++ would have probably taken a week. reply bmitc 15 minutes agorootparentIn F#, it would just be let results = files |> Array.Parralel.map processFile Literally that easy. Earlier this week, I used a ProcessPoolExecutor to run some things in their own process. I needed a bare minimum of synchronization, so I needed a queue. Well, multiprocessing has its own queue. But that queue is not joinable. So I chose the multiprocessing JoinableQueue. Well, it turns out that that queue can't be used across processes. For that, you need to get a queue from the launching process' manager. That Queue is the regular Python queue. It is a gigantic mess. And yes, asyncio also has its own queue class. So in Python, you literally have a half a dozen or so queue classes that are all incompatible, have different interfaces, and have different limitations that are rarely documented. That's just one highlight of the mess between threading, asyncio, and multiprocessing. reply sitkack 2 hours agorootparentprevFraught with quirks sounds quite ominous. Quuuiiirkkksss. I agree though. reply bmitc 4 hours agorootparentprevThat's not really correct. Python is by far the slowest mainstream language. It is embarrassingly slow. Further more, several mainstream compiled languages are already multicore compatible and have been for decades. So comparing against a single-threaded language or program doesn't make sense. All this really means is that Python catches up on decades old language design. However, it simply adds yet another design input. Python's threading, multiprocessing, and asyncio paradigms were all developed to get around the limitations of Python's performance issues and the lack of support for multicore. So my question is, how does this change affect the decision tree for selecting which paradigm(s) to use? reply masklinn 3 hours agorootparent> Python's threading, multiprocessing, and asyncio paradigms were all developed to get around the limitations of Python's performance issues and the lack of support for multicore. Threading is literally just Python's multithreading support, using standard OS threads, and async exists for the same reason it exists in a bunch of languages without even a GIL: OS threads have overhead, multiplexing IO-bound work over OS threads is useful. Only multiprocessing can be construed as having been developed to get around the GIL. reply bmitc 1 minute agorootparentNo, asyncio's implementation exists because threading in Python has huge overhead for switching between threads and because threads don't use more than one core. So asyncio was introduced as a single threaded solution specifically for only network-based IO. In any other language, async is implemented on top of the threading model, both because the threading model is more efficient than Python's and because it actually supports multiple cores. Multiprocessing isn't needed in other languages because, again, their threading models support multiple cores. So the three, relatively incompatible paradigms of asyncio, threading, and multiprocessing specifically in Python are indeed separate attempts to account for Python's poor design. Other languages do not have this embedded complexity. Zyten 12 hours agoparentprevWhat you’re describing is basically using MPI in some way, shape or form. This works, but also can introduce a lot of complexity. If your program doesn’t need to communicate, then it’s easy. But that’s not the case for all programs. Especially once we’re talking about simulations and other applications running on HPC systems. Sometimes it’s also easier to split work using multiple threads. Other programming languages let you do that and actually use multiple threads efficiently. In Python, the benefit was just too limited due to the GIL. reply carapace 3 hours agoparentprev> What is the real world benefit we will get in return? None. I've been using Python \"in anger\" for twenty years and the GIL has been a problem zero times. It seems to me that removing the GIL will only make for more difficulty in debugging. reply cma 13 hours agoparentprevThere will be consumer chips with 64 cores before long reply bilsbie 4 hours agoprevThis sounds silly but I’ve actually turned off garbage collection in short running, small memory programs and gotten a big speed boost. I wonder if that’s something they could automate? I’m sure there are some weird risks with that. Maybe a small program ends up eating all your memory in some edge case? reply Arch-TK 4 hours agoparentReliable implementation most likely involves solving the halting problem. reply jordan_bonecut 4 hours agorootparentDisagree, there are practical trivial subsets of the halting problem to which I imagine many short-running scripts would conform. reply Arch-TK 43 minutes agorootparentMost short running scripts call into the standard library or into other packages. I think the trivial subset is too small to bother with such an optimisation. reply tln 4 hours agoparentprevTurn on GC after the first sbrk reply theossuary 4 hours agoparentprevYou'd run into the halting problem. Maybe for some small subset of programs it'd be possible to prove a short runtime, but in general it wouldn't be, so this type of automation wouldn't be impossible. It sounds like maybe you want GCs to be very tunable? That way, developers and operators can change how it runs for a given workload. That's actually one of the (few) reasons I like Java, its ability to monitor and tune its GC is awesome. No one GC is going to be optimal for all workloads/usages. But it seems like the prevailing thought is to change the code to suit the GC where absolutely necessary, instead of tuning the GC to the workload. I'm not sure why that is? reply nmstoker 8 hours agoprevJust a minor correction: it's looking like the release will be 7th October, back from 2nd October, for the reasons discussed here: https://discuss.python.org/t/incremental-gc-and-pushing-back... reply drewsberry 6 hours agoparentThanks for pointing this out, I hadn't seen that – I've just pushed an update to reflect this. reply sandos 10 hours agoprevSo is it impossible to optimize the no-GIL case more really? 20% slower sounds like a lot really. reply Ralfp 9 hours agoparentWhere it says that? Its simply that Python releases features in yearly cycles and thats what was completed for release. Idea is to let people experiment with no-GIL to see what it breaks while maintainers and outside contractors improve the performance in future versions. reply klranh 6 hours agorootparentNo that was not the idea. The feature went in under the assumption that the single thread slowdown would be offset by other minor speed improvements. That was literally the official reason why it was accepted. Now we have slowdowns ranging from 20-50% compared to Python 3.9. What outside contractors would fix the issue? The Python ruling class has chased away most people who actually have a clue about the Python C-API, which are now replaced by people pretending to understand the C-API and generating billable hours. reply pininja 13 hours agoprevI remember first discussions about removing the GIL back in 2021 and a lot of initial confusion about what the implications would be. This is a great summary if, like me, you weren’t satisfied with the initial explanations given at the time. reply ffsm8 10 hours agoparent2021 wasn't the first discussion about that. You can find forum and Reddit posts going back 15-20 years of people attempting to remove the GIL, Guido van Rossum just made the requirement that single core performance cannot be hurt by removing it, this made ever previous attempt fail in the end I.e. https://www.artima.com/weblogs/viewpost.jsp?thread=214235 reply pansa2 10 hours agorootparentDid this attempt manage to preserve single-threaded performance, or was the requirement dropped? reply rightbyte 10 hours agorootparentHe folded. The patches dropped some unrelated dead weight such that the effect is not as bad. reply klranh 6 hours agorootparentThe effect is still 20-50% slowdown for single thread, even with the purported unrelated speedups to make the feature more palatable. That is absolutely in the range of previous attempts, which were rejected! The difference here is that its goes in now to gratify Facebook. reply djoldman 6 hours agoprevSlightly off topic: does anyone have a link to recent work done toward automatic parallelization? (Write single threaded code and have a compiler create multithreaded code) https://en.m.wikipedia.org/wiki/Automatic_parallelization_to... reply Vecr 6 hours agoparentRust's Rayon could probably have a mode with additional heuristics, so it wouldn't multi-thread if it guessed the memory usage increase or overhead would be too much. Not really automatic, but for some iterator chains you can just slap an adapter on. Currently you have to think and benchmark, but for some scripting type applications the increased overhead of the heuristics might be justified, as long as it was a new mode. reply bilsbie 5 hours agoprevI wonder if they could now add a way for the interpreter to automatically find instances where it could run your code in parallel? You’d think certain patterns could be probably safe and the interpreter could take the initiative. Is there a term for this concept? reply jerf 3 hours agoparentAutomatic parallelization, as someone else linked to. It is another one of those things that many programmers ask \"Why hasn't this been tried?\" and the answer is, it has, many times over. It just failed so hard and so fast you've never heard of the results. Toy problems speed up by less than you'd hope and real code gets basically nothing. Your intuition says your code is full of parallelization opportunities; your intuition turns out to be wrong. A subset of the general problem that even very experienced developers still never get great at knowing how code will perform without simply runninga profiler. It has failed hard and fast in languages much friendlier to the process than Python. Proving something is truly parallel-safe in Fortran is hard enough. Proving it in Python is effectively impossible, you just don't know when something is going to dynamically dynamic the dynamics on you. reply biorach 1 hour agorootparent> you just don't know when something is going to dynamically dynamic the dynamics on you. I'm going to use that! reply bmitc 4 hours agoparentprevNo way that's possible or even desirable with Python's OOP and mutable nature and scoping structure. reply zahsdga 8 hours agoprevThe performance degradation with nogil is quoted as 20%. It can easily be as much as 50%. The JIT does not seem to help much. All in all a very disappointing release that may be a reflection of the social and corporate issues in CPython. A couple of people have discovered that they can milk CPython by promising features, silencing those who are not 100% enthusiastic and then underdeliver. Marketing takes care of the rest. reply jononor 8 hours agoparentWhy are you disappointed? Do you think that the progress should be faster? Or that this work should never have been started? Or that they should wait until it works better before integrering in master and having it in a mainline release? reply gavindean90 2 hours agorootparentThe last one. reply akkad33 2 hours agorootparentprevI think they explained why they are disappointed reply nmstoker 8 hours agoparentprevCould you clarify what/who you mean by the final sentence. It gives the impression you didn't take on board the articles mention about the three phases. reply William_BB 8 hours agoprevCan someone explain this part: > What happens if multiple threads try to access / edit the same object at the same time? Imagine one thread is trying to add to a dict while another is trying to read from it. There are two options here Why not just ignore this fact, like C and C++? Worst case this is a datarace, best case the programmer either puts the lock or writes a thread safe dict themselves? What am I missing here? reply 0xFEE1DEAD 7 hours agoparentLet me preface this by saying I have no source to prove what I’m about to say, but Guido van Rossum aimed to create a programming language that feels more like a natural language without being just a toy language. He envisioned a real programming language that could be used by non-programmers, and for this to work it couldn’t contain the usual footguns. One could argue that he succeeded, considering how many members of the scientific community, who don’t primarily see themselves as programmers, use Python. reply William_BB 6 hours agorootparentAll the answers were good, but I think this explained it the best. Thank you reply oconnor663 3 hours agoparentprevIt's harder to ignore the problem in Python, because reference counting turns every read into a write, incrementing and then decrementing the recount of the object you read. For example, calling \"my_mutex.lock()\" has already messed with the recount on my_mutex before any locking happens. If races between threads could corrupt those refcounts, there's no way you could code around that. Right now the GIL protects those refcounts, so without a GIL you need a big change to make them all atomic. reply William_BB 1 hour agorootparentOh wow, I haven't thought about refcounts on mutexes reply micheles 2 hours agoparentprevI am scientific Python programmer. 99% to 100% of my programs require parallelism, but it is ALWAYS embarrassingly trivial parallelism, nothing is ever mutated and I never need locks. Right now I am forced to use multiprocessing to get the performance, with all problems of multiprocessing, the major one being that I need to use more memory. For me using multithreading could mean the difference between running out of memory and not running out memory. The GIL removal matters for people like me, the proponents of the GIL removal comes from the scientific community. reply masklinn 3 hours agoparentprev> Worst case this is a datarace [... w]hat am I missing here? That the worst case being memory unsafety and a compromised VM is not acceptable? Especially for a language as open to low-skill developers as Python? reply William_BB 1 hour agorootparentHm I get the memory safety part, but could you elaborate on the compromised VM part? I'm not sure I understand that.. Specifically I don't understand what you mean by VM here reply Numerlor 7 hours agoparentprevThere is an expectation of not dealing with data races from Python code. Apart from being a language where people expect these things to not be an issue it is also the behaviour with thw Gil in place and would be breaking reply fulafel 6 hours agoparentprevWe don't want Python programs to become the same kind of security swiss cheese as C/C++ programs. reply tliltocatl 7 hours agoparentprevMemory safety, heap integrity and GC correctness i guess. If you ignore facts like C your language will be as safe as C, except it's worse because at least C doesn't silently rearrange your heap in background. reply LudwigNagasena 7 hours agoparentprevWhy not just use C and C++ at that point? People use Python because they don't want to manage data races or re-implement a hash table. reply William_BB 7 hours agorootparentThis was not a gotcha, sorry if it came across that way. It was a genuine question reply v3ss0n 10 hours agoprevExperimentally JIT WHILE there is totally stable , 4-100x faster, almost 100% compatible PyPy exist and all they need is adopt it but didn't due to some politics. Yeah right.. reply qeternity 7 hours agoparentPython allows a lot of paradigms that are really difficult to JIT. I have personally seen many instances where PyPy is actually slower than CPython for a basic web app because of this. reply v3ss0n 4 hours agorootparentIt is only slow when you using C-Extension. Basic webapp don't use C-Extensions. If you are going to claim such please provide evidence. We have a chat server in production written in PyPy and Tornadoweb which had sped up 20x from CPython version and test benchmarks against GoLang based Websocket with Channels and Nodejs - The pypy implementation is 30% faster than node and 10-20% faster than Golang . Big plus is PyPy version drops memory usage by 80% . CPython version have around 200MB Ram usage and pypy version only about 40MB. On Heavy load (10k concurrent test) PyPy and Golang version are stable but Node version stop responding sometimes and packet losses occurs. reply ksec 9 hours agoparentprevI don't follow Python closely. So why PyPy isn't adopted? reply v3ss0n 4 hours agorootparentThe main reason GVR said is \"May be this is future of Python , but not now\" . At that time pypy was already stable at 2.7.x version but python 2-3 split happen. PyPy team have to rewrite everything for Python3 and took them a while to catchup. I had managed to talk with one ex pypy developer that the main reason GVR and Python community do not promote much about PyPy is due to NIH - Not invented here - Since PyPy is developed by seperate but full time computer scientist and researchers, PHDs , not by the Guido community . https://pypy.org/people.html One problem about popularity of pypy is they don't do any advertisement , promotion which I had critically voiced about it in their community - they moved to Github finally. Only other problem is CPython Ext , which is compatible but a little bit slower than CPython - that only pain point we have - which could be solved if there are more contributors and users using pypy. Actually Python , written in Python should be the main reply dagw 8 hours agorootparentprevThe biggest reasons is that it isn't compatible with the python C API. So any library that uses the C API would have to be rewritten. reply tekknolagi 8 hours agorootparentThey have gotten much better about that recently and it's much much more compatible now. reply jononor 7 hours agoparentprevThankfully everyone is free to use PyPy, if they consider it the better solution! reply v3ss0n 3 hours agorootparentSplitting efforts aren't good . PyPy is decade effort of making JIT Python and they had big success. But it doesn't get the praise by the python community - and they are only doing NIH solutions again and again. (First attemped by Google and they failed - unladen python , now this experiment results dosen't sound good). Why ignore millions of dollars spent in a decade effort of fulltime PHD researchers' work and doing their own thing? Yeah NIH is helluva drug. reply wruza 10 hours agoprevIt’s worth mentioning that there is a bit more overhead in using multiple processes as opposed to multiple threads, in addition to it being more difficult to share data. There’s probably a whole generation of programmers (if not two) who don’t know the feeling of shooting yourself in the foot with multithreading. You spend a month on a prototype, then some more to hack it all together for semi-real world situations, polish the edges, etc. And then it falls flat day 1 due to unexpected races. Not a bad thing on itself, transferrable experience is always valuable. And don’t worry, this one is. Enough ecos where it’s not “difficult to share data”. reply jononor 7 hours agoparentThis. Multi-threading is very prone to nasty, hard to reproduce, bugs if used liberally in a codebase. It really should be used with care, and compartmentalized to areas where it demonstratively brings critical performance improvements. reply neonsunset 6 hours agorootparentLet’s not tout abysmal state of multi-tasking in Python as an advantage. Somehow, it’s perfectly fine in C#, Java, now Go, Rust and many other languages, with relatively low frequency of defects. reply jononor 6 hours agorootparentThat is not all what I did. But there are several languages/ecosystems which are very prone to such issues - notably C and C++. It is critical important (in my opinion) that Python does not regress to anywhere near that state. This might seem like an invalid concern - after all Python is a high-level, memory-safe language - right? The problem is that use of extensions (mostly in C or C++) are very common, and they rely on particular semantics - which are now changing. A vast amount of Python programs make use of such extensions, probably the majority of programs (even if excluding the standard library). Some of the biggest challenges around nogil and multi-threading in Python are mostly around extension/C related stuff. It was notably also on of the main challenges faced by PyPy. So maybe it's actually a little bit tricky to get right - and not just the developers being stupid or lazy ;) I mean in addition to the the usual general trickiness of major changes making to the core of an interpreter relied by thousands of companies/projects, in a wide range of fields, built over decades, with varying level of quality and maintenance.... reply neonsunset 5 hours agorootparentRight, I should have phrased it better - did not intend to make it sound like criticism of your reply. Was just aiming at the tendency to dismiss valid concerns with \"it's actually a good thing we don't have it\" or \"it can't be done well\". Of course changing the concurrency guarantees the code relies on and makes assumptions about is one of the most breaking changes that can be made to a language, with very unpleasant failure modes. reply jononor 5 hours agorootparentUnderstood. There has been some amount of that in the past. And probably this kind of pushback will rise up again as the work starts to materialize. I think some are a bit fearful of the potential bad consequences - and it remains unclear which will materialize and which will be non-issues. And of course some have other things they wish to see improved instead, cause they are satisfied with current state. Actually many will be quite happy with (or at least have accepted) the current state - cause those that were/are not probably do not use Python much! What I see from the development team and close community so far has been quite trust building for me. Slow and steady, gradual integration and testing with feature flags, improving related areas in preparation (like better/simplified C APIs), etc. reply mg74 8 hours agoprevThe number one thing I wish was addressed in future version of Python is the semantically significant whitespace. Python is absolutely the worst language to work in with respect to code formatters. In any other language I can write my code, pressing enter or skipping enter however I want, and then the auto formatter just fixes it and makes it look like normal code. But in python, a forgotten space or an extra space, and it just gives up. It wouldn't even take much, just add a \"end\" keyword and the LSP's could just take care of the rest. GIL and JIT are nice, but please give me end. reply dbrueck 6 hours agoparentWhitespace is semantically significant in nearly all modern programming languages, the difference is that with Python it is completely significant for both the humans and the tools - it is syntactically significant. I've actively used Python for a quarter of a century (solo, with small teams, with large teams, and with whole dev orgs) and the number of times that not having redundant block delimiters has caused problems is vanishingly small and, interestingly, is on par with the number of times I've had problems with redundant block delimiters getting out of sync, i.e. some variation of if (a > b) i++; j++; Anytime I switch from Python to another language for awhile, one of the first annoying things is the need to add braces everywhere, and it really rubs because you are reminded how unnecessary they are. Anyway, you can always write #end if you'd like. ;-) reply saagarjha 17 minutes agorootparentNever commented out a loop or a condition, have you? reply ec109685 2 hours agorootparentprevThe parent’s point was that you don’t have to care about white space when composing code in other languages since the LSP can auto format. So you could theoretically never press return or space more than once, and always have perfectly correctly functioning and formatted code at the end. reply biorach 1 hour agorootparent> So you could theoretically never press return or space more than once, There are people that actually do this? reply alfiopuglisi 8 hours agoparentprevThe day your wish is fullfilled is the day I stop working with Python. I can't stand all those useless braces everywhere, why are they there at all since good practice mandates proper indentation anyway? I am at the point where I prefer single quotes for strings, instead of double quotes, just because they feel cleaner. And unfortunately pep8 sometimes mandates double quotes for reasons unknown. reply eviks 4 hours agorootparentSingle quotes are also easier to type on the default layout, no Shift reply mg74 8 hours agorootparentprevNo need for braces. Just add \"end\" for marking block ending to match the already block starting keyword \":\". reply pansa2 6 hours agorootparentA while ago, when thinking about syntax design for a new language, I considered this combination (`:` and `end`, as opposed to `do` and `end` as used by Lua etc). Are there any languages that use it, or is Python unique in using `:` to begin a block? reply stavros 8 hours agoparentprevI don't have this problem, and I've been writing Python for more than twenty years. Sure, I may have the occasional wrong space somewhere, but it's maybe a few times a month, whereas I'd otherwise have to type \"end\" for every single block. reply mg74 8 hours agorootparentI dont think this is a problem anymore in todays world of LSPs and auto formatters. I almost never have to type \"end\" in Elixir for instance, it is always autocompleted for me. reply stavros 6 hours agorootparentHow does it know when to end a block? reply tredre3 1 hour agorootparentThe autoformatter does it based on indentation, but when writing code the editor just inserts it when you open a block (after your cursor), same way } is added when you type { in other languages. reply pansa2 8 hours agoparentprev>>> from __future__ import braces SyntaxError: not a chance reply mg74 8 hours agorootparentThank you, but I rather not inject a tool that hasn't been updated in 6 years into my build chain. Thats how we do things in the Javascript world and frankly it sucks. reply benediktwerner 7 hours agorootparentThis is a joke that's actually built into Python. The __future__ module is where you can import/enable features that will become the default in future versions. The point it makes by giving \"SyntaxError: Not a chance\" is that Python will never add braces. And IMO for good reason. It makes the code so much cleaner and it's not like it particularly takes effort to indent your code correctly, especially since any moderately competent editor will basically do it for you. Tbh I actually find it much less effort than typing braces. reply marliechiller 8 hours agoparentprevThis suggestion gives me chills. I literally never face this issue. Are you using vim? Theres autoindent and smartindent features you can enable to help you. reply mg74 8 hours agorootparentNeovim + ruff lsp. I have gone through many formatters to try and get this better, but it is always worse than any other language where whitespace is not semantic. reply zo1 4 hours agorootparentThe one nice perk about the state of things atm in python is I can very easily filter out devs by their choice of python IDE (or lack thereof). reply mixmastamyk 38 minutes agoparentprevExpecting formatters to fix your broken blocks is a false economy with Python and smart indentation. It takes fewer keystrokes to write Python correctly, than to add delimiters so you could write it incorrectly. Python’s tradeoffs pay dividends every day, at the expense of few questions a year. Also code is read 10x more than written, where the extra delimiters lower signal to noise. reply geor9e 4 hours agoprevis 3.13 bigger than 3.9 reply punnerud 11 hours agoprev [–] The ones saying they will never use Python because it’s slow, is the probability high that their “fast” language is not thread safe? reply bschwindHN 4 hours agoparentWith Rust, code runs at native speed, multithreading is easy and safe, and the package manager doesn't suck. I will never use Python if I can help it, but not just because it's slow. reply Quothling 10 hours agoparentprevHow many teams actually decide against using Python because it's \"slow\"? I'll personally never really get why people prefer interpreted languages in the age of Go, but even if we go back a few years, you would just build the computation heavy parts of your Python in C. Just like you would do with your C#, your Java or whatever you use when it was required. Considering Instagram largely ran/run their back-end on Python, I'd argue that you can run whatever you want with Python. Maybe I've just lived a sheltered life, but I've never heard speed being used as a serious argument against Python. Well, maybe on silly discussions where someone really disliked Python, but anyone who actually cares about efficiency is using C. reply wongarsu 9 hours agorootparentYou occasionally hear stories about teams writing something in Python, then rewriting it in another language because Python turned out to be slow. I have one such story. With the excellent Python/Rust interop there is now another great alternative to rewriting the heavy parts in C. But sometimes the performance sensitive part spans most of your program reply tgv 7 hours agorootparentprev> How many teams actually decide against using Python because it's \"slow\"? At least mine. Also because of the typing. It's probably improved, but I remember being very disappointed a few years ago when the bloody thing wouldn't correctly infer the type of zip(). And that's ignoring the things that'll violate the specified type when you interface with the outside world (APIs, databases). > anyone who actually cares about efficiency is using C. Python is so much slower than e.g. Go, Java, C#, etc. There's no need to use C to get a better performance. It's also very memory hungry, certainly in comparison to Go. reply neonsunset 6 hours agorootparentprevExcept “like you would do with your C#, your Java…” does not happen w.r.t. native components - you just write faster code and in 98% situations it’s more than enough. Now, Java and C# are different between each other when it comes to reaching top end of performance (Java has better baseline, but C# can compete with C++ when optimized), but we’re talking about the level far above the performance ceiling of Go. reply imtringued 7 hours agorootparentprevPython is a lot like PHP. A uniquely bad value proposition in almost all aspects. It is a slow interpreted language, but that isn't the only argument against it. It has abandoned backwards compatiblity in the past and there are still annoying people harassing you with obsolete python versions. The language and syntax still heavily lean towards imperative/procedural code styles and things like lambdas are a third class citizen syntax wise. The strong reliance on C based extensions make CPython the only implementation that sees any usage. CPython is a pain to deploy crossplatform, because you also need to get a C compiler to compile to all platforms. The concept behind venv is another uniquely bad design decision. By default, python does the wrong thing and you have to go out of your way and learn a new tool to not mess up your system. Then there are the countless half baked community attempts to fix python problems. Half baked, because they decide to randomly stop solving one crucial aspect and this gives room for dozens of other opportunistic developers to work on another incomplete solution. It was always a mystery to me that there are people who would voluntarily subject themselves to python. reply continuational 11 hours agoparentprevPython isn't \"thread safe\", not even its lists are. What are you getting at? reply meindnoch 10 hours agoparentprevPython is not thread safe. reply guenthert 7 hours agoparentprevMost of us use Python, just not for stuff which needs to be fast. Not sure, what you mean by thread-safe language, but one of the nice things about Java is that it made (safe) multi-threading comparatively easy. reply IshKebab 11 hours agoparentprev [–] What do you mean by thread safe exactly? Instead of Python I would use Typescript or Rust or possibly Go. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "On 7th October 2024, Python core developers will release CPython v3.13.0, featuring a \"free-threaded\" version that allows disabling the Global Interpreter Lock (GIL) and support for experimental Just-in-Time (JIT) compilation.",
      "The removal of the GIL, a long-debated topic due to its performance impact on single-threaded programs, aims to improve multi-threading efficiency, especially with the rise of multi-core processors.",
      "The JIT compiler in Python 3.13 compiles bytecode into machine code just in time for execution, potentially enhancing performance, and uses a simpler \"copy-and-patch\" technique."
    ],
    "commentSummary": [
      "Python 3.13 introduces significant changes, including Just-In-Time (JIT) compilation and the removal of the Global Interpreter Lock (GIL), aimed at enhancing multi-threading capabilities.",
      "The community is divided on these changes due to potential performance hits, increased complexity, and the need for users to compile JIT versions from source, which may limit adoption and feedback.",
      "Despite the performance benefits, the adoption of PyPy has been slow due to compatibility issues with the Python C API and lack of promotion."
    ],
    "points": 285,
    "commentCount": 148,
    "retryCount": 0,
    "time": 1727486581
  },
  {
    "id": 41674382,
    "title": "AMD Unveils Its First Small Language Model AMD-135M",
    "originLink": "https://community.amd.com/t5/ai/amd-unveils-its-first-small-language-model-amd-135m/ba-p/711368",
    "originBody": "Browse AMD Community Sign In Communities AI Adaptive SoC & FPGA Developers PC Drivers & Software PC Graphics PC Processors Radeon ProRender Red Team Server Processors General Discussions Blogs AI Adaptive Computing Adaptive SoC & FPGA Design Corporate Server Processors Gaming Instinct Accelerators Red Team Groups Adaptive SoC & FPGA AMD Alumni Red Team Modders Knowledge Base Adaptive SoC & FPGA Adrenalin Release Notes Browse All community This category Blog Knowledge base Users cancel Turn on suggestions Showing results for Search instead for Did you mean: AMD Community Blogs AI AMD Unveils Its First Small Language Model AMD-135... AMD Unveils Its First Small Language Model AMD-135M AMD_AI Staff 1 0 31.8K Subscribe to RSS Feed Bookmark Subscribe Printer Friendly Page Report Inappropriate Content yesterday In the ever-evolving landscape of artificial intelligence, large language models (LLMs) like GPT-4 and Llama have garnered significant attention for their impressive capabilities in natural language processing and generation. However, small language models (SLMs) are emerging as an essential counterpart in the AI model community offering a unique advantage for specific use cases. AMD is excited to release its very first small language model, AMD-135M with Speculative Decoding. This work demonstrates the commitment to an open approach to AI which will lead to more inclusive, ethical, and innovative technological progress, helping ensure that its benefits are more widely shared, and its challenges more collaboratively addressed. AMD-135M: First AMD Small Language Model AMD-135M is the first small language model for Llama family that was trained from scratch on AMD Instinct™ MI250 accelerators utilizing 670B tokens and divided into two models: AMD-Llama-135M and AMD-Llama-135M-code. Pretraining: The AMD-Llama-135M model was trained from scratch with 670 billion tokens of general data over six days using four MI250 nodes. Code Finetuning: The AMD-Llama-135M-code variant was fine-tuned with an additional 20 billion tokens of code data, taking four days on the same hardware. The training code, dataset and weights for this model are open sourced so that developers can reproduce the model and help train other SLMs and LLMs. Optimization with Speculative Decoding Large language models typically use an autoregressive approach for inference. However, a major limitation of this approach is that each forward pass can only generate a single token, resulting in low memory access efficiency and affecting overall inference speed. The advent of speculative decoding has solved this problem. The basic principle involves using a small draft model to generate a set of candidate tokens, which are then verified by the larger target model. This approach allows each forward pass to generate multiple tokens without compromising performance, thereby significantly reducing memory access consumption, and enabling several orders of magnitude speed improvements. Inference Performance Acceleration Using AMD-Llama-135M-code as a draft model for CodeLlama-7b, we tested the inference performance with and without speculative decoding on the MI250 accelerator for data center, and Ryzen™ AI processor (with NPU) for AI PC. For the particular configurations that we tested using AMD-Llama-135M-code as the draft model, we saw a speedup on the Instinct MI250 accelerator, Ryzen AI CPU[2], and on Ryzen AI NPU[2] versus the inference without speculative decoding.[3] The AMD-135M SLM establishes an end-to-end workflow, encompassing both training and inferencing, on select AMD platforms. Next Steps By providing an open-source reference implementation, AMD is not only advancing its AI capabilities but also fostering innovation within the AI community. To learn more about AMD-135M, read the full technical blog: Introducing the First AMD SLM (Small Language Model): AMD-135M Model Fuels AI Advancements Additional Resources For information about the training, inferencing and insights of this model, please visit AMD Github repository to get access to the code. Visit Hugging Face Model Card to download the model file. Apply for Instinct accelerator card access on the AMD Developer Cloud. For any questions, contact us by email amd_ai_mkt@amd.com. Explore, innovate, and together, let us push the boundaries of AI. Footnotes [1] The training code for AMD-135M is based on TinyLlama, utilizing multi-node distributed training with PyTorch FSDP. [2] Test ran on AMD Ryzen 9 PRO 7940HS with Radeon 780M Graphics. The Ryzen AI APU Architecture includes CPU and NPU kernels. [3] These are the configurations that we tested. You might get different results on other configurations. [4] The performance had been tested on AMD Instinct MI250 + ROCmTM 6.0 using standardized tests with lm-evaluation-harness. Additionally, the model performance tests are independent of the hardware environment. [5] Hellaswag is dataset and metrics that tests how well that LLMs can reason about physical situations; WinoGrande is a dataset and codebase for evaluating natural language understanding models on a challenging task of Winograd Schema; SciQ is a dataset of closed-domain question answering tasks with text inputs and outputs; MMLU is a dataset of multiple-choice questions on abstract algebra topics, such as groups, rings, fields, and polynomials; ARC-Easy is a dataset of grade-school level science questions for testing advanced question answering systems. SlimPajama is a deduplicated version of RedPajama and sources from Commoncrawl, C4, GitHub, Books, ArXiv, Wikpedia and StackExchange. We drop the Books data from SlimPajama due to license issues; [6] Test ran on AMD Ryzen 9 PRO 7940HS w/ Radeon 780M Graphics. The Ryzen AI APU Architecture includes CPU and NPU kernels. Labels AI Announcements Instinct AI Ryzen AI 1 Like Labels Adaptive AI (Versal Zynq Kria)15 AI Announcements25 AI Software35 EPYC AI7 Instinct AI18 Radeon AI30 ROCm3 Ryzen AI34 Tips and Tricks6 « Previous Next » li.common.scroll-to.top AMD Community Facebook Instagram LinkedIn Twitch Twitter YouTube Subscriptions Terms and Conditions Privacy Trademarks Statement on Forced Labor Fair and Open Competition UK Tax Strategy Cookie Policy Cookie Settings AMD Community Terms of Use © 2024 Advanced Micro Devices, Inc",
    "commentLink": "https://news.ycombinator.com/item?id=41674382",
    "commentBody": "AMD Unveils Its First Small Language Model AMD-135M (amd.com)276 points by figomore 23 hours agohidepastfavorite89 comments diggan 23 hours ago> The training code, dataset and weights for this model are open sourced so that developers can reproduce the model and help train other SLMs and LLMs. Wow, an actual open source language model (first of its kind [from a larger company] maybe even?), includes all you need to be able to recreate it from scratch. Thanks AMD! Available under this funky GitHub organization it seems: https://github.com/AMD-AIG-AIMA/AMD-LLM reply wrs 22 hours agoparentWe (developers and tech managers) really need to hold the line on this terminology. This is a full actual open source LLM. The usual “open inference” model is not. reply boulos 22 hours agorootparentI assume by \"open inference\" you mostly mean \"weights available\"? reply wrs 22 hours agorootparentUsually “open source” for an LLM means you get the weights and the inference code, which I’ve started calling “open inference”. It’s certainly good and useful, but it’s not the actual source of the model. I find people get into silly arguments about the terminology because they’re focused on whether the “source” is “open” and not on what the “source” is actually the source of. “Weights available” indicates even the weights aren’t “open” in the usual software meaning of the term, as they typically come with restrictive licenses (more restrictive than copyleft or attribution). reply throwawaymaths 2 hours agorootparentBy \"source\" do you mean training data? reply nickpsecurity 19 hours agorootparentprevI call them open weights or just freeware like when we got only the EXE’s on Windows. reply amelius 18 hours agorootparentprevOpen source is what you would get if an academic institution would release it. reply ProllyInfamous 28 minutes agorootparentI have worked part-time (hardware technician) at two US-based companies, entirely open-source (including firmware), that are still profitable (one for decade+). My limited understanding (finance side) is that most customers prefer to buy from (even open-source) companies, for several reasons: 1) They don't have time/desire to assemble hundreds of components, prefering drop-in solution 2) Our manufacturing facility has experience / protools, produces products within tighter tolerances 3) Many people understand their own manufacturing limitations and would prefer warranted solutions, without their understood dangers of DIY I personally dropped out of US grad school because it was the antithesis of open-source licensing. Disclosure: I am an AMD shareholder, excited about this recent announcement reply zdragnar 13 hours agorootparentprevAren't academic institutions more likely to claim ownership of anything produced than they are to totally open source something? reply amelius 7 hours agorootparentI don't think so. Do you have examples? reply wmf 20 hours agorootparentprevYou're not wrong, but if you come up with a definition that no one is willing to meet you're just making that definition irrelevant. reply ok_dad 18 hours agorootparentPlenty of people publish actual open source software, the definition isn’t the problem, it’s the people who misuse it that are the problem. reply wmf 17 hours agorootparentThere's a huge difference between software and AI models. We can debate why that happens but it's a fact. Companies are willing to release open weights but virtually no one is willing to create open source models. Shaming and well actuallying has achieved nothing so far. reply wrs 15 hours agorootparentAnd I'm not arguing that they should release open source models. There's no shame in releasing an open-inference model. But I think I'm fair in saying they should use an accurate term for what they do release. reply yazzku 17 hours agorootparentprevThere is nothing \"source\" about the \"open source models\" that companies typically release. The use of the term \"open source\" is deliberate marketing BS. If you want to argue there's a difference between software and a model, then don't use software terms that are already well-defined to refer to some property of the model. https://opensource.org/osd reply cassianoleal 10 hours agorootparentIt’s a lot worse than marketing BS. It’s deliberate misdirection. Essentially a con. reply GeekyBear 22 hours agoparentprev> Wow, an actual open source language model (first of its kind Apple research has previously released another example of a model with open training code, data, and weights, but their model was sized for running inference workloads on mobile devices. However, Apple has a mobile device line of business and AMD has an enterprise AI accelerator line of business, so they are both doing work relevant to their bottom line. reply diggan 6 hours agorootparentThanks, seems you're talking about the OpenELM family of models: https://github.com/apple/corenet/tree/main/projects/openelm reply jerrygenser 22 hours agoparentprevThis would be another example of open source. Not from such a large company but a good reference including code, data, weights, etc. https://allenai.org/olmo reply brianjking 19 hours agorootparentMolmo even more so! The 7b is wild. reply NitpickLawyer 8 hours agoparentprev> Wow, an actual open source language model I find it funny that the AI field has somehow normalised the goalpost moving from capabilities all the way to definitions about open source. And people seem really tribal about it... There absolutely are open source LLMs already. Phi3.5 (MIT), various Mistral models (Apache2.0), various Qwen2 models (Apache2.0) and so on. LLamas are not open source, nor are Gemmas. But to say this is \"an actual open source model\" is weird nitpicking for the sake of nitpicking, IMO. Requiring the methods and datasets that someone used to create some piece of IP is in no way a requirement for open sourcing said IP. It never has been! Imagine this analogy: A dev comes up with a way to generate source code that solves a real problem. This dev uses a secret seed, that only they know. The dev also uses thousands of hours of compute, and an algorithm that they created. At the end of the exercise they release the results on github, as follows: - here is a project that takes in a piece of text in english, and translates it into french. - the resulting source code is massive. 10 billions LOC. The lines of code are just if statements, all the way down, with some hardcoded integer values. - source code licensed under Apache 2.0, written in let's say python. - users can see the source code - users can run the source code - users can modify the source code and re-release the code Now, would anyone pre LLMs say \"this isn't true open source\" because it's too complicated? Because no one can reasonably understand the source code? Because it uses hard coded int values? Because it's 10b LOC? Because the dev never shared how they got those values? Of course not. The resulting code would have been open source because Apache 2.0 is open source. It's the same with model weights. Just because they're not source code, and just because you don't know how they were created, it does not mean the weights are not open source. You can see the weights. You can change the weights. You can re-distribute the weights. It's open source. The definition of something being open source does not cover you understanding why the weights are like they are. Nor do they require you having access to the methods of creating those weights. Or datasets. Or whatever the devs had for breakfast. reply Nab443 3 hours agorootparentGreat, with that definition we can call all binaries opensource ! reply NitpickLawyer 2 hours agorootparentThis is the greatest misconception in this field. Weights are not a binary form! In fact you can't \"run\" the weights as they are. They only represent some fixed values. Whenever you use an LLM you \"load\" the weights, using (usually open source) code and you run inference with that code. The weights are not binary and the analogy to the binary form of distributing software is not valid, IMO. That is why I used the analogy of a python code with ifs all the way, based on hardcoded values. That is what you are arguing is not open source. The weights are just \"hardcoded values\". Open source never had the requirement of the author explaining what, why or how they got a hardcoded value in their shared code. Why it suddenly does for LLMs is what I find funny. reply tga_d 1 hour agorootparentBy that argument, all bytecode is open source, because it has to be run in some other environment, and you can technically modify it if you want to. Open source is supposed to refer to the human-interpretable elements of the code. E.g., kernel modules that are technically formatted as C code but contain non-human readable firmware as values are still considered \"binary blobs\" and not part of the free/open source kernels some distros ship. reply aloknnikhil 3 hours agorootparentprevI completely disagree with you. The fundamental problem with your concept of open source is it goes against what open source really is. The ability for you to completely change what a piece of software can do. IMO, even with LLMs, models are \"executables\" and weights are \"configuration\". Yes, of course you can tune the weights by changing the values, but that's the most I can do. Can I actually add \"features\" to the model? Perhaps you \"open-sourced\" an LLM model trained on the United States Constitution. Can I change the model to then be a specialist in real estate law? Not with weights. I need it to learn case histories to extend its \"feature-set\". Without data and the mechanism to reproduce the model, how is this \"open-source\"? reply NitpickLawyer 2 hours agorootparent> Can I actually add \"features\" to the model? Yes. You can use a number of libraries to add, mix, merge, etc. layers [1] > Not with weights. I need it to learn case histories to extend its \"feature-set\". Again, yes. You can add attention heads, other features, heck you can even add classification if you want [2]. Because you are working with an open architecture! What you think of weights are not binary blobs. That is a common missconception. [1] - https://github.com/arcee-ai/mergekit [2] - https://github.com/center-for-humans-and-machines/transforme... reply aloknnikhil 39 minutes agorootparentAt first glance, that just seems like a bunch of libraries linked together to form a binary. That is not open-source. I completely agree with you that there is just not enough clarity out there. For my education, following up with my earlier example, can I remove the layers that have references to all chapters / laws in the constitution except for the ones meant for real-estate? How would I do that with the approaches you mentioned here? Fundamentally, if I have to \"reverse-engineer\" something, then it's not open-source. reply diggan 7 hours agorootparentprev> that the AI field has somehow normalised the goalpost moving from capabilities all the way to definitions about open source The problem is that Facebook and others are trying to move the goalpost, while others like me would like the goalpost to remain where it is, namely we call projects \"Open source\" when the required parts to build it on our own machines, is sufficiently accessible. As I probably wouldn't be a developer in the first place if it wasn't for FOSS, and I spend literally all day long contributing to others FOSS projects and working on my own, it's kind of scary seeing these large companies trying to change what FOSS means. I think you're forgetting about the intent and purpose of open source. The goal is that people can run software for whatever purpose they want, and they can modify it for whatever purpose. This is the intent behind the licenses we use when we \"create FOSS\". This means, in practice, that the source code has to be accessible somehow, so the compiler I have on my computer, can build a similar binary to the one the project itself offers (if it does). The source code has to be accessible so I can build the project, but also modify it for myself. Taking this idea that mostly only applied to software before (FOSS) but applying it to ML instead, it's clear to see what we need in order to 1) be able to use it as we want and 2) be able to modify it as we want. > You can see the weights. You can change the weights. You can re-distribute the weights. It's open source. Right. If I upload a binary to some website, you can see the binary, you can change the binary and you can re-distribute it. Would you say the binary is open source? The weights are the binary in ML contexts. It's OK for projects to publish those weights, but it's not OK to suddenly change the definition and meaning of open source because companies want to look like they're doing FOSS, when in reality they're publishing binaries without any ways of building those binaries with your own changes. Imagine if the Linux kernel was just a big binary blob. Yes, you can change it, re-distribute and what not, but only in a binary-blob shape. You'd be kind of out there if you insist on calling this binary-blob kernel FOSS. I'm sure you'd be able to convince some Facebook engineers about it, seems they're rolling with that idea already, but the rest of us who exist in the FOSS ecosystem? We'd still have the same goalpost in the exact same spot it's been for at least two decades I've been involved. reply NitpickLawyer 5 hours agorootparent> Would you say the binary is open source? Great question. Is the assembly code in a git, with an open source license? Then yes! It's open source! Think about it this way: just because someone wrote hello world in c and then a compiler translated that into assembly, doesn't invalidate the quality of that assembly code being open source! That's the point. Something is open source or not if the resulting stuff is published under an open source license. Can you see the assembly code? Can you change it? Can you re-publish it? If all of these are yes, then it's open source! > Imagine if the Linux kernel ... That is semantics. The linux kernel is published in c because it's easier for people to reason in that abstracted language, but it would not suddenly become \"closed source\" if it were written in asm, assuming it would still be published under an open source license. In other words, you having access to the \"dataset\" would not make the weights any easier to work with. They would still be in a \"blob\" as you call it. reply yencabulator 2 hours agorootparent> Think about it this way: just because someone wrote hello world in c and then a compiler translated that into assembly, doesn't invalidate the quality of that assembly code being open source! Meanwhile: > The source code must be the preferred form in which a programmer would modify the program. https://opensource.org/osd reply NitpickLawyer 2 hours agorootparentThen, given the fact that both you and Mistral LLC modify the program in the exact same way, that portion still holds. People view weights as an intended obfuscation by the party releasing it. It is not! In fact, it is equally as hard for them to \"understand\" why a certain value at a certain index is what it is, as it is for you! Just ask Anthropic. They are also doing poke this weight, see what pops with their own models. Again, that is why I used the analogy above. You are arguing that if someone uses a hardcoded value in their code, and won't share how they derived that value, it somehow isn't open source. That, IMO, is wrong. reply diggan 1 hour agorootparent> Again, that is why I used the analogy above. You are arguing that if someone uses a hardcoded value in their code, and won't share how they derived that value, it somehow isn't open source. That, IMO, is wrong. It feels like you deliberately ignore the source part of \"open source\". If you have X that produces Y, then X is the source, Y is the output. You cannot \"open source\" Y as Y isn't a source to anything, it's the output from the source. The only part you can \"open source\" is the source part, which is X in this case. reply diggan 4 hours agorootparentprevInteresting points, regardless of how much I disagree with them, so thank you for sharing your views :) > Think about it this way: just because someone wrote hello world in c and then a compiler translated that into assembly I understand your point, since it's technically assembly, you could license that assembly under a FOSS license and now the thing you distributed is \"open\". I agree you could do this, but you shouldn't use \"open source\" to describe what you're doing there, unless the actual source for building that asm is open too. The binary might be available, but \"open source\" is something that applies to source code, not to what we distribute. If your source is C and your output is assembly, but you only try to apply a FOSS license to the output, not the source, it'd be a hard sell to call the source is open and available. The closest I've come to finding some sort of backing to this view I hold is what OSI echos here: > What if I do not want to distribute my program in source code form? Or what if I don’t want to distribute it in either source or binary form? > If you don’t distribute source code, then what you are distributing cannot meaningfully be called “Open Source”. And if you don’t distribute at all, then by definition you’re not distributing source code, so you’re not distributing anything Open Source. [...] Open Source licenses are always applied to the source code — so if you’re not distributing the source, then you’re not distributing the thing to which an Open Source license applies https://opensource.org/faq#non-distribution Similarly, I wouldn't call a song I release as \"open source\" (not that it makes much sense in this case) unless the actual \"source\" of how it was produced is public under a FOSS license, even if you can technically read the sound data however you want, and modify it by patching the audio file. Instead, some other liberal license is more suitable that allows using/modifying/redistributing the output however you want (Creative Commons is common for those use cases), but not a license that is specifically about source code. > That is semantics. The linux kernel is published in c because it's easier for people to reason in that abstracted language, but it would not suddenly become \"closed source\" if it were written in asm, assuming it would still be published under an open source license. I agree with this too, if suddenly the kernel was written in asm, and it's being distributed as asm, then you can license that asm as \"open source\" and that'd be OK. What wouldn't be \"open source\", would be if it's written in C, but that C code isn't licensed \"open source\", but the authors tries to argue that the compiled asm output is \"open source\". It's output, not source, so you cannot license the output as \"open source\" as it's missing that last part, the \"source\". > In other words, you having access to the \"dataset\" would not make the weights any easier to work with. They would still be in a \"blob\" as you call it. Precisely. So the requirements end up something like: Can I build this thing from scratch myself, granted I have the required equipment + knowledge + time? For LLM models, at least the training script + the dataset has to be available without restrictions for that to be possible. If they're not available, or available but under restrictions (usage or otherwise), then it's not open source. reply NitpickLawyer 1 hour agorootparentHaha, having lengthy discussions, especially when we disagree, is healthy IMO. That's how we get to experience other viewpoints, and hopefully become better for the effort. > Can I build this thing from scratch myself You absolutely can. Everything you need is in the model config (layers, stuff) and there are training scripts all over the net. Now, granted, you will not necessarily get the same results, but then again neither is Mistral or Meta. > but the authors tries to argue that the compiled asm output is \"open source\". It's output, not source, so you cannot license the output as \"open source\" as it's missing that last part, the \"source\". Replying here because I can't in the other subthread. I think you are using a misconception on what is source code, and what is a weight. In the LLM world, you already have the source code for inferencing. This would be either pytorch or c code or whatever. You also have the architecture code. You can see what the model looks like, what layers it has, what ops it does to reach a result. That is also open! So you get the source to run inference. You get the source to \"load\" the model (i.e. the architecture, layers, etc). And you get a bunch of hardcoded values. What you don't get is the why behind the question \"why is this value x and not y\". And for the most part, no one knows. > If they're not available, or available but under restrictions (usage or otherwise), then it's not open source. Let's take another (famous) example. Quake is famous for having a hardcoded value somewhere in the source code, that speeds up some geometry computations. Now, you can change that value, but things will be messed up in the engine. Collisions will happen weirdly, things will look bad. Now, is quake any less of \"open source\" if you or I don't understand why the original coder chose that value? Of course not! Well, now just multiply that with 1B hardcoded values. It's the exact same thing. You could change any of the values, but the game would look wonky as you do so. But, at the end of the day, it would not be any less open source. I guess what I'm trying to say is that weights are not binary blobs. Weights are not an obfuscation attempt. Weights are distributed exactly how they are intended to be used, and how they are being used by the creators as well. You can change the architecture of a model (see above for details). You can add layers, you can remove layers. You can perform \"abliterations\", or you can do fine-tuning. Everything is exactly done as the \"creators\" intended. The only thing you don't have is \"how they got those exact same numbers\". But you don't need that. And it's funny that somehow for LLMs that's a bridge too far. It never used to be for any other project. reply kypro 21 hours agoparentprevSmart move from AMD. Helps develop an ecosystem around their tech and for their GPUs. reply jeff_carr 16 hours agorootparentHas anyone tried it? I mean, I would, but as far as I can tell understand I need 4 boxes with 4 GPU's. Plus an interconnect. I mean, I could put in an order for my homelab but at around $80k per box and maybe $20k for the right switches and some other gear, my wife will probably frown at me ordering a $340,000 rig to try this code that I don't know what to do with it if it works. Maybe some other heavy hitter out there can explain what all this whatchamacallit newfangled synergy producing matrix algebra does after you have it running? reply Shadowmist 14 hours agorootparent> that I don't know what to do with it if it works. After you get it up and running you can just ask it what to do with it. reply Rinzler89 6 hours agorootparentHope it tells you to buy more AMD HW because that would be so funny. reply bubaumba 22 hours agoparentprevNo, it's not open source till someone can actually reproduce it. That's the hardest part. For now it's open weights open dataset. Which is not the same. reply diggan 22 hours agorootparentThat's... Not how open source works? The \"binary\" (model weights) is open source and the \"software\" (training scripts + data used for training) is open source, this release is a real open source release. Independent reproduction is not needed to call something open source. Can't believe it's the second time I end up with the very same argument about what open source is today on HN. reply dboreham 22 hours agorootparentBut wouldn't failure to achieve independent reproduction falsify the open claim? Similar to you publish the source for Oracle (the database), but nobody can build a binary from it because it needs magic compliers or test suites that aren't open source? Heck when the browser was open-sourced, there was an explicit test where the source was given to some dude who didn't work for Netscape to verify that he could actually make a working binary. It's a scene in the movie \"Code Rush\". reply wrs 22 hours agorootparentprevThe interesting part of the product we’re taking about (that is, the equivalent of the executable binary of an ordinary software product) is the weights. The “source” is not sufficient to “recompile” the product (i.e., recreate the weights). Therefore, while the source you got is open, you didn’t get all the source to the thing that was supposedly “open source”. It’s like if I said I open-sourced the Matrix trilogy and only gave you the DVD image and the source to the DVD decoder. (Edit: Sorry, I replied to the wrong comment. I’m talking primarily about the typical sort of release we see, not this one which is a lot closer to actually open.) reply littlestymaar 21 hours agorootparent> The “source” is not sufficient to “recompile” the product (i.e., recreate the weights). Therefore, while the source you got is open, you didn’t get all the source to the thing that was supposedly “open source”. What's missing? reply wrs 20 hours agorootparentWell, I’m not experienced in training full-sized LLMs, and it’s conceivable that in this particular case the training process is simple enough that nothing is missing. That would be a rarity, though. But see my edit above — I’m not actually reacting to this release when I say that. reply littlestymaar 12 hours agorootparentOK, so you just like to be a contrarian… reply bubaumba 22 hours agorootparentprevYou are missing key points here. \"reproduce\" means produce the same. Not just train similar model. I can simplify the task, can you convincingly explain how the same model can be produced from this dataset? We can start simple, how you can possibly get the same weights after the first single iteration? I.e. the same as original model got. Pay attention to randomness, data selection, initial model state. Ok, if you can't do that. Can you explain in believable way how to prove that given model was trained on give dataset? I'm not asking you for actually doing all these things, that could be expensive, only to explain how it can be done. Strict 'open source' includes not only open weights, open data. It also includes the word \"reproducible\". It's not \"reproduced\", only \"reproducible\". And even this is not the case here. reply Sayrus 21 hours agorootparentReproducible builds are not a requirement for open source software, why is it one for open source models? reply wrs 20 hours agorootparentI would say that functionally reproducible builds are sort of inherent in the concept of “source”. When builds are “not reproducible” that typically just means they’re not bit-for-bit identical, not that they don’t produce the same output for a given input. reply prophesi 17 hours agorootparentOnce neural networks enter the scene, I don't think giving the same output for a given input is possible in the field currently. I believe this is as open as language models can be, and what people mean when they say it's a \"fully open source\" model. reply Zamiel_Snawley 18 hours agorootparentprevIf they provide the training code, and data set, how is that not enough to reproduce functionally equivalent weights? I don’t have any experience in the AI field, what else would they need to provide/define? As others have mentioned, reproducible builds can be quite difficult to achieve even with regular software. Compiler versions, build system versions, system library versions, time stamps, file paths, and more often contribute to getting non-identical yet functionally equivalent binaries, but the software is still open source. reply bavell 7 hours agorootparentprevI think you are erroneously conflating open source with deterministic builds. Yes, there is a random element when \"producing the binary\" but that doesn't mean it isn't open source. reply worewood 21 hours agorootparentprevHow often do people expect to compile open-source code and get _exactly_ the same binary as the distributed one? I've seen this kind of restriction only on decompilation projects e.g. the SM64 decompilation -- where they deliberately compare the hashes of original vs. compiled binaries, as a way to verify the decompilation is correct. It's an unreasonable request with ordinary code, even more with ML where very few ones have access to the necessary hardware, and where in practice, it is not deterministic. reply e12e 20 hours agorootparentI expect that if I compile your 3d renderer, and feed it the same scene file you did - I get the same image? reply TylerE 16 hours agorootparentWhy would you expect that? 3D renderers are not generally deterministic. Many will incorporate, for instance, noise algorithms. They will frequently not produce byte-identical renders on the same hardware using the same binary. reply Jabrov 22 hours agorootparentprevWhat’s the difference? reply frontalier 14 hours agorootparentthe goal posts moved?! reply avaldez_ 20 hours agorootparentprevReproducibility? I mean what's the point of an open technology nobody knows if it works or not. reply n_ary 22 hours agoprevNow this here is the beginning on real innovation of AI. With AMD coming in(albeit late and slowly), meta with LLama improving, we will soon see some real adaptation and development in next few thousand days. At this moment, I see OAI as the yahoo of the pre-Google era. reply imjonse 14 hours agoparent\"next few thousand days\" can we stick to years as a unit of measure and not spread Sam Altman's phrase :) reply washadjeffmad 13 hours agorootparentTwenty two thousand days Twenty two thousand days It's not a lot, it's all we got Twenty two thousand days - Sam Altman? reply raffraffraff 2 hours agorootparentI was just listening to that song in the last hour reply highfrequency 21 hours agoprevLooks like they are using sixteen $13k GPUs [1] (around $210k hardware) for 6 days of training. Anyone know the recommended cloud provider and equivalent rental price? [1] https://www.wiredzone.com/shop/product/10025451-supermicro-g... reply layoric 19 hours agoparentMI250s definitely aren’t a common card to rent so only can find Runpod at $2.10 per hour each. This results in a training cost of $4838 + fine tuning of $3225. However this doesn’t include the 11TB of storage or time taken to get the setup actually running the tasks. So likely you wouldn’t see much change from $10k usd if any. - https://www.runpod.io/gpu/mi250 reply lhl 19 hours agoparentprevRunpod.io rents the next-gen MI300X's for $4/hr, although since they also rent H100's for $3/hr (that are easier to work with/faster for training) it might be more of a novelty. reply highfrequency 18 hours agorootparentI thought the whole selling point of AMD GPUs was that they were a lot cheaper than Nvidia GPUs? reply dagmx 14 hours agorootparentCheaper for the cloud company. But that doesn’t always translate to cheaper for the end user. Maybe they cost more to run or maybe there’s fewer of them so they’re more expensive to book? reply knotimpressed 13 hours agorootparentAt least a couple years ago, a big advantage of Nvidea cards was how much cheaper they were to run power wise-often the dies that made it into cloud level cards would be binned consumer dies. Not sure if that’s still the case, but I’d say it’s plausible. reply lostmsu 13 hours agorootparentImpossible. Power costs for H100-like cards are dwarfed by the cost of the cards themselves. H100 at full load will consume ~$3500 (rough estimate) of power in 5 years at $0.12/kWh. reply yencabulator 2 hours agorootparentData centers are more constrained by availability of power, and the matching cooling, than the actual bulk cost of it. For example, I've been in situations where we had to deploy fewer hard drives per server unit than we otherwise would have, just because we knew we couldn't power & cool the racks if we fully stocked them. reply wmf 20 hours agoparentprevHot Aisle seems to the (only?) place to rent AMD. (Ryan, please don't spam this thread. It's not a good look.) reply benterix 22 hours agoprevI'm happy to see a truly open source model. Actually, AMD has excellent reasons to make this kind of development and I hope they continue. reply luyu_wu 22 hours agoprevThe section on speculative execution is interesting. \"This approach allows each forward pass to generate multiple tokens without compromising performance, thereby significantly reducing memory access consumption, and enabling several orders of magnitude speed improvements.\" Does anyone know if the \"several orders of magnitude speed improvement\" is accurate? I'm doubtful. Very interesting though! I'll be playing around with this on the weekend! reply lhl 20 hours agoparentOrders of magnitude seems a bit ambitious. The implementation from the DeepMind paper achieved a 2-2.5X https://arxiv.org/pdf/2302.01318 and most of the tests I've seen [1][2] have been similar, but there are different variations (Medusa, Ouroboros, etc) that can do better/be combined. Recently Together.ai published SpecExec, a SD variant which did claim to get a 10-18X speedups: https://www.together.ai/blog/specexec [1] https://www.reddit.com/r/LocalLLaMA/comments/17h4rqz/specula... [2] https://arxiv.org/pdf/2402.01528v3 reply lhl 10 hours agorootparentBTW, I got a chance to read through the model card and there's a section that shows their SD gains: https://huggingface.co/amd/AMD-Llama-135m#speculative-decodi... - 1.75x-2.80x on MI250 - 2.83x-2.98x on NPU - 3.57x-3.88x on CPU Note they were testing on AMD-Llama-135m-code as draft model for CodeLlama-7b, both of which do similarly badly on Humaneval Pass@1 (~30%), so it's likely if they were using a similarly trained 135m to SD for say, Qwen2.5-Coder (88.4% on HumanEval), the perf gains would probably be much worse. reply craftkiller 22 hours agoprevI see multiple mentions of NPU on this page, but its still not clear to me: is this something that can finally use the NPU on my processor? reply lhl 19 hours agoparentThere's actually seems to be a bunch of stuff now: * https://github.com/amd/RyzenAI-SW - has a list of demos and how to use it directly (including apparently w/ PyTorch and LLMs) * https://github.com/huggingface/optimum-amd - can use RyzenAI to use the NPU for HF transformers There's now a Linux driver even https://github.com/amd/xdna-driver although it looks like a sufficiently PITA that I haven't even bothered to try it (my 7940HS only has like 10 TOPS anyway, so not much point even if it worked perfectly). reply loufe 23 hours agoprevIt's always encouraging to see wider hardware platform competition for AI inference and training. Access to affordable and capable hardware for consumers will only benefit (I imagine) from increasing competition. reply bjt12345 19 hours agoprev> [1] The training code for AMD-135M is based on TinyLlama, utilizing multi-node distributed training with PyTorch FSDP. I thought PyTorch didn't work well with AMD architecture, and read of many people using JAX instead? reply Decabytes 21 hours agoprevSince most people can’t run these LLMs locally, I wonder what a model would look like where we have hyper tuned models for specific purposes, IE a model for code, a model for prose, etc. you have a director model that interprets what downstream model should be used and then it runs that. That way you can run the model locally, without needing beefy GPUs. It’s a trade off of using more disk space vs needing more vram reply wmf 20 hours agoparentThe whole point of this model is that it's so tiny that even a weak RPi could run it. Apple has also done some interesting work with a common IE a model for code That's already very much a thing. Codestral, Phind, Starcoder etc. Fine tuning models on whatever you want is quite accessible if you have a good dataset and a 100 bucks of budget reply rsolva 22 hours agoprev [–] Can this model run on ollama? reply suprjami 19 hours agoparenthttps://huggingface.co/QuantFactory/AMD-Llama-135m-GGUF reply rsolva 6 hours agoparentprev [–] I tried the Q_8, but it was all over the place, so not really trained for instruction and chat yet, I think :) reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "AMD has unveiled its first small language model (SLM), AMD-135M, which includes two variants: AMD-Llama-135M and AMD-Llama-135M-code, trained on AMD Instinct MI250 accelerators.",
      "The model employs speculative decoding to enhance inference speed and memory efficiency, a significant improvement over traditional autoregressive approaches.",
      "AMD has open-sourced the training code, dataset, and weights, encouraging developers to reproduce and innovate upon the model, fostering a collaborative AI community."
    ],
    "commentSummary": [
      "AMD has introduced its first small language model, AMD-135M, which is fully open-sourced, including the training code, dataset, and weights.",
      "This release is notable because it allows developers to reproduce the model and train other small language models (SLMs) and large language models (LLMs), promoting innovation and competition.",
      "Unlike typical \"open inference\" models that only provide weights and inference code, AMD's model offers a comprehensive open-source package, marking a significant move in the AI hardware and software industry."
    ],
    "points": 275,
    "commentCount": 89,
    "retryCount": 0,
    "time": 1727463905
  },
  {
    "id": 41676653,
    "title": "If WordPress is to survive, Matt Mullenweg must be removed",
    "originLink": "https://joshcollinsworth.com/blog/fire-matt",
    "originBody": "If WordPress is to survive, Matt Mullenweg must be removed Published: September 27, 2024 Updated: September 28, 2024 Note This post is a little more hasty than some of my others, in the interest of expedience. I hope you’ll bear with the jumble of thoughts. It’s being actively edited, though I’ve noted updates made since the original publish date. I also usually avoid cussing on my blog, but I do a little here because it feels warranted. Cover image from this Etsy store (unaffiliated). There are some people who think being right about something gives them the right to do whatever they think should be done about it; a license to act however they see fit in order to correct that wrong. This, of course, is never the case. Doing the wrong thing for the right reason never makes it the right thing. No matter how egregious the original infraction, there are some responses it never justifies. Two wrongs don’t make a right, to be pithy about it. The ends don’t justify…you know how it goes. Matt Mullenweg appears to be one of those people who believe the ends do indeed justify the means, as he’s effectively blowing up massive swaths of the WordPress community in his fight with some of its landlords. Matt has, for far too long, enjoyed unchecked powers at the top of WordPress—powers which are all too often a direct and flagrant conflict of interest. And while we’ve seen this power abused before, we’ve never seen it on this scale. Yes, Matt’s original point might be warranted. But his egregious actions utterly nullify any previous merit. A line has been crossed, and the entire community is worse for it. I believe that if WordPress is to survive, let alone thrive, Matt Mullenweg must be removed from all forms of official WordPress leadership, as expediently as possible. Wait, who are you and why do you care? Let’s get this out of the way right off: I’m not the best person to be talking about this. I haven’t really been involved in WordPress for about five years now. Honestly, I couldn’t tell you the last time I even logged in to a WordPress site. That said, however: I spent some six or seven years of my life deep in the WordPress world. I built and customized WordPress sites for clients as a designer; I taught a WordPress development course (focused on building custom themes in PHP) for about five years; and I worked in support for Flywheel, a managed WordPress hosting company, for a little over five years. It was there I transitioned to full-time frontend work, building tools to help support WordPress sites. So while I’ve been out of the WordPress game for a good while now, I still might be considered an expert next to your average Joe. I’d like to think I could still sling some theme templates with the best of ‘em. (Hell, some days I even get a little nostalgic and think about booting up a Local site just for fun.) You might have spotted the word “Flywheel” up there and realized that company was acquired by WP Engine—the company with which Mullenweg is publicly feuding at the moment—back in 2019. That might reasonably raise questions of my objectivity, so let’s get this out of the way: Yes, I used to work for WP Engine. I even kinda liked them, for a while (mostly while they just kinda left us alone for the first year or so). But I wouldn’t say my time at the company left a good taste in my mouth. We don’t need to dredge up a bunch of old and buried stuff that isn’t really important anyway, but suffice to say: I really don’t have any reason to be a WP Engine cheerleader. Most of the people I knew there have left, and I’ve watched from the sidelines as the company has implemented a bunch of scummy policies and shady sales tactics to squeeze money from their customers and make it harder to leave. On most days, if you wanted to have a conversation about how much WP Engine sucks, frankly, I’d be a happy participant. So this post might be a lot of things, but I can assure you it’s not me defending my old company just because I used to work for them. I’ve got literally no reason to do that. To the extent I’m on WP Engine’s side, it’s not because of any sense of loyalty to the company or to the remaining good people I know there; it’s because I believe what Matt’s doing is deeply wrong and foolishly destructive. I’ll also go on record as saying I got pretty far in the interview process at Automattic once, a few years back. And, since we’re being honest, it was the absolute worst interview process I’ve ever taken part of as a web professional (though the people themselves were lovely). But that alone ain’t gonna get a post out of me. I’m not wasting my time and yours just to gripe about an interview I chose to drop out of over three years ago. Just thought it merited a mention. I still regarded Matt Mullenweg himself pretty highly after that, up until the last year or so. This post isn’t long enough to get into the details, but Matt had already become a pretty “problemattic” character well before any of this went down. So in summary: I’m not a big fan of either party, and I don’t have any good reason to side with either one of them. I am, however, somebody who still cares deeply about WordPress. It’s what gave me my start, and I still recommend it to a lot of people when they ask me what system might best suit their needs. It’s a wonderful community, all in all, and despite my inactivity, I still feel invested in WordPress, and interested in seeing it continue to be a productive way to democratize the web. Finally: I am not a lawyer, and since it’s Friday now and this feud had already reached lawyers-involved level by Monday morning, I should be careful to clarify any legal commentary here is expressly my personal, non-expert opinion. I’m sorry, what happened? For those of you who haven’t been following the story thus far (read: aren’t chronically online web nerds like me), let’s hit the highlights. Automattic approaches WP Engine to offer a “license” Sometime in or around July of this year, Automattic (Matt Mullenweg’s for-profit company, which owns, among other things, WordPress.com, a major WordPress hosting company) reached out to WP Engine (also a for-profit company that offers WordPress hosting, and probably Automattic’s largest business rival). Automattic was offering WP Engine some kind of “licensing,” at a rate of 8% of total business revenue, adding up to the eye-popping sum of several million dollars per year. WP Engine apparently turned down this offer, presumably because it doesn’t appear they actually need any license. The term “WP” is explicitly not covered by the WordPress trademark policy, and using the term “WordPress” to describe products and services (e.g., calling yourself a “WordPress specialist,” or saying you offer “WordPress hosting”) is fully allowed, according to the policy. They’ve also been in business for like 15 years now, and somehow none of this has come up before. Besides, I could name dozens of companies just off the top of my head also using one or both of those terms. So the “you need a license to say this” argument seems highly targeted and extremely dubious. Matt’s rejected, so he tries new strategies Immediately following WP Engine’s rejection, the WordPress Foundation (the nonprofit that governs WordPress, the open source software, and which Matt Mullenweg also runs, in effect if not nominally) filed to trademark the terms “Managed WordPress” and “Hosted WordPress.” Neither trademark has been granted at this point, nor should they; they’ve been in use for ages, and are obviously far too generic for any one organization to hold. Most reasonable and knowledgeable people seem to share this opinion. Companies have been describing themselves as one or both of those terms for around 15 years at this point. (We freely called Flywheel a “managed WordPress hosting company” the entire time I worked there, and we were far from the first. We were also at one point one of WordPress.org’s recommended hosts. So…obviously, not a big deal.) Anyway, this filing of spurious trademarks makes it appear very much like Matt’s endgame was to extract money from WP Engine, but he just needed more of a foundation to do it (pun intended?). So, following that initial rejection, Matt set the Foundation arm of WordPress working on securing highly dubious trademarks, which, again, I and most reasonable observers think and hope will fail. Meanwhile, Matt also began sending a series of very apparently extortive messages to WP Engine leadership, essentially demanding they pay up or else. (This is all in WP Engine’s letter to Automattic, which I’m getting to, but which comes later in the story.) All of this was in the run-up to WordCamp US, the largest WordPress event of the year, at least in North America. (Of note: WP Engine sponsored this event at the highest level, as did WordPress.com.) Matt let WP Engine leadership know, via private DMs, that he intended to “go nuclear” and “scorched earth” on WP Engine in his keynote at the conference—that is, if WP Engine failed to acquiesce to his monetary demands, i.e., 8% of total revenue, i.e., tens of millions of dollars. It appears he repeated the “just pay up and I’ll make this all go away” offer up to the literal last minute before he went on stage. Let’s not beat around the bush: words like “threat” and “extortion” very much apply to Matt’s behavior here. [Edit 9/28/24: I left out that Matt’s demands included the alternative option for WP Engine to “pay” in contribution hours; i.e., instead of cash, they could just donate employee work hours to WordPress. I didn’t feel this was important, since ultimately it all shakes out the same (ha), but I suppose it’s relevant information.] Again: this demand was ostensibly in exchange for a “license” to use terms like “WordPress,” “WordPress hosting,” “WooCommerce,” etc.—none of which appear to be actually necessary. The only possible exception seems to be “WooCommerce,” which is a trademark (and product/company) owned by Automattic. However, the lines are very blurry on what is and is not permissable when it comes to using the WooCommerce name. WP Engine does indeed call one of its offerings “WooCommerce hosting,” which is explicitly called out in the guidelines. So I don’t know, maybe there’s validity there. Maybe. However, for one thing, it’s hard to know whether, or how much the trademark guidelines might have changed. Matt made several changes to the WordPress license page in the last week, among other things, to call out WP Engine. That makes me not trust that the WooCommerce license page I’m looking at today is the same as it was last week—which, all on its own, should be setting off raging alarms for even the most casual of observers. It’s extremely bad news when the company you’re doing business with can just decide what the new terms are with no warning or recourse. Anyway, Matt keeps sending the DMs all the way up until the literal last minute, offering not to excoriate WP Engine onstage during his keynote at the country’s (continent’s? world’s?) largest WordPress event, provided they simply pay up. Once more: I’m no lawyer, but I’m pretty sure that’s called extortion. WP Engine says no (actually, they ask for more time, which Matt denies and takes as a no), so he proceeds with operation “scorched earth,” and blasts WP Engine both onstage at WordCamp US, and in several other venues. Wait—what’s Matt’s actual deal? Why is he doing this? Aside from the licensing issue, which I covered above (and which seems like a mostly flimsy premise to me), Matt’s got some other complaints with WP Engine. Some have validity, some seem completely made-up. Let’s walk through them. Matt claims WP Engine is misrepresenting itself Among Matt’s complaints: that WP Engine is “misrepresenting” itself as an entity that’s officially affiliated with and/or endorsed by WordPress itself. Matt’s repeatedly used as an example his own mom’s confusion; she apparently thought WP Engine was somehow affiliated with WordPress.com (I guess because they also use the word “WordPress,” and are maybe a vaguely similar shade of blue). I’m sure it’s frustrating, having taken over half the internet and being worth hundreds of millions of dollars, only to find out your own mom still doesn’t really understand what you do, but: come on, bro. First, tons of companies use “WP” in their names, and/or the names of their products. Why isn’t Matt going after them? Second, as many people have already noted: Matt effectively runs both wordpress.com and wordpress.org, which are entirely separate entities that do two completely different things. You wanna tell me that’s clear, but somehow WP Engine and WordPress.com are too similar? Really? Third, my kindergartner and every kid in his class could tell the difference between the WordPress W and WP Engine’s dumb logo. (WP Engine’s logo has always been a grid of weird, almost-square shapes that’s apparently meant to vaguely resemble an engine, but which makes no sense to pretty much anyone who’s ever seen it, far as I can tell. It’s a bad logo, in my professional opinion as a designer, even the slightly better version they just released recently. But I digress. Point is: it looks literally nothing like any WordPress logo. Also: it’s not the same color. I have color vision deficiency, and even I can tell that.) Finally, for the whole two years I worked for Shopify, most of my family thought I was at Spotify. Now I’m at Deno, and nobody in my family has any clue what a JavaScript runtime is, and my dad basically thinks I work for Java. Family members don’t always get tech. That’s not a sign that something is wrong, and it’s most certainly not a sign that any wrongdoing has been committed, let alone deliberately. (Which, I assume, probably wasn’t Matt’s mom’s point to begin with, but that didn’t stop him from running with it.) [Update 9/28/24: As an additional point here: if the problem was confusion around WP Engine’s name, why not just ask them for a name change? Why all the contribution stuff, too? Conversely, if Matt’s beef was with WP Engine’s lack of contribution, why is he going after their name and marketing? It feels very much like Matt’s just trying to cobble together all the reasons he can think of to justify his assault, in my opinion.] Matt claims WP Engine is selling a “cheap knock-off” of WordPress Matt also claims WP Engine is selling “something that they’ve chopped up, hacked, butchered to look like WordPress.” His reason for this wild claim? Because WP Engine disables revisions (a default feature of WordPress, albeit a pretty small one). Literally, that’s it. One tiny feature. The whole thing’s been “hacked and butchered” because they just chose to modify one mostly insignificant detail. Of all Matt’s spurious claims, this one might be the one that reeks the most of absolute made-up bullshit. WP Engine will just turn on revisions if you want them to, but that’s beside the point. First, if I decide to build something with, say, Laravel, but decide there’s one feature I want to turn off, I’m not “hacking and butchering” Laravel. That’s obviously ridiculous. Second, pretty much all hosts limit revisions in some way or another anyway, because they take up a ton of memory and most people don’t really need them that bad. And third, it’s open-source software! You don’t get to tell people how they use it! We could also get into the utter hypocrisy that many of WordPress.com’s plans do far, far, far more invasive modifications of WordPress core (you can’t even install themes and plugins, FFS!), but again, that’s all beside the point. It’s open-source. They can do that. Anyone can. It’s in the license. This claim is clearly total garbage. Matt says WP Engine doesn’t give back enough Matt’s other complaint—and I think this is what everything else really boils down to—is: WP Engine doesn’t give back enough to WordPress, in Matt’s estimation. Matt showed some numbers onscreen at WCUS, comparing Automattic’s contributions to WP Engine’s. But I’m not going to repeat them because I’m certain they’re distorted. Besides, I’m not sure the two companies’ work can, or should, be considered directly comparable in the first place. They do different things in different ways, and there’s no law or license mandating either of them do anything to begin with. Regardless, Matt seems irked that WP Engine isn’t abiding by the “Five for the Future” program, outlined on WordPress.org. Five for the Future asks that if you benefit from WordPress, you give back 5% of your time directly to that open-source project, which I think pretty much everyone can agree is a very noble and admirable aspiration that companies such as these involved should probably be doing. But it’s not a requirement, or a policy, and enforcing it as such—acting unilaterally as the WordPress police, let alone so suddenly and violently—is extremely questionable and deeply troubling. (Not to mention a likely deterrent for people and organizations who might want to participate in the WordPress space.) Matt’s claimed he/Automattic have been soliciting WP Engine for increased contributions for “years,” and that they’ve given “$0” to the WordPress foundation. To the best of my knowledge, neither of those claims has been substantiated, but I suppose they don’t really change this discussion much either way, because again: Matt’s taken it upon himself to act as the WP PD to enforce a law that isn’t even a law. So that’s it; that’s what Matt’s mad about. There’s some substance there, and in a vacuum, I think he’d probably have a lot of people on his side. But we’re not in a vacuum; there’s a lot of context here. So I’d like to talk about that next. An aside on motivations and justifications Having explored Matt’s complaints, I’d like to pause for a moment, because this is where the sides seems to diverge. The relatively small number of people in the community who appear to remain on Matt’s side (which seems to be mostly made up of his own employees and some people with their own reasons for hating WP Engine) appear to be sticking with him because they agree with this core point, i.e.: WP Engine should be doing more—maybe much more—especially considering that they’re a company owned by private equity and making significant money off WordPress. On its own, I think that claim seems perfectly fair. We could disagree about the details, or how much is too much or too little, but I don’t think it’s unreasonable to say a company the size and profitability of WPE probably owes quite a lot to the open-source software it’s built on (ethically, at least; likely not legally). So it bears mentioning that WP Engine actually does do a pretty good deal for WordPress. You can cherry-pick specific ways it hasn’t contributed much, and you could certainly make a reasonable case they should be doing more. But to say they’ve given “$0” strikes me as pretty deliberately misleading. WP Engine pays several staff members to contribute work hours to WordPress core (again, maybe the number should be greater than it is, but it’s definitely not zero), on top of the full-time maintenance of plugins, themes, and apps like Advanced Custom Fields, WP Migrate, WP GraphQL, Genesis, Local WP, and many others—all of which used by countless thousands of WordPress users every day. This is to say nothing of WP Engine sponsoring of WordCamps, creating their own tutorials and educational material, their own events, and so on and so forth. Point is: WP Engine does do a lot more than zero. You could argue those contributions are not “pure” (Matt does), and that they’re ultimately in service to WP Engine, and not the WordPress community. But in fairness: sure, they’re all marketing tools in some form or another, but you don’t have to pay for any of them. They all get maintained, they all have tons of users both on and off WP Engine, and they all work no matter what host you choose. (I’m sure they’re all used on WordPress.com. I’d even use some of those things if I had to spin up a WordPress site tomorrow, even if I probably wouldn’t host on WP Engine, personally. I’d probably choose SpinupWP, myself, which is another company with “WP” in the name that Matt apparently doesn’t care about.) Besides, Matt’s company does exactly the same thing with Jetpack, which charges $5–$50 per month, depending on tier, so…not sure where that moral high ground is supposed to be coming from. Is Automattic really gonna claim Jetpack’s paid features are purely for the altruistic benefit of the community? Why do they get a pass on paid features? I think you could fairly, if crudely, paraphrase Matt’s argument as: “WP Engine is in it for the money, and we are in it for WordPress.” That’s a really flimsy stance in my view, without even getting into whether we can, or should, have exactly the same expectations of both companies in the first place (which is at least questionable; Automattic has their hands in a lot more things than WP Engine does, including Tumblr, PocketCasts, Longreads, and many others things that may or may not be related to WordPress, along with at least two hosting companies). Still, once more: there’s probably some validity there. WP Engine is a big company that makes lots of money, and it probably can and should do more. Matt could’ve made that point. I think most people would’ve agreed with him, if he had gone about it properly. We’d probably be lining up with him. There was a way to rally the community around this. If Matt Mullenweg had done this the right way. But Matt, being Matt, didn’t make that point in a good way. (Sorry, this post is already too long without me going into all the times in the past he’s stirred up drama and just generally been a toxic jerk to undeserving people in the WordPress community. But if you’re not aware: it’s become increasingly common. He was even adding public snarky comments on WP Engine employees’ posts, ones who had given decades of their life to the project, as recently as yesterday.) Matt tried extortion, and threats, and petty, childish tantrums, and when none of that worked, he fully exercised his unmatched and unchecked powers in an inconscionable way, in order to extract millions of dollars from WP Engine to put in his own for-profit competitor’s bank account. But I’m getting ahead of myself. So that’s the core of this whole thing; Matt thinks private equity is ruining everything and taking too much without giving enough back. It’s an easy home run of a point to make in this economy. Pretty much nobody disagrees with that. Maybe he thought he’d come off like Robin Hood in this whole deal. I don’t know. But if there was a way to tactfully and gracefully thread that needle, it wasn’t the rampaging hippopotamus approach Matt took. The split in the community seems to lie in whether that core point justifies Matt’s actions. It seems to me that most people agree it does not; that Matt’s committed too many flagrant fouls of his own for the original infraction to matter. Matt had a problem with the landlords, so he carpet bombed the neighborhood. He didn’t like Alderaan’s leaders, and so he fired the Death Star. And now it doesn’t really matter what his original point was; he’s made himself the bad guy. Anyway, back to the timeline. (Note: I may have the chronology slightly mixed up here on a few of the points, but I don’t think it should really matter.) The WordCamp US fallout and Matt’s abuse of power At some point in this chaos (during his keynote at WCUS, or shortly after), Matt used his sway over every branch of power in the WordPress government to write a blog post called ”WP Engine is not WordPress,” (which isn’t something anybody seems to have been confused about, except of course Matt’s mom). That post lambasts WP Engine for all of the already discussed reasons. It also, crucially, went up on WordPress.org, which on its own seems questionable. WordPress.org is ostensibly the website for the nonprofit foundation; it’s supposed to exist to prevent any one for-profit company from having too much power over the WordPress ecosystem. It’s supposed to be agnostic. Not only was that boundary ignored, but since the post was published as WordPress news, it was then syndicated to each and every WordPress admin dashboard in the world. Forget for a second whether you agree with Matt or not; we’re getting into some of the worst of the conflicts of interest and abuses of power here. This type of maneuver, plainly, is anti-competitive. It’s a flagrant exploitation of Matt’s many roles and the wild control he has over many branches of WordPress, many with conflicting priorities. It’s bullying, really; WP Engine doesn’t have any tools to strike back like that. It can’t. (Maybe it wouldn’t, since to date, WP Engine appears to be the company with grown-ups in the room, who know to behave as though their actions will be examined in a courtroom one day.) This would be like Meta one day deciding it didn’t like how a competitor was using React, and serving every single Facebook user a story on their home feed, brutally disparaging that competitor. It’s clearly a dramatic overreach. I think Matt thought WP Engine had no retaliation. I think he was counting on this maneuver being yet another push towards their eventual acquiescence. But I guess it doesn’t matter; that’s just my speculation. In any case, Matt wasn’t done. Matt went on flexing (read: abusing) his power by updating the WordPress trademark policy to retroactively disincentivize the use of the term “WP” in titles of products and companies. (Here’s the source on that change.) You know why you constantly get notifications saying “we’ve updated our terms”? Because you legally have to do that. To just change the terms without letting people know is shady at best, and actively malicious at worst. Well, Matt just went in and changed the terms. Altering the WordPress trademark policy is yet another abuse that should make any remotely impartial observer shudder. Why would anyone want to use a software with an oligarch dictating the terms, and changing them on a whim, with no warning? It’s around this point in the story Matt is really losing the plot. His whole complaint with WP Engine is that they’re not helping WordPress enough. But yet…he’s burning WordPress to the ground to make that point. WP Engine’s reaction Following all this, WP Engine—quite understandably—doesn’t really care for all of their users seeing that negative messaging in their wp-admin. So, WP Engine finds a way to block the news feed on WP Engine sites. That would be questionable in a vacuum, to be sure. But we’re steeped in context at this point. (A lot of users either turn it off or ignore it on their own anyway, for what it’s worth.) Following WordCamp US, WP Engine also sent a cease-and-desist to Automattic. It’s pretty damning, and does a good job laying out all the points I tried to cover above. (In short: Matt tried to extort money from WP Engine for spurious licensing claims, and used disinformation, or at least heavily slanted data, to do it.) One of the biggest revelations here is: Matt wanted the money he was trying to get from WP Engine to go to Automattic, which, again, is Matt’s for-profit company. There are some pretty obvious conflicts of interest here. First and foremost, Automattic (or WordPress.com, at least) is a direct competitor of WP Engine’s. Second, while Automattic does apparently own the WooCommerce copyright, it does not own the WordPress copyright. That is owned by the WordPress Foundation. But it gets even murkier from there, as the Foundation is maybe (or maybe not) WordPress.org? And either way, the Foundation is apparently three people, and Matt Mullenweg is not only one of them, he appears to be the only active one! Of the other two board members, one is a blogger whose company Matt bought out, and who apparently is no longer in the industry. The other is apparently a Partner and Managing Director at—surprise!—a private equity firm (not to mention a twice-failed Republican politician). Wait…isn’t private equity bad? I guess not if it’s on Matt’s side. (For the record, Matt and his companies are tied up in private equity in other, more substantial ways than this, but that’s not worth getting into. It’s all pretty hypocritical.) It appears neither of the other two Foundation board members is active, and therefore, Matt is essentially, behind the curtains, the King, Prime Minister, and Pope when it comes to WordPress. Nobody holds any ability to check his power or challenge him. (That’s very relevant to what happens next.) Also: Matt apparently kinda sorta owns WordPress.org, too. So he has a dizzying interweaving of conflicts of interest and power abuses here. (Source for all that about the foundation here.) Let’s not leave unspoken the irony that the guy who basically is WordPress.com, and WordPress.org, and the WordPress Foundation, wants you to think the name “WP Engine” is confusing. Anyway. Automattic responded by sending its own cease-and-desist to WP Engine, claiming mainly that WP Engine is deliberately confusing people, and that it owes licensing to…someone. Automattic, I guess, though the lines are so blurry it’s clear the separations between WordPress entities were only ever little more than a smokescreen. I should mention: most people believe WordPress.com and WordPress.org/the Foundation are two (three!?) separate entities. I sure did, before this week. I thought the two had separated many years ago, with the express intent of preventing any one for-profit company from abusing the WordPress name. I guess they technically are. But when one person apparently enjoys unchecked control over all of them… [Guitar begins strumming with Alanis Morissette vocalizing] Matt melts down Two really weird things happened on Wednesday. First, out of nowhere, Matt decides to publish a post on his personal blog outlining his charitable donations. He really frames it as though he’s being victimized and bullied into revealing this information, and I suppose some people were probably (reasonably) asking how much he gives, since he spent the whole week blowing up half the internet over how much WP Engine gives. In the post, he also spends a lot more time defending himself against claims of being a “mafia boss” than most people who aren’t mafia bosses or acting like mafia bosses ever feel the need to do. Weird move all around. Especially since the implication seems to be…what? “I’m a good guy so I can’t do bad things”? I tried my best to look up Matt’s net worth and work out what percentage he’s giving, and by the best figures I could find, we’re likely at or below 5% here. (He’s said to be worth around $400 million, although that figure appears to be a little outdated—especially since he may or may not have sold a shitload of user data to AI companies earlier this year.) Which, fine, that’s still millions of dollars going to charity, and that’s objectively a good thing. But also: if my wife and I gave that percentage of our income, it wouldn’t even be enough money to get a tax deduction for it. So it’s worth mentioning that just for scale. Contextually, Matt’s donating at below the standard deduction level for somebody of his net worth. (And, most likely, enjoying significant tax benefits for it.) Anyway, no matter which way you look at it, that’s all weird, but it doesn’t even really matter in the case of this larger discussion. It has major “oh yeah? Well would a bad guy do THIS?” energy. You know…the sort of thing actual good guys don’t usually have to do. Almost like Matt was trying to distract from something… Matt goes nuclear The next move, and most recent development in this story, is still shocking to me. I think it should be shocking, and deeply disturbing, to any observer. WordPress.org banned WP Engine sites from accessing the plugin repository. No more doing anything with plugins via the WordPress admin area. No installing, no updating. Not if you’re on WP Engine/Flywheel. There are many layers to this. First: again, this is the .org arm of WordPress enforcing this brutal new edict. The Organization, or Foundation, or whatever, is not supposed to be controlled solely by an oligarch who can bend it to their own will, to directly benefit their own interests. It’s supposed to be agnostic. WordPress.org’s entire reason for existence, as I understood it (and I think as it was pitched to a lot of people), was explicitly to prevent things like this from happening. Second: not being able to update plugins is a massive deal. You could very well be exposing your site to security vulnerabilities if plugins don’t update (to say nothing of bugs). There are nonprofits, charities, government agencies, and public services that host on WP Engine, on top of countless businesses. All of those are just being thrown under the bus to serve one man’s whims. (Yes, it’s possible to manually update plugins, but nobody’s gonna do that. Certainly not the agencies and freelancers who oversee dozens or hundreds of sites on WP Engine.) This is bombing civilians. This is putting innocent bystanders in harm’s way. This is firing the Death Star. What Matt’s done is unforgivable, no matter how right he might have been at the beginning. To unleash harm on actual users of WordPress, indiscriminantly, solely over where they choose to host their sites, is an unconscionable, terroristic abuse of power. (In the middle of all this, Pressable, a separate host Automattic owns, started offering promos to help people migrate to them from WP Engine. That alone should be majorly headline-grabbing, but Matt’s abuses up to this point are so egregious it barely even registers on the scale.) You don’t hurt users because you’re beefing with their host. You don’t bomb civilians because they live near a criminal, you don’t shoot at innocent bystanders because a terrorist is hiding behind them, and you don’t fire the Death Star because you disagree with Alderaan’s government. It no longer matters what this was all about at that point, or whether you were originally right or not. You are irreversibly the bad guy now. It’s also worth calling out a side effect of this move, which may or may not have been deliberate: Matt’s actions have ensured his hosting companies are now the only WordPress hosts that can guarantee something like this will never happen to their users. I mean, he can just flip the switch at any time. He can change the rules whenever he wants to. So what company is safe? None. Except his. I hope I don’t need to go into how anti-competitive that is, all on its own, or what an egregious abuse of power it is to have put himself and his company in that position by using WordPress.org to do it. If Matt cares about WordPress, he should step down immediately. And if Matt won’t step down, he should be removed. The weapons Matt Mullenweg has wielded unilaterally in this war shouldn’t even exist, let alone be controlled by one person. I believe the ability to block an entire hosting provider from accessing the plugins repository is a power that nobody should have. If such unthinkably drastic measures could ever be justified, this case most certainly doesn’t seem extreme enough for that. Imagine if Microsoft got into a dispute with Apple, and decided to block npm for anyone using a Mac. Imagine if Apple got into a dispute with Google, and blocked all text messages from Android phones. Imagine if Google had a dispute with Amazon, and blocked all Amazon communications in Gmail. Or with Walmart, and prevented store locations from showing up on Google Maps. And imagine if one person at any one of those companies had the power to make that decision, unilaterally and without challenge. This is the scale of thing we’re talking about. This is the collateral damage Matt has unleashed on the WordPress community, and it’s not to anyone’s benefit except maybe Matt’s and his own companies’. (For now, anyway. We’ll see how it all shakes out; it seems pretty inevitable that a class action suit will follow and this all gets dragged into court.) Virtually no WordPress users are happy about this, no matter how they felt about WP Engine. Certainly, none benefit. No reasonable person could argue WordPress is in a better place today than it was a week ago, or is on a better path now than it was then. It’s less secure, less trustworthy, more volatile, and overall just not something anybody is as excited about as they were a week ago. People who spent the majority of their lives working on this software are leaving it. Professionals are looking at new tools to sell their clients. Major sites are considering changing platforms, when they wouldn’t have before. The neighborhood we all lived in just rocked, by a man who’s enjoyed unchecked power as the head of every branch of the current government, as it were. And he insists he’s doing the right thing by us for blowing up a whole bunch of our homes. (Forgive me, I know the metaphor is beleaguered by this point, but it seems apt.) Matt’s clearly willing to burn it all down to score a pyrrhic victory, and that’s not a power he or anybody else should ever have over any community, let alone one this size. Matt has to go. I don’t expect him to be removed from Automattic leadership (although I think others in leadership absolutely should be considering whether that’s the right move). But in any case: It’s clear that the blurry lines between WordPress.org and WordPress.com should be turned into unbreachable walls, with no one company on both sides, or able to exercise power over the Foundation and/or Organization. I don’t care about Automattic giving 5% to WordPress anymore. I want it to give up Matt’s unchecked, unilateral power. Because it’s clearer than ever he can’t be trusted with it. I'm Josh Collinsworth, a frontend developer, designer, teacher, and writer. I currently live in Kansas City, and work for Deno as a senior frontend developer. Buy me a coffee Send me a note about this post More about me Posted in: opinion web wordpress Back to top",
    "commentLink": "https://news.ycombinator.com/item?id=41676653",
    "commentBody": "If WordPress is to survive, Matt Mullenweg must be removed (joshcollinsworth.com)232 points by graeme 19 hours agohidepastfavorite194 comments mvkel 18 hours agoGiven the age and ubiquity of Wordpress, I am shocked at the relative immaturity of Matt's communication skills. He thinks the world has all the historical understanding and nuance of the situation. Why would they? This looks like a world record speedrun attempt (any%) at destroying a legacy. It's worth noting that WPEngine looked like this all the way back in 2011: https://web.archive.org/web/20110112043959/http://wpengine.c... They have never pretended to be anything else. Why now, Matt? reply atonse 18 hours agoparentExactly. This comes off as a totally unhinged and immature rant, unbecoming of the CEO of a company that likely has a 8 or 9 figure revenue. I didn’t know (or really care) about this battle, but I’ve always passively seen Matt as one of the insightful grandfathers of the blogging era, having insights from the observations from his perch. This blog post erased that. reply dawnerd 16 hours agorootparentSomething about these CEOs becoming or at least showing in public how unhinged they are. We’ve had quite a few in the last couple years. Almost like they think it’s cool. reply hackerbeat 9 hours agorootparentprev> I didn’t know (or really care) about this battle Then why comment on it? reply atonse 6 hours agorootparentI mentioned it’s because I have held Matt Mullenweg in higher regard as someone who’s been insightful about the evolution of the web. reply moralestapia 18 hours agorootparentprev>I’ve always passively seen Matt as one of the insightful grandfathers of the blogging era. I've always felt he was an asshole but could never ground that to a concrete observation. Now I'm certain of it. reply mvkel 17 hours agorootparentHe's not an asshole, he's just \"post-economic\" reply BlueTemplar 3 hours agorootparentWhat does this mean ? That he doesn't have to care about earning a living any more, and so he's acting like... an asshole ? reply _acco 18 hours agoparentprevThat 2011 snapshot actually makes the opposite point: the WordPress logo is prominently displayed next to the \"WP Engine\" title on the screenshot! It does look like they fixed just a few months later, though: https://web.archive.org/web/20111001085943/http://wpengine.c... (How fun to see the selling point \"Digg-Proof Scalability\") reply interestica 16 hours agorootparent> Digg-proof What's the modern soon-to-be-obsolete equivalent? reply philistine 16 hours agorootparentYour blog will be compatible with the blockchain! reply snowwrestler 16 hours agorootparentprevShitty-AI-crawler-proof reply chmod775 17 hours agoparentprevUnrelated, but that site is so much better than their current one. Now I finally know what they do and offer! reply Zamiel_Snawley 18 hours agoparentprevGiving Matt the benefit of the doubt, the answer to “Why now?” is that enough is enough. Why does Matt deserve the benefit of the doubt? Because his companies have been contributing to WordPress while WP Engine has not. Matt claims he has been privately discussing with WP Engine for ~18 months about their level of contribution. Automatic contributes the equivalent of 75 full time employees to WordPress and WP Engine contributes 1, despite the companies being comparable in size. Matt’s actions may have been bad for optics, but I do not fault him for using the resources at his disposal to correct what he sees as injustice. reply minimaxir 17 hours agorootparent> Matt’s actions may have been bad for optics, but I do not fault him for using the resources at his disposal to correct what he sees as injustice. Of course, there are consequences to using said resources inappropriately. reply glenstein 16 hours agorootparentNote the attempt to reframe bad actions as bad optics, to make it about perceptions instead of concrete actions. But perceptions are downstream from actions in a smoke/fire kind of way, and it shouldn't be used as a way to get out of answering for actions. reply Zamiel_Snawley 15 hours agorootparentNo. Put me on the record as saying his actions are just. I’m not reframing or deflecting anything, I agree with Matt’s actions. The public perception seems to be against him, and managing public perception is important. reply glenstein 6 hours agorootparentWell as I said before, perception is downstream from the actions themselves. And attempting to reroute the conversation away from actions towards the phenomenon of how they're being perceived is, despite your protestation to the contrary, a way of reframing the conversation that focuses on something other than the merits of what he chose to do. reply FireBeyond 14 hours agorootparentprevWhy, then, is he asking for WP Engine to pay licensing fees to Automattic, which is not the open source project, nor the Foundation, but his for-profit competitor. Ostensibly, Automattic has no ability to license WordPress (an open source project that it does not own). That sounds like an injustice to me. That sounds like someone who has been using Automattic and WP.com (and Pressable), WP.org, and the Foundation interchangeably, depending on what best fits his needs. reply glenstein 6 hours agorootparentElsewhere in this thread I'm seeing people say that Automattic is the exclusive commercial licensee with the ability to sub license. So it's possible that that's the answer there. reply lolinder 4 hours agorootparentThe code is GPL, that's not even possible. reply evanelias 3 hours agorootparentThe dispute is about trademarks and use of the brand name, so the relevant issue is a trademark license, not a software license. reply FireBeyond 3 hours agorootparentUp until a couple of days ago, when Matt retroactively changed (which is going to be hard to make stick) the trademark license explicitly permitted the use of \"WP\" by anyone and everyone. The trademark license also cannot prohibit nominative usage - that's protected. If you actually, factually offer WordPress hosting, you can say so, in those exact words. You may need to call out somewhere (and WPEngine does) \"WordPress is a trademark of the WPF\", but no license can prohibit you saying so. What they can't (but don't) say, is anything that implies that they are WordPress - which is exactly why Matt is trying to make some big deal in his head that \"my mom thought they were us\" (while ignoring the elephant in the room of \"Well, wordpress.COM isn't WordPress, either, it's just a licensee\", because of course, nobody could be confused by that). reply evanelias 2 hours agorootparentI was only responding to the statement about the GPL, which simply does not apply to this situation, since it is a trademark dispute and not a copyright dispute. I wasn't expressing an opinion one way or the other, regarding the validity of the trademark infringement. reply wslh 17 hours agorootparentprevI really wonder what that mean contributing to WordPress... from the cybersecurity point of view in 2024, there is/were no contributions: it is common to be hacked when you use Wordpress (e.g. [1]). [1] https://www.reddit.com/r/ProWordPress/comments/1cv15mt/would... reply claudiulodro 4 hours agorootparentWordPress is secure enough that whitehouse.gov runs on it and zero-day vendors pay $100,000+ if you have an exploit for the core WordPress software. It's not \"magically secure\" though -- you wouldn't say that AWS is insecure because some people set it up wrong or use bad integrations. reply wslh 22 minutes agorootparentI'm curious, do you have any information on how much whitehouse.gov spends on cybersecurity testing and customization? I imagine it's considerably more than the $100,000 you mentioned for a WordPress exploit. I work in this space and have experience with offensive security tests, including on Amazon itself. reply tempaccount420 1 hour agorootparentprevTime for a Rust rewrite? reply wslh 19 minutes agorootparentI don't think the first problem is about Rust or not but about having a security mindset to develop software. Even if it is PHP. reply dns_snek 11 hours agorootparentprevThat discussion is largely about themes and plugins. Severe vulnerabilities in WP Core are quite rare. reply yencabulator 2 hours agorootparentThemes and plugins are the severe vulnerability of WP Core. reply wslh 7 hours agorootparentprevI believe WP Core and plugins should not be viewed separately in terms of security. Plugins are omnipresent. Integrating security measures across both could create a more robust system overall. reply underseacables 18 hours agoparentprevIt really does feel like Matt is saying \"how dare you question me\" reply pilgrim0 18 hours agoparentprevAccording to his recent interview on Primeagen, he argues that WPEngine operations incur high costs to his company, due to the millions of installations consuming resources from Wordpress.org. And despite being a very large player, they contribute nothing back to the ecosystem. He argues that they even illegally modified code attributions (stripe plugin) which diverts millions from Automattic. The fact that it took, apparently, so long for him to take some action can be interpreted in his favor, because he tolerated a lot along the years. Add the fact that they had somewhat good relations before WPEngine being taken over by private equity. So this is not about trademark, trademark is the best weapon he has to fight back against a very bad neighbor. And being the sole trademark holder, Automattic can enforce it arbitrarily, as it sees fit. Taking side with WPEngine I think is not only rationally baseless, but also immoral, since they put nothing and only take, which is in the very opposite of what Matt represents, whether or not you like Wordpress. reply caseysoftware 16 hours agorootparent> He argues that they even illegally modified code attributions (stripe plugin) which diverts millions from Automattic. From reviewing the code, this is not \"attribution\" in the sense of \"here's who wrote this code\" but specifically and literally an partner code.. aka an affiliate code or what we'd normally call \"a setting\" As GPL'd software, they cannot prevent people from modifying this code and - if they do - Automattic's only counter is to complain about it. Further, since it's a revenue-generating code, it should be disclosed in the README, etc of the plugin and changeable via the Admin. It doesn't appear either is true. Ref: https://github.com/woocommerce/woocommerce-gateway-stripe/pu... reply sjs382 18 hours agorootparentprev> he argues that WPEngine operations incur high costs to his company To the foundation? Ok, then work with them to create a mirror and share the load. Even publicly shamed them into doing so. Do what's good for the community and product rather than engaging in this embarrassing spat. reply Atotalnoob 17 hours agorootparentprevActually no, trademark is a “use it or lose it” state, waiting so long will not be in their favor reply throwawaymaths 17 hours agorootparentIANAL, but If they are making trademark licensing deals with other companies, and periodically pestering the infringer without taking it fully legal I think that counts as \"using it\". A judge is likely to be lenient since legal costs are pretty bad and a PITA reply LordAtlas 1 hour agorootparentThe issue here is that Mullenweg is on record everywhere (including on HN) saying that the \"WP\" is not the trademark problem; it's \"Wordpress\" and \"WooCommerce\". But a cursory glance at Google results for \"Wordpress hosting\", \"woocommerce hosting\", or \"managed wordpress hosting\" will lead you to hundreds of results from a plethora of web hosting companies that have been doing this, many for more than a decade. The Wordpress Foundation (that owns the \"Wordpress\" trademark) has not taken any legal action against any of these companies for precisely the same use it's accusing WPEngine of. A judge could well rule that they have not defended their trademark and this claim holds no water. reply throwawaymaths 58 minutes agorootparentAgain, IANAL, but it's generally not the case that you are required to go after all infringers of your trademark (I imagine because that would be overly burdensome requirement, as someone nefarious could spin up even a blatantly offensive use in some remote town in Alaska for example and go \"ha-haw you failed to defend\"). You just have to not never defend it. reply jcranmer 16 hours agorootparentprevPart of the clusterfuck going on here is... a) the nonprofit owns a trademark b) gave an exclusive license with right to sublicense to a for-profit c) which appears to be run by the same person d) who is demanding of a competitor to sublicense the trademark e) paying the money to the for-profit, not the nonprofit f) when it's not clear that the competitor is infringing on the trademark in the first place. The litigation here is not likely to go Wordpress or Automattic's way, I think. reply throwawaymaths 11 hours agorootparentAccording to elsewhere the for profit had the trademark first, and then handed it over to the nonprofit while remaining a licensee. (Automattic founded 2005, WordPress foundation founded 2010) That's a very above board move. And it makes sense for the for profit to take on the legal aspects, because the extraordinary spend won't ruffle donors feathers. reply glenstein 6 hours agorootparentI'm not sure I understand the reasoning here. Either they are licensee or they're not. The history leading up to that doesn't convey any special rights or exceptions. And referring to that transfer as \"very above board\" makes it sound like you're talking about the same thing as the commenter above when in fact you're talking about different things. Because however gracious you find that decision to be, that's a different subject than whether it's above board to, subsequently commingle those responsibilities. And whether or not you feel it's practical, in some sense, it doesn't seem to have anything to do with anything. Either Automatic has The authority to collect licensing payments on behalf of WordPress or they don't and that should be reflected in a charter or something, somewhere. I don't think referencing the history of Automatic previously owning the trademark has anything to do with anything in this context. reply throwawaymaths 26 minutes agorootparentThere's pretty broad license for for-profits to share IP with non-profits, e.g. Novartis + GNF, or generally speaking, any university and any spinoff (I refuse to call them startups) created by professors thereof. If you think what automattic is doing seems legally sketchy, you may want to recalibrate your expectation of what sketchy is and just how much the industrial complex of the US would simply not work (for better or worse -- i for one think it would be better, fwiw) if it adhered to your standards. reply underseacables 17 hours agorootparentprevBut who owns the trademark? The for-profit company, or the nonprofit foundation? reply patmcc 16 hours agorootparentThe foundation owns it - https://tsdr.uspto.gov/#caseNumber=78826734&caseSearchType=U... A claim I've heard is that the for-profit company has the exclusive commercial license (along with the right to grant sub-licenses). reply ValentineC 14 hours agorootparent> A claim I've heard is that the for-profit company has the exclusive commercial license (along with the right to grant sub-licenses). That sounds about right, and is what's written on the WordPress Foundation's trademark page [1]: > If you would like to use the WordPress trademark commercially, please contact Automattic, they have the exclusive license. Their only sub-licensee is Newfold. I read something different [2] while reading up on this whole debacle: > The WordPress Foundation was launched in January 2010. Automattic transferred the trademarks later that year in September. As part of the transfer, Automattic was granted use of WordPress for WordPress.com, but not for any future domains. Matt was granted a license for WordPress.org and WordPress.net. I wonder if something changed along the way. [1] https://wordpressfoundation.org/trademark-policy/ [2] https://wordpress.org/book/2015/11/the-wordpress-foundation/ reply nickff 17 hours agorootparentprevWordpress is using their trademark though… do you mean to say that you must enforce a trademark to keep it? That’s different, and a subject of some controversy; I’m not sure that it’s been decided conclusively. Please correct me if I’m wrong here (in any way). reply mvkel 17 hours agorootparentIt's been decided conclusively, but it is also relatively rare in a practical sense. Trademark law requires the owner to protect their trademark to avoid dilution or genericide, which occurs when a trademark becomes so common it loses its distinctiveness (like what happened with \"Aspirin\" or \"Escalator\"). Not a lawyer, but talked about it at length with a trademark attorney when having to defend over the years. It was conveyed that if we aren't willing to legally defend a mark, we could potentially lose it. reply nickff 17 hours agorootparentDo you think WordPress is diluted? It definitely doesn’t seem to generically refer to blogging or website building software. It seems to me that even WP Engine isn’t diluting it, they’re hijacking it. reply mvkel 17 hours agorootparentAgreed, but that goes back to defensibility. WP Engine has been doing their thing since 2011 without a peep from Matt. He should have been issuing cease and desists, etc. back then. reply blackeyeblitzar 16 hours agorootparentMaybe it’s that WP Engine operates differently now. They got a massive investment from Silver Lake, a terrible private equity firm with a track record of doing evil things. They probably have changed how they support Wordpress (the open source project), how they pollute the ecosystem, how they make it harder to leave, etc. In other words, they’re free riding on Matt’s creation and extracting all they can from it. reply mvkel 4 hours agorootparentAs is their right, given the terms. reply FireBeyond 14 hours agorootparentprevOh yes, Matt's all about how evil Private Equity is and how they leech from communities and add zero value... It's an interesting take, given that the three board members of the WordPress Foundation are Matt and a ... managing partner of a PE firm (the third is a retired coder, I believe). reply FireBeyond 14 hours agorootparentprevYou need to look up nominative usage. \"We offer WordPress hosting\" is a perfectly legal thing for people to say if they actually offer WordPress hosting, and no amount of trademarks can prevent that. This is specifically called out to try to avoid trademark fuckery. Sure, everyone refers to the \"Big Game\" versus the \"Super Bowl\", but that's largely because the NFL can afford more lawyers than they can, and it's not worth the fight. reply mvkel 17 hours agorootparentprevExactly. Statute of limitations is a factor here reply chuckadams 18 hours agorootparentprev> He argues that they even illegally modified code attributions (stripe plugin) which diverts millions from Automattic. Which turned out to be blatantly false, quelle surprise I'm currently asking @photomatt elsewhere what his plans are to help others lift the load from wp.org by way of supporting alternate plugin/theme repositories. I'll keep you posted. reply ydlr 17 hours agorootparentMaybe banning wpengine customers from the official repo is exactly the push the community needs to create alternatives. reply karmajunkie 18 hours agorootparentprevi think the point of TFA was that those allegations are a gross distortion of the record, and MM’s ultimate actions are grossly inappropriate and harm the community he is pretending to stand up for. reply _bent 17 hours agoprevFrom Mullenwegs personal website (https://ma.tt) > Afterward, I also privately shared with [ThePrimeagen] the cell phone for Heather Brunner, the WP Engine CEO, so she can hop on or debate these points. As far as I’ve heard she hasn’t responded. Why is WP Engine scared of talking to journalists live? this is not normal. reply mtndew4brkfst 14 hours agoparentCalling Prime a journalist even by implication is hysterically funny. reply djbusby 16 hours agoparentprevAt least give Heather a few days to respond/engage, given the big-ass-drama that Matt created. reply aithrowawaycomm 11 hours agorootparentShe shouldn't be engaging with or responding to ThePrimeagen at all, he is a jackass who solely appeals to ignorant young developers: https://youtube.com/@ThePrimeagen/videos He is not even close to a journalist, he is a dumb tech bro. I hope Heather blocked his number. This is insanely scummy and stupid behavior from Mullenweg. reply BlueTemplar 3 hours agorootparentWith nearly half a million followers, it does make him a journalist in the sense of \"wielder of the 4th power\". (But perhaps this only makes matters worse, depending on how he uses that power.) reply talldayo 17 hours agoparentprevThis one really rubbed me the wrong way: https://ma.tt/2024/09/charitable-contributions/ > I have kept my personal philanthropy private until now. > This is something I’ve tried to keep quiet, because true philanthropy isn’t about recognition. ... > If Lee Wittlinger, who controls Silver Lake’s investments in the WordPress ecosystem, or Heather Brunner, the CEO of WP Engine, would like to publish their charitable contributions over the past 12 years, they are welcome to do so. Is he trying to avoid turning charity into a pissing contest, or is he trying to use his affluence to shame a competitor? This guy literally can't keep his story straight over the course of a single blog post. reply patmcc 16 hours agorootparentI'm also very curious, given his other statements and how he seems to conflate actions of the foundation, automattic, his own person, and wordpress.org, whether these contributions are his or one of theirs. And to what causes, at that; can you give to your own foundation, I wonder? reply aimazon 16 hours agorootparentThe contributions are legit. Despite his claim to have never spoken publicly about his contributions before, he has spoken about them (when it was convenient to make a point about his moral superiority). He donated hundreds of thousands per year to the Apache Foundation, for example. Additionally the WordPress Foundation’s financials are public, they don’t receive many donations (tens of thousands per year). reply patmcc 16 hours agorootparentHuh, that's very interesting - https://projects.propublica.org/nonprofits/organizations/205... I though part of Matt's point was that Automattic \"gave back\" to the foundation? reply aimazon 16 hours agorootparentThe WordPress Foundation is irrelevant to WordPress itself, it just holds events. The WordPress project is owned by Matt and that’s what Automattic donates to (in the form of Automattic employees working roughly 4k hours per week on the WordPress project). There’s also the money spent on running WordPress.org by Automattic but that’s entirely opaque (nobody knows how much it is, although it’s claimed to be millions). reply Mistletoe 17 hours agoparentprevThe thing is that none of this matters. This is what happens when you get too zoomed in on the tiny little island you live on and make a total ass of yourself. reply QuantumGood 15 hours agorootparentA sad but seemingly near-perfect example of the elitism / privilege that comes from living in a bubble and not knowing it. reply interestica 16 hours agoparentprevWhat if Trinidad changed residency requirements for its TLD? ... reply iambateman 17 hours agoprevI hope, for the good of the community, that Matt will choose either nonprofit leader or tech CEO. It’s become clear that both roles cannot live within one person. If he were just the leader of the WordPress foundation, this whole thing would just be an embarrassing PR failure. As it is, I wonder if his actions will rise to the level of criminal. After watching his interview with The Primeagen, it seems like he is mentally wearing the clothes of a righteous prophet…the misunderstood advocate of a disrespected organization. Unfortunately, he’s ignoring the fact that he invested in WPE years ago, is CEO of a direct competitor, has publicly said he hopes WPE loses billions of dollars as a result, apparently has no proof in writing, and is pulling thousands of innocent developers into his petulant crusade. reply blinded 17 hours agoparent\"apparently has no proof in writing\" sounds like there are emails and what not. But those should come out as the lawsuit progresses. Just because he didn't make any public now doesn't mean they don't exist. reply iambateman 17 hours agorootparentWe will see. He was asked in the interview twice and he _did_ respond and it wasn’t good. He pulled his calendar out and read out dates he met with their CEO. reply blinded 16 hours agorootparentSomething for discovery imo (if it gets that far). He seemed put off that wp engine would release his texts, would be unfair of him to do the same? I'm not sure. reply cm2012 18 hours agoprevVery informative article. Matt is definitely in the wrong here. I also loved how this was formatted, it was quite long but easy to read with a nice font. reply brailsafe 15 hours agoprevHmm. Maybe this is a bit of an outlandish take, but although his decisions do seem at least superficially sus, I have a hard time agreeing with Josh's take. He chooses to make a lot of highly agreeable comparisons, but to me it seems more like a city (or some level) of government severely turning up the temperature on a particularly egregious contingent of landlords, landlord, or demographic, who've been abusing the system we've all been chill with until it's just not cool anymore. You could crank up property tax by double or triple the next year, and not offer the ability to vote on the issue of zoning anymore, or you could do the thing that no politician has the balls or the power to do and fundamentally change the system in a way that takes those issues off the table entirely, making the whole system more equitable in the future. The repercussions could be dire for some people, but it is what it would take to give a giant fuck you to the people who hold the reins. If the landlords can always hold the poor vulnerable freelancers that live in their basement suite over the heads of anyone who could otherwise theoretically change the rules, nothing will ever really change, and the people who've been along for the profitable ride won't want it to. But if you don't do it, you might risk your future hypothetical economy. Now, I know that's all a bit of a reach, but it's hard for me to not think asking for Matt to be removed for this reason is just like all those people who ostensibly would want public transit to be funded for all those other people who can't afford cars, until they finally got around to ripping up the street you use to drive to work every day and jacked up your taxes by $50. But, all that said, I've also never liked WordPress at all and don't have a dog in the fight, this is just a thought experiment. However, if I had to move because my landlord eventually got screwed for not reporting my rent on their income tax, I'd be like \"well sometimes that happens, was nice while it lasted but my place was a shithole, they weren't competing fairly, and they were constantly showing up to city council meetings trying to block a mid-rise from going up while I was at work paying their mortgage\". reply salesynerd 17 hours agoprevThe general consensus of commentators seems to be that Matt is wrong in the way he approached this matter. Going by the wisdom of the crowds, maybe that's true. However, my question is this: has WPE given a factual rebuttal to Matt's claims? Especially, considering that their entire business is dependent on WordPress? I am concerned that in the eagerness to judge Matt's conflict of interest, we should not throw out the baby with the bath water. reply aimazon 17 hours agoparentMatt hasn’t made any claims that need a rebuttal. Matt’s claims are factually correct. The issue is that they’re immaterial. Matt has demanded that WPEngine pay 8% of their revenue to Matt’s company (Automattic). Matt has retroactively changed the terms of use of the WordPress trademark to create a violation by WPEngine. Matt has engineered the situation, we can’t separate the claims from the conduct because they’re one and the same. Matt’s position is (ostensibly) based on his hard line views about the moral obligation to contribute created through the use of open-source. The trademark sideshow is based on Matt’s understanding that a moral argument isn’t going to convince a private equity backed company to spend money they don’t need to spend. Matt believes WPEngine has a moral obligation to contribute and the trademark licensing fee is the easiest tool he has to force action. Matt is making a moral argument. WPEngine don’t care because they’re driven by money not morals. reply salesynerd 2 hours agorootparentI have, often, come across comments on HN threads that corporations that are driven only by money are evil. For example, many threads with Google, Facebook, et al have expressed such sentiments. If we agree that to be true, then WPE should also be considered evil, shouldn't it? Then, why so much defence for them and all vitriol for Matt? And, if we accept that WPE are right to focus only on the legality of their action, then should we not apply the same logic to all corporations when they focus on maximizing their revenues and profits? reply Zamiel_Snawley 16 hours agorootparentprevI think those final 8% demands we’ve heard about have been after months of stalled conversations with WP Engine. Given the lack of reliable information right now, I’m going to believe the individual that has a decades-long track record indicating that they care about open source over the private company that is legally obligated to pursue profit as its only objective. reply jcranmer 16 hours agorootparentI do not think it appropriate to believe the individual who is in two roles and is trying to use his position in one role to benefit his other role, especially while trying to muddy in which role he is acting. reply aimazon 16 hours agorootparentprevWhat are you choosing to believe? The 8% isn’t disputed. Matt has acknowledged it is true. Matt has acknowledged his actions are because he believes WPEngine are not fulfilling their moral obligation. The facts are settled, the question is whether you side with Matt’s belief about WPEngine’s obligations and how you feel about Matt’s actions (in the context of Matt operating a competitor). reply Zamiel_Snawley 15 hours agorootparentI’m saying that his 8% demand sounds like the last line in a long conversation with an interlocutor acting in bad faith, trying to slow walk the inevitable demise of the relationship. If WP Engine had acted in good faith, Matt wouldn’t have had to come up with terms unilaterally. reply hn_throwaway_99 16 hours agorootparentprev> I think those final 8% demands we’ve heard about have been after months of stalled conversations with WP Engine. Who gives a shit, it doesn't matter. Why would he think WP Engine would pay anything they're not contractually obligated to pay? This has all played out similarly elsewhere, e.g. to the point that some companies have started to carve out a new types of licensing so that all of the \"open source revenue\" doesn't just get vacuumed up by the big clouds/hosting providers (e.g. see the \"Fair Source\" movement being promoted by Elastic and others). Matt could have gone down that route. I could easily imagine a million ways he could have handled this better and gotten the community on his side. Instead he's acting like a collosal asshole. reply salesynerd 2 hours agorootparentI believe that we should give a shit, and it does matter. The \"balkanization\" of open-source licensing into more restrictive versions is ultimately going to adversely impact all of us. And, if Matt had not chosen to be an asshole, would this issue have gotten the prominence that it has got? Also, WPE could easily have taken the wind out of Matt's sails by declaring their (direct or indirect) commercial support for WordPress.org while reducing to pay money to Automattic. As far as I know, they have chosen to not do so. For all I know, Matt may lose the battle; but, open source would lose the war if companies and individuals continue to use the kind of arguments that WPE and it's defenders are making - that, they are legally not obligated to care two bits about the open source software on which their entire businesses are built, leave apart what is moral. reply Zamiel_Snawley 15 hours agorootparentprevThat is exactly why he cut them off. They aren’t contractually obligated to pay. The only “contract” is the implied social contract of building your company on open source. If they want to play hardball about what’s required instead of acting generously, like Matt has done for decades, then they are getting their just deserts. Matt isn’t obligated to be nice to dick heads. WordPress.org isn’t obligated to provide service for free. WP Engine decided that they would only do what’s good for them. Fine. If they piss in the pool, they can’t be mad when everyone else gets out. It is irrational for WordPress to continue acting like there isn’t an extractive entity in their midst. reply glenstein 6 hours agorootparent>If they want to play hardball about what’s required instead of acting generously, like Matt has done for decades, then they are getting their just deserts. I can't help but notice that once it gets to the question of whether there's any actual authority to demand a licensing fee, the conversation stops being about what is or isn't legal, who is authorized to do what, considerations of proportionality or collateral damage or any of that, and just start slipping into this mode of speaking like mobsters from the 1920s. If that's the cadence you find yourself slipping into it might be an indicator of whether you're the good guy. reply blackeyeblitzar 16 hours agorootparentprevAre contracts everything? Matt created Wordpress. I think he’s more deserving of the spoils than some company whose owner is Silver Lake, one of the most evil PE firms. Reminder: https://www.wired.com/2011/06/skype-silver-lake-evil/ reply nsonha 15 hours agorootparentAccept it, it is the deal with opensource. It's also the basis that people should be using when debating OSS versus other models. People should not be making business or policies or economic decisions based on some unenforceable honor system reply Zamiel_Snawley 15 hours agorootparentAgreed, WP Engine shouldn’t have left their customers wellbeing up to the whims of an organization they were antagonizing. WP Engine was banking on the free stuff from Wordpress.org, and they got burned because they bit the hand that was feeding them. reply hn_throwaway_99 12 hours agorootparentThat's the entire reason people are so pissed, and what TFA is about. WordPress.org is supposed to be part of the foundation, one that has a charitable purpose to support the WordPress community. It's fine to argue WP Engine was a bad community member, but cutting off access to WPE customers (after demanding payment to Automattic) looks exactly like extortion. Matt has shown he simply can't be trusted to keep his roles as head of WordPress Foundation and Automattic CEO independent. reply glenstein 6 hours agorootparentprev>shouldn’t have left their customers wellbeing up to the whims of an organization they were antagonizing. The point of the article is that it's precisely these actions that have damaged the integrity of WordPress for everybody, because we can now no longer look at WordPress as having stable stewardship, but as something ready to whimsically descend into unpredictable retaliatory actions, without any rhyme or reason or structure. Once you start talking that way, it seems to me you've completely lost sight of what it is to be the healthy steward of a norms driven foundation. The reason you work out things in charters, and in terms of service and so on is precisely to avoid situations like this, where there are spirals of escalation all hinging on subjective interpretations of everything. reply FireBeyond 3 hours agorootparentprevMatt hates PE firms so much, the only other active member of the WordPress Foundation board, as appointed by Matt is... ... the Managing Partner of one of those evil PE firms. reply hn_throwaway_99 16 hours agorootparentprevAgain, who gives a shit? I'm in no way saying WP Engine is some sort of angelic organization, and I don't care. All I see is childlike behavior from someone who definitely should not be in control of both Automattic and the WordPress Foundation, and my guess is that if the board doesn't force his ouster that WPF will have serious issues with the IRS. Also, the whole point of open source is that you don't \"own it\" after it's open sourced. If you don't like those terms, license them under different ones, which exactly what the whole recent \"Fair Source\" movement is about and what other companies like Sentry have handled in a much more dignified fashion. reply salesynerd 2 hours agorootparentMaybe, the newcomers have learnt the lessons from the travails of the old open source projects? That doesn't mean that the oldies should just suck it and keep quiet. reply cscurmudgeon 16 hours agorootparentprev> the individual The same guy running a for profit company? reply glenstein 16 hours agoparentprev>However, my question is this: has WPE given a factual rebuttal to Matt's claims? I mean... The article talks about this extensively. And the article is my first exposure to the issue but presumably it's not the first place where these points have been presented. The authority or obligation to give back isn't a legal one, the authority to demand 8% raises all kinds of conflict of interest issues and appears dubious if not outright illegal, the global message posted to admin dashboards was an abuse of power, and the banning from using plugins doesn't even pretend to have a legitimate pretense. Apparently part of the backstory here is there's a dizzying context, and there might be some subjectivity involved in surfacing these as the pertinent issues, but I wasn't left with the impression that the ball is in WPE's court to explain any kind of smoking gun that hasn't been accounted for by the discussion in the article. reply timeon 10 hours agorootparent> The article talks about this extensively. The article is trying to manufacture consent with vague authority: \"Most reasonable and knowledgeable people seem to share this opinion.\" While it is someone who is not impartial. He claimed that he worked in WPE before and than 'extensively' writes why it shouldn't matter - without telling anything. I'm not saying that what 'Matt' did is OK. Seems to me no party here is in right. But that is not my point. My point is that these kind of articles - especially lengthy vague ones - are just increasing the drama. reply glenstein 7 hours agorootparentI understand that part of hn policy is that comment through should be getting more nuanced over time, but this seems like a bunch of zoomed out fuzziness that barely touches on any details. The author makes all kinds of specific arguments that don't have anything to do with consensus, and the structure of the arguments is grounded on the inherent rightness or wrongness of interpretation of various rules, the existence are non-existent of copyright, the disproportionate and escalatory choices Matt has made, a whole host of specific arguments that don't have anything to do with where the consensus falls. They do mention that Matt has appeared to have turned many in the community against him, but the arguments are pretty freestanding even if you want to set that aside, and there's also another interpretation other than manufacturing consent which is simply that it's a legitimate observation about what's really happening. I can't say for sure, but there's so much more going on here that's more specific to the issue of right and wrong within zooming out and saying \"gosh this sure is a lot of drama\". I think if that's the level at what you're engaging in the conversation it's just making everything fuzzier. reply minimaxir 18 hours agoprevMany techies hate it when CEOs are boring and only say comms that are vetted by their companies PR and legal teams... But this whole debacle is what happens when a CEO doesn't do that. There can be unexpected, company-ending results. reply groby_b 18 hours agoparentThere's... a bit of a spectrum from completely blandified corporate comms to what Matt's doing there. It is perfectly possible to have a distinct voice without setting the house on fire. There is a middle ground. reply Aerroon 18 hours agorootparentI don't really think so. CEOs are still people. Eventually they're going to mess up and since they are constantly scrutinized it will be blown up. reply Crosseye_Jack 16 hours agorootparentThere are mitigating techniques to work around that. Another set of eyes, not to rewrite it into safe corporate speech, but just as a safety guard to say “hold up there bud… Stop and count to 10” Talking about “counting to 10”, you don’t hit the publish button right away, save it as a draft and leave it a few hours/over night and come back to it when your not as fired up. Hitting publish when your fired up is rarely a good thing. If you want to vent, swear and shout. Write that angry post, but then select all and delete it, take a breath, and start again. 9 times out of 10 you are going to be a lot less fired up the second time around. Granted the last two require some self control, but that’s what the first one is for. As you said, CEOs are human too, and as humans we all need a helping hand every once and awhile, nothing wrong about that. reply deathanatos 17 hours agorootparentprevThere are plenty of people who have their own voice that is between \"soulless corporation\" and \"watch it all burn\"… reply blinded 17 hours agoprevIn the Prime interview it sounded like there were attempts in the past to ensure the trademark was not violated and he expressed the feeling like they were stringing him along. If he had only hired lawyers, not made that talk with his accusations, and let the lawyers handle it would more people be on his side? reply glenstein 16 hours agoparentI think that was a major take away from this article. There was a right way to navigate it, but by navigating it in a harmful way you also become an actor capable of, and accountable for, the way you've chosen to respond. reply Zamiel_Snawley 16 hours agoparentprevI think it probably would have been quieter, but people would be accusing him of not being transparent instead. I think it’s kind of nuts to not give Matt the benefit of the doubt while more information comes out. He is the party with a past that demonstrates good behavior and intentions. reply philistine 16 hours agorootparentHe obliterated his past when he fired the Death Star (blocking access to the repository for all WP Engine sites). That's the whole point of the article! reply Zamiel_Snawley 15 hours agorootparentWP Engine, a for profit company, should be providing what their customers need, not relying on free services from a foundation for required functionality. Especially if they have a fraught relationship with the provider of those free services. System76 provides the PopOS repos that are based on Ubuntu. They don’t freeload. Canonical provides the Ubuntu repos that are based on Debian. They don’t freeload. WP Engine is freeloading, no doubt about it. reply sleepybrett 14 hours agorootparentWere they ever asked to run a mirror of the plugin repo for use with their customers? If the bandwidth/infra costs to support all of WPEngines customers were so much, then they must have, right? If they were asked and refused that's certainly one thing. If they were never asked, and then when they refused the demands for money.. then it looks like matt was just looking for a wrench to hit them with and this one came to mind. reply timeon 10 hours agorootparent> Were they ever asked to run a mirror of the plugin repo for use with their customers? No need to ask someone with common sense who is already doing it. reply FireBeyond 3 hours agorootparentprev> Canonical provides the Ubuntu repos that are based on Debian. They don’t freeload. At least for Ubuntu, the packages they distribute generally don't have the same checksum as Debian, so they're not the same packages (at least as binaries). Also, \"wordpress.org\" is hard-coded as the plugin repo source all over WordPress. reply youngtaff 9 hours agorootparentprev> He is the party with a past that demonstrates good behavior and intentions. Go talk to other people who've had dealing with Matt and you’ll find plenty who don’t agree with that statement reply mrinfinitiesx 18 hours agoprevI watched the video/stream he did, and have very little good to say about it. while he may be right about it, cool; trademark infringement, yeah, they didn't give back, there's a feud; I'm sure it goes deep. He disabled millions of wordpress sites from being able to update/access things. Plugins.. functionality.. Sure, they don't deserve to get free API access and all that; none of that matters. What about non-profits for animal shelters, programs like st judes, things where livelihoods depend on it and they don't even know what an API or domain name is let alone what all this stuff is about and their whole stream of operations comes crumbling down because they paid somebody to set it all up for them and all they know to do is long in to wp-admin and press 'update' and make blog posts and check their 'payments' etc and modify/add things like their woo commerce plugins? We're smart, we know what all this means, a lot of people I come across in the real world can utilize wordpress because it's easy for them, but if I explain in depth how things work they look at me like I'm speaking a foreign language. He doesn't care. I don't need that question answered. I already know. I don't have an opinion on it, but when 75% of the internet is running wordpress, have some tact. reply aphroz 18 hours agoparentIf they don't know, now they know reply edm0nd 18 hours agorootparentThe Notorious B.I.G. and all his homies hate WordPress reply AlienRobot 8 hours agoparentprevWell, first, your website won't explode just because you can't update the plugins. If it's running just fine now, it can probably keep running just fine for months. Second, WP Engine can create their own repository of plugins to update their customers' plugins instead of relying on Wordpress.org. Third, WP Engine customers can just leave to another host if WP Engine can't actually provide the service they are selling. reply timeon 10 hours agoparentprevAs far as I know any WP instance is able to upload plugins/themes. reply blinded 17 hours agoprevAuthor assumes contribution data from wp engine is distorted simply because they don't agree with Matt's communication / response from the trademark dispute. That doesn't seem to fair? If they worked there for that long perhaps they can provide insight into the upstreaming of issues strategy or if it was an org focus or an afterthought? reply ipaddr 16 hours agoparentDistorted because both companies are very different and putting resources in helps Automaticc in different ways. Having 47 people working on specific things is a strategic thing where the other company is just hosting. reply blinded 16 hours agorootparentFair point. reply tolerance 16 hours agoprevThis man is having a breakdown disguised as a legal battle. reply deepfriedchokes 13 hours agoparentThis definitely screams mental health. reply r721 10 hours agoprev@photomatt answers to some questions in this subthread: https://news.ycombinator.com/item?id=41676411 reply daft_pink 17 hours agoprevYou would think that a company as invested in WordPress as WP engine is would invest more in the community and contribute to the software and also want to contribute to get features that would help it save money. reply bastawhiz 17 hours agoparentDo all the WordPress plugins that they maintain not count? reply youngtaff 9 hours agorootparentOr the sponsorships of WordCamp? reply hk1337 17 hours agoprevLet it die. reply b0ner_t0ner 7 hours agoparentAll the cool PHP kids use Laravel anyway. reply ergonaught 16 hours agoprevMatt has effectively threatened the businesses of companies whose web presence is hosted via WPE/FW. He’s got to go. reply underseacables 17 hours agoprevThis was a really good article, something that jumped out at me was that there might be a serious legal issue with the IRS. WordPress, the for-profit company, may be too intertwined with WordPress the 501c3 foundation. I'm not a lawyer, but a nonprofit is supposed to be very careful about how it operates. Matt's post on wordpress.org is clearly crossing the line by blending the for-profit company, with the nonprofit foundation. Perhaps it's not illegal, but it is certainly unethical. reply sublinear 15 hours agoprevIf wordpress is to survive it needs to not be wordpress reply hackerbeat 6 hours agoprevThis guy clearly doesn’t know what he’s talking about. reply cachedthing0 18 hours agoprevThe word 'press' in wordpress suddenly got another meaning, what a mattfia.... reply halfjoking 15 hours agoparentOwn the community hate, and change the name to WorstPress? reply keane 15 hours agoprevOn this issue, there's been a lot of discussion along the lines of \"the trademark for an open-source project should work the way I prefer which is…\" or \"if I was in a decision making position I would simply…\" or \"in a perfect world…\". Others, like this post, unwisely include appeal to motive. It would be better for us to stick to discussion that is able to limit itself to the substance of both parties' claims. The first thing I think cannot be neglected to be mentioned in posts about the dispute is that (1) Matt created the project (yes, a fork counts), (2) his friend coined the name, (3) Matt's company originally registered the trademark. Then (4) Matt's company donated the mark to a foundation to make it widely available for noncommercial use while they retained the exclusive commercial license to the mark. No mention of this in this presentation. To be fair to commentators, part of the trickiness surrounding this dispute is an old issue regarding open source projects: do the open source software licenses imply a trademark license? The answer is generally understood to be: no. Having a license to software does not grant you a license to a trademark. For more on this I found illuminating the 2009 article in the International Free and Open Source Software Law Review by Tiki Dare JD (Director of Trademarks at Sun Microsystems, Inc.) and Harvey Anderson JD (General Counsel of the Mozilla Corporation) titled \"Passport Without A Visa: Open Source Software Licensing and Trademarks\": https://www.jolts.world/index.php/jolts/article/view/11/37 As one is not given a license to the trademark, a common understanding is that one can: – limit one's use of the trademark to nominative or descriptive fair use (A) – use the mark under supplemental guidelines from the trademark owner (B) – acquire a dedicated license to the trademark (C) At https://wpengine.com/plans (take your screenshots now) they have titled services they offer simply \"Core WordPress\", \"Essential WordPress\", and \"Enterprise WordPress\". It could be claimed this branding exceeds nominative use. It is far beyond the mentioned descriptive use of a \"managed WordPress hosting company\". If this branding exceeds fair use, it needs to comply with justifications (B) or (C). It very clearly does not comply with the published guidelines, both before and after recent modifications, that read \"All other WordPress-related businesses or projects can use the WordPress name and logo to refer to and explain their services, but they cannot use them as part of a product, project, service, domain name, or company name…\". You can also see examples of use (current/cached and perhaps somewhat inadvertent) of \"WordPress Engine\" itself at https://www.google.com/search?q=site%3Awpengine.com+%22wordp... Many commentators seem hung up on the fact that using the letters 'WP' was and remains an allowed practice according to (B). However, with regard to any trademark guidelines it could safely be assumed that a mark owner is not suggesting that one may use protected marks in ways that cause confusion as this is counter to the purpose of trademarks. Commentators are likewise hung up on the idea that the guidelines were subject to change or are despairing about the recent edits that clarified that the use of 'WP' under (B) must avoid uses that could imply the product or service were synonymous with WordPress itself. For similar open source software trademark guidelines and as a useful point of comparison, I think commentators should take a look at Red Hat's public guidelines, which explicitly remind users that guidelines like these can be changed: https://www.redhat.com/en/about/trademark-guidelines-and-pol... Other commentators are focusing on the length WP Engine had 'WP' in their name. With use since 2010, some have implied that a statute of limitations has passed but the Lanham Act has no such time limit. These commentators don't seem to be considering Automattic's confusion claim. WP Engine has claimed in their materials that they are \"The most trusted WordPress platform\" and \"The Most Trusted WordPress Tech Company\". 'Trusted' can be read with the meaning 'seen as trustworthy' rather than the meaning 'utilized' which could be found to be creating confusion. The most [seen as trustworthy] platform would presumably be the project itself (in an expansive understanding of 'platform' that a non-technical user might perceive). If CNET started calling itself \"The Most Trusted Firefox Source\" I would expect The Mozilla Foundation to ask them to stop. Many commentators appear to be suggesting there should be no enforcement of the WordPress mark, which seems an unusual position, or otherwise seem to take issue with Automattic's original trademark registration in the first place. Regardless, if WP Engine's uses of the marks exceeded rationales (A) and (B), they needed a license. This is what Matt was seeking, even allowing such a license to be paid in kind. At this point, a court will likely decide if their use exceeded (A) and (B). Calling for Matt to have a role change is one thing but to likely libel Matt with the term extortion, a criminal offense, especially after only moments before admitting \"maybe there's validity there\" (regarding infringement of Automattic's WooCommerce mark) is absolutely reckless and it's disappointing to see this unserious blog post promoted here. To see uncareful defamation coming from someone who made their living for many years off the software their target of ire created is especially bleak. reply jaredthirsk 8 hours agoparentWe seem to live in an age of narrative over strict substance, unfortunately, and I think the author captured the narrative quite well, with a weakness or two on detail that you pointed out, so I appreciate your attempt to elevate the precision of the discussion. I don't think people using the word extortion understand how it is defined in a legal sense and the gravity of the criminality. The word extortion as people use it could be replaced with 'threat' or 'ultimatum', with some sentiment of unethicality or unfairness added back in. Legally, it may be fine for Matt to make ultimatums: \"contribute in one of the ways I demand, or my free WordPress.org API that I provide is no longer accesible to you\", but as a community steward, it seems unfair to the many users of WPE who were not given that ultimatum with any notice (initially), or enough notice after the \"reprieve\" (Oct 1 still isn't enough notice.) The only argument of Matt's I find compelling is that WPE's plan names look like a potential misuse of the WordPress mark. If I was WPE, that would be the only thing I would be worried about, and consider changing (though to do it right now might look like an admission of guilt.) If I was a judge, I would consider slapping WPE's wrist on that point, and considering WPE at most 1% at fault in this entire debacle based on facts available. I don't find the overall trademark confusion argument compelling (especially in light of WordPress.org vs WordPress.com confusion, and WordPress.org as Foundation vs WordPress.org as Matt the CEO of a competitor to WPE confusion), though if Matt wants me to believe his own mom is clueless, I will let him have that point. After fault finding regarding acts of harm is done, then I'd be willing to consider which companies, including WPE, are leeching in a way that makes them not healthy members of the community, but only after all this is sorted out. Putting aside all legal arguments, I agree with the directionality of Josh Collinsworth's main point regarding the health of the ecosystem. To put in my own words: Matt's behavior with banning a host's customers from security and feature plugin updates from WordPress.org without sufficient warning (or clear enough reason) has damaged trust in a core single point of failure in the WordPress ecosystem -- I see no excuse for this -- and it is important for the ecosystem to restore this trust as soon as possible. It's an unacceptable situation to begin with, that something that powers 43% of all sites on the Internet can have security updates degraded on the whim of one individual, no matter how much he contributed to the software in the past. The most direct way to repair trust would seem to be at the very least to put WordPress.org's update server in the ownership and operation of someone else, preferably a functioning board who was bound to serve the community/ecosystem in a way that included minimizing ecosystem disrupting events like this one, and who established transparent guidelines on what sort of behavior can get a company banned from using these servers (and few mention they are also banned from future conferences). That this event came without warning to many users seems outrageous. Another thing Josh has right: I and virtually all people hate, to a high degree, greedy ownership of corporations that intentionally lets quality rot as pricing is jacked up and money is squeezed out, so it is very remarkable that so many people think the more critical infraction to the community here is what Matt has done. This isn't about the greedy private equity firm or trademarks right now. It's about a bigger and more urgent problem. We have plenty of time to get back to corporate greed after the current emergency is resolved. reply ChrisNorstrom 17 hours agoprev\"WordPress powers 43.5% of all websites as their CMS. Around 478 million websites are built on WordPress\" Thanks to Matt Mullenweg's leadership and now you want him removed because he has flaws? Let me guess you want another Mark Zuckerberg in there? What's wrong with you people? He needs a stern talking to, not complete removal. This is another re-occuring case of \"create hero, destroy hero\" where the public likes to build someone up, find a flaw with them, act like they are irredemable, act like you're so distraught and hurt by their behavior, cry your tears, and destroy the hero you once celebrated. reply minimaxir 17 hours agoparent> He needs a stern talking to, not complete removal. He's getting one from his customers. reply ipaddr 16 hours agoparentprevA CEO who takes 43.5% and reduces it to 25% should be asked to leave. He may have gotten to 75% (now down to 43.5%) but even Jobs was fired. reply PedroBatista 16 hours agoprevWhile Matt comes looking \"not good\" and his apparent impulsiveness and \"less than optimal\" communication skills are doing more damage than good, I find it \"interesting\" most of the people attacking him either choose to ignore of gloss over the absolute trash WPEngine management people are. It's rich coming from people demanding a high standard for Matt but not for the WPEngine people. In the end all of it will get \"solved\" because there's way too much money in this to go any other way. As Danny Glover famously said: \"I'm too old for this shit\" reply chenmike 16 hours agoparentHonest question: what am I supposed to be demanding from WPEngine? They’re not the ones out there posting unhinged rants. reply PedroBatista 16 hours agorootparentThat's not an honest question. If you are completely fine with WPEngine's commercial practices, trademark violations in their marketing materials ( like it or not ) and moral OK for them not picking up part of the bill of what they consume ( talking about infra resources, not even talking about code ), then.. why the f** are you so bothered by an unhinged rant from some guy? It's GPL after all.. Don't you see the irony? or you just want to see it because Matt is kind of an unlikable dbag? Is that the level of depth we are at when cheerleading for this stuff? reply chenmike 15 hours agorootparentHow are you allowed to tell me if that's an honest question or not? Do I have some hidden WP Engine shill comments in my history or something? Can you read my mind? I don't have a strong opinion on WP Engine's behavior, because I'm not convinced by Matt's arguments. I do have an opinion on Matt's behavior though. I think it's unhinged. Feel free to respond but I'm done with this conversation, given how unpleasant I feel it's going to be given the incredibly uncharitable tone in your response. I recommend taking a walk or something. reply cynicalsecurity 18 hours agoprevWordPress will survive all of us. reply minimaxir 18 hours agoparent.org or .com? reply c0brac0bra 18 hours agoparentprevAnd that's a shame reply throwaway984393 18 hours agoprevPlease, for the love of god, let it die. WordPress's horrible security, annoying maintenance, and cookie cutter sites have filled the internet with malware and garbage content. It's so annoying to maintain that we have to find these managed hosting places that provide very little value. And despite them, you still need to add a bunch of extra stuff to make a reliable site. Time to create something modern. reply 1over137 18 hours agoparentLet me guess, you are a web programmer? Because for those of us that are not, WordPress is a pretty easy and pretty capable way to self host something. Without it, lots more stuff would be centralized with Big Tech. reply leonidasv 16 hours agorootparentIf you can self-host something, you can run a SSG*-based blog platform (Hugo, Jekyll, etc). And static webpages are even simpler to self-host. If you are willing to bend a little bit for big tech, you can just host them dropping generated files in any S3-like object storage. *Static Site Generator reply timeon 10 hours agorootparent> S3-like object storage They are not web programmers maybe not even in devops. So It is easier to just buy basic shared hosting wit button 'install wordpress' in hosting-admin. Statically generated site would be better (especially for readers of the site) but they still need some CMS. reply wmf 17 hours agorootparentprevIf WordPress didn't exist there would be other blogging software. reply bdcravens 17 hours agorootparentprevWP has emerged as a de facto standard, but there were alternatives before, and there will be alternatives to come. reply kmeisthax 17 hours agoparentprev...and replace it with what, exactly? I mean, I liked Drupal, still do, but the business for custom Drupal sites dried up and moved onto WordPress with (insert builder of choice here). Teaching anything on Drupal was like pulling teeth, even in the D7 and D8+ eras. And I'm not aware of any demand for Backdrop / Concrete5 / whatever sites. WordPress even ate up whatever demand there was for Magento (good riddance IMO). Everyone moved to WordPress because it's easy to teach clients on, the dev costs are reasonable, and there's a really good plugin ecosystem to counter the platform's faults. You can absolutely build unique sites on WordPress and lock them down too. If WordPress dies, everyone moves to SquareSpace (which is proprietary) or socials (in other words, doesn't have a web presence). reply chuckadams 16 hours agorootparentI have some hopes for Drupal's Starshot Initiative. Tho it now bears the crushingly drab sobriquet of \"Drupal CMS\" :-P reply sleepybrett 14 hours agorootparentprevlol 10 years ago I worked with a shop that used expression engine for everything. It was certainly more flexible, more pleasant to write plugins for, and less of a security nightmare at the time. The community was significantly smaller, but many of the plugins that existed were very well maintained, there was a small but healthy set of developers with paid plugins. I don't want anything to do with that industry anymore, whenever I want to put up a little site I just use a static site generator (hugo). I think there is an interesting potential market for a wp admin like UI that could be used to create hugo posts/etc and manage the theming configuration. Then you click publish, it fires off a hugo container that generates the static site and updates the host... maybe such a thing exists. reply Dalewyn 18 hours agoparentprev>Time to create something modern. The track record of \"modern\" would suggest Wordpress is the second coming of Jesus, so no thanks. Wordpress is fine compared to \"modern\" software. reply breck 17 hours agoprevMatt just needs to let copyright and license go. A long walk could cure him of his blindless. Then Wordpress will be fine. In general, anyone who doesn't wake up to E=T/A! will go extinct. Copyrights are for cons. Patents are for parasites. Licenses are for losers. You can only ignore nature for so long. reply paulryanrogers 16 hours agoparentWhat about this situation is the undeniable force of nature? reply breck 16 hours agorootparentThere are 2 groups of businesses: those like in this Wordpress battle that are fighting over copyrights, patents, and trademarks & those that have realized these things are retarded and make them innovate atI haven’t really been involved in WordPress for about five years now....Yes, I used to work for WP Engine. I even kinda liked them, for a while (mostly while they just kinda left us alone for the first year or so). But I wouldn’t say my time at the company left a good taste in my mouth. > We don’t need to dredge up a bunch of old and buried stuff that isn’t really important anyway, but suffice to say: I really don’t have any reason to be a WP Engine cheerleader. Most of the people I knew there have left, and I’ve watched from the sidelines as the company has implemented a bunch of scummy policies and shady sales tactics to squeeze money from their customers and make it harder to leave. If this isn't good enough for you then you're not being honest, you are just desperately looking for any excuse to defend Mullenweg. reply Zamiel_Snawley 16 hours agorootparentIf he didn’t have a financial stake in his former company, WP Engine, he would have put it at the top of his list of disclaimers. His company was acquired by WP Engine, it would be crazy if he didn’t get stock as part of that deal. The omission of that suggests that he does have a financial stake. reply Zamiel_Snawley 18 hours agoprev [–] This reads like part of the smear campaign predicted by Matt in his interview with Michael Paulson. Also note that the author doesn’t disclaim any financial stake in the company he used to work for, WP Engine, after his company was acquired. He merely claims that he isn’t a “fan of either party”, so we should value his opinion and trust that it is impartial. As for myself, I’ve never used WordPress or any CMS, I’m a lowly embedded software engineer. If Matt, the progenitor and steward of one of the most successful open source products in the world, asserts that an entity in the ecosystem is a leech, I’m inclined to believe him. reply mvkel 18 hours agoparent> asserts that an entity in the ecosystem is a leech By that definition, TinyMCE should be able to disable all textareas in Wordpress, since the product has used it freely for years without contributing anything. And Lodash should be able to disable TinyMCE. And QUnit should be able to disable Lodash. It's open source. Should every Linux user be -required- to contribute? I'd bet the OS quality would degrade if that were so. I am left with the question: why, after 13 years of WP Engine doing the exact same thing, is Matt now willing to burn down the house? reply mitjafelicijan 17 hours agorootparentI partially agree with you that this could be a slippery slope, but there is a difference here. TinyMCE is not burning through API and CDN resources of the dependencies it is using. Matt mentioned it's costing them millions per year to provide them with automatic updates and other services they provide. And he also mentioned that they can host these services on their own, since the code is GPL2. They don't do this because they don't want to pay these costs themselves. reply bastawhiz 17 hours agorootparentWhere's the pricing page? Where's the terms that say \"once your customers use our service enough you have to pay\"? Where's the option in WordPress to switch to another backend? reply tpmoney 48 minutes agorootparentThe converse of this is where’s the contract or terms that says WP has to keep providing API access for free? It’s open source software after all, almost all of which comes with some form of “no warranty or guarantees” of any kind. Certainly I’ve never seen one offering contractual access to any and all future updates or patches. reply ValentineC 14 hours agorootparentprev> Matt mentioned it's costing them millions per year to provide them with automatic updates and other services they provide. A single hosted plugin repository is a huge multiplier towards ecosystem growth, but WordPress always had the option of distributing the load by asking others to set up mirrors, and offering a selection of mirrors. Y'know, redundancy. They've never done so. There's also no option in the WordPress core to easily point towards a different repository URL. Maybe this whole debacle would lead to slightly more decentralisation of WordPress, which might be a good thing for the long-term health of the project and community. reply mvkel 17 hours agorootparentprevPerfectly acceptable to make that claim, which would be a ToS violation. Why couch it as something else? if WP Engine contributed to source, would they suddenly be allowed to rack up API and CDN bills? reply Zamiel_Snawley 17 hours agorootparentprev1. Provide proof of your claim that they don’t contribute, or a request for them to do so. I suspect Matt and his companies have contributed to many of their dependencies. 2. All those products have no marginal cost for users, WordPress.org does. 3. Just because bad behavior has been tolerated for a time doesn’t mean it must be tolerated forever. reply mtndew4brkfst 18 hours agoparentprevAlso note that the author doesn’t disclaim any financial stake in the company he used to work for It would hardly be shocking if he had some stock or whatever, given how prevalent that is for tech workers' comp. However, if it were me, and I did have a financial stake in WPE, I would not/could not write either of these sentences from TFA under my personal view of honesty: I really don’t have any reason to be a WP Engine cheerleader. I can assure you it’s not me defending my old company just because I used to work for them. I’ve got literally no reason to do that. reply Zamiel_Snawley 17 hours agorootparentIf it were you and you did have a financial stake, would you make a post like this? I doubt it, and it would be wrong to do so. If he didn’t have a financial stake, he would have listed it in his myriad reasons to trust him. reply minimaxir 17 hours agorootparentIt's worth noting that Matt Mullenweg invested in WP Engine's Series A. reply glenstein 16 hours agorootparentprevI think the quotes you choose arguably are not even the strongest, because the author also said: >I’ve watched from the sidelines as the company has implemented a bunch of scummy policies and shady sales tactics to squeeze money from their customers and make it harder to leave. And >On most days, if you wanted to have a conversation about how much WP Engine sucks, frankly, I’d be a happy participant. reply hn_throwaway_99 17 hours agoparentprev [–] The author goes to pretty great lengths to be transparent about his background and employment history. More to the point, he lays out a ton of specifics about how Matt acted like a Grade A Asshat. If you have any issue with those specifics, you can and should state your objections. But your comment here is just nebulous BS. Saying \"This reads like part of the smear campaign predicted by Matt in his interview with Michael Paulson\" sounds exactly to me like when Elizabeth Holmes gave her now infamous retort \"first they think you're crazy, then they fight you, and then all of a sudden you change the world\". Both are equally vacuous statements that don't address the specific criticisms put forth. reply Zamiel_Snawley 17 hours agorootparent [–] I will concede that my remark about a “smear campaign” is worthless or detrimental to the discussion. 1. The author claims dozens of other companies behave similarly and claims that Matt has not taken any action against them. However, the author makes no claim of what level of contribution to WordPress, cost to Wordpress.org, or any contractual agreements those companies may have with WordPress entities. Multiple companies do pay for trademark licenses. Scale is also an important factor here. Estimates are in the hundreds of millions of revenue for WP Engine. They are one of the largest companies in the ecosystem, of course they should be contributing more than smaller players. 2. The author claims that Matt attempted “extortion” and refused to give WP Engine more time to address his demands. It does seem true that Matt refused to give them more time. However, Matt claims this conversation has been going on for more than a year. A deadline seems appropriate if your counterparty keeps delaying. 3. The author claims that trademark confusion between WordPress and WP Engine is an unfounded concern, citing confusion Shopify and Spotify. This is ludicrous on its face, those companies serve completely different market needs, compared to the WP Engine literally offering the product produced by the trademark holder. The author claims that WordPress.com and WordPress.org is confusing the trademark, but doesn’t state whether WordPress.com has a license to use the trademark. 4. The author claims that Matt’s dissatisfaction with WP Engine’s contribution is unreasonable because there aren’t terms and conditions or a contract. This is missing the point entirely. The point is that good members of the community shouldn’t need to be forced to contribute. If they want to play by the letter of the law, Matt isn’t obligated to provide the free services that their business relies on, just as they aren’t obligated to give back. It cuts both ways, and if they won’t operate in good faith, it is self destructive to continue to enable them. I’m out of energy to continue, so I’ll stop here. reply glenstein 16 hours agorootparent [–] >2. The author claims that Matt attempted “extortion” and refused to give WP Engine more time to address his demands. It does seem true that Matt refused to give them more time. However, Matt claims this conversation has been going on for more than a year. A deadline seems appropriate if your counterparty keeps delaying. What would \"seem appropriate\" would be some form of terms and conditions, or contract, or legal authority that would warrant any of this business about imposing any deadlines or demands of any kind whatsoever, in the first instance. The time isn't Matts to give or not give to begin with. >The point is that good members of the community shouldn’t need to be forced to contribute. I think the author was at pains to emphasize through the beginning middle and end of the article that there was such a thing as a right way to make this case. And the problem is weaponizing certain levers at WordPress in ways that raise all kinds of conflict of interest issues, have the potential to cause all kinds of collateral damage, and undermining credibility and integrity of WordPress as a long-term project. reply Zamiel_Snawley 16 hours agorootparent [–] I agree that there is a right way to make this case, but we don’t know if it was already made behind closed doors. The ultimatum Matt made in the texts highlighted by WP Engine appears short, but it seems unlikely this was the first time Matt brought up these issues. reply jaredthirsk 9 hours agorootparent [–] Even assuming the best was done behind closed doors, isn't it reasonable to WP Engine customers to give them notice of at least a few weeks or months that \"unless your host complies, you will lose access to WordPress.org updates\"? If Matt can pull the rug on one host's customers without notice, he can do it again. He has been on streams saying no other host is in the doghouse with Matt, but a week ago almost nobody knew WPE was on thin ice. If Matt didn't do the reasonable thing to warn WPE customers, what other unreasonable things is he capable of doing in the future? The \"reprieve\" of a new Oct 1 deadline is still far from reasonable in my opinion to the point that it is further infuriating that he is using it to virtue signal, though it is at least a tiny start of an implicit acknowledgement that he screwed up. But it's a matter of rebuilding trust in the ecosystem now, and I think Matt is still digging a hole, and this can't be fixed until he apologizes, and the governance of the WordPress.org update server is clarified (best case: out of Matt's hands), or somebody more neutral creates a competing update server, fracturing the ecosystem. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Matt Mullenweg, co-founder of WordPress, is accused of abusing his power, causing harm to the WordPress community through actions against WP Engine.",
      "Allegations include extortion attempts, anti-competitive behavior, demands for licensing fees, and blocking WP Engine sites from accessing the plugin repository, which has exposed users to security risks.",
      "The call for Mullenweg's removal from WordPress leadership is based on his perceived abuse of power and conflicts of interest, which are seen as detrimental to the community's well-being."
    ],
    "commentSummary": [
      "Critics argue that Matt Mullenweg's poor communication and erratic behavior are damaging WordPress's legacy and community trust.",
      "There are concerns about Mullenweg's demands for WP Engine to pay licensing fees to Automattic, his for-profit company, which may blur the lines between WordPress's nonprofit and for-profit entities.",
      "Some believe that removing Mullenweg could restore stability and address potential legal issues within the WordPress ecosystem."
    ],
    "points": 232,
    "commentCount": 194,
    "retryCount": 0,
    "time": 1727480955
  },
  {
    "id": 41676297,
    "title": "US Trademark Office Cancels Marvel, DC's 'Super Hero' Marks",
    "originLink": "https://www.reuters.com/legal/litigation/us-trademark-office-cancels-marvel-dcs-super-hero-marks-2024-09-26/",
    "originBody": "reuters.com#cmsg{animation: A 1.5s;}@keyframes A{0%{opacity:0;}99%{opacity:0;}100%{opacity:1;}}Please enable JS and disable any ad blockervar dd={'rt':'c','cid':'AHrlqAAAAAMAzCuHEZ_UefMAFKwGrw==','hsh':'2013457ADA70C67D6A4123E0A76873','t':'bv','s':43909,'e':'8ab2fe81bcc31226ac5dfef1e3714a9ecdd40c85a04df1b43da2b57699a709a3','host':'geo.captcha-delivery.com'}",
    "commentLink": "https://news.ycombinator.com/item?id=41676297",
    "commentBody": "US Trademark Office Cancels Marvel, DC's 'Super Hero' Marks (reuters.com)189 points by h2odragon 20 hours agohidepastfavorite65 comments tape_measure 13 hours agoInteresting points from https://fingfx.thomsonreuters.com/gfx/legaldocs/myvmamnnavr/... 5. In 1980—decades after the birth of super heroes—DC and Marvel jointly registered SUPER HEROES as a trademark. 6. DC and Marvel claim that no one can use the term SUPER HERO (or superhero, super-hero, or any other version of the term) without their permission. DC and Marvel are wrong. Trademark law does not permit companies to claim ownership over an entire genre. SUPER HERO is a generic term that should not be protected as a trademark. 7. Trademark law also does not allow competitors to claim joint ownership over a single mark. The purpose of a trademark is to identify a single source of goods and services. 20. DC has accused Superbabies of infringing DC’s “SUPER”-related trademarks, has filed an opposition to Superbabies’ trademark applications (TTAB Trademark Opposition No. 91290757), and has threatened further legal action. DC has asserted the exclusive right to use “the prefix SUPER followed by a generic term for a human being.\" There's also some examples of SUPER HERO used as a generic term by DC and Marvel. I know of some companies being famously strict about trademark use (example https://www.velcro.com/original-thinking/the-velcro-brand-tr...), and yet these uses seem benign. For example, a splash at the top of a comic book \"DCs BOLDEST new super-hero\" (without TM, with dash). Now I have to be careful about using any of my company's trademarks. I'm not sure I fully understand how this example is generic and harmful. reply hn_throwaway_99 12 hours agoparent> 7. Trademark law also does not allow competitors to claim joint ownership over a single mark. The purpose of a trademark is to identify a single source of goods and services. I'm baffled how this was ever allowed in the first place. It's like Marvel and DC went to the trademark office and said \"Yes! We'd like to collude to prevent any other competitors from using these terms.\" and the trademark office was like \"Collusion it is! Have a nice day!\" reply greatgib 12 hours agorootparentI'm not surprised that you can pretend to do that in a trademark office, but I'm more surprised that it does not trigger an antitrust investigation by authorities as it is clearly the 2 dominant players colluding to prevent having any competition! reply em-bee 12 hours agorootparentprevyou can license others to use your trademark. so one of them could have trademarked it and licensed it to the other in an exclusive deal. reply hn_throwaway_99 12 hours agorootparentBut that seems to be clearly not what happened in this case, at least by the explanations in the court's ruling. reply em-bee 1 hour agorootparenti didn't mean to imply that it did, just that the idea of sharing a trademark is not as ridiculous if you consider that licensing a trademark i possible and the result would mostly have been the same. reply thaumasiotes 10 hours agorootparentprevWhat would happen if DC and Marvel established a body of organizations concerned with comics, which just happened to consist of the two of them and nobody else, and that body was the one holding the trademark? As far as I'm aware, that's a completely normal set of events, but the effect is the same. reply acdha 3 hours agorootparentI think that’d lead to the same challenges: it’d be trivial to show that usage of the term predated that organization by decades, and anyone making an argument about collusion or antitrust would be able to point to the existence of a closed group created by the top two competitors as evidence rather than a defense. reply hn_throwaway_99 2 hours agorootparentprevWhat you've described is essentially what led to the Sherman Antitrust Act in the first place (\"trusts\" in this case were exactly the kind of body of companies in the same industry that you describe). The two most dominant companies in the industry are simply not allowed to collude like this to the exclusion of competitors. reply MBCook 17 hours agoprevI would never have guessed the term was trademarked. It seems far too generic. And if it was granted in the late 60s, that’s what 30 years after Superman? Shouldn’t it have been common by then? reply bawolff 14 hours agoparentTrademarks do not expire ever. The intended point of a trademark is essentially to prevent scams. E.g. nobody is allowed to sell something called an \"apple computer\" except apple. The interest doesnt change with time. (In contrast the theoretical point of copyright and patents is to allow people to recoup r&d costs. Eventually at some point the holder has had a fair shot at recouping the cost, so there is a time limit) reply amelius 7 hours agorootparent> The intended point of a trademark is essentially to prevent scams. This is why I think that companies should lose their trademarks in case of scandals, e.g. privacy infringements or security breaches. This is a much better punishment than fines which are often just a slap on the wrist. And to the consumer, a scandal often feels similar to being scammed. A trademark is a symbol of trust. And companies who are not trustworthy should not be able to use it. reply MereInterest 7 hours agorootparentAnd why trademarks should be dissolved in bankruptcy, not treated as a transferable asset. If there’s a break in continuity of the provider, then the public has no guarantees about the quality after such a break. reply j-bos 6 hours agorootparentOh that's good. I would love to see someone pursue this as a legal claim. reply username332211 5 hours agorootparentprevWhat sort of a break of continuity do you think happened in General Motors in 2009? reply Dylan16807 12 hours agorootparentprevThe comment was not about expiration in any way, it was surprise that the trademark was granted long after the term had started being in wide use. Imagine if \"PC compatible\" got trademarked in 1997. reply bawolff 7 hours agorootparentAh, you are right, i misread. reply fph 8 hours agoparentprevFriedrich Nietzsche's Übermensch is from 1883. reply ezfe 15 hours agoparentprevMarvel and DC were not the original holders of the term reply tourmalinetaco 15 hours agoparentprevIt was. However trademarks, much like patents, are made as generic as possible in order to give the company as much control and value as possible. Tech Dirt is filled with articles of trademark trolling. reply lupire 5 hours agorootparentTech Dirt is an equally generic Trademark as Super Hero! reply snowwrestler 15 hours agoprev> The USPTO's Trademark Trial and Appeal Board ruled for S.J. Richold's Superbabies Ltd after Disney's Marvel and Warner Bros' DC did not file an answer to Superbabies' request to invalidate the marks. So, canceled after the companies declined to defend them. reply lolinder 14 hours agoparent> Marvel and DC have cited their marks in opposing dozens of superhero-related trademark applications at the USPTO, according to the office's records. It's not like they haven't been using them, they just knew that at this point they'd have lost if they tried to actually fight it. Most previous groups probably folded immediately under pressure from the giants. reply IG_Semmelweiss 14 hours agorootparentSo they sued others into folding. But superbabies did not. Was it overconfidence, or a a gigantic blunder in not doing their diligence ? (by the DC legal dept team) reply biorach 10 hours agorootparentI imagine that the DC legal team were well aware that the trademark was indefensible but figured that the expense of counter suing a well funded industry giant would cause most small players to fold immediately. Their luck ran out in the end but they got a few decades out of it. reply IG_Semmelweiss 3 hours agorootparentI feel like you missed my point If the DC legal team knew their TM was indefensible, they ought to have picked their battles FAR more carefully. Because sending a demand letter to a target likely to contest the TM, would be the end of the TM. Which is exactly what happened. They could have literally kept their TM - to sue another day - if they had chosen to look the other way with superbabies. So, my point was whether it was hubris that led to the decision to go after superbabies, or failure instituting unsexy (but necessary) internal dilligence checklists. reply biorach 1 hour agorootparentMaybe there's some context missing? Was there something about superbabies that made them more likely to contest the TM? reply martyvis 15 hours agoprevA quick search on the Australian National Library archive finds an article titled \"A British Super Hero\" in a 1918 newspaper. https://trove.nla.gov.au/newspaper/article/129947369 reply fsckboy 14 hours agoparent>an article titled \"A British Super Hero\" in a 1918 newspaper trademark does not work like patents, prior art is not a thing. The question is whether anyone else uses the mark in trade, exchanging money for goods/services. Usage outside of that context does not matter. you get diluted and lose your trademark when the public uses the term generically in trade, in your line of business, and not in reference to your product, not just because they use the term. for example, the automobile Mercury Comet is named after two generic things, a Greek god and an space body. So what, they are used in trade for particular automobiles. Comet is also the name of a cleaning product. The two are not in the same line of business, so they don't get confused, and there is no conflict, but you can't start selling another Comet cleaner, or Comet car. While there is no prior art, there is prior use, in trade. But in that case, the trademark belongs to the prior user, not to the world at large. If the prior user stops using it, like Aunt Jemima is no longer used for pancake syrup, then the term becomes free for anybody to use for pancake syrup. (I'll bet that company still uses that name for some pancake syrup product, like for institutional use, so they can stop anybody else from using it.) reply em-bee 12 hours agorootparentI'll bet that company still uses that name for some pancake syrup product a related story: in many european countries the product \"twix\" was named \"raider\", until some day they decided to unify the brand and rename it to twix. but apparently, every few years they sell a batch under the old name \"raider\" presumably just so they can keep the trademark. reply zimpenfish 10 hours agorootparent> apparently, every few years they sell a batch under the old name \"raider\" presumably just so they can keep the trademark. Oh, I wonder if that's why Mars are doing a limited run of \"Snickers\" named as \"Marathon\". https://www.theguardian.com/business/2024/sep/20/mars-brings... reply prmoustache 5 hours agorootparentprevI think it is more to trigger some nostalgia induced sales from those who were kids in the 80's. Despite the renaming we used to call the raiders for years as teenagers in the 90's reply acdha 3 hours agorootparentI think that’s the other side of the same theory: the reason it’s worth owning as a trademark is that millions of people remember it. Keeping it alive is cheap as long as the cost of printing labels is lower than the profits from a limited run. reply bigstrat2003 12 hours agorootparentprev> If the prior user stops using it, like Aunt Jemima is no longer used for pancake syrup, then the term becomes free for anybody to use for pancake syrup. You know, I never thought of that. I kind of wonder why nobody has thought to revive those trademarks and take the market that the company had built up. I don't really believe that you would have enough pushback (because some people find the trademark offensive) to make it not worth one's while to get an instant market for their competing pancake syrup. reply olddustytrail 5 hours agorootparentprev> the automobile Mercury Comet is named after two generic things, a Greek god and an space body Roman god. The Greek version would be Hermes. reply martyvis 15 hours agoparentprevAnd here is one in the plural from 1928 referring to our Anzac (veteran's) day march. http://nla.gov.au/nla.news-article115521528 reply martyvis 15 hours agorootparentAnd even a \"super-super hero\" in 1926! http://nla.gov.au/nla.news-article21042874 Apparently Charles Lindburgh was known in the USA as a \"super-hero\". http://nla.gov.au/nla.news-article95784232 reply tedunangst 12 hours agoprevClearing out the easy stuff before facing the final boss: space marine. reply lupire 5 hours agoparentDo any of these space marine games have beaches? reply bithead 3 hours agoprevI wrote a role playing game I called 'Heroes' in the 80s. I should file something. reply userbinator 17 hours agoprevI wonder if this will escalate to fights over trademarking \"Ultra Hero\" or other superlatives next. reply spacebacon 8 hours agoparentSuperb Hero FTW reply lupire 5 hours agorootparentHawkeye? reply mhandley 19 hours agoprevHere's the full article, but turns out it isn't really any longer that you can see from outside the paywall: https://archive.is/jQtuc reply CM30 9 hours agoprevHonestly surprised the trademark wasn't nullified earlier. It's pretty clear the term has become genericised at best, and was in common use before the trademark at worst. Guess it shows you the dangers of uneven legal resources, since I suspect if the folks whose trademarks were shot down using this had fought back, it probably would have cancelled way earlier. reply Ekaros 8 hours agoparentNo one cares until someone cares. If trademark is not used in any actions to supress others, no one really tracks them. So there is no impetus to clean it up. And I don't think government has resources to iterate over trademark and invalidate generic ones automatically. reply shagie 6 hours agorootparent... and many things that we consider to be generic ones may have started out specific and become generic through various means. https://en.wikipedia.org/wiki/List_of_generic_and_genericize... reply cuddlyogre 19 hours agoprev [–] Good. Now I hope someone figures out how to abolish software patents. reply gjsman-1000 19 hours agoparent [–] Never going to happen; too much investment. Not without something resetting the whole patent system all at once. And hey, I’m not actually opposed to all patents. H.265 - if you put tens of millions into compression research, or hundreds of millions into database scaling research at PlanetScale, a temporary exclusivity period makes sense. 95% of software patents don’t reach that level. I think some of the bad rap also comes from technology advancement. Amazon’s 1-click Checkout patent is notorious; but nobody talks about how much of an accomplishment that technology was in 1997. It actually was very impressive when that patent was granted, particularly in getting the credit card networks to agree to the security design. reply AnthonyMouse 16 hours agorootparent> Never going to happen; too much investment. Because of the nature of software patents the investment is worthless anyway. One of the biggest problems with software patents is that they're unreasonably broad or ambiguous and then the claims read on arbitrary software the authors of which have never even heard of the patent. Another is that companies purposely patent interfaces that are needed for compatibility, and then the patent isn't needed because it's so great, it's needed to interoperate with existing systems and thereby offers no ability for competitors to design a better alternative because better is different is incompatible. You have to license H.264 even if you build something better yourself -- or you've already licensed H.265 -- because you still have to be able to interact with media and clients that use H.264. Then as between large companies, they all need each others' patents and just end up cross licensing everything. All the effort is for nothing because it just cancels out. As between large companies and small companies, the large companies can sue the small one, but the small company probably doesn't have any money anyway and the suit makes the large company look like a bully and creates PR losses that likely outweigh any benefit from filing the suit. The small companies, on the other hand, can't sue the large ones because the large company would just file counterclaims and (at best) force the same cross-licensing that exists between large companies. So that's worthless. Which leaves the only entities that really like software patents: Patent trolls. Eliminating them is a major economic benefit of eliminating software patents. reply pmontra 14 hours agorootparent> Then as between large companies, they all need each others' patents and just end up cross licensing everything. All the effort is for nothing because it just cancels out. As you explain in the next paragraph, that creates a moat the protects the large companies from the small ones. They compete against each other but they also collectively defend their own kind. reply AnthonyMouse 13 hours agorootparentYou're describing another major benefit of eliminating software patents. Even large companies don't actually benefit from that because their suppliers and companies in complementary markets do the same thing, and you lose any time any of those companies can maintain a moat with which to extract rents out of your own market. These are deadweight economic losses. They hurt everybody to benefit the company doing them, but even that company is suffering a net loss because of all the companies doing it back to them. Yet they still do it because it's a tragedy of the commons, unless you remove the mechanism that enables it, i.e. software patents. reply ndiddy 6 hours agorootparentprevH.265 is a great example of software patents going wrong. As it was the first MPEG video standard created after the rise of widespread commercial video streaming, all the patent owners involved wanted to be able to get as much money as possible from the streaming companies. Because of this, we went from the relatively reasonable H.264 licensing terms (pay one patent pool a per-device licensing fee, capped at a total royalty payment of $14 million) to H.265 being covered by three separate patent pools. Between all of them you have to pay royalties on decoding hardware, software, and per-item encoded, and some of the pools don't have caps on royalties. Additionally, some major patent holders aren't in any pools and you have to work out deals with them individually. Here's a summary of the H.265 licensing situation: https://www.slashcam.de/images/news/HEVC-Patent-Pools-14134_... The result is that H.265 hasn't gotten much commercial adoption (the one major use is 4K Blu-Ray). Instead, most major streaming and tech companies have been pushing AV1, which doesn't have licensing fees and takes a \"mutually assured destruction\" approach to patent enforcement (the AV1 patent license states that if any patent holder tries to sue an AV1 user for patent infringement, they automatically lose the rights to all AV1 patents, opening them up to a countersuit). reply Dylan16807 12 hours agorootparentprevThere are plenty of people working on video codecs both in and out of patented realms, with patents hindering progress more than they incentivize it. For PlanetScale, are you sure the patents are necessary when they have copyright on all their code? I'd say that productivity-enhancing software patents are so vanishingly rare that we barely need to consider them. Also software is math, it's not supposed to be patentable. reply rightbyte 11 hours agorootparentprev> Amazon’s 1-click Checkout patent is notorious; but nobody talks about how much of an accomplishment that technology was in 1997. How exactly is removing the confirmation prompt for the purchase basket a technical accomplishment? reply cuddlyogre 19 hours agorootparentprevAs a compromise, I suggest the source code must be made public for patented ideas. reply tourmalinetaco 15 hours agorootparentIn an ideal world, all intellectual property would become public domain after 10-15 years, including all research, schematics, wire diagrams, source code, marketing materials, etc. When you go to the various offices to get your IP recognized you must also submit various materials and continue to do so for the life of your property rights. Again though, in an ideal world. In reality any major changes to something like copyright would probably get you killed even faster than judges who are hard on drugs. The most that we, the people, can do until there’s some amount of backbone in our various countries is to remove ourselves from the primary market wherever we can. For instance, I have been on a successful Nintendo boycott for the last 8 years, and it’s been even longer for Disney. I buy anything I want secondhand or pirate it directly, I don’t pay into SaaS but use alternatives, and I feel a lot happier being ungovernable in this way. reply gjsman-1000 19 hours agorootparentprevPatents already require that all information be available, for someone similarly invested in the craft, to be able to completely reproduce the invention. That doesn’t require an implementation - but that mirrors our regular patent office, which does not require physical functioning prototypes to demonstrate. reply justinclift 16 hours agorootparent> Patents already require that all information be available ... That's not always the case. For example, patents around nuclear technology: • https://en.wikipedia.org/wiki/Invention_Secrecy_Act reply yazzku 16 hours agorootparentprev\"All information be available\". Have you filed or read any software patents? Many are so vague that they do not embody any significant \"idea\" or contribution, and are mostly just a hindrance to actual innovation. And some are just plain stupid, like the patent to average two integers without overflow. Like the parent said, a compromise could be \"source or GTFO\". But even that seems of questionable value. The shit show gets to the point where many companies file patents defensively. They'll file a patent just in case their competition does it first, even if they have nothing to show for it. And this naturally affects smaller companies disproportionately because they do not have the funds to pay lawyers (there is a hilarious interview on Youtube of a small startup CEO that explains how his company spends more on lawyers than engineers.) So tl;dr, we'd probably be better off without software patents altogether. reply teddyh 18 hours agorootparentprev [–] I direct your attention hither:reply gjsman-1000 17 hours agorootparent [–] A wiki specifically on the topic written by non-lawyers is interesting; but I don’t see why it should be considered an unbiased list of ideas. Sometimes the status quo is imperfect but okay. reply jgeada 16 hours agorootparentHave you ever wondered why lawyers themselves have nothing in their field remotely similar to patents? reply Nevermark 12 hours agorootparentA though provoking question! But 99.9% of legal arguments are copies. I.e. ideas with precedence. Copying is to be encouraged. If legal ideas, which are the fallback of all our rights, could be owned, not even a veneer of justice would remain. reply teddyh 17 hours agorootparentprev [–] Why did you specify “non-lawyers”? Did you mean to imply that something written by lawyers would be unbiased? And where did I ever claim that this was unbiased? It’s the “End Software Patents Wiki”; it’s about as biased as it gets. But I thought you wanted arguments, so I linked it. If you want to dismiss them without reading them, that’s up to you. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "The US Trademark Office has canceled Marvel and DC's 'Super Hero' trademarks, which were jointly registered in 1980.",
      "Trademark law prohibits ownership of generic terms and joint ownership by competitors, leading to the cancellation after Marvel and DC did not defend against Superbabies' request.",
      "This decision underscores the limitations of trademark law and the difficulties in maintaining trademarks on commonly used terms."
    ],
    "points": 189,
    "commentCount": 65,
    "retryCount": 0,
    "time": 1727477479
  },
  {
    "id": 41674677,
    "title": "Obsessed with Cuttle: Parametric CAD for prototyping, producing, and procrastin",
    "originLink": "https://hannahilea.com/blog/cuttle-obsession/",
    "originBody": "@hannahilea: homeprojectsblogcontact",
    "commentLink": "https://news.ycombinator.com/item?id=41674677",
    "commentBody": "Obsessed with Cuttle: Parametric CAD for prototyping, producing, and procrastin (hannahilea.com)157 points by todsacerdoti 23 hours agohidepastfavorite33 comments tqs 21 hours agoI am one of the creators of Cuttle. It stems from my research building direct manipulation + programming environments like http://recursivedrawing.com/ and http://aprt.us/ From a programmer's perspective, you can think of Cuttle as a direct manipulation vector editor (like Inkscape or Adobe Illustrator) that can be driven with parameters and JS code where you need it. Unlike my previous research projects, this is a commercial startup mostly catering to laser cutting small businesses, though you can use it for anything where you want a 2D vector editor + some programmatic capabilities. I'll try to answer questions that come up in this HN thread. Thank you for sharing your work Hannah! Very cool stuff! reply mft_ 1 hour agoparentHey, I've just spent an hour creating my first design with Cuttle - and I'm very impressed! You're right that it has a very short learning curve, especially coming from a hobbyist CAD and programming background. A few random thoughts/questions/suggestions (and apologies if I've missed options or existing functions): * Would it be possible to add edge snapping? * Could there be a slicker way to switch from the central origin for shapes? (Central is frustrating as it means having to change position every time I tweak the size.) Being able to easily select an arbitrary corner as the origin with a click would be great; maybe even auto-selecting the origin based on snapping two shapes together - i.e. 'anchoring' the shape based on snapping it to another one? * Any way of exporting multiple components in one go, without clicking through for each one? * Being able to set custom rounded corners with a list is great, but it's not easily discoverable! * It might be neat to have a pop-up offering boolean options when snapping or overlapping shapes? * Given the (I think?) JS under the surface, is it possible to import data, either as a CSV or an image from which pixel values can be read? * Lastly, any thoughts on offering an intermediate 'hobbyist' paid tier? I'd be strongly tempted to support you and access some of the paywalled features, but $19/month is honestly too high for an occasional/fun hobby product. reply tqs 56 minutes agorootparentCuttle’s snapping is very good (imo). But you have to drag from the snap point to the snap point. So if you want to snap a particular midpoint (say) to a particular corner, you need to drag from that midpoint to that corner. That is, your drag is explicit about the from and to snap points. To export multiple components, we usually create a component (called “Cut Layout” for example), then drag out each of the other components (the pieces) into that component. There is a modifier called Flatten which might be useful for you for doing lots of Boolean operations at once. I think this video shows the workflow, https://www.youtube.com/watch?v=LGHKRfIC6QA Yes it’s all JS. You could copy and paste your data in and then process it and create geometry. (Sorry we don’t have any data import other than copy and paste at the moment.) Our scripting documentation is pretty good: https://cuttle.xyz/learn/scripting/getting-started-with-scri... We are experimenting with being able to read pixel values of raster images. Coming soon likely. Perhaps some kind of middle tier that gives full access to the Editor but does not give full access to the Pro templates makes sense. Many of our customers subscribe for access to those templates rather than the Editor! You are also free to subscribe when working on a project and need the Pro features, then cancel when you’re dormant. We don’t delete your projects when you downgrade to free, you can still access/edit/download them. I appreciate the feedback and the questions! reply throwaway2562 10 hours agoparentprevCuttle is v cool, congratulations! I had previously seen Apparatus and liked it, so I can clearly see the genetic resemblance now ;) I’m wondering how many of you are on the team, and does it actually support you as a business yet? Even for a quite niche-y app Cuttle deserves to be better known, and higher-priced, imo. reply tqs 2 hours agorootparentThanks! There are 5 of us on the team. Yes, the business is profitable. reply dekhn 18 hours agoparentprevI'm mainly curious whether the concepts in Cuttle could be exposed as plugins in Inkscape, or as a standalone application written in Qt-Python. reply tqs 18 hours agorootparentA Cuttle project is — behind the scenes — a program. Each “component” is a function. “Modifiers” are functions that take input geometry (and parameters) and use JS code to create arbitrary output geometry. All of this code can be live edited. At the same time you can do arbitrary “drawing” with a bezier pen tool and move/transform shapes. In this case you are essentially using the canvas drag-and-drop to manipulate literals in the program. But fundamentally a Cuttle project is a program and the Cuttle Editor is an IDE that looks like a vector editor on the surface. Because of this I’m not sure how much of Cuttle could be grafted onto a program whose architecture is more rooted as an editor of static vector graphics. I do know that Inkscape has some “live effects” which are similar to Cuttle’s “live” modifiers. If you are interested in Cuttle’s architecture, I did a one hour walkthrough on this interview, https://www.youtube.com/watch?v=2el-85vG-IU reply ferfumarma 5 hours agorootparentVery cool work. FWIW the aparatus site says > Apparatus is under active development. Discuss how Apparatus should evolve on the Apparatus Google Group. Which seems like it's no longer accurate. Has it moved to a different repo? reply tqs 2 hours agorootparentHmmm.. I would say Apparatus is no longer under active development. Researcher Joshua Horowitz was doing some work on it for a bit, but yeah I don’t think it’s changed much in several years. It should be regarded as a research project that scouted out several areas of the “programming experience” world that others can build on. reply pbronez 16 hours agoparentprevIt would be neat to have STL and STEP output for 3D printers. reply tqs 14 hours agorootparentThanks for the suggestion! Yes, we should do this. I've been seeing more and more people use Cuttle for 3D printing (exporting a DXF, then bringing that into another program to extrude and output a STL). reply phkahler 3 hours agorootparentprevSee Solvespace for that. reply 1oooqooq 6 hours agorootparentprevthere's openscad reply s1mon 1 hour agoprevI’m very curious about the fonts available in Cuttle. Are these all free fonts, or did some need to be licensed? Is it possible to use professional fonts which are on your local machine (without exporting as outlines)? How much adjustment needed to be done to make them appropriate for laser cutting etc? Many fonts which make sense for screen or printing need a lot of detail work to make sense for CAD work. reply tqs 50 minutes agoparentThe free fonts in Cuttle are mostly from Google Fonts and other freely licensed fonts. With a Pro account you can upload your own font files. You may be interested in Cuttle’s Connected Text feature. This will automatically connect dots on i’s, etc so you can cut out text in one piece. There’s also an option to “thicken” text so it’s not as delicate. You can try a live demo here: https://cuttle.xyz/@cuttle/Connected-Text-29M9IXUSr5yr That page also describes the algorithm we use to make this work with any font. reply thatguy288 5 hours agoprevI got excited when I read \"parametric\"… thinking it would be akin to what Autocad Inventor had 20+ years ago (setting angles between lines, setting lines to be parallel etc.) since I was recently looking for a simple CAD tool that could do that. Alas. reply jononor 4 hours agoparentFor hobby/maker/semi-pro that would be Fusion360, Onshape, FreeCAD or Solvespace. And more, probably... reply bschwindHN 4 hours agoparentprevCheck out SolveSpace! It might be just what you want, don't be deceived by its retro UI. reply emmelaich 14 hours agoprevPedant alert. Learning curve is skill/time. > with an exceptionally shallow learning curve I'm used to long learning time to incorrectly described as steep, but I haven't seen shallow used as short. Time to officially deprecate steep and shallow, and use short and long instead. reply cjbgkagh 13 hours agoparent“The common English usage aligns with a metaphorical interpretation of the learning curve as a hill to climb.” Wikipedia Both steep and shallow work in this context reply emmelaich 12 hours agorootparentMaybe the fault such as it is belongs to the original coiner. Should've been time/skill not skill/time. reply leoedin 11 hours agoprevI was briefly a member of a makerspace with a laser cutter - and it was brilliant! The ratio of effort to results was far better than any other CNC tools I’ve used. But then COVID and moving house put an end to that. I’d love to recreate something similar at home - but at a budget. Does anyone have a low cost laser cutter that actually works? It seems like there’s an increasing number of Chinese ones out there, but quality and capabilities are unclear. reply jononor 4 hours agoparentThere is a saying among those that operahe makerspaces: \"people come for the 3d printer,and stay for the laser\". We have Redsail CO2 lasers for nearly 10 years now, in our volunteer ran makerspace in Oslo. It works, but does requires a bunch of setup and maintenance. Of course with hundreds of different users yearly, anything will need that... But it is more and gives poorer results than GCC/Epilog (which another local lab has, albeit with more professional maintenance crew). GlowForge targets a user group which accept less tinkering. PS: expect to spend at least the same amount of money/time on ventilation system as the machine itself!! reply wood_spirit 22 hours agoprevAlways so wowed by posts about maker spaces :) Is it normal in the states? And is it full of cool projects? reply ajb 8 hours agoprevPerhaps someone on this thread will know the answer to this: is it possible to make V shaped grooves with a laser cutter, or would they end up stepped? reply jononor 8 hours agoparentA laser cutter has no inherent depth control. But you can modulate the power, which depending on the exact laser+material combination will give varying depth. In \"raster\" mode it can be modulated continuous so smooth transitions/slopes are possible. But the exact effect will need to be tested on samples. reply mtreis86 8 hours agoparentprevMaybe. Many laser cutters are 3dof so there is no way to tip the cutter to a different angle. Find one with a couple extra axis and then sure. reply RobotToaster 8 hours agoparentprevThat would be a lot easier with a CNC mill/router and the correct shaped cutter. reply jononor 8 hours agoprevCuttle looks amazing for 2D geometric patterns. I really must find time to play with it some day and make something. Maybe on leather. reply amelius 21 hours agoprevWhat do they mean by \"create 5 free projects\"? Will they laser-cut it for you and send it to you by mail? reply tqs 21 hours agoparentOn a free account you can create up to 5 projects in the Cuttle Editor (and you can delete them if you want to create more...) We don't laser cut anything for you. You can download your project as an SVG file (or DXF, etc) which you can then send to a laser cutter hooked up to your computer. The product is designed for people who have access to a laser cutter, e.g. at home or at a makerspace. reply m00dz 18 hours agorootparentI am loving your program making designs for my 3d printer. Thank you for sharing it to the world! reply volta-do-mar 20 hours agoprev [–] Totally dig this, especially the doorbell chime cover & music box! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Cuttle is a parametric CAD tool designed for prototyping and production, especially for laser cutting small businesses, combining vector editing with programmable parameters and JavaScript code.",
      "Users appreciate its ease of use and features like edge snapping, custom rounded corners, and boolean options, with suggestions for adding STL and STEP output for 3D printing.",
      "The small but profitable team behind Cuttle is recognized for creating a flexible tool with potential applications in various fields, including 3D printing and leatherwork."
    ],
    "points": 157,
    "commentCount": 33,
    "retryCount": 0,
    "time": 1727465732
  },
  {
    "id": 41678840,
    "title": "Meta fined $102M for storing passwords in plain text",
    "originLink": "https://www.engadget.com/big-tech/meta-fined-102-million-for-storing-passwords-in-plain-text-110049679.html",
    "originBody": "Read full article Big Tech Meta fined $102 million for storing passwords in plain text The Irish Data Protection Commission found that the company violated several GDPR rules. mariella moon Contributing Reporter Fri, Sep 27, 2024, 4:00 AM MST·2 min read 0 panida wijitpanya via Getty Images The Irish Data Protection Commission (DPC) has slapped Meta with a $101.5 million (€91 million) fine after wrapping up an investigation into a security breach in 2019, wherein the company mistakenly stored users' passwords in plain text. Meta's original announcement only talked about how it found some user passwords stored in plain text on its servers in January that year. But a month later, it updated its announcement to reveal that millions of Instagram passwords were also stored in easily readable format. While Meta didn't say how many accounts were affected, a senior employee told Krebs on Security back then that the incident involved up to 600 million passwords. Some of the passwords had been stored in easily readable format in the company's servers since 2012. They were also reportedly searchable by over 20,000 Facebook employees, though the DPC has clarified in its decision that they were at least not made available to external parties. The DPC found that Meta violated several GDPR rules related to the breach. It determined that the company failed to \"notify the DPC of a personal data breach concerning storage of user passwords in plaintext\" without undue delay and failed to \"document personal data breaches concerning the storage of user passwords in plaintext.\" It also said that Meta violated the GDPR by not using appropriate technical measures to ensure the security of users' passwords against unauthorized processing. \"It is widely accepted that user passwords should not be stored in plaintext, considering the risks of abuse that arise from persons accessing such data. It must be borne in mind, that the passwords the subject of consideration in this case, are particularly sensitive, as they would enable access to users’ social media accounts,\" DPC's Deputy Commissioner, Graham Doyle, said in a statement. The DPC has also given the company a reprimand in addition to the penalty. We may know more about what that means for Meta exactly when the commission publishes its full final decision and other related information in the future.",
    "commentLink": "https://news.ycombinator.com/item?id=41678840",
    "commentBody": "Meta fined $102M for storing passwords in plain text (engadget.com)153 points by redbell 10 hours agohidepastfavorite113 comments foota 9 hours agoIt's mentioned in nested comments, but (as you'd probably expect) meta does not intend to store passwords in plaintext. There was a bug where they were logging plaintext passwords for some period of time e.g., when someone tried to log in etc.,. reply nolok 8 hours agoparentAnd they're not fined for storing in plaintext, nor for storing in plaintext by mistake, they're fined because the law give a time limit for you to notify the regulator after you notice it and they waited too long. And in this specific case just to be clear it's not about taking too long to notify the public / customer, but about taking too long to notify regulator (the delay is much shorter). And they're not supposed to have perfect facts when they notify it, there is no sanction for notifying and saying \"but we're not sure yet\" if you're being honest, or coming back later with correction, there is a sanction for not telling them in time. We often see \"companies should be responsible / should have to inform me\" and that's part of that regulation, and it only works if there are clear defined delay and sanction when they're not respected. reply jart 8 hours agorootparentHow long has that even been a regulation and in which countries does it apply? Software engineers are trained to view these kinds of things as bugs. Legal isn't trained to monitor bug trackers. reply oliwarner 7 hours agorootparent> Software engineers are trained to view these kinds of things as bugs Competent engineers —software or other— must have an education in safety standards and legal regulations. I had a pretty formal education in data protection at both A-Level and undergrad. I know real engineers get tetchy about us programmers edging in, so if you want any claim to an engineering title, ignoring the ramifications of your code in the real world is unacceptable. But that doesn't seem to be the problem here. Somebody did know it was bad, did fix it urgently, did report it internally and did an impact assessment. The problem was they needed to notify the regulator earlier so they knew it could have been a problem. If these passwords were in the wild, delaying notification by however many days means attackers have more time to use stolen credentials. $100m sounds like a lot but a lot of these regulatory rules scale with the company so that punishments like this have impact. They need to improve how they handle security notification. reply bigtimesink 8 minutes agorootparentAn education in legal regulations teaches you the best solution is to make the fix and don't say anything. reply bcrl 19 minutes agorootparentprevI think the big difference is that engineers have had to care about people dying for the past 100 years. Today there are software developers that throw code out into the world that kills people without any repercussions. At some point this needs to change. reply alexmorley 8 hours agorootparentprevIt’s part of GDPR. I’ve been given training on it at all (3) companies I’ve worked for and training has always included what constitutes a breach and what to do. I would hope any company would treat it as an incident rather than just a bug where senior enough folks would be involved to know what their responsibilities are. reply JCharante 8 hours agorootparentprevit's GDPR from the subheader > The Irish Data Protection Commission found that the company violated several GDPR rules. this is why lots of websites block the EU from accessing. You basically need to consult with lawyers to make sure you're not accidentally breaking the law when writing a codebase. reply omnimus 8 hours agorootparent“lots” not really as most companies want accesss to european market. Also no you dont need to consult lawyers when writing code. You just dont track and save data and do questionable stuff with it. Saving passwords in logs is surely security issue first before its GDPR issue. reply JCharante 7 hours agorootparent> “lots” not really as most companies want accesss to european market. Plenty of foreign newspapers block the EU from accessing their sites. The EU is not that a big market. reply mdhb 7 hours agorootparentThis is also factually incorrect https://tradingeconomics.com/country-list/gdp?continent=euro... reply JCharante 7 hours agorootparentHow so? Your source shows that Asia + North America make up like 67% of global GPD. reply mdhb 1 hour agorootparentThe $18349 Billion GDP. What did you think I was referring to? Were you trying to be daft? reply JCharante 7 hours agorootparentprevyes it's a security issue but you wouldn't \"expect\" to get fined millions of dollars. Do I think we should punish companies for storing passwords in plaintext? Yes. Would I expect that a bug and devs untrained in GDPR best practices could lead to fines? No. Usually in software engineering you don't get your company fined for making terrible mistakes unless you're in a field like finance. This was just passwords which most sites have, not something like PCI DSS stuff reply nolok 1 hour agorootparent> yes it's a security issue but you wouldn't \"expect\" to get fined millions of dollars. Which is exactly why companies don't care, which is why this regulation was made and those fines decided. > Usually in software engineering you don't get your company fined for making terrible mistakes unless you're in a field like finance. You're not fined for a mistake, you're fined for a mistake AND that mistake huer the customer more than you AND you don't disclose it swiflty to him. reply maeil 7 hours agorootparentprevI see this comment pop up here often in these threads about EU fines and regulations. \"Apple/company should just call the EUs bluff and stop selling in the EU!\". Apple's a good example because they're such an incredibly global brand, who should be less reliant on EU customers. Yet Europe is responsible for >20% of their revenue. Shareholders would eat you alive for just \"nope\"ing away from that. Yes, US GDP/capita is far above the EU average, but the EU still represents 450 million, on average fairly wealthy people. So companies simply play ball. And that excludes the UK, whose data protection laws are similarly strict. reply Eddy_Viscosity2 6 hours agorootparentNoping out of europe would create a vacuum that would be filled by a competitor who would fully comply with the consumer protecting EU rules. Rules that many consumers in other nations would love to have themselves but don't because of regulatory capture and regular corruption. Those consumer friendly products would become popular outside of Europe. Big-US-monopoly-company would lose dominance. They would either have to change to be more consumer friendly or eat the loss of market share. reply JCharante 7 hours agorootparentprev20% of revenue isn't that much. Would you rather focus on your core product and double your revenue, or focus on getting that 20%? Yes giant companies have the resources and experience less YoY growth so they will work with the EU market, but most companies would do better to ignore the EU. reply nolok 58 minutes agorootparent> 20% of revenue isn't that much It is in reality gigantic, especially at that scale. And in this specific example, Apple net profit is 24% of their revenue. > Would you rather focus on your core product and double your revenue Saying you no longer sell to people with blue eyes or wearing short is not in any way increasing your sales to other people. I'm sorry to you your messages sound like you're not very knowledge about the subject matter. reply philistine 6 hours agorootparentprevExplain to me how not selling in the EU could double a company's revenue? How removing yourself from your third largest market would enlarge the others? How millions of people in the US don't buy an iPhone because you can buy iPhones in Germany? reply mdhb 7 hours agorootparentprevI probably encounter this like 5 times a year, your statement is wrong. reply dotps1 7 hours agorootparentprevTo be clear, they are absolutely being fined for storing passwords in plaintext. They chose not to mitigate the fine by following proper procedure. reply oefrha 9 hours agoparentprevTo the (intentionally?) obtuse responses: intending to store passwords in plaintext usually means storing plaintext passwords in databases and doing authentication with that; and that’s what the gazillion of commenters replying to the title are implying. Mistakenly logging credentials because of e.g. badly interacting HTTP middleware is still a very nasty bug, but it doesn’t count as intending to store passwords in plaintext. And something similar has happened to me, and I’m sure many others: logging middleware storing sensitive user data that shouldn’t be visible to engineers with mere access to logs (not as bad as logging credentials in my case, but still). reply nicolas_t 9 hours agorootparentAccording to the pci dss auditor I had when doing a pci das level 1 audit. The most common credit card leak vector is logging middlewares logging credit card nunbers reply Mountain_Skies 8 hours agorootparentprevWhen I did application security I had to argue with developers about PCI and PII data in logs all the time. They would insist that there was \"no other way\" and that it was secure inside our system. I'd refuse to change the status of the vuln to anything other than critical. I found many similar vulns in our database that had been marked as false positives or non-critical by other infosec people in the company. The common thread was none of them had a background in software development so they just trusted what the developer told them. This seems to happen frequently in places where there's a culture of compliance being more important than actual security. reply miningape 8 hours agorootparentprevCan confirm, have broken GDPR before through logging customer (and customer's customer) PII. It lasted for about a month before we realised and I had to go through the logs 1 by 1 over 3 days to remove all the data. reply slim 9 hours agorootparentprevunless you are Facebook and tightly collaborating with police and intelligence in countries with no respect for human rights reply nomilk 9 hours agorootparentprevhmm.. that's a very sympathetic take. Most frameworks blot out passwords from logs by default, so even a newbie programmer on their first day doesn't make the mistake of logging plaintext passwords, yet facebook somehow made that mistake... It should raise eyebrows when the security practices of SWEs at a billion dollar company are outperformed by any newbie developer working a toy project. reply vidarh 8 hours agorootparentPasswords are just data. If said data is not tagged in a way that makes it clear it is a password, finding an algorithm that will successfully blot out passwords in the general case is intractable without being far too aggressive to be useful. All such tools rely on assumptions about what will be logged following certain rules that the logging can check against - it's not hard to accidentally convert data to a format that when logged happens to fail these kinds of tests. They should have caught it. But it's not surprising that it occasionally happens. reply nomilk 8 hours agorootparentNot that hard [1]: Rails.application.config.filter_parameters += [ :passw, :email, :secret, :token, :_key, :crypt, :salt, :certificate, :otp, :ssn, :cvv, :cvc ] [1] https://github.com/rails/rails/blob/8a2e28d7451d5ae4cb194fcc... reply vidarh 7 hours agorootparentAssuming, of course, that the data is logged as a parameter rather than as a raw string, or as an instance variable in another object, or any number of other ways. Developers thinking it is \"not that hard\" is a big red flag to me, suggesting odds are high your logs are full of things that should not be there. Using filters is a first step only. reply tjoff 8 hours agorootparentprevHN does it, if I post my password it will automatically change it to stars, see: ************* reply benmmurphy 8 hours agorootparenthunter2 reply yakshaving_jgt 8 hours agorootparentprevcorrect-horse-battery-staple Hmm… Not sure if it worked? reply yunohn 9 hours agorootparentprevI think it’s actually much easier to make such mistakes at large companies with sprawling codebases and potential for settings that inadvertently end up logging sensitive data. Especially for nobody to notice either. reply nomilk 9 hours agorootparentTrue but they're also better resourced in terms of humans (and their experience) and tooling that should prevent or at least catch any blunders quickly. reply yunohn 8 hours agorootparentYou’d be surprised, but from my first hand FAANG experience that is definitely not the case. Tooling can capture things that are known to it - but not everything is setup perfectly. reply mulmen 8 hours agorootparentprev> It should raise eyebrows when the security practices of SWEs at a billion dollar company are outperformed by any newbie developer working a toy project. Facebook isn’t “a billion dollar” company it’s “a 1,435 billion dollar” company. Excuses start to run thin. reply sschueller 9 hours agoparentprevBoeing did not intend to have its plane crash when it installed the MCAS. reply sealeck 8 hours agorootparentAnd it turns out that plane crashes are much more serious failures than logging sensitive data internally. Not to say what Facebook has done isn't an embarrasing failure that really shouldn't happen, but they're clearly not the same thing. reply nudgeee 5 hours agoparentprevIndeed, this is a common vector for leaking PII and sensitive data. For example, what looks like an innocuous logging/print statement in an exception handler ends up leaking all sorts of data. And it gets more messy when you start to ingest and warehouse data logs for on-call monitoring/analytics/etc, and now you have PII floating around in all sorts of data stores that need to be scrubbed. In a previous job, we handled credit card numbers. We added PII detectors to logging libraries that would scrub anything that looked like a credit card number. We used client-side encryption where the credit card numbers are encrypted on the client before sending to the backend, so the backend systems never see the plain credit card numbers, except for the system that tokenizes them. reply tzs 3 hours agoparentprevAlso, it is not against GDPR per se to store passwords in plain text. It is required to keep user data safe from unauthorized access and processing and encryption is one way to help with that, but if you have it sufficiently protected by other means it would be OK under GDPR to have it in plain text. Avoiding ever storing passwords (or credit cards) in plain text [1] is actually harder than you might think. Even if you outsource password handling by using some third party authorization service so passwords should never even touch your servers, and handle credit card payments by having them handled on your checkout page by a frame or JavaScript or something that only ever sends them directly to your payment processor, you can still end up with the damn things in plain text on your servers. How? Because you receive emails to support that look like this: > Hello; I'm a subscriber to your service, account \"Bob666\", password \"S3cr!t\", billed to my credit card 4111111111111111 with security code 123. That card is expiring. My new card is 4012888888881881 with security code 712, expiration date May 2026. Please use my new card for renewals. Thanks! You may also receive messages like that in chat if you offer support via chat. And maybe even to email addresses other than support. So now you've got a plain text password and two plain text credit numbers along with their security codes stored in the inboxes of everyone who is on the support list, and possibly also somewhere in the database of your ticketing system if mail to support automatically creates a ticket. It gets worse. If you offer support by phone and that goes to voicemail after support hours you will find passwords and credit card numbers in there. [1] Note: technically probably almost nothing is actually stored in plain text nowadays. It's almost always going to stored on a filesystem that is using filesystem level encryption, and that filesystem is likely on a block device that is doing block level encryption. But I believe when people talk about \"plain text\" storage it means at a higher level. If I store the string \"this is a secret\" in foo.txt, that counts as plain text even though foo.txt is on an encrypted filesystem on an encrypted disk. reply croes 8 hours agoparentprevThat's why the fine is only $102M reply delusional 9 hours agoparentprev> Meta does not intend Is an odd concept. Is the argument that nobody noticed? If somebody noticed, but the cleanup was deffered, them they did \"intend to\". It's like defending a bank robber by saying that he didn't intend to rob the bank, he just had a gun in his hand, and then he figured the damage was already done, so he may as well get some money. reply locallost 9 hours agorootparentI think the comment is the context of being a software developer. \"Everyone\" knows you shouldn't do that, so it would be a bit odd if the company of Facebook's size would. But if it was accidental, then it makes it clearer how it happened. It's still a grave mistake, but not unthinkable. I personally write bugs all the time. reply cogman10 9 hours agorootparentI gotta level with you, not everyone knows you shouldn't do that. There's a number of devs that don't think twice about storing sensitive keys in a git repo. I could 100% see how someone would do this, see log messages with passwords in plain text, and then faff off being the last person to actually look at those logs. \"K, this case is done, what's next\" reply vidarh 8 hours agorootparentI worked somewhere not long ago where the \"solution\" was to run code to scrub repos before building release packages because these packages were in some cases installed in customer networks that drastically increased the chance they might leak. At no point did it seem developers or the devops team realised that the fact they saw the need to do this meant that maybe they should apply the same checks used to scrub in a hook to root them on commit in the first place. reply osullip 9 hours agorootparentprevLogging passwords on the fly is probably common. Some debug or log action setup and forgotten. However, if you ever see a password in plain text you should raise alarms to the highest level. In this case, I don't think the alarm was raised. reply cogman10 8 hours agorootparentI agree, but also I know of devs that don't understand the basic security implications of passwords being in logs. I could easily see how someone, maybe even a couple of people, could see these logs and think nothing of them. reply vidarh 8 hours agorootparentprevVast quantities of logs are never reviewed by anyone.... reply Culonavirus 10 hours agoprev> a senior employee told Krebs on Security back then that the incident involved up to 600 million passwords. Some of the passwords had been stored in easily readable format in the company's servers since 2012. They were also reportedly searchable by over 20,000 Facebook employees I thought this was gonna be some limited faux pas... but no. That's terrible. reply nh2 8 hours agoprev0.1 % of current revenue fine. If your company made a billion $ revenue per year, it'd have to pay $100k. Doesn't feel like a great incentive to do it right. If they improved debuggability by logging all requests to make the company more than 0.1 % efficient, it's a good deal for them. reply hannofcart 9 hours agoprevOh so the hashing and rainbow table attack questions they ask in Meta interviews is basically a cry for help? reply Topfi 9 hours agoparentSalt and pepper? Sure you aren't applying to our canteen? reply ChrisArchitect 2 hours agoprev[dupe] Discussion: https://news.ycombinator.com/item?id=41669912 reply s_dev 5 hours agoprevPrevious discussion: https://news.ycombinator.com/item?id=41669912#41672163 reply tveyben 8 hours agoprev102M$ might sound like a large sum - but the math shows that a leaked clear text password here is just fined with less than one dollar… (Yes I have read the fine is triggered by not informing the authorities in due time) Interesting how the affected user is actually valuated… reply can16358p 9 hours agoprevI really don't get how companies so large do stupid things like this. Hashing and salting passwords isn't some newly introduced advanced rocket science, it's literally a 101-level \"obvious\" thing. How can a huge corporation like Meta/Facebook can do this is beyond my imagination. reply Negitivefrags 9 hours agoparentThe usual way this happens is accidentally logging passwords. Or even other cases where passwords happen to be included in something else. It can happen more easily than you think. Like for example, if you collect server side crash dumps, are you really taking care that there is no sensitive information sitting in the memory image stored in them? reply grayhatter 4 hours agorootparentthe word you should use is carelessly, as in carelessly logging passwords. When working with data that you can reasonably expect to contain secrets, you should behave as if it does contain secrets. It worries me that you mention you're aware that server side crash dumps may contain sensitive data, but you also speak as if it's reasonable to not protect them knowing they do, or they might. I'd hope or expect anyone would mention or at least imply that it'd be negligent to behave so recklessly with someone else's secrets. reply bongodongobob 2 hours agorootparentWhat you're saying makes sense for a small startup with 20 people. Have you worked in an org of 10000+ that's been around for 30 years? Where the people who built the systems no longer work there? Where the people who maintain them are 3+ career cycles removed? Things get so baked in \"one doesn't simply make changes\" because no one person understands how everything works, so you do your best and keep closing tickets. \"Hey we should really investigate whether or not passwords are included in memory in our 2000 severs across the globe!\" \"Lol ok maybe when you close out the 30 tickets on your plate.\" reply grayhatter 40 minutes agorootparent> Have you worked in an org of 10000+ that's been around for 30 years? yes. Would you still make this argument for something else known to be dangerous? Hey this gigantic lathe doesn't have a safety shut off switch. Ahh yeah the guy that did the wiring for that left years ago so we just don't touch it. It's fine as long as no one puts their hands near it when it's running. Hey, isn't this freshly dug 20 foot hole supposed to have an escape route, and side wall reinforcements so it doesn't collapse on someone? lol ok new guy, let me know when you've finished pouring that concrete and then we'll look at that idea. To go back to the example; Taking steps to protect them could be as simple as restricting who can access core dumps, enforcing they don't get stored unencrypted, and the only people who can copy and inspect them have are already in a trusted role where they can get root on that production server. Or an even better option would be restricting the service that can read clear text passwords. These machines don't crash, and when they do, we don't write a coredump. (but this trusted team can start if we see the system crashing suddenly) > Things get so baked in \"one doesn't simply make changes\" because no one person understands how everything works, so you do your best and keep closing tickets. This is a super disappointing attitude. It's hard, and it's the way we've always done it, so that means it's impossible, or not worth the effort? Yeah, I'm not likely to buy into that mentality. I believe I can fix things and have a positive impact. It's one thing to say it's impossible because you don't understand how to do it, that's wrong, but I guess if that's what you need to believe to sleep at night, I can pretend to understand. It's another thing entirely if you don't have the autonomy at work to try to improve something you know to be broken, and a risk to users, and the company. If that's actually what you meant to describe, you might wanna consider if you can find another job. You're clearly not an idiot, and there's plenty of companies that wont treat you like a code monkey. reply progbits 9 hours agoparentprevIt's unbelievable how little most developers care about security. At this point I've given up on educating them since it went nowhere, instead I'm locking down permissions to things like firewall and secret vault so random people don't fuck it up. reply radicalbyte 9 hours agorootparentI don't see that in capable developers (who usually end up pushed to \"backend\") but it is absolutely endemic in frontend and extremely problematic in organisations which have full stack created frontend-first. All too often even the super seniors / leads have a very limit knowledge of security (or performance, reliability etc). reply wahnfrieden 9 hours agorootparentprevTheir managers don't incentivize them spending time on it, and their PM will fight security tickets they don't understand the need for. Most devs have little autonomy at orgs today and operate under a strict hierarchy of command at the ticket level. reply pbhjpbhj 8 hours agorootparentPresumably, \"we're been storing 600 Million passwords in plaintext\" is understandable to their PMs given its understandable to complete laymen. Aren't FAANG companies supposed to employ the very brightest minds. Hard to imagine this wasn't done on purpose. reply throwawaysleep 9 hours agorootparentprevUsing the terms, wallet, threat, or prestige, make me as a non aultristic person care about the security of the systems my employer owns. reply herval 4 hours agoparentprevAn accidental print statement on the login page. That’s all it takes. reply fulafel 9 hours agoparentprevBig organizations just have a lot of bureucracy attempting to codify common sense. I bet they have paper saying you shouldn't do this. reply vanjajaja1 9 hours agoparentprevit happens because you have one component logging everything for traceability, and it hosts the interconnect between 2 other components which need to communicate passwords. thus the password accidentally slips into a plain text log file generally there are tools that search for & flag PII logging, if it slips through the tools its because there are layers of indirection involved reply rjzzleep 9 hours agoparentprevMost of the times it's when people reinvent stuff from scratch. That's why I'm always careful when people implement their own framework in the next shiny thing. Lack of hashing, CSRF, basic XSS issues happen all the time in rust, golang and especially the node.js web crowd. reply tjpnz 9 hours agoparentprevWhen you select candidates based on whether they know how to invert a BST and other trivia it's not terribly surprisingly. reply FartyMcFarter 9 hours agorootparentHow would you select candidates to make sure they avoid this kind of security bug? reply pbhjpbhj 8 hours agorootparentAvoiding the bug in the first place is not the big issue, surely. An employee can leave a door open to a secure area, if you employ security people they should have worked to mitigate that, and to catch it if it did happen. Well \"we want to run an audit to find if any passwords are stored in plaintext in our file systems\". Sounds like a problem any highschooler could answer? The idea of an audit might need a person who has done a remedial level of security work. reply Arbortheus 9 hours agorootparentprev“You are building an account authentication system and need to store user passwords, what design measures would you take to ensure this would be done securely?” Follow up question. “What measures would you take to ensure this complies with relevant data protection regulations?” If they give a somewhat competent answer, you at least know they’re the sort of developer that can critically analyse this sort of thing. reply tjpnz 8 hours agorootparent>What measures would you take to ensure this complies with relevant data protection regulations? We hire devs to work on safety critical systems and do gauge for familiarity with standards applicable to our industry. This happens regardless of whether you're mid career or a new grad. I don't think it unreasonable to expect similar of devs working on systems where there are security or privacy considerations (which might as well be everything). reply bongodongobob 9 hours agoparentprevI'm honestly never surprised by any of this stuff. I've done some contracting and file access is always a shitshow. Picture this: > Intern or contractor gets hired. > Someone runs a script to create the user because permissions have turned into a rats nest that no human can understand > No one knows how the script works anymore, it's probably outdated and only does 60% of the job > User is added to a quagmire of groups > I cant access the Citrix apps > \"Oh you don't have access and I'm waiting on that app owner to give you access, give me a few days\" > Ok it works but I don't have a login for \"the program I need\" > \"Looks like there's a licensing issue, give me a few days\" > Ok it works but I don't have credentials for the database connection > \"It's legitimately complicated, give me a few days\" > IT gets tired of fucking with the back and forth on this 13 day old ticket \"Just give him everything, so he can get X done\" > Get access > Finally. Let's just dump this out into another file/folder/location so I don't have to go through hoops again and I can actually do my job. Now that dump is in a remote profile folder or /temp folder somewhere in a 100TB blob that will be backed up for 7 years or more. I've been on both sides. reply bongodongobob 1 hour agorootparentTo the comment below about measuring security... Usually this comes from putting security theater over pragmatism. Everything is so granular it's impossible to figure out what people need so you fuck around with it for a day and then give up and just give em local admin because actual work needs to be done. You can't get away from the fact that work requires write access. All those groups and policies are meaningless when the rubber hits the road. In the end you just have to hire people who can be trusted to do the right thing and not burn the house down. reply bberrry 9 hours agoprevI would hope it's not the authentication team's systems that are logging payloads with passwords.. they should definitely know better. Presumably it happened some infrastructure component owned by another team. reply junon 5 hours agoparentWhy are passwords leaving the auth services though? reply DexesTTP 10 hours agoprevContext: This is for a 2019 data breach on a system that was created in 2012. The GDPR was instated in 2018 (has it really been that long? Wow feels like yesterday) and Meta failed to disclose the 2019 data breach properly under GDPR, hence the fine. reply sakisv 9 hours agoparentHonest question: How was it discovered? Was it reported by a pentester? (ex-)employee? Facebook itself? How do we know that it goes back to 2012? I know in the public sector you have to disclose such things to ICO, but does that also apply to private companies? Who is going to hold them accountable? reply chrismorgan 9 hours agoparentprevI was concerned, reading your thing first, that the title (“Meta fined $102M for storing passwords in plain text”) was going to be false—that they were actually only fined for not disclosing the breach. But the article says the decision also claimed a GDPR violation for storing the passwords in plaintext, so that’s good: > The DPC found that Meta violated several GDPR rules related to the breach. It determined that the company failed to \"notify the DPC of a personal data breach concerning storage of user passwords in plaintext\" without undue delay and failed to \"document personal data breaches concerning the storage of user passwords in plaintext.\" It also said that Meta violated the GDPR by not using appropriate technical measures to ensure the security of users' passwords against unauthorized processing. reply pibefision 10 hours agoparentprevGDPR fine is 4% of global turnover from previous fiscal year. 102m seems low to me. reply poincaredisk 9 hours agorootparentThat's the maximal fine (that was never used as far as I know, at least on a large company). In this case the fine is understandably much smaller, since the privacy incident is not critical, and Facebook reported the problem to the authorities on its own. reply bootsmann 9 hours agorootparentprevThats the maximal fine I think, the judges can set the amount depending on the severity of the violation. reply AmericanChopper 10 hours agoprevThis is a very imaginative use of the word “breach”, according to the details reported in the article at least. Internal staff (inadvertently) had access to users plaintext passwords. The article doesn’t mention any use of these credentials in a breach though, and doesn’t make any refutation of Meta’s claim that this never occurred. Internal staff having access to my data is what I would normally expect from a service like the ones Meta operates. It’s a bad mistake to make, but contriving these circumstances into being a “breach” is a bit more mask-off than I’m used to the Data Protection agencies being. Hope Ireland makes good use of its $102M. reply nomilk 9 hours agoparentIn many senses, internal staff having access to plaintext passwords is a breach. reply AmericanChopper 9 hours agorootparentIt’s a control failure, not a breach. It would also be an incident, one that could result in a subsequent breach, or one that warrants some work to be done to ensure it does not turn into a breach. But it has not resulted in an unauthorised party gaining access to the data, and is therefor not a breach. reply ethbr1 9 hours agorootparentI think it's impossible to say there was no breach, given they were exposed for 7 years. reply AmericanChopper 9 hours agorootparentI would agree. Which is why I’m suggesting that the Irish Data Protection Commission and Engadget should refrain from saying that. It’s also impossible to say that I am not responsible for a breach of your private data either. How much should the Irish Data Protection Commission fine me? reply ethbr1 5 hours agorootparent\"Potentially breached\" would probably be an accurate phrasing. If you had my private data written in the back of a notebook, that you carried around with you to coffee shops, for a few days, I'd feel substantially better than if you did it for a few years. Likelihood that someone peeked scales with time. reply AmericanChopper 4 hours agorootparentSure, and I don’t disagree that it’s a bad situation for Meta to have created. It’s being fined for “potentially violating” a statute that I find objectionable. Being breached implies that some harm befell consumers, this article (and the others I’ve read about this incident) don’t make any reference to an actual harm being uncovered. reply ethbr1 4 hours agorootparentI think there should be a \"reasonable expectation\" of a breach having happened. Secrets laying in an accessible place for 7 years... reasonable expectation is someone looked at them. reply AmericanChopper 3 hours agorootparentIt’s a reasonable possibility, it’s also a reasonable possibility that nobody did. But somebody incidentally seeing them, and maybe, maybe not recognising they were passwords is not a breach. Somebody intentionally misusing them would be, and I haven’t seen anything to suggest there’s a reasonable expectation that that occurred. A reasonable expectation is also not the standard of proof I’d generally like to see from a government attempting to enforce a penalty. reply ethbr1 3 hours agorootparentI think knowledge of them would absolutely be a breach, because you wouldn't be able to guarantee that person didn't remember and subsequently misuse them. reply AmericanChopper 2 hours agorootparentIf the data was publicly leaked then this argument would have some merit. But it wasn’t, the only people who could have accessed these passwords were insiders, and those passwords were used to protect data that many of them would have access to anyway. There is no evidence any unauthorised party gained access to any private data as a result of this incident. There is no evidence that any authorised party misused data as a result of this incident. A “breach” that involves no unauthorised access, and no misuse is not a breach. A control failure occurred, and it was remedied in the most appropriate way possible. reply markarichards 7 hours agorootparentprevIt's relatively common for publications to lazily only reference an action that resulted in a legal outcome, rather than the justification provided for the outcome. For instance, Bob imprisoned for car bomb rather than Bob imprisoned after judgement rules deaths unlawfully resulted from Bob's malicious car bombing. Had Bob's car bomb been on a film set and no one hurt, Bob would hopefully be fine. If you read coverage with this in mind, then what matters is more a case of how likely an action is to be unlawful and thus how lazy the publication is being. If someone blows up a car, we'd assume it was unlawful. If a company stores passwords unlawfully we'd assume it was unlawful and hopefully for good reason... From GDPR: \"personal data breach’ means a breach of security leading to the accidental or unlawful destruction, loss, alteration, unauthorised disclosure of, or access to, personal data transmitted, stored or otherwise processed\" A typical security policy for securing passwords is to never store them in plaintext. It would be a rare situation for the storage to not be accessible (what would be the point of storing it). Thus it would seem fair to assume that in most cases plain text storage of passwords would be a breach of security (internal controls breach) would implicitly also be a breach of personal data (legal definition) as it would at the very least be accidentally accessible to staff, contractors or third parties (whoever hosts the storage). So, it will likely fit the definition of a breach. But, it still needs to escalate to a point where it would be recognised as serious enough to warrant action (like reporting to data subjects or regulators). There are situations where storing passwords in plaintext may not warrant reporting or fines, such as if upon realising the breach it was evident that nobody had accessed the data and it was destructed before harm could be realised; but I doubt anyone would ever know about these situations happening in companies so it's fair to assume they wouldn't reach major news sites. reply AmericanChopper 7 hours agorootparentEven by the broadest possible definition of a breach, this is still just a control failure rather than a breach. The control that failed might have made it possible for Meta employees to perpetrate a breach, but the article makes no mention of that happening, or provides any suggestion that there is evidence that it might have happened. At at least one point in my career, I have also accidentally mishandled password data (I accidentally leaked them into a log one time - well one time that I know of at least). When I did that I caused a control to fail, and I caused a security incident that required follow up remediation work (including password resets and disclosure), which is exactly what happened here. But I did not cause a data breach to occur. I struggle to image a world where I could have caused my employer to be fined $102M for that incident, and for that to be deemed a data breach, when there is no evidence (presented or referenced in this article at least) that a breach ever occurred. If I leave the office and forget to lock the door, I've caused a control failure. But if nobody comes in to rob us, then I haven't caused a robbery or a breach or anything else like that to occur, even if a typical security policy might require me to lock the door before leaving. The creativity required to come to this conclusion doesn't do anything to improve the credibility of the GDPR, which from an outside perspective really doesn't look like anything other than an import tariff on foreign tech in disguise. reply markarichards 18 minutes agorootparentI like to think of a breach as hole through into the hull... they don't mean the boat will sink or even ever will sink; just that the layers of security protections has been compromised. In the case you mention it seems that happened too: internal actors could reach plaintext passwords and thus for safety the company responded by forcing password reset and disclosure (commendable as I know of companies that would not). The term \"personal data breach\" is useful because it defines the range of breaches that the law focuses on (it's not interested in business data or incidents where the first layer of defence fell but the second kept it secure). I feel it's a bit like having a determination for \"road traffic incident\". It helps the public, police, etc identify what is in scope... just because you have one doesn't mean you'll lose your licence or be fined - that depends on a range of factors regarding the lead up to the incident: what happened before, during and after. Similar with data breaches. If a company has a breach it does not mean much in GDPR unless other factors are considered, so I wouldn't worry about being too focused on the term breach. reply shprd 9 hours agoparentprev> This is a very imaginative use of the word “breach” You're mistaken. You might be thinking of breach in terms of \"hacking into\", but they used it as: personal data breach Which accurately means \"unauthorised access to personal data\"[0] and seem to be the language used by the DPC. [0] - https://ico.org.uk/for-organisations/law-enforcement/guide-t... reply AmericanChopper 9 hours agorootparentThat describes an entirely different incident to the one referenced in this article. reply shprd 9 hours agorootparentEdited the link out. It doesn't make a difference anyway for the purpose of this discussion. reply AmericanChopper 9 hours agorootparentYou didn’t edit the link out, you replaced your comment with a completely different one. I always though HN was pretty good at preventing ninja edits like that… reply smittywerben 7 hours agoparentprevI don't know about you but if tens of millions of passwords stored in plaintext are accessible to 80k people they're as good as useless now. You're thinking too much like \"hacker selling data security\" and not enough like \"stalker who works at facebook logged into my gmail because I use the same password as my facebook\" regular bob security. Just because you didn't end up in a dataset on some forum doesn't mean that someone's ex has a boyfriend who started his first day at facebook and left his laptop unattended and the ex saw see your facebook password is your \"cat's name + 123\" in the debug log and nobody at Facebook says anything for years or something. Anyways I think it's fine for them to define breach as the loss/destruction of data i.e. making a password known, which destroys it's value. reply AmericanChopper 4 hours agorootparentIf this control failure allowed malicious insiders to access private data, and misuse people’s personal accounts, then a data breach would have actually occurred. But I haven’t seen any suggestion that this happened, only references to the possibility that it might have happened. I’m really just thinking like somebody who believes that if the government is going to punish you for something, then I believe the event you’re being punished for should have actually occurred, and also that they should be able to prove it occurred. If reference to standard security policies formed part of the basis of this decision (as the article states), then the harm that you’re trying to contrive into existence here also has no merit. There is no framework of information security that allows for a password to permanently retain its value as a secret keeping tool. Conventionally passwords have only retained their value for a set period of time, and even the most modern security standards for managing secrets requires you to rotate them at even the most remote possibility that they were exposed. The idea that a password rotation has harmed Facebook users, and the implication that their password was a valuable asset that they could reasonably expect to retain its value forever is quite ridiculous. reply smittywerben 1 hour agorootparentI could agree that this fine is bureaucratic Big Compliance enforcing its made-up standards. At the same time, it's hard for me to feel bad for Facebook. If someone's violating internal auditing procedures, those same procedures won't catch them. It's dangerous because it's a violation of the procedure itself. Proving such violations without tools like no-knock warrants or the NSA moving in is nearly impossible. So you end up with a misappropriated circus of Big Compliance issuing fines over no wrongdoing and internal audits finding no wrongdoing when you rarely hear about this type of internal abuse unless someone is careless enough to brag about it to their Tinder date. reply Myrmornis 8 hours agoprevWho gets the money and what will it be spent on? reply qwerty456127 8 hours agoprevWow I didn't know this is illegal. reply grayhatter 4 hours agoparentyou didn't know that it was illegal to be careless in a way that's well known to cause harm to other people? reply cubefox 8 hours agoparentprevCertainly depends on the country. reply schleck8 9 hours agoprevCan't wait to give them access to everything I do on the daily by wearing their AR glasses. reply nomilk 10 hours agoprev [2 more] [flagged] Mountain_Skies 8 hours agoparent [–] Most of them have signed an NDA promising not to do it. Is that not enough? /s reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Meta has been fined $102 million by the Irish Data Protection Commission (DPC) for storing user passwords in plain text, violating GDPR rules.",
      "The investigation found millions of Instagram passwords stored in an easily readable format, accessible to over 20,000 Facebook employees but not external parties.",
      "The DPC criticized Meta for failing to promptly notify them of the breach and not using appropriate security measures, issuing a reprimand with further details to be published later."
    ],
    "commentSummary": [
      "Meta was fined $102M for delaying notification to regulators about a bug that stored passwords in plain text, violating GDPR rules.",
      "The incident involved up to 600 million passwords being accessible to over 20,000 employees since 2012.",
      "The fine underscores the critical importance of timely disclosure and proper handling of sensitive data."
    ],
    "points": 153,
    "commentCount": 113,
    "retryCount": 0,
    "time": 1727512684
  },
  {
    "id": 41676646,
    "title": "Arch Linux and Valve Collaboration",
    "originLink": "https://lists.archlinux.org/archives/list/arch-dev-public@lists.archlinux.org/thread/RIZSKIBDSLY4S5J2E2STNP5DH4XZGJMR/",
    "originBody": "lists.archlinux.org Sign In Sign Up Manage this list Sign In Sign Up × Keyboard Shortcuts Thread View j: Next unread message k: Previous unread message j a: Jump to all threads j l: Jump to MailingList overview Arch Linux and Valve Collaboration older RFC Final Comment Period: License... Levente Polyak 27 Sep 2024 27 Sep '24 11:06 p.m. We are excited to announce that Arch Linux is entering into a direct collaboration with Valve. Valve is generously providing backing for two critical projects that will have a huge impact on our distribution: a build service infrastructure and a secure signing enclave. By supporting work on a freelance basis for these topics, Valve enables us to work on them without being limited solely by the free time of our volunteers. This opportunity allows us to address some of the biggest outstanding challenges we have been facing for a while. The collaboration will speed-up the progress that would otherwise take much longer for us to achieve, and will ultimately unblock us from finally pursuing some of our planned endeavors. We are incredibly grateful for Valve to make this possible and for their explicit commitment to help and support Arch Linux. These projects will follow our usual development and consensus-building workflows. [RFCs] will be created for any wide-ranging changes. Discussions on this mailing list as well as issue, milestone and epic planning in our GitLab will provide transparency and insight into the work. We believe this collaboration will greatly benefit Arch Linux, and are looking forward to share further development on this mailing list as work progresses. [RFCs]: https://rfc.archlinux.page/ Attachments: OpenPGP_signature.asc (application/pgp-signature — 833 bytes) 8 0 Show replies by date 1 Age (days ago) 1 Last active (days ago) List overview Download 0 comments 1 participants Add to favorites tags participants (1) Levente Polyak Powered by HyperKitty version 1.3.11.",
    "commentLink": "https://news.ycombinator.com/item?id=41676646",
    "commentBody": "Arch Linux and Valve Collaboration (archlinux.org)140 points by jrepinc 19 hours agohidepastfavorite15 comments uyzstvqs 18 hours ago> Valve is generously providing backing for two critical projects that will have a huge impact on our distribution: a build service infrastructure and a secure signing enclave. It sounds like Valve is investing in the security of Arch Linux's build infrastructure to prevent supply chain attacks. reply cyanmagenta 15 hours agoprevI really hope Arch moves to a model like Debian where all packages are built by a central build server. The current strategy—having dozens of different developers compile stuff on their laptops, sign it personally, and then upload the binary blob—leaves a bit to be desired for obvious reasons. reply AmpsterMan 6 hours agoparentAs a novice Arch user, I never realized this is why I needed to update keys often reply wiktor-k 11 hours agoparentprevThat's exactly what's happening. reply tetris11 9 hours agoparentprevIf builds are reproducible, what's the issue? reply cyanmagenta 7 hours agorootparent> If the builds are reproducible, what’s the issue? Because they’re not! Many packages (including the kernel itself, python, gcc, et al.), are not reproducible. See https://reproducible.archlinux.org/ reply NekkoDroid 4 hours agorootparentThe Kernel isn't reproducible due to modules being signed with a temporary key that gets created and discarded during build. This is to allow loading those modules when secure boot is enabled, without the user needing to sign each and everyone themselves reply hedora 4 hours agorootparentThat sounds like a design gone horribly wrong. Why not have the package maintainer build once (with all the modules that arch distributes), sign the output with temporary keys, and then add the signatures to the package source? If the build is reproducible, the signatures will match the output of future builds. If the user wants to use a custom kernel module, they’ll need to either rebuild with a new key, or turn off safe boot. That’s vastly preferable than opening the entire kernel to build infrastructure attacks! reply NekkoDroid 3 hours agorootparent> If the build is reproducible, the signatures will match the output of future builds. Anything that contains traces of a private key is not reproducible. The public key needs to be embedded in the kernel to be able to load the signed modules. If you distribute them without signature you can't load them due to the kernel not trusting them, if you sign them in any way with a private key they aren't reproducable since you don't want to hand out the private key. And to prevent additional random stuff being signed with the private key it gets discarded. > If the user wants to use a custom kernel module, they’ll need to either rebuild with a new key, or turn off safe boot. Or you can just sign the additional modules (e.g. DKMS) with the same key you sign the kernel & bootloader that you need to enroll into the UEFI anyway. It is less work on the users end and if the distro themselves wanna enable secure boot without user intervention via shim they need to do the signing stuff anyway. reply beeflet 18 hours agoprev>secure signing enclave wonder what this involves? TPM stuff? reply T3OU-736 15 hours agoparentUsually, at that level, an HSM (Hatdware Security Module (ex: https://www.entrust.com/products/hsm), but also a fair number of processes and procedures around things like private key generation, key attestation, key verification, certificate renewals, etc etc etc). There are some parallels with a TPM, but also a great deal of divergence (more so than in common, really). reply Foxboron 12 hours agoparentprevPartially. David Runge held a talk about the Secure Signing enclave at All-Systems-Go. https://media.ccc.de/v/all-systems-go-2024-263-boring-infras... reply brnt 13 hours agoprev [–] I hope a stable branch may result. reply RandomThoughts3 9 hours agoparentArch already has a stable branch. If you mean a branch which doesn’t update packages and where \"maintainers\" pretend they back port \"essential\" fixes by randomly patching what they ship, I hope it never happens because it would mean Arch is truly dead. reply Am4TIfIsER0ppos 4 hours agoparentprev [–] Stability is attained by never updating. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Arch Linux is partnering with Valve to develop a build service infrastructure and a secure signing enclave, addressing significant challenges without solely depending on volunteers.",
      "This collaboration aims to accelerate progress and enable new projects, with development following standard workflows and transparent communication through RFCs, mailing lists, and GitLab.",
      "The partnership is anticipated to significantly benefit Arch Linux, with further updates to be provided via the mailing list."
    ],
    "commentSummary": [
      "Valve is supporting two key projects for Arch Linux: a build service infrastructure and a secure signing enclave to improve security.",
      "There is a debate among users about adopting a centralized build model similar to Debian's, due to security concerns with the current decentralized approach.",
      "Valve's involvement seeks to tackle issues with reproducible builds, particularly for the kernel, and the complexities of secure boot."
    ],
    "points": 140,
    "commentCount": 15,
    "retryCount": 0,
    "time": 1727480871
  },
  {
    "id": 41679994,
    "title": "Remember That DNA You Gave 23andMe?",
    "originLink": "https://www.theatlantic.com/health/archive/2024/09/23andme-dna-data-privacy-sale/680057/",
    "originBody": "Health Remember That DNA You Gave 23andMe? The company is in trouble, and anyone who has spit into one of the company’s test tubes should be concerned. By Kristen V. Brown Illustration by Akshita Chandra / The Atlantic. Source: Getty. September 27, 2024, 3:38 PM ET Share Save Listen-1.0x+ 0:007:56 Produced by ElevenLabs and News Over Audio (NOA) using AI narration. 23andMe is not doing well. Its stock is on the verge of being delisted. It shut down its in-house drug-development unit last month, only the latest in several rounds of layoffs. Last week, the entire board of directors quit, save for Anne Wojcicki, a co-founder and the company’s CEO. Amid this downward spiral, Wojcicki has said she’ll consider selling 23andMe—which means the DNA of 23andMe’s 15 million customers would be up for sale, too. 23andMe’s trove of genetic data might be its most valuable asset. For about two decades now, since human-genome analysis became quick and common, the A’s, C’s, G’s, and T’s of DNA have allowed long-lost relatives to connect, revealed family secrets, and helped police catch serial killers. Some people’s genomes contain clues to what’s making them sick, or even, occasionally, how their disease should be treated. For most of us, though, consumer tests don’t have much to offer beyond a snapshot of our ancestors’ roots and confirmation of the traits we already know about. (Yes, 23andMe, my eyes are blue.) 23andMe is floundering in part because it hasn’t managed to prove the value of collecting all that sensitive, personal information. And potential buyers may have very different ideas about how to use the company’s DNA data to raise the company’s bottom line. This should concern anyone who has used the service. DNA might contain health information, but unlike a doctor’s office, 23andMe is not bound by the health-privacy law HIPAA. And the company’s privacy policies make clear that in the event of a merger or an acquisition, customer information is a salable asset. 23andMe promises to ask its customers’ permission before using their data for research or targeted advertising, but that doesn’t mean the next boss will do the same. It says so right there in the fine print: The company reserves the right to update its policies at any time. A spokesperson acknowledged to me this week that the company can’t fully guarantee the sanctity of customer data, but said in a statement that “any scenario which impacts our customer's data would need to be carefully considered. We take the privacy and trust of our customers very seriously, and would strive to maintain commitments outlined in our Privacy Statement.” Certain parties might take an obvious interest in the secrets of Americans’ genomes. Insurers, for example, would probably like to know about any genetic predispositions that might make you more expensive to them. In the United States, a 2008 law called the Genetic Information Nondiscrimination Act protects against discrimination by employers and health insurers on the basis of genetic data, but gaps in it exempt providers of life, disability, and long-term-care insurance from such restrictions. That means that if you have, say, a genetic marker that can be correlated with a heart condition, a life insurer could find that out and legally deny you a policy—even if you never actually develop that condition. Law-enforcement agencies rely on DNA data to solve many difficult cases, and although 23andMe says it requires a warrant to share data, some other companies have granted broad access to police. You don’t have to commit a crime to be affected: Because we share large chunks of our genome with relatives, your DNA could be used to implicate a close family member or even a third cousin whom you’ve never met. Information about your ethnicity can also be sensitive, and that’s encoded in your genome, too. That’s all part of why, in 2020, the U.S. military advised its personnel against using consumer tests. Read: Big Pharma would like your DNA Spelling out all the potential consequences of an unknown party accessing your DNA is impossible, because scientists’ understanding of the genome is still evolving. Imagine drugmakers trolling your genome to find out what ailments you’re at risk for and then targeting you with ads for drugs to treat them. “There’s a lot of ways that this data might be misused or used in a way that the consumers couldn’t anticipate when they first bought 23andMe,” Suzanne Bernstein, counsel at the Electronic Privacy Information Center, told me. And unlike a password that can be changed after it leaks, once your DNA is out in the wild, it’s out there for good. Some states, such as California, give consumers additional genetic-privacy rights and might allow DNA data to be deleted ahead of a sale. The 23andMe spokesperson told me that “customers have the ability to download their data and delete their personal accounts.” Companies are also required to notify customers of any changes to terms of service and give them a chance to opt out, though typically such changes take effect automatically after a certain amount of time, whether or not you’ve read through the fine print. Consumers have assumed this risk without getting much in return. When the first draft of the human genome was unveiled, it was billed as a panacea, hiding within its code secrets that would help each and every one of us unlock a personalized health plan. But most diseases, it turns out, can't be pinned on a single gene. And most people have a boring genome, free of red-flag mutations, which means DNA data just aren’t that useful to them—at least not in this form. And if a DNA test reveals elevated risk for a more common health condition, such as diabetes and heart disease, you probably already know the interventions: eating well, exercising often, getting a solid eight hours of sleep. (To an insurer, though, even a modicum of risk might make someone an unattractive candidate for coverage.) That’s likely a big part of why 23andMe’s sales have slipped. There are only so many people who want to know about their Swedish ancestry, and that, it turns out, is consumer DNA testing’s biggest sell. Read: DNA tests are uncovering the true prevalence of incest Wojcicki has pulled 23andMe back from the brink before, after the Food and Drug Administration ordered the company to stop selling its health tests in 2013 until they could be proved safe and effective. In recent months, Wojcicki has explored a variety of options to save the company, including splitting it to separate the cash-burning drug business from the consumer side. Wojcicki has still expressed interest in trying to take the company private herself, but the board rejected her initial offer. 23andMe has until November 4 to raise its shares to at least $1, or be delisted. As that date approaches, a sale looks more and more likely—whether to Wojcicki or someone else. The risk of DNA data being misused has existed since DNA tests first became available. When customers opt in to participate in drug-development research, third parties already get access to their de-identified DNA data, which can in some cases be linked back to people’s identities after all. Plus, 23andMe has failed to protect its customers’ information in the past—it just agreed to pay $30 million to settle a lawsuit resulting from an October 2023 data breach. But for nearly two decades, the company had an incentive to keep its customers’ data private: 23andMe is a consumer-facing business, and to sell kits, it also needed to win trust. Whoever buys the company’s data may not operate under the same constraints. About the Author Kristen V. Brown is a staff writer at The Atlantic. More Stories Who Should Get to Have Kids? Public-Health Officials Should Have Been Talking About Their Sex Parties the Whole Time",
    "commentLink": "https://news.ycombinator.com/item?id=41679994",
    "commentBody": "Remember That DNA You Gave 23andMe? (theatlantic.com)126 points by _Microft 5 hours agohidepastfavorite122 comments wenc 4 hours agoOne of the concerns I had when I did 23andMe many years ago was that someone in the future would be able to recreate me Jurassic-Park-style. Now I'm not an expert, but based on my reading of how they collect data, turns out 23andMe doesn't actually have my DNA sequence data. They do what is called \"genotyping\"[1] which is much cheaper than full \"sequencing\" [2], but which only probes for a limited set of known variants. So it's only partial information. Since then 23andMe has launched a more expensive 23andMe+ Total Health offering, which does full sequencing, but like most people, I never subscribed to that package. In fact I had no occasion to interact with the company after the first result, which may be why they are struggling financially. DNA testing is something most people might do at most once in their lives -- then they lose interest. [1] https://customercare.23andme.com/hc/en-us/articles/202904610... [2] https://customercare.23andme.com/hc/en-us/articles/202904600... reply bilsbie 4 hours agoparentIt wouldn’t be you in any case. Basically the same as an identical twin. reply AuryGlenz 4 hours agorootparentAlso, why would they bother? Maybe if you were a celebrity of some sort, maybe, but even then.. reply layer8 2 hours agorootparentThey could sell the organs back to you when you need one. Or the whole body for a brain transplant. (This was a plot point in a sci-fi story I vaguely remember.) reply braunshedd 2 hours agorootparentYou're probably thinking of \"The Island\"[1]. Good movie, I saw this myself a long time ago. [1] https://m.imdb.com/title/tt0399201/ reply layer8 2 hours agorootparentIt’s a different one and I actually remember it better, but I didn’t want to spoil the plot twist for that story by naming it. reply Teever 3 hours agorootparentprevI see a near future scam where people seek out genetic material of the wealthy to create children so that they can sue them for child support. Imagine someone doing this to the judge that ruled they owe child support for their child from a previous marriage out of spite. reply alexey-salmin 2 hours agorootparentprevEven that is unlikely. We don't yet know what exact information is enough to recreate a human (or his twin) but by now we know that DNA alone is not. reply Dibby053 4 hours agoparentprev23andMe being a Google spin-off I wouldn't be surprised if they do a full sequencing regardless of whether you pay for it or not. At the very least I would expect them to store the samples indefinitely so they can do it once it's economically viable. reply blktiger 3 hours agorootparentAs I understand it, full sequencing is a _lot_ more expensive. 23 and me currently shows the genotyping is like $100 while sequencing is $1000. It’s probably too expensive and time consuming for them to sequence everybody’s samples unless they pay for it. reply AbstractH24 3 hours agorootparentOnly a couple years ago full sequencing was closer to $10,000 Everyone knew price would drop in time, but, assuming you’re correct, it’s amazing to hear that’s happening reply glitchc 2 hours agorootparentprevHousing the samples in a controlled environment for an extended period of time is likely way more expensive than sequencing. I suspect the samples are discarded once genotyped. reply aag 2 hours agorootparentprevIt's not technically a Google spin-off. Google and Sergey Brin invested in it, but it was never part of Google. reply falcolas 2 hours agoparentprevThis data would be of much more interest to your health insurance company. The ultimate preexisting condition pricing guide. Mostly a US concern, as usual. reply pfdietz 2 hours agorootparentHealth insurance companies are prohibited by law in the US from considering preexisting conditions. reply collingreen 2 hours agorootparentThe point of this attack vector is to drop patients who are high risk BEFORE they get diagnosed so they can claim it isn't for any existing condition. This is especially scary in small, closely related communities like Iceland or Utah where a relatively low number of collected samples can be extrapolated to large swaths of the population that didn't give their genetic info away. Also that protection is relatively new in the US and is constantly being attacked politically so it isn't unreasonable to think about a world where it is removed like other longstanding health related protections. reply yumraj 2 hours agorootparentprevYou mean, “at present”.. No way to know how the regulation will change in the future depending on the amount of lobby dollars that are thrown. reply pfdietz 2 hours agorootparentIf you want to hand wring about future changes you don't need to look at dubious inference of risk from DNA. Just consider cancellation of insurance policies if you come down with some illness that requires expensive long term treatment. reply personalityson 4 hours agoparentprevnext [6 more] [flagged] pixelpoet 4 hours agorootparentCurt imperative demand makes this sound like an AI prompt, and I truly hate the trend (see also \"sound on\"). reply stavros 3 hours agorootparentWhat's \"sound on\"? The thing on videos? reply jonathrg 4 hours agorootparentprevWhat is \"sound on\"? reply pixelpoet 4 hours agorootparentYou see it all the time in videos, where they demand you not view with sound off. Every time I see it I can only think \"No, I don't think I will. Get absolutely fucked.\" reply jonathrg 3 hours agorootparentRight, I've seen this occasionally. I don't really mind since it's usually because there is some unexpected audio that you might miss if you're watching on mute. reply carimura 4 hours agoprevWhat options do we have to delete our data? Anyone looked in detail? [edit] From their website: Data retention 23andMe will retain some information to comply with legal obligations, including your DNA, sex, and date of birth So apparently you can permanently delete your data, except for, oh just your DNA.... [edit 2] From NYT (https://archive.is/ynvDR) However, 23andMe uses a laboratory that must follow regulations under the Clinical Laboratory Improvement Amendments, or CLIA. This means that some data, including your DNA, sex and date of birth will be retained in order to comply with these regulations. The company will no longer use that information, though. You can read more about the company’s deletion processhere. reply hedora 4 hours agoparent…because it would be illegal for them not to keep it? I wonder which law supposedly says this. reply matrix2003 4 hours agorootparentI misinterpret 23andme results and believe I have a horrific genetic condition. I then delete my data, commit suicide, and my family sues the company. I’m not saying this is what they are worried about, but it could be something along those lines. I work in big finance, and there is a LOT of regulation around data retention, and it’s a lot more nuanced than people think. Should we be allowed to delete the data? Absolutely. This will likely be a hallmark case setting president for the future. reply notpushkin 3 hours agorootparentThat’s one amusing typo. (Won’t lie, I did consider for a moment what would happen if setting president was done by someone’s DNA.) reply sorokod 1 hour agorootparentprevWhy would the data being deleted be a precondition to a law suit in this scenario? reply ossyrial 2 minutes agorootparentIt wouldn't, but 23andme keeping DNA data means that they would easily be able to deal with the lawsuit in this hypothetical scenario. rolph 3 hours agorootparentprevstonewalling, using the law as an excuse for noncompliance. i suspect heavy coercion has occured, its too useful to agencies to let it be deleted. reply freeqaz 4 hours agoparentprevThat's absurd. I wonder why? Because they need it to make their family tree work? Does anybody know why? reply jayknight 3 hours agorootparentHere are the CLIA retention regulations. https://www.ecfr.gov/current/title-42/chapter-IV/subchapter-... reply zero-sharp 5 hours agoprevGattaca here we come. Can't wait until I start getting denied for service/opportunity X because some computer somewhere used DNA from a distant relative to determine I'm a risk. reply ZitchDog 3 hours agoprevIt's not just people who gave them data who should be worried. If a close relative gave their info to 23andme, they probably have enough data to associate your DNA with that relative. For instance my mom did 23andme and now 50% of my DNA is in this private commercial database without my consent and this data is completely unencumbered by HIPAA privacy restrictions. reply layer8 2 hours agoparentAll humans share 99% of their genome, so it’ll be more than 50%. ;) reply webninja 5 hours agoprevhttps://12ft.io/https://www.theatlantic.com/health/archive/2... reply cebert 5 hours agoprevWe need more legislation and legal protections in this area. reply n3storm 5 hours agoparentbut that will end up business drive! don't you dare or lot of stakeholders will have to take the bus! reply KingOfCoders 4 hours agoparentprevnext [3 more] [flagged] ryanschaefer 4 hours agorootparentWhat laws are present in the EU that individuals in US could look to to write to our representatives about? reply dns_snek 1 hour agorootparentGDPR would be the obvious one, particularly the bits about them not being allowed to refuse service if you don't agree to data processing that isn't strictly necessary to provide the service (e.g. sharing data with insurance companies) reply accrual 5 hours agoprevI checked the \"destroy my sample after analysis\" checkbox. So the analysis is there but not the actual sample. Maybe this is somewhat better? reply neilv 4 hours agoparentYou're confident that the sample was destroyed? reply a0123 3 hours agoparentprevI'll bet everything I have they haven't destroyed the sample. Honest and sincere question: why would you even use their service in the first place? reply bitnasty 2 hours agorootparentI assume anytime a company says they will delete my data that they will just remove the ability for me to access it. reply CatWChainsaw 1 hour agorootparentThis is the way. reply barbazoo 4 hours agoprevhttps://archive.ph/mjaHb reply cmsonger 4 hours agoprevPaid them for a sample kit. Never used it after I carefully read the ToS and thought about what might happen. reply robertclaus 3 hours agoprevHaving worked at startups, my guess is that 23andMe doesn't even have the data a malicious company would want. Best case someone will buy them with some evil plan and realize the data is useless anyways. reply steelframe 4 hours agoprevThe article makes a point that 23andMe isn't bound by HIPAA, but even if it were, I wouldn't consider that adequate. The bar for collecting and holding PII, particularly medical, needs to be much, much higher than it is today. A doctor I recently visited whipped out his iPhone and asked if I was okay with him recording our conversation so that some fly-by-night rando AI company could vacuum up our private conversation and spit out some LLM-generated summary of our visit. \"Not to worry,\" he insisted, \"they're HIPAA compliant!\" I probably should have walked out of the office right then and there, but instead I simply told him no, not under any circumstances may he record our private conversation and send it off to some third party over the Internet. He seemed a bit taken aback because I guess I am the only patient he's had push back on it. He tried saying that the service \"really helped him\" or something like that. It seemed like he was trying to make me feel bad for \"making his job harder.\" I simply replied that HIPAA compliance didn't prevent the last 5 or 6 letters I've received from both hospitals and insurance companies about \"cybersecurity events\" leading to the compromise of my PII. And not just any PII, mind you. It was my medical information, supposedly \"protected\" by HIPAA. These were major insurance companies and hospitals. And you want me to believe that some fly-by-night AI startup is going to somehow be a safe place for a goddamned fscking full audio recording of our private visit, just because they claim to be HIPAA compliant? Are you kidding me? I've made it a point to start writing my representatives in government about these issues. They need to wake up and start doing something meaningful to protect the people who are being bamboozled by all the yahoos who play fast-and-loose with their privacy, especially medical PII. reply samkater 4 hours agoparentI had a similar experience where I was also assured the data was “doubly protected, it’s secured by a password here and re-secured at the remote site.” Besides that immediately making me question their security, it is a great example how people trust things without much thought. I’ve heard of calls for statistics to be pushed over calculus to improve math literacy in the general population, perhaps some cybersecurity courses should be pushed over “learn to code” to improve tech literacy. reply tengbretson 4 hours agoparentprevHIPAA has nothing to do with PII you voluntarily disclose. reply kstrauser 4 hours agorootparentNot if the other party is a HIPAA covered entity and it’s in the context of healthcare. reply hedora 4 hours agorootparentI’m not sure it really matters in practice at this point. As a condition of getting a flu and covid vaccine, CVS made me agree to give them permission to share my medical history, test results, etc. with my employer and their affiliates. reply spondylosaurus 2 hours agorootparentJust thinking here: is it possible that's a catch-all disclosure agreement aimed at employers who require certain vaccinations (I know CVS offers TB shots, for example, which are mandatory for working with some older/vulnerable populations), and this agreement lets CVS send those records to employers when requested? Either way, it's still a too-broad agreement, but my assumption is that CVS thinks it's easier to opt everyone in by default than to ask patients to opt in as needed, and then inevitably have some patients not opt in when they should have, and then deal with the resulting bureaucratic nightmare when the nursing home they work for calls and demands to see immunization records. reply tengbretson 3 hours agorootparentprevRight. So definitely not 23andme. reply akudha 4 hours agoparentprevIt is possible your doctor doesn’t fully understand concerns here. Or maybe he does and doesn’t give a shit. If it is the first case, maybe there is some hope - we can try and educate them doctors. I don’t know how to accomplish this, but we need to educate as many people as we can about privacy reply DrillShopper 3 hours agorootparentIf my doctor doesn’t understand or care about something as basic as doctor/patient confidentiality then I feel like there may be deeper problems reply teucris 3 hours agorootparentDon’t conflate doctor/patient confidentiality and data security. If someone broke into an office and stole medical records, that’s not a violation of doctor/patient confidentiality, even if the doctor chose crappy locks on their doors. reply chimeracoder 3 hours agoparentprev> Yet another example why \"HIPAA compliant\" means nothing. \"HIPAA compliant\" doesn't mean nothing. It means a whole lot. It's just not relevant here, because - as mentioned at the beginning of the article - 23 and Me is not regulated under HIPAA. reply j33zusjuice 4 hours agoparentprevHIPAA is a joke in the first place. How to implement HIPAA compliance is entirely up to the company dealing with the data. There are no prescriptive standards to protect your data. Who isn’t HIPAA certified? It has to be the easiest thing to certify for from a technical perspective. Research teams run records through some NLP shit to depersonalize them, but we all already know it’s trivial to reverse engineer that data to its origin. reply zdragnar 13 minutes agorootparentHIPAA is a legal framework to describe lawful disclosure of health information- defining who and when, and what steps must be taken when unauthorized / impermissible disclosure happens. It is technologically agnostic, because it applies whether your doctor is fully remote and everything uses electronic records, or if the provider is still using pen and paper and carrier pigeons. For actual security details, there may be some regulations with the change to the mandating of electronic records, but nothing in HIPAA ourself. For that, you want to look for organizations that have a certification like SOC2 or similar. reply baran1 3 hours agorootparentprevHIPAA is not a joke, employees can be held personally liable for breeches. At Helix we take HIPAA very seriously reply pluc 4 hours agoprevThis is turning out to be a really bad Christmas gift. reply y-curious 3 hours agoparentLooking to open a service that lets you gift someone a billboard with their first pet's name and mother's maiden name. I feel inspired reply h4ch1 3 hours agoprevSending a private corporation your DNA is a bad idea the second you hear about it no matter what the \"benefits\" are. Why would people, especially those who frequent HN and are aware of the data privacy debacles throughout history even trust them with something like this? reply drowntoge 1 hour agoparentAny sufficiently large corporation possesses the resources to gain trust by portraying itself as the 'nice guys,' unlike the others, which is often convincing enough to fool many. The conviction that every corporation is inherently evil or can turn evil at any point in the future never seems to fail, but many people just aren't that skeptical. reply nextlevelwizard 3 hours agoparentprevHindsight is always 20/20. My excuse is that I was young and even now I don’t see this as a huge disaster. But maybe I am just naive. reply mrweasel 2 hours agorootparentI think that depends on when you used their service. In the last 5 years, then yes, probably very naive. 23andMe was founded at a different point in time, where things looked more optimistic, funding was a different game and we worried less about companies misusing our personal information. It might not be a huge disaster, but to me the issue is that the company can't make any real promises about how they might profit from the DNA of it's customers in the future. It's not a problem unique to 23andMe, I will never sign up to another social network, because of Facebooks behavior. I'll never sign up to another service such as Gmail, Outlook, YouTube or Reddit, because I've seen what those companies did and how they behaved I can no longer trust any online service. The trust that existed in the early 2000s is gone, the idea that if we didn't like something we could just leave and delete everything is gone. I don't envy someone trying to bootstrap a new service, the previous generation of companies have poisoned the well. reply avalys 3 hours agoprevWhat is a specific plausible scenario of something that could be done with “my DNA” that I should be concerned about here? reply phkahler 3 hours agoparentInsurance companies denying coverage, or charging more. Being added to a database searched by police - this has its own hazards even for non criminals. There are certainly other possibilities, but once the cat is out of the bag you can't avoid them. reply goosejuice 3 hours agoparentprevDNA replaced latent fingerprints to place individuals at the scene of a crime. We know fingerprints have led to wrongful conviction. Partial DNA profiles can lead prosecutors to individuals who were not involved. Legal changes that allow insurance companies to use genetic information to increase or deny coverage. Not just to you but your entire lineage. reply thomassmith65 2 hours agoparentprevIn a decade or two, death could be a real concern. It's probably already possible, given sufficient resources, to tailor-make a virus that targets a specific person, family, or ethnic group. Presumably it will get easier to develop designer virii, as time marches on, not more difficult. reply layer8 2 hours agorootparentSo, biological ransomeware? “Send $amount Bitcoins to $address and we’ll send you the antidote.” reply thomassmith65 2 hours agorootparent\"YOUR PERSONAL CELLS HAVE BEEN INFECTED WITH MUUSE-7A! THE ANTLERS WILL CONTINUE TO GROW UNTIL YOU SEND 3BTC TO THE FOLLOWING WALLET ADDRESS\" reply 0-_-0 3 hours agoparentprevI'd like to know that as well reply bitnasty 2 hours agoparentprevTargeted ads reply Mistletoe 4 hours agoprevI’m glad I at least downloaded my data so I can have it forever and upload it where I like. I don’t regret having 23andme genotype it. I knew the pros and cons when I did it and the pros outweighed the cons for me. reply jamescun 5 hours agoprevUK citizen and 23andMe customer here. How likely is the sale of UK/EU customer data, or is it worth submitting a GDPR deletion request anyway? Get my data deleted before it's sold. reply EDEdDNEdDYFaN 5 hours agoparentgdpr might help you with data in a web database or data warehouse but if they have anything outside of that you're still screwed. I doubt a failing company has the time, energy, or resources to comprehensively clean up your data everywhere. Definitely submit the request but don't expect it to prevent your info from being resold reply KingOfCoders 4 hours agorootparentGDPR covers all personal data, that would include any DNA. It also includes the prevention of creating profiles without your consent. But as 23andme is an US company, it is not under the jurisdiction of the GDPR. The legal situation isn't clear, the EU would claim some jurisdiction, but I (IANAL) think it's more like you go to the US, walk into a Walgreen and give up your data. reply dahart 4 hours agorootparentAccording to the GDPR, its jurisdiction is global via “public international law” and mutual government agreements, but you’re right that’s not entirely clear and they are claiming untested jurisdiction. The law says it applies to non-EU companies if the company establishes any marketing or sales presence either located in the EU, or markets or sells to EU residents (which might apply if the company so much as analyzes sales data by country), or if the company is “monitoring” the behavior of EU residents in any way, where monitoring does not seem to be defined in Article 4 so could mean a lot of things including doing anything with collected data or corresponding with customers. https://gdpr.eu/article-3-requirements-of-handling-personal-... I’m sure there are US companies that happen to sell to EU residents that happen to acquire some PII but don’t know and can’t correlate it with the EU, and so aren’t subject to the GDPR. But according to the law’s language, it seems as though something simple on a company’s website like using Google Analytics, which does identify and “monitor” the behavior of people by location, might trigger GDPR. I might expect 23AndMe to trigger applicability for multiple reasons, including that they are using DNA to identify regional heritage and relatives, the samples may be delivered with EU addresses on them, and the samples are as personally identifying as it gets. That’s on top of whatever the website, account registration, and sale process collects. reply KingOfCoders 1 hour agorootparentThe problem of something like Google Analytics is that a company in the EU (EU company, US subsidiary, ...) exports PII to the US which it can't do (law interpretation is not clear inside the EU, e.g. is it legal if GA doesn't store IPs or if using GA without consent is generally illegal). And exporting data to the US is illegal because US companies can't guarantee that the EU citizen data is protected (which is the goal of the GDPR). But then again, it is not clear if this applies if an EU citizen goes to a company in the US (real or website in US datacenter) and leaves their data there. reply layer8 2 hours agorootparentprev23andme markets and sells services in the EU and is therefore subject to the GDPR. And they know this very well: https://www.23andme.com/en-eu/gdpr/ reply KingOfCoders 1 hour agorootparentYes, because of \"The GDPR applies to 23andMe because we market and provide the Personal Genetic Service in EU Member States through our UK, EU and International sites.\" The problem is that the EU parliament thinks this does not work, because US companies can be (secretly) coerced into giving data to the US government, even without telling the affected EU citizens (the EU commission has a different view). And the EU cititzen have no way of going to court over this. And a US company can't guarantee in any way to protect EU citizen data. Which also the reason that all the *Shields failed and were killed by EU courts [0] The view of the parliament is that you can't export personal data to the US at all as a company, so 23andMe can put up anything on the website they want, either they don't export data to the US (my Walgreen example) or they do, then they do it illegally. So I (again, IANAL) would say this is marketing speak aimed towards users and has no relevancy. [0] https://en.wikipedia.org/wiki/EU%E2%80%93US_Privacy_Shield reply layer8 8 minutes agorootparentI agree that the EU–US data transfer frameworks are unlikely to provide complete privacy safety, and this is an open problem. However, I was addressing whether 23andme is subject to the GDPR or not, and it clearly is. The data transfer frameworks are what supposedly allows them to transfer data to the US and still be GDPR-compliant. But regardless of whether they are actually compliant or not, they are indisputably subject to the GDPR. reply leinelissen 4 hours agorootparentprevNotably, the GDPR applies depending on customer jurisdiction rather than company jurisdiction. If they’re serving EU (or UK) customers, the GDPR definitely applies. reply Ylpertnodi 4 hours agorootparentHappy to be told the uk falls under the actual gdpr....do they (i thought after brexit, the uk wasn't covered...and they have their own version)? reply rsynnott 2 hours agorootparentFrom the ICO website: > The GDPR is retained in domestic law as the UK GDPR, but the UK has the independence to keep the framework under review. The UK GDPR. It’s like the GDPR, only with a Union Jack and a bulldog slapped on the side. Now, in practice, companies seem significantly less scared of the ‘UK GDPR’ than its full-fat European progenitor (probably for good reason; even before brexit, ICO was one of the less aggressive regulators, with its largest GDPR fine ever only being 20mn pounds), and of course the EU has a number of _newer_ consumer protections in this general area (DMA, DSA, AI Act etc) which the UK has _not_ implemented, but, for the moment at least, the UK still has some degree of data protection. reply sgtrx 4 hours agorootparentprevThat's not how GDPR works. GDPR doesn't care where your company is registered or does business; if they process the personal data of EU citizens then GDPR applies. reply notpushkin 3 hours agorootparentSupposedly. I was an Estonian resident a while ago, and I wanted to delete data in my old VK.com account (a Russian company). They didn’t do anything, naturally, so I wrote to Estonian data protection inspector or something. They said that (surprise!) they can’t do anything either. Things might be better now, but my bet is if you register a company in, say, Seychelles, and your business is purely digital, you can ignore GDPR all you want. EU can, in theory, tell payment processors to stop working with you, but I haven’t heard of such cases. Even then it won’t help if you don’t sell anything (apart from user data). Some EU countries have started blocking websites (by spoofing DNS) – this could actually work to put some actual pressure on non-compliant companies, but also is kinda too authoritarian for EU? Tl;dr: GDPR has good intentions, it just doesn’t work right now if the data processor is not in EU. reply chgs 4 hours agorootparentprevMostly. Howver if I am in New York and walk into Sam’s deli GDpR doesn’t apply. If Sam were to target an EU citizen then it would. reply raverbashing 3 hours agorootparentCorrect. If 23&M sells their services in the EU (and you bought the service while in the EU) then GDPR would apply But if you just walk into a pharmacy in the US and send your sample from there GDPR has nothing to do with it reply KingOfCoders 1 hour agorootparentNo if this is the case, they can't service EU citizens at all because US companies can't have any EU data because they can't protect EU citizen data. The only way to service EU customers is when we assume entering data on an US website is not exporting data from the EU to the US by the US company. Just like when I go into a Walgreen in NYC as an EU citizen. For the last decade US and EU companies have ignored the fact that it is/was mostly illegal do transfer EU citizen data to the US (it is currently legal but will be illegal again) - also every EU company that exports data to the US (e.g. by using Mailchimp) needs to guarantee the safety of the data by auditing Mailchimp, no one does and there have been no fine for now, but I assume there will in the future. See the discussions around https://en.wikipedia.org/wiki/EU%E2%80%93US_Data_Privacy_Fra... \"The EU parliament raised substantial doubts that the new agreement reached by Ursula von der Leyen is actually conform with EU laws, as it still does not sufficiently protect EU citizens from US mass surveillance and severely fails to enforce basic human digital rights in the EU. In May 2023 a resolution on this matter passed the EU parliament with 306 votes in favor and only 27 against, but so far has stayed without consequences.\" reply Copenjin 4 hours agoparentprevIt depends on the ToS they had at the time, when they started they explicitly had protections (privacy, data handling) only for US customers pointing to some local law, no details on how the data and samples from outside the us would have been handled. And that's why I never used they service. I think the GDPR road is well worth a try, good luck. reply hedora 3 hours agoparentprevThey had a massive data breach that hit about 50% of their customers last year. There’s a good chance the data’s already being resold by brokers: https://techcrunch.com/2023/12/04/23andme-confirms-hackers-s... reply dcchambers 2 hours agoprevIt's hard to feel bad for people that willingly gave their DNA samples to a private company. Come on y'all, use some common sense. reply ProllyInfamous 1 hour agoparentWhat about feeling bad for people whose family members submitted their own similar DNA — specifically an identical twin? reply CatWChainsaw 1 hour agoparentprevWhile I agree with you that anyone worried about privacy was a bit short-sighted to use 23AndMe, it's the same deal with everything, everywhere. Your favorite social media platform surveils all your posts/pictures/videos and is probably training an AI model on them by now, even though when you signed up for Friendster in the 1940s all of today's AI developments were distant sci-fi fever dreams. Outlook and Gmail feast on your emails. Your car is sharing everything you do with the manufacturer and the dealer and probably the government. Your washer is using up huge amounts of data for some reason, and you likely didn't even set up the app it has for reasons unfathomable. Data is money, so of course companies will pull out every stop to harvest it, monetize it, deprive you of control over it, and ransom you with it. reply voisin 4 hours agoprevIs there any way to use privacy laws to have 23andMe wipe your data? reply louthy 4 hours agoparentIf you’re in the EU or UK, yes. File a GDPR request for removal of all data relating to you. reply robterrell 4 hours agorootparentRemoving just your data probably isn't enough? Despite my curiosity, for privacy reasons I made the decision to not use 23andMe. (Basically - feels like information an insurer will inevitably want to use against me.) My wife did, however, and over the years our kids did too, for various reasons (an interest in genealogy, a kid with celiacs looking to trace the genetic component, etc). Recently I was very surprised to look at the app on my wife's phone and see that they have a shadow account for me with a lot of details filled in, due to my wife/kids/siblings/cousins having used the service. I should not be suprised -- this is how they caught the golden state killer, after all. reply louthy 3 hours agorootparent> shadow account That sounds like a GDPR breach to me, you should report it to the ICO (if you’re in the UK, not sure what the EU equivalent is). They should not hold data on any UK or EU citizen without the citizen’s consent. reply slowmotiony 4 hours agorootparentprevThey'll definitely do it. They promise. reply louthy 3 hours agorootparentWe can only use the laws that exist. Bad actors will act bad regardless. reply bjoli 3 hours agoprevA relative id mine sent her DNA to 23andMe or something like it. I was angry then, and I still haven't forgiven her. reply Shank 3 hours agoparentWhy is it your business what your family members do with their bodies? reply absisbdbjs 3 hours agorootparentIronically, I think it’s this kind of attitude that creates an environment where the parent is rightfully upset. We are not isolated units. Almost all our choices have impact on others. Lack of a shared culture creates societies where people are rightfully scared what the next isolated unit will do with their sensitive data. It’s possible to live in a high trust society. reply teeray 3 hours agorootparentprevIf you can infer things about living relatives from a DNA sample (preexisting conditions, for example), then you should need their consent to release that sample. It’s not only your information. reply inglor_cz 3 hours agorootparentprevThis is a genuinely interesting ethical question. While our phenotypes (bodies) are separate, our genotypes are very much a shared resource (at least for read access); an extra special case are monozygotic twins, where if you obtain sample from one of them, you just mapped both. Fortunately we don't see applications like \"personalized poisons\" yet, but it is likely inevitable. If, say, an insurance company denies you some policy because of what they learnt from your relative's DNA, you suffered a concrete harm from that sampling decision. reply purple_ferret 3 hours agorootparentprevbecause if they murder someone, they'll be easily traceable if they leave genetic data behind reply mchannon 4 hours agoprev [–] The US Government already has my DNA. Because of 23andme, I was able to discover I had one copy of delta-32, and that’s pretty cool. I was also able to find out where I came from and connect with distant relatives. To those who are tightly connected with their huge family, you’re privileged. I’d be sad if this resource went away but I don’t fear it being used for nefarious purposes. I can rest assured the US government is already miles ahead toward that end. reply ungreased0675 4 hours agoparentYou don’t worry about an additional potentially malicious actor having your information because one already has it? reply mchannon 4 hours agorootparentIf I was worried I wouldn't have furnished my DNA to a corporation with very little accountability in the first place. I got enough out of the deal (instead of nothing from the government) that it was in my mind an acceptable tradeoff. No one's about to start cloning me. Your DNA is not secret. You leave it everywhere you go. You have no reasonable expectation of privacy for your litter when you litter. It's only a matter of time and of tech before everybody has a copy of everybody's DNA. reply jpeeler 6 minutes agorootparentThat is the crux of the entire privacy argument. Why strive for privacy when \"I have nothing to hide?\" Also, how sure are you that having a copy of everyone's DNA data will become widespread? At a minimum, perhaps if you delay making the data easy to extract one can at least hold out hope that privacy laws will catch up. Of course, there's zero guarantee in that happening either. Lastly, security through obscurity is not something to be relied upon. But it can work for a period of time. reply dahart 3 hours agoparentprev [–] There is a list of reasons several agencies in US government like the FBI collects DNA from some people, but they don’t have DNA for all US citizens and I don’t expect the government to have my DNA. What nefarious purposes do you imagine the government has? Is matching suspected criminals against the crime database a nefarious purpose? Would you care if 23AndMe sold your DNA & analysis to a private for-profit medical insurance data provider who could recommend hiking your price or denying coverage, based on your genetic markers, without having to tell the insurance company why and without having to share your DNA? This is one of the private business nefarious purposes I worry about, based on having a friend who worked in credit processing saying that they were looking for legal ways to sell purchasing habits to medical insurance companies. reply mchannon 3 hours agorootparentPerhaps it was inadvertent diction, but your use of “imagine” appears to ridicule my opinion. Not cool. I’ve done time with an individual who got (I believe) wrongfully convicted due to genetic genealogy. A lay jury watches Law and Order, hear “DNA”, and will proverbially buy the Brooklyn Bridge from prosecutors. Get too unpopular with those in power, and maybe your DNA can be traced to a shell casing for an unsolved assassination a continent away from you. Annie Dookhan wrongfully convicted thousands upon thousands upon her doctored drug tests. Someone just like her could do it to you or someone else with your DNA test. There are laws against insurers citing preexisting conditions to deny coverage, and most DNA is equivocal as to whether you’ll develop expensive maladies. So that doesn’t worry me either. reply metaphor 1 hour agorootparent> There are laws against insurers citing preexisting conditions to deny coverage, and most DNA is equivocal as to whether you’ll develop expensive maladies. In the US, those laws have been under persistent attack by Republicans since enactment, and there hasn't been a major election cycle where its repeal wasn't a campaign dog whistle[1]. And since when has for-profit industry required unequivocal evidence to strengthen their balance sheets and fatten their bottom lines?? These gamified business decisions are always beyond opaque and the burden of proof is always unfavorably shifted onto consumers in harm's way. [1] https://www.whitehouse.gov/briefing-room/statements-releases... reply dahart 2 hours agorootparentprevI’m confused by that. I didn’t ridicule you, no need to make negative assumptions. I’m simply asking what you know about “nefarious purposes”, given that the government certainly doesn’t admit having such intentions. Okay, yes convictions can be messy and wrong, and juries can believe stuff from TV that isn’t true. Neither of those demonstrates government intent. None of the lawyers nor the juries nor the producers of Law and Order necessarily work for the government. You complained about my use of “imagine” and then threw out a completely hypothetical and vague scenario (three, actually). Even abuses of power by government employed individuals seeking some kind of retribution don’t demonstrate nefarious government purpose on the whole. There are laws against wrongful convictions and untrue testimony and abuse of power too. Annie Dookhan went to prison, and convictions based on her false evidence are being dropped and overturned. Why do you choose to feel safe with insurance laws made by the government and not trial laws? reply mchannon 1 hour agorootparentPersonal experience. Unlike most, I have been wrongfully convicted on fabricated evidence but never denied insurance coverage. I strongly encourage you to get in the habit of proofreading your posts for tone. You write with pique, a habit I find familiar, as I used to do the same when I was younger. It’s not just what you say but how you say it, and tone can either further your contribution or get in the way. reply dahart 1 hour agorootparentI’m sorry my use of “imagine” offended you. I did not intend for that to be a slight, but I apologize that it came off that way nonetheless. I intended it to be an advance acknowledgement of the fact that it may be difficult to prove the government as a whole has intent to use DNA in questionable or “nefarious” ways. I was simply asking your reasons for making such claims. I know the government does crappy things sometimes, even things that contradict its own laws. I’m still curious, piqued if you will, about how DNA can be used by the government against me, what things I/we should be potentially concerned about. Personal experience is fair. It’s also the reason I lean towards fear of DNA being used against me by private for-profit companies more that I worry about the government. reply howard941 3 hours agorootparentprevI lack faith in the longevity of laws regarding preexisting conditions, both the one in PPACA and the one in GINA. One vice presidential candidate is currently advocating against continuing the preexisting condition protections. There's too much money in the insurance industry to keep up a bulwark for these protections. reply a0123 3 hours agorootparentprev [–] > What nefarious purposes do you imagine the government has? Is matching suspected criminals against the crime database a nefarious purpose? This is just strange. Do you have no imagination whatsoever or have you never set foot in school or do you know literally nothing about history (maybe you were born yesterday and really quickly figured out how to write, I don't know)? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "23andMe is facing severe challenges, including potential stock delisting, shutting down its drug-development unit, and mass board resignations.",
      "CEO Anne Wojcicki is contemplating selling the company, which could jeopardize the DNA data of 15 million customers due to privacy policy loopholes.",
      "Concerns are rising about potential misuse of genetic data by new owners, especially since 23andMe is not regulated by HIPAA, and the company has a history of security breaches."
    ],
    "commentSummary": [
      "Concerns were raised about 23andMe's data collection, specifically the potential misuse of genetic data, such as cloning or unauthorized use by insurance companies.",
      "23andMe primarily uses \"genotyping\" rather than full DNA sequencing, unless users opt for the premium service.",
      "Discussions included the effectiveness of GDPR for data deletion and broader privacy issues, highlighting the need for stronger regulations."
    ],
    "points": 126,
    "commentCount": 122,
    "retryCount": 0,
    "time": 1727528749
  },
  {
    "id": 41680156,
    "title": "Role of Deliberate Practice in the Development of Creativity (2014)",
    "originLink": "https://repositories.lib.utexas.edu/server/api/core/bitstreams/c8cc4a4f-e641-462b-9a72-654e60f71485/content",
    "originBody": "",
    "commentLink": "https://news.ycombinator.com/item?id=41680156",
    "commentBody": "Role of Deliberate Practice in the Development of Creativity (2014) (utexas.edu)122 points by JustinSkycak 5 hours agohidepastfavorite55 comments smokel 5 hours ago> In order to assess the difference between deliberate practice habits between elite-level performers and moderate-level performers, it was also necessary to recruit moderate-level performers. Hehe :) Edit: Very interesting read, with nice examples of both successful and unsuccessful artists in various fields. One key trait in becoming successful seems to be willing to put in the effort. This in turn seems to only work if you actually enjoy putting in some effort. It makes one wonder if this can be a learned trait, or whether enjoying something is the actual (proxy) talent someone is born with. reply gchamonlive 2 hours agoparentSimone Weil body of work about attention can serve as a good starting point in this case. It is an answer that complements the usual disciplinary approach to effort, where a different kind of relation with the subject is developed where the interaction with it starts to be less effortful and more natural. Take drawing for instance, which is something I practice actively. The act of putting effort in drawing is quite reductive as drawing is a broad area. Sure you can will yourself into drawing 100 faces and you will invariably be better at drawing faces, but it'll take you nowhere nearer being a more creative artist. But sometimes approaching drawing laterally, that is reaching to other techniques, subjects and skills (like shading, drawing lines, using pens and such...) might give you a broader set of tools that in turn will help increase the chances you will find something that catches your attention and absorbs you into it. Sure you can get lost in the generics with lateral thinking and never reach a level of masterery that might be necessary for you to grow as an artist, so that is why attention isn't a replacement for discipline. You need both. But bottomline is that you also need to develop a relation with the subject that will reduce resistances, increase satisfaction and make it more likely that you will get absorbed by the task at hand. reply hinkley 1 hour agoparentprevAt least some of my work is aversion. I’m doing this because it will save me from having to do that in the future, and I hate doing that. reply randcraw 2 hours agoprevIt's helpful to know the dissertation hails from UTexas' department of advertising. In that context, 'creativity' is not about artists using imagination, cognition, and innovation to surprise or enlighten or edify. It's about creating better spam. reply smokel 1 hour agoparentThat's a bit harsh. Note that studying advertising might differ significantly from practicing it. For all you know, there could be a PhD out there researching the moral implications of Facebook's advertising practices during the 2010s. I have actually read most of the dissertation today, and it sure is about the creativity you reject it to be about. reply hinkley 1 hour agoparentprevI wonder how much intent and outcomes colors motivation. Probably need a similar study but with two different domains. reply creer 1 hour agoparentprevThe dissertation is about understanding how it works. You are then free to apply the learnings to anything you want or hate. reply tough 2 hours agoparentprevCreativity can and will be applied for the most horrendous destructive things. See: wars, arms, drugs. It's all on how you held those tools. I hate advertising too tho lol reply creer 1 hour agoprevIt's unfortunately very hard to isolate creativity from many competing and interfering aspects. Is an artist creative or are they successful in a field where by tradition every piece must be different (say, music videos). Is an engineer creative because they live in a discipline of severe constraints (say, spacecraft at the edge of the possible). A known issue for artists is having a recognized body of work: many new clients now want some of THAT - and not the precursors of the next body of work, so the artist feels the pressure to produce more of THAT. Is creativity only recognized (and so, favored) when it's followed by success? What about mechanical aspects of creativity - like good executive skills / habits? How about helps from the environment: constraints are one, but also early viewers, managers, critiques, partners that are encouraging - in the right way. \"Practice\"?! In what? \"Taste\" is a known aspect with the recognition that it can be hard on newcomers who may already have \"good taste\" but not yet the technical, gestural skill to produce and meet that bar. Teachers (in all meanings) that make sense and are capable of explaining how they or others operate. And on and on. So I have been trying to focus on specific antagonists. Recognizing what forms of creativity matter to me; Solving for \"block\"; Solving for \"time\". reply scrapcode 4 hours agoprevHaving someone around you who either intentionally or unintentionally creates an environment that makes you want to, and therefor enjoy putting in the effort, is crucial. I am just finishing up The Talent Code by Daniel Coyle [0] and it has been an interesting short read. In a nutshell, it boils talent down deep practice, ignition, and master coaching. [0] https://www.goodreads.com/en/book/show/5771014-the-talent-co... reply taylorius 4 hours agoparentI am a counter example that rule (though perhaps it is indirectly in agreement). I grew up alone, in the middle of the UK countryside without much regular contact with other kids. My inner monologue grew constant and loud - it acted like a companion, urging me to create things and ideas. reply kody 3 hours agoparentprevPainfully accurate. Nothing is worse for me creatively when the work environment is paranoid, ass covering, disinterested in the work. reply hinkley 1 hour agorootparentOr one person has a monopoly on creativity. reply sbarre 4 hours agoparentprevA good simple example: I don't like cooking alone. Cooking for my wife and me together? Love it.. reply airstrike 2 hours agorootparentThere are six human needs, they say: certainty, variety (uncertainty), significance, love, growth and contribution Cooking alone doesn't check the same boxes. It probably gives you only more \"certainty\" of being fed, but you're not craving more of that. Cooking for you and your wife probably gives you some significance (you like being appreciated for making a nice meal), love and a sense of contribution. Way more satisfying! reply scrapcode 4 hours agorootparentprevGoing deeper it can mean anything from having a person you want to impress, a parent that you just can't ever seem to make proud, or seeing someone succeed their way out of your impoverished upbringing. reply fredgrott 4 minutes agoprevCreativity comes from focusing on small piece of deliberate practice... An example, ask any well known guitar player....their greatest rift came from practicing some chord progression and noticing something different about it...the rift from Sweet Child was discovered that way.... reply Stem0037 4 hours agoprevWhile deliberate practice is undoubtedly crucial for developing creativity and expertise, I think there's an important nuance we often overlook - the role of diverse experiences and cross-pollination of ideas. Deliberate practice helps refine skills and deepen domain knowledge, but breakthrough creativity often comes from making unexpected connections between disparate fields. Some of history's most creative figures - like Leonardo da Vinci or Benjamin Franklin - were polymaths who excelled in multiple domains. reply mbivert 2 hours agoparent> the role of diverse experiences and cross-pollination of ideas Add to this: giving room for ideas to grow: the more you wait, the more diverse and numerous the life experiences, all of them having the potential to shape those uncrystallized ideas. reply Instantnoodl 4 hours agoparentprevThis! Most of my creativity in private projects stems from having build a broad space of knowledge/experiences. Having tinkered with a lot of different disconnected things really helps me find interesting bits to combine in a new and creative way that I never had imagined before :) reply rasengan 4 hours agoparentprevThis is why AI can in fact create things that haven’t yet been. reply eyelidlessness 4 hours agorootparentSure, but so can pure randomness, for the same reason. It is creative in the literal sense, but not in the ineffable sense that humans tend to describe in humans. reply creer 1 hour agorootparentNothing wrong with randomness combined with \"taste\" in the hands of the creator. Which is exactly the plan with generative AI. reply rasengan 3 hours agorootparentprevYou're absolutely right - and to identify the creation within randomness is also a form of creativity. Not all humans create (and identify) with the same methodologies! reply eyelidlessness 1 hour agorootparentIn hindsight, I wish I’d included the disclaimer that I have creative pursuits (of the ineffable variety) which leverage creative tools in the more literal sense (not AI, not purely random either). I don’t mean to disparage the entire class of machine-generated creation per se. But I do think that there is an important distinction between incorporating it in some form into a person’s expression, versus being the whole of the expression. Even if that incorporation is mere curation, at least that imbues some semblance of meaning, to someone capable of experiencing meaning. And perhaps that’s a snobbish perspective. Maybe it deserves reexamination. reply bbor 3 hours agorootparentprevWell put! Well, the first sentence is -- I think there's ample evidence that chatbots are creative in the same manner as humans, for the simple reason that they speak coherently. I'm sure we all remember pre-2023 chatbots, which were cute but ultimately produced gibberish; the current chatbots reach the same limits if given a hard enough task, which I think is fantastic evidence that they are ineffably creative before that limit. In Chomsky's words, quoting Wilhelm von Humboldt: Language is a process of free creation; its laws and principles are fixed, but the manner in which the principles of generation are used is free and infinitely varied. Even the interpretation and use of words involves a process of free creation. The normal use of language and the acquisition of language depend on what Humboldt calls the fixed form of language, a system of generative processes that is rooted in the nature of the human mind and constrains but does not determine the free creations of normal intelligence or, at a higher and more original level, of the great writer or thinker... The many modern critics who sense an inconsistency in the belief that free creation takes place within – presupposes, in fact – a system of constraints and governing principles are quite mistaken; unless, of course, they speak of “contradiction” in the loose and metaphoric sense of Schelling, when he writes that “without the contradiction of necessity and freedom not only philosophy but every nobler ambition of the spirit would sink to that death which is peculiar to those sciences in which that contradiction serves no function.” Without this tension between necessity and freedom, rule and choice, there can be no creativity, no communication, no meaningful acts at all. - https://chomsky.info/language-and-freedom/ reply soxletor 55 minutes agorootparentprevI must not be using the right models because this is exactly what AI can not currently do IMO. reply shahzaibmushtaq 4 hours agoprevIt will take some time to read the 129 pages before I come to any conclusion, but I can say one thing for sure, and those who know what deliberate practice is, will agree with me. Deliberate practice is a lonely process, which can only be accomplished with courage, dedication and grit whether you have a mentor/coach/master or not. reply kumarvvr 4 hours agoprevI always believed that while creativity can be developed through focus and practice, the pace of learning varies from person to person. In general, creative people are highly intelligent, work with tremendous focus and are dedicated individuals. reply creer 1 hour agoparentPerhaps. Other artists work in the very narrow direction THEY like and can manage technically - so of course they look like they have focus! Of course they look like they are dedicated to their art. But perhaps too it's the only thing they can do and they stumbled on something that has an audience. That does not mean they aren't amazing ... at that. That does not mean their art is not significant. That doesn't mean that their specific creativity is not worthy. It also doesn't mean that their creativity's mode of operation isn't totally irrelevant to some other person. Might be. Might not. reply mtalantikite 4 hours agoprevI think anyone that has undertaken an art form of any sort knows that it is all down to practice. There's just different levels of dedication. Charlie Parker famously was thrown out of a jam session in Kansas City as a teenager by Jo Jones (who threw a cymbal at him), and decided to spend the next 3-4 years practicing 12-15 hours per day. There's a retreat called chilla [1] that some South Asian musicians do which is 40 days of isolation and intensive practice. I saw a video of Mike Tyson helping train a young boxer recently and at the beginning he says: \"You know, it has nothing to do with styles or size. It's all about the moral of the fighter. How important is it to you? Is it more important than breathing? Is it more important than eating? It's up to the individual.\" A great mentor is crucial too. I know for myself having my music teacher listen and force me to not move on from what I'm working on is necessary. Having my Muay Thai trainer throw down his pads and silently demonstrate what I need to embody is invaluable. My meditation teacher pointing out my misunderstandings. Etc. [1] https://en.wikipedia.org/wiki/Chilla_katna [2] https://www.youtube.com/watch?v=TmJJK7Ac4Fk&t=20s reply creer 1 hour agoparentA teacher might tell you \"it's about brush mileage - and I have now told you all that matters\". Dump that teacher. If it's all they could teach you, you are done with them. reply mtalantikite 1 hour agorootparentOh for sure, there's a difference between just putting in time and putting in effective time. reply zeptian 3 hours agoprevThese kind of studies are dubious. The PhD report could have been generated by an LLM in about a day, and no one would know any better. It works like this: Take any hypothesis. And have a lot of verbiage around it with dubious experiments to \"statistically\" validate it. and write a giant report which would eventually turn into a book. Steve Pinker and his likes excel in this kind of stuff. Psychology/Sociology and sometimes economics are filled with these sorts of studies. It is more persuation than science. And one could could argue that science itself is a certain kind of persuation. reply jeffreyrogers 3 hours agoparentYour comment adds nothing to the discussion, reveals your prejudice against social science, and could be copied and pasted anytime a non-rigorous subject comes up. I'm actually interested in criticisms of this work, but your comment doesn't even rise to that level. reply zeptian 3 hours agorootparentguilty as charged ! But Bohm's \"On Creativity\", to me presents a much deeper \"philosophical\" take on a) what is creativity and b) how to foster it. And I dont see it referenced in this text at all. Again, since this is about persuation, it is what the reader wants to believe. reply creer 1 hour agorootparentThere are (probably) several billion works on creativity. Just reading and listing your own sources of inspiration on creativity is quite the endheavor. And that is not going to be exhaustive even in a PhD thesis. I'll give leaway there - on the contrary, mine THEIR list for stuff I missed. reply zeptian 1 hour agorootparentfair take, but my view these days is the following. there is way too much information-garbage floating around. hence I try to stick to time-tested classics particularly when it comes to certain topics. now, your time-tested classic may be different from mine and certainly, i want to see if there are things I missed, and hence I mentioned Bohm's work, as something the author of this PhD missed. reply throwanem 2 hours agorootparentprevComparative critique is far more persuasive than a series of baldly asserted and sweeping pejorations. reply ysofunny 2 hours agorootparentprevI found that the line \"science is a certain kind of persuassion\" quite informative. I was not aware of such a skeptical thread of thought near science reply zeptian 2 hours agorootparentEnd of the day, there are many belief systems that we human hold onto, but we need a method to settle opinion. And science happens to be a certain kind of a method for settling opinion. CS Peirce wrote about it so beautifully in his 1877 essay: \"On the fixation of belief\". go read it. here is a link saving you a google search. https://www.peirce.org/writings/p107.html reply jeffreyrogers 2 hours agorootparentprevThe comment was edited after I commented, it was originally much less substantive, and is improved now. reply metalman 4 hours agoprevspeaking from experience creativity is a condition and or compulsion often associated with basic functional deficits can it be channeled,directed,optimised,and comodified? sometimes for a while reply creer 1 hour agoparentYes! And it would be super useful if the rest of the population could benefit from insights extracted from \"them\". Currently humanity operates on creativity and other forms of knowledge work. There is a lot of value in \"better creativity\" or \"more exploitable creativity\". reply bbor 4 hours agoprevA) love the overall thesis/focus. The key points seem solid. B) I’m not sure how scientific this is. “We looked for instances of deliberate practice and found some” seems more like self-help advice than rigorous sociology? Or… anthropology? It certainly isn’t psychology, but funnily enough it doesn’t actually say what degree this was for. C) The theory section needed a much more serious engagement with the philosophy discussed, rather than just taking 1-2 sources on each 800y period as gospel. Let’s just say that not all Ancients thought nature was the peak of creativity, and that the doctrine of the Catholic Church wasn’t the only thing going on 400-1600, even if we restrict the view to Europe. Also desperately needs more engagement with postmodern conceptions of creativity, given that they basically dominate many parts of the “fine art” world to this day! reply relaxing 4 hours agoparentB- It was a Ph.D. in Advertising. reply bbor 4 hours agorootparentThanks, was just coming back to edit that in! Should’ve known HN would get it faster. That does explain my negative reaction to the method — if I had to pick a single archenemy among the modern academies, Advertising would likely win top billing! I mean, I just now learned that it exists at all, which doesn’t help. I guess PhD’s in Manipulation wouldn’t look nearly as good on the mantle… reply swayvil 4 hours agoprevSpeaking as a creative monster, I never practiced. I learned to draw by drawing, program by programming etc. Always because I was into it. Never because I was into a dream of future mastery. reply smokel 3 hours agoparentPractice on the job is still practice, no? reply mwidell 4 hours agoprevnext [5 more] [flagged] ada1981 3 hours agoparentCool! is this out of the box functionality for NotebookLM and if not what was your process? EDIT checked it out, looks like the audio deep dive feature. So wild. reply coreyp_1 3 hours agorootparentI can't respond to the comment above, b/c it's already been flagged, but I just wanted to say that I was surprised with the quality of the output as well. I know it's not magic. I know it's not sentient. I know it's not perfect. BUT IT'S IMPRESSIVE, and things like this demonstrate the huge leap that has happened in computing possibilities, and I'm frustrated that it gets flagged because some people are triggered by their \"AI bad... human good\" false dichotomy. Thank you to the person who posted it. reply mwidell 2 hours agorootparentWhy did it get flagged? Is it not allowed to post links in comments? Or drive links specifically? reply ada1981 2 hours agorootparentprevWild that I got a downvote for sharing my enthusiasm with the OP. The amount of unprocessed trauma and misery in this community is impressive. reply dr_dshiv 4 hours agoprev [3 more] [flagged] probably_wrong 3 hours agoparent [–] I didn't downvote that comment when you commented, but I did it now. I don't come to HN to read the auto-generated output of a system. If I wanted that I could generate the result myself. I come to HN because I'm interested in what people in the community have to say. Showing up to a thread and saying \"this is what an LLM generated\" does not \"gratify my intellectual curiosity\" (per the FAQ) and, in my opinion, is not a practice to be encouraged. reply bbor 3 hours agorootparent [–] Oh damn do I love a metadiscussion! I appreciate you taking a stand for quality discourse, and I agree 100% on your basic motivations, but I think this is overly dismissive. I'd say an audio discussion of this 100+ page dissertation is directly gratifying my intellectual curiosity, if I'm someone who is interested but doesn't have time to skim 100+ pages. And yeah anyone could figure out how to do such a thing, but a) most people don't even know it's possible yet, b) it presumably takes some amount of expertise and/or paid credits, and c) it saves people time. I'd consider it to be pretty similar to the usual archive link comments, in that way; anyone could probably find it themselves, but they might not even think to try. Finally, if I had to quote any one part of the guidelines to support my \"LLMs are welcome on HN\" claim: Please don't use Hacker News for political or ideological battle. That tramples curiosity. The guidelines don't mention any sort of institutionalized human-centric speciesism, so I'd say it's fair game. As my fave philosopher David Gunkel says: #RobotRights ;) Obviously you're 100% correct to downvote a comment you think is distracting, I just thought I'd share my response on the broader question. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "A 2014 dissertation from the University of Texas' advertising department explored the role of deliberate practice in developing creativity, comparing elite and moderate-level performers.",
      "Key findings emphasized that effort and enjoyment are crucial for success, while commenters highlighted the importance of diverse experiences and cross-pollination of ideas.",
      "The discussion also considered the scientific rigor of such studies, the influence of environment and mentorship, and the potential role of AI in creative processes."
    ],
    "points": 122,
    "commentCount": 55,
    "retryCount": 0,
    "time": 1727530548
  },
  {
    "id": 41675637,
    "title": "Lion Cove: Intel's P-Core Roars",
    "originLink": "https://chipsandcheese.com/2024/09/27/lion-cove-intels-p-core-roars/",
    "originBody": "Lion Cove: Intel’s P-Core Roars September 27, 2024 clamchowder 9 Comments Intel’s mobile CPUs have undergone massive changes over the past couple generations as Intel defends its laptop market against AMD, Qualcomm, and to a lesser extent Apple. Meteor Lake adopted aggressive chiplet design with separate compute, GPU, SOC, and IO extender tiles. Lunar Lake switches things up again, putting all compute on one tile while a second “platform controller” tile deals strictly with low speed IO. Amidst this whirlwind of change, Intel’s performance oriented P-Cores have remained a constant. P-Cores aim to deliver maximum per-thread performance, an important metric in client designs where responsiveness is vital and many programs don’t scale across many cores. Lion Cove is Intel’s latest and greatest high performance architecture, and fills the P-Core role in Lunar Lake. While its goals have remained constant, its design has very much not. Compared to Redwood Cove P-Cores in the previous generation Meteor Lake, Lion Cove has been overhauled with both performance and energy efficiency in mind. Chip P-Core Architecture Test System Source Intel Core Ultra 7 258V (Lunar Lake) Lion Cove ASUS Zenbook S 14 UX5406SA Sampled by ASUS Intel Core Ultra 7 155H (Meteor Lake) Redwood Cove ASUS Zenbook 14 OLED UX3405MA Purchased by Clam AMD Ryzen AI 9 HX 370 Zen 5 Mobile ASUS ProArt PX13 HN7306WU Sampled by ASUS Here I’ll be looking at Lion Cove as implemented in the Core Ultra 7 258V. Acknowledgments We’d like to thank Asus for kindly sampling us with a Zenbook S 14 UX5406SA test system. Without their help, a look at Intel’s latest mobile chip wouldn’t have been possible. System Architecture Lion Cove cores sit on a ring bus interconnect, a familiar design that traces its roots back to Sandy Bridge in 2011. Over time though, ring bus agents have come and gone. P-Cores got to share the ring bus with E-Core clusters in 2021’s Alder Lake. Meteor Lake saw Intel kick the iGPU off the ring bus. Lunar Lake gives E-Cores the boot, leaving just the P-Cores and their associated L3 slices on the ring bus. L3 slices on Lunar Lake continue to have 3 MB of capacity just as they did in Meteor Lake. However, Lunar Lake caps out at four cores and four L3 slices, dropping L3 capacity in half. In exchange, Intel is dealing with a smaller and simpler ring bus. Possibly because of this, Lunar Lake’s L3 latency has dramatically improved compared to Meteor Lake. It’s not as good as AMD, which had a very strong L3 design since early Zen generations. But AMD’s L3 latency advantage isn’t as drastic as it was before. DRAM latency also improves. Lunar Lake’s new chiplet design places the memory controller and CPU cores on the same tile, so memory access no longer have to traverse a cross-die link. The LPDDR5X DRAM chips themselves have been brought on-package too. Like Meteor Lake, Lunar Lake complicates DRAM latency measurements because the memory controller likes to stay in a low power state if there isn’t enough DRAM traffic. A plain memory latency test sees about 131.4 ns of DRAM latency. Creating some artificial bandwidth load drops latency to 112.4 ns. AMD’s Strix Point had 128 ns of DRAM latency for comparison, with margin-of-error differences under moderate bandwidth load. While DRAM latency with LPDDR5(x) will never match desktop DDR5, Lunar Lake’s DRAM latency is good for a mobile platform. Even with on-package memory, DRAM accesses are power hungry. Lunar Lake tackles this with a 8 MB memory side cache. 8 MB is less capacity than Lion Cove’s 12 MB cache, but that’s not a huge issue because the memory side cache is aimed towards blocks like the NPU or display engine that don’t have large caches of their own. Because improving CPU performance isn’t the main goal, the memory side cache doesn’t have great latency. In fact, estimating its latency is difficult because test sizes contained within the memory side cache will inevitably see a significant number of L3 hits. In those test ranges, latency is roughly 30 ns from a P-Core. DRAM bandwidth is quite impressive on Lunar Lake. Going to LPDDR5X-8533 brings noticeable improvements over Meteor Lake’s LPDDR5-7467. Four Lion Cove cores alone can pull more bandwidth than all of Meteor Lake’s cores. L3 bandwidth is less impressive, though that hasn’t been Intel’s strong point for years. AMD Strix Point’s four high performance Zen 5 cores can pull over 600 GB/s from L3 with reads alone. To Intel’s credit, quad core L3 bandwidth at least doesn’t regress compared to Redwood Cove. But L3 isn’t the focus of recent Intel designs. Rather, the company uses larger L2 caches to make their cores less sensitive to L3 performance. Having a slow cache is fine if you don’t hit it often. A Mid-Level Cache For Your Mid-Level Cache And Intel cores have trended towards bigger L2 caches. But increasing cache capacity isn’t so simple because larger caches often come with higher latency. Intel’s L2, which the company often refers to a mid-level cache, isn’t immune. Generation L2 Cache L3 Cache Skylake (Client) 256 KB, 12 cycle latency 6 MB, 37 cycle latency (Core i5-6600K) Tiger Lake, Willow Cove 1280 KB, 14 cycle latency 24 MB, 58 cycle latency (Core i7-11800H) Meteor Lake, Redwood Cove 2 MB, 16 cycle latency 24 MB, 75 cycle latency (Core Ultra 7 155H) Lunar Lake, Lion Cove 2.5 MB, 17 cycle latency 12 MB, 51 cycle latency (Core Ultra 7 258V) Lion Cove counters this by adding a mid-level cache for the mid-level cache so you can avoid mid-level cache latency if you hit in the faster mid-level cache. Intel calls this new mid-level cache a L1, and renames the first level cache to L0. The new “L1” has 192 KB of capacity with 9 cycles of load-to-use latency. I disagree with Intel’s terminology change. While first level cache latency does drop from five to four cycles, it merely matches Zen 5’s L1D in capacity and latency terms. The 192 KB “L1” has much higher latency than L1D caches in competing cores. You could really stretch things by comparing to a Cortex A72 in Graviton 1. That has 1.75 ns of L1D latency, which is in the same ballpark as the 1.88 ns of measured latency on Lion Cove’s 192 KB L1. But Cortex A72 and Lion Cove don’t have comparable design goals. I’m going to be stubborn and call the 48 KB first level cache a “L1”, and the 192 KB cache a “L1.5” from now on. Lion Cove’s L1.5 has better bandwidth than Redwood Cove’s L2, though not with a read-only pattern. I needed a read-modify-write pattern to sustain more than 32 bytes per cycle, so I think it’s only a minor improvement. The L1.5’s bandwidth is nowhere near that of the 48 KB L1D on either Lion Cove or Redwood Cove. That’s a lot of cache levels But I don’t think bandwidth is Intel’s main goal with the new L1.5. Rather, it looks aimed at reducing average L1D miss latency. Catching some L1D misses and servicing them at 9 cycle latency helps of course. Beyond that, the L1.5 lets Intel make a bigger L2, reducing latency for more difficult accesses by servicing more of them within full speed core-private caches. Going from 2 MB on Redwood Cove to 2.5 MB on Lion Cove might not feel like much. However, Intel’s slides show Lion Cove can support up to 3 MB of L2 capacity. That’s as much capacity as last level caches on some old Intel dual core chips like the Core i5-3317U. Possibly because L1.5 absorbs a good chunk of L1D miss traffic, Intel didn’t focus as much on L2 bandwidth. Lion Cove’s L2 Bandwidth tops out at 32 bytes per cycle, regardless of whether I’m using a read-only pattern or a read-modify-write one. Intel’s focus is really about giving the L2 more capacity, rather than providing higher L2 bandwidth. If workloads spill out of L2, L3 bandwidth is quite mediocre. L3 read bandwidth from a single Lion Cove core regresses to just over 10 bytes per cycle, down from 16 bytes per cycle on Redwood Cove. Lion Cove enjoys lower L3 latency and a larger L2 miss queue (80 entries, compared to 64 on Redwood Cove). It’s a combination that should give a single core access to more L3 bandwidth, but that doesn’t show through in testing. A read-modify-write pattern achieves higher bandwidth, at 17-18 bytes per cycle. Neither figure approaches Zen 5, which goes right to the limit of its 32 byte per cycle L2 to L3 interface. A read-modify-write pattern exercises the 32 byte per cycle link in the other direction, bringing total L3 throughput to 64 bytes per cycle. The Ryzen AI 9 HX 370 can run its Zen 5 cores at up to 5.15 GHz compared to 4.8 GHz on the Core Ultra 7 258V. AMD’s clock speed advantage further inflates its L3 bandwidth advantage. Out-of-Order Execution Engine One of those sweeping changes applies to the schedulers, which have been reorganized with a view towards scalability. Since the Pentium Pro from 1995, Intel has served both integer and FP/vector operations with a unified scheduler. Scaling a large unified scheduler can be difficult, so Intel split the scheduler over time. Skylake put memory address generation ops on a separate scheduler. Sunny Cove split the memory scheduler, and Golden Cove revised the memory scheduler split. Lion Cove finally splits the unified math scheduler into separate ones for integer and floating point/vector ops. Intel also split register renaming for floating point and integer operations. That’s not visible from software, but it does suggest Intel’s core is now laid out a lot like AMD’s Zen. Both do register renaming separately for integer and vector operations, and use separate schedulers for those operations. After the split, Lion Cove’s vector and integer schedulers each have similar capacity to Redwood Cove’s unified math scheduler. Combined integer and vector scheduling capacity is simply massive with over 200 available entries. The memory schedulers are no joke either. In every category, Lion Cove’s scheduling capacity beats Zen 5’s. Category Lion Cove Available Scheduler Entries Zen 5 Available Scheduler Entries Scalar Integer Math 97 88 Floating Point/Vector Math 114 76 Memory Accesses 62 58 Besides big schedulers, Intel uses non-scheduling queues to let Lion Cove track more more operations waiting for an execution unit. If a scheduler fills up, the renamer can place micro-ops into an associated non-scheduling queue instead of stalling. Micro-ops sitting in a non-scheduling queue won’t be considered for execution, but can enter a scheduler later when entries become available. AMD’s Zen line and Intel’s own E-Core line have used non-scheduling queues over the years, and it’s great to see the Intel adopt it on P-Cores too. Lion Cove’s schedulers feed a massive 18 execution ports, up from 12 in Redwood Cove. Much of that execution port count increase comes from moving FP/vector units off the integer scheduler. Execution capacity for common instruction categories hasn’t increased that much. Scalar integer adds get an extra ALU port. A third store address port helps discover memory dependencies faster, though sustained store throughput is limited two per cycle. FP/vector operations are handled by four ports instead of three. Lion Cove’s FP/vector execution setup is curious too, because now it very closely resembles AMD’s going back to Zen 1. AMD and Intel now handle FP/vector execution with four pipes. Two deal with floating point multiplies and multiply-adds, while two others handle FP adds. Vector integer adds can go to any of the four ports. Unlike Zen 5, Lion Cove maintains single cycle vector integer add latency even when the schedulers are full and micro-ops are woken up by single cycle ops (other vector integer adds). In AMD’s favor, cores from the Zen line don’t suffer a significant penalty when a floating point multiply with normalized inputs generates a denormal output. Such an event costs 132 cycles on Lion Cove, which is worse than the 124 cycles I saw on Redwood Cove. Skymont behaves like Zen, and doesn’t suffer a significant penalty for denormal results. Micro-ops leave the schedulers after execution units generate their speculative results. But all the way until they’re retired, micro-ops require entries in various structures like the reorder buffer, register files, and load/store queues. Those queues, buffers, and register files ensure the core can correctly produce results as if instructions were executed in program order. Lion Cove grows those structures, letting the core keep more instructions in-flight. In turn, that makes the core more resilient against long latency events like cache misses. But not every structure got equal treatment. Structure Required if an instruction… Lion Cove Redwood Cove Zen 5 Reorder Buffer (ROB) Exists 576 512 448 Integer Register File Writes to a scalar integer register ~290 280 240 Floating Point/Vector Register File Writes to a floating point/vector register ~406 332 384 Mask Register File Writes to an AVX-512 mask register Intel CPUs alias MMX/x87 registers to the same physical register file ~166 ~158 ~146 Load Queue Reads from Memory ~189 192 , ~202 measured Store Queue Writes to Memory 120 114 104 Branch Order Buffer Affects control flow 180 128 96 Lion Cove’s ROB sees a 12.5% capacity increase. It’s a noticeable improvement, if nowhere near the 45% or 40% ROB size growth that Golden Cove or Zen 5 got over their respective previous generations. However, some of Lion Cove’s supporting resources are much bigger than the corresponding ones in Golden Cove/Redwood Cove. Lion Cove can have over 40% more branches in flight. The floating point register file also sees substantial growth, likely to keep pace with increased floating point scheduling and non-scheduling queue capacity. Since Skylake, Intel allocates both AVX-512 mask registers and MMX/x87 registers out of the same register file. I can’t test reordering capacity for mask registers because Intel stopped supporting AVX-512 on consumer chips. But testing with MMX registers shows a small increase in rename capacity over Redwood Cove. Intel may still be making AVX-512 oriented improvements, and some of those effects are visible even on client cores. Improvements elsewhere are minor. The integer register file grew by less than a dozen entries and still doesn’t cover ROB capacity well. Intel added a few store queue entries too. As far as I can tell, the load queue either didn’t get any entries added, or even had a few removed. Load/Store Unit A CPU’s load/store unit often occupies plenty of die area, and is responsible for ensuring memory accesses appear to execute in program order. That feels challenging with a lot of memory accesses in-flight, because a load’s address has to be checked against all prior store addresses. If they overlap, the load has to read data from the store queue instead of the data cache. Tested using Henry Wong’s methodology at https://blog.stuffedcow.net/2014/01/x86-memory-disambiguation/ Lion Cove maintains Golden Cove’s zero latency, two-per-cycle store forwarding capability for exact address matches. Latency slightly regresses if the load is contained within a store but addresses don’t match exactly, but that shouldn’t be common unless you’re dealing with network packets or something. Partial overlaps are handled with much higher latency, and are likely handled by blocking the load until the store commits, after which the load can get data from the L1D cache. If so, Zen 5 has a much shorter pipeline from store address generation to retirement. Case Lion Cove Golden Cove Zen 5 Exact Address Match 2 per cycle 0 cycle latency 2 per cycle 0 cycle latency 2 per cycle 0 cycle latency Load Contained within Store 8-9 cycle latency 5-6 cycle latency 7 cycle latency Load/Store Partially Overlap 19 cycles 19-20 cycles 14 cycles Independent Misaligned Load 1 per cycle 1 per cycle 4 per 3 cycles Independent Misaligned Store 2 cycles per store 2 cycles per store 1 per cycle Independent accesses can face delays too depending on how they interact with the underlying data cache. Tracking cached data at the byte level would be far too expensive, so caches maintain tags and state at the cache line granularity. That’s typically 64 bytes. Intel’s architectures do worse when an access crosses a 64 byte cache line boundary, taking an extra cycle for a store. Loads do a bit better probably because the data cache has more load ports and can absorb the extra bandwidth needed for a misaligned access. But misaligned loads still aren’t handled as fast as on AMD. Address Translation Programs operate on virtual addresses, which have to be translated to physical addresses that correspond to locations in DRAM. The load/store unit has to carry out these translations according to page tables set up by the operating system. Page tables are actually multi-level structures, so CPUs cache frequently used address translations in translation lookaside buffers (TLBs). For generations Intel has used a very complex TLB setup with separate TLBs for different page sizes and access types. Who uses 1 GB pages anyway Lion Cove brings the 4K page load-only DTLB’s capacity up to 128 entries, from 96 in Redwood Cove. None of the other TLB sizes have changed. That should reduce average memory access latency across a wide variety of client programs, because 4K pages are most commonly used there. However, AMD’s Zen 5 and even Zen 4 can cache far more address translations in a L2 TLB. AMD’s cores therefore have a better chance of avoiding expensive page table walks. As on Redwood Cove, getting a translation from Lion Cove’s L2 TLB adds 7 extra cycles of latency. That penalty also matches Zen 5. Rename and Allocate: Feeding the Backend The rename and allocate stage allocates micro-ops into backend structures, while carrying out register renaming and other optimizations to break false dependencies. Register renaming is an inherently serial task because which physical registers correspond to an instruction’s inputs depends on how prior renames have been carried out. Probably for that reason, the renamer is often the narrowest part of a core’s pipeline. AMD and Intel’s latest cores are no exception. Lion Cove widens the renamer to handle 8 micro-ops per cycle, up from 6 in Redwood Cove. That makes Lion Cove an 8-wide core overall, matching AMD’s Zen 5. Intel’s renamer received some impressive capabilities in Golden Cove, including the ability to execute up to 6 dependent adds with small immediates per cycle. That’s carried forward to Lion Cove, though not widened to match the renamer’s full width. Test Comment Lion Cove IPC Redwood Cove IPC Zen 5 IPC XOR r,r Commonly used to zero registers. The exclusive-or of two identical values is always zero 7.31 5.7 5.01 XOR xmm, xmm Same as above but for a vector/FP register 7.31 5.71 4.99 Dependent MOV r,r >1 indicates move elimination 7.02 5.56 6.65 Independent MOV r,r Easy 7.25 5.71 5.01 Dependent increment Actual math, normally would create a dependency chain limiting the test to 1 IPC 5.6 5.53 1 Dependent add immediate As above but adding small numbers up to 20 instead of just 1 4.36 5.47 1 Other easier optimizations like move elimination and zeroing idiom recognition can be carried out at or near the renamer’s full width. Zen 5 is no slouch for those, but often can’t carry out those optimizations at eight per cycle. I’m not sure if it makes a big performance difference, but it does show Intel’s focus on building a very powerful rename stage. Frontend Fetch and Decode The frontend has to feed the rename stage by bringing instructions into the core and decoding them into micro-ops. Lion Cove’s frontend uses a similar strategy to prior P-Cores. A conventional instruction cache feeds a decoder, which both sends micro-ops downstream and fills them into a micro-op cache. Lion Cove widens the decoder to handle eight instructions per cycle, up from six in Redwood Cove. Micro-op cache capacity increases to 5250 micro-ops, up from 4096 on Redwood Cove. Bandwidth from the micro-op cache went up to, from eight to 12 micro-ops per cycle. Unlike AMD Zen 5’s clustered decoder, all eight decode slots on Lion Cove can serve a single thread. Lion Cove can therefore sustain eight instructions per cycle as long as code fits within the 64 KB instruction cache. After that, code fetch throughput from L2 is limited to 16 bytes per cycle. L3 code fetch bandwidth is similar to data-side bandwidth, so Lion Cove’s branch predictor can run very far ahead of fetch to hide even L2 miss latency. The same doesn’t apply to Zen 5, which has lower code fetch throughput from L3. Longer instructions can run into cache bandwidth bottlenecks. With longer 8-byte NOPs, Lion Cove can maintain 8 instructions per cycle as long as code fits within the micro-op cache. Strangely, throughput drops well before the test should spill out of the micro-op cache. The 16 KB data point for example would correspond to 2048 NOPs, which is well within the micro-op cache’s 5250 entry capacity. I saw the same behavior on Redwood Cove. Once the test spills into the L1 instruction cache, fetch bandwidth drops to just over 32 bytes per cycle. And once it gets into L2, Lion Cove can sustain 16 instruction bytes per cycle. Branch Predictor: Directing the Core Instruction fetch is steered by the branch predictor, which plays an important role in improving both performance and power efficiency. Everyone tends to improve their branch predictors with every generation, and Lion Cove does so too. A single branch sees little to no penalty (evidence of a mispredicts) even when throwing a 12K long random pattern at it. Intel definitely made some changes to direction predictor, but the scope of this change seems to be narrow. Lion Cove performance monitoring events haven’t been documented yet, but Intel does guarantee some architectural performance monitoring events will work across different generations. Events for retired branches and retired mispredicted branches are among those events. If I look at the geometric mean of branch prediction accuracy across all SPEC CPU2017 workloads, Redwood Cove and Lion Cove differ by well under 0.1%. Lion Cove has a tweaked branch predictor for sure, but I’m not seeing it move the needle in terms of accuracy. AMD’s Zen 5 still does a bit better overall, and can gain an especially significant edge with difficult workloads like 541.leela and 541.xz. There, AMD’s latest branch predictor sees a 11.4% and 3.84% reduction in mispredicts per instruction compared to Intel’s. Within SPEC CPU2017’s floating point suite, Lion Cove struggles in 526.blender. Branch predictor speed matters too, because the point of a branch predictor is to minimize delays from control flow dependencies. Intel continues to use a triple level branch target buffer (BTB) setup to cache frequently used branch targets, but each level has been tweaked compared to Redwood Cove. To start, both architectures can handle two taken branches per cycle likely by unrolling small loops within the micro-op queue. Lion Cove and Redwood Cove both have a 192 entry micro-op queue, but perhaps Lion Cove can’t track as many branches within it. Next, a L1 BTB is fast enough to do zero bubble branching, which means handling taken branches with just a single cycle of latency. On Lion Cove, the L1 BTB appears to cover 2 KB of code, regardless of how many branches are in it. Redwood Cove can track up to 128 branches in its L1 BTB, mostly independently of branch spacing. Then there’s a 6K entry BTB on both cores with 2 cycle latency, followed by a 12K entry BTB. That large last level BTB has 3-4 cycles of latency on Lion Cove, and is difficult to characterize on Redwood Cove. Returns are predicted via a return stack, which has grown to 24 entries from 20 in Redwood Cove. Prediction latency is better when the tested call depth doesn’t exceed 12, so I suspect this is a two level structure. For comparison AMD has opted for a larger 52 entry return stack on Zen 5, which is duplicated per-thread for a total of 104 entries. Capacity isn’t the only factor, and I have to point out how fast Intel’s return prediction is. Lion Cove and Redwood Cove can handle a call+return pair every cycle. AMD’s Zen 5 takes four cycles to do the same, or an average of two cycles per branch. Lion Cove trades some speed for a few extra return stack entries, and averages one branch per cycle up to a call depth of 24. AMD may be faster for direct branches thanks to its giant 1024 entry zero-bubble BTB. But Intel is faster for other categories of branches like calls and returns. Core Summary All those caches help feed Lion Cove’s core, which has huge upgrades over Redwood Cove. The pipeline is wider, structures are larger, and a reorganized out-of-order engine helps Intel achieve higher scheduling capacity. Much like Redwood Cove, Lion Cove is a wide and high clocked out-of-order design. But it’s easily the biggest change to Intel’s performance oriented architecture since Golden Cove. After Redwood Cove’s minor changes over Raptor Cove, and Raptor Cove barely doing anything over Golden Cove, it’s great to see Lion Cove’s sweeping changes. Intel must have put a lot of effort into Lion Cove’s design. Compared to Redwood Cove, Lion Cove posts 23.2% and 15.8% gains in SPEC CPU2017’s integer and floating point suites, respectively. Against AMD’s Strix Point, single threaded performance in SPEC is well within margin of error. It’s an notable achievement for Intel’s newest P-Core architecture because Lunar Lake feeds its P-Cores with less L3 cache than either Meteor Lake or Strix Point. A desktop CPU like the Ryzen 9 7950X3D only stays 12% and 10.8% ahead in the integer and floating point suites respectively. Getting that close to a desktop core, even a last generation one, is also a good showing. Results here aren’t comparable to ones in the prior article because I re-ran with -O3 -mtune=native -march=native to let GCC use whatever instruction set extensions the CPU supports. They also aren’t comparable to Intel’s performance estimates, which took a variety of workloads into account at fixed frequencies. Performance gains will vary across different workloads as SPEC CPU2017 subscores show. But there’s little doubt that Intel succeeded in delivering a generational performance uplift with Lion Cove. Final Words P-Cores have been Intel’s bread and butter long before the company started calling them P-Cores. Progress with Intel’s performance oriented cores hasn’t always been fast. Redwood Cove was only a slight tweak over Golden Cove. Skylake filled out five generations of Intel designs the same architecture. Going back further, Intel used the P6 architecture on the Pentium Pro, Pentium II, and Pentium III with just minor tweaks and clock speed increases in between. Lion Cove is a much improved architecture compared to Redwood Cove, and shows Intel still has potent engineering muscle despite recent setbacks. Traditionally Intel delivered significant architecture changes during a “tock” in a tick-tock cycle. That reduces risk by separately handling process node and architecture changes. Lunar Lake not only combines a new architecture with a move to a new node, but also drops system level changes on top. At a time when Intel’s facing increased pressure from all sides, a move like Lunar Lake is a sign that Intel can adapt and survive. Intel’s upcoming Arrow Lake desktop CPU will let Lion Cove stretch its legs with more cache and a larger power budget. Lower latency DDR5 should improve performance even further. After seeing Lion Cove perform well in a mobile form factor, I’m optimistic about what the same architecture can do on desktop. Recently Intel has been sitting on a rather unstable foundation with Raptor Lake, and Arrow Lake’s release will be a great time to put the company’s high performance chips back on stable footing. Again, we would like to thank ASUS for sending us over a Zenbook S 14 for review and if you like our articles and journalism, and you want to support us in our endeavors, then consider heading over to our Patreon or our PayPal if you want to toss a few bucks our way. If you would like to talk with the Chips and Cheese staff and the people behind the scenes, then consider joining our Discord. Author clamchowder View all posts Don’t miss our articles! Email Address * Related Posts",
    "commentLink": "https://news.ycombinator.com/item?id=41675637",
    "commentBody": "Lion Cove: Intel's P-Core Roars (chipsandcheese.com)117 points by luyu_wu 21 hours agohidepastfavorite67 comments kristianp 15 hours agoAbout 94.9 GB/s DRAM bandwidth for the Core Ultra 7 258V they measured. Aren't Intel going to respond to the 200GB/s bandwidth of the M1 Pro introduced 3 years ago? Not to mention 400GB/s of Max and 800GB/s of the Ultra? Most of the bandwidth comes from cache hits, but for those rare workloads larger than the caches, Apples products may be 2-8x faster? reply adrian_b 11 hours agoparentAMD Strix Halo, to be launched in early 2025, will have a 256-bit memory interface for LPDDR5x of 8 or 8.5 GHz, so it will match M1 Pro. However, Strix Halo, which has a much bigger GPU, is designed for a maximum power consumption for CPU+GPU of 55 W or more (up to 120 W), while Lunar Lake is designed for 17 W, which explains the choices for the memory interfaces. reply Dylan16807 10 hours agorootparentThat's good. And better than match, that's 30% faster, at least until the M4 Pro launches with a RAM frequency upgrade. On the other hand, I do think it's fair to compare to the Max too, and it loses by a lot to that 512 bit bus. reply kvemkon 6 hours agorootparentprev> LPDDR5x of 8 or 8.5 GHz Not 8000 or 8500 MT/s and thus the frequency is halved? reply adrian_b 4 hours agorootparentSorry, I meant the frequency of the transfers, not that of some synchronization signal, so 8000 or 8500 MT/s, as you say. However that should have been obvious, because there will be no LPDDR5x of 16000 MT/s. That throughput might be reached in the future, but a different memory standard will be needed, perhaps a derivative of the MRDIMMs that are beginning to be used in servers (with multiplexed ranks). MHz and MT/s is really the same unit. What differs is the quantity that is measured, e.g. the frequency of oscillations and the frequency of transfers. I do not agree with the method of giving multiple names to a unit of measurement in order to suggest what kind of quantity has been measured. The number of different quantities that can be measured with the same unit is very large. If the method of giving multiple unit names had been applied consistently, there would have been a huge number of unit names. I believe that the right way is to use a unique unit name, but to always specify separately what kind of quantity had been measured, because having only a numeric value and the unit is never sufficient information, without having also which quantity has been measured. reply wtallis 14 hours agoparentprevLunar Lake is very clearly a response to the M1, not its larger siblings: the core counts, packaging, and power delivery changes all line up with the M1 and successors. Lunar Lake isn't intended to scale up to the power (or price) ranges of Apple's Pro/Max chips. So this is definitely not the product where you could expect Intel to start using a wider memory bus. And there's very little benefit to widening the memory bus past 128-bit unless you have a powerful GPU to make good use of that bandwidth. There are comparatively few consumer workloads for CPUs that are sufficiently bandwidth-hungry. reply nox101 13 hours agorootparentwith all of the local ML being introduced by Apple and Google and Microsoft this thinking seems close to \"640k is all you need\" I suspect consumer workloads to rise reply throwuxiytayq 11 hours agorootparentI think the number of people interested in running ML models locally might be greatly overestimated [here]. There is no killer app in sight that needs to run locally. People work and store their stuff in the cloud. Most people just want a lightweight laptop, and AI workloads would drain the battery and cook your eggs in a matter of minutes, assuming you can run them. Production quality models are pretty much cloud only, and I don’t think open source models, especially ones viable for local inference will close the gap anytime soon. I’d like all of those things to be different, but I think that’s just the way things are. Of course there are enthusiasts, but I suspect that they prefer and will continue to prefer dedicated inference hardware. reply 0x000xca0xfe 6 hours agorootparentMicrosoft wants to bring Recall back. When ML models come as part of the OS suddenly there are hundreds of millions of users. reply throwuxiytayq 2 hours agorootparentI have some difficulty with estimating how heavy Recall’s workload is, but either way, I have little faith in Microsoft’s ability to implement this feature efficiently. They struggle with much simpler features, such as search. I wouldn’t be surprised if a lot of people disable the feature to save battery life and improve system performance. reply layer8 3 hours agorootparentprevIt remains to be seen whether users want what Microsoft wants. reply tucnak 8 hours agorootparentprev> AI workloads would drain the battery and cook your eggs in a matter of minutes, assuming you can run them M2 Max is passively cooled... and does 1/2 of 4090's token bandwidth in inference. reply MindSpunk 3 hours agorootparentUm, no? The MBP has fans? The base level M2 in a MacBook Air is passively cooled and thermal throttles very aggressively when you push it. reply Onavo 11 hours agorootparentprev> I think the number of people interested in running ML models locally might be greatly overestimated [here]. There is no killer app in sight that needs to run locally. People work and store their stuff in the cloud. Most people just want a lightweight laptop, and AI workloads would drain the battery and cook your eggs in a matter of minutes, assuming you can run them. Production quality models are pretty much cloud only, and I don’t think open source models, especially ones viable for local inference will close the gap anytime soon. I’d like all of those things to be different, but I think that’s just the way things are. Of course there are enthusiasts, but I suspect that they prefer and will continue to prefer dedicated inference hardware. Do you use FTP instead of Dropbox? reply cvs268 1 hour agorootparentDo you use Dropbox instead of rsync? ( Couldn't resist (^.^) ) reply wtallis 2 hours agorootparentprevLocal ML isn't a CPU workload. The NPUs in mobile processors (both laptop and smartphone) are optimized for low power and low precision, which limit how much memory bandwidth they can demand. So as I said, demand for more memory bandwidth depends mainly on how powerful the GPU is. reply epolanski 9 hours agorootparentprevThe few reviews we have seen now show that lunar lake is competitive with m3s too depending on the application. reply formerly_proven 8 hours agorootparentprevIs the full memory bandwidth actually available to the CPU on M-series CPUs? Because that would seem like a waste of silicon to me, to have 200+ GB/s of past-LLC bandwidth for eight cores or so. reply adrian_b 3 hours agorootparentI do not know for the latest models, but at least in M1 the CPU was limited to a fraction of the total memory throughput, IIRC to about 100 GB/s. reply phonon 3 hours agoparentprevM3 Pro is 150 GB/s (and that should be compared to Lunar Lake's nominal memory bandwidth of 128 GB/s) and the cheapest model with it starts at $2000 ($2400 if you want 36 GB of RAM). At those price levels, PC laptops have discrete GPUs with their own RAM with 256 GB/s and up just for the GPU. reply wmf 14 hours agoparentprevThe \"response\" to those is discrete GPUs that have been available all along. reply Aaargh20318 8 hours agorootparentDiscrete GPUs are a dead end street. They are fine for gaming, but for GPGPU tasks unified memory is a game changer. reply kristianp 14 hours agorootparentprevTrue, but I thought Intel might start using more channels to make that metric look less unbalanced in Apple's favour. Especially now that they are putting RAM on package. reply tjoff 9 hours agorootparentWhy the obsession of this particular metric? And how can one claim something is unbalanced while focusing on one metric? reply sudosysgen 13 hours agoparentprevNot really, the killer is latency, not throughput. It's very rare that a CPU actually runs out of memory bandwidth. It's much more useful for the GPU. 95GB/s is 24GB/s per core, at 4.8Ghz that's 40 bits per core per cycle. You would have to be doing basically nothing useful with the data to be able to get through that much bandwidth. reply adrian_b 3 hours agorootparentFor scientific/technical computing, which uses a lot of floating-point operations and a lot of array operations, when the memory is limiting the performance almost always the limit is caused by the memory throughput and almost never by the memory latency (in correctly written programs, which allow the hardware prefetchers to do their job of hiding the memory latency). The resemblance to the behavior of GPUs is not a coincidence, but it is because the GPUs are also mostly doing array operations. So the general rule is that the programs dominated by array operations are sensitive mostly to the memory throughput. This can be seen in the different effect of the memory bandwidth on the SPECint and SPECfp benchmark results, where the SPECfp results are usually greatly improved when memory with a higher throughput is used, unlike the SPECint results. reply sudosysgen 3 hours agorootparentYou are right that it's a limiting factor in general for that use case, just not in the case of this specific chip - this chip has far less cores per lanes, so latency will be the limiting factor. Even then, I assure you that no scientific workload is going to be consuming 40 bits/clock/core. It's just a staggering amount of memory, no correctly written program would hit this, you'd need to have abysmal cache hit ratios. This processor has two lanes over 4 P-cores. Something like an EPYC-9754 has 12 lanes over 128 cores. reply adrian_b 2 hours agorootparentI agree that for CPU-only tasks, Lunar Lake has ample available memory bandwidth, but high memory latency. However, the high memory bandwidth is intended mainly for the benefit of its relatively big GPU, which might have been able to use even higher memory throughputs. reply fulafel 6 hours agorootparentprev40 bits per clock in a 8-wide core gets you 5 bits per instruction, and we have AVX512 instructions to feed, with operand sizes 100x that (and there are multiple operands). Modern chips do face the memory wall. See eg here (though about Zen 5) where they in the same vein conclude \"A loop that streams data from memory must do at least 340 AVX512 instructions for every 512-bit load from memory to not bottleneck on memory bandwidth.\" reply adrian_b 3 hours agorootparentThe throughput of the AVX-512 computation instructions is matched to the throughput of loads from the L1 cache memory, on all CPUs. Therefore to reach the maximum throughput, you must have the data in the L1 cache memory. Because L1 is not shared, the throughput of the transfers from L1 scales proportionally with the number of cores, so it can never become a bottleneck. So the most important optimization target for the programs that use AVX-512 is to ensure that the data is already located in L1 whenever it is needed. To achieve this, one of the most important things is to use memory access patterns that will trigger the hardware prefetchers, so that they will fill the L1 cache ahead of time. The main memory throughput is not much lower than that of the L1 cache, but the main memory is shared by all cores, so if all cores want data from the main memory at the same time, the performance can drop dramatically. reply sudosysgen 2 hours agorootparentprevThe processors that hit this wall have many many cores per memory lane. It's just not realistic for this to be a problem with 2 lanes of DDR5 feeding 4 cores. These cores cannot process 8 AVX512 instructions at once, in fact they can't do it at all, as it's disabled on consumer Intel chips. Also, AVX instructions operate on registers, not on memory, so you cannot have more than one register being loaded at once. If you are running at ~4 instruction per clock, to actually go ahead and saturate 40 bits per clock on 64 bit loads, you'd need 1/6 of instructions to hit main memory (not cache)! reply unsigner 11 hours agorootparentprevThere might be a chicken-and-egg situation here - one often hears that there’s no point having wider SIMD vectors or more ALU units, as they would spend all their time waiting for the memory anyway. reply adrian_b 3 hours agorootparentThe width and count of the SIMD execution units are matched to the load throughput from the L1 cache memory, which is not shared between cores. Any number of cores with any count and any width of SIMD functional units can reach the maximum throughput, as long as it can be ensured that the data can be found in the L1 cache memories at the right time. So the limitations on the number of cores and/or SIMD width and count are completely determined by whether in the applications of interest it is possible to bring the data from the main memory to the L1 cache memories at the right times, or not. This is what must be analyzed in discussions about such limits. reply immibis 39 minutes agorootparentprevCPUs generally achieve around 4-8 FLOPS per cycle. That means 256-512 bits per cycle. We're all doing AI which means matrix multiplications which means frequently rereading the same data bigger than the cache, and doing one MAC with each piece of data read. reply jart 8 hours agorootparentprevThe most important algorithm in the world, matrix multiplication, just does a fused multiply add on the data. Memory bandwidth is a real bottleneck. reply adrian_b 3 hours agorootparentThe importance of the matrix multiplication algorithm is precisely due to the fact that it is the main algorithm where the ratio between computational operations and memory transfers can be very large, therefore the memory bandwidth is not a bottleneck for it. The right way to express a matrix multiplication is not that wrongly taught in schools, with scalar products of vectors, but as a sum of tensor products between the column vectors of the first matrix with those row vectors of the second matrix that share with them the same position of the element on the main diagonal of the matrix. Computing a tensor product of two vectors, with the result accumulated in registers, requires a number of memory loads equal to the sum of the lengths of the vectors, but a number of FMA operations equal to the product of the lengths (i.e. for square matrices of size NxN, there are 2N loads and N^2 FMA for one tensor product, which multiplied with N tensor products give 2N^2 loads and N^3 FMA operations for the matrix multiplication). Whenever the lengths of both vectors are are no less than 2 and at least one length is no less than 3, the product is greater than the sum. With greater vector lengths, the ratio between product and sum grows very quickly, so when the CPU has enough registers to hold the partial sum, the ratio between the counts of FMA operations and of memory loads can be very great. reply svantana 6 hours agorootparentprevIs it though? The matmul of two NxN matrices takes N^3 macs and 2*N^2 memory access. So the larger the matrices, the more the arithmetic dominates (with some practical caveats, obviously). reply perryh2 18 hours agoprevIt looks awesome. I am definitely going to purchase a 14\" Lunar Lake laptop from either Asus (Zenbook S14) or Lenovo (Yoga Slim). I really like my 14\" MBP form factor and these look like they would be great for running Linux. reply jjmarr 17 hours agoparentI constantly get graphical glitches on my Zenbook Duo 2024. Would recommend against going Intel if you want to use Linux. reply skavi 16 hours agorootparentIntel has historically been pretty great at Linux support. Especially for peripherals like WiFi cards and GPUs. reply jauntywundrkind 15 hours agorootparentTheir \"PCIe\" wifi cards \"mysteriously\" not working in anything but Intel systems is enraging. I bought a wifi7 card & tried it in a bunch of non-Intel systems, straight up didn't work. Bought a wifi6 card and it sort of works, ish, but I have to reload the wifi module and sometimes it just dies. (And no these are not cnvio parts). I think Intel has a great amazing legacy & does super things. Usually their driver support is amazing. But these wifi cards have been utterly enraging & far below what's acceptable in the PC world; they are not fit to be called PCIe devices. Something about wifi really brings out the worst in companies. :/ reply transpute 14 hours agorootparent> not fit to be called PCIe devices They might be CNVi in M.2 form factor, with the rest of the \"wifi card\" inside the Intel SoC. In CNVi, the network adapter's large and usually expensive functional blocks (MAC components, memory, processor and associated logic/firmware) are moved inside the CPU and chipset (Platform Controller Hub). Only the signal processor, analog and Radio frequency (RF) functions are left on an external upgradeable CRF (Companion RF) module which, as of 2019 comes in M.2 form factor. Wifi7 has 3-D radar features for gestures, heartbeat, keystrokes and human activity recognition, which requires the NPU inside Intel SoC. The M.2 card is only a subset. reply zaptrem 12 hours agorootparent> Wifi7 has 3-D radar features for gestures, heartbeat, keystrokes and human activity recognition, which requires the NPU inside Intel SoC. The M.2 card is only a subset. Source? Google turned up nothing. reply zxexz 10 hours agorootparentSounds like some LLM hallucination to me. EDIT: Right after that I found another HN comment [0] by the same user (through a google search!)! [-1] Interesting IEEE rfc email thread on related to preamble puncturing misc (I have not yet read these through beyond the abstracts): A preprint in ArXiV related to the proposed spec [1] A paper in IEEE Xplore on 802.11bf [2] NIST publication on 802.11bf [3] (basically [2] but on NIST) [-1] https://www.ieee802.org/11/email/stds-802-11-tgbe/msg00711.h... [0] https://news.ycombinator.com/item?id=38811036 [1] https://arxiv.org/pdf/2207.04859 [2] https://ieeexplore.ieee.org/document/10467185 [3] https://www.nist.gov/publications/ieee-80211bf-enabling-wide... reply adrian_b 2 hours agorootparentThe part about the supposed features of WiFi 7 looks like a hallucination or perhaps only a misinterpretation of proposed features. How to do the actual sensing functions does not belong in a communication standard. What has been proposed, but I do not think that the standard amendment has been finalized, is to make some changes in the signals transmitted by WiFi stations, which would enable those desiring to implement sensing functions with the WiFi equipment to do that, without interfering with the normal communication functions. So if Intel would create some program for Lunar Lake CPUs, possibly using the internal NPU, for purposes like detecting the room occupancy, that application would not be covered by the WiFi standard, the standard will just enable the creation of such applications and such an application would be equally implementable with any PCIe WiFi card conforming to the new standard, not only with an Intel CNVi card, whicd uses an internal WiFi controller. However it is correct that there are 2 kinds of Intel WiFi cards that look the same (but they have different part numbers, e.g. AX200 for PCIe and AX201 for CNVi), but one kind of cards are standard PCIe cards that work in any computer, while the other kind of cards (CNVi) works only when connected to compatible Intel laptop CPUs. reply transpute 2 hours agorootparent> Even if it is possible to implement such features It has been possible for years with custom firmware, search \"device free wireless sensing\" in Google Scholar. > they would not be incorporated in a communication standard at this time. One would hope so, right? https://www.technologyreview.com/2024/02/27/1088154/wifi-sen... When the new standard comes out in 2025, it will allow “every Wi-Fi device to easily and reliably extract the signal measurements,” Yang says. That alone should help get more Wi-Fi sensing products on the market. “It will be explosive,” Liu believes.. cases imagined by the committee include counting and finding people in homes or in stores, detecting children left in the back seats of cars, and identifying gestures, along with long-­standing goals like detecting falls, heart rates, and respiration. Wi-Fi 7, which rolls out this year, will open up an extra band of radio frequencies for new Wi-Fi devices to use, which means more channel state information for algorithms to play with. It also adds support for more tiny antennas on each Wi-Fi device, which should help algorithms triangulate positions more accurately. With Wi-Fi 7, Yang says, “the sensing capability can improve by one order of magnitude..” WiGig already allows Wi-Fi devices to operate in the millimeter-wave space used by radar chips like the one in the Google Nest.. [use cases include] reidentifying known faces or bodies, identifying drowsy drivers, building 3D maps of objects in rooms, or sensing sneeze intensity (the task group, after all, convened in 2020).. There is one area that the IEEE is not working on, at least not directly: privacy and security. In advance of the IEEE 802.11bf standard, Intel implemented presence detection with WiFi 6E starting with 2023 Meteor Lake sensors and NPU, https://www.intel.com/content/www/us/en/content-details/7651... Intel Wi-Fi can intelligently sense when to lock or wake your laptop Walk Away Lock: Wi-Fi senses your departure & locks the PC in seconds Wake on Approach: Wi-Fi senses your return & wakes the PC in seconds Intel Labs 2023 PoC demo of breathing detection, https://community.intel.com/t5/Blogs/Tech-Innovation/Client/... The solution detects the rhythmic change in CSI due to chest movement during breathing. It then uses that information to detect the presence of a person near a device, even if the person is sitting silently without moving. The respiration rates gathered by this technology could play an important role in stress detection and other wellness applications. > such an application would be equally implementable with any PCIe WiFi card conforming to the new standard Yes, this would be possible on AMD, Qualcomm and other \"AI\" PCs. Some Arm SoCs include NPUs that could be used by routers and other devices for WiFi sensing applications. > looks like a hallucination or perhaps only a misinterpretation of proposed features Is there a good term for reality conflicting with claims of hallucination and misinterpretation? reply jauntywundrkind 2 hours agorootparentprev> They might be CNVi in M.2 form factor, with the rest of the \"wifi card\" inside the Intel SoC. Already answered: > And no these are not cnvio parts (Oops, CNVi not cnvio.) reply transpute 1 hour agorootparentWhich model number(s) exhibited this behavior? reply silisili 16 hours agorootparentprevI get them also in my Lunar Lake NUC. Usually in the browser, and presents as missing/choppy text oddly enough. Annoying but not really a deal breaker. Hoping it sorts out in the next couple kernel updates. reply jjmarr 11 hours agorootparentDo you get weird checkerboard patterns as well? reply gigatexal 16 hours agorootparentprevGive it some time. Probably needs updated drivers, intel and Linux have been rock solid for me too. If your hardware is really new it’s likely a kernel and time issue. 6.12 or 6.13 should have everything sorted. reply rafaelmn 10 hours agorootparentGiven the layoffs and the trajectory spiral I wouldn't be holding my breath for this. reply amanzi 18 hours agoparentprevI'm really curious about how well they run Linux. e.g. will the NPU work under Linux in the same way it does on Windows? Or does it require specific drivers? Same with the batter life - if there a Windows-specific driver that helps with this, or can we expect the same under Linux? reply ac29 17 hours agorootparentYou can look at the NPU software stack here: https://github.com/intel/linux-npu-driver/blob/main/docs/ove... The Linux driver is specific to Linux, but the software on top of that like oneAPI and OpenVINO are cross platform I think. reply wmf 18 hours agorootparentprevAll NPUs require drivers. https://www.phoronix.com/news/Intel-Linux-NPU-Driver-1.5 reply formerly_proven 8 hours agoparentprevI’d wait for LG honestly reply RicoElectrico 18 hours agoprev> A plain memory latency test sees about 131.4 ns of DRAM latency. Creating some artificial bandwidth load drops latency to 112.4 ns. Can someone put this in context? The values seem order of magnitude higher than here: https://www.anandtech.com/show/16143/insights-into-ddr5-subt... reply toast0 17 hours agoparentThe chips and cheese number feels like an all-in number; get a timestamp, do a memory read (that you know will not be served from cache), get another timestamp. The anandtech article is latencies for parts of a memory operation, between the memory controller and the ram. End to end latency is going to be a lot more than just CAS latency, because CAS latency only applies once you've got the proper row open, etc. reply wtallis 17 hours agorootparentGetting requests up through the cache hierarchy to the DRAM controller, and data back down to the requesting core's load/store units is also a non-trivial part of this total latency. reply jart 8 hours agoparentprevUse Intel's mlc (memory latency checker) tool to measure your system. On a GCE instance I see about 97ns for RAM access. On a highly overclocked gaming computer with a small amount of RAM I see 60ns. Under load, latency usually drops to about 200ns. On workstation with a lot of RAM and cores I see it drop to a microsecond. reply foota 18 hours agoparentprevI think the numbers in that article (the CAS latency) are the latency numbers \"within\" the DRAM module itself, not the end to end latency between the processor and the RAM. You could read the article on the latest AMD top of the line desktop chip to compare: https://chipsandcheese.com/2024/08/14/amds-ryzen-9950x-zen-5... (although that's a desktop chip, the original article compares the Intel performance to 128 ns of DRAM latency for AMD's mobile platform Strix Point) reply Tuna-Fish 6 hours agorootparentCAS latency is only the latency of doing an access from an open row. This is in no way representative of a normal random access latency. (Because caches are so large that if you were frequently hitting open rows, you'd just load from cache instead.) The way CAS has been widely understood as \"memory latency\" is just wrong. reply Sakos 10 hours agoparentprevThat article is about RAM latency in isolation. See this Anandtech article that shows similar numbers to chips and cheese when evaluating a CPU's DRAM latency (further down on the page): https://www.anandtech.com/show/16214/amd-zen-3-ryzen-deep-di... reply adrian_b 11 hours agoprevI completely agree with the author that renaming the L1 cache memory as L0 and introducing a new L1 cache, as done by Intel is a completely misleading terminology. The correct solution is that from the parent article, to continue to call the L1 cache memory as the L1 cache memory, because there is no important difference between it and the L1 cache memories of the previous CPUs, and to call the new cache memory that has been inserted between the L1 and L2 cache memories as the L1.5 cache memory. Perhaps Intel did this to give the very wrong impression that the new CPUs have a bigger L1 cache memory than the old CPUs. To believe this would be incorrect, because the so called new L1 cache has a much lower throughput and a worse latency than a true L1 cache memory of any other CPU. The new L1.5 is not a replacement for an L1 cache, but it functions as a part of the L2 cache memory, with identical throughput as the L2 cache, but with a lower latency. As explained in the article, this has been necessary to allow Intel to expand the L2 cache to 2.5 MB in Lunar Lake and to 3 MB in Arrow Lake S (desktop CPU), in comparison with AMD, which has an only 1 MB L2 cache (but a bigger L3 cache). According to rumors, while the top AMD desktop CPUs without stacked cache memory have an 80 MB L2+L3 cache (16 MB L2 + 64 MB L3), the top Intel model 285K might have 78 MB of cache, i.e. about the same amount, but with a different distribution on levels: 2 MB L1.5 + 40 MB L2 + 36 MB L3. Nevertheless, until now there is no official information from Intel about Arrow Lake S, whose launch is expected in a month from now, so the amount of L3 cache is not certain, only the amounts of L2 and L1.5 are known from earlier Intel presentations. Lunar Lake is an excellent design for all applications where adequate cooling is impossible, i.e. thin and light notebooks and tablets or fanless small computers. Nevertheless, Intel could not abstain from not using unfair marketing tactics. Almost all the benchmarks presented by Intel at the launch of Lunar Lake have been based on the top model 288V. Both top models 288V and 268V are likely to be unobtainium for most computer models, while at the few manufacturers that will offer this option they will be extremely overpriced. Most available and affordable computers with Lunar Lake will not offer any better CPU than 258V, which is the one tested in the parent article. 258V has only 4.8 GHz/2.2 GHz turbo/base clock frequencies, vs. 5.1 GHz/3.3 GHz of the 288V used in the Intel benchmarks and in many other online benchmarks. So the actual experience of most Lunar Lake users will not match most published benchmarks, even if it will be good enough in comparison with any competitors in the same low-power market segment. reply AzzyHN 17 hours agoprev [–] We'll have to see how this compared to Zen 5 once 24H2 drops. And once more than like three Zen 5 laptops come out. reply deaddodo 16 hours agoparentThe last couple of generations have had plenty of AMD options. Razer 14, Zephyrus G14, TUFbook, etc. If you get out of the performance/enthusiast segment, they're even more plentiful (Inspirons, Lenovos, Zenbooks, etc). reply nahnahno 16 hours agoparentprev [–] the review guide had everyone on 24H2; there were some issues with one of the updates that messed up performance for lunar lake pre-release, but appears to have been fixed in time for release. I’d expect lunar lake’s position to improve a bit in coming months as they tweak scheduling, but AMD should be good at this point. Edit: around 16 mark, https://youtu.be/5OGogMfH5pU?si=ILhVwWFEJlcA3HLO. The laptops came with 24H2 reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Intel's new Lion Cove architecture in Lunar Lake CPUs focuses on per-thread performance, offering significant improvements over its predecessor, Redwood Cove, in both performance and energy efficiency.",
      "Key enhancements include a simplified ring bus interconnect, improved L3 and DRAM latency, a new mid-level cache (L1.5), and an enhanced out-of-order execution engine, leading to notable performance gains.",
      "Lion Cove achieves 23.2% and 15.8% performance improvements in SPEC CPU2017’s integer and floating point suites, respectively, positioning it as a strong competitor against AMD’s Strix Point and even desktop CPUs like the Ryzen 9 7950X3D."
    ],
    "commentSummary": [
      "Intel's new Core Ultra 7 258V processor, part of the Lunar Lake series, shows a DRAM bandwidth of 94.9 GB/s, which is significantly lower than Apple's M1 Pro, Max, and Ultra chips.",
      "AMD's upcoming Strix Halo, set for early 2025, will feature a 256-bit memory interface for LPDDR5x, potentially matching or exceeding the M1 Pro's performance, but with higher power consumption.",
      "The discussion highlights the trade-offs between memory bandwidth and power efficiency, with Intel's Lunar Lake focusing on lower power consumption compared to Apple's higher bandwidth solutions."
    ],
    "points": 117,
    "commentCount": 67,
    "retryCount": 0,
    "time": 1727471913
  },
  {
    "id": 41674379,
    "title": "The Architecture of London Pubs (1966)",
    "originLink": "https://thelondonmagazine.org/archive-the-architecture-of-london-pubs-by-stephen-gardiner/",
    "originBody": "Stephen Gardiner The Architecture of London Pubs . In the mid-sixties, architect and writer, Stephen Gardiner, wrote recurrent socio-cultural architectural analysis for The London Magazine. This installment, on the state of that bastion of so-called English cultural activity, the pub, originally appeared in the December 1966 edition of The London Magazine. . . What’s happened to the pub, that most personal piece of English belongings? The place where you stand up and drink, where there are scrubbed oak benches to sit on, partitions to conceal private conversations, men with pipes and caps, and there is sawdust and beer on wooden floors? What’s happened to those powerful bar tops, the glass flaps over the counter and the bottle-crammed shelves; the complicated cut-glass that fortified you from the fog and the snow, and through which the indecipherable interior form of figures, furniture and lights made impossible shapes from the wet streets outside? What’s happened? – they’re going, or gone, most of them. The great brewers – Watneys, Whitbreads and so on—are disposing of all that rubbish: that’s out now, finished with, they say. That’s dead wood, old hat. We’re living in a Modern Age. Dickens is dead, you know. Did you know that? Well, some people don’t. Anyone would think Victoria was still alive! Good God – there are new materials now. Chromium plate, plastic (marvellous stuff, formica – doesn’t burn). Wonderful new lacquers you can see your face in and, of course, artificial flowers. Last forever. Well, you can’t beat that, can you? – flowers that last forever. The Germans have patented an artificial scent – you just spray it on, each morning – Rose, Gardenia – have to watch you don’t get your sprays mixed up, of course – not that anyone would notice. No, we don’t like bare boards, these days. We have to cater for the Young. Yes, yes, I know what you mean but we don’t encourage that sort of customer any more they’ve got their own pubs to go to this place has changed hands, the street’s ‘turned over’ and that’s all there is to it, we’re afraid. Drinking’s a business now, not a hobby. It’s double gins not pints of beer we’re after. Why doesn’t someone introduce the treble gin? – it’ll come, sir, it’ll come – amazing how much money there is about – even in the Freeze – just flowing like gin. Yes, we like the place to look friendly – plenty of flowers (and leaves, they make rather a nice dark green variety), and I like pink net formica myself although Market Research generally settles matters of taste. Close-carpeting is best, so warm – avocado, salmon, peach and shrimp are popular colours. And naturally, as you see, sir, we do food now… A little Italian prosciutto? broad beans? a spot of corn? tomatoes? – they’re lovely today, really beautiful. French bread? No?… No, I’m afraid the crabs are just part of the décor. They’re wax. Sketches all from the essay and by Stephen Gardiner. Pubs, in fact, are going the way of all those other things one cherishes – the countryside, eighteenth-century bits of towns and villages, and the only sort of structure it seems nobody dares touch are churches and this is probably for some bogus superstitious reason. Otherwise the ghastly formula for modernisation is applied wholesale – a formula which is a middle-class conception of what the twentieth-century is all about, a sixth-hand version of contemporary design, and so far as pubs are concerned the best parallel in the building business is the shoddy reproduction Georgian terrace which contains the latest (and cheapest) thing in aluminium sinks, cookers and the rest. It seems to me that the organisers of these projects are, in general, deliberately out to destroy anything which is unique, unusual or eccentric because they feel such things must conform with their pattern of life – the pattern they understand and which makes them feel safe and secure. Places must be the same. Friends must be the same sort of people, they must wear the same sort of clothes, say the same things. If they are not the same, then the friend gets struck off some list or other, and while it may possibly be reasonable to file away people into cabinets with smoothly sliding drawers, such things as buildings and pubs come into a different category. Architecture belongs to everyone and ought to be protected by the few who understand it. Of course it isn’t, because there is no one to do the job, or so it seems. How can you, therefore, expect anything but the destruction of pubs on a universal scale? Pubs are a universal English business. A formula has to be found to fit the majority taste. Something which is much the same – like battery hens and cattle factories. The first thing to go, when the formula is applied, are the separate compartments and partitions. In this way the variety provided by different bars – public, saloon, private etc. – is eliminated at a stroke. This happened, for example, at two excellent Chelsea pubs – the King’s Head and the Phene – when they were given the treatment two or three years ago. Both were devastated, both look as though a giant vacuum cleaner has passed over them leaving them forlorn, empty, desperately clean, and with just a few fixtures like a structural column or two. Otherwise everything was sucked out of them—all that was good and of real value like the old bar tops, the high backed wooden seats, the grimy mahogany tables with their curvaceous cast-iron legs, the lovely intricate shelves and polished brass rods and knobs, the dark panelling and even the old men with caps and pipes (imagine them sitting in glittering, close-carpeted no-man’s-land!). How can it happen? – and it is happening everywhere. Credits: Stephen Gardiner. Quite shortly the English pub will be extinct, part of history. The trouble is that the wretched brewers, in their hurry to find a modern equivalent of the traditional interior, neither stop to think nor to find proper architects and designers. Pubs shouldn’t be in the hands of hacks in their ‘interior department’; but then it clearly has not filtered through to them that the contents of these old places do have a value, as any antique dealer could tell them. And instead of rushing ahead with the steamroller that sticks down the rule-of-thumb carpets, wallpapers and plastic tops they should make it their business to discover what the past has to offer in the way of lessons in design and why the kind of plans that were used in the old days were so good, and seem particularly so when set against the dreadful ‘schemes’ that are being done now. It’s no use the customer objecting. (I tried it once, saying angrily, as if I had been tricked in some way: ‘What’s happened here?’ The manager merely said stonily: ‘The customers like it.’) No, it’s up to the brewer. And there are a few obvious arguments that can be put to him. After all, to start with an elementary point, when you are inserting a new inside to a building of quality belonging to a good architectural period the normal and correct procedure is to consider your design in relation to what happens on the outside – the scale, character and proportion of it, the detailing of the windows and so on; and by this I mean the sort of close observation which catches the atmosphere of the design and the period. If instead the inside is considered in isolation – and it is part of an architect’s responsibility to see that the whole of a problem is solved and not pieces of it – then the total result is bound to end as an architectural disaster. If you are landed with a Georgian or an early Victorian structure an answer has to be found which is somehow, although by no means literally, in sympathy with it. Should the brewer reject this argument as unimportant he is in fact dismissing something which is fundamental to the making of architecture of any time, and he is not worth talking to. But if he agrees one can go on. And if Henekey’s in the Portobello Road can make a good job of an interior with a few shelves, barrels, and glass-panelled partitions, then surely an architect of today can too – provided he has insight, feeling and some imagination, and is not too conscious of being an architect. Credits: Stephen Gardiner. The main problem is the plan: it always is, in any architectural problem. Once you’ve got your idea for the plan straight you can settle down to filling in the other gaps – structure, materials, finishes and so on. Well, what is the most successful sort of pub plan? And the only possible point one can start from with any design is one’s own experience – what sort of pub plan does one like best? Some pubs are huge and lavishly decorated with engraved glass like the Salisbury in St Martin’s Lane; others are huge but spare in their finishes and resemble great barns which contain things like dart boards and bar billiards; others are small and lavish like the Red Lion in Duke Street W1, or the Victoria in Sussex Place W8, and don’t have darts; others, again, are small and sparse like the Nag’s Head in Kinnerton Street SW1, or The Raven in Battersea Church Road and this one does have darts. But all the best pubs, the pubs which have the genuine atmosphere which is as solid as the people who inhabit them, do possess one thing in common and this is the position of the bar: it is always sited so that, like an altar in a church, it is the focus of attention. It is either in the centre of the whole space or it is arranged so that it forms clearly defined areas around it, and it is so shaped that it appears to personally control these areas. From this point on, the plan is developed to emphasise the areas, to increase their privacy and to simplify the control of them. Privacy is an important factor in pubs: the subtle suggestion of a room where there is exclusive service takes one back a bit – to those small inns embedded deep in the country, or within listening distance of the waves breaking, with their high-backed black dining seats – and there is something comforting and a little mysterious about it all, with a vague sense of nostalgia added in, a nostalgia for something remembered from history books and romantic stories and evoked by such details as the barrels at Henekey’s that have the white lettering on them. And this is why these partitions with their engraved glass panels have a real charm. The sense of privacy is complete but you can still see through the smoky glass, enough at any rate to identify a shadowy silhouette beyond. So the ideal plan seems to be, to my mind at least, one where the bar is in the centre (or approximates to this position), and where there are a number of compartments which are separated by partitions which have a somewhat temporary look, but which radiate from the bar. Credits: Stephen Gardiner. We can go on from there. The ideal pub has a low ceiling because this is correct for the scale of the compartments and increases the suspense; you see it carrying on above the top of the partitions which it misses by about a foot, and you wonder what’s going on over the other side. The ideal pub also has some division between the compartment and the bar areas and this is sometimes in the form of glass flaps, as at Henekey’s, or in the form of unaffected but beautifully made shelves such as one finds at the Victoria in Sussex Place, the City Barge at Strand-on-the-Green and the Hansom Cab in the Earl’s Court Road. These shelves are always filled with bottles (and the colour of bottles really does glow, particularly when you think of the different colours of the drinks that fill them) and all kinds of glasses – those huge brandy glasses, for example, which reflect light so admirably. This brings us to the question of display in general. The chief centre for display – in the ideal pub – is right there in the middle of the bar area and that’s where pretty well everything for sale is on show: this is the shop window. This is where labels matter and become, like the lettering and the inn signs outside, the décor. A concentration of labels and calligraphic handwriting can be marvellous. The ideal pub has a genius for the ideal display. So it seems that, in the final analysis, the design of the pub comes down to a few known quantities: ceiling heights, partitions, simple country scrubbed furniture or something lavish in leather (whatever the call), shelves and glass, the loving display of a collector and the dominating father-figure, the bar. But there are obviously no cut-and-dried rules. You don’t have to have partitions, and the Hansom Cab, for example, with its irregular, rambling plan manages both suspense and intimacy very well indeed without them. And ceilings can be high provided the total space – like that of the Salisbury – requires and sustains such height. And what about the brewer? Well, he might as well know that there are no good modern pubs: the Hoop from Finch’s is absurdly over-elaborate, The Champion is self-conscious, the Hansom Cab is (although well done) a reproduction piece and the Ranelagh in Pimlico is really terrible. For the brewer there are two rules which he must obey if the English pub tradition is to be protected. They are quite simple. If a good pub comes up for redecoration and alterations the best things and the authentic details must be preserved. If a bad pub comes up – and there are a depressingly large number of them – start again from scratch, but with the right architect. Lastly, don’t forget the dart board and the bar billiards which the TV set, in some curious, and odious, fashion, seems to have replaced. And no artificial flowers, by request, please. . . Stephen Gardiner was a British architect, teacher and writer. Born and raised in Chelsea, he wrote regularly for The London Magazine and The Observer. To discover more content exclusive to our print and digital editions, subscribe here to receive a copy of The London Magazine to your door every two months, while also enjoying full access to our extensive digital archive of essays, literary journalism, fiction and poetry.",
    "commentLink": "https://news.ycombinator.com/item?id=41674379",
    "commentBody": "The Architecture of London Pubs (1966) (thelondonmagazine.org)114 points by youbet 23 hours agohidepastfavorite46 comments PaulRobinson 10 hours agoI know a lot about London pubs, and this made me smile. If you want to see the kind of old layout he’s talking about, almost any Sam Smith pub in London will do - they pride themselves on keeping it traditional - with the best and most striking example probably being the Princess Louise near Holborn. Just don’t expect any beer names you recognise - it’s a brewery pub that only sells stuff made by Sam Smiths (the beer), or branded Sam Smiths (the spirits, the snacks…) Most of the others still exist, but I think have been refurbished quite extensively and not in a way he’d like. However, there is some hope. Newer bars are opening that are trying to tap into a less sports-focused vibe. Some focusing on food, some on entertainment, quite a few on a wider range of more unusual beers (the “Tap” chain near train stations and just down the road from Farringdon for example). Of course the dominant player in the mega pub “hall” space is Wetherspoons. Caverns - low-ceilinged cathedrals almost - to cheap beer and Brexit politics. They’re cheap, and so attract clientele who are price sensitive. That leaves more room in all the others for those of us who value something else, I guess. The pub trade in the U.K. though is in trouble. It’s interesting that Europe’s largest consumer lobby group is based in the U.K.: CAMRA. It’s most interesting that the CAMpaign for Real Ale, started to protect traditionally brewed cask ale from being obliterated by the sorts of breweries that thought beer should be tankered like petrol, has had to change it’s target. CAMRA basically thinks the war for Real Ale has been won. The rise of microbreweries has meant a plentiful supply of good quality beer is secure. But the pub is not. So now it’s become a bit more CAMPUB, and campaigns to save the business of public houses itself, the traditional bar games (skittles or bar billiards, anyone?), and the communities that sit in them. The architecture is important, the interior should be considered, the screens have a place in some - but not all - pubs. But it’s the people that matter, and at the moment the industry is in a mess. It’s remarkable so many pubs in this article still exist. I don’t think many of them will survive another 60 years, perhaps not even another 10. Enjoy them while you can. reply helsinkiandrew 5 hours agoparent> So now it’s become a bit more CAMPUB, and campaigns to save the business of public houses itself, the traditional bar games (skittles or bar billiards, anyone?), and the communities that sit in them. Isn't it the case that the \"pub scene\" is healthy - revenues and number of pub employees hasn't decayed significantly, but the number of pubs has. A big part of that is people preferring the larger pubs - going to Wetherspoons for a cheaper pint from a wide range of beers (perhaps with food and sports) than a cosy local that is more expensive and has a limited range. Unfortunately I can't see CAMRA being able to do much about the economics of a small traditional pub vs the current rental or sale value of the building its in. reply DrBazza 9 hours agoparentprevPubs used to be 11-3, and 6-11, give or take, whereas High Street restaurants, that can often also serve alcohol and family friendly, are 11-11, so it's not much of a surprise that they're converging, slowly to the same business model: there's not a pub in my area that doesn't do food, or coffee, or indeed breakfast. If you have a building that needs heating 24 hours a day, having revenue for most of that time is going to help, so a small number of those additionally offer free wifi for the WFH types, which actually seems to be beneficial. Some of those pubs local to me, that have been purchased and gutted by smaller boutique brewery chains, have been turned into something indistinguishable from a coffee shop - the dangling light bulbs, brickwork, copper pipes. Coincidentally, I'm off to my local #1 CAMRA pub later, and it is much like the article describes. A typical pub. Dark wood, central bar, low ceilings, two bars (saloon and public), darts, and one tv screen. And it will be full by mid-afternoon through to the end of the day, which is unusual for pubs now. It just seems like the main problem for pubs, and in fact, most of British industry, is costs, and that seems to be the exorbitant cost of electricity at the moment. reply specproc 7 hours agorootparentA lot of the problems started under Blair. Licensing for music was a horrible policy move. The smoking ban necessary, but brutal for pubs; ditto a crackdown on underage drinking. In my forties, and I feel that my generation was the last to enjoy a particular pub experience which is now a rapidly receding memory. reply PaulRobinson 5 hours agorootparentThe smoking ban caused a small bump - and we suddenly realised all the pubs had stinking toilets that the smoke had masked - but I think it actually resulted in both better environments for a wider audience, and a massive benefit to public health, particularly in working class communities. Long term, I think it led to a better pub environment for more people. There have been consistent and regular crackdowns on underage drinking for well over a century - I don't recall a particularly large crackdown in recent years, but the licensing has changed: the Police now have more leverage over whether a publican and their property should keep trading than they did before, and that's meant a lot of idiot landlords who didn't give a damn about the social problems their idiocy caused have been forced out of the industry. Those who run a tighter ship stay in business. By far the biggest shift seems to me, generational attitudes to drinking. When I was in my early 20s, I was in the pub pretty much every night (and many lunchtimes), and I wasn't alone, and was drinking with colleagues and friends who were the same age all the way up to retirement age and beyond. The people I work with today in their early 20s might go out twice a month, and even then might only have 2-3 drinks all night. They're more likely to go to the gym in the evening, or to head home and watch Netflix or read their Kindle than to go to the pub. It's interesting that low alcohol drinks are the biggest growth sector, and I've seen 0% beer on draught a little more in recent months. It might seriously help the sector if we just accept getting sloshed every night isn't good for people, people are realising it, and that you need to cater for that. reply gwervc 41 minutes agorootparentI don't know about the UK but in my country it's not uncommon to have pints priced at 8€, which is 0.5% of the minimal salary. It is a really pricy way to spend time. reply jessriedel 21 hours agoprevTangential: As an American, one of the things I liked most about London pubs when I first started visiting in the ‘00s was the lack of screens, which were hard to escape in American bars. Unfortunately this was only temporary, as the majority of the London pubs I’ve seen on recent visits are covered with screens like home. reply scrlk 21 hours agoparentSounds like you'd enjoy visiting a pub owned by Samuel Smith. > Our pubs are havens from the digital world – there are no TVs or background music. The use of mobile phones, laptops and other tech is not allowed in our pubs. https://samuelsmithsbrewery.co.uk/pubs/ reply craz8 18 hours agorootparentIn the 80s, Sam Smith pubs had a ‘25 pubs in London’ challenge. Get a drink in each of the 25 and get a T-shirt. It took me and a friend several weeks. There was a story of some guys doing it in a weekend. Hard because of travel AND opening times of some of the financial centre ones. Good Times! And of course, no screens and no-one had phones (except in the financial centre and those came with an external battery) reply williamdclt 20 hours agoparentprevAs others say, Londoners/brits make a distinction between “pub” and “sports pub”, the former don’t usually have any TV (or it’s off, only used for big England games when every pub becomes a sports pub). Contrary to your experience, I’m pretty sure that most pubs are not sports pubs in London reply tetris11 19 hours agorootparentThey do though. The old guard keeping the depressing pubholes alive do so by watching their football there. It's usually just one or two screens, granted, but they're there. Thankfully they can be easy ignored. reply specproc 7 hours agorootparentI'm not into them myself, but a lot of the ones that are struggling these days are the (non-chain) old man, football, working-class pubs in struggling towns. I'm back visiting for the first extended period of time in a decade, and the bifurcation of the drinking/eating sector is striking. So many new fancy, up-market places with food, craft beer and eye-watering prices; so many shuttered old-school pubs. It says a lot about where we are as a country. reply tetris11 2 hours agorootparentBlackpool meets London reply jessriedel 3 hours agorootparentprevThis can’t be defined away. Most of the members of the set ({pubs} U {sports pubs}) have screens in London. reply habosa 18 hours agoparentprevNo screens and at most of them no music either. Very few people drinking while standing. Just a pleasant place to have some beers with friends. When I moved back from London to the US (where I’ve spent 90% of my life) I was so much more distracted by the screens than I had ever been before. reply tempusalaria 18 hours agoparentprevgo to smaller pubs. They don’t have the footfall to justify the exorbitant commercial sports license fees and so don’t have screens. Fancier pubs and gastropubs also tend not to have screens reply jessriedel 3 hours agorootparentSure there are of course plenty of great places. They weren’t like purged or something. But now you have to go searchings, like in the US reply ljm 2 hours agorootparentI don’t think you do - every few years there are articles of pubs shutting down and it being a crisis. Happened as long as I’ve lived. You don’t have to go far precisely because your local craft beer haunt, gastropub, sports bar and boozer all serve different clientele. reply matt-p 20 hours agoparentprevThat's a sports pub. reply overcast 21 hours agoparentprevStay away from sports pubs/bars. reply LtWorf 19 hours agorootparentI've been in one which had tvs over the orinals, in sweden. reply amenhotep 19 hours agorootparentThat's kinda brilliant. Nothing worse than missing a goal because you had to answer a call of nature. reply dexterdog 18 hours agorootparentWhat about whizzing on your hands because of your bad timing? reply fsckboy 12 hours agorootparentyou were trying to head a corner kick? reply lifeisstillgood 11 hours agoprevWow there is a lot to unpick here. 1. I think all the pubs he mentions have gone. 2. He was born and raised in Chelsea. That’s pretty rare now - Londonnhas undergone a paroxysm of middle class selling up to wealthy (foreign) investors and I would be amazed if any architects today could be born and raised there. 3. I love the detail of the balance bars on the pub lanterns. They are all gone because an electric bulb can operate even when swinging - but a candle or gas just need to remain upright - wow. 4. Cars - cars are hardly mentioned because this was 1966 and you could drink and drive, you can park anywhere because most people did not have / need a car 5. Men not families - again still the sixties 6. The rise of food and Gastropubs - it’s rare a pub can survive on drinking alone and being part of the lunchtime food trade is almost as profitable as evening drinking Our “third spaces” do matter and reflect on us in interesting ways - going to come back to this article :-) reply PaulRobinson 10 hours agoparentMost of the pubs exist still, I think. The food thing has always been the case, it’s just that in the past you could make a bit of stew or have some pies in a warmer at the end of the bar, and you could sell them for a reasonable price, and make more profit than you would on same spend on beer (alcohol duty has been around a long time), but now, needs have changed. That’s come from two directions. First, those pie warmers and stew pots would struggle with modern health and safety rules in relation to food, and compliance with the regs costs more money so you need more expensive product. Secondly, consumer demand. A lot of central pubs now are dealing with far more tourists than were around in the 1960s as a consequence of cheaper air travel and changing drinking habits of local resident populations. Those two groups mean pubs have had to move to sit down meals, and at a near-restaurant price point. A few go a little under that level (Greene King and Fuller’s for example, they seem to do very well on food at a non-gastro price point), but they always knew food made more money than beer. I think it interesting that Sam Smith pubs segregate the food. You can’t just order food to your table - you have to go to the dining room. This means intent has to be decided on as you walk in. I like it a bit, but actually, I’d prefer the Fullers experience more, in that if I have a couple of pints and then want to order a battered whitebait with a jenga of chips and some crushed peas, I can do that. :) reply lifeisstillgood 9 hours agorootparentI think the “women in the workplace” social change has had a much more impactful change than the surface “gastropub”. Looking at say the 30s to the 60s pubs were a mostly male preserve, and the vast majority of the spenders. As society evened up its finances a little, women coukd choose and that choice was fairly obviously away from male dominated drinking establishments - over time of course. Anyway the shift to more geneder equality has had HVD impacts across the board - weakening trades unions, holding down wages etc I think I am wondering off the point but I reckon there is a six part tV series in “chnaging britain in a dozen pubs” reply eadmund 8 hours agoparentprevYou forgot 7: the men with caps and pipes are no longer there because of the smoking ban. It’s just not a proper pub if there’s not some smoke spiralling up to the ceiling. reply 082349872349872 7 hours agoprevA view from (1946): https://www.orwellfoundation.com/the-orwell-foundation/orwel... [snugs and gardens with plane trees and playground are still common in my area; they occur every 3 km or so] EDIT: looks like \"The Sloaney Pony\" might have a garden? no, I think I'd call that a terrace. reply zabzonk 23 hours agoprevthere was a bad time in the 1960s, when this article was published, but the pubs that are managing to survive nowadays (non-survival for a variety of reasons - covid, taxation to name two) are much better than suggested. reply mjirv 18 hours agoparentOut of curiosity, I googled several of the pubs he mentioned. All but one* was still around. *I found a pub called the Ranelagh, but it’s not in Pimlico, so I assume it’s a different one. It was the one he described as “really terrible,” so no big loss, I suppose. Addendum: the other interesting thing I noticed was the ones he derided as having been “modernized” in the 1960s were also newly renovated today, with airy, Scandinavian, 2020s aesthetics. Presumably because unlike the traditional pubs, the 60s style became dated pretty quickly. reply zabzonk 14 hours agorootparentThere is a pub called The Ranelagh in Bounds Green, North London which is near to what used to be a Middlesex Polytechnic site where the computer centre was located (DEC 10, two IBM 4381s, several VAXen and a couple of Primes) and where I worked in the mid to late 1980s. It was a hole then (still there, but I haven't been in for many years), but that didn't stop us programmers drinking there. reply abridges6523 15 hours agorootparentprevMuch denser world network now reply surfingdino 11 hours agoparentprevIf they can survive being converted in housing stock. They are disappearing fast. reply bluesounddirect 4 hours agoprevSimilarly the bars / pubs of the north east of the US had a similar vibe until the late 90s . I remember 3 places Jersey City , Hoboken , Union City had tons of no screen places or just one screen . But nyc had tons by this point . Now not having 2 tv is the song of death. reply ProxCoques 22 hours agoprev> The great brewers – Watneys, Whitbreads and so on—are disposing of all that rubbish: that’s out now, finished with, they say. So was this the start of the great decline in the quality of brewing in the UK during the 70's that led to CAMRA and eventually to the microbrewery renaissance we had in the late 90's to 00's? reply laurencerowe 14 hours agoparentI moved to the US about a decade ago, but I feel like microbreweries were pretty rare in the UK during the 90's and 00's, I only really came across a couple living in Manchester at the time (a couple more have opened since) while there are several within a couple miles of me in San Francisco. Most real ale in Britain was brewed in traditional breweries that had been going for a century or more that had either escaped being rolled into one of the majors or revived one of the old breweries abandoned by them, like Black Sheep in the old Lightfoot's Brewery. By contrast the UK microbreweries often seemed more influenced by the US craft beer style which developed from home brewing since their traditional breweries were all shut down during prohibition. reply ProxCoques 4 hours agorootparentI see - I got the impression that the 70-80's was a sort of dark age for beer in the UK, with mass-produced low-quality stuff from the likes of Watneys and Carslberg etc. taking over, which CAMRA was a reaction against. reply ljm 22 hours agoprev> Quite shortly the English pub will be extinct, part of history. Plus ça change, plus c’est la même chose Many of the described style of pubs are alive and well, often in the form of a Sam Smith’s. reply f_allwein 6 hours agoprevBeautiful bit of pub history in this video: Only You - Flying Pickets (apparently, The Red Lion & Pineapple): https://youtu.be/The Red Lion & Pineapple?si=RJXNiaY5xe5pOtzx reply papa-whisky 8 hours agoprevTweedy Pubs on YouTube has many videos detailing historic pubs in London, worth a watch if you found TFA interesting: https://youtube.com/@tweedypubs (No affiliation, I just enjoy the channel) reply ggm 21 hours agoprevI mainly drank around UCL in bloomsbury and down at the Princess Louise near high holborn. Cosy snugs and a refurbished Victorian ambiance in the early 80s. My parents drank around Shepherd's Bush in the 50s and 60s and \"the goons\" used to refine their schtik in the pub. Fun times! reply PaulRobinson 10 hours agoparentPrincess Louise is still there, same as it always was. Around the corner is the Hercules Pillars which has been substantially refurbished, but still very comfortable and has some separation going on. Most of the pubs around Bloomsbury have gone though. There are a few, but hard to keep it all going in an area where a lot of the housing lies empty - just owned by foreign investors, who are using it as a store of value they hope will appreciate faster than other asset classes. reply tetris11 19 hours agoparentprevThere was one pub not far off TCR that had a nice sofa and fireplace. We'd always have one of us duck out early to secure the spot an hour or two before. reply retzkek 15 hours agoprevA random blog I found through Kagi Small Web is one man's journey to visit all the pubs in the Good Beer Guide: https://simeyeveritt.wixsite.com/brapa It's such an interesting look into these slices of life, both current and former, that are so unlike my own experiences. reply codedeep 12 hours agoprev [–] UK Pubs always have a style, they stick out when abroad. I've not noticed the same consistency of style in US bars/pubs. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "In a 1966 essay, architect Stephen Gardiner criticized the modernization of traditional English pubs, lamenting the loss of unique features like oak benches and intricate glasswork.",
      "Gardiner argued that pubs should retain their historical charm and be designed with sensitivity to their architectural context, emphasizing elements like central bars, low ceilings, and partitions.",
      "He concluded that preserving authentic details and employing skilled architects are crucial for maintaining the English pub tradition."
    ],
    "commentSummary": [
      "The post discusses the architecture and evolution of London pubs, highlighting traditional layouts and the impact of modern refurbishments.",
      "It mentions the challenges faced by the UK pub trade, including economic pressures, changing consumer preferences, and regulatory impacts like the smoking ban.",
      "The post underscores the cultural significance of pubs, noting efforts by organizations like CAMRA (Campaign for Real Ale) to preserve traditional pubs and their communities."
    ],
    "points": 114,
    "commentCount": 46,
    "retryCount": 0,
    "time": 1727463878
  },
  {
    "id": 41679972,
    "title": "SunVox: Powerful Modular Synthesizer and DAW",
    "originLink": "https://www.warmplace.ru/soft/sunvox/",
    "originBody": "WarmPlace.ru News Forum About Donate РусскийSoftware SunVox PixiTracker Virtual ANS PhonoPaper Fractal Bits Relic Waves Relic Flow Pixilang PixiVisorPixiScope Quantum VJ HD More...Hardware Mobile audio input Quantum DJ Quantum VJ More...Music & Video Music Generative music Video SunVox Latest release - v2.1.1c (29 nov 2023) Русская версия здесьDownload: SunVox for Windows, macOS, Linux and Windows CE SunVox for iOS SunVox for Android ChangeLog SunVox library for developersJS SunVox Player Please support further SunVox development About User manual VideoTutorialsScreenshotsLogo MusicSamplesModules ForumFBTelegram chat/news Old versions What is SunVox SunVox is a small, fast and powerful modular synthesizer with pattern-based sequencer (tracker). It is a tool for those people who like to compose music wherever they are, whenever they wish. On any device. On any system. And it's free for most of the systems, except the Android and iOS. Key features: highly optimized synth algorithms, flexible architecture; SunVox works on a variety of devices with different CPUs; supported systems: Windows (2000+); macOS (10.13+); Linux (x86, x86_64, ARM (Raspberry Pi, PocketCHIP, N900, etc.), ARM64 (PINE64, etc.)); iOS (12.0+); Android (4.1+); Windows CE (including Pocket PC and Windows Mobile; ARM only); supported sound systems: ASIO, DirectSound, MME, ALSA, OSS, JACK, Audiobus, IAA; SunVox as a plugin: AU instrument/effect for iOS; support of 16/24/32bit WAV, AIFF and XI samples; multitrack WAV export; MIDI In/Out/Import/Export; XM (FastTracker) and MOD (ProTracker, OctaMED) import; real-time sample recording; real-time recording of any types of sound events (notes, Touch Theremin, controllers); powerful microtonal (ultra-chromatic) sequencer; generative music features: random selection of notes, random values of controllers, probability effects; see examples; free cross-platform library for developers is available; so it's possible to use SunVox engine in your own applications; a lot of built-in modules (synths and effects) + ability to make very complex connections between them. More information... (user manual, installation guide, system requirements, FAQ, etc.) Screenshots Video Video tutorials More videos on my YouTube channel... Music created in SunVox Сompetitions: SunVox Compo 2024.02 SunVox Compo 2023.02 SunVox Compo 2022.02 SunVox Compo 2021.02 SunVox Compo 2020.02 SunVox Compo 2019.03 SunVox Compo 2015.10 SunVox Compo 2015.02 SunVox Compo 2014.10 SoundCloud: best of 2022, 2021, 2020, 2019, 2018, 2017, 2016, 2015. Please mark your songs with the #SunVox hashtag, so that anyone can easily find it. Music from NightRadio Old versions SunVox 1.8.1 for PalmOS and MeeGo Archive © Alexander Zolotov nightradio@gmail.com",
    "commentLink": "https://news.ycombinator.com/item?id=41679972",
    "commentBody": "SunVox: Powerful Modular Synthesizer and DAW (warmplace.ru)100 points by timkq 6 hours agohidepastfavorite27 comments mastazi 4 hours agoAround 2000-ish I was making music with a similar piece of software called Jeskola Buzz which is also a modular[1] tracker[2]. About 5 years ago I remember thinking it would be cool if I could use that type of UI again and it would be great if something like that existed that could run on my tablet. So I searched and I was amazed when I found SunVox. It's a very capable piece of software, highly recommended [1] https://en.wikipedia.org/wiki/Modular_synthesizer [2] https://en.wikipedia.org/wiki/Music_tracker reply puppetmaster 2 hours agoparentJeskola Buzz[1] is a very influential tracker. The source code was lost for a while, but I recently learned that development re-started around 2008. [1] https://en.wikipedia.org/wiki/Jeskola_Buzz reply qwertox 36 minutes agoprevImmediately reminded me of Jeskola Buzz [0]. While most screenshots don't show it, it also had a tracker included [1]. [0] https://en.wikipedia.org/wiki/Jeskola_Buzz [1] https://youtu.be/gLwWMfJLXcM?t=454 reply Rochus 59 minutes agoprevThis is a great sounding synthesizer; but closed-source as it seems; the compiled shared libraries can be downloaded and I found an old version of the source code here: https://github.com/bohwaz/sunvox. reply gldnspud 26 minutes agoparentThe audio/synth engine source is MIT licensed as part of pixilang: https://www.warmplace.ru/soft/pixilang/ reply ofalkaed 1 hour agoprevOne of the very few softsynths that has good interface for touchscreens, think it is the only one I have found that I actually enjoy using. Not a big fan of its sound, just not what I am after, but I don't mind it and it works great as a portable scratchpad. reply packetlost 2 hours agoprevI've been using SunVox on and off for like 15 years now. I'm not really musically inclined, but I always come back to toy around for a bit. The creator is the sort of mad genius type, he seems to have made his own programming language and cross-platform rendering toolkit to make it all work and has a number of projects based on it. Because of this, SunVox is available on basically any platform you can think, including both major mobile OSes app stores and Linux for a few dollars ($5, iirc) and is definitely worth it. If you're familiar with a sequencer workflow (other notable projects: LSDJ [^0]), it doesn't seem too \"out there\", though the built-in synthesizers tend to lean towards a more airey feel. Highly recommend loading up the example projects, lots of people (including the creator) have some samples in there to poke around with. It's really interesting seeing how different people use the tools provided to compose, sometimes taking wildly different approaches to get similar results. TLDR: download it and load up the sample projects, it's really fun [0]: https://www.littlesounddj.com/lsd/index.php reply pixelpoet 39 minutes agoprevSo much love and respect for SunVox, watch this unbelievably beautiful example song with crazy surprise about halfway through: https://youtu.be/AHFSrxlouh8 reply honkfestival 3 hours agoprevFor the adventurous, a new beta was released earlier this month: https://warmplace.ru/forum/viewtopic.php?t=7005 reply gldnspud 1 hour agoprevAh, SunVox. It's like an instrument that Alexander Zolotov (AKA NightRadio) made for his own (excellent) music, and has generously shared with the world, creating a virtuous cycle of refinement in both his music and the instrument. I can't say enough nice things about SunVox. When I first saw it I was looking for trackers, and didn't spend much time with it. The second time I was looking for modular synthesizer apps, and that's when I fell in love. SunVox was my \"gateway drug\" to deeper learning about audio synthesis and processing techniques. You can create entire compositions and useful effects processors with SunVox using only the modular synthesis parts of it. It's very tracker-oriented, and you can do lots of tracker things with it, but don't be fooled into thinking it's a tracker. It HAS a tracker, and that is a strong part of its history and common usage, but it is much more than that. The modular interface is very approachable and powerful once you get accustomed to some of the fundamentals. \"MetaModules\" are one of its secret weapons. They let you package an entire SunVox project into a module, and expose an interface of up to 96 controllers, along with audio and note I/O. They can be arbitrarily nested… MetaModules all the way down. Besides sharing full compositions, MetaModules are one of the primary ways people share their creativity in the SunVox community. One prolific producer just released a collection of 236 modules built over the last four years. [1] Heck, it's even Turing-complete. Someone implemented a CPU using SunVox! [2] SunVox has a library version that lets you embed the audio engine into your own app [3] and there's even a WASM version. It's particularly well-suited for games, because you can control up to 16 independent SunVox instances at a time (to separate music and SFX for example) and it will mix them together. During my own explorations of SunVox I reverse-engineered and documented the file format [4] and wrote a library called \"Radiant Voices\" [5] for Python and TypeScript that lets you read and write SunVox files. If you read/write to specific filesystem locations, you can effectively hook into the SunVox clipboard, making it possible to write auxiliary apps that smoothly integrate with SunVox. One of my favorite experiments combining those techniques was to create a \"MetaModule Construction Kit\", which lets you use Python to create and manipulate MetaModules parametrically, experiment with them using MIDI and an alternative UI, then copy them over to SunVox itself once you are happy with the resulting MetaModule. [6] (Sadly, I don't find myself having enough time as of late to keep those side projects up-to-date with the latest versions of SunVox. YMMV if you decide to explore them. Contact me if you want to chat about them at all, especially if anyone's interested in collaborating to help bring them back in sync with the latest version of SunVox.) I could go on and on singing praises about this software (and other apps created by the same author), but I'll spare both the reader and myself… for now. :-) [1] https://vekonvekon.itch.io/acheney-modules [2] https://logickin.net/logicprocessing/the-most-ambitious-proj... [3] https://warmplace.ru/soft/sunvox/sunvox_lib.php [4] https://radiant-voices.readthedocs.io/en/1.0.0-dev/sunvox-fi... [5] https://github.com/metrasynth/radiant-voices/tree/sunvox-2.0... [6] https://www.youtube.com/watch?v=Ckgn4xGt8bc reply atorodius 2 hours agoprevWow can't believe this ran on a Palm. I wish I had known this back in ~2005 reply ashdnazg 2 hours agoprevI used it in a few game jams and can thoroughly recommend it. The interface might be intimidating at first, but with two wave generators and a couple of patterns you can already conjure a passable background track. A bit more complexity and it can sound pretty good. reply sneak 2 hours agoprevFor the iOS version: $6, no subscription, no tracking? Bought it before even looking at all the screenshots, if for no other reason than supporting developers like this. reply sillyenski 4 hours agoprevhttps://en.wikipedia.org/wiki/SunVox reply mouse_ 3 hours agoprevman that's so frickin' beautiful reply Carrok 3 hours agoparentWe have vastly different perceptions of beauty. reply bwanab 2 hours agorootparentIt's possible they mean the sound as opposed to the look. reply slackfan 2 hours agorootparentprevIt truly is in the eye of the beholder reply bowsamic 3 hours agoprev [–] I always thought this software was weird. To me modular implies parameter modulation for sound design, but this seems to be designed around sequencing fixed sounds, or am I missing something? reply packetlost 2 hours agoparentThere's almost no fixed sounds/sampling in it. It has that, but it's a relatively small part that doesn't usually get used in isolation. The sequencing is for notes and time-domain parameterization of the synthesizers. Regardless it's modular in the software/building block sense, which was what was being referred to in the description. reply smnc 2 hours agoparentprevThere are a bunch of modules, you can connect them in various modulation configurations. > Module is a basic element of the SunVox. There are several types of modules (...) https://www.warmplace.ru/soft/sunvox/manual.php#mod reply multjoy 3 hours agoparentprev [–] Modular, not modulator. reply bowsamic 3 hours agorootparent [–] But typically modular synthesis allows you to modulate almost any parameter reply TeddyDD 3 hours agorootparenthttps://www.warmplace.ru/soft/sunvox/manual.php#ctlauto It seems it's possible reply JodieBenitez 2 hours agorootparentIt's nice to have, but it's not audio-rate modulation. reply gldnspud 2 hours agorootparentYou can do audio-rate modulation (up to 32768 Hz) of any controller using Sound2Ctl: https://www.warmplace.ru/soft/sunvox/manual.php#sound2ctl reply JodieBenitez 3 hours agorootparentprev [–] Give Drambo and Bitwig a try. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "SunVox is a versatile modular synthesizer with a pattern-based sequencer, suitable for music composition across multiple platforms including Windows, macOS, Linux, iOS, Android, and Windows CE.",
      "Key features include optimized synth algorithms, flexible architecture, multitrack WAV export, MIDI support, real-time sample recording, and generative music capabilities.",
      "SunVox is free for most systems except Android and iOS, and developers can integrate the SunVox engine into their applications."
    ],
    "commentSummary": [
      "SunVox is a powerful modular synthesizer and digital audio workstation (DAW) that runs on multiple platforms, including tablets, and is highly recommended for music creation.",
      "A new beta version of SunVox was released earlier this month, generating renewed interest among users and developers.",
      "SunVox features a modular interface and \"MetaModules\" for creative audio synthesis and processing, and it is even Turing-complete, making it a versatile tool for both beginners and advanced users."
    ],
    "points": 100,
    "commentCount": 27,
    "retryCount": 0,
    "time": 1727528427
  },
  {
    "id": 41679471,
    "title": "MtCellEdit – Lightweight Spreadsheet Program",
    "originLink": "https://www.marktyler.org/ced/",
    "originBody": "Home About Links Screenshots About mtCellEdit is a lightweight spreadsheet program that I created from scratch. It is a small, no frills program which is designed to handle simple day to day spreadsheet tasks. I have put the most important facilities I want into a small customized program which means I can avoid the problems of larger programs such as slow operating speeds and overcomplicated user interfaces. The GUI is presented using Qt5. The core of the program is a shared C/C++ library which can be used by any C/C++ program to read, write and manipulate spreadsheets. The source code suite also contains other example programs that use this library, such as command line tools to allow scripts to access the API. The default file format mtCellEdit uses is portable and transparent as it is a ZIP file containing TSV text files. I deliberately designed it this way so that data can be extracted and read by any modern spreadsheet program on any operating system without requiring mtCellEdit. Over the years I have been frustrated by binary and XML formats that make data only accessible via a single program (or by me wasting time manually converting each file, or by me writing a file format conversion program). This format ensures that my data never becomes stranded, and can be manipulated by whichever tool I choose for a particular job (i.e. spreadsheet programs, text editors, command line tools, etc). System Requirements mtCellEdit is designed for GNU/Linux operating systems. Hardware wise it has been tested on x86_32, x86_64, and ARM_32 platforms. License The source code is distributed under the GNU General Public License version 3 or later. Read more about this here https://www.gnu.org/copyleft/gpl.html Links Documentation Downloads Screenshots Copyright (C) 2017-2024 Mark Tyler All Rights Reserved Privacy",
    "commentLink": "https://news.ycombinator.com/item?id=41679471",
    "commentBody": "MtCellEdit – Lightweight Spreadsheet Program (marktyler.org)95 points by Mr_Minderbinder 7 hours agohidepastfavorite34 comments ravetcofx 20 minutes agoFor something even more lightweight with GUI, Nebu, written in uxntal https://wiki.xxiivv.com/site/nebu.html reply soheilpro 2 hours agoprevIs there anything similar for macOS? reply bloopernova 5 hours agoprevhttps://web.archive.org/web/20240502195544/https://ced.markt... reply igtztorrero 4 hours agoprev\"I created from scratch\" love these words... 18 years of creativity, awesome. reply pdyc 35 minutes agoprevshameless plug, if you want to view csv with charts you can also give my free viewer a try https://newbeelearn.com/tools/csvonline reply leeoniya 24 minutes agoparentis it OSS? what do you use for parsing? reply pdyc 14 minutes agorootparentno. I am using papaparse, i am a fan of your tiny libs. reply leeoniya 13 minutes agorootparentnext [–]Are there low-hanging features you miss from that page? It doesn’t mention ‘formula’, and that is something that the original had in 1979 (http://www.bricklin.com/history/vcexecutable.htm) reply saulpw 1 hour agorootparentIt is unfortunately hidden under \"derivative columns\": https://www.visidata.org/docs/columns/ Use = with a Python expression. Not cell-based formula but column-based Python. reply kristopolous 4 hours agorootparentprevThis looks more like a database interface and less like a tabular calculator. Supposedly part of excels good design is it was always agnostic as to whether you're using it as a spreadsheet, poor man's database, or pretty table maker. reply anthk 2 hours agoparentprevsc-im it's a true spreadsheet and with gnuplot you can do plots and charts. reply Gys 4 hours agoprevA little off topic but I am looking for a lightweight word processor program. For MacOS. One that can edit docx documents with some minimal features. Most word processors like LibreOffice are very bloated. reply diegof79 3 hours agoparentYou should try Pages. The Apple office apps -Pages, Numbers, and Keynote- are often overlooked, but they can edit simple docx, xlsx, and pptx files. For example, Pages can also serve as a basic desktop publishing software as it supports page layouts with text box flow, and Numbers has the best spreadsheet UI that I’ve used (why any other spreadsheet software doesn’t allow to handle “floating” spreadsheets as Numbers? Once you use it is so evident and natural). reply nextos 1 hour agorootparentI second the Pages recommendation. Keynote is fantastic and Pages has much better typographic functionality than Word. I can immediately tell whether a document was produced using Word, or LibreOffice, because of poor ligatures and spacing. I think Numbers falls short of Excel, which is probably one of the best products Microsoft ever created. If you don't mind dealing with slightly antique UIs, both LyX and TeXmacs are WYSIWYG and can produce outstanding documents with little effort. reply Gys 3 hours agorootparentprev> but they can edit simple docx, xlsx, and pptx files. No, open yes (taking some time to do conversions) but not save, only export. reply tgv 2 hours agoparentprevI'm partial to Mellel. It's pretty snappy. It has a different approach to styles, though, which gives it a bit of a learning curve, but it feels very direct. https://www.mellel.com/ reply Brajeshwar 4 hours agoparentprevHave you tried Pages that come free with macOS? I find it to be pretty capable. reply pcl 4 hours agoparentprevHow do you measure weight? Apple’s Pages app works on macOS, can edit docx files, and has some minimal features. reply Gys 3 hours agorootparentI now use an application called WordTime and it opens instantly. Unfortunately is seems no longer supported and it has some quirks. Pages is really very slow in comparison to open files. Also Pages can also export to docx, not save. Another hurdle in working with docx files. reply phonon 3 hours agoparentprevYou can try WPS Office.... reply nnurmanov 3 hours agoprev [–] How does it store data? Is it some nosql database that stores cells? reply andrea76 3 hours agoparentIt's written in the website. Read the 3rd paragraph for all information you need. reply zie 3 hours agoparentprev [–] Seriously go read the link and answer your own question. It's literally there on the linked page. I mean if the linked page didn't include the information, sure asking is fine. reply pheatherlite 2 hours agorootparent [–] See the comment below yours answered in fewer words and provided more information. Berating people to read something is just noise. Improve the SNR reply layer8 2 hours agorootparentYou’re not improving the SNR. ;) reply tuwtuwtuwtuw 2 hours agorootparentprev [–] No. People asking about things which are answered by the linked page is noise and a waste of time - both for the person asking it and for the people reading the thread. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "mtCellEdit is a lightweight spreadsheet program designed for simple day-to-day tasks, avoiding the slow speeds and complex interfaces of larger programs.",
      "It uses Qt5 for the GUI and a shared C/C++ library for core functionalities, with source code including example programs and command line tools for API access.",
      "The default format is a ZIP file containing TSV text files, ensuring compatibility with modern spreadsheet programs, and it is designed for GNU/Linux on x86_32, x86_64, and ARM_32 platforms under the GNU General Public License version 3 or later."
    ],
    "commentSummary": [
      "MtCellEdit is a lightweight spreadsheet program, gaining attention for its simplicity and efficiency.",
      "Users are discussing alternatives and similar lightweight tools, such as Nebu and sc-im, highlighting a demand for minimalistic software solutions.",
      "The conversation also touches on lightweight word processors for macOS, with recommendations like Apple’s Pages and Mellel, indicating a broader interest in streamlined productivity tools."
    ],
    "points": 95,
    "commentCount": 34,
    "retryCount": 0,
    "time": 1727521856
  }
]
