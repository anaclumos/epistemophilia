[
  {
    "id": 41886256,
    "title": "The long road to lazy preemption in the Linux CPU scheduler",
    "originLink": "https://lwn.net/SubscriberLink/994322/45aa5211a50bc63a/",
    "originBody": "LWN .net News from the source Content Weekly Edition Archives Search Kernel Security Events calendar Unread comments LWN FAQ Write for us User: Password:| Subscribe / Log in / New account The long road to lazy preemption [LWN subscriber-only content] By Jonathan Corbet October 18, 2024 The kernel's CPU scheduler currently offers several preemption modes that implement a range of tradeoffs between system throughput and response time. Back in September 2023, a discussion on scheduling led to the concept of \"lazy preemption\", which could simplify scheduling in the kernel while providing better results. Things went quiet for a while, but lazy preemption has returned in the form of this patch series from Peter Zijlstra. While the concept appears to work well, there is still a fair amount of work to be done. Some review Current kernels have four different modes that regulate when one task can be preempted in favor of another. PREEMPT_NONE, the simplest mode, only allows preemption to happen when the running task has exhausted its time slice. PREEMPT_VOLUNTARY adds a large number of points within the kernel where preemption can happen if needed. PREEMPT_FULL allows preemption at almost any point except places in the kernel that prevent it, such as when a spinlock is held. Finally, PREEMPT_RT prioritizes preemption over most other things, even making most spinlock-holding code preemptible. A higher level of preemption enables the system to respond more quickly to events; whether an event is the movement of a mouse or an \"imminent meltdown\" signal from a nuclear reactor, faster response tends to be more gratifying. But a higher level of preemption can hurt the overall throughput of the system; workloads with a lot of long-running, CPU-intensive tasks tend to benefit from being disturbed as little as possible. More frequent preemption can also lead to higher lock contention. That is why the different modes exist; the optimal preemption mode will vary for different workloads. Stay on top of Linux kernel development with a one-month free trial subscription to LWN, no credit card required. Most distributions ship kernels built with the PREEMPT_DYNAMIC pseudo-mode, which allows any of the first three modes to be selected at boot time, with PREEMPT_VOLUNTARY being the default. On systems with debugfs mounted, the current mode can be read from /sys/kernel/debug/sched/preempt. PREEMPT_NONE and PREEMPT_VOLUNTARY do not allow the arbitrary preemption of code running in the kernel; there are times when that can lead to excessive latency even in systems where minimal latency is not prioritized. This problem is the result of places in the kernel where a large amount of work can be done; if that work is allowed to run unchecked, it can disrupt the scheduling of the system as a whole. To get around this problem, long-running loops have been sprinkled with calls to cond_resched(), each of which is an additional voluntary preemption point that is active even in the PREEMPT_NONE mode. There are hundreds of these calls in the kernel. There are some problems with this approach. cond_resched() is a form of heuristic that only works in the places where a developer has thought to put it. Some calls are surely unnecessary, while there will be other places in the kernel that could benefit from cond_resched() calls, but do not have them. The use of cond_resched(), at its core, takes a decision that should be confined to the scheduling code and spreads it throughout the kernel. It is, in short, a bit of a hack that mostly works, but which could be done better. Doing better The tracking of whether a given task can be preempted at any moment is a complicated affair that must take into account several variables; see this article and this article for details. One of those variables is a simple flag, TIF_NEED_RESCHED, that indicates the presence of a higher-priority task that is waiting for access to the CPU. Events such as waking a high-priority task can cause that flag to be set in whatever task is currently running. In the absence of this flag, there is no need for the kernel to consider preempting the current task. There are various points where the kernel can notice that flag and cause the currently running task to be preempted. The scheduler's timer tick is one example; any time a task returns to user space from a system call is another. The completion of an interrupt handler is yet another, but that check, which can cause preemption to happen any time that interrupts are enabled, is only enabled in PREEMPT_FULL kernels. A call to cond_resched() will also check that flag and, if it is set, call into the scheduler to yield the CPU to the other task. The lazy-preemption patches are simple at their core; they add another flag, TIF_NEED_RESCHED_LAZY, that indicates a need for rescheduling at some point, but not necessarily right away. In the lazy preemption mode (PREEMPT_LAZY), most events will set the new flag rather than TIF_NEED_RESCHED. At points like the return to user space from the kernel, either flag will lead to a call into the scheduler. At the voluntary preemption points and in the return-from interrupt path, though, only TIF_NEED_RESCHED is checked. The result of this change is that, in lazy-preemption mode, most events in the kernel will not cause the current task to be preempted. That task should be preempted eventually, though. To make that happen, the kernel's timer-tick handler will check whether TIF_NEED_RESCHED_LAZY is set; if so, TIF_NEED_RESCHED will also be set, possibly causing the running task to be preempted. Tasks will generally end up running for something close to their full time slice unless they give up the CPU voluntarily, which should lead to good throughput. With these changes, the lazy-preemption mode can, like PREEMPT_FULL, run with kernel preemption enabled at (almost) all times. Preemption can happen any time that the preemption counter says that it should. That allows long-running kernel code to be preempted whenever other conditions do not prevent it. It also allows preemption to happen quickly in those cases where it is truly needed. For example, should a realtime task become runnable, as the result of handling an interrupt, for example, the TIF_NEED_RESCHED flag will be set, leading to an almost immediate preemption. There will be no need to wait for the timer tick in such cases. Preemption will not happen, though, if only TIF_NEED_RESCHED_LAZY is set, which will be the case much of the time. So a PREEMPT_LAZY kernel will be far less likely to preempt a running task than a PREEMPT_FULL kernel. Removing cond_resched() — eventually The end goal of this work is to have a scheduler with only two non-realtime modes: PREEMPT_LAZY and PREEMPT_FULL. The lazy mode will occupy a place between PREEMPT_NONE and PREEMPT_VOLUNTARY, replacing both of them. It will, however, not need the voluntary preemption points that were added for the two modes it replaces. Since preemption can now happen almost anywhere, there is no longer a need to enable it in specific spots. For now, though, the cond_resched() calls remain; if nothing else, they are required for as long as the PREEMPT_NONE and PREEMPT_VOLUNTARY modes exist. Those calls also help to ensure that problems are not introduced while lazy preemption is being stabilized. In the current patch set, cond_resched() only checks TIF_NEED_RESCHED, meaning that preemption will be deferred in many situations where it will happen immediately from cond_resched() in PREEMPT_VOLUNTARY or PREEMPT_NONE mode. Steve Rostedt questioned this change, asking whether cond_resched() should retain its older meaning, at least for the PREEMPT_VOLUNTARY case. Even though PREEMPT_VOLUNTARY is slated for eventual removal, he thought, keeping the older behavior could help to ease the transition. Thomas Gleixner answered that only checking TIF_NEED_RESCHED is the correct choice, since it will help in the process of removing the cond_resched() calls entirely: That forces us to look at all of them and figure out whether they need to be extended to include the lazy bit or not. Those which do not need it can be eliminated when LAZY is in effect because that will preempt on the next possible preemption point once the non-lazy bit is set in the tick. He added that he expects \"less than 5%\" of the cond_resched() calls need to check TIF_NEED_RESCHED_LAZY and, thus, will need to remain even after the transition to PREEMPT_LAZY is complete. Before then, though, there are hundreds of cond_resched() calls that need to be checked and, for most of them at least, removed. Many other details have to be dealt with as well; this patch set from Ankur Arora addresses a few of them. There is also, of course, the need for extensive performance testing; Mike Galbraith has made an early start on that work, showing that throughput with lazy preemption falls just short of that with PREEMPT_VOLUNTARY. It all adds up to a lot to be done still, but the end result of the lazy-preemption work should be a kernel that is a bit smaller and simpler while delivering predictable latencies without the need to sprinkle scheduler-related calls throughout the code. That seems like a better solution, but getting there is going to take some time. Index entries for this article Kernel Preemption Kernel Scheduler to post comments Copyright © 2024, Eklektix, Inc. Comments and public postings are copyrighted by their creators. Linux is a registered trademark of Linus Torvalds",
    "commentLink": "https://news.ycombinator.com/item?id=41886256",
    "commentBody": "The long road to lazy preemption in the Linux CPU scheduler (lwn.net)182 points by chmaynard 11 hours agohidepastfavorite47 comments Hendrikto 7 hours ago> It all adds up to a lot to be done still, but the end result of the lazy-preemption work should be a kernel that is a bit smaller and simpler while delivering predictable latencies without the need to sprinkle scheduler-related calls throughout the code. That seems like a better solution, but getting there is going to take some time. Sounds promising. Just like EEVDF, this both simplifies and improves the status quo. Does not get better than that. reply amelius 8 hours agoprev> A higher level of preemption enables the system to respond more quickly to events; whether an event is the movement of a mouse or an \"imminent meltdown\" signal from a nuclear reactor, faster response tends to be more gratifying. But a higher level of preemption can hurt the overall throughput of the system; workloads with a lot of long-running, CPU-intensive tasks tend to benefit from being disturbed as little as possible. More frequent preemption can also lead to higher lock contention. That is why the different modes exist; the optimal preemption mode will vary for different workloads. Why isn't the level of preemption a property of the specific event, rather than of some global mode? Some events need to be handled with less latency than others. reply jabl 11 minutes agoparentIt's sort of what this patch does, from https://lwn.net/ml/all/20241008144829.GG14587@noisy.programm... : > SCHED_IDLE, SCHED_BATCH and SCHED_NORMAL/OTHER get the lazy thing, FIFO, RR and DEADLINE get the traditional Full behaviour. reply btilly 7 hours agoparentprevYou need CPU time to evaluate the priority of the event. This can't happen until after you've interrupted whatever process is currently on the CPU. And so the highest possible priority an event can happen is limited by how short a time slice a program gets before it has to go through a context switch. To stand ready to reliably respond to any one kind of event with low latency, every CPU intensive program must suffer a performance penalty all the time. And this is true no matter how rare those events may be. reply zeusk 6 hours agorootparentThat is not true of quite a few multi-core systems. A lot of them, especially those that really care about performance will strap all interrupts to core 0 and only interrupt other cores via IPI when necessary. reply xedrac 3 hours agorootparentI learned this when I pegged core 0 with an intensive process on a little quad core arm device, and all of my interrupts started behaving erratically. reply btilly 35 minutes agorootparentprevThis strategy minimizes the impact by making one core less necessary. But it does not eliminate it. reply Someone 6 hours agorootparentprev> You need CPU time to evaluate the priority of the event. Not necessarily. The CPU can do it in hardware. As a simple example, the 6502 had separate “interrupt request” (IRQ) and “non-maskable interrupts (NMI) pins, supporting two interrupt levels. The former could be disabled; the latter could not. A programmable interrupt controller (https://en.wikipedia.org/wiki/Programmable_interrupt_control...) also could ‘know’ that it need not immediately handle some interrupts. reply themulticaster 5 hours agorootparentThe user you replied to likely means something different: The priority of the event often depends on the exact contents on the event and not the hardware event source. For example, say you receive a \"read request completed\" interrupt from a storage device. The kernel now needs to pass on the data to the process which originally requested it. In order to know how urgent the original request and thus the handling of the interrupt is, the kernel needs to check which sector was read and associate it with a process. Merely knowing that it came from a specific storage device is not sufficient. By the way, NMI still exist on x86 to this day, but AFAIK they're only used for serious machine-level issues and watchdog timeouts. reply wizzwizz4 3 hours agorootparentThis, too, can be done in hardware (if nothing else, with a small coprocessor). reply refulgentis 3 hours agorootparentThis doesn't shed light Generally, any given software can be done in hardware. Specifically, we could attach small custom coprocessors to everything for the Linux kernel, and Linux could require them to do any sort of multitasking. In practice, software allows us to customize these things and upgrade them and change them without tightly coupling us to a specific kernel and hardware design. reply btilly 5 minutes agorootparentExactly the point. We can compile any piece of software that we want into hardware, but after that it is easier to change in software. Given the variety of unexpected ways in which hardware is used, in practice we went up moving some of what we expected to do in hardware, back into software. This doesn't mean that moving logic into hardware can't be a win. It often is. But we should also expect that what has tended to wind up in software, will continue to do so in the future. And that includes complex decisions about the priority of interrupts. wizzwizz4 2 hours agorootparentprevWe already have specialised hardware for register mapping (which could be done in software, by the compiler, but generally isn't) and resolving instruction dependency graphs (which again, could be done by a compiler). Mapping interrupts to a hardware priority level feels like the same sort of task, to me. reply sroussey 0 minutes agorootparent> We already have specialised hardware for register mapping (which could be done in software, by the compiler, but generally isn't) Wait, what? I’ve been out of compiler design for a couple decades, but that definitely used to be a thing. amluto 6 hours agorootparentprevLinux runs actual C code when an event occurs — this is how it queues up a wake up of the target task and optionally triggers preemption. reply RandomThoughts3 6 hours agoparentprev> Why isn't the level of preemption a property of the specific event, rather than of some global mode? There are two different notions which are easy to get confused about here: when a process can be preempted and when a process will actually be preempted. Potential preemption point is a property of the scheduler and is what is being discussed with the global mode here. More preemption points mean more chances for processes to be preempted at inconvenient time obviously but it also means more chances to properly prioritise. What you call level of preemption, which is to say priority given by the scheduler, absolutely is a property of the process and can definitely be set. The Linux default scheduler will indeed do its best to allocate more time slices and preempt less processes which have priority. reply biorach 7 hours agoparentprevArguably PREEMPT_VOLUNTARY, as described in the article is an attempt in this direction which is being deprecated. reply acters 8 hours agoparentprevMostly because such a system would install in fighting among programs that all will want to be prioritized as important. tbf it will mostly be larger companies who will take advantage of it for \"better\" user experience. Which is kind of important to either reduce to a minimal amount of running applications or simply control it manually for the short burst most users will experience. If anything cpu intensive tasks are more likely to be bad code than some really effective use of resources. Though when it comes to gaming, there is a delicate balance as game performance should be prioritized but not be allowed to cause the system to lock up for multitasking purposes. Either way, considering this is mostly for idle tasks. It has little importance to allow it to be automated beyond giving users a simple command for scripting purposes that users can use for toggling various behaviors. reply biorach 8 hours agorootparentYou're talking about user-space preemption. The person you're replying to, and the article are about kernel preemption. reply withinboredom 7 hours agorootparentGames run in a tight loop, they don’t (typically) yield execution. If you don’t have preemption, a game will use 100% of all the resources all the time, if given the chance. reply vvanders 58 minutes agorootparentGames absolutely yield, even if the rendering thread tries to go 100% you'll likely still be sleeping in the GPU driver as it waits for back buffers to free up. Even for non-rendering systems those still usually run at game tick-rates since running those full-tilt can starve adjacent cores depending on false sharing, cache misses, bus bandwidth limits and the like. I can't think of a single title I worked on that did what you describe, embedded stuff for sure but that's a whole different class that is likely not even running a kernel. reply ahoka 6 hours agorootparentprevMaybe on DOS. Doing any kind of IO usually implies “yielding”, which most interactive programs do very often. Exhausting its quantum without any IO decreases the task’s priority in a classic multilevel feedback queue scheduler, but that’s not typical for programs. reply Tomte 6 hours agorootparentprevGames run in user space. They don't have to yield (that's cooperative multitasking), they are preempted by the kernel. And don't have a say about it. reply harry8 6 hours agorootparentMake a syscall for io. Now the kernel takes over and runs whatever it likes for as long as it likes. Do no syscalls. Timer tick. Kernel takes over and does whatever as well. No_HZ_FULL, isolated cpu cores, interrupts on some other core and you can spin using 100% cpu forever on a core. Do games do anything like this? reply biorach 5 hours agorootparentPinning on a core like this is done in areas like HPC and HFT. In general you want a good assurance that your hardware matches your expectations and some kernel tuning. I haven't heard of it being done with PC games. I doubt the environment would be predictable enough. On consoles tho..? reply vvanders 55 minutes agorootparentWe absolutely pinned on consoles, anywhere where you have fixed known hardware tuning for that specific hardware usually nets you some decent benefits. From what I recall we mostly did it for predictability so that things that may go long wouldn't interrupt deadline sensitive things(audio, physics, etc). reply biorach 40 minutes agorootparentNice, thank you reply chainingsolid 4 hours agorootparentprevThinking about it the threads in a game that normally need more CPU time are the ones that are doing lots of sys calls. You'd have to use a fair bit of async and atomics, to split the work into compute and chatting with the kernal. Might as well figure out how to do it 'right' and use 2+ threads so it can scale. Side note the compute heavy low sys call freqency stuff like terrain gen belongs in the pool of back ground threads, normaly. reply acters 7 hours agorootparentprevYeah you are right, however some of what I said does have some merit as there are plenty of things I talked about that apply to why you would need dynamic preemption. However, the other person who mentioned the issue with needing to take cpu cycles on the dynamic system that checks and might apply a new preemptive config is more overhead. The kernel can't always know how long the tasks will take so it is possible that the overhead for dynamically changing for new tasks that have short runtime will be worse than just preemptively setting the preemptive configuration. But yeah thanks for making that distinction. Forgot to touch on the differences reply ajross 3 hours agoparentprev> Why isn't the level of preemption a property of the specific event, rather than of some global mode? Some events need to be handled with less latency than others. How do you know which thread is needed to \"handle\" this particular \"event\" though? I mean, maybe you're about to start a high priority video with low latency requirements[1]. And due to a design mess your video player needs to contact some random auth server to get a DRM cookie for the stream. How does the KERNEL know that the auth server is on the critical path for the backup camera? That's a human-space design issue, not a scheduler algorithm. [1] A backup camera in a vehicle, say. reply AtlasBarfed 5 hours agoparentprevAren't we in the massively multi-core era? I guess it's nice to keep Linux relevant to older single CPU architectures, especially with regards to embedded systems. But if Linux is going to be targeted towards modern cpu architectures primarily, accidentally basically assume that there is a a single CPU available to evaluate priority and leave the CPU intensive task bound to other cores? I mean this has to be what high low is for, outside of mobile efficiency. reply kbolino 5 hours agorootparentAs I understand it, kernel preemption of a user thread happens (and has to happen) on the core that's running the thread. The kernel is not a separate process, but rather a part of every process. What you're describing sounds more like a hypervisor than a kernel. That distinction isn't purely semantic; the two operate at different security levels on the CPU and use different instructions and mechanisms to do their respective jobs. edit: That having been said, I may be misinterpreting what you described; there's a comment in another thread by @zeusk which says to me that more or less this (single core used/reserved for making priority decisions) is already the case on many multi-core systems anyway, thanks to IPI (inter-processor interrupts). So, presumably, the prioritization core handles the preemption interrupts, then runs decision logic on what threads actually need to be preempted, and sends those decisions out to the respective core(s) using IPI, which causes the kernel code on those cores to unconditionally preempt the running thread. However, I'd wonder still about the risk of memory barriers or locks starving out the kernel scheduler in this kind of architecture. Maybe the CPU can arbitrate the priority for these in hardware? Or maybe the kernel scheduler always runs for a small portion of every time slice, but only takes action if an interrupt handler has set a flag? reply yndoendo 26 minutes agorootparentprevMassive multi-core is subjective. Most industrial computers are still low core. You are more likely to find dual or quad core in these environments. Multi-core costs more money and increases the cost of automation. Look at Advantech computers specifications for this economic area. Software PLCs will bind to a core which is not exposed to the OS environment and will show a dual core is a single or a quad core as a tri core. reply weinzierl 9 hours agoprev\"Current kernels have four different modes that regulate when one task can be preempted in favor of another\" Is this about kernel tasks, user tasks or both? reply GrayShade 9 hours agoparentKernel code, user-space code is always preemptible. reply fguerraz 5 hours agorootparentNot true when the user-space thread has RT priority. reply temac 5 hours agorootparentRT threads can be prempted by higher prio RT, and IIRC some kernel threads run at the highest prio. Plus you can be prempted by SMI, an hypervisor, etc reply simfoo 8 hours agoprevCan't find any numbers in the linked thread with the patches. Surely some preliminary benchmarking must have been performed that could tell us something about the real world potential of the change? reply biorach 7 hours agoparentFrom the article, second last paragraph: > There is also, of course, the need for extensive performance testing; Mike Galbraith has made an early start on that work, showing that throughput with lazy preemption falls just short of that with PREEMPT_VOLUNTARY. reply spockz 7 hours agoparentprevHow would you benchmark something like this? Run multiple processes concurrently and then sort by total run time? Or measure individual process wait time? reply hifromwork 5 hours agorootparentI guess both make sense, and a lot of other things (synthetical benchmarks, microbenchmarks, real-world benchmarks, best/average/worst case latency comparison, best/average/worst case throughput comparison...) reply hamilyon2 6 hours agoprevHow tight is scheduler coupled to the rest of kernel code? If one wanted to drastically simplify scheduler, for example for some scientific application which doesn't care about preemption at all, can it be done in clean, modular way? And will be any benefit? reply p_l 5 hours agoparentIf you want to run a set of processes with as little preemption as possible, for example in HPC setting, your most powerful option is to reboot the system with a selection of cores (exact amount will differ on your needs) set as isolated cpus and manually put your task there with taskset - but then you need to really manually allocate tasks to CPUs, it's trivial to end up with all tasks on wrong CPU. The standard way is to set interrupt masks so they don't go to \"work\" cpus and use cpusets to only allow specific cgroup to execute on given cpuset. reply toast0 4 hours agoparentprevYou can get 95% of the way there by running a clean system with nearly no daemons, and your application setup to run with one os thread per cpu thread, with cpu pinning so they don't move. Whatever the scheduler does should be pretty low impact, because the runlist will be very short. If your application doesn't do much I/O, you won't get many interrupts either. If you can run a tickless kernel (is that still a thing, or is it normal now?), you might not get any interrupts for large periods. reply marcosdumay 3 hours agoparentprevLast time I looked, it was surprisingly decoupled. But the reason for drastically simplifying it would be to avoid bugs, there isn't much performance to gain compared to a well-set default one (there are plenty of settings tough). And there haven't been many bugs there. On most naive simplifications you will lose performance, not gain it. If you are running a non-interactive system, the easiest change to make is to increase the size of the process time quantum. reply kevin_thibedeau 4 hours agoparentprevI'd just use RT Linux. That has its own basic scheduler with the kernel scheduler running as the idle task. Real time tasks get priority over everything else. reply AvaSayes 5 hours agoprev [–] Yeah, it’d be cool if preemption could adapt based on the event, but managing that for all events might mess with system stability. It’s like using tools like Tomba Finder for lead gen you gotta balance precision (targeted leads) with efficiency so everything runs smoothly overall. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Linux kernel's CPU scheduler is introducing \"lazy preemption,\" a new concept designed to simplify scheduling and improve system performance.",
      "Lazy preemption aims to replace existing modes PREEMPT_NONE and PREEMPT_VOLUNTARY with PREEMPT_LAZY, using a new flag, TIF_NEED_RESCHED_LAZY, to allow tasks to run longer unless immediate preemption is necessary.",
      "This change seeks to make the kernel smaller and simpler, with more predictable latencies, though it requires extensive testing and adjustments before full implementation."
    ],
    "commentSummary": [
      "The Linux CPU scheduler is exploring lazy preemption to simplify the kernel and achieve more predictable latencies, similar to the EEVDF (Earliest Eligible Virtual Deadline First) model.",
      "Preemption, which allows systems to quickly respond to events, can negatively impact overall throughput and increase lock contention, necessitating a balance between different preemption modes for various workloads.",
      "Initial testing indicates that lazy preemption slightly reduces throughput compared to the existing PREEMPT_VOLUNTARY mode, highlighting the complexity of managing event priority and preemption levels in the kernel."
    ],
    "points": 182,
    "commentCount": 47,
    "retryCount": 0,
    "time": 1729322996
  },
  {
    "id": 41884740,
    "title": "US probes Tesla's Full Self-Driving software in 2.4M cars after fatal crash",
    "originLink": "https://www.reuters.com/business/autos-transportation/nhtsa-opens-probe-into-24-mln-tesla-vehicles-over-full-self-driving-collisions-2024-10-18/",
    "originBody": "reuters.com#cmsg{animation: A 1.5s;}@keyframes A{0%{opacity:0;}99%{opacity:0;}100%{opacity:1;}}Please enable JS and disable any ad blockervar dd={'rt':'c','cid':'AHrlqAAAAAMAmQqtropq9N8ArLdNww==','hsh':'2013457ADA70C67D6A4123E0A76873','t':'bv','s':46743,'e':'47cc662cc2929cd47469dd3c7d92e0016a0037e54e24751e5484a534c22e1980','host':'geo.captcha-delivery.com','cookie':'fqImnFMQVg9pnIu6_PVr5FO8AMG~P0CoZ5RVsuuVfYpUvW0kSjsgMhRPW98oY2r3_4JjqLoCcK~aXTCjpJZ_ecWyIwrNizYASg0MuVCPRLdKbGLT4M8inPmYPKXxEq_J'}",
    "commentLink": "https://news.ycombinator.com/item?id=41884740",
    "commentBody": "US probes Tesla's Full Self-Driving software in 2.4M cars after fatal crash (reuters.com)139 points by elsewhen 18 hours agohidepastfavorite7 comments ChrisArchitect 17 hours ago [–] Earlier: https://news.ycombinator.com/item?id=41880649 reply dang 59 minutes agoparentComments moved thither. Thanks! reply metadat 16 hours agoparentprev [–] (flagged, dead, 22 comments) reply ra7 16 hours agorootparent [–] This one will be too. Some Tesla supporters here have admitted to flagging negative stories: https://news.ycombinator.com/item?id=40217615 reply metadat 16 hours agorootparentThen please send an email to: hn@ycombintor.com. I believe the standard is high and also reasonable, please give it a shot. reply bbor 16 hours agorootparentprev [–] :( I know a lot of us would like to see more features on here, but at the very least I’d like to see them adopt a more mature stance to moderation and content approval sometime soon. This doesn’t have to be a goofy “meh it’s how I was feeling” style website anymore, especially with y-combinator unquestionably entering a new era. Maybe we move slowly and break a few less things? TBF if I had to pick any two benevolent dictators these two are pretty damn great, but still… it’s a matter of structure and consistency, not individual discretion. reply rblatz 15 hours agorootparent [–] Where are you going that has better moderation and standards? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "The United States is conducting an investigation into Tesla's Full Self-Driving (FSD) software, which is installed in 2.4 million vehicles, due to a fatal accident.",
      "This investigation highlights ongoing concerns about the safety and reliability of autonomous driving technologies.",
      "The scrutiny of Tesla's FSD software could have significant implications for the future of self-driving car regulations and development."
    ],
    "points": 139,
    "commentCount": 7,
    "retryCount": 0,
    "time": 1729298775
  },
  {
    "id": 41882955,
    "title": "Express v5",
    "originLink": "https://expressjs.com/2024/10/15/v5-release.html",
    "originBody": "Express Home Getting started Guide API reference Advanced topics Resources Support Blog Posts Introducing Express v5: A New Era for Node.js Framework Express Never Ending Support Launched by HeroDevs and Express.js September 2024 Security Releases Welcome to The Express Blog! Introducing Express v5: A New Era for Node.js Framework By Wes Todd and the Express Technical Committee 15 Oct 2024 Ten years ago (July 2014) the Express v5 release pull request was opened, and now at long last it’s been merged and published! We want to recognize the work of all our contributors, especially Doug Wilson, who spent the last ten years ensuring Express was the most stable project around. Without his contributions and those of many others, this release could not have happened. Eight months ago we went public with a plan to move Express forward. This plan included re-committing to the governance outlined years ago and adding more contributors to help kickstart progress. Many people may not realize that robust project governance is critical to the health of a large open-source project. We want to thank the OpenJS Foundation Cross Project Council and its members for helping us put together this plan. So what about v5? This release is designed to be boring! That may sound odd, but we’ve intentionally kept it simple to unblock the ecosystem and enable more impactful changes in future releases. This is also about signaling to the Node.js ecosystem that Express is moving again. The focus of this release is on dropping old Node.js version support, addressing security concerns, and simplifying maintenance. Before going into the changes in this release, let’s address why it was released v5 on the next dist-tag. As part of reviving the project, we started a Security working group and security triage team to address the growing needs around open source supply chain security. We undertook a security audit (more details to come on that) and uncovered some problems that needed to be addressed. Thus, in addition to the “normal” work done in public issues, we also did a lot of security work in private forks. This security work required orchestration when releasing, to ensure the code and CVE reports went out together. You can find a summary of the most recent vulnerabilities patched in our security release notes. While we weren’t able to simultaneously release v5, this blog post, the changelog, and documentation, we felt it was most important to have a secure and stable release. As soon as possible, we’ll provide more details on our long-term support (LTS) plans, including when the release will move from next to latest. For now, if you are uncomfortable being on the bleeding edge (even if it is a rather dull edge) then you should wait to upgrade until the release is tagged latest. That said, we look forward to working with you to address any bugs you encounter as you upgrade. Breaking changes The v5 release has the minimum possible number of breaking changes, listed here in order of impact to applications. Ending support for old Node.js versions Changes to path matching and regular expressions Promise support Body parser changes Removing deprecated method signatures There are also a number of subtle changes: for details, see Migrating to Express 5. Ending support for old Node.js versions Goodbye Node.js 0.10, hello Node 18 and up! This release drops support for Node.js versions before v18. This is an important change because supporting old Node.js versions has been holding back many critical performance and maintainability changes. This change also enables more stable and maintainable continuous integration (CI), adopting new language and runtime features, and dropping dependencies that are no longer required. We recognize that this might cause difficulty for some enterprises with older or “parked” applications, and because of this we are working on a partnership with HeroDevs to offer “never-ending support” that will include critical security patches even after v4 enters end-of-life (more on these plans soon). That said, we strongly suggest that you update to modern Node.js versions as soon as possible. Changes to path matching and regular expressions The v5 releases updates to path-to-regexp@8.x from path-to-regexp@0.x, which incorporates many years of changes. If you were using any of the 5.0.0-beta releases, a last-minute update which greatly changed the path semantics to remove the possibility of any ReDoS attacks. For more detailed changes, see the path-to-regexp readme. No more regex This release no longer supports “sub-expression” regular expressions, for example /:foo(\\\\d+). This is a commonly-used pattern, but we removed it for security reasons. Unfortunately, it’s easy to write a regular expression that has exponential time behavior when parsing input: The dreaded regular expression denial of service (ReDoS) attack. It’s very difficult to prevent this, but as a library that converts strings to regular expressions, we are on the hook for such security aspects. How to migrate: The best approach to prevent ReDoS attacks is to use a robust input validation library. There are many on npm depending on your needs. TC member Wes Todd maintains a middleware-based “code first” OpenAPI library for this kind of thing. Splats, optional, and captures oh my This release includes simplified patterns for common route patterns. With the removal of regular expression semantics comes other small but impactful changes to how you write your routes. :name? becomes {:name}. Usage of {} for optional parts of your route means you can now do things like /base{/:optional}/:required and what parts are actually optional is much more explicit. * becomes *name. New reserved characters: (, ), [, ], ?, +, & !. These have been reserved to leave room for future improvements and to prevent mistakes when migrating where those characters mean specific things in previous versions. Name everything This release no longer supports ordered numerical parameters. In Express v4, you could get numerical parameters using regex capture groups (for example, /user(s?) => req.params[0] === 's'). Now all parameters must be named. Along with requiring a name, Express now supports all valid JavaScript identifiers or quoted (for example, /:\"this\"). Promise support This one may be a bit contentious, but we “promise” we’re moving in the right direction. We added support for returned rejected promises from errors raised in middleware. This does not include calling next from returned resolved promises. There are a lot of edge cases in old Express apps that have expectations of Promise behavior, and before we can run we need to walk. For most folks, this means you can now write middleware like the following: app.use(async (req, res, next) => { req.locals.user = await getUser(req); next(); }); Notice that this example uses async/await and the getUser call may throw an error (if, for example, the user doesn’t exist, the user database is down, and so on), but we still call next if it is successful. We don’t need to catch the error in line anymore if we want to rely on error-handling middleware instead because the router will now catch the rejected promise and treat that as calling next(err). NOTE: Best practice is to handle errors as close to the site as possible. So while this is now handled in the router, it’s best to catch the error in the middleware and handle it without relying on separate error-handling middleware. Body parser changes There are a number of body-parser changes: Add option to customize the urlencoded body depth with a default value of 32 as mitigation for CVE-2024-45590 (technical details) Remove deprecated bodyParser() combination middleware req.body is no longer always initialized to {} urlencoded parser now defaults extended to false Added support for Brotli lossless data compression Removing deprecated method signatures Express v5 removes a number of deprecated method signatures, many of which were carried over from v3. Below are the changes you need to make: res.redirect('back') and res.location('back'): The magic string 'back' is no longer supported. Use req.get('Referrer') || '/' explicitly instead. res.send(status, body) and res.send(body, status) signatures: Use res.status(status).send(body). res.send(status) signature: Use res.sendStatus(status) for simple status responses, or res.status(status).send() for sending a status code with an optional body. res.redirect(url, status) signature: Use res.redirect(status, url). res.json(status, obj) and res.json(obj, status) signatures: Use res.status(status).json(obj). res.jsonp(status, obj) and res.jsonp(obj, status) signatures: Use res.status(status).jsonp(obj). app.param(fn): This method has been deprecated. Instead, access parameters directly via req.params, or use req.body or req.query as needed. app.del('/', () => {}) method: Use app.delete('/', () => {}) instead. req.acceptsCharset: Use req.acceptsCharsets (plural). req.acceptsEncoding: Use req.acceptsEncodings (plural). req.acceptsLanguage: Use req.acceptsLanguages (plural). res.sendfile method: Use res.sendFile instead. As a framework, we aim to ensure that the API is as consistent as possible. We’ve removed these deprecated signatures to make the API more predictable and easier to use. By streamlining each method to use a single, consistent signature, we simplify the developer experience and reduce confusion. Migration and security guidance For developers looking to migrate from v4 to v5, there’s a detailed migration guide to help you navigate through the changes and ensure a smooth upgrade process. Additionally, we’ve been working hard on a comprehensive Threat Model that helps illustrate our philosophy of a “Fast, unopinionated, minimalist web framework for Node.js.” It provides critical insights into areas like user input validation and security practices that are essential for safe and secure usage of Express in your applications. Our work is just starting We see the v5 release as a milestone toward an Express ecosystem that’s a stable and reliable tool for companies, governments, educators, and hobby projects. It is our commitment as the new stewards of the Express project to move the ecosystem forward with this goal in mind. If you want to support this work, which we do on a volunteer basis, please consider supporting the project and its maintainers via our sponsorship opportunities. We have an extensive working backlog of tasks, PRs, and issues for Express and dependencies. Naturally, we expect developers will continue to report issues to add to this backlog and open PRs moving forward, and we’ll continue to collaborate with the community to triage and resolve them. We look forward to continuing to improve Express and making it useful for its users across the world. Documentation translations provided by StrongLoop/IBM: French, German, Spanish, Italian, Japanese, Russian, Chinese, Traditional Chinese, Korean, Portuguese. Community translation available for: Slovak, Ukrainian, Uzbek, Turkish, Thai and Indonesian. Terms of Use Privacy Policy Code of Conduct Trademark Policy Security Policy License",
    "commentLink": "https://news.ycombinator.com/item?id=41882955",
    "commentBody": "Express v5 (expressjs.com)137 points by saikatsg 22 hours agohidepastfavorite37 comments dylanlacom 21 hours agoI just want to express my gratitude to Wes and the team of people who worked on this. I had to go back and read it twice that it's been 10 years since the PR for v5 was opened. That's wild! I can only imagine the immense amount of work it must have been to change the inertia of this project. Cheers to a new chapter! reply cbarrick 12 hours agoprev> Unfortunately, it’s easy to write a regular expression that has exponential time behavior when parsing input This footgun can be easily avoided without ripping out regex support altogether. Just switch to a regex engine that is actually regular, like re2. See also \"Regular Expression Matching Can Be Simple And Fast\" by rsc [1]. [1]: https://swtch.com/~rsc/regexp/regexp1.html reply afavour 18 hours agoprevCongrats to the Express team! I know there are many other Node JS web server frameworks out there but I've never really moved beyond Express because it does everything I need. Long may that continue. reply tlhunter 21 hours agoprevCuriously, it still isn't `latest`: https://www.npmjs.com/package/express?activeTab=versions reply stefanos82 21 hours agoparentThey have explained their reasoning in https://expressjs.com/2024/10/15/v5-release.html > Before going into the changes in this release, let’s address why it was released v5 on the next dist-tag. As part of reviving the project, we started a Security working group and security triage team to address the growing needs around open source supply chain security. We undertook a security audit (more details to come on that) and uncovered some problems that needed to be addressed. Thus, in addition to the “normal” work done in public issues, we also did a lot of security work in private forks. This security work required orchestration when releasing, to ensure the code and CVE reports went out together. You can find a summary of the most recent vulnerabilities patched in our security release notes. > > While we weren’t able to simultaneously release v5, this blog post, the changelog, and documentation, we felt it was most important to have a secure and stable release. > > As soon as possible, we’ll provide more details on our long-term support (LTS) plans, including when the release will move from next to latest. For now, if you are uncomfortable being on the bleeding edge (even if it is a rather dull edge) then you should wait to upgrade until the release is tagged latest. That said, we look forward to working with you to address any bugs you encounter as you upgrade. reply mceachen 17 hours agoparentprevI couldn’t upgrade PhotoStructure to v5.0.0 due to a showstopping error thrown deep in the new router (for a valid path spec). I haven’t tried the new patch release yet (nor even had time to properly trace where the issue in express was to file a reasonable issue). Kudos to the team for pushing express forward! reply pfraze 22 hours agoprevAppreciate your work on this, Wes & crew. Express is still my go to for nodejs, and it's really good to have this active governance. reply _fat_santa 20 hours agoprevWhy did it take 10 years to release v5? reply simple10 20 hours agoparentMy guess is Express has been stable for a long time with no real need to evolve it. And old versions of node have effectively expired, so they can now drop support. Maybe there are more API changes planned for v6 and v5 is the stepping stone? Many thanks to the Express devs! It's been a reliable part of the stack for a long time. For personal projects, I've been loving https://hono.dev/ The DX is fantastic and it runs in bun and CloudFlare workers. Shoutout to the hono developers! For larger team projects, I end up using Fastify and NextJS. No real reason other than it's what's already running or other devs on the projects prefer it. But Express is always a great option. reply yas_hmaheshwari 13 hours agoparentprev[ I have used express in my last company, and I am asking this as a user who had used it and found it to be really pleasant to use ] I am really confused by this. Is it really that stable ? For any software that was released this back, I would have thought its abandoned. Are there no features that the community had demanded in this time reply alpinisme 5 hours agorootparentStable doesn’t mean “no room for growth.” It means, “you can count on it to work (for the features it supports) and keep working the way it does.” Absolutely the worst part of the JavaScript ecosystem is the churn. reply throw4950sh06 5 hours agorootparentBut what churn? I started using Webpack, Express, TypeScript and React around 2013/2014. I still use the exact same stack. The only difference is that my config files are simpler now. React switched to functions/hooks but class components still work; all of my code from the beginning runs just fine and the professional projects I worked on continue to be developed. I switched from classic .NET Framework because they deprecated WinForms and WPF was not usable and they churned the entire damn platform and replaced it with alpha-quality software. How many UI frameworks, and how much other churn happened in the past 11 years there? Way too much, everybody is so fed up that the past decade of my career consisted of rewriting desktop apps to this stack. NPM makes it quite easy to publish anything and for others to start using it. Just don't, stick with the proven solutions. There's - adjusted for user base - the same or worse amount of churn on Pypi and Nuget. If you wouldn't choose a random library nobody uses in the C# land, don't do it in the Node land. At least the Node libs continue to grow, while the Nuget stuff continues to die. reply throw4950sh06 13 hours agorootparentprevThere are a few features that were missing but it wasn't so bad. The most significant (added in v5) is proper async handler support. reply c0brac0bra 17 hours agoprevperl 6 eat your heart out reply yas_hmaheshwari 13 hours agoparentCan you elaborate this comment? Haven't used Perl in the last 12 years, so I fit the persona of \"living under a rock\" reply steve_adams_86 10 hours agorootparentPerl 6 took like 15 years to be released, and was met with fairly lukewarm reception, sometimes very negative. It ended up being renamed because it was so different from Perl 5, so it’s now called Raku. It was a flop. reply yas_hmaheshwari 9 hours agorootparentGot it! Thank you for the explanation Read this line on Wiki: \"In Perl 6, we decided it would be better to fix the language than fix the user - Larry wall\" Looks like a very good philosophical statement. Maybe there's a lesson here for other programming language ( The other Perl slogan: \"Easy things should be easy and hard things should be possible\" is also really good ) reply nailer 18 hours agoprevThe whole idea of having the response be part of the input for the route seems very 2010s. Nearly everything else these days has a route that takes a request and returns a response. This matches HTTP more closely. reply aphantastic 18 hours agoparentCuriously, the express model ends up being a more clean style when creating long lived writer streaming endpoints, which is becoming much more popular with LLM’s. All that was old will be new again. reply bhouston 17 hours agorootparentAre they using SSE? Or web sockets? reply aphantastic 16 hours agorootparentThe standard set by openai and shared by everyone else is to use SSE when “stream: true” is set. Personally I like to strip all the excess data as soon as possible and expose the raw response as a plain “transfer-encoding: chunked” stream. That way the client can decide if they want to accept each chunk as they come or simply await the whole response with limited code change. Either way, being able to easily pass the response writer around and set headers, write, close, as needed without being tied to a single function’s control flow is nice. It can be done in the more modern frameworks ofc, but ends up being a bit more machinery required at each endpoint. reply nailer 15 hours agorootparentprevA generator function that yields responses would fit that scenario perfectly. reply aphantastic 15 hours agorootparentNot that perfectly. You still have the question of how to represent the one-time header data. reply nailer 13 hours agorootparentI'm imagining: yield the headers (Content-Type: text/event-stream) the first time, yield the newline-formatted event streams on subsequent yields. I haven't done it though, I used to really like SSE (it's a little more REST-y) but the world seems to have decided on websockets. reply aphantastic 9 hours agorootparentThat could work. Probably it’d be a very easy bit of middleware/express extension to write, it’s basically just wrapping the existing writeHeader, write, end, in the hot new syntax. Probably you’d get some benefits in composability, and lose some performance in the extreme. reply johnny22 12 hours agorootparentprevI think that's how it works with nestjs's observer setup with rxjs. I just return the SSE stream and it works. I use it with fastify, not express. reply silverwind 3 hours agoparentprev`return` statements are inflexible as they prevent doing stuff after them (like logging, which should not delay the response). I prefer express's way. reply bhouston 17 hours agoparentprevFastify has a model where the request handler takes both a Request and a Reply: https://fastify.dev/docs/latest/Guides/Getting-Started/#your... reply petesergeant 20 hours agoprevI've probably been living under a rock, but I hadn't seen the \"transfer the old to a private company to provide support\" model before https://www.herodevs.com/support/express-nes reply ersoft 18 hours agoparentIt was for Rails. https://railslts.com/ reply andai 17 hours agoparentprevI think Flash is still being developed (as a fork of Adobe Air maintained by a 3rd party). reply JoeyJoJoJr 15 hours agorootparentI believe it is a single developer at Harman who is maintaining Adobe Air. I don’t think there is any work being done on Flash. It is more so things like keeping Adobe Air builds working for latest operating systems, fixing old bugs, etc, just for purposes of keeping legacy apps functional. reply andai 9 hours agorootparentYou mean literally one guy? There's something bittersweet about that. reply JoeyJoJoJr 6 hours agorootparentYep, I’m pretty sure it’s just one guy and the bug tracker with 1000 issues. I’d be interested to know what Air apps are so important to Harman that they are keeping it (barely) running. reply andai 1 hour agorootparentThere's a bunch of enterprise stuff built on Flash and Air right? I heard China's train network ran on Flash. They also kept using XP for ages for similar reasons (maybe still are?) reply vivzkestrel 13 hours agoprevso how does it compare to koa, fastify etc? reply wg0 16 hours agoprev [–] Give enough persistence, in ten years they could theoretically rewrite v5 in async rust. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Express v5 has been released, marking a significant update for the Node.js framework, focusing on stability, security, and maintenance simplification.",
      "Key updates include dropping support for older Node.js versions, improved path matching, enhanced security, promise support, and changes to the body parser.",
      "Deprecated method signatures have been removed for a consistent API, and a detailed migration guide is available for those upgrading from v4."
    ],
    "commentSummary": [
      "Express v5 has been released after a decade, featuring major updates such as enhanced security and support for asynchronous handlers.- The release was delayed to prioritize security audits and ensure a stable version, reflecting the team's commitment to reliability.- While some users face challenges upgrading, the Express team is commended for their efforts, and comparisons are made with other frameworks like Fastify and Koa."
    ],
    "points": 137,
    "commentCount": 37,
    "retryCount": 0,
    "time": 1729281731
  },
  {
    "id": 41888061,
    "title": "Have McKinsey and its consulting rivals got too big?",
    "originLink": "https://www.economist.com/business/2024/03/25/have-mckinsey-and-its-consulting-rivals-got-too-big",
    "originBody": "BusinessThe lost art of self-management Have McKinsey and its consulting rivals got too big? The golden age for CEO whisperers may be coming to an end photograph: ricardo tomás Mar 25th 2024 Share A n anonymous memo briefly circled the web in March. The authors, who claimed to be former partners at McKinsey, rebuked the illustrious strategy consultancy for its pursuit in recent years of “unchecked and unmanaged growth”, and chastised its leadership for, of all things, a “lack of strategic focus”. With humility typical of McKinseyites, they warned that “an organisation of genuine greatness” was at risk of being lost. Business March 30th 2024 Have McKinsey and its consulting rivals got too big? Making accounting sexy again A marketing victory for Nike is a business win for Adidas The pros and cons of corporate uniforms Regulators are forcing big tech to rethink its AI strategy Dave Calhoun bows out as chief executive of Boeing Meet the digital David taking on the Google Goliath Share Reuse this content the economist today Handpicked stories, in your inbox A daily newsletter with the best of our journalism Sign up Yes, I agree to receive exclusive content, offers and updates to products and services from The Economist Group. I can change these preferences at any time. Discover more What if carmaking went the way of consumer electronics? The Foxconnification of electric vehicles The horrors of the reply-all email thread Easy to start, impossible to stop BHP and Rio Tinto are heading in different directions The strategies of the world’s two most valuable miners are diverging Poland’s stockmarket has a hot new entrant The IPO of Zabka could help revive Warsaw’s beleaguered bourse Pity the superstar fashion designer Creative directors are coming and going faster than the latest trends Can artificial intelligence rescue customer service? The adoption of AI is surging in call centres",
    "commentLink": "https://news.ycombinator.com/item?id=41888061",
    "commentBody": "Have McKinsey and its consulting rivals got too big? (economist.com)114 points by godelmachine 4 hours agohidepastfavorite105 comments nelblu 3 hours agohttps://archive.is/qlF5n OptionOfT 1 hour agoprevIt was a really interesting place to work at a Software Engineer. It made me understand the business. It made me understand that doing the right thing isn't valued. You do the thing that has the shortest ROI. It also made me realize that it is horrible to build software with people who expect short term deliveries like the usual McKinsey engagement. People who expect that the automation of an Excel file takes the same time as getting a BA to do it. I am now in a full time engineering position. I don't talk to clients anymore. What I miss the most is coming into contact with people with a huge variety of backgrounds. Which surprisingly were the people with who I had to spent the most amount of time explaining how software works. Maybe I'm bad at it? Who knows. But I learned a lot, and I'm happy where I'm at now, so any bitterness would be misplaced. Not to mention they paid for my GC. reply ambicapter 1 hour agoparent> What I miss the most is coming into contact with people with a huge variety of backgrounds. > Which surprisingly were the people with who I had to spent the most amount of time explaining how software works. How is this surprising? I read this as \"huge variety of backgrounds\", meaning, all kinds of backgrounds which are NOT software. It would make sense to me they don't understand how software works. reply 1123581321 1 hour agorootparentI took it to mean that you’d think it would have been annoying to have to repeatedly explain things, but he misses them. reply burnte 17 minutes agoparentprevI work with a guy who used to be at McKinsey. He's literally the worst coworker I've ever had. Everything needs to be done yesterday, except he takes weeks between responses. He delegates nothing to his people, and constantly tries to take over things from other departments, making himself an INCREDIBLE constraint and burden. They have an insanely toxic culture there. reply mtlynch 1 hour agoparentprev>Not to mention they paid for my GC. What does GC mean in this context? reply hammock 1 hour agorootparentGreen card reply ljm 47 minutes agoparentprevI had a hard time working for an ex-McKinsey/Deloitte/MBA type a while ago for similar reasons: it was always favourable to push a quick hack to resolve an immediate issue, and literally nothing else mattered. If you had to fight fires all hours day, night and weekend to keep on top of it, then so what? That’s the job. Getting heart palpitations because the red circle came up on the Slack icon on your screen? That’s the job. Even with a clear path to a mid-term or even sustainable solution, it was like you weren’t building software but in a constant race to keep ARR ahead of churn, like in Wallace and Gromit where Gromit is frantically laying down track to keep his train going. Does the software even work? Who cares… it’s the $$$ that count. I wasn’t really built for that, I felt like I was at odds with my own passion and I didn’t really want to put my name to the work I was doing. reply Discordian93 21 minutes agorootparentOther than code related data annotation for LLMs, this is the only kind of work I've ever been able to find in software development, all the consulting shops big or small work like that here. When I read about people having proper testing, code review, product managers setting actual expectations of what the software should do... It sounds like a wonderland to me. reply throw4950sh06 17 minutes agorootparentYeah, unfortunately you need a proper startup culture at your location to have access to companies small and big like that. I don't think there is a single place like that in Europe. Fortunately, it's quite easy to find work for American companies. And well paid. reply ned_at_codomain 2 hours agoprevUsed to be at BCG. I think it's worth bearing in mind that relatively little consulting revenue -- even at the top strategy shops -- comes from pure strategy work anymore. You can push much, much more volume and absolute impact through by running big merger integrations, digital transformation, and other large scale change projects at big companies. It is basically a better business to become something like a premium Accenture, a \"get stuff done\" kind of consultancy. You can staff an army of junior people for a very very long time on those kinds of projects. It's just not that easy to keep people staffed on 5-6 person teams solely on 8-12 week pure strategy engagements. These kinds of projects are also the first discretionary spending yo get cut when times get tough. If you're going to be focused on the pure strategy work, you'll probably want to stay really really small. We've seen some of this in investment banking with firms like Allen & Co or Qatalyst. Challenge is that consulting doesn't come with scalable monetization via success fees. It's just not great business to be a boutique consultancy, I think. reply ghaff 2 hours agoparentIt’s true in the IT analyst business too. While there are people in 1-10 person firms who earn a living, they’re mostly not getting rich and this is a long term trend line with a lot of consolidation of medium sized firms over time. To be reasonably successful you probably need a real focus and be good at selling. Had a conversation with an ex-Gartner analyst—now at a product company—and his comment is that even at the big analyst firms, comp isn’t great at least below senior management. reply samdung 2 hours agoprevI have a small story about McKinsey from friend of mine in the Indian Bureaucracy from about 10-12 years ago. McKinsey was doing some work for their dept. I asked him what they did. He said, \"McKinsey asked us for lots of information. Then they put it into a dossier and gave it back to us.\" reply crazygringo 1 hour agoparentTo be fair, a lot of organizations can't do this on their own. Which is why they hire a consultancy. Information is siloed, teams compete rather than cooperate, any team's own dossier is going to be seen as biased and unobjective. There's real value in hiring a neutral, competent vendor to come in, assemble the relevant information using best practices, and present a \"dossier\" with common-sense conclusions. Then the leader who hired them can use that as political cover for taking the necessary actions they wanted to in the first place, because the leader is no longer siding with one bureaucratic faction against another, but merely taking objective advice from an outsider. That's actually worth a lot. reply neilv 1 hour agorootparent> teams compete rather than cooperate [...] political cover for taking the necessary actions they wanted to in the first place That looks like three deeper problems than the consultants were tasked to solve. (Not only do you have counterproductive, misaligned culture; but even the CEO can't/won't fix it; and the CEO even has to play political games, just to work around the bad culture, for smaller goals.) reply toast0 45 minutes agorootparent> That looks like three deeper problems than the consultants were tasked to solve. No, this is exactly the reason the consultants were hired. Not to solve the cultural problems, but to work the broken process. It's not really in the consultants interest to solve the cultural problems anyway, because it drives repeat business. reply throw4950sh06 13 minutes agorootparentVery often it's yet another consulting company proving to their clients that the big cultural change they are suggesting needs to be done. There is big money in doing that too - you gain a client for life if you're successful, and you get to recommend all your friends in Professional Services companies who give you a cut/forward strategy business your way. reply datavirtue 1 hour agorootparentprevThis comment and the parent sums up GE exactly. These consulting engagements were/are a constant stream there. However, they were often ordered by ineffectual, siloed pretend managers and absolutely nothing came of it aside from a bill and the manager getting to feel like a manager for triggering the engagement. reply samdung 1 hour agorootparentprevYup makes sense from that point of view. reply neilv 1 hour agoparentprevThere've been variations on this saying/joke for at least as long as I've been working, not specific to McKinsey. Example: \"A consultant is someone who charges you $100K, to tell you at the end, what you told them at the beginning.\" reply nothercastle 1 hour agorootparentIt’s part of the executive game. Make their position or opinion seem legitimate reply geoka9 2 minutes agoparentprevThat's how any financial consultant/auditor works: an outside team sets up shop in one of your offices with your management's permission to ask any person in your company for any related information. Then they compile a report/financial statements, send it to your management and collect their fee. reply brtkdotse 16 minutes agoparentprev> McKinsey asked us for lots of information. Then they put it into a dossier and gave it back to us.\" To be fair, that’s exactly what a psychologist does reply LaundroMat 38 minutes agoparentprevAs the saying goes, you pay a consultant to look at your watch and tell you what time it is. Sometimes this means consultants bring you back to the right perspective, sometimes it means they don't add any value. reply swiftuser 2 minutes agoprevThe mentioned memo can be read here: https://web.archive.org/web/20240316081510/https://obligatio... McKinsey has been doing \"silent layoffs\" in the last few review cycles, i.e., shrinking overall headcount after performance reviews – as there's not enough work to go around. Hard do meet the bar for a one-year BA, if you've only been on 1-2 studies – which is not exactly your fault. reply mentalgear 1 hour agoprevIt's not just that they're too big—it's that they're morally corrupt and largely unregulated. These firms have been involved in everything from tobacco lobbying to the 2008 financial crisis to pushing opioids, and that’s just scratching the surface. Despite their role in these crises, they’ve faced zero accountability and continue to rake in massive profits. As for their supposed value (which comes directly from ex-employees): big consulting firms are essentially hired as a liability shield for the C-suite. Their main job is to back up whatever the CEO already wants to do (usually cost-cutting). This way, executives can claim: a) \"McKinsey recommended it, so it must be right,\" and b) \"If it goes wrong, it’s on McKinsey, not us.\" reply terminalbraid 1 hour agoparent> they’ve faced zero accountability https://www.insurancejournal.com/news/national/2024/10/18/79... Is that not an example of accountability directly for the things you're complaining about? reply eesmith 1 hour agorootparentHow much did they make from helping lying drug dealers? The article doesn't say. If damages weren't at least 3x revenue + attorney fees, it's the cost of doing business. It's not like every shady thing they do gets caught. Better would be if people faced jail time. reply whatever1 2 hours agoprevWith labor becoming fungible in the eyes of c-suites, the holders of institutional knowledge are the libraries of the consulting firms. Consulting firms also have tech talent that traditional businesses had no access to in the past, so any large scale data driven related project is done with consultants. However: 1. The costs are insane and probably we reached the point where the benefits do not justify the prices they are charging. 2. Wfh is a cheat code to get access to cheap tech personnel that is pissed with the RTO of big tech. I keep hearing tech folks working at traditional manufacturing shops remotely these days. reply sevensor 2 hours agoparentThis is a surprising take, considering the consultants I’ve worked with never took the trouble to learn anything about the business despite sitting shoulder to shoulder with us for weeks, and then proceeded to give us spectacularly terrible technical advice. “You need machine learning for this. Trust us, we’ve done this before.” They hadn’t, and it became clear they’d wasted our time and hadn’t listened to a word we said. reply whatever1 2 hours agorootparentThere is nobody left, old enough, to remember that the last project with that consulting firm failed. reply jumping_frog 1 hour agorootparentAre we living in middle ages that electronic records don't exist of what happened in the past engagement. Maybe one can locate the last McKinsey dossier on OneDrive? The problem is and always was of incentives of people involved. reply whatever1 28 minutes agorootparentWould you trust a random soggy doc for which you don't know the person who wrote it and their competence, or the flashy deck from McKinsey? The answer is 99% of the times: McKinsey. Only time that answer is different is when a person in power is still around and they pretty much veto the engagement with a consulting company. reply Scalestein 49 minutes agorootparentprevGenerally there Firms prize \"first principles\" thinking and don't put a ton of effort into leveraging previous engagements. Think of it like how engineers will throw out a solution and rebuild it rather than understand and build upon it. It is also quite difficult to surface useful information amongst all the noise in the giant archive of documents reply candiddevmike 2 hours agoparentprevThe economics of consulting are pretty raw: they basically arbitrage the hourly rate of folks. They pay you X and then bill you for X*1.3 (minimum). Folks are starting to realize they could just go direct to the customer, and the recent non-compete uncertainty has made a dent in the quality of talent these places have on their bench. Not to mention most of the talent at these shops are just window dressing for cheap offshore labor that actually does the work with mixed results. reply whatever1 2 hours agorootparentMore likely they pay you X and they charge 3X, but yes I agree. reply jncfhnb 2 hours agorootparentprevI’m pretty sure my average multiple is 6x reply nothercastle 1 hour agoparentprevTraditional mfg companies don’t understand tech. They don’t like that tech employees demand higher wages and benefits and hire bottom tier talent or try to convert existing non tech employees into programmers. reply whatever1 37 minutes agorootparentWhich kinda makes sense. The work of an SWE at big tech can bring in a ton of cash due to (almost) infinite scaling of software and (almost) zero marginal cost. But as an SWE at a manufacturing company, how much can your work increase their revenue/profit? The bulk of their income is still coming from the widget/commodity they are producing. reply watwut 2 hours agoparentprevI have yet to see big consultant company to improve things on technical levels. They are great at talking to upper levels of management and avoiding accountability for results. They don't improve how things run. reply jncfhnb 2 hours agoprevCurrent McKinsey here. I don’t think anyone expects the current difficulties to last indefinitely. Its fairly well established that consulting struggles during times of economic uncertainty (but does fine when things are outright bad or good). reply lenerdenator 1 hour agoparent> Its fairly well established that consulting struggles during times of economic uncertainty (but does fine when things are outright bad or good). The culture engendered into corporate America by businesses like McKinsey is exactly what causes economic uncertainty. When the only thing that matters is the number at the bottom of the piece of paper being big enough for some analyst in lower Manhattan to be happy, humans will do all sorts of unpredictable things to make that number be that way. reply jncfhnb 1 hour agorootparentI thought for sure it was war in Europe, bubbles burst in Chinese real estate, and a general fracturing in the west vs the rest globalization but ok reply candiddevmike 2 hours agoparentprev> Its fairly well established that consulting struggles during times of economic uncertainty Only the bullshit kind of consulting McKinsey is known for, IMO. The value of someone coming in and repeating what the smart folks on the team are saying is no longer there. Plenty of boutique consulting shops are eating just fine right now. Delivering real results/solutions and being a SME is always in demand. reply jncfhnb 1 hour agorootparentI don’t do that kind of thing but for what it’s worth The value of coming in and listening to what smart people are saying and then telling the C Suite to actually do it is hugely valuable when the organization is otherwise paralyzed by bureaucracy. reply blinded 1 hour agorootparentFor sure! Just a real frustrating aspect of change in enterprise. reply mordymoop 1 hour agorootparentprevI have worked in specialist consulting for 10 years and I agree with this take. Our business does the best during times of uncertainty. During lean times, operators want to make sure they are allocating capital intelligently, so they pay for our insights and advice. In the good times, they just plow all their revenue into making the flywheel spin faster, no need to be cautious or thoughtful when money is coming in no matter what you do. reply willcipriano 2 hours agoparentprevWould there be any advantage for anyone to state they feel the opposite to your view publicly? Seems like a career limiting move. reply breakfastduck 2 hours agorootparentDepends how private the account they're posting the opinion on is! reply jncfhnb 2 hours agorootparentprevI mean we’re not allowed to say anything either way reply kolbe 2 hours agorootparentprevIf all comments here were eyed with the same cynicism, we'd have no genuine discussions. And maybe we don't. reply willcipriano 2 hours agorootparentI don't think the people behind the opioid crisis should be trusted to be speaking in good faith. reply alkonaut 2 hours agoprevThe big firms just need to be big and known. There value delivered isn’t in the PowerPoint explaining that cutting costs and increasing revenue will increase profits. The value added is that companies buy a stamp from a famous firm when they deliver staff cuts, reorganizations or other controversial news. As such it doesn’t really hurt to go for scale. They’re a standards org for headcount reduction. reply betaby 1 hour agoparentWhy though? Why a company with a board, which is already independent, would need some consulting? Isn't the board to convey ideas to CEO? reply nothercastle 58 minutes agorootparentThe board is neither independent nor objective or often really in touch with the business. Therefore they need cover from a big name firm reply fsndz 40 minutes agoprevYes. Company continue to pay them for political reasons mostly. Just look at how many McKinsey partners became CEOs and failed terribly at it (latest example Starbucks reply ainiriand 1 hour agoprevI've por es as an external consultant to KPMG as a software engineer and I loved that all processes are clearly defined. I think is big because all steps are clear for everyone in the company so the do not misstep that often.. I also felt like we had so few meetings and a lot of work was done in little time with almost 0 interference from management. reply LunaSea 1 hour agoprevQuick reminder that McKinsey helped Purdue Pharma & co boost their opioid drugs sales, starting the opioid dependency and overdose epidemic in the US. https://en.m.wikipedia.org/wiki/McKinsey_%26_Company reply openrisk 1 hour agoprev> The quicker corporate clients become comfortable with chatbots, the faster they may simply go directly to their makers in Silicon Valley. If that happens, the great eight’s short-term gains from AI could lead them towards irrelevance. That is something for all the strategy brains to stew on. The cloud divisions of \"big tech\" might be the catalysts for this upcoming disintermediation. reply datavirtue 1 hour agoparentIn the very near future your grandma is going to carry on a conversation with a machine at her bank and she will have no idea it was not human. After the election everything that remotely touches AI is going to be at eleven. Everything from companies laying pipe to startups who are delivering services that integrate AI with bespoke business processes. The upswing is going to be far bigger than anyone is imagining right now. The market PE could shave ten points and it will still be gangbusters in AI. Don't fight the trend. reply photochemsyn 1 hour agoprevDon't all these consulting firms have a really bad record when it comes to large-scale infrastructure projects? Some of the worst examples are Ciudad Real International Airport (Spain/McKinsey), Karuma Hydroelectric Project (Uganda/Deloitte), Chongwe River Water Supply Project (Zambia/Boston Consulting Group), Isimba Dam Project (Uganda/McKinsey), Haramain High-Speed Rail Project (Saudi Arabia/Deloitte), and Shoreham Nuclear Power Plant on Long Island, New York (McKinsey). In constrast, China's infrastructure projects are highly successful - high-speed rail now covers 42,000 km across 100 corridors, and the first one was only completed in 2008. Based on their example, the most efficient way to build modern infrastructure is to cut the consultancy firms out of the loop entirely. reply albert_e 2 hours agoprevIs it fair to say that ... Consulting is the oldest profession ;) reply hulitu 44 minutes agoparent> Consulting is the oldest profession ;) Some would dissagree. Though i think prostitution comes after politician. /s reply smitty1e 3 hours agoprevDeloitte sends a large delegation to Davos annually, but they do it ironically. reply ProllyInfamous 2 hours agoprevLet me propose an alternate theory: fines and enforcement have become too small, so-as to become just a cost of doing business. Many of these employees are so-incentivized by fiscal profit that they fail to see the immorality, just seeing opportunity. Maybe if our regulators weren't in bed with so many of the major funds... \"If you think there's a solution, you're part of the problem.\" —George Carlin (Conan O'brien interview) reply xbar 2 hours agoparentFines are trivial. reply nothercastle 56 minutes agorootparentIt Usally costs more to process the fine internally then the nominal value of the fine reply paulddraper 2 hours agoparentprevYou're gonna have to connect some dots to legal fines, etc. reply ProllyInfamous 2 hours agorootparentYour request requires a consultation. Fortunately we also receive miniscule enforcement/penalties. Press `X` to accept terms & conditions. reply alexashka 2 hours agoparentprevYes, but go on taking the logical steps: why are the regulators in bed with so many of the major funds? You very quickly boil it down to morality and ethics. Americans have a broken ethics code that says the free market magically solves the need for anyone to be a good person - if someone's a selfish ass, the free market and 'competition' will magically fix it. Therefore I can be in bed with whoever I like, whenever I like - both literally and figuratively. Life wasn't too bad as long as Americans believed in religions and this extremely stupid free market idea was balanced by an extremely stupid idea of God that wants you to be 'good'. Now that Americans are unconstrained by anything and further emboldened by extremely stupid ideas of magical free markets - they're well on their way to societal collapse. This is why Jordan Peterson is trying to bring God back - he knows Americans are a collapsing, degenerate culture. It's the wrong move and it won't work but he (unlike 99% of people) at least understands the root the problem. Oh well. I wouldn't mind it too much because stupid societies full of stupid people should and do inevitably collapse but these stupid people have nuclear weapons. The way it's going - we will have nuclear war. ps. most other cultures are also extremely stupid (they just have a different set of extremely stupid ideas). Americans just happen to have become the world ruler so they're of greater interest. reply lucianbr 1 hour agorootparentYou should read about the popes in medieval times. Point being, everything you wrote is about humans, not about americans. And having religion changed nothing. Who knows what those people believed inside their minds, but outward they had religion, and outward is all we can see for anyone. > stupid societies full of stupid people should and do inevitably collapse No society endures forever it seems (maybe I'm misinformed?), but no society is immune to stupidity, so I'm not sure your statement about stupid societies actually has meaning. Collapse is certain, the causes less so. I take solace in the fact that either nuclear annihilation or some kind of climate disaster, seems inevitable, but also nobody's fault in particular. We could avoid it if we weren't human. But I'm not sure that I want that. The good would likely be gone with the bad. Maybe the cokroaches will manage better, or the robots, but maybe the Fermi paradox solution is that life always evolves to be too greedy for it's own good. reply ProllyInfamous 2 hours agorootparentprev>why are the regulars in bed with so many of the major funds? \"Show me the incentive, and I'll show you the outcome\" —Charlie Munger (RIP) Money? Is the answer to your question money? reply pphysch 2 hours agorootparentprevWhich contemporary cultures are not \"extremely stupid\", in your opinion? reply alexashka 1 hour agorootparentI've provided all the necessary information to piece it together without explicitly telling you. That was not an accident. reply klelatti 2 hours agoprev> Excitement among clients over this type of “generative” AI is also creating opportunities for new work. Mr Schweizer says that BCG has already completed hundreds of projects with clients around the technology. Accenture has booked $1.1bn-worth of generative-AI work in the past six months. During a gold rush teach people how to use their shovels. reply throwawaycons 13 minutes agoparent[throwaway because ...reasons; also, I ran my comment thru ChatGPT to anonymize it] I'm currently working at Accenture, collaborating with a well-known German car manufacturer (OZJ). We receive a new RFP almost every other month, and due to our long-standing relationship with them, we end up securing the majority of the projects. We manage to deliver on these projects—or at least ensure they are billed. Occasionally, we engage in some RAG work or even delve into image generation, though the quality of these outputs tends to be quite subpar. Sometimes, it surprises me that the client even accepts it, but ultimately, they receive a functioning product (most of the time), we get compensated, and the end consumers cover the costs. reply detourdog 3 hours agoprevI think more than becoming to big the solutions are founded on a narrow matrix of logic or group think. reply throw4950sh06 3 hours agoparentThat's by design. They have few verticals that they focus on and have templates for every situation that might arise. (Former McK SW engineer here) reply toomuchtodo 2 hours agorootparentWould you say McK is the consulting equiv of SAP? Where they push the business into the vertical template vs do the actual legwork to improve the org in the vertical in question? reply throw4950sh06 59 minutes agorootparentIn many cases, yes. But I wouldn't say it's not valuable if you're in these few verticals they care a lot about (one of them is mining, as an example). They won't provide you the data directly, but they do have it and it's central to everything they do - from across the industry, large and small clients, at all stages of life, and they stayed with many of them through significant transitions. They invested hundreds of millions of dollars into custom internal tools to make the data and derived analysis accessible to consultants. Where you have theories, experiments and anecdotes, they have statistics. And nobody has better than they do. Don't fall into thinking they just want to make some easy money and don't care about your situation. They most probably saw hundreds or thousands of clients like you, and know that unless you're a unicorn, the \"standardized\" approach will be better for you. If you were an unicorn, you wouldn't need to hire McKinsey. Remember, most of us aren't Gates/Jobs/... This is just like tech startups thinking they're an unicorn and spending an absurd amount of effort (and thus money - also opportunity cost) on reinventing the wheel and building for infinite scalability, instead of simply doing what works well for everybody else. Not saying they don't want money, but their approach to making easy bucks is to add a zero to their quotes, not by not doing anything worthwhile. The quotes seem crazy to us mere mortals, but the clients are usually happy - what's a few million dollars when they helped you save tens or hundreds of millions. You don't care how simple was what they did - you probably tried many times yourself before hiring them, and if they made a stupid thing work, you're grateful. reply robertoandred 2 hours agoprevGotten* C'mon, The Economist reply js2 2 hours agoparentPretty sure it's a US/UK thing. reply seabass-labrax 1 hour agorootparentHistorically speaking, it's common to both UK and US English because it predates the European colonization of America: http://www.miketodd.net/encyc/gotten.htm Personally, for a situation like this I'd use the somewhat more formal 'become' instead! reply spenrose 3 hours agoprevnext [2 more] [flagged] marcosdumay 3 hours agoparentIt's not a fair question. Any size larger than 0 would satisfy it. reply jakubmazanec 2 hours agoprevObligatory John Oliver video on the topic: https://youtu.be/AiOUojVd6xQ?si=anI1_FKkDM4oDI1M reply artursapek 2 hours agoparentNothing about John Oliver is obligatory reply datavirtue 55 minutes agorootparentAgreed. I watched an a show on something I knew a lot about and was appalled out how much he missed the mark and/or had an agenda. I always liked his show and tended to agree with his take until then. Great rehtoric, Im just concerned that people might be eating his shit and forming \"informed\" opinions on things because Oliver's team \"already did all the research.\" Keep some salt handy. reply stonethrowaway 2 hours agorootparentprevLess than nothing. It has flipped to the other side in the bullshit of it all. Pundit hot takes have ruined quality dialogue. Frankly I’d rather have discussions with people who do their own research and form their own opinions, no matter how faulty those opinions may be (there is always room for improvement precisely because there is room for discussion), than to hear them regurgitate party-speak drivel shilled by actors. Talking heads are a cancer upon society. edit: to the folks responding saying something to the effect of “you don’t know what you’re talking about!” - I could have saved you the trouble and written your responses down ahead of time. They’re trite and reflexive. Oliver is a shill, down to the so-called data. He’s an approved mouthpiece on behalf of the state. He discusses and he frames only what is allowed to be said in a context it’s allowed in, and his edgy woke takes are the complete opposite of. They’re edgy because of how milquetoast the Overton window has become. Put another way, if you’re in unison cheering for someone who is allowed to have their own TV show, it’s time to ask yourself if you’re being played with bread and circuses. You won’t hear the opposition because, well, in older times and other places they’d be disappeared/imprisoned, but today it’s mostly people like this twat (and yourselves) drowning out the contrarian voices. Job well done lads. reply rolandog 2 hours agorootparentI disagree. The phrase \"doing one's own research\" has been co-opted by people doing conspiracy-windowshopping. It's designed to get lazy people stuck in the muck while sending them on a Shawshank-Redemption-style crawl through the sewers of the Internet... Only those who can cut through the bull — pun slightly intended — make it to the other side. You have to start trusting people at some point. Researchers and scientists have proven to be trustworthy, if (IFF!) the right incentives system is in place. (Edit: and talking heads, of course; my claim is that John Oliver and his team of researchers and writers is more trustworthy due to the effort that they put into making things as accurate as possible, while funny as well... All while elevating the quality of newscasters by showing attribution to what they are saying and connecting the dots on why it is relevant to the viewers). But we are living at an awkward stage of civilization: the very rich are backing political leaders who make a religion out of economic systems and don't view them as useful tools to balance development and inequality. reply malfist 2 hours agorootparentprevI get the feeling you haven't watched Jon Oliver. He does deep dives on relevant issues and does a ton of research. I was recently interviewed about his process and it's pretty enlightening. He complains about how activists often twists perfectly good data to make a point but under scrutiny it falls apart, even if the original data would have been good enough. He's about as far from a hot takes engine as you could get reply jakubmazanec 2 hours agorootparentprevIf only you did your own research on how John Oliver does his research. reply karmakurtisaani 1 hour agorootparentSource: trust me bro? reply jprd 1 hour agorootparenthttps://youtu.be/Q9kNMJ8SguQ?t=898 reply rc_mob 2 hours agorootparentprev??? John Oliver has always does a great job of understanding and communicating an issue in depth. He is exactly the opposite of everything you are accusing him of. reply kevinventullo 1 hour agorootparentEven on topics where I agree with him directionally, I find he misrepresents the truth or omits significant caveats. reply mschuster91 3 hours agoprevOf course they got too big, and especially they got too enmeshed with the auditing business. It's time to break them all up. The \"firewall\" that should be there from a legal perspective is a joke in practice, and when there are only three to four companies, there is no place for fresh blood and with it fresh ideas to enter the market. reply andsoitis 2 hours agoparent> they got too enmeshed with the auditing business. McKinsey does not have an auditing business. reply insane_dreamer 2 hours agoparentprev> too enmeshed with the auditing business that's not the issue; consulting pays much more than auditing, which is why the Big4 would probably be fine spinning off their auditing businesses reply jncfhnb 2 hours agoparentprevMcKinsey isn’t in the auditing business at all. And those that do play both went the other way, transitioning from auditing into consulting. There’s a ton of competition in consulting in general. reply metadat 2 hours agorootparentThis is false, the Big 4 are all in the auditing business, in addition to high-end management consulting. reply jncfhnb 1 hour agorootparentMcKinsey is not a part of the Big 4. The Big 4 refers to the big four auditing firms. reply SargeDebian 2 hours agorootparentprevWhat part of the statement is false? reply CPLX 2 hours agoparentprevThe firewall is of course a fiction. The entire point of these consultancies is to allow collusion between competitors, which is otherwise illegal. reply imperialdrive 1 hour agorootparentInteresting, I never considered that thought before. reply fHr 1 hour agoprev [–] fuck mck, every company they get their hands on is turning into a piece of shit human capitalism slavery where max profit is the only important thing fuck consultancies reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "An anonymous memo from former McKinsey partners criticizes the consultancy for its rapid growth and lack of strategic focus, suggesting that its reputation is at risk.",
      "This memo highlights broader industry concerns about whether major consulting firms, like McKinsey, have expanded too much.",
      "Other trending business topics include Nike's marketing success, big tech's artificial intelligence strategies, and the increasing use of AI in customer service."
    ],
    "commentSummary": [
      "The discussion critiques large consulting firms like McKinsey for prioritizing short-term gains, fostering toxic work environments, and engaging in questionable ethical practices.",
      "Concerns are raised about these firms' significant influence in corporate decision-making, often acting as a liability shield for executives and providing political cover rather than strategic value.",
      "Criticism extends to their involvement in controversial projects, with suggestions that their size and influence contribute to a lack of accountability."
    ],
    "points": 114,
    "commentCount": 105,
    "retryCount": 0,
    "time": 1729349208
  },
  {
    "id": 41885231,
    "title": "How to leverage Claude's capabilities with interactive visualization",
    "originLink": "https://github.com/anthropics/anthropic-quickstarts/tree/main/financial-data-analyst",
    "originBody": "Claude Financial Data Analyst A sophisticated Next.js application that combines Claude's capabilities with interactive data visualization to analyze financial data via chat. Features Intelligent Data Analysis: Powered by Claude (Using Claude 3 Haiku & Claude 3.5 Sonnet) Multi-Format File Upload Support: Text/Code files (.txt, .md, .html, .py, .csv, etc) PDF documents (Regular PDF with text, scanned documents not supported) Images Interactive Data Visualization: Based on the context and data analyzed, Claude can generate the following charts: Line Charts (Time series data & trends) Bar Charts (Single metric comparisons) Multi-Bar Charts (Multiple metrics comparison) Area Charts (Volume/quantity over time) Stacked Area Charts (Component breakdowns) Pie Charts (Distribution analysis) Getting Started Prerequisites Node.js 18+ installed Anthropic API key (For Claude integration) Installation Clone the repository: git clone https://github.com/yourusername/financial-ai-assistant.git cd financial-ai-assistant Install dependencies: npm install Create a .env.local file in the root directory: ANTHROPIC_API_KEY=your_api_key_here Run the development server: npm run dev Open http://localhost:3000 with your browser to see the result. Technology Stack Frontend: Next.js 14 React TailwindCSS Shadcn/ui Components Recharts (For data visualization) PDF.js (For PDF processing) Backend: Next.js API Routes Edge Runtime Anthropic SDK Usage Examples The assistant can help with various financial analysis tasks: Data Extraction & Analysis: Upload financial documents Extract key metrics Analyze trends and patterns Visualization Creation: Generate charts based on data Customize visualizations Compare multiple metrics Interactive Analysis: Ask questions about the data Request specific visualizations Get detailed explanations Interesting Use Cases While primarily designed for financial analysis, the AI assistant can be adapted for various intriguing applications: Environmental Data Analysis: Analyze climate change trends Visualize pollution levels over time Compare renewable energy adoption across regions Sports Performance Tracking: Upload athlete performance data Generate visualizations of key metrics Analyze trends and patterns in team statistics Social Media Analytics: Process engagement data from various platforms Create charts showing follower growth and interaction rates Analyze sentiment trends in user comments Educational Progress Tracking: Upload student performance data Visualize learning progress over time Compare different teaching methods or curriculums Health and Fitness Monitoring: Process personal health data from wearables Create charts for metrics like steps, heart rate, and sleep patterns Analyze long-term health trends and provide insights You can even use charts and images to create interesting results, like the ability to see what's most common inside a picture using a pie chart.",
    "commentLink": "https://news.ycombinator.com/item?id=41885231",
    "commentBody": "How to leverage Claude's capabilities with interactive visualization (github.com/anthropics)98 points by allan666 16 hours agohidepastfavorite26 comments doctorpangloss 2 hours agoYesterday my company changed a single line to switch our application’s LLM backend API from Claude to ChatGPT, because Claude started adding stuff its answers in QA style prompts that it wasn’t before, at least since August 1st. I wish I could pay for a guarantee of performance, really quantization, which seems so simple but because it can 2-4x decrease their costs, LLM API providers keep quantizing and distilling without telling anyone. It’s a longer journey: to be an enterprise API. Which by the way, is a terrible business to be in. reply mellosouls 6 hours agoprevThis is interesting and appreciated but I'm not sure it's a Show HN unless the OP is representing Anthropic? reply rubslopes 5 hours agoparentAll 3 submissions of OP start with \"Show HN\". I think they are not aware of HN protocols. reply diggan 5 hours agorootparentAll 9: https://i.imgur.com/TflImv5.png To OP: > Show HN is for something you've made that other people can play with. HN users can try it out, give you feedback, and ask questions in the thread. > The project must be something you've worked on personally and which you're around to discuss. https://news.ycombinator.com/showhn.html reply dash2 9 hours agoprevThis seems like Anthropic showing people how to build a thin layer around Claude. Can building a thin layer around Claude be a valuable business model? If there are good profitable UIs for Claude, wouldn't Anthropic implement them itself? reply anonzzzies 8 hours agoparentI would indeed (like Altman promised for OpenAI; 'we're gonna steamroll you'), implement all these cases (and many more) on their side, so their 'chat' becomes a full toolkit for building, visualising, prompting etc and allow people to plugin their data/processes (maybe with a few partners for that part which they can easily replace or have multiple of). Currently, the \"Added to project\" button that remains for n seconds and you have to wait for to add another file (sometimes Claude generates 4-5 files per chat) is such an annoyance that I guess they should stick to training and nothing else. reply peer2pay 8 hours agoparentprevI think the idea here is to build a thin layer but BYOD. I currently work for a company where most our value add lies in the data collection, cleaning and running of proprietary algorithms. A UI like this would be a game changer for us and something that Anthropic couldn’t easily replicate due to all the IP in our data pipeline. reply billsunshine 4 hours agoparentprevThis...so this. There is no value capture in building a shell around Claude reply Viliam1234 3 hours agorootparentPerhaps there is a lot of money you could get in short term. Enough to pay the costs and generate some profit. Also, most people are not computer experts; if you show them something can be done using your website, they will continue to do it using your website long after others have added the same functionality. reply fragmede 8 hours agoparentprevOpportunity cost. Anthropic's deal is in training Claude and whatever they choose to call their next model, not whatever weird little niche you're going after. I might not go after programming, but, say, a dnd character backstory generator would be a wrapper that's probably not interesting enough for them to build themselves to compete with yours. Or maybe it is, but your DND character backstory generator also doesn't have to use anthropic as the backend, there are others for you to choose from, so it's a bit of a standoff. reply Larrikin 4 hours agorootparentBut is there any reason to use them besides a demo to investors while you actually build the business on llama? Why build a business with a permanent subscription that's the entire core to your business? reply zurfer 10 hours agoprevWell done. We are building something similar [1] and found that generating UIs (mostly charts) on the fly works surprisingly well in most cases, but can be a bit frustrating if you know exactly what you want and just can't prompt it (as a user) to do that because of some edge case. While this is a cool demo that shows what LLMs can do I am a bit surprised how polished and advanced it looks (even PDF upload) for a quick start. Anyway I love that it's open source so we can learn from it. [1] https://getdot.ai reply zurfer 10 hours agoparentHere is a link to the prompt. https://github.com/anthropics/anthropic-quickstarts/blob/9d5... On a second glance I'd say that the visualizations are pretty limited but good enough for a demo. reply fblp 2 hours agoparentprevAre there any other services out there like getdot.ai that help do analysis? reply zurfer 44 minutes agorootparentThere are a bunch that I put into this list: https://github.com/Snowboard-Software/awesome-ai-analytics ( probably missing a similar amount since many people try) I would be curious to understand why you are looking for an alternative? reply croes 7 hours agoparentprevThey want and need your data, of course they make it easy for you to give them what they want reply bl4ckneon 11 hours agoprev(didn't look at the code yet but) Would a challenge of building an app like this that heavily depends on a LLM be getting a deterministic response back? I guess you could code for it to check if it gave you a certain format of data or if it was what you expected, but if I upload something that Claude doesn't understand and it gives back something that breaks the data analysis then that seems it would be tricky to handle that case. Please correct me if I am wrong. Thanks! reply SparkyMcUnicorn 11 hours agoparentAnthropic and OpenAI let you define a JSON schema to adhere to for tool calling. Here's the part you're looking for: https://github.com/anthropics/anthropic-quickstarts/blob/mai... reply cj 6 hours agorootparentFor some reason, the guarantee in the format of the response doesn't seem sufficient in preventing backwards incompatible changes that may happen to models. Yes, the response might be in a standard format. But a well formed response can still be bad/broken. Another way to think about it, is it can \"pass QA\" one day, and \"fail QA\" the next day even if the API response is identically formatted/structured. reply Kiro 9 hours agoprevIt says it supports PDF but I don't see that in https://github.com/anthropics/anthropic-quickstarts/blob/mai... What am I missing? reply ideashower 3 hours agoprevCan you take these resulting interactives and export them to publish? reply weinzierl 9 hours agoprevI wish they'd focus more on getting the basics solid. Currently Claude can't even render anything beyond the most basic form of a table. For example, try to let it turn multiple items in a table cell into a bulleted list. It just outputs a mess of literal HTML tags. reply troupo 7 hours agoprevDo they have any plans on opening up APIs to private individuals? reply albert_e 11 hours agoprev [–] Looks very interesting. I am more familiar with React - am looking for a React example that achieves similar UI, any working examples I can take inspiration from? reply SparkyMcUnicorn 11 hours agoparent [–] This is React. reply albert_e 3 hours agorootparent [–] My bad. I had a brainfade - registered something else on my first skim. Thanks. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Claude Financial Data Analyst is a Next.js application leveraging Claude's AI for interactive financial data analysis through chat, featuring intelligent data analysis and multi-format file support.",
      "The app supports various data visualizations, including line, bar, area, stacked area, and pie charts, and requires Node.js 18+ and an Anthropic API key for setup.",
      "It is built with a technology stack comprising Next.js, React, TailwindCSS, and Recharts for the frontend, and Next.js API Routes and Anthropic SDK for the backend, offering use cases in diverse fields like environmental data and social media analytics."
    ],
    "commentSummary": [
      "A company transitioned its Large Language Model (LLM) backend from Claude to ChatGPT due to unforeseen changes in Claude's responses, highlighting the importance of consistent API outputs.",
      "Discussions arise around the challenges and potential of developing applications with Claude, with some questioning the value of creating minimal layers over it and others seeing niche opportunities.",
      "The conversation emphasizes the need for reliable API responses, interest in open-source projects, and the demand for more robust basic features in Claude."
    ],
    "points": 98,
    "commentCount": 26,
    "retryCount": 0,
    "time": 1729305569
  }
]
