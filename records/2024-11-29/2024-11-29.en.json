[
  {
    "id": 42268580,
    "title": "Send someone you appreciate an official 'Continue and Persist' Letter",
    "originLink": "https://ContinueAndPersist.org",
    "originBody": "Every day, thousands of Cease and Desist letters are issued, telling people to stop what they’re doing (Looking at you, David Chang). What a bummer!That’s why we created: The Continue and Persist Letter. A official-looking legal letter that encourages and uplifts people, one that tells people to keep doing what they’re doing! Surprise someone you appreciate by sending them a Continue and Persist Letter.",
    "commentLink": "https://news.ycombinator.com/item?id=42268580",
    "commentBody": "Send someone you appreciate an official 'Continue and Persist' Letter (continueandpersist.org)1219 points by adnanaga 21 hours agohidepastfavorite209 comments Every day, thousands of Cease and Desist letters are issued, telling people to stop what they’re doing (Looking at you, David Chang). What a bummer! That’s why we created: The Continue and Persist Letter. A official-looking legal letter that encourages and uplifts people, one that tells people to keep doing what they’re doing! Surprise someone you appreciate by sending them a Continue and Persist Letter. revscat 18 hours agoAs someone who is coming to increasingly believe that 99% of the world problems are caused by powers seeking to intentionally divide us one from the other, I cannot tell you how much joy this brings me to see. I wish you great success in this effort, however small it might appear to be in the scheme of things. reply ldoughty 16 hours agoparentI agree so much, but the pessimist in me says that this isn't _really_ free, and I don't want to plug in a cool person's info without knowing who the info is being sold to. If this was publicly available document so I could fill in the missing bits, I'd send a dozen of these tomorrow. reply jmml97 10 hours agorootparent> If this was publicly available document so I could fill in the missing bits, I'd send a dozen of these tomorrow. It is: you can click on 'WRITE YOUR OWN CONTINUE AND PERSIST HERE!'[1] to be sent to a Google Doc with the template. [1]: https://docs.google.com/document/d/1_Um9z87Zv_zLRZRRHpii-Mwm... reply cootsnuck 14 hours agorootparentprev> I don't want to plug in a cool person's info without knowing who the info is being sold to. Have we reached peak data privacy paranoia? Harmless lil projects that harken back to the good ol' days of the internet are somehow actually devious PII honeypots? Why do people think their data is so valuable on its own without being connected to their actual consumer related behavior? Truly, what is a name and address worth vs. anonymous user on smart TV id_8z6748dxzh watched 3 hours of Hoarders on Amazon Prime, skipped 85% of ads, but did not skip 50% of ads relating to early onset male pattern baldness, and resides in Ohio? We somehow both overestimate and underestimate the value of our personal data. Which leads to unwarranted paranoia in inappropriate contexts and alarming indifference in the most common but mundane contexts. reply kynetic 12 hours agorootparentThe privacy policy on the website specifically states that they are collecting the PII and may use it to offer products or services, either by themselves or via a \"business partner\". It isn't paranoia when the threat is real. > We may share Your information with Our business partners to offer You certain products, services or promotions. > To provide You with news, special offers and general information about other goods, services and events which we offer that are similar to those that you have already purchased or enquired about unless You have opted not to receive such information. reply walterbell 9 hours agorootparentprev> Harmless lil projects Currently: a letter choosing formal legal vocabulary (/s) to create social network metadata, endorse human activity (\"you'd like the recipient to continue doing\"), disclose someone else's physical address and record the interaction in the national postal system. Future, https://continueandpersist.org/terms-of-service-privacy-poli... We may share Your personal information in the following situations: For business transfers: We may share or transfer Your personal information in connection with, or during negotiations of, any merger, sale of Company assets, financing, or acquisition of all or a portion of Our business to another company. With business partners: We may share Your information with Our business partners to offer You certain products, services or promotions. reply davidron 13 hours agorootparentprevThe name and address is valuable because it can be matched to offline behavior through a bill you pay or rewards membership you are enrolled in to further enrich the data associated with id_8z6748dxzh and combine it with your shopping history at Macy's and Safeway, for instance. This is even more valuable when combined with your cellular bill. I've work in ad tech,and with CDPs for nearly 20 years. reply brookst 3 hours agorootparentWhy wouldn’t they just buy name and address in bulk from one of the many providers that has essentially all of our info? reply heysammy 1 hour agorootparentIt's easier to target Hackernewses by advertising on their own site. reply BubbleRings 5 hours agorootparentprevI have to agree with cootsnuck here. If you are one of the people that found it necessary to raise red flags in these comments about this web site, here’s why I think you got your level of paranoia wrong in this case: 1. The site was never going to scale. The guys are printing physical letters and hand-inserting them in envelopes with stamps, for free! 2. So some entrepreneurial folks on the internet have gathered a hundred physical addresses, and they know a self-reported name and IP address, and maybe some persistent cookie info about a human that might be the first person’s friend. So what? Go bring up https://www.beenverified.com You can gather more info there in 15 minutes than the Continue and Persist guys will get over their whole project. 3. Learn to recognize a fun project that was done out of kindness and a spirit of adventure! Yeah maybe they should have not put up the language of “we get to sell your data if somebody offers to buy our web site”. But so what! The whole thing is just a kind adventure that brought a smile to the faces of some strangers, and will never be more than that. I appreciate it! reply brookst 3 hours agorootparent100% As insidious as data harvesting is, I am even less of a fan of the pearl clutching / performative cynicism that is so popular these days. Why yes, when I say “good morning” to the barista who hands my my coffee, it is possible that the shop is recording me and will use my voice in an elaborate voice cloning scam to get grandma to transfer her life savings to Nigeria. But breathlessly alerting me to this impending disaster and soberly advising me to never use my voice in public is not going to impress me. reply krisoft 1 hour agorootparentprev> The guys are printing physical letters and hand-inserting them in envelopes with stamps These things can be done at massive scales cheaply. > for free! That increases my level of concern, not decreases. > Yeah maybe they should have not put up the language of “we get to sell your data if somebody offers to buy our web site”. Maybe? What the heck do they need the data for a second after they sent the envelope? > But so what! So they don’t get my friends addresses. So that. > The whole thing is just a kind adventure that brought a smile to the faces of some strangers, and will never be more than that. You say that. But that is at contention here. reply heysammy 1 hour agorootparentprevI can't think of a better example of gaslighting, wow. reply handfuloflight 13 hours agorootparentprevThis specific service will know who your friends are. And if enough people used it... they might have to reincorporate in California. reply neilv 11 hours agorootparentprev> Have we reached peak data privacy paranoia? Harmless lil projects that harken back to the good ol' days of the internet are somehow actually devious PII honeypots? Safe assumptions with most any \"tech\" industry company or individual now are that they will behave completely like sociopaths when it comes to personal data. It's so baked into \"tech\" culture now, even people who may be ethically inclined don't recognize it as a problem. So I object to blaming the victim, or gaslighting, suggesting that people who are aware of this crisis of widespread antisocial behavior are being paranoid. reply brookst 3 hours agorootparentI’m old enough to remember the white pages, where essentially everyone’s name, phone number, and address were published and distributed. But it’s hard to have a conversation about appropriate calibration of what is private, and what are reasonable expectations, when extremists from both pro-privacy and scorched-earth commerce are so strident. reply heysammy 1 hour agorootparentIt was a lot harder to SWAT people when the White Pages existed. American society was much higher-trust then; people didn't show up to pizza parlors with assault rifles because of something they read online. You'll have to forgive those of us who simply want to remain safe. reply neilv 3 hours agorootparentprevMafia Enforcer: \"It's hard to have a conversation about what's an appropriate level of protection racket, when you're always screaming about your kneecaps being smashed. I'm reaching out to you to talk, but I just can't take you seriously when you're behaving like this.\" reply dudeinjapan 6 hours agorootparentprevIt’s not paranoia if they are really out to get you. reply FollowingTheDao 6 hours agorootparentprevIt is not only the value of my personal data, but it is the fact that they are hiding their intent behind some feel good wishy washy do nothing campaign that will only lead to more pollution and CO2 being released for the sake of makeing money without actually producing anything. What we have reached is peak neoliberalism. reply adnanaga 14 hours agorootparentprevit truly is free hahah we're using some free survey website plan and then Mark would fill in a template in google docs, print, and post the official letter. I think we hit our limit on the form though. We didn't expect it to blow up! Mark is gonna kill me when he checks his email reply walterbell 8 hours agorootparent> some free survey website plan What's their privacy policy? reply adnanaga 8 hours agorootparenthttps://www.jotform.com/privacy/ reply richardw 13 hours agorootparentprev“Yeah we pivoted to hiring and blackmail only later. Killer skills went to hiring, dodgy office romances went to blackmail. Profits are excellent.” reply nolok 10 hours agorootparentWhile true, it doesn't even have to be a pivot. It can be a sale of asset (volontary or necessary), an acquihire where the project and data just become some old vault to be disposed of and dozens other scenario. I'm sure no one at Yahoo actually thought or had any plan for all of that personal data to be data mined wholesale by Verizon a decade later. And nobody at Google right now believes all that data could be sold to Comcast or whatever. But things happen and then... reply hackernewds 15 hours agorootparentprevsend one to temp mail then copy it. if the privacy policy weren't so lax I'd not share the cynicism with you, but it state it'll be collected and shared reply nicbou 9 hours agoparentprevGratitude goes a long way. My job puts me on the receiving end of a lot of gratitude, and I absolutely adore it. It's one of the main perks of the job. After experiencing it, I have become a lot more lavish in my praise, and frequently email people to thank them for their work. Most of them are very appreciative of it, especially when they make their work freely available. reply skeeter2020 4 hours agorootparent>> I’ll contact someone if I loved their book or music or work. I’ll find a way to contact the author or the creator and say, “Holy shit. That was brilliant. I loved it. This was amazing. Count me as a fan.” >> I’ve done this about 20 times. I never got a response only once or twice, but the rest responded warmly. Many of these people have turned into friends. We actually call each other on the phone and shoot the shit. A couple of them have even turned into best friends. more here: https://sive.rs/2020-04-email reply _kb 3 hours agorootparentHighly recommend any work that has a non-trivial creation/publishing threshold, regardless of medium or domain. To tie the back to a HN context - this works great for academic papers. I've had some great email exchanges from original authors including a few people I've absolutely idolised. Turns out they're human too. reply thunky 3 hours agorootparentprev> Gratitude goes a long way. It sure does. And lack of gratitude goes a long way in the wrong direction. Big time demotivator. reply 127 7 hours agoparentprevNah, 99% of the world problems is caused by fraud. reply brookst 3 hours agorootparentAnd 1% by dubious statistics? reply bargainbot3k 17 hours agoparentprevkarma farming plays right into their game, too. reply rfl890 16 hours agorootparentnext [14 more] [flagged] conjectures 9 hours agorootparentIf you click on your profile name (shown top right) you can check your 'karma' score. reply BolexNOLA 15 hours agorootparentprevNo need to be patronizing. You clearly know what they mean, it’s a term that I’ve definitely seen beyond Reddit. reply rfl890 5 hours agorootparentthe only place where people care enough about imaginary internet points to comment \"stop karma farming\" is reddit... reply BolexNOLA 4 minutes agorootparent\"karma farming\" is just another way of saying \"playing to the base\" on any site. No one actually thinks the points here are worth anything or worth tracking. hackernewds 15 hours agorootparentprevkarma is definitely a reddit term. but besides that, nothing wrong with using reddit reply com2kid 14 hours agorootparentIt started out on slashdot, the forum that invented the entire concept back in the late 90s. reply nine_k 14 hours agorootparentIt's still around, and looks alive. I remember when Slashdot introducing the karma cap, far below my then extant karma. It mostly made karma farming irrelevant. I also like their mod points plus meta moderation approach. reply nl 14 hours agorootparentSlashdot has a karma cap now?... (goes and logs in for the first time in maybe 10 years) How do you even see your karma now? Is having a 4-digit UID from before the new 1998(?) green look still cool? reply seb1204 13 hours agorootparentIce cool reply fuzztester 13 hours agorootparentprevHN user pg, aka Paul Graham, the (co-)founder of both this site, i.e. HN, and YC itself, the \"parent\" site, was probably the guy (or one of his early colleagues) who implemented the karma feature here on HN. I am saying this because the feature has been there from pretty early on. No need to believe me, go verify it yourself, by any means you choose. And see: https://en.m.wikipedia.org/wiki/Karma https://commons.m.wikimedia.org/wiki/File:EndlessKnot03d.png reply Two4 15 hours agorootparentprevkarma is a term _used_ by reddit, but it definitely isn't exclusively used there. Outside of its hindu origin, it's used by quite a few forums that track user submission/comment votes as a user score, including hackernews. reply chrisweekly 14 hours agorootparentYeah, it's literally the term used for \"points\" of social capital in places like HN and Slashdot before it. reply BolexNOLA 5 hours agorootparentprevOh there is definitely something wrong with using Reddit after their API fiasco revealed how much disdain they have for their own users and moderators, but that’s besides the point reply omoikane 15 hours agoprev> A official-looking legal letter [...] Surprise someone Having received actual legal letters before, my personal experience is that I would much rather receive normal looking holiday cards to encourage me in continuing whatever I was doing, rather than being surprised by official-looking letters. It might be different if this service offered help in other ways, but currently it seems like a practical joke of dubious taste. reply growt 12 hours agoparentI agree. I would rephrase it „send someone you appreciate and that you wish had a mild heart attack this official looking letter“ reply egorfine 7 hours agoparentprevDepends. I would be spooked and frightened but then I would be delighted. reply cubefox 4 hours agorootparentIt reminds me of jokes where the inoffensive ones tend to be less funny than the borderline too offensive ones. Where you think: I shouldn't be laughing about that. reply Tempest1981 14 hours agoparentprevYeah, rather dark lawyer humor, I guess. No thanks. reply krisoft 9 hours agorootparentYeah. It is one of those kind of things where one might think that the recipient will first carefully read the letter and then form an opinion on it. But in reality people will form an emotional response just based on the appearance of it. And even after they read it they might be left perplexed and some might spend further resources to figure out if this is a legal threat in some way. I would find such a letter in incredibly bad taste, and think of whoever sent it to me or caused it to be sent worse than before. reply technothrasher 6 hours agorootparentIt really depends upon you knowing the recipient. I've got a friend with whom, over the years, I've exchanged many a \"legal\" demand letter back and forth. It's a running joke that we have, and it's pretty fun and funny. But, yeah, there are plenty of other friends who I would terrify if I sent them something official looking. reply albumen 1 hour agorootparentNo, it doesn't rely on you knowing the recipient. If I got this letter from someone I didn't know, I would enjoy it. Is it worth the risk that someone would find it \"incredibly bad taste\" (parent post, not yours)? Obviously humour is subjective, but perhaps such people need to have their Sense of Humour shifted, Overton-window style. reply krisoft 30 minutes agorootparent> No, it doesn't rely on you knowing the recipient. The point is not that you need to know the recipient, but that if you don’t know them you can’t say if they will enjoy this kind of humour or not. I know people for whom such a letter would be the highlight of their day. I know people who would be worried about it for days if not weeks. Without knowing your audience you can’t know who will react how. > Is it worth the risk that someone would find it \"incredibly bad taste\" (parent post, not yours)? Is it? Depends on how much you care how much you care about being an asshole accidentally. I have friends who we play pranks on each other where we sneak up on the other and do a jumpscare. Would never do the same with a random person though. This is the equivalent action, but instead of threatening people with bodily harm it is threatening them in a legal/bureaucratic way. > Obviously humour is subjective Yes. > perhaps such people need to have their Sense of Humour shifted, Overton-window style Or maybe your sense of empathy needs adjusting. The problem is not with their sense of humour but with their sense of “how likely it is that an official looking letter is bringing ruin on me”. I’m glad that you feel secure enough that you can’t even understand that feeling. reply DavidSJ 15 hours agoprevOn a few occasions I’ve called the phone number on a professional vehicle’s “how’s my driving?” bumper sticker to report the driver for good driving. It’s always a fun experience. reply unsupp0rted 5 minutes agoparentI've tried similar things and the person on the other end isn't interested. I'm bothering / interrupting them by trying to call in and praise someone. They're not being incentivized to take praise calls. reply sharkweek 14 hours agoparentprevAmazing idea. I’m sure the person on the line is… not used to this. I remember one time having a super helpful person at Home Depot walk me through a home project I needed some advice on so I asked to talk to their manager to compliment/thank them. After a few minutes the manager came over and looked so ready for me to berate them about something only to look incredibly shocked that I was actually complimenting the service I had just received. Was nice for everyone. reply mdorazio 14 hours agorootparentI’ve done this a few times in the past as well. But I found it’s less stressful for everyone if you preface it first like, “You’ve been really helpful. Can I tell your manager or mention you in a survey or something?” (Note: direct managers at stores don’t always have the ability to do much to reward good employees and it might be more helpful to fill out a survey or something that corporate sees.) reply foobarian 1 hour agorootparentI would be worried about angering the manager if they learned that this super-helpful employee spent half an hour chit-chatting with a customer instead of doing other tasks. Kinda like how agents are under time pressure in call centers. reply technothrasher 6 hours agorootparentprevThat's quite nice of you, though I don't think it is as rare as you suspect. My son works retail as a supervisor and his report is that, while complaints are the main reason customers escalate to him, he does reasonably frequently also get customers who want to compliment an employee. reply Pikamander2 8 hours agoparentprevI've thought about doing that kind of thing but always worry that it might end up hurting them if the company has some kind of binary \"number of complaint line phone calls\" metric for each employee. I've heard too many horror stories about employees/stores getting punished when you give them 4 out of 5 stars since (according to upper management) it implies that something was less than perfect. reply j5r5myk 7 hours agorootparentI once hit one star by accident so I called back and navigated multiple transfers back to the same call center's manager to let them know it was intended to be five stars. Everyone I spoke to seemed quite charmed as it had never happened before. reply RyJones 4 hours agoparentprevI’ve done that in bad weather. The professional drivers will keep the speed over the passes. I love reporting this. I’ve seen so many people drive in the ice and snow like it’s dry pavement and end up crashed. reply nicbou 9 hours agoparentprevI often give drivers a thumbs up when they are careful around bikes. German drivers are especially wary of cyclists, and I'm often surprised by how constantly they look out for me. reply lnsru 7 hours agorootparentBe careful there! You need only one idiot to become a meat in a wheelchair. It’s my personal data point. Luckily it was only very close call. I can walk, run and dance again. reply nicbou 6 hours agorootparentWhich is why I am grateful for decent drivers. There’s a reason I don’t ride motorcycles anymore though! reply croisillon 8 hours agoparentprevI once called the \"send us your ideas\" feedback hotline of a highway station, suggesting this and that. The person on the phone was like \"ok... but do you have complains? why are you even suggesting ideas?\" reply latexr 8 hours agorootparent— Alright, I’d like to also lodge a complaint. — Now we’re talking. What is it? — I want to complain you’re not taking my suggestions seriously. reply teaearlgraycold 3 hours agoparentprevWhen working with technicians I like to tell them and their manager that they’re doing a great job, 5/5 stars. I usually also throw in that whatever they’re being paid, it’s not enough. reply stogot 15 hours agoparentprevI should start doing this. Fine idea reply DonHopkins 12 hours agoparentprevThe customer service people at 1-800-EAT-SHIT laughed at me when I did that. Then they sold me some of my own \"Don't like my driving? Call 1-800-EAT-SHIT!\" bumper stickers. reply bragr 18 hours agoprevThis reminds me of how my father (a lawyer) sent fake subpoenas to invite people to his brother's bachelor party. Most found it hilarious but it freaked a few people out. I suspect that this would have the same problem. reply sharkweek 14 hours agoparentI remember in college (early 00s when music piracy was huge) someone set up a site where you could enter an email and it’d send them a fake letter from the RIAA about the downloading they were doing. Sent one to my college roommate and sat quietly while he got home from class and checked his email. He obviously started panicking until the link in the email revealed the joke. He didn’t think it was funny and swore cold blooded revenge that I’ve yet to receive. Still keep one eye open at night. reply latexr 8 hours agorootparent> swore cold blooded revenge that I’ve yet to receive. Still keep one eye open at night. You’re receiving it right now. It’s the uncertainty, the uneasiness you have to carry with you every day, that is the revenge. reply costcofries 14 hours agoparentprevnext [7 more] [flagged] rsanek 13 hours agorootparenteven if that were true, do you want to be the person to trigger that for them? i don't. in general i think the issue is simpler tho. comedy is just subjective, not everyone enjoys the same stuff reply throw4839355 11 hours agorootparentprevNah, for normal people, getting a legal looking letter is nerve wracking. reply anxiety2024 9 hours agorootparentprevHowdy. I'm someone who's been diagnosed with Generalized Anxiety Disorder. I have panic attacks somewhat regularly, I take medication, I've done therapy in the past, etc. Just to clear up what I perceive as a misconception - it almost sounds like you're suggesting people prone to anxiety have neglected to reflect on that, like they're just being lazy about it out something. Buddy, I know I have anxiety. I know about the traumatic experiences I had in the past, and my family history, that contribute to my condition. I've reflected on it plenty. \"Working through it\" doesn't do what you think it does. It's not a cure. What you learn to do is understand the patterns of thought you get into, to recognize when you're falling into an unhealthy pattern, and tools that you can deploy to manage that situation. It's more like managing hazardous waste than flicking a light switch. I wish it were otherwise! reply dmix 13 hours agorootparentprevDepends what someone does for a living. If you deal with lawyers often you might have more emotional baggage seeing one. reply lukan 11 hours agorootparentAnd if you don't have much experience with lawyer letters, you have heard enough to want to avoid them usually. reply TeMPOraL 10 hours agorootparentprevNot really. For most people, nothing good ever comes with such letters. reply NortySpock 3 hours agoprevI have found postcards to be a great way to send encouragement, sympathy notes, or thank-you notes. The small space on the post card for writing means you can usually only fit a few sentences in (keeping the time investment short) and you don't even need to fold up the letter into an envelope. You can start with a few post cards bought from a gift shop, and if you like the habit, you can get themed boxes of 100 postcards from an online retailer for $20. (e.g. birds, or cats, or Disney Princesses). reply rietta 15 hours agoprevAs a business owner, I can tell you certain letters have a look to them before opened and can trigger anxiety and “heart skip a beat.” A letter looking like its from a law firm is going to trigger concern and even calling an attorney. Not fun to then have the rug pulled out. Reminds me of the terrible pranksters who worry people and then say “it’s just a prank.” reply sharkweek 14 hours agoparentWhen I owned a small business every letter from the IRS, no matter how benign, was nightmare inducing. Never got anything serious from them as most were “we’ve update some random piece of info on your profile” notifications, but still made me nervous every time. reply rietta 6 hours agorootparentSame! I regularly get address change notifications from the IRS in duplicate despite nothing visible changing about the address. reply Ylpertnodi 8 hours agorootparentprevWouldn't anything really, really legal be recorded delivery? reply skeeter2020 4 hours agorootparentfrom experience - no. Requests for more info (the precursors), reassessment notices (the nightmares), bills (the consequences) reply UniverseHacker 14 hours agoparentprevReally? I think most people get plenty of benign letters from law firms. I do for things like consumer class action lawsuits I’m eligible to join, and stuff related to getting paid for things I invented…. reply lukan 11 hours agorootparentI am pretty sure you are a outlier. \"stuff related to getting paid for things I invented….\" reply sph 5 hours agoprevPro tip: Send it to Nintendo with a picture of Mario to receive a Cease and Desist letter in exchange. reply usmanmehmood55 5 hours agoparentReal reply thepuppet33r 19 hours agoprevIt's a clever concept, but I'm leery of having someone send a letter on my behalf to someone. You all haven't trademarked the idea, have you? I think I'll just write a few of these up myself and send them out. reply jagged-chisel 19 hours agoparent> I'm leery of having someone send a letter on my behalf to someone. Agreed. I don’t know what other junk mail will be included, nor how my recipient’s data will be used. reply CamelCaseName 18 hours agorootparentor... maybe this is just a fun side project they whipped up as a joke? Some side project for a resume? Just look at OP's comment history, one example: > I absolutely love making chrome extensions - my very first one was creating an extension for tinder.com that would swipe left on anyone with the first name Chad All they require here is your name and email, and the recipient's name and address. Both are (probably) public somewhere and in combination are almost certainly still worthless (or certainly worth less than the postage they're giving to you for free) reply collingreen 17 hours agorootparentWhy not just make a pdf or a copy paste version? I get suspicious when extra work is done to make worse ux; my assumption is that it rarely happens without a hidden incentive. reply adnanaga 13 hours agorootparentOP here - We just wanted to make it easier on folks to send out! We just posted the template on the site now if people want to write their own! reply kijin 17 hours agorootparentprevI think part of the package is the official-looking envelope from the LLP, transparent window and all. It won't have the same effect if you just download a PDF, print it out, and mail it in a generic envelope with a handwritten address on it. reply collingreen 14 hours agorootparentFair enough, although I'd love to see that as an upsell instead. reply krisoft 9 hours agorootparentI'm not sure if they changed it after your comment, but right now there is clearly two options. One where you can get a google doc and just use theirs as a template, and one where they send it for you. reply adnanaga 9 hours agorootparentYes! We added it in later so people could make their own! reply collingreen 36 minutes agorootparentnext [–]We may share Your information with Our business partners to offer You certain products, services or promotions. Which somewhat takes the shine off this in my opinion. (That, and also: How hard is it to post a letter yourself?) reply FloorEgg 19 hours agoparentprevTrademarked the idea? I don't think that word means what you think it means... I'm not OP, and I'm no lawyer, but I'm sure you're free to try this concept for yourself. Three ways to protect IP Trademark is for logos and names Parents are for methods and inventions Copyright is for works of art and writing They might be able to trademark the phrase \"continue and persist\", but it's not likely. If they had they would have put a little TM or (r) next to it. It's not sufficiently original to qualify for copyright protection. And it's so far from being patentable I hope I don't have to explain why. reply jagged-chisel 19 hours agorootparentIn the US: - a trademark may be claimed with TM even if not registered. A trademark registered with, and accepted by, USPTO should use circle R. - these folks hold the copyright on their specific letter. Avoid infringement (and potential negative legal ramifications) by not reproducing their letter. This is not limited to photocopying a physical paper. Retyping verbatim will infringe. - the concept itself cannot be protected with existing (US) intellectual property law. reply cyanmagenta 18 hours agorootparentTo complete the legal analysis a bit more rigorously, mostly because I’m drunk and bored on Thanksgiving: - The service name “continue and persist” is probably trademarkable by them if they wanted (assuming it’s not already in use). And as you point out, formal registration wouldn’t be necessary, although it does come with various benefits. Doing so could block someone from creating a similar service with the same or confusingly similar name. - For copyright, reproduction doesn’t have to be verbatim to infringe. The standard is typically substantial similarity. So you couldn’t just change a few words here or there in the letter. - Publicly disclosed concepts can be protected under US law, but you have to go through patent law. Haven’t done a prior art search, but seems unlikely that there’s much patentable here. There is also the section 101 (abstract idea) issue, but that is hard to evaluate without looking at the exact patent claims at issue. reply thayne 15 hours agorootparentprev> And it's so far from being patentable I hope I don't have to explain why. I would say the same thing about a lot of things the USPTO has handed out patents for. At this point, it wouldn't surprise me if someone could obtain a patent for this, if the patent application was written the right way. Whether such a patent would hold up in court is an entirely different question. reply heysammy 1 hour agoprevWhat's stopping you from just... sending a letter or a postcard? Benefits: 1. It comes from your heart. 2. You don't leak PII to internet randos. 3. It shows you put some thought into it. 4. Writing letters is fun, and the post office needs all the help and support it can get. While you're at it, why don't you give THOSE folks some praise, too? 5. A personalized letter doesn't look like something official and frightening. Seriously, you're underestimating how many people this is going to unnecessarily scare, at a time when our institutions are failing more and more. Maybe we don't need to automate away every last bit of being human. reply royal__ 19 hours agoprevCool idea! It sounds like you're offering it for free, with the option for a donation. How are you going to deal with the influx of what I imagine will be hundreds of free requests? Won't that get pretty expensive fast? reply MichaelCharles 17 hours agoparentThe pessimist in me says this is the regular old \"If it's free, then you're the product\" situation. This seems like a clever way to collect valid mailing addresses. People are also likely to include personal information in their praise messages, which could be valuable data. Their Terms and Conditions and Privacy Policy state they reserve the right to share collected information with service providers, business partners, and affiliates. They can use your data for \"other purposes\" including \"data analysis\" and \"identifying trends.\" They can share your information with \"business partners to offer you certain products, services or promotions.\" reply frickinLasers 15 hours agorootparentThe terms look like boilerplate that only addresses \"your\" data--not the information you enter about the target. If they end up selling the addresses/names/activities of unwilling participants, their \"don't sue us\" clause from the video may not hold up in court.* *ianal reply aoanevdus 15 hours agorootparentprevAren’t there data dumps freely available online with contact information for pretty much anyone? In that context, why would the data from this small project have any monetary value? reply cootsnuck 14 hours agorootparentIt wouldn't. This site just has a strong bias towards reactionary yum yuckers. reply adnanaga 13 hours agoparentprevOP here - There will be a lot we won’t be able to get to Hahahh but we want to try and send out as many as we can! reply edm0nd 2 hours agorootparentWell that's super lame. A USPS Forever Stamp is $0.73. Unless yall are rolling in VC funds or a lot of extra cash, a few hundred or even thousand orders is going to nuke the entire idea. Are you still going to harvest and use all of the collected data of people who never got anything mailed? reply qingcharles 17 hours agoprevI tried to donate the $3 but Ko-Fi won't let you donate less than $5. I'm trying to donate the $5 but the blue Pay Now button does literally nothing. Will try again later :) Please continue. reply gcr 17 hours agoparentSame. It seems like the Ko-Fi platform won't let me select any payment method. Thank you for running this service! I'll pay it forward when things recover. reply darnfish 17 hours agoparentprevContinue and…? reply hunter2_ 15 hours agorootparentnext [3 more] [flagged] qingcharles 15 hours agorootparentwut reply bigallen 14 hours agorootparentIt’s a play on words. It helps to break it down into syllables. Each group answers one of the patient’s questions Cont In ue Per sist reply ChilledTonic 18 hours agoprevIt would be nice to have a pdf copy of the letter downloadable for those who are outside of the USA. reply adnanaga 13 hours agoparentJust added it to the site! reply 0x1ceb00da 16 hours agoparentprevThat's their moat. reply remram 1 hour agoprevI don't think I would enjoy receiving one of these. It looks scary, and the PII side of things is creepy. If you like my work, say so on social media. That does way more for me than this novelty panic-inducing letter. reply g4zj 4 hours agoprevNo thanks. Receiving an \"official-looking\" letter would only make me anxious. reply davely 16 hours agoprevLove the idea! But! I don't love the idea of sending PII for friends and family to an unknown source in order to facilitate this. You can ask your friendly neighborhood LLM for a template and go to town! \"I'd like to create a positive spin on \"Cease & Desist\" letters for the holidays that I could send to friends. Let's call them a \"Continue & Persist\" letter. How can I make something cute and uplifting that I could send to friends for the holidays. I guess I need some personalization options like name, and one thing I'd like them to continue doing (optional). Can you help me create an official looking, but fun form letter that I can fill in the send to people? Thank you!\" --- [Your Name or Family Name] Department of Holiday Joy & Encouragement [Your Address or City, if desired] [Date] OFFICIAL NOTICE Re: Continue & Persist Order Dear [Recipient's Name], In the spirit of the holidays and in recognition of your outstanding contributions to the world, the Department of Holiday Joy & Encouragement has issued this Continue & Persist order for you. This order officially requires you to continue being your amazing self and persist in spreading joy and kindness throughout the year. Please note the following specifics of your order: 1. You must continue to [something wonderful they do, e.g., make everyone laugh, inspire others, or bake the best cookies]. Your efforts in this area have been truly exceptional and have brought countless smiles. 2. Under no circumstances are you to cease being the incredible [friend/parent/artist/etc.] that you are. 3. This order is valid indefinitely and cannot be revoked (even if you try to protest with modesty). Failure to comply with this order will result in an overflow of love, gratitude, and probably more letters like this one. We at the Department of Holiday Joy & Encouragement commend your contributions to making life brighter and sincerely hope you have the happiest of holidays. Warm regards, [Your Name or Family Name] Director of Holiday Cheer reply pvaldes 1 hour agoparent\"...And here is a magnet pin with the logo of the company, to fix this letter on your fridge at home if you want, so your children and partner can read it also\". That would be chef kiss reply dmurray 10 hours agoparentprevThis is pretty good! An LLM is actually the right tool to generate this kind of copy that demands a given style but is ultimately soulless. reply mvkel 15 hours agoprev\"A happy person tells one; an unhappy person tells nine.\" Little efforts like this help even things out reply TripleChecker 5 hours agoprevHaving received a C&D letter or two in my day, I can still remember the unpleasant feeling of getting one. This would be a pleasant surprise, once the user recovers from thinking it was a cease and desist :) Also, 1 typo to fix ('acknoledgement' on homepage: https://triplechecker.com/s/771583/continueandpersist.org) reply borski 13 hours agoprevThis is brilliant. This is one of the best things I’ve seen all year. My father was diagnosed with liver cancer about a month ago, and I have had a rough time of it since. I needed this positivity in my life. Thank you for reminding me of the good in the world; sometimes we all need a dose of that. If I can buy you guys beers, please don’t hesitate to reach out. I’d love to help build some future boondoggles. :) Email is in my profile. (The irony of my domain being owlpost.lol is not lost on me haha) reply oneeyedpigeon 9 hours agoprevWhat a brilliant idea! It's a shame this is USA-only - would be nice to get that added to the HN title. I can see why they would have this restriction, but there IS a payment option, so they could offer it with whatever feels is required. Still, all the best US cousins! reply aszantu 18 hours agoprevif it's free, you're the product, better yet, you're giving away your friends and families adresses for free. I like the idea though^^ might send a few on my own reply 7373737373 17 hours agoprevLove the idea, it's similar to the formal letters of http://www.bureauofcommunication.com/ reply tomashubelbauer 7 hours agoprevI think something like Keep and Persist would roll off the tongue better, though it is not perfect English. I can't think of another verb that would still rhyme and fit better. Overall well meant idea, but I echo others sentiment that this could really freak some people out. I'm trying to decide whether I'm in that group or not. Probably yeah. reply Towaway69 7 hours agoprevSomething similar that I once created was https://spread-the.love which is a service that sends a random person somewhere on the planet a positive SMS. Of course the SMS is discontinued because of costs but the idea still exists. reply gwd 9 hours agoprevMake sure and watch the video -- excellent satire on the \"local ambulance chaser ad\". These guys are having a lot of fun. reply aetherspawn 20 hours agoprevI think the rules are you have to put a in the title when you self post reply dec0dedab0de 19 hours agoparentI don’t think that’s a rule, but it does give you a better chance of being noticed because it goes on the show list. reply mintplant 18 hours agorootparentThey're also downranked a little, IIRC. reply dang 14 hours agorootparentShow HNs aren't downranked. reply ocal5 20 hours agoparentprevYes, but, they admit they’re not lawyers :’ ) reply callumprentice 8 hours agoprevhttps://cyberlaw.stanford.edu/blog/2007/03/second-lifes-resp... Our legal dept (person) did this in 2007 and it made me so happy. reply jakubsuchy 6 hours agoprevOne often overlooked aspect of owning a trademark : once you have it, you have to defend it. Not a surprise David Chang's lawyers are trying to enforce it. They have to reply indus 12 hours agoprevAs a startup founder, my journey is super lonely. There is seldom a flutter of thanks—-rarely does anyone come to me saying, “Splendid!” Yet, the day continues, operated by a calendar. The lack of thanks does not make me anxiouz, but I continue on my journey because I signed up—to create. Create something out of thin air, something out of my figment of imagination. But there are moments where I am vulnerable, alone. In those moments, I look for a post-it where it says, “Continue and Persist.” Thanks for creating this. I’ll wait for this. Maybe someone, someday. reply spuds 14 hours agoprevI have to say I'd be absolutely delighted to receive a letter like this. Not just the indicator that I'm doing something right (already huge for me, given the degree to which I struggle with people pleasing), but also the humor and extra care/time involved. Feels like it would strike enough of the right nerves to be quite moving even. Definitely agreeing with others that we could use more of this in the world (well, I could definitely use more of it, at least.) reply brna-2 7 hours agoprev@adnanaga the e-mail you specified for contact on the site does not work: continue@boondoggle.io gmails says it is miss-configured. I was wondering for EU, could I get the document template. reply adnanaga 7 hours agoparentYes! It's on the site too - https://docs.google.com/document/d/1_Um9z87Zv_zLRZRRHpii-Mwm... reply theendisney 17 hours agoprevLong long ago i and friends would smoke a ton of weed and write letters that we never send. They went something like... Dear sir, you must feel privilaged to receave a letter from someone as important and distingushed as me. It was well earned and deserved i assure you (which is the best kind of assurance) It of course pains me greatly to see my greatness overshadowed but there is joy in being reduced to an observer in awe..... Ill let myself out reply GregTravel 17 hours agoparentJuding by the content you were not far from the truth... :-) reply Ylpertnodi 8 hours agoparentprev\"Long ago....\". reply nashashmi 5 hours agoprevAnd if you don’t contine and persist you will be in violation of the order. reply mattclarkdotnet 14 hours agoprevVery funny, completely American, mostly unrelatable for the rest of the world. Please continue and persist. reply ghusto 12 hours agoparentI don't mind America-only stuff — I think countries should have _more_ country specific stuff! — but it's rather frustrating when you only find out half way through the process of signing up / filling out a form. The presumption that all English speaking people are American is tiresome. On the other hand, I can't blame people for it either, since it's equally tiresome to be world-inclusive by saying \"This is for an northern American audience\" at the top of each website that requires it. If only there were some way to un-obnoxiously denote this ...TLDs... reply infinitedata 7 hours agoprevRegarding data privacy, but from a different angle, here you are not getting the consent from the actual person receiving the letter, you are getting the consent from someone else, does that allow you to use their information when the consent is not coming from the affected party? reply anandbaburajan 12 hours agoprevThank you for creating this wholesome, weird thing! reply Raed667 9 hours agoprevThe video is a gem ! Love what you're doing ! Can I send you a Continue and Persist Letter? reply neom 19 hours agoprevCool that they have an office in the capital of the north east corner of the GTA. reply fuzztester 18 hours agoprevAchtung! Halten! Verboten! reply brna-2 9 hours agoprev@continueandpersist.org please continue and persist :) reply alabhyajindal 10 hours agoprevThis is amazing! The promo video is very funny! CONGRATS!! reply jonplackett 8 hours agoprevThis is just marvellous. Well done reply Koshkin 14 hours agoprevThe negation of ‘cease and desist’ is ‘continue or persist’. reply andytratt 15 hours agoprevstripe integration on ko-fi seems broken. i sent an email to ko-fi thru 2 different channels, with loom, cc'd boondoggle... hope everyone comes back to donate once it gets fixed. reply emptiestplace 18 hours agoprevHypothetically, if I had to violate someone's personal privacy like this, I think I'd be leaning toward doing it for someone I don't like. And even then, I would never. reply setr 17 hours agoparentHow is this a violation of personal privacy? Sending mail? Submitting name & address to a business, a readily available datapoint for probably anybody? reply davely 15 hours agorootparentA home address is considered PII (Personally Identifiable Information) because, when combined with a name, it directly identifies where someone lives. When people share their address with businesses, they are generally protected under laws like GDPR and CCPA. OP’s site seems a bit hosed at the moment so I can’t dig into their privacy policy and see how they handle GDPR and CCPA deletion requests. reply thayne 15 hours agorootparent> When people share their address with businesses, they are generally protected under laws like GDPR and CCPA. Only in the EU or California. reply DocTomoe 14 hours agorootparentThat's 500 million people (EU), 25 Million people (California), and of course you forget about all the other places that have PII data protection laws, like Virginia, Colorado, Connecticut, Utah, and increasingly more US states. Then, of course, there is Canada, the UK, Brazil, South Korea, Japan, Australia, New Zealand, India, South Africa, Singapore, Israel, all of which have GDPR-compatible laws or are quickly approaching it. I am no lawyer. Before collecting data, better talk to one. reply karlgkk 17 hours agoparentprev> violate someone's personal privacy Lmao what explain your math reply GlacierFox 16 hours agoparentprevWhat's going on here? What planet are you from to come up with a comment like this? Haha wtf. reply wslh 5 hours agoprevYou make my day! instantly brought to mind the many people I care about. It's one of the simplest yet most meaningful, and impactful ways to express respect, appreciation, influence, and encouragement to those closest to us. reply wouldbecouldbe 13 hours agoprevThis will freak some people out ;) reply DonHopkins 11 hours agoparentOnly if they're doing something they know they should cease and desist doing, which is why it's so fun! reply billpg 8 hours agoprevCan I opt out of receiving letters from this service please? reply FollowingTheDao 6 hours agoprevTwo words: Climate Change reply ossobuco 6 hours agoprevWhere's the usual hardcore libertarian complaining about the \"All This For Free!\" nomenclature? reply UniverseHacker 14 hours agoprevThis is an excellent idea! reply delichon 16 hours agoprevIllegitimi non carborundum. reply ecmascript 8 hours agoprevReally great idea, but why free? At least take like $5 or something. reply blitzar 4 hours agoprevI am more likely to use one of the wonderful shit-in-a-box services. reply lanewinfield 15 hours agoprevI love this so much. reply bongodongobob 11 hours agoprevSmells like a way to collect addresses to me. reply benatkin 14 hours agoprevThePersistence reply zoklet-enjoyer 13 hours agoprevIn 2024, Chang's company, Momofuku, drew criticism after sending cease and desist notices over the use of their trademark \"chili crunch.\"[65] In a statement to the Los Angeles Times, Momofuku argued that the trademark was not intended to stifle any competition surrounding the sauces, stating that \"When we created our product, we wanted a name we could own and intentionally picked 'Chili Crunch' to further differentiate it from the broader chili crisp category.\"[66] On April 12, 2024, Chang issued a public apology regarding the issue on his podcast, The Dave Chang Show, stating that Momofuku had not intended to upset with their legal actions, and noted that Momofuku would no longer attempt to enforce the trademark. https://en.m.wikipedia.org/wiki/David_Chang reply 71bw 11 hours agoparentHow does this relate to the topic? reply ayewo 10 hours agorootparent> How does this relate to the topic? The person you responded to is providing additional context regarding David Chang. His name might not be familiar to readers as someone who is famous for sending large volumes of cease and desist letters. He is mentioned in the very first sentence of this HN post: > Every day, thousands of Cease and Desist letters are issued, telling people to stop what they’re doing (Looking at you, David Chang). What a bummer! reply metalman 17 hours agoprevget agitated and change something reply DocTomoe 14 hours agoprevOh, wow. I mean, I love the sentiment. It's a cute idea. But something that on first look looks ike another legalese quagmire on my desk would raise my blood pressure way beyond what my doctor tells me is healthy. By the time I realised this was meant as an encouragement, the damage would already have been done. Maybe show appreciation in something that masquerades less as yet another fire to put out. reply yieldcrv 16 hours agoprevThis would freak me out but I like the concept reply romnon 18 hours agoprevUmm, proceed and persist sounds better imo. reply chrismorgan 4 hours agoparentOr “pursue and persist”. “Cease and desist” has nice assonance, I feel “pursue and persist” matches a little more in that regard, though both are decent. As regards meaning, “proceed” and “pursue” are both good, taking slightly different angles but to essentially the same purpose. reply xanderlewis 17 hours agoparentprevMaybe, but ‘continue’ is a better negation of ‘cease’. Proceed can mean (usually means?) to do something one has not yet started, rather than to not cease. reply asveikau 16 hours agorootparentBy the latin roots: Continue - hold together (cum + tenere -- same root as contain) Proceed - go forward (pro + cedere) reply ultrasounder 18 hours agoprev@adnanaga looking at you as a future AJ. And @chan you as the DOPE(Department of pentagon efficiency). I mean the \"bar\" is too low now. All you need to do is Pass the bar. Pun intended. Love it guys.I just posted a continue and persist letter to you guys. You guys made me smile and laugh. So Thank You on this Thanksgiving day reply Valid3840 16 hours agoprevfight milk! reply bastloing 5 hours agoprevWe need that here. So many down votes and flagged comments because one person might be offended because they don't understand the other person's perspective. So they choose to silence, shadow ban, or outright ban. Shame on you! reply 21 hours agoprev[deleted] hsuduebc2 15 hours agoprevDamn that's so wholesome. Thank you reply gala8y 6 hours agoprevIf I have a friend doing... stuff, he/she already has my support and they kinda know it, you know. This is superficial, 7 yrs old kid's approach. Plus, as others mentioned already - it's none of your business to meddle around with their or mine contact data. No, thanks. Real communication with real people for me. reply rini17 3 hours agoprev [–] Very good idea, except over the top execution might backfire with neurodivergent people. Such overt gratitude might be perceived as demand, and demands are prone to cause triggers in many types of neurodivergence. One's autonomy might feel threatened, in extreme cases leading to meldown. If you suspect this might be the case, please consider subtler ways to show gratitude. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [],
    "points": 1219,
    "commentCount": 209,
    "retryCount": 0,
    "time": 1732828615
  },
  {
    "id": 42269227,
    "title": "Engineers do not get to make startup mistakes when they build ledgers",
    "originLink": "https://news.alvaroduran.com/p/engineers-do-not-get-to-make-startup",
    "originBody": "Share this post The Payments Engineer Playbook Engineers Do Not Get To Make Startup Mistakes When They Build Ledgers Copy link Facebook Email Notes More Discover more from The Payments Engineer Playbook Deep dives on the stack behind the magic. Over 1,000 subscribers Subscribe Continue reading Sign in Engineers Do Not Get To Make Startup Mistakes When They Build Ledgers Practical accounting for fintech engineers, and also how not to repeat my silly mistakes. Alvaro Duran Sep 11, 2024 16 Share this post The Payments Engineer Playbook Engineers Do Not Get To Make Startup Mistakes When They Build Ledgers Copy link Facebook Email Notes More 4 Share Not losing track of money is the bare minimum for fintech companies. And yet, I used to work for a startup that, on every transaction, simply lost track of a couple of cents. As if they fell from our pockets every time we pulled out our wallets. At this startup, a stock trading platform, the engineering team had followed the mantra of “make it work, make it right, make it fast”; we refused to build a double-entry accounting system. If you’ve never worked at a startup, you might be shocked. But it’s perfectly normal. Engineers at startups have to buy time from wherever they can, to make room for the key design decisions. It’s just that…it goes without saying that fintech companies should know better. We could’ve taken the time to build it right. We could’ve done things better. But we didn’t. Soon after we launched, we started noticing those few cents. Our vendor acknowledged one amount, and we acknowledged an amount that was almost exact. Just a few cents away. We were full of excuses: numbers were almost right. They were only a few cents here and there. We called them dancing cents. Stories like this don’t get aired very often because they’re embarrassing. But I believe it happens all the time. The problem wasn’t the few cents. Startups are willing to burn way more than that if they can grow exponentially. The problem was that users were furious. When they bought $5 of Apple stock, and saw an order for $4.98, they instantly hit our customer support chat. Hell I am pissed off when I get charged a custodial fee on my brokerage account, and that’s a standard fee. Imagine if you got charged for no apparent reason! Furious users weren’t recommending us. Our startup wasn’t growing as a result. So what did we do? We bit the bullet. Our CEO ordered customer support to manually compensate those cents when a wrong transaction happened. We even built a Slack bot. On desperately trying to fix the dancing cents, I learned a lot. It was an intense course on accounting and engineering, at the highest stakes possible. And in this week’s article, I’ll tell you all the lessons I took away from it. I’m Alvaro Duran, and this is The Payments Engineer Playbook. Scroll for while on Youtube and you’ll find tons of tutorials that show you how to pass software design interviews that use payment systems. But there’s not much that teaches you how to build this critical software for real users and real money. The reason I know this is because I’ve built and maintained payment systems for almost ten years. I’ve been able to see all types of interesting conversations about what works and what doesn't behind closed doors. And I thought, “you know what? It’s time we have these conversations in public”. In The Payments Engineer Playbook, we investigate the technology that transfers money. All to help you become a smarter, more skillful and more successful payments engineer. And we do that by cutting off one sliver of it and extract tactics from it. Today’s post is about how to design a ledger. Ledgers are systems that track down money. But money is a pain in the ass to track down. Why? Sam Denby from Wendover Productions put it very eloquently: money is created when it moves. Most people don’t think too much about it. It’s one of those things that just works. But money is just a way to keep the score. Without an accounting system, that score is meaningless. Cash is only needed when you need to keep track of value you expect to get or to give in the future. Money, conceptually, is assets in the future. If you knew nothing about accounting, you’d probably start tracking down what you sell to your customers. This is what we did at the startup: this user paid $5, that user paid 6$, and so on. We naively thought that we only needed to monitor the money coming in, and going out. What a huge mistake. I bet that you’ve already noticed that bank transfers aren’t immediate. They’re pretty slow, by Internet standards. Most banks clear transfers on the next business day. Payments online are fast, but transfers aren’t. This became a headache for us. See, once a payment went through, we were certain that we were going to get some money someday. And we had to use that future money to buy shares on the stock market now. The way we were tracking money didn’t account for that. We had to have some pending amount somewhere that would clear a few days later, and another amount had to go out immediately to our broker to buy some shares. Rolling everything back when something went wrong was very difficult. In some corner cases, we didn’t even try. Single-entry systems make normal use cases very complex. That’s because money is the natural consequence of designing a double-entry system, and not the other way around. Think about it: It is impossible to decouple the need to record the promise of future assets from the amount that is owed to the owner of those assets. Not only is it money that makes debt possible: money and debt appear on the scene at exactly the same time. Some of the very first written documents that have come down to us are Mesopotamian tablets recording credits and debits, rations issued by temples, money owed for rent of temple lands, the value of each precisely specified in grain and silver. Some of the earliest works of moral philosophy, in turn, are reflections on what it means to imagine morality as debt-that is, in terms of money. — David Graeber, Debt: The First 5000 Years A double-entry system is an accounting method that tracks money at both its source and destination. There’s been many accounting for developers guides published already (like Beancount’s, Martin Kleppmans’s and Modern Treasury’s). In this article, I’ll get you up to speed, so that you can read those references later with much more context. I’ve included them at the end. But before I start, I want to highlight the importance of context when building ledgers. In The Most Important Book In Payments Is a Data Systems Book, I challenged the generalists: Those who believe that they are ready to tackle any kind of engineering problem, particularly money software, with generic engineering practices. Ledgers fly in the face of the generalists. Building a ledger correctly is: Straightforward in the beginning A business enabler down the line Impossibly difficult to retroactively “make them right” Single-entry ledgers, like the one we rushed to build at my startup, can provide information about the flow of funds, but not why those flows happened. If we wanted to know why a particular movement of money happened, we had to stitch together data from different models. Sometimes, it wasn’t even possible. Single-entry ledgers are undebuggable: I think [there] is a very kind of important difference [between] the way we view debugging versus the way it’s been viewed traditionally. Debugging is not merely the act of making bugs go away. It is the act of understanding, gaining knowledge, new knowledge about the way the system works. — Bryan Cantrill, Debugging Under Fire: Keep your Head when Systems have Lost their Mind On the other hand, double-entry ledgers give you the what and the why. Adding an extra bit of complexity, grounded in thousands of years of use, you can see money flowing from the point of view of any account. Single-entry ledgers journals; double-entry ledgers are complete maps. That’s why dancing cents were impossible to fix in our single-entry system. Once gone, we didn’t know where they were anymore. It could’ve been the foreign exchange, or the broker’s rounding to even mechanism, or the FINRA TAF fees that were collected at the end of the day. We couldn’t understand the way the system works. And so, we couldn’t make bugs go away. Had we had a double-entry accounting system, each and every cent would’ve been accounted for. That’s because, under a double-entry system, money always moves from one account to another. That’s what context gives you: the ability to gain knowledge. And with that said, let’s discuss the data model of a double-entry ledger. The data structure of money The very first thing that many engineers do when they want to track money is to do that within their domain models. I call that the balance as property approach: when the Order has a price attribute, or when the expenses table has an amount column, you’re doing exactly that. Like many bad designs, this approach works best when you’re starting out. It lets you go fast. But, over time, reporting becomes more sophisticated and slow, and payment processing and analysis becomes more difficult. If your overnight report job takes multiple hours to run, using the balance as property approach is probably the underlying cause. If you’re about to build a ledger from scratch, I suggest you treat it as a completely separate data model from which you can derive any financial transaction in your system. Getting the data model right is crucial in finance. It’s one of the reasons there are many companies that specialize in this. Ledgers are conceptually a data model, represented by three entities: Accounts, Entries and Transactions. Most people think of money in terms of what’s theirs, and Accounts are the representation of that point of view. They are the reason why engineers naturally gravitate towards the “simplicity” of single-entry systems. Accounts are both buckets of value, and a particular point of view of how its value changes over time. Entries represent the flow of funds between Accounts. Crucially, they are always an exchange of value. Therefore, they always come in pairs: an Entry represents one leg of the exchange. The way we ensure that Entries are paired correctly is with Transactions. Ledgers shouldn’t interact with Entries directly, but through the Transaction entity. Not every ledger has Transactions, but I think they’re necessary. In Building a powerful Double Entry Accounting system, Nubank’s Lucas Cavalcanti says that they’ve designed Entries with references both to the credit and the debit accounts. That way, the balancing of accounts is enforced by design. But I think that’s a mistake. Transfers can fail for many reasons, and such a design would make it very complex to represent partial failures. Transactions are very helpful with that. The Saga pattern fits very well with this way of representing money flows in a ledger. Sagas trade atomicity for availability. Rather than having a slow transaction locking many tables, you have smaller, discrete actions. That break down the same process, while exposing intermediate checkpoints along the way. At any of those points, another transaction can come in and do their work. That makes throughput better. — How To Scale Payment Systems With The Saga Pattern The Two Ledger Systems Accounting and Engineering are the two ledger systems. The Accounting system is how the ledger is seen from the outside, the interface. Ledgers are meant to expose its data aggregated from multiple points of views: Reporting, Financial ratios, and Business Intelligence. The Engineering system is how the ledger sees itself, the implementation. Ledgers are meant to have checks and balances in place, and must ensure the consistency and accuracy of the data. They’re for fintech companies what CRMs are to the sales team: a system of record, the source of truth. This, by the way, is the key reason why ledgers are so difficult to scale: the Accounting system pulls towards high availability and low latency, whereas the Engineering system pulls towards strong consistency and schema-on-write checks. How Entries work Entries can be in one of three statuses: pending, discarded and posted. Entries are always created with pending status, containing information on the value exchanged, the direction (credit or debit) and the account they reference. A common mistake here is to use positive and negative numbers in the amount to represent the direction. I’ll tell you more about this in How Accounts work. Entries are immutable, except for one thing: pending Entries can be discarded in order to create posted Entries. There’s a clear alternative design here: To create reversal entries to post a previously pending one in order to maintain full immutability. But I think that Modern Treasury’s design is better. While valid, this approach [create a reversal Entry to undo a pending one] leads to a messy Account history. We can’t distinguish between debits that were client-initiated and debits that are generated by the ledger as reversals. Listing Entries on an Account doesn’t match our intuition for what Entries actually comprise the current balance of the Account. Discarding solves this problem by making it easy to see the current Entries (simply exclude any Entries that have discarded_at set), while also not losing any history. — Matt McNierney, How to Scale a Ledger, Part II In a double-entry system, the collective amount of all non-discarded Entries with type credit and the collective amount of all non-discarded Entries with type debit is the same. Conceptually, this means that no matter how you move money in your pockets, the amount will stay the same. A few special accounts, those that represent the outside world (consolidated into the Profit and Loss statement) are exceptional: They can’t be balanced. How much money does the world have, anyway? How Transactions work Entries are created in pairs, and we use Transactions to make sure that everything goes how it’s supposed to go: A Transaction is posted only when its associated Entries are either posted or discarded (and have been replaced by the posted ones) A Transaction that fails partially can be semantically undone with compensating Entries. Again, this approach makes a lot of sense in the context of Sagas. How Accounts work From the point of view of a single account, the ledger looks as if it implemented a single-entry system. It is associated with multiple Entries in a one-to-many relationship, and the total balance should match the aggregation of all its entries’ individual balances. With one caveat: depending on the Accounts normal balance, the way to calculate the total is different. Remember when I said that using negative numbers in an Entry was a mistake? This is because some accounts are meant to be “net credit” and others “net debit”. “Meant to be” is the key here: just because an Account is supposed to be net debit (e.g., the cash in the bank) doesn’t mean it cannot be negative (e.g., overdrawn). Bundling the amount with the sign is a huge accounting no-no, because you’re left wondering if being negative or positive is the right state of affairs. Instead of using negative numbers, Accounts have normal balance: normal credit balance literally means that they are normal when its associated entries with type credit have a total amount that outweighs its associated entries with type debit. The reverse is true for normal debit balance. Ledgers are the clearest example of a hard computer science problem disguised as a non technical discipline. As many payments engineers know, and as the news will attest to, building ledgers are hard to get right without proper context. Feel free to bookmark this article and use it as a handbook when you work with ledgers. It should give you enough to keep you going. I’ve drawn from many references to put this article together. My favorite “Accounting 101 for Engineers” is this article by Anvil called An Engineer’s Guide to Double-Entry Bookkeeping, which includes some basic Python code to guide the conversation along. Django Hordak, the plug-and-play double accounting system Django package, has a thorough explanation on Double Entry Accounting For Developers. If you want to go in depth, Modern Treasury has a fantastic 6 part series on ledgers (Here’s part I). I mentioned earlier that there are a few high quality “Accounting for Developers” guides in circulation. Beancount’s, Martin Kleppmans’s and Modern Treasury’s are all worth reading, especially because they try to explain accounting from different angles. See which one resonates with you the most. Or maybe you want the full tutorial. In which case, Peter Selinger’s is the best And last, companies like Uber, Square and Airbnb have published how they’ve implemented double-entry ledgers in their systems. Now that you know more, feel free to check them out. This has been The Payments Engineer Playbook. I’ll see you next week. Thanks for reading The Payments Engineer Playbook! Subscribe for free to receive new posts and support my work. Subscribe PS: I want to ask you a question. When I decided to write a post on ledgers, I already knew there were a few good resources out there to help me. But I’ve been reading and watching and reading again, and after more than 100 hours on it, I haven’t finished yet. Granted, 100 hours is an estimation. But I would be surprised if the actual number was less than 99. As you can imagine, I have enough to write a small book on How to Build a Future-Proof Ledger (title TBD). Such a book would make fintech startups like the one I used to work for ready to tackle much more valuable problems than keeping track of their users’ money. Or maybe do it at a much bigger scale, who knows. Would you buy such a book if it existed? You’ve made it this far in the article, I’m assuming the topic was interesting. The question that’s percolating in my mind is: would somebody buy a book on engineering ledgers? I thoroughly enjoyed writing my first book, The Databases of Money, and it would be amazing if enough people express their interest in this new book. If so, I’ll be writing a few more posts on the engineering aspects of a scalable ledger, a “building in public” project kind of thing. Otherwise, I will drop the topic altogether. No hard feelings. So, if having a defined engineering strategy for building a double-entry ledger is valuable to you, you can do two things. The first thing is to give a like to this article on LinkedIn. Not only helps other people find this newsletter, it also allows people who aren’t subscribers the opportunity to express their interest in my writing. A 👍 is all it takes. The second thing is to tell a friend. I suggest you share this article with this convenient button below. But word of mouth works too! Share And if someone you respect shared this article with you, do me a favor and subscribe. Every week I feel I’m getting better at this. That means that my best articles on how to build payment systems are probably yet to be written. You can only find out if you subscribe to The Payments Engineer Playbook. I’ll see you around. 16 Share this post The Payments Engineer Playbook Engineers Do Not Get To Make Startup Mistakes When They Build Ledgers Copy link Facebook Email Notes More 4 Share PreviousNext",
    "commentLink": "https://news.ycombinator.com/item?id=42269227",
    "commentBody": "Engineers do not get to make startup mistakes when they build ledgers (alvaroduran.com)373 points by fagnerbrack 19 hours agohidepastfavorite382 comments hn_throwaway_99 16 hours agoTell that to Synapse customers. Many millions of dollars are missing. Banks have to follow strict rules to account for where all the money goes. But the way fintechs work, they usually just have one or a couple underlying \"FBO\" accounts where all the pooled money is held, but then the fintech builds a ledger on top of this (and, as the article points out, to varying levels of engineering competence) to track each individual customer's balance within this big pool of money. In Synapse's case, their ledger said the total amount of all of their individual customer balances ended up being much more than the actual funds held in the underlying FBO accounts. Lots of folks are assuming fraud but I'm willing to put money that it was just a shitty, buggy ledger. FWIW, after seeing \"how the sausage is made\", I would never put money into a fintech depository account. Use a real bank. Fintechs also often put out the fake promise that deposits are FDIC insured, but this only protects you if the underlying bank goes belly up, not if the fintech loses track of your money. See https://www.forbes.com/sites/zennonkapron/2024/11/08/what-th... reply chairmansteve 2 minutes agoparent\"Fintechs also often put out the fake promise that deposits are FDIC insured, but this only protects you if the underlying bank goes belly up, not if the fintech loses track of your money\". Would you count Wealthfront as a fintech? I was finding their marketing compelling, but this thread makes me think twice. reply mrfox321 16 hours agoparentprevAt a big co I worked at, the lack of consistency between trading systems caused money to (dis)appear (into)out of thin air. Prior to one of these hiccups, I hypothesized, given how shitty the codebase was, that they must be tracking this stuff poorly. This led to an argument with my boss, who assumed things magically worked. Days later, we received an email announcing an audit one one of these accounting discrepancies. JPMC proposed using crypto, internally, to consistently manage cash flow. Not sure if it went anywhere. reply HolyLampshade 8 hours agorootparentAt all of the exchanges and trading firms I’ve worked with (granted none in crypto) one of the “must haves” has been a reconciliation system out of band of the trading platforms. In practice one of these almost always belongs to the risk group (this is usually dependent on drop copy), but the other is entirely based on pcaps at the point of contact with every counterparty and positions/trades reconstructed from there. If any discrepancies are found that persist over some time horizon it can be cause to stop all activity. reply ajb 6 hours agorootparentWait, pcap as in wireshark packet capture? reply tnlnbn 5 hours agorootparentI'm not the commenter, but yes, often trading firms record all order gateway traffic to from brokers or exchanges at the TCP/IP packet level, in what are referred to as \"pcap files\". Awkwardly low-level to work with, but it means you know for sure what you sent, not what your software thought it was sending! reply pclmulqdq 4 hours agorootparentThe ultimate source of truth about what orders you sent to the exchange is the exact set of bits sent to the exchange. This is very important because your software can have bugs (and so can theirs), so using the packet captures from that wire directly is the only real way to know what really happened. reply generic92034 2 hours agorootparentBut then the software capturing, storing and displaying the packets can also have bugs. reply bostik 2 hours agorootparentAmong all the software installed in a reputable Linux system, tcpdump and libpcap are some of the most battle tested pieces one can find. Wireshark has bugs, yes. Mostly in the dissectors and in the UI. But the packet capture itself is through libpcap. Also, to point out the obvious: pcap viewers in turn are auditable if and when necessary. reply baq 3 hours agorootparentprevThat makes sense - but it's still somewhat surprising that there's nothing better. I guess that's the equivalent of the modern paper trail. reply thomasjudge 2 hours agorootparentprevThe storage requirements of this must be impressive reply bostik 1 hour agorootparentStorage is cheap, and the overall figures are not that outlandish. If we look at a suitable first page search result[0], and round figures up we get to about 700 GB per day. And how did I get that figure? I'm going to fold pcap overhead into the per-message size estimate. Let's assume a trading day at an exchange, including after hours activity, is 14 hours. (~50k seconds) If we estimate that during the highest peaks of trading activity the exchange receives about 2M messages per second, then during more serene hours the average could be about 500k messages per second. Let's guess that the average rate applies 95% of the time and the peak rate the remaining 5% of the time. That gives us an average rate of about 575k messages per second. Round that up to 600k. If we assume that an average FIX message is about 200 bytes of data, and add 50 bytes of IP + pcap framing overhead, we get to ~250 bytes of transmitted data per message. At 600k messages per second, 14 hours a day, the total amount of trading data received by an exchange would then be slightly less than 710GB per day. Before compression for longer-term storage. Whether you consider the aggregate storage requirements impressive or merely slightly inconvenient is a more personal matter. 0: https://robertwray.co.uk/blog/the-anatomy-of-a-fix-message reply tetha 33 minutes agorootparentAnd compression and deduplication should be very happy with this. A lot of the message contents and the IP/pcap framing overheads should be pretty low-entropy and have enough patterns to deduplicate. It could be funny though because you could be able to bump up your archive storage requirements by changing an IP address, or have someone else do that. But that's life. reply w23j 5 hours agorootparentprevI would also really like to know that! It generally seems to be a thing in trading: https://databento.com/pcaps There is also this (though this page does not specify what pcap means): https://www.lseg.com/en/data-analytics/market-data/data-feed... reply alexwasserman 3 hours agorootparentLook up Corvil devices by Pico. Commonly used in finance. https://www.pico.net/corvil-analytics/ reply alexwasserman 3 hours agorootparentprevTypically not a literal pcap. Not just wireshsrk running persistently everywhere. There are systems you can buy (eg by Pico) that you mirror all traffic to and they store it, index it, and have pre-configured parsers for a lot of protocols to make querying easier. Think Splunk/ELK for network traffic by packet. reply cjalmeida 55 minutes agorootparentExcept it is literal “pcap” as they capture all packets at layer 3. I don’t know the exact specifications of Pico appliances, but it would not surprise me they’re running Linux + libpcap + some sort of timeseries DB reply alexwasserman 51 minutes agorootparentWell, probably, but I meant more like it's not typically someone running tcpdump everywhere and someone analyzing with Wireshark, rather than a systems configured to do this at scale across the desktop. reply Loic 6 hours agorootparentprevI suppose Pre-Calculated Aggregated Positions, but I am not an expert in the field. reply SnorkelTan 5 hours agorootparentprevLooking at the order messages sent to and received from another trading system was not uncommon when I worked in that neck of the woods reply naasking 4 hours agorootparentprev> JPMC proposed using crypto, internally, to consistently manage cash flow. Yikes, how hard is it to just capture an immutable event log. Way cheaper than running crypto, even if only internally. reply imglorp 3 hours agorootparentHarder than you'd think, given a couple of requirements, but there are off the shelf products like AWS's QLDB (and self hosted alternatives). They: Merkle hash every entry with its predecessors; normalize entries so they can be consistently hashed and searched; store everything in an append-only log; then keep a searchable index on the log. So you can do bit-accurate audits going back to the first ledger entry if you want. No crypto, just common sense. Oddly enough, I worked at a well known fintech where I advocated for this product. We were already all-in on AWS so another service was no biggie. The entrenched opinion was \"just keep using Postgres\" and that audits and immutability were not requirements. In fact, editing ledger entries (!?!?!?) to fix mistakes was desirable. reply rdpintqogeogsaa 2 hours agorootparentFootnote to QLDB: AWS has deprecated QLDB[1]. They actually recommend using Postgres with pgAudit and a bunch of complexity around it[2]. I'm not sure how I feel about such a misunderstanding of one's own offerings of this level. [1] https://docs.aws.amazon.com/qldb/latest/developerguide/what-... [2] https://aws.amazon.com/blogs/database/replace-amazon-qldb-wi... reply imglorp 2 hours agorootparentYeah. I'm surprised it didn't get enough uptake to succeed, especially among the regulated/auditable crowds, considering all the purpose built tech put into it. reply naasking 1 hour agorootparentI think you're forgetting how many businesses are powered by Excel spreadsheets. This solution seems too advanced and too auditable. reply baq 2 hours agorootparentprevI'll just leave that here for no particular reason at all: https://www.sec.gov/enforcement-litigation/whistleblower-pro... reply voidfunc 2 hours agorootparentBetter hurry, Elon is gonna dismantle the SEC in about 45 days reply fragmede 2 hours agorootparentprevImportantly, the SEC is empowered to give 10-30% of the money siezed via whistleblowing too the whistle blower. reply limit499karma 3 hours agorootparentprevTheoretically they even have a better security environment (since it is internal and they control users, code base and network) so the consensus mechanism may not even require BFT. reply DanielHB 4 hours agorootparentprev> I hypothesized, given how shitty the codebase was, that they must be tracking this stuff poorly. That is like half of the plot of Office Space reply im3w1l 11 hours agorootparentprevIf its for internal why not just use a normal append only log. x amount transferred from account y to account z. A three column csv oughta do it. reply tonyhart7 7 hours agorootparentit literally ledger, its only show where money went but not showing \"why\" the money move double entry with strong check that ensure its always balance fix this reply sneak 9 hours agorootparentprevAny time your proposal entails a “why not just”, it is almost certainly underestimating the mental abilities of the people and teams who implemented it. A good option is “what would happen if we” instead of anything involving the word “just”. reply qazxcvbnmlp 2 hours agorootparent“Just” usually implies a lack of understanding of the problem space in question. If someone says “solution X” was considered because of these factors which lead to these tradeoffs however since then fundamental assumption Y has changed which allows this new solution then it’s very interesting. reply jknoepfler 18 minutes agorootparentSure. When I ask \"why don't we just\" I'm suggesting that the engineering solutions on the table sound over-engineered to the task, and I'm asking why we aren't opting for a straightforward, obvious, simple solution. Sometimes the answer is legitimate complexity. Equally as often, especially with less experienced engineers, the answer is that they started running with a shiny and didn't back up and say \"why don't we just...\" themselves. reply PittleyDunkin 3 hours agorootparentprevCounterfactuals strike me as even less useful than underestimating competency would be. Surely basic double-entry accounting (necessarily implying the use of ledgers) should be considered table stakes for fintech competency. reply foobarbecue 5 hours agorootparentprevLots of threads on this here, most recently https://news.ycombinator.com/item?id=42038139#42038572 . I think this example is perfect, with the \"oughta do it\" reply foobarbecue 5 hours agorootparentprevhttps://news.ycombinator.com/item?id=42038139#42038572 reply hooverd 15 hours agorootparentprevIt's all merkle trees under the hood. I feel like the crypto coin stuff has overshadowed the useful bits. reply trog 11 hours agorootparentIs a Merkle tree needed or is good old basic double ledger accounting in a central database sufficient? If a key requirement is not a distributed ledger then it seems like a waste of time. reply Onavo 10 hours agorootparentMerkle tree is to prevent tampering, not bad accounting practices reply agentultra 31 minutes agorootparentWrite-Once, Read Many drives also prevent tampering. Not everything needs crypto. reply nly 9 hours agorootparentprevIt only prevents tampering if the cost of generating hashes is extremely high. Internally in your company you're not going to spend millions of $'s a year in GPU compute just to replace a database. reply xorcist 8 hours agorootparent\"Prevents tampering\" lacks specificity. git is a blockchain that prevents tampering in some aspects, but you can still force push if you have that privilege. What is important is understand what the guarantees are. reply limit499karma 3 hours agorootparentprev? If I use something like Blake3 (which is super fast and emits gobs of good bits) and encode a node with say 512 bits of the hash, you are claiming that somehow I am vulnerable to tampering because the hash function is fast? What is the probable number of attempts to forge a document D' that hashes to the very same hash? And if the document in structured per a standard format, you have even less degrees of freedom in forging a fake. So yes, a Merkel tree definitely can provide very strong guarantees against tampering. reply oconnor663 1 hour agorootparentFwiw, increasing the BLAKE3 output size beyond 256 bits doesn't add security, because the internal \"chaining values\" are still 256 bits regardless of the final output length. But 256 bits of security should be enough for any practical purpose. reply jchanimal 6 hours agorootparentprevWe launched Fireproof earlier this month on HN. It’s a tamperproof Merkle CRDT in TypeScript, with an object storage backend for portability. See our https://news.ycombinator.com/item?id=42184362 We’ve seen interest from trading groups for edge collaboration, so multi-user apps can run on-site without cloud latency. reply nearting 3 hours agorootparentThis doesn't address the question in any way except to note that you also use Merkle Trees. Do you reply to any comment mentioning TypeScript with a link to your Show HN post as well? reply hluska 3 hours agorootparentprevWhat disrespectful marketing. We don’t care that you use Merkle trees because that’s irrelevant. I guess I can add Fireproof to my big list of sketchy products to avoid. It’s embarrassing. reply jchanimal 3 hours agorootparentI figured the responses would be more interesting. Questions about CRDT guarantees etc. Perhaps worth seeding the convo with a remark about finality. reply csomar 7 hours agorootparentprevCrypto/Blockchain makes it harder to have an incorrect state. If you fk up, you need to take down the whole operation and reverse everything back to the block in question. This ensures that everything was accounted for. On the other hand, if you fk in a traditional ledger system you might be tempted to keep things running and resolve \"only\" the affected accounts. reply necovek 5 hours agorootparentIt's mostly a different approach to \"editing\" a transaction. With a blockchain, you simply go back, \"fork\", apply a fixed transaction, and replay all the rest. The difference is that you've got a ledger that's clearly a fork because of cryptographic signing. With a traditional ledger, you fix the wrong transaction in place. You could also cryptographically sign them, and you could make those signatures depend on previous state, where you basically get two \"blockchains\". Distributed trust mechanisms, usually used with crypto and blockchain, only matter when you want to keep the entire ledger public and decentralized (as in, allow untrusted parties to modify it). reply koolba 5 hours agorootparent> With a traditional ledger, you fix the wrong transaction in place. No you don’t. You reverse out the old transaction by posting journal lines for the negation. And in the same transactions you include the proper booking of the balance movements. You never edit old transactions. It’s always the addition of new transactions so you can go back and see what was corrected. reply delfinom 6 hours agorootparentprevIt's a question of business case. While ensuring you are always accounted correctly seems like a plus, if errors happen too often potentially due to volume, it makes more business sense sometimes to handle it while running rather than costing the business millions per minute having a pause. reply TexanFeller 3 hours agoparentprevI’m skittish about real banking institutions as well. Vanguard for example outsourced a bunch of their dev work to India a few years ago. Had a friend that worked as a sysadmin for BoA. They were required to keep certain logging for 7 years but he would just delete it anyway when disks were starting to get full. reply hn_throwaway_99 2 hours agorootparentBut the fundamental difference is that the regulatory structures are in place to recover your money if a bank loses it. That's not the case with fintech middlemen. Take the Synapse case: * End customers are really a customer of Yotta, a (silly IMO) fintech where interest was essentially pooled into a sweepstakes prize. * Yotta was a customer of Synapse - they used Synapse BaaS APIs to open customer accounts (again, these accounts were really just entries in Synapse's ledger, and they underlying funds were supposed to be stored in an FBO account on Evolve). * Synapse partnered with Evolve, who is the FDIC insured bank. Synapse went bankrupt, and Yotta customers are finding out they can't access their money. But the bankruptcy court is at a loss as to really what to do. FDIC isn't getting involved, because as far as they can tell, Evolve hasn't failed. Synapse is basically out of the picture at this point as they are bankrupt and there isn't even enough money left to do a real audit, and Yotta is suing Evolve alleging they lost customer funds. But, in the meantime, Yotta customers are SOL. If you had a direct relationship with an FDIC-insured bank, and they lost your money, there would be a much clearer path for the FDIC to get involved and make you whole (up to $250k). reply itake 33 minutes agorootparentFDIC would only get involved if the bank was insolvent? If your bank and you have a disagreement over how much money should be in your account, then FDIC wouldn't be involved? reply Thorrez 1 hour agorootparentprevHow about Wealthfront Cash accounts? Wealthfront provides me a statement that shows how my deposited money is distributed among its FDIC insured partner banks, and each transfer they do to and from one of those partner banks. Wealthfront does use a middleman, somewhat similar to how Yotta used Synology as a middleman. But Wealthfront's middleman is FDIC insured: Green Dot Bank. reply tw04 9 minutes agorootparentprev> They were required to keep certain logging for 7 years but he would just delete it anyway when disks were starting to get full. I’m highly skeptical of this claim. Every bank I’ve worked with adheres to their records requirements like it’s life or death (because it kind of is for the bank). Tell your friend he’s exposing himself to hard prison time if he’s not just making up a story. If his boss tells him that they don’t have budget to retain the logs he should be whistle blowing, not violating federal banking laws to save what is a rounding error in their IT budget. reply phonon 14 hours agoparentprevSynapse says that it was actually the Bank (Evolve) that made the accounting mistakes, including missing transactions, debits that weren't reported, sending in flight transaction to Mercury while debiting Synapse incorrectly etc. https://lex.substack.com/p/podcast-what-really-happened-at-s... reply hn_throwaway_99 13 hours agorootparentThanks for posting this, I will definitely listen to it. While I haven't listened yet, one thing I don't really buy when it comes to blaming Evolve is that it should fundamentally be Synapse's responsibility to do reconciliation. This is what completely baffled me when I first worked with another BaaS company - they weren't doing any reconciliation of their ledgered accounts with the underlying FBO balances at the partner bank! This was insane to me, and it sound likes Synapse didn't do it either. So even if Evolve did make accounting mistakes and have missing transactions, Synapse should have caught this much earlier by having regular reconciliations and audits. reply phonon 12 hours agorootparentThey claim they did, Evolve kept putting them off, until they ran out of money. There's a full transcript (with some images) below the player btw. reply rawgabbit 12 hours agorootparentprevRambling interview. As best as I can tell Synapse said there were technical issues with Evolve the bank. Meanwhile this article said Synapse withdrew from Evolve the end user funds. Mr. Holmes of Evolve said the bank “transferred all end user funds” to other banks at the request of Synapse, but declined to identify them https://www.nytimes.com/2024/07/09/business/synapse-bankrupt... reply phonon 11 hours agorootparentI'm sure the spokesperson for Evolve who then says \"“It’s complicated,” he wrote in an email Friday, declining to elaborate further.\"\" is fully trustworthy and not eliding any important details. reply zimbatm 9 hours agorootparentprevWise also recently switched their US bank provider from Evolve to Community Federal Savings Bank. Maybe they had similar issues? reply Aspos 4 hours agorootparentI see no reason why CFSB would be in any way different from Evolve, they are just not caught up in the mess yet. Synapse problem was fundamental and it stems from the same mistake OP is making: never ever build your own, homegrown ledger if you can avoid it. reply kdmtctl 3 hours agorootparentYou can't avoid this. It is either your client or bank's client. And no bank will take the burden to account every $0.2 transaction for you, spending its own computing power. It just a quite expensive thing to do. That is why banks often separate the main ledger and retail ledger[s]. Each system tuned for a different performance profile. reply Aspos 2 hours agorootparentOne should not build their own cryptography, one should not build their own ledger, that's what I am saying. reply abenga 13 hours agorootparentprevFrom a cursory look at how it describes itself (BaaS, etc), Evolve is hardly a \"bank\" in the traditional sense of the word. reply hgomersall 11 hours agoparentprevI had money disappear from my HSBC account. As in, the double entries didn't match by a small amount (it was a payment between two of my accounts that didn't match up, which I couldn't trivially reconcile in the books). I pursued this for a while but they never even properly acknowledged it let alone fix it. I had my unfounded suspicion it was some internal subtle theft going on, but incompetence is probably a better explanation. reply tossandthrow 8 hours agorootparentIf you live in a developed country it should be sufficient to ask them to account for it with a note that a formal complaint will be sent to relevant authorities if nor dealt with in timely manner. That stuff like this is in order is the foundation of kapital societies and is taken quite seriously. reply hgomersall 8 hours agorootparentYou'd think so wouldn't you? But alas, the effort required to solve this was at least more than I was willing to make. reply sailfast 3 hours agorootparentIf in the US, the CFPB would handle this for you. reply hombre_fatal 2 hours agorootparentFunnily enough, that's an agency Musk wants to gut: https://www.politico.com/live-updates/2024/11/27/congress/de... reply pera 6 hours agorootparentprevI had a similar situation with Santander many years ago: it was a small amount and happened when I was closing my account, bank manager couldn't explain it and escalating the problem was a pain - especially because I was about to move to another country and had more urgent things to do. I wonder how common issues like these are... reply imp0cat 4 hours agorootparentI think it's quite common, it's just that people do not notice these things. I also had it happen one time, the bank eventually figured it out and fixed some error on their part. reply lores 6 hours agorootparentprevYou're likely correct it was theft. I was told by a CTO there that topping up accounts with bank money where it has been hacked away was daily routine and cheaper than fixing the systems. Incompetence would not manifest on routine operations like this. reply DanielHB 4 hours agorootparentprev> I had my unfounded suspicion it was some internal subtle theft going on Had you watched Office Space recently? reply superzamp 10 hours agoparentprevCoincidentally written something about this yesterday [1], but the gist of my take summed up is that the nature of accounting oriented data models doesn’t help when dealing with multiple FBO accounts. The main problem is that accounting defaults to full fungibility of monetary amounts represented in a ledger, which has the effect of losing track of the precise mapping between assets and liabilities, so you end up in a position where you simply cannot tell precisely to a bank _who_ are the actual customers they owe money to. [1] https://www.formance.com/blog/engineering/warehousing-promis... reply bradleyjg 57 minutes agoparentprevIn Synapse's case, their ledger said the total amount of all of their individual customer balances ended up being much more than the actual funds held in the underlying FBO accounts. Lots of folks are assuming fraud but I'm willing to put money that it was just a shitty, buggy ledger. If there was no malfeasance then no money would be gone. The totals would add up, they just wouldn’t no know who was owed what. Since the totals don’t add up, someone got the extra. reply e40 3 hours agoparentprevI had been debating the merits of using Flourish, but I'm sticking with SNOXX on Schwab. Same rate and I think SNOXX has to be safer, right? Even with the Flourish FDIC guarantee, as others have pointed out, it's only for the underlying back not Flourish itself. reply PittleyDunkin 3 hours agoparentprev> Lots of folks are assuming fraud but I'm willing to put money that it was just a shitty, buggy ledger. I'm not sure there's much difference. Intent only matters so much. reply taeric 3 hours agorootparentI mean... Fraud is defined by intent. You can argue negligence over mistake. But fraud definitely requires intent. reply PittleyDunkin 1 hour agorootparentI guess my point is it's as harmful as fraud regardless if we can throw someone in prison. reply taeric 1 hour agorootparentOh, for sure. But treatment/remediation will heavily change between the cases. Right? reply throwaway2037 10 hours agoparentprev> Fintechs also often put out the fake promise that deposits are FDIC insured Does this still happen? reply koblas 10 hours agorootparentMany fintechs are not licensed to hold funds and work with bank partners who hold your actual funds. That allows them to say they're insured because they're not co-mingled with the corporate funds in the event of insolvency. This doesn't stop them from making accounting errors. reply zie 1 hour agorootparentprevThe FDIC said you can't do this anymore starting Jan 1, 2025. So I expect it to stop in about 30 days. The FDIC will probably find a few laggards and throw some fines at them, and the process will then probably completely stop. reply fragmede 3 hours agorootparentprevthe problem is the discrepancy between what the Fintech means when they say fdic insured, and what the customer hears when they're told fdic insured. the customer (erroneously) assumes it means that if the Fintech or anyone else has problems, the customer is covered up to the 250k fdic limit. what the Fintech means, is that there's someone they're partnered with that is a bank and is fdic covered. How the money is deposited into the bank is up for interperation. if there Fintech is being dishonest, they have one bank account at a bank, and all of the customers money goes into that one shared account, they're not technically lying - the money is fdic insured. unfortunately for the customers, that's not the same as each of them being fdic insured is the Fintech goes under. fdic doesn't seem to want to clarify this issue either, which is a problem. reply ajuc 1 hour agoparentprev> In Synapse's case, their ledger said the total amount of all of their individual customer balances ended up being much more than the actual funds held in the underlying FBO accounts. Lots of folks are assuming fraud but I'm willing to put money that it was just a shitty, buggy ledger. Bugs are as likely to show more and less money than there really are. But bugs in production will almost always show more :) reply moffkalast 4 hours agoparentprevThe fact that practically all funding most of the world runs on these days is just a bunch of variables in some shitty program never stops being weird to think about. All it takes to create or destroy trillions is one (or maybe a few) CPU ops. It really stretches the belief into fiat money to the absolute limit. reply bitwize 4 hours agorootparentDade: This is every financial transaction Ellingson conducts, yeah? From million dollar deals to the ten bucks some guy pays for gas. Kate: The worm eats a few cents from each transaction. Dade: And no one's caught it because the money isn't really gone. It's just data being shifted around. reply serbuvlad 16 hours agoparentprev> In Synapse's case, their ledger said the total amount of all of their individual customer balances ended up being much more than the actual funds held in the underlying FBO accounts. When the banks do this it's called \"fractional reserve banking\", and they sell it as a good thing. :) reply Aurornis 15 hours agorootparentI’m constantly amazed by how much the crypto community thinks they understand fractional reserve banking while getting it so completely wrong. In fractional reserve banking, money that is loaned out is accounted for as liabilities. These liabilities subtract from the overall balance stored (reserved) at the bank. The bank is not printing money new money, no matter how many times this idea gets repeated by people who are, ironically, pumping crypto coins that were printed out of thin air. I think it’s incredible that cryptocurrencies were literally manifested out of bits, but the same people try to criticize banks for doing this same thing (which they don’t). reply throw0101a 4 hours agorootparent> In fractional reserve banking, money that is loaned out is accounted for as liabilities. Yes, that is how a fractional reserve banking works. But that is not how the current banking system works. * https://www.stlouisfed.org/publications/page-one-economics/2... * https://www.pragcap.com/r-i-p-the-money-multiplier/ Banks do not lend out deposits. This was called the \"Old View\" by Tobin in 1963: * https://elischolar.library.yale.edu/cowles-discussion-paper-... The Bank of England has a good explainer on how money is created: * https://www.bankofengland.co.uk/quarterly-bulletin/2014/q1/m... See also Cullen Roche: * https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1905625 * https://rationalreminder.ca/podcast/132 reply notahacker 4 hours agorootparentMoney that is loaned out is still accounted for as liabilities. Sure, those liabilities are accounted for in an eventually consistent matter by reconciling imbalances on interbank lending markets at the end of the day with the government topping up any systemic shortfall rather than by counting out deposit coins in the vault But that's fundamentally much closer to the \"Old Money\" view than to the OP's claim about fractional reserve being like an FBO inflating customer deposits by failing to track trades properly. All the credit extended by the bank is accounted for, and all of it that isn't backed by reserves is backed by the bank's obligations to someone else. reply throw0101a 1 hour agorootparent> Money that is loaned out is still accounted for as liabilities. To be clear: * Money is \"loaned out\" in the sense that a bank credits your account. * Money is not loaned out in the sense that which \"goes into\" your account did not come out of someone else's account. Rather it was created 'out of thin air' by the bank without reference to anyone else's deposits. reply notahacker 1 hour agorootparentTo be clear: I am familiar with your links, for quite some time actually. I never said that the money came out of someone else's account. What I did say was that it was accounted for as liabilities. It's the bank's liability to the loanee (or their bank), which the bank absolutely can be obliged to pay with reserves or cold hard cash (and it can only get these from borrowing, selling assets or customers paying cash into their account). And so banks lend it out to people attached to a slightly larger liability repayable to them and keep track, because if they don't all this money they're \"printing\" represents losses in terms of obligations they can't \"print\" their way out of. That's quite different from the ledger screwup its being compared with, or indeed people creating tokens (not backed by debt or anything else) out of thin air to sell to other people reply nradov 1 hour agorootparentprevNo. Banks classify outstanding loans as assets, not liabilities. reply notahacker 1 hour agorootparentYes, the loan is the bank's asset. The deposit created aka \"the money\" is the bank's liability. I don't think we're in disagreement here. A corollary of this is that contra popular suggestions otherwise, the accounts net to zero and the bank obtains no gain from \"printing money\", only from interest earned on repayments. reply simula67 12 hours agorootparentprev> The bank is not printing money new money, no matter how many times this idea gets repeated by people who are, ironically, pumping crypto coins that were printed out of thin air. It is now widely accepted that bank lending produces new money[1][2] [1] https://www.bankofengland.co.uk/-/media/boe/files/quarterly-... [2] https://www.youtube.com/watch?v=K3lP3BhvnSo reply hgomersall 11 hours agorootparentThere's an inordinate amount of nonsense being espoused in this thread, when the answer is in that first link. I can only assume it's the miseducation that economics textbooks perpetuate. reply Agingcoder 11 hours agorootparentprevYes. That’s how it was taught to me years ago, that’s how it’s understood in the banking industry as well, incl the private sector. From a large eurozone bank : https://group.bnpparibas/en/news/money-creation-work reply jnwatson 11 hours agorootparentprevThe \"liabilities\" aren't subtracted from the deposit amount when counted as M1 supply. (Actually loans are accounted for as assets and deposits are liabilities, but that's beside the point). If customer A deposits $100 in cash, and customer B borrows $100 from the bank and deposits it back in the bank, M1 goes up because there are now two checking accounts with $100 in it. That the bank's internal bookkeeping balances doesn't change the fact that the Fed considers that more money exists. reply notahacker 1 hour agorootparent> That the bank's internal bookkeeping balances doesn't change the fact that the Fed considers that more money exists. The Fed considers that more M1 exists and the same amount of M0 exists. Both are considered monetary aggregates, but M0 is the \"money\" the bank needs to worry about to stay solvent, and it can't \"print\" that. Whilst it's semantically correct to refer to both M1 and M0 as money, it's pretty clear that it's wrong for people people to elide the two to insinuate that banks are printing themselves balances out of thin air like token issuers or insolvent companies that screwed up their customer balance calculations, which is what the OP was covering. And the Fed wouldn't consider more money to exist if the bank's internal bookkeeping didn't balance... reply gus_massa 6 hours agorootparentprevI agree. The main point is that if B knows that they don't have to repay the $100 until 10 years in the future, then for the 10 next years everyone can pretend there are $200 in total. reply kevin_nisbet 13 hours agorootparentprevThis is a good explanation, I've had to explain this topic a few times as well, it seems like it's one of those topics that is very missunderstood. To just expand a bit, I believe some of the confusion around printing of money comes from the way some economics reports are built. As a micro example, Assume a 10% required reserve, If Alice deposits $100 and the bank lends $90 to Bob. Alice ($100 deposits) + Bob ($90 cash) think they have $190 in total. This is mainly useful for economists to understand, study, and report on. However, when the reports get distributed to the public, it looks like the banks printed their own money, as we now see $190 on the report when there is only $100 of cash in our example system. Whether the system should work on a fractional reserve is it's own debate, but we need to know what it is to debate the merits and risks of the system. reply neilwilson 8 hours agorootparentAnd how does that work when the 'required reserve' is zero as it is now, and has been in the rest of the world since time immemorial? Nobody deposits in a bank - it's just a retag of an existing deposit. The bank Debits a loan account with the amount owed, and Credits a deposit account with the advance. It's a simple balance sheet expansion in double-entry bookkeeping. I'm really not sure why this myth persists given that central banks debunked the concept over a decade ago. Loans create deposits, and those deposits are then converted into bank capital when a deposit holder buys bank capital bonds or equity. [0]: https://www.bankofengland.co.uk/-/media/boe/files/quarterly-... reply immibis 4 hours agorootparentMost people deposit in a bank by transferring from another bank. There is more than one bank. reply neilwilson 1 hour agorootparentNow do the balance sheet journals for such a transfer. [0] Then you'll see that for a bank to transfer to another bank the destination bank has to take over the deposit in the source bank (or swap that liability with another bank somewhere). You have an infinite regress in your thinking. [0]: https://new-wayland.com/blog/why-banks-pay-interest-on-depos... reply stackghost 11 hours agorootparentprev>These liabilities subtract from the overall balance stored (reserved) at the bank. The bank is not printing money new money Hi, this is factually incorrect and you should educate yourself before attempting any further condescending comments on Hacker News. reply victorbjorklund 7 hours agorootparentprevBut Banks are increasing the money supply with fractional reserve bank. But that is of course on purpose and account for by the govt. reply serbuvlad 13 hours agorootparentprevFirst of all, I take offense to being thrown in as part of the crypto community, with which I have nothing to do, and for which I do not have much hope. So now if you are unhappy with the monetary system you are automatically a crypto bro and can be dismissed? Secondly, the problem with fractional reserve banking is as follows: Suppose Larry makes a deposit of one dollar, which the bank guarantees can be retrieved at any time. The bank loans this dollar to Catherine, which uses it to buy something from Steve. Now Steve has one dollar, which he deposits with the bank. The bank lends this dollar to Catherine2, which uses it to buy something from Steve2. And so on, up to CatherineN and SteveN Now, in so far as transactions can take place in the economy with bank IOUs, which are considered perfect money substitutes, the amount of money in the economy has been multiplied by a factor of N. Where before only Peter had a dollar (or a dollar IOU, which are supposedly the same), now Pere AND Steve, Steve2, up to SteveN all have a dollar IOU. This leads to an inflationary pressure. Now it is true that upon the Catherine's repaying of the debt, these extra dollars will go away. However, in reality there is no such thing as negative dollars. The supply of money has been increased by the bank. An objection could be raised that Catherine's extra demand for money to pay off her debt will exactly offset the extra supply of money. This is nonsense! Everyone demands money all the time. If Catherine did not demand money to pay off her loan, she would demand money in order to satisfy her next most urgent want which could be satisfied by money. The increase in the demand for money is negligible. reply nly 9 hours agorootparentYour explanation of fractional reserve banking is somewhat correct, but missing the big picture Licensed banks can and do write loans at any time without having any deposits to 'lend out'. In doing so they create both the loan (an asset) and a deposit (a liability) simultaneously from thin air. The books immediately balance. The deposit created is then paid to the borrower and the liability vanishes. The bank is left with only the asset - the one that they created from thin air. For short term liquidity a bank can always use the overnight lending facility at the central bank. Doing so just makes all their loans far less profitable as this is at a floating daily rate. In reality the limit to which the money supply grows is not dictated by 'fractional reserves', but solely by interest rate policy and the commercial viability of being able to make loans and demand in the economy. reply neilwilson 8 hours agorootparentNot quite. The deposit is paid to the borrower as an advance, and the deposit is transferred to the payee (or the receiving bank if the payee is at another bank) The liability can never vanish - balance sheets have to balance. Bank liabilities are what we call 'money'. Hence how you are 'in credit' at the bank. reply immibis 4 hours agorootparentAnd when we look at the bank assets which back those liabilities, we find that (say) 10% are government-printed money, and the remaining 90% were created by banks. reply neilwilson 1 hour agorootparentWe don't. What we see is both of those are loans made. Technically the commercial bank lends to the central bank. That's why they receive interest on it. That's just a loan like all the other loans on the asset side. The difference is that the interest rate is set by the borrower not the lender. Holding a deposit is just different name for a particular type of loan. reply namaria 10 hours agorootparentprevYour mistake was saying Synapse merely did what banks do. Banks don't lose track of money when they increase the money supply. reply serbuvlad 4 hours agorootparentMy comment was meant as a tounge-in-cheek joke, with a dig at the banking system. It was not meant as a serious equivocation between what Synapse did and what banks do. reply immibis 4 hours agorootparentprevIn fractional reserve banking, the total deposits at a bank can be greater than the amount of physical money it holds. Since the rest of society is willing to accept bank deposits as an alternative to physical money, this is a form of printing money. Physical currency is not printed, but bank deposit currency (which is money, by de facto agreement) is. reply zmgsabst 14 hours agorootparentprevClarifying question: So for every $1 deposited, I can lend $0.90 but must hold $0.10 as my reserve? reply hectormalot 14 hours agorootparentIt’s a bit more complicated than that. At the point I make a loan, 2 things happen on my balance sheet: I have a new liability to you (the increased balance in your account), and I have a new asset (the loan that you’re expected to pay back). They cancel each other out and it therefore seems as if I’m creating money out of thin air. However, the moment you actually use that money (eg to buy something), the money leaves the bank (unless the other account is also at this bank, but let’s keep it simple). Liabilities on the balance sheet shrink, so assets need to follow. That needs to come from reserves because the loan asset keeps its original value. The reserve comes from the bank, not from you. Added layer here: Banks can borrow money from each other or central banks if their cash reserves runs low. Finally: it tends to be the case that the limit on lending is not the reserves, but on the capital constraints. Banks need to retain capital for each loan they make. This is weighed against the risk of these loans. For example: you could lend a lot more in mortgages than in business loans without collateral. Ask your favorite LLM to explain RWAs and Basel III for more. reply AnthonyMouse 12 hours agorootparent> However, the moment you actually use that money (eg to buy something), the money leaves the bank (unless the other account is also at this bank, but let’s keep it simple). Liabilities on the balance sheet shrink, so assets need to follow. That needs to come from reserves because the loan asset keeps its original value. \"Everything should be made as simple as possible but no simpler.\" You're omitting the thing that causes the money to be created out of thin air. If the other account is at the same bank, now that customer has money in their account that didn't previously exist. And the same thing happens even if the money goes to a customer at another bank -- then that bank's customer has money in their account that didn't previously exist. Even if some reserves are transferred from one bank to another, the total reserves across the whole banking system haven't changed, but the total amount of money on deposit has. And the transfers into and out of the average bank are going to net to zero. The created money gets destroyed when the loan is paid back, but the total amount of debt generally increases over time so the amount of debt-created money goes up over time as banks make new loans faster than borrowers pay them back. reply gastonmorixe 8 hours agorootparentprevThe bank could also sell the loan instead of borrowing if they are in need of capital. reply throw0101a 4 hours agorootparentprev> So for every $1 deposited, I can lend $0.90 but must hold $0.10 as my reserve? The GP is completely wrong on how modern finance works. Banks do not lend out deposits. This was called the \"Old View\" by Tobin in 1963: * https://elischolar.library.yale.edu/cowles-discussion-paper-... The Bank of England has a good explainer on how money is created: * https://www.bankofengland.co.uk/quarterly-bulletin/2014/q1/m... See also Cullen Roche: * https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1905625 * https://rationalreminder.ca/podcast/132 reply ianburrell 14 hours agorootparentprevThat is exactly what happens. Reserve ratio used to be 10%, same as your example. The reserve ratio is currently zero, lowered in 2020 during pandemics. But banks still can't lend out more than deposits. reply dataflow 12 hours agorootparent> The reserve ratio is currently zero, lowered in 2020 during pandemics. I saw this during the pandemic, and it bewildered me how little coverage of it there was. How is this not going to cause another financial catastrophe? And if we're so sure it isn't, then what makes people think they under economics so well, given that they clearly thought a minimum was necessary just a few years ago? reply throw0101a 4 hours agorootparent> I saw this during the pandemic, and it bewildered me how little coverage of it there was. How is this not going to cause another financial catastrophe? The banks in Australia, Canada, etc have had zero reserve requirements for thirty years: * https://en.wikipedia.org/wiki/Reserve_requirement#Countries_... The US had reserve requirements leading up to the 2008 GFC which started off with mortgages/loans, and yet those requirement didn't stop the disaster. Canada et al did not have requirements, and yet it didn't have a financial meltdown (not itself, only as 'collateral damage' to what happened in the US). reply AmirS2 1 hour agorootparentprevBecause what matters is _Capital_ requirements, which differ by the _risk_ of the loan. A bank's Capital is what limits their ability to lend. Reserve requirements are irrelevant in the modern banking system. reply schnitzelstoat 9 hours agorootparentprevMany central banks like the Bank of England don't even have a reserve requirement and rely on the bank rate to control it instead. The equivalent for the USA would be the Federal Funds Rate, I suppose. The reserve requirement is just one tool among many. reply hgomersall 11 hours agorootparentprevFortunately, loans create deposits, so they are always in balance. reply immibis 4 hours agorootparentprevDeposits≥Loans is a tautology since every time loans increase, so do deposits. It doesn't mean anything or provide any insight. reply willmadden 12 hours agorootparentprevThe bank IS printing new money. You are ignoring the money multiplier effect where the money lent by bank 1 is deposited into bank 2, bank 2 lends 90% of that deposit, which is deposited into bank 3, ... repeating the process over and over. With a 10% reserve requirement, a 1,000,000 USD deposit will result in up to 10 times that much money being lent out. The formula is 1/r, where r is the reserve requirement. reply neffy 6 hours agorootparentThat´s not correct unfortunately, although it has been widely taught in economics text books, and you can blame Keynes for that. Keynes used that example to try and explain the process to parliament, and also to argue that the system didn't expand the deposit money supply over time. Ironically even the data (in the Macmillan report) he supplied contradicted him. It´s confusing as well, because the fundamental rules have changed over time. Banks can lend up to an allowed multiple of their cash or equivalent reserves (gold standard regulation), and in the Basel era are also regulated on the ratio of their capital reserves to their loans. This acts to stop hyperflationary expansion, but there is a feedback loop between new deposits and new capital so the system does still expand slowly over time. This may be beneficial. In engineering terms, Banks statistically multiplex asset cash with liability deposits, using the asset cash to solve FLP consensus issues that arise when deposits are transferred between banks. It´s actually quite an elegant system. reply itsoktocry 3 hours agorootparent>Banks can lend up to an allowed multiple of their cash or equivalent reserves And what is the current reserve requirement in the US? Zero. https://www.federalreserve.gov/monetarypolicy/reservereq.htm Edit: Whoops, someone beat be to it below. reply ArnoVW 11 hours agorootparentprevinterestingly, the Fed's page on Reserve Requirements states: As announced on March 15, 2020, the Board reduced reserve requirement ratios to zero percent effective March 26, 2020. This action eliminated reserve requirements for all depository institutions. So in effect, the multiplier is infinity. https://www.federalreserve.gov/monetarypolicy/reservereq.htm reply gastonmorixe 8 hours agorootparentI remember this. Have they ever rolled it back? reply neffy 6 hours agorootparentThe Basel Capital rules dominate at the moment. If that ever gets rolled back... buy gold immediately. reply thaumasiotes 16 hours agorootparentprevThere's more to it than that; balances are exceeded by the sum of \"assets held by the bank\" and \"assets owed to the bank\". reply tomgp 6 hours agoprevThis reinforces for me the importance of domain knowledge in engineering leadership. i.e. if you work for a finance company you need to have a decent understanding of finance in order to make the correct technical decisions and tradeoffs, same goes for journalism, same goes for commerce etc. etc. Successful organisations I've worked for have always included domain specific, non-technical, questions to tech team interviews whilst some of the most technically acomplished teams I've worked with have floundered through a lack of domain insight. reply sotix 3 hours agoparentI’m a software engineer and a CPA, so I’ve got the domain knowledge. However, I’m not sure how to find a gig where I can apply that. It seems everywhere I look vastly prefers someone with twice my experience as a software engineer and zero domain knowledge over my years of experience in accounting and fewer years of experience as a software engineer. Any insight on how to utilize it effectively? reply softfalcon 2 hours agorootparentUnfortunately, the likely solution to this is to keep banging out work at wherever you can gain experience as a software engineer. Modern hiring practices are too rigid and filter-friendly for you to likely appear as a strong candidate based on the fact you have good accounting experience on top of your growing software skills. What will really help you though, is having friends who work at a bank in the software departments. It's almost always who you know in this world. You need to network, network, network! reply scott_w 2 hours agorootparentprevAs someone who worked on a bookkeeping system without being an accountant (or bookkeeper in any sense), I'd say your challenge is that it's very possible to learn enough to build a decent system, assuming your engineering knowledge is strong. I don't say this to blow my own trumpet, only to say that the non-engineering leadership at the company in question were very invested in the product details and making sure I understood basic accounting principles. That said, I went from that role to working in a billing system that was originally built by a team where leadership weren't invested in the details. The result was a lot of issues and frustration from Finance leadership. Storing monetary values in float was not the biggest issue, either. That being said, maybe branch out of just looking at accounting/bookkeeping and market yourself as \"I know how money works in software systems.\" Your skills are extremely transferrable and knowing the Finance expectations will make it easier to make good design choices further upstream. reply gamblor956 25 minutes agorootparentprevYour ideal job for utilizing both would either be to work for one of the ERP providers or accounting software providers (Oracle, NetSuite, Workday, Xero, etc.) or to launch your own targeting a specific need. reply manyxcxi 2 hours agorootparentprevI may be able to help, I just sent you an email. reply salawat 2 hours agorootparentprevYou know, it's kind of funny, but it seems like most businesses are not interested in doing things \"right\". As doing things \"right\" is time consuming, hard, and cuts off avenues to \"innovate\". \"That's not how accounting works\" is like telling management to clean their room. No one wants to hear it. reply crazygringo 1 hour agoparentprevI don't know about that -- I've always understood that to be the role of the product manager, to have all the domain knowledge. It's the PM's job to work with engineering to ensure that the requirements are correct, and that the built product meets those requirements. And in an agile setting, those conversations and verifications are happening every single sprint, so something can't go for too long without being caught. If you don't have a PM, then sure I guess your engineering team had better have deep domain knowledge. But otherwise, no -- it's not engineering's responsibility. It's products's responsibility. reply spencerchubb 3 hours agoparentprevever since i started working for an insurance company, i realized that understanding the insurance industry is far more difficult than understanding the codebases. if the codebase is acting weird, at least i can step through it with a debugger! reply collinmcnulty 5 hours agoparentprevI agree with your sentiment here, but I think it helps to think of those domain specific questions as technical questions, just from a different technical domain. Finance is technical, mechanical engineering is technical, even sports management and sociology have big technical components too. I think having an expansive view of what technical competence is breeds the humility necessary to work across domains. reply tomgp 5 hours agorootparentYes, sure, if it helps you to think in those terms I guess that makes sense. Being a bit reductive perhaps but I think what it comes down to is having an interest in the \"why\" as much as the \"what\" and the \"how\". reply figassis 5 hours agoprevThis is the type of thing where it helps to hire the right people from the start. You hire a bunch of leetcode experts, but never stop to ask if, besides conjuring data structures and algos in their minds they actually can build the thing you want to build. If your people know how to build the thing, you don't need to sacrifice growth, it gets built right from the start. Sometimes you need engineers that have some other type of education, maybe accounting, maybe finance, maybe biology. I always thought that the most important part of my career was understanding every industry I built for, deeply, and knowing experts in those areas so you could ask the really important questions. That is problem solving and engineering. The rest is programming/coding. reply manyxcxi 5 hours agoparentI have never come across a post on HN that was so scarily describing my current day to day and with a comment I agree with so wholeheartedly. I’ve spent the majority of my career in tech with a finance angle to it, mostly sales and use tax compliance. What I never fully appreciated was how much those accountants, controllers, and lawyers were rubbing off on me. I was recently advising a pretty old startup on their ledgering system and was beyond appalled at what it looks like when a bunch of engineers with no finance or accounting background build an accounting system. We don’t have to find magical accountant engineers either, it will wholly suffice if we sit actual accountants in with our engineering team during the design process. After my design of their complete overhaul I had a friend who is a CPA completely review my work, we found a few holes in certain scenarios, but by and large we were good. Money is a difficult engineering problem because with money comes all the human goofery that surrounds it. reply gregw2 17 hours agoprevPardon me an old story... I never built a double entry accounting system but decades ago I did build a billing system for a internet/telcom startup that grew to a modest 8 figures revenue. By accident and not knowing any better as a young dev, I ended up building the billing logic from day one, and for better and worse building it in two places in the system (on a consumer-facing billing webpage, and on a separate backed process that generated invoices and charged credit cards.) It turned out to be remarkably hard to keep them in sync. We were constantly iterating trying to get traction as we burned down our capital, releasing new products and services, new ways of discounting and pricing (per use, per month, first X free, etc), features like masterpayer/subaccounts for corporate accounts, user-assignable cost centers, tax allocation to those cost centers with penny allocation, etc such that new wrinkles and corner cases would keep popping up causing the numbers on my two screens/methods not to match. Being personally responsible for the billing, I would go over all the invoices by hand for a couple days each month to insure they matched before we charged the credit cards and mailed out printed invoices as a final check to prevent mistakes. There was always/often some new problem I'd find affecting one or a small handful of customers which I would then fix the code before we billed. I never felt good letting go and not doublechecking everything by hand. I thought about refactoring the billing logic to occur in one place to eliminate these mismatches and my manual crosschecking, but after a lot of thought I realized I wasn't comfortable with a single codebase and liked having two codebases as it helped me catch my own errors. I then just made it easier and easier to run automate and crosschecks between the two. The billing code was a little too gnarly to be proud of, but I was very proud of the outcome in how accurate our billing was, the lack of complaints and many near misses we avoided for many years. I do feel twinges of guilt for the complexity I left my successors but I still don't really regret it. After that experience, the motivation for double entry bookkeeping has always made a lot of sense to me. I had sort of reinvented it in my own hacky way with double logic billing code to prevent my mistakes from causing problems for my customers... reply EdwardDiego 10 hours agoparentBilling, or anything else involving money, is so easy to get wrong. The data team I ended up leading at a previous company, had an unfortunate habit of \"losing\" money - it wasn't real money being lost in transit elsewhere, but records of something we should charge a customer. Or if the team wasn't losing revenue, it was double charging, etc. etc. Took us 3 years of hard work to regain the trust of the business leaders. reply mydogcanpurr 2 hours agoparentprevThis is also very similar to N-version programming :) reply loeber 12 hours agoparentprevGood comment, thanks for sharing reply icapybara 13 hours agoparentprevNo offense but this sounds like a nightmare. It also sounds like you did a fantastic job achieving accuracy despite the complexity of the system. That’s something to be proud of. reply namaria 10 hours agorootparentThis is The Nightmare. Devs building systems they barely understand, complexity leaking all over the place, and someone inheriting that awful job of keeping it running without having made any bad decisions themselves. Software is full of these systems. reply rkharsan64 7 hours agoprevMost of the comments here echo what the article is criticising. I can see countless threads of back-and-forth defending single-entry bookkeeping. Sure, single-entry bookkeeping might be easier and more normalized, but sometimes it is a good idea to just stick with the systems and abstractions that have been developed over centuries. Just use double-entry bookkeeping unless you definitely need something else. Sure, it might be icky for the programmer inside you, but I think you'll be thankful if you ever need to get actual accountants involved to sort out a mismatch. On a related note: does anybody know of any good resources for programmers in payments and adjacent fields? Something like an \"Accounting for Programmers\"? reply karanbhangui 4 hours agoparentThese articles might be a good starting point: https://martin.kleppmann.com/2011/03/07/accounting-for-compu... https://www.moderntreasury.com/journal/accounting-for-develo... reply inopinatus 14 minutes agorootparentI read an article from Modern Treasury that advocated for mutable pending transactions to vary entry amounts by replacing entries¹, which is just about the worst idea I ever heard in the design of a DE system, and their reasoning boiled down to: if you're running a settlement system but are too lazy to implement a clearinghouse layer separately, no worries, just violate the entire DE covenant instead. So I'd take anything they write with a pinch of salt. [1] https://www.moderntreasury.com/journal/how-to-scale-a-ledger... reply fragmede 3 hours agorootparentprevI consider that first link, Accounting for Computer Scientists, as the canonical guide for computer scientists as to wtf double entry accounting is and why it's the right way to do it. reply sotix 3 hours agoparentprevA Basic Introduction to Accounting[0]. [0]: https://www.winstoncooke.com/blog/a-basic-introduction-to-ac... reply neuroelectron 54 minutes agoprevI don't know what the fuck I just read here. I've worked on ledgers for years with massive codebases with millions of loc and I've never seen an error of \"a few cents\". Feels like I just read a alibi. Don't develop financial software from scratch. reply 392 10 minutes agoparentThe lesson here is not to never develop from scratch. In fact it may be faster and safer to do so than to vet all of the open source junk that modern ByteByteGo-system-designers think they need. TigerBeetle and Jepsen-proof-engineering are paths to dig down here, along with basic knowledge of computer number storage. The lesson for me here is that there is no substitute for knowing or caring about what you're doing. Dancing cents don't happen when eng cared about correctness in the first place. I feel like I just read a college freshman explaining how there's no possible way they could have passed their first exam. Better yet he's here to sell us on his thought leadership without ever explaining why he failed the first exam. reply watt 9 hours agoprevI don't understand why the author chooses to bring up the mantra “make it work, make it right, make it fast“ in a negative light, perhaps he misunderstands where the \"make it fast\" comes in? to clarify: \"make it right\" is the second step, and until you make things work correctly (\"right\"), that's where the work stops, you have to make the system work soundly. the \"make it fast\", as in, optimize, comes in only after you have got it right, that all correctness and soundness issues are resolved perfectly. then you can start optimizing it (making it run fast). it has nothing to do with delivery speed. it has nothing to do with working quickly. it's about optimizing only as a last step. perhaps the author is lamenting the fact that it is possible for something to sort of \"work\", but to be so far from being \"right\" that you can't go back and make it right retroactively, that it has to be \"right\" from the inception, even before it starts barely working? reply badcppdev 9 hours agoparentI think your last sentence is correct. I think author is saying that you can't make it work and THEN make it right with payment systems. This is my opinion as well and I've been involved in the audit of a fintech system where auditors had to download EVERYTHING into excel spreadsheets and make the numbers balance before they would sign off on the books. That took a lot of time and money I'm guessing made a difference of at least .1 unicorns in the liquidity even that took place 3 years later. reply jeremyjh 5 hours agoparentprevThey said the engineering team followed this mantra, which includes the phrase \"make it right\". But they didn't. They didn't even bother to find out what they didn't know about fintech before they started building a product. Given the context in which he used it, I think the misunderstanding you suggest in the first sentence is most likely. Immediately afterward he talks about the time pressure startups face. reply zaphodias 9 hours agoparentprevI think author chose the wrong mantra too! What happens in fast paced startups is that you ship what essentially is a MVP as soon as possible (i.e. you stop at the \"make it work\" step) because you need to build your customer base, finances, etc. A better mantra would've been Facebook's \"move fast and break things\". But, that only works if you can fix the things later. You wouldn't do it if you're building an aircraft for example. reply bob1029 4 hours agoprevIf you are trying to reinvent parts of banking, you could maybe start with what already works as a thought experiment. Here's an example of a core banking deposit transaction schema that has been extensively battle tested in many small & mid-size US institutions: https://jackhenry.dev/open-enterprise-api-docs/operational-d... You may note fields like \"Effective Date\" & \"Affects Balance/Interest\", which imply doing this correctly may involve exploring some interesting edge cases. Wouldn't it be cool if you could just cheat and start with an approach that already considers them? reply Aeolun 18 hours agoprevNo tests either? If you lose track of enough money every transaction that you can make an example of 'Every $5 purchased resulted in $4.98 in the transaction log' I think your problem is far, far bigger than not having double entry bookkeeping. Who builds a financial system like that an considers it normal? The compensation is one thing, but you'd flee a service like that with all possible haste. reply hotsauceror 17 hours agoparentThese guys. They said it themselves. “We could have built it right, but we didn’t.” They chose not to. It was not an accident. They made jokes about “dancing cents”. They did these things because there would never be meaningful consequences for doing them, and they knew it. They move fast, they broke things - money things - and they laughed about it. And now they’re lecturing people as if having willfully made these decisions gives them both moral and technical authority. This is magnificently pompous, startup VC culture nonsense. reply fallingknife 16 hours agorootparentSounds like they refunded anything that went wrong so it's not really as bad as you make it sound. reply rsanek 15 hours agorootparentcritically, only when customers reached out. which means tons of people that weren't eagle eyed got defrauded. reply Retric 16 hours agorootparentprevRefunds limit damages in a lawsuit, but don’t prevent legal issues. Especially important when explicitly saying you’ve done these things. reply doctorpangloss 3 hours agoparentprevHow do you figure the story is true? reply tyre 17 hours agoparentprevYeah that was my thought as well. Ledgers have tons of benefits but they’re not going to fix your dancing cents problem. You’re going to have bad number on the ledger. Sure, maybe that points you to the bugs, but so would writing basic tests. reply amluto 17 hours agorootparentA ledger where you insist that every entry touches exactly two accounts, in a business where transactions regularly involve various types of fees, could easily misplace or misattribute a few cents here and there. This type of business can also have fun dealing with accruals. One can easily do a bunch of transactions, have them settle, and then get an invoice for the associated fees at variable time in the future. reply akira2501 15 hours agorootparent> where you insist that every entry touches exactly two accounts A ledger is where every transaction balances to 0. It can involve multiple accounts, but the sum of all transfers between all accounts in a single transaction must sum to 0. This is the property of double entry that actually matters. reply hgomersall 8 hours agorootparentMaybe I'm being naive, but this seems to be not too difficult... You have a general ledger in which the invariant that it always balances is properly enforced. The hard bit is scaling and performance. There are policies around fractional transactions, but you should never get mismatched entries. reply qznc 7 hours agorootparentI think the trick is to store the movement not the balances. You may cache the balance, but what matters is the journal of money moving from one account to another. reply bobnamob 7 hours agoprevFor a glimpse at the _real_, human consequences of this kind of slipshod mentality toward money shuffling software see the Post Office scandal[1]. https://en.wikipedia.org/wiki/British_Post_Office_scandal Anything that moves money should be treated with upmost seriousness and be aware of as many historical mistakes as possible. reply throw0101a 4 hours agoparent> https://en.wikipedia.org/wiki/British_Post_Office_scandal The four-part miniseries with Toby Jones (mentioned above in §Media:Dramatisation) was really good and goes over things pretty well: * https://en.wikipedia.org/wiki/Mr_Bates_vs_The_Post_Office reply stavros 2 hours agoprevIt really irks me that the author assumes I know how double-entry accounting works and doesn't mention a single sentence about it. I read half way through the article and couldn't follow it, except that single-entry is bad and double-entry is good. reply antgiant 1 hour agoparentCould have been clearer but it is there. Here is the relevant section. In short single entry is basically the Account view and Double entry is the full ledger. (Called double because of the hard requirement that all Entries come in pairs.) > Ledgers are conceptually a data model, represented by three entities: Accounts, Entries and Transactions. > Most people think of money in terms of what’s theirs, and Accounts are the representation of that point of view. They are the reason why engineers naturally gravitate towards the “simplicity” of single-entry systems. Accounts are both buckets of value, and a particular point of view of how its value changes over time. > Entries represent the flow of funds between Accounts. Crucially, they are always an exchange of value. Therefore, they always come in pairs: an Entry represents one leg of the exchange. > The way we ensure that Entries are paired correctly is with Transactions. Ledgers shouldn’t interact with Entries directly, but through the Transaction entity. reply jessriedel 1 hour agorootparentThat’s more than halfway through the post. The GP’s point that railing for pages about how X is good and Y is bad before defining them stands. reply remram 1 hour agoparentprevA double-entry system is one where you can't change the balance of an account, you can only record transfers between accounts. This means it's impossible to forget to update the other side of a transaction, it's a single step. A consequence of that is you can check that the sum of all accounts is always 0. In practice you have virtual accounts like \"cloud expenses\" and \"customer subscription\" that only go up/down over time, to be the counter-party for transactions in/out of your company. So it's not impossible to mess up, but it eliminates a class of mistakes. reply stavros 1 hour agorootparentHm, but what's the second entry? Do you not just add a single entry from X to Y? reply Izkata 1 hour agorootparentEntry1: Account A: receives 1000 from Account B Entry2: Account B: sends 1000 to Account A And from GP: > A consequence of that is you can check that the sum of all accounts is always 0. Entry1 + Entry2 = 1000 + -1000 = 0 Amusingly, when I made my own personal money tracking program over a decade ago for my bank accounts, this was how I implemented transfers between them just because it was simpler to do. Years later when I heard this name, I also had trouble understanding it because I assumed I had done it the bad way and banks did somehow did something more in-depth. reply inopinatus 58 minutes agorootparentWhat you have done is record credit and debit entries in the same column, distinguishing credits and debits by sign. This is a design choice in the data structure for DE systems. It's a tossup whether that's better than either of the alternatives. In the case of moving money between regular bank accounts in the same institution, you regard that as a movement between two asset accounts, whilst the bank regards that as a movement between two liability accounts. So their entries would have the same magnitude as yours, but inverted signs. reply stavros 1 hour agorootparentprevSo basically it's not really the transfer that's the base of the system, but the account? Ie each account gets its own entry, and because there are two parties in each transaction, we naturally get two entries? reply inopinatus 1 hour agorootparentThere are not \"parties\" in an DE transaction. The legal entity or entities involved - if any - would be described in the linked commercial document that explains the transaction, and the chart of accounts that describes the meaning of each account code. There is no requirement for a transaction to have exactly two entries. The term \"double-entry\" is slightly misleading; it is only trying to express that both sides of the accounting equation are equal. When recording a sale, for example, it is more likely that three or more entries are journaled, due to the additional coded entries for sales tax, and those produced for separate line items for each SKU, shipping etc. A better phrase is \"two-sided accounting\", which is also used, but less commonly than \"double-entry\". reply stavros 1 hour agorootparentI see, and would that transaction get four entries (two for the money, two for the tax), or three (two for the money, one for the tax)? reply inopinatus 1 hour agorootparentNeither of those cases precisely. The vendor's transaction record for such a sale would include a debit to cash, a credit to revenue, and a credit to sales tax liability. The total amount of the credits would equal the cash debit. reply Izkata 1 hour agorootparentprevI may have the terminology a bit off, but the core table in my implementation represents transactions. The columns are Account, Description, Type, Value, Date. Type is to distinguish transfers between my accounts and actually adding/spending money. SUM(Value) on just the transfers between my accounts adds to 0 just like the example above. SUM(Value) on everything tells me how much money I have across all my accounts. reply foobarian 2 hours agoparentprevI've been trying to figure out double-entry accounting for years now and still don't get it. Most explanations are along the lines of \"Here is a simple explanation that you are guaranteed to understand: \" which lacks intuition on why this is useful. I suspect I would need to do a gig as an accountant and run into some error conditions that double-entry solves to really grok it. Edit: no offense but sibling comment is an example :P reply tetha 1 hour agorootparentFrom what I've learned from a few guys, double ledger accounting is a technique which optimizes for consistency, error detection and fraud detection. Each movement of money, or material should always be written down in two or possibly more places. Ideally by independent people or systems. Once you pair this with another entity or company doing the same, it becomes very hard for money or goods to vanish without the possibility to track it down. Either your books are consistent (sum of \"stock\" + sum of \"ingress\" - sum of \"egress\" - sum of \"waste\" makes sense), or something is weird. Either your incoming or outgoing goods match the other entities incoming or outgoing records, or something is amiss in between. This is more about anomaly detection, because paying a clerk can be cheaper than losing millions of dollars of material by people unloading it off of a truck en-route. reply stavros 2 hours agorootparentprevAgreed, exactly the same here. I was hoping the article would explain, but sadly, no. reply stocknoob 2 hours agorootparentprevImagine you’re running a physics simulation. With conservation of energy: if energy shows up somewhere, it went down somewhere else. We shouldn’t assume a balance changed on its own without any other effects in the system. If you start “losing net energy” you can see what’s going on. reply jjmarr 2 hours agoparentprev> A double-entry system is an accounting method that tracks money at both its source and destination. There's an entire section on double-entry accounting in the article. The tl;dr is that if you take money out of an account, you need to place money in another account and vice versa. So, you have a table called \"accounts receivable\" which track money the company is owed. If you actually get paid, you remove money from \"accounts receivable\" and add money to the \"cash\" account, instead of just increasing the amount of cash you have. It makes it much more difficult to lose track of money or have it stolen. In a single-entry system, I could receive money from a customer for services owed and just keep it for myself. reply anonzzzies 2 hours agoprevI saw this so many times. It is rather depressing. Even experienced devs hardly know enough about floats, rounding etc to implement these things, but they do. We were asked to troubleshoot a project for an online insurer ; the devs did two conversions; js and php. So the amount from the frontend ended on the server and got converted and got into the db as an entry. Definitely depressing how frontend numbers are converted and actually this means something to the end result. But the php conversion was also wrong, so it was often cents off. I believe people should never be allowed to have a 'senior developer' job again, but this is really quite commonplace. Most companies don't have enough revenue to notice or care, but given how often we see it, I would say it's actually normal. reply grishka 1 hour agoparentWhile I've never worked on financial systems myself, I was told many times that monetary amounts should always be stored and manipulated as integer amounts of smallest units of currency (cents). One reason being that integer math is always exact, while floating-point isn't. And when money is involved, you want your math to be exact so everything always adds up every step of the way. So I'm always calling it out when someone is using floats to store money. reply zitterbewegung 15 hours agoprevIf told people that I was using a database system where one and 100th of the data was missing after every 10 transactions would you seriously take my advice as an engineer blog post like these that seem to be calmly focused toward an advertisement of a person’s or a group’s promotion and that it’s just introducing concepts. reply namaria 10 hours agoparentWat reply fifilura 9 hours agorootparentThe user name \"zitterbewegung\" sounds like a German. They build sentences like that. You have to think about a sentence like a stack and read it slowly and carefully and take note of the push and pop operations. Germans are like RPN where everyone else is a regular calculator. reply zitterbewegung 5 hours agorootparentI don't know German I was using the voice recognition on an iPad. reply namaria 9 hours agorootparentprevI'm afraid it's not just sentence structure that is the problem here: > one and 100th of the data was missing No idea what that means > an advertisement of a person’s or a group’s promotion What in god's name is an \"advertisement of a group's promotion\"? reply zitterbewegung 5 hours agorootparentI was illustrating to the concept if you have a database solution that loses data randomly it would be seen as a joke compared to a dual entry ledger seen here. It feels like they were promoting themselves instead of addressing a real problem since dual entry ledgers have been used before computers in finance. reply monktastic1 3 hours agorootparentDid you mean just \"one hundredth?\" (i.e., 1%)? \"One and one hundredth\" suggests 101% to a native English speaker. Edit: ah, you probably spoke \"one one hundredth\" and got a transcription error. reply roelschroeven 7 hours agorootparentprevI read that as \"(an advertisement of a person) or (a group's promotion)\". reply callmeal 6 hours agorootparentprev>> one and 100th of the data was missing >No idea what that means 1/100th of a dollar is a cent - goes towards the \"missing cents\" glossed over by calling that \"dancing cents\" in the blog post. reply monktastic1 3 hours agorootparentYes, but why one and 1/100th? reply tekla 1 hour agorootparentTurns out people have different ways of saying the same concept because language is ultimately human and not a mathematical proof. reply 2mol 11 hours agoprevGreat article. I have an observation to the \"engineers should know this and do good engineering\" though: I work for a payments company and there is a fundamental career problem with becoming the ledger ninja: It's not enough work, and it's eventually done! I've seen the following major phases of this work: 1) Build the ledger (correctly), and it will work well for a while. 2) Add convenience code for the callers, assist finance in doing reports/journaling, fix some minor bugs, take care of the operational bits (keep the database up). 3) Reach the scaling limits of your initial approach, but there are some obvious (not trivial) things to do: re-implement the transaction creation directly in the database (10x perf gain), maybe sharding, maybe putting old tx into colder storage, etc. This is spread out over a while, so I haven't seen it be a full-time job, even at real startup-level (+10% MoM) growth. Even if it was, that's one person, not a whole team. I understand engineers that instead are pulled towards projects where they are in higher demand. In another comment somebody said ledger systems are trivial when done right and super hard when done wrong - so if you did a good job it kinda looks like you just created 3 tables and some code. That seems thankless, and job searching as this type of specialist is harder than just being a generalist. reply scary-size 10 hours agoparentI suspect this is a \"small company\" problem which don't sell too many different things. A larger enterprise might have a platform for connecting businesses with businesses and businesses with customers. They might sell services (subscriptions) but also one-time purchases which require different forms of revenue recognition. Those might even be split up across different revenue streams. You end up building sub-ledgers for each one of them because the ERP can't handle the scale. Oh, and you're a public company so make sure everything is SOX compliant and easy to audit. Ah, and you operate on a global scale so do all those things in different currencies for different legal entities. There's a reason Stripe is as successful as it is. And then there's a world where a company outgrows Stripe. There are worse career choices (\"prompt engineer\" LOL) than financial engineering. reply fragmede 3 hours agorootparentStripe isn't a bookkeeper/accountant. Something like http://Pilot.com is though. stripe on bookkeeping: https://stripe.com/guides/atlas/bookkeeping-and-accounting reply jes5199 10 hours agoparentprevI’ve built my career on cleaning up should-have-used-a-ledger messes. it’s hard but there’s always another company that needs it, and I get bored staying in one place. recently I discovered that in a medical billing context the system is way, way weirder than I had seen before. I shipped the three tables, but getting everything into it seems like it might be an endless battle reply naasking 1 hour agoprevEveryone keeps focusing on double entry book keeping, but that's a ledger that's more suited to manual book keeping. We're in the computer age, people should be using the richer accounting model of REA: https://en.wikipedia.org/wiki/Resources%2C_Events%2C_Agents You can see that this model has all of the features discussed in the article, and then some, and REA events map naturally to something like event sourcing. You can project a REA dataset into a double entry ledger, but you often can't go the other way around. reply MarkMarine 53 minutes agoparentI read the source you sited but it seems like a foundational part of the model is basically double book accounting: “At the heart of each REA model there is usually a pair of events, linked by an exchange relationship, typically referred to as the \"duality\" relation. One of these events usually represents a resource being given away or lost, while the other represents a resource being received or gained.” This is what I see as the main difference between single book accounting and double book accounting, with REA having some OO things added to the model to more accurately represent business objects when a computer is used. What am I missing about REA that makes it better than double book as implemented in the way this post was talking about implementing it? reply naasking 39 minutes agorootparentDouble entry bookkeeping (DEB) records entries in ledgers. There are no ledgers in REA, therefore REA is not double-entry bookkeeping. You could argue that when representing both DEB and REA in an entity-relational model, they might have some similar looking tuples and relations, but that does not entail that they have the same data model. As I said in my initial post, REA is a richer data model that captures more information. You can reproduce the ledgers of DEB from a REA system, but you cannot go the other way in all cases. reply amluto 17 hours agoprevDoes anyone have a good explanation for why a ledger database should care about credits and debits and normal balances? This has always seemed more natural to me as a front-end / presentation layer concept. Your ledger entries always sum to zero for each transaction, your income account has a negative balance, and you display it in a sensible manner. I’m also surprised that this whole article starts by discussing stock trading but has no mention of how to represent stock trades. I assume they are “Sagas” consisting of money moving from the customer to the clearinghouse (or prime broker or PFOF provider or whatever) and shares moving from that provider to the account at which the shares are held. And maybe other associated entries representing fees? It seems to me that this is multi-entry accounting, which is quite common, and that entries don’t actually come in pairs as the article would like us to think. reply BillyTheKing 12 hours agoparentyes, agree, I think a 'source' 'destination' model is significantly more straight-forward. Just record the 'source' account and the destination account and you essentially end up with a ledger as a directed graph (Martin Kleppmann wrote a great post on it) I also wrote a super short post on how to model such a system on postgres https://blog.nxos.io/A-simple-double-entry-ledger-with-sourc... Blockchain actually kinda nails it, that's in essence a source/destination ledger, no 'postings' or similar needed, and from a balance calculation POV has been working pretty well One reason this model isn't applied in accounting, in my personal view :), is simply historical and the fact that the number 0 didn't exist when accounting principles were created. Wrote another post on how to model debit/credits on a source/destination ledger here: https://blog.nxos.io/Debit-and-Credits-on-a-Source-Destinati... It's very straight-forward, you just have to accept that asset accounts have negative balances and present the absolute amount instead of a negative amount in a view. reply qwertox 5 hours agorootparent> you essentially end up with a ledger as a directed graph The page contains a comment from Matheus Portela who pointed to a blogpost of his about \"Double-Entry Bookkeeping as a Directed Graph\" [0]. \"I've also had the same problems you described here and double-entry bookkeeping is the way to go for financial accuracy. As a programmer, things clicked when I realized this system is an extended directed graph.\". It turned out that: \"Hi Matheus! Would you believe me if I told you that I read your post in preparation for this article?\" [0] https://matheusportela.com/double-entry-bookkeeping-as-a-dir... reply jnwatson 11 hours agorootparentprevIt isn't always clear which is \"source\" and which is \"destination\" and now you need a bunch of new conventions about these things. Accounting already has these (admittedly arbitrary) conventions so we might as well use those. reply jes5199 10 hours agorootparentprevokay but how do you model a three-party transaction? say you want to collect fees or taxes reply BillyTheKing 9 hours agorootparentyou simple create two records linked by one 'transaction', the source in both cases is the same account, while the destination for one of those postings is the fee account and the other destination is a merchant or similar account. And you can link as many of those postings under a single transaction reply jes5199 2 hours agorootparentokay so now you’ve got double-entry bookkeeping except all of your credits/debits have two dollar values instead of one. let’s call it “quadrupal-entry” reply tossandthrow 5 hours agorootparentprevYou never have 3 party transactions - if you do, you would not be able to track money flow. You can have multiple transactions. One to pay tax, one to pay fees and one to pay the actual thing. You bundle these things in another abstraction, eg. An invoice. reply rawgabbit 14 hours agoparentprevIt is best explained by common scenarios an Italian merchant in the Middle Ages experienced. The basic concept is Assets==Liability (plus Equity). Where positive Assets are entered on the left hand side (debit). And positive Liabilities are entered on the right hand side (credit). In accounting, debit and credit just means left and right. 1. Merchant takes out a loan for $5,000 and receives $5,000 in cash. • Assets (Cash) increase by $5,000 (Debit). • Liabilities (Loan Payable) increase by $5,000 (Credit). • Equity remains unchanged. 2. Merchant buys inventory for $1,000 cash. • Assets (Cash) decrease by $1,000 (Credit). • Assets (Inventory) increase by $1,000 (Debit). • Total assets remain unchanged, and liabilities and equity are unaffected. 3. Merchant sells all inventory for $1,500 cash. • Assets (Cash) increase by $1,500 (Debit). • Assets (Inventory) decrease by $1,000 (Credit) (recording cost of goods sold). • Equity (Retained Earnings) increases by $500 (Credit), representing the profit ($1,500 sales - $1,000 cost). 4. Customer1 deposits $500 in cash for future delivery of goods. • Assets (Cash) increase by $500 (Debit). • Liabilities (Unearned Revenue) increase by $500 (Credit). • Equity remains unchanged. 5. Customer1 transfers half of the future delivery of goods to Customer2. • No changes to assets, liabilities, or equity occur at this point. The merchant’s obligation to deliver goods (reflected as Unearned Revenue) is still $500 but now split between two customers (Customer1 and Customer2). Internal tracking of this obligation may be updated, but the total financial liability remains the same. reply noisy_boy 4 hours agorootparentActually it is clear as long as you remember that main point you made: debit and credit just means left and right. We are all spoiled by thinking of debit/credit as equal to decrease/increase respectively because that how we interpret our bank accounts. That understanding totally collides with formal accounting where debit/credit DON'T mean decrease/increase respectively. I think this is the root cause of all confusion about double-entry accounting. I may be wrong about this, happy to be corrected but that is the bit my brain grinds against when trying to make sense of things. E.g. I replaced all instance of debit with \"Left\" and credit with \"Right\" in your example: 1. Merchant takes out a loan for $5,000 and receives $5,000 in cash. • Assets (Cash) increase by $5,000 (Left). • Liabilities (Loan Payable) increase by $5,000 (Right). • Equity remains unchanged. 2. Merchant buys inventory for $1,000 cash. • Assets (Cash) decrease by $1,000 (Right). • Assets (Inventory) increase by $1,000 (Left). • Total assets remain unchanged, and liabilities and equity are unaffected. 3. Merchant sells all inventory for $1,500 cash. • Assets (Cash) increase by $1,500 (Left). • Assets (Inventory) decrease by $1,000 (Right) (recording cost of goods sold). • Equity (Retained Earnings) increases by $500 (Right), representing the profit ($1,500 sales - $1,000 cost). 4. Customer1 deposits $500 in cash for future delivery of goods. • Assets (Cash) increase by $500 (Left). • Liabilities (Unearned Revenue) increase by $500 (Right). • Equity remains unchanged. 5. Customer1 transfers half of the future delivery of goods to Customer2. • No changes to assets, liabilities, or equity occur at this point. The merchant’s obligation to deliver goods (reflected as Unearned Revenue) is still $500 but now split between two customers (Customer1 and Customer2). Internal tracking of this obligation may be updated, but the total financial liability remains the same. I find this much more easier to reason with. reply rawgabbit 3 hours agorootparentYes exactly. With assets liabilities and equity having a left and right entry, they were following the convention when posting a journal entry to the ledger, left entries must equal right entries. (Debits must equal credits). Because A=L+E, we get assets to the left and liabilities to the right. reply amluto 14 hours agorootparentprevI understand this. But we’re talking about computers, not Italian merchants. Italian merchants had actual pieces of paper. Computers have tables and views and frontends that are separate from the tables. Any self-respecting accounting system should be able to produce a balance sheet that matches the conventions you’re describing. I don’t think it follows that the actual numbers in the database that get summed to produce the total liabilities should be positive. reply FredPret 13 hours agorootparentI've often wondered about this in the shower. Why debits and credits, when we can just make income negative and let everything sum to 0? Then you can track the balance for each account in a single field in the database. And the answer is that \"0\" first entered Europe around the time they invented double-entry bookkeeping there. Negative numbers reached Europe centuries after that. I showed the internals of a number-line-based accounting systems to an accountant once, and he was so confused by the negative incomes. https://en.wikipedia.org/wiki/Negative_number#History https://en.wikipedia.org/wiki/Double-entry_bookkeeping#Histo... reply rawgabbit 12 hours agorootparentprevI think we are talking about two different things. Yes, of course you can build an accounting system using whatever database algorithm and programming framework you like. But your users expect debits and credits and A=LE or A-L=E because that’s what their auditors expect. In the scenario four I presented earlier, I believe it is intuitive to think of unearned revenue (liability) as a positive number. When the customer picks up the order, the unearned revenue will be transferred to equity. reply bvrmn 8 hours agorootparentprevThank you for an example. But I don't see how it explains why debit/credit should be used instead of simple signed amount. Like how Transaction(from, to) where `from` and `to` are Entry(account, credit|debit, unsigned amount) make things easier than Entry(account, signed amount). You basically used different labels for positive or negative amount in the example. reply rawgabbit 6 hours agorootparentThe story I was told and what I believe is that the journal entry is and always is the source of truth. A merchant may have several journals. A separate one for each line of business and maintained by separate clerks. The different journals would then be consolidated into a single ledger. So he can tell what his equity is. When transferring the journal entry to A=L+E. Those early accountants used their version of Excel. For Assets, They took a page and drew a vertical line. For Liabilities, they also drew a vertical line. Same for equity. They called the left side debit and the right side credit. We don’t know why the Italians named it this way. We can only assume the first ledgers dealt with paying down amounts of credit they owed others. Anyways this early “excel” allowed simple ledgers to have two columns. Positive asset changes go to the left and negative to the right. Positive liabilities changes to the right and negative changes to the left. Same thing for equity. I assume this was mantra they told themselves to ensure correctness or reconciliation. When transferring a journal entry to the ledger there must be a debit and a credit or there is fraud. For example an unscrupulous clerk may have taken a loan out. The journal entry may not tell where that m",
    "originSummary": [
      "The Payments Engineer Playbook underscores the critical role of accurate ledgers in fintech startups, highlighting the pitfalls of neglecting double-entry accounting systems.",
      "Alvaro Duran shares a cautionary tale from his startup experience, where the absence of a double-entry system led to financial discrepancies and customer dissatisfaction.",
      "Duran is contemplating authoring a book on creating scalable and future-proof ledgers, seeking feedback on its potential interest."
    ],
    "commentSummary": [
      "Engineers in financial systems must avoid errors, as seen in Synapse's case, where their ledger inaccurately showed more customer balances than actual funds, resulting in missing millions.- Fintech companies often promise FDIC insurance, but it only applies if the bank fails, not if the fintech mismanages funds, highlighting the importance of accurate financial tracking.- Double-entry bookkeeping is vital for accuracy, ensuring every transaction is balanced and traceable, which helps prevent errors and potential fraud in financial systems."
    ],
    "points": 373,
    "commentCount": 382,
    "retryCount": 0,
    "time": 1732835027
  },
  {
    "id": 42273505,
    "title": "Don't Fuck with Scroll",
    "originLink": "https://dontfuckwithscroll.com",
    "originBody": "Don't Fuck With Scroll Momentum (aka. smooth or interia) scrolling plugins (example here), while marketed as enhancements, are a plague upon the internet. They disrupt the natural, efficient, and predictable web browsing experience in countless ways, by they often degrading usability, accessibility, and performance. Here are ten reasons why they ruin the web for everyone. Violates User Expectations Users know how scrolling works. It's been the same since the dawn of the web: you scroll, content moves. Momentum scrolling plugins hijack this fundamental behavior. Instead of instant and predictable movement, users get some bizarrely animated experience that feels more like playing a bad video game. Muscle memory? Tossed out the window. It's like giving people a steering wheel in a car and making it turn the opposite direction—just for kicks. Impact: This disrupts muscle memory and established habits, which users rely on for efficient navigation. The experience feels artificial and clunky, making users feel like they're fighting against the page instead of interacting with it naturally. Causes Motion Sickness Not everyone wants a theme-park ride when reading an article. Momentum scrolling plugins introduce floaty, swoopy animations that feel like watching a shaky cam on repeat. Users prone to motion sickness or vertigo (and there are more than you think) can't handle these unnecessary flourishes. What's worse, many sites don't even offer an option to turn it off, leaving people stuck feeling queasy just trying to read your blog post. Impact: Users prone to motion sickness or vertigo may find the website literally nauseating to use. Is your content so unimportant that you'd rather users leave than scroll down? Probably not. Reduces Accessibility for Disabled Users Web accessibility is a right, not an optional feature. Momentum scrolling plugins laugh in the face of inclusivity. They disrupt assistive technologies like screen readers and keyboard navigation by introducing timing delays. For users with disabilities, such as motor impairments or visual limitations, these delays can render a site unusable. A feature that excludes millions of users isn't a feature—it's a flaw. Impact: Users with disabilities may struggle to interact with the site. Do you really want to alienate an entire demographic just to make scrolling look a little fancier? Accessibility isn't optional—it's a baseline requirement. Inconsistent Performance Across Devices Momentum scrolling plugins don't care if you're on a state-of-the-art gaming rig or a five-year-old budget phone. They load JavaScript that can lag, stutter, or outright break on older or lower-end devices. Why make your website cater only to people with perfect hardware? The web is supposed to work for everyone, not just the ones who can afford shiny new gadgets every year. Impact: Instead of feeling \"Momentum,\" the page feels broken on weaker devices. Great job—now you've ensured your site only works properly for people with high-end tech. Good UX means working for everyone, not just the rich. Impairs Usability for Power Users Power users exist. They're the ones who zoom through documentation, rapidly scroll through pages, and want precision. Momentum scrolling spits in their faces. It slows down their workflow by forcing them to endure molasses-like animations. They're not here for your slow, cutesy effects—they're here to get things done. Stop standing in their way. It's like making a racecar driver go 20 mph because it \"looks prettier.\" Impact: These users will hate your site. And let's be clear, power users are often the ones most likely to recommend or share content. Do you really want to alienate your most enthusiastic audience? Increases Page Load Times Momentum scrolling plugins add bloated JavaScript libraries, extra dependencies, and more CPU cycles to render animations. Guess what? That comes at a cost. Impact: Slower load times annoy everyone. On mobile networks, especially in areas with poor connectivity, your fancy scrolling makes the page slower and less accessible. Congrats, you've sacrificed performance for aesthetics. Breaks Native Browser Features Modern browsers already include Momentum scrolling settings for users who want them. Adding third-party plugins often overrides or conflicts with these native features, breaking custom scroll gestures or momentum scrolling. Impact: Your site ends up less functional than the browser it's displayed in. Users who expect their preferences to work (e.g., system-wide reduced motion settings) will feel betrayed. You just took something that worked and made it worse. Makes Scroll Position Unclear Momentum scrolling animations introduce a delay between action and result, making it hard to tell exactly where you are on the page. Combine this with long pages, and users are left feeling lost. Impact: Users now spend extra time figuring out their position. Nobody likes a website that feels like a guessing game. Quick navigation becomes a chore instead of a feature. Great job, you just turned scrolling into a problem. Adds Maintenance Overhead Momentum scrolling plugins aren't \"set it and forget it.\" They need regular updates to stay compatible with modern browsers, operating systems, and devices. And every update risks introducing new bugs. Impact: You've now created extra work for your development team, all for a feature nobody really asked for. That's time and money that could've been spent making your site faster, more secure, or better optimized. But sure, keep chasing that \"Momentum\" scroll. Disrespects the User's Control Users come to your website for content, not for an overly choreographed scrolling experience. Overriding default scrolling assumes your vision of scrolling is better than the user's preferences or needs. Impact: By enforcing Momentum scrolling, you're essentially telling users: \"We know better than you.\" That arrogance doesn't go unnoticed. Respect your users' autonomy—don't dictate how they interact with your site. Conclusion Momentum scrolling plugins are the web equivalent of turning a functional bike into a unicycle because it \"looks cool.\" It adds unnecessary complexity, degrades usability, and frustrates users. Instead of reinventing scrolling, stick to what works: native, predictable, fast scrolling behavior. Don't make scrolling a thing. Just let people scroll. And if you're still not convinced, go to the smooth version and try to tell me it's better. Epilogue Inspired by the geniuses behind Motherfucking Website.",
    "commentLink": "https://news.ycombinator.com/item?id=42273505",
    "commentBody": "Don't Fuck with Scroll (dontfuckwithscroll.com)306 points by a_siekierski 6 hours agohidepastfavorite111 comments onemoresoop 5 hours agoI’d add don’t fcuk with with urls and browser navigation and the back button but it’s a long lost battle, SPAs broke the web and made it much worse. reply Cthulhu_ 4 hours agoparentBut SPAs don't need to, they chose to. The history API is fine and can be used to have SPAs and their navigators work with the browser and user, instead of against it. But a lot of developers broke it because they ignored bookmarkability and navigation and the like because they didn't care. A good SPA works just like a regular website and other than it being fast and responsive, you shouldn't even notice it is one. But alas. reply safety1st 2 hours agorootparentIt's because when you build a SPA you give up a bunch of stuff that comes with the traditional web page model, and it's stuff that most teams overlook until it's too late. They are usually fixated on a certain set of performance issues that drove them to do an SPA in the first place, their energy goes into that, and then they kind of look around at all the other stuff and go, oh, we don't have time for those things. This is why I typically fight SPAs every step of the way with our clients, if any other architecture will work I see if there is a good case to be made for it, because I know that SPAs are usually going to end up shipping broken in some way. reply ervine 1 hour agorootparentWhat kind of stuff do you give up with an SPA? reply nurettin 48 minutes agorootparentYou give up server redirects and cookie auth. (well you don't lose cookie auth, but you need extra steps for it to prevent CSRF) reply ffsm8 3 hours agorootparentprevThis didn't come through pwas. We've had ajax for ages before they became a thing and mpa also had opaque state for modals/detail views etc. Blaming SPAs for such things is like blaming the ocean for a hurricane... Sure, you'd have less hurricanes without the ocean, but it's ultimately not the reason for them reply axegon_ 3 hours agorootparentprev> fast and responsive I'm sorry but I have to disagree. I've never seen a fast or responsive SPA, not to mention the absurd amounts of resources they require. A tab taking up 400 mb of memory is pure insanity and this is the de facto standard these days. Look at HN: this is the definition of fast and responsive. It will work equally well on my dual-xeon workstation and a first gen raspberry pi zero. This is fast and responsive. Most people who use a computer to navigate the internet would not care if the browser blinks when you navigate a page. Mobile users don't really use the web beyond social media and chats so it makes no difference to them either. SPAs solved a problem no one had by creating 1000 other problems. Just take us back to server side html rendering and I guarantee most ordinary people as well as most developers will love it. reply danpad 1 hour agorootparent> I've never seen a fast or responsive SPA Here, let me blow your mind. It's in Czech, but you can click the orange buttons to try the activites: https://gramazing.com/features . Made with NextJS (client side only), Django, hosted in Germany on a cheap VPS. reply robertlagrant 30 minutes agorootparentprev> SPAs solved a problem no one had The problem was: can we make the same data APIs to serve the web and mobile apps? reply aredox 4 hours agorootparentprevWhy did we let them be able to do so? reply ikari_pl 3 hours agorootparentif done properly, this allows the url to represent where you are in the app, and makes back/forward work - like in typical web pages, just without a reload. Most often though, I see it done inconsistently, and it's a painful mess reply marcosdumay 1 hour agorootparentprevBecause some sites need to change those things. So they can't just deny it for everybody. reply pdimitar 4 hours agorootparentprevWho is \"we\" and how could \"we\" stop them exactly? reply afavour 3 hours agorootparentprevBecause when implemented properly it is a better user experience. reply karmakaze 3 hours agoparentprevYou know what I don't like? Having to hit the back button far too many times while I was interacting with an element on the page. It makes sense that I should be able to bookmark or send a particular state but I much prefer if the URL is changed in-place (without creating a navigation step) to achieve this. reply ervine 2 hours agorootparentYah agreed - updating params when filtering / sorting / paginating should replace instead of push to the history stack. Easy enough to do. reply agos 4 hours agoparentprevthere are very easy ways to make SPAs which don't fuck with URLs (since, I don't know, ten years?). People who don't care about the web fundamentals/urls/back button/scroll UX/etc have broken the web, not SPAs reply Habgdnv 4 hours agorootparentI have a single SPA deployed in my life. About 5-6 years ago. I did nothing to preserve the history, it was just working back then. Imagine how more polished are things now. I am thinking that people who are breaking the history should take special steps to do so, otherwise it will just work out of the box with most frameworks/libs. reply tehwebguy 4 hours agoparentprevOr the escape button, Squarespace! reply zelon88 2 hours agoparentprev> the back button Especially Microsoft TechNet articles and the like. Microsoft should be ashamed breaking back functionality on people searching for solutions to windows problems. Just adds one more complaint to the list. reply MrMcCall 2 hours agoprevAmen! I've been on the internet since the dang Gopher days (so, pre-HTML), and I appreciate the bare HTML aesthetic's minimalism. As well, their link at the bottom to the website that inspired this opinion piece page is chef's kiss. Viewing the page source on these kinds of pages gives me warn fuzzies, not in a nostalgic way, but in a \"these folks get it\" kind of way. It's like in the Helvetica documentary where the guy explains what it must have felt like for designers to newly be able to choose Helvetica instead of those 1950s crap script fonts. Bravo. reply hypercube33 4 hours agoprevAlso don't mess with scroll bars. Seems to be a more recent trend to make them about 1px wide on everything reply analog31 4 hours agoparentIndeed, tiny scroll bars make it impossible to use a touch screen. If you really want your app to be ergonomic, make sure it works with one and two finger gestures, such as dragging a finger to scroll. reply karmakaze 3 hours agorootparentTouch screens should support two-finger drag scrolling which is a much bigger hit-target than any scrollbar. reply zeta0134 1 hour agorootparentSure, but I can grab the scroll handle and yank it down to 75% instantly. How much superfluous motion do I have to endure on a long page to even find 75%? Can I see how far along I am on the page with the invisible scroll bar that barely renders anything? Is the rendering of its overall progress consistent between applications, if not completely broken? The boring, \"ugly\" built-in OS widget works extremely well. Every attempt to replace it with something better for one use case inevitably throws out a dozen others that the designer didn't think about. reply Findecanor 4 hours agoparentprevI'm OK with them popping up on mouse-over, many web sites have taken the easy route and disabled them completely on one or both axes. Then the only way to scroll on desktop is to highlight text across the frame's edge. reply alt227 4 hours agorootparentMiddle click your mouse button then move mouse to the right, thats the best way to do it. reply badmintonbaseba 4 hours agorootparentshift + mouse scroll for horizontal scroll works too, at least on Firefox. reply bongodongobob 4 hours agorootparentprevNah, I hate that. Frequently it gets in my way when I have my screen split with two browsers side by side. I'll try to hover over the scrollbar on the right hand side of the left page, try to click, and end up clicking the right window. Give me the whole damn scrollbar please. reply CamperBob2 2 hours agorootparentExactly. I have a 5120-pixel-wide monitor. I can afford the space needed to display a scroll bar. reply zwnow 4 hours agoparentprevI usually hide them. I try to not use overflowing content, but when it has to overflow I hide the scrollbar because scrollbars disrupt the design and the standard ones are extremely ugly. reply Cthulhu_ 4 hours agorootparentHow can you make an assertion like \"they are extremely ugly\" when they're different or absent entirely depending on device and browser? Don't make decisions for your users, this breaks expectations and accessibility. reply zwnow 4 hours agorootparentI dont want users that need handholding anyway. I want my app to look good. Almost all scrollbars on all of my devices and browsers look ugly. Users that need handholding usually also are the more problematic users. reply dwb 3 hours agorootparentWhat a horrible, anti-social attitude. I hope you aren’t working on anything important. reply zwnow 3 hours agorootparentShocker, apps do not have to be for everyone. reply dwb 1 hour agorootparentYou don't know who you are excluding by working against accessibility. It's very naive and small-minded of you to think that you are only inconveniencing some caricature in your head. reply zwnow 1 hour agorootparentI am inconveniencing my time which is important to me. I dont have to build my apps accessible to everyone as they are clearly not purposed for everyone. Whats so hard to understand with that? reply TimK65 4 hours agorootparentprevWay to spit in the faces of disabled people! reply zwnow 4 hours agorootparentSo if i am building my own house i should also spend thousands of dollars more to make it accessible? If my house isnt accessible do i also spit disabled people in the face? reply rusty_venture 3 hours agorootparentThat's a poor analogy - a house is private property which only you and invited guests may enter. A website you publish on the Internet can be viewed by anyone who discovers it. Not including accessibility features excludes disabled users from reaping the benefits of your site. So a better analogy would be \"if I build a public space do I have to make it accessible?\" You betcha! reply zwnow 2 hours agorootparentI guess this is a philosophical discourse. I simply build my own apps not for everyone and its not my problem if someone has a problem with that. If I build something thats meant for the public I try to build it accessible, but only if I feel like its needed. Seems like there is a market for accessibility software, why not jump on it? Why isn't the accessibility software good enough to not make the devs go extra steps on projects that are clearly not meant for everyone? The internet is for everyone, sure. My apps aren't. Simple as that. reply amatecha 2 hours agorootparentSooner or later, you will be less able than you are now. Everyone faces disability, it's just a matter of when, or how rapidly it happens. reply zwnow 1 hour agorootparentI already face disability as i have to wear glasses to see properly. Still wont change the fact that I wont put the burden of making my apps accessible onto myself as they clearly aren't meant for disabled people and/or people that I don't want to use my app. reply awkwardpotato 1 hour agorootparentprev> Why isn't the accessibility software good enough to not make the devs go extra steps on projects that are clearly not meant for everyone? Is it not an extra step to intentionally hide the scroll bar? And thus, simply no extra work to just... not? Scrolls bars are a convenient feature for all users, nobody is judging the appearance of your website around how it looks with a scroll bar. reply zwnow 1 hour agorootparentTry building a modern looking website only for scrollbars to look like they are from 1990. They break immersion. Especially on artsy websites that try to immerse people. reply wizzwizz4 0 minutes agorootparentThis is like people who use synonyms prolifically, replacing all of their words so that lexemes do not repeat. It appears jarring and somewhat pretentious, past a certain point: readers will rarely notice the common UI paradigms which have come to be expected by them, but viewers assuredly become aware upon contradictory circumstances. Some feel these phenomena are grating. owittnan_ 3 hours agorootparentprev> So if i am building my own house i should also spend thousands of dollars more to make it accessible? If my house isnt accessible do i also spit disabled people in the face? not hiding the scrollbars on your website doesn't cost you thousands of dollars so I don't really see the equivalence reply zwnow 3 hours agorootparentYea but theres a difference in showing scrollbars and making the whole website accessible. reply arkh 3 hours agorootparentprev> So if i am building my own house i should also spend thousands of dollars more to make it accessible? Well, you don't know what tomorrow you will need. I remember my grandmother needing to make the house accessible after my grandfather lost control of half his body after a stroke. And those accessibility enhancements are not a problem even if they're not directly needed anymore: no steps between rooms, big sliding doors, lot of space for the shower etc. Even able people can appreciate those. reply zwnow 3 hours agorootparentWhats the issue with getting them once they are needed? reply arkh 3 hours agorootparentIt may be harder to had them or more expensive. And the fact you suddenly need them also mean there are low chances you'll be able to do the work yourself. Like: build a house on only one floor so no stairs => no need for elevators later on. But it requires more floor space. reply written-beyond 2 hours agorootparentprevI would've agreed with you if I didn't start with Firefox on my web design journey. I never found scroll bars to that problematic because FF has pretty sleek ones. So I always ensured scrolling worked on overflow, when I started to more seriously test and use chrome I was absolutely disgusted by those vile thick scroll bars that would ruin my amateur design. But by that time I had understood that scroll bars are a need not a want. reply zwnow 1 hour agorootparentThey are a want. You can keep scrolling functionality without having scrollbars. If you write apps that serve the biggest demographic possible I get your sentiment. I dont want to serve to the biggest demographic possible, I want to serve specific people. reply bayindirh 4 hours agoparentprevI love vanishing scrollbars. The moment I move my mouse, they're there. When I stop, they're gone. Want to see progress? Nudge the mouse. reply MrMcCall 2 hours agorootparentI'm glad at least one person likes that \"feature\". I guess folks who rotate their monitors to portrait mode would appreciate not wasting the entire height on a permanent scroll bar, but I like knowing how far I am into the page (not that that's really a true indication of where the page will end due to how large the modern header and footer regions tend to be nowadays). reply Philpax 1 hour agorootparentDon't forget lazy-loaded content as a further perversion of the scrollbar :'( Discourse is my mortal enemy. reply myth2018 3 hours agoprev> you're essentially telling users: \"We know better than you.\" That arrogance doesn't go unnoticed. Respect your users' autonomy IMHO, that assessment and corresponding answer applies perfectly not only to Momentum scrolling but also to most of the current trends in UX design. I wonder how we got into this worship of aesthetics to the detriment of everything else. reply bitnasty 3 hours agoparentI don’t think it’s fair to blame the problems on “trends in UX design.” It’s more the lack of UX design. reply myth2018 3 hours agorootparentTheoretically, I agree. UX shouldn't be about making systems look good. In practice, though, this appears to be the main concern (maybe because it's the easiest one to grasp?), leaving all the other ones subordinated to it. Hence I would blame the \"excess\" of UX design. reply riperoni 3 hours agoparentprevYeah, that part reminds me of the flatteringly named \"Call to Action\" buttons. There was a nice blog article named something like \"Button presses You\" which i can't seem to find. Essentially, it goes on the trajectory of the purpose of any application telling you what and how to do instead of you, the user, deciding what the programm should do. reply myth2018 2 hours agorootparentIndeed. CTA buttons and everything. Instead of simple, readable text-based menus, icons everywhere because they are \"more intuitive\". Navigation aids because, otherwise, \"users get lost\". Useless animated pictures together with apologetic error messages whenever the system (frequently) crashes. Everything facilitated by tools like Figma, enabling designers to be creative by pointing and clicking and leaving the multiple implementation and maintenance details as an exercise to developers who have now to \"specialize\" on front-end. What a hell. Yesterday I had to use a government web app and got lost. They even bothered to create some video tutorials but unfortunately the software went recently through a redesign to \"improve UX\" and now the icons don't match the ones in the tutorial anymore. The amount of damage they create with their cutesy interfaces and their condescending attitude is astonishing. reply cooolbear 51 minutes agorootparent> Instead of simple, readable text-based menus, icons everywhere because they are \"more intuitive\". Navigation aids because, otherwise, \"users get lost\". I'm not especially dumb but after they dropped \"hamburger button\" I think it took me about 10 years to ever look for the \"hamburger button\" for basic functionality, especially on non-touch devices. What horrors lie beneath the hamburger? What void of regular function could there be? reply Joeboy 4 hours agoprevHijacking Ctrl+F or Ctrl+K are my two top peeves lately. reply glonq 2 hours agoparentI'll add hijacking selection and Ctrl-C too. reply karmakaze 3 hours agoparentprevCtrl+F hijacking, even if it provides a useful app search feature is something I despise. Browsers should have an alternate Shift+Ctrl+F or something that can be used in those cases. I just discovered Shift+Cmd+F that does something on Firefox. reply CodesInChaos 3 hours agorootparentThat doesn't really help if the website removes the parts of the page that are outside the current screen from the DOM. reply mdf 2 hours agorootparentA prime example: JIRA's backlog view. In the self-hosted version, you could easily find the issue you were looking for in the backlog view by just using the browser's search, press Ctrl+F, write some words, you have the issue you were looking for. The cloud version Atlassian forced their users into, OTOH, features their own implementation hijacking Ctrl+F combined with dynamically removing the issues not currently visible from the DOM, to ensure that no-one can have the convenience of the browser's built-in search. reply JoshTriplett 3 hours agorootparentprevDon't do that either. reply NegativeK 3 hours agoparentprevOutlook for web hijacks ctrl+r. Don't fuck with the standard key combos. Stop surprising users by taking away functionality from other applications. reply Joeboy 3 hours agorootparentCome to think of google docs spreadsheet thing does this too, and also F5. It also often fails to render correctly so I need to refresh the page. reply layer8 2 hours agorootparentprevThis is why applications shouldn’t live on top of a browser on top of an OS GUI. That’s just restricting the workable UI conventions too much and makes for a confusing mess. reply silon42 2 hours agoparentprevAnd Alt+Left / Alt+Right reply danaris 3 hours agoparentprevHonestly, this is one that I think can go both ways. In Google Sheets on the Mac recently, Cmd-F for searching within the spreadsheet has stopped working—and while it's true that the way it worked was by hijacking that system shortcut, it didn't actually replace it with anything. So the only way to search was to go to the Sheets menu bar and find it in the menu. reply partomniscient 4 hours agoprevWhile we're at it, my hatred for thesymbols on rows of things on previously explicitly only scroll up/down pages is just as intense today as when I first experienced it. Streaming media platforms are probably the most notorious offenders in this space, but are definitely not alone. reply ValentinA23 4 hours agoprevI'd say this also includes fancy landing pages where scrolling influences what is shown in the background or is used to segment the page itself. For instance: https://webflow.com/made-in-webflow/website/Translate-Webflo... reply venkat223 3 hours agoparentOnly a few applications behave like that because of defective coding hamster browser is a good scroller .Tilt scrolls are available in iPad and this helps old people like me to scroll quickly read me there is an automatic scroll also which is very good reply nyc_data_geek1 2 hours agoprevAlso don't fuck with that thing where scrolling down progresses an animation, moves things sideways, gets you past the scrollgate until you can continue scrolling down the page. Terrible, jarring, disruptive, user-hostile design. reply drd0rk 4 hours agoprevThe best part is that the \"Back to the normal version\" link is only clickable if you smooth-scroll all the way back to the top. You really got me with that. reply timnetworks 5 hours agoprevEven the scroll bar is broken on the (other) demo page. Ouch! reply lapcat 2 hours agoprevThe example uses the plugin luxy.js: https://min30327.github.io/luxy.js/ It appears that the plugin basically does two things: 1. Repeatedly calls window.requestAnimationFrame with a callback 2. Set a custom style on a specified element (position:fixed, width:100%, transform:translate3d) You'd have to stop both to override the plugin. reply robin_reala 4 hours agoprevSee also: https://modem.io/blog/scrollbars/ reply giantrobot 3 hours agoparentIntentionally broken scroll on mobile? It's like Poe's Law but for insipid web UX. reply agos 4 hours agoparentprevreally really really well done reply rayiner 4 hours agoprevWhy is it even possible to fuck with scroll. reply PhilipRoman 1 hour agoparentThe inner platform effect is unavoidable. At the extreme end, there will always be someone reimplementing a browser on top of webgl/wasm/whatever. reply extraduder_ire 3 hours agoparentprevIf you try hard enough, it's possible to fuck with almost any aspect of normal web UI. Trying to prevent existing ways of doing it will lead to worse ways being used by people who think they know better. reply dankobgd 3 hours agoparentprevbecause this is web and nothing makes sense. reply dominicrose 3 hours agoprevI enabled a smooth scrolling behavior in my logitech mouse settings and it created a bug where bootstrap dropdowns would appear and disappear immediately. It was far from obvious that this was the cause. In fact I completely forgot about this setting. I thought the problem came from the website or an extension or the browser or bootstrap, damn. reply TheChaplain 4 hours agoprevMy most annoying scroll-experience is Grafana, too easy for people to make dashboards with a bunch of scrollbars and I always fail pick the correct one to scroll down. reply taeric 4 hours agoprevI'm fine if sites make creative choices for things, if they are a creative site. Homestar runner would be a classic example of this done extremely well. The time he \"broke out\" of the frame was amazing. But, if you are making something with plenty of precedent, please follow it. There is a reason we wind up with mandated nutrition labels and such. It is good to be similar to things. Especially if you are, in fact, similar reply Enginerrrd 4 hours agoprevCan we also talk about fucking with zoom? That's my personal pet peeve. When I zoom on an image, it should zoom with the rest of the page, not resize itself to shrink to the same size. That behavior is infuriating. reply robinsonb5 4 hours agoparentnext [–]reply fermigier 4 hours agoprevYeah. Don't fuck with scroll, don't fuck with crtl/cmd-click, don't fuck with copy/paste (specially in password fields), don't fuck with the back button, etc. reply unfixed 4 hours agoprevI really don't face this issue ever, but I tried the demo page and the experience is garbage. I use vimium to navigate on the web and made scrolling through that website unusable. Just don't reply extraduder_ire 3 hours agoprevAre there any examples of sites using this for real? I assume the example is made especially bad to make a point. I hadn't even considered this existing. From the title, I assumed it was about those pages set up to stay fixed on one page and then change entirely to the next one when you've scrolled down far enough. reply Aurornis 3 hours agoparentNow that I think about it, I haven’t seen this in years. There was a period of time where this was popular for over-designed product landing pages, but that fad faded out as quickly as it appeared. If a page with this behavior ever gets linked on HN you’ll know immediately by the 100s of comments raging about it. :) reply stogot 5 hours agoprevSpeaking of which, does Firefox offer the ability to turn this off in about:settings ? Or will that break the event detection ? reply veganjay 3 hours agoparentI'm also curious if this functionality is possible to achieve through browser configuration. Alternatively, a browser extension could potentially provide a helpful solution. I suspect the implementation may vary on a case-by-case basis. The example [1] on the page utilizes the luxy.js [2] library. For this specific page [1], I found that I can disable the smooth scrolling behavior by running the following command in the developer tools console: luxy.init({ wrapperSpeed: 1.0}); 1. https://dontfuckwithscroll.com/smooth.html 2. https://min30327.github.io/luxy.js/ reply mezi 4 hours agoprevCouldn't agree more reply creativenolo 4 hours agoprevOh the irony, the scrollbars on this pages covers the last letters of each line. reply OJFord 4 hours agoparentThat's your browser (and lack of margin on the page). reply dark-star 3 hours agoprevI have never encountered a website that did something like that. Are there any examples? reply cubefox 4 hours agoprevI'm a bit confused. Smartphone operating systems do momentum scroll since the first iOS. Everywhere, not just in the browser. Is this website specifically talking about websites which try to emulate Android & iOS inertia scrolling on desktop browsers? (I guess momentum scrolling doesn't work well with mouse wheels, where the mouse wheel doesn't have inertia.) reply ziml77 4 hours agoparentThey are talking about adding that kind of scrolling using JS as demonstrated in the alternate version of the page that they link to. I don't understand why anyone would think it's needed though when scrolling already works like that at the OS or browser level. Doing it in JS just doubles up on the effect, leading to a very strange feel. reply Finnucane 3 hours agoparentprevAt least on my iPad here, when the screen is scrolling, and you touch the screen, it stops. On the demo page, it doesn’t. Who would want that, or think that was better? If I encountered that, I’d go away from that website and never go back. reply mouse_ 5 hours agoprevMy god, please boost this. reply mromanuk 5 hours agoprev [6 more] [flagged] ziddoap 5 hours agoparentThat is the example, so I'm not sure I see the irony. reply ChrisMarshallNY 5 hours agoparentprevThat's an example of bad scroll. reply close04 5 hours agoparentprev [–] > And if you're still not convinced, go to the smooth version and try to tell me it's better. It's a demo of why it's bad and how not to do it. reply pixelsort 4 hours agorootparent [–] The demo being such a bad example of smooth scroll made me feel ambivalent toward the entire article. On the one hand, perhaps the author is so bad at designing UX that this was their best attempt at a demo and not just a bad faith attempt at proving their point. On the other hand, if someone so clearly informed about the UX issues is inept at implementing it well then perhaps it really is too much power to wield over web standards. reply lapcat 3 hours agorootparent [–] The demo simply uses the luxy.js plugin: https://min30327.github.io/luxy.js/ This is mentioned in the first sentence of the article: \"Momentum (aka. smooth or interia) scrolling plugins (example here), while marketed as enhancements, are a plague upon the internet.\" The article author did not create these plugins. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Momentum scrolling plugins are criticized for disrupting the natural web browsing experience by degrading usability, accessibility, and performance.- Key issues include violating user expectations, causing motion sickness, reducing accessibility, and inconsistent performance, particularly on older devices.- These plugins also increase page load times, break native browser features, and add maintenance overhead, leading to user frustration and complexity."
    ],
    "commentSummary": [
      "The discussion emphasizes frustrations with Single Page Applications (SPAs) affecting web navigation, particularly concerning URLs, browser navigation, and the back button.- Critics argue that improperly implemented SPAs compromise web fundamentals like bookmarkability and navigation, prioritizing performance over user experience.- The conversation also addresses issues such as hijacking standard browser functions and the importance of respecting user autonomy in web design, advocating for maintaining traditional web functionalities."
    ],
    "points": 306,
    "commentCount": 111,
    "retryCount": 0,
    "time": 1732884771
  },
  {
    "id": 42268475,
    "title": "Hetzner raises prices while significantly lowering bandwidth (US)",
    "originLink": "https://adriano.fyi/posts/hetzner-raises-prices-while-significantly-lowering-bandwidth-in-us/",
    "originBody": "Hetzner raises prices while significantly lowering bandwidth (US) 2024-11-28 Content Overview Hetzner raises prices Update New price and bandwidth table Hetzner’s email Hetzner raises prices⌗ This morning I received an email from Hetzner stating that they are raising prices in the US while significantly reducing bandwidth. The largest price percentage increase is 27.52% for CPX21 servers, and the smallest is 4.17% for CX3+ servers. Bandwidth allotments are decreasing on average, across all products, 88.19% from previous allotments. I’ve been a big fan of Hetzner. Unfortunately they’ve made a feeble attempt to dress this change up in the name of “fairness”. With the new tariff structure, we want to make conditions for our customers around the world as fair as possible … Until this change, customers who have used fewer resources have covered the costs, in a way, for other customers who have used much more resources. We want to make things more balanced. The new prices will give our customers the best possible price for the resources they use. Hetzner I’d love to see a better explanation for why prices are increasing so much in exchange for such a significant drop in bandwidth allotments. A lot of Hetzner customers cite generous bandwidth allotments as one of the reasons they’ve chosen Hetzner as a provider. As far as I can tell, that generosity has evaporated in the great US of A. And the fact that this was announced on Thanksgiving Day, a major holiday in the US, certainly deserves a dishonorable mention. Update⌗ Now that I’ve looked at my invoice, Hetzner’s price change and justification is even more frustrating. I personally use 0TB per month across 6 CPX21 servers (I know I’m over-provisioned; that’s not the point). The point is I’ve been paying €42.30 + €1.00/mo for 20TB of bandwidth, of which I’ve used zero on average. What Hetzner’s statement tells me is that even at that price, I was subsidizing some heavy users. And to thank me for it, I can now pay €53.94/mo in exchange for 12TB, for fairness. New price and bandwidth table⌗ From the updated bandwidth and pricing table provided in Hetzner’s email, here are the price changes for CCX and CPX servers in Ashburn and Hillsboro. Product Old(€) New(€) Increase(%) Old Traffic (TB) New Traffic (TB) Traffic Decrease (%) CPX11 3.85 4.49 16.62% 20 1 95.000000 CPX21 7.05 8.99 27.52% 20 2 90.000000 CPX31 13.10 15.99 22.06% 20 3 85.000000 CPX41 24.70 29.99 21.42% 20 4 80.000000 CPX51 54.40 59.99 10.28% 20 5 75.000000 CCX13 11.99 12.99 8.34% 20 1 95.000000 CCX23 23.99 25.99 8.34% 20 2 90.000000 CCX33 47.99 49.99 4.17% 30 3 90.000000 CCX43 95.99 99.99 4.17% 40 4 90.000000 CCX53 191.99 199.99 4.17% 50 6 88.000000 CCX63 287.99 299.99 4.17% 60 8 86.666667 LB11 5.39 5.39 0.00% 20 1 95.000000 LB21 16.40 16.40 0.00% 20 2 90.000000 LB31 32.90 32.90 0.00% 20 3 85.000000 Hetzner’s email⌗ Your client number: XXXXXXXXXX Dear Adriano Caloiaro We are writing to inform you about important changes to the tariff structure of our Cloud servers (CCX and CPX lines) and our Load balancers at our US locations in Ashburn and Hillsboro. What will change? Starting on 1 December 2024, 01:00 am CET, we will begin charging new prices for newly-created Cloud servers and introduce new amounts for included traffic for Cloud Servers and Load balancers at the US locations in Ashburn (ASH) and Hillsboro (HIL). This also applies to existing Cloud servers and Load balancers that are switched to a different tariff using the “Rescale” function. For any existing Cloud servers and Load balancers you have at these locations, the new prices and the new amounts for included traffic will apply later, starting on 1 February 2025, 01:00 am CET. The price for traffic overage will remain unchanged in the new price structure. What are the new prices and amounts of included traffic? Below, you can see a list of the old and new prices and the included traffic. Product Old price New price Old included traffic New included traffic CPX11 € 3.85 € 4.49 20 TB 1 TB CPX21 € 7.05 € 8.99 20 TB 2 TB CPX31 € 13.10 € 15.99 20 TB 3 TB CPX41 € 24.70 € 29.99 20 TB 4 TB CPX51 € 54.40 € 59.99 20 TB 5 TB CCX13 € 11.99 € 12.99 20 TB 1 TB CCX23 € 23.99 € 25.99 20 TB 2 TB CCX33 € 47.99 € 49.99 30 TB 3 TB CCX43 € 95.99 € 99.99 40 TB 4 TB CCX53 € 191.99 € 199.99 50 TB 6 TB CCX63 € 287.99 € 299.99 60 TB 8 TB LB11 € 5.39 unchanged 20 TB 1 TB LB21 € 16.40 unchanged 20 TB 2 TB LB31 € 32.90 unchanged 20 TB 3 TB All monthly prices are excl. VAT and excl. IPv4 addresses. Why are we making these changes? With the new tariff structure, we want to make conditions for our customers around the world as fair as possible. To do that, we will calculate our prices based on local conditions in Europe, Singapore, and the USA. Until this change, customers who have used fewer resources have covered the costs, in a way, for other customers who have used much more resources. We want to make things more balanced. The new prices will give our customers the best possible price for the resources they use. What else do I need to know? Displayed prices and included traffic on Cloud Console For technical reasons, we will display the new prices and amounts of included traffic on the Cloud server and Load balancer detail pages on Cloud Console starting on 1 December 2024. This will also be true for your existing Cloud servers and Load balancers. However, the invoice will still be issued according to the previous conditions. A corresponding note will be displayed for your existing affected products. On the invoice preview on the “Usage” page, you will see the correct existing price for your existing servers. Displayed prices on the Cloud API In the transition time period between 1 December 2024 and 1 February 2025, the Cloud API will already return the new prices and included traffic. This will also be true for your existing Cloud servers and Load balancers. But this will only affect the display here. The invoice preview will display the correct existing price. And of course, your invoice itself will be for the correct existing price. Traffic warning You may see early traffic warnings during the transition time period starting on 1 December 2024 for your existing Cloud servers and Load balancers. Please note that the previous traffic included for existing Cloud servers and Load balancers remains valid until 31 January 2025. If you do not agree with this adjustment, you can cancel the affected products at any time. We ask for your understanding for why we are making these changes. We truly believe that we can continue to offer you our Cloud Servers and features at the best price-performance ratio despite these adjustments. We will be happy to answer any questions you may have. Please write us a support request using your account on Cloud Console at https://console.hetzner.cloud/support Thank you for your trust and support. Kind regards Hetzner Online Hetzner Online GmbH, Industriestr. 25, 91710 Gunzenhausen, Germany CEO: Martin Hetzner, Stephan Konvickova, Günther Müller Register Court: Registergericht Ansbach, HRB 6089, VAT Reg. No.: DE812871812 Terms and Conditions • Legal Notice • Data Privacy • System Policies Comments With an account on the Fediverse or Mastodon, you can respond to this post. Since Mastodon is decentralized, you can use your existing account hosted by another Mastodon server or compatible platform if you don't have an account on this one. Known non-private replies are displayed below. Load comments Read other posts ChatGPT is Slipping →",
    "commentLink": "https://news.ycombinator.com/item?id=42268475",
    "commentBody": "[dupe] Hetzner raises prices while significantly lowering bandwidth (US) (adriano.fyi)296 points by acaloiar 22 hours agohidepastfavorite4 comments trollied 21 hours agoDupe of https://news.ycombinator.com/item?id=42264427 reply dang 15 hours agoparentComments moved thither. Thanks! reply gnabgib 21 hours agoparentprevLots of dupes of this one today 107 points/50 comments https://news.ycombinator.com/item?id=42264668 88 points/38 comments https://news.ycombinator.com/item?id=42264789 reply ChrisArchitect 20 hours agoprev [–] [dupe] Earlier: https://news.ycombinator.com/item?id=42264427 reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Hetzner is increasing prices in the US by up to 27.52% for CPX21 servers while reducing bandwidth by an average of 88.19%.- The changes, announced on Thanksgiving, are intended to align costs with resource usage, as lighter users were subsidizing heavier users.- New pricing and bandwidth limits will take effect from December 2024 for new servers and February 2025 for existing ones, with Hetzner maintaining it offers the best price-performance ratio."
    ],
    "commentSummary": [
      "Hetzner, a web hosting company, has increased its prices and reduced bandwidth offerings in the United States.",
      "The topic has generated significant discussion, leading to multiple duplicate posts on Hacker News, a popular tech news platform."
    ],
    "points": 296,
    "commentCount": 4,
    "retryCount": 0,
    "time": 1732827365
  },
  {
    "id": 42269925,
    "title": "Car tires shed a quarter of all microplastics in the environment",
    "originLink": "https://phys.org/news/2024-11-car-quarter-microplastics-environment-urgent.html",
    "originBody": "400 Bad Request Your request has been blocked by our server's security policies. If you believe this is an error, please contact our support team.",
    "commentLink": "https://news.ycombinator.com/item?id=42269925",
    "commentBody": "Car tires shed a quarter of all microplastics in the environment (phys.org)266 points by geox 17 hours agohidepastfavorite285 comments kordite 14 hours agoA friend and I were talking about the weight of EVs and we assumed my EV would be heavier than his car, a BMW 3. The BMW was heavier. Maybe the average EV is heaver than the average ICE, but if you compare what the EV has replaced for that owner, it might be that the EVs aren't noticeably heavier. I just checked the car I had before the Leaf - a Subaru Outback. It was also heavier. I don't think that taxing vehicles based on weight is the right option though. If the pollution is from tires, then tax tires. Do this based on the compounds present in that tire. If someone drives rashly, doing donuts all over the place, then they as a greater polluter will need to pay more. I don't really know anything about Formula 1, but get them to do the race on a single set of tires. Not for pollution, but for solutions that might make it into regular tires. reply wodenokoto 13 hours agoparentDenmark used to tax cars based on weight as it was considered that weight was equal to wear and tear on the roads. Although that logic should probably have been weight * distance driven. Now there’s a fun tax to implement! https://da.m.wikipedia.org/wiki/V%C3%A6gtafgift (Sorry in danish only) reply bboygravity 12 hours agorootparentWeight * distance is actually literally fun for governments to implement. Why? Because it gives them more taxes, bigger government and as a bonus more spying on citizens. The Dutch government has been talking about this type of tax for decades. The idea is to put a mandatory live-tracking device in every car that sends data to the government about where you are at all times. Currently the tax is based on weight and type of energy source of the car and some of the highest taxes on fuel in the world. This boils down to the same as the weight * distance tax. But why keep it simple if you could complicate it further AND get free live spying as a bonus? reply bcraven 12 hours agorootparentIn UK my yearly MOT records how far my car has driven. Taxing distance doesn't need to track _where_ the car has been, just how far it's gone. reply Ntrails 10 hours agorootparentI seem to recall when selling cars the V5 transfer also has a mileage (so easy to attribute) It isn't precisely easy (MOT and tax timings won't line up etc) and arrears rather than advance etc. We definitely have enough data to do a fair approximation - just high operational overheads to collect reply amanaplanacanal 5 hours agorootparentprevThat depends. Do you also tax when the car is driven outside the country? reply rdsubhas 4 hours agorootparentIt's legitimate to track entry and exit on national borders though. A tax exception based on entry/exit times is doable and better than constant geo tracking. reply wodenokoto 12 hours agorootparentprev> The Dutch government has been talking about this type of tax for decades. The idea is to put a mandatory live-tracking device in every car that sends data to the government about where you are at all times. God damn. I would have gone with \"mandatory service where the odometer is sent to the government, and government keeps track of when last service was done and fines owners who are late\" reply klodolph 12 hours agorootparentprevWhy would you not just use the odometer reading? Cars get sold, eventually. You put the odometer reading on the paperwork to transfer the car. Check that against tax records. Purchaser has incentive to check that the recorded mileage is correct, otherwise they’ll have to pay the tax. The odometer is already tamper-resistant. Not perfectly so, and there is fraud, but there is always tax fraud. reply Tagbert 2 hours agorootparentThe reason I have read is that the car can only be taxed within a given jurisdiction. If you travel outside that jurisdiction then the vehicle would not be taxed by that authority. I could be taxed by an authority in another jurisdiction. The analogy is probably a fuel tax that is paid at the point of purchase. Still, it seems that we could agree that taxes for a vehicle be paid in the jurisdiction where it is registered and just use odometer readings to calculate the distance traveled. reply londons_explore 8 hours agorootparentprev> there is fraud, but there is always tax fraud. It's also a type of fraud which is fairly easy to detect. If a car is recorded as driving just 2000 miles per year, yet freeway cameras detected it driving 100 miles every weekday all year, open a fraud case. reply Bluestrike2 2 hours agorootparent> It's also a type of fraud which is fairly easy to detect. If a car is recorded as driving just 2000 miles per year, yet freeway cameras detected it driving 100 miles every weekday all year, open a fraud case. Sure, but why bother? That would involve a ton of overhead and server time for a system that's still going to miss a lot of travel, thereby limiting revenue. I question whether the added expense of that kind of surveillance system would even recover enough revenue to break even. The same goes for mandatory GPS reporting devices, plus the civil liberties issues associated with such systems would make passing such a tax even more difficult. Most countries have some sort of annual safety/emissions inspection, so any mileage-based tax could just use the odometer readings from the inspection. Sure, a mechanic could falsify paperwork, but how likely is that when it'll eventually come to light? If you want to sell the car, you're going to have to eventually admit the miles you hid so that they match the odometer reading at the time of title transfer. That means you're going to have no choice but to pay the tax eventually. No need to try and build a more perfect mouse trap. reply londons_explore 2 hours agorootparentNah - you get citizens to self report odometer readings annually, or use annual inspections. And you employ a few people to run your 'fraud team' which will use CCTV to catch fraud, and auto-mail out letters with fines. 15 peoples civil service salaries = $1.5M say. They will contact local car park owners, municipalities and states who have ANPR cameras, etc. From each, they'll get a spreadsheet of plate no, date/time and camera lat/lon. Many police departments already centralize that info to search for stolen cars etc. They'll then run the whole lot through a python script to make a database of plate num + annual mileage. They'll then compare that to the self-reported mileage and investigate any underreporting. Assume that this is implemented in the USA, and 1% of people fake the odometer by 50%. Assume the tax is 5 cents a mile. Total vehicle miles traveled is 3e12 miles, and assume we can easily detect 30% of offenders, due to them driving long distances on highways, and fine all those detected 3x the fraudulent amount. Total takings: $337M. Clearly worth enforcing. reply tzs 12 hours agorootparentprev> The Dutch government has been talking about this type of tax for decades. The idea is to put a mandatory live-tracking device in every car that sends data to the government about where you are at all times. Why would they need a tracking device for this? If Google is to be believed the Dutch government requires periodic vehicle inspections. Couldn't they just go by the odometer difference between inspections to get the mileage? reply OptionOfT 12 hours agorootparentprevThey want the live tracking service because they want to change the price per mile driven based on congestion on the road. So your trip from Maastricht to Eindhoven will be a lot cheaper at 2am than it will be at 8am. reply Gud 5 hours agorootparentprevWhy not just tax the fuel? reply bzzzt 3 hours agorootparentThe Netherlands already has one of the highest fuel taxes in the world. There's pushback from fuel station owners near the borders because many people fill up their cars abroad. reply dmichulke 12 hours agorootparentprevIsn't weight * distance proportional to gas consumption? Sure, you'll under-tax the more efficient cars but I don't necessarily see this as a problem. reply amanaplanacanal 5 hours agorootparentNot for EV's. Which is why my state has been researching the move from a fuel tax to one based on mileage. reply benj111 5 hours agorootparentMy current favourite pet policy (UK) is to introduce a zero rate tax band on energy, to help those least well off, and have a higher band beyond some average consumption. You encourage people to use less, and also tax things such as EVs that use more electricity. Of course it doesn't quite capture everything discussed though. reply notpushkin 12 hours agorootparentprevI think just taxing the tires is the solution! More wear on roads should closely correlate with more wear on tires, and each tire likely has a lifetime determined by weight * distance. You need to account for tire structure, but even if you tax all tires the same, it should be a good approximation of what we're looking for. reply jackvalentine 12 hours agorootparentThis is a good way to incentivise people to buy the shittiest tyres they can and stretch them well past usable life because it’ll cost them a motza to replace. reply ed_elliott_asc 12 hours agorootparentprevIs there a way to tax tires (tyres?) based on wear and not when they have holes in? reply jfim 7 hours agorootparentOne potential way to do this would be to weigh the tires at sale and charge a tax based on the weight of the tire. When the tire is ultimately disposed of, a refund is issued based on the weight of the disposed tire. This would encourage both avoiding tire wear and proper disposal of tires. That assumes obviously that people don't cheat the system by making tires heavier somehow when returning them. reply zuhsetaqi 2 hours agorootparentAnd how would a system work, to check all that? To me that sounds like a lot of overhead to make sure no one outsmarts the system reply RobinL 12 hours agorootparentprevIt's actually a fourth power law, not a linear relationship. So trucks are polynomially more damaging. https://en.wikipedia.org/wiki/Fourth_power_law reply tonyedgecombe 10 hours agorootparentI wonder if that applies to the amount of material the tyres shed as well. reply hinkley 9 hours agorootparentWell they have more tires than four vehicles do. reply porphyra 14 hours agoparentprevThe Tesla Model 3 Long Range weighs 1823 kg. The BMW M340i xDrive weighs about 1818 kg. Seeing as these two cars are similar in size, capacity, and performance (0-60 mph in 4.2 s), it is nice to see that the electric option weighs about the same as an ICE car of similar specs. The Leaf, of course, is a very budget car that can hardly be compared to the BMW 3 series. While EVs are just as bad as ICE cars on the tyre microplastics front, they are at least slightly better in terms of brake dust thanks to regenerative braking. reply yreg 13 hours agorootparentI expect they are also slightly worse thanks to the immediate acceleration. Even if 0-60 is similar to a particular ICE car, 0-5 or 0-10 is not. By the way, there are EV tyres - does anyone know why? Is it just marketing or do they have some special properties? reply Moru 11 hours agorootparentI think the difference isn't so much between car models, it's the drivers behaviour that wears down the tyres. Something happens to drivers of cars that has the power to accellerate fast from 0. Electric wears down the tires faster because of how the average driver uses the pedals. reply narimiran 12 hours agorootparentprev> By the way, there are EV tyres - does anyone know why? Is it just marketing or do they have some special properties? They have lower rolling resistance, in attempt to extract a bit larger range. reply zamfi 12 hours agorootparentprevOften they have sound damping foam, and try to be more efficient. reply DanielHB 8 hours agorootparentprevJust to be pedantic, the fuel has some weight too. I think your average cars have a 50 to 80l tank, which is an extra ~40-60kg when full. reply oliwarner 2 hours agoparentprevThose are two fairly substantially different cars though. A Fiat 500e is ~30% heavier than a Fiat 500. I don't think you're wrong that we assume every EV to weigh as much as a Cybertruck, but there is a weight cost to EV power storage. reply pjerem 13 hours agoparentprev> If the pollution is from tires, then tax tires. Why not but … > Do this based on the compounds present in that tire. I’m not certain we’d want to create incentive for less polluting compounds over security. Also, tires are generally expensive and people are already driving with worn out tires regularly for this reason. So why not, in absolute, I agree, but it may create issues. reply rixed 12 hours agorootparent> I’m not certain we’d want to create incentive for less polluting compounds over security. Sure we do, if that pollution causes a greater risk. Those two security costs seam hard to compare though. reply mrweasel 11 hours agoparentprev> If the pollution is from tires, then tax tires. I don't think you can tax tires high enough that it will make a difference, but the same is also true if we attempt to tax by weight. Any tax is going to be the equivalent of slapping €10 on a plane ticket. It's not enough to stop the behaviour, but might be enough to keep some people driving on dangerously worn down tires. It also doesn't matter if the car is an EV or ICE, the behaviour we want to limit is driving. The idea of taxing the tires could of cause lead to development of tires that doesn't shed microplastic. reply cocoa19 11 hours agorootparent“ the behaviour we want to limit is driving” By we, what do you mean? If this was true, we’d build walkable cities, public transportation and promote home office as much as possible. reply mrweasel 10 hours agorootparent\"We\" as in a society that wants to reduce the amount of microplastics from tires (or who wants to reduce environmental impact as much as possible). It's sadly also the same \"we\" that is more interested in preserving the status quo in the name of the al might holy economy as it exists today. The same \"we\" that doesn't want to upset voters. The same \"we\" who won't vote for the greener option because \"we\" can't imagine a future different from yesterday. reply akira2501 14 hours agoparentprev> before the Leaf - a Subaru Outback. It was also heavier. A bigger car with more cargo capacity was heavier? Is that a surprise? How _much_ heavier was the Outback? Only 100 lbs or so? That's the surprise. reply AnthonyMouse 13 hours agorootparentThe Outback is also bigger and has more cargo capacity than the BMW 3 but likewise only weighs 100 pounds more. The weight of electric cars is more proportional to their range than their size, and they also shed the ICE powertrain and exhaust/emissions systems, so the breakeven range where they weigh the same as an equivalent ICE car is a range of something like 200-300 miles. Which is why the BMW 3 and Tesla 3 have a similar weight. The difference is that as new battery chemistries improve energy density, the weight of the electric car can go down. Whereas ICE powertrains are extremely mature with not a lot of low-hanging fruit, so significant improvements in power to weight ratio are less likely to be forthcoming. reply akira2501 11 hours agorootparent> proportional to their range than their size Well and total vehicle weight impacts EV range far more than it does ICE range. So the leaf is both small and with shorter range but the gain is a smaller battery pack. > as new battery chemistries improve energy density You can have improved density today. You're just not going to like the charge and discharge characteristics very much. EVs have lots of multi variable problems due to their efficiency and utilization aims. To be fair it just is a harder problem to solve but the years of development and lack of clear gains are still showing obvious bottlenecks. > so significant improvements in power to weight ratio are less likely to be forthcoming. You can have vastly improved PWR now. Just ride a motorcycle. I think Power train Weight to Total Vehicle Weight is what you really want to think about. In either EV or ICE case there are still plenty of gains to be had here. reply AnthonyMouse 1 hour agorootparent> Well and total vehicle weight impacts EV range far more than it does ICE range. So the leaf is both small and with shorter range but the gain is a smaller battery pack. The Leaf isn't really that small. It's 10\" shorter in length but 4\" taller in height than the Tesla or BMW 3, which in terms of aerodynamics would actually make it worse (larger frontal area), but it's a few hundred pounds lighter because it has less range. > To be fair it just is a harder problem to solve but the years of development and lack of clear gains are still showing obvious bottlenecks. Are they? The GM EV1 from the 1990s was using lead acid batteries. A decade later hybrids were generally using NiMH. Current electric cars are generally lithium ion with double the energy density of NiMH. Things like zinc air or lithium air batteries haven't been effectively commercialized yet but they're under development and lithium air batteries would have an energy density on par with gasoline -- without the weight of the ICE powertrain. By contrast, what's in the pipe that is going to make the ICE powertrain weigh significantly less? > You can have vastly improved PWR now. Just ride a motorcycle. That's just changing the size of the vehicle. There exist electric motorcycles that do around 200 miles to a charge and weigh the same as a Harley. But you obviously can't use a motorcycle for everything you can use a car or truck, and they're incredibly dangerous. As in, for a third of people who ride a motorcycle as their primary vehicle, that's their cause of death. reply juliusgeo 14 hours agoparentprevNot really relevant to your overall point, but I found it interesting that apparently F1 already tried that: In 2005, tyre changes were disallowed in Formula One, therefore the compounds were harder as the tyres had to last the full race distance of around 300 km (200 miles). Tyre changes were re-instated in 2006, following the dramatic and highly political 2005 United States Grand Prix, which saw Michelin tyres fail on two separate cars at the same turn, resulting in all Michelin runners pulling out of the Grand Prix, leaving just the three teams using Bridgestone tyres to race. [1] https://en.wikipedia.org/wiki/Formula_One_tyres#History [2] https://en.wikipedia.org/wiki/2005_United_States_Grand_Prix reply Cumpiler69 12 hours agorootparentTire changes were disallowed in 2005 to break Ferrari's dominance who's strategy relied on super soft sticky tires being charged often. reply fyolnish 6 hours agorootparentWhy couldn’t the other teams change tires as often? reply leejo 3 hours agorootparentFerrari had an exclusive deal with Bridgestone that the other top teams didn't: https://au.motorsport.com/f1/news/the-bridgestone-and-ferrar... # The Bridgestone tire was superior to the Michelin. reply Cumpiler69 6 hours agorootparentprevThey all could, but Ferrari built their car + strategy + drivers' style for multiple fast laps with multiple pit stops as the winning formula. Having just multiple tire changes without the same car, strategy and driver won't have the same results reply raxxorraxor 6 hours agoparentprevUsually you calculate with 20% more weight from EV. Significant, but not as much as the general trend to build housewife tanks. reply danpad 11 hours agoparentprevYour first sentence implies there is some relevance of taking an arbitrary EV and comparing it to an arbitrary ICE. There's not... I am gonna bet anything your random EV is gonna be heavier than a Suzuki Swift. reply d1sxeyes 10 hours agoparentprevNo-one actually wants to do this but the answer is to tax fuel. UK road tax for example already taxes bigger/heavier cars more (albeit not particularly granularly), so there’s your weight component. Fuel consumption is a decent proxy for distance. Obviously there’s some maths needed on how to apply the tax to both ICE and EVs, and to think about edge cases (super efficient but hard on tyres), although my gut says that likely the harder you are on tyres, the harder you’ll be on fuel. reply extraduder_ire 2 hours agorootparentFuel is already heavily taxed in most countries. reply darkwizard42 14 hours agoparentprevIt seems a number of factors contribute to this beyond weight: how you drive (braking and accelerating aggressively), what conditions you drive in, the state of your tires, etc. Weight seems to be one easy to understand and affecting factor. Why not start there? reply AnthonyMouse 13 hours agorootparentBecause the thing you're worried about is microplastics, which are directly proportional to tire wear regardless of whether the wear is from heavier vehicles or more miles driven or idiots doing donuts, so if you just tax the externality directly you don't have to worry about which thing is causing it. reply ArnoVW 11 hours agoparentprevIdeally, sure. But it will result in people stretching their tires more and more, and thus more accidents. We already tax 'distance' by putting tax on the petrol. reply jnsaff2 12 hours agoparentprevDamage to the roads increases with the fourth power of axle weight. It follows that all passenger cars heavy or not, EV or ICE are insignificant for road-damage. One could surmise that there is a similar relation with tire wear and therefore pollution from them as well. But taxing tires is I think a good idea as it is a consumable and the wear and it's impact can be directly measured. The problem I see with taxing tires is twofold: - how is taxing going to solve this problem, it's unlikely that it is going to have a significant impact on driving - can taxes be fed into tire research in a way that reduces the impact on the environment? Are there any solutions that need funding? reply RecycledEle 5 hours agoparentprevDoing a Formula 1 tace on a single set of tires would change the strategy in auto racing. I strongly disapprove. reply swifthesitation 16 hours agoprevThis reminded me of an old article I read about this from `Straight Dope` years ago (2006 in fact!)[0]. > Pollution studies in the Los Angeles basin in the 1980s concluded that more than five tons of breathable tire dust were released into the atmosphere there each day, and there’s no reason to think that figure’s gone down since. [0] https://www.straightdope.com/21343778/when-the-rubber-meets-... reply slavik81 14 hours agoparentWhat's the dust that coats everything stored in a parking garage? Is that tire dust or something else? reply Two4 14 hours agorootparentTire and brake dust, sand, little bit of grease, some soot settling out of the smog, maybe some salt (if you're by the sea) reply hinkley 9 hours agorootparentOr in an area that salts the roads in winter. reply voidfunc 13 hours agorootparentprevI used to have an apartment abutting a parking garage and also above a busy three lane road.. the worst black dust and grime everywhere. Pretty sure moving out of that apartment probably added years back to my life. reply rootusrootus 15 hours agoparentprevFive tons sounds like a lot. But on the other hand, the Los Angeles basin sounds pretty big. (a quick check suggests it is about 1200 square miles) The AQI there looks good. reply pajko 15 hours agorootparentDon't assume that the distribution is uniform. reply rootusrootus 15 hours agorootparentI'm only making the assumptions that I can given what little information is contained in the original statement. We would need better quality information to make any further inferences. reply dv_dt 15 hours agorootparentprevAqi doesn't measure this, does it? reply rootusrootus 15 hours agorootparentIs tire particulate too big to show up in the AQI measurement? If so, does it fall out of the air so fast that it isn't really a problem? reply dv_dt 2 hours agorootparentThe Aqi is an index which is guided only in part by Particulate Matter measurements. The index part was definitely designed before microplastics research without knowledge or regard to is effects. Pm100 doesnt register them, PM2.5 may register them, but grossly with any other particulates of that size - so you dont know if the captured particulates are microplastics or something else or something in between reply bix6 17 hours agoprevWe need to start making manufacturers pay for their negative externalities. reply Aurornis 15 hours agoparentManufacturers aren’t making tires and then turning them into microplastics alone. Pretending consumers aren’t part of the problem is misleading. We could add fees to tire manufacturers, but be honest: It will just get added to the price of the tire. That’s fine if the goal is economic incentives or funding remediation, but people start to lose interest in such fines as soon as they realize it comes out of their own pockets instead of from some imagined slush fund manufacturers are keeping to themselves. (See similar problems with conversations about tariffs, which people only like until they realize they will be paying for them.) reply BriggyDwiggs42 14 hours agorootparentManufacturers don’t make tires expecting them to not be driven on, so that’s besides the point, but regardless. The goal should be to tax manufacturers so that there’s a strong incentive/an opportunity for market competition to produce tires that don’t shed microplastics. reply meowkit 15 hours agorootparentprevIts just one disincentive. Tax driving overall to push people to more efficient (from a tire plastic/energy usage) standpoint. Use those taxes to fund public transportation. reply brianwawok 14 hours agorootparentAmerica generally isn’t laid out that well for public transit. You could build it and have it for free, in many places no one would ride it. reply nox101 14 hours agorootparentplenty of places in America could have far better public transportation than they do. Take the Bay Area vs Switzerland Size: Switzerland 15,940 mi², Bay Area 6,966 mi² Population: Switzerland 8.85 million, Bay Area 7.76 million So given that, the bay area is twice as dense as Switzerland Miles of train tracks: Switzerland 3,241 miles, Bay Area ~300 miles? SF Bay Area has a bay, Switzerland is all mountains so it's not like Switzerland is particularly easier to cover in public transportation Plenty of other places in the USA could be covered in trains. LA for example used to have the largest public transit system in the world. It was all torn down between ~1929 and ~1975. A few lines have been created since but, the problem in the USA is, except for maybe NYC and Chicago, public transportation is seen as a handout to poor people instead of the transit the masses use like most saner places. (Most cities in Europe and Asia). Getting it back to that point seems nearly impossible. Building one track at a time, each taking 10-20 years with Nimbys fighting them all the way means the density of tracks always is too small to be useful, and so no usage. reply rsanek 13 hours agorootparentis there a statistic that can show us the density distribution? my intuition says that the bay area would have a pretty gradual slope (people living mostly everywhere of mostly low density), whereas Switzerland would have lots of areas mostly uninhabited while having a few high concentration cities. looking at the two respective largest cities: Zurich is about twice as densely populated as San Jose. this has a huge impact on public transit viability. reply CalRobert 13 hours agorootparentprevTrue, America bulldozed their cities to build parking lots and roads, which made them much worse for anything but driving. reply lotsofpulp 14 hours agorootparentprevPublic transit only works if people don’t have an option for private travel in a luxurious car. reply rscho 13 hours agorootparentYou never went to western Europe or rich Asian countries ? You should try it and see for yourself. reply lotsofpulp 10 hours agorootparentI have, and everywhere people use public transit, it’s far more expensive or tedious to use a nice, big car. The houses, driveway, garage, and parking situation are inferior to those of 90% of the US, where you can easily take a Ford F150 or full size SUV almost anywhere you want. Cars need space. Walking and bicycling (and public transit) need density. The environment for optimizing for each of those is completely opposite. And once a person has invested in a car (the car itself and a home with enough space to store the car), and they use that car on a daily basis to commute to work or drop the kids off at school, they will be very unlikely to support taxes to pay for public transit, something they will almost never use, since they are already leaving the house in a car, they are going to do all their errands while out in a car. reply fosk 12 hours agorootparentprevDifferent population (and business) density for most of America which is entirely suburban except for the dangerous downtown areas. reply FredPret 2 hours agorootparentprevYou also need law and order. Years ago living in Toronto, I stopped taking transit when the crazies started getting on the train along with the innocent commuters. reply amrocha 13 hours agorootparentprevMaking driving way more expensive takes care of that. reply CalRobert 13 hours agorootparentprevWeirdly the Dutch take the train plenty and also have lots of cars reply amanaplanacanal 5 hours agorootparentAnd also have the best bike infrastructure in the world. I wonder how the average car miles driven per year compares between the Netherlands and, say, the US. reply adrianN 13 hours agorootparentprevThere is evidence all around the world that this is not true. reply ihm 13 hours agorootparentprevConsumers are not part of the problem. There is literally no action a consumer can take to ameliorate this situation because there are no tires produced that don't have this problem, and many consumers need to have a car to live. reply amanaplanacanal 5 hours agorootparentSure there is. Drive less, walk and take public transportation more. People can change their behavior if they are incentivized to do so. And as others are pointing out, buy and drive smaller cars. reply DecoPerson 17 hours agoparentprevManufacture in country A and sell in country B. Or vice versa. But never manufacture and sell in the same country, or the government might try to get you to pay for your negative externalities! And now, there’s this annoying predicament where as you introduce more laws and more enforcement, you only cripple your own economy and rarely cause any significant improvement along the lines of what you hope. Look at Australia — we have all these appliance safety laws, but all of the appliances are made overseas and there’s no good point for the government to inspect and enforce compliance with those laws. I just bought a generic vacuum sealer from an online shop the other day. It was cheaper than buying at a brick & mortar store, even with delivery, and it definitely does not comply with Australia safety standards. We’ve killed our local industry, and our economy is suffering for it. I don’t think the answer is to remove the safety/etc laws, but instead to tax all imports enormously. Be aggressive and unfair so that local industry is immediately viable. It’ll be painful, but it’s what most countries need. Comparative advantage turned out to be a terrible basis for international trade. reply danielheath 16 hours agorootparentThis is what tariffs do well. When you tax a local manufacturer, you impose an equal tariff on imports. reply seventhtiger 16 hours agorootparentThat only makes it fair within your country, but it doesn't remove the self-crippling effects. reply david-gpu 16 hours agorootparentThat is why international agreements like the Montreal Protocol are so important. reply revscat 15 hours agorootparentThat was 40 years ago. In the interim capitalism has won and democracy is failing. Agreements like Montreal will never happen again, at least not in our lifetimes. Look no further than the failure of the Paris Agreement and the ascent of authoritarianism worldwide. No one cares about environmental agreements, certainly not those in the rarified airs of billionaires, oligarchs, and other captains 9f industry. reply david-gpu 9 hours agorootparent> Agreements like Montreal will never happen again They happen all the time. Just look at how the European Union operates on a day-to-day basis. This and the Montreal Protocol wasn't achieved with a self-defeating attitude, though. reply maeil 5 hours agorootparent> This and the Montreal Protocol wasn't achieved with a self-defeating attitude, though. What's clear is that the attitudes of those of us in favour of such measures has only achieved the opposite is the last decade, as the user you're replying to has rightfully pointed out. Optimism has gotten us nowhere. reply maeil 5 hours agorootparentprevThey're not self-crippling, that's the whole point of internalizing negative externalities. reply XorNot 15 hours agorootparentprev...and then the price is added to the price of tyres. Like, where do you think the money is going to go? People can't easily substitute their car use, and there's nothing out there replacing rubber that's road legal, so all you're doing is just adding a tax to car use. You could do this just as easily with gas taxes, registration fees or any other system. reply danielheath 12 hours agorootparent> People can't easily substitute their car use... so all you're doing is just adding a tax to car use. So long as you don't have to pay the actual costs associated with your car use, why would you _want_ to find an alternative? > You could do this just as easily with gas taxes, registration fees or any other system. Registration fees tax ownership of a car, not use. IMO that's... not great; if you want to own a car you rarely drive, why should you pay for everyone else's pollution? Gas taxes could be a fair way to target CO2 emissions, but (given heavy EVs don't pay them) are a poor way to target tyre particulate pollution. As a response to particulate pollution specifically, a tyre tax is quite closely targeted (although possibly ill-advised for other reasons, as I mentioned in my comment). reply Spooky23 14 hours agorootparentprevWell the political party going into power believes they have a mandate to go back in time to when things were great. Right now that means protective tariffs are a fashionable “something” to do. reply CalRobert 12 hours agorootparentprevIs taxing car use bad? reply XorNot 10 hours agorootparentAsk your poorer constituents. I work from home and get paid an enormous salary. I literally do not care. But (1) in turn I make decisions which are purely convenience based because of that disposable income and (2) I'm just one vote. The message you'll be selling to everyone else is: \"hey, that multi-thousand dollar vehicle you use for getting to work because there's no public transport and your job requires you on-site? Pay more money to have it.\" Or did the US not just have an entire election apparently determined by the price of eggs and the cost of living? reply CalRobert 9 hours agorootparentI can’t really afford a car and take the train to work. It is paid for in part by high taxes on personal vehicles. reply maeil 5 hours agorootparentprevAs has been mentioned dozens of times in these comments, do it by car weight. Then poorer constituents do have a choice. reply tonyedgecombe 10 hours agorootparentprevThen other countries will retaliate with tariffs on the goods you export. Ultimately we are all left worse off. reply maeil 5 hours agorootparent> Ultimately we are all left worse off. This is simply not true. Protectionism can have massive benefits. China making it impossible for foreign companies to gain serious ground there independently has been incredibly beneficial, else they wouldn't have done it. I happen to live in Korea which is similar in ways, and here too it's an enormously good thing for the country and its citizens. The dream that protectionism is bad by definition is truly one of the biggest deceptions in economy of the post-Reagan era. It's a great thing because it's basically funneling money from global megacorps to local corporations - which might still be huge, but nothing compared to e.g. Coca Cola or Google. This is a positive thing for everyone except for those companies' shareholders, and in a way the US as that's where almost all these megacorps are based. This is really an important thing to realize, and I can't stress this enough. It's exactly like the EU imposing lots of regulations on Apple et. al. Apple isn't just going to take their bags and not sell there, nor have they raised prices to EU customers as a result of these rules. They simply comply. Imagine if, say, Germany ruled that to sell Coke in Germany as an international company, you have to set up a 50-50 owned JV with an existing German company unrelated to Coke. You think Coke is neither going to give up on Germany nor are they going to raise their prices. They're simply going to be making less of a profit in Germany. Great for everyone. It has played out this way in every country with such rules in place. reply acidburnNSA 17 hours agoparentprevAlso we need incentives to convince people to choose to drive lighter and smaller cars. Carrots and/or sticks should be considered. Alternatively, new tire technologies could maybe also solve the problem. reply forgotoldacc 16 hours agorootparentIf it's not already obvious, in a lot of places, when regulations are promoted to reduce waste and benefit public/environmental health, a large number of people will get angry and vote for those who'll want to maximize damage just because. If regulations promoting smaller cars were ever suggested in these places, some smiling politician would announce a mandate that vehicles be 5 tons or greater with anything smaller being banned, and compilations of people who worried about the environment would be circulated and heavily mocked online. reply AnthonyMouse 12 hours agorootparentThis is misunderstanding why people respond that way. There are a lot of people who can't easily change their behavior, e.g. because your theory is that they should buy smaller cars but their business requires a vehicle that can carry heavy loads once a week and they can't afford to buy a separate vehicle for that so the larger vehicle has to be their daily driver. Then a tax meant to induce a change in behavior is received by them as an unavoidable tax hike, which they naturally resent and oppose, and because of the nature of politics they'll then propose the opposite of whatever you're trying to do to them. What you really need to do is to make it more possible for them to do the thing you want. For example, right now if you want to have a modern compact car for most use and an old truck you use once a week for truck stuff, you have to register and insure two vehicles. That isn't currently economical, but it's what you want to happen so they're not just driving the truck at all times. What you want to do is to make it economical. Only charge a registration fee for someone's primary vehicle and waive the cost for a second one, and make insurance work in such a way that having two vehicles doesn't have any higher liability premiums than driving the same total number of miles in one vehicle. Then they can do what you want, and in fact have the incentive to, because the smaller car will save them gas most of the time but they still have the truck when they need it. reply LeifCarrotson 15 hours agorootparentprevIncentives and penalties need to also exist to encourage manufacturers to offer smaller cars. Many domestic manufacturers are finding that giant luxury SUVs and 100+ kW high-end BEVs are highly profitable, and aren't even selling small and light vehicles at all for customers to choose. reply danielheath 16 hours agorootparentprevTyre wear is proportional to the fourth power of wheel load; reducing weight per wheel is the key here. Hiwever, taxing new tyres may be counterproductive, since encouraging folk to keep using their worn tyres is not a good outcome for road safety. reply potato3732842 16 hours agorootparentTyre wear is proportional to the fourth power of wheel load; reducing weight per wheel is the key here. No, it's not. You're taking a very loose rule of thumb for road surface wear and baselessly applying it to tires. Tire wear follows the rubber the tire is made out of. Soft rubber wears faster. Once you control for that it's acceleration and braking loads (i.e. driving style) that dominate. After that is when weight starts mattering. If what you said was even remotely true then heavy vehicles would get obviously less life out of tires compared to compact cars when in reality they get about the same reply david-gpu 16 hours agorootparent> If what you said was even remotely true then heavy vehicles would get obviously less life out of tires compared to compact cars when in reality they get about the same Are you assuming that the tires of heavy vehicles have the same thickness as lighter vehicles? My bike has much thinner tires than any car, and they can last ten thousand kilometers. reply danielheath 12 hours agorootparentprevI'm not sure where you're getting these ideas from. Best estimate I can see of prime mover tyre lifespan is 40,000-120,000 km. I'd be quite happy if I could get that kind of lifespan out of my cars tyres. reply qwerty_clicks 16 hours agorootparentprevTires these days are expensive. To make them cheaper, they have reduced quality as well. Likely wearing faster and with worse material reply Tiktaalik 2 hours agorootparentprevWe need alternatives to cars too. reply SideQuark 15 hours agoparentprevWe need to start making consumers pay for their negative externalities. Until the externality cost is not baked into product cost it won’t be paid for. reply beala 15 hours agorootparentYou can tax producers, who will then increase prices. Or you can apply a tax to the product directly, and make it appear that the consumer is paying. But who is actually paying it is a question of tax incidence and a function of demand and supply elasticities.[1] 1: https://en.wikipedia.org/wiki/Tax_incidence reply Aurornis 15 hours agorootparentprevMaking manufacturers pay is equivalent to making consumers pay. The price is passed on to the consumer. The idea of “making manufacturers pay” in commoditized markets like tires is a feel-good myth. Any additional fees will go to the consumer price. reply mvkel 15 hours agorootparentConsumers are ultimately the party responsible for this pollution though, so we should pay. reply Spooky23 14 hours agorootparentPaying taxes doesn’t reduce the harm. You can’t change a complex system with one knob. reply amanaplanacanal 5 hours agorootparentRaising the cost incentivizes finding alternatives. Your statement would only be true if no alternatives are ever available. reply HeatrayEnjoyer 15 hours agorootparentprevCustomers have a price ceiling though. reply bdangubic 15 hours agorootparentprevconsumers are already paying heftily… in virginia we pay 4.56% on the value of the vehicle every year plus there is an electic vehicle tax and also million other taxes and fees added. funny that state with “don’t thread on me” license plate is a bastion of socialism where you are not allowed to own a car but have to pay each year to the state for the right to own the car… the problem of course is all that insane amount of money collected will never be used for anything other than to pay for pensions for former government employees :) reply amanaplanacanal 5 hours agorootparentDon't know about your state in particular, but most places in the US vehicle and fuel taxes are not enough to pay for road maintenance, and it is being subsidized out of other taxes. My state realized a couple of decades ago that they were going to have the same kind of problem with their pension system and recreated it to be self-funding. They still have the old pensions to cover but at least they aren't continuing to dig themselves a deeper hole. reply bdangubic 4 hours agorootparentThe original commenter stated: \"We need to start making consumers pay for their negative externalities.\" Alls I was trying to say is that consumers are already paying crazy money. 26 states have property taxes on cars! In VA even with all that PLUS a special tax for EVs PLUS most of the roads around the DC metro area are tollroads it is still not enough :) I was being facetious talking about pension funds - what I was basically trying to say that whatever money is collected isn't going to where it should be going - if there is a budget shortfall (and wouldn't you know - there always is...) money gets appropriated to other things... reply amanaplanacanal 3 hours agorootparentYeah my first paragraph was the important one. Building and maintaining roads is really expensive. reply bdangubic 2 hours agorootparentNo doubt. And in the United with all the corruption happening at the Local levels it is A LOT more expensive than it needs to be (just look at what happens when some major issue arises - https://www.forconstructionpros.com/infrastructure/article/2... - this was fixed in two weeks - if this project was actually done as \"normal\" infrastructure project it would have take a year at a cost of like $891 million). But money is already being collected for these things through 89 different taxations - so more revenue is 100% not the way to fix this problem. reply Palomides 17 hours agoparentprevthe majority of the pollution probably comes from semi trucks rather than passenger cars, due to the huge weight and number of wheels reply snibsnib 16 hours agorootparentWhere i live, 80% of all vehicles are passenger vehicles. I'm not sure that the extra wheels on semis would make up for that difference, especially with the slow increase in size of passenger vehicles. reply Retric 16 hours agorootparentSomething like 98% of ware from road vehicles is caused by semi’s vs 2% from cars and trucks. 20% * 18 = 3.6 vs 80% * 4 = 3.2, so barring some 3rd category semi’s would have more tires. They also have a lot more weight on each of those tires. reply Aurornis 15 hours agorootparentRoad wear is proportional to weight. Semi tires are hard, long-lasting compounds relative to soft consumer tires with deep treads and soft rubber. reply rootusrootus 15 hours agorootparent> Road wear is proportional to weight No, it scales at the fourth power of the axle weight. reply Retric 8 hours agorootparentprevHard long lasting compounds don’t actually make up for fully loaded semi’s weight. They are much larger tires and with consistent heavy loads may only last 25k miles (or 100k with light loads). So more and much larger tires and fairly similar lifespan = they liked make up a significant majority of tire pollution. reply emmelaich 15 hours agorootparentprevAgree, the damage to the road (and the tyres, presumably) is proportional to weight^4. https://en.wikipedia.org/wiki/Fourth_power_law > The fourth power law (also known as the fourth power rule) states that the stress on the road caused by a motor vehicle increases in proportion to the fourth power of its axle load reply missinglugnut 14 hours agorootparentI hate that the 4th power law is called a law. It's not a law of nature, it's a lazy curve fit. Think about this...if car does 'x' damage to the road, 2 cars does 2x damage. 2 cars welded together side by side (axle to axle so the axle count stays the same) would also do 2x damage, but the 4th power law says it does 16x damage. If it's wrong by a factor of 8 in the simplest thought experiment it's not a law. You can obviously make a heavy load act like many small ones, or concentrate a light load so it does a lot of damage. Constant * X^4 just coincidentally went through the data in a single 1950s dataset...and for some reason we're calling it a law 70 years later, when it's really just a loose trend that we could easily break with a little engineering. And we probably have broken it...tires, roads, and vehicles have changed a fair bit in 7 decades. reply vel0city 14 hours agorootparentIf you're welding the two cars together connecting the axles you're still having 8 contact patches instead of 4, so the axle load is the same as 2 separate vehicles they're just moving in tandem. You'd need to stack the two cars on top of each other to increase the axle load. In which case I'd say it's not obvious how much more the road wear would be without looking at data. I'm not saying the 4th power law is absolute truth, I truly don't know what the wear patterns would look like on a modern surface. But your example isn't proving it wrong at all. reply icehawk 14 hours agorootparentprev> Think about this...if car does 'x' damage to the road, 2 cars does 2x damage. 2 cars welded together side by side (axle to axle so the axle count stays the same) would also do 2x damage, but the 4th power law says it does 16x damage. Are you removing the two inner wheels from the axle? Those would also support weight reply rapjr9 14 hours agorootparentprevA little off topic, but what are the roads releasing into the environment as they wear down? Asphalt is often somewhat radioactive since it's made from oil? Is there benzene in there? What is the scale of asphalt nanoparticles compared to tire nanoparticles? reply hackernewds 16 hours agorootparentprevthere are 200000x the number of private cars. are you sure? reply Palomides 16 hours agorootparentit's widely accepted that trucks cause the majority of road wear, considering the tire is the softer part in contact there, it seems pretty plausible I don't have a citation to point to, though! edit: there are roughly 100x registered passenger cars in the US as semis reply SideQuark 15 hours agorootparentWell, given a semi only averages 8x the miles of a car per year, your initial claim is wrong. https://afdc.energy.gov/data/10309 reply vigna 8 hours agorootparentHi SideQuark. I'm writing you here as there's no contact email on your profile--I wanted to ask you if you ever tried a SAT solver on RomuTrio to find cycles (or you can give me some hints). I'm referring to this: https://news.ycombinator.com/item?id=22457101. You can write me at sebastiano.vigna@gmail.com. Thanks for any info! BTW, thanks for explaining that you cannot prove things about a single permutation using random permutations--it's so obvious that it is very difficult to explain, and the same absurd argument pops over and over. reply Aurornis 15 hours agorootparentprevRoad wear depends on weight. Semi truck tires have hard, slow-wearing compounds. reply Spooky23 14 hours agorootparentprevA fully loaded tractor trailer has 5 axles and weighs 80,000 lbs, it does 9000-10000x more damage than a passenger car. This is why states operate weigh stations — overweight trucks cause significant damage. reply qwerty_clicks 16 hours agorootparentprevSemi’s drive 12+ hours a day most days of the year. Passenger cars just go to work and they store and back. reply bsder 16 hours agorootparentprevYour estimate isn't even close--you're off by about 4 orders of magnitude. Fact: In California, the number of trucks is about 300K vs cars at 14M (about 40x). Fact: California AADT on roads for trucks ranges from a couple of percent up to almost 50%. Very few roads have less than 10% AADT from trucks. Fact: Damage to roads goes as fourth power of axle load. Speculation: Given that tires are the primary means to transmit that damage to the roadway, it wouldn't surprise me if the trucks are responsible for the vast majority of tire particulates. Reference: https://dot.ca.gov/-/media/dot-media/programs/traffic-operat... reply UniverseHacker 14 hours agoparentprevIn this case, how can the negative externalities actually be mitigated with money? Maybe R&D to develop a less toxic tire? reply Hammershaft 17 hours agoparentprevrevenue neutral externality taxes are great policy but terrible politics... reply bamboozled 6 hours agoparentprevIsn't the plan to basically destroy all regulation in the US from Jan 2025? reply psychlops 14 hours agoparentprevThis seems like an easily corruptible idea. For example, who measures the negative externalities? Certainly there are many, also certain is the ambiguity in measurement. Plenty of ways to game the system and for the system to play favorites. reply hx8 13 hours agorootparentIt's actually very common to fine manufacturers for negative externalities. We even tax some manufacturers for some negative externalities. For example, we have a federal tax on cigarettes that pay for some health programs. reply revscat 17 hours agoparentprevI personally doubt that the American government has the power to be able to do such things. Regardless, the incoming administration will under no circumstances impose such restrictions, or push for them to be created. I seem to recall that the previous Trump administration removed restrictions around asbestos, to give you an idea. Libertarians gonna libertarian. Even if it kills everyone. reply rootusrootus 15 hours agorootparent> Libertarians gonna libertarian. Even if it kills everyone. Plenty of blue states have shot down additional taxes. When it comes to pigovian taxes, nearly everyone in America is a libertarian. reply revscat 15 hours agorootparentOh, absolutely. If you look at voting records, the overlap between neoliberals and libertarians is incredibly strong in this regard. reply 123yawaworht456 7 hours agoparentprevyou folx really hate working class scum, don't you reply justinator 14 hours agoprevI try to tell people about this, but people are married to their cars and they're in denial that this is really a problem. reply chilldsgn 12 hours agoparentI literally cannot walk or ride a bike where I live. I will get robbed and/or killed and the weather doesn't play nice. Distances to places I need to go are also quite lengthy. Ordering stuff online doesn't solve the problem, they also drive vehicles with tires. It is out of necessity that people in my part of the world are reliant on vehicles, and I assume most other people in other countries are too. This is a very complex problem that is extremely difficult to solve and shouldn't just be dismissed as people just being married to their cars. However, I'd love to see a less polluting compound used for vehicle tires, which is also cost effective for the end user. In my country the railroad system is totally buggered due to crime and corruption, so there are trucks everywhere. Reducing reliance on semi trucks for freight could possibly reduce the pollution a bit. In my country at least... reply therealdrag0 12 hours agorootparentFwiw ordering online is more efficient on average than driving yourself. reply darknavi 1 hour agorootparentTrue, but in my experience being able to \"Amazon\" something to my house by 8AM tomorrow really lowers the bar for things I purchase, meaning I buy wayore than if I went to the store to get it, resulting in way more miles for my household. reply xnx 17 hours agoprevI'd love to see some of the tire wera data that Waymo has accumulated. Despite being heavier vehicles, I bet they do well on tire wear from no hard stops or starts, driving the speed limit, keeping properly inflated, and (I assume) optimizing driving to maintain momentum (e.g. not accelerating into red lights like I see so many human drivers do). reply ramraj07 16 hours agoparentHave you experienced waymo in SF? It actually drives faster than regular folks and brakes much more harder because of that. The speed limit doesn’t apply to the streets of San Francisco and it typically accelerates to the limit as fast as possible (especially electric). reply xnx 15 hours agorootparentNot in SF, only in Phoenix. My rides seemed much less erratic than a typical Uber ride. reply metalman 16 hours agoprevtire dust is behind major die offs of juvinile salmon, thete are specific toxic compounds in it, and when therevare major rain events, all the dust in the ditches gets washed into rivers and streams in high enough concentrations to kill all the fish. from memory the toxic compound is some sort of biocide put in tires, to keep them from bieng eaten by ? algea? fungus? whatever, not an important detail, but an additive that can be eliminated or replaced, $$$$$$$$ reply cyberax 16 hours agoparent> biocide It's an anti-oxidant, to protect tires from UV and ozone degradation. Tire companies are in the process of switching to a different anti-oxidant. reply treme 13 hours agoprevOn previous thread of this topic, someone on hn claimed to have worked in tire industry and that some small additive that would cost few percent extra would largely address thos issue. reply NoPicklez 12 hours agoparentWhat would that additive do to prevent microplastics? reply SapporoChris 8 hours agorootparentI found the following after a bit of searching. I'm sure there's more work being done. https://www.canada.ca/en/environment-climate-change/news/202... Applied Quantum Materials Inc., in Edmonton, Alberta, is developing a specialized reinforcing additive for tires to minimize microplastic release over different road and temperature conditions. Nova Graphene Canada Inc., in Dartmouth, Nova Scotia, is developing a graphene-enhanced rubber that could reduce tire-wear shedding and extend the life of tires. Stema Punch and Die Inc., in Cambridge, Ontario, is creating specialized compositions for tires to improve their wear and help stop the shed of microplastics. reply qwerty_clicks 16 hours agoprevAnd noise pollution! Even electric cars (which are quite heavy and produced a lot of tire wear) create loud roaring at moderate to high speeds for anyone the car is passing. reply fastball 16 hours agoparentEVs have the benefit of regen breaking though, which means less break dust contributing to the pollution. reply prpl 16 hours agorootparentLess than a vehicle of equivalent mass, sure. reply rootusrootus 14 hours agorootparentEVs are broadly comparable to the ICEV equivalent. We're talking 10-15% difference, sometimes less, and the gap closes a bit every year. Won't be long until EVs are consistently lighter than the gas equivalent. reply jayd16 15 hours agoparentprevIt's really not that much heavier, less than double. A Model 3 and a 3 series BMW weigh about the same. reply fsckboy 16 hours agoprevsounds about right, I'm surprised it's so low; we should probably stop doing the other things putting microplastics into the environment since they account for 3/4. tires are an essential part of most people's lives, and even city people who don't have cars rely heavily on wheeled vehicles. When I think about the plastics that I take out into the environment where I subject them to wear, I can't really think of anything apart from tires. reply hackernewds 16 hours agoparentit is not necessary though that 8 tons of energy displacement moves 60 kg individuals. we should absolutely cut down on private transit reply rootusrootus 15 hours agorootparentMost cars are more like a ton and a half to two tons, not eight. But in any case, it's not clear that there is a lot of low hanging fruit in cutting down on private transit. Aside from a few outliers, most places with pretty great public transit still have a lot of private transit too. It has many use cases that public transit is unable to fill. reply paulryanrogers 16 hours agorootparentprevPerhaps street cars and trains could take efficiency even further, with metal wheels. reply hanniabu 14 hours agorootparentAnd maybe put them on rails reply lioeters 13 hours agorootparentAnd have them run on a regular schedule, within and between cities. Imagine the benefit to the public and the environment. It could even be funded by tax payers. reply ssl-3 7 hours agorootparentWe used to have that in the flatter parts of Ohio (which is ~most of it), with street cars, and interurban lines linking many of even the tiniest towns together, and with passenger rail between larger cities. It was privately-funded. It worked. ...until the automobile became more common and people stopped started driving cars instead. (The literal-conspiracy between General Motors and Firestone Tire didn't help, either.) Here's a map from 1908: https://curtiswrightmaps.com/product/electric-railway-map-of... reply dyauspitr 16 hours agorootparentprevAt a certain point people will revolt at that suggestion. Most people I know are not going to give up their suburban/rural acres to live in a cramped, walkable city with no space to do everything they love. reply nosbo 14 hours agorootparentThere needs to be more choices. American suburbia is on another level I know, but there is no reason you can't have a bit more density, walk-able or bicycle friendly neighbourhoods, centred around amenities and public transport. reply david-gpu 16 hours agorootparentprevBut some people will, and the easier we make alternative modes of transportation, the more people will choose them. Want to sit in traffic all day while an LRT zips by? Nobody is stopping you. reply rootusrootus 14 hours agorootparentI occasionally feel bad for the folks on the LRT alongside I-84 on the days I commute to the office. The traffic is congested and very slow at times, and I still beat the train. And that's even before getting to downtown, where the train just crawls from station to station. I wish we could have a real subway. I suspect the best option for most suburban cities to reduce traffic and air pollution is to strongly incentivize employers to allow remote work when feasible. reply recursive 16 hours agorootparentprevThat vehicle would require a commercial license. reply rootusrootus 15 hours agorootparentThe lower limit in most jurisdictions in the US is 13 tons. And also, in most jurisdictions there is an exemption for vehicles used for personal recreation (e.g. you can absolutely drive a Volvo tractor to pull your enormous fifth wheel toy hauler with nothing more than a basic driver's license). reply jpalawaga 14 hours agoparentprevI walk most places and when I don’t walk, I take the train, which has steel (and not rubber) tires. reply wkirby 15 hours agoprevGenuinely curious if there are any real efforts to address this available to the consumer. In the kind of idiot who will buy more expensive tires because they shed less plastic, but as far as I’m aware I don’t actually have that opportunity. reply jcrben 6 hours agoparentMichelin is at least looking into it: https://www.michelin.com/en/media/magazine/michelin-is-takin... Nokia is also working on green tires with more biodegradation https://www.nokiantyres.com/about-us/news-article/nokian-tyr... reply wkirby 13 minutes agorootparentThank you for a real answer. reply tim333 5 hours agoparentprevI guess when the tyres wear down it all ends up as bits of microplastic so long life tyres would be better. There tends to be a bit of a trade off between long life and high grip though. reply prawn 14 hours agoparentprevDoesn't address the tires directly, but smaller, lighter cars are available. Maybe greater awareness about pollution from tires would help that become more of a sales factor. reply wkirby 16 minutes agorootparentI already drive the tiniest car I can reasonably fit a 7 month old int and I’m strong advocate for the death penalty for anyone who drives a car larger than a mid-size sedan. reply Gigachad 15 hours agoparentprevSteel wheels on a steel track don’t have this issue. reply wkirby 15 minutes agorootparentThis is a fair point except I live in small-town New England. You think there’s transit? There’s not even a sidewalk. reply rootusrootus 14 hours agorootparentprevIn the sense that they don't emit plastic particles, sure. But trains absolutely do create metal particle air pollution. I think it would be an interesting comparison, given how often our local light rail trains operate nearly empty. If it has less than about 20 people per train car it is toting around more weight per passenger than a private vehicle with just the driver. reply adrianN 13 hours agorootparentSteel dust has the nice property of turning into mostly harmless metal oxides rather quickly. reply rootusrootus 12 hours agorootparentIron isn’t the only metal in those wheels, some are more poisonous. reply adrianN 10 hours agorootparentStainless is mostly iron and chromium, sometimes nickel. Some oxidation states of chromium are indeed toxic, but the most stable form is chromium(III), which is harmless enough that you can buy it as a supplement. Nickel also occurs naturally in soil and many foods and is not accumulating in the body. The kidneys can remove it. Only large doses are problematic. reply rootusrootus 3 hours agorootparentprevHuh. Downvoted for mentioning a fact that has been studied in depth and is uncontroversial. You guys crack me up. reply _carbyau_ 15 hours agoparentprevMaybe I have this wrong somehow but I thought that could be categorised as \"wear\". IE don't buy the tyres that last 1000[preferred units of measure], buy the tyres that last longer. But then you could get into \"performance per unit of wear\" and how that is all defined is a wombat hole of discussion. reply limitedfrom 12 hours agorootparentYou would have to make some trade-offs with emissions/fuel economy though. Harder wearing compounds generally lead to higher rolling resistance, which means higher energy use per distance. I’m inclined to think that microplastics pollution might be worse than tailpipe emissions (or electricity generation pollution depending on your source)?, but it wouldn’t be straightforward. While there will probably always be uses for autos, we should encourage other methods of transport whenever possible. reply adrianN 13 hours agoparentprevBuy a (motor-)bike and only use the car when you really need it. reply wkirby 13 minutes agorootparentI have a bike, I love it. I also have a 7-month old daughter and live in small-town New England — a car is non-negotiable because of a century of choices about the built environment. reply bufferoverflow 11 hours agorootparentprevAnd increase your chance of death by 40x? https://i.redd.it/ybzo8fbxlcm21.png reply amanaplanacanal 4 hours agorootparentI would guess most of those deaths are cause by collision with a car. So getting rid of the cars would solve that problem. reply bufferoverflow 2 hours agorootparentSo you moved the goal posts from \"switch to motor bike\" to \"switch everyone to motor bike\". reply adrianN 10 hours agorootparentprevYou can influence your chance of accidents by driving safely. Or take public transport. reply qprofyeh 17 hours agoprevI truly hope this encourages manufacturers to build smaller, lighter, and slower cars. reply revscat 17 hours agoparentWhy on earth would you think that that would ever happen? reply fractallyte 10 hours agorootparentBecause governments could mandate it. Yes, we're at the stage now where democracies need to become far more dictatorial, otherwise their populations will continue mindlessly along the path of 'Tragedy of the Commons'. reply tim333 5 hours agorootparentYou don't even need dictatorial. Just tax breaks and the like for small light vehicles and penalties for large heavy ones. reply SoftTalker 17 hours agoparentprevThe only thing that will do that is regulation, either mandating the types of cars we can buy, or taxing the kinds of cars we don't favor. People in the majority prefer large, powerful cars, given the choice. And those cars already cost a lot, so any strategy of price increases to discourage them will have to be significant. This is why most people in Europe drive small, economical cars. Big cars are heavily taxed, and so is the fuel they use. reply trimethylpurine 16 hours agorootparentI drove smaller cars in Europe simply because bigger cars don't fit in many places. I don't think it has much to do with price. Small streets mean fewer drivers. That means your overhead demands that you charge more per customer since you have fewer of them. All that said, smaller roads mean smaller trucks which means more trucks more often which means more tires. reply porphyra 14 hours agoparentprevI really wish we could buy a Citroen Ami in the US, which is a small, light, and slow (28 mph top speed) electric car for city driving. Plus, it looks super cute! reply sigzero 16 hours agoparentprevThe vast majority of people would not buy those. reply tim333 5 hours agorootparentIt depends on the incentives. In Japan the top selling car is the Honda N-Box https://en.wikipedia.org/wiki/Honda_N-Box which is on the small light and slow side. The US has dumb mileage policies to get everyone to drive huge trucks, and pretty much bans things like the N-Box. In Japan you get a parking exemption with cars like the N-Box and it seems quite useable - top speed 87 mph. Review https://www.motortrend.com/reviews/honda-n-box-slash-review-... I wouldn't mind them bringing in something like that - I'm in the UK where they don't have N-Boxes either but it would do the job and I mostly get around by e-bike these days so I prefer others to drive small light cars. reply tmnvix 15 hours agorootparentprevDepends entirely on price. reply rootusrootus 14 hours agorootparentI hear this a lot and I am quite skeptical. Price-sensitive buyers do not buy new cars, they buy used ones. What is available on the used market is directly controlled by what the minority of people who buy new cars put their priorities on. reply fordfordford 15 hours agorootparentprevnext [4 more] [flagged] _carbyau_ 15 hours agorootparentI see it as less a masculinity problem and more a \"common view of physics\" problem. IE it's less about masculinity and more about the perception of \"winning a crash\". Cars can pass all the safety standards they like but the common view is a multi-ton ladder chassis truck keeps the kids (and themselves, their loved ones, friends...) safer than the small city car (containing others). So stupidly sized trucks are desirable... The only way out I think is regulation. Otherwise the \"outsizing\" will continue. reply rootusrootus 14 hours agorootparentprevThat's quite the hot take, more worthy of Reddit than HN. Pickups are super useful. Especially if you live in a place that does not have narrow roads, and if you have a family to tote around. It's in many ways the modern equivalent to the luxury barges of the 70s. Plus, modern trucks are frequently as efficient as a mid size sedan from 10-15 years ago, which is pretty wild. Some of them, like my Lightning, are more efficient than pretty much any car which uses gasoline. Lastly, 80K is a lot even for a truck. The vast majority are more like 40-50K. 80K is a top trim, brand new, no discounts price. reply bdangubic 15 hours agorootparentprevexcept there aren’t any affordable smaller cars anymore… reply chuckwolfe 15 hours agoparentprevA slower car? They already make plenty of them reply wfhrto 16 hours agoprevI drive a lot less working from home reply add-sub-mul-div 16 hours agoparentI drive a lot more now. But I preferred the old city life, taking the subway to the office instead of driving around the suburbs. reply randmeerkat 3 hours agoprevThere’s so much waste with every household owning a car. A lot of this could be avoided with affordable mass transit options like high speed trains and subways instead. reply emmelaich 15 hours agoprevEVs are heavier but I suspect the wear is less than you might think because the braking is gentler with regenerative braking, so less wear on the tyres. Also, there's quite a bit of pollution from break pads and discs, also reduced because of regenerative braking. I couldn't find numbers though, with a brief bit of googling. reply mattmaroon 15 hours agoparentI’m not sure it’s gentler or in any way different from a tire’s perspective than braking. If I pull my foot off the one pedal driving, the deceleration I get is fairly comparable to about 3/4 braking in an ICE. (Off the top of my head estimate for illustration purposes only.) But the tires are made of different materials and are different in other ways too. I can tell you first hand you spend far more on tires if you own an EV. Our model y spends more on tires than our giant diesel pickup, and far more than an ICE of similar size. As for the relative amount of microplastics they emit, I really couldn’t know. You definitely wear your brake pads out far less with an EV. Just eyeing it though, volumetrically, brake pad wear has to be relatively insignificant compared to tire wear. I’d guess EVs still emit substantially more tire particles, and fewer from brakes but nowhere close to compensating. But I’d not be shocked to death if someone studies it and you turn out to be right. And on the plus side, those are probably issues materials science could solve (make the tires out of something benign) whereas EVs emit a lot less of other bad things that I’m sure are not easy to solve or they already would have been. reply hansvm 15 hours agoparentprevYour hypothesis then, roughly, is that braking causes more pollution when it's more extreme (since the total braking work being done is at least as great -- in practice much greater because of the increased mass), so smoother braking will reduce tire wear. I'm inclined to believe an extreme version of that hypothesis -- I doubt 200k miles at 1mph would wear the tread substantially -- but in practice I don't think that's the case. Electric cars tend to replace tires around 10k miles sooner, so the net effect of everything involved (heavier cars, regenerative braking, rich young guys driving faster, mostly city driving, ...) is 15-50% more tire wear per mile. reply whartung 15 hours agoparentprevPretty sure most wear comes from the back tires (I should say the \"power tires\" to consider FWD vehicles). Many electric vehicles accelerate quite quickly, which just wears their tires even more. Braking is braking. If you're stopping in N meters, regardless of how the braking force is applied (regenerative brake vs discs), the tire is the artifact taking the load. Even then, most cars don't routinely brake as hard as they accelerate. Motorcycles, with their high performance, are notorious for eating rear tires much faster than front tires, and they can't be rotated. Then, there's my vehicle, full time 4WD (not AWD, there's a difference), it wears its tires quite evenly in contrast to 2WD/AWD vehicles. reply nickff 15 hours agorootparentBraking is usually much quicker than accelerating, for almost all vehicles (because brakes can absorb much more energy than engines can output). For this reason, I suspect most particulates are caused by breaking. reply _aavaa_ 15 hours agorootparentprev> Even then, most cars I believe you mean most drivers. All of this talk about EV tires wearing faster than ICE tires is driven by people accelerating aggressively simply because they now can. reply zeusk 15 hours agorootparentIt’s about the weight of the vehicle; aggressive driving just makes it even worse. EVs are generally quite heavier compared to similar class of ICE vehicles. reply lmm 15 hours agorootparentprevElectric cars are mostly drive-by-wire, so if the same driver input results in accelerating faster in an EV then I'd say that's the car's responsibility. reply rootusrootus 15 hours agorootparentThat makes no sense. Every car on the road, regardless of power source, has what amounts to an infinitely adjustable pedal controlling acceleration. The relationship between pedal input and actual acceleration varies between cars, and can vary even on the same car in different drive modes. How fast you accelerate is 100% under your own control. reply lmm 14 hours agorootparentIf you put effort into controlling it, sure. Most people have other priorities, they push the pedal when they want to go and they leave it to the car to accelerate at a reasonable speed when they do. Mass-market cars should be built for the population that exists, and that means having a default accelerator-response that does sensible things for the way that normal people normally use the accelerator. reply Toutouxc 12 hours agorootparent> Most people ... push the pedal when they want to go and they leave it to the car to accelerate at a reasonable speed when they do. I'm going to need a source for that. There are literally tens of thousands of accelerator pedal types, with different hinges, different travel, different stiffness and different engines with different ECUs and different drive modes attached to them. There's no one default way to step on the gas that people would just pull out of muscle memory. reply Gibbon1 13 hours agorootparentprevI have nothing to back this up other than I'd like to see a comparison between EV's and other luxo models for tire wear. Because the tire design matters a lot when it comes to wear. I'm a bit suspicious about the weight being the primary driver since I have a van that weighs 6000lbs and it doesn't burn through tires. I can see two things. One maybe EV tires are spec'd to be more sporty. Or possible tires aren't optimized for the extra weight. Pointed comment: No one but no one cares about higher weight trucks and suv's at all when it comes to tire wear. Only EV's get singled out. reply sfblah 15 hours agoparentprevMy EV runs through tires way faster than my gas car. reply rootusrootus 15 hours agorootparentI, too, have a lead foot. EVs are quick, and tires are the price. reply zeusk 15 hours agorootparentThere’s quick ICE cars too, but they don’t weigh as much as the 100 or so kWh battery pack in EVs reply rootusrootus 15 hours agorootparentTypical EVs are about 10-15% heavier than the comparable ICE. Yes it makes a difference, but only marginally. Normal drivers without a lead foot get 10s of thousands of miles from tires, just like ICE vehicles. Also, few EVs that are not pickups have 100 kWh batteries; more typically 60-75. reply zeusk 13 hours agorootparent2024 Mazda3 Sedan: 3,124 to 3,395 lbs 2024 Tesla Model3: 3,862 to 4,054 lbs That's 20% heavier, and it gets worse when you look at EVs not built from ground up by legacy manufacturers. reply seanmcdirmid 13 hours agorootparentA mazda 3 is a sub-compact, and a Model 3 is a mid-size. At least compare it to a mazda 6, which is also a mid-size: 2024 Mazda6: 3,437 to 3,582 lbs reply zeusk 12 hours agorootparentmodel3 is a mid-size sedan much like mazda3; even the marketing depts agree, but sure whatever makes you happy. 2024 Tesla Model S: 4,560 to 4,776 lbs reply seanmcdirmid 2 hours agorootparentMazda 6 is mid sized, Mazda 3 is compact, whether it has a tech back or not is irrelevant. You are basically comparing a Honda civic to an Honda accord and saying they are the same size. reply Toutouxc 12 hours agorootparentprevWhat are you talking about? Model 3 is larger in every single dimension, in some by a lot. https://www.carsized.com/en/cars/compare/mazda-3-2019-5-door... reply zeusk 12 hours agorootparentBecause you’re comparing the hatchback version. https://www.carsized.com/en/cars/compare/mazda-3-2017-sedan-... reply bearjaws 15 hours agoparentprevI'd say it's slightly worse, mainly because when you first get the car you tend to launch it a few times and take advantage of all the power. My first set of tires only lasted 27k miles. After a year you drive normally, I get about 35k miles out of 40k mile rated tires, similar to my old Audi. reply seanmcdirmid 13 hours agoparentprevMy wife doesn't buy that argument. To her, when I have regen braking on, we slow down a lot more than we would otherwise with the brakes. It could just be the way I'm driving though. reply naming_the_user 9 hours agoprevCool, let’s make tyres that degrade when in small chunks with high surface area. It’ll cost a bit more to replace them every few years (since they’ll also degrade a bit more whilst on the car) but will be a far better solution than the kneejerk “lol ban all the cars” stuff. reply PlunderBunny 16 hours agoprevLike many efforts (effective or otherwise) to solve environmental problems, fixing this might require new tyres sold to environmentally conscious consumers initially, to prove that it can be done. Cost effectiveness comes later, paired with government regulations. I'm assuming there are currently no tyres on the market that contain nothing that degrades into microplastics, but please correct me if I'm wrong. reply david-gpu 16 hours agoparentStreetcars, subways and LRTs do not require new fancy tire compounds to stop releasing microplastics. Their brake pads can, though, although I imagine that regenerative braking helps somewhat. Bicycles are so lightweight that the amount they release is negligible compared to a car. reply porphyra 14 hours agorootparentI wonder if rubber-tyred metros like the Montreal metro and some Paris metro lines release a lot of microplastics? reply therealdrag0 12 hours agorootparentCould possibly filter it out at exhausts reply PlunderBunny 15 hours agorootparentprevAgreed, and I like all those forms of transport, and use some of them when I can. But I do have to drive sometimes (for a certain definition of 'have to' of course). We're not going to win these things by saying \"just don't drive\". Driving less is part of the solution, but it's not going to work by itself. reply incanus77 12 hours agoprevI remember asking adults about this when I was probably seven years old. Once I learned that tires wore down, eventually smooth, it made sense to me that the material had to go somewhere. No one really had much of a good answer as to where it all went. reply tim333 5 hours agoparentI think the bulk is dust on the road that gets washed off by rain and down the gutter. But some goes in the air. reply boredatoms 15 hours agoprevCan they make tires without these problems? reply jcrben 6 hours agoparentProbably. Mentioned a couple manufacturers working on greener tires: https://news.ycombinator.com/item?id=42273418 reply danparsonson 15 hours agoparentprevTyres trade material for grip, so, I think it's unlikely. reply tomrod 15 hours agoparentprevSure. They already do. We use them on bikes. reply uoaei 15 hours agorootparentBikes have completely different needs and are very light compared to cars. It's apples to oranges. reply tomrod 6 hours agorootparentBoth are sufficient modes of transport, one a death machine and the other much broader use. reply uoaei 4 hours agorootparentTotally, bicycles are much more popular worldwide than heavy cars that have little regard for the safety of pedestrians. reply switch007 11 hours agoprevQuick, let's make something more expensive! Everything is killing people and everything needs taxing more, naturally. It's OK because I myself can afford it and we can just tell poor people to use the public transport that doesn't exist. The public transport that does exist is also expensive and/or unreliable because ideology in government is that cars are better. But that's also ok, I think, anyway doesn't affect me --- Sarcasm aside people really need to stop this reflexive \"scientists say X is bad\" -> more taxes and higher prices reaction. reply amanaplanacanal 4 hours agoparentI guess we can ignore all these problems and keep making the environment worse, as an alternative strategy. I don't have any kids, but I hope yours are going to be ok. reply crvst 7 hours agoprevRubber is not a plastic reply amanaplanacanal 4 hours agoparentMost tires are not made from natural rubber, but from synthetics. reply crvst 2 hours agorootparentYet, it is still not literally a plastic by definition. I feel the term “plastic” is already demonized enough, and now it often serves as a generic umbrella term for alarmist purposes. reply pvaldes 9 hours agoprev> If the pollution is from tires, then tax tires. Tax tires, move worn tires to a recycling plant, bill a recycling tax when the owner buys a new tire... ... and then set fire to the tires, keep the money for recycling safe in your pocket, and then keep lying about how good is that we \"recycle\" things. \"In Spain, a recycling plant burns every four days, a figure that has increased since 2018. In 2023, the record was broken with 109 recycling plants that were [deliberately] set on fire\" [1] \"In 2019, a tire recycling plant in Seseña burned for days. This fire produced pollution greater than that generated by the city of Madrid for a whole year\" [1] Or grind it and put the stuff in each children park, as currently. ------------------- [1] (In Spanish) https://www.cuatro.com/noticias/sociedad/20240130/alerta-aum... reply taeric 15 hours agoprevI confess I am disappointed that nobody is bothering to mention paint being the leading contributer. Guessing textiles are also higher. As someone that bikes or walks over driving, it is still frustrating how much we'd rather blame driving over anything else. reply therealdrag0 12 hours agoparentGot a source on paint? reply taeric 5 hours agorootparentI just followed links in this article. https://tireindustryproject.org/faq/are-tires-the-main-sourc... is the link in the article, which relies on https://www.e-a.earth/plastic-paints-the-environment/. I confess confusion, here, as the article links in this as support of the tire claim. Which is in contradiction to what it says. Specifically, the article links it as support of a 28% claim, when the link is challenging that number. reply RecycledEle 6 hours agoprevIt's not exactly the point of this story, but I found a way to reduce the number of waste tires I generate. Tires with 6/32\" to 8/32\" of tread are a scam. They get flats, and wear down to the legal minimum (in Texas) of 3/32\" very quickly. I buy tires with 19/32\" of tread. No nail has yet managed to go through 19/32\" of tread (or that much air gap) plus through the equivalent of a 10-ply design to puncture my tire, at least not until I wear them down to 6/33\". If we wanted puncture-proof, run-flat, handle-like-touring-tires, last-forever tites, we could have them. They are out there. reply andrewstuart 12 hours agoprevSome places in India require by law that plastic waste goes into roads. Think how much plastic that will release. reply pcunite 15 hours agoprevWell, maybe people will stop frowning at me for wearing my wicking running shirts now. reply znpy 8 hours agoprevThis looks like a good time to remind that forcing people back in the office means forcing a ton of people to use cars, most likely with a single passenger (the driver) through long commute. They'll emit a bunch of CO2 and pollution in the environment and they'll shed a ton of micro-plastics in the environment through car tires. And don't even bother commenting about electric cars or whatever: electric cars are a minority of the vehicles in circulations, there are a ton of SUVs and the majority of people live far from their workplace and will have to use cars anyway. People living in cities at walking distance from their workplace or served by functioning/efficient/effective public transport are a small lucky minority. We should really push for public transport (ON RAILS!) and for more work-from-home arrangements. reply luxuryballs 13 hours agoprevWe just need to make the tires out of better stuff reply giantg2 15 hours agoprevIt's disappointing they don't list any sort of detailed solutions or future tech. All they do is promote taxation and mention that tire innovation can help. reply uoaei 15 hours agoprevRelated: tire wear rate is proportional to (total weight / number of axles)^4. https://en.m.wikipedia.org/wiki/Fourth_power_law This has serious implications for the future of automobile development in the age of heavy batteries. reply vel0city 13 hours agoparentThats stress on the road, as in things like the road surface, not tire wear. The physics about what's happening to the tire surface are pretty different from what's happening to the road. reply uoaei 11 hours agorootparentAsphalt is a petroleum product, so it's not irrelevant. But even so you're missing some physics in your comment. Considering that friction is symmetric, road wear implies tire wear and vice versa, at roughly the same order of magnitude of severity, relative to respective hardnesses. Tires are generally softer than road surfaces, so they wear faster than roads. reply vel0city 5 hours agorootparent> Asphalt is a petroleum product, Uhh, what? Not all roads are asphalt, tons aren't. But even then that's irrelevant. If I made the tire out of Vaseline would it have the same wear characteristics? After all it's also made of petroleum. Road wear isn't just about the friction. A lot of it is the stresses of the road surface being squished into the ground and deformed. If you've ever seen a boat planing over the water, the road surface kind of wants to make a similar pattern but due to rigidity it can't quite do that. So the internal stresses of the system of the road surface are pretty radically different than the layers of a tire being stretched and pressed into an air bubble. reply SubiculumCode 15 hours agoprevAssigning blame is less interesting to me than whether there are potential avenues for an alternative that both serves as tires and avoids this micro plastics issue. reply Calamitous 14 hours agoprev [–] Whatever became of the tweel? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Car tires are a major source of microplastic pollution, contributing to a quarter of all microplastics in the environment.- There is a debate on whether to tax tires based on their compounds to address pollution, as opposed to taxing vehicles by weight.- The discussion includes privacy and practicality concerns regarding taxing based on weight and distance driven, with examples from the UK and the Netherlands considering different methods for implementation."
    ],
    "points": 266,
    "commentCount": 285,
    "retryCount": 0,
    "time": 1732842573
  },
  {
    "id": 42268310,
    "title": "The success and failure of Ninja (2020)",
    "originLink": "https://neugierig.org/software/blog/2020/05/ninja.html",
    "originBody": "The Success and Failure of Ninja May 12, 2020 Around nine years ago I published Ninja, a build system that is mostly comparable to Make. At the time I was a bit embarrassed to share my side project but since then it has become widely popular. I can't list all of the users off the top of my head anymore, but some of the \"big\" projects that use Ninja include: Chrome, which eventually removed all of its non-Ninja builds; Android, which uses it for some large component of the system that I've never quite understood; all Meson projects, which appears to increasingly be the build system used in the free software world; many other projects that that use Ninja with CMake (for one random example, the build instructions for the Swift programming language tell you to install Ninja). Ninja has been by far my most successful open source project, depending on how you quantify success. (Other projects of mine like Chrome have more users, but I was responsible for only parts of Chrome; Ninja also has had important contributions by collaborators but it feels more like \"mine\".) I released Ninja in 2011, gave ownership of the Ninja project away in 2014, and it has since been passed on again to a third maintainer, so now that my part in the story is pretty much over I here would like to reflect on what I learned. If I were to summarize what I learned in a single sentence, it would be this: we talk about programming like it is about writing code, but the code ends up being less important than the architecture, and the architecture ends up being less important than social issues. That is, as programmers we like to talk about problems as if they are primarily technical — \"how do I optimize this loop to squeeze more qps out of this service?\" — when in my experience the tech almost always ends up secondary to bigger picture factors. I have since seen that same observation in other words stated by many others as describing the journey from a \"junior\" to \"senior\" engineer which also rings very true against my career, so I also hope in this retrospective to give some insight into what that means. The tech Let's start by getting some technical details out of the way. What Ninja specifically does is pretty simple; given a description of the requirements, I'd expect a competent undergrad, perhaps in a systems course, to be able to bang out a basic version of it without much help. To summarize, the user gives Ninja a ninja.build file which (omitting some details) contains all the commands you'd like Ninja to run, along with which files each command consumes and produces. Ninja loads this file, checks the modification timestamps of the various files, and executes in parallel the commands needed to bring everything up to date. As compared to Make (which does pretty much the same thing) Ninja provides fewer features in its input build language, and is primarily structured around making the few things it does very fast. The few things Ninja does are: (1) parse and interpret that build file; (2) check the modification times of its inputs; (3) execute the needed commands. The goal is to get to step 3 as fast as possible even on huge (>100k input files) projects, and doing so is a collection of careful but small optimizations. For one small example, Ninja is careful to map each input file path to a unique in-memory object as early as possible, and then on it uses pointer comparisons between those objects for testing path equality (interned strings, effectively). I wrote a chapter for the book \"The Performance of Open Source Software\" about Ninja that tells some of the stories of the lower level technical details of making that fast which you can read online. Many people have done rewrites of Ninja over the years. It's a small enough project that it's fun to try implementing in your favorite language. For some examples, llbuild and Shake both support Ninja files as inputs, and samurai is a nearly file-by-file reimplementation (with less code, but with fewer features and no tests(!)). Ninja is pretty easy to implement for the fun 20% of it and the remaining 80% is \"just\" some fiddly details. To my knowledge nobody has ever made a faster implementation. Some architecture notes Some pieces of Ninja took struggle to get to and then are obvious in retrospect. I think this is true of much of math, that once you have distilled the ideas to their essence they seem obvious. The power comes from having the right way of thinking about the problem. I mostly stumbled through Ninja's design but once I was on the other side of it I came to see I accidentally hit some good points in the design space. Here are a few examples. The graph representation. Make doesn't well handle the case when build rules produce multiple files. I don't know how Make is structured internally, but I would guess that it represents the build structure as a graph between files since that is what the input syntax looks like and that structure would generate that behavior. Ninja instead uses a bipartite graph between files and commands, where file nodes are edges into command nodes which have edges back out to files. This representation better captures the structure of builds: the command is out of date if any of its inputs change, and it updates all its outputs when it runs. (The one graph invariant is that a given file may have at most one input edge.) For another consequence of this, note that the command line itself can be thought of as an input to command nodes, in that the command is out of date (and consequently so are its outputs) if the command-line flags change. The deps log. To get C header dependencies right you need to consume additional dependency data produced by the C compiler. It's described more in the book chapter. I recall struggling with whether to bring in a database and how to reconcile that with my aspirations of simplicity until I finally hit upon a representation format that ended up pretty tight. (It's unfortunately still wrong in some important ways but oh well.) End-to-end / crash-only. Ninja is not a persistent daemon process but instead does all its work from scratch on each execution. This is intentional and a mixture of the insights of the end-to-end principle and crash only software, which is to say: given that you need to run Ninja from scratch sometimes, if you make that fast, then you don't need to build a second \"online\" codepath. Projects that can stay memory-resident tend to eventually let their startup performance languish. File status. The reason programmers sometimes expect build tools to be memory resident is so they can cache status about files on disk. But in practice, the kernel is already caching this information in memory and caching it again in userland doesn't save you much; fetching file status from Linux is extremely fast. Ninja does it all from a single thread even. On a machine that was \"fast\" ten years ago you can stat 30k files in 10s of milliseconds. (A programming joke: half of performance problems are fixed by introducing a cache; the other half are fixed by removing one.) Orders of magnitude. A rule of thumb is that you can get scale by 2x with optimization, but to scale by 10x you need to rearchitecture. Ninja was designed around Chrome's build which at the time had around 30k build steps. These days it's used in much smaller settings where it's likely not needed (see below discussion on speed) and in larger settings like the Android build where it is failing to scale and will likely need an alternative approach. Underspecifying and overspecifying. Ninja executes commands in parallel, so it requires the user to provide enough information to get that correct. But at the other extreme, it also doesn't mandate that it has a complete picture of the build. You can see one discussion of this dynamic in this bug in particular (search for \"evmar\" to see my comments). You must often compromise between correctness and convenience or performance and you should be intentional when you choose a point along that continuum. I find some programmers are inflexible when considering this dynamic, where it's somehow obvious that one of those concerns dominates, but in my experience the interplay is pretty subtle; for example, a tool that trades off correctness for convenience might overall produce a more correct ecosystem than a more correct but less convenient alternative, if programmers end up avoiding the latter. (That could be one reason Haskell isn't more successful. Now that I work in programming languages I see this dynamic play out regularly.) And finally, the biggest architectural insight is: The 'assembler' metaphor When people think of build systems they think of a broad range of features, so broad that often the way build systems talk about themselves sometimes aren't even comparable across different tools. The marketing text for these tools often talks about how user-friendly the input syntax is. Ninja's insight (discovered in retrospect) is that all of these tools, no matter the high-level features, ultimately eventually must construct some sort of graph of the actions: the files they intend to keep up to date and which commands to execute. Ninja only implements that action graph and leaves it to the user to choose another \"generator\" program on top. I originally invented this two program split just because it neatly fit into the project (Chrome) I was working on at the time, but I have since come to see it as Ninja's primary contribution. On one side, it allowed me to make Ninja stupid but quick, because anything costly (such as \"glob for *.c\") is forced into the generator. Compared to other build systems that do all the work in one pass, Ninja's design effectively forces you to \"snapshot\" the action graph to disk once you've computed it. Another way to look at this is that it effectively has you cache the action graph across builds. On the other side, it also meant Ninja was useful in very flexible ways, because the generator could be as high level as the user wanted (\"tests are found by globbing the entire source tree for files with 'test' in their name\"). Importantly, it forced the developer who used Ninja to decide what they were going to pay for. If their generator program wanted to glob all over the disk looking for files, it was welcome to, but then it would be more obvious to them why their builds were slow. (I should note here that a clean separation between a generator and the resulting action graph is not as easy as I make it out to be. Ninja ultimately has a lot of fiddly details that are all struggling around which layer the work belongs in, but it's hard to write up the lessons learned.) The irony of this aspect of Ninja's design is that there is nothing preventing anyone else from doing this. Xcode or Visual Studio's build systems (for example) could just as well do the same thing: do a bunch of work up front, then snapshot the result for quick reexecution. I think the reason so few succeed at this is that it's just too tempting to mix the layers. Ninja's closest relative is Make, which attempts to encompass all of this programmer-facing functionality (with globbing, variable expansions, substringing, functions, etc.) that resulted in a programming language that was too weak to express all the needed features (witness autotools) but still strong enough to let people write slow Makefiles. This is vaguely Greenspun's tenth rule, which I strongly attempted to avoid in Ninja. Defaults dominate By default, Ninja executes the desired commands in parallel. Make is capable of this too; Ninja borrows the same flag name for this capability (-j) and just uses a different default value. However, because Make defaulted to running commands serially, it is relatively easy to write a Makefile that underspecifies dependencies such that it is unsafe to execute it in parallel. In fact, there's even some commercial vendor that offers some sort of \"Makefile accelerator\" tool that helps people discover and repair underspecified dependencies. In contrast, because Ninja always executes commands in parallel (even on a single-core system) it ends up revealing mistakes like these earlier. This means that programs that build with Ninja typically end up safe to build in parallel. (Ninja has no fancy system for detecting when you've gotten things wrong, it just causes wrong builds more often.) In contrast, users often forget or are unaware of the flag to Make that makes it also run in parallel. It is embarrassing to take credit for this because it's just a flag, but just because of its default value, Ninja in practice will end up being \"twice as fast as Make\" or more for users who aren't careful. The lesson is that all the optimization in the world doesn't matter if your users don't actually see it. Speed In this post I have talked about performance a few times, and it's important to note that there are lots of different kinds of performance metrics to care about in a build system. For example, one might care \"how long does the build take when I start from scratch?\" Ninja was solely focused on the edit-compile cycle of incremental builds in large codebases, which is to say you've run a build, you've edited one file, and you run the next build. When I wrote Ninja, I had a memory of blaze (aka bazel) being very fast, but it had been years since I had last used it. Because of this memory, I kept trying to make Ninja even faster, to try to catch up with my memory of blaze. Ironically it turns out blaze is not particularly fast on the speed metric I cared about; because blaze is a Java program, even having blaze print its \"help\" output is pretty slow. It is maybe silly of me to fixate on incremental builds, but I strongly believe that iteration time has a huge impact on programmer satisfaction, and Ninja is used exactly in the edit-compile loop where the difference of 1 second and 4 seconds is critical. I think I am personally more latency-sensitive than the average programmer, but I also believe that programmers feel latency and it affects their mood even if they don't notice it. (Google has recently done some research in this area that kinda confirmed my belief, here's hoping they'll publish it publicly!) It is very difficult to communicate to users the many possible meanings of \"fast\". The Ninja manual tries to warn people away from using it on small programs. Literally the second paragraph after the introduction says \"If your project is small, Ninja's speed impact is likely unnoticeable\" and recommends using a different build system. Unfortunately \"fast\" sells, and the Ninja list often had users trying to use it for their miniature apps and getting frustrated by its lack of features. Though Ninja focused on incremental rebuild performance, some users reported that Ninja improved their end-to-end build performance too. This was unintentional, but is because Ninja (again by virtue of doing nearly nothing) consumes very little CPU while running those builds, while comparable programs for whatever reason consume more CPU while running and that takes CPU away from the underlying build. In my post \"What does it mean for a browser to be fast\" I go into how there are many aspects to speed, and that ultimately what's important is the user's perception of speed. Ninja is very terse in its output: for most successful builds, it prints a single line. In contrast, other build systems tend to print a bunch of (often gratuitously colored) output with timing numbers about the various stages of the build it's going through, which makes them feel heavy. Ninja, by virtue of saying little, makes it feel even more like it's not there. CMake I originally built Ninja to work with Chrome's wacky one-off build system and left it at that. Somehow a kind stranger named Peter Collingbourne found Ninja and did the work to plug it into the much more popular CMake build system. Ninja's design fits well into CMake, but there were (as always) a lot of details to work out and Peter did most of this, initially to use Ninja to work on LLVM. This was more than just CMake, but also required building new semantics into Ninja. If anyone is responsible for making Ninja succeed out there in the real world, Peter is due the credit. The CMake authors eventually took over this integration and I feel bad about how poorly I supported them; they were very kind and patient with me but I never really had the time to answer their requests or concerns. Brad, if you read this, I am very sorry! To this day I've never actually used CMake and I never could find the time to worry about it. Windows Because the motivating project for Ninja was Chrome and Chrome targets Windows too, we got it working on Windows. (This was another part of Ninja that was primarily written by a contributor.) At a technical level, supporting Windows is mostly a big hassle. In the places where the Linux code doesn't work as is, it requires either uninteresting abstractions or major redesigns. For an example of the former, spawning processes and capturing their output is very different between the platforms, but mostly in that you need to learn a totally different API. For an example of the latter, Ninja's design centrally relies on the property that you can get a kernel-cached file's last modification time quickly, and that is just not true on Windows. But Windows is still a huge platform in terms of developers, and those developers are starved for tools. The underlying dynamic is that when someone makes a neat tool for Linux, the impulse is to share it, but when they do so for Windows, the impulse is to sell it, and so because of that there aren't as many tools freely available on Windows. It was a surprise to me how many of the early Ninja users were Windows users, but in retrospect it is kind of obvious: even if only one in a hundred Windows developers cared about Ninja, there are so many people on Windows that they would eventually show up. (Sometimes in the Linux Chrome early days we talked about it this way: even if we got ~all desktop Linux users using Chrome, in terms of total humans that's only like getting an additional 5% of Windows users. You can disagree with the specific value of that number but hopefully you get my point.) Related work I mentioned that I stumbled through Ninja's design. I regret not spending more time researching before building, but I intended the whole project as just a weekend demo hack, not a serious thing. (Relatedly, please forgive me for the embarrassing name.) Since then I have come to appreciate how important it is to actually understand the design space when building a thing. I now find myself noticing how rare it is for programmers to discuss related work and it now drives me mad. The term I used above (\"action graph\") is not how I thought about it in Ninja, but is instead taken from Google's build system (\"blaze\"/\"bazel\"). In bazel, they explicitly talk about how there's a graph of targets (higher-level user concepts like \"library\" and \"binary\") and how that generates a graph of actions (commands). I wrote above a bit about how the command-line text can be viewed as an 'input' to commands in the same way that files are. This is a specific instance of the broader concept of incremental computation, which covers not only build systems but also incrementality in UI. My friend Rado has been reading the research in this area for the past year or so(!) and is working on a series of blog posts that attempt to summarize it; watch for it. The Jane Street blog has had some work on summarizing this area too; as you can see there, there's even a connection to our recent renaissance in UI construction as found in React. The fantastic paper \"Build Systems à la Carte\" discusses incremental computation in the context of build systems. I wish this paper had existed before I wrote Ninja. Open source I'd like to wrap this up by talking a bit about being an open source maintainer. As you might have read elsewhere, it ends up not being especially fun. (This talk \"The Hard Parts of Open Source\" is worth your time.) When I try to take stock of my overall feelings about the project, it is a mixture of the occasional pride when I see someone on proggit or HN say something nice about it and a larger overall sense of disappointment. I made this thing that I thought was cool and I gave it away, and what I got back were occasionally friendly people who nicely requested things from me, but more often angry users who demanded things of me, and rarely anyone saying thanks. People repeatedly threatened to fork the project when I didn't agree to their demands, never once considering the possibility that I had more context on the design space than they did. A different source of sadness were the friendly and intelligent people who made reasonable-seeming contributions that conflicted with my design goals, where I wanted to repay their effort with a thorough explanation about why I was turning them down, and doing that was itself exhausting. I got into programming via free software, and I wrote code with the desire to give back to the people who gave me so much. (If you are reading this post then I predict you are benefiting from one or more one of my free software contributions.) But today I see that free software is not really about sharing between equals anymore; people instead think of themselves as customers and treat authors as if they can go complain to the manager. Another way of saying this is that today I am motivated by just trying to impress or live up to the ~ten hackers that I admire, people like apenwarr or agl or bradfitz or graydon, and while it's occasionally cool to meet someone and have the reputation of my software precede me, I think a lot of \"succeeding\" was mostly just kind of a burden. I think I could have learned about as much with a much smaller fraction of the success. Final acknowledgements I mentioned a few people already in this post, but I'd like to also specifically thank Nico Weber, both for being a careful collaborator and for taking on maintainership of Ninja for years, and also Jan Niklas Hasse, who took over after Nico and whom I don't know at all but who seems to be doing a fine job. And finally, thanks to the many other authors of Ninja.",
    "commentLink": "https://news.ycombinator.com/item?id=42268310",
    "commentBody": "The success and failure of Ninja (2020) (neugierig.org)256 points by quincepie 22 hours agohidepastfavorite68 comments defer 3 hours agoThis is hilarious to me: Android, which uses it for some large component of the system that I've never quite understood Ninja is really a huge part of AOSP, the build system initially used makefiles. Things got complex really fast with a custom declarative build system (soong) and a failed/aborted migration to bazel. Google developed kati (https://github.com/google/kati) which converts Makefiles to ninja build files (or should I say file), which really is huge: λ wc -l out/build-qssi.ninja 3035442 out/build-qssi.ninja Going from makefiles/soong to ninja is painful, it takes several minutes even in a modern machine but it simply flies once ninja picks it up. reply zelphirkalt 46 minutes agoparentAs someone, who has not used Ninja, what advantage is there, compared to Makefiles? And is it worth introducing yet another tool, to translate one to the other? Especially, when the Ninja files are that huge, possibly human-unreadable. reply willvarfar 21 hours agoprev> we talk about programming like it is about writing code, but the code ends up being less important than the architecture, and the architecture ends up being less important than social issues. A thousand times this! This puts into words something that's been lurking in the back of my mind for a very long time. reply nuclearnice3 21 hours agoparentStrongly agree. Peopleware 1987 [1] > The first chapter of the book claims, \"The major problems of our work are not so much technological as sociological in nature\". The book approaches sociological or 'political' problems such as group chemistry and team jelling, \"flow time\" and quiet in the work environment, and the high cost of turnover [1] https://en.wikipedia.org/wiki/Peopleware:_Productive_Project... reply no_wizard 20 hours agorootparentI’ve been drumming this for so long now, even before I heard of (let alone read) this book. I feel that the development of psychology and sociology has been lost on the workplace and it isn’t well applied. Executives want everyone to be widgets except themselves, even when study after study shows that for companies to perform optimally their workers must feel well compensated, well valued, balanced freedom in the workplace, chances for advancement etc. In many respects you could apply psychology and sociology to how products should / could behave etc. as well, which I’m sure due to the monetary component some companies have taken seriously at least in some periods of their lifecycle, like Apple under Steve Jobs in his comeback reply pydry 10 hours agorootparent>Executives want everyone to be widgets except themselves Of course. This maximizes their relative power within the company. Some executives are focused on the health of a company as a whole but not many. To most of them the pie can be assumed to be a fixed size and their job is to take as much of it as possible. reply zemvpferreira 6 hours agorootparentFor businesses or business areas where excellent isn’t necessary and good will do, this attitude can even be considered to be in the best interest of the company. The more fungible employees are made, the less bargaining power they have. reply BOOSTERHIDROGEN 15 hours agorootparentprevWhat if the company has significant constraints on its financial health? reply lmm 14 hours agorootparentThen it's all the more important to avoid unnecessary employee turnover. reply mst 3 hours agorootparentPeople tend to vastly underestimate how much the time needed for a new hire to come up to speed costs the employer. This is true even of (theoretically simple) things like retail jobs, because even if you're proficient in the basic skill set on day one, coming up to speed on the rhythm of a specific workplace still takes time. I'm buggered if I can remember where I saw it, but there was a study once that showed that (in that specific instance, I have no clue as to whether or not it generalises) a minimum wage increase actually *saved* retail/service employers in the area money overall, just because the reduced churn meant that over the lifetime of an employee with the company the fact that said lifetime was longer meant they were getting enough more value per hour out of each employee to more than compensate for the higher cost per hour. Of course the study could always have been wrong, but it didn't seem obviously so back when I looked at it and it at the very least seems plausible to me. reply zelphirkalt 44 minutes agoparentprevIt is not a dichotomy though, as a good architecture manages to fulfill the requirements people have for the system _and_ keeps it understandable for human beings. reply transpute 20 hours agoparentprevhttps://en.wikipedia.org/wiki/Conway's_law > Organizations which design systems (in the broad sense used here) are constrained to produce designs which are copies of the communication structures of these organizations. — Melvin E. Conway, How Do Committees Invent? reply transpute 16 hours agorootparentSource code repos could have USER.md and DEVELOPER.md files to record social context. reply lou1306 9 hours agorootparentBut again, that is at best infrastructure documentation, not code. Unless you dilute the term \"code\" until it loses nearly all utility. reply mihaaly 21 hours agoparentprevConsidering that programing and tools used for it are not for computers but humans, and that apart from most trivial things more than one people is necessary to make something that work on/with computer(s), it is no surprise that SE is much more social science than many would like to admit or feel comfortable with, over-emphasizing its natural science part to the level of failure eventually (on the product level aimed at addressing needs of the people). Probably because social sciences are very fluid and much less reliable than natuaral sciences, so we have an inner tendency avoiding the social bit, or handling it on a very primitive level? I do not know, this is a feeling. So much focus on atomic details of technology yet the group effort of the product is still rubbish too many times. reply Swizec 21 hours agoparentprevIn my experience roughly 80% of technical issues are because 2 people (or teams) didn’t want to just sit down together and talk it out. reply IgorPartola 20 hours agoparentprevThis precisely describes why Google Glass failed. reply mattigames 11 hours agorootparentElaborate? reply IgorPartola 5 hours agorootparentDoesn’t matter how good the platform was, it wasn’t a socially acceptable product. reply mst 3 hours agorootparentI *really* wanted basically \"google class without the frigging camera.\" Being able to overlay an 80x24 terminal over one of my eyes (and drive it with a bluetooth keyboard or whatever) would've been fantastic for me. Unfortunately for me, this is enough of an outlier desire that it doesn't seem likely anybody will ever want to sell me that at a price point I can convince myself of. reply kevin_thibedeau 3 hours agorootparentEven Realities is approaching that now: https://www.youtube.com/watch?v=bckifBIPlHI&t=136s reply dang 22 hours agoprevDiscussed at the time: The Success and Failure of Ninja - https://news.ycombinator.com/item?id=23157783 - May 2020 (38 comments) (Reposts are fine after a year or so! links to past threads are just to satisfy extra-curious readers) reply bakudanen 57 minutes agoprevI had my stint with build systems. Nx, Bazel to name a few. In the past I was always the go to guy to configure these stuffs. OP said that ninja is small enough to be implemented in your favorite programming language. I wonder if there is step by step tutorial to create your own build system? reply high_priest 18 hours agoprev> I also believe that programmers feel latency and it affects their mood even if they don't notice it. (Google has recently done some research in this area that kinda confirmed my belief, here's hoping they'll publish it publicly!) Anyone knows if it happened? Has the google research on latency been published? reply quincepie 16 hours agoparentNot sure if it's the exact research that the author is referring to, but it could be this one: https://www.computer.org/csdl/magazine/so/2023/04/10176199/1... reply marcosdumay 2 hours agorootparentI don't think anybody talking about \"latency\" without a qualifier is thinking about build latency. But it's a nice article. The idea that giving-up on waiting for a delay has a simple exponential distribution is something that I never thought. (And now I'm fixed on understanding why... Something must have biased me against it.) reply pjmlp 19 hours agoprevGiven that ninja is required for C++20 modules when using CMake, it is going to stay around for quite a bit. reply mgaunard 21 hours agoprevI switched to samurai for the few things I have that still used ninja; it's an improvement in every possible way. But regardless, I think those kinds of build systems are just wrong. What I want from a build system is to hash the content of all the transitive inputs and look up if it exists or not in a registry. reply dikei 7 hours agoparentYes, basically any build system that supports distributed caching use digest instead of timestamp when checking modification: Bazel, Pants, Buck, etc. They're all hugely complex though. For local build only, I think SCons and Waf both use hash for changes detection. reply mgaunard 6 hours agorootparentAny build system is overly generic and it's up to the user to define how things should be built. So what happens is that at the end of the day every project ends up with a poorly made build system layered on top of a third-party generic tool but without abstracting away its complexity or abstractions. My opinion is that a build system should figure out on its own how to build files, that is its job. The last thing I want to do is to define targets or dependencies. All of this is already implicit from the code itself and is useless busywork. I should just point it to a file or directory and that is it. I prefer to just build my own build systems, bespoke to each project or environment, that just does what it should, no more and no less, leveraging the conventions in place and neatly integrating with team workflows (debugging, sanitizers, continuous integration, release, packaging, deployment, etc.) I find that when you do that, there isn't much value in using any of the tools, they just add noise or make things slow. Running a graph of compiler and linker commands in parallel is fairly trivial and can be done in 20 lines of Python. The hard part is figuring out where the dependencies live, which versions to pick, and how the code implies those dependencies; for which the tools do nothing. reply dikei 2 hours agorootparentThe problem with handcrafted build system is only the author can effectively maintain it. When he moves on, someone has to spend the time ripping it out and replace with something more standard. I've been on both end of this situation and would rather not do it again, so I'll use whatever is the de-facto standard, but you do you. reply mgaunard 1 hour agorootparentAny project effectively has a handcrafted build system, whether it's built on top of CMake, Bazel, Scons or built from scratch doesn't really affect that. And if it's doing everything from scratch, it's more likely to be simple and self-contained, making it easier to maintain. reply Sesse__ 19 hours agoparentprevYou might be interested in n2, from the author of ninja. reply chubot 15 hours agoparentprevWhat’s better about Samurai? I thought it was a compatible subset of ninja Also, “not the thing I wanted” doesn’t mean “wrong”, simply because there are other people in the world with different preferences reply mgaunard 9 hours agorootparentOne thing in particular that's always been a problem with Ninja is the output. It does too much buffering, removes colors without being able to force them back, and in general leads to an experience where for me it's not usable since I want to pipe its output to a pager. When I used ninja I needed to maintain builds with all sorts of patches to fix it. With samurai it just did the right thing out of the box. reply phyrex 11 hours agoparentprevThat's how metas buck2 works reply bonzini 7 hours agoparentprevIs Samurai still alive? I have sent a pull request to improve signal handling but it has been sitting ignored for over half a year. reply tc4v 12 hours agoparentprevsounds like https://gittup.org/tup/ to me. reply TOGoS 20 hours agoparentprevI think that was the idea behind NetKernel. I've built something similar, a Deno library called \"TDAR\"[1], and it works well, but it takes some work to wrap up all the command-line tools that expect to work in some mutable filesystem so that you can pretend you're calling pure functions. [1] I haven't got around to pulling it out of the parent project[2], but I talked about it in this youtube video: https://youtu.be/sty29o8sUKI [2] If you're interested in this kind of thing you could poke me to open up the source for that thing. togos zero zero at gee mail dot comb reply dima55 20 hours agoparentprevThat's called \"ccache\" reply mgaunard 20 hours agorootparentccache is just a hack to make traditional build systems less stupid. Good build systems have native support for these things. reply edflsafoiewq 13 hours agoprevMost interesting point to me > You must often compromise between correctness and convenience or performance and you should be intentional when you choose a point along that continuum. I find some programmers are inflexible when considering this dynamic, where it's somehow obvious that one of those concerns dominates, but in my experience the interplay is pretty subtle; for example, a tool that trades off correctness for convenience might overall produce a more correct ecosystem than a more correct but less convenient alternative, if programmers end up avoiding the latter. reply bakudanen 54 minutes agoparentThis is goldmine. This is why Pyhon, Go, and TypeScript/JavaScript is way more popular than Haskell/OCaml. reply grobibi 21 hours agoprevI thought this was going to be about people buying less air fryers. reply Krastan 19 hours agoparentI thought this was going to be about the Fortnite streamer reply airstrike 20 hours agoparentprevI thought of the smoothie blenders first too, but I can't see how they would ever have failed given how great they are. My life has changed since buying the first such blender about 4 months ago reply firesteelrain 15 hours agorootparentOh don’t call the Ninja a blender - there is a giant thread on one of the main FB groups. OP is getting ripped reply Spivak 13 hours agorootparentIs there some fun tea here? Ninja themselves describe them as blenders, has the community mythologized them into something else? reply firesteelrain 9 hours agorootparentMore like an ice shaver that adds air is what the community likes to call it Because blenders don’t turn things into an ice cream texture reply ultrafez 8 hours agorootparentWe're conflating the Ninja Creami with Ninja's smoothie makers and blenders - they are separate product lines reply firesteelrain 6 hours agorootparentOk reply zX41ZdbW 19 hours agoprev> Relatedly, please forgive me for the embarrassing name. The name is great! PS. It's possible to make it even faster if we implement this: https://github.com/ninja-build/ninja/issues/2157 But you explained in the article that the tool intentionally lacks state, even tiny hints from previous runs. reply forrestthewoods 8 hours agoprevNinja is pretty popular with gamedevs. I was amused by this line: > But Windows is still a huge platform in terms of developers, and those developers are starved for tools. As a primarily Windows dev I feel that it is poor Linux devs who are starved for tools! Living life without a good debugger (Visual Studio) or profiler (Superluminal) is so tragic. ;( It does feel like in recent years the gap between the two platforms is increasingly minimal. I definitely like all the Rust utilities that generally work crossplatform for example. reply marcosdumay 2 hours agoparentEvery time somebody puts \"Visual Studio\" on the same sentence as \"good\" I get that strange feeling the Universe is a simulation and other people are interacting here, but do come from a different one. I can't make for any other explanation. I can't think on any interaction with it that I would describe as \"good\". I can think of a few \"minimally ok\", but debugging isn't one of them. (But at least on the 2022 the debugger isn't full of bugs anymore. Maybe that's what this is about.) reply dahart 1 hour agorootparentI don’t like working in Visual Studio much, and I am a big fan of gdb too (and of Chrome’s debugger when working in JavaScript), but for C++ debugging, the Visual Studio debugger is excellent, and has been near the top of the class for a long time, compared to other debuggers. That is the explanation. I don’t doubt there are warts, but for you what’s missing or sub-par from VS that is better elsewhere? What debuggers do you consider better? Gdb is also excellent, but in a different way. Gdb is programmable and that maybe makes it more powerful. (I don’t know if VS debugging is scriptable, I think it wasn’t last time I tried.) But gdb’s learning curve, lack of UI (even with tui), and lack of discoverability is a major impediment to it’s use. You mentioned interaction, and interaction is what holds back gdb. reply forrestthewoods 1 hour agorootparentprevVisual Studio debugger for C++ is still best in class. It’s far from perfect. But Linux doesn’t even have anything that attempts to compete. Command line GDB and LLDB are not comparable. reply 3836293648 7 hours agoparentprevIn what world do you live in where the visual studio debugger in considered good? Or have they finally got around to fixing it? Last I tried it was unbearably slow, like seconds to step a single line reply 71bw 5 hours agorootparentSounds like a hardware issue, works fine on my machine. No speed issues at all. reply rnewme 6 hours agorootparentprevI do admit I haven't used it for almost a decade but wasn't vs debugger (at least for cpp) considered top notch and unrivaled? What's better nowadays? reply forrestthewoods 1 hour agorootparentprevThe world where I’ve used it professionally debug C++ for almost 20 years? It’s certainly not perfect. But “seconds to step a single line” is not normal. Certainly not what I experience. Even when debugging very large code bases like Unreal Engine. reply pjmlp 3 hours agoparentprevThere is a community that thinks UNIX is the be all, end all of developer tools, and then they miss the trees from the forest. I know UNIX pretty well, since being introduced to Xenix in 1993, used plenty of variants, and yet my main use of WSL is to run Linux docker containers and nothing else. reply einpoklum 21 hours agoprev### Statistics ### ninja has ~26 kloc, ~3,100 commits, and only a quarter of them by the original author (although by loc changed their weight is higher). Interesting! https://github.com/ninja-build/ninja/graphs/contributors ### Bunch of other comments ### > users of ninja ... all Meson projects, which appears to increasingly be the build system used in the free software world; So, AFAICT, that hasn't turned out to be the case. > the code ends up being less important than the architecture, and the architecture ends up being less important than social issues. Well... sometimes. Other times, the fact that there's good code that does something goes a very long way, and people live with the architectural faults. And as for the social issues - they rarely stand in opposition to the code itself. > Some pieces of Ninja took struggle to get to and then are obvious in retrospect. I think this is true of much of math Yup. And the some of the rest of math becomes obvious when some re-derives it using alternative and more convenient/powerful techniques. > I think the reason so few succeed at this is that it's just too tempting to mix the layers. As an author of a library that also focuses on being a \"layer\" of sorts (https://github.com/eyalroz/cuda-api-wrappers/), I struggle with this temptation a lot! Especially when, like the author says, the boundaries of the layers are not as clear as one might imagine. > I strongly believe that iteration time has a huge impact on programmer satisfaction I'm pretty certain that the vast majority developers perform 10x more incremental builds than full builds. So, not just satisfaction - it's just most of what we do. It's also those builds which we wait-out rather than possible go look for some distraction: https://xkcd.com/303/ OTOH, the article doesn't mention interaction with build artifact caching schemes, which lessen the difference between building from scratch and building incrementally. > Peter Collingbourne found Ninja and did the work to plug it into the much more popular CMake ... If anyone is responsible for making Ninja succeed out there in the real world, Peter is due the credit. It is so gratifying when a person you didn't know makes your software project that much more impactful! Makes you really feel optimistic again about humanity and socialism and stuff. reply a_t48 21 hours agoparentIm going to have to give your CUDA wrapper a look later. :) reply einpoklum 19 hours agorootparentI should say that unlike the author of ninja though, I am _very_ interested in user complaints and criticism, even if its not fully articulated and respectful. I _need_ contradiction and opposition to go beyond the bounds of my own conceptions as a almost-always-sole developer and sole maintainer of the library. I may not accept/agree with everything, but I'll at least know to take the concerns into consideration. And I've already refactored quite a bit over the years based on use cases user have pointed out to me. reply mst 3 hours agorootparentSome people are wired to find disrespectful complaints and unconstructive criticism genuinely upsetting (which is unfortunate in a bunch of ways, but OTOH the same personality traits often also make for somebody who's fantastic at handholding polite newbies through learning something). I am excellent at finding such things either hilarious or grounds to say \"well, if you're going to be like that, I can't say I care about your opinion, piss off\" and moving on to the next complaint in the hopes I can get useful feedback out of that one. But there's a fair swathe of newbies where I have to step back and let other people help them instead, because if I try I'll end up accidentally driving them off and feeling like a dickhead afterwards :D (I have tried and failed repeatedly at \"Not Being a Bastard,\" so I've settled for leveling up in \"Being a Self Aware Bastard\" instead; at least that reduces how often I end up causing *un*intentional offence ;) reply a_t48 18 hours agorootparentprevSame :) I started down the rabbit hole of abstracting CUDA for our robotics framework, but it’s not really something I want to maintain right now. reply santoshalper 21 hours agoprevMan, I was so afraid this was going to be about Fortnite. Turns out it was a fantastic read. I feel really sad but unsurprised about his description of what it's like to be an Open Source maintainer. reply burrish 4 hours agoprev [–] Damn and here I was expecting real Ninjas reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Ninja, a build system created as a side project, has gained significant popularity and is now used by major projects like Chrome, Android, and Meson.",
      "The system is designed for speed and efficiency, utilizing a bipartite graph for better build structure and focusing on fast incremental builds.",
      "The creator reflects on the challenges of being an open-source maintainer, noting the high demands and limited gratitude despite Ninja's success."
    ],
    "commentSummary": [
      "The discussion focuses on the Ninja build system's role in Android's AOSP (Android Open Source Project) and the transition complexities from makefiles to Ninja.",
      "Users debate Ninja's advantages over Makefiles, considering architecture, social issues, and the impact of iteration time on programmer satisfaction.",
      "The conversation also mentions other build systems like Samurai and the challenges of maintaining handcrafted build systems, highlighting the role of social dynamics in tech environments."
    ],
    "points": 256,
    "commentCount": 68,
    "retryCount": 0,
    "time": 1732825274
  },
  {
    "id": 42270378,
    "title": "How much memory do you need in 2024 to run 1M concurrent tasks?",
    "originLink": "https://hez2010.github.io/async-runtimes-benchmarks-2024/",
    "originBody": "How Much Memory Do You Need in 2024 to Run 1 Million Concurrent Tasks? Did you still remember the memory consumption comparison between asynchronous programming across popular languages in 2023? Now at the end of 2024, I wonder how things changed in the span of one year, with the latest version of languages. Let's do the benchmark again and see the results! Benchmark The program to benchmark is the same with the one in the last year: Let's launch N concurrent tasks, where each task waits for 10 seconds and then the program exists after all tasks finish. The number of tasks is controlled by the command line argument. This time, let's focus on coroutine instead of multiple threads. All benchmark code can be accessed at async-runtimes-benchmarks-2024. What is a coroutine? Coroutines are computer program components that allow execution to be suspended and resumed, generalizing subroutines for cooperative multitasking. Coroutines are well-suited for implementing familiar program components such as cooperative tasks, exceptions, event loops, iterators, infinite lists and pipes. Rust I created 2 programs in Rust. One uses tokio: use std::env; use tokio::time::{sleep, Duration}; #[tokio::main] async fn main() { let args: Vec = env::args().collect(); let num_tasks = args[1].parse::().unwrap(); let mut tasks = Vec::new(); for _ in 0..num_tasks { tasks.push(sleep(Duration::from_secs(10))); } futures::future::join_all(tasks).await; } while another one uses async_std: use std::env; use async_std::task; use futures::future::join_all; use std::time::Duration; #[async_std::main] async fn main() { let args: Vec = env::args().collect(); let num_tasks = args[1].parse::().unwrap(); let mut tasks = Vec::new(); for _ in 0..num_tasks { tasks.push(task::sleep(Duration::from_secs(10))); } join_all(tasks).await; } Both are popular async runtime commonly used in Rust. C# C#, similar to Rust, has first-class support for async/await: int numTasks = int.Parse(args[0]); List tasks = new List(); for (int i = 0; ithreads = new ArrayList(); for (int i = 0; i{ try { Thread.sleep(Duration.ofSeconds(10)); } catch (InterruptedException e) { // Handle exception } }); threads.add(thread); } for (Thread thread : threads) { thread.join(); } } } While there's a new variant of JVM called GraalVM. GraalVM also offers native image, which is a similar concept to NativeAOT in .NET. So we added the benchmark for GraalVM as well. Test Environment Hardware: 13th Gen Intel(R) Core(TM) i7-13700K OS: Debian GNU/Linux 12 (bookworm) Rust: 1.82.0 .NET: 9.0.100 Go: 1.23.3 Java: openjdk 23.0.1 build 23.0.1+11-39 Java (GraalVM): java 23.0.1 build 23.0.1+11-jvmci-b01 NodeJS: v23.2.0 Python: 3.13.0 All programs were launched using the release mode if available, and support for internationalization and globalization was disabled as we did't have libicu in our test environment. Results Minimum Footprint Let's start from something small, because some runtimes require some memory for themselves, let's first launch only one task. We can see that Rust, C# (NativeAOT), and Go achieved similar results, as they were compiled statically to native binaries and needed very little memory. Java (GraalVM native-image) also did a great job but cost a bit more than the other statically compiled ones. The other programs running on managed platforms or through interpreters consume more memory. Go seems to have the smallest footprint in this case. Java with GraalVM is a bit surprising, as it cost far more memory than Java with OpenJDK, but I guess this can be tuned with some settings. 10K Tasks A few surprises here! The two Rust benchmarks achieved very promising results: they both used very little memory, which didn't grow too much compared to minimal footprint results, even though there were 10K tasks running behind the scenes! C# (NativeAOT) followed closely behind, using only ~10MB of memory. We need more tasks to put more pressure on them! The memory consumption grew dramatically in Go. Goroutines are supposed to be very lightweight, but they actually consumed far more RAM than Rust required. In this case, virtual threads in Java (GraalVM native image) seem to be more lightweight than Goroutines in Go. To my surprise, both Go and Java (GraalVM native image), which were compiled to native binaries statically, cost more RAM than the C# one running on a VM! 100K Tasks After we increased the number of tasks to 100K, the memory consumption of all the languages started to grow significantly. Both Rust and C# did a really good job in this case. A big surprise is that C# (NativeAOT) even cost less RAM than Rust and beat all other languages. Really impressive! At this point, the Go program has been beaten not only by Rust but also by Java (except the one running on GraalVM), C#, and NodeJS. 1 Million Tasks Let's go extreme now. Finally, C# undoubtedly beat all other languages; it's very competitive and has really become a monster. And as expected, Rust continues to do a good job on memory efficiency. The distance between Go and the others increased. Now Go loses by over 13 times to the winner. It also loses by over 2 times to Java, which contradicts the general perception of the JVM being a memory hog and Go being lightweight. Final Word As we have observed, a high number of concurrent tasks can consume a significant amount of memory, even if they do not perform complex operations. Different language runtimes have varying trade-offs, with some being lightweight and efficient for a small number of tasks but scaling poorly with hundreds of thousands of tasks. Many things have changed since last year. With the benchmark results on the latest compilers and runtimes, we see a huge improvement in .NET, and .NET with NativeAOT is really competitive with Rust. The native image of Java built with GraalVM also did a great job in terms of memory efficiency. However, goroutines continue to be inefficient in resource consumption. Appendix Some folks pointed out that in Rust (tokio) it can use a loop iterating over the Vec instead of join_all to avoid the resize to the list introduced by join_all. So I added a new test case Rust (tokio-for) here: use std::env; use tokio::time::{sleep, Duration}; #[tokio::main] async fn main() { let args: Vec = env::args().collect(); let num_tasks = args[1].parse::().unwrap(); let mut tasks = Vec::new(); for _ in 0..num_tasks { tasks.push(sleep(Duration::from_secs(10))); } for task in tasks { task.await; } } Note that this won't work for async_std as it needs you to poll explicitly until the task being scheduled and executed, so switching to a loop will make it run the tasks sequentially. Let's see what will happen with the new test code. Minimum Footprint 10K Tasks 100K Tasks 1M Tasks This shrinks the cost of Rust (tokio) by about a half, which makes Rust the absolute lead in this benchmark. Good job, Rust.",
    "commentLink": "https://news.ycombinator.com/item?id=42270378",
    "commentBody": "How much memory do you need in 2024 to run 1M concurrent tasks? (hez2010.github.io)245 points by neonsunset 16 hours agohidepastfavorite177 comments AkshitGarg 14 hours agoI feel this benchmark compares apples to oranges in some cases. For example, for node, the author puts a million promises into the runtime event loop and uses `Promise.all` to wait for them all. This is very different from, say, the Go version where the author creates a million goroutines and puts `waitgroup.Done` as a defer call. While this might be the idiomatic way of concurrency in the respective languages, it does not account for how goroutines are fundamentally different from promises, and how the runtime does things differently. For JS, there's a single event loop. Counting the JS execution threads, the event loop thread and whatever else the runtime uses for async I/O, the execution model is fundamentally different from Go. Go (if not using `GOMAXPROCS`) spawns an OS thread for every physical thread that your machine has, and then uses a userspace scheduler to distribute goroutines to those threads. It may spawn more OS threads to account for OS threads sleeping on syscalls. Although I don't think the runtime will spawn extra threads in this case. It also depends on what the \"concurrent tasks\" (I know, concurrency != parallelism) are. Tasks such as reading a file or doing a network call are better done with something like promises, but CPU-bound tasks are better done with goroutines or Node worker_threads. It would be interesting to see how the memory usage changes when doing async I/O vs CPU-bound tasks concurrently in different languages. reply n2d4 13 hours agoparentActually, I think this benchmark did the right thing, that I wish more benchmarks would do. I'm much less interested in what the differences between compilers are than in what the actual output will be if I ask a professional Go or Node.js dev to solve the same task. (TBF, it would've been better if the task benchmarked was something useful, eg. handling an HTTP request.) Go heavily encourages a certain kind of programming; JavaScript heavily encourages a different kind; and the article does a great job at showing what the consequences are. reply rtpg 11 hours agorootparentBut you wouldn't call a million tasks with `Promise.all` in Node, right? That's just not a thing that one does. Instead, there's usually going to be some queue outside the VM that will leave you with _some_ sort of chunking and otherwise working in smaller, more manageable bits (that might, incidentally, be shaped in ways that the VM can handle in interesting ways). It's definitely true to say that the \"idioamatic\" way of handling things is worth going into, but if part of your synthetic benchmark involves doing something quite out of the ordinary, it feels suspicious. I generally agree that a \"real\" benchmark here would be nice. It would be interesting if someone could come up with the \"minimum viable non-trivial business logic\" that people could use for these benchmarks (perhaps coupled with automation tooling to run the benchmarks) reply hamandcheese 11 hours agorootparent> But you wouldn't call a million tasks with `Promise.all` in Node, right? That's just not a thing that one does. But neither would you wait on a waitgroup of size 1 million in Go... right? reply ricardobeat 10 hours agorootparentYou could, and the tasks would run concurrently. Node is single threaded so unless you used one of the I/O calls backed by a thread pool, they would all execute sequentially . reply rfoo 9 hours agorootparentprevFor a goroutine doing nothing, no. But if I have 1 million tasks which spent 10% of their time on CPU-bound codes, intermixed with other IO-bound codes, and I just want throughput and I'm too lazy to use a proper task queue, then why not? reply rtpg 11 hours agorootparentprevyeah, right? I mean I don't have a dog in this race, just wished we could get into \"normal\" repros without having to wonder if some magic is kicking in reply Quothling 13 hours agorootparentprev> Go heavily encourages a certain kind of programming; True, but it really doesn't encourage you to run 1m goroutines with the standard memory setting. Though it's probably fair to run Go wastefully when you're comparing it to Promise.All. reply n2d4 13 hours agorootparentOf course! That's why the article is telling you that some languages (C#, Rust) are better at it than others (Go, Java). Doesn't mean that Go and Java are bad languages! Just that they aren't good to do this thing. reply Quothling 12 hours agorootparentThe article is telling us that you can run really inefficient code. Goroutines should be run with worker pools and a buffered channel and it's silly to not do that and then compare it to things like an optimized Rust crate like Tokio. reply Aeolun 12 hours agorootparentIs that the ideomatic way to do it, or the best way you can imagine? reply Quothling 12 hours agorootparent> Is that the ideomatic way to do it Well... I'm actually not sure what ideomatic means (English isn't my first language), but it's the standard way of doing it. You'll even find it as step 2 and 3 here: https://go.dev/tour/concurrency/1 > or the best way you can imagine I would do a lot much more to tune it if you were in a position where you'd know it would run that many \"tasks\". I think what many non-Go programmers might run into here is that Go doesn't come with any sort of \"magic\". Instead it comes with a highly opinionated way of doing things. Compare that to C# which comes with a highly optimized CLR and a bunch really excellent libraries which are continuously optimized by Microsoft and you're going to end up with an article like this. The async libraries are maintaining which tasks are running (though Promise.All is obviously also binding a huge amount of memory you don't have to), while the Go example is running 1 million at once. You'll also notice that there is no benchmark for execution time. With Go you might actually want to pay with memory, though I'd argue that you'd almost never want to run 1 million Goroutines at once. Though to be fair to this specific author, it looks like they copied the previous benchmarks and then ran it as-is. reply guitarbill 12 hours agorootparentThe post was edited, previously it just said roughly this part: \"step 2 and 3 here: https://go.dev/tour/concurrency/1\". Which - as far as I can tell - does not mention worker pools... reply Quothling 11 hours agorootparentYou're right. It is using channels and buffers, but you're right. It's not part of the actual documentation either, at least not exactly: https://go.dev/doc/effective_go#concurrency You will achieve much the same if you follow it, but my answer should have been yes and no as far as being the \"standard\" Go way. reply pbhjpbhj 11 hours agorootparentprevIdiomatic is the word the parent was looking for. The base word is idiom. It was probably the intent of the parent to mean 'making use of the particular features of the language that are not necessarily common to other languages'. I'm not a programmer, but you appear to give good examples. I hope I'm not teaching you to suck eggs... {That's an idiom, meaning teaching someone something they're already expert in. Like teaching your Grandma to suck eggs - which weirdly means blowing out the insides of a raw egg. That's done when using the egg to paint; which is a traditional Easter craft.} reply Quothling 11 hours agorootparentI actually did find \"idiomatic\" when I looked it up, but I honestly still didn't quite grasp it from the cambridge dictionary. Thanks for explaining it in a way I understand. reply jchw 12 hours agorootparentprevI'm torn. As far as practicality goes I actually agree with you: if I knew I were trying to do something to the order of 1,000,000 tasks in Go I would probably use a worker pool for this exact reason. I have done this pattern in Go. It is certainly not unidiomatic. However, it also isn't the obvious way to do 1,000,000 things concurrently in Go. The obvious way to do 1,000,000 things concurrently in Go is to do a for loop and launch a Goroutine for each thing. It is the native unit of task. It is very tightly tied to how I/O works in Go. If you are trying to do something like a web server, then the calculus changes a lot. In Go, due to the way I/O works, you really can't do much but have a goroutine or two per connection. However, on the other hand, the overhead that goroutines imply starts to look a lot smaller once you put real workloads on each of the millions of tasks. This benchmark really does tell you something about the performance and overhead of the Go programming language, but it won't necessarily translate to production workloads the way that it seems like it will. In real workloads where the tasks themselves are usually a lot heavier than the constant cost per task, I actually suspect other issues with Go are likely to crop up first (especially in performance critical contexts, latency.) So realistically, it would probably be a bad idea to extrapolate from a benchmark this synthetic to try to determine anything about real world workloads. Ultimately though, for whatever purpose a synthetic benchmark like this does serve, I think they did the correct thing. I guess I just wonder exactly what the point of it is. Like, the optimized Rust example uses around 0.12 KiB per task. That's extremely cool, but where in the real world are you going to find tasks where the actual state doesn't completely eclipse that metric? Meanwhile, Go is using around 2.64 KiB per task. 22x larger than Rust as it may be, it's still not very much. I think for most real world cases, you would struggle to find too many tasks where the working set per task is actually that small. Of course, if you do, then I'd reckon optimized async Rust will be a true barn-burner at the task, and a lot of those cases where every byte and millisecond counts, Go does often lose. There are many examples.[1] In many cases Go is far from optimal: Channels, goroutines, the regex engine, various codec implementations in the standard library, etc. are all far from the most optimal implementation you could imagine. However, I feel like they usually do a good job making the performance very sufficient for a wide range of real world tasks. They have made some tradeoffs that a lot of us find very practical and sensible and it makes Go feel like a language you can usually depend on. I think this is especially true in a world where it was already fine when you can run huge websites on Python + Django and other stacks that are relatively much less efficient in memory and CPU usage than Go. I'll tell you what this benchmark tells me really though: C# is seriously impressive. [1]: https://discord.com/blog/why-discord-is-switching-from-go-to... reply Quothling 11 hours agorootparentI agree with everything you said and I think you contributed a lot to what I said making things much more clear. > I'll tell you what this benchmark tells me really though: C# is seriously impressive. The C# team has done some really great work in recent years. I personally hate working with it and it's \"magic\", but it's certainly in a very good place as far as trusting the CLR to \"just work\". Hilariously I also found the Python benchmark to be rather impressive. I was expecting much worse. Not knowing Python well enough, however, makes it hard to really \"trust\" the benchmark. A talented Python team might be capable of reducing memory usage as much as following every step of the Go concurrency tour would for Go. reply neonsunset 11 hours agorootparentUserspace scheduling of Goroutines, virtual stack and non-deterministic pointer type allocation in Go are as much magic if not more, the syntactic sugar of C# is there to get the language out of your way and usually comes at no cost :) If you do not like the aesthetics of C# and find Elixir or OCaml family tolerable - perhaps try F#? If you use task CEs there you end up with roughly the same performance profile and get to access huge ecosystem making it one of the few FP languages that can be used in production with minimal risk. reply jonathanstrange 9 hours agorootparentprevNo professional Go programmer would spawn 1M goroutines unless they're sure they have the memory for it (and even then, only if benchmarks indicate it, which is unlikely). Goroutines have a static stack overhead between 2KiB to 8KiB depending on the platform. You'd use a work stealing approach with a reasonable number of goroutines instead. How many are reasonable needs to be tested because it depends on how long each Goroutine spends waiting for I/O or sleeping. But I can go further than that: No professional programmer should run 1M concurrent tasks on an ordinary CPU no matter which language because it makes no sense if the CPU has several orders of magnitudes less cores. The tasks are not going to run in parallel anyway. reply OutOfHere 8 hours agorootparentThe basis for running 1 million concurrent tasks is to support 1 million active concurrent user connections. They don't need to run in parallel if async is used. As shown, Rust and C# do well. How would you support it in Go? reply theamk 14 minutes agorootparentPay extra $1/month for bigger server? The numbers are really small here. reply jonathanstrange 5 hours agorootparentprevThe servers I use have limits far below 1M active connections, realistically speaking about 60k simultaneously active connections. So I can't really answer that question. However, it's easy to find answers to that question online [1]. Go is not forcing you to spawn Goroutines when you don't really need them. As I said, the correct way in Go is to use worker pools, the size of which depends on measurable performance because it is connected to how much i/o each Goroutine performs and how long it waits on average. [1] https://www.freecodecamp.org/news/million-websockets-and-go-... reply YetAnotherNick 12 hours agorootparentprevThe fundamental problem is there are two kind of sleep function. One that actually sleeps and other that is a actually a timer that just calls certain callback after a desired interval. Promise is just a syntactic sugar on top of second type. Go certainly could call another function after desired interval using `Timer`. I think better comparison would be wasting CPU for 10 seconds instead of sleep. reply SPascareli13 14 hours agoparentprevAs far as I know there is no way to do Promise like async in go, you HAVE to create a goroutine for each concurrent async task. If this is really the case then I believe the submition is valid. But I do think that spawning a goroutine just to do a non-blocking task and get its return is kinda wasteful. reply n2d4 13 hours agorootparentYou could in theory create your own event loop and then get the exact same behaviour as Promises in Go, but you probably shouldn't. Goroutines are the way to do this in Go, and it wouldn't be useful to benchmark code that would never be written in real life. reply peterhon 13 hours agorootparentI guess what you can do in golang that would be very similar to the rust impl would be this (and could be helpful even in real life, if all you need is a whole lot of timers): func test2(count int) { timers := make([]*time.Timer,count) for idx, _ := range timers { timers[idx] = time.NewTimer(10 * time.Second) } for idx, _ := range timers {Apparently allocating memory is really fast? Apart from i/o, allocating memory is usually the slowest thing you can do on a computer, in my experience. reply lmm 12 hours agorootparentprev> The requirement is to run 1 million concurrent tasks. That's not a real requirement though. No business actually needs to run 1 million concurrent tasks with no concern for what's in them. reply OutOfHere 8 hours agorootparentIf you want to support 1 million concurrent active users, you can need it. reply lmm 4 hours agorootparentMaybe. But in that case you will need to do something for each of those users, and which languages are good at that might look quite different from this benchmark. reply gleenn 12 hours agoparentprevAlso, for Java, Virtual Threads are a very new feature (Java 21 IIRC or somewhere around there). OS threads have been around for decades. As a heavy JVM user it would have been nice to actually see those both broken out to compare as well! reply codetiger 12 hours agorootparentThe original benchmark had the comparison between Java thread and Java Virtual thread. https://pkolaczk.github.io/memory-consumption-of-async/ reply xargon7 10 hours agoprevThere's a difference between \"running a task that waits for 10 seconds\" and \"scheduling a wakeup in 10 seconds\". The code for several of the languages that are low-memory usage that do the second while the high memory usage results do the first. For example, on my machine the article's go code uses 2.5GB of memory but the following code uses only 124MB. That difference is in-line with the rust results. package main import ( \"os\" \"strconv\" \"sync\" \"time\" ) func main() { numRoutines, _ := strconv.Atoi(os.Args[1]) var wg sync.WaitGroup for i := 0; iSeq.map (fun _ -> task { let timer = PeriodicTimer(TimeSpan.FromSeconds 1.0) let mutable count = 10 while! timer.WaitForNextTickAsync() do countTask) |> Task.WaitAll To Go's credit, it remains at consistent 2.53 GB and consumes quite a bit less CPU. We're really spoiled with choice these days in compiled languages. It takes 1M coroutines to push the runtime and even at 100k the impact is easily tolerable, which is far more than regular applications would see. At 100K .NET consumes ~57 MB and Go consumes ~264 MB (and wins at CPU by up to 2x). reply neonsunset 10 hours agoparentprevSpawning a periodically waking up Task in .NET (say every 250ms) that performs work like sending out a network request would retain comparable memory usage (in terms of async overhead itself). Even at 100k tasks the bottleneck is going to be the network stack (sending outgoing 400k RPS takes a lot of CPU and syscall overhead, even with SocketAsyncEngine!). Doing so in Go would require either spawning Goroutines, or performing scheduling by hand or through some form of aggregation over channel readers. Something that Tasks make immediately available. The concurrency primitive overhead becomes more important if you want to quickly interleave multiple operations at once. In .NET you simply do not await them at callsite until you need their result later - this post showcases how low the overhead of doing so is. reply piterrro 11 hours agoprevI don't know what's a fair way to do this for all languages listed in the benchmark, but for Go vs Node the only fair way would be to use a single goroutine to schedule timers and another one to pick them up when they tick, this way we don't create a huge stack and it's much more comparable to what you're really doing in Node. Consider the following code: package main import ( \"fmt\" \"os\" \"strconv\" \"time\" ) func main() { numTimers, _ := strconv.Atoi(os.Args[1]) timerChan := make(chan struct{}) // Goroutine 1: Schedule timers go func() { for i := 0; ihigh number of concurrent tasks can consume a significant amount of memory note absolute numbers here: in the worst case, 1M tasks consumed 2.7 GB of RAM, with ~2700 bytes overhead per task. That'd still fit in the cheapest server with room to spare. My conclusion would be opposite: as long as per-task data is more than a few KB, the memory overhead of task scheduler is negligible. reply pkulak 12 hours agoparentExcept it’s more than that. Go and Java maintain a stack for every virtual thread. They are clever about it, but it’s very possible that doing anything more than a sleep would have blown up memory on those two systems. reply bilbo0s 11 hours agorootparentI have a sneaky suspicion if you do anything other than the sleep during these 1 million tasks, you'll blow up memory on all of these systems. That's kind of the Achille's Heel of the benchmark. Any business needing to spawn 1 million tasks, certainly wants to do something on them. It's the \"do something on them\" part that usually leads to difficulties for these things. Not really the \"spawn a million tasks\" part. reply vlovich123 3 hours agorootparentThe “do something” OP is referring to is simple things like a deeply nested set of function calls and on stack data structures allocated and freed before you sleep. This increases the size of the stack that Go needs to save. By comparison stackless coroutines only save enough information for the continuation, no more no less. That’s going to be strictly smaller than saving the entire stack. The argument you seem to be making is that that could be the same size as the stack (eg heap allocations) but I think that’s being unreasonably optimistic. It should always end up being strictly smaller. reply theamk 17 minutes agorootparentI have no doubt it's going to be strictly smaller, it's just the difference could be too small to care. Like say you are making a server, and each client has 16KB of state. Then memory usage would be 17KB in Node vs 19 KB in Go. Smaller? Yes. Smaller enough that you want to rewrite the whole app? Probably not. reply cperciva 14 hours agoprevThis depends a lot on how you define \"concurrent tasks\", but the article provides a definition: Let's launch N concurrent tasks, where each task waits for 10 seconds and then the program exists after all tasks finish. The number of tasks is controlled by the command line argument. Leaving aside semantics like \"since the tasks aren't specified as doing anything with side effects, the compiler can remove them as dead code\", all you really need here is a timer and a continuation for each \"task\" -- i.e 24 bytes on most platforms. Allowing for allocation overhead and a data structure to manage all the timers efficiently, you might use as much as double that; with some tricks (e.g. function pointer compression) you could get it down to half that. Eyeballing the graph, it looks like the winner is around 200MB for 1M concurrent tasks, so about 4x worse than a reasonably efficient but not heavily optimized implementation would be. I have no idea what Go is doing to get 2500 bytes per task. reply masklinn 13 hours agoparent> I have no idea what Go is doing to get 2500 bytes per task. TFA creates a goroutine (green thread) for each task (using a waitgroup to synchronise them). IIRC goroutines default to 2k stacks, so that’s about right. One could argue it’s not fair and it should be timers which would be much lighter. There’s no “efficient wait” for them but that’s essentially the same as the appendix rust program. reply jakewins 13 hours agorootparentFair or not, it’s a strange way to count - Go isn’t using that RAM. It’s preallocating it because any real world program will. reply masklinn 12 hours agorootparentGo is absolutely using that ram, it’s not available to other services on the system. reply xarope 10 hours agorootparentThe argument then, is what if we DO load 2K worth [0] of randomized data into each of those 1m goroutines (and equivalents in the other languages), and do some actual processing. Would we still see the equivalent 10x (whatever math works it out to be) memory \"bloat\"? And what about performance? We, as devs, have \"4\" such resources available to us, memory, network, I/O and compute. And it behooves us to not prematurely optimize on just one. [0] I can see more arguments/discussions now, \"2K is too low, it should be 2MB\" etc...! reply masklinn 10 hours agorootparentSo the argument is “if you measure something completely different from and unrelated to the article you do not get the same result”? I guess that’s true. And to be clear, I do agree with the top comment (which seems to be by you), TFA uses timers in the other runtimes and go does have timers so using goroutines is unwarranted and unfair. And I said as much a few comment up (although I’d forgotten about AfterFunc so I’d have looped and waited on timer.After which would still have been a pessimisation). And after thinking more about it the article is in also outright lying: technically it’s only measuring tasks in Go, timers are futures / awaitables but they’re not tasks: they’re not independently scheduled units of work, and are pretty much always special cased by runtimes. reply jakewins 12 hours agorootparentprevYou know what I mean. If this was a real world program where those million tasks actually performed work, then this stack space is available for the application to do that work. It’s not memory that’s consumed by the runtime, it’s memory the runtime expects the program to use - it’s just that this program does no useful work. reply aeturnum 11 hours agorootparentI am not u/masklinn - but I don't know what you mean. Doesn't the runtime consume memory by setting it aside for future use? Like what else does \"using\" ram mean other than claiming it for a time? reply jakewins 10 hours agorootparentIf the example was extended to, say, once the sleep is completed then parse and process some JSON data (simulating the sleep being a wait on some remote service), then how would memory use be affected? In the Go number reported, the majority of the memory is the stack Go allocated for the application code anticipating processing to happen. In the Node example, the processing instead will need heap allocation. Point being that the two numbers are different - one measures just the overhead of the runtime, the other adds the memory reserved for the app to do work. The result then looks wasteful for Go because the benchmark.. doesn’t do anything. In a real app though, preallocating stack can often be faster than doing just-in-time heap allocation. Not always of course! Just noting that the numbers are different things; one is runtime cost, one is runtime cost plus an optimization that assumes memory will be needed for processing after the sleep. reply hatefulmoron 10 hours agorootparentprevI think he means that if the Go code had done something more useful, it would use about the same amount of memory. Compare that to another implementation, which might allocate nearly no memory when the tasks don't do anything significant but would quickly catch up to Go if they did. reply flockonus 11 hours agorootparentprevI'll yield it would be interesting to have a similar benchmark but instead of sleeping - which indeed by itself is nonsense, to instead each task compute a small fib sequence, or write a small file; something like that. reply winternewt 12 hours agorootparentprevPlease elaborate. If each stack is 2KB then surely all of that virtual memory is committed to physical RAM, and hence is using actual memory? reply scrapheap 11 hours agorootparentYes and no. If that memory isn't being used and other things need the memory then the OS will very quickly dump it into swap, and as it's never being touched the OS will never need to bring it back in to physical memory. So while it's allocated it doesn't tie up the physical RAM. reply cperciva 12 hours agorootparentprevAha, 2k stacks. I figured that stacks would be page size (or more) so 2500 seemed both too small for the thread to have a stack and too large for it to not have a stack. 2k stacks are an interesting design choice though... presumably they're packed, in which case stack overflow is a serious concern. Most threading systems will do something like allocating a single page for the stack but reserving 31 guard pages in case it needs to grow. reply masklinn 12 hours agorootparentGoroutines being go structures, the runtime can cooperate with itself so it doesn't need to do any sort of probing: function prologues can check if there's enough stack space for its frame, and grow the stack if not. In reality it does use a guard area (technically I think it's more of a redzone? It doesn't cause access errors and functions with known small static frames can use it without checking). reply uluyol 12 hours agorootparentprevGo stacks are dynamically copied and resized. Stack overflow is not a concern. reply cperciva 12 hours agorootparentOh yuck. Invalidating all the pointers to the stack? That's got to be expensive. I guess if you're already doing garbage collection moving the stack doesn't make things all that much worse though... still, yuck. reply masklinn 11 hours agorootparentYeah it’s the drawback, originally it used segmented stacks but that has its own issues. And it’s probably not the worst issue because deep stacks and stack pointers will mostly be relevant for long running routines which will stabilise their stack use after a while (even if some are likely subject to threshold effects if they’re at the edge, I would not be surprised if some codebases ballasted stacks ahead of time). Also because stack pointers will get promoted to the heap if they escape so the number of stack pointers is not unlimited, and the pointer has to live downwards on the stack. reply YZF 12 hours agorootparentprevA goroutine stack can grow. (EDIT: With stack copying AFAICT... so no virtual pages reserved for a stack to grow... probably some reason for this design?) reply liveoneggs 14 hours agoparentprevhttps://tpaschalis.me/goroutines-size/ https://github.com/golang/go/blob/master/src/runtime/stack.g... reply Mawr 12 hours agoprev> Now Go loses by over 13 times to the winner. It also loses by over 2 times to Java, which contradicts the general perception of the JVM being a memory hog and Go being lightweight. Well, if it isn't the classic unwavering confidence that an artificial \"hello world\"-like benchmark is in any way representative of real world programs. reply phillipcarter 3 hours agoparentYes, but also, languages like Java and C# have caught up a great deal over the past 10 years and run incredibly smoothly. Most peoples' perception of them being slow is really just from legacy tech that they encountered a long time ago, or (oof) being exposed to some terrible piece of .NET Framework code that's still running on an underprovisioned IIS server. reply blixt 11 hours agoprevWhile it’s nice to compare languages with simple idiomatic code I think it’s unfair to developers to show them the performance of an entirely empty function body and graphs with bars that focus on only one variable. It paints a picture that you can safely pick language X because it had the smaller bar. I urge anyone making decisions from looking at these graphs to run this benchmark themselves and add two things: - Add at least the most minimal real world task inside of these function bodies to get a better feel for how the languages use memory - Measure the duration in addition to the memory to get a feel for the difference in scheduling between the languages reply tossandthrow 11 hours agoparentThis urge is as old as statistics. And I dare to say that most people after reading the article in question are well prepared to use the results for what they are. reply blixt 11 hours agorootparentI can’t say I share your optimism. I’ve seen plenty of developers point to graphs like these as a reason for why they picked a language or framework for a problem. And it comes down to the benchmark how good of a proxy it actually is for such problems. I just hope that with enough feedback the author would consider making the benchmark more nuanced to paint a picture of why these differences in languages exist (as opposed to saying which languages “lose” or “win”). reply sfn42 8 hours agorootparentprevAnd by use the results for what they are you mean ignore them because they are completely useless? reply JyB 10 hours agoprevI’m still baffled that some people are bold enough to voluntarily posts those kind of most-of-the-time useless “benchmark” that will inevitably be riddled with errors. I don’t know what pushes them. In the end you look like a clown more often than not. reply wiseowise 9 hours agoparentThe fastest way to learn truth is by posting wrong thing on the internet, or something. reply enginoid 9 hours agoparentprevTrying things casually out of curiosity isn’t harmful. I expect people understand that these kinds of blog posts aren’t rigorous science to draw foundational conclusions from. And the errors are a feature — I learn the most from the errata! reply aba_cz 12 hours agoprevRegarding Java I'm pretty sure that benchmark is broken at least a little bit and testing something else as not specifying initial size for ArrayList means list of size 10 which gets resized all the time when `add()` is called, leading to big amount of unused objects needing garbage collection. reply brabel 11 hours agoparentYeah that is a junior mistake... They should've pre-sized the ArrayList, or better, used an array because that's more memory efficient (and I would say would be what any decent dev would do when the size of tasks is known beforehand). > Some folks pointed out that in Rust (tokio) it can use a loop iterating over the Vec instead of join_all to avoid the resize to the list Right, but some folks also pointed out you should've used an array in Java in the previous blog post, 2 years ago, and you didn't do that. And folks also pointed out Elixir shouldn't have used Task in the previous benchmark (folk here being the creator of Elixir himself): https://github.com/pkolaczk/async-runtimes-benchmarks/pull/7 reply sfn42 11 hours agorootparentThe difference between an arraylist with correct initial size and an array is almost nothing. Arraylist itself is just a wrapper around an array. reply brabel 7 hours agorootparentIt can be a big difference if boxing is involved. Or if the list is very big, because all access to items in the list require casting at the bytecode level (due to type erasure). reply bekantan 11 hours agoparentprevIt would indeed be better to create appropriately sized storage. However, I don't think that underlying array is resized every time `add` is called. I'd expect that resize will happen less than 30 times for 1M adds (capacity grows geometrically with a=10 and r=1.5) reply jeswin 11 hours agoprevGood to see NativeAOT getting positive press. Go won because it served a need felt by many programmers: a garbage-collected language which compiled to native code, with robust libraries supported by a large corp. With Native AOT, C# is walking into the same space. With arguably better library selection, equivalent performance, and native code compilation. And a much more powerful, well-thought-out language - at a slight complexity cost. If you're starting a project today (with the luxury of choosing a language), you should give C# + NativeAOT a consideration. reply sfn42 8 hours agoparentC# is my daily driver and I'd use it for almost anything, great language. However I think \"slight complexity cost\" is an understatement. It's a very complex language by my standards, and they keep adding more stuff. A lot of it is just syntax sugar to do the same things in a different way, like primary constructors. It's nice to have that stuff when you know the language, but it does make the learning curve steeper and it can be a bit annoying when working in a team. Even after 4 years of using it professionally I still see code some times that uses obscure syntax I had no idea existed. I would describe C# as a language for experts. If you know what you're doing it's an amazing language, maybe actually the best current programming language. But learning and understanding everything is a monumental task, simpler languages like go or Java can be learned much faster. reply polyrand 4 hours agoprevOut of curiosity, I checked if using uvloop[0] in Python changed the numbers. This is the code: # /// script # requires-python = \">=3.12\" # dependencies = [\"uvloop\"] # /// import asyncio import sys import uvloop async def main(num_tasks): tasks = [] for task_id in range(num_tasks): tasks.append(asyncio.sleep(10)) await asyncio.gather(*tasks) if __name__ == \"__main__\": num_tasks = int(sys.argv[1]) # uvloop.run(main(num_tasks)) asyncio.run(main(num_tasks)) I ran it with 100k tasks: /usr/bin/time -l -p -h uv run async-memory.py 100000 On my M1 MacBook Pro, using asyncio reports (~170MB): 170835968 maximum resident set size Using uvloop (~204MB): 204259328 maximum resident set size I kept the `import uvloop` statement when just using asyncio so that both cases start in the same conditions. [0]: https://github.com/MagicStack/uvloop/ reply jillesvangurp 11 hours agoprevDid a similar benchmark in Kotlin using co-routines. import kotlin.time.Duration.Companion.milliseconds import kotlin.time.measureTime import kotlinx.coroutines.async import kotlinx.coroutines.awaitAll import kotlinx.coroutines.coroutineScope import kotlinx.coroutines.delay suspend fun main() { measureTime { coroutineScope { (0..1000000).map { async { delay(1.milliseconds) } }.awaitAll() } }.let { t -> println(\"Took $t\") val runtime = Runtime.getRuntime() val maxHeapSize = runtime.maxMemory() val allocatedHeapSize = runtime.totalMemory() val freeHeapSize = runtime.freeMemory() println(\"Max Heap: ${maxHeapSize / 1024 / 1024} MB\") println(\"Allocated Heap: ${allocatedHeapSize / 1024 / 1024} MB\") println(\"Free Heap: ${freeHeapSize / 1024 / 1024} MB\") } } This produces the following output: Took 1.597011084s Max Heap: 4096 MB Allocated Heap: 2238 MB Free Heap: 1548 MB So whatever is needed to load classes and a million co-routines with some heap state. Of course the whole thing isn't doing any work and this isn't much of a benchmark. And of course if I run it with kotlin-js it actually ends up using promises. So, it's not going to be any better there than on the JVM. reply octacat 8 hours agoprevWhere is erlang? Sleeping is not running, by the way. If you just sleep, in Erlang you would use a hibernated process. I feel this is so misleading. For example, by default after spawning, Erlang would have some memory preallocated for each process, so they don't need to ask the operation system for new allocations (and if you want to shrink it, you call hibernate). Do something more real, like message passing with one million processes or websockets. Or 1M tcp connections. Because, the moment you send messages, here is when the magic happens (and memory would grow, the delay when each message is processed would be different in different languages). Oh, and btw, if you want to do THAT in erlang, use timer:apply_after(Time, Module, Function, Arguments). Which would not spawn an erlang process, just would put the task to the timer scheduling table. And Elixir was in the old article, and they implemented it all wrong. Sad. reply davidatbu 14 hours agoprevI write (async) Rust regularly, and I don't understand how the version in the appendix doesn't take 10x1,000,000 seconds to complete. In other words, I'd have expected no concurrency to take place. Am I wrong? UPDATE: From the replies below, it looks like I was right about \"no concurrency takes place\", but I was wrong about how long it takes, because `tokio::time::sleep()` keeps track of when the future was created, (ie when `sleep()` was called) instead of when the future is first `.await`ed (which was my unsaid assumption). reply claytonwramsey 14 hours agoparentThe implementation of `sleep` [1] decides the wake up time by when `sleep` is called, rather than when its future is polled. So the first task waits one second, then the remaining tasks see that they have already passed the wake-up time and so return instantly. [1]: https://docs.rs/tokio/latest/tokio/time/fn.sleep.html reply davidatbu 14 hours agorootparentThis makes total sense! reply vbsd 12 hours agoparentprev> because `tokio::time::sleep()` keeps track of when the future was created, (ie when `sleep()` was called) instead of when the future is first `.await`ed I’m not a Rust programmer but I strongly suspect this updated explanation is erroneous. It’s probably more like this: start time is recorded when the task execution is started. However, the task immediately yields control back to the async loop. Then the async loop starts another task, and so on. It’s just that the async loop only returns the control to sleeping task no earlier than the moment 1s passes after the task execution was initialy started. I’d be surprised if it had anything to do with when sleep() was called. reply davidatbu 10 hours agorootparentSomeone linked the code in another comment, and the start time is most definitely recorded when the future is created: https://docs.rs/tokio/1.41.1/src/tokio/time/sleep.rs.html#12... reply vbsd 4 hours agorootparentHuh, you're right about this, thanks. On the other hand, I maintain that this is an incidental rather than essential reason for the program finishing quickly. In that benchmark code, we can replace \"sleep\" with our custom sleep function which does not record start time before execution: async fn wrapped_sleep(d: Duration) { sleep(d).await } The following program will still finish in ~10 seconds. #[tokio::main] async fn main() { let num_tasks = 100; let mut tasks = Vec::new(); for _ in 0..num_tasks { tasks.push(wrapped_sleep(Duration::from_secs(10))); } futures::future::join_all(tasks).await; } reply fuzzybear3965 14 hours agoparentprevYeah, I think you're wrong. It should only take ~10s. tokio::time::sleep records the time it was called before returning the future [1]. So, all 1 million tasks should be stamped with +/- the same time (within a few milliseconds). [1]: https://docs.rs/tokio/1.41.1/src/tokio/time/sleep.rs.html#12... reply davidatbu 14 hours agorootparentThis makes total sense! reply ch33zer 14 hours agoparentprevTokyo::sleep is async reply davidatbu 14 hours agorootparentI think the points people made in other replies make sense, but \"Tokio::sleep is async\" by itself is not enough of an explanation. If it were the case that `Tokio::sleep()` tracked the moment `.await` was called as it's start time, I believe it would indeed take 10x1,000,000 seconds, _even if it's async_. reply promiseofbeans 13 hours agoprevIt would be nice if the author also compared different runtimes (e.g. NodeJS vs Deno, or cpython vs pypy) and core language engines (e.g. v8 vs spider monkey vs JavaScript core) reply afavour 12 hours agoprevMaybe I’m missing something here but surely Node isn’t doing anything concurrently? Promises don’t execute concurrently, they just tidy up async execution. The code as given will just sequentially resolve a million promises. No wonder it looks so good. You’d need to be using workers to actually do anything concurrently. reply charlotte-fyi 12 hours agoparentThat's not entirely true. There's a thread pool of workers underneath libuv. Tasks that would block do indeed execute concurrently. reply afavour 12 hours agorootparentOh, I know. But the code used in this test doesn’t utilize that thread pool at all. It just uses setTimeout. reply Izkata 12 hours agoparentprevYou're thinking of parallelism. Concurrency doesn't require them to actually be running at the same time. reply afavour 12 hours agorootparentFair point. Kind of makes the comparisons between languages a little unfair, though. Go and Rust would be executing these operations in parallel, Node would not. Would make a significant difference to real world performance! reply Izkata 12 hours agorootparentThe measurement is memory, not performance. Paused/queued tasks in the sequential node version still count, and in theory could be worse since the Go and Rust ones would be consuming them in parallel and not building up the queue as much. reply lenkite 9 hours agorootparent> The measurement is memory, not performance. But, then they can measure memory by simply using a threat pool of size 1 and then submitting tasks to it right ? That would be the equivalent comparison for other languages. They should launch a million NodeJS processes. reply joshka 9 hours agoprevRUST The rust code is really checking how big Tokio's structures that track timers are. Solving the problem in a fully degenerate manner, the following code runs correct correctly and uses only 35MB peak. 35 bytes per future seems pretty small. 1 billion futures was ~14GB and ran fine. #[tokio::main] async fn main() { let sleep = SleepUntil { end: Instant::now() + Duration::from_secs(10), }; let timers: Vec = iter::repeat_n(sleep, 1_000_000_0).collect(); for sleep in timers { sleep.await; } } #[derive(Clone)] struct SleepUntil { end: Instant, } impl Future for SleepUntil { type Output = (); fn poll(self: Pin, cx: &mut Context) -> Poll { if Instant::now() >= self.end { Poll::Ready(()) } else { cx.waker().wake_by_ref(); Poll::Pending } } } Note: I do understand why this isn't good code, and why it solves a subtly different problem than posed (the sleep is cloned, including the deadline, so every timer is the same). The point I'm making here is that synthetic benchmarks often measure something which doesn't help much. While the above is really degenerate, it shares the same problems as the article's code (it just leans into problems much harder). reply iforgotpassword 11 hours agoprevCan someone explain the node version to me? My js knowledge is from a decade ago. AFAIK, setTimeout creates a timer and returns a handle to it. What does promisify do? I'd assume it's a general wrapper that takes a function that returns X and wraps it so that it returns Promise. So that code actually runs 10k tasks that each create a timer with a timeout of 10 seconds and return immediately. reply throwitaway1123 1 hour agoparentPromisify converts a callback based function into a promise returning function [1]. Functions are objects in JS, and if the function object has a `promisify.custom` method, `promisify` will simply return the `promisify.custom` method instead of wrapping the original function. Calling `promisify` on `setTimeout` in Node is redundant because Node already ships a built in promisified version of `setTimeout`. So the following is true: setTimeout[promisify.custom] === require('node:timers/promises').setTimeout You could of course manually wrap `setTimeout` yourself as well: const sleep = n => new Promise(resolve => setTimeout(resolve, n)) [1] https://nodejs.org/docs/latest-v22.x/api/util.html#utilpromi... reply vrnvu 8 hours agoprevConclusion by author: > Now Go loses by over 13 times to the winner. It also loses by over 2 times to Java, which contradicts the general perception of the JVM being a memory hog and Go being lightweight. Note that Go and Java code are not doing the same! See xargon7 comment. reply pgAdmin4 14 hours agoprevWhy C with pthreads missing in this benchmark ? reply throwaway81523 14 hours agoparentI don't think 1M posix threads is a thing. 1K is no big deal though. reply liontwist 13 hours agorootparent~100k is a thing on Linux. reply fulafel 13 hours agorootparentOn Linux, 400k threads was a thing on even on 2002 hardware that was 32-bit and thus limited to 4GB: https://news.ycombinator.com/item?id=37621887 In the titular post there's a link to a previous comparison between approaches, and plain OS threads used from Rust fare quite well, even if the author doesn't up the OS limits to keep that in the running for the higher thread cases: https://pkolaczk.github.io/memory-consumption-of-async/ reply thesnide 10 hours agoparentprevthat. Or just using a C coroutine lib. reply samsartor 12 hours agoprevReminder that Rust does not automatically schedule anything. Unless you _explicitly_ call `tokio::spawn` or `async_std::spawn` you are still living entirely in state-machine land. Rust's `join_all` uses `FuturesUnordered` behind the scenes, which is pretty intelligent in terms of keeping track of which tasks are ready to make progress, but it does not use tokio/async_std for scheduling. AFAICT the only thing being measured about tokio/async_std is the heap size of their `sleep` implementations. I'd be very interesting in seeing how Tokio's actual scheduler performs. The two ways to do that are: - using https://docs.rs/tokio/latest/tokio/task/join_set/struct.Join... to spawn all the futures and then await them - spawn each future in the global scheduler, and then await the JoinHandles using the for loop from the appendix As other commenters have noted, calling `sleep` only constructs a state machine. So the Appendix isn't actually concurrent. Again, you need to either put those state machines into the tokio/async_std schedulers with `spawn`, or combine the state machines with `FuturesUnordered`. reply jhgg 12 hours agoparentI did this measurement, and using time -v, the maximum resident size in KB comes out to 440,424 kb for 1m tasks, 46,820 kb for 100k, and 7,156 kb for 10k. reply martypitt 11 hours agoprevSeriously impressive results from C#. I'm a JVM guy by day, and long-time admirer of C# as a language, but always assumed the two were broadly comparable performance-wise. This is a sample of 1 usecase, (so questionable real-worldness) but the difference is really eye-opening. Congrats to the C# team! reply ReptileMan 11 hours agoparentJust a minor nitpick - this is the .NET Runtime vs JVM. My personal observations are that CPU wise they are close, but for reasons unknown JVM has always been more memory hoggish. The JIT compiler that microsoft created has been nothing short of amazing. reply renox 6 hours agoprev.Net AoT looks good but when you look at the compatibility matrix Android and iOS are \"experimental\".. https://learn.microsoft.com/en-us/dotnet/core/deploying/nati... reply jakobnissen 10 hours agoprevJust tried this in Julia: 16.0 GB of memory for 1M tasks! I believe each task in Julia has its own stack, so this makes sense. Still, it does mean you've got to take account of ~16 KB of memory per running task which is not great. reply citrin_ru 11 hours agoprevIt would be more interesting to see a benchmark where a task will not be empty but would have an open network connection e.g. would make an HTTP request to a test server with 10 seconds response time. Network is a frequent reason real world applications spawn 1M tasks. reply abdellah123 12 hours agoprevCan we do something more real world at least? what's the cost (hetzner monthly) of maintaining 1M concurrent websocket connection where each make a query to a postgres db randomly every 1-4 seconds. The cost wouldn't be just Memory because the network card and CPU also enter the game. reply rcarmo 11 hours agoprevNo Erlang, though. That ought to be amazingly small for that kind of synthetic benchmark. reply nolist_policy 10 hours agoprevReminds me of the famous SCO: \"thread creation is about a thousand times faster than on native Linux\" linux kernel mailing list thread where Linus Torvalds replies Talk is cheap. Show me the code. https://lkml.org/lkml/2000/8/25/132 https://lkml.org/lkml/2000/8/26/52 reply win32_func 11 hours agoprevThe JS version can quickly be improved to use less memory (~10%). async function main() { const numTasks = parseInt(process.argv[2], 10); const taskDuration = 10000; // 10 seconds const tasks = Array.from({ length: numTasks }, () => new Promise(resolve => setTimeout(resolve, taskDuration)) ); await Promise.all(tasks); // Wait for all tasks to resolve console.log(\"All tasks completed.\"); } main().catch(err => { console.error(\"Error occurred:\", err); }); reply m4r1k 11 hours agoprevFor those interested in Java Native Image, I previously wrote about its advantages beyond just memory footprint reduction, such as the dramatic improvement in startup time -> https://medium.com/google-cloud/cut-container-startup-time-f... reply bilbo-b-baggins 13 hours agoprevThis benchmark is nonsense. Apart from the fact that Go has an average Goroutine overhead of a 4kB stack (meaning an average usage of 3.9GB for 1M tasks), the code written is also in a closure, and scheduling a 2nd Goroutine in the wg.Done(), so unlike some of the others it had at least 2M function calls on the event loop stack in addition to at least 1M closure references. So yeah, it’s a great example of bad code in any language. reply neonsunset 13 hours agoparentHere's an implementation in C# that more faithfully matches what you have to do in Go: var cnt = int.Parse(args[0]); var evt = new CountdownEvent(cnt); for (var i = 0; iand avoids the allocation: https://github.com/dotnet/runtime/blob/1f01bee2a41e0df97089f... reply neonsunset 13 hours agoparentprevIt doesn't take that much space, and not all languages have option to easily map an initial range onto an iterator that produces tasks. Most are dominated by the size of state machines/virtual threads. Please note that the link above leads to old code from .NET Framework. The up-to-date .NET implementation lives here: https://github.com/dotnet/runtime/blob/main/src/libraries/Sy... reply lxe 14 hours agoprevNodeJS does what it was designed to do well. reply ankit70 14 hours agoparentI wonder how compiled (using Deno or others) JS would perform. reply anonymousDan 9 hours agoprevWould be interesting to see the numbers for elixir/Erlang. reply reverseblade2 7 hours agoprevIt is odd that C# version doesn't increase list capacity: List tasks = new List(numTasks); reply datadeft 10 hours agoprevNo Erlang/Elixir?? The author would be surprised. reply skyake 3 hours agoparentErlang/Elixir failed the 1M tasks benchmark that's why it was excluded. reply SirGiggles 2 hours agorootparentMost likely they did not adjust +P which bumps the max process limit reply signa11 9 hours agoprevfta, please consider reading this: https://hauleth.dev/post/beam-process-memory-usage/ as well to see how erlang (elixir) fares as well. reply pjmlp 11 hours agoprevWhile the idea in general is interesting, the whole site is unreadable in Firefox, text gets all over the place the graphics. reply mrweasel 11 hours agoparentSeems fine to me. reply pjmlp 11 hours agorootparentStrange, killing Firefox and going again did it for me. I guess some FF bug then. Thanks. reply voodooEntity 10 hours agoprevI came here to rage (im just honest) because the go code example is bad and absolute not representative. Im coding go code for multiple years, especially alot of multithreading, and what is presented there as result is just wrong. Apart from no necessaty to use a waitgroup for threading, as many others here already have stated even with waitgroup you cacn reduce the memory significantly down to like 130mb for 1mio threads. Also some other languages seem to be missrepresented. Seems like someone had good intentions but no idea about the languages he tried to compare and the result is this article. reply wiseowise 8 hours agoprevRemember, folks, none of it matters if nobody uses your software. reply tzahifadida 12 hours agoprevThis test is no real. These languages works differently. Garbage collected and manually allocating and deallocating. If you do not configure a garbage collected language correctly it will spin out of control in memory consumption because it will just not garbage collect. If you would have configured the garbage collection to be low for java and go then go would look like rust. reply fortran77 11 hours agoprevHe should have looked at Erlang. reply neonsunset 11 hours agoparentErlang is likely going to have about the same or greater starting overhead as Go here from what I measured[0] with Elixir. Each Erlang process carries its own independent GC which allows it to isolate allocation impact, contributing to the robustness of its implementation. I assume this is where the cost comes from. If you do measure Erlang - please post the numbers. Processes in Erlang, Goroutines in Go and Virtual Threads in Java do not fully replace lightweight asynchronous state machines - many small highly granular concurrent operations is their strength. [0]: https://gist.github.com/neon-sunset/8fcc31d6853ebcde3b45dc7a... (disclaimer: as pointed out in a sibling comment it uses Elixir's Task abstraction which adds overhead on top of the processes) reply bn-l 11 hours agoprev123 MB of ram to run 1 million tasks in rust. That is mind blowing. reply jeffreygoesto 10 hours agoprevHow would Erlang/Elixir do in this? reply ww520 10 hours agoprevHow do they measure the memory usage? reply davidelettieri 11 hours agoprevDo we know how memory consumption? reply liontwist 14 hours agoprevNo baseline against UNIX processes? reply masklinn 13 hours agoparentA million processes? reply liontwist 13 hours agorootparentI've seen 100k. What happens at a million? How many is unworkable? reply Izkata 11 hours agorootparent$ cat /proc/sys/kernel/pid_max 4194304 My computer can handle that many processes, after that no new processes can be spawned (see: forkbomb) reply blitzar 11 hours agorootparentI am left wondering what happens at 4194305 reply masklinn 10 hours agorootparentYou can fork bomb your system and observe. reply ankit70 14 hours agoprevNodeJS is better at memory than go? reply hu3 11 hours agoparentThey are doing different things in this benchmark. NodeJS has one thread with a very tight loop. Go actually spawned 1M green threads. Honestly this benchmark is just noise. Not to say useless in most real world scenarios. Specially because each operation is doing nothing. It would be somewhat useful if they were doing some operation like a DB or HTTP call. reply erik_seaberg 14 hours agoparentprevI'd expect that because Promises are small Javascript objects while goroutines each get a stack that grows from at least 2 KB. reply dboreham 14 hours agorootparentOtoh Go actually supports concurrency. reply jpgvm 14 hours agorootparentWell they are all concurrent. I think what you mean is Go is also parallel. As is C#, Rust and Java in this bench. reply amazingamazing 14 hours agoprevYet again nodejs surpasses my pre-read expectations 3rd best (generalized) for a million? Wow. I must be missing something - isn’t Go supposed to be memory efficient? Perhaps promises and goroutines aren’t comparable? reply brabel 11 hours agoparentIf you want to run 1 million coroutines that just sleep in your app, yeah nodejs looks very efficient. The problem is that when each coroutine needs to allocate memory, which I would suppose anything real would do, the 2Kb Go pre-allocates will be an advantage - as it will probably be required except for the most trivial workloads (like in this benchmark) - and then because Go actually runs them in parallel, unlike nodejs, you would likely see a huge improvement in both performance and memory usage with Go or Rust. reply fuzzybear3965 14 hours agoparentprevI'm not sure what \"memory efficient\" means. But, Go sprung as a competitor to Java (portability, language stability, corporate language support/development) and C++ (faster compile times). Can't beat C++ in terms of memory management (performance, guys, not safety) much. But, you can fare well against the JVM, I'm guessing. reply jpgvm 14 hours agorootparentIn this benchmark actually no, Go doesn't fare well. There is actually higher static overhead per goroutine than JVM VirtualThread. I presume this is because of a larger initial stack size though/ This probably doesn't matter in the real world as you will actually use the tasks to do some real work which should really dwarf the static overhead is almost all cases. reply rwaksmunski 14 hours agoprevBe sure to read the Appendix, Rust's state machine async implementation is indeed very efficient. reply Comma2976 10 hours agoprevI'd be ashamed to publish such shoddy \"work\" using any real account or name reply lowyek 13 hours agoprevdepends on the tasks. reply bob_alderman 10 hours agoprevgood thing you didn't benchmark ruby because we probably couldn't see the other chart bars. reply indulona 10 hours agoprevGo's performance has been neglected in past 10y. I even posted about it https://news.ycombinator.com/item?id=41827288 reply neonsunset 14 hours agoprev [–] To add a data point for Elixir: https://gist.github.com/neon-sunset/8fcc31d6853ebcde3b45dc7a... Note 1: The gist is in Ukrainian, and the blog post by Steve does a much better job, but hopefully you will find this useful. Feel free to replicate the results and post them. Note 2: The absolute numbers do not necessarily imply good/bad. Both Go and BEAM focus on userspace scheduling and its fairness. Stackful coroutines have their own advantages. I think where the blog post's data is most relevant is understanding the advantages of stackless coroutines when it comes to \"highly granular\" concurrency - dispatching concurrent requests, fanning out to process many small items at once, etc. In any case, I did not expect sibling comments to go onto praising Node.js, is it really that surprising for event loop based concurrency? :) Also, if you are an Elixir aficionado and were impressed by C#'s numbers - know that they translate ~1:1 to F# now that it has task CE, just sayin'. Here's how the program looks in F#: open System open System.Threading.Tasks let argv = Environment.GetCommandLineArgs() [1..int argv[1]] |> Seq.map (fun _ -> Task.Delay(TimeSpan.FromSeconds 10.0)) |> Task.WaitAll reply eproxus 11 hours agoparent [–] Note that the Task library in Elixir uses supervised processes so it adds a lot more overhead. It would be interesting to see the benchmark with just normal Erlang processes. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A 2024 benchmark tested memory consumption for running 1 million concurrent tasks using coroutines across various programming languages, including Rust, C#, Go, and Java.- C# with NativeAOT demonstrated the best memory efficiency, outperforming other languages, while Go consumed more memory than anticipated, even more than Java with GraalVM.- The study underscores that some languages, like .NET with NativeAOT and Java's GraalVM, have improved scalability, whereas Go's goroutines were less efficient, and Rust's memory usage was optimized by adjusting task handling."
    ],
    "commentSummary": [
      "In 2024, the memory required to run 1 million concurrent tasks varies by programming language and concurrency model, with Rust and C# using around 0.12 KiB per task, while Go uses about 2.64 KiB per task.",
      "Node.js, despite appearing efficient with promises, does not execute tasks in parallel, highlighting differences in concurrency handling among languages.",
      "The benchmark underscores that real-world applications involve more complex operations than simple tasks, impacting memory usage and performance."
    ],
    "points": 245,
    "commentCount": 177,
    "retryCount": 0,
    "time": 1732848659
  },
  {
    "id": 42268819,
    "title": "Goodbye, Rust. I wish you success but I'm back to C++ (sorry, it is a rant)",
    "originLink": "https://old.reddit.com/r/rust/comments/1h15md8/goodbye_rust_i_wish_you_success_but_im_back_to_c/",
    "originBody": "whoa there, pardner! Your request has been blocked due to a network policy. Try logging in or creating an account here to get back to browsing. If you're running a script or application, please register or sign in with your developer credentials here. Additionally make sure your User-Agent is not empty and is something unique and descriptive and try again. if you're supplying an alternate User-Agent string, try changing back to default as that can sometimes result in a block. You can read Reddit's Terms of Service here. if you think that we've incorrectly blocked you or you would like to discuss easier ways to get the data you want, please file a ticket here. when contacting us, please include your ip address which is: 172.190.111.197 and reddit account",
    "commentLink": "https://news.ycombinator.com/item?id=42268819",
    "commentBody": "Goodbye, Rust. I wish you success but I'm back to C++ (sorry, it is a rant) (reddit.com)242 points by veidelis 20 hours agohidepastfavorite284 comments ksec 18 hours ago>Rust is a superior technology to C++....it disciplines you to do the things the right way. >But Rust is better in the same way that Betamax was better than VHS, Mastodon is better than Twitter, Dvorak keyboards are better than QWERTY, Esperanto is better than English and Lua is better than Javascript: in communication technologies, adoption and critical mass are 90% of the game and the other 10% are bikeshedding, at best. And programming languages are a communication technology, they're means to communicate ideas to other programmers. Love this. May be I should repost this every time there is an argument about Rust. And to expand on that, in communication technology, the one easier to use is going to get wider spread / adoption. And while there were never much hype around Go, I think it is a very good example. But a lot of Rust [1] ideas are now being polished and used elsewhere like Swift. I am sure we will continue to be experimented in other languages and real world to test whether the trade off is worthwhile. [1] ( ok may be Cyclone or something earlier in case someone wants to nitpick ) reply carlmr 10 hours agoparent>the one easier to use is going to get wider spread / adoption Between Rust and C++, this is Rust. In C++ you need to a lot more memory safety verification in your head, there are thousands of footguns that you need to be aware of at all times, the template metaprogramming facilities are really hard to grok with their weird syntax. Rust is strict, but for the kinds of applications that you would replace C++ in this makes it a lot easier to learn. I think the sentiment that Rust is hard comes from people that come from managed languages (Python/Java/C#) that aren't used to manual memory management. From that perspective Rust is indeed hard. reply grayhatter 3 hours agorootparentI'm used to writing C, and think pointers are easy to understand. I wouldn't call rust 'hard' to learn, but it is without a doubt painful. You also aren't doing manual memory management in rust either... I'd assume the sentiment comes from the irrelevant hoops rust makes you jump through to just compile something simple. You don't need to be aware of 1000s of foot guns for every block of non rust code. But you do have to remember dozens of pedantic rules, or stdlib wrappers just to trick rust into allowing you to compile your simple/demo code. reply ipdashc 1 hour agorootparent> You don't need to be aware of 1000s of foot guns for every block of non rust code. But you do have to remember dozens of pedantic rules Have you tried using rust-analyzer? I'm usually a bit of an IDE skeptic, but Rust is one of those languages where tight integration with the IDE really helps IMO. It's night and day, I honestly wouldn't want to write Rust without rust-analyzer, but with it it's quite pleasant. reply UncleMeat 2 hours agorootparentprevYep. I've been developing in C++ professionally for a decade and I still have to look up \"what's the safe way of making a string literal available in a header file\" when it comes up. reply chucke1992 4 hours agorootparentprevC++ has much more bigger inertia. It is like the \"default\" for all the areas where it is being used. So when learning a language people would rather go to C++ first and then to Rust, if they are really planning to work in corresponding area (game dev etc.). Or even go to C. Rust will probably replace C++ in some areas where it is a legacy app (or part of the app) that is not changed frequently anymore. reply BWStearns 16 hours agoparentprevI largely agree with the \"programming languages are a communication technology [thus critical mass is critical]\" bit, we spend more time reading and explaining code than we do writing it. That's why I'm really hopeful for Rust web frameworks like Loco to get to mainstream-ish. There are just _so goddamn many_ webdevs that a reasonably popular web framework can bootstrap a language out of the mid-popularity uncanny valley. On the flipside, a lot of webdevs are interested in Rust, but aren't really personally set up to do some embedded or systems stuff, so the activation energy for a given developer getting into it remains slightly higher than other languages. reply synergy20 16 hours agoparentprevGolang is not much more widely used comparing to rust? both are 17+ years old and they're around 2% of the market. reply serbuvlad 16 hours agorootparentI think a question that must be asked is \"what market?\" Go is a very niche language in it's application: it is ubiquitous in the Docker/Kubernetes DevOps world. It's also successful as an alternative to Node in the server/micro services world. And there are many CLI tools written in it (but CLI tools can be written in any language whether or not it has critical mass - there are popular ones even in Haskell!). Excluding CLI, Go's use cases virtually did not exist when Go was created. So Go was \"the right language at the right time\" to grow up with it's niches (decent performance, easy to learn, easy to reuse library code, easy to deploy, easy concurrency). It is virtually non-existent anywhere else. On the other hand, Rust competes in exactly the same high-performance/low-overhead space that C++ dominates. Because of Rust's complexity, it is very unlikely that it can compete anywhere else. Rust must displace C++, or at least be a viable alternative to it. reply kstrauser 11 hours agorootparentI wrote a BBS in Rust. It need sleep neither high performance nor low overhead, but it was a delight to write. For example, the compiler error messages are so detailed and explanatory that I learned a lot of the language from it telling me what I was doing wrong and offering suggestions. I would’ve written it in Python a year ago. Now I wrote it in Rust because it was genuinely fun and the results came out nicely. I have zero interest, none, nada, in C++. (Readers: please don’t try to win me over. I know more about the subject than you might think and it’s an informed opinion.) For me, Rust doesn’t compete with C++ at all. It’s probably replacing my Python coding more than anything else. The output looks and feels quite Pythonic, but with strict types and AOT compilation to native code. I’m loving it. reply jjav 10 hours agorootparent> I wrote a BBS in Rust. A BBS? Does this have a different meaning now than in 1986? Where do I dial up to it? reply kstrauser 3 hours agorootparentSame meaning. You get a Meshtastic radio and send it commands through a chat. See https://honeypot.net/2024/11/28/first-light-of.html for screenshots. reply binary132 6 hours agorootparentprevWhat do you mean when you say “Go’s usecases did not exist”? It was invented to write concurrent servers in that would have readable, performant implementations suitable for use across teams, to make code review faster and better, to displace C++, and to prove that certain design principles from Dis / Plan9 were superior to those in Java. I’m not really clear how those usecases did not exist at the time. If you mean “Go is for scheduling containers”, you are wrong, but just because one popular application exists that doesn’t imply that is “what it’s for”. reply glonq 2 hours agorootparentI inherited a project that uses Go for an industrial control system. It needs to manage valves and pumps and sensors in a performant and accurate and reliable manner, and that's what the developer (an electrical engineer) chose. FWIW the system has been in production for a couple years and works splendidly. The me of yesteryear probably would have developed such a thing in C++. In an alternate universe, there's a version of me that learned Rust 5-10 years ago and would have chosen that. reply zozbot234 1 hour agorootparent> an industrial control system. It needs to manage valves and pumps and sensors in a performant and accurate and reliable manner How does it deal with GC pauses? Wouldn't they introduce unacceptable jitter for this kind of application? Anyway, it's relatively straightforward to rewrite from Go to Rust if you aren't doing things that essentially rely on GC (which is quite rare). reply glonq 20 minutes agorootparentA PLC controller deals with anything that requires tight timing, so the Go portion is mostly higher-level control of the system and states. reply alfiedotwtf 14 hours agorootparentprev> Because of Rust’s complexity I would argue there is 2 types of complexities here: 1. Complexities when developing (Rust) 2. Complexities once running in production (C++) I’d much much prefer to have errors and frustrations developing than users dropping out of the sky at 40,000ft reply dartos 12 hours agorootparentThe issue is that for all rusts complexities, it can still have production issues. reply bennettnate5 4 hours agorootparent... and for all of C++'s perceived simplicity compared to Rust, it can still have compilation issues (looking at the compiler errors output by template classes can be a downright nightmare) reply zozbot234 5 hours agorootparentprevRust is not \"17+ years old\" in any relevant sense. Rust 1.0 only came out in 2015, and the first genuinely usable version arguably was only in late 2018 after non-lexical lifetimes (NLL) were added. Golang is a lot older than Rust, practically speaking. reply Koshkin 3 hours agoparentprev> they're means to communicate ideas to other programmers This is somewhat of an anthropocentric point of view. (In particular, assembler is as far as it gets from this.) reply michaelmrose 15 hours agoparentprevBeta wasn't better. VHS matched broadcast quality for less money and substantially more runtime per tape. Dvorak isn't particularly better because typing speed isn't worth optimizing for if you are good enough and the difference if any is small. Lua isn't better than js if it needs to run in the browser natively. reply ranger_danger 14 hours agorootparentAs with most things, I think that not only is \"better\" subjective, but such dogmatism is a sign that you are not considering that other relevant perspectives or context may exist that can disagree with you. I think true intelligence by definition requires humility and realizing that we can't know everything, there are things we don't know that we don't know, and the world is not so black and white. reply dismalaf 5 hours agorootparentprev> Dvorak isn't particularly better because typing speed isn't worth optimizing for if you are good enough Is Dvorak even quicker? I tried learning it and found some quirks, for example, lots of English words end in \"ght\" (thought, bought, draught, taught, etc...) which is extremely awkward to type on a Dvorak layout (you need to strike all three with one finger whereas on QWERTY it's split between both hands so you can alternate). I agree with your point though. I type 80+ WPM on QWERTY with no maintenance and honestly, I type way faster than I can even think about what to type (especially for programming). Now that my college essay days are way behind me, I don't think it even matters. reply FpUser 15 hours agoparentprev>\"Rust is a superior technology to C++....it disciplines you to do the things the right way.\" It is not. The only thing it disciplines one to do is to bend them to confines of particular safety model. And Rust creators do not have the ultimate authority to say what is the right way anyways. No one does. There are multiple concepts fitting different situations. Rust ignores many. As a general language it looks way too opinionated to me. reply zozbot234 5 hours agorootparent> Rust ignores many [safety models]. As a general language it looks way too opinionated to me. I'd like to see some actual examples of this, and I assume many Rust devs would react the same way. In practice, most proposed \"safety models\" that Rust complains about don't really work when considering that code must be enabled to interact not just within a single module but rather across modules. This is where Rust nudges developers to make the right choices for successful programming \"in the large\", while still providing the \"unsafe\" featureset for safety assurances within a single, self-contained context. reply FpUser 2 hours agorootparent> Rust ignores many [safety models] ... >I'd like to see some actual examples When I said opinionated and ignores I did not mean safety models. I meant programming concepts like OOP for example. And do not tell me that it is not needed, wrong, etc. It is totally fine and useful for some situations and not so much for others. Developer should have a choice. reply macgyverismo 9 hours agorootparentprevThis is what keeps tripping me up, it is very difficult to be both general and not opinionated. C++ is general, you can use it however you want. Especially in many unsafe ways. Like it or not, this is great news for a general low-level language. A language that tells me how to build, where my files should live and what tools I am to use may be a great user experience, it is not very general. Still not sure what to make of this and where I land on this. reply FpUser 2 hours agorootparent>\"Like it or not,\" I do not give a flying fuck. Language is a tool. I work with many languages. If clients wants language XYZ then XYZ it is. My private opinion does not matter in this case reply devnullbrain 20 hours agoprevThis maps to my own experience in the UK. Every time I search for a C++ job, I inevitably end up discussing my fondness of Rust but inability to use it at work. The interviewer will typically reply mentioning discussions of using it for greenfield projects - but I know it won't result in me writing anything of substance. 2 years ago, seeing a somewhat applicable Rust job-description made me 90% certain it was about cryptocurrency fintech. Now, a few defence roles are creeping in, presumably due to the US government distancing itself from unsafe languages. Neither are fields I really want to work in. And what a shame it would be if such a great language was relegated to being an Ada alternative. I try to keep on top of Rust, - it's the most likely candidate to put me out of a job - but it will be a long time before there are no more legacy C++ codebases. Being the COBOL guy of the future doesn't sound too bad. reply CaptainJack 19 hours agoparentBeing in the uk, we were fortunate enough to choose rust as a main language with my co-founder about two years ago. We chose it after trying it out for some toy projects, and with no real experience with it (but both of us having heavy experience of C++, C#, Python, Ruby, and having tested many others). We chose it because it felt \"right\", giving us c++ performance, productivity when writing, and a feeling of cleanliness from its type system I had not experienced since ... Ocaml. But what we did not expect was how great it was from a talent perspective. We started hiring at a time where lots of rust developers were being laid off crypto, and the caliber of candidates is just ... amazing. Rust devs enjoy working with the language, and you get a type of developer who likes producing good code, and is usually quite passionate about coding. So, I understand rust jobs are not easy to get by, but being on the other side of the table, it's a wonderful talent magnet for our team, allowing us to hire great developers. reply pdimitar 19 hours agorootparentApologies for the off-topic but I am currently in a job search. I'd rate myself below the guys and girls you hired but I am very enthusiastic working with Rust. You hiring? Again, sorry. :( But I am not getting any networking opportunities lately and couldn't resist replying to your message. reply wizzwizz4 19 hours agorootparentThere's a hiring thread: you might find that useful. https://news.ycombinator.com/submitted?id=whoishiring The next one's coming out in a few days. reply pdimitar 19 hours agorootparentAlready did, thank you, I am simply trying my luck at a direct connection. Sanctioned HR processes are a meat grinder. reply fragmede 18 hours agorootparentprevThe crypto bit is an interesting filter. Wether the developers you found are true believers, grifters in on the scam, or just mercenaries for hire welding crates, they can't not have an opinion on cryptocurrency so my question to you is how much of that do they bring to work? Are there coffee chats about Ayn Rand and politics or do they steer clear of any of that. reply lkt 13 hours agorootparentI think you have a very skewed view of what people interested in cryptocurrency are like. reply guenthert 9 hours agorootparent> I think you have a very skewed view of what people interested in cryptocurrency are like. Given that it's a heavenily gift for criminals (including those wishing to evade taxation on otherwise legit commercial activity) while leeching a good share of world's energy resources, I think he's excused. reply rcxdude 8 hours agorootparentprevI think it's a pretty accurate view of the average non-developer-crypto-enthusiast. I'm not sure how well it translates to the developer side. reply fragmede 10 hours agorootparentprevAbsolutely! It's a spectrum and people lie somewhere on it. I can't have a picture of the entirety of it (though I'd love to), but I can only go off my own experiences. I try and get as full a picture as I can, but it's unfortunately going to be skewed. yours is too. I'd love to hear which way your POV skews towards reply alfiedotwtf 13 hours agorootparentprevIt’s funny, I’ve been in crypto for a few years now and not once have I seen these grifters you talk about. Also, you should maybe stop associating right wing crypto YouTubers with crypto developers. I would say most I know are left of central. reply rcxdude 8 hours agorootparentNever? The highest profile things in crypto are the scams, so you gotta be pretty laser-focused on your niche to not see them. reply alfiedotwtf 5 hours agorootparentThe highest profile things in crypto are Bitcoin ETFs which are institutionally run, so I guess you’re not wrong there reply chikere232 10 hours agorootparentprevif you can't see the grifters, you're the mark reply alfiedotwtf 4 hours agorootparentYou’re a Java developer and you’re calling me the mark? reply JohnFen 18 hours agoparentprev> Being the COBOL guy of the future doesn't sound too bad. This is where I arrived for different reasons. It seems that Rust might become an important language, career-wise, so I learned it to the level of basic competency. In the course of doing that, I learned that I really hate Rust. Not on technical grounds, but I find the language itself unpleasant, and I dislike much of the tooling around the language (crates, etc.) Rust wouldn't be the first language I've learned and dislike, but this time, I decided to just take a pass. Even if Rust becomes the predominant language, there will always be work in other languages. I'll be content with that. reply pdimitar 18 hours agorootparentI am not sure I can decipher from your comment what do you exactly find unpleasant in the language and the tooling? reply JohnFen 5 hours agorootparentI dislike the language's syntax. I dislike the crates concept, but that's a minor thing I probably shouldn't have mentioned. reply pdimitar 4 hours agorootparentYou probably should not have because I'd immediately ask what is the alternative to crates. :D Dependency management is a fact of life for most of us out there in the trenches. It cannot be escaped. We don't want to roll our own. reply gigel82 10 hours agorootparentprevNot OP but the syntax is off-puting for me, it's simply difficult to read. I can parse and read all C/C++/TypeScript/Java/C# code with ease, it all makes sense and reads like poetry. Rust's syntax is just ugly, I have a visceral unpleasant feeling sort-of like when I have to read Perl code or (to a lesser degree) Objective-C. reply ribadeo 10 hours agorootparentYou don't like the turbofish? reply rcxdude 8 hours agorootparentlifetime annotations seem to be the biggest yuck for most people, given it's the most un-C++ like syntax (and single-quotes are always balanced in most languages). reply rightbyte 7 hours agorootparentSingle quotes are not balanced in GNU Octave or Lisps. I have never seen that as a critique of those. reply rcxdude 7 hours agorootparentI think they have more obvious syntax complaints (from the perspective of the C-syntax language developer), to be fair reply otabdeveloper4 2 hours agorootparentprevSorry buddy, you're not allowed to dislike Rust on the internet. You will now be arrested by the internet police and banned. reply pdimitar 2 hours agorootparentHope you're joking. reply mixmastamyk 17 hours agorootparentprevI did the same with golang. And too bad because it gets several things right that I’d like to use. reply sien 19 hours agoparentprevThis is also true where I am in Australia. There are very few Rust jobs. However, from the first two comments on the article : \"We use Rust at AWS (in my org) for every new project that would have previously been written in c++. [–]rigmaroler 104 points 23 hours ago Microsoft is the same. All new services running on VM-hosting nodes (i.e. domains where C# is explicitly not allowed) have to use Rust now. It's a top-down mandate. There's also AI investment in converting C/C++ services to Rust, but I have a negative opinion on that investment\" So there is clearly a substantial amount of Rust being written in some places. reply devnullbrain 19 hours agorootparentThat's a kind of tier of employer that wouldn't even invite me to interview but you are right and I expect it will trickle down to my level in the long-run. reply menaerus 8 hours agorootparentDon't beat yourself about it. My advice would be to get not too crazy about programming languages. Become proficient at one. Pick either C++ or Rust. Don't use HN as a source for making your next moves in career - it is very skewed. Rather use that energy to invest into building domain-specific knowledge - that will get you a lot further than being a programming language afficionado and a language lawyer. IMHO. reply AlotOfReading 19 hours agoparentprevNow, a few defence roles are creeping in, presumably due to the US government distancing itself from unsafe languages. I know a few contractor teams that have moved to Rust not because of any federal pressure, but simply because they're small and hierarchical enough that 1-2 people who understand the benefits are able to set the development language. reply eggy 17 hours agorootparentThey've been using safe languages like Ada and SPARK2014 for decades, and those are hardly unsafe. In fact they are safer and more mature than Rust and easier to learn IMHO. reply AlotOfReading 16 hours agorootparentTried hiring people for Ada jobs, or finding open source stuff to use in it? Ada has failed to find a market even in the safety-critical niches it should be dominating. Heck, AdaCore is putting out more Rust stuff these days than Ada. reply pjmlp 11 hours agorootparentYet, 7 companies are still in business selling Ada compilers, in a world where most devs refuse to pay for tools while expecting to be paid themselves. reply AlotOfReading 10 hours agorootparentAnd presumably they're all selling almost exclusively to the aerospace industry. There's like a dozen COBOL compiler vendors too. It's not indicative of the health of the language ecosystem. reply pjmlp 10 hours agorootparentAda is used all over the place in high integrity computing, that is much more than aerospace. Well, COBOL just got ISO COBOL 2023 out of the door, and both Visual COBOL and NetCOBOL, still offer a much better development experience than many FOSS toolchains. reply galangalalgol 15 hours agorootparentprevI don't think spark is easier to learn. Ada probably is. Ada sacrifices some performance for some of the benefits rust gets for free though, and deallocating memory is unsafe in the versions I've played with. If forbidding free is the way we choose to eliminate use after free, then there are a lot more memory safe languages. reply pjmlp 11 hours agorootparentOnly if you never moved beyond Ada83. Controlled Types and SPARK provide the mechanisms to deallocate only when it is actually safe to do so. Additionally unbounded collections, are just like graphs in Rust, they provide safe ways to managed dynamically sized collections, while hiding the unsafe code in implementation. reply causal 20 hours agoparentprevWhy would staying on top of Rust put you out of a job? reply Twirrim 20 hours agorootparentThey're saying they stay up to date with Rust, because it's the most likely to do away with C++ (their job), making sure they have relevant skills for if that happens. reply quickslowdown 20 hours agorootparentprevThe Rust language is what he's saying will put him out of a job, over tim as it replaces C (the language I'm guessing they work with professionally). reply pdimitar 19 hours agoparentprev> Now, a few defence roles are creeping in, presumably due to the US government distancing itself from unsafe languages. Neither are fields I really want to work in. Well, I am interested. Got any names? reply sam0x17 19 hours agoparentprevWhere are these C++ jobs? I've only seen Rust jobs :D reply causal 20 hours agoprevThis is kind of silly to me. You don't have to break up with Rust to use C++. Look for opportunities to adopt Rust, especially greenfield projects. We don't have to eliminate all C++ codebases first. reply arccy 19 hours agoparentmaybe you can only fit one really complex language in your head at a time while watching out for footguns reply galleywest200 18 hours agorootparentThen use a language with fewer footguns? reply sitharus 17 hours agorootparentprevBut that's not a problem with Rust, and it's not something that anyone should feel ashamed about. Just acknowledge \"Hey I can't use Rust because I can't commit the brain space needed to it\". reply n144q 19 hours agoparentprevExactly. VSCode and Chrome already have Rust code, although only a very small part. But if giant, complex projects like these can find places where Rust fits, most other projects can as well. reply dralley 18 hours agorootparentAndroid, Windows, Discord, Dropbox, Cloudflare, AWS reply pjmlp 11 hours agorootparentPluton firmware, Azure network cards firmware, new Azure Sphere SDK. reply zahlman 18 hours agoparentprevAside: I've been programming for... 35 years or so by my reckoning, but I can't recall ever hearing the term \"greenfield project\" before this year, and now it's seemingly everywhere. What's up with that? What happened to just... making new things? reply sph 6 hours agorootparentI've used the term \"greenfield development\" for 10 years and I'm not even a native speaker. Though I have been on this site longer than that, so I might have picked that up by osmosis. reply scubbo 18 hours agorootparentprevBeen around programming for ~20 years, professionally engineering for 13 of them, and I think I first heard the term (and have heard it consistently since then) at least a decade ago. Rejoice! You are one of today's Lucky 10,000[0]! [0] https://xkcd.com/1053/ reply fragmede 18 hours agorootparentprevhttps://trends.google.com/trends/explore?date=all&geo=US&q=g... Google trends only go back to 2004, but it's older than that. reply chris_wot 18 hours agorootparentprevIt's probably the circles you are around. The term has been around for a long time. reply yen223 17 hours agorootparentprevWait til you learn about \"brownfield\" projects, which apparently is also a thing! (It's the opposite of greenfield) reply ilrwbwrkhv 19 hours agoparentprevThis and also what people don't realize is that because right now Rust is a passion project for most people almost everything in it is of very high quality. Jobs will come soon because stability and speed is good for business. And then we will have a bunch of lower quality stuff but more jobs. So enjoy the good things at each stage. reply pdimitar 19 hours agorootparentI agree with your take and want to add that Rust has stellar tooling and code analysis of it also looks either easier or just more people are doing it so I think linting and semi-automatically applying better practices will be easier compared to, say, Python. reply ilrwbwrkhv 18 hours agorootparentAbsolutely. It is a better designed language by far. reply tayo42 19 hours agoparentprevIme you kind of do, at least did. I learned rust twice I think. First time it had a lot of time hype so I sat down and learned it. Then kind of like this rant never used it so it forgot it. Then it came up at work and the language changed enough that I had to learn it again. Features were added, the \"community approved\" libraries changed, tools changed, coding conventions changed. I never had that feeling with any other language I've used in similar ways. Javascript, ruby, python go I always felt like I could learn, stop using and come back to use pretty easily. reply pdimitar 19 hours agorootparentI have difficulty picking Rust again for semi-different reasons than yours: it simply has a huge surface, not only the core language but also the libraries; the amount of those you really must know to be able to call yourself a commercial Rust programmer seems to grow with time. (You mentioned this last point, hence the \"semi-different reasons\" expression.) I know Rust quite fine as a language but put me in a commercial project and I'll definitely need a few weeks to learn what should be used for i.e. error handling, logging, OpenTelemetry, and such. reply goku12 16 hours agorootparentThat's odd! Finding libraries (crates) has been the easy part for me. You usually get the answer directly from crates.io. Even in cases where there are multiple alternatives, it's easy to choose one based on the statistics available on crates.io. And in the rare case where you still can't decide, a web search reveals the frontrunner with detailed articles on why. reply pdimitar 16 hours agorootparentOh, I'm not saying it's difficult. I'm saying that at one point it becomes too much. And if I don't actively work with Rust I end up forgetting and having to relearn in the future. reply goku12 14 hours agorootparentHow is it different from the situation in other languages? Especially the ones like C and C++ that don't have a canonical source registry either? (Not a rhetorical question) reply pdimitar 4 hours agorootparentNormally yes but f.ex. my favorite Elixir has a community that very strongly prefers and orbits around singular solutions of thorny problems. You will not find many ORMs / DataMappers in that ecosystem, it's one that has been super hard worked on and nearly everyone accepts it and loves it and contributes to it when the need arises. Meanwhile in many other languages, Golang and Rust included, there are many ways to do the same thing. That introduces difficulty to keep well up to date. But I can easily agree this point gets very weakened after you have worked with the language for a certain amount of time and on. reply esafak 16 hours agorootparentprevC++ has changed a lot. reply npalli 18 hours agoprevI wonder how much of this love of Rust stems from the fact that he actually didn't have a full time job programming in Rust. When one starts off doing hobby projects in Rust the experience can be very pleasant thanks to cargo (btw, it is a bit puzzling that vcpkg is not adopted more broadly in C++ land since the gap from cargo is not that bad). However, once you get a job writing programs and find out you need to quickly prototype, refactor the code or make changes under a strict deadline, not good, it quite literally is the worst language to make large scale updates beating even C++. This is not counting the other memory safe languages (golang, C#, Java, Kotlin, etc..) which is where you should first target. reply lvkv 18 hours agoparent> it quite literally is the worst language to make large scale updates beating even C++ Having worked in both C++ and Rust codebases professionally, my experience has been the complete opposite. I have to be extremely cautious when making changes to C++ codebases, (especially async ones) and it’s always a huge time sink trying to just get the damn thing and its tests compiled. Rust’s compiler and tooling, on the other hand, allow me to make high-pressure changes much more confidently. It’s not perfect, but I’ll take it over the average C++ project every time reply echelon 17 hours agorootparentRust is the easiest language I've ever made refactors in. This article is nonsense. I've got a massive multi-binary monorepo and it's so easy to make sweeping refactors to core libraries that impact all build targets. I can't fathom doing this in C++, or worse, a dynamic language. reply saghm 17 hours agoparentprevI'm not sure what's happened on the Rust projects you've worked on professionally, but having used Rust for over nine years now personally and about five years professionally, every experience I've had trying to refactor large projects in Go, C++, Python, and Ruby at work has been strictly worse than even my worst experience having to do so in Rust. Following the compiler errors makes it super easy to follow a chain of cascading changes from needing to change something that gets used elsewhere, which combined with the usual strategy of using the test suite to detect regressions makes it as easy as I could ever imagine. I've often had the experience where I've done large refactors that touch dozens of files and thousands of lines of changes where the tests pass immediately after finishing the changes to get it to compile, and in the refactors at least that size (including ones larger than that where it isn't as cinnamon to where that doesn't happen as often), the remaining work to fix things beyond stuff the compiler can catch is still much smaller than I've had to deal with in other languages. Obviously, some amount of this is due to my own level of comfort in Rust and not specific to the language, but even if I accepted your premise that Rust is horrendous for large changes, that still would mean that being comfortable with a language can easily make up for any inherent issues with refactoring due to the language itself. If you strongly dislike working on large Rust codebases, I agree that it's probably better to avoid trying to use it professionally, but my experience is so radically different from what you describe that I have trouble believing that this is due to Rust being objectively unfit for large refactors to the extent that you describe. reply bryanlarsen 18 hours agoparentprevThe disadvantage of doing a big refactor in Rust is that you can't do a partial refactor -- it won't compile until you've fixed everything. The advantage of doing a big refactor in Rust is that you can't do a partial refactor -- it won't compile until you've fixed everything. reply echelon 17 hours agorootparentWhy would you want a refactor into a broken state? You always need to figure out how much you're going to bite off with each refactor. It's usually possible to do things incrementally. reply zsyllepsis 17 hours agorootparentI don’t think that’s what the parent was saying. There are cases when refactoring Rust code where it’s possible to hit limits in the compiler related to e.g. lifetime inference. When these limits are hit, simple straightforward refactorings that are perfectly safe become more complicated - suddenly you’re forced to manually annotate lifetimes, and to thread those lifetimes through function calls, and… And your small, incremental refactor suddenly isn’t. It doesn’t happen all that often, and they’re working to reduce how often users run into these challenges, but a number of cases like this still exist. And when you run into them it can be a frustrating experience. reply estebank 11 hours agorootparentPersonally I'm convinced that the solution to that is not for the language to be more implicit but rather to make tooling for refactoring more front and center. A task for \"add a lifetime to this struct everywhere it's mentioned\" is already catered to by modifying the original type and then applying rustfix, but more advanced but relatively common changes should also be mechanized away. The annotations are there not only for the benefit of the compiler but also the developers. reply jayd16 15 hours agorootparentprevYou might want to test some optimization or refactor without crossing all the Ts. Lost of reasons you'd want to quickly iterate without doing the maximum amount of work needed upfront. reply efficax 16 hours agoparentprevYou hear this about refactoring a lot and I don't get it. I've done several multi-thousand line refactors of a 500k line rust codebase in the past few years, and it's absolutely fantastic for this task. The type system guides you along, and once the thing compiles, it almost always works exactly as you intended it to. the \"quick prototyping\" languages like say python are a nightmare to do large refactors in, in my experience. at least if you don't want type system timebombs lurking in infrequently executed corner cases. reply Tuna-Fish 18 hours agoparentprevMy experience is the exact opposite. Rust is slow for the initial programming, but great for making changes in, because the type system holds your hand and makes sure your changes are sound. reply Maxatar 17 hours agoparentprevRefactoring C++ is an absolute nightmare due to templates and overloading. If you rename a function for example, tools can't propagate that change through a template because there's no way to know if that template is only ever used to call that particular function, and not some other function that happens to share the same name (some overload). Another way to think of it is that in C++, templates are dynamically typed, and so templates suffer from all of the same refactoring problems that dynamically typed languages do. In Rust that problem doesn't exist, there is no function overloading and generics are \"statically-typed\" via traits, so you can factor code that resides within generics just as you would refactor any other code and it all just works. reply tcfhgj 15 hours agorootparentYou probably are looking for duck typing reply brson 18 hours agoparentprevRust was specifically designed to be refactorable and in my experience it is. It was part of the dogfooding process of building Rust in Rust - lots of changes to the language, lots of changes to the compiler, lots of churn. Rust's strong type system means you can refactor and be confident that programs continue to work. reply ribadeo 10 hours agoparentprevRust is one of the easiest languages for code refactoring, in my experience: the compiler amd clippy tell me everything i forgot to do. Just literally doing what clippy suggests often gets it compiled. reply Buttons840 17 hours agoparentprev> you need to quickly prototype, refactor the code or make changes under a strict deadline, not good How many projects like this shouldn't be written in C++ in the first place? C++ isn't automatically faster than Java, and with the level of care you're describing (\"strict deadlines, just get it done\") you're unlikely to realize the advantage of C++. reply zozbot234 5 hours agorootparentRust is a lot better at quick prototyping than C++. There is a specific idiom to writing prototype \"throwaway\" code in Rust though, where you .clone() and use RefCell and Rc a lot more than you normally would (not to mention really niche features like Any). The advantage is that this boilerplate guides you when refactoring the code for better reliability and performance later on. reply pkulak 17 hours agoparentprevI use Kotlin all day at work and still think it’s a wonderful language. I never use Rust at work and also think it’s a wonderful language. Rust is really just a fun language to write apps in. Is it okay for us to admit that yet? reply lmm 17 hours agorootparentFunnily enough I read the article and thought of Scala, which I love but can no longer find a job doing - and can't help thinking it's in large part due to FUD spread by the Kotlin people. reply vips7L 6 hours agorootparentI’ve casually used Scala at work and home over the past 5 years. I mostly think this is because of the community and also the tooling and libraries. WRT the community the functional zealots have pushed out everyone that doesn’t align to their ideology. Everyone that just wanted a better Java has moved on to Kotlin or back to modern Java. WRT the tooling: scalac and sbt are so slow it’s painful and even IntelliJ can’t figure out what’s going on in Scala sometimes. WRT libraries: every single time I have to upgrade play or the scala version there is a breaking change that either forces me to refactor or is undocumented and screws me at runtime. So mostly at least for me, I’ve been burned by the language too many times to start a new project in it, especially at work where it will be long lived and I’ll eventually have to upgrade things. reply fsckboy 16 hours agoprevI like C. I learned it a long time ago, it was one of my favorite languages to learn, and I'm comfortable using it. I'm open to learning new languages, but learning new languages from people who talking nothing but trash about C does not inspire confidence. Nobody who loves unix says \"you need to try Windows.\" At the same time, Windows lovers trash Unix. So the two schools remain separate. if you want to sell Rust, find people who like C who will recommend Rust and shut everybody else up, then people who like C will listen. edit: thought of one, the soles of shoes reply pornel 15 hours agoparentBefore Rust, C was my preferred language. It's really hard to talk about Rust in a way that doesn't sound like talking trash about C, because Rust's main reason to exist is preventing safety issues that… people programming in C usually say are not C's fault, so there's nothing to fix there. The other aspect is that Rust has been developed recently, and didn't have to deal with all the legacy constraints of C. The features that Rust has would be mostly boring compared to modern GC or scripting languages, but they're notable for being in a language that can be as low-level as C. Comparing them directly to C does like trashing C — namespaced modules instead of textual inclusion, macro preprocessor based on AST (do-while tricks not needed), no headers, no need for build scripts in most programs, package management that works on Windows and macOS too, etc. Many other languages have this stuff, but C doesn't, so they're an interesting upgrade from C just because C is so old. reply azangru 19 hours agoprev> But Rust is better in the same way that Betamax was better than VHS, Mastodon is better than Twitter, Dvorak keyboards are better than QWERTY, Esperanto is better than English and Lua is better than Javascript Esperanto is certainly not better than English; and I really doubt Lua is better than Javascript. reply david-gpu 19 hours agoparent> Esperanto is certainly not better than English; and I really doubt Lua is better than Javascript Doesn't that depend on how one decides to \"score\" a language? And I think that's what the author is getting at, too. reply flysand7 19 hours agorootparentI caught a different meaning from this entire sentence. I think the author was alluding to the fact that even if you replace a technology with something \"better\", at the end it doesn't matter, because Most People will keep using Twitter, Most People will keep using QWERTY layout, count all of your acquaintances, I doubt any of them speak Esperanto. Well at least the idea comes through, but I don't think it makes sense to argue whether Lua is actually better than JavaScript or not. reply david-gpu 19 hours agorootparentIndeed. If one primarily values certain technical aspects, Beta was \"better\" than VHS. But if one primarily values popularity, profitability or practicality, then VHS was \"better\". And so on with the other examples. So when we go back to this: > Esperanto is certainly not better than English; and I really doubt Lua is better than Javascript All I get from it is \"I personally have a strong opinion about what makes a language 'better'\". Nothing wrong with that, but it's independent from the argument made by TFA. Perhaps I misinterpreted. reply coldtea 18 hours agorootparentprev>Doesn't that depend on how one decides to \"score\" a language? Not as much, because not all ways of scoring a language are as good either, and in the ways of scoring that matter, English is better than Esperanto. reply fragmede 18 hours agorootparentIn terms of not being a bag of special cases, and thus easy to use, Esperanto is better than English. Like, if you were to design a language from first principles, you'd come up with Esperanto. The only problem is the massive inertia English has, which makes it \"better\". In terms of design, it's absolute trash. But it's what humanity's got. Trying to learn English as a second or third or fifth language is just so difficult. reply mf_tomb 17 hours agorootparentThose special cases are useful or the natural result of speakers coming from other languages. If esperanto were used widely for many years, it would also develop special cases over time. In 200 years, it would be as irregular as english, as it borrowed words and phrases from other languages. reply coldtea 16 hours agorootparentprev>Like, if you were to design a language from first principles, you'd come up with Esperanto That's the problem. It's design by commitee instead of evolutionary organic design. It's how cyborgs think people oughta talk. reply jszymborski 19 hours agoparentprevI think maybe they meant it is a better lingua franca than English, which I'm a little more likely to agree with (Esperanto is by no means perfect for this) reply prmph 9 hours agoparentprevFor me, one major weakness of English is having adjectives appearing before nouns. The more time passes, the more I hate that aspect of English. It messes up all sorts of things. Most other languages don't have this problem. Consider that in programming, technical docs, even normal writing, it is almost always better to the most important aspect of a name come first. The adjective-noun order in English makes this awkward all the time. reply kstrauser 2 hours agorootparentI love its contribution to wordplay. You can write things like “yesterday we sat down to a meal of golden, moist, roast whisky” because the adjectives lead up to a set of expectations about the subject that turn out to be untrue. reply rightbyte 7 hours agorootparentprevYou can go for 'Achilles the swift' order if you want though. reply yyuugg 19 hours agoparentprevHaving worked with Lua and JS professionally for years, Lua is far, far worse than JS. reply darknavi 1 hour agorootparentI love Lua for game dev, but JS works fine and Typescript is awesome. I loath 1-based indexes though. Just odd. reply joshdavham 18 hours agoparentprev> Esperanto is certainly not better than English Agreed. Until Esperanto starts having actual native-speaking communities, I will look at it the same way I look at Klingon or Elvish. reply dogmatism 18 hours agorootparentWell, there are denaskuloj, but aside from that, I don't think languages are adopted into communities to replace other languages based on their merits, but based on socio-religious factors that have nothing to do with the language itself reply catlifeonmars 19 hours agoprevIMO choice of language is a design decision. Weighing the pros and cons of a particular language without specifying the use case and project requirements seems ass backwards to me. To illustrate, consider if we rewrite the title: “Goodbye, Phillips head screwdrivers, I wish you success, but I’m back to flat heads”. Seems silly, doesn’t it? To be fair though, I imagine this article is written as a reaction to Rust proponents making similar arguments. reply pdimitar 18 hours agoparent> Weighing the pros and cons of a particular language without specifying the use case and project requirements seems ass backwards to me. Practically every company I talked with about Rust positions named very good reasons to move to it. N=1 and all, yup, but your opinion is N=1 as well. There are several very engineering-sound reasons to choose Rust and people are aware of them and are appealing to them when considering it. reply dartharva 13 hours agoparentprevMost regular hard-and-fast organizations outside of the SV bubble don't subscribe to this. They will rather tweak design around to match whatever labor pool is available (e.g. Java for most code farms in the past decades). reply cedws 19 hours agoprevC and C++ are languages of an era. C++ will be unseated eventually but it will take another decade at the minimum for a new generation of programmers who have used the 'modern' languages from day one to overthrow the incumbents. Then another few decades to rewrite everything. reply stefanos82 19 hours agoparent> Then another few decades to rewrite everything. This will never going to happen for obvious reasons; if it ain't broken, don't fix it! Create interoperability with other languages, like what Swift is currently attempting to do, yes; but to completely rewrite an entire project that consists of millions of lines, I don't think so. reply pdimitar 19 hours agorootparent> if it ain't broken, don't fix it! All the CVEs found in the last several years would like a word with you. reply AnimalMuppet 17 hours agorootparentAll the code rewrites from the last several decades would like a word in return. A bunch of those rewrites managed to miss critical things that were encoded in the existing logic in non-obvious ways. They broke a bunch of workflows that way. A bunch of others went wildly over-budget and wound up getting shut down before they produced anything usable. But surely the documentation will tell you all the use cases, and the expected inputs and outputs, right? Right? Oh, you don't have documentation like that? Yeah, then it's used in ways you don't know about, and your rewrite is likely to break them. (And that's true even if you do have such documentation, because it's incomplete.) Rewrites are hard. They're deceptively hard. They look like they should be easy, and they very much are not. \"But this time we're going to use Rust!\" Yeah, but the thing that makes rewrites so hard was never the language, so using Rust isn't going to actually fix it. reply 8fingerlouie 9 hours agorootparent> Rewrites are hard. They're deceptively hard. They look like they should be easy, and they very much are not. This is a major reason why so many financial/healthcare/insurance/government institutions still runs COBOL software on mainframes. Many times, you're looking at what is essentially a monolith (in many parts, but everything is connected in some way) that was 50+ years in the making. Rewriting it is a huge and extremely expensive task, and had it not been because COBOL is considered a \"bad language\" by the young people, and thereby causing problems with recruitment, i'm guesssing the mainframe would be around for 100+ years to come. As it is now, these companies are forced to migrate away from COBOL, and most companies in the financial sector would probably go for Java, which is posed to becomming the new COBOL. If you start a career today as a young COBOL programmer, you will most likely have a high salary (for your career) for your entire work life. The mainframe is certainly on the way out, but it's a big beast to move, and most \"estimates\" i hear from financial institutions is that they plan to migrate away in 10-30 years, and predicting what happens in 30 years is kinda pointless. Suffice to say that the Mainframe and COBOL is probably here for another 30-50 years. reply pdimitar 4 hours agorootparentprevI agree with your take and want to add that \"we want less CVEs\" and \"we want meaningful rewrites\" are not necessarily at odds with each other. We can have both. I'll also note that efforts like rewriting a lot of UNIX userland in Rust also introduces the benefits of rewrites -- you have to think super hard if you want that obscure command-line switch that only works in X way if another switch has another value or it does the thing Y if two other switches have these other values. I forgot the examples but that's what happened there: people figured \"we will not implement command switches X and Y\" and lo and behold, people using those tools never even noticed, myself included. So, rewrites have positives as well. And not everyone doing rewrites is clueless. reply AnimalMuppet 3 hours agorootparentI didn't say that those who do rewrites are clueless. I said that rewrites are really hard - far harder than most people expect. That weird switch behavior? I almost guarantee that someone, somwhere, some when depended on that behavior. Either they no longer use the switch behavior or they no longer use the tool, or they just don't use the rewritten version. But how do you know that? In the best case, the code would have a comment: \"This code is to simulate Berkeley 3 behavior for users porting from...\" But even that doesn't tell you whether anybody today still depends on it. I mean, it could be a valid strategy to not rewrite weird behaviors - any of them - and see what breaks, and then only add the ones that are still needed. The upside is a much cleaner rewrite, with fewer (and, hopefully, better documented) weird behaviors. The downside is that you have to break stuff temporarily to find out. (Hopefully you break it in live staging rather than in production...) reply cedws 19 hours agorootparentprevAll code has a lifespan, either because it rots or it becomes irrelevant/unneeded. Existing C++ code will gradually expire and the most important pieces will indeed be rewritten, accelerated by shrinking expertise. reply davidcbc 19 hours agorootparentThis may be technically true on a long enough timeline, but COBOL is still the backbone of the financial sector even though companies have dumped millions into attempting to convert it to something new with limited success. reply cedws 18 hours agorootparentFor sure, we can debate the timeline, we can debate what language will replace C++. The way I read stefanos82's reply is that they don't believe C++ code will ever be rewritten, which is obviously not true. COBOL running the financial sector is a myth in my opinion. There are still companies running aspects of their business on it, but calling it the 'backbone' is an overstatement. If you tallied up lines of code by language in the sector, I would wager COBOL makes up less than 1%. reply davidcbc 18 hours agorootparentThe backbone is a minority of the bones in the body too, but without it you're screwed. My previous job was at one of the largest investment companies in the world, if the COBOL went away the company would collapse. reply throwaway2037 15 hours agorootparentWhat company? reply jamesfinlayson 17 hours agorootparentprevYep, I've worked for two companies where Fortran handles all of a major department's business - in both cases it was buried deep under layers of more modern stuff... but it was hugely risky to change it so it stayed. reply goku12 16 hours agorootparentWhile the argument is generally correct, Fortran and COBOL aren't in the same league. Fortran is still being upgraded and has some use cases that are hard to achieve in another language. I've seen the Rust subreddit recommending use of old Fortran code with Rust, rather than rewriting it in Rust. Unlike most other languages like C++ and Rust, Fortran kept its scope mostly limited to numerical processing - making it still one of the best solutions to code highly parallel numerical algorithms in. Fortran isn't surviving just because nobody wants to rewrite Fortran code. Unless you're talking about Fortran77 code. reply jamesfinlayson 15 hours agorootparentOh agreed - Fortran is much more alive than COBOL - but in both cases I think the Fortran had outstayed its welcome. I never saw one of the codebases but the one I did see was Fortran 77 I believe, and there were a lot of gotos and labels. reply throwaway2037 16 hours agorootparentprev> COBOL is still the backbone of the financial sector Really? Is that commercial banks or ibanks? I think this is a myth. reply leptons 18 hours agorootparentprevRewriting existing code in another language might be one thing LLMs can actually get good at. I wouldn't doubt if they could convert complex projects someday, without too much error or hassle. reply jayd16 14 hours agoparentprevC++ is really long in the tooth from a language design and tooling perspective. In that sense its due for disruption. So yes, it seems like C++ is on the way out. On the other hand, Rust is not a strict upgrade. In game dev, runtime performance and iteration time are priorities. Safety is secondary. Rust isn't gaining that much ground in games because Rust appears to be unnecessarily burdensome. Async is especially useful for UI and game-thread style programming and that seems to be a weak spot of Rust as well. There's still room for something besides Rust to steal C++ market share. reply nightowl_games 16 hours agoparentprev\"Few decades\". Im thinking more like at least a century. reply timbit42 2 hours agorootparentIt will be faster than you think. At some point a big hack is going to occur and governments are going to step in and legislate to ensure it never happens again and we're going to have to rewrite everything written in C/C++ in safer languages. Rust and Ada are safe but I'm hoping something better comes along before the government steps in. reply aninteger 19 hours agoparentprevAt first I thought you were serious, but I can appreciate a good attitude with sarcasm. reply eggy 17 hours agoprevI like the Betamax vs. VHS, X vs. Mastodon bit, because technically if you want a safe, high-integrity programming language today, you would choose SPARK2014, the Ada language that has a legacy of mission-critical real-world software projects for decades, proven verification tools and and it is easy to read and write. Rust gained favor with the tech crowd, while SPARK was being used in avionics, aerospace, and other high-assurance software. Why would you choose Rust a relatively new, unproven language (cite some mission critical software that has been running for a decade written in Rust besides a browser) for crypto, aerospace, AWS, and other critical areas? Programming language adoption is just as much about fashion as it is about a true pros/cons in tech. reply woodruffw 16 hours agoparentI think the answer to “why” here is (and will always be) familiarity: Rust bears syntactic and semantic resemblance to already popular languages. Ada doesn’t to the same degree. (This could be framed as a fashion choice, but I think the more neutral framing is precedence: Rust achieves desirable language-level properties without exhausting the novelty budget for people writing non-government software.) > cite some mission critical software that has been running for a decade written in Rust besides a browser This will be hard for anyone to do, given that Rust 1.0 was in 2015, so 9 years ago. However, if you want examples of Rust running in mature, critical environments: my understanding is that Firecracker has been a key part of AWS’s serverless control plane for years now. Similarly, my understanding is that Windows has been shipping Rust in the NT kernel for the last year. reply srj 17 hours agoprevAm I the only one who loves writing C++? I've used some newer languages like Kotlin, and that's good too, but I always come back to C++. I think a lot of the criticism is from the old way it was written, before C++11/17. reply MathMonkeyMan 17 hours agoparentC++ is a lot of fun, and there exist many compelling subsets, but the language as a whole is a minefield that you need years of experience to navigate, and probably can never master. reply nightowl_games 16 hours agorootparentKinda. The vast majority of c++ I encounter is simple orthodox c++. No one I know cares about new features in c++ and bemoans having to ever write a template. No one really cares about bulletproofing types, ie: copy/move constructors and all that stuff. I'm sure it's different at big companies but the vast vast majority of c++ is just simple procedural code. reply 8fingerlouie 9 hours agorootparent> the vast vast majority of c++ is just simple procedural code. Probably because where C++ is needlessly complex, C is beautifully simple. I've used both C and C++ in my career, spending 10+ years as a C programmer (kernels, applications and embedded stuff), as well as 4-5 years as a C++ programmer in a financial institution. I find the thing most \"new C/C++ programmers\" complain about is memory management and type safety, but honestly, once you have a few years worth of experience, memory management is almost second nature. Most of the time i write the free() statement at the same time as i write the malloc() statement (if applicable). reply weebull 5 hours agorootparentOne of the things I loved about Python when I learnt it was how it dealt with `public`\\`private`\\protected``. It was \"we're all responsible adults. No need to hide anything. We'll just use a naming convention for members that we don't expect people to directly use.\" \"Enforce encapsulation\" suddenly became. \"Respect encapsulation\" in my head and a bunch of Java/C++ problems evaporated. reply kstrauser 2 hours agorootparentI do love that about Python. The encapsulation is there but you can work around it if you’re willing to take responsibility for breaking the gentlemen’s agreement. That’s a lovely pattern. reply macgyverismo 8 hours agoparentprevI love it too, it tends to bend to my will, for better or worse. It feels like it doesn't stand in my way, and that further translates to feeling like there is nothing between my program and the hardware it runs on. No need to school me on how that isn't true, I'm just describing my feeling and why I love C++. reply pjmlp 11 hours agoparentprevI love C++, it was my next language after Turbo Pascal, there was a small stint with C, but already in 1992 it felt outdated. However, during the last decade C++ culture suffered, probably it has taken too many C refugees. What you call old way, it is still how many folks write C++ in big corporations, I tend to complain that I only see Modern C++ in conference slides, and my own hobby coding. reply jayd16 14 hours agoparentprevTooling is a pain, packages are a pain, compiles are slow, the language is inconsistent, header files why..., the list goes on and on and its not outdated complaints. (lots of reasons to like it, too) reply jvanderbot 17 hours agoprevThe root issue I have with this and many other articles is the false dichotomy around this vs that programming language. I would feel comfortable interviewing for a Rust, C++, job, and probably a C, Pytbon position as well, just because the industry I'm in might require it. reply synergy20 16 hours agoprevFor me, Rust has a size issue, its std lib after strip is at least 3x as to libstdc++, and Rust is default to statically link to that lib! When you have a few Rust binaries, the storage space adds up quickly, not a good fit for embedded boards where storage is limited(same problems for Go, by the way). Why can't Rust have a mode to do what c/c++/etc doing, that is, a reasonably sized standard library to link to? Yes I'm aware of 'how to minimize rust size' and 'prefer-dynamic', still comparing to c/c++, it is so much worse when storage size is a concern. reply yolovoe 2 hours agoparentAn increasing amount of AWS EC2 dataplane (Nitro firmware/software) is using Rust. We hit this issue too, and our solution was to go busybox-style with binaries. One binary that then gets linked to with different names. Solved the problem for us. reply synergy20 5 minutes agorootparentfirmware probably needs no stdlib at all,just the language itself,so rust for low level is fine when stdlib is needed then size becomes a big issue, yes busybox approach helps,but. reply robertlagrant 14 hours agoparentprevMaybe it can. Why don't you propose[0] a list of features that would fit your requirements? You never know! [0] https://lang-team.rust-lang.org/how_to/nominate.html reply synergy20 14 hours agorootparentI'm not very hopeful as it has been discussed in the past. If Rust will ever replace C++, binary size is one of its biggest weakness in my opinion. Been safer is far from enough to replace the incumbent, you also need win in simplicity, speed, size, ecosystem,etc. Size at the moment is the most obvious problem as far as I can tell. reply zozbot234 5 hours agoparentprevDid you try https://github.com/johnthagen/min-sized-rust ? reply synergy20 5 hours agorootparentyes as mentioned,it's still too large compared to c++ when i have multiple executables reply tikhonj 17 hours agoprev\"Popularity is the only thing that matters, and Rust is not popular enough.\" What a fundamentally narrow and depressing point of view. \"Nobody gets fired for IBM\" should not be an aspirational sentiment! And the top comments take the first part axiomatically and only disagree on how (un)popular Rust is. Lots of ideas and technologies can be useful and successful without becoming massively popular. There's a reason we don't all eat McDonalds and listen to Pop music. So why shouldn't we expect the same from different ways of thinking about and practicing programming? It's clearly possible to be productive and effective in \"unpopular\" languages—I've seen it first-hand with OCaml and Haskell teams, and secondhand with lots of other tech—so why not encourage that? I'd rather have an industry that values good taste than slavish trend-following, thank you very much. reply akira2501 17 hours agoparent> What a fundamentally narrow and depressing point of view. I want to get work done. Not be a part of some \"software taste revolution.\" If you can't help me get my work done you're not on the list. It's your point of view that is narrow. If popularity is not material then the loss of a single developer will have no impact and Rust will be the same as it ever was. Otherwise popularity matters and you should endeavor to observe and catalogs Rusts shortcomings so they can be addressed. reply robertlagrant 14 hours agorootparent> If popularity is not material then the loss of a single developer will have no impact and Rust will be the same as it ever was If lots of articles are written saying that popularity is all that matters, then people will start believing it, and it becomes a self-fulfilling prophecy. That's the narrowness. reply akira2501 14 hours agorootparent> then people will start believing it I don't subscribe to this philosophy. People will repeat it in certain contexts to avoid sounding out of touch and incurring social bullying but I have genuine doubts that their beliefs can be so easily manipulated. > and it becomes a self-fulfilling prophecy If you do believe this then your answer is just to write a bunch of articles in the opposite direction. It's a gross sort of world to accept living in; though, this is probably why I hold the belief I've espoused above. reply makeitdouble 16 hours agoparentprev> \"Popularity is the only thing that matters, and Rust is not popular enough.\" This is unfair to the article, when the actual focus is on finding a job and paying the bills. It doesn't have any of moral bankrupcy of the \"Nobody gets fired for IBM\" situation, where we'd be enabling corrupt companies because everyone else is also turning a blind eye. reply tikhonj 15 hours agorootparentIf the post had just been \"I'm doing C++ instead of Rust because I could find a C++ job but not a Rust job\", that would have been totally reasonable and I would not have said anything. But that was not what the rant was saying—otherwise it would have been boring and not made it to HN and /r/rust. reply dartharva 13 hours agoparentprev> What a fundamentally narrow and depressing point of view. Maybe not for you HN patricians, but most regular people in the world want stability and have other immediate priorities far above becoming a programming language revolutionary. reply echelon 17 hours agoparentprevRust is extremely popular and is only getting bigger with every passing year. It's got incredible velocity and is about to break into the TIOBE top ten. The Rust critics can't stop other people from using and enjoying it. It's unstoppable at this point. > For higher level stuff (e.g.: web backends) Go offers faster iteration cycles than Rust Rust is excelling in this space and in so many other use cases it was never originally imaged for. Have you seen the Actix/Axum adoption? It's insanely popular, and it's such a nice and low defect rate language to write web endpoints in. It's actually highly productive in this use case. > Yes, Rust adoption is rising but competing technologies are also getting better Other languages are not \"catching up\" on competing with Rust. Adding sum types is great, but that won't deliver the same benefits. Rust features make it fit into a truly unique space. > a big push in advancing programming languages was the support of big corporations No big companies using Rust? All of the big companies are using it! > the first rule of Rust jobs is that no one talks about Rust jobs There are Rust jobs! I'm hiring Rust devs. reply ethagnawl 16 hours agorootparent> It's insanely popular, and it's such a nice and low defect rate language to write web endpoints in. It's actually highly productive in this use case. I concur. Lots of folks here and elsewhere will make claims about how Rust isn't a good fit for this space because _it's a systems language_ or because the iteration cycle is too slow and it isn't meant to replace Python/JavaScript/whatever, anyways. To that, I call BS. It's (objectively) a general purpose programming language and can be used to do whatever you need it to. To your point, I've also found it extremely productive for web development. Even if it does require more work up front, that's (usually) work you're not doing on the back-end chasing and squashing bugs or trying to cover corner cases which you hadn't accounted for -- but the Rust compiler would have. reply fny 19 hours agoprevI deeply appreciate the mood, but when the NSA started pushing memory safety and TRACTOR launched, I doubled down. reply mattgreenrocks 19 hours agoparentTRACTOR seems like a helluva Hail Mary. The C and C++ abstract machine had so many sharp pointy bits that need to be reified exactly that I find it hard to translate to Rust. Whoever wins it (not sure if they’ve announced it yet?) will be doing God’s work but it feels like an enormous undertaking. And I sometimes like these DARPA-hard projects. reply kazinator 13 hours agoprevYou can make a pointer-based graph structure with cycles in C++ without a fuss. You can decide how the memory managed and implement exactly that with minimal barriers in your way. reply an_d_rew 13 hours agoparentTrue... but whether the ability to so is a superpowers strength or a debilitating weakness depends on both your values and point of view! reply astonex 19 hours agoprevJust give me a modern C++ like language with fast compilation, easy to pull in packages, and build tooling. Basically Go with no GC. reply pjmlp 19 hours agoparentCheck Odin. reply joshlemer 19 hours agorootparentThe language seems to be heavily branding itself as \"data-oriented\", mentioning it front and center on the website and repo, but never mentions in the docs what makes it data-oriented, or what it even means by that. Is it data-oriented in the Clojure sense, in contrast to \"place-oriented\"? Or is it data-oriented as in programming with tables of rows, or something else? reply dgellow 18 hours agorootparenthttps://en.wikipedia.org/wiki/Data-oriented_design reply throwaway2037 15 hours agoparentprevHow is fast compilation possible with C++-like templates? That seems impossible. reply pjmlp 11 hours agorootparentC++ modules, in VC++ and clang/CMake/ninja today. Add to the potion, binary libraries and extern templates for the common types. reply jayd16 14 hours agorootparentprevSo redo the templates. reply AlotOfReading 13 hours agorootparentThey did, hence concepts and constexpr. Rust has more powerful versions of both in traits and macros. reply jayd16 13 hours agorootparentAren't Rust compiles slow as well? I'm suggesting they take a different approach. Are these systems really needed seeing as other languages do without? reply AlotOfReading 11 hours agorootparentOther languages don't do without them, the costs are just worse in Rust (and C++). Go for example consciously tried to avoid shipping generics and was forced to implement them by community pressure. They now pay the same kinds of monomorphization costs as C++ and Rust because the alternatives have greater runtime costs. Go is simply a faster-compiling language overall though and the problem is less noticeable. Macros are similarly ubiquitous in other functional languages, but most of them aren't compiled or meaningfully optimized. Rust is complex, highly expressive, and does a lot of optimization. None of that's good for compile times. C++ is even worse because of how awful the template abuse gets and the amount of duplicated code that's fed into the compiler from how #includes work. reply cemdervis 9 hours agoparentprevSwift reply vips7L 5 hours agorootparentArc is a GC. reply whatyesaid 19 hours agoprevRust is slow to develop and prototype in. It's good if you're a big company not the average Joe. We need a better C++ (\"systems\") base language that could have an optional borrow checker or bounded model checker enabled. reply rtpg 19 hours agoparentIs C++ easier to prototype in? Of course there's the borrow checker, but I feel like Rust at least shows up with some expressiveness niceties that gets you some productivity wins in the prototyping stage. Though really I suppose \"comfort with a language \" is such a big factor, seeing people really belt out stuff in C is always a bit impressive to me (yeah yeah, bug filled etc) reply actionfromafar 18 hours agorootparentC forces me to do the barest minimum and avoid prematurely astronaut architect everything. That often makes it faster to prototype in. Not because I can do more. But because I have to reframe the problem so it fits. But large codebases in C are tedious to work with and slow to refactor. reply catlifeonmars 18 hours agorootparentprev> Is C++ easier to prototype in? Maybe if you’ve already got the toolchain set up and boilerplate templates etc. reply devnullbrain 19 hours agoparentprevIf the borrow checker slows you down, the C++ you write instead would just be buggy reply etyp 19 hours agorootparentSafe programs extend beyond those that Rust's borrow checker accepts though. There is more than one way to make a program safe, not all of them would be valid Rust. reply goku12 16 hours agorootparentThere are more than one way to achieve safety in Rust. Two more, infact - runtime safety and fully manual (unsafe with some additional work). Runtime safety is a very fast way to overcome the resistance offered by the borrow checker. While it does slowdown the code a little bit, that should be pretty fine for prototyping. A more careful use of runtime checks will still give you a program more performant than in most other languages. reply dzaima 13 hours agorootparentRuntime safety still comes with a good amount of complication source-wise, having to spam RefCell/Arc everywhere, and whatever derefs/.get()s they necessitate, and probably a different approach to passing references. Refactoring either that or unsafe away after being done with prototyping isn't trivial, if even possible without entirely rethinking the approach. reply goku12 9 hours agorootparentIsn't that a bit dramatic? How much extra code or care do you need to implement runtime safety checks? This isn't the first time I hear these sort of arguments either. It gives the impression that a crowd is making up one exaggeration after another to drive the perception that Rust is somehow not suitable for general purpose programming. reply dzaima 9 hours agorootparentIt is a bit dramatic, though less so for some approaches to programming. And of course prototyping is very different from general purpose programming. With Rust, if you really want the borrow checker to actually never bother you, you really do just have to spam the dynamically-checked wrappers everywhere; when in the groove, having to stop and wrap some existing field (or its holder) and all its usages just to be able to mutate it because you previously didn't think you'd need to is just completely awful if you don't find that \"fun\". Whereas in C, if you want to mutate a thing, you can always do so whenever with no concerns (whereas using unsafe to mutate a field of a &Thing in Rust is just UB, though maybe you can get away with using unoptimized builds for a bit) and deal with previous code that may have been written with the assumption it wouldn't change Later™ (but, importantly, still allowing properly testing the modified thing, and perhaps the entire project). Want to change the lifetime of a thing? Just move where you free it, no need to ever change lifetime args across a dozen functions. And in Java the most anything may take is removing a \"final\" on a field. reply tylersmith 19 hours agorootparentprevnext [8 more] [flagged] mkipper 19 hours agorootparentIf you remove \"develop\" from the OP and stick to \"prototype\", it's a totally valid criticism, and you come across as a condescending jerk if you suggest that software can't be \"working\" unless it's bug-free. I can't count the number of times I've wanted to try out some library and whipped up a quick prototype app in an hour or two to play around with it. I don't give a damn if that app is memory safe, handles signals safely, satisfies arcane aliasing rules or deals with any of the other million footguns in C/C++. I'm happy if it compiles and does what I want it to. I deal with that stuff when I inevitably rewrite it from scratch the next day and have an actual design in mind. FWIW, I'm comfortable enough with Rust that I would personally choose it over C or C++ for most stuff like this these days since the standard library makes a lot of boilerplate prototyping stuff (e.g. setting up a project, pulling in dependencies, handling files and I/O) much more pleasant. But suggesting that anyone who writes unsafe C/C++ in any context doesn't know what they're doing is ridiculous. reply truculent 18 hours agorootparentWhat trade-offs are you typically evaluating with the prototype? Have you ever found that decision harder to make because of shortcuts etc taken during prototype? reply mkipper 18 hours agorootparentLike everything, “it depends”. I used to work on a product that handled video, and I would regularly spin up prototypes where I’d stitch together gstreamer pipelines with different plugins to try things out. For reasons that aren’t important, it was easier to use an app linked against the gstreamer libraries than to use its CLI. There were all sorts of ways I’d evaluate some plugin I was trying out. Maybe I’d look at some test pattern for artifacts. Maybe I’d measure latency. Maybe I’d profile its resource usage. But I can confidently say that in all of those cases, the memory safety of the 100 lines of C++ code that told gstreamer how to plug things together had absolutely no impact on what I was evaluating. Sometimes the code you’re writing is basically unrelated to what you’re interested in from a prototype. reply catlifeonmars 18 hours agorootparentprevWorking software doesn’t always have to be correct or safe. This is highly use case dependent. Rust’s guarantees aren’t free, it comes with a handful of tradeoffs, such as learning curve, implementation speed, (this one is personally annoying) compilation speed, and portability. I’m a huge proponent of using the right language for the job. Sometimes rust is the obvious choice, sometimes it’s Python, Go, Lua, Java, Prolog, C, brainfuck etc. reply pdimitar 18 hours agorootparent> Working software doesn’t always have to be correct or safe. I feel like I am missing some very obvious point of yours because that statement in isolation I cannot agree with (I've read the rest of your comment and still can't find the extra context). Can you clarify? reply tylersmith 18 hours agorootparentprevnext [3 more] [flagged] suby 18 hours agorootparentYou are really being a jerk in this entire thread. Here is an 18,000+ word article on one domain where Rust is not a great fit. https://loglog.games/blog/leaving-rust-gamedev/ reply placebo 18 hours agorootparentprevWorking software and correct software are two different things and understanding that like in most things there is a tradeoff might be beneficial. reply AlotOfReading 19 hours agoparentprevIsn't that just D? No one uses D not because it's a bad language, but because the actual language isn't nearly as important for adoption as the community behind it. reply redman25 19 hours agoparentprevRust isn’t too bad if you throw clones everywhere. Otherwise, you might as well use a garbage collected language. reply Philpax 19 hours agoparentprevIf it's optional, people are unlikely to use it, and we'll be back at square one again regarding pervasive memory safety issues. That being said, keep an eye on Carbon. reply dralley 18 hours agoparentprevPrototype, sure. Develop? Debatable. At a certain scale of project, all the static guarantees become quite helpful. And a \"prototype\" written in Rust is often much closer to production-ready than you might think. Rust makes it easier to see which corners you've been cutting than most languages. reply stephen_g 19 hours agoparentprevApple is positioning Swift as this (and are also moving it from Apple's space on Github etc. into its own and gradually trying to build more of a community), so I wonder if it will catch on more. If people are already using it in the app space, it might be able to branch out into other areas. Over the last year and a bit they've also been working on strong C++ interop to be able to start using it in their own projects without having to do rewrites, and also better cross-platform and static linking support on Linux which could all make it a lot more attractive. (For context, like Rust, Swift is both memory safe and data-race safe) reply n144q 18 hours agorootparentSwift uses garbage collection (I know, arc, but that's still garbage collection), so it more or less is in the same category as Go. And I don't see people choosing Swift over Go any time soon. C++ and Rust are a different thing. reply mikestew 15 hours agorootparentSwift uses garbage collection (I know, arc, but that's still garbage collection) That’s simply not true, starting with ARC being compile-time and GC being run-time. reply linhns 18 hours agoparentprev> We need a better C++ (\"systems\") base language that could have an optional borrow checker or bounded model checker enabled. Are you describing Rust? reply PeterWhittaker 18 hours agoprevAnother commenter mentioned the NSA and safe languages, which is what prompted our move... ...and I have to say, Rust is fantastic. I was reading the O'Reilly book when I was doing some gnarly three-star C work and I kept thinking that it would likely be far easier and safer in Rust. We're now at the point where all new work is Rust and that gnarly, performant AF piece of C that just flies will be replaced with a simpler, safer Rust version. Another commenter mentioned speed of prototyping. That was a concern for me, too, but so far real life has shown me the opposite: prototyping is only slightly slower, mostly because I am having to learn various Crates instead of using system calls and their libc variants I've known for years, but incremental debelopment from proto to poc to mvp to product is faster because there is far less dumb at each step: if it built, it is safe(r) to extend. Maybe it is because so much of what we do is low level high performance system programming, I dunno, but, after 37 years, I may soon have invoked a C compiler for the last time. reply tschellenbach 19 hours agoprevcome to the land of no drama, no changes and always productive Go :) reply pdimitar 18 hours agoparentI have written no less than 20 hobby mini-projects with Go but it's hardly an alternative to Rust. They serve fairly different purposes. Sure you can write almost any project X with either language but most of the time Go or Rust would be a bad fit for project X and a near-perfect fit for project Y. reply stackedinserter 18 hours agorootparentWhat kind of projects that you can realistically be involved in, fit Rust but not Go? reply pdimitar 18 hours agorootparentHighly loaded network backbones come to mind. At one point almost every GC language starts to struggle. I've written Rust services in financial companies and it maintained stable memory footprint and near-nonexistent higher P95 latency... for months without restarts. reply throwaway2037 15 hours agorootparentYour second paragraph: I expect that this must be a microscopic part of their total source code footprint. Is the software used for (equity/futures/options) exchange connectivity? (Oh no, please don't tell me \"crypto\".) I cannot think of anything else that would warrant the investment in the 2020s. reply pdimitar 6 hours agorootparentDepends what you mean by microscopic. If the entire project is a traffic forwarder, proxy, firewall-like, dispatcher / router etc. then that's the heart of the project and having the performance benefits of Rust there is absolutely crucial. It was not only crypto. We're also talking telecom nodes and there are a lot of different protocols and needs around them (I was surprised how many, and some telecoms are finally waking up to the 21st century and started utilizing proper containerization and virtual networks to enforce compartmentalization / isolation / security). On the broader topic: nowadays I default to Golang due to its better productivity and shorter dev iteration cycles, for what it's worth. However, there are projects where picking Golang over Rust would be irresponsible. Not the majority of all projects out there, absolutely, but there is an important subset that cannot be ignored. PERSONAL NOTE: Sadly most of us will never work on those, and I am trying hard lately to change that for myself. Sick of web dev, I started hating it so much that I can do it extremely efficiently and almost with my eyes closed but I still can't forever ignore my needs; we are creative creatures and forcing ourselves to be assembly line workers only leads to spiritual and mental health death. reply galleywest200 18 hours agorootparentprevI was thinking about this earlier today. I decided, albeit probably in a manner way too naiive... use Go if you want to write an app and use Rust if you want to write a program. reply pdimitar 18 hours agorootparentI don't have your context so I'm not sure what you mean but in general, it's rather easy to write a web backend with Go, yes. And Rust is a very good fit for many types of servers. reply __turbobrew__ 17 hours agoparentprevI can tell you that the website for a very large company with billions in revenue has gone down several times due to nil pointer errors in golang. I love golang and it is my language of choice but the type system still lets you shoot your foot off sometimes. reply tail_exchange 19 hours agoparentprevI love Go, but they are not really interchangeable. If you do systems programming and you need a very low-level language, Go is not a good alternative. Though if we are talking about more high-level applications like web servers, then I totally agree. reply TylerE 18 hours agorootparentI could argue that at least 90% of software does t need a very low level language. reply goku12 16 hours agorootparentThat 90% is moot if you're trying to replace C++. Rust is a better fit than Go. Granted that Go iteration times are better. But Rust is by no means just a low level language. The abstractions in Rust are fantastic. They feel very ergonomic and high level. I think the real problem with Rust is that many people can't come to terms with the borrow checker. It's whole another skill set to know what the BC is trying to achieve, how to solve BC errors and the alternatives available when you can't. Once you know that though, Rust feels very productive - even extremely helpful in resolving problems in advance. reply TylerE 15 hours agorootparentBelieve it or not it’s possible, even desirable, to use an appropriate solution to the problem at hand, rather than treating everything as a nail just because you’re find if your hammer. reply goku12 14 hours agorootparentThat argument is selectively and rather condescendingly applied to Rust far too often. In this particular context (both the parent comment and the article), that criticism should apply more to Go than to Rust. Besides, I don't understand the argument of asking everyone to learn a dozen different languages for an 'appropriate solution at hand' when the differences between them don't justify such effort. I don't dislike Go. But it's disingenuous to argue that Rust shouldn't be used where Go can be. reply arccy 6 hours agorootparentperhaps it's applied to rust more often because rust evangelists are... louder. reply neonsunset 17 hours agoparentprevBesides few areas, Go is a backwards language. I'd rather use F# or Kotlin. And for performance with GC escape hatch it's borderline impossible to beat C#. reply rezmason 19 hours agoprevI don't think the goal of Rust has to be to unseat C++, just like the goal of Macs is no longer to unseat Windows. Here are some nobler goals: * be a language that users and non-users believe is better * be a language that people learn things in before entering the industry * be a language for solving important problems, if not every problem * if a billionaire buys C++ and makes it terrible, be an alternative Edit: unrelated— I really struggle with writing lists on HN. :-( reply pdimitar 18 hours agoparent> I don't think the goal of Rust has to be to unseat C++ Agreed, but it's also true at the same time that projects you would choose C++ for are also very legitimate candidates for Rust. So they are competing, even if that's not the goal of both communities. > be a language that users and non-users believe is better Well, lately one person that contributes Rust to the Linux kernel got attacked with ridiculous screaming like \"you will never force us to use Rust!\" -- and he never did that. So he (or was he somebody else? can't remember now) basically stepped down because it turned out that nobody could challenge his technical work so they started attacking him on every other platform. Inevitably, you get enough of this and want to be left alone. Sad... but that's humans apparently. As for non-users, even more difficult. But having studies like those of Microsoft and Google that clearly demonstrate that between 60% to 75% of all CVEs are due to memory unsafety, helps a lot. > be a language that people learn things in before entering the industry That one is needlessly difficult because a lot of professors are stuck in the early 2000s. Read: they only know Python or JS or Java or C# and they will never learn anything else. Young minds are impressionable so these fossils only perpetuate the problem of indoctrinating people and preventing them from utilizing the innovations in the PL area. > be a language for solving important problems, if not every problem Too generic to work, I am afraid. :/ You'll find people on HN arguing that you can solve every problem in the CS area just fine with Brainfuck and that we are all brainless lemmings for not seeing the light. Now replace that language with literally every other and you'll still find people saying the exact same thing... > if a billionaire buys C++ and makes it terrible, be an alternative That one is not so bad IMO, be it Rust or any other language really. If this happens the community will still push to do things like they want to, ultimately resulting in a fork if the buyer is stubborn enough. No money in the world can make people suddenly write terrible code if they are passionate about doing the opposite. reply rezmason 17 hours agorootparent> You'll find people on HN arguing No I won't!! Sorry. reply librasteve 19 hours agoprevgoodbye raku, it’s back to perl reply dysoco 20 hours agoprev> The majority of the Rust programming jobs asks primarily for deep knowledge in specialized technologies: cryptocurrencies/blockchain, finance trading, machine learning/data analysis, obscure network protocols, cybersecurity, etc. I agree, but that's a lot of fields and C and C++ jobs ask for the same: finance trading, videogames, machine learning, electronics, legacy protocols, etc. So I don't see how this is unique to Rust but doesn't apply to C++. Yeah I see very few and far between job offers for Rust but I can say the same for C or C++, and new companies are using Rust not the former languages. Maybe it's easier in the USA. Usually C or C++ jobs ask for embedded knowledge, whereas Rust ones don't. reply ndiddy 19 hours agoparentWhat country are you looking in? Everything I’ve seen (US) is like the linked post describes. There’s a fair amount of C or C++ jobs, while the Rust ones are all either crypto startups or extremely specialized (5 years professional rust experience plus 10 years professional Linux kernel development experience, or 4 years professional rust experience plus 8 years compiler development experience, stuff like that). reply ribadeo 10 hours agoprevMeh, I'll take his closing advice. If this were written in 2018 i might agree. Rust is currently exploding, however. C++ is footgun central, which is the opposite of typesafe. Go doesn't care about typesafety, and had it's heydey already. Rust is eating everything in 2024. People have begun to appreciate an approach to maintaining comprehensive typesafety from the db to user input. Rust is capable of holding much info about what it specifies, and is basically lossless as a specification. Rust is already eating the world, it just took a couple of decades of committee bikeshedding to achieve it. People are starting to understand why typesafety matters, as they flail around in bug soup. reply skobes 20 hours agoprev> And the problem with Rust is that it just doesn't have critical mass and, frankly, I don't think it will ever have. Isn't this premature? Rust is still pretty new. reply causal 20 hours agoparentIt is premature. And furthermore, adoption will mostly be driven by new projects written in Rust, not conversion of C/C++. That takes time. reply emn13 19 hours agorootparentAt the risk of dating myself, I suspect the author of that self-described rant either has forgotten how long it took for C++ to mostly dethrone plain C, and perhaps practically more relevantly, how painfully long it took for new versions of C++ to be widespread enough that people dared to rely on them. This stuff isn't easy at scale; adoption is slow. Doesn't mean rust will ever grow like that, but merely slow adoption doesn't sound like a death knell by itself to me. reply qayxc 19 hours agorootparent100% this. It's still absurd to me how codebases in some industries are still basically C++98 and haven't even begun to transition to at least C++17 (which forces new code that adds or changes features to be written C++98-style as well). Hard to imagine those companies switching to a different language anytime soon. Maybe the new US government push towards \"memory safe\" languages will help with that. reply orbat 20 hours agoparentprevIt's not that new; nearly 10 years old now I think. Go's adoption was faster reply dysoco 19 hours agorootparentI think Go thrives in a much more popular field (web services, etc.) and it's also designed to be easy to pick up so I'm not surprised this was the case. reply rahkiin 19 hours agorootparentprevIsnt the first stable language version from 2018? reply estebank 18 hours agorootparent1.0 was in May of 2015, but that was closer to an MVP with stability guarantees than the full language. It wasn't until 2018 that Rust was as usable for most tasks. Everything you might have learned back then is still available, just a lot of restrictions have been removed, and there are more to come. reply steveklabnik 19 hours agorootparentprevMay 2015. reply dotancohen 20 hours agoparentprev> Isn't this premature? Rust is still pretty new. The old lady said both those things to me this week. In two different conversations. reply dmitrygr 13 hours agoprevFor personal projects, I use C because it is fun and brings me joy without getting in my way, while Rust brings me pain, and (when i must seek help) derision from the community. Here, I choose fun and joy. For work I use C because that is what all existing non-toy kernels are written in and will continue to be written in until long after i've retired, been buried, and decomposed. Here, i choose what employers want and will pay for. reply jksmith 18 hours agoprev\"There is an huge junkyard of technologies that failed to gain broad acceptance, many of them far more revolutionary than Rust (e.g.: Lisp, Smalltalk). I don't see why those technologies' story can be avoided.\" Yeah, but I think more importantly much of the value that Rust brings would have been available 30 years ago if language development/selection wasn't so siloed, full of biases, and driven by (often undeserved) popularity. reply bobajeff 2 hours agoparentAs someone who is fascinated with Smalltalk and to a lesser extent Lisp. I think the reason those didn't catch on more broadly is that they were too early and different. Smalltalk I believe was designed to run on computers that wouldn't be around until the 90s and and when the 90s arrived it didn't have a big company pouring gobs of money into marketing it unlike Java. There are other factors that also contributed it's unpopularity. Like who wants to distribute a whole system for each smalltalk program they sell? I think also Lisp and APL weren't designed to run on the weak PCs in their day. You needed to use a timesharing system to program in them. reply joshdavham 18 hours agoprev> the problem with Rust is that it just doesn't have critical mass and, frankly, I don't think it will ever have Let’s check back in a couple of years! reply goku12 16 hours agoparentI think it already has. It just isn't very visible to us. Rust becomes easy (especially the borrow checker) once you know how the hardware works. This is unfortunately true for even non-system programming in Rust. However, those who deal with hardware - OS devs, web engine devs, DB engine devs, etc - can see the problem and appreciate what Rust offers. That's why we see big companies like Microsoft, Google, Amazon and even Apple adopt Rust quickly. It's just not visible to us on the surface. reply zb3 20 hours agoprevIf Rust was really that superior, would companies using it not have a competitive advantage in the market? reply n144q 19 hours agoparent> have a competitive advantage in the market In theory, yes, but there are two problems: (1) You need to either hire or train someone to write in Rust, and neither is trivial amount of effort. And at the end of the day, you need to push features out. Let's be real: you can find someone to write JavaScript to do full stack development, anywhere in the world, for a cheap price. Rust? The code may be more robust and easier to maintain in the long run, but it's hard to do that unless you already set up a team for that. (2) Most software companies don't actually care that much about the language they use, the tech stack, or their infrastructure",
    "originSummary": [],
    "commentSummary": [
      "A Reddit user criticized Rust for its complexity and limited job opportunities, comparing it to Betamax and Esperanto, which are technically superior but not widely adopted.",
      "While some users appreciate Rust's strictness and safety features, others note its slower adoption rate compared to languages like Go.",
      "Despite these challenges, Rust is gaining traction in major companies like AWS and Microsoft, with some viewing it as a potential future replacement for C++."
    ],
    "points": 242,
    "commentCount": 284,
    "retryCount": 0,
    "time": 1732831332
  },
  {
    "id": 42272927,
    "title": "GIMP 3.0 is on the way",
    "originLink": "https://lwn.net/SubscriberLink/998793/6c8d00bd1b2a7948/",
    "originBody": "LWN .net News from the source Content Weekly Edition Archives Search Kernel Security Events calendar Unread comments LWN FAQ Write for us User: Password:| Subscribe / Log in / New account GIMP 3.0 — a milestone for open-source image editing [LWN subscriber-only content] Welcome to LWN.net The following subscription-only content has been made available to you by an LWN subscriber. Thousands of subscribers depend on LWN for the best news from the Linux and free software communities. If you enjoy this article, please consider accepting the discount offer on the right. Thank you for visiting LWN.net! Special discount offer Subscribe to LWN now at the \"professional hacker\" level for at least six months, and you will receive a special discount of 25%. November 28, 2024 This article was contributed by Roland Taylor The long-awaited release of the GNU Image Manipulation Program (GIMP) 3.0 is on the way, marking the first major update since version 2.10 was released in April 2018. It now features a GTK 3 user interface and GIMP 3.0 introduces significant changes to the core platform and plugins. This release also brings performance and usability improvements, as well as more compatibility with Wayland and complex input sources. Modernized interface GIMP 3.0 is the first release to use GTK 3, a more modern foundation than the GTK 2 base of prior releases. GTK 4 has been available for a few years now, and is on the project's radar, but the plan was always to finish the GTK 3 work first. Moving to GTK 3 brings initial Wayland compatibility and HiDPI scaling. In addition, this allows for GIMP users to take advantage of multi-touch input, bringing pinch-to-zoom gestures to the program, and offering a better experience when working with complex peripherals, such as advanced drawing tablets. These features were not previously possible due to the limitations of GTK 2. A secondary result of the transition to GTK 3 is a refreshed user interface (UI), now with support for CSS themes included. In this release, four themes are available by default, including light, dark, and gray themes, along with a high-contrast theme for users with visual impairments. Additionally, this release has transitioned to using GTK's header bar component, typically used to combine an application's toolbar and title bar into one unit. To maintain familiarity with previous releases, however, GIMP 3.0 still supports the traditional menu interface. When GIMP 3.0 is launched, users will be greeted with a new welcome screen, seen on the right. This dialog presents various useful links for the GIMP project, such as tutorials, documentation, and donation options. It also provides users with quick functions for setting up GIMP as needed before starting their work. This includes the option to choose GTK and icon themes, and for placing the menu in the title bar, saving vertical space. In the \"Create\" tab of the dialog, users can create a new image, select from their recent work, or open an existing image from the filesystem. The welcome screen can be disabled if desired. Better workflow, performance, and color management A major focus of this release is greater integration with the Generic Graphics Library (GEGL), first introduced in 2000 for the purpose of improving GIMP's image-processing capabilities through a scene-graph-based architecture. As part of this effort, there have been numerous optimizations to GIMP's core and to its standard plugins. In tandem with memory management and multi-threading improvements, these changes should bring significant speed boosts when applying filters and effects, even on larger images. GEGL allows image-processing operations to be chained together in such a way that the original image data is preserved, along with a record of every edit. This is referred to as non-destructive editing, and GIMP 3.0 is the first stable release of the project to make this workflow available, though there is still more work to be done. Users can apply filters and effects to any layer without altering the original image. As a result, effect parameters can be changed even after they've been applied. This change removes the need to perform an undo any time a filter or effect does not have the desired result. Filters, and any plugins that use GEGL operations, now offer real-time previews. GIMP 3.0 continues efforts to improve color management under the initiative referred to as \"space invasion\", and delivers significant color-correctness results. This will improve the consistency of color reproduction across different devices and workflows. The foundation of these improvements is the babl library, responsible for handling pixel-format conversions and color-space management. Color profiles are now automatically managed when opening files with an included color profile. In the 2.10.x series, manual intervention was required when loading these files. GIMP 3.0 supports palettes outside of the \"Standard Red Green Blue\" (sRGB) range, such as \"Cyan Magenta Yellow Key\" (CMYK) and (CIELAB). This expanded color support, especially for CMYK, is essential to those who work with print and desktop publishing. However, GIMP continues to use sRGB, grayscale, and indexed colors for storing color information internally for now. Conversion to other color spaces is done on output, where necessary. Version 3.0 delivers CMYK output for some file formats, specifically JPEG, TIFF, and Photoshop (PSD). When working with these color formats, users often need to use soft-proofing, a digital preview of how colors will look when printed, helping ensure color accuracy in the final output. GIMP's 3.0 soft-proofing workflow has been improved, and it saves this information in its XCF format, preserving settings between sessions. Layers and file-format improvements The layer workflow has been upgraded in GIMP 3.0, with new features that bring greater parity with other advanced photo editors such as Adobe Photoshop and Affinity Photo. Layer operations can now be applied in bulk, and multiple layers can be selected and grouped. Layers can even be moved, reordered, duplicated, merged, and deleted en masse, whether or not they are contiguous within the layer dialog. Each layer now displays a distinct \"Fx\" icon to represent when filters and effects are in use. Filters and effects can be managed from a popover, which is opened up by clicking on this icon. The developers intend to further improve the layer-effects workflow with tighter integration of these features and a better UI. In future releases, GIMP's non-destructive functionality will be extended to layer masks, and layer channels as well. For users who wish to use a destructive workflow (where filters are applied to the original image data), there is a \"Merge Filter\" option available in the filter dialog. Another new feature in GIMP 3.0's layer workflow is auto-expanding layers. Layers may need to extend outside of the canvas in order to preserve parts of an image that need to later be moved or transformed, or within animations, where part of the animation needs to be made invisible temporarily. An auto-expanding layer will extend beyond its borders when painting outside of the canvas, though the canvas itself will retain its size. To enable this feature, users will need to check the \"Expand Layers\" option on any brush tool. Layer masks can be set to expand with the layer; the \"Align And Distribute\" tool has been completely reworked to recognize the layer's contents and not just its boundaries. Improvements to guides, snapping, and the text editor also help make GIMP's layer workflows more appealing in a way that will be familiar to users of other tools in this space. This update brings improved support for many image formats. PSD images now preserve their layer order on import, and Paintshop Pro images (PSP) preserve several features, including International Color Consortium (ICC) profiles, grids, and guides. In a crucial bug fix for working with GIFs, GIMP 3.0 detects whether these files contain animations, and correctly saves animated GIFs when overwriting existing files. New image formats have been added, with import and export supported for each format. The highly versatile SwatchBooker palette format, SBZ, is now supported, bringing parity with Scribus and Krita. Unlike most other palette formats, SBZ files can save complex details such as layouts, textures, gradients, named colors, color-space information, and even multiple palettes in a single file. For processing raw images (formats that preserve unprocessed image data), GIMP has long relied on external applications such as darktable and RawTherapee. However, recent changes in darktable's API broke this integration, causing GIMP to be unable to detect and recognize if it was installed. To rectify this issue, dedicated darktable integration is now available thanks to a collaborative effort between the two projects. This update introduces a new extension system and file format, GEX, allowing for easier distribution of plugins, themes, brushes, and other means of extending and supplementing the core application. This extension system even allows for multiple features, such as plugins, brushes, and themes, to be packaged and managed simultaneously. For example, a project such as PhotoGIMP could use this system to completely transform GIMP's standard functionality with a single package. In addition, an extension manager has been developed and is available in this update. It's not immediately exposed to users, however, because the backend infrastructure for distributing extensions is still under development. To access it, users will need to use GIMP's command search, which can be found in the menu under \"Help>Search and Run a Command\". Once extension support has matured, it will allow managing other features on the fly. Future developments Apart from the myriad user-visible features, several under-the-hood changes will either land in this release or come to fruition in near future releases. GIMP 3.0 contains the infrastructure for non-destructive layer types (such as vector layers and link layers), animations in the core application, multi-page files, and native support for working in color spaces outside of the RGB range. The changes in GIMP 3.0 have necessitated an API break so it is no longer compatible with plugins from older releases. To lessen the impact, the developers have decided to couple this shift with the incompatible changes of switching to GTK 3. Because of this, the development of the 2.10.x branch has continued, with some features being backported from GIMP 3.0 during this development cycle. Future releases within the 2.10.x branch are still possible, but unlikely, with the foundation of the 3.x series reaching maturity. Do note, however, that most of GIMP's existing plugins will need to be updated to support the new API, which is expected to provide developers with the benefit of increased functionality and improved performance. These are just some of the changes coming to GIMP in version 3.0. Exactly when GIMP 3.0 will be released remains to be seen, but users interested in trying the new features before release can download development builds; updates are regularly posted on the GIMP web site. The first release candidate has already landed on November 6, following the project reaching string freeze on the main branch. So long as there are no major bugs or regressions, the final release will be built from the release candidate. However, additional candidates may be necessary if any major problems are found. The developers have already announced that subsequent releases should land faster, with smaller, feature-focused releases providing more stability. Overall, GIMP 3.0 represents a big step forward for the free photo and graphics editor, and sets the stage for even greater improvements to come. Index entries for this article GuestArticles Taylor, Roland to post comments Dialog button placement Posted Nov 29, 2024 14:26 UTC (Fri) by rrolls (subscriber, #151126) [Link] (3 responses) I love GIMP, and it looks like they've implemented a lot of features of the kind I'd always pondered over while using GIMP 2. As ever, however, I cannot stand modern GNOME. While I definitely appreciate the news that GIMP 3 can be told to use a proper (traditional) title bar and menu on its main window, does the option also exist to force _all_ windows to have a proper title bar, and to force OK/Cancel/etc. buttons to be at the bottom in a reasonable order? The screenshot of the \"Softglow\" dialog illustrates the problem well - the only thing wrong with it is the position of four buttons and the total height of the window, but this alone makes it a total mess. \"Cancel - Help - _____ - Reset - OK\" is about the silliest button order I've ever seen. \"Help - Reset - _____ - Cancel - OK\", please. Or swap \"Help\" and \"Reset\", and/or swap \"Cancel\" and \"OK\", and/or move the gap one button to the left in this lot. Or center the lot and have \"OK - Cancel - Reset - Help\". The important thing is for \"Cancel\" and \"OK\" to be at either the bottom right or bottom center, and to be adjacent to each other. Writing this, I'm suddenly realising why the UI in Factorio (one of my favorite games!) always very slightly bugged me ever since the last big rejig they did prior to 1.0. It's because while they put their \"OK\"-type buttons at the bottom right, they put their \"Cancel\"-type buttons at the bottom left. Somehow I'm now happy that I've figured this out. Silver linings! Am I opinionated on this particular subject? Yes. But I can't be the only one, can I? Dialog button placement Posted Nov 29, 2024 15:11 UTC (Fri) by serafean (guest, #121786) [Link] > does the option also exist to force _all_ windows to have a proper title bar Yes : use a window manager that enforces such things. You might end up with windows with 2 titlebars, but for me, it's worth it. I use kwin + window rules. Dialog button placement Posted Nov 29, 2024 16:04 UTC (Fri) by lunaryorn (subscriber, #111088) [Link] (1 responses) I'd like to point out that the dialog in question is not a standard Gnome dialog and doesn't use Gnome's design patterns. Gnome's HIG does not define dialogs with multiple extra buttons in its header bar as a design pattern. Gimp designed and implemented this dialog, not Gnome. The dialog isn't great, but that's just not Gnome's doing. Good contemporary Gnome applications would not use a dialog like that. Now, you're certainly not the only one who dislikes Gnome's UI, but what's that to say? There are also a lot of people who like it. Like, perhaps, all those people who spend their time writing applications for Gnome ;) Dialog button placement Posted Nov 29, 2024 17:42 UTC (Fri) by rrolls (subscriber, #151126) [Link] Ah, fair enough - I guess I can't blame this one on GNOME, then! Which is good news, because if it was GIMP's decision, then it's more likely they might change it to something more palatable later. (Or maybe they already have - the RC1 post has a screenshot of a \"Smooth by Domain Transform\" dialog that has the buttons sensibly placed like in GIMP 2, but I'm not sure what their level of consistency between dialogs is.) I'm not _intimately_ familiar with GNOME's design guidelines, but it's still true that whenever I see a screenshot of something from GNOME 3 onwards, I don't like what I see going on with the header bar. So I certainly made an assumption here about the rationale behind that button placement - my apologies. Classic Icon Theme Posted Nov 29, 2024 14:32 UTC (Fri) by rrolls (subscriber, #151126) [Link] To add a little more positivity to balance out my previous comment, though - one thing I'm _very_ excited and happy to see, skimming through the RC1 newspost, is that they're fully vectorising the currently-so-called \"Legacy\" icon theme with a view to renaming it as \"Classic\". Switching to \"Legacy\" icons is one of the first things I do when installing GIMP (along with taking it out of dark mode), and the montage in their newspost of what these icons will look like moving forward is absolutely beautiful. Some distros packaging the 3.0 RCs Posted Nov 29, 2024 15:31 UTC (Fri) by dowdle (subscriber, #659) [Link] Checked what version I have on Fedora 41 and it is currently 3.0 RC1. Even EL 9 has 2.99. Flathub appears to only be offering the 2.10.x series. I'm a Patreon subscriber for two GIMP devs and ZeMarmot team had a post yesterday about the upcoming 3.0 RC2 build. They have done some bug fixes but also more new features with API fixing with regards older-plugin compatibility. Overall, I'm very excited about this upcoming major release and am glad I am already using RC1 on Fedora. I'll continue to financially support the development of GIMP and encourage others to join in. Copyright © 2024, Eklektix, Inc. Comments and public postings are copyrighted by their creators. Linux is a registered trademark of Linus Torvalds",
    "commentLink": "https://news.ycombinator.com/item?id=42272927",
    "commentBody": "GIMP 3.0 is on the way (lwn.net)230 points by chmaynard 8 hours agohidepastfavorite141 comments vanderZwan 5 hours ago> GIMP 3.0 supports palettes outside of the \"Standard Red Green Blue\" (sRGB) range, such as \"Cyan Magenta Yellow Key\" (CMYK) and (CIELAB). This expanded color support, especially for CMYK, is essential to those who work with print and desktop publishing. However, GIMP continues to use sRGB, grayscale, and indexed colors for storing color information internally for now. Conversion to other color spaces is done on output, where necessary. The wording here is confusing, because it makes it sound like CMYK/CIELAB will only be applied at the very end of the image transformation pipeline. That would really limit the usefulness of adding these extra color spaces, since doing the manipulation in a different color space than RGB is often the point. But the linked blog post on GIMP.org[0] words it a little differently: > What it means for color correctness in particular is that we will now do color conversion only when needed (last-second conversion) and therefore won’t lose information when it could have been avoided. For instance, say you color-pick color from an image: if we were to convert to an intermediate format, before using it on a second image (which may or may not be in another color format), we’d do 2 conversions. Which means more possibility of precision loss. The issue is even more flagrant if the input and output formats are the same (i.e. no conversion should happen at all). And this will be even more a problem when we will have core CMYK backend (we really want to avoid doing a round-trip to an intermediate format with CMYK, which doesn’t have bijective conversion with most other color models, even when working unbounded and ignoring precision issues). That sounds more like the information of the original color space will be kept as well, and transformations will be applied only when necessary, to avoid lossy roundtrips. Which is not quite the same. [0] https://www.gimp.org/news/2024/02/21/gimp-2-99-18-released/#... reply chongli 3 hours agoparentThis doesn’t make any sense to me. If I have 2 CMYK images open I should be able to colour pick and copy/paste between them and do any other sorts of manipulations with them without any colour space conversions taking place. The only colour space transformation that should happen when working with a CMYK image is when the image is displayed on screen. The CMYK data is interpreted with the attached colour profile (perhaps provided by the commercial printer I’m using) and then the colours are converted to RGB via my monitor’s profile and displayed on screen. None of these converted colours are ever saved in the file, and they need to be updated whenever any part of the image is altered (say using a dirty region -> repaint system). Now if I do happen to open both a CMYK image and an RGB image and then try to copy and paste part of the RGB image into the CMYK one then a one-time conversion to CMYK needs to take place. Otherwise if I’m working only with CMYK images then no conversions should take place. It sounds to me like the GIMP may have been written so that all of its operations are specialized to work with RGB pixels and so they cannot implement native editing on CMYK images without doing round trip conversions all over the place? If that’s the case then they need to buckle down and do the hard work of rewriting everything to be colour space independent. I also want to note that Photoshop added CMYK support in version 2.0 which was released in 1991. This was before they even added layers! Photoshop was essentially designed for print almost from the very beginning. Having all the colour space stuff figured out before adding huge numbers of features was a major advantage. Trying to retrofit CMYK support into the GIMP seems like a bit of a nightmare. reply cmyk_student 2 hours agorootparentHi! That's actually what we were doing for GIMP 3.0. Originally, GIMP stored pixels in structs that just contained the pixel values (like GimpRGB's r, g, and b values). Now all pixel data is stored in GeglColor objects, which contain the color model, space, and profile information in addition to pixel values. So you can just request, say, the 8bit CIE LAB or 16bit CMYK version of the pixel and retrieve it from the object. I made a proof-of-concept CMYK mode for GIMP before this conversion was done - it would be even easier now. I hope to return to it and implement in GIMP proper in a near future release. reply lastdong 7 hours agoprevFinally updated UI! I really hope it inspires more updates and brings in even more people, just like when Blender revamped their UX a few years back and saw an amazing boost in popularity. reply rkharsan64 7 hours agoparentDoes this change include any UX improvements? The article only mentions updated visuals and theming. From the discussions I've read, it's the UX of GIMP that holds it back. reply cmyk_student 2 hours agorootparentI'd say so. Non-destructive editing means you don't have to Ctrl-Z over and over again when you want to change a filter, which is a better user experience. Same with built-in text outline features, which makes that process much easier than in GIMP 2.10. Multi-selection instead of the chain tools is another nice UX improvement. Not to say that there isn't more work to be done, but I think there's a lot of good work done by volunteers already. reply pessimizer 6 hours agorootparentprevIt's just Photoshop addicts needing the UI to be identical to Photoshop because when they use GIMP their muscle memory is broken. To be fair, though, all industry professionals are forced to be Photoshop addicts. But Photoshop's UI is objectively awful; it's the 10,000 hours you spent in it that makes it seem sane. You could have learned Thai in 10,000 hours, too. The real weaknesses in GIMP have been in its lack of some necessary functionality, especially some that is necessary for print. The great thing about being GPL is that when the stuff is eventually added, you own it forever. reply cameronh90 6 hours agorootparentPhotoshop's UX is poor, but everyone is used to it. GIMP's UX is even worse, and nobody is used to it. And based on those screenshots in the article, it has, if anything, got even weirder and less intuitive. I'd probably try and power through if there was even close to feature parity, but it's only just now catching up with where Photoshop was in 1994. reply KaiserPro 5 hours agorootparentGimp's UX was ok in 0.9, same with photoshop. GIMP was meant to be a drop in replacement for early photoshop, so it makes sense that it follows that UX idea. reply inferiorhuman 4 hours agorootparentprevIt's just Photoshop addicts needing the UI to be identical to Photoshop because when they use GIMP their muscle memory is broken. Nah. Sometimes I just want to edit something without having to do the export song and dance. reply somat 1 hour agoparentprevWith no disrespect to the blender team they are doing a bunch of good high quality ui work. As a longtime casual blender user(since the late 90's version 1.7 on irix) that was less a \"complete ui change\" than it was a \"we released a press release saying we did a complete ui change\" the biggest actual difference was to swap the mouse buttons. See, blenders ui was always good, it was designed as a professional tool, that is, designed to be used for many hours a day. it had a very quick and smooth operating interface flow. However it did have a reputation, deserved, as having a bit of a learning cliff. when your primary design paradigm can be best described as having a 101 button mouse. there is a bit of learning involved. but two things changed, one, blenders ui flow started being copied in other 3d programs and became more mainstream and acceptable. and two, the blender team kept working backwards, adding slower, but more discoverable paths to the tools. Now I am not a member of the dev team, actually not involved in the community in any way so I am probably wrong, but the way I see it, there was a sort of very sticky reputation blender had that it was \"too hard\", this was mentally blocking a lot of potential users, so the only solution was to loudly say \"hey we redid the whole ui to make it a lot easier\", coughs, adds dark mode. it is still just as \"hard\" as it always was, but because people think it is easier they will take the time to learn it. reply ginko 7 hours agoparentprevI hope you can still configure it to have all the different toolboxes in separate windows like before. Also who thought putting stuff like \"Cancel\" and \"OK\" into the title bar was a good idea?! reply squarefoot 4 hours agorootparent> Also who thought putting stuff like \"Cancel\" and \"OK\" into the title bar was a good idea?! That's one of the things I dislike more in GTK3. It seems dictated by the false belief that everyone is running software on a tablet screen in which windows don't need to be dragged around and open always at full screen, so the more space is used for widgets and input/output the better. reply exe34 5 hours agorootparentprevI hate this buttons-in-the-title-bar thing with a passion. It takes ages to find a safe place to click and drag a window around without accidentally causing something else to happen. reply blueflow 4 hours agoparentprevYeah, but now it has the GNOME/GTK thing. I internally threw up upon seeing the screenshots. reply skowalak 7 hours agoprevReally looking forward to see more non-destructive editing. For me this has been one of the major reasons not to use GIMP in the past. The integration of GEGL is a huge milestone for GIMP imho. reply peter-m80 7 hours agoprevis it possible to draw a circle without using a combination of 3 different tools? reply peter-m80 6 hours agoparentI found a simple 8-step guide to draw a circle: https://alvinalexander.com/gimp/gimp-how-to-create-draw-circ... Please gimp devs, fix the basics reply kookamamie 6 hours agoparentprevNo, that kind of technology is still beyond us. reply asddubs 6 hours agoparentprevor move a selection on a layer without having to know a 3-key combination shortcut reply mFixman 4 hours agorootparent\"M\"? reply rererereferred 4 hours agoparentprevUse Inkscape for that. reply BeFlatXIII 2 hours agorootparentI have to use Inkscape for rendering Devanagari text to import as SVG stroke paths in the Affinity Suite. Very annoying. reply peter-m80 2 hours agorootparentprevinkscape for pixel art? reply pjmlp 6 hours agoparentprevor any other kind of geometric form, withouth going into path land, nope. reply canistel 7 hours agoprevCMYK, finally. Hopefully this will lead to wider adoption, especially in the professional circles... reply rafaelmn 7 hours agoparentHow relevant is print these days ? Coming from that background it's depressing to see my former coworkers working in a shrinking market, and the only people in graphics I know that are making reasonable money are in digital. Not to say that it's irrelevant just that at this point it feels like it kind of missed the boat in this space. reply martin_a 7 hours agorootparent> How relevant is print these days? Still quite relevant, especially in some areas. Think about packaging and labeling, there's just not really a way around print in these areas. Besides that, digital print is the future. Print also needs to become clever and data-driven, more personalized and tailored to the recipient, but that's hard work. Example: Just last week I received a catalogue with the fall/winter collection of a larger clothing brand. I threw it away immediately. Lots of things in it that are not my size, or my style or whatnot. A personalized product would have helped. Pick articles similar to those I own (you got this data from my previous orders), only show articles that are available in XXL or larger (look at the sizes I kept and did not return) and that's it. \"Hey Martin, these are _your_ pieces for the winter season, enjoy!\" Maybe it's only 16 pages then instead of 50+ but it would have been a much better experience for me. Also cheaper to print and ship for the store but with a much higher value. But yeah, programmatic printing is hard(er) to do then order 100k catalogues from the cheapest shop you'll find. reply kookamamie 7 hours agorootparentI don't think anyone working in print would use Gimp, though? reply pessimizer 6 hours agorootparentThe reason they didn't use GIMP is because they couldn't use GIMP. It simply didn't have necessary capability. When I was working in prepress, I would have done anything to use GIMP. That desperation to escape Photoshop is why Affinity took off. If Inkscape could get a UI for precision positioning, something you could e.g. design an entry form in; and Scribus could polish up, I think a lot of people would move to a FOSS workflow. reply martin_a 6 hours agorootparentprevNaaah... For better or worse we're all set up with our Adobe Creative Clouds... reply glimshe 6 hours agorootparentThe Affinity suite - now for $80 due to Black Friday - is a viable competitor to Photoshop for many applications. It's much better than Gimp. reply martin_a 5 hours agorootparentThat's true, I got one myself, but as everybody is educated and trained on Adobe products switching is not that easy. reply em-bee 6 hours agorootparentprevwhy is that though? isn't that the bulk of the criticism? bad UX and lack of features needed for print? reply kookamamie 6 hours agorootparentI guess - of course it's a chicken-egg sort of a problem. No one's going to use it for print, before it has print-related capabilities - the same can be said for much of the UX, in general. reply carlosjobim 6 hours agoparentprevProfessionals have no problem purchasing their work tools, and no reason to use subpar tools. Edit: To the FOSS hackers who are down voting. You can buy state of the art professional image editing and design software for less than a hundred dollars. Deliver work to one client and you've paid for all your tools. Why would a professional waste their time with GIMP, when they can use all those hours working for clients with good tools and get paid? reply leephillips 5 hours agorootparentIt’s not the price, or even the fact that one has to pay. There are huge practical, security, and privacy reasons to never put closed-source software on your computer. reply carlosjobim 2 hours agorootparentSorry, not gonna buy into the paranoia that exists within every cult, including the FOSS cult. If your graphic design work has to be extremely secretive, then you'll do it on a machine without an internet connection. Do you think secretive companies like Apple or car manufacturers use GIMP for their graphical work? Of course not. We're speaking about professionals now. If they need to, they will use devices without any personal data for their work. And it's not like your average graphical designer can go through the trillions of lines of code in an open source environment to determine how secure it is. They're busy doing their job. reply mixmastamyk 1 hour agorootparentGiven enough resources, floss tools are often best in class. With your short-sighted attitude, Linux, Firefox, Blender, etc wouldn’t exist and we’d all be a lot poorer for it—fully controlled by the Oracles of the world without option. reply carlosjobim 1 hour agorootparentFOSS tools are usually worst in class for the actual user, with some exceptions. The exceptions are generally dev tools and sys admin tools. Linux is not best in class in anything unless you're a developer or a server admin. The free market is a much more effective process for getting quality tools for the rest of us. There's nothing short sighted about it. As a professional I pay a fair price for my tools and the developers get a salary so they can continue to improve it. In the sector of image editing and graphic design, Affinity is a perfect example: cheap and top quality. Open source software will not improve for any end user by more people using it. Only programmers can improve it, and only if they want to. Usually they don't want to, because the users are not paying them. reply hu3 3 hours agoprevI used GIMP earlier today. So thank you all involved! After so many years I keep going back to it for quick edits. reply mdaniel 1 hour agoprevI got excited because I thought the headline meant it was out, but no, the first sentence says \"3.0 is on the way\" but given this project's release cadence that sentence could mean any number of time units reply joaomacp 7 hours agoprevI use Gimp from time to time, and often get frustrated with its... unique UI. It's nice to see they're hearing feedback and working on it :D A tip for others that feel the same: if you've used Photoshop before and are used to its UI, try the free Photopea website. It's a Photoshop \"clone\" that works really well in web (I believe it's a solo dev doing it too). It's replaced Gimp for me lately. reply 0points 6 hours agoparentI would recommend Krita instead of a website. Websites are not automatically free or opensource, they also require internet access and can sneakily copy the files you are working with. If photopea is free today, it may cost money tomorrow. Krita exists for Windows and macOS too nowadays. https://krita.org/en/ reply cxr 4 hours agorootparent> Websites[...] can sneakily copy the files you are working with You have made one of the most baffling logical errors that commonly crop up when people criticize browser-based apps. Browser-based apps execute in a sandbox. They are more constrained in what they can do in comparison to a traditional program running on your machine. Any nefarious thing a browser-based app can do, a local program can do, too, and not just that, but they can do it in a way that's much harder to detect and/or counteract. There are good reasons available if you want to criticize browser-based apps. This is not one of them. reply em-bee 4 hours agorootparenti can remove network access capabilities from a desktop app after it is installed. i can't easily do that with an app running in a browser. likewise monitoring and detecting network access per application is easy. tracking down which browser tab is making which network connection is a lot harder. reply mardifoufs 2 hours agorootparentGo to the network tab of your browser's dev tools. It's literally easier than with a desktop app. reply cxr 3 hours agorootparentprevYou are either confused about something, or you're simply refusing to engage with reality. > Any nefarious thing a browser-based app can do, a local program can do, too, [or worse!] reply porridgeraisin 5 hours agorootparentprev> require internet access It's a PWA and works offline perfectly. reply burrish 4 hours agorootparentprevKrita is more geared toward digital drawing than image processing, I recommend Affinity Photo reply idoubtit 4 hours agoparentprevMany years ago, I lost my work because of this \"unique UI\" and pledge never to use Gimp again, unless its behavior changed. When you open a non-Gimp file, for instance a PNG, and you want to update the source file, you need to \"export\" to PNG. And if you close the tab, Gimp warns you that your work isn't saved, because it hasn't been saved in its native xcf format. There is no way to know if the work has been saved to the original file. At least, that was the behavior at the time. So I had opened a dozen of (versioned) PNG files, modified them, then overwritten the PNG files. On closing, Gimp warned me that none of the images was saved. I ignored the warning since I didn't want to track the changes in xcf files. It turned out one the files had not been \"exported\" to PNG. reply flufluflufluffy 2 hours agorootparentThis is standard behavior in pretty much any kind of art/content creation app. You have a project file which can be saved and reopened in the app, saving the state of the layers/effects/etc to be edited later, and can “export” a final render to a specific format for your medium. Image/video editing, digital audio workstations, 3D-modeling programs, they all behave like this, for good reason since it usually takes a long time to export to a specific format, and when you do, you lose the ability to change anything. Think of it like source code, and each exportable file type is like a compilation target. reply prmoustache 1 hour agorootparentprevThere is literally an \"overwrite file\" command in the file menu. You didn't lose data because of bad UI but because you are illiterate. You just said it, it warns you. If you can't understand what \"none of the images was saved\" means, there is no UI that can save you except autosave. But autosave is something you clearly don't want in a photo/image editor, even smartphone apps do not autosave photo edits. reply sam_lowry_ 5 hours agoparentprev> frustrated with its... unique UI It's a matter of habits. For me, Gimp is the primary image editing tool and all others feel alien. reply James_K 4 hours agorootparentThere's habits sure, but GIMP also just has a lot of bad UI. For instance if you insert text, you have to click exactly on the black region of the character to select the text. This is really awkward because it means when you click on a letter to try and move some text, sometimes your click will go through the hole in the middle of the letter and select the thing behind the text. Also worth noting that this update is the one allowing people to edit rotated text and it took 20+ years. This is really bad UI/UX. reply agumonkey 4 hours agorootparentprevThat's interesting. I have used and enjoyed a ton of software in different domains (from nothingreal shake to gnu ed) and so far gimp still wins the gold medals of triggering me. A rare feat. reply prmoustache 1 hour agoparentprevI've loaded photopea, krita and gimp side by side and really there are absolutely no major differences in UI. This is the kind of dismissive posts thrown by people who haven't used gimp since 1999 and keep repeating the same lies every gimp release. reply Mashimo 6 hours agoparentprevYes, even though I never use photoshop and used Gimp for over 15 years it's a frustrating UI. I dislike it. Non destructive editing is a big upgrade though. I also use Photopea from time to time. Can recommend. reply GaggiX 6 hours agorootparentIf only a mad man would make a Photopea/Photoshop clone open source, then everyone (who has the skills) would be able to not only use a decent open source image editor, but one that can be fully customized to your needs. reply mfld 6 hours agoparentprevI like using pinta for the \"easy\" cases. reply Sloowms 5 hours agoparentprevI don't use any photo editing tool but I know there is photogimp which makes gimp look like photoshop. reply cranberryturkey 6 hours agoparentprevHave you tried Krita yet? reply soulofmischief 7 hours agoprevI started using GIMP around 2003 as a small child, and I remember being hyped for 3.0 and non-destructive editing even back then. I'm in my thirties now. Slow and steady wins the race. Congrats to the GIMP team, can't imagine the catharsis they will experience when 3.0 officially drops. reply npteljes 8 minutes agoparent>Slow and steady wins the race. Not in software. I can't think of a single example of that. Slow doesn't win the race, it means that the dependencies will go out of date, and some of the work has to be repeated over and over to match the changing landscape. I do like GIMP though, it's my default image editor. reply teleforce 5 hours agoparentprev> Congrats to the GIMP team, can't imagine the catharsis they will experience when 3.0 officially drops. Thanks for the catharsis word, I have to look it up for the meaning. 20 years or equivalent to 5 times Olympics games, is a very long time to develop and improve a software. It's now comparable to the real-time Linux kernel, another open-source software albeit it's a kernel not a user application [1]. Any other open-source software that has a comparable development time that I'm not aware of? But as the old adage says it is better to be a tortoise than a hare, as long you're winning the game. [1] 20 years later, real-time Linux makes it to the kernel: https://news.ycombinator.com/item?id=41584907 [2] The Tortoise and the Hare: https://en.wikipedia.org/wiki/The_Tortoise_and_the_Hare reply weberer 4 hours agorootparent>Any other open-source software that has a comparable development time that I'm not aware of? GNOME famously had that file picker thumbnail issue open for 18 years. https://www.omglinux.com/gnome-thumbnails-file-picker/ reply rbonvall 3 hours agorootparentprevSince no other pedants have chimed in yet, I'm required to point out that 20 years is five olympiads, which is the timespan in between six Olympic games. reply DavidPeiffer 4 hours agorootparentprevBlender started ad in-house 3d modeling back in 1994, was open sourced in 2002, and continues with very active and sustainable development today. I used it most 2010-2012, and it has been incredible to see how much has happened since then. https://www.blender.org/about/history/ reply authorfly 4 hours agorootparentWhat's been added since then? I remember Blender back then. The days of watching squares slowly render over 2-7 minutes each for some cool project I made with lighting. Has much new been added to editing/functionality or just speed optimisations? reply __s 3 hours agorootparentBeing old grows ecosystem. Lits of progress in N64 decompilation, & behind that is https://github.com/Fast-64/fast64 for using blender in romhack development reply misnome 3 hours agorootparentprevA vast, vast array of features. Multiple new renderers, material systems (lumping node systems into this), 2D animation systems (mostly via grease pencil AIUI), video compositing features, sculpting (I think is new since then?) I mean, even if you aren't using it now, it's probably worth looking at e.g. the latest release notes https://www.blender.org/download/releases/4-3/ just to see the array of things improving and adding onto stuff that mostly didn't exist 12 years ago. reply medo-bear 1 hour agorootparentprev> Any other open-source software that has a comparable development time that I'm not aware of? Emacs. Free and open source. User software. 40 (50) years in development. Mythical reply amlib 4 hours agoparentprevI remember \"acquiring\" Gimp in 2000 from a CD-ROM magazine as a child and using that for a good while until an uncle gave me a pirated copy of Photoshop 7. I actually disliked using Photoshop because I was so used to the way of doing things in Gimp. Eventually I learned all the more advanced functions in Photoshop, specially the non destructive editing stuff, and couldn't really go back to Gimp. The muscle memory of using it eventually atrophied and nowadays I have a hard time using Gimp. All that said, I don't do much 2D/3D work nowadays, so I've been using Krita for almost a decade and it feels like a decent PS alternative, with a more similar interface... reply asddubs 6 hours agoparentprevnot sure if i would call taking 20 years for a release \"steady\" reply prmoustache 1 hour agorootparentThere have been many many releases between 2.0 and 3.0. And many releases in the 2.99.x branch specifically. Many other projects would have simply switched to a different versionning scheme and got rid of beta versions to simply iterate on releases like web browsers do nowadays. reply glonq 2 hours agorootparentprevCongrats to the GIMP team for being more spry than the HURD team ;) reply KaiserPro 5 hours agoparentprevI too used GIMP as a kid, but alas I am somewhat older. Its great to see it improving, and adding big things like CMYK reply chasil 4 hours agoparentprevI remember getting a copy of the Motif-based GIMP for AIX sometime in 1996 or so. reply verisimi 6 hours agoparentprevIndeed, you can't hurry v3. reply trumpeta 6 hours agorootparentangry Python noises reply albertzeyer 7 hours agoprevI find it a bit weird that Gimp does not use the latest GTK (i.e. GTK4, which was considered stable since 2020), even though GTK originates in the Gimp project itself. It actually seems to be quite a bit behind: This is now the first release of Gimp which started to use GTK3, i.e. before, it even still used GTK2 (reached end-of-life in 2020)? reply onli 7 hours agoparentHaving had to migrate a very simple project from GTK2 to GTK3, I don't think it's all that weird. The migration was utterly difficult, in the areas that hit me seemingly no effort was made to give proper migration paths. Only with some later published documentation (+ help from chatgpt) was it possible to restore some functionality later, after the initial migration. Finally that even meant calling the xlib directly. And note that the software used wxWidgets, so most of the changes were encapsulated there. Only a very small part of GDK/GTK was used directly, with wnck already used as a helper layer (but the functionality in question broke there as well). So even if GTK came from GIMP, if the later development in GTK was not made specifically for and by the GIMP project, the migration must have been a nightmare. Especially in a project that had so many other things to worry about, non-destructive editing alone. And to repeat such a migration now again for GTK4 will not be very enticing. reply mid-kid 7 hours agorootparentFrom what I've heard surrounding this, GTK3 to GTK4 isn't as big of a jump as GTK2 to GTK3 was. The GTK3 port was finished first because there was already work in place for that, but we can expect a GTK4 port to be faster. That said, I haven't seen many apps that aren't specifically GNOME apps start using GTK4 in the first place, and as such I'm currently not using any GTK4 applications. I expect it to take a while before more things move to GTK4. reply creshal 7 hours agoparentprevGTK stands for \"Gnome first every other user literally does not matter break them hard, break them often, make them give up ToolKit\" these days, and has for quite a while. reply sirwhinesalot 7 hours agoparentprevGTK4 removed a bunch of APIs for stupid reasons and GIMPs move to 3 started before 4 even existed. Had 4 at least tried to maintain some degree of compatibility then switching from 3 to 4 near the end would have been feasible. But that's not the case. If you aren't Gnome, GTK is not for you. reply kombine 3 hours agorootparentMight as well port the next version of Gimp to Qt - they seem to be much more reasonable as API providers. reply sirwhinesalot 2 hours agorootparentQt also introduces some messy changes between major versions but they tend to be a lot more reasonable than what GTK does. Since Qt4 at least there's usually some attempt at providing alternatives for each deprecated function or class (the alternatives are shown in the error messages) and sometimes full-on compatibility layers for missing features are provided like with Qt5Compat. Not sure Gimp devs would be willing to switch to C++ for Qt though. reply heavensteeth 4 hours agorootparentprevI feel this. I tried using GTK4 a little while ago and almost immediately switched library when I realised it's simply incapable of doing certain things I needed, usually because either Wayland can't do it or GNOME doesn't need it. reply kuschkufan 6 hours agorootparentprevGuess what the \"G\" in GTK stands for historically reply pjmlp 6 hours agorootparentThat was before GNOME took over, as it was supposed to be the answer against KDE and original Qt license. And here we are, having written articles about Gtkmm to The C/C++ Users Journal, I no longer care and run systems from Microsoft/Apple/Google instead. reply mattl 5 hours agorootparentI think Harmony was the Qt replacement but I don’t think it lasted very long. reply pjmlp 3 hours agorootparentThat was after Nokia acquisition from Trolltech. https://blogs.kde.org/2009/01/14/cute-harmony-qt-goes-lgpl/ reply mattl 3 hours agorootparentDevelopment ceased at the end of 2000, when Qt was released under the GPL, removing the perceived need for the Harmony Project to exist. In January 2009 Qt itself was made available under the GNU LGPL, along with the previous license options. reply skerit 7 hours agoparentprevTrue but the leap from GTK2 to GTK3 is a lot bigger than from GTK3 to GTK4. I'm not sure when the \"port\" to GTK3 started, but if it was from before GTK4 was a thing, it makes sense that they wanted to finish the GTK3 stuff first. reply oblio 7 hours agorootparent> I'm not sure when the \"port\" to GTK3 started Probably more than 10 years ago. reply themerone 3 hours agoparentprevGTK has become the Gnome toolkit , and the Gnome developers don't care about developers outside their umbrella. reply soulofmischief 7 hours agoparentprevFrom the second paragraph of the article: > GTK 4 has been available for a few years now, and is on the project's radar, but the plan was always to finish the GTK 3 work first. reply mdaniel 1 hour agorootparentMaybe so, but did you actually open that \"on the radar\" link? It was closed WONTFIX because moving to the latest release of their bespoke library was considered \"tech debt\" :-/ https://gitlab.gnome.org/GNOME/gimp/-/issues/6440#note_12726... I actually think it was terribly disingenuous of LWN to use \"on the radar\" language pointing at a closed issue. Maybe there is another issue (hiding in the 12,000 issues) that is actively tracking the Gtk4 migration, but that one ain't it reply cmyk_student 46 minutes agorootparentWe actually had a Google Summer of Code project this summer that explored porting one of our main GTK3 widgets to be compatible with GTK4. It's definitely on our radar, but it's not a major focus at this point. reply mdaniel 22 minutes agorootparentDo you have the correct issue that one could follow since 6440 isn't it? Also, since you're here: is it just a matter of glucose, and thus if someone were to port it to GTK4 that patch would be accepted, or it's quite literally \"no user cares about library versions\"? reply cmyk_student 1 minute agorootparentIf someone submitted a patch that ported everything in GIMP to GTK4, I'm quite sure it'd be accepted after review. The trouble is that GTK4 deprecates or breaks a number of things as well. For instance, while the icon scaling system is much more flexible in GTK4, it's different than in GTK3 so all that work would have to be redone. GtkTreeViews are also becoming obsolete, and since GIMP relies on that for the layer/path/channel views, it'll be another big change. At the moment, new development is encouraged to follow the GTK3 -> GTK4 migration guidelines (e.g. use gtk_widget_set_visible () rather than gtk_widget_show (), don't use gtk_widget_destroy () since it's been removed, etc). I don't know a specific issue tracking GTK4 at the moment, but I can check. lovegrenoble 6 hours agoprevaffinity is the best one: https://affinity.serif.com Affinity V2 Universal Licence For macOS, Windows & iPadOS reply david_draco 6 hours agoparentDoesn't seem to be open source: https://opensource.org/licenses?ls=affinity reply adham-omran 6 hours agoparentprevUnfortunately, they seem to have moved to a different vendor for payment processing in the past year or two and now I get \"The product you selected is not available in your location.\" Lucky me I still have V1 from a few years back, it's a shame though. reply rmsaksida 4 hours agoparentprevAffinity is good but it's a shame they don't have a Linux version. reply Farfignoggen 5 hours agoprevDoes Gimp 3.0 still behave the same for GIF Animations and I use that feature extensively and the behavior for that needs to remain the same for cropping, scaling, and frame rates editing or I'll have remain on the older GIMP. So there needs to be someone testing Gimp 3.0 against the previous version to see of any behaviors are not the same and sometimes that can affect workflows greatly! reply rererereferred 4 hours agoparentSounds like you would be great at it since you know the feature inside-out! You can download the RCs and send bug reports. reply cmyk_student 2 hours agoparentprevHi! I don't think there's been any specific changes for GIF animations, outside of improvements (like fixing a bug where overwriting a GIF animation without the GUI would lose the animation status, or adding the ability to import non-square pixel resolutions). But yes, please test and let us know - we'll be happy to look at it and fix as we can! reply lottin 4 hours agoprev> this release has transitioned to using GTK's header bar component, typically used to combine an application's toolbar and title bar into one unit This is a step backward. Access to the window manager menu becomes difficult as a result. reply extraduder_ire 2 hours agoparentDo you mean the menu you get from holding super and right clicking on the window? I don't think I ever access that menu from the titlebar. But that's mostly because the unity DE is where I built most of my muscle memory. reply superkuh 31 minutes agoprevI hope this doesn't lead distros to drop gtk2 libraries. There are still many programs beyond gimp that use them. But without a big name like gimp to justify their continued packaging we may lose them. reply mdaniel 19 minutes agoparentSurely there is no sane distro that would just YOLO a \"Depends:\" for an application, and what are depends declarations for except for dragging gtk2 or gtk0.59beta libraries in to support the requested install reply superkuh 14 minutes agorootparentYeah. I thought the same thing before it happened to the gtk1 libs and the programs I used to use (xmms, etc) stopped being supported generally. reply bastloing 5 hours agoprevGimp seems way to complicated still. I played around with krita, it's way better for what I do. reply siliconpotato 7 hours agoprevbecause of the unpleasant name of the app (which the devs refuse to change), lets normalise pronouncing the G in GIMP like the G in GIF... reply peter-m80 7 hours agoparentI see what you did there reply Euphorbium 7 hours agoparentprevGimp pronounced like jimp? Might save this disgusting name. reply losthalo 1 hour agorootparentAnd thus the Gimp-Jimp wars began, even more brutal than the XEmacs-GNUEmacs wars. reply chris_wot 6 hours agoprevCMYK - the K is Klein, or black. Surely not key? Or has something changed? reply zuminator 3 hours agoparentSurely is key. The \"key\" plate in printing is the main plate with the most detail in the color separation, and it's typically black, hence key equals black. reply RugnirViking 6 hours agoparentprevKlein? that means small. Schwartz is black reply mqus 4 hours agorootparentmini-nitpick for everyone that might be confused: it's \"schwarz\" (without t), Schwartz is a name of a lot of people. reply mauricioc 3 hours agorootparentValid point, but I will also amend your nitpick: Schwarz is also a name of a lot of people :) Both refer to hair colour, I think. \"Schwartz\" was a valid spelling for the colour many centuries ago (see, e.g., item 15 in [1]), which makes sense since both spellings are pronounced the same way. Surnames don't follow spelling changes, so that's the only situation in which both variants coexist now. In maths, you have the Cauchy-Schwarz inequality and Schwartz's theory of distributions. [1] https://woerterbuchnetz.de/?sigle=DWB&lemid=S04549 reply chris_wot 5 hours agorootparentprevI completely stand corrected. reply macronitis 5 hours agoprevnext [2 more] [flagged] rererereferred 4 hours agoparentWasn't there already a fork that changed the name? reply James_K 5 hours agoprev [–] I'm told it's taken them two decades to release this essential feature. What have they been doing for all this time? I think they need to talk to people who edit photos, and focus their efforts on the many low-hanging fruit which would drastically improve the UX of their app (much like Blender did). Perhaps they are doing this to a degree, I see mentions of UI changes, but I wonder why they only think of it now and if they're involving the right people. reply vanderZwan 4 hours agoparentI was about to leave this as a top-level comment, but it might be more appropriate as a response to your question. \"They\" are a handful of maintainers doing relatively thankless work. This is not a well-paid full-time job for them. Take Øyvind Kolås, the maintainer and lead developer of babl/GEGL, which is the technology underlying GIMP 3.0. He's barely being paid for this work. Nowadays he has a patreon, which also is directly linked from the gimp.org website, encouraging you to directly support him[0][1]. Right now those patreon donations add up to about... $1300 a month. And the number used to be much lower when I first started donating. A lot of people here complain that we cannot directly give money to, say, the development Firefox. With GIMP you can directly support its core individual maintainer, and almost nobody does. [0] https://www.patreon.com/c/pippin/membership [1] https://www.gimp.org/donating/ reply James_K 4 hours agorootparentWould giving him more money improve the quality of his output? He also seems to not be in charge of the UI which is the major problem area. It's also worth noting that I could buy a subscription to Photoshop if we're talking about paying money to have software developed. reply SiempreViernes 3 hours agorootparentIf you want to be a whiny customer then pay for commercial software and post your comments on their user forums, what are you doing here? reply James_K 1 hour agorootparentAnd if GIMP's proposition to its users is \"you should use Photoshop instead\", do you see why it won't succeed? reply JackSlateur 4 hours agorootparentprevWith more money, he could leave his daily job and work full time on GIMP If you cannot / will not give money, you can also send patches: in most opensource projects, help is welcome reply James_K 4 hours agorootparentI get that this evangelism is useful, but it's also annoying. I'm also doing things with my life and I don't have the time to fix GIMP. My purpose in writing these comments is not to volunteer my time or money to the GIMP project, but to suggest they improve it by focusing on the many basic UI issues it has, which for some reason they haven't done in the last 20 years. Instead of laying the ground work for non-destructive editing, they should be making it so you can reliably click on text without selecting the thing behind. reply cmyk_student 2 hours agorootparentThe challenge is that there's only so many volunteers contributing to GIMP, and lots of people with requests. The loudest/most frequent ones tend to be addressed first (as volunteers are able). For instance, I worked on non-destructive editing because so many people said it was an essential missing feature that also needed to be there 20 years ago. Now that it's implemented, I've seen a lot of happy people who appreciate it and what it does for their workflow. At the same time, that means there are new requests for the next \"essential missing feature\". :) reply James_K 2 hours agorootparentThe idea that you can never make a good piece of software is backwards and wrong. There should come a point where your software has the essential features and reasonable UX. Producing software which does what the user wants without being actively hostile to them is the bare minimum. It doesn't take 20 years, it's not an endless process. If you spend 20 years doing it, you are managing your time badly. GIMP is managing their time badly. The \"loudest/most frequent\" requests is not a good metric to use because most of those issues are filed by software developers and not users. reply cmyk_student 2 hours agorootparentTo clarify, I wasn't talking about the developers' priorities as a whole. I personally get inspiration for \"big\" projects to work on by scrolling various platforms and seeing what the most frequent complaints are, but that's not how development decisions are made. And I can promise you, non-destructive editing was repeatedly requested by users, not just software developers. I have also seen a large number of users happy about the feature being implemented, as well as videos and screenshots of it being used effectively. It was a net positive in my opinion, even if there's more work to do in addition. That's not to say your particular issue won't be addressed of course! You're just the first person I've seen to make that specific complaint, compared to the larger number of users asking for more CMYK support, shape tool, built-in Resynthesizer etc. What's been really nice about the RC1 release is that more members of the community are willing to try out 3.0 (compared to the various 2.99 releases) and give feedback. People use GIMP in lots of different ways, and if we don't use it in those ways then we don't see their specific problems. reply James_K 17 minutes agorootparent> compared to the larger number of users asking for more CMYK support, shape tool, built-in Resynthesizer etc This is because people with the complaint about text tend to open the software, see how bad moving text is, then close it and never open it again. They aren't the ones going onto your forums to complain. And when they do the bug stays open for 8 years (and counting) with the best response currently being to tell users that they're wrong for not knowing how to do it. https://bugzilla.gnome.org/show_bug.cgi?id=768667 https://gitlab.gnome.org/GNOME/gimp/-/issues/933 https://gitlab.gnome.org/GNOME/gimp/-/issues/8399 However GIMP gets feedback from users, it's producing bad outcomes. Out of \"CMYK support, shape tool, built-in Resynthesizer\", I think users want a shape tool. But they'll get CMYK support. Here's the shape tool issue and it's 23 years old. https://gitlab.gnome.org/GNOME/gimp/-/issues/12 Issue 12. Over 10,000 issues later and it hasn't been added. The existing solution to this is to select a shape and fill it. Why can they not add a tool which does both of these things in sequence? Again, perhaps a small change of code saves every new user of GIMP the annoyance of having to look up on google \"how to draw circle in GIMP\". Development should be prioritised in areas like this, where small changes to the code produce big wins in terms of UX. Instead, they focus on things like CMYK. I'm guessing that's a big change that won't affect most users. There's no point in developing these parts of the software if your UI has put off all the potential users. Look at Photopea, it has just one developer but eats GIMP's lunch on everything I just mentioned. GIMP needs to find a way of managing its devs to outdo a single person working on their own. > I personally get inspiration for \"big\" projects to work on by scrolling various platforms and seeing what the most frequent complaints are, but that's not how development decisions are made. A better approach would be to find someone who uses Photoshop, ask them to try GIMP, and record it. Then take the issues they ran into, and those are the most important things. If you want people to use any piece of software, you need to make them stick around long enough to actually do stuff in it, and that doesn't happen if the first thing they experience is bad. Basic things like they decided to put the button to search menus in a menu, so you might not be able to find it if you didn't already know it was there. reply squigz 5 hours agoparentprev [–] https://developer.gimp.org/core/submit-patch/ reply James_K 4 hours agorootparentI fell for this before. Wasted a few weeks' free time adding thumbnails to the GTK file chooser, only to find out that the library itself has bugs which they refused to fix that make it difficult to do. Now I just have a patched version installed locally. The only way to improve these projects is to change how they're managed, which you can't do with a patch. reply StefanBatory 3 hours agorootparentprev [–] If I see a helicopter that crashed down on a tree, I can say something is wrong even if I don't have license for flying. reply SiempreViernes 3 hours agorootparentIf you had to elbow your way trough a big crowd to actually see the crash, positing \"A crash!\" on social media is not an useful service for anyone. reply squigz 2 hours agorootparentprev [–] Possibly true, but it doesn't mean you can talk about the competency of the pilot(s), either. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "GIMP 3.0 is anticipated to release soon, marking the first major update since 2018, featuring a modernized GTK 3 interface and improved performance.- Key updates include non-destructive editing, enhanced color management, improved layer workflows, and a new extension system for easier plugin management.- Although the exact release date is unknown, development builds are available, indicating a significant advancement for the open-source image editor with more frequent future updates expected."
    ],
    "commentSummary": [
      "GIMP 3.0 is under development, adding support for color palettes beyond sRGB, such as CMYK and CIELAB, which are important for print and publishing.",
      "The update seeks to enhance color accuracy by converting to other color spaces only when necessary, minimizing data loss.",
      "Despite advancements like non-destructive editing, some users are concerned about GIMP's user interface and functionality compared to Photoshop, citing slow development due to a small volunteer team."
    ],
    "points": 230,
    "commentCount": 141,
    "retryCount": 0,
    "time": 1732877433
  },
  {
    "id": 42270389,
    "title": "Vince – A self hosted alternative to Google Analytics",
    "originLink": "https://github.com/vinceanalytics/vince",
    "originBody": "For the past 3 years I have been working on a Golang port of plausible analytics dashboard.vince is a single binary, single user with multiple website system with zero runtime dependency.Key featues:- Automatic TLS - Outbounds link tracking - File downloads tracking - 404 pages tracking - Custom event trackingAnd so much more, basically everything that you see on plausible dashboard except funnels and custom properties.You can use vince as a drop in replacement for plausible for personal websites.The goal is to make the plausible dashboard easily accessible for people who like to self host.All features not related to the dashboard are non goal, hence not implemented.Full dashboard demo hosted on 6$ vultr instance https:&#x2F;&#x2F;demo.vinceanalytics.com&#x2F;share&#x2F;vinceanalytics.com&#x2F;v1&#x2F;...",
    "commentLink": "https://news.ycombinator.com/item?id=42270389",
    "commentBody": "Vince – A self hosted alternative to Google Analytics (github.com/vinceanalytics)188 points by gernest 16 hours agohidepastfavorite56 comments For the past 3 years I have been working on a Golang port of plausible analytics dashboard. vince is a single binary, single user with multiple website system with zero runtime dependency. Key featues: - Automatic TLS - Outbounds link tracking - File downloads tracking - 404 pages tracking - Custom event tracking And so much more, basically everything that you see on plausible dashboard except funnels and custom properties. You can use vince as a drop in replacement for plausible for personal websites. The goal is to make the plausible dashboard easily accessible for people who like to self host. All features not related to the dashboard are non goal, hence not implemented. Full dashboard demo hosted on 6$ vultr instance https://demo.vinceanalytics.com/share/vinceanalytics.com/v1/... vextea 0 minutes agoThere seems to be some mentions of selling licenses (and pricing) in the source. What are the plans around that? https://github.com/vinceanalytics/vince/blob/f0c2c3cc38cbd8c... reply zoidb 7 hours agoprevMy go-to self hosted GA alternative is goatcounter https://www.goatcounter.com. It would be interesting to know what advantages it has over it. reply huhtenberg 2 hours agoparentDoes it allow filtering visited page list by a specific referrer and vice verse? reply james-bcn 6 hours agoparentprevOh I like that main dashboard. Very simple. reply TravisPeacock 3 hours agorootparentIf you like that there is https://www.piratepx.com/ which is even more minimal (though less data), I also built something even MORE minimal (only API calls) https://github.com/teamcoltra/ninjapx but I'm certainly not recommending it. It is super simplistic (also the readme is embarrassing) reply gonafr 50 minutes agoprevHow this compares to umami (https://umami.is/)? reply pdyc 7 hours agoprevLooks exactly like plausible, may be change the ui a bit to avoid legal issues. reply carlosjobim 6 hours agoparentI was going to say that it looks exactly like BeamAnalytics, and now I'm confused to who's copying who... reply serial_dev 3 hours agorootparentI'm wondering when copying becomes just following industry best practices... Twitter, Threads, Mastodon, Blusky all look the same. Project management apps all reuse the same UI patterns. The \"AI\" logo looked pretty much the same for all companies for a while. Video sharing websites all use YouTube's layout. Forums like Reddit and HN share quite a lot in their looks. If you want to display website analytics, you will want to show the most important metrics at a glance, you'll need graphs showing visitors over time, top sources and pages... There is only so much you can do to display those and have users understand what's going on on your website. reply dewey 5 hours agorootparentprevBecause everyone is using: https://tailwindui.com/components#product-marketing reply huhtenberg 2 hours agorootparentIt's not just the looks that are the same. The UX / mechanics are way too similar too, e.g. how you can apply filters (by URL, by referrer, by browser, etc.) to narrow down the stats view. reply rkuodys 13 minutes agorootparentI would say pretty much the idea is as follow: \"Let's do it so User would know how to use it before we are big\", and once you're big enough - you can set the trend. But at the beginning it's just not worth it and highly risky reply just-tom 11 hours agoprevThe screenshot on your homepage looks very similar to plausible's https://plausible.io/ which is also open-source analytics software. Is it based on it? What are the differences? Edit: Just noticed the feature comparison in the readme. reply dewey 5 hours agoparentAlso Plausible is almost stock TailwindUI elements + including the default color, so many sites look like that. reply paradite 3 hours agoprevNot sure why I would use this over Plausible CE on docker. Does it consume less memory/CPU? Also I am pretty sure Plausible CE doesn't limit number of sites / events, unlike what's listed in \"Comparison with Plausible Analytics\". reply written-beyond 3 hours agoprevCode quality is pristine, really great job! I see that you've used protocol buffers, can you expand on why? I am aware of the benefits it offers but I think it adds a bit of mental overhead initially due to it being an additional type system you have to understand. Also why are you using pebble exactly? I was interested in seeing how you're managing your geo databases because that's usually the most mind numbing part of handling analytics if your cloud provider doesn't add that information into the request header already. However, I can't understand why you'd use pebble over something like sqlite. reply rgbrgb 13 hours agoprev> Full dashboard demo hosted on 6$ vultr instance https://demo.vinceanalytics.com/share/vinceanalytics.com/v1/... 404 page not found reply thangngoc89 10 hours agoparentI found a link from github https://demo.vinceanalytics.com/v1/share/vinceanalytics.com?... reply brokegrammer 10 hours agoprevThis is amazing! I self host Plausible but don't like depending on Clickhouse and Postgres because they're annoying to upgrade. What kind of database is this using though? I don't know enough Go to figure it out from the source. reply tricked 10 hours agoparentI checked the go.mod and it seems to be importing a module named pebble by cockroachdb i assume that's where everything is stored https://github.com/cockroachdb/pebble reply akshayshah 10 hours agoparentprevIt uses Pebble, the key-value store that backs CockroachDB. reply colesantiago 9 hours agorootparentJust saw this notice: > WARNING: Pebble may silently corrupt data or behave incorrectly if used with a RocksDB database that uses a feature Pebble doesn't support. Caveat emptor! Slightly worrying for now running this in prod if there is a risk for silent data corruption, but hopefully in a few years Vince would have drivers for Postgres / Clickhouse. reply rickette 9 hours agorootparentThis just warns about using Pebble with an existing RocksDB which isn't the case here. Pebble powers CockroachDB which is a Serious Database. reply dangoodmanUT 6 hours agorootparentprevReread the sentence, it says if you mix it with RocksDB (another database that has compatible file formats) reply skeptrune 1 hour agoprevCool that there are so many of these now. Currently self hosting plausible and it does seem quite barebones. Will have to give this a shot! reply lovegrenoble 10 hours agoprevIs is a Plausible clone? https://plausible.io reply __jonas 9 hours agoparentFrom the Readme: > vince started as a Go port of plausible with a focus on self hosting. reply cebert 16 hours agoprevIf you haven’t checked it out yet, Serverless Website Analytics, is a great solution for this too. It’s easy to deploy and very inexpensive to run. I’ve been using it and am quite happy with it. https://github.com/rehanvdm/serverless-website-analytics reply gernest 15 hours agoparentInteresting, I just checked the readme. Very similar but looks like it only works with AWS and has a lot of moving pieces. How do you deal with location data, do you purchase maxmind db license or use their free versions. Both maxmind and db-ip free versions of city data miss city geo id values, rendering city data useless for many cases. With vince, I had to index embed the whole city data from geonames database to work around this. reply t0mas88 10 hours agoprevIt says GDPR compliant and no cookies on the project page. How are unique visitors calculated? And I'm assuming it can't link conversions to campaigns without some cookie-alternative? reply withinboredom 9 hours agoparentNo idea, but generally, a bloom filter would get you there without any identifying information being stored. The counts would merely be estimates at that point, not exact values. reply beeb 7 hours agoparentprevAt least for Plausible, they state this (https://plausible.io/blog/google-analytics-cookies): > Instead of tagging users with cookies, we count the number of unique IP addresses that accessed your website. Counting IP addresses is an old-school method that was used before the modern age of JavaScript snippets and tracking cookies. Since IP addresses are considered personal data under GDPR, we anonymize them using a one-way cryptographic hash function. This generates a random string of letters and numbers that is used to calculate unique visitor numbers for the day. Old salts are deleted to avoid the possibility of linking visitor information from one day to the next. We never store IP addresses in our database or logs. reply chrismorgan 5 hours agorootparent> Since IP addresses are considered personal data under GDPR, we anonymize them using a one-way cryptographic hash function. Um... hashing IPv4 addresses, even with salt, does literally nothing to anonymise (assuming the output space is at least ~32 bits, which I think is safe to assume): they’ll still be PII. IPv6 addresses I’m not so confident about; maybe it would be sufficient for some parts, but it’s definitely inadequate for some concerns. (For IPv4, enumerating all four billion inputs is so completely practical that “one-way” is nonsense.) I’m almost certain this is legal theatre. reply Semaphor 5 hours agorootparentOne way if you have a salt? Enumerating won’t help, you need to know the salt, which gets deleted. That said, the whole IP thing is weird to me. Not only are we allowed to log IPs directly for security reasons, we even *have* to log IPs in certain cases (newsletter subscriptions). reply kadoban 3 hours agorootparent> That said, the whole IP thing is weird to me. Not only are we allowed to log IPs directly for security reasons, we even have to log IPs in certain cases (newsletter subscriptions). The point of designating something as PII isn't that we then _never_ store or use it, it's to carefully consider if we actually need it or not (and what protections we can add for the values we do need to store/use). We're meant to stop the practice of just collecting and storing all data, without consideration for the harms that causes. reply kadoban 5 hours agorootparentprevIf what they're doing is using a secure salt and then throwing the salt away once a day that _might_ be doing something. reply chrismorgan 5 hours agorootparentWhat I understand they’re doing is storing the salt in one place, a set of hashed IP addresses in another place, then daily trashing the lot after counting the number of elements in the set and storing that. Information-theory-wise, this is no different to just storing the actual IP addresses (and deleting them daily after tallying, as before). It does mean that you need to obtain two things instead of just one, but if you get access to it all, it’s straightforward to reverse the lot (though computationally a little expensive), and easy to check a single value for a match. The technique may be considered reasonable effort at protecting against casual abuse, but it’s not technically effective of itself, and it doesn’t stop the data from being PII. The important aspect is that the PII is deleted within 24 hours. My personal opinion is that the hashing part should probably be considered snake oil and whitewash, at least for what they’re claiming—I don’t say it’s useless, but it definitely doesn’t do what they’re touting it for. Unless they’re actually keeping the hashed values for some reason after one day, and associating them with other records? In which case, disregard part of what I say, it’s obviously better than persisting IP addresses long-term! But also it’s extremely dubious to call that anonymisation as they do, because you can so often tie things together, behavioural patterns and such, to deanonymise. It’s frighteningly effective. reply tingletech 3 hours agorootparentIf you throw away the daily random salt (but keep the obscured IP address), how can you check a single value for a match the next day? reply jszymborski 3 hours agorootparentprevWhat matomo does is mask parts of the IP address (you choose how much). reply gizzlon 5 hours agorootparentprevhm.. are you saying they need scrypt or something similar? reply chrismorgan 5 hours agorootparentThe “PII” label is taint that is probably impossible to dispel completely/perfectly, and difficult to dispel sufficiently (and deanonymising is an arms race). Lossless techniques do nothing to dilute that taint. Lossy techniques are necessary to get anywhere, such as disregarding certain bits of the address, or Bloom filters. reply kadoban 5 hours agorootparentprevThe problem, in general with hashing IP addresses (especially ipv4) is that there's not that many of them. If I tell you the value is either 1 or 2, but I hashed it with sha256 to make it secure, that's bullshit, right? You can just hash both and see which it is. Same concept applies regardless of the hash algo, and still applies if you have more than 2 possible values, 4 billion or so possible ipv4 addresses is _not_ that many values to a computer. Other common places this problem occurs is with any other restricted set of values, eg phone numbers and email addresses (most are at like 5 domains and are easy to guess/know). reply pdyc 7 hours agoparentprevmost likely through one way ip hashing bounded by time duration. If you have utm's in your url than it can track otherwise probably not. reply aaronbrethorst 12 hours agoprevLooks interesting. What sort of memory requirements does it have and how does it persist data? reply samdung 9 hours agoprevThis is great. I'm def going to use it. Minor bug: \"See Live Demo Dashboard\" url is wrongly pointed. reply cpursley 6 hours agoprevHow would y’all go about building analytics into a professional marketplace type of app where you can provide the professional with their own profile page stats (in a reliable way)? reply rasso 6 hours agoprevDoes this work on your average 10,-/month shared hosting server? If so, it might really be „for everyone“. Otherwise, we are stuck with matomo. reply diggan 5 hours agoparent> Does this work on your average 10,-/month shared hosting server? Since they usually offer software via cPanel and alike, seems unlikely unless you give it lots of time for the project to first get popular enough to get on the \"admin panels\" mind, and secondly for them to integrate it. Besides, do people really pay 10 USD/month for shared hosting? Sounds really expensive when you can grab VPSes for half that price and run whatever software you want, not just what they've packaged for you. I guess ongoing maintainace is included in that price, but still sounds kind of expensive for what you get. reply rasso 3 hours agorootparentI don‘t know… around here (Germany), that‘s pretty common. No need to manage anything, no usage-based cost, … my favourite is https://all-inkl.com. OG no-bs hosting for boring tech. reply notRobot 7 hours agoprevThe dashboard demo isn't working :( reply manishsharan 3 hours agoprevI think the reason some of us continue using Google Analytics is its demographic data. That information is not available elsewhere as far as I know , which I admit is not a lot. reply 8ig8 13 hours agoprevMatomo is another one… https://matomo.org/ reply colesantiago 11 hours agoprevGreat project keep it up it's good to see competition in this space. Plausible gets crazy expensive on their hosted option and it complex to setup (needs elixir + high memory requirements) If Vince gets 1:1 parity with plausible and has the option to use clickhouse, I'll consider moving a few servers and people I know over. Love that Vince is also a single binary as well. reply Oras 6 hours agoprevIf you don't have plans to offer saas, what are you trying to achieve from it? I mean, it is quite nice to have binary installation hosted on a single VPS, but will you support it? reply rrr_oh_man 6 hours agoparentFOSS lives! reply drchaim 10 hours agoprev [–] this is great, congrats! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Vince is a Golang-based port of the Plausible Analytics dashboard, designed as a single binary, single-user system for multiple websites without runtime dependencies.- It includes features like automatic TLS (Transport Layer Security), outbound link tracking, file download tracking, 404 page tracking, and custom event tracking, making it nearly equivalent to the Plausible dashboard.- Vince aims to make the Plausible dashboard accessible for self-hosting, serving as a drop-in replacement for personal websites, with a demo available on a $6 Vultr instance."
    ],
    "commentSummary": [
      "Vince is a self-hosted alternative to Google Analytics, developed as a Golang port of the Plausible Analytics dashboard, focusing on dashboard-related features.",
      "It offers features such as automatic TLS (Transport Layer Security), outbound link tracking, and custom event tracking, with no runtime dependencies, making it suitable for personal websites.",
      "Vince is GDPR (General Data Protection Regulation) compliant, uses Pebble for data storage, and is available as an open-source project on GitHub, providing an accessible self-hosting option without Plausible's setup complexity."
    ],
    "points": 188,
    "commentCount": 56,
    "retryCount": 0,
    "time": 1732848840
  },
  {
    "id": 42270966,
    "title": "Australian Parliament bans social media for under-16s",
    "originLink": "https://apnews.com/article/australia-social-media-children-ban-safeguarding-harm-accounts-d0cde2603bdbc7167801da1d00ecd056",
    "originBody": "Just a moment...*{box-sizing:border-box;margin:0;padding:0}html{line-height:1.15;-webkit-text-size-adjust:100%;color:#313131;font-family:system-ui,-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,Noto Sans,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji}body{display:flex;flex-direction:column;height:100vh;min-height:100vh}.main-content{margin:8rem auto;max-width:60rem;padding-left:1.5rem}@media (width Enable JavaScript and cookies to continue(function(){window._cf_chl_opt={cvId: '3',cZone: \"apnews.com\",cType: 'managed',cRay: '8ea4da10cf53d650',cH: 'oUesEMipjwCIEDU6uslNTr3LfOy3WDEipiHLRt5EtpA-1732906944-1.2.1.1-sMgP42rqZoZraHex4Fu3dAK6MvEQw3I9HHlpri7xu8H7MF9kv7MLcYaoGnvtAli0',cUPMDTk: \"\\/article\\/australia-social-media-children-ban-safeguarding-harm-accounts-d0cde2603bdbc7167801da1d00ecd056?__cf_chl_tk=FBo1wNEjZ70KMByoxr.bneNuyhh4.eqi99qWBrSsmJg-1732906944-1.0.1.1-vbhHEdeEZ6JdE.zlefNgQSoMdDIGj2FWTd8q6.69uDk\",cFPWv: 'b',cITimeS: '1732906944',cTTimeMs: '1000',cMTimeMs: '390000',cTplC: 0,cTplV: 5,cTplB: 'cf',cK: \"\",fa: \"\\/article\\/australia-social-media-children-ban-safeguarding-harm-accounts-d0cde2603bdbc7167801da1d00ecd056?__cf_chl_f_tk=FBo1wNEjZ70KMByoxr.bneNuyhh4.eqi99qWBrSsmJg-1732906944-1.0.1.1-vbhHEdeEZ6JdE.zlefNgQSoMdDIGj2FWTd8q6.69uDk\",md: \"YE0QgPOMd7WLxICf5BmLCEGJ0ijPVWBVOmfWS1zbaOY-1732906944-1.2.1.1-ts2WxstYPUEEya3ple7qIP3kX364Emf4rhHSsz2sNrE7f3W5BxS7UDb5cz3k5gRnV8KFf9PSX8194TegbomOLulrZNixgFEz58Nbz1s8aigRO_fCOdC_cIHqG3zXMGjAmDw2Xm18I7YzRxHPg_nyucu1VAvD0YyXm.fHmuh7M4.qwxkZbzatKkIN5qwd2te_lHRN.OQJt6DrcWNkmhZ_mBe7J.KNMBeWniEcmvQOkVJHANqcPYuAx_IyiWfvWWGbaBGZuh9ztYDhFmdUs.rmE6DYz3LqUgrJihT8TWPMOykS40Ex6llR8tKTpcWDQoWKP_Dlr1R02R_YEfIC7i9r_fueJ39td5L2KCBFQ9hmYy2Sjn0PKjvKgpPbDxzqUcAWvfYBhbUVUMDFDxQ7FG.XSzWvKjsr1NdRLqe7_cWm7YChPUxwiYKVVlw8RnrEa4V38K_NlUNpMAL6ZPol8zwUuRgQ3j1Ta7ICTGLyPRxWKE37XtJdEFDjvpZsaTtHliXdjTEW6ApzLQm4OFTUG26tueYtOKWI4Ypp2qCeAeaOb3He3QjrUqwVNSNeQZaxhSyeky7iFbqCLPZssVuptg0IVsIu9bxrVqrw8dnD0gCkJBihHDBhKbBJcLmHlHqOOUDyhV5WQki616awo97NV7XyCWwkTI3LQU9bPniYPYND1LRU7z0uL2qc2SmcqFksl22AjeAGQagwgIsp3ZskfDkGA7esNridwzJNWZhFC.62Hz9i3cPCdIAoGeOR3CwhYGOTFoCvPVzemP5WASH4Wn6ubQ1ZtduLgxvdf6MNpTEA1dTng0OmWRJvda5i9PM4Oije_ym2sJ8J_qymzHTPHQuVcYiFbYPQodscjZHT_RJMDcLgeTSJQ9HwI2RA4EhOlYv9UVYjU9zMq.hnUUOhiF1FUChVBsHbZ52PhiR13pSh8m2HQ.FkMlghvJguigFnpesA82SqKc666WnNahWV1_.2WvZsVyJfYKrclz9pwXd.BA6SMa.eP97KrGe8x3flDgRDU3ss33T5Z1ymjk2vbW7uu.6amnn.eCwoSExPWZYV54MviwWnp2FZxZVFIMGJFM8YGDv5wkC2NxF1M_izPtmVE86OM9IWmSyuevVBX6_ijhzoVgbPei1e1MZPRyMsoqN088tCz02qOaVQTsw0jTGdhkbUlZ7JbkZPZFbMmLm9ulTBHaEiLm0Wit0mVcRAtuLiwhCIbPk5C9SAa_nIic3inC0jf_RkPLZ0YRnpkYCxlg5FdbUWRvGFjEsHFDDgq3Xr1K9vN37.ugfay4pQzaJbjDLmDPAWxUNzCsIBRxxS8baad.eVOdJMQDKUgjiB2ZndJ7vIlHOtT88Aj2UR5g.iKybw.8v7vBlVD5Kk21cOFVt8o_47aXwzbQGUiEzSMyBCETmzU13vtL1Tl26cHP.d.9zdP2CY.gHPkSScslt1jFH7g7GndzxFbOCYOfxXcpEprRHZrLO73mK6jJXGJgcD2sI0DZaLjolLwSR5MJS8MLE3NtpLzsRAPlKPiE_.iUxYAON0NbVqqhWB90iCM5VaErFyO2.ZN2H5Y0tWEbkpyUhQu1tBY_VIAx7Hs10mv8gd.zOkLnwNCED4VqsCEmJeDfnNRuuDxpP5Kv6bsWZzLTgWfOmDbVVk0ku1Nw..M1ZqlJq56IZrUtGQA0_CM.jC3pmllsAeYq1y_DAhXEoOzYKP5Pyxb1x_6xHgM.WZbW8xcGfIOFIT.z03Snjv8TGoEwVH7G7lz5tELQT6TBaanfa9URlC.Sm33S.tWtWJmqdy5yOsqxIhA0tf7RWa6VutLdgyXA0BRP7jc4v2izyJbOiFKDuQ5aIbH6KoDRNK617ySXiJetx2HZ3oU2qxrUn9AwpgUWAcgn6QTtLKZJk3RxTlpSZrYJ8ln1tkrGhuoV.a.zFl_OQIzJoIo6nsbXL6SAAzHvFF2FSSRJzi_S2YY_dUilbNj9AcMeasWmb0hSKPqUZbuFB9vjB.NRlQaYK3JChzWR.JsBkpfgdIgrGIvUXM31gvpDbof0LoXJ.2EPTVhB6CgYeZBk2Eko1.BXZl8nOf8yL_NmikpVD8c8q6Vjdzy4oa1U_P.lEva6iQRgLTxr6yo3LXxCOA0.6uoNp3DTp2_GiQOJ88TfxJSYoRSB7sUmI1v43OtX1r6Peu31Gq61lT2d_vKnW0opDLe1Wfjl6EOHZQv7ceczxO8uvLIzQMDvMDdkBkOLcXXzRnD6CFCg7iyNibbYPPyO9icqkFKaQ_OV5Wkxdw45Y6NPFde83I8ffpDsVDvIJawhb4YMObkJWqfY_l5soPDrCm84zdOdOQxnv5S6RHiKG1.9pbmYKYrxrTy5N.3ZhvgGb5LqyrEDHd_Q0g3ix7NR7VYKoZZGwwbYJOaUq899LwLlgStxh4io1z7dBSdjIujiN4dtC6NWSPuK6nZ4BCadKV5T43AA8IhcnlGjauspk_0vPJeuFUmZOpDuqJSrU7LEa65nhozKljvctMFuljn5kYwek4L_Qn0Sn.3KvKonCMeNyPlMr.4d1HBvKnOIDzmL7DHg2jSHt_5PRruQKLBR4ScDytl86bKcFmhIZCwAyrHbWhOpO1yVr_eYV.0WWzQRCibWHuVH2GPI_CdxWVP8hKdc6oQFSLhsksBT8Drm48MoDhrZ.rZlv2UU60GnGSrJ.M9MPhfdywX2Rww8jWiL2UZhaCdcZC6MFBAPdy5tK1kerZU7qatFj_eaufgDFKFw6ssl2mxZceaalhRln8i6vi.BT8Ew\",mdrd: \"qhjaA7WDWzc442KcCFRmU64tRwBnYN7hGvMP1x7yUV8-1732906944-1.2.1.1-Jpn4Pe.EuLZ.3s6A8O7WUkc5e6_3vEr3YbgQLoMdfKV4.XCRoAkygwHz2MZZ0z2gWBD0Jj.6GhO_pcIkRalnpJi6mZJE8xQjuNhRnTAfubFesm1LYCoTyCbTvaP9CA0WExihGF.EXEWsRFW51jyXbo7qRhJuf9FOTas78X1CATW_9NlNaSUXZRHgAJHDrEq7z4tu88PpxfvORS0eO93YgvGbnfLEze24.rz.RRNsVvkbi5XxyUHd6W9FHCfJ1rDsI87VopKTbf7730qHZ9objkdIJgLE.AiQ_0oBRwafCi1LS7W.vzXQzQoCSb9t8whd.Ts3VK4Sg4EESF.IruhdTsMVoWIlcKo3zpL4VpPBlnUsE.BZIchvyCc_2MTMdhjQit187tkF2LaOBd.lovRjS2cZI9QrIacOl4LAZKg1mnsJMpyyNJ6cWUbas2GNU0j08mk_uUqs4hLNc3dwpPbcvCQIbHsWNVkUhk8rm0nRvPz4LFnRA8lao_mtcvOffLUoxcFhhXccAIyL8jD7JCPO8dJIgiagXK_RJcb7U3eTXdi2ZTSD1c5Arz8XHk3TsfalbhlozrV.LbTb1eKVBDjzM84zBa3XaJyNp6._C6jdTikuPf6rKgqdT7aTenb.OvvgUm6lY3oST8F9Y7sx_KTGTJ7mBYQGzmI5P0Z1efl6_PZy6Oltcf7LaWWRmCz5AwVWTFoZMpDz43uVFfeaLLp3oi1aGKP2O24iCOBbkX8Km70Uhgpdln3prodiPYnek4wRnVKfgTEdn5ydioinNw0n0KuJzMlglCz74mXoeFXfD0cpZTO7cBxIUBXdkjb6GnH8XjazyDiTQYps_qaC57yhsYVbTDorfRJjZFtV_Ccp5POwwd8zBQB1Eqj5P6cw59m25H.Mhc8c2POkmxRy5b1ZtSNkLHaex15PQGgMhge7JRABHf2YmYn9.qrS4WNXvfw3tJer5X1.f4mupy.MHL4.C8C5t20mIMYbogrp2c1X.PADF0jdIOSR2MLpj35CiAOVMIJKwH4.FjskZUN6xOpkriskq44mlqHuseMH1oVjVCcfP0x4cRE5ECe.EUKf82jkniGNkP_UZk_ORnM67wpQOeJ1iSkNNvZjAe7dsxmTPA91ncsUUxg48xrtI5jY8Wvlaaec_b7AjJ9DBLJ74mEqqpr6.b8vtZgri9qnNTht.D0YfEjEKxjHRR.led.A_21LLEiCAkt2AI0rxVrWzMwt.kVn5pEx9Bx9r3KZrRnI7.ML4halHD__C2aPf6ZuZrWGLtZJlGF4svuwDYN4dlSbVweAQOkvajC0wMFHYhWPpKxF1xhhaVeToOWWxYWI2_l43UZ5m3wLCzxfxEJkRGZcVpg87hOILhi5GgxrYggMicDmIFC.Y0g8.QWxeRMPZcDcRa8oNris.GYlWwwXLyqhArvrGwQR9ifIeOptHN5VgXplHIh9qbe_JpMZk1flr0T6jz6PJtHhJNrKGme1D02eCiyCDMtaYUy67qI20gmlGFrlbn8jh1SfMunfzPHh3bFe4.oSS0jQA0hRmstRDRMcAV0jjqwMzpmN7NrtVeHowGCsCK5NXoXLYUBVbdlxjaRCKfDfofVrZ9Hy1Lysyrv9tXuGsauQJdxyD25_DBnvj5Vhb9bgM6pbalRNYWxHVD9BSXGUvI0SSPfEicq5YcEMjxAUlANekslXismMsFhBw3IYI66KzOETPtX0EkowQsg5o7XIgv8.VgeS57hqrFClqjnw_710A8x8UKaZlh9M4qBpKKsk7TVlkKlG3qu1QDw5eSmOxAP3Oywtl6qgtbQZlEEmT6ShTts.YHm0qMQSnxC7WjmaesVarLhs68_NJft72qazxy2EOLNL6rDpulrkJkXmKUL_xqkvOJgYZczyNJUlzPH68v.c2cgCeLxh1NGuKj2Lj_A70U7TxtV5s5WBAR2jLZ2PSG57mQXsHV.1.dYqaI0B3_NoAs7fVreeGXpywaQodZRNlo9IRiTEwI37oR_Q9vnNl.kwFDv.SDvpiqo2mxnimBLuc9hVefp4PfVfsTanrWzcggJJz4H9yOSvJ73DaeFoInJYg12LqBqfSVn3G4xtZulKgKhEG2gI93ccXUaY1lKcWVfDV4Apscj6hDtHbsktpm.q0JAsAl.QFYzz2os4326aniNjiFsCDSulF3e.1Ig1e_mU3L_XQHLi85Hv31DZtaHlfds54TdRYgjWXOEIKHWSYxhm6sMM_5oFkqbtYkDlENXnbO87U47SzIEcJ1f5qYjBVu1n2xGLeO7_VVg_JFeyHN8PDhKJ74jIif54J8QhU6Zj9h3fH4o_GQTmERZ8y2DcHQGo68u.pHLBxWbs4Ep3oYyqcqPdo4MEoBzqbsVA2jULMLd1l5TxIw\"};var cpo = document.createElement('script');cpo.src = '/cdn-cgi/challenge-platform/h/b/orchestrate/chl_page/v1?ray=8ea4da10cf53d650';window._cf_chl_opt.cOgUHash = location.hash === '' && location.href.indexOf('#') !== -1 ? '#' : location.hash;window._cf_chl_opt.cOgUQuery = location.search === '' && location.href.slice(0, location.href.length - window._cf_chl_opt.cOgUHash.length).indexOf('?') !== -1 ? '?' : location.search;if (window.history && window.history.replaceState) {var ogU = location.pathname + window._cf_chl_opt.cOgUQuery + window._cf_chl_opt.cOgUHash;history.replaceState(null, null, \"\\/article\\/australia-social-media-children-ban-safeguarding-harm-accounts-d0cde2603bdbc7167801da1d00ecd056?__cf_chl_rt_tk=FBo1wNEjZ70KMByoxr.bneNuyhh4.eqi99qWBrSsmJg-1732906944-1.0.1.1-vbhHEdeEZ6JdE.zlefNgQSoMdDIGj2FWTd8q6.69uDk\" + window._cf_chl_opt.cOgUHash);cpo.onload = function() {history.replaceState(null, null, ogU);}}document.getElementsByTagName('head')[0].appendChild(cpo);}());",
    "commentLink": "https://news.ycombinator.com/item?id=42270966",
    "commentBody": "Australian Parliament bans social media for under-16s (apnews.com)184 points by asah 13 hours agohidepastfavorite304 comments thombles 11 hours agoAs an Australian parent this is really pleasing to see. It's clearly inspired heavily by the work of Jon Haidt and Jean Twenge et al. The kids are not okay and the main thing stopping a worried parent from addressing it is that \"all the other kids are on it\". If you have an addicted/bullied child would you make them a social pariah and create interminable arguments by taking away their devices? Collective action is needed and this is what it looks like. We don't need watertight compliance, verification or enforcement - just raise the level of difficulty for kids to get on and create a culture among kids, parents and schools that social media is not allowed. This will be enough breathing room for parents to say no and kids not to feel like they're the only ones missing out. reply bigstrat2003 10 hours agoparent> If you have an addicted/bullied child would you make them a social pariah and create interminable arguments by taking away their devices? Yes. That is literally your job as the parent. If there was some teenage trend of injecting heroin, nobody would accept \"but my kid's social life might suffer\" as a reason to not stop them from doing heroin. It similarly does not suffice as a reason to not stop your kids from using social media. If it really is that bad for your kids, you have a duty to stop them regardless of what other kids (and their parents) do. reply neumann 10 hours agorootparentYes, but because the entire world agrees that heroin is bad (similarly to social media) governments have laws to restrict its access. As for the job part. There is so much stuff kids don't need that other parents cave into, that unless you have the means, willpower and energy to find alternative peer groups, schools, teaching mechanisms, your kids _can_ become social pariahs if there isn't sufficient like minded parents in the community. Of course you want your kids to internalise self-wort, a sense of confidence, and independence. Hoping they follow your mold and not a potentially damaging peer-groups. But that doesn't mean that some regulation from a body (government/school) isn't helpful. We live in communities. reply pulse7 10 hours agorootparentprevAs a parent I literally support bans of social media for under-16s AND I also do my job as a parent: my kids get a smart phone at 16, until then they have a feature/dummy phone (which they hide from peers because they are ashamed to show it). reply sandos 8 hours agorootparentNot having a smartphone would make my kids \"social pariahs\" for sure. We gave them phones fairly early, but locked down. It was not really a huge problem when they were younger, but social media (no, we shouldnt even call it is, its is 110% ADS FOR CHILREN!) is getting to be a real problem now I feel. reply t0bia_s 41 minutes agorootparentWhy dont you use any kind of adblock? Why don't you speak with them about harmful consequences of algorithms? Instead of ban that noone learn from, communicate issues. reply rnl82 9 hours agorootparentprevBeing a parent is not easy, and fights like these are really difficult. As a human you have only so much fight in you, and if a majority of parents adhere to the same rules this becomes so much easier to enforce. Government rules set the moral bar, yes they are not enforceable but it creates a standard that most parents will adhere to. As a dutch parent i wish we had the same rules, my kids are young enough that its easy to ban social media, but its getting harder as they are slowly becoming teenager. reply bill_joy_fanboy 9 hours agorootparentprevIt is definitely a parent's job to say \"no\" to degeneracy. Saying \"no\" is easier when your child isn't offered a buffet of degeneracy by the outside world. reply tene80i 10 hours agorootparentprevThe point is clearly that you are causing other serious harms by doing so, unlike with the heroin example. You can’t tear kids away from a huge part of their social lives without serious consequences. Coordinated action means it’s not a choice between two terrible options. reply rightbyte 9 hours agorootparentSocial media is not social anymore. The algorithmic feeds messed up almost all utility. Facebook was a great platform for local communities and democracy but later turned into a threat to civilization more or less when they introduced the 'algo' 2010 something and wanted to get money for 'promoted posts'. reply matwood 10 hours agorootparentprevIf every kid was injecting heroin we would not just expect, but demand the government step in. This is in addition to whatever parents are doing. reply t0bia_s 38 minutes agorootparentWouldn't be better to educate children about deadly consequences of using heroin and what does it makes to your body? Noone is learn from ban, and it also cost a much more money. reply theteapot 9 hours agorootparentprevSo true. In addition, if every kid suddenly became a vampire, we would not just expect, but demand the government step in. reply partomniscient 9 hours agorootparentNot only that, at some point the vampires will form a government for vampires which will be at odds and at war with the non-government vampire and there will be individual vampires that disagree with and feel a lack of representation from their vampire government. reply triceratops 3 hours agorootparentprevWell...yeah. How would be scrounge up enough blood to keep them fed otherwise? reply thombles 9 hours agorootparentprevAccording the stats in Haidt's book, between 2010 and 2020 girls in the US saw a 188% increase in emergency room visits for self harm per capita - never mind the less extreme presentations of mental health issues. Imagine you have an anxious, depressed and potentially suicidal child. This is a horrifying situation. How do you feel about being the hard-arse parent knowing that they will be furious? Is that actually your best move in that moment? Wouldn't it be nice if you didn't have to make that kind of choice? reply jjav 9 hours agorootparentprev> > by taking away their devices > That is literally your job as the parent. Tell me you're not a parent without telling me you're not a parent! This only works if there is broad support from the entire social group, meaning school and all (or most) of the other parents. If you try this all alone, all you can accomplish is to apply more suffering to your poor child. For us, here in silicon valley, fortunately that broad support existed in elementary school. Not a single kid had a phone, so it was the norm. In middle school the support is still broad, although less absolute. A few kids have phones, but they are outliers, so it's still normal not to have one. reply chris_wot 9 hours agorootparentI am a parent. That literally is your job. reply triceratops 3 hours agorootparentprev> If there was some teenage trend of injecting heroin, nobody would accept \"but my kid's social life might suffer\" as a reason to not stop them from doing heroin. You know that's actually one of the reasons they made heroin illegal? So parents wouldn't be the bad guy when they stopped their kids from shooting up /s reply InDubioProRubio 9 hours agorootparentprevAnd as a parent that duty can be outsourced into legislation and policework. Everyone has a fulltime job- and its okay to limit liberties and delegate. reply abc123abc123 3 hours agorootparentPro tip! If that is how you feel, don't have children. Wanting to outsource your childs upbringing to the government because it is \"annoying\" or \"hard work\" indicates that you are not ready to have a child. reply 1vuio0pswjnm7 20 minutes agoparentprevThere are some serious ads running on TV in the US about special accounts for under 16-yr-olds on Instagram. It appears Big Tech has reason for concern when anyone interferes with their process of indoctrinating young people into its total takeover of the www. Expecting an aggressive PR counteroffensive to experiments like Australia. reply noufalibrahim 10 hours agoparentprevAs a non Australian parent, I quite envy you. Parents are almost as addicted as children and often take dozens of photos of every tiny event and post it on IG without permission of other children who are there in the pictures. Many kids have their own channels which are promoted by parents and their identities are based around them. I do what I can (don't give them devices till they're 18 etc.) but it's hard when the whole society around you is pushing the other way. Some kind of societal regulation like what's happened in Australia will be a good thing. It will add friction to kids getting onto these platforms and that will significantly slow down adoption. reply rightbyte 9 hours agorootparent> Many kids have their own channels which are promoted by parents and their identities are based around them. What kind of sociatal cast do they belong to? I'd imagine this is mainly a thing among wannabe elite, wannabe 'influencers'? Like failed pro sports parent forcing their children to go for it to. reply throwawaythekey 10 hours agoparentprevAre these same parents unable to tell their children \"no you can't stay out late it's a school night\" reply GoblinSlayer 10 hours agoparentprevMy nephew is denied a smartphone due to low grades. Seems like a good enough excuse. Granted, it can work only if your country allows low grades. reply trogdor 2 hours agorootparentWhat countries don’t allow low grades? reply 71bw 10 hours agoparentprev> If you have an addicted/bullied child would you make them a social pariah and create interminable arguments by taking away their devices? They'd be addicted/bullied regardless? reply cchi_co 10 hours agoparentprevThis perspective highlights a key aspect often overlooked in debates about such laws: the social dynamics of parenting. You’re absolutely right that the “all the other kids are on it” pressure can make it almost impossible for individual parents to set boundaries without isolating their child reply XorNot 10 hours agorootparentBut all the other kids will simply be on the next thing. They're on social media because that's accessible, but the assumption is the demand for such things will vanish because you banned them? I'm (apparently) one of the few millenials who seems to actually remember and empathize with my teenage self, because the early forms of social media were something I really wanted. I posted on Usenet, used ICQ etc. None of these things were easily accessible, but they fulfilled a need. The situation being addressed is basically \"I am taking no steps to limit my child from access to something I already disapprove of\" and I don't see how this would address anything. You got them off Snapchat or TikTok but why wouldn't word of mouth just find a new service to act as a virtual third space? Social media works just fine in a web browser. Basically there's a strain of assumption which could be summarized as \"children aren't smart enough to use Mastodon!\" as though to do so is not just \"visit this URL here's the QR code\". reply neumann 9 hours agorootparentIt's a lowest energy state approach. The hope is that the hurdle will be raised high enough that enough kids will go to another outlet that is less harmful. Personally, I think the cat is out of the bag. The information dense but fractured culture we are in makes it so kids will find otherways to communicate and share 'memes' (original sense of the world) and that this is whack-a-mole. But, for many kids it will be enough to at least allow the parent's some appeal to authority to minimise their kids screen time and access to social media. reply dmalik 9 hours agorootparentprevThe internet still exists outside of social media sites designed to keep you there. I hope kids under 16 get addicted to anything else and don't turn to social media when they're old enough. reply justsee 9 hours agoparentprevIt's very hard not to see the whole marketing of the legislation as an exercise in misdirection. The media campaign was led by Rupert Murdoch's News Ltd (who see a commercial opportunity in cutting a demographic from their competitors audience for advertisers), and the government's marketing of it entirely focused on children being restricted, avoiding until the last weeks an admission it actually meant all adults would have to undertake verification processes to use social media. The gulf between the practical effect of the legislation (all adults are impacted) and the claimed intent (only children under 16 are the focus), is so large the claims shouldn't really be accepted at face value. Combine that with the abuse of democratic process: what would usually be weeks of public feedback was shortened to 24 hours, and after passing the lower house the government attempted to force the legislation through in the senate with no debate, finally conceding to a brief 1 hour debate before passing it in the final hours of the last sitting day of the year. When opposition arose earlier in the week to the idea this is about a backdoor mechanism to force the government's recently rebranded myId (from myGovId), the government hastily made changes to the bill and loudly said social media companies would not be allowed to ask for government id, in physical or digital form. But that itself was misdirection. A clause was added saying this does not apply if an alternative option is also provided. But age assurance based on biometrics from webcams has poor accuracy, as one senator argued in the final debate. Fresh-faced youths will have no option but to go for the myID solution on their 16th birthday - which as that same senator pointed out is the age at which you can apply for a myID account. On the topic of digital id and myID: it was also apparent that the Government seemingly feigned ignorance that this system, designed specifically for this purpose, would be the ideal and primary solution. Yet we know just how much focus and energy has been going into designing these systems and working out how to get the public to accept them: witness this piece from 2018 [1]. It's of course fine to argue pros and cons of digital id, both philosophically, and in terms of specific implementation details, but that's not what happened here. Everything clearly indicates the legislation was designed to sidestep any substantial debates on this topic. What was also sidestepped was any reasonable discussion of specific implementation options. Instead we have vague details of an age assurance trial being run for many more months, with no specifics that help us understand how this legislation may work, or fail to work. [1] https://www.aspi.org.au/report/preventing-another-australia-... reply bdzr 2 hours agorootparentBut didn't you know that social media is causing all of the problems we have right now? Haven't you seen the graph that shows society with no problems until social media, and now we have all the problems? reply trhway 10 hours agoparentprev>We don't need watertight compliance, verification or enforcement the law basically requires document verification, i.e. no anonymous access anymore for anybody. Just government protecting children, as usual. reply somishere 8 hours agoparentprevAs another Australian parent (with school age kids) I heartily agree. Eager to see how this plays out. reply chris_wot 10 hours agoparentprevAs an Australian parent myself, let me say this legislation is useless and you need to stop hiding behind stupid government legislation pandering to the lowest common denominator in place of your parenting. The “the government won’t let me let you use social media” is one of the most stupid and ridiculous arguments I’ve heard a mature adult make to a juvenile. Your kid is likely to see it as spineless, then go ahead and make a fake login to their favourite social media site without telling you. I have a 14 year old and a soon to be 17 year old. Just have a conversation with them about social media. Now they are just going to hide their use of it with you and you won’t be able to guide them when their use of it goes wrong. reply wokwokwok 10 hours agorootparentI’m not for this legislation, but your argument doesn’t hold up. If people are hiding it, that in itself normalises that people are not using social media. That’s the point; taking someone’s phone away makes them an outcast. If no one at school is “allowed” to have a phone, then it’s normalised, it’s normal to not have one. It is pandering to the minority, but your argument hides behind insults to avoid facing the reality which is that it will normalise people not being on social media, which will make it easier for parents to ban their kids from using social media. This isn’t some black market crap where the kids are nipping down to the servo to buy some vapes and get a slap on the wrist if their caught; if they’re banned and they’re caught, their account will get suspended. That's instant death on SM, because of early adopter network effects. It really will do something. You don’t understand the dynamics on SM if you think otherwise. Whether you agree it’s good or not, is certainly open to debate. What it will do is open to debate; hidden group chats and an escape to platforms that are “technically not SM”, who knows… but the current landscape of SM & TikTok follower accumulation will change, dramatically. reply chris_wot 9 hours agorootparentExcept it wasn’t up for debate. It was opened for public scrutiny on the 22nd, submissions closed on the 23rd, then it was put through Committee (who asked for only 1 or 2 page submissions because they themselves didn’t have time to review it properly) and was initially put to the House on the 26th and passed on the 29th. The Act has no provisions for checking ID (that will be decided by the eSafety commissioner with no debate or discussion on the mechanisms they choose), and it doesn’t even define which social media companies will be affected. To say this was up for debate is ridiculous. There was no debate! reply wokwokwok 9 hours agorootparentYou’ve taken literally one word from my comment, out of context. Yes, you’re right, they rushed it through. …and that sucks. Now what? Is that really the only meaningful complaint you can come up with about it? I don’t support it because I think it’s overreach by a government that is already trying to make a nanny state. …but I’m not naive enough to think it’s not going to work. It will work. It probably has unintended consequences; that’s the problem. reply chris_wot 9 hours agorootparentI didn’t take you out of context. You literally say that whether the Act is good or not is up for debate. Before legislation is passed, it needs public consultation and debate. There literally was no public debate and now this is law! Things like: 1. It will be trivial to bypass the restrictions. 2. The argument given in support of this Act by the OP was that it gives parents an excuse to prevent their kids from using social media. That’s going to work out great. 3. The mechanisms for verifying ID are not defined and will be decided without public debate. Adults will be forced to get a government controlled digital ID, or alternately social media companies will need to collect your official documentation to prove your age. Neither of these are palatable. None of this has been considered. None of these concerns were allowed to be expressed by interested parties and the general public with sufficient time for comment. That more than sucks. This is a government who wants to shove badly flawed legislation onto the Australian people because they are immensely unpopular. I’m a Labor voter. Not next year. reply ghiculescu 9 hours agorootparentWho will you vote for instead? Everyone supported it :( reply chris_wot 9 hours agorootparentAn independent. Labor is more worried about the Greens and Independents than they are about the LNP. Albanese and Co, who I voted for and quite liked, have shown that they need to be forced into a hung parliament to prevent their worst excesses. A huge pity. reply ghiculescu 9 hours agorootparentThey’ve been shockingly bad. reply chris_wot 5 hours agorootparentprev\"The Act has no provisions for checking ID (that will be decided by the eSafety commissioner with no debate or discussion on the mechanisms they choose), and it doesn’t even define which social media companies will be affected.\" reply contingencies 9 hours agoparentprevAs an Australian parent, congratulations: you are the problem. I for one was on a public panel with politicians discussing how ridiculous this proposal was, a couple of months ago, before it passed. The government gives all children email addresses while simultaneously banning them from social media. Only a bureaucracy could do this with a straight face. IMHO as a technology-engaged parent the answer to complexity online (and in the entire world) for young people is education, not \"ask the Nanny state to change the rules\". Timelessly compressed: \"Those who would give up essential Liberty, to purchase a little temporary Safety, deserve neither Liberty nor Safety.\" - Benjamin Franklin. reply thombles 8 hours agorootparent> The government gives all children email addresses while simultaneously banning them from social media. Only a bureaucracy could do this with a straight face. If you think it's email that is causing youth mental health issues then I see why you are confused about my enthusiasm. reply arlattimore 13 hours agoprevWhile I appreciate the sentiment of this legislation, removing social media access for everyone under 16 years to address the concerns of a few is a nanny state act. As a parent, you should be making these decisions for your children - not your government. Additionally, I would suggest it won't address the actual issue. The named/identified social media companies will comply as required, younger internet users will simply go elsewhere (the internet is a big place after all) and therefore the problem will ultimately go unaddressed. reply kumarvvr 12 hours agoparentIt is undeniable that big tech has warped the way even adults deal with each other. It warps social norms, social habits, human interactions, perspectives on the world and a lot more, all for the worse than the better. For children, it is very important for them to build their social skills based on interactions with other kids and adults, rather than from social media. They must learn about the world from the world, rather than through a commercial filtered lens of a big tech company, that is focussed on clicks, views and profits. True, its a nanny state move. But, in my view, it is absolutely essential, as big tech seldom seems to worry about their products and their effects on children. Sometimes, when the problem is big, the action must also be big. reply throw0101d 6 hours agorootparent> It warps social norms, social habits, human interactions, perspectives on the world and a lot more, all for the worse than the better. See Penny Arcade's \"John Gabriel Greater Internet Fuckwad Theory\": * https://www.penny-arcade.com/comic/2004/03/19/green-blackboa... reply BLKNSLVR 12 hours agorootparentprev> It is undeniable that big tech has warped the way even adults deal with each other. It warps social norms, social habits, human interactions, perspectives on the world and a lot more, all for the worse than the better. Also true for television. Old Media had the monopoly on opinion-guidance back then. Part of the (encouragement of the) negative reaction to social media is it's threat to the status quo, much like Australia's previous legislation to force Facebook and Google to pay old media for linking to their news articles. Different groups of people will have different lists of things they don't like about society and where it's headed; violent video games, advertising, music videos (that tend to be soft-porn these days), internet browsing tracking. Where does a government draw the line on what to act on and what to leave to the responsibility of parents? (how does this help other agendas? Will this look like we're \"doing something\"? Will this score votes? which way is the breeze blowing on this topic?) Social media is a concentrator. I'm not convinced it's a 'cause'. There seems to be an epidemic of (social) anxiety, and maybe Facebook is the cause, but to me it feels societally deeper. World leadership is demonstrably not \"the best of us\" (I'm not just saying that because of Trump), to me, it feels as if there's a pervasive attitude of \"I'll get mine and fuck the rest of you\", which predictably trickles throughout the society whose leaders portray that attitude. Social media being one outlet of this, but I see plenty of 'us vs. them' polemics in traditional media. Just my theory. I'm already poking holes in it mentally, but anyway. I don't have any useful answers. I guess we'll see if this ban makes a difference. reply bamboozled 7 hours agorootparentprevI think it has the potential to basically ruin society as we know it, the amount of hysteria, hype, division, lies, distraction and hate it breeds is enormous. I look at some of the stuff on Reddit, especially some of the right wing stuff, it's actually alarming the stuff people say, and believe on there. I know they are extreme examples but holy shit. The ability to amplify falsehoods and certain narratives is ridiculous. I remember reading 1984, and being terrified, some of the stuff I read in online communities makes me at least that scared and worse. reply fraboniface 11 hours agoparentprevIt's very hard for parent to resist the insistent demands of their children to get a smartphone when their classmates all have one, and not having one means being or feeling excluded. This is the kind of coordination problem better solved by regulation. reply nulld3v 9 hours agorootparent> It's very hard for parent to resist the insistent demands of their children to get a smartphone when their classmates all have one, and not having one means being or feeling excluded. This is the kind of coordination problem better solved by regulation. But they banned \"social media\" not smartphones. If they actually banned smartphones, this regulation would be much easier to enforce. But by just banning a select few apos, kids are just going to move onto the next app. And not only that, it only bans kids from having an account, they can still view content while logged out. So Im going to bet that this not be effective, not be enforcable. reply XorNot 10 hours agorootparentprevIf you're not going to police that smartphone then how will this legislation do anything? What stops \"US Mastodon\" being the new place all the kids in your school are posting? Like that is the goal here, because observably any phone with a web browser has social media access, it just can't be a company which tries to do business in Australia directly. But 4chan has existed for years (that's not the end point, but certainly creating a selective pressure to push teenagers to less regulated, shadier social media services doesn't seem like it's quite the result people want). The short version I'd say is, this move seems solely aimed at taking away a virtual third space for adolescents and doesn't plan to offer anything back. But people didn't go seeking it because it offered nothing. reply anonnon 4 hours agorootparentAdolescents shouldn't have a non-anonymous presence on the internet. reply attila-lendvai 9 hours agorootparentprev...and no one talks about a toxic effect of comousory education in this context: your child is locked together with children whose parents you would never be friends with because of conflicting values. maybe the peer pressure for phones issue would look completely different without compulsory education. reply openrisk 10 hours agoparentprevYou can cast any regulation as a \"nanny state act\". Life is more complicated than that. The ability of individuals to be aware and take conscious action against harmful behaviors is clear and should be used when possible. But that ability gets exhausted very fast in a hyperspecialized and complicated world. Most parents are digitally illiterate, can't protect even themselves online, let alone their kids. Delegating to collective institutions that can pool the required expertise and weigh-in the pros and cons is the means to empower individuals to better handle these hard to evaluate risks. If our institutions really dont produce good regulation the obvious thing to do is check and fix that, taking into account that the complaints about over-regulation, bad regulation etc. might be by the offenders or aspiring abusers that have something to lose. reply throwawaythekey 6 hours agorootparentIs there a time when you would find \"nanny state\" to be a useful concept? What would make that situation different to the social media ban? reply openrisk 4 hours agorootparentI think the answer in implicit in my comment: When all the affected individuals can readily make their own judgement about the risks and benefits of their choices (1) they have all the relevant information, (2) they are able to comprehend it and (3) they can act on it using a menu of options. 99% of people are entirely clueless as to what happens behind the scenes in social media platforms because the information is not there, and it is doubtful they would be able to evaluate it anyway. Plus, there are hardly any differentiated alternatives available. So when these conditions are not met, anybody throwing around terms like \"nanny state\" has ulterior motives to exploit vulnerable populations (ignorant, addicted, low information, trapped etc.) for their own gain. Which is \"fine\" or at least understandable, moral values are not universal. But lets make clear the starkly different visions. reply pawelmurias 9 hours agoparentprevThe idea is to make a group decision to stop all the kids from using social media rather then get your kid alienated by just blocking him. Some people on hackernews want their kids to have zero friends and play SNES games but that's not what we should aim for. reply pyuser583 10 hours agoparentprev“Nanny state” is fine when you’re talking about children. reply null0pointer 10 hours agorootparentDid you even read the parent comment? > As a parent, you should be making these decisions for your children - not your government. reply Raz2 9 hours agorootparentAlcohol, tobacco, drugs, sex, driving and many other things are heavily regulated for children. Should a parent be able to allow his 12yo daughter to drink, smoke or marry 40yo guy? We're living in a society. Social media is just another item in the list. reply null0pointer 7 hours agorootparentI’ve seen many people make comparisons between these things and the social media ban, but I actually think they’re quite poor comparisons. The main concern is around enforcement, which is still TBD for this ban. I assume you mean illicit drugs which are banned for everyone so I’ll skip that as it’s not relevant. For sex/marriage, these are basically not enforced until people notice or they cause a problem so again are not relevant. Driving is an interesting one but I’ll come back to it. The closest comparison here is alcohol and tobacco. For alcohol and tobacco sales the rule of thumb staff are trained on in Australia is if the customer looks younger than 25 they ask for ID. The customer present their ID, the cashier visually checks the DoB, and the sale goes through. This does not affect the majority of Australians who either do not smoke or drink, or look old enough that no ID check is done. Enforcement for a social media ban would be onerous on all Australians who use social media (I don’t have numbers but I’m sure it’s more more than (smokers ∪ drinkers)) plus the scope for potential abuse or infringement of rights is far greater. Compare this to social media. How would such a ban be enforced? The kids are not stupid, they will find a way around whatever the enforcement mechanism is. So either the enforcement will be a) trivially circumventable to the point where the legislation is completely useless for its ostensible purpose, or b) devolve into an endless cat-and-mouse game trampling Australians’ rights every step of the way. Depending on how eager the Aus gov is to enforce this it could easily extend to VPN bans, destruction of anonymity online, and yet more means to eliminate free speech. These things are all extremely important for a functional society where people, especially vulnerable and marginalised people, can speak up without fear of retaliation. A cashier checking your ID at the shop is nowhere near the top of a slope as steep, nor slippery as this. Driving is a really interesting comparison here actually. I’m not sure I would be opposed to a social media license. In the same way the purpose of a license is to ensure that road users can do so in a safe manner, maybe something similarly focused on education would be more helpful here than a ban. I’ve actually long blamed a kind of tragedy of the commons for the sorry state of the modern internet. Most users are simply not savvy enough to know better than to use it in all but the dumbest ways, fall for the dumbest scams, and basically allow themselves to be corralled like cattle into the sterile advertiser-friendly pens big tech companies have constructed for them. So in anger I’ve sometime said we should only allow licensed users online. Anyway that’s a bit off topic but a social media license is an interesting concept. reply cchi_co 10 hours agoparentprevIt seems like this law is more about creating cultural change reply mastodon_acc 9 hours agoparentprevBy that logic expecting citizens to stop at traffic light or expecting them to not violate speed limits is a nanny state act. reply ruthmarx 10 hours agoparentprevAustralia has always been a nanny state though. Not even letting some games or movies in to the country because they think it's inappropriate for adults to consume for example. Ridiculous. reply oblio 12 hours agoparentprevCuring addictions is very hard and some addictions are very strong. Some addictions are also guarded by business interests leveraging close to a trillion dollars to increase this addiction. We didn't reduce smoking through personal responsibility, stop using \"nanny state\" tropes. In democracies the government is an imperfect representation of the will of the people and sometimes it decides individuals can't fight some things. reply hcfman 12 hours agorootparentGood point bringing up smoking. Why don’t they just make smoking illegal in every country ? Because it’s impractical. Same as this law. reply throwawaythekey 6 hours agorootparentI find it uncomfortable that so many of the arguments focus on practicality and that so few focus on liberty. It almost seems like a forgotten concept. reply xxs 10 hours agorootparentprevThe point of smoking is about the origins of the term 'nanny state' [0]: [0]: https://wordhistories.net/2020/10/03/nanny-state/ reply DavidPiper 11 hours agorootparentprevAnd yet the things they have done have gone a long way to reducing the real harms caused by smoking. A 5% spread of harm in a population is better than spread across 80% or 90%, etc. Taking action to reduce the surface area of harm, even if 0% is impractical, is beneficial to a functioning society. reply thaumasiotes 9 hours agorootparentprev> Why don’t they just make smoking illegal in every country ? Because it’s impractical. Is that also the reason they don't make cocaine illegal in every country? reply cma 11 hours agorootparentprevI agree parents should have the choice for kids using social media, but this example of impracticality is a bit off: cigarettes are illegal for kids in nearly all developed countries. reply matwood 10 hours agorootparentSmoking is also highly regulated now. It’s almost a surprise to see someone smoking nowadays. reply XorNot 9 hours agorootparentprevAnd yet over half my high school year smoked, and I was absolutely peer pressured to start. reply dools 11 hours agoparentprev\"to address the concerns of a few\" lol reply obblekk 13 hours agoprevThe votes are strong majorities of both houses - great to see a functional democratic government act on such an important issue. They specifically don’t prescribe any particular age verification methods. This would be a great time to follow up with legislation that updates their national IDs to be able to provide cryptographically secure proofs of age without leaking identity. Absent that, I’m sure many of the comments to come will worry about the privacy implications. It really would be great to see the government act with expertise to solve the problem in a way compatible with a free and open society. reply vermilingua 12 hours agoparent> great to see the government act with expertise I’ve seen the Australian government accused of many things, but that’s certainly a first. This is the same country whose prime minister once said “The laws of mathematics are very commendable, but the only law that applies in Australia is the law of Australia”. reply globalnode 11 hours agorootparentThat was back in 2017 when the govts were all up in arms over encryption. Apparently they don't mind it too much now, which means it all must be back door'able for them. reply vermilingua 11 hours agorootparentHas been backdoor(ed|able) for FVEY (via Australia) since 2018. https://www.homeaffairs.gov.au/about-us/our-portfolios/natio... reply globalnode 11 hours agorootparent5y in jail if you refuse to give a password and power to covertly access devices and conceal that fact if they desire. Yikes, they don't mess around. reply Der_Einzige 11 hours agorootparentBy far the most bootlicking country in the Anglo world. Disgusting, and their political system should be shattered into the wind reply HPsquared 10 hours agorootparentNature or nurture? Australia did start off as a penal colony in the early days. And the USA started, in large part, as a destination for religious extremists. Funny how these things keep coming up again and again! reply ruthmarx 10 hours agorootparentprevThe problem is Australians are fine with it. They might be the population with the most people saying \"if you've done nothing wrong you have nothing to worry about\" in the world, by a large margin. reply kreetx 11 hours agorootparentprev> over 95 per cent of the Australian Security Intelligence Organisation’s (ASIO) most dangerous counter-terrorism targets use encrypted communications Some humorous sentences in there :). reply linuxandrew 12 hours agoparentprev> They specifically don’t prescribe any particular age verification methods. This would be a great time to follow up with legislation that updates their national IDs to be able to provide cryptographically secure proofs of age without leaking identity. Hard disagree. We do not need an internet driver's license. Australians are supposed to have a right to interact with organisations with privacy protections under the Privacy Act 1988, like APP2 which allows individuals to deal with organisations anonymously or pseudonymously. Social media companies are doing a great deal of harm to society, but banning under-16s is tackling the symptom, not the problem. All people should have more rights and protections, like opting out (or better, opting in) to infinite scroll and algorithmic content suggestions as opposed to subscribed content. Algorithmic content today is akin to spam in the early 2000s which governments regulated and has had some impact on bad behaviour by local companies (of course I am not under any pretense that spam will ever really be solved). Social media users should be able to opt in or out of content categories which AI could potentially help with that categorisation, ideally in an uber-transparent way. I'm young enough that \"modern\" social media was just starting up when I was a teenager. It's not clear that banning under 16s from modern digital communication would provide any benefits (which, by the way, social media is very loosely defined under the amendment). > Absent that, I’m sure many of the comments to come will worry about the privacy implications. The big issue is that we are importing the UK model which will see identity outsourcing to companies like Yoti and AU10TIX, the latter which was hacked in 2021 and led to some pretty serious implications for affected users. Of course the reality is that Meta is already doing age and identity verification on users who use privacy-protecting technologies like Firefox Container Tabs, at least in Australia, and has been for a number of years. This usually leads to an account being blocked until the user provides their ID via a photo. This will become formalised so that accounts that are detected as possibly being U16 (via various techniques like profiling and data matching against external sources) will be requested ID, and Yoti will likely be used to actually perform that verification. Another big concern will be that this is forced onto smaller operators like Australian Mastodon sites, internet forums, mailing lists and others. reply ehnto 11 hours agorootparentThey can opt out, by not participating with the site. No one is mandated to use social media. But I would also want to see things go the other direction anyway, default to non-algorithmic feeds. Those with the awareness to opt-out are not the people at highest risk. I agree with basically everything else you said, and I think social media is generally a blight on society. But we can opt-out already, if you are on social media platforms with algo feeds, you are signalling that this works for you. You need to accept that responsibility in the same way it's up to ourselves not to drink 40 beers a day at home. reply Springtime 11 hours agorootparentReddit is among the range of sites deemed 'social media' per the article. Reddit is practically a glorified forum where users directly influence which submissions rise above others via personal voting and self-curation of communities to follow. There's no voodoo there forcing non-subscribed things in one's feed unless one is logged out (ie: non-participating anyway). Given the timeframe to come up with how it's meant to be practically implemented it's not hard to imagine on various services all users of all ages from the region would be required to submit standard ID rather than an idealized age verification the GP suggests that prevents either storing or leaking identity (in either direction). If it went that way it'd be a major blow to user privacy and data security concerns. Looking at criticism of the legislation there were a range of organizations pointing out such issues, including UNICEF. reply rebuilder 11 hours agorootparentprevIsn’t this kind of like saying the solution to robocalls is not having a phone? reply nxpnsv 11 hours agorootparentIt works though reply linuxandrew 7 hours agorootparentprev> They can opt out, by not participating with the site The definition of social media under the legislation is essentially any form of digital communication that allows two or more people to communicate, as decided by the minister. > No one is mandated to use social media OK, I'll bite. What if you want to join an interest group (crafts, technical, political etc) that organises meetings digitally on a social media site? Sure, you have the choice to not use a social media website, and if you do, in all likelihood not join your choice of interest group. The point is that Meta long used unfair and anti-competitive means to corner the market, and obviously not to interoperate so that it is difficult for people to leave. This might surprise you but the relationship between platforms and users is usually coerced and not really consensual. If you do not find yourself in this position, good for you. I'm a very firm believer that anti-competitive social media companies should be regulated in positive ways, like forcing interoperability and forcing companies to making algorithmic content opt-in. > if you are on social media platforms with algo feeds, you are signalling that this works for you Yeah except people are usually on those platforms for many reasons, like access to group chats and messages, as the platforms have a wide reach. A lot of people become outcasts by quitting social media, myself included, because our friends choose to continue to use it. reply liammclennan 12 hours agorootparentprevDid you see the \"without leaking identity\" part? Your comment seems to ignore that. reply wqaatwt 11 hours agorootparentOnly for now. After requiring an ID to sign up is normalized governments will inevitably try to eliminate any anonymity to “protect children”/catch criminals/censorship/etc. reply XorNot 9 hours agorootparentCase in point: COVID tracing was immediately ruined by the contact database being used by police for surveillance. As a nation we have utterly terrible organizational firewalls for the public interest. reply pixxel 4 hours agorootparentprevYou must be a child, yourself. Incredibly naïve. reply gloosx 12 hours agoparentprev>the government act with expertise to solve the problem Do you have an idea which problem did they solve? Did banning certain psychoactive drugs solved the problem of drug abuse? Maybe banning alcohol removed it from the streets during great depression? Banning gambling? Kids will find a way to get into their social media accounts anyway, and then these democrats will tell you they need to ban every VPN service and set-up Deep Packet Inspection devices for every ISP, make their own govt CA, and trust me all of it will be done in the name of people and child safety. Sad to see a dysfunctional govt. which bans and calls it a solution to the problem. When I will be in the office I would ban the whole concept of banning itself once and forever, and any politician who proposed a single ban in his life would be banned from service. I will of course step out for proposing this ban immediately. reply gloosx 11 hours agorootparentFor all the proponents of the ban here – I will just tell you what works – for your education. It is endorsing and subsidising healthy and active lifestyle, supporting and promoting strong family wellbeing as well as upholding public psychological and physiological health. Only doing these instead of issuing bans would really contribute to kids choosing virtues of real life over screen time, but unfortunately addressing root causes takes more effort and time than issuing a ban. reply thrixton 10 hours agorootparentThat's great for those that can implement that, many cant (don't have the time, education, willpower, etc...), maybe the majority. Given the challenges of rebuilding a proper society, maybe this is a step in the right direction (maybe). We don't allow kids to have other addictive substances, there's definitely an argument (and the co's agree, with 13 yo minimums?) for restricting an addictive medium. reply 123124dfasd 12 hours agorootparentprevJust because the War on Drugs failed does not mean the argument extend on to every other fields. reply gloosx 12 hours agorootparentSocial media is similar to a drug because it is dopaminergic, and banning it is very similar to War on Drugs scenario, just a knee-jerk reaction, not an expertise-driven policy reply Timwi 11 hours agorootparentprevAnd yet, the argument does extend. It applied to Prohibition, too. reply oblio 12 hours agorootparentprevBanning smoking in many places plus banning advertising it plus banning selling it to kids reduced smoking by unfathomable amounts. reply gloosx 12 hours agorootparentDo you have data do back it up? From quickly looking at historical records online, I can't see any unfathomable amounts reduction, It's levelled on 20-30% of smokers among kids, and it remains a core challenge for child and adolescent health to the current day (according to 2020 WHO report[0]), plus they started to smoke vapes. So did the bans you sampled really work, or do they just smoke more discreetly and use tricks to buy cigarettes now, making the whole thing more inaccessible and desirable for an average child? [0] https://www.who.int/europe/news/item/05-06-2020-smoking-stil... reply somishere 8 hours agorootparentWell, in Australia at least, the number of smokers in the 18-54 age cohort has almost halved in the decade upto 2022 (older smokers still tend to smoke), and more than halved in the 15-24 category. A \"national tobacco strategy\" was introduced in 2011/2012 that brought plain packaging, increased taxation and a bunch of other measures. Official data is here if you're interested: https://www.abs.gov.au/articles/insights-australian-smokers-... There's also Wikipedia and any number of other resources that go into more detail on the history of decline: https://en.m.wikipedia.org/wiki/Smoking_in_Australia reply Timwi 11 hours agorootparentprevI might be wrong but I read the OP’s comment as sarcastic. reply oblio 11 hours agorootparentYou are wrong. Banning smoking worked. reply null0pointer 9 hours agorootparentThey mean the root of this comment tree, not your parent comment. reply refurb 6 hours agorootparentprevHow do you know? At least in the US, smoking rates dropped substantially by the 1980's, long before many anti-smoking laws were in effect (you could still smoke on planes in the 80's!). https://www.statista.com/statistics/184418/percentage-of-cig... reply ruthmarx 10 hours agorootparentprevPeople were already moving away from smoking. The aggressive policies don't deserve all the credit. reply oblio 8 hours agorootparentThere are many articles and documentaries about the impact of smoking adoption due to tobacco companies advertising primarily to kids. Even if we disregard all the science, the fact that the very companies themselves were targeting kids shows that they knew where their money was coming from. reply oblio 11 hours agorootparentprevSmoking used to be at much higher levels (50%+) and absolutely everywhere. Also vaping is probably better than smoking, and don't worry, vaping is next on the ban list. It just takes time to build the case. reply Aeolun 12 hours agorootparentprevThe very graphic lessons at school showing what a smokers’ lung looks like after a life spent smoking didn’t hurt either. reply Terr_ 13 hours agoparentprev> national IDs to be able to provide cryptographically secure proofs of age Nah, this is an antipattern we've seen before. A veritable Pandora's Box whispering to be opened. There is a much simpler and safer solution: 1. A disclosure law, which requires sites to somehow (e.g. HTTP headers) show their nature as a social media site, porn site, etc. 2. Parents can choose to purchase devices/software for their children with a parental-lock, set those filters and permissions to match their own locality or personal preferences, and whitelist any necessary exceptions. This way the implementation costs of the shifting, complex, never-ending demands will fall onto the groups that actually want to use it, instead of all sites in the world being potential legal jeopardy for failing to implement all the censorship rules of every possible visitor. It also means that most enforcement (and exceptions) move out into a physical realm which parents are at least able to see and control. > without leaking identity Leaking identity to the site is only half the problem, the other is leaking activity to the government. I'd ratehr not have a Government Internet Decency Office with an easy list of every single site I ever tried to view or register-for, without any kind of warrant or other due-process. reply kumarvvr 12 hours agorootparentThe problem with this approach is that you will have two groups of children, the ones who have access and the ones who don't have access. This is a worse problem than allowing it for all. Its another vector of temptation, distraction, in-equality, etc. reply Terr_ 12 hours agorootparent> in-equality If your concern is that some parents will be able to afford to give their children their own devices, but not afford any parental-control software with them... Well, that's better-addressed with an explicit \"Digital Tools For Needy Parents\" program. If you mean some parents will choose to give their kids more autonomy... Well, isn't it proper for that to be their decision? I have little sympathy for neighbors who use the logic of: \"You are banned from giving your child $thing, because I'm tired of hearing my kids whine that they want it too.\" reply RichEO 11 hours agorootparentI don’t think the concern is economic equity, it’s social harm. Kids whose parents choose to restrict will suffer social consequences vs parents who don’t. The whole point of the law is to reduce the social harm caused by social media. reply hcfman 12 hours agorootparentprevBut not a worse problem than blocking it for everyone reply oblio 12 hours agorootparentprevI'd say the next steps is that you also force institutions for kids to ban it, regardless of parent choice. Parents must present proof of disabling said websites for their children. Or their kids can just not have those devices with them. Do this for schools, activity clubs, restaurants, fast for food places, etc, and you've basically hit 80% of places where kids are all the time. reply mianos 12 hours agoparentprevI highly doubt they have the expertise to implement anything remotely 'cryptographic.' Their level of competence seems stuck in the 1950s, stamping paper forms, while anything more complex is handed off to consultants in Australia. These consultants appear far more interested in lining their pockets than in understanding technology or math. The far more likely scenario is they piss a couple of hundred billion away on the first company that shows up with a slick-sounding, half-baked platform, claiming it can magically solve all their problems with just a few \"minor\" tweaks. I'm an interested party, I have a 13 year old daughter who would benefit from a little less time on social media. But that's my problem, and my belief that these idiots in our government could help me with that is zero percent. I am probably in the .1% of households where dads know more than kids. (If photometric id comes in, I want to be in the fake moustache business). reply turbojet1321 12 hours agorootparentThere is myID (formerly myGovID) which would be the logical vehicle for a government provided age verification service. I've heard (but can't find a source) that it's build on OIDC/OAuth, so extending it to be an IdP exposing only specific claims (ie, age) shouldn't be a huge leap. reply linuxandrew 12 hours agorootparentmyID as it stands is a bit of a farce. It uses OIDC under the hood, but it only supports end users that download the myID app on their smartphone via the Google Play Store or Apple App Store. Security is effectively outsourced to Google and Apple as the user's identity is \"pinned\" to their smartphone. Take myGov in contrast which is web-capable and supports users to use a Yubikey or Passkey/Webauthn-capable device to authenticate. Under the Australian Digital ID scheme myGov will likely be usurped by myID, which is, in my view, an inferior scheme which blatantly ignores basic standards. reply ENGNR 13 hours agoparentprevWe’re a nanny state, the government is just going to use it to track us reply pimeys 11 hours agorootparentNanny state is a term coined by tobacco industry in their lobbying against tobacco laws. Is it really a term you want to use here? https://www.tobaccotactics.org/article/countering-industry-a... reply tirant 10 hours agorootparentWhy not? It is about treating people as adults or children. reply bigstrat2003 10 hours agorootparentprevWho cares? Whether or not it applies in this particular case, it's a useful term. Rejecting ideas because of who they come from is the very antithesis of intellectual maturity. reply refurb 6 hours agorootparentprevJust because an \"evil\" group used, or even coined, a term doesn't mean it's not a useful term. reply ruthmarx 10 hours agorootparentprev> Nanny state is a term coined by tobacco industry in their lobbying against tobacco laws. Is it really a term you want to use here? Sure, because most people h ave no idea where the term originates from and it now has a life of its own. It's the standard term for this sort of thing. reply threeseed 12 hours agorootparentprevASIO has been able to track you for decades since they have real-time metadata feeds from Telstra, Optus, NBN etc. They have access to your location estimate, URLs of sites you've visited, people you talk via email/phone etc. And we know that this dataset is shared to the Five Eyes. So if you are concerned about being tracked I would strongly recommend leaving Australia. reply null0pointer 9 hours agorootparentIt’s more about who has access to the data and how easily. Plus the potential of creating a massive target for hackers. reply ENGNR 12 hours agorootparentprevAnd now they’ll be able to see the groups you go to within pages, read comments, see what we write, etc etc. It also goes from being a defence capability to used for all sorts of things and eventually leaked. It’s not bad enough to leave, better to engage with the politics and try to get some rights before it spreads further outwards reply RichEO 11 hours agorootparentThis is conspiracy theory thinking, why on earth would age verification give them access to all of this additional information? reply 71bw 11 hours agorootparentIn what way do you think they'll be able to verify your age reliably without you having to, yourself, \"willingly\", hand over your PII? reply Pedro_Ribeiro 13 hours agorootparentprevAren't videogames where you consume drugs for gameplay boosts banned there? I always found that weird for a first world country. reply ponco 12 hours agorootparentFor a time, we did not have an “R” rating for video games and this sort of content called for this rating, which legislation said could not be given. Fortunately saner heads prevailed and they created an “R” rating for video games and this oddity went away. reply Der_Einzige 12 hours agorootparentprevForced voting is proof of it. Australia is by far the least free Anglo nation. I will never step foot on that god forsaken island. They have no culture, and that’s the opinion of Australians: https://en.m.wikipedia.org/wiki/The_Lucky_Country https://en.m.wikipedia.org/wiki/The_Australian_Ugliness reply linuxandrew 12 hours agorootparentHa! We have compulsory voting but unlike many Anglo countries we don't require voter ID, vote registration etc. In fact you do not need to provide any ID to vote, because voting fraud is so statistically low (see https://www.rmit.edu.au/news/factlab-meta/voting-fraud-negli...). We simply provide a name and address and fill out the ballot. We have so many issues, but compulsory voting is not one of them, in my opinion. If you feel so strongly to not vote you can abstain by an informal vote like roughly 5% of the country does on any given election (https://www.aec.gov.au/Voting/Informal_Voting/) or simply pay the AU$20 (roughly US$13) fine like apparently around 5-10% of Australians do on any given election (https://www.aec.gov.au/Elections/non-voters.htm). In my view, and in the view of many Australians, people encouraging further \"freedom\" to not vote are attempting to suppress votes, a major issue in the United States and other countries with optional voting. reply pyuser583 1 hour agorootparentThe state I live in has an average of about 4 elections a year. Sometimes as many as 8. Being required to vote in all of them would be a large burden. reply scripturial 11 hours agorootparentprevIn Australia they ask to see your ID but you can say you don’t have it on you. I think they mostly just ask for ID so it’s easier to look up your name with the correct spelling. reply bobnamob 11 hours agorootparentCounter anecdote, I’ve never been asked for id when voting in South Australia or NSW in my 20 year voting history reply linuxandrew 7 hours agorootparentMe neither, in Qld. reply jiggawatts 10 hours agorootparentprevThey don't. Source: Australian citizen that has voted many times. reply rob74 10 hours agorootparent...in the same election? Seriously, if they don't ask for any id, how do they prevent fraud? reply Hawxy 10 hours agorootparentYou get your name + address marked off the roll when you go to vote. If you get your name marked off multiple times it would indicate fraud. reply thaumasiotes 9 hours agorootparentSo, I can save my neighbors a hassle and a $20 fine by appearing at the polls for them? reply linuxandrew 7 hours agorootparentMaybe but that would be voter fraud and quite a serious offence, like going to prison for ten years, which is why not many people do that. reply ruthmarx 10 hours agorootparentprevStopping people suppressing votes should not be solved by taking away freedoms. reply stubish 10 hours agorootparentprevYou won't win any arguments with Australians on forced voting. The major parties would love to kill it, but it is something the (forced) voters will refuse to give up. It may not be 'free', but it helps keep things free. If you are going to cite sources about 1960's Australian culture, back in the oppressive dark ages of 'White Australia', make sure you compare it with other 1960's cultures. Or try some sources from this millennium that have come to terms with not being part of the British Empire. reply Hawxy 10 hours agorootparentprev> Forced voting is proof of it. Forced voting is a net benefit, the biggest being that it forces parties to the center rather than having to say/promise stupid stuff to appeal to the fringes that have firm political positions (see: USA). Mandatory voting + preferential voting, alongside a well-run independent election commission has resulted in very high trust in our democratic process. reply ern 11 hours agorootparentprevCompulsory voting means that a large part of the electorate that doesn’t pay attention to politics is easily frightened by scare campaigns. An example of this is that Australia is sorely in need of tax reform, but any party that pushes for it at state or federal level is damaged at the polls, often fatally. reply jiggawatts 10 hours agorootparentThen how did John Howard get the GST passed? reply ern 10 hours agorootparentHe lost a lot of seats in the 1998 election and suffered a big swing against him. reply turbojet1321 12 hours agorootparentprevYou have to remember, Australia as a nation is young, and has an interesting history - forced migration of convicts, high levels of immigration (IIRC 25% of the population are 1st or 2nd generation immigrants), and of course the difficulty of dealing with colonial treatment of the Aboriginal population. \"No culture\" is patently absurd; everywhere with people has a culture. And on mandatory voting: yes, in one way, that's a curtailment of freedom, but in another way, it's enshrining freedom. reply threeseed 12 hours agorootparentprevAustralia is one of the freest countries in the world: https://worldpopulationreview.com/country-rankings/freest-co... Higher than UK and the US. reply throwawaythekey 10 hours agorootparentIn day to day life we are fairly free but for example we have much weaker freedom of speech/opinion than the USA. For example if you raise your arm at a particular angle you can now be sent to jail [1]. We also had some of the longest/harshet COVID lockdowns in the world in my state. [1] - https://www.dw.com/en/australia-man-convicted-for-performing... reply wqaatwt 11 hours agorootparentprev> Higher than UK Not according to https://rsf.org/en/index or https://en.m.wikipedia.org/wiki/World_Press_Freedom_Index I assume that other indices that show “data” that fits whatever narrative one wants are easy to find… Which makes things like this meaningless in isolation. reply liammclennan 12 hours agorootparentprevCompulsory voting seems to push against the current trend of political polarization. Also, not sure Australia is, or wants to be, an \"Anglo nation\". reply ern 11 hours agorootparentIt’s not polarised, but it is quite sclerotic. reply lmm 13 hours agoparentprevThis is the opposite of democracy - a group imposing rules \"for their own good\" on people it doesn't represent who have no recourse. If they had any integrity they'd be banning it for adults not children. reply trial3 13 hours agorootparentthis is what a seven-year-old would sound like if they were writing a persuasive essay about how they shouldn’t have a bedtime reply kumarvvr 12 hours agorootparentprevHow can you protect children from the absolute devastating effects of social media on children, by banning it for adults? reply voidfunc 12 hours agorootparentDemonizing social media is this generations version of demonizing rock and roll. It's so tiring. reply usr1106 11 hours agorootparentThe harm of social media is closer to alcohol or tobacco. Addiction, even though with milder physical conseques. Nothing to do with demonizing. reply poilcn 11 hours agorootparentprevIs it? You can see the actual harm caused by web sites. Doomscrolling in all age groups. Going to bed very late at night. Girls having body dysmorphia cause of instagram. Young men having porn addition and having no ambition to interact with real women reply Aerroon 11 hours agorootparentNone of these sound like they are a very big problem compared to many alternative things people do. People talk about social media as though it's lead paint. I particularly liked that one Facebook study that is usually taken out of context. reply eimrine 11 hours agorootparentprevThe latter is the scariest but the least binded to a social media (except of underground ones which will never have age verification anyway). reply throwthrow4567 11 hours agorootparentprev> young men having porn addition and having no ambition to interact with real women this made me lol so much. porn addition is not the cause, but the consequence. do you have any idea how hard it is for males to find a willing mate nowadays? most females have men fighting over them, while most men must always do the fighting to get even one low-quality female in their entire life. reply ryan-c 9 hours agorootparentI wish you could see your comment as most people see it. When you refer to \"males\" and \"females\", and especially use terms like \"low-quality female\" you sound absolutely unhinged. reply throwthrow4567 4 hours agorootparentor realistic maybe? people are too keen to forget that we share 98.8% DNA with chimps... reply GeoAtreides 11 hours agorootparentprevjust because thing A was (unjustly) demonized doesn't mean demonizing thing B is without merit, even more so when thing A and thing B are completely unrelated. reply Timwi 11 hours agorootparentprevI agree with the OP that the ban is woefully undemocratic, and that banning it for children only is a grave misstep. I think what they should ban instead is recommendation algorithms. If I subscribe to a source, and explicitly unsubscribe from another, it should be illegal to withhold some of the first’s postings and shove the second’s in my face. This should be a no-brainer and has nothing to do with the age of the user; but it's easier to just ban the people who, as OP correctly noted, have no representation and no recourse. reply labster 12 hours agorootparentprevI mean there has been at least one genocide planned on social media. Maybe adults were the real danger after all. reply eimrine 11 hours agorootparentDoes Ukrainian's Revolution of Dignity counts? reply krisoft 11 hours agorootparentprevDoes your reasoning also apply for laws which ban underage smoking or underage alcohol consumption? Do you feel the same way about those prohibitions too? reply lmm 11 hours agorootparent> Does your reasoning also apply for laws which ban underage smoking or underage alcohol consumption? Up to a point, but AIUI there is credible medical evidence for those being disproportionately harmful (in physical, objectively verifiable ways) to the young. I think setting the same standard of harm and applying it to all ages is reasonable; maybe this law is based on some claim that social media harms children in a way it doesn't harm adults, but bluntly given how much the topic is biased and politicised I just don't trust today's social science establishment enough to justify this kind of law. reply doctorpangloss 12 hours agorootparentprevYes, but then how could they find audience for political ads, if not in social media? reply refurb 6 hours agorootparentprevSave your breath, Australians don't have the same views on government or individual rights as America (or even Canada!). I lived in Singapore for a while and it's a \"flawed democracy\" where the government has stacked the deck against any opposition party to a degree that's breathtaking. But a poll in Singapore showed that 70% felt that \"social harmony is more important than democracy\". Even if Singaporeans knew how undemocratic the government was, they wouldn't care. reply Der_Einzige 12 hours agorootparentprevCorrect. Children deserve rights, representation, and bodily autonomy. Today they have none of this reply threeseed 12 hours agorootparentWe need to turn down the volume here. Not being exposed to social networks for a few years is no way comparable to depriving them of bodily autonomy. reply BrawnyBadger53 12 hours agorootparentThey are already mostly segmented off from their friends after school hours. They have killed off one of the final mediums for interaction and are preventing interaction with the rest of their generation's culture. They're also narrowing their world view to be more controlled by the state. reply doxick 12 hours agorootparentHasn't that been the case for thousands of years? It's not like they can't see each other after school hours... I know i used to. And i still see my friends after my workday. I have much deeper connections with the 20 people i see in real life than the 1000 people on my linkedin profile. Do people thrive more in their mental health when they are supposedly 24/7 accessible? Is it necessary? Is it wanted? reply lmm 12 hours agorootparent> It's not like they can't see each other after school hours... I know i used to. Things have gotten much worse since then. These days children get reported if they're out and about unsupervised. reply thombles 7 hours agorootparentprevActually this is a separate problem. If you look at the work of Jon Haidt who promotes the kinds of measures we're talking about here, it's only half the story. The other half is that we have become ridiculously overprotective in parenting in recent decades. Kids need independence and the ability to hang out and play with their peers away from direct adult supervision. The goal isn't to take away the internet and leave kids with nothing, it's to bring back the real-world contact and relationship-building. reply lmm 4 hours agorootparent> The goal isn't to take away the internet and leave kids with nothing, it's to bring back the real-world contact and relationship-building. Ok but what's the betting on which of those the actual effect of the law is going to be? reply Der_Einzige 11 hours agorootparentprevNo, we need to turn it WAY up. We circumcise AKA male genital mutilation to hundreds of millions of male children all across the world, including much of the USA and Australian populations. That’s just one tiny example, and no one is calling for circumcison bans simultaneously. Letting the mutilated children have some social media is the least the state can do for them. Australia is a tyrannical hellscape. reply turbojet1321 12 hours agorootparentprevUm, that's exactly what a democracy is - a tyranny of the majority. reply Timwi 11 hours agorootparentYou missed the point. Tyranny of the majority is one thing if the minority can at least vote and participate in the political process. Shutting the minority out entirely is quite different. reply gonzo41 13 hours agorootparentprevWe do have recourse. What you'll see is more independent candidates get voted in to overturn the law. Once the government of the day starts badly implementing it the conservatives who voted with the center left party will split off and start attacking for it's repeal. It's a nothing burger law designed to look tough and do nothing. reply larrysalibra 13 hours agorootparentunder 16 year olds cannot vote, are not represented and have no recourse. when one group votes to make rules for another group that cannot vote, it can be called many things, but \"democracy\" is not one of them. reply bluefirebrand 12 hours agorootparentWhen adults make decisions for children it's called \"parenting\" Acting like we should be seriously treating children and teens as an equal political group is a joke reply Timwi 11 hours agorootparent> When adults make decisions for children it's called \"parenting\" Is it? Last time I checked I thought that was only when parents or legal guardians do it to a small number of children in their care, not when politicians do it to all children in an entire country. But even if I accept your premise, your comment makes me wonder if you've never heard of people who are bad at parenting, or who are downright abusive to the children in their care. reply bluefirebrand 2 hours agorootparentLet me ask you something: Do you support removing liquor laws banning underage people from being sold alcohol? Or removing laws that ban the sale of cigarettes to children? How about gambling or buying lotto tickets for childrenm I think it's clear that as a society we have already decided that government has a role in establishing legal protections to prevent children from falling afoul of systems that are designed to be predatory This is just another layer of that Which also establishes a social norm that letting children drink alcohol, smoke cigarettes, or gamble is not a good thing, so people who are bad parents know at least some baseline of what they should not be doing reply casey2 10 hours agorootparentprevWho defines what a child is? Who defines what a elderly person is? Are these questions also jokes to you? You seem pretty flippant about deciding which groups shouldn't have political power. Many countries already tried using objective criteria to decide who gets to vote, and this always results in policy that screws nonvoters at the benefit of voters. Do you think the housing crisis all around the western world is an accident? It's not. The electorate chose this because it benefits them. Is it an accident that the last 2 US presidents are pedophiles? Probably, but that would be much less likely with a younger electorate. A decision was made about how children's lives should function without their input. Right now do you believe that the class of parent voters votes as representatives of the interests of their children and their future? Or do you believe that all persons under 16 have no concept of time or political interests and couldn't even understand if a politician was making them a good deal through a political ad? Children understand brand new toys better than anyone; by high school, pretty much all of them understand that they get better teachers if you pay more. Are students not interested in getting better grades for \"free\"? The government is not anyone's parent, it doesn't give a lick if your kid dies tomorrow, cause kids don't vote. reply amatecha 12 hours agorootparentprevHey, didn't you get the memo that teenagers know everything, have the simple & straightforward solutions to all of life's problems, and are never wrong?? ;) reply pizza 11 hours agorootparentprevI'm not sure what's worse: giving parental responsibility to politicians or equating craven paternalism to benevolent parenting. reply eimrine 11 hours agorootparentprev> When adults make decisions for children it's called \"parenting\" Children issue is just the excuse for government to get people obey. Sadly but \"kids protecting\" propaganda is one of two the most effective ones, works great and there are lots of alternatively gifted persons that do not get the real attitude. reply photonthug 10 hours agorootparentprev> Acting like we should be seriously treating children and teens as an equal political group is a joke Yeah this thread is wild, maybe because those speaking “on behalf” of children here are actually all children? Age restrictions for social media are as logical and necessary as they are for driving, drinking, etc. It isn’t just a concern about self-harm. The general public has a stake in this too in the long run, and it’s a safety and security issue for them as well. (If you don’t believe this is true, just think about how much power Facebook already has over elections, and how much more they will have if literally everyone alive grows up on Facebook and doesn’t think that power is worth questioning) Years from now we’ll all be surprised we didn’t arrive at this conclusion sooner. reply threeseed 12 hours agorootparentprev> are not represented Young Greens/Labor/Liberal allow members as young as 14. And these groups have significant and direct input into political policy. So simply not true to say that have no representation. reply matt_trentini 13 hours agoparentprevYou're far more optimistic than I about our government being able to implement a secure, reasonable solution for age verification. COVIDSafe was the last technical undertaking and it was expensive and a completely inept implementation. The MyGov website is another failed attempt at keeping personal data secure. Further, it seems likely that social media companies are likely to come out of this with even more information about us. Government and tech do not mix well (at least in Australia). reply russelg 13 hours agoparentprevPeople are far more worried about the government knowing that you're using a social media site, than they are about the social media site knowing who you are. I don't see a way this could be implemented where the govt doesn't know what site is requesting the verification. I'm assuming it'll be an openid type flow where the social media sites will have to register client IDs with the govt myID, in which case the govt will directly be able to tie a person to what social media they use. It won't tell them what account it links to on the social media side, but depending on what data is returned, they can easily just ask the social media company for this info later on. reply tzs 12 hours agorootparent> I don't see a way this could be implemented where the govt doesn't know what site is requesting the verification Blind signatures. Briefly, a blind signature is a way for a party to sign a document without seeing the contents of the document. The cryptographic forms of this, at a high level, work like this: 1. You do a keyed reversible transformation on document D that produces a transformed document D'. This is called \"blinding\" the document. 2. They sign D' with signature S'. 3. You apply the reverse transformation to S', which gives you a signature S from them for D. This is \"unblinding\". Use a random key each time you need to get something blind signed and throw away the key afterwards. Even if they later see D and S they can't match them up with any D' and S' because they don't know the key. For age verification D would be some kind of token you obtain from the social media company during age verification. You'd then have the government blind sign that with a signature that is only used when the government has verified you are at least 16. You'd unblind the signature and give that back to the social media company. There are also protocols to do this using zero knowledge proofs. reply hcfman 12 hours agorootparentprevI suspect that it is technically possible to make an anonymous identification service because the result to the social media site just had to be yes or no. In the Netherlands you have a government identification service that identifies people to other government sites. And a bank service that uses the banks identification service also roll to identify to other sites. Technically it would be possible to delete any trace afterwards. However. I have never ever in my life seen any government choose not to take advantage of an opportunity to exert more control over their citizens if the possibility exists. Plus rather than force it on everyone it should be a choice of the parents. Clearly not doing this is better but in the absence of that parents deciding is better for the others. reply greggsy 13 hours agorootparentprevThe proposed digital ID solution is designed to protect the individual from this type of information disclosure. reply kaladin-jasnah 13 hours agoparentprevIt's going to be interesting to see how one can use cryptography to do this privately. I wonder if the cost and complexity of such a thing would result in big companies simply requiring some kind of \"take a picture of your ID\" style verification. Estonia's ID card https://www.id.ee/en/ could certainly be a model; still not sure how to do age verification. My best guess would be some sort of cryptographic signing that refuses to sign if you are below a certain age. reply chris_wot 9 hours agoparentprevlol - hardly! Submissions were announced on the 22nd, this closed on the 23rd, the Committee was so blindsided they literally requested submissions should 1-2 pages long, the bill was introduced to the House on the 26th and passed on the 29th. That’s some really well reviewed legislation. Parliament sitting days close on the 31st. reply magnetowasright 9 hours agoparentprev> functional democratic government Wait, where? Not in my Australia. Saying our govt has any competency is also gobsmacking. Yep, it's better here than other places (for certain groups) but when the notorious hate rag daily mail condemns (literal) neo-nazis marching and saluting in support of transphobe Kellie-Jay Keen-Minshull you've really got to question what kind of functional democratic government or expertise leads and legislates this country. Literally the daily mail thinks our neo-nazi problem is a bit much. I genuinely can't wrap my head around that. The daily effing mail. That's the state of this country; what a healthy democracy that literal neo-nazi groups are free to demonstrate against human rights while the police force people doing nothing more for human rights than holding the pathetic signs back from the streets so the frog marching idiots can continue unimpeded. Utterly disgraceful state of affairs. It's so embarrassing. This legislation isn't going to stop kids accessing social media or prevent any of the related social harms, unfortunately. It's just going to give licence to erode privacy even further. All our pollies are so dangerously incompetent, especially when it comes to anything relating to computers, it's just enraging. It's a disgrace. Our governments don't give a single shit about human rights or the welfare of the people in our country. This law is no different, but it sure is a great distraction. https://www.dailymail.co.uk/news/article-11879737/Neo-Nazis-... reply isaacremuant 13 hours agoparentprevOr, you know, don't keep going into this path of authoritarianism. Your comment is absolutely disingenuous pretending that this Draconian move can be implemented with open society or transparency. This is the same nation that went absolutely bonkers with their COVID policies and their inhumane treatment of anyone who desired freedom or bodily autonomy. reply lobochrome 11 hours agoparentprevDear god, no. This doesn't need more technology. The whole purpose of this is only to give parents a reason to say no. Often, a single kid in a class with an unrestricted iPhone is enough to poison the well. Now it's much, much simpler. Kids understand when you say it's against the law. reply 71bw 11 hours agorootparent>Kids understand when you say it's against the law. Jesus, were you born 40? I don't know a single person who didn't willingly pirate media, games and break other laws (usual suspects like trespassing, underage + public drinking...) as kids and teens. They will not care about the law if they want to do something it limits. reply lobochrome 9 hours agorootparentI’m not called Jesus and I age normally. Do you have children? reply bongodongobob 5 hours agorootparentJust stop. You're being incredibly naive. reply est 12 hours agoparentprevyou must be joking, that's exactly how communist government works. Given strict laws but \"don’t prescribe any particular ... methods\" so one day law enforcements can extort a large sum of fine from companies as needed. The only difference is Commie leaders invent bullshit \"laws\" by themselves and no voting. reply llm_trw 13 hours agoparentprevnext [2 more] [flagged] infradig 13 hours agorootparentAustralia was founded in 1901 as a federation of free states. reply JeremyStinson 9 hours agoprevJust so it's clear - this new law will require everyone over 16 to provide proof-of-age, and thus identity, to use the most popular portions of the internet. Saving the children is the voter-acceptable way of getting it past public scrutiny, and paves the way for a national DigitalID that will be required by all citizens. reply underwater 9 hours agoparentWhere does the legislation say that? My reading is that it specifically says that social networks have to provide an alternative verification mechanism that doesn’t rely on government ID. reply JeremyStinson 9 hours agorootparentThat's the outcome of the law. Think of it this way - how will YOU, specifically, prove you are actually over the age of 16 without having some proof of age object that is tied to your device(s) or usage patterns? If a 15 year old will have to prove they're 16 to use a service, so will a 35 year old. It's not just the kids proving their age. reply underwater 7 hours agorootparentThe legislation is literally > A provider of an age-restricted social media platform must take reasonable steps to prevent age-restricted users having accounts with the age-restricted social media platform. It doesn’t specifically require them to collect IDs. However it does say the opposite and that the site can only collect government ID or a digital ID information if: > the provider provides alternative means [not involving IDs] for an individual to assure the provider that the individual is not an age-restricted user; and (b) those means are reasonable in the circumstances. I’m not going to argue that the legislation is perfect. But it doesn’t actually do what most opponents are accusing it of doing. reply bongodongobob 5 hours agorootparentprevSo a burner email account? How do you think this is going to work? reply neumann 9 hours agoparentprevCan you elaborate what you mean? We already need identity for ISP access and mobile phones, no? reply JeremyStinson 9 hours agorootparentOnly for activation of a service, which has nothing to do with the actual user. Purchase a phone using cash from any store selling mobile phones, and then use free WiFi (however insecure that may be) anywhere you can find it. If you want mobile data, get someone to register a prepaid SIM card, then add money to it using a prepaid debit card from AusPost, which you don't need ID to do. reply znyboy 12 hours agoprevI personally welcome this change. Anecdotally, I experienced unimpeded and unsupervised internet access throughout my younger teenage years, and was exposed to some truly horrendous material courtesy of the bigger social platforms. My parents knew I used social media. They believed it was fine because it was \"just MySpace and Facebook\". Meanwhile, I witnessed large amounts of open bullying between my peers, recordings of physical assaults, underage revenge pornography, and a massive decline of people physically spending time together outside of online contexts, culminating in widespread loneliness and isolation. In recent times I have seen the dramatic failure of social media companies to moderate their content, actively promoting extremist content, and even openly protecting the originators of illicit material. Rates of youth suicide skyrocketed shortly after the rise of smartphones, social media and always-available internet access as evident by published statistics. While none of these can be individually attributed as the cause of this rise, all are undoubtedly a contributing factor, as suggested by countless studies globally. reply throwawaythekey 10 hours agoparentI started using the internet unsupervised heavily in my tweens in the early 2000's and came across some horrendous stuff pretty early. Despite that, for me I think it's been a net positive and I would like my children to grow up in a similar environment. The lasting qualities I think it's given me are: - An open mindedness and the ability to examine issues from multiple angles - A thick skin against shocking material/online forms of bullying - A stronger understanding of how technology interacts with society/power structures I think the country needs to be building digitally strong youth, not trying to put the genie back into the bottle. reply anal_reactor 10 hours agorootparentNothing I have ever seen on the internet comes anywhere close to what I was dealing with IRL. reply lysp 11 hours agoprevThe \"behind the scenes\" of this was a repeated campaign based on News Corp's (ie. Fox News / The Murdoch's) newsrooms (TV, radio + paper). The legislation was primarily based on their campaign - likely as an attack against TikTok and other social media companies, who are taking their viewership and advertising revenue. I believe in the premise of the change (to protect young children), just not the motivation or interests behind this law. reply willyt 10 hours agoparentThere’s an interview on The Rest is Politics Leading podcast with the politician Peter Malinauskas the premier of New South Wales who started the introduction of this law in his state first. I think it was maybe then taken up by the federal government. https://podcasts.apple.com/gb/podcast/leading/id1665265193?i... It’s very insightful and from listening to this it seems unlikely that murdoch was a huge influence on this. reply kreetx 10 hours agorootparentEven if they were, aren't they are unlikely to gain much from this. I.e, when the average teenager can't consume social media platforms anymore, they surely won't go back to a more traditional outlet? reply ptstomp 10 hours agorootparentprevJust as a note. Peter is premier of South Australia. Same political party as national and NSW though reply willyt 10 hours agorootparentApologies you’re right, I looked it up but my brain had obviously already decided it was NSW as that’s what I wrote. It’s a great interview, he seems like a good guy. reply krige 11 hours agoparentprevI rather doubt under-16s ever watched any news corpos and the new corpos are more than likely aware of this from their own research so I don't think fighting over viewership is a factor in this. reply lmm 10 hours agorootparent> I rather doubt under-16s ever watched any news corpos Well, no, because social media is a better alternative. Hence why they're getting it banned. reply krige 10 hours agorootparentNo, because kids don't care about news channels. It's not a darn kids on tiktoks thing, before tiktok or twitter or facebook they were outside, or playing video games, or reading books, or any number of things because underage kids don't give a hoot about talking heads newsstations. reply lysp 9 hours agoparentprevEdit: Quick summary of this: https://www.youtube.com/watch?v=S8SkLRxFRVM (First 8 minutes) reply SamPatt 13 hours agoprevI don't like nanny state policies like this, but hopefully we'll get some data about social media usage from this after a few years. Prediction: the policy was both difficult to implement and difficult to determine if it made any impact, positive or negative. reply chrishare 12 hours agoparentI've always wondered, when do policies become nanny state policies? If they have data showing that under-age use of social media leads to higher rates of suicide, why would that be a 'nanny state' policy? Does there have to be a gun or drug involved, rather than a phone? reply niek_pas 11 hours agorootparent“Nanny state policy” just means “restriction of freedom that I don’t personally like”. reply xxs 10 hours agorootparentI am sure someone else would point the origins of 'nanny-state' comes from. Yet, it has been used initially in the UK to support advertising of smoking, later for support of not using seat belts, and so on. It's a rather ironic twist in this case... and a very favorable read. reply dools 11 hours agoparentprevI don't like people who refer to government regulation as \"nanny state policies\". reply chris_wot 10 hours agoparentprevThis legislation has no teeth. There is nothing that forces social media companies to verify ID. In fact in the legislation social media companies can’t force users to provide ID. It’s stupid legislation pandering to the lowest common denominator by a Labor government who have no vision and no idea how to deal with very real cost-of-living problems for Australians. The only problem is the LNP are even worse, an the Greens are nuts, so we’re stuck either anthem because they are the best of a terrible bunch. reply nixpulvis 12 hours agoprevThis is a bad idea. We should be forcing demonetization of the platforms content creators instead of hamstringing children. No I'm not joking. Content creators making money should be outside where I post about my cat. I'm fucking sick of engagement bait. reply OsrsNeedsf2P 12 hours agoparentThe money driven content creators are going to thrive on platforms that support them. If you don't want to have their content on the same feed as your cat pics, you're going to have to find some alt networks reply nixpulvis 12 hours agorootparentSupposedly Bluesky isn't monetized? reply nomilk 12 hours agoprevWhen the laws come in, many Australians will get VPNs rather than provide ID to random websites. If social media sites are compelled block VPNs, is that technically achievable/viable? Cloud providers and VPN services have predictable IP ranges, so are easily blocked (/majorly inconvenienced) by sites motivated to do so. But social media companies might not want to block VPNs since a lot of legitimate traffic comes through them (e.g. people anywhere in the world who simply wish to have that extra layer of privacy from their ISP and wifi owner). Are there VPN strategies that will allow an Australian to reliably access social media as though their traffic is coming from another country? I also guess Australians who try this might have to access social media sites via a browser on their phone (as opposed to native app). Asking for a friend, of course. reply throwawaythekey 10 hours agoparentDue to the shared nature of the internet it's very hard to implement a technically foolproof block. For example you could rent a server on AWS, forward your traffic to it (e2e encrypted) and then have that forward your traffic to a regular VPN. Now just imagine that the VPN does the forwarding for you and offers it as a service. reply t0bia_s 44 minutes agoprevWhat is a definition of social media? If there will be another platform, that is not listed on banned platforms, will they add it to regulation later... And later again, another...? reply tigerlily 40 minutes agoprevI wonder what counts as social media? What about traditional bbs, or irc? reply defrost 13 hours agoprevIn the past 24 hours: Australia: Kids under 16 to be banned from social media after Senate passes laws https://www.abc.net.au/news/2024-11-28/social-media-age-ban-... 256 points|llui85|17 hours ago|385 comments - https://news.ycombinator.com/item?id=42264733 ( and seven others still alive and several others flagged [dead] ) reply michaelhoney 12 hours agoprevThis is a bad idea, and it’ll be very hard to implement. Better to ban old people from watching Sky News and using Facebook, that’s the real danger to our democracy reply discordance 12 hours agoparentThey’re not mutually exclusive, and this isn’t about democracy. We know that developing brains have been deeply impacted by the easy dopamine engineered hits Meta and Bytedance deliver. To me this feels like a contemporary version of the opium wars, but instead of nation states at war, it’s companies vs people. reply hcfman 12 hours agorootparentPerhaps we should also make it illegal to report on the war in the Middle East in case young people find it traumatising to see that that the western democracies are supporting dropping 2000 pound bombs on children in other countries. Can you image the damage being caused to young minds seeing that death and destruction visually through their own eyes ? reply discordance 9 hours agorootparentThis is not about reporting or content. This is about the health outcomes of young people being negatively impacted by companies maximizing engagement metrics at all costs. Reporting on war happenings occurred before social media was around. reply hcfman 4 hours agorootparentI'm referring to the ridiculous concern about young minds being exposed to one sort of content, yes it is about content, whilst another sort there's no concern about in the slightest. The choices are bizzare. I assume that discord channels relating to specific pieces of software will also not be allowed anyone under 16. So very clever young kids trying to build software will be disadvantaged now as well. reply nixpulvis 12 hours agoparentprevFacebook already does this with users under 13. Read here about how they do it: https://about.fb.com/news/2021/07/age-verification/ But I agree it's a bad idea. See my comment above or below. reply walrus01 12 hours agorootparentThe idea that your operating system might share with meta your personal data like your age is pretty messed up. reply switch007 10 hours agoparentprevI had to correct my auntie that no, Morocco did not manipulate the weather to cause the floods in Spain. She didn't really believe me. The Facebook propaganda is intense reply mhh__ 12 hours agoparentprevWe should also ban people who disagree with me from not agreeing with me reply cchi_co 10 hours agoprevThis legislation is a bold step, but it raises a lot of questions about implementation and unintended consequences. While protecting children from online harms is undoubtedly important, enforcing an age limit like this on social media platforms could be incredibly challenging reply xxs 10 hours agoparentIt already works for gambling. It will reduce the access to adults, of course, but I can't say it'd see it as a negative reply itshossein 9 hours agorootparentgambling is a little bit different, as you have to pay and receive money (easily enforceable bans). But here we are talking about a free service which is also offered by many foreign alternatives (which of course are not going to cooperate.) reply xxs 9 hours agorootparentThe age verification is on creating the account prior to any deposits. As for not cooperating - I'd expect to ban them altogether (ISP) unless they comply. Gambling is exactly the same. reply hcfman 9 hours agorootparentprevThose who will trade privacy for safety deserve neither. reply semi-extrinsic 10 hours agoprevA solution I would be very OK with is the following: * Enforce the age limits only on smartphones and tablets. This solves >95% of the problem and still leaves a way for people to be anonymous. * Smartphone OS vendors come up with a common age verification framework that's based on zero knowledge proofs constructed over the electronic certificate in your passport - there are already working PoCs of this on Github * Each identity can only be installed on one device at a time. Then parents cannot share identity with kids (unless the parents completely give up on all social media) * For people who don't have a passport, offer a manual and th",
    "originSummary": [],
    "commentSummary": [
      "Australia has implemented a ban on social media usage for individuals under 16, igniting discussions about child protection versus parental rights.- The law mandates age verification, which has raised privacy concerns and fears of increased surveillance.- The legislation was expedited with minimal public input, leaving its effectiveness and enforceability in question."
    ],
    "points": 184,
    "commentCount": 304,
    "retryCount": 0,
    "time": 1732856517
  },
  {
    "id": 42269897,
    "title": "Screen Sharing in the Browser",
    "originLink": "https://github.com/tonghohin/screen-sharing",
    "originBody": "Seeing all the hype around Bolt.new, I decided to give it a try, and within just 30 minutes, I built a fully functional web app for screen sharing.How It Works:- One user opens a room and receives a unique code.- Participants join by simply entering the code to view the shared screen.After that initial build, I moved the project to Cursor to continue iterating and enhancing its features.Github: https:&#x2F;&#x2F;github.com&#x2F;tonghohin&#x2F;screen-sharingDemo: https:&#x2F;&#x2F;share-your-screen.vercel.app",
    "commentLink": "https://news.ycombinator.com/item?id=42269897",
    "commentBody": "Screen Sharing in the Browser (github.com/tonghohin)168 points by tonghohin 17 hours agohidepastfavorite39 comments Seeing all the hype around Bolt.new, I decided to give it a try, and within just 30 minutes, I built a fully functional web app for screen sharing. How It Works: - One user opens a room and receives a unique code. - Participants join by simply entering the code to view the shared screen. After that initial build, I moved the project to Cursor to continue iterating and enhancing its features. Github: https://github.com/tonghohin/screen-sharing Demo: https://share-your-screen.vercel.app hobo_mark 35 minutes agoSimilar tool I like and used to self-host for personal use: https://screego.net/#/ reply thrdbndndn 14 hours agoprevDid a quick test on Demo site. It works like a charm. One issue: when I stopped sharing, my audience (my another computer) can still see my shared screen (I assume the last frame), what I think it shouldn't. And there is no indication or whatsoever that I have stopped sharing. reply tonghohin 14 hours agoparentThanks for the feedback! I’ll check out the issue. It still has a lot to improve! reply atmanactive 7 hours agoprevGreat! Thanks. While we're on the subject of WebRTC, if anyone would need MIDI over WebRTC, here is my take on it: https://github.com/AtmanActive/webmidi-rtc-transport reply Ylpertnodi 4 hours agoparentPerfect timing! Seems to suit (some of) my needs, thanks. And +1 for mentioning sonobus - that's a lot of fun, too. reply ghusbands 4 hours agoparentprevBrowser-based screen sharing is the subject. Is it normal to advertise unrelated projects in HN comments? reply atmanactive 1 hour agorootparentI don't know. Is it? The afore mentioned webapp is based on WebRTC so, not totally unrelated. I have nothing to advertise as I have nothing to gain from this. I just wanted to offer additional solution for people that might find it useful. Also, judging by the other comments, it seems that someone did find it useful. Furthermore, reading through other comments on this thread, there are a lot of other WebRTC projects mentions anyway, and many of them have nothing to do with screen sharing. reply karhuton 6 hours agoprevTried making one that would work on almost any device: https://bluescreen.live https://github.com/karhuton/bluescreen Runs on Hetzner EU currently. Maybe I should open source it. reply mvkel 4 hours agoprevWhat I love about developing with cursor et al is it forces devs to keep the app simple. No custom UI bloat when shadcn will do; no feature creep when the outline is fulfilled. Simple appschoose proceed as anonymous -> In your space, use chat or other apps. It's end to end encrypted (using the hash-frag in url as key, AES-128 bit GCM) reply tasoeur 9 hours agoparentprevSomething like this? https://offrecord.ca/#lobby reply croisillon 10 hours agoparentprevJitsi Meet? reply cess11 9 hours agoparentprevDo some Phoenix tutorials and you have your own. reply chuanliang 10 hours agoprevNice work. Similar to https://pin.gl/ reply IshKebab 11 hours agoprevIf you can get remote control to work that would be amazing. I assume that's impossible with current browser APIs though. A real pain point of Google Meet. reply atoav 11 hours agoparentI too yearn for the day where my parents webbrowser can run Win+R and delete their System32 directory after they clicked \"Ok\" on a prompt whose message they wouldn't be able to tell me if I paid them a billion Euros for it. reply IshKebab 9 minutes agorootparentIt can already do that. The prompt just says \"Click here to download\". reply mandeepj 14 hours agoprevGave it a quick run! Works great. The notification to share your screen goes away quickly. Is there any way to get back to it? I generated a new code to fire the notification again. reply tonghohin 2 hours agoparentI just fixed that, the share button should stay forever now until you click it. reply yroc92 13 hours agoprevDoes this share your screen across domains? If so, how? reply 5evOX5hTZ9mYa9E 9 hours agoprevDoesn't work on Linux? reply sgrodnik 10 hours agoprevI'm not sure, but is it possible to make it work on Android? I've tried on my pixel 7a, it gave me the code, I pasted it in another device in Safari, but it was only Loading... on the button. reply tonghohin 2 hours agoparentYeah, it's not working that well on phones right now, working on it... reply 1oooqooq 14 hours agoprevwhy the source have an accordion and carousel component? :) reply ddtaylor 14 hours agoparentAre you talking about in components/ui? If so that's ShadCN and it works by fetching components and copy/pasting them into your project rather than trying to continue to keep the components up to date. reply tonghohin 14 hours agoparentprevdon't know why Bolt.new downloaded all the shadcn components for me lol reply reynaldi 14 hours agoprevAwesome demo! Are you paying for the TURN server? reply tonghohin 2 hours agoparentNo, it's not using TURN server now. reply humptybumpty 12 hours agoprev- reply kapitanjakc 11 hours agoparentFor mobile I find google meet the best, I think even Whatsapp is now allowing to screen share. reply lopkeny12ko 3 hours agoprev [–] Immediately upon loading the homepage: > Application error: a client-side exception has occurred (see the browser console for more information). reply tonghohin 2 hours agoparent [–] I guess you visited the page right after a new deployment. This happens sometimes due to a version mismatch in Next.js after deployment. Refreshing the page should fix it! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Bolt.new is a tool that allows users to rapidly create web applications, demonstrated by building a screen-sharing app in just 30 minutes.- The app functions by generating a unique code for a room, which others can use to join and view the screen being shared.- The project was further developed using Cursor and is available for viewing on GitHub, with a live demo hosted on Vercel."
    ],
    "commentSummary": [
      "A browser-based screen-sharing app was developed using Bolt.new in just 30 minutes, allowing users to create and join rooms with unique codes for screen sharing.- The app is available on GitHub, with a demo provided, and has received feedback regarding screen sharing stoppage issues and mobile compatibility.- The app does not utilize a TURN server, which is typically used to relay media in WebRTC applications when direct peer-to-peer connections fail."
    ],
    "points": 168,
    "commentCount": 39,
    "retryCount": 0,
    "time": 1732842261
  },
  {
    "id": 42274660,
    "title": "Prometheus 3.0",
    "originLink": "https://prometheus.io/blog/2024/11/14/prometheus-3-0/",
    "originBody": "Following the recent release of Prometheus 3.0 beta at PromCon in Berlin, the Prometheus Team is excited to announce the immediate availability of Prometheus Version 3.0! This latest version marks a significant milestone as it is the first major release in 7 years. Prometheus has come a long way in that time, evolving from a project for early adopters to becoming a standard part of the cloud native monitoring stack. Prometheus 3.0 aims to continue that journey by adding some exciting new features while largely maintaining stability and compatibility with previous versions. The full 3.0 release adds some new features on top of the beta and also introduces a few additional breaking changes that we will describe in this article. What's New New UI Remote Write 2.0 UTF-8 Support OTLP Support OTLP Ingestion UTF-8 Normalization Native Histograms Breaking Changes Here is a summary of the exciting changes that have been released as part of the beta version, as well as what has been added since: New UI One of the highlights in Prometheus 3.0 is its brand-new UI that is enabled by default: The UI has been completely rewritten with less clutter, a more modern look and feel, new features like a PromLens-style tree view, and will make future maintenance easier by using a more modern technical stack. Learn more about the new UI in general in Julius' detailed article on the PromLabs blog. Users can temporarily enable the old UI by using the old-ui feature flag. Since the new UI is not battle-tested yet, it is also very possible that there are still bugs. If you find any, please report them on GitHub. Since the beta, the user interface has been updated to support UTF-8 metric and label names. Remote Write 2.0 Remote-Write 2.0 iterates on the previous protocol version by adding native support for a host of new elements including metadata, exemplars, created timestamp and native histograms. It also uses string interning to reduce payload size and CPU usage when compressing and decompressing. There is better handling for partial writes to provide more details to clients when this occurs. More details can be found here. UTF-8 Support Prometheus now allows all valid UTF-8 characters to be used in metric and label names by default, as well as label values, as has been true in version 2.x. Users will need to make sure their metrics producers are configured to pass UTF-8 names, and if either side does not support UTF-8, metric names will be escaped using the traditional underscore-replacement method. PromQL queries can be written with the new quoting syntax in order to retrieve UTF-8 metrics, or users can specify the __name__ label name manually. Currently only the Go client library has been updated to support UTF-8, but support for other languages will be added soon. OTLP Support In alignment with our commitment to OpenTelemetry, Prometheus 3.0 features several new features to improve interoperability with OpenTelemetry. OTLP Ingestion Prometheus can be configured as a native receiver for the OTLP Metrics protocol, receiving OTLP metrics on the /api/v1/otlp/v1/metrics endpoint. See our guide on best practices for consuming OTLP metric traffic into Prometheus. UTF-8 Normalization With Prometheus 3.0, thanks to UTF-8 support, users can store and query OpenTelemetry metrics without annoying changes to metric and label names like changing dots to underscores. Notably this allows less confusion for users and tooling in terms of the discrepancy between what’s defined in OpenTelemetry semantic convention or SDK and what’s actually queryable. To achieve this for OTLP ingestion, Prometheus 3.0 has experimental support for different translation strategies. See otlp section in the Prometheus configuration for details. NOTE: While “NoUTF8EscapingWithSuffixes” strategy allows special characters, it still adds required suffixes for the best experience. See the proposal on the future work to enable no suffixes in Prometheus. Native Histograms Native histograms are a Prometheus metric type that offer a higher efficiency and lower cost alternative to Classic Histograms. Rather than having to choose (and potentially have to update) bucket boundaries based on the data set, native histograms have pre-set bucket boundaries based on exponential growth. Native Histograms are still experimental and not yet enabled by default, and can be turned on by passing --enable-feature=native-histograms. Some aspects of Native Histograms, like the text format and accessor functions / operators are still under active design. Breaking Changes The Prometheus community strives to not break existing features within a major release. With a new major release we took the opportunity to clean up a few, but small, long-standing issues. In other words, Prometheus 3.0 contains a few breaking changes. This includes changes to feature flags, configuration files, PromQL, and scrape protocols. Please read the migration guide to find out if your setup is affected and what actions to take. Performance It’s impressive to see what we have accomplished in the community since Prometheus 2.0. We all love numbers, so let’s celebrate the efficiency improvements we made for both CPU and memory use for the TSDB mode. Below you can see performance numbers between 3 Prometheus versions on the node with 8 CPU and 49 GB allocatable memory. 2.0.0 (7 years ago) 2.18.0 (4 years ago) 3.0.0 (now) It’s furthermore impressive that those numbers were taken using our prombench macrobenchmark that uses the same PromQL queries, configuration and environment–highlighting backward compatibility and stability for the core features, even with 3.0. What's Next There are still tons of exciting features and improvements we can make in Prometheus and the ecosystem. Here is a non-exhaustive list to get you excited and… hopefully motivate you to contribute and join us! New, more inclusive governance More OpenTelemetry compatibility and features OpenMetrics 2.0, now under Prometheus governance! Native Histograms stability (and with custom buckets!) More optimizations! UTF-8 support coverage in more SDKs and tools Try It Out! You can try out Prometheus 3.0 by downloading it from our official binaries and container images. If you are upgrading from Prometheus 2.x, check out the migration guide for more information on any adjustments you will have to make. Please note that we strongly recommend upgrading to v2.55 before upgrading to v3.0. Rollback is possible from v3.0 to v2.55, but not to earlier versions. As always, we welcome feedback and contributions from the community!",
    "commentLink": "https://news.ycombinator.com/item?id=42274660",
    "commentBody": "Prometheus 3.0 (prometheus.io)150 points by dmazin 3 hours agohidepastfavorite19 comments the_duke 1 hour agoI'm curios: are many people here actually still running mainline Prometheus over one of the numerous compatible solutions that are more scalable and have better storage? (Mimir, Victoria, Cortex, OpenObserve, ...) reply robinhoodexe 29 minutes agoparentWe’re running standard Prometheus on Kubernetes (14 onprem Talos clusters, total of 1.1k cpu cores, 4.75TiB memory and 4k pods). We use Thanos to store metrics in self-hosted S3 (seaweedfs) with 30 days retention, aggressively downsample after 3 days. It works pretty good tbh. I’m excited about upgrading to version 3, as is does take a lot of resources to keep going, especially on clusters with a lot of pods being spawned all the time. reply rad_gruchalski 33 minutes agoparentprevMimir reply raffraffraff 45 minutes agoparentprevNope. Mimir. Before that, Thanos. reply kuon 2 hours agoprevMigration to victoria metrics has been on my list for nearly a year, but the licensing of it always scared me a bit. My main issue is CPU and memory usage of prometheus, so maybe this upgrade will fix that. reply smw 2 hours agoparentApache 2 is scary? Maybe I'm missing something. reply alphabettsy 1 hour agoparentprevVictoria Metrics is well worth the migration. Much better performance and lower resource utilization in my experience. reply nine_k 2 hours agoparentprevIt's a reminder to us all that when we think: \"Hey, why sweating over this memory layout or that extra CPU expenditure, it's small and nobody will notice\", there will be times when everybody will notice. Maybe notice as much as to switch to our competitors' products. reply hinkley 1 hour agorootparentDevelopers tend to ignore C in order of complexity calculations but customers don’t. Game developers and HFTs seem to understand this, and very few regular devs I’ve interacted with do. I’ve seen customers say they switched to someone else for speed reasons. And I’ve worked on projects where the engineers were claiming this as fast as we can make it, and they were off by at least a factor of three. We like to think that being off by 10 or 30% doesn’t matter that much but lots of companies run on thin margins and publicly traded companies’ stock prices reflect EBITDA, it matters. Particularly in the Cloud era, where it’s much easier to see how sloppy programming leads more directly to hardware cost excess (as opposed to already purchased servers running closer to capacity) reply therealdrag0 4 minutes agorootparentThose margins also mean you have to pick your battles. Most software is not as performance sensitive as video games or HFT. I take an efficient market hypothesis on this. Obviously devs can make stuff faster, and they do where it matters, as can be seen in games and HFT. In other software it’s a discussion with product of trade offs. reply Fizzadar 2 hours agoparentprevVM was a game changer for us, 7x reduction in memory and 3x CPU, plus the scaling flexibility. reply oulipo 1 hour agorootparentHmmm but the documentation seems poorly written, what is the team behind? reply c0balt 3 hours agoprevThat's good news, especially the reduced memory usage and OTLP ingestion support look nice. I have experimented with OTLP metrics before but eventually fell back to prometheus to avoid adding another service to our systems. reply RoxaneFischer1 2 hours agoprevA solid upgrade :))) reply dvh 1 hour agoprev [–] I've read entire page and still don't know what it is. Release notes are communication tool and this was a failure as such. You are losing random passerbys by not telling what your product is in first sentence of release notes, especially x.0 reply moondev 1 hour agoparentBefore reading the entire page did you consider clicking the header. This will bring you to the main landing page of the product/project which more often than not contains a helpful summary of what it is and why it exists. You can also apply this pattern to other unknown things you come across. reply arccy 1 hour agoparentprevif you can't tell what it is but still read the whole thing... i think the problem is on you? reply soupbowl 1 hour agoparentprevhttps://prometheus.io/ reply cess11 1 hour agoparentprev [–] I think most people that read release notes and changelogs want the text to be concise and easy to interpret when they're doing due diligence to decide when to start rolling out upgrades. They know what the software is about and don't care for some sales pitch. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Prometheus 3.0, the first major release in 7 years, introduces a modernized UI, Remote Write 2.0, UTF-8 support, OTLP support, and native histograms.- Key features include improved protocol efficiency with Remote Write 2.0, enhanced interoperability with OpenTelemetry through OTLP ingestion, and more efficient metrics with native histograms.- Users should be aware of some breaking changes and consult the migration guide, but the release maintains backward compatibility and encourages user feedback."
    ],
    "commentSummary": [
      "Prometheus 3.0 has been released, introducing features like reduced memory usage and support for OTLP (OpenTelemetry Protocol) ingestion.- The release has prompted discussions on its scalability compared to alternatives like Mimir, Victoria, and Cortex, with some users opting for these solutions for enhanced performance.- There are concerns about the clarity of the release notes, indicating a need for improved communication from the developers."
    ],
    "points": 151,
    "commentCount": 19,
    "retryCount": 0,
    "time": 1732895564
  }
]
