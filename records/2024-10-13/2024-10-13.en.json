[
  {
    "id": 41827362,
    "title": "Starship Flight 5: Launch and booster catch [video]",
    "originLink": "https://twitter.com/SpaceX/status/1845152255944819015",
    "originBody": "Watch Starship&#39;s Fifth flight test https://t.co/LVrCnTv797— SpaceX (@SpaceX) October 12, 2024",
    "commentLink": "https://news.ycombinator.com/item?id=41827362",
    "commentBody": "Starship Flight 5: Launch and booster catch [video] (twitter.com/spacex)972 points by alecco 6 hours agohidepastfavorite583 comments ChuckMcM 10 minutes agoPretty impressive. The engineering team at SpaceX is really something. Some thoughts; The 'chopsticks catch' was amazing to watch. Seems like it adds a lot of risk and clearly the booster needs additional fire suppression systems :-) perhaps the tower could mount something that sprays the booster like the barges have for the F9 boosters. The heatshield held out for a much longer time, the asymmetric heating on the flaps was interesting. I had guessed that all four flaps would have equivalent heating based on an approach that was basically that side of the rocket perpendicular to the flow but it seems like that isn't the case. Still it seems like they are close to having something workable here. The detonation at the end was pretty spectacular too, but I suspect that structurally the tanks failed as the rocket hit the water vs anything that was an engineering failure. Engineering it to be strong enough to land on water would presumably compromise the cargo to orbit number. The use of Starlink was really interesting. The ability to get live video for the entire re-entry is pretty game changing for engineering. I'd guess there are even more 'views' than they showed (there would be if I were running things :-)) but overall that capability is something that really helps evaluate the changes made. I can easily imagine that flight 6 will be nominal end to end without any unintended damage. That would enable, perhaps, one of their 'massive' Starlink missions to test cargo delivery. It will also start to give us some better numbers on exactly how much cargo Starship can put in orbit in 'full reuse' mode which is essential if they want to create a fueling station on orbit for the Artemis program. Again, hats off to engineering at SpaceX, y'all did good. reply nordsieck 3 minutes agoparent> The detonation at the end was pretty spectacular too, but I suspect that structurally the tanks failed as the rocket hit the water vs anything that was an engineering failure. It's possible that SpaceX programed the AFTS to trigger some time after the rocket touched down in the water. Just to make sure that it completely submerges quickly. > I can easily imagine that flight 6 will be nominal end to end without any unintended damage. I think it depends on what you mean by \"nominal\". SpaceX ultimately wants to catch the 2nd stage as well. I suspect that they are a ways off of that, since it would have to approach over land. The FAA is going to need to have very high confidence that it will do exactly what it's designed to do before they're going to allow that. reply ghc 6 hours agoprevNothing could have prepared me for how that catch looked. I was sure the rocket was careening into the tower at the last second before it straightened out. The control algorithms must be incredible for the landing system to work within those small tolerances. reply dtquad 4 hours agoparentMIMO and nonlinear control theories are probably some of the hardest topics in all of engineering. SpaceX control system also has to compensate for the fuel moving inside their rockets so the control algorithms probably involve some kind of fast numerical fluid simulation. Another interesting thing SpaceX is doing is to use consumer-grade chips in triple redundancy configurations instead of using $100,000+ radiation-hardened aerospace/defense grade chips. reply arijo 3 hours agorootparent> Another interesting thing SpaceX is doing is to use consumer-grade chips in triple redundancy configurations instead of using $100,000+ radiation-hardened aerospace/defense grade chips. This has been known in the high availability and safety systems industry for a while and a good book to learn these reliability engineering techniques is \"Reliability Evaluation of Engineering Systems\". The book is available on amazon: https://a.co/d/1nH824K reply wildzzz 2 hours agorootparentOne downside of using non-rad hard parts is degradation from TID (gamma) and latch up effects. You can have chips monitoring other chips to reset whenever they latch up but TID is mostly permanent. The good thing is that TID in LEO, where SpaceX mostly operates, is relatively lower than GEO so they can get by with mostly commercial parts. It's not like the big defense contractors haven't figured out the same thing, they do fly stuff using commercial parts as well, they are just slower to adopt the same culture. SpaceX and the companies that built components using commercial parts are building the new-space industry. reply christophilus 1 hour agorootparentCould you surround your components with gamma-shielding materials and get away with off-the-shelf parts deeper in space? reply gafferongames 1 hour agorootparent\"To block gamma rays completely, you need about 13.8 feet of water, 6.6 feet of concrete, or about 1.3 feet of lead.\" https://www.epa.gov/radiation/radiation-basics#:~:text=Gamma.... reply jiggawatts 3 minutes agorootparentJust put the electronics in the middle of the header tank. christophilus 1 hour agorootparentprevA box with 1.3’ walls seems doable, actually, depending on how small the chips are. Might still be cheaper and more effective than specialized chips. But I know nothing, so am probably wrong. reply jazzyjackson 1 hour agorootparentI think the trouble is such a cube would weigh 12,400 pounds (a sphere maybe more reasonable at 6,510lb - without any room for electronics inside) https://www.wolframalpha.com/input?i=+lead+cube+with+side+le... reply paledot 1 hour agorootparentHah, beat me to the nerd snipe. Moreover, that sphere would cost $10k to make and, at a launch price of $1500/kg, cost $4.5 million to launch into orbit. reply xmprt 1 hour agorootparentprevLead is extremely dense so carry 1.3' walls of lead is probably more expensive than just having more redundancy or using better quality chips. reply persnickety 1 hour agorootparentprevCan it even be blocked completely? Every layer of material geometrically reduces the proportion of rays going through. Or am I wrong about that? reply Filligree 1 hour agorootparentNo, that's correct. Of course there's still some level of reduction beyond which the gamma rays don't matter, but where you want to place it is somewhat arbitrary. reply piyh 1 hour agorootparentprevMore water in orbit sounds like a good idea to me reply arijo 1 hour agorootparentprevYou don't need to block gamma radiation completely to increase the electronics reliability :) Maybe you could improve the system availability considerably by a bit of gamma radiation protection combined with some more parallelism of the components .. reply adastra22 50 minutes agorootparentUsually partial blockage is worse, because you end up with a spray of secondary particles instead of a single ray. reply arijo 42 minutes agorootparentMakes intuitive sense, thanks for the insight. A second layer blockage for the secondary particles wouldn't have to be as dense or am I missing important physics? (I guess a lot of gamma radiation would still reach this second layer so please ignore my question :) reply adastra22 41 minutes agorootparentKeep adding layers until you get to 1.3 feet of lead and it’ll work. reply arijo 34 minutes agorootparenthaha thanks for the correction - I was under the Turtles All the Way Down mindset :) reply numpad0 26 minutes agorootparentprevIsn't that more like how you make bombs than armors more effective - with backside spalling and secondary fragmentations? reply exe34 4 minutes agorootparentprevcan you block a hemisphere? the other 2pi steradians are shielded by the earth... WalterBright 1 hour agorootparentprevIn reading Musk's biography, \"Hollman also found that creativity got him a long way. He discovered, for example, that changing the seals on some readily available car wash valves made them good enough to be used with rocket fuel.\" \"Elon Musk\" by Vance pg 123 reply WalterBright 1 hour agorootparentprevAviation pioneered the use of redundancy in order to survive failures. The Fukushima and Deepwater Horizon disasters show that this knowledge has not penetrated other industries. reply nmca 20 minutes agorootparentprevThank you very much for the reference! reply macawfish 22 minutes agorootparentprevPeople don't realize how powerful applied math (especially in the areas you've mentioned) has become. Same tools can be applied to people in the ad tech/social media. reply alexpotato 3 hours agorootparentprevReminds me of the early days of Google File System. They used trios of regular consumer grade disks/servers etc as a cluster and it looked like a single node. They had to replace a LOT of hardware but this was still cheaper than big iron industrial grades servers. reply moffkalast 3 hours agorootparentInterestingly cheap redundancy is also how life does things for the most part. Most biological organisms just replicate a lot to guarantee success, so it's clearly a good strategy and an efficient use of energy. reply szundi 2 hours agorootparentEfficient as in evolution or in business: you just need to be more efficient than most of the peers reply autonomousErwin 4 hours agorootparentprevI always thought the liquid sloshing would be one of the hardest to simulate (considering how chaotic fluid mechanics is). Interestingly, I think this caused the 2nd Falcon launch to fail (the LOX sloshing). reply JorgeGT 4 hours agorootparentIt is difficult, but there are modeling approaches that work, such as VoF (https://en.wikipedia.org/wiki/Volume_of_fluid_method). Basically, in addition to velocity, pressure, temperature, etc., you store an additional scalar in each cell of your computational mesh representing the liquid's volume fraction. Then, you solve an additional equation to transport that scalar. reply s1291 4 hours agorootparentSolving the Navier-Stokes equations numerically in 3D is very time-consuming, even on HPC clusters, not to mention the additional modeling required for multiphase flows. Your answer implies that the solutions are obtained almost instantaneously, which is not the case. reply krisoft 3 hours agorootparentI think the reason these kind of simulations are fast enough is because they are very coarse and approximate. Don't think of asking how exactly the foam swirls around the individual longerons, more like a very rough estimation of which side of the tank the liquid is slumped to. Remember it doesn't have to be \"exact\" just close enough to be useful. By their very nature model predictive controllers operate in a world where not everything is perfectly modelled. Engineers do their best and whatever is left is the \"error\" the controller is trying to deal with. reply Log_out_ 3 hours agorootparentOr you compute variations ahead of time and do a situation based lookup, hoing through loops if a situation ressembles another one. reply keepamovin 2 hours agorootparentMaybe they don't need to model the fluid dynamics, they just need to detect the mass movement / acceleration forces caused by it, and use those sensor inputs to inform a picture that's fed into their correction thursting. Sort of like how you can balance a few pitchers of beer on a tray in your hand by remaining aware of the weight, even when people remove one! hahaha :) reply m4rtink 2 hours agorootparentStill if there indeed is \"free\" mass moving about, you need to make sure your control inputs don't make it slosh harder, so you compensate for that, so it sloshes even harder, etc - basically avoiding oscilation. :) reply keepamovin 1 hour agorootparentYeah..ah, control theory. Heh :) reply jjk166 2 hours agorootparentprevThey don't need to solve the Navier-Stokes equations, they don't care how the fluid is actually behaving, they just need to approximate how the mass is moving within a margin of error that the control system can handle. reply KeplerBoy 4 hours agorootparentprevMaybe the tank is just not a large hollow structure but contains fins/compartments/whatever to restrict the sloshing motion and it's not that big a contribution to the overall motion. If it's no stronger than a sudden wind gust, it's just something the controller has to be able to take care of without a heads-up. reply kaonwarb 3 hours agorootparentThese are indeed part of the solution and are known as baffles. They have risks of their own, e.g.: https://wccftech.com/baffling-baffles-musk-explains-why-spac... reply nuancebydefault 39 minutes agorootparentIn the first spacex rocket Musk thought that it was a good idea to not install baffles. He learned from experience that they are indeed needed. reply vocram 2 hours agorootparentprevThat’s how tanks in race cars are made. Another solution is fill the tank with some kind of sponge-like material. reply 486sx33 1 hour agorootparentSometimes… the baffles break off, and then become surfboard projectiles inside the tank. More fluid dynamics reply m4rtink 2 hours agorootparentprevThat would be far too heavy in this case. :) reply TowerTall 3 hours agorootparentprevThat is how they build the tank in Formula One Racing (and probably many other race cars, I guess) reply nothercastle 4 hours agorootparentprevThey probably pre solved a bunch of scenarios and interpolate between them known solutions reply j_maffe 4 hours agorootparentThat usually doesn't work for chaotic systems. reply WalterBright 1 hour agorootparentprevIf the computation is too difficult, another approach is build a test stand and try methods until it works. Which is why we use wind tunnels, for example. reply swader999 3 hours agorootparentprevWouldn't it burn most of the fuel to mitigate the effect? reply sobellian 3 hours agorootparentprevAt least for the retro-propulsive landing burn, I think the modeling problem is probably aided by the high G-forces that must keep the fuel very close to the bottom of the tank. Even before re-light the booster is falling near terminal velocity (I think?), so the fuel is likely sitting at the bottom. I think it's a huge problem when re-lighting the engines in orbit, though. reply m4rtink 2 hours agorootparentAlso IIRC the massive main tanks in Super Heavy should be basically empty at landing & the landings propelants come from a set of small header tabks that are near the central axis of the vehicle & arr completely full. This should reduce or even fully remove sloshing issues at landing time. reply chasd00 2 hours agorootparentprevIirc cold gas thrusters are used before ignition to provide some acceleration to force the fuel to the bottom of the tank. reply baq 49 minutes agorootparentThe technical term for that is 'ullage'. See also: https://en.wikipedia.org/wiki/Ullage_motor reply dtquad 4 hours agorootparentprevI think some Kerbel Space Program players have attempted to approximate the liquid sloshing as an inverted single or double pendulum problem inside the rocket that the control algorithm has to take into consideration in addition to the primary control of the rocket. reply dmitshur 3 hours agorootparentprevHas it been considered to spin the fuel via some centrifuge mechanism as a way to remove sloshing from the equation, or is that more complex/expensive/error-prone than just predicting it via simulation? reply phkahler 2 hours agorootparentprevMaybe they just use pressure sensors to tell where the fluid is within the tank. Even a real-time simulation should have some measurements to self correct to some degree. Otherwise it'll diverge. reply macinjosh 2 hours agorootparentprevThere wouldn’t be a whole lot of fuel left by the time it’s back to land so likely an irrelevant factor. reply mlsu 48 minutes agorootparentprevI know MPC takes a LOT of compute power. It's not like a finely tuned PID loop or even a cascade of PID loops, computationally. Does anyone know (or have educated speculation on) what kind of hardware is running these algorithms? Like, do they have a linux machine that's running the control loops? Are we talking megabytes, gigabytes of SRAM? I would think no -- you would definitely need hard real time for something like this. But my only experience with real time systems is in tiny MCUs with kb of SRAM. That's definitely too small for a controller like this. Really curious about the nuts and bolts of this. reply mlyle 38 minutes agorootparentMPC doesn't need to take a ton of compute power. It all comes down to how sophisticated the underlying model is. You can have a MPC with 20 variables and run it at multiple kilohertz on a tiny microcontroller. When you build something like this, you're torn between having a big model that represents everything and a smaller model that is easier to validate and reason about. Based on simulation, you might go for a smaller model that \"knows\" to stay away from operating areas where hidden variables (like really complicated tank slosh) invalidate the small model. I doubt the actual control loop is too much processing, but it's certainly possible to build controllers with SDRAM, millions of variables of state, and hard realtime processing, though I wouldn't build it on top of preempt-rt. ;) reply erikerikson 34 minutes agorootparentprevI have been told by people who worked on them that you get radiation hardened aerospace/defence grade chips by backing off the clock speed about 20% to give signal stabilization slightly longer time. I can understand the population being confused about this but industry being confused seems to have more to do with regulatory capture and beaurocratic moats which SpaceX does seem to be bypassing. reply WalterBright 2 hours agorootparentprevReliability is not based on a system that cannot fail. It is based on a system that can survive failure. reply mrdoops 2 hours agorootparentThe canonical paper on handling software failures: https://erlang.org/download/armstrong_thesis_2003.pdf reply WalterBright 1 hour agorootparentThere's much more to it than the programming language. Algorithms can be faulty as well. reply mlyle 35 minutes agorootparent> There's much more to it than the programming language. Which was never claimed. That paper is a little bit about Erlang and a whole lot about OTP and other methodology and design technique. It is still, very much \"the paper\" for distributed systems, though its applicability to this particular problem is limited. reply szundi 2 hours agorootparentprev… or cannot fail reply prox 2 hours agorootparentprevJust as a note, Space Engineers has a mod that accounts for fuel in the tanks and also various orbital mods. If one feels inclined to try it for themselves ;) reply serf 1 hour agorootparentas someone who absolutely loves SE -- please don't. the orbital and planetary mechanics kind of suck. They're meant to provide a decent 'arcade realism' for the sake of player/player interactions and pvp/pve. if you want to experience fuel slosh/weight during a vertical ascent/descent go with kerbal. It models a lot of that stuff without mods -- and mods can make the model even more accurate. reply sidewndr46 4 hours agorootparentprevhow much liquid fuel is present when it lands? I assumed all the clouds it was giving off was it dumping fuel to make sure it was empty on landing reply sephamorr 4 hours agorootparentLess than 5% of a full load. Any extra fuel you brought to the edge of space and back is lost performance, so substantial efforts are made to minimize this lost mass fraction. reply jccooper 1 hour agorootparentprevThat's probably venting of excess pressurants and/or an engine purge. reply arijo 3 hours agorootparentprevI never thought of using fluid dynamics in the rocket stabilization algorithm—maybe it's something that could be useful to prevent many of the accidents involving liquid-transport trucks reply wildzzz 2 hours agorootparentprevI spent 2 semesters covering controls and I barely felt like I learned anything. reply bob1029 4 hours agorootparentprevhttps://en.wikipedia.org/wiki/Model_predictive_control http://cse.lab.imtlucca.it/~bemporad/publications/papers/ecc... reply stetrain 1 hour agoparentprevHere’s a video from a farther vantage point that gives a better perspective on the landing process: https://x.com/shaunmmaguire/status/1845444890764644694 reply pj_mukh 2 hours agoparentprevI know the control algorithms are the mind-blowing part here but, does anyone have any literature about how the Rocket localized itself with respect to the chopstick arms? It must've been some combination of GPS and Radar pings to the arms? And then the onboard IMU to make sure it hits it straight. reply ThouYS 1 hour agorootparentsuper good question, especially with all the tilting involved, which would make visual servoing difficult. Maybe some form of beacons on the ground? reply starspangled 6 hours agoparentprevThat was one of the greatest things I've ever seen. reply marssaxman 1 hour agorootparentLiterally brought me to tears, watching that happen. reply drexlspivey 4 hours agoparentprevThe booster was falling at 4500 Km/h 30 seconds before the catch with 2-3% fuel left. How is that amount of fuel remotely enough to stop the downward momentum? reply magicalhippo 2 hours agorootparentFirst off, the booster was going about 1250 km/h when it started its landing burn, it relied purely on drag to get it slowed down to that speed. Going by the telemetry of the seconds before the landing burn and noting the speed vs time, it seems drag was around 40 m/s^2 when it was going at around 3000 km/h. Since drag depends on velocity squared though, it had reduced to just above 10 m/s^2 just before the engines lit at 1250 km/h, and so would quickly become negligible once the engines lit. Going by Wikipedia, the Super Heavy[1] has 3400000 kg of fuel at launch, so 3% of that is about 102000 kg. For the landing burn, it used 13 Raptor v3 engines[2] to scrub speed. Each Raptor flows about 650 kg/s max, so 3% fuel is enough for about 12 seconds for the 13 engines. The empty mass of the Super Heavy is about 275000 kg, so about 377000 kg before the landing burn with 3% fuel. Using the sea-level vs vacuum performance of the Raptor v2 engines, one can estimate that each Raptor v3 produces about 2.45 NM of force at sea-level. So 13 of them would produce about 31.85 MN of force. Using Newton's second law, F=ma, this gives an initial deceleration of about 84 m/s^2 and about 104 m/s^2 when empty. If we do a rough spreadsheet integration, we get that a burn of roughly 4 seconds is needed to scrub the speed assuming no other forces. Now, comparing this with reality, the full 13 engines were lit for a little over 5 seconds. In my simplified calculations I was assuming full throttle the whole way, which obviously isn't realistic, and I also assumed 3% fuel. So over all I think that's a pretty decent estimation. [1]: https://en.wikipedia.org/wiki/SpaceX_Super_Heavy#Engines [2]: https://en.wikipedia.org/wiki/SpaceX_Raptor reply rsync 33 minutes agorootparent\"... Super Heavy[1] has 3400000 kg of fuel at launch ...\" So, 34M kg of fuel has to be burned (in this booster alone) to facilitate a flight ... and I see that the propellant is CH4 / LOX[1]. Burning methane is much, much better than simply releasing methane but the release becomes CO2 instead ... What is the back-of-the-envelope conversion of 34M kg CH4 vs., for instance, 34M kg of kerosene/JP ? [1] https://en.wikipedia.org/wiki/SpaceX_Super_Heavy#Engines reply ylere 0 minutes agorootparentMethane has 28% more energy per kg than kerosene and also produces slightly less CO2 (2.75kg CO2/kg burned vs 3.00 for kerosene) when burned [1]. SpaceX uses a 78:22 LOX to CH4 ratio, so for 34M kg of fuel burned, 20.57M kg of CO2 are produced (34×0.22×2.75). [1] https://www.engineeringtoolbox.com/co2-emission-fuels-d_1085... [2] https://x.com/elonmusk/status/1298426245991063554?lang=en CrazyStat 20 minutes agorootparentprev3.4 million, not 34 million. And that includes the weight of the liquid oxygen, which is heavier than the methane. reply stetrain 4 hours agorootparentprevBecause the rocket weighs a small fraction of what it did at launch, specifically because it only has 2-3% fuel left. Fuel is the vast majority of the vehicle weight at launch, kind of like an empty vs full can of soda. reply justin66 2 hours agorootparentThe atmosphere does its bit to slow down the booster as well. It’d be interesting to see a plot of the power output over time on reentry but I’ve always assumed the motors aren’t doing a lot of work other than keeping the booster stable, until the very end. reply stetrain 2 hours agorootparentYes, for most of the booster return it’s ‘gliding’ with the rocket engines completely shut down. They ignite a subset of engines just a few seconds before landing for the final slowdown and maneuvering. Edit: here is a video from further away that shows the rocket gliding in under control of the grid fins before the engines light and execute the final landing maneuver: https://x.com/shaunmmaguire/status/1845444890764644694 reply chedabob 4 hours agorootparentprevIt weighs about 10% of what it did at liftoff, but half of the engines fire to slow it down. Also I don't think the telemetry on the feed is that accurate, so with all of the atmospheric braking, it was probably going a bit slower than the 1200km/h at engine reignition. reply jjk166 1 hour agorootparentprevNote that the energy of 3% of the propellant (~100 GJ) could theoretically get the empty booster (100,000 kg) to a little over 5000 km/h if properly applied. reply monkeydust 5 hours agoparentprevWatched several times, amazing stuff. Had flashbacks to playing Jupiter Landing on the C64! https://en.m.wikipedia.org/wiki/Jupiter_Lander Edit: SpaceX should create a simple 'catch' the rocket game. Play in browser style. Just for kicks and marketing. reply sobani 5 hours agorootparent> SpaceX should create a simple 'catch' the rocket game. Play in browser style. Just for kicks and marketing. They did: https://starshipthegame.spacex.com/ reply CodeWriter23 16 minutes agorootparentI succeeded with a High Score of -287! reply miyuru 5 hours agorootparentprev> Edit: SpaceX should create a simple 'catch' the rocket game. Play in browser style. Just for kicks and marketing. there is, it was discussed in the FAA thread. https://news.ycombinator.com/item?id=41821220 reply renewiltord 2 hours agorootparentThis is a pretty fun game. Not so easy. Good job to the creator for having added some mechanics that make it work. reply KORraN 4 hours agorootparentprevThis one is not from SpaceX, but gives way more fun: https://mechazilla.io/ reply HPsquared 1 hour agorootparentThat's great, really shows how they are \"threading a needle\" on the control side of things. reply dominicdoty 5 hours agorootparentprevShameless plug for my own project with a similar bent - write your own lunar lander autopilot in browser lunar.unnecessarymodification.com reply poorlyknit 4 hours agorootparentVery cool :) I got a good enough score in the basic scenario by playing around a bit but it would be cool if you could link some kind of tutorial (e.g. to a digital PID video or something like that). reply demarq 4 hours agorootparentprevvery fun got 51! with fuel limit + mass enabled. reply demarq 2 hours agorootparentprevso now managed to save 63% fuel, but score doesn't seem to take this into account (still 51).. reply tails4e 4 hours agoparentprevFelt special witnessing history, was delighted my kids and their friends all were glued to the TV for it also. reply Torkel 3 hours agoparentprevI thought the same, screamed out \"ouch that doesn't look good!\" right before the catch. The last part of the live stream they showed footage from a different angle and there it didn't look too bad though! For sure controlled. Scott Manley put out a tweet that they went down towards a non-tower position until they were at three engine controlled burn, and only then did the side shift. reply jatins 4 hours agoparentprevtimestamp for \"catch\" is around 40:00 for those curious reply jccooper 1 hour agoparentprevThere's also a very impressive sensor and actuator story. reply shellfishgene 2 hours agoparentprevThe simulation they show at 21 min into the video is almost exactly like it happend in the end, to it seems it went perfectly as planned. reply brcmthrowaway 2 hours agoparentprevDo the control algorithms use AI? reply lutorm 13 minutes agorootparentNo. Way too hard to validate. reply arijo 2 hours agorootparentprevActually this is a good question - there's a lot of control engineering research on Data-Driven Control Systems. Check Steve Brunton youtube channel, he is one of the leaders in this area: https://youtu.be/gb_C9LcjDSI?si=xUjqUZ9-0MIFohX6 reply ThouYS 1 hour agorootparentprevIf I had to bet, I would bet against it. Boston Dynamics for example, for the longest time, didn't use anything other than Model-Predictive Control. Only recently have they started using RL reply blackeyeblitzar 4 hours agoparentprevWhen Musk first proposed this, I thought he was crazy. It seemed like something a school boy would draw up. Now I think this will become routine and forgettable after a few more successes. Is there a word for that - something out of fiction becoming mundane? reply api 3 hours agorootparentI thought it was plausible given the accuracy of F9 landings, though I still wonder how it will work at scale if one failure destroys the landing site. That could ruin the cost benefits. Where his vision hit a lot of speed bumps is second stage reusability. Starship is a beautiful second stage to throw in the ocean. They’ll probably get it landing but the heat tiles will require a lot of refurbishment between flights. They’re going to have to figure something else out. reply cryptonector 6 minutes agorootparentI'm expecting that SpaceX will have lots of towers, not just one (currently) or 3 or 4 (under way). It won't just be for redundancy. The duty cycle of a tower might simply not allow for the cadence that SpaceX wants to maintain. With Falcon 9 they currently have a 3x weekly launch cadence (which is unbelievable enough). With full reusability they might be able to get to daily and better cadence, so if the duty cycle of one tower does not support that (I assume right now it does not) then they'll need more towers. reply DiggyJohnson 3 hours agorootparentprevI really don’t think it will ruin the cost benefit as much as you suggest, especially when they have multiple sites and multiple locations. It wound be catastrophic, but they are presumably building Tower like the rest of their hardware, and every time they launch it represents a $100M saving compared to the competition. reply ericcumbee 1 hour agorootparentprevIt took months to more than a year to repair shuttle’s heatshield in some cases. SpaceX replaced the entirety of the heatshield with a new design and a new ablative underlayment in a matter of weeks. I suspect they will be able to do it even quicker with design and process improvements. Small scale repairs of the heatshield between flights probably wont be all that big of a deal. reply dmix 6 hours agoprevVideo of the catch > Mechazilla has caught the Super Heavy booster! https://x.com/spacex/status/1845442658397049011 reply nomilk 4 hours agoparent10 minutes from 3h25m of this video shows launch and catch. Historic viewing :) https://www.youtube.com/watch?v=YC87WmFN_As&t=3h25m14s reply afh1 2 hours agorootparentThis angle is even more impressive IMO https://x.com/kimbal/status/1845451222750306344 reply aaronharnly 6 hours agoparentprevNaive question: I obviously expect there to be flames from the engines, but there were flames on the lower sides for quite a while after the catch – is that expected? reply generuso 5 hours agorootparentIt is common for Starship prototypes to have uncontrolled fires, but it is obviously not a good thing. For example, prototype number 10 exploded 8 minutes after landing [1] because of a seemingly insignificant fire at the bottom. After today's flight there was a long lasting fire in the engine section, with occasional flaming pieces of plumbing raining down from the rocket. Examining the aftermath should help SpaceX to understand what improvements need to be made to prevent this from happening. [1] https://youtu.be/XOQkk3ojNfM?t=38346 reply mrandish 17 minutes agorootparentThis highlights another thing I love about watching SpaceX's unprecedented rate of progress. They're managing the complex balance of risk, learning, time and budget extremely well. I'm not an expert in the relevant domains but even I've noticed and appreciated that the typical SpaceX development test always manages to get big chunks of new data, while still having some notable things not quite working. It's an object lesson in rapid engineering development. If everything goes perfectly in a development test, it's a sign you're not moving fast enough (meaning not taking enough risk per increment to maximize learning). As valuable as Falcon, Starlink and Starship are, the biggest near-term value of SpaceX may be providing such a clear demonstration of well-executed \"fail fast, learn fast\" engineering that even politicians and bureaucrats can understand it. reply ls612 3 hours agorootparentprevThis same issue was common on early Falcon 9 landings and took a while to fully eliminate. reply cryptonector 4 minutes agorootparentprevIn some of the views of the landing it's clear that the flames are from a methane vent. I.e., it's really not a problem. reply dotnet00 6 hours agorootparentprevProbably not expected, but the nice thing is that they don't have to pull the vehicle off the bottom of the ocean to study what happened there. reply ceejayoz 6 hours agorootparentprevYou usually see some down near the engines after a Falcon 9 landing. The vent out the side before touchdown didn’t look right, though. Something blew, but non-critically. reply lexicality 3 hours agorootparentprevIt looked to me like the fire was on the fuel intake valves and if you watch carefully that area was scoured by the nozzle output when it was first slowing down so it probably blew through the shutoff valves or something reply tankenmate 5 hours agorootparentprevIt does look like venting, but on the Everyday Astronaut video feed it also looked like a COPV inside one of the strakes looked like it exploded as well. reply thepasswordis 6 hours agorootparentprevMy guess is that it's venting out the system pressure, and it's safer to burn it. reply rkagerer 1 hour agorootparentprevI'd love to see followup analysis of what that fire was all about. reply inglor_cz 5 hours agorootparentprevIf not an intended vent, probably some methane leak. Given that they have the first stage intact, they will know exactly what happened very soon. Yet another advantage of having the rocket returned instead of sinking it in the ocean. reply exitb 6 hours agorootparentprevDesirable? Probably not. Expected? Yeah. Especially given that is the was pretty unexpected to work on the first try. reply sourcecodeplz 6 hours agorootparentprevnext [11 more] who cares... they did it and it didn't burn up or destroy the platform. rest is just nitpicking reply Denvercoder9 5 hours agorootparent> who cares... I bet SpaceX does. They've solved the big problems, now it's time to solve the small problems and make reuse a reality. reply haliskerbas 4 hours agorootparentprevIsn’t this a forum of hackers caring about the news? Everyone seems excited here and that excitement naturally leads into curiosity for many who identify as hackers. reply swarnie 6 hours agorootparentprevOutside of maybe Boeing \"who cares...\" isn't a valid answer in engineering. reply kiba 5 hours agorootparentIt's a prototype. reply ben_w 4 hours agorootparentThat's why it's not a catastrophe that something was unexpectedly on fire, not a reason to dismiss the question. reply jjk166 1 hour agorootparentprevSo the whole purpose of it is to identify issues to be resolved. reply panick21_ 6 hours ago [flagged]rootparentprevnext [4 more] How about you let people be happy 30min for a while before you do the 'but actually' routine? reply dang 5 minutes agorootparentPlease don't respond to a bad comment by breaking the site guidelines yourself. That only makes things worse. https://news.ycombinator.com/newsguidelines.html ethbr1 5 hours agorootparentprevIt's an interesting engineering question, not a slight. Obviously the primary mission was successful, in spite of anomalies (good system design!), but people are and should be curious about this aspect. reply tonyarkles 3 hours agorootparentThere’s a bunch of people crapping on you who clearly haven’t been through a flight test campaigns. 100% with you. The teams I’ve worked with would be celebrating and trying to figure out what’s burning at the same time. And especially trying to figure out if there’s anything that they need to do to collect evidence for that investigation (eg zooming the remote PTZ cameras in on specific areas or things like that) reply zitterbewegung 4 hours agorootparentprevThey have mission goals which were achieved (booster was caught). Goals that they didn't think they could do (Starship being within the buoys). While there was flames and those could be dangerous you judge the mission based on the planned outcomes but they will try to eliminate the anomalies to improve the next mission while still achieving the goals. reply tempestn 43 minutes agorootparentThe thing is that no one is judging the mission based on this imperfection. It's just intellectual curiosity, which is a good thing. The comments that are getting down voted are all assuming some negative motive that just isn't present. reply credit_guy 5 hours agoparentprevI wonder what will happen when they get to 99% reliability? They clean up and rebuild the Mechazilla every once a hundred catches, on that occasion that one fails? reply bombcar 5 hours agorootparentI suspect there's already a whole \"refurbishment\" process for the crane even for non-reusable launches, and once it's working darn reliably, they can just have a bunch of them ready to go, and cycle once in a while. reply creer 3 hours agorootparentThe next one is already mostly built and very different from this one. They move fast. reply dotnet00 1 hour agorootparentprevThe booster aims towards the shore until the landing burn starts, only then does it swing towards the tower. So, for the most part, failures should mean that the booster safely crashes into the water. reply brianwawok 4 hours agorootparentprevI also expect if something fails a test in the way down you crash in the water rather than splat your catcher reply bufo 4 hours agorootparentprevThe plan is to have many, many Mechazillas. reply gizajob 6 hours agoparentprevAmazing achievement. reply treespace8 6 hours agoprevI'm just so happy to see this level of progress. This another big step for opening up space. To think that one day this will be considered normal. 150 Metric tons sent on a fully reusable rocket. Thats like a 747 to space. reply flaburgan 6 hours agoparentSo, reusable is supposed to reduce the cost. But the space shuttle was reusable and it has been shutdown because it was too expensive. What is the differences between the two? reply JumpCrisscross 5 hours agorootparent> space shuttle was reusable SpaceX builds vehicles. The Shuttle was “reusable” because they needed a term between the default for transportation capital expenditures (e.g. trains, planes, cars and ships) and the modified missiles that defined post-War spaceflight. “Reusable” in the Shuttle’s context meant months of specialist overhaul time and the cost of a Falcon 9 launch in SRB booster replacements alone [0]. At the end of the day, in 2010, “the incremental cost per flight of the Space Shuttle was $409 million, or $14,186 per kilogram” [1]. ($591mm and $20,512 in 2024 dollars, respectively [2].) SpaceX’s prices per kg are around $3,170 on Falcon 9 [3] and $1,520 for Falcon Heavy [4]. Starship should bring those costs below $1000. [0] https://forum.nasaspaceflight.com/index.php?topic=51959.0 [1] https://en.m.wikipedia.org/wiki/Space_Shuttle_program [2] https://www.usinflationcalculator.com/ [3] https://www.spacex.com/media/Capabilities&Services.pdf LEO [4] https://en.m.wikipedia.org/wiki/Falcon_Heavy LEO, theoretical reply vardump 5 hours agorootparent> Starship should bring those costs below $1000. It might even bring the costs below $100/kg. reply ben_w 4 hours agorootparentIt might, but it's also at a scale where people can dust off the old plans for orbital rings and ask if this time the economics work out. (My guess is the economics are fine, but the politics would kill it on earth, so the moon or mars will get one, but that's just an interested amateur opinion). reply vardump 4 hours agorootparentOnce you put something big in the LEO, you'll have to be able to boost its orbit indefinitely, because the orbit will otherwise slowly decay. reply piombisallow 4 hours agorootparentIf you have the infrastructure to build an orbital ring, you have the infrastructure to keep it supplied too reply pantalaimon 2 hours agorootparentThere is some SciFi story about civilization collapsing and the survivors worrying that the sky will fall on them hidden in there. reply dyauspitr 2 hours agorootparentGotta make the individual pieces small enough to mostly burn up completely on rentry. reply jaggederest 16 minutes agorootparentTerm of art for this is \"design for demise\", i.e. make everything of pieces small enough and materials ablative enough that there's less than 1 in 10k chance that any debris will survive to the surface. https://www.esa.int/Enabling_Support/Space_Engineering_Techn... ben_w 4 hours agorootparentprevContiguous rings filling an entire orbit don't have much air resistance, and what little they do have is small in comparison to their momentum. But yes, eventually things decay. reply DarmokJalad1701 4 hours agorootparentprevThat depends on how high the orbit is. If it is high enough, the decay will take centuries. reply lucianbr 4 hours agorootparentThe L in LEO means we're talking about orbits that are not very big. reply ben_w 4 hours agorootparentprevIIRC, above something like geostationary they tend to decay upwards? Though the old orbital ring white paper wasn't suggesting anything like that, this was an alternative to needing to go so high in the first place. (I may be misremembering or getting confused with a thing specific to tidal locking?) reply AdammadA 3 hours agorootparentI wonder if they could have an orbit high enough to move away from earth with some kind of drag cables dangling from them into low orbit to counter the outward movement. Would that work? reply dotnet00 1 hour agorootparentYou mean something like a skyhook? reply credit_guy 5 hours agorootparentprevThe main difference is that this is built by a private corporation who can't afford to throw money away, while the Space Shuttle was build by the government, and moreover it had to fulfill a number of conflicting requirements, and commercial profit was not one of them. But on a more technical level. I think the vertical landing is the main difference. Vertical landing was obviously known and done by NASA, this is how the lunar modules landed on the Moon. But doing it on Earth, with vehicles weighing hundreds of times more, I don't think the world had that technical readiness a few decades ago, when the space shuttle was designed. And another major difference is the mass manufacturing idea. From the start SpaceX planned for getting to mass manufacture its rockets. The Falcon rockets are much cheaper than any other alternatives even if you remove the reusability. Then it's the methane burning engines. This was pure old fashioned engineering progress. SpaceX's engines are miracles of rocket engineering. Aside from that, the fuel choice is extremely smart. Methane is better than all other fuels, except for hydrogen. Hydrogen was the fuel of the space shuttle, but it's very tricky to work with. It has very low volumetric density, so the tank of the space shuttle was absolutely humongous. Hydrogen needs to be stored at an absurdly low cryogenic temperature, so this adds to the complexity. And that tank was not reusable, so it adds to the cost. reply WalterBright 1 hour agorootparentIn order to land as a glider, you'll need wings, landing gear, doors, rudder, stabilizer, flight controls, streamlining, all the structure needed to support it, and a heat shield for all of it. All that complexity has to work reliably, too. All of that adds tremendous weight, complexity, and cost. reply WalterBright 1 hour agorootparentprev> and commercial profit was not one of them But it's high cost was the end of it. reply bamboozled 1 hour agorootparentprevWhat is all this rocketry doing for climate change if they're burning methane? Hopefully it's methane from landfills etc? reply credit_guy 9 minutes agorootparentI did the math, and the impact is not negligible. One single launch releases the equivalent of 5000 tons of CO2. Elon wants to get to the point where there are thousands of Starships, each doing a few trips to orbits per day. That would be more than one millions launches per year, or more than 5 GT of CO2-equivalent. That's about 10% of the worldwide emissions today. reply haspok 5 hours agorootparentprevSpacex might be a private company, but this project is funded by NASA, meaning the American taxpayer. Approved by a person whose last act was this approval before leaving NASA and joining Spacex (effectively putting money in their own pocket). It is also yet to be seen how Starship will ever be profitable (outside of spending government money), who is going to pay for those launches and for what purpose. Other than Starlink, of course. reply throwaway4aday 4 hours agorootparentWith a payload volume of 8m diameter by 22m height you could fit a James Webb size telescope inside with minimal folding. The sunshield (21.2 m by 14.2m) would only need to fold along one axis and the mirror (6.6 m) could be monolithic instead of having to fold, probably only requiring the mounting points for the primary and secondary to be hinged. This shouldn't be discounted because it makes telescope design much simpler and less expensive. It also allows for launching individual space station modules that have almost the same volume as the entire ISS in one launch. Their plans for refuelling on orbit with tanker versions of the starship open up the entire solar system to unmanned missions with much shorter timelines and much higher payload size and weight. The fact the entire system is re-usable will make it both cheaper and faster to use than any other launch system. All of this combined mean that it won't just be countries and space programs bidding for space on launches, it puts space within reach of many corporations and some private individuals. This isn't conjecture, it's already happening with the Falcon 9. Starship will make it even more accessible. reply foobarian 4 hours agorootparent> With a payload volume of 8m diameter by 22m height you could fit a James Webb size telescope inside with minimal folding. Of course, that means you could also fit a James Webb-folded telescope except make it a lot bigger :-) reply throwaway4aday 4 hours agorootparentYes! that too. reply Earw0rm 3 hours agorootparentprevProbably need on-orbit refueling to work for that to happen, but they're working on it. reply Denvercoder9 5 hours agorootparentprev> this project is funded by NASA Partially. They have a fixed-price contract to land humans on the Moon, and notably got that contract because they severely undercut the other bids and were the only bid that actually fit within the available budget: they bid $2.94B, while Blue Origin bid $5.99B and Dynetics $9.08B. That 3 billion is also much less than what they're spending on the project. reply bryanlarsen 4 hours agorootparentprev> but this project is funded by NASA About 10% funded by NASA. Starship is a >$10B program; SpaceX is getting $3B for Artemis of which >2/3 is for operational tasks and moon-specific stuff that SpaceX aren't relevant for SpaceX's goals of LEO and Mars. reply throwaway48476 5 hours agorootparentprevThe problem is that launch costs went down fast but satellite costs haven't gone down as fast and still have long development timelines. The other problem is the market for satellite services hasn't developed as fast as anticipated, except for starlink. reply ufmace 2 hours agorootparentThe whole design process for them is based around launches being expensive and taking a long time to plan. It will be very interesting to see what happens when the whole process gets used to launches being relatively cheap and frequent. No need to spend years making sure the design is perfect and will definitely last a long time if you can launch a new one in a week if you make a mistake. reply baq 4 hours agorootparentprevStarlink for all intents and purposes is the market for satellites now. All the other launches are nice to have extras. Now personally I’m looking forward to NASA, ESA and JAXA to launch outer solar system probes like new horizons but with tons of fuel left in the tank to safely make orbit around there. reply XorNot 4 hours agorootparentHaving enough lift capacity to take a shot at putting a pair of telescopes out far enough to exploit solar gravitational lensing to resolve exo-planet surfaces would be a hell of a thing. Orbital refueling would mean we could reasonably build something big enough to be able to boost out that far (would still take decades to arrive). reply mhandley 3 hours agorootparentprevThings can only be cheap if you mass produce them. That tends to require standardization of components, and inevitably standardized components are a compromise between requirements, where up until now, saving mass was a critical requirement. If you don't have to care nearly so much about mass and volume, then that opens up many avenues for much cheaper standard satellite components. reply oceanplexian 4 hours agorootparentprevStarlink is predicted to have something like 6.6B in revenue. SpaceX isn't a rocket company they are an ISP that launches rockets. reply varjag 2 hours agorootparentIs that long term or this year? Because honestly 6.6B is not a lot for their scale of operations. reply ben_w 4 hours agorootparentprev> Spacex might be a private company, but this project is funded by NASA, meaning the American taxpayer. So was the space shuttle, so that's not a difference between the launch costs of the two vehicles. reply zitterbewegung 4 hours agorootparentprevThe purpose of funding SpaceX with taxpayer money is to make competitors that can launch rockets to space so that it is cheaper. reply mempko 3 hours agorootparentExactly. Private companies like space X would not exist if NASA didn't deliberately make the market for Private space companies. That's what governments do, make markets. reply adt2bt 5 hours agorootparentprevThink of the word ‘reusable’ in this case as less a binary descriptor but more of a scale of reusability. Yes, both systems are reusable, but there are key differences in the refurbishment of the systems that partly explains the cost difference. It took more labor, resources and time to refurbish the shuttle. Also consider rapid reusability was a stretch goal when it was being designed, but we have come a loooong way since, spacex in particular has had it as a driving competitive differentiator for years now. Another big difference is that NASA post Cold War was a skilled jobs program, with an incentive to do distributed, high overhead work to appease their bosses (congress), while SpaceX has the opposite. reply cubefox 3 hours agorootparent> Yes, both systems are reusable, but there are key differences in the refurbishment of the systems that partly explains the cost difference. It took more labor, resources and time to refurbish the shuttle. Starship uses essentially the same ceramic heat shield tiles as the Space Shuttle, so the fact the Shuttle had so much trouble with refurbishment doesn't mean that SpaceX has solved these refurbishment issues with the Starship upper stage. Though the Starship lower stage, which contains the most expensive engines, doesn't have this problem. Since it doesn't need a heat shield. So partial reusability should be pretty realistic. reply timschmidt 2 hours agorootparentShuttle's tiles were each unique. Starship is mostly clad with identical hexagonal tiles which can be mass produced and eventually refurbished by machine. A robot already welds on the tile fittings. reply BurningFrog 1 hour agorootparentThat was 24,300 unique parts rather than 1! reply azernik 2 hours agorootparentprevThe tiles are very similar; the attachment system is very different (a big part of why Shuttle's were a pain to maintain) and Starship's simple shape means most of the tiles are the same (the ridiculous number of SKUs was another factor in Shuttle TPS costs). reply fastball 2 hours agorootparentprevShuttle itself was refurbishable, but not rapidly re-usable. It was also incredibly expensive to build and refurbish. A shuttle launch also utilized boosters that were not re-usable. Starship is supposed to be (and clearly well on the way to being) fully rapidly re-usable. That means all stages (in this case two) are re-usable, and that the capital and time required to get either stage flight ready again after a flight should be minimal. Said another way – it is cheaper for SpaceX to build an entirely new Starship + Booster than it was to refurbish a Shuttle between flights, by a factor of about 4x ($90M for a Starship+Booster / $400m for Shuttle refurb). reply shoxidizer 1 hour agorootparentprevYou've got a lot of responses on the difference of reusability, but the shuttle was also more expensive because it had to carry a lot of capabilities with it every time. If you were launching a satellite, you were carrying along the crew compartment and a couple astronauts. If you were bringing a few astronauts to the space station, you brought a cargo bay. And in either circumstance, you brought big wings. Starship can be filled with all cargo. And if you're just changing crew on the ISS, you could... not use Starship and launch a Falcon 9 instead. One of the mission profiles required by the Air Force for the shuttle was that it be able to rendezvous with a satellite, put it in the cargo bay, and return to Earth, all under 2 orbits and along a path that avoided flying over the Soviet Union, which required a rather large turn in-atmosphere to make it back to landing on the west coast. reply ijustlovemath 1 hour agorootparentprevNo one has answered with one of the biggest issues with the shuttle: each one was extremely custom. Every single heat shield tile was unique to a specific position and a specific shuttle. There were probably over a hundred million individual components in the Shuttle, and with many critical ones being custom, the time to refurbish it for a new launch was much longer. This is in contrast to something like the falcon, which has a very standardized mfg process and components, which allows for really rapid iteration reply YoumuChan 5 hours agorootparentprevEconomy of scale: starship can be \"mass\"-produced Material: stainless steel is much cheaper Percentage of reusability: boosters of shuttle cannot be reused, maintenance of shuttle itself is also very expensive (heat shields were pricey). whereas the starship stack has higher reuse percentage and allegedly cheaper to maintain. reply moffkalast 3 hours agorootparentThe shuttle was not even reusable by any modern metric, the main tank was always expended, the boosters had to be recovered, fully disassembled and cleaned. reply alemanek 2 hours agorootparentprevFalcon 9 has already proven that partial reusability is economical. SpaceX has dominated the entire worldwide launch industry and their competitors are nation states with no need to make a profit. The difference is that they have already proven to be the lowest cost and most reliable launcher due to reuse. This is them lapping the industry with second stage reusability. reply modeless 1 hour agorootparentprevThe Space Shuttle was not fully reusable as the biggest single part, the orange tank, was destroyed every time. But more importantly, the orbiter and boosters needed 2+ months of refurbishment/rebuilding after every flight. One of the design goals of Starship is for the booster and ship to relaunch with zero refurbishment. To literally land over the launchpad, refuel, and go back to orbit within hours without people even approaching them. The heat shield is the biggest risk to that goal IMO, and we saw today that it definitely sustained serious damage despite improvements. But if they ever get there then per-launch costs will be a tiny fraction of the Shuttle with 6x the payload. reply kortilla 5 hours agorootparentprevSRB and tank were not re-usable. That’s the equivalent of the first stage you just saw getting caught reply throwaway48476 5 hours agorootparentShuttle SRB segments were fished from the ocean and refilled. reply baq 5 hours agorootparentYeah but salt water makes it about as difficult as building from scratch. reply throwaway48476 5 hours agorootparentThen why didn't they stop? reply JumpCrisscross 5 hours agorootparent> why didn't they stop? They did [1]. [1] https://en.m.wikipedia.org/wiki/Space_Shuttle_retirement reply altcognito 3 hours agorootparentprevBeyond the fact that they eventually did quit, the shuttle program was the public face of the US space program, and part of having more than one way to launch military stuff. reply adt2bt 5 hours agorootparentprevSRBs were in fact reusable. reply sobellian 1 hour agorootparentprevThe majority of damage to shuttle's TPS apparently came from foam strikes from the external fuel tank. Superheavy's optimized profile certainly helps here, since there are no large cryogenic tanks hanging ominously over the TPS while being shaken violently by solid rocket boosters. reply HarHarVeryFunny 5 hours agorootparentprevThe Shuttle consisted of the shuttle (orbiter) itself, the external tank (not reusable), and the two boosters which could be reused after ocean recovery. The orbiter itself was slow and expensive to reuse since (among other things) all the heat shield tiles were inspected and 30-100 replaced between each launch. I don't know how much work was done to the engines between launches, but SpaceX's parts and cost reduction on the Raptor engine have to give it an advantage there. StarShip consists of the Super Heavy booster that we saw \"caught\" today, and the StarShip (orbiter) itself. Having the booster return to launch site vs requiring ocean recovery should potentially increase cadence and reduce cost of reuse. StarShip is also meant to be reusable, although it remains to be seen how that will pan out. On the previous flight there was burn through from inadequate heat shielding - maybe we'll see an improvement with today's vehicle. I'd expect SpaceX to iteratively arrive at a quicker and more cost effective orbiter reuse procedure than NASA had with the shuttle, but how quick remains to be seen. Of course they are planning many of these to go on one-way trips to Mars rather than being reused. reply cubefox 3 hours agorootparent> The orbiter itself was slow and expensive to reuse since (among other things) all the heat shield tiles were inspected and 30-100 replaced between each launch. Worth noting that Starship's heat shield is very similar to the one of the Shuttle. They actually got the manufacturing method from NASA. reply philistine 2 hours agorootparentThat's why I remain very skeptical about the easy refurbishing of Starship. Initially, way back around 2016, the plan was to vent liquids to create a cushion around the ship. That sounded more easily reusable. reply cubefox 2 hours agorootparentYeah. I think they originally planned to use ceramic tiles only for certain spots and still transpiration cooling for the rest. Then they fully switched to ceramic tiles. In an interview with Everyday Astronaut on YouTube, if I recall correctly, Elon Musk said they first believed the ceramic tiles to be lighter. reply ErneX 1 hour agorootparentprevThe boosters were not (or at least not with the quick turnover and lower costs like the ones from SpaceX) reply bryanlarsen 4 hours agorootparentprevStarship is supposed to be fully & rapidly reusable. Neither adjective applies to the space shuttle. reply inglor_cz 5 hours agorootparentprevMassive. The designers of Starship studiously avoided all the problems that made Space Shuttle expensive to refurbish: a. Cheaper, more durable material (stainless steel). b. Cheaper, easier to manufacture engines. c. Easier to use fuel (methane is much \"tamer\" than hydrogen). d. Standardized heat shield with much smaller requirements for manual work. reply fragmede 5 hours agorootparentprevthe space shuttle was \"reusable\". It had to be taken apart and meticulously cleaned and tested and basically had to be rebuilt after each flight, in a process called turnaround. SpaceX's rockets are much closer to what you'd consider reusable. reply usrusr 4 hours agorootparentI wonder how much of that difference is because the space shuttle was human rated from the start: F9 eventually got there, but only after plenty of \"testing in production\" with disposable payloads. The other big difference, an elephant in the room grade difference I think, is that SpaceX reliability was developed with memories of a reusable vehicle failing mostly due to turnaround costs and risks on everyone's minds. That clearly wasn't the case when the space shuttle was designed, they were the first and enjoyed the privilege of making all the beginner mistakes. reply WalterBright 57 minutes agorootparentI had an email exchange with Homer Hickam, before SpaceX existed, where I remarked that the shuttle design looked like a giant kludge, and a winged reentry vehicle was a fundamentally bad design, for various reasons which I enumerated elsewhere in this thread. He emailed back that he agreed with my reasons and had argued that case with NASA in the early stages of the shuttle program. So NASA was aware of this at the time. reply mhh__ 24 minutes agorootparentI was under the impression tha they also thought this themselves but got \"persuaded\" with the prospect of more money if it could handle certain payloads of use for defense? reply moffkalast 3 hours agorootparentprevYeah the right word is refurbishable, plus the main tank was not reused and the solid boosters got a salt water bath each launch. reply noitpmeder 5 hours agorootparentprevThis is significantly less expensive reply inemesitaffia 4 hours agorootparentprevRead the article \"SpaceX flight Ops\" reply dangus 1 hour agorootparentprevTo answer this oversimplified question with a simple answer, the Space Shuttle couldn't be a more different vehicle than this one. It truly is a comparison between apples and oranges. Let's start with the fact that it was designed in the 1970's. If you had a Cadillac DeVille from 1970 it would get 8-12 miles per gallon. Just the mere fact that the design is about 70 years old makes that vehicle too expensive to operate, and that's before we even start talking about other issues with the design (performance, safety, reliability, etc). reply dyauspitr 2 hours agorootparentprevThe difference is only a small part- the shuttle itself- was reusable. The booster that put it into space was scrapped every time. reply _dain_ 5 hours agorootparentprevManufacture and maintenance contracts for the Shuttle were deliberately spread out across many companies and states, especially in key congressional districts. It was a jobs programme; waste was a feature not a bug. Same thing for SLS. reply pfdietz 4 hours agorootparentMore fundamentally, there were contracts. SpaceX does things itself; there's no legal friction internally. This gets back to the \"Theory of the Firm\" for why firms exist in the first place (transaction costs). The need to codify what work is to be done in contacts is antithetical to SpaceX's rapid development processes. https://en.wikipedia.org/wiki/Theory_of_the_firm reply pfdietz 0 minutes agorootparents/contacts/contracts/ Waterluvian 6 hours agoparentprevIs that 150t of payload or total? What’s the cost in fuel alone (let’s ignore maintenance and operations costs for now)? I’m trying to get a feel for the relative scale compared to today’s commercial flight. reply dotnet00 6 hours agorootparentThey previously threw around a number of around $1M per flight, as mostly fuel costs. Also, while 150t is the target payload capacity, the current test vehicles are closer to 50t in payload capacity, there are revisions in the pipeline based on data from these test flights which will bring it up to 150t. reply pfdietz 4 hours agorootparentTo put this in perspective: at 150t/launch, if a launch is $1M, then for the cost of an SLS launch (at least $2B) Starship could launch 300,000 tons, about the mass of three Nimitz-class nuclear aircraft carriers. reply Waterluvian 2 hours agorootparentWe could use a few of those in orbit. reply pclmulqdq 2 hours agorootparentprevNone of the vehicles have demonstrated any payload capacity yet. 50 tons is the on-paper capacity only, and seems quite high given how little fuel is left when the bring an empty starship to orbital altitude. I assume that as the engines and launch procedures get more efficient, they will start being able to bring stuff to orbit (and quite a bit of stuff, too). reply jillesvangurp 6 hours agorootparentprevMethane is about 900-1500$ / ton. About 1000 tons is used for the launch in addition to 3600 tons of lox. That should be a bit cheaper than methane per ton. Ballpark, the propellant might cost around 2M$. A modern airliner on a long flight might burn around 80 tons of kerosene. It's slightly cheaper than methane. Call it 75-80K$. reply bombcar 5 hours agorootparentThat's $7 of fuel per pound of payload, that is not bad at all. reply pfdietz 4 hours agorootparentIndeed. You sometime see an argument that launch to space is expensive because of the propellant and therefore energy required. And as you note this argument is utterly wrong. reply throwaway4aday 4 hours agorootparentnot sure how anyone can miss the \"throwing away the airplane\" part as being the cost driver. reply pfdietz 3 hours agorootparentIt's the same cognitive error as thinking nuclear energy must be cheap because the fuel is cheap. reply zaroth 4 hours agorootparentprevIs the $1000/ton a law a physics, or could that ever possibly scale up and come down an order of magnitude? reply pfdietz 4 hours agorootparentThere are no dollars in the laws of physics. It's connected ultimately to productivity of all the activities involved, and there's no obvious upper bound to productivity. reply WalterBright 54 minutes agorootparentThere's the rocket equation. https://en.wikipedia.org/wiki/Tsiolkovsky_rocket_equation reply pfdietz 48 minutes agorootparentWhich doesn't have dollars anywhere in it. reply WalterBright 17 minutes agorootparentThe point of the rocket equation is there are hard physical limits on what rockets can do, and dollars won't change it. reply pfdietz 9 minutes agorootparentThat's nice. It doesn't imply a lower bound for costs, just of ratios of costs. treespace8 6 hours agorootparentprevIt was just a quick google. 150t reusable. No idea about the other costs. reply interludead 3 hours agoparentprevThis opens up immense possibilities for exploration reply teractiveodular 6 hours agoprevIt's difficult to overstate how important the milestone of catching the booster is. Now we have a reusable rocket an order of magnitude larger than anything we've had before, and the cost of kg to orbit just nosedived. reply HarHarVeryFunny 5 hours agoparentI'm not sure how critical \"catching\" the booster is to reusability, but it does save weight by not needing legs for landing, and perhaps the booster suffers less stress this way? Note that the booster is not really being \"caught\" although this is the word it seems we're stuck with. It's really more like landing on the arms, since it throttles to a hover at that point. reply Denvercoder9 4 hours agorootparent> I'm not sure how critical \"catching\" the booster is to reusability Not necessarily for reusability, but it helps significantly for rapid reusability: it eliminates the need to transport the booster from the landing site to the launch site. Given that it's 9m x 70m and weighs 270 tonnes, that's not an easy process. reply mshockwave 4 hours agorootparentprevI think saving weight is definitely one of the main issues, see those proportionally large legs on Falcon-9, I guess it simply doesn't scale on bigger vehicles like Starship / Heavy booster. Also, by catching the booster on site, they can even save the transportation and do the refurbishing on site, so even shorter turnaround time I guess. reply pfdietz 4 hours agorootparentAlso, saving on damage to the landing pad. reply chasd00 3 hours agorootparentThat’s a good point, they used the sprinklers on landing to mitigate damage to the pad and the booster was caught well off the ground. reply shirro 4 hours agoparentprevSecond stage reuse seems the far more challenging problem. Other companies should have reusable boosters soon but if significant amounts of Starship continue to ablate on the way down they could be faced with a disposable Starship competing with smaller and cheaper second stages that are well sized for typical payloads. We already knew boosters can be flown back to launch sites reliably with high accuracy. We don't know if it is possible to make rapidly reusable thermal protection systems that can operate on an orbital vehicle of Starship's size until it is demonstrated. reply audunw 25 minutes agorootparentI found Jeff’s Bezos interview with everyday astronaut really illuminating on this topic. Supposedly they’re working on both a reusable and cost optimised non-reusable second stage at the same time. And they don’t really know yet which one will end up being cheaper. You also see this kind of thinking with Rocket Labs neutron rocket. Where they focus on making the reusable booster do more, while making the second stage smaller, cheaper and simpler. I think if it wasn’t for the rocket engine this wouldn’t be a question at all. The tank doesn’t have much value. It’s just a thin shell and probably a fraction of the cost of the fuel. So I’m thinking, perhaps the optimal solution is something like this: the bottom part of the second stage with the engines separates, and a small engine and fuel tanks places the engines in a stable orbit. The tank itself is deorbited and burns up. At some point later something like the Starship collects several second stage engines and deorbits them safely to be reused. Or perhaps just the engines can be immediately deorbited with an inflatable heat shield and parachutes. reply philistine 2 hours agorootparentprev> Other companies should have reusable boosters soon You're way too optimistic. Starship will deliver commercial payloads, with SpaceX phasing out Falcon 9 outside of ISS launches, before anyone has a reusable Falcon 9 equivalent. It pains me to say this, but SpaceX is in a class all its own. reply WalterBright 50 minutes agorootparentWhy does it pain you? Musk's SpaceX has produced several enormous advancements in space technology. SpaceX is one of the greatest, if not the greatest, private technology company in history. reply andrewstuart2 40 minutes agorootparentI think it pains some of us to say it because of the person Musk has turned out to be, which is the opposite direction I think many of us were hoping his character development would take him. reply WalterBright 18 minutes agorootparentWhat terrible thing has Musk done? reply dotnet00 57 minutes agorootparentprevNew Glenn finally has flight hardware undergoing pre-flight testing. I think they're pretty likely to manage to fly in early/mid 2025, and they do aim to recover the booster in their first try. reply thinkcontext 1 hour agorootparentprevThe Chinese could have a reusable booster before then. One of their companies is doing 10 km hop tests. reply signatoremo 1 hour agorootparentSpaceX couldn’t manage the first Falcon 9 landing until 2015. The first Falcon 9 reuse wasn’t until 2018, so 3 years to achieve reusability. The Chinese prototype hasn’t yet succeeded at sub-orbital landing. I wouldn’t be surprise if it’d take them longer than 3 years to have a reusable rocket. Starship would have been routine at that time. Blue Origin plans to launch New Glenn in a month, with landing planned. They are a wild card. reply api 4 hours agorootparentprevDidn’t they look at all kinds of ideas earlier like squirting some propellant or water out over the skin on the way down, and wasn’t steel chosen for its thermal robustness? Did they get into the problem and realize it’s a lot harder and abandon those things for tiles? Maybe they will have to sacrifice more payload mass for active or passive shielding or more fuel for powered deceleration. That would yield a less impressive lifter but with full reusability. reply WalterBright 46 minutes agorootparentInterestingly, the inside surface of a rocket nozzle is covered with tiny holes. Fuel is circulated in a jacket around the nozzle both to pre-heat the fuel and to cool the nozzle. Additional cooling of the nozzle comes from fuel leaking into the combustion chamber through those holes, carrying away the heat so it doesn't melt the nozzle. It's called boundary layer cooling. It was one of the technological breakthroughs of von Braun's team with the V2. reply rini17 3 hours agorootparentprevThey already plan refueling infrastructure in orbit. That would include stuff to \"squirt\" on the way down if necessary. If they can use one extra launch to reuse 5-10 starships that might be interesting. Noone knows yet if it's actually needed. reply wolf550e 1 hour agorootparentprevThey dropped transpiration cooling, at least for now. reply chasd00 3 hours agorootparentprevYeah for rapid reusability tiles aren’t going to work, too fragile. Iirc it was with a lot of reluctance they went with tiles and will have to make breakthroughs on the heat shielding to get where they want to be. reply cma 48 minutes agorootparentInspection might be easier with computer vision advances since the space shuttle. reply interludead 3 hours agoparentprevIt's a game change reply stevage 4 hours agoparentprevThey're a long way from having a reusable super heavy booster. It's still copping a lot of damage on the landing. reply throwaway4aday 4 hours agorootparenttrue but don't forget this is test flight #5 reply h2odragon 4 hours agoprevMy wife asked \"Why is this a big deal?\", so I gave her a link to Handmer's 2021 explanation: https://caseyhandmer.wordpress.com/2021/10/28/starship-is-st... She's not an enthusiast; she's got an impression from SciFi that going to mars shouldn't be that hard. reply wolf550e 1 hour agoparentRobert Zubrin's Mars Direct would have got \"flags and footprints\" on Mars without needing something like Starship. He worked on a version of SLS in 1989 in Martin Marietta that would have cost $1B and been ready in the second half of the 90s and would be retiring soon after many successful flight to Mars (and to the Moon), but instead we got SLS and Orion so we need Starship to get anything done. reply hnpolicestate 9 minutes agorootparentDidn't Von Braun have a plan too? Nuclear rockets or something. reply moffkalast 3 hours agoparentprevWell I mean going to Mars isn't that hard per se, a fair few countries have done it... but carrying enough supplies and shielding to last people 2 years until the return trip and then actually getting back are way harder problems. Seeing Starship get burnthrough on these suborbital launches really shows how hard it'll be to do Mars return entry with a fair few extra km/s. reply mrandish 45 minutes agorootparentGetting to Mars is hard but not that hard compared to getting a large payload to Mars surface and back to Earth. Some people may assume Mars return is around twice as hard but it's orders of magnitude harder. And that's before even contemplating doing so with a cargo of humans and the necessary tonnage for them to survive on Mars at least 26 months. reply diebeforei485 58 minutes agorootparentprevOnly the US and China have landed successfully on Mars. China achieved this in 2021. Russia/USSR tried multiple times and didn't succeed. Other countries have sent satellites to orbit around Mars, which is not the same thing. reply ro_bit 6 minutes agoprevThe catch looks amazing, but one thing I don't understand is why SpaceX needs the technology to catch a booster they've already demonstrated the ability to land boosters on barges. Is the arm more cost effective to scale compared to a barge? reply mlyle 4 minutes agoparentThe argument is that you don't need to spend the vehicle mass on legs, so you get performance. On the other hand, I imagine you could wipe out any benefits to $/mass launched pretty quickly by blowing up the tower. reply valine 3 minutes agoparentprevThe landing legs are heavy. SpaceX would rather have more payload than carry legs to space on every launch. Also landing on the launch pad means you don’t have to transport the rocket. Just have the arms set it down and you’re ready to launch again. reply VohuMana 2 hours agoprevWow, I knew they were going to try to catch the booster this time but I really didn’t think it was going to work on the first try, I was just hoping they didn’t destroy the launch tower in the process. Congrats to the SpaceX team, absolutely amazing! Hope y’all are celebrating reply ptek 6 hours agoprevVery inspiring, glad I woke up to watch it. Congratulations to the SpaceX team. This is what competition looks like. It’s funny how last decade SpaceX had to sue the government to get more launches. reply WalterBright 38 minutes agoparentCalifornia officials reject more SpaceX launches, with some citing Musk's X posts https://twitter.com/shellenberger/status/1845131546501734440... reply XorNot 5 hours agoparentprevThe FAA needs more funding at this point. They're not staffed for the rate of launch requests which commercial space has grown too. reply chasd00 3 hours agorootparentMore money would make the situation worse. They need competent leadership. Everyone like to just say “add more money” because it means no actual operational change to the status quo. reply acd10j 1 hour agorootparentYou are right they are already trying to slow SpaceX as much as possible. More people in their hands more people finding ways to decelerate SpaceX reply atrus 1 hour agorootparentHardly, the FAA is handing permits out the instant the mandated waiting time is up. They couldn't go faster without a literal act of congress. reply gamblor956 1 hour agorootparentprevThe FAA already has competent leadership and staff. They don't have the budget to handle the increasing number of tasks they have been given to manage. The FAA manages more flights and space launch operations than it did in the 1980s, when it had a budget more than quadruple the size. To put it bluntly: you get what you pay for. reply nmca 12 minutes agorootparentYou get what you incentivise, not what you pay for. reply bombcar 5 hours agorootparentprevWe're going to need a dedicated FSA (federal space agency) at this rate, which should do most of the heavy lifting (ha) for the FAA and just use them for NOTAMs and TFRs. reply mkoubaa 5 hours agorootparentprevWhat makes you think the issue is funding? reply systemvoltage 4 hours agorootparentprevNeed to remove regulations instead and fast track SpaceX. Every fucking time we don't need to increase the size of the Government. It only gets bigger. Never smaller. reply dawnerd 3 hours agorootparentNo, launching big heavy explosive things into space shouldn’t have fewer regulations. That’s insane to think they should be able to just do it without much oversight. reply tomschlick 2 hours agorootparentSure the safety part of it is something that should be verified before launch. But when they are launching multiple of these things and the variables are all mostly the same you don't need to do the same analysis over and over again. A prime example is the environmental impact stuff. They have already done that multiple times. Nothing really changed. If it succeed and doesn't blow up the impact is X, if it blows up the impact is Y. Yet these approvals take weeks and months. There are also multiple agencies that put their foot down. Famously fish and wildlife was worried starship would crash in the water and hit a shark/whale. No seriously. https://youtu.be/kS8G5D9fg3g?t=21 Then there is the story that at Vandenburg air base, they had to strap a seal to a board and play rocket noises through headphones to see if it was distressed. Keep in mind Vandenburg has been a military rocket launch site for decades. But only now when its SpaceX do these agencies put up road blocks. https://www.youtube.com/watch?v=3SvJP5wfN4k reply ceejayoz 1 hour agorootparent> Keep in mind Vandenburg has been a military rocket launch site for decades. Not one of which ever returned to base with a sonic boom. I assure you, the folks who tried to kill goats via mental powers and staring at them have required weirder tests than the seal. reply systemvoltage 1 hour agorootparentprevLook at California Government. Elon is going to sue them tomorrow. Enemy of progress. Worthless bureaucrats that HN crowd worships. reply derektank 2 hours agorootparentprevIf by the size of government you mean the budget, there have been multiple instances of the government shrinking. As a percent of GDP the 90s saw cuts to federal spending, the 2011 sequestration saw a drop in spending even in nominal terms, and obviously the end of WW2 saw a massive collapse in government spending and the end of multiple different programs reply tacticalturtle 3 hours agorootparentprevIsn’t this exactly what happened with Boeing, where the FAA delegated its authority, on the assumption that Boeing knew what it was doing? reply dmix 54 minutes agorootparentRegulations aren't going to fix the rot that's inside Boeing. It's like puttinf water in groundhog tunnels, clamp down on one area and it will pop up somewhere else. Boeing's issues are due to it's own incompetence. But whatever they do they continued to receive contract after contract. Mostly because they are protected like they are part of the gov because they are 'essential' to national security and as a jobs provider. When you mess up incentives like that of course they have little motivation to do their jobs well. They need to die off and get replaced by better competition when they mess up. And their market centralization ground to a halt instead of encouraged because of 'jobs' and the total risk aversion of modern culture. reply renewiltord 4 hours agorootparentprevThe great conceit of government is that if something gets money when it is slow, that it will get faster as more money is given. reply WalterBright 32 minutes agorootparentSocialism: when it fails, provide more funding. When it succeeds, cut funding. Business: when it fails, cut off the funding. When it succeeds, increase the funding. reply coding123 46 minutes agorootparentprevOr dismantled, lol reply whimsicalism 3 hours agorootparentprevperhaps we need less thorough reviews reply whimsicalism 17 minutes agorootparentNot sure why this is downvoted. Our risk-to-regulatory oversight cost benefit analysis seems extremely off - we overregulate small things that do not have large-scale meaningful impact while basically grandfather in stuff like depleting the Great Salt Lake, coal emissions, etc. which have vastly less regulatory $ spend per environmental impact. Ideally, if we could quantify all environmental impacts into a single number (ie. CO2 emissions but also wildlife impact, etc.), the $/impact spent across different fields would be equal. Currently, however, we are likely spending 100x+ on a per-impact basis on SpaceX regulation. reply blackeyeblitzar 4 hours agoparentprevWell that may still continue to happen. The wonderful and competent state of California is literally trying to block SpaceX launches over Musk’s constitutionally protected political speech: https://www.politico.com/news/2024/10/10/california-reject-m... I cannot remember a more explicit case of authoritarian government abuse in a developed nation in recent memory, and it’s especially infuriating given SpaceX and Musk are one of the most important and innovative companies and leaders of all time. No other country would think to look at such accomplishments and try to undermine themselves through petty politics and lawfare. reply blisterpeanuts 31 minutes agorootparentIt is a state commission composed of political appointees. It's unlikely it has the jurisdiction over Vandenberg to ban any activities, except when such activities violate state environmental laws. Even then, the federal government might be able to override them. It will be interesting to see what comes of this. Ideally, the commission will withdraw its objections and allow the launches; SpaceX obviously chose this site both for its proximity to the Pacific Ocean and for redundancy in case of poor weather in Florida (like this past week for example). reply blackeyeblitzar 12 minutes agorootparentIt has jurisdiction over Vandenberg for private launches, and there is debate over what counts as private versus federal. However, regardless of possible environmental concerns, they’ve already explicitly said they’re targeting SpaceX because of Musk’s political speech, which is morally reprehensible and also illegal per the constitution. reply delichon 6 hours agoprevI would like to shake the hands of the steeley eyed rocket men and women who just landed a skyscraper with centimeter precision. Respect. This one's for you. Hit it Perry. https://www.youtube.com/watch?v=_VJlHWESyLI reply dotnet00 6 hours agoprevAll the renders in the world couldn't have prepared me for how crazy the actual catch looked! reply choonway 6 hours agoparentok now they have an actual reference and can animate it better haha. reply lquist 4 hours agoprevWhat does Starship reusability mean for $/kg to LEO? I know there are longer term targets of $10/kg but that supposes efficiencies that aren’t here yet. Would be helpful to understand before Starship reusability where the state of the art was in terms of $/kg to LEO and where we would be with impending Starship reusability. reply augusto-moura 4 hours agoparentI don't think we have a number for it yet. But it will definitely be the cheapest launch system at the time of launch. People say 200$/kg just with booster reuse, and 20$/kg with full reuse. Of course this might be too optimistic, but I truly believe we might reach under 50$ in this decade. reply quotemstr 3 hours agorootparentEven $50 is within \"going to the moon for my honeymoon\" range. Wow. reply augusto-moura 3 hours agorootparent$50 is a number for LEO (Low-earth orbit). $/kg to a Moon orbit (or flyby) might be significantly more expensive. Not only that, because it is further so it needs more fuel, but also it is a few days trip which would need a bunch more kilos in provisioning food, water and other things. So yeah, unfortunately not that cheap to have a honeyMoon in the moon (heh) There is a lot of possibilities to make a trip to the moon cheaper though. If we make LEO that cheap, we can build a lot of infrastructure in the space that would make the tourism to moon more affordable. Like keeping a few starships always in orbit as some kind of space-hotel-metro system. This will probably take a few more decades, though. reply Valgrim 1 hour agorootparentArtemis by Andy Weir dwells a lot on this kind of infrastructure, but I never could understand the orbital mechanics described in the book. reply itishappy 2 hours agorootparentprevOnly if you don't care about coming back! reply moffkalast 3 hours agorootparentprevThis might be the best time to get into cubesat development as a hobby, lol. reply jackcviers3 2 hours agorootparentprevWhat is a good estimate of the number of times these boosters and engines will be reusable? reply thinkcontext 3 hours agoparentprevThe dominant variable is how often they can reuse the stages. Last I heard Musk was targeting dozens of reuses for the upper stage and hundreds for the booster. If they are short of the cost per kg goes up. reply acover 4 hours agoparentprevhttps://www.reddit.com/r/SpaceXLounge/comments/1bgmsm9/cost_... reply JumpCrisscross 4 hours agoparentprev> What does Starship reusability mean for $/kg to LEO? All we can say is under $1,000/kg. Which is conservative, that limit being about two thirds that of Falcon Heavy’s theoretical cost to LEO in a reüsable configuration. reply thinkcontext 3 hours agorootparentWe can't give any estimate. The costs depend on how many times the stages are reused. They have targets but we don't know what will actually happen. reply autonomousErwin 6 hours agoprevThat was crazy, 50% of me thought as it was coming in, especially as it pitched towards the tower, \"they've overcompensated and are going to bring the whole tower down\" but they absolutely nailed it. reply foobarian 4 hours agoparentEven without the catch step, I always feel like their boosters are coming down way too fast way too late, with engines reigniting startlingly close to surface. Never ceases to surprise me. reply mshockwave 2 hours agorootparentI have the same feeling watching Starship when the telemetry on the bottom right of the screen showing “0km” but landing burn hasn’t started. Later did I know that it was meant to start the burn only a few hundreds _meters_ from the ground reply grecy 1 hour agorootparentprevPrior to today the only landings you've seen are Falcon 9, which has to do a \"hoverslam\" because it can't throttle low enough to actually hover. Even just one engine in minimum thrust would make the rocket go up when empty.. so the computer lights the engine at the precise right moment so it will have 0 velocity at 0 altitude, then it cuts off the engine. \"Hoverslam\". The Starship booster is different, it can actually hover. reply hobofan 2 hours agorootparentprevYeah, it also always catches me off guard how late they reignite, while at the same time I'm always surprised by how slow the liftoff is even though all the boosters are on full throttle. Of course both things make sense given the respective mass at both points in the process, but given that it looks like the same rocket from the outside, a bit unintuitive. reply phkahler 5 hours agoparentprevYeah but they did pass some flame down much of the tower. Pretty quick though, probably just cleaned it with fire. reply ivanjermakov 4 hours agorootparentSaving up those refurbishment costs, cleaning the tower ahead of time! reply Havoc 3 hours agoparentprevYeah it definitely looked sketchy for a sec but was probably fully as intended reply agumonkey 2 hours agoprevSpaceX is really incredible in how regular the improvements are coming. Something in their structure is well balanced. reply bamboozled 1 hour agoparentIt has to be Elon, he is focusing a lot of energy on the company right now. reply shmerl 16 minutes agorootparentGiven his completely horrible jerk personality, I'd expect the opposite. reply fabrice_d 1 hour agorootparentprevnext [6 more] [flagged] piyh 1 hour agorootparentHistorically speaking, fascism and rockets are like peanut butter and jelly reply bamboozled 1 hour agorootparentprevnext [5 more] [flagged] WalterBright 28 minutes agorootparentI think the guy is an idiot You've got to work really hard to maintain such an opinion, given that: 1. Musk arrived in the US with a couple suitcases, and had so little money he stayed at a hostel 2. Musk worked low wage dirty jobs for a while 3. Musk started multiple industry leading companies, becoming the richest man in the world The US could use a lot more idiots like Musk! reply agumonkey 50 minutes agorootparentprevI wonder when he stopped having any value at his companies (\"if ever\" might add some) reply bamboozled 47 minutes agorootparentIt's hard to tell, I really can't believe the guy I see on TV lately has anything to do with any of this stuff, kind of makes me wonder if he is even real. It's a really unfortunate situation in my opinion. reply bloopernova 24 minutes agorootparentI wonder if we'll ever know the truth of what happened to him? He seemed to lose a large part of his outward filter since the joe rogan \"elon smokes pot!\" interview. reply valenceelectron 6 hours agoprevAlthough this is by far not the first landing of SpaceX boosters I've seen, this looked like scifi to me even in 2024. reply perlgeek 1 hour agoprevDuring/before the water landing of Ship, the telemetry indication for the rocket motors stayed off. Does that mean the landing burn was unsuccessful? Or was that just a glitch in the telemetry? reply stetrain 50 minutes agoparentIn the video you can see the engines light (reflecting off the metal control flap and water) and the ship make a soft touchdown on the water. It then tips over and the tanks pop from the force of smacking the water sideways. So the landing burn seems to have been a success. reply sebzim4500 36 minutes agoparentprevIt's weird, that happened last time too so you'd think if it was a glitch they would have fixed it. reply redserk 6 hours agoprevThe SpaceX engineers are incredible for being able to pull this off. reply DarmokJalad1701 5 hours agoprevRight on target, next to their buoy! Insane achievement! reply pfdietz 4 hours agoparentAt a buoy, SpaceX. reply yborg 34 minutes agoprevAmazing achievement, but also reminds me that what NASA and its contractors accomplished during the 1960s still stands with current day systems engineering. The 3rd Saturn V launch put three men into orbit around the Moon. The 6th landed two of them there. reply travisporter 5 hours agoprevRyan Hansen had an incredible (although unofficial) 3d modeling video of the mechanism. https://m.youtube.com/watch?v=ub6HdADut50 reply h_tbob 30 minutes agoprevWe live in a day and age where it seems that all the innovations we see now are new tech that has to do with electronics. Good to see some good ol hardware breakthroughs in another industry. reply tayloramurphy 5 hours agoprevThis got me as excited as the original Falcon 9 landing in 2016! Now I got to share this with my kids! So incredible... reply gadders 6 hours agoprevIt still blows my mind how the landings are more exciting than the launches. reply bombcar 5 hours agoparentI'm completely mind-blown that not only can they do this, but we get 1080p video for almost the whole time. What cameras do they have that can look at something 30 KM up in the air that well? reply generuso 4 hours agorootparentUsually it is a small telescope on a nice motorized mount weighing a ton. NASA had several of those around Cape Canaveral [1]. [1] https://www.youtube.com/watch?v=xig3sL_rRH8 reply yonatan8070 2 hours agorootparentprevThe thing that impresses me even more is the kinds of cameras they mount on the rocket. How do they shield it from all the heat and debris with a material that's optically clear? reply jjk166 1 hour agorootparentSapphire glass windows: they're good for up to 2000 C, mechanically strong and chemically resistant, and remarkably cheap. reply ceejayoz 1 hour agorootparentprevThey put it in places the heat and debris don’t go. reply throwaway48476 5 hours agorootparentprevAt one time they were using gopros. reply rhegart 2 hours agoprevCoolest video by far, but this still places 3rd for me behind F9’s first landing, and the dual heavy landing. The future is bright! reply sekai 6 hours agoprevMagnificent, it's hard to image what we'll be able to do once these do roundtrips daily. For those are not aware, that booster is 71 m (233 ft) tall! reply paulryanrogers 4 hours agoparentImprovements that make space flight more sustainable are welcome ... unless that means an order of magnitude more pollution in a less controllable form, like emissions. [Due to more frequent flights] Daily trips to space likely also mean more debris in space and falling to earth. I hope there is a balance that includes the lives of people near these sites and all of us sharing the same atmosphere. reply timeon 4 hours agorootparentI hope this is not going to be used for 'Earth point A -> Earth point B' travel. reply modeless 36 minutes agorootparentSpaceX proposed it but I doubt there is any chance of this architecture ever achieving the airliner-level reliability needed for people to accept routine Earth-to-Earth passenger service. That's several levels beyond what you'd need to fly astronauts. Maybe a future architecture with more redundancy could get there someday. reply zaroth 4 hours agorootparentprevnext [6 more] [flagged] Gazoche 3 hours agorootparentPlease don't use ChatGPT as a primary source. There's no way to tell if it's making up those facts or not. reply nickpp 2 hours agorootparentnext [3 more] [flagged] simulosius 1 hour agorootparentWhat kind of credible evidence can you bring that \"main stream media\" are making up facts? reply nickpp 41 minutes agorootparentI did not say that, please reread my post. reply paulryanrogers 3 hours agorootparentprev> Considering the safety and prosperity that can be brought online from 1,000 launches / 150,000 tons in orbit, that’s the deal of the century. Regardless of the (in)accuracy of ChatGPT, you're assuming a lot can be accomplished with those flights. I strongly suspect 600K cars getting people where they need to go has far more utility than 1K flights of anything into / out of NEO. > Debris is an absolutely non-issue. Astronomers probably don't want 1000x (or even 10x) as many satelites obstructing their view. > There is zero environmental downside, only Luddite foot stomping. Name calling isn't going to help your cause. And the luddites had a good point, they didn't mind innovation. They minded being cut out of the benefits of innovation without any say in the matter. reply squigz 2 hours agorootparentprevI'd rather take Luddites and their valid concerns over people like you dismissing them in the vague name of 'progress' reply DarmokJalad1701 6 hours agoprevThey caught it! Holy sh*t! reply gyre007 1 hour agoprevIncredible achievement, my American friends! Congrats! We, Europeans, can only feel jealous, but hey at least we have free [but comically broken and dysfunctional] healthcare, so, there's that. I hope you'll bring a lot more progress to humankind beyond space exploration! reply mrandish 56 minutes agoparentThe long-term goal is becoming a multi-planet species, so in that context it's not about countries. Go Earth! (also, while NASA has been generally helpful to SpaceX's efforts, American FAA bureaucrats have managed to inject unnecessary delays and uncertainty into the process (in addition to some necessary delays). reply metalman 4 hours agoprevanybody not watching these live better be delivering a baby,or saving someones life its a very short list of things that should take precidence over what are the most astounding things happening for our species,ever reply themgt 6 hours agoprevNSF video: https://x.com/nasaspaceflight/status/1845442658203734384 reply robwwilliams 1 hour agoprevAwesome achievement. Watched it with my sister at home and brought back memories from the 60s in front of the B&W TV watching some of the early amazing flights and landings. Question for hackers: How does this reorient space programs world-wide? If I were a politician or technocrat in China, Russia, or the EU this would",
    "originSummary": [],
    "commentSummary": [
      "SpaceX's Starship Flight 5 successfully launched and utilized the \"chopsticks catch\" system to catch its booster, marking a significant engineering achievement.- This method eliminates the need for landing legs, reducing weight and enabling rapid reusability by avoiding transportation from landing sites.- The success of this mission underscores SpaceX's rapid progress and innovation in developing fully reusable rockets, which could significantly lower the cost of space travel."
    ],
    "points": 972,
    "commentCount": 583,
    "retryCount": 0,
    "time": 1728822197
  },
  {
    "id": 41826402,
    "title": "Diffusion for World Modeling",
    "originLink": "https://diamond-wm.github.io/",
    "originBody": "DIAMOND 💎 Diffusion for World Modeling: Visual Details Matter in Atari Eloi Alonso*1, Adam Jelley*2, Vincent Micheli1, Anssi Kanervisto3, Amos Storkey2, Tim Pearce†3, François Fleuret†1 1University of Geneva, 2University of Edinburgh, 3Microsoft Research *Equal Contribution †Equal Supervision NeurIPS 2024 Spotlight Paper arXiv Code Atari CSGO DIAMOND 💎 (DIffusion As a Model Of eNvironment Dreams) is a reinforcement learning agent trained entirely in a diffusion world model. The agent playing in the diffusion model is shown above. DIAMOND's diffusion world model can also be trained to simulate 3D environments, such as CounterStrike: Global Offensive (CSGO). Abstract World models constitute a promising approach for training reinforcement learning agents in a safe and sample-efficient manner. Recent world models predominantly operate on sequences of discrete latent variables to model environment dynamics. However, this compression into a compact discrete representation may ignore visual details that are important for reinforcement learning. Concurrently, diffusion models have become a dominant approach for image generation, challenging well-established methods modeling discrete latents. Motivated by this paradigm shift, we introduce DIAMOND (DIffusion As a Model Of eNvironment Dreams), a reinforcement learning agent trained in a diffusion world model. We analyze the key design choices that are required to make diffusion suitable for world modeling, and demonstrate how improved visual details can lead to improved agent performance. DIAMOND achieves a mean human normalized score of 1.46 on the competitive Atari 100k benchmark; a new best for agents trained entirely within a world model. To foster future research on diffusion for world modeling, we release our code, agents and playable world models at https://github.com/eloialonso/diamond. CSGO DIAMOND 💎 Diffusion World Model Demonstrations All videos generated by a human playing with keyboard and mouse inside DIAMOND's diffusion world model, trained on CSGO. Try it for yourself Try out our playable CSGO and Atari world models for yourself: Installation Instructions git clone git@github.com:eloialonso/diamond.git cd diamond conda create -n diamond python=3.10 conda activate diamond pip install -r requirements.txt To play our Atari world models: python src/play.py --pretrained For our CSGO world model: git checkout csgo python src/play.py How does it work? We train a diffusion model to predict the next frame of the game. The diffusion model takes into account the agent’s action and the previous frames to simulate the environment response. The diffusion world model takes into account the agent's action and previous frames to generate the next frame. The agent repeatedly provides new actions, and the diffusion model updates the game. The diffusion model acts as a world model in which the agent can learn to play. Autoregressive generation enables the diffusion model to act as a world model in which the agent can learn to play. To make the world model fast, we need to reduce the number of denoising steps. We found DDPM (Ho et al. 2020) to become unstable with low numbers of denoising steps. In contrast, we found EDM (Karras et al., 2022) to produce stable trajectories even for 1 denoising step. The DDPM-based model is unstable for low numbers of denoising steps due to accumulating autoregressive error, while the EDM-based model remains stable. Lower denoising steps enables a faster world model. But in Boxing, 1-step denoising interpolates between possible outcomes and results in blurry predictions for the unpredictable black player. In contrast, using more denoising steps enables better selection of a particular mode, improving consistency over time. Larger numbers of denoising steps n enable better mode selection for transitions with multiple modes. We therefore use n=3 for Diamond's diffusion world model. Interestingly, the white player's movements are predicted correctly regardless of the number of denoising steps. This is because it is controlled by the policy, so its actions are given to the world model. This removes any ambiguity that can cause blurry predictions. We find that diffusion-based DIAMOND provides better modeling of important visual details than the discrete token-based IRIS. DIAMOND's world model is able to better capture important visual details than the discrete token-based IRIS. Training an agent with reinforcement learning on this diffusion world model, DIAMOND achieves a mean human-normalized score of 1.46 on Atari 100k (46% better than human); a new best for agents trained in a world model on 100k frames. Check out our paper for more details! BibTeX @inproceedings{alonso2024diffusionworldmodelingvisual, title={Diffusion for World Modeling: Visual Details Matter in Atari}, author={Eloi Alonso and Adam Jelley and Vincent Micheli and Anssi Kanervisto and Amos Storkey and Tim Pearce and François Fleuret}, booktitle={Thirty-eighth Conference on Neural Information Processing Systems}} year={2024}, url={https://arxiv.org/abs/2405.12399}, } This website (source code) was adapted from the popular Nerfies project page and is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.",
    "commentLink": "https://news.ycombinator.com/item?id=41826402",
    "commentBody": "Diffusion for World Modeling (diamond-wm.github.io)358 points by francoisfleuret 9 hours agohidepastfavorite170 comments smusamashah 6 hours agoThis video https://x.com/Sentdex/status/1845146540555243615 looks way too much like my dreams. This is almost exactly that happens when I sometimes try to jump high, it transforms me to a different place just like that. Things keep changing just like that. It's amazing to see how close it is to a real dream experience. reply jvanderbot 18 minutes agoparentThis is why I'm excited in a limited way. Clearly something is disconnected in a dream state that has an analogous disconnect here. I think these models lack a world model, something with strong spatial reasoning and continuity expectations that animals have. Of course that's probably learned too. reply kleene_op 4 hours agoparentprevI noticed that all text looked garbled up when I had some lucid dreams. When diffusion models started to gain attention, I made the connection that text generated in generated images also looked garbled up. Maybe all of those are clues that parts of the human subconscious mind operate pretty close to the principles behind diffusion models. reply qwertox 1 hour agorootparentI don't think lucid dreaming is a requirement for this. Whenever I dream my environment morphs into another one, scene by scene, things I try to get details from, like the content of a text, refuse to show clearly enough to extract any meaningful information from it, no matter what I try. reply smusamashah 3 hours agorootparentprevI also lucid dream occasionally. Very rarely things are very detailed, most often the colors and details are just as bleak and blurry and keep changing as these videos. I walk down a street, take a turn (or not), its almost guaranteed I can't go back to where I came from. I usually appreciate when I can track back the same path. reply sci_prog 4 hours agorootparentprevAlso the AI generated images that can't get the fingers right. Have you ever tried to look at your hands while lucid dreaming and try counting fingers? There are some really interesting parallels between the dreams and diffusion models. reply dartos 3 hours agorootparentOf course, due to the very nature of dreams, your awareness of diffusion models and their output flavors how you perceive even past dreams. Our brains love retroactively altering fuzzy memories. reply hombre_fatal 2 hours agorootparentOn the other hand, psychedelics give you perceptions similar to even early deepdream genai images. On LSD, I was swimming in my friend’s pool (for six hours…) amazed at all the patterns on his pool tiles underwater. I couldn’t get enough. Every tile had a different sophisticated pattern. The next day I went back to his place (sober) and commented on how cool his pool tiles were. He had nfi what I was talking about. I walk out to the pool and sure enough it’s just a grid of small featureless white tiles. Upon closer inspection they have a slight grain to them. I guess my brain was connecting the dots on the grain and creating patterns. It was quite a trip to be so wrong about reality. Not really related to your claim I guess but I haven’t thought of this story in 10 years and don’t want to delete it. reply Jackson__ 2 hours agorootparentprevThis may be a joke, but counting your fingers to lucid dream has been a thing for a lot longer than diffusion models. That being said, your reality will influence your dreams if you're exposed to some things enough. I used to play minecraft on a really bad PC back in the day, and in my lucid dreams I used to encounter the same slow chunk loading as I saw in the game. reply thegabriele 3 hours agoparentprevWe are unconsciously (pun intended) implementing how brains work both in dream and wake states. Can't wait until we add some kind of (lossless) memory to this models. reply soraki_soladead 3 hours agorootparentWe have lossless memory for models today. That's the training data. You could consider this the offline version of a replay buffer which is also typically lossless. The online, continuous and lossy version of this problem is more like how our memory works and still largely unsolved. reply hackernewds 3 hours agorootparentprevAny evidence to back this lofty claim? reply earnesti 3 hours agoparentprevThat looks way too much to the one time I did DMT-5 reply TechDebtDevin 2 hours agorootparentMachine Elves reply marcyb5st 7 hours agoprevSo, this is pretty exciting. I can how this can already be used to generate realistic physics approximations in a game engine. You create a bunch of snippets of gameplay using a much heavier and realistic physics engine (perhaps even CGI). The model learn to approximate the physics and boom, now you have a lightweight physics engine. Perhaps you can even have several that are specialized (e.g. one for smoke dynamics, one for explosions, ...). Even if it allucinates, wouldn't be worse than the physics bugs that are so common in games. reply monsieurbanana 6 hours agoparent> Even if it allucinates, wouldn't be worse than the physics bugs that are so common in games. I don't know about that. Physic bugs are common, but you can prioritize and fix the worst (gamebreaking) ones. If you have a blackbox model, it becomes much harder to do that. reply bobsomers 2 hours agoparentprevWhat makes you think the network inference is less expensive? Newtonian physics is already extremely well known and pretty computationally efficient to compute. How would a \"function approximation\" of Newtonian physics, with billions of parameters, be cheaper to compute? It seems like this would both be more expensive and less correct than a proper physics simulation. reply twic 5 hours agoparentprevDo you think that inference on a thirteen million parameter neural network is more lightweight than running a conventional physics engine? reply tiagod 2 hours agorootparentIn some cases, the model will be lighter. There is no need for 14M parameters for physics simulations, and there's a lot of promising work in that area. reply procgen 4 hours agorootparentprevConvincing liquid physics (e.g. surf interacting with a beach, rocks, the player character) might be a good candidate. reply epolanski 5 hours agorootparentprevEvery software that can be implemented in a JavaScript, ehm, LLM, will eventually be implemented in an LLM. reply kendalf89 3 hours agorootparentAre you predicting node.llm right now? reply crazygringo 1 hour agoparentprevYeah, I definitely wouldn't trust it to replace basic physics of running, jumping, bullets, objects shattering, etc. But it seems extremely promising for fiery explosions, smoke, and especially water. Anything with dynamics that are essentially complex. Also for lighting -- both to get things like skin right with subsurface scattering, as well as global ray-traced lighting. You can train specific lightweight models for these things, and they important thing is that their output is correct at the macro level. E.g., a tree should be casting a shadow that looks like the right shadow at the right angle for that type of tree and its types of leaves and general shape. Nobody cares if each individual leaf shadow corresponds to an individual leaf 10 feet above or is just hallucinated. reply amelius 3 hours agoparentprev> boom, now you have a lightweight physics engine lightweight, but producing several hundred watts of heat. reply Thorrez 5 hours agoparentprevWould that work for multiplayer? If it's a visual effect only, I guess it would be ok. But if it affects gameplay, wouldn't different players get different results? reply killerstorm 5 hours agorootparentWell, it doesn't make sense to use this exact model - this is just demonstration that it can learn world model from pixels. An obvious next step towards a more playable game is to add state vector to the inputs of the model: it is easier to learn to render the world from pixels + state vectors than from pixels alone. Then it depends what we want to do. If we want normal Counter Strike gameplay but with new graphics, we can keep existing CS game server and train only the rendering part. If you want to make Dream-Counter-Strike where rules are more bendable then you might want to train state update model... reply hobs 7 hours agoparentprevA physics bug would be a consistent problem you can fix. There's no such guarantee about an ML model. This would likely only be ok in the context of a game specifically made to be janky. reply fullstackwife 6 hours agorootparentThis is one of the fallacies of current AI research space: they don't focus on the end-user too much. In this case the end-user would be the gamer, and while playing games you expect a valid gameplay, so those kind of hallucinations are not acceptable, while I'm pretty sure they give the AI research authors a strong dopamine trigger. We have a hammer and now we are looking for a nail, while you should ask a question first: what is the problem we are trying to solve here? Real world usage will be probably different, and maybe even unexpected by the authors of this research. reply jsheard 6 hours agorootparent> This is one of the fallacies of current AI research space: they don't focus on the end-user too much. In this case the end-user would be the gamer Or from another angle the end-user is a game developer trying to actually work with this kind of technology, which is just a nightmarish prospect. Nobody in the industry is asking for a game engine that runs entirely on vibes and dream logic, gamedev is already chaotic enough when everything is laid out in explicit code and data. reply stale2002 1 hour agorootparentprev> they don't focus on the end-user too much. Of course they don't. Stuff like this is a proof of concept. If they had a product that worked, they wouldn't be in academia. Instead, they would leave the world of research and create a multi billion dollar company. Almost by definition, anything in academia isn't going to be productized, because if it was, then the researchers would just stop researching and make a bunch of money selling the product to consumers. Such research is still useful for society, though, as it means that someone else can spend the millions and millions of dollars making a better version and then selling that. reply badpun 6 hours agorootparentprevThe whole purpose of academia is literally to nerd out on cool, impractical things, which will ocasionally turn out to have some real-life relevance years or decades later. This (hallucinated CS) is still more relevant to real world than 99% of what happens in academic research. reply dartos 3 hours agorootparentYes to the first part, no to the random “99% useless” number you made up. I’m no fan of academia, but it undeniably produces useful and meaningful knowledge regularly. reply kqr 6 hours agorootparentprevThis obsession people have with determinism! I'd much rather take a low rate of weird bugs than common consistent ones. I don't believe reproducibility of bugs makes for better gameplay generally. reply paulryanrogers 6 hours agorootparentReproducibility does make bugs more likely to be fixed, or at least fixable. Also, games introduce randomness in a controlled way so users don't get frustrated by it appearing in unexpected places. I don't want characters to randomly appear and disappear. It's fine if bullet trajectory varies more randomly as they get further away. reply skydhash 4 hours agorootparentAlso most engines have been worked on for years. So more often than not, core elements like audio, physics, input,... are very stable and the remaining bugs are either \"can't fix\" or \"won't fix\". reply NotMichaelBay 5 hours agorootparentprevIt might be fine for casual players, but it would prevent serious and pro players from getting into the game. In Counter-Strike, for example, pro players (and other serious players) practice specific grenade throws so they can use them reliably in matches. reply kqr 5 hours agorootparentI'm not saying one can make specifically Counter-Strike on a non-deterministic engine -- that seems like strawmanning my argument. People play and enjoy many games with varying levels of randomness as a fundamental component, some even professionally (poker, stock market). This could be made such a game. reply monsieurbanana 4 hours agorootparentEither the physics engine matter, in which case you want a deterministic engine as you said, or it doesn't like in a poker game and you don't want to spend much resources (manpower, computer cycles) into it. Which also means an off-the-shelf deterministic engine. reply mrob 5 hours agorootparentprevThe whole hobby of speedrunning relies heavily on exploiting deterministic game bugs. reply dartos 3 hours agorootparentprevYou don’t play a lot of games, huh? Consistent bugs you can anticipate and play/work around, random ones you can’t. Just look at pretty much any speed running community for games before 1995. Say goodbye to any real competitive scene with random unfixable potentially one off bugs. reply hobs 4 hours agorootparentprevMake a fun game with this as a premise and I will try it, but it sounds just an annoying concept. reply francoisfleuret 8 hours agoprevThis is 300M parameters model (1/1300th of the big llama-3) trained with 5M frames with 12 days of a GTX4090. This is what a big tech company was doing in 2015. The same stuff at industrial scale à la large LLMs would be absolutely mind blowing. reply gjulianm 5 hours agoparentWhat exactly would be the benefit of that? We already have Counter Strike working far more smooth than this, without wasting tons of compute. reply ben_w 5 hours agorootparentAs with diffusion models in general, the point isn't the specific example but that it's generalisable. 5 million frames of video data with corresponding accelerometer data, and you get this for genuine photorealism. reply gjulianm 1 hour agorootparentGeneralisable how? The model completely hallucinates invalid input, it's not even high quality and required CSGO to work. What's the output you expect from this and what alternatives are there? reply ben_w 7 minutes agorootparentIt did not require CSGO, that was simply one of their examples. The very first video in the link shows a bunch of classic Arati games, and even the video which is showing CSGO is captioned \"DIAMOND's diffusion world model can also be trained to simulate 3D environments, such as CounterStrike: Global Offensive (CSGO)\" — I draw your attention to \"such as\" being used rather than \"only\". And I thought I was fairly explicit about video data, but just in case that's ambiguous: the stuff you record with your phone camera set to video mode, synchronised with the accelerometer data instead of player keyboard inputs. As for output, with the model as it currently stands, I'd expect a 24h training video at 60fps to be \"photorealisic and with similar weird hallucinations\". Which is still interesting, even without combining this with a control net like Stable Diffusion can do. reply stale2002 1 hour agorootparentprevTo answer your question directly, the benefit is that we could make something different from counter strike. You see, there are these things called \"proof of concept\"s that are meant to not be a product, but instead show off capabilities. Counterstrike is an example, meant to show off complex capabilities. It is not meant to show how the useful thing of these models is to literally recreate counterstrike. reply gjulianm 1 hour agorootparentWhich capabilities are being shown off here? The ability to take an already existing world-model and take lots of compute to have a worse, less correct model? reply nuz 4 hours agorootparentprev\"What would be the point of creating a shooter set in the middle east? We already have pong and donkey kong\" reply eproxus 5 hours agorootparentprevBut please, think of the shareholders! reply GaggiX 8 hours agoparentprevIf 12 days with an RTX4090 is all you need, some random people on the Internet will soon start training their own. reply cs702 1 hour agoparentprevCame here to say pretty much the same thing, and saw your comment. The rate of progress has been mind-blowing indeed. We sure live in interesting times! reply Sardtok 6 hours agoparentprevTwo 4090s, but yeah. reply Sardtok 6 hours agorootparentNever mind, the repo on Github says 12 days on a 4090, so I'm unsure why the title here says two. reply croo 8 hours agoprevFor anyone who actually tried it : Does it respects/builds some kind of game map in the process or is it just a bizarre psychedelic dream walk experience where you cannot go back the same place twice and space dimensions are just funny? Is a game map finite? reply InsideOutSanta 7 hours agoparentJust looking at the first video, there's a section where structures just suddenly appear in front of the player, so this does not appear to build any kind of map, or have any kind of meaningful awareness of something resembling a game state. This is similar to LLM-based RPGs I've played, where you can pick up a sword and put it in your empty bag, and then pull out a loaf of bread and eat it. reply anal_reactor 7 hours agorootparent> you can pick up a sword and put it in your empty bag, and then pull out a loaf of bread and eat it Mondays reply aidos 8 hours agoparentprevJust skimmed the article but my guess is that it’s a dream type experience where if you turned around 180 and walked the other direction it wouldn’t correspond to where you just came from. More like an infinite map. reply lopuhin 6 hours agorootparentI don't think so, what they show on CS video is exactly the Dust2 map, not just something similar/inspired by it. reply twic 5 hours agorootparentIt's trained on moving around dust2, so as long as the previous frame was a view of dust2, the next frame is very likely to be a plausible subsequent view of dust2. In some sense, this encodes a map; but it's not what most people think of when they think about maps. I'd be interested to see what happens if you look down at your feet for a while, then back up. If the ground looks the same everywhere, do you come up in a random place? reply arendtio 4 hours agorootparentprevIt probably depends on what you see. As long as you have a broad view over a part of the map, you should stay in that region, but I guess that if you look at a mono-color wall, you probably find yourself in a very different part of the map when you look around yourself again. But I am just guessing, and I haven't tried it yet. reply delusional 8 hours agoparentprevJust tried it out, and no. It doesn't have any sort of \"map\" awareness. It's very much in the \"recall/replay\" category of \"AI\" where it seems to accurately recall stuff that is part of the training dataset, but as soon as you do something not in there (like walk into a wall), it completely freaks out and spits out gibberish. Plausible gibberish, but gibberish none the less. reply neongreen 8 hours agorootparentCan you upload a screen recording? I don’t think I can run the model locally but it’d be super interesting to see what happens if you run into a wall reply kqr 6 hours agorootparentprevThis should mainly be a matter of giving it more training though, right? It sounds like to amount of training it's gotten is relatively sparse. reply treyd 5 hours agorootparentIt doesn't have any ability to reason about what you did more than a couple of seconds ago. Its memory is what's currently on the screen and what the user's last few inputs were. reply delusional 1 hour agorootparentprevTheoretically. In practice, that's not clear. As you add more training data you have to ask yourself what the point is. we already have a pretty good simulation of Counter Strike. reply cousin_it 8 hours agoprevI continue to be puzzled by people who don't notice the \"noise of hell\" in NN pictures and videos. To me it's always recognizable and terrifying, has been from the start. reply npteljes 7 hours agoparentWhat do you mean by noise of hell in particular? I do notice that the images are almost always uncanny in a way, but maybe we're not meaning the same thing. Could you elaborate on what you experience? reply taneq 7 hours agoparentprevLike a subtle but unsettling babble/hubbub/cacophony? If so then I think I kind of know what you mean. reply TechDebtDevin 2 hours agorootparentThere's definately a bit of an uncanny valley in the land of top tier diffusion models. A generative video of someone smiling is way more likely to illicit this response for me than a generative image or single frame. It definately has something to do with the movement. reply cousin_it 1 hour agorootparentprevYes, that's exactly it. reply HKH2 8 hours agoparentprevEyes have a lot of noise too. reply mk_stjames 8 hours agoprevThis was Schmidhuber's group is 2018: https://worldmodels.github.io/ Just want to point that out. reply afh1 5 hours agoparentAhead of its time for sure. Dream is an accurate term here, that driving scene does resemble driving in dreams. reply jmchambers 7 hours agoprevI _think_ I understand the basic premise behind stable diffusion, i.e., reverse the denoising process to generate realistic images but, as far as I know, this is always done at the pixel level. Is there any research attempting to do this at the 3D asset level, i.e., subbing in game engine assets (with position and orientation) until a plausible scene is recreated? If it were possible to do it that way, couldn't it \"dream\" up real maps, with real physics, and so avoid the somewhat noisy output these types of demo generate? reply furyofantares 2 minutes agoparent> but, as far as I know, this is always done at the pixel level Image models are NOT denoised at the pixel level - diffusion happens in latent space. This was one of the big breakthroughs that made all of this work well. There's a model for encoding/decoding between pixels and latent space. Latent space is able to encode whatever concepts it needs in whichever of its dimensions it needs, and is generally lower dimensional than pixel space. So we get a noisy latent space, denoise it using the diffusion model, then use the other model (variational autoencoder) to decode into pixel space. reply desdenova 7 hours agoparentprevI think the closest we have right now is 3D gaussian splatting. So far it's only been used to train a scene from photographs from multiple angles and rebuild it volumetrically by adjusting densities in a point-cloud. But it might be possible to train a model on multiple different scenes, and perform diffusion on a random point cloud to generate new scenes. Rendering a point cloud in real time is also very efficient, so it could be used to create insanely realistic game worlds instead of polygonal geometry. It seems someone already thought of that: https://ar5iv.labs.arxiv.org/html/2311.11221 reply jmchambers 7 hours agorootparentInteresting, I guess that takes things even further and removes the need for hand-crafted 3D assets altogether, which is probably how things will end up going in gaming, long-term. I was suggesting a more modest approach, I guess, one where the reverse-denoising process involves picking and placing existing 3D assets, e.g., those in GTA 5, so that the process is actually building a plausible map, using those 3D assets, but on the fly... Turn your car right and a plausible street decorated with buildings, trees and people is dreamt up by the algorithm. All the lighting and physics would still be done in-engine, with stable diffusion acting as a dynamic map creator, with an inherent knowledge of how to decorate a street with a plausible mix of assets. I suppose it could form the basis of a procedurally generated game world where, given the same random seed, it could generate whole cities or landscapes that would be the same on each player's machine. Just an idea... reply skydhash 4 hours agorootparentThe thing is that, there are generators that can do exactly this, no need to have an LLM as the middle man. Things like terrain generation, city generation, crowd control, character generation, can be done quite easily with far less compute and energy. reply magicalhippo 4 hours agorootparentprevTechnically I guess one could do a stable diffusion-like model except on voxels, where instead of pixel intensity values it producing a scalar field which you could turn into geometry using marching cubes or something similar. Not sure how efficient that would be though, and would only work for assets like teapots and whatnot, not whole game maps say. reply jampekka 6 hours agoparentprevNot exactly 3D assets, but diffusion modems are used to generate e.g. traffic (vehicle trajectories) for evaluating autonomous vehicle algorithms. These vehicles tend to crash quite a lot. For example https://github.com/NVlabs/CTG Edit: fixed link reply tiborsaas 6 hours agoparentprevGenerating this at pixel level is the next level thing. The reverse engineering method your described is probably appealing because it's easier to understand. Focusing on pixel level generation is the right approach I think. The somewhat noisy output will be improved upon probably in a short timeframe. Now that they proved with Doom (https://gamengen.github.io/) and this that it's possible, probably more research is happening currently to nail the correct architecture to scale this to HD and minimal hallucination. It happened with videos alredy so we should see a similar level breakthrough soon. reply gliptic 7 hours agoparentprev> I _think_ I understand the basic premise behind stable diffusion, i.e., reverse the denoising process to generate realistic images but, as far as I know, this is always done at the pixel level. It's typically not done at the pixel level, but at the \"latent space\" level of e.g. a VAE. The image generation is done in this space, which has fewer outputs than the pixels of the final image, and then converted to the pixels using the VAE. reply jmchambers 7 hours agorootparentFrantically Googles VAE... Ah, okay, so the work is done at a different level of abstraction, didn't know that. But I guess it's still a pixel-related abstraction, and it is converted back to pixels to generate the final image? I suppose in my proposed (and probably implausible) algorithm, that different level of abstraction might be loosely analogous to collections of related game engine assets that are often used together, so that the denoising algorithm might be effectively saying things like \"we'll put some building-related assets here-ish, and some park-related flora assets over here...\", and then that gets crystallised in to actual placement of individual assets in the post-processing step. reply StevenWaterman 5 hours agorootparent(High level, specifics are definitely wrong here) The VAE isn't really pixel-level, it's semantic-level. The most significant bits in the encoding are like \"how light or dark is the image\" and then towards the other end bits represent more niche things like \"if it's an image of a person, make them wear glasses\". This is way more efficient than using raw pixels because it's so heavily compressed, there's less data. This was one of the big breakthroughs of stable diffusion compared to previous efforts like disco diffusion that work on the pixel level. The VAE encodes and decodes images automatically. It's not something that's written, it's trained to understand the semantics of the images in the same way other neural nets are. reply ilaksh 5 hours agoprevI wonder if there is some way to combine this with a language model, or somehow have the language model in the same latent space or something. Is that was vision-language models already do? Somehow all of the language should be grounded in the world model. For models like Gemini that can answer questions about video, it must have some level of this grounding already. I don't understand how this stuff works, but compressing everything to one dimension as in a language model for processing seems inefficient. The reason our language is serial is because we can only make one sound at a time. But suppose the \"game\" trained on was a structural engineering tool. The user asks about some scenario for a structure and somehow that language is converted to an input visualization of the \"game state\". Maybe some constraints to be solved for are encoded also somehow as part of that initial state. Then when it's solved (by an agent trained through reinforcement learning that uses each dreamed game state as input?), the result \"game state\" is converted somehow back into language and combined with the original user query to provide an answer. But if I understand properly, the biggest utility of this is that there is a network that understands how the world works, and that part of the network can be utilized for predicting useful actions or maybe answering questions etc. ? reply LarsDu88 3 hours agoparentTo combine with a language model simply replace the action vector with a language model latent. Alternative as of last year there are now purely diffusion based text decoder models reply DrSiemer 8 hours agoprevWhere it gets really interesting is if we can train a model on the latest GTA, plus maybe related real life footage, and then use it to live upgrade the visuals of an old game like Vice City. The lack of temporal consistency will still make it feel pretty dreamlike, but it won't matter that much, because the base is consistent and it will look amazing. reply davedx 8 hours agoparentA game like GTA has way too much functionality and complex branching for this to work I think (beyond eg doing aimless drives around the city — which would be very cool though) reply DrSiemer 1 hour agorootparentGta 5 has everything Vice City has and more. In the Doom AI dream it's possible to shoot people. Maybe in this CS model as well? I think the model does not have to know anything about the functionality. It can just dream up what is most probable to happen based on the training data. reply InsideOutSanta 7 hours agoparentprevJust redrawing images drawn by an existing game engine works, and generates amazing results, although like you point out, temporal consistency is not great. It might interpret the low-res green pixels on a far-away mountain as fruit trees in one frame, and as pines in the next. Here's a demo from 2021 doing something like that: https://www.youtube.com/watch?v=3rYosbwXm1w reply sorenjan 6 hours agoparentprevIn addition to the sibling comment's older example there's new work done with GTA too. https://www.reddit.com/r/aivideo/comments/1fx6zdr/gta_iv_wit... reply DrSiemer 1 hour agorootparentCool! Looks fairly consistent as well. I wonder if this type of AI upscaling could eventually also fix things like slightly janky animations, but I guess that would be pretty hard without predetermined input and some form of look ahead. Limiting character motion to only allow correct, natural movement would introduce a strange kind of input lag. reply sorenjan 25 minutes agorootparentNvidia has done work in AI driven character animation too. https://www.nvidia.com/en-us/on-demand/session/gtcspring22-d... reply skydhash 4 hours agoparentprevWhy not just creating the assets with higher resolution? reply DrSiemer 2 hours agorootparentBecause that is a lot more work, will only work for a single game, potentially requires more resources to run and will not get you the same level of realism. reply mungoman2 9 hours agoprevThis is getting ridiculous! Curious, since this is a strong loop old frame + input -> new frame, What happens if a non-CS image is used to start it off? Or a map the model has never seen. Will the model play ball, or will it drift back to known CS maps? reply Arch-TK 8 hours agoparentLooks like it only knows Dust 2 since every single \"dream\" (I'm going to call them that since looking at this stuff feels like dreaming about Dust 2) is of that map only. reply fancyfredbot 8 hours agoprevStrangely the paper doesn't seem to give much detail on the cs-go example. Actually the paper explicitly mentions it's limited to discrete control environments. Unless I'm missing something the mouse input for counterstrike isn't discrete and wouldn't work. I'm not sure why the title says it was trained on 2x4090 either as I can't see this on either the linked page or the paper. The paper mentions a GPU year of 4090 compute was used to train the Atari model. reply c1b 8 hours agoparentCSGO model is only 1.5 gb & training took 12 days on a 4090 https://github.com/eloialonso/diamond/tree/csgo?tab=readme-o... reply fancyfredbot 8 hours agorootparentThanks, that's the detail I was looking for on the training. It's amazing results like this can be achieved at such a low costs! I thought this kind of work was out of reach for the GPU poor. The part about the continuous control still seems weird to me though. If anyone understands that then very interested to hear more. reply gadders 6 hours agoprevCool achievement, but I want AI to give me smarter NPCs, not simulate the map. reply thelastparadise 5 hours agoparentThe NPCs need a model of the world in their brain in order to act normal. reply shahzaibmushtaq 6 hours agoprevAs I used to play CS 1.6 and CS: GO in my free time before the pandemic, this playable CS diffusion world map has been trained by a noob player for research purposes. After reading the comments I can assume that if you play outside of the scope it was trained on, the game loses its functionality. Nevertheless, R&D for a good cause is something we all admire. reply crossroadsguy 6 hours agoparentHow is the last version CS 2.0 (I think)? It’s been free to play like GO I guess. Is it like GO where physics felt too dramatised (could just be my opinion)? Or realistic in a snappy way like 1.6? reply shahzaibmushtaq 4 hours agorootparentHonestly, I heard about CS 2.0 from you. And you are right what you just said about GO. reply Zealotux 6 hours agoprevCould we imagine parts of game elements to become \"targets\" for models? For example hair and fur physics have been notoriously difficult to nail, but it should be easier to use AI to simulate some fake physics on top of the rendered frame, right? Is anyone working on that? reply ThouYS 8 hours agoprevI don't really understand the intuition on why this helps RL. The original game has a lot more detail, why can't it be used directly? reply jampekka 8 hours agoparentIt is used as a predictive model of the environment for model-based RL. I.e. agents can predict consequences of their actions. reply ThouYS 8 hours agorootparentOh, I see. I was somehow under the impression that the simulation was the game the RL agent learns to play (which kinda seemed nonsensical). reply visarga 7 hours agoparentprevIt can use the game directly but if you try this with real life robots, then it is better to do neural simulation before performing an action that could result in injury or damage. We don't need to fall with our cars off the road many times to learn to drive on the road because we can imagine the consequences. Same thing here. reply FeepingCreature 8 hours agoparentprevIn the real world, you can't just boot up a copy of reality to play out strategies. You need an internal model. reply tourmalinetaco 8 hours agorootparentSo, effectively, these video game models are proof-of-concepts to say “we can make models with extremely accurate predictions using minimal resources”? reply usrusr 6 hours agorootparentNot sure where you see the \"minimal resources\" here? But I'd just counter all questions about \"why\" with the blanket response of \"for understanding natural intelligence\": the way biology innovates is that it throws everything against the wall and not pick the one thing that sticks as the winner and focus on that mechanism, it keeps the sticky bits and also everything else as long as their cost isn't prohibitive. Symbolic modeling (\"this is an object that can fall down\"), prediction chains based on visual similarity patterns (this), hardwired reflexes (we tend to not trust anything that looks and moves like a spider or snake) and who knows what else, it's all there, it all runs in parallel, invited or not, and they all influence each other in subtle and less subtle ways. The interaction is not engineered, it's more like crosstalk that's allowed to happen and has more upside than downside, or else evolution would have preferred variations of the setup that have less of the kind of crosstalk in question. But in our quest to understand us, it's super exciting to see candidates for processes that perhaps play some role in our minds, in isolation, no matter if that role is big or small. reply vbezhenar 6 hours agorootparentprevMay be I'm wrong but my understanding is that you can film some area using, say, dashcams and then generate this kind of neuro model. Then you can train robot to walk in this area with this neuro-model. It can perform billions of training sessions without touching physical world. Alternatively you can somehow perform 3D scan of area, recreate its 3D model and use, say, game engine to simulate, but that probably requires more effort and not necessarily better. reply usrusr 6 hours agorootparentAnd the leg motions we sometimes see in sleeping dogs suggest that this is very much a way how having dreams is useful! reply LarsDu88 3 hours agoprevIterative denoising diffusion is such a hurdle for getting this sort of thing running at reasonable fps reply thenthenthen 8 hours agoprevWhen my game starts to look like this, I know it is time to quit hahha, maybe a helpful tool in gaming addiction therapy? The morphing of the gun/skins and the environment (the sandbags) wow. Would like to play this and see what happens when you walk backwards, turn around quick, use ‘noclip’ :D reply advael 9 hours agoprevDang this is the first paper I've seen in a while that makes me think I need new GPUs reply w-m 8 hours agoprevIf you're not bored with it yet, here's a Deep Dive (NotebookLM, generated podcast). I fed it the project page, the arXiv paper, the GitHub page, and the two twitter threads by the authors. https://notebooklm.google.com/notebook/a240cb12-8ca1-41b4-ab... (7m59s) As always, it's not actually much of a technical deep dive, but gives a quite decent overview of the pieces involved, and its applications. reply thierrydamiba 7 hours agoparentHow did you get the output to be so long? My podcasts are 3 mins max… reply w-m 1 hour agorootparentOh wow, really? Even if you feed it whole research papers? The ones I tried until now were more in the 8-10 minute range. I haven’t looked in to how to control the output yet. Hopefully that’ll get a little more transparent and controllable soon. reply delusional 8 hours agoprevI just checked it out right quick. It works perfectly well on an AMD card with ROCM pytorch. It seems decent in short bursts. As it goes on it quite quickly loses detail and the weapon has a tendency to devolve into colorful garbage. I would also like to point out that none of the videos show what happens when you walk into a wall. It doesn't handle it very gracefully. reply styfle 5 hours agoprevBut does it work on macOS? (The latest CS removed support for macOS) reply iwontberude 6 hours agoprevThis is crazy looking, I know it’s basically useless but it’s cool anyways. reply mixtureoftakes 8 hours agoprevthis is crazy when trying to run on a mac it only plays in a very small window, how could this be configured? reply 6510 9 hours agoprevCan it use a seed that makes the same map every time? reply madaxe_again 9 hours agoprevI earnestly think this is where all gaming will go in the next five years - it’s going to be so compelling that stuff already under development will likely see a shift to using diffusion models. As this is demonstrating, a sufficiently honed model can produce realtime graphics - and some of the demos floating around where people are running GTA San Andreas through non-realtime models hint as to where this will go. I give it the same five years before there are games entirely indistinguishable from reality, and I don’t just mean graphical fidelity - there’s no reason that the same or another model couldn’t provide limitless physics - bust a hole through that wall, set fire to this refrigerator, whatever. reply qayxc 8 hours agoparentI think you're missing the most important point: these models need to be trained on something and that something is a fully developed, working game. You're basically saying that game development would need to do the work twice: step 1: develop a fully functional game, step 2: spend ridiculous effort (in terms of time and compute) on training a model to emulate the game in a half-baked fashion. It's a solution looking for a problem. reply manmal 8 hours agorootparentThe world model can still be rendered in very low res, and then the diffusion skin/remaster is applied. And this would also be an exciting route to go at remastering old games. I‘d pay a lot to play NFS Porsche again, with photorealism. Or imagine Command & Conquer Red Alert, „rendered“ with such a model. reply qayxc 8 hours agorootparentNVIDIA's RTX Remix [1] suite of tools already does that. It doesn't require any model training or dozens of hours of pre-recorded gameplay either. You can drop in low-res textures and have AI tools upscale them. Models can be replaced, as well as lighting and the best part: it's all under your control. You're not at the merci of obscure training material that might or might not result in a consistent look-and-feel. More knobs, more control, less compute required. [1] https://www.nvidia.com/en-us/geforce/rtx-remix/ reply manmal 7 hours agorootparentTIL, thanks for posting. The workflow I was sketching out is simpler though: Render a legacy game or low fidelity modern game as-is, and run it through a diffusion model in real time. reply FeepingCreature 8 hours agorootparentprevYou can crosstrain on reality. reply qayxc 8 hours agorootparentSure - and that'll work great on titles like Ratchet & Clank [1] or Tiny Tina's Wonderland [2] because art and style are dead and everything must be a mirror-like reflection of reality in order to be a fun game... [1] https://en.wikipedia.org/wiki/Ratchet_%26_Clank [2]https://playwonderlands.2k.com reply casenmgreen 8 hours agoparentprevNot a chance. There are fundamental limitations with what are in the end all essentially neural nets; there is no understanding, only prediction. Prediction alone is not enough to emulate reality, which is why for example genuinely self-driving cars have not, and will not, emerge. A fundamental advance in AI technology will be required for that, something which leads to genuine intelligence, and we are no closer to that than ever we were. reply fancyfredbot 8 hours agorootparentLooking at the examples of 2600 games in the paper I'm not sure you can tell that they are just predictions. Have you considered how you'd tell the difference between a prediction and understanding in practice? reply francoisfleuret 8 hours agorootparentprev\"there is no understanding, only prediction\" I have no idea what this means. reply nonrandomstring 8 hours agorootparent> > \"there is no understanding, only prediction\" > I have no idea what this means. You can throw a ball up in the air and predict that it will fall again and bounce. You have no understanding of mass, gravity, acceleration, momentum, impulse, elasticity... You can press a button that makes an Uber car appear in reality and take you home. You have no understanding of apps, operating systems, radio, internet, roads, wheels, internal combustion engines, driving, GPS, maps... This confusion of understanding and prediction affects a lot of people who use technology in a \"machine-like\" way, purely instrumental and utilitarian... \"how does this get me what I want immediately?\" You can take any complex reality and deflate it, abstract it, reduce it down to a mere set of predictions that preserve all the utility for a narrow task (in this case visual facsimile) but strip away all depth of meaning. The models, of both the system and the internal working model of the user are flattened. In this sense \"AI\" is probably the greatest assault on actual knowledge since the book burning under totalitarian regimes of the mid 20th century. reply binary132 7 hours agorootparentI think GP is saying that understanding is measured by predictive capability of the theory and in case you hadn’t noticed, that kind of uncomprehending slopthink has been going on for a lot longer than the AI fad reply GaggiX 7 hours agorootparentprevWhat if the model actually understands that the ball will fall and bounce because of mass, gravity, acceleration, momentum, impulse, elasticity? I mean you can just ask ChatGPT and Claude, I guess you would answer that in this case it's just prediction, but if they were human then it would be understanding. reply nonrandomstring 7 hours agorootparent> I guess you would answer that in this case it's just prediction, No I would answer that it is indeed understanding, to upend your \"guess\" (prediction) and so prove that while you think you can \"predict\" the next answer you lack understanding of what the argument is really about :) reply GaggiX 6 hours agorootparentI think I understand the topic quite well, since you deliberately deviate from answering the question. You made a practical example that doesn't really work in practice. reply tourmalinetaco 8 hours agorootparentprevThe MLM has no idea what it’s making, where you are in the map, what you left behind, and what you picked up. It can accurately predict what comes next, but if you pick up an item and do a 360° turn the item will be back and you can repeat the process. reply GaggiX 8 hours agorootparentprevWhen a human does it, it's understanding, when an AI does it, it's prediction, I thinks it's very clear /s reply therouwboat 7 hours agorootparentDoes what? In normal game world things tend to stay where they are without player having to do anything. reply GaggiX 6 hours agorootparentWe are talking about neural networks in general, not this one or that one, if you train a bad model or the model is untrained it would not indeed understand much or anything. reply killerstorm 8 hours agorootparentprevThat's bs. You have no understanding of understanding. Hooke's law was pure curve-fitting. Hooke definitely did not understand the \"why\". And yet we don't consider that bad physics. Newton's laws can be derived from curve fitting. How is that different from \"understanding\"? reply madaxe_again 7 hours agorootparentEinstein couldn’t even explain why general relativity occurred. Sure, spacetime is curved by mass, but why? What a loser. reply killerstorm 6 hours agorootparentIt's very illustrative to look into the history of discovery of laws of motion, as it's quite well documented. People have an intuitive understanding of motion - we see it literally every day, we throw objects, etc. And yet it took literally thousands of years since discovery of mathematics (geometry, etc.) to formulate a concept of force, momentum, etc. Ancient Greek mathematicians could do integration, so they were not lacking mathematical sophistication. And yet their understanding of motion was so primitive: Aristotle, an extremely smart man, was muttering something about \"violent\" and \"natural\" motion: https://en.wikipedia.org/wiki/Newton%27s_laws_of_motion#Anti... People started to understand the conservation of quantity of motion only in 17th century. So we have two possibilities: * everyone until 17th century was dumb af (despite being able to do quite impressive calculations) * scientific discovery is really a heuristic-driven search process where people try various things until they find a good fit I.e. millions of people were somehow failing to understand motion for literally thousands of years until they collected enough assertions about motion that they were able to formulate the rule of conservation, test it, and confirm it fits. And only then it became understanding. You can literally see conservation of momentum on a billiard table: you \"violently\" hit one ball, it hits other balls and they start to move, but slower, etc. So you really transfer something from one ball to the rest. And yet people could not see it for thousands of years. What this shows is that there's nothing fundamental about understanding: it's just a sense of familiarity, it is a sense that your model fits well. Under the hood it's all prediction and curve fitting. We literally have prediction hardware in our brains: cerebellum has specialized cells which can predict, e.g. motion. So people with damaged cerebellum have impaired movement: they still can move, but their movement are not precise. When do you think we find specialized understanding cells in the human brain? reply mrob 5 hours agorootparentIt seems to me that your evidence supports the exact opposite of your conclusion. Familiarity was only enough to find ad-hoc heuristics for specific situations. It let us discover intuitive methods to throw stones, drive carts, play ball games, etc. but never discovered the general principle behind them. A skilled archer does not automatically know that the same rules can be used to aim a mortar. Ad-hoc heuristics are not the same thing as understanding. It took formal reasoning for humans to actually understand motion, of a type that modern AI does not use. There is something fundamental about understanding that no amount of familiarity can substitute for. Modern AI can gain enormous amounts of familiarity but still fail to understand, e.g. this Counter-Strike simulator not knowing what happens when the player walks into a wall. reply killerstorm 3 hours agorootparentPeople found that `m * v` is the quantity which is conserved. There's no understanding. It's just a formula which matches the observations. It also matches our intuition (a heavier object is hard to move, etc), and you feel this connection as understanding. Centuries later people found that conservation laws are linked to symmetries. But again, it's not some fundamental truth, it's just a link between two concepts. LLM can link two concepts too. So why do you believe that LLM cannot understand? I middle school I did extremely well in physics classes - I could solve complex problems which my classmates couldn't because I could visualize the physical process (e.g. motion of an object) and link that to formulas. This means I understood it, right? Years later I thought \"But what *is* motion, fundamentally?\". I grabbed Landau-Lifshitz mechanics textbook. How do they define motion? Apparently, bodies move in a way to minimize some integral. They can derive the rest from it. But it doesn't explain what a motion is. Some of the best physicists in the world cannot define it. So I don't think there's anything to understanding except feeling of connection between different things. \"X is like Y except for Z\". reply mrob 3 hours agorootparentUnderstanding is finding the simplest general solution. Newton's laws are understanding. Catching the ball is not. LLMs take billions of parameters to do anything and don't even generalize well. That's obviously not understanding. reply killerstorm 3 hours agorootparentYou're confusing two meanings of world \"understanding\": 1. Finding a comprehensive explanation 2. Having a comprehensive explanation which is usable 99.999% people on Earth do not discover any new laws, so I don't think you use #1 as a fundamental deficiency of LLMs. And nobody is saying that just training a LLM produces understanding of new phenomena. That's a strawman. The thesis is that a more powerful LLM together with more software, more models, etc, can potentially discover something new. That's not observed yet. But I'd say it would be weird if LLM can match capabilities of average folk but never match Newton. It's not like Newton's brain is fundamentally different. Also worth noting that formulas can be discovered by enumeration. E.g. `m * v` should not be particularly hard to discover. And the fact that it took people centuries implies that that's what happened: people tried different formulas until they found one which works. It doesn't have to be some fancy Newton magic. reply mrob 2 hours agorootparentI'm certain that people did not spend centuries trying different formulas for the laws of motion before finding one that worked. The crucial insight was applying any formula at all. Once you have that then the rest is relatively easy. I don't see LLMs making that kind of discovery. reply madaxe_again 7 hours agorootparentprevYet we have no understanding, only prediction. We can describe a great many things in detail, how they interact - and we can claim to understand things, yet if you recursively ask “why?” everybody, and I mean everybody, will reach a point where they say “I don’t know” or “god”. An incomplete understanding is no understanding at all, and I would argue that we can only predict, and we can certainly emulate reality, otherwise we would not be able to function within it. A toddler can emulate reality, anticipate causality - and they certainly can’t be said to be in possession of a robust grand unified theory. reply jiggawatts 8 hours agorootparentprevFor simulations like games, it's a trivial matter to feed the neural game engine pixel-perfect metadata. Instead of rendering the final shaded and textured pixels, the engine would output just the material IDs, motion vectors, and similar \"meta\" data that would normally be the inputs into a real-time shader. The AI can use this as inputs to render a photorealistic output. It can be trained using offline-rendered \"ground-truth\" raytraced scenes. Potentially, video labelled in a similar way could be used to give it a flair of realism. This is already what NVIDIA DLSS and similar AI upscaling tech uses. The obvious next step is not just to upscale rendered scenes, but to do the rendering itself. reply viraptor 8 hours agoparentprevIt's not that great yet. Given a model which can generate the game view in ~real time and a model which can generate the models and textures, why would you ever use the first option, apart from a cool tech demo? I'm sure there's space for new dreamy games where invisible space behind you transforms when you turn around, but for other genres... why? Destructible environment has been possible for quite a while, but once you allow that everywhere, you can get games into unplayable state. They need to be designed around that mechanic to work well: Noita, Worms, Teardown, etc. I don't believe the \"limitless physics\" would matter after a few minutes. reply Arch485 8 hours agoparentprevIt seems extremely unlikely to me that ML models will ever run entire games. Nobody wants a game that's \"entirely indistinguishable from reality\" anyways. If they did, they would go outside. I think it's possible specific engine components could be ML-driven in the future, like graphics or NPC interactions. This is already happening to a certain degree. Now, I don't think it's impossible for an ML model to run an entire game. I just don't think making + running your game in a predictive ML model will ever be more effective than making a game the normal way. reply jsheard 8 hours agorootparentYep, the fuzziness and opaqueness of ML models makes developing an entire game state inside one a non-starter in my opinion. You need precise rules, and you need to be able to iterate on those rules quickly, neither of which are feasible with our current understanding of ML models. Nobody wants a version of CS:GO where fundamental constants like weapon damage run on dream logic. If ML has any place in games it's for specific subsystems which don't need absolute precision, NPC behaviour, character animation, refining the output of a renderer, that kind of thing. reply advael 9 hours agoparentprevI'm not sure that's a warranted assumption based on this result, exciting as it is, we are still seeing replication of an extant testable world model, rather than extrapolation that can produce novel mechanics without them being in the training data. I'm not saying this isn't a stepping stone to that, I just think your prediction's a little optimistic based on the scope of that problem reply TinkersW 8 hours agoparentprevIt requires a monster GPU to run 10 fps at what looks like sub 720p... I think it may be abit more than 5 years.. reply snickerer 8 hours agoprevI see where this is going. The next step to create training data is a real human with a bodycam. There is only the need to connect the real body movement (step forward, turning left, etc) to typical keyboard and mouse game control events, to feed them into the model, too. I think that is what the devs here are dreaming about. reply CaptainFever 8 hours agoparentOr a cockpit cam for the world's most realistic flight simulator. /lighthearted reply devttyeu 8 hours agoparentprevThe \"We live in a simulation\" argument just started looking a lot more conceivable. reply tiborsaas 6 hours agorootparentI'm already very suspicious, we just got the same room number in the third hotel in a row. Someone got lazy with the details :) reply iwontberude 6 hours agorootparentprevNot really because is it simulators all the way down? Simulation theory explains nothing and only adds more unexplainable phenomenon. reply devttyeu 8 hours agoparentprevCould probably make a decent dataset from VR headset tracking cameras + motion sensors + passthrough output + decoded hand movements reply TealMyEal 8 hours agoprev [–] What's the end goal here? personalised games for everyone. ultra-graphics. i dont really see how this is going to be better than our engine based systems. I love being a horse in the 1900s that automobilewill never take off /s reply visarga 7 hours agoparentThe goal is to train agents that can imagine consequences before acting. But it could also become a cheap way to create experiences and user interfaces on the fly, imagine if you can have any app UI dreamed up like that, not just games. Generative visual interfaces could be a big leap over text mode. reply qayxc 8 hours agoparentprev [–] It's a research paper. Not everything that comes out of research has an immediate real-world application in mind. Games are just an accessible and easy to replicate context to work in, they're not an end goal or target application. The research is about AI agents interacting with and creating world models. Such world models could just as well be alien environments - i.e. the kind of stuff an interstellar and even interplanetary probe would need to be able to do, as two-way communication over large distances is impractical. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "DIAMOND is a reinforcement learning agent utilizing a diffusion world model to enhance visual detail and performance, particularly in image generation.",
      "It achieves a mean human-normalized score of 1.46 on the Atari 100k benchmark, surpassing traditional discrete token-based models like IRIS in visual detail capture.",
      "The model employs EDM (Efficient Diffusion Models) for stable trajectories with fewer denoising steps, improving speed and consistency in simulations."
    ],
    "commentSummary": [
      "Diffusion models are being investigated for world modeling, drawing parallels to dream-like experiences due to their lack of spatial reasoning and continuity.",
      "There is potential for these models to simulate realistic physics in gaming, though concerns about unpredictability and computational efficiency persist.",
      "The technology is currently viewed as a proof of concept, with discussions on its future applications in AI-driven game elements, despite debates on its practicality and necessity compared to traditional methods."
    ],
    "points": 358,
    "commentCount": 170,
    "retryCount": 0,
    "time": 1728811084
  },
  {
    "id": 41822178,
    "title": "PayPal (USA) will automatically share data about you to participating stores",
    "originLink": "https://www.paypal.com/us/legalhub/upcoming-policies-full",
    "originBody": "Menu Back To Home Page Download Printable PDF Policy Updates Policy Updates Last updated on September 23, 2024 Notice of Amendment(s) to the United States PayPal Agreement(s) This Page This page details and/or previews updates to PayPal users of changes to the United States PayPal User Agreement or other online agreements, policies, or statements that require notice. You can also review Past Policy Updates notices. Note that additional changes could be made to previewed agreements on or before the effective dates listed, provided the applicable notice requirements are met. Upcoming Changes We’re making changes to certain agreements (listed below) that govern your relationship with PayPal. These changes will take place automatically on the corresponding effective date(s) shown below. Actions Needed Please carefully review the notices below and familiarize yourself with the upcoming changes. By continuing to use our services after the changes take effect, you agree to be bound by those changes. Otherwise, no further action is needed from you to accept such changes. However, if you would prefer to decline them, then you will need to close your PayPal account prior to the applicable effective date, as described in the user agreement. Notices Issued: September 23, 2024 Amendments to the PayPal Privacy Statement Effective November 27, 2024: We are updating our Privacy Statement to explain how, starting early Summer 2025, we will share information to help improve your shopping experience and make it more personalized for you. The key update to the Privacy Statement explains how we will share information with merchants to personalize your shopping experience and recommend our services to you. Personal information we disclose includes, for example, products, preferences, sizes, and styles we think you’ll like. Information gathered about you after the effective date of our updated Privacy Statement, November 27, 2024, will be shared with participating stores where you shop, unless you live in California, North Dakota, or Vermont. For PayPal customers in California, North Dakota, or Vermont, we’ll only share your information with those merchants if you tell us to do so. No matter where you live, you’ll always be able to exercise your right to opt out of this data sharing by updating your preference settings in your account under “Data and Privacy.” We are also making other updates to our Privacy Statement including some additional disclosures related to your right, depending on the jurisdiction in which you reside, to ask us for a list of the third parties to which we have disclosed personal information, and to provide other clarifying information. Review the updated PayPal Privacy Statement Review the new California, North Dakota, and Vermont Supplemental Financial Privacy Notice Amendments to PayPal's Seller Protection Program Effective November 18, 2024: We are revising the ‘Basic Requirements’ for PayPal’s Seller Protection program to include integration requirements for sellers who have integrated a PayPal Checkout product with their website. We are revising PayPal’s Seller Protection Program to extend eligibility to Guest Checkout transactions received by accounts registered in certain countries. Review the updated Seller Protection Program Amendments to the PayPal Online Payment Services Agreement Effective November 18, 2024: We are changing this agreement to introduce a new service which will enable merchants to accept Automated Clearing House (“ACH”) payments, allowing their customers to use their bank accounts as a payment method. Consequently, we are also renaming the PayPal Online Card Payment Services Agreement, and all references thereto, to the PayPal Online Payment Services Agreement. We are also changing our Merchant fee page to introduce new fees relating to this service. Review the updated PayPal Online Payment Services Agreement Review the updated Merchant Fees page Review the new ACH Services Addendum Discontinuation of the PayPal Fundraisers Program Effective: October 7, 2024: On October 7, 2024, PayPal will disable the ability to create Fundraisers. All current Fundraisers will remain open until they expire. You will need to move any funds from your existing Fundraisers into your PayPal balance no later than January 12, 2025. If you do not do so, we will move any remaining funds into your PayPal balance on January 13, 2025. If you have created a Fundraiser for Charity, please keep a copy of the list of contributors to your Fundraiser for Charity for your records, as this information will no longer be available after January 12, 2025. You will be able to continue to track contributor information for other Fundraisers in your account activity. The PayPal Fundraisers Terms and Conditions will be removed from our Legal Agreements for PayPal Services page on January 13, 2025. If you wish to retain a copy of the relevant terms, please visit our website and download a copy prior to that date. A pdf copy of the PayPal Fundraisers Terms and Conditions and corresponding fees are also available using the links below. PayPal Fundraisers Terms and Conditions Merchant Fees page Consumer Fees page Help Contact Fees Security Center Privacy Center Shop Apps Enterprise Partners About Newsroom Jobs Investor Relations Government Relations © 1999–2024 Accessibility Privacy Statement Cookies Legal PayPal is located at 2211 N 1st St. San Jose, CA 95131",
    "commentLink": "https://news.ycombinator.com/item?id=41822178",
    "commentBody": "PayPal (USA) will automatically share data about you to participating stores (paypal.com)278 points by xyst 22 hours agohidepastfavorite115 comments nurtbo 21 hours agoPrivacy laws actually work! Let’s pass more of them. > Information gathered about you after the effective date of our updated Privacy Statement, November 27, 2024, will be shared with participating stores where you shop, *unless you live in California, North Dakota, or Vermont.* For PayPal customers in California, North Dakota, or Vermont, we’ll only share your information with those merchants if you tell us to do so reply tmpz22 21 hours agoparentIn 1999 the show writers of the West Wing accurately predicated this in an episode about the selection of a Supreme Court Judge: \"It's not just about abortion, it's about the next 20 years. In the '20s and '30s it was the role of government. '50s and '60s it was civil rights. The next two decades are going to be privacy. I'm talking about the Internet. I'm talking about cell phones. I'm talking about health records and who's gay and who's not. And moreover, in a country born on the will to be free, what could be more fundamental than this?\" - Sam Seaborn (Rob Lowe) West Wing (ep: The Short List) 1999 - We've seen massive breaches of EMR systems - We've seen massive breeahes from dating apps (Grindr) outing Gay individuals - None of these entities faced significant consequences for their actions and continue to operate with large amounts of profit. - 2 years after this episode the Patriot Act was passed. We've failed on privacy so far. reply m463 19 hours agorootparentThe government has failed the constituents. Regulatory capture is still the highest ROI investment, and we should work on that. reply potato3732842 18 hours agorootparentIt's a perverse feedback loop. The more power the .gov has to regulate the better the ROI of regulatory capture.' I'm not sure how we get out of this situation without it getting way worse. reply hobobaggins 16 hours agorootparentReduce the power of the govt to regulate things at the federal level and instead move that power to the states. This will return power to the people, and people will naturally move to states that are delivering for them (whatever that is that they're looking for). Across 50 states, this makes it 50 times harder (literally) to practice regulatory capture, and 50 times as likely that they'll be caught out by it, and because news travels at the speed of light today (unlike in the Constitutional era), 50 times as likely that the other state residents will find out about it. Everything becomes fifty times better once we just return to the principles of federalism: \"The powers not delegated to the United States by the Constitution, nor prohibited by it to the States, are reserved to the States respectively, or to the people.\" (Tenth Amendment) Decentralization still works. Push the power back downward to the people who care the most -- you and me. Even though it's one of the rare examples of a federal governing agency that is mostly apolitical and actually functions semi-effectively, the FTC, for example, is a largely pointless, neutered entity that pales in comparison to state powers. For example, the state Attorneys General were able to effectively destroy Big Tobacco. The federal government didn't even come close. reply paulryanrogers 16 hours agorootparentPeople cannot move so easily. Some states have more influence than others. It also makes the decision of 'where' to move more complex. Do I choose a state where my daughters have few rights or one where corporations control everything or where the pollution is so bad my kid's IQ will suffer? reply fragmede 14 hours agorootparentAlso, the one without school shootings please. reply paulryanrogers 5 hours agorootparentStates have open borders, so I'm afraid that can't be solved by local measure alone reply Hikikomori 10 hours agorootparentprevSounds like a free market approach, we know how that usually goes, and we already see how bad it is for women and abortions. reply b3ing 16 hours agorootparentprevYeah and only 1 or 2 states will get around to doing anything about that 5-10 years down the road. reply r-w 16 hours agorootparentprevHuh? Why can't you just regulate the flow of private funds to public servants and leave it at that? Not sure why you seem to be arguing that passing one bill expands the power of government as a whole. reply bee_rider 16 hours agorootparentprevSomeone should come up with a form of government where we just, like, ask the people what they want the government to do and then it does that. reply UncleMeat 4 hours agorootparentThe supreme court has said that partisan gerrymandering is a-okay, making it totally viable for states with 50-50 populations to end up with supermajorities in their state legislatures and house delegations. The federal senate is not allocated according to the population and does not directly reflect the will of the majority. Efforts to limit access to the franchise have been upheld by the courts and key protections from federal legislation like the voting rights act have been undone. Even when the federal government does pass laws or regulations, the courts step in to strike them down through increasingly spurious reasoning. reply paulryanrogers 16 hours agorootparentprevSupreme Court said money is speech and corporations are persons. We'll need to unwind some of the crazy first. reply Loughla 16 hours agorootparentprevI was at breakfast this morning and overheard a conversation at the table next to me. This conversation was about how a recent thunderstorm had small hail accompanying the rain. And then that this small hail was the leftover seeds from \"the jet planes spraying that stuff to control the weather.\" Direct voting on issues terrifies me. reply Yeul 14 hours agorootparentYes the American system is based on the admirable but false idea of the intelligent citizen. It probably worked a lot better when all the voters were wealthy land owners. reply bitnasty 5 hours agorootparentWorked better for who? reply jh00ker 15 hours agorootparentprevI was at a coffee shop a couple weeks ago and I heard two guys going on about how the moon landing never could have happened because it seems impossible. They didn't have any supporting data. They just kept saying things like \"all the footage looked so totally fake.\" My favorite was \"and how did they even get back to Earth? I don't remember ever seeing video of then launching a rocket off the moon, do you?\" SMH reply orbisvicis 14 hours agorootparentThe astronauts drew straws, and Sandy Koufax had the bad fortune to draw the short straw. He would stay behind to roll tape while Neil Armstrong and Buzz Aldrin were to return home. The two astronauts would remain suited for the ascent. Buzz Aldrin would monitor the instrument consoles while Neil Armstrong was to remain tethered to the open hatchway in what was later termed \"a daring attempt\" to recover the film canister thrown by Sandy Koufax. Unfortunately Neil Armstrong was unable to recover the canister. He described it whizzing past his gloved fingers merely inches away during the docking maneuvers with the command module. No one knows what became of the footage. To this day it is likely tumbling around the cold dark expanse of space, perhaps as a new lunar satellite. ... and that's why we don't have footage of the lunar ascent. reply orbisvicis 10 hours agorootparentAs for his heroic efforts, Sandy Koufax was posthumously inducted into the baseball Hall of Fame, and to this day continues to be known for the \"shot that went around the moon\". reply bee_rider 14 hours agorootparentprevEh, I guess, although I’ve also heard some pretty wild things by elected officials. What can we do? reply bloomingeek 13 hours agorootparentI'm a boomer, the best thing is for my generation to die out. We are the most selfish and ignorant generation ever in modern times. And yet we had access to the most information! My kids don't like me to talk like this, my reply is always, \"prove me wrong\". (I'm referring to the mess we have caused the world, not necessarily the mankind helping accomplishments.) reply bee_rider 8 hours agorootparentAh, well don’t be too rough on yourself. My parents are boomers, they are allright, I’m sure you are too. I won’t say your generation didn’t make some mistakes politically. But in any group it is the kind and introspective that feel guilt about the group’s negative actions, while the selfish just go on happily being selfish. You don’t have to make up for the selfish folks who happen to have been born around the same time as you, but if you want to, live a long happy kind life. reply awkwardpotato 17 hours agorootparentprevThere are so many beautiful quote I love from the West Wing... but this one stands out for me because of how (a decade off but) shockingly accurate it is. reply SturgeonsLaw 20 hours agorootparentprevImagine if the real government was as competent and good faith as the West Wing government reply switch007 11 hours agorootparentRight! The West Wing was political porn, and I loved it (and still re-watch it occasionally) reply darknavi 21 hours agoparentprevThis is so open faced and gross. It reminds of someone talking about getting paid minimum wage. If you get paid minimum wage, what your employer is saying is, \"I would pay you less if I was legally allowed to do so.\" It also reminds me of State Farm's (auto/home insurance in the US) website with this link at the bottom: > Do Not Sell or Share My Personal Information (CA residents only) reply heavensteeth 15 hours agorootparent> If you get paid minimum wage, what your employer is saying is, \"I would pay you less if I was legally allowed to do so.\" Doesn't this apply to all pay rates? It's not like high-paying jobs are high-paying for the love in the employer's heart. When does a wage stop being gross? 1c over minimum wage? $1 over? reply toomuchtodo 21 hours agorootparentprevDon’t get mad, get active. Keep cranking on the policy ratchet, progress and success is clearly possible. reply mrkramer 21 hours agorootparentprevThat's what Disqus has at the bottom of every of their comments section[0]. I find it ridiculous. [0] https://electricdusk.com/img/disqus-gdpr-violation-marketing... reply blackeyeblitzar 19 hours agorootparentprevI would like a tax relating to privacy violations to be retroactive in all these other states. It’s actually legal to apply a retroactive tax, so why not? reply seneca 18 hours agorootparentprev> If you get paid minimum wage, what your employer is saying is, \"I would pay you less if I was legally allowed to do so.\" The minimum wage is the government saying \"if you produce less value than this arbitrary cut off, you aren't allowed to work\". reply kelnos 17 hours agorootparentAh, lovely, you're one of those people. If you produce less value than the cutoff (whatever that means; wages are set based on how little a company can get away with paying, not on some arbitrary \"value\" you've assigned to the work), companies that employ you still have to pay you a living wage. Or not even, since minimum wage usually lags a living wage. The funny thing is, I bet you're also the kind of person who is against welfare programs. So if the minimum wage didn't exist, people in these sorts of jobs would get paid so little that they'd end up on welfare. Not sure how that would be an improvement. reply majormajor 16 hours agorootparentprevWages are more a factor of supply and demand and negotiation resulting from that than of value produced. Otherwise we can have a long argument about if NFL players today truly produce 2-3x more \"value\" than 20-30 years ago for playing the same game. (You might say \"value\" itself is coming from supply demand and that yes if more people have demand for NFL tickets or advertising spots during NFL games now then yes, the players are producing more value... but at that point when we acknowledge how interconnected and shape-able it all is, we could say that minimum wage is the government redirecting labor and businesses away from roles and behaviors that aren't even enough to cover the cost of living towards ones that are more valuable. Which would be... good?) reply krapp 18 hours agorootparentprevNo it isn't, because wages aren't set based on some objective measurement of the quality of value produced. If that were the case, increases in productivity would have resulted in a commensurate increase in wages, but the only increase is the gap between wages and productivity. reply LegitShady 18 hours agoparentprevyou know what works better? delete your paypal account and dont use them as a service. I did this years ago and in fact have never missed not having one of these accounts. and since I'm not using paypal they're not sharing info on my to stores when i shop, whether my local laws allow it or not. reply OJFord 7 hours agorootparentThey refuse to delete the account I never intentionally created (their scammy credit card checkout tactics when I had no other option) unless I provide them a bunch of data I've never previously given them in order to 'verify' my identity. reply guywithahat 18 hours agoparentprevThey also raise the bar of entry for companies, reducing competition. I don't use PayPal and won't use Venmo in stores because of this, however I certainly wouldn't be putting trust in more legislation solving this. reply davidlumley 18 hours agorootparentCan you explain what bar has been raised unless you were planning to sell PII as part of your business model? reply levocardia 21 hours agoprevI've gotten the sense that PayPal is circling the drain lately. Whenever I log on (which is rare) I'm bombarded by modals and banners offering payment plans, digital coupons, etc. When I receive money, there's lots of clever UI design that's trying to trick me into transferring it instantly (for a fee) or keeping it in PayPal, as opposed to withdrawing it to my bank account. I suppose this is just one more sign the company is in financial trouble. reply homebrewer 20 hours agoparentLately? It's been that way for decades. I'll add one more to your list: since many things are paid in USD, and most of us live outside the US, your payment goes through a currency conversion. Up until a few years ago it was possible to configure PayPal to let your bank convert using bank's own rates, although it was hidden behind dark UI patterns. Then they removed it and now force you to run conversion at PayPal rates, which are just terrible, much worse than any bank I've ever used. This was about the time I stopped using PayPal completely, so I can't say exactly how much of a price hike this caused, but it's probably around 10%. reply bee_rider 15 hours agorootparentI have thought PayPal was circling the drain for so long that, honestly, it probably says more about me than PayPal. Still though, I don’t get it, as far as I can tell they are just… essentially doing the same thing as credit card companies, but with less regulatory protection for their customers.(?) Like they made sense 20 years ago when banks didn’t know about the internet. But now they are befuddling. reply gruez 16 hours agorootparentprev>Then they removed it and now force you to run conversion at PayPal rates, which are just terrible, much worse than any bank I've ever used. When was this? As of a few months ago it was working fine for me, although it required a bit of finagling to get it to work on new cards I added. reply Novosell 19 hours agorootparentprevI can still defer conversion to my bank. reply tiffanyh 19 hours agoparentprevLooking at their revenue charted tells a different story. One in which they are thriving. https://www.statista.com/statistics/382619/paypal-annual-rev... PayPal ($29B) also does more revenue than Mastercard ($25B). reply yellers 16 hours agorootparentRevenue != Profit reply 404mm 19 hours agoparentprevDespite my bottomless hate for PayPal, I’ve been keeping my account active because there was always some worse option to give your credit card number to. But with Privacy.com, I’m finally ready to let go. reply UberFly 17 hours agorootparentI'm also all in on privacy.com. I hope they stick around because it's such an awesome service. reply koyote 19 hours agorootparentprevSame here, I absolutely despise PayPal but end up using them often because I simply don't trust most retailers with my card information :( (and it's often faster checkout if I don't have my CC details handy) That being said, there's quite a bit of competition these days from BNPL vendors (klarna, afterpay etc.). I don't believe they are any more virtuous though... reply loa_in_ 16 hours agorootparentImagine shopping and not trusting the shop enough to pull out your wallet inside reply Symbiote 9 hours agorootparentBefore chip cards were introduced in Europe (which was 20 years ago so most of my experience was as a child) this could happen. My parents would avoid using a credit card in some restaurants and (less often) at remote petrol stations. reply AlbertCory 17 hours agorootparentprevEvery time I bring this up, someone says \"oh, they'll ruin your credit rating!\" No, actually they don't, unless you do something fraudulent. If you cancel your subscription legitimately and then kill the credit card, you've just made sure there aren't any \"accidental\" charges. reply onesociety2022 15 hours agorootparentMost people want to use privacy.com so you don’t have to cancel the subscription because websites like to put up artificial barriers like having to call a phone number and wait on hold to cancel a subscription. So what happens in that situation where you technically never officially canceled the subscription and so the company continues to bill you and when the payments don’t succeed, they send it to collections? reply AlbertCory 14 hours agorootparentHypothetical. In my experience, they say \"your credit card is failing, please give us another one.\" Because credit cards fail all the time. It's not Red Alert. You go to the page to change your credit card, and THEN you usually can find the \"cancel my account\" link. As long as you're not still using their service, you don't owe anything. They cancel you, and that's that. No collections agency. If someone has an actual event, not a hypothetical, speak up. reply xyst 22 hours agoprevAutomatic data sharing policy to take effect on November 27, 2024. If you do not want PP to share your data with “participating stores”, you need to “opt out”. Per e-mail sent regarding this policy update: “You can opt out of this at any time in your profile settings under \"Data and Privacy.\" reply _huayra_ 21 hours agoparentHere is the precise page, for anyone who wants to easily do it and amortize my settings perusing: https://www.paypal.com/myaccount/privacy/settings/recommenda... reply accrual 17 hours agorootparentThank you, this worked fine for me in the US, \"Sharing is off\". reply RustyRussell 20 hours agorootparentprevDon't see anything relevant in Australia... reply grogenaut 19 hours agorootparentprevthanks, the direct link works but I can't find it by browsing reply thwarted 22 hours agoparentprevJust logging in now, for the first time in years, I found two options that seem relevant. For the first one: it presented that I had given permission to \"paypal shopping\" to share my identity. I removed that permission. This may be the automatically added permission that I had to opt out of. This was in a list of discrete permissions, or apparently so, since there was only one entry, but it could be removed by clicking on a trashcan icon. For the second one, it was explicitly described with words like \"share your info\". This was a checkbox-slider, and it was already set to \"off\". reply xyst 21 hours agorootparentAlso, maybe it defaults to off if you reside in USA and live in one of these states: > Information gathered about you after the effective date of our updated Privacy Statement, November 27, 2024, will be shared with participating stores where you shop, unless you live in California, North Dakota, or Vermont reply johnnyanmac 19 hours agorootparentI'm in California and it was indeed off by default. reply xyst 21 hours agorootparentprevI didn’t revoke the “PayPal shopping” permission. But now I will. Wonder if PP system automatically moves that slider to off if you revoke the “PayPal Shopping” permission. I honestly don’t remember granting that explicitly. Then again, I have only used PP for rare occasions reply TavsiE9s 22 hours agoparentprevAny indication on _which_ setting under \"Data and Privacy\"? reply xyst 21 hours agorootparentOne thing I realized. This appears to be a change in their privacy policy for _US_ PayPal. If you are not in USA (ie, Canada), then it likely won’t be there as it’s not rolling out to you. For me: I opened the app, navigated to Profile > “Data and privacy” > “Personalized shopping” (under “Manage shared info” header) > toggle off “Let us share products, offers, and rewards you might like with participating stores” Haven’t checked on paypal.com reply blackeyeblitzar 19 hours agoparentprevIt’d also ridiculous that I can only delete my data by closing my account. reply LegitShady 18 hours agoparentprevoption 2 - delete your paypal account, you dont need it. reply zaptrem 20 hours agoprev> No matter where you live, you’ll always be able to exercise your right to opt out of this data sharing by updating your preference settings in your account under “Data and Privacy.” There is currently no opt out button or switch on this page on their website when signed in through either my personal or business account. Edit: Someone further down direct linked to it here https://www.paypal.com/myaccount/privacy/settings/recommenda... No idea how they got this screen, as when I click back it gives me the same menu I saw before where this option doesn’t exist and the website design is different. reply kelnos 17 hours agoparentI managed to get there through the main privacy settings screen, but I'm a California resident, so PayPal is legally required to give me that option. Perhaps you live somewhere else? But maybe PayPal hasn't removed access to the setting, just the link to it? reply sourcepluck 19 hours agoprevI have tried to delete my PayPal account on ten different occasions (I mean, something like that), and never succeed to do so. A maze of dark patterns and crappy UI and buttons that go nowhere. I even sent a couple of emails one of the times, and then gave up when there was no response. They're a heinous company. reply dghlsakjg 18 hours agoparentI battled this. Turns out the key was to get the CFPB involved. Within a few weeks I received a letter of explanation from their legal department and my account was deleted. Companies hate getting a letter from regulatory agencies because it frequently has to be dealt with in part by legal, which is an expensive department to clog. Edit: I see you are in Europe. So the above doesn’t directly apply, although there may be a regulatory agency that is equivalent that you can gripe to. reply Symbiote 9 hours agorootparentThe option \"Close Account\" is halfway down the page under \"Settings\". So just a single press to find it after logging in. (EU here.) reply dghlsakjg 1 hour agorootparentThat’s what I tried, but there was an issue with another account, not under my name, sharing my email for some reason. I got stuck in a loop of customer service reps escalating, saying no, having to reopen, and generally not solving the issue. After about a year of receiving someone else’s account emails, and trying to get it corrected, I realized that I didn’t want them anywhere near my money and escalated to the regulators. When I tried to close my account they refused since there was still an account (not mine) associated with my email. Thanks for letting me know about the close account button. I already knew there was a happy path, I was pointing out that PayPal doesn’t have a good customer facing process for issues that can’t be solved normally. reply OJFord 7 hours agorootparentprevBeen there, tried that. You must first provide photo ID, name, address, phone number - all sorts of data not presently on the account - in order to verify identity to delete the account. And then of course they'll retain it all under the guise of financial regulation (legitimately) and probably sell it (scummily). reply latexr 19 hours agoparentprevAnecdotally, I deleted my PayPal account a few weeks ago and found it pretty easy. I’m in Europe, which may have made the difference. reply sourcepluck 18 hours agorootparentI'm in Europe! I haven't tried in ages, a few years I suppose. I'm going to try again, bolstered by your comment. Thank you, kind netizen. Maybe they've been obliged to stop messing around as much with some of the new laws of the past years. reply kelnos 17 hours agoparentprevI see you're in Europe; don't you have a regulatory body you can contact to help you deal with this? I assume account deletion is a requirement covered by the GDPR. reply Symbiote 9 hours agorootparentThe option \"Close Account\" is halfway down the page under \"Settings\". So just a single press to find it after logging in. (EU here.) reply OJFord 7 hours agorootparentprevYeah good luck with that. Threatening that (or just using keywords like GDPR) works great for most companies when you can't find it on the website or mailing list has no unsubscribe link or whatever, but you're forgetting quite how much of a scummy company PayPal is. reply usehackernews 18 hours agoprevI work in Payments. This must be related to their new product - Fastlane. Fastlane is an express checkout product, similar to ShopPay. Even if you have never used a website, you authenticate with OTP and all your information (Address & Payment Methods) is available. Originally, merchants could not use this data to make customer accounts. This was not ideal for us merchants as there was no method to login to track your order information. PayPal came to me this week saying they were updating their legal agreement to allow merchants to create customer accounts. (Express checkout options will soon be everywhere - Stripe, Shopify, PayPal, Zelle/Paze are all competing in this space now) reply kelnos 17 hours agoparent> This was not ideal for us merchants as there was no method to login to track your order information. You don't need accounts for that. Allow customers to check their order status using the order number and some other identifying bit that you are allowed to get/keep, such as last name or billing zip code. Merchants have been doing this since the very start of e-commerce. (If you can't keep anything, then just make the order numbers long random strings, and use that alone, and/or generate a random, unique URL to send in the order confirmation email.) If a merchant creates an account for me without my consent, I delete that account and never buy from them again. Stop abusing your customers' personal information. I'm glad I live in California, and have already opted out to PayPal sharing my information for this, as is my legal right. reply usehackernews 3 hours agorootparentWe do allow customers to check order status based on order id. I was just using an example. The only information paypal is sharing is name and shipping address. We’re aren’t talking significant data here. reply hakfoo 16 hours agorootparentprevOr email them the actual tracking number. I don't need to go to your website to read the same information as usps.com but in a different header. reply dgregd 11 hours agoparentprevHow is that different from Apple Pay or Google Pay, where you click one button and provide all your card details to a new merchant? reply usehackernews 2 hours agorootparentIt’s not different. It’s a direct competitor. These new wallets are all to compete with Apple and Google Pay. reply javiramos 6 hours agoprevDoes anyone know of a browser plugin or similar that could summarize, in real-time as I browse, a given website’s privacy and data use policy. Seems like reading through lengthy and verbose privacy policies could be great application of LLMs. reply kotaKat 3 hours agoprevJoke's on PayPal. I got an email that said \"you can't do business with PayPal anymore\" for no reason, so I guess they've got jack and shit to share from me? reply noname120 21 hours agoprevIs it only in the US? My PayPal account is from Switzerland. I haven't received an email and the option doesn't appear in \"Data and Privacy\" either. reply xyst 21 hours agoparentyes - appears to be only US, for now. edit: updated title to add (USA) reply kevin_thibedeau 21 hours agorootparentUS PayPal is not a bank. Other incarnations are and will be subject to the relevant banking regulations. reply actionfromafar 20 hours agorootparentHa, I forgot about that. The US Freedom is really shiny over at Paypal, a bank free to not follow banking regulations, and no pesky liberal privacy, except in California. reply javiramos 6 hours agoprevDoes anyone know if Venmo has a similar policy? Lately I’ve been trying to use apple pay/cash as much as I can for peer to peer payments but Venmo is still quite popular. reply autoexec 21 hours agoprevPayPal has always been a disaster. There have been so many stories of people having their accounts frozen for long periods of time while whatever passes for \"customer service\" at PayPal jerked them around or outright ignored them that I stopped using PayPal a very long time ago and I've never once regretted it. I see that paypalsucks.com is gone now. Not sure when that happened (archive.org is down too), but really, people have had more than enough warning about what a shit company PayPal is, so anyone continuing to get screwed over by them now is pretty much asking for it. reply OrvalWintermute 17 hours agoprevWill be axing my paypal account if this goes through reply kelnos 17 hours agoparentThat's not an \"if\". They're doing it. I'd suggest you start the deletion process now; I hear it's tricky if you live in a place without decent privacy laws. reply OJFord 7 hours agorootparentI've been trying for years in a place with GDPR. reply wintermutestwin 22 hours agoprevThe whole reason why I use paypal for some purchases is to keep the vendor from knowing my PII. Paypal Enshitification level = 11 reply toomuchtodo 21 hours agoparentIs privacy.com an option for you? Disposable virtual payment card numbers and whatnot. reply dynm 20 hours agorootparentStrongly recommend not using privacy.com. It's full of dark patterns and has lots of crazy behaviors like randomly locking your account and demanding good old picture of you with photo ID. For a privacy focused service, this is hilarious. reply toomuchtodo 18 hours agorootparentThis has not been my experience using it for years. You should expect KYC (know your customer) processes and requirements for any financial service provider with a US nexus or interconnection to US financial infrastructure. If you’re attempting to be completely anonymous with regards to value transfer, crypto and cash are your only potential options. The privacy, in this use case, is from other parties between you and the merchant (and including the merchant, if you’re not providing PI). reply beeflet 13 hours agorootparentprevI couldn't even sign up and create a virtual card. The reason? \"An error has occurred, please contact support or try again later\". Yeah I don't think so, you had one job privacy.com reply tdeck 21 hours agoparentprevDoesn't the vendor get your name and address when paying through PayPal? reply mrkramer 21 hours agorootparentS/he probably meant payment information as credit card number, cvv etc. And there was no way before this for merchant X to know what Alice or Bob bought from merchant Y. reply n_plus_1_acc 21 hours agorootparentprevAnd E-Mail and phone number afair. reply AStonesThrow 21 hours agoparentprevPayPal already shares enough contact info for the purpose of shipping when I order physical goods. This new sharing appears to consist of marketing data on items you shop for, not necessarily PII, unless you consider your inseam to be classified top secret. reply xyst 21 hours agorootparentWhat PayPal lists in the Privacy Policy for the US is only an example. I interpreted the change as “PayPal collects any number of information on you. Gives you seemingly innocuous data such as inseam. But in reality, we may collect political affiliation, contacts, salary information, and share that with retailers that want our data” reply raytopia 19 hours agoprevAre there payment companies that don't sell user's data? reply grahamj 20 hours agoprevGlad I deleted my account some time ago reply sub7 15 hours agoprevThese asshats also have the world's worst OTP page that does not auto submit when you type out the 6 numbers and still doesn't submit on pressing Enter. Everytime I have to move my mouse to the Submit button, I think about how their UX to nickel and dime microfees from you is super slick. reply mrkramer 21 hours agoprev [7 more] [flagged] conradev 21 hours agoparent [–] All of that fancy crypto goes through a few regulated CEX entry points to become USD because people and businesses pay for expenses (notably, taxes) in USD. reply mrkramer 21 hours agorootparentYou can exchange your existing crypto for another crypto in numerous ways and there use to be localbitcoins.com where you could buy crypto with cash and vice versa p2p in person. reply kelnos 17 hours agorootparent\"Used to be\" isn't really helpful. reply mrkramer 6 hours agorootparentThey probably couldn't figure out the legal part behind p2p in person crypto exchange but I didn't follow the crypto scene for the last few years so idk if there are any alternatives to localbitcoins.com The most safe and private way would be to mine crypto coins and then spend them or exchange them for other crypto or digital currencies. Even Satoshi was pro exchange[0]. [0] https://satoshi.nakamotoinstitute.org/posts/bitcointalk/95/ reply cjbgkagh 21 hours agorootparentprev [–] I think it's more the know your customer (KYC) laws that are shutting down the alternatives. reply conradev 11 hours agorootparent [–] Those laws predate crypto, and so from the government's perspective the alternatives were operating illegally: https://www.fincen.gov/news/news-releases/fincen-penalizes-p... https://bitcoinmagazine.com/culture/man-sentenced-illegal-mo... reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "PayPal is updating its agreements and policies, with changes automatically taking effect on specified dates; users must review these updates to continue using the services or close their accounts to decline.",
      "Key updates include a new Privacy Statement allowing information sharing with merchants for personalized shopping, changes to the Seller Protection Program, introduction of ACH payments for merchants, and the discontinuation of the PayPal Fundraisers Program.",
      "Users in California, North Dakota, or Vermont are exempt from automatic information sharing unless they opt-in, and all users can opt-out via account settings."
    ],
    "commentSummary": [
      "PayPal plans to automatically share user data with participating stores from November 27, 2024, unless users choose to opt out, with exceptions for residents of California, North Dakota, and Vermont who must opt in.",
      "Users have the option to manage their data sharing preferences through their account settings, highlighting the importance of user awareness and control over personal data.",
      "The announcement has sparked privacy concerns and discussions about regulatory capture, leading some users to contemplate deleting their PayPal accounts."
    ],
    "points": 278,
    "commentCount": 115,
    "retryCount": 0,
    "time": 1728764372
  },
  {
    "id": 41827043,
    "title": "Large language models reduce public knowledge sharing on online Q&A platforms",
    "originLink": "https://academic.oup.com/pnasnexus/article/3/9/pgae400/7754871",
    "originBody": "Skip to Main Content Advertisement Journals Books Search Menu Menu Sign in through your institution Navbar Search Filter PNAS Nexus This issue Computer Sciences Economic Sciences NAS Journals Medicine and Health Science and Mathematics Social Sciences Books Journals Oxford Academic Mobile Enter search term Search Issues Advance articles Subject Biological, Health, and Medical Sciences Browse content in Biological, Health, and Medical Sciences Administration Of Health Services, Education, and Research Agricultural Sciences Allied Health Professions Anesthesiology Anthropology Anthropology (Biological, Health, and Medical Sciences) Applied Biological Sciences Biochemistry Biophysics and Computational Biology (Biological, Health, and Medical Sciences) Biostatistics Cell Biology Dermatology Developmental Biology Ecology Emergency Medicine Environmental Sciences (Biological, Health, and Medical Sciences) Evolution Genetics Immunology and Inflammation Internal Medicine Medical Microbiology Medical Sciences Microbiology Neurology Neuroscience Nutrition Obstetrics and Gynecology Ophthalmology Pathology Pediatrics Pharmacology Pharmacy Physical Medicine Physiology Plant Biology Population Biology Psychiatry Psychological and Cognitive Sciences (Biological, Health, and Medical Sciences) Public Health and Epidemiology Radiation Oncology Radiology Rehabilitation Surgery Sustainability Science (Biological, Health, and Medical Sciences) Systems Biology Veterinary Medicine Physical Sciences and Engineering Browse content in Physical Sciences and Engineering Aerospace Engineering Applied Mathematics Applied Physical Sciences Astronomy Bioengineering Biophysics and Computational Biology (Physical Sciences and Engineering) Chemical Engineering Chemistry Civil and Environmental Engineering Computer Sciences Computer Science and Engineering Earth Resources Engineering Earth, Atmospheric, and Planetary Sciences Electric Power and Energy Systems Engineering Electronics, Communications and Information Systems Engineering Engineering Environmental Sciences (Physical Sciences and Engineering) Industrial, Manufacturing and Operational Systems Engineering Materials Engineering Mathematics Mechanical Engineering Physics Statistics Sustainability Science (Physical Sciences and Engineering) Social and Political Sciences Browse content in Social and Political Sciences Anthropology (Social and Political Sciences) Demography Economic Sciences Environmental Sciences (Social and Political Sciences) Political Sciences Psychological and Cognitive Sciences (Social and Political Sciences) Social Sciences Sustainability Science (Social and Political Sciences) Submit Author guidelines Submission site Open access policy Self-archiving policy Why submit to PNAS Nexus The PNAS portfolio For reviewers Alerts About About PNAS Nexus About National Academy of Sciences Editorial Board Journals on Oxford Academic Books on Oxford Academic NAS Journals Issues Advance articles Subject All Subject Expand Expand Biological, Health, and Medical Sciences Administration Of Health Services, Education, and Research Agricultural Sciences Allied Health Professions Anesthesiology Anthropology Anthropology (Biological, Health, and Medical Sciences) Applied Biological Sciences Biochemistry Biophysics and Computational Biology (Biological, Health, and Medical Sciences) Biostatistics Cell Biology Dermatology Developmental Biology Ecology Emergency Medicine Environmental Sciences (Biological, Health, and Medical Sciences) Evolution Genetics Immunology and Inflammation Internal Medicine Medical Microbiology Medical Sciences Microbiology Neurology Neuroscience Nutrition Obstetrics and Gynecology Ophthalmology Pathology Pediatrics Pharmacology Pharmacy Physical Medicine Physiology Plant Biology Population Biology Psychiatry Psychological and Cognitive Sciences (Biological, Health, and Medical Sciences) Public Health and Epidemiology Radiation Oncology Radiology Rehabilitation Surgery Sustainability Science (Biological, Health, and Medical Sciences) Systems Biology Veterinary Medicine Physical Sciences and Engineering Aerospace Engineering Applied Mathematics Applied Physical Sciences Astronomy Bioengineering Biophysics and Computational Biology (Physical Sciences and Engineering) Chemical Engineering Chemistry Civil and Environmental Engineering Computer Sciences Computer Science and Engineering Earth Resources Engineering Earth, Atmospheric, and Planetary Sciences Electric Power and Energy Systems Engineering Electronics, Communications and Information Systems Engineering Engineering Environmental Sciences (Physical Sciences and Engineering) Industrial, Manufacturing and Operational Systems Engineering Materials Engineering Mathematics Mechanical Engineering Physics Statistics Sustainability Science (Physical Sciences and Engineering) Social and Political Sciences Anthropology (Social and Political Sciences) Demography Economic Sciences Environmental Sciences (Social and Political Sciences) Political Sciences Psychological and Cognitive Sciences (Social and Political Sciences) Social Sciences Sustainability Science (Social and Political Sciences) Browse all content Browse content in Submit Author guidelines Submission site Open access policy Self-archiving policy Why submit to PNAS Nexus The PNAS portfolio For reviewers Alerts About About PNAS Nexus About National Academy of Sciences Editorial Board Close Navbar Search Filter PNAS Nexus This issue Computer Sciences Economic Sciences NAS Journals Medicine and Health Science and Mathematics Social Sciences Books Journals Oxford Academic Enter search term Search Advanced Search Search Menu Article Navigation Close mobile search navigation Article Navigation Volume 3 Issue 9 September 2024 Article Contents Abstract Introduction Data and methods Results Discussion Materials Notes Acknowledgments Supplementary Material Funding Author Contributions Preprints Data Availability ReferencesArticle Navigation Article Navigation Journal Article Large language models reduce public knowledge sharing on online Q&A platforms R Maria del Rio-Chanona, R Maria del Rio-Chanona Department of Computer Science, University College London , London WC1E 6EA , United Kingdom Bennett Institute for Public Policy, University of Cambridge , Cambridge CB3 9DT , United Kingdom Complexity Science Hub Vienna , Vienna 1080 , Austria https://orcid.org/0000-0002-0189-7919 Search for other works by this author on: Oxford Academic Google Scholar Nadzeya Laurentsyeva, Nadzeya Laurentsyeva Faculty of Economics, LMU Munich , Munich 80539 , Germany Sanofi Hungary , Budapest 1138 , Hungary https://orcid.org/0009-0003-3790-0889 Search for other works by this author on: Oxford Academic Google Scholar Johannes Wachs Johannes Wachs Department of Network Science, Corvinus University of Budapest , Budapest 1093 , Hungary Institute of Economics, HUN-REN Centre for Economic and Regional Studies , Budapest 1097 , Hungary Complexity Science Hub Vienna , Vienna 1080 , Austria To whom correspondence should be addressed: Email: johannes.wachs@uni-corvinus.hu https://orcid.org/0000-0002-9044-2018 Search for other works by this author on: Oxford Academic Google Scholar Competing Interest: The authors declare no competing interests. Author Notes PNAS Nexus, Volume 3, Issue 9, September 2024, pgae400, https://doi.org/10.1093/pnasnexus/pgae400 Published: 11 September 2024 Article history Received: 16 April 2024 Accepted: 27 August 2024 Published: 11 September 2024 Corrected and typeset: 24 September 2024 PDF Split View Views Article contents Figures & tables Video Audio Supplementary Data Cite Cite R Maria del Rio-Chanona, Nadzeya Laurentsyeva, Johannes Wachs, Large language models reduce public knowledge sharing on online Q&A platforms, PNAS Nexus, Volume 3, Issue 9, September 2024, pgae400, https://doi.org/10.1093/pnasnexus/pgae400 Select Format Select format .ris (Mendeley, Papers, Zotero) .enw (EndNote) .bibtex (BibTex) .txt (Medlars, RefWorks) Download citation Close Permissions Icon Permissions Share Icon Share Facebook Twitter LinkedIn Email Navbar Search Filter PNAS Nexus This issue Computer Sciences Economic Sciences NAS Journals Medicine and Health Science and Mathematics Social Sciences Books Journals Oxford Academic Mobile Enter search term Search Close Navbar Search Filter PNAS Nexus This issue Computer Sciences Economic Sciences NAS Journals Medicine and Health Science and Mathematics Social Sciences Books Journals Oxford Academic Enter search term Search Advanced Search Search Menu Abstract Large language models (LLMs) are a potential substitute for human-generated data and knowledge resources. This substitution, however, can present a significant problem for the training data needed to develop future models if it leads to a reduction of human-generated content. In this work, we document a reduction in activity on Stack Overflow coinciding with the release of ChatGPT, a popular LLM. To test whether this reduction in activity is specific to the introduction of this LLM, we use counterfactuals involving similar human-generated knowledge resources that should not be affected by the introduction of ChatGPT to such extent. Within 6 months of ChatGPT’s release, activity on Stack Overflow decreased by 25% relative to its Russian and Chinese counterparts, where access to ChatGPT is limited, and to similar forums for mathematics, where ChatGPT is less capable. We interpret this estimate as a lower bound of the true impact of ChatGPT on Stack Overflow. The decline is larger for posts related to the most widely used programming languages. We find no significant change in post quality, measured by peer feedback, and observe similar decreases in content creation by more and less experienced users alike. Thus, LLMs are not only displacing duplicate, low-quality, or beginner-level content. Our findings suggest that the rapid adoption of LLMs reduces the production of public data needed to train them, with significant consequences. AI, web, online public goods, ChatGPT Significance Statement This study examines the impact of ChatGPT, a large language model, on online communities that contribute to public knowledge shared on the Internet. We found that ChatGPT has led to a 25% drop in activity on Stack Overflow, a key reference website where programmers share knowledge and solve problems. This substitution threatens the future of the open web, as interactions with AI models are not added to the shared pool of online knowledge. Moreover, this phenomenon could weaken the quality of training data for future models, as machine-generated content likely cannot fully replace human creativity and insight. This shift could have significant consequences for both the public Internet and the future of AI. Introduction Over the last 30 years, humans have constructed a vast and open library of information on the web. Using powerful search engines, anyone with an internet connection can access valuable information from online knowledge repositories like Wikipedia, Stack Overflow, and Reddit. New content and discussions posted online are quickly integrated into this ever-growing ecosystem, becoming digital public goods used by people all around the world to learn new technologies and solve their problems (1–4). These public goods are essential for training AI systems, in particular, large language models (LLMs) (5). For example, the LLM in ChatGPT (6) is trained to recognize patterns, facts, and information from vast repositories of online public text by predicting the next words in sequences. It answers users’ questions by generating responses that not only integrate and contextualize this information but also infer underlying meanings and connections. The remarkable effectiveness of ChatGPT is reflected in its quick adoption (7) and application across diverse fields, including auditing (8), astronomy (9), medicine (10), and chemistry (11). Randomized control trials show that using LLMs significantly boosts productivity and quality in computer programming, professional writing, customer support tasks, consulting, and writing job applications (12–16). Indeed, the widely reported successes of LLMs, like ChatGPT, suggest that we will observe a significant change in how people search for, create and share information online. Ironically, if LLMs like ChatGPT, substitute for traditional methods of searching and interrogating the web, they could displace the very human behavior that generated their original training data. As people begin to use ChatGPT or similar LLMs instead of online knowledge repositories to find information, traffic and contributions to these repositories will likely decrease, diminishing the quantity and quality of these digital public goods. Previous work refers to this sort of displacement as the “paradox of re-use”: for example, the information on platforms like Wikipedia powers Google search (via information boxes and summaries) while reducing the need to visit Wikipedia (17, 18). While such a shift could have significant social and economic implications, we have little evidence on whether people are indeed reducing their consumption and creation of valuable digital public goods as LLMs’ popularity grows. The aim of this article is to evaluate the impact of LLMs on the generation of open data on popular question-and-answer (Q&A) platforms. We focus on the effects of the most widely adopted LLM as of now—ChatGPT. Because ChatGPT performs relatively well on software programming tasks (15), we study Stack Overflow, the largest online Q&A platform for software development and programming. Preliminary studies have shown that ChatGPT’s quality is competitive with answers from Stack Overflow in specific fields (19, 20). We present three results. First, we examine whether the release of ChatGPT has decreased the volume of posts, i.e. questions and answers, published on the platform. We estimate the causal effect of ChatGPT’s release on Stack Overflow activity using a difference-in-differences model. We compare the weekly posting activity on Stack Overflow against that of four comparable Q&A platforms. These counterfactual platforms are less likely to be affected by ChatGPT either because their users experience difficulties with accessing ChatGPT or because ChatGPT performs poorly in questions discussed on those platforms. We find that posting activity on Stack Overflow decreased by about 25 % relative to the counterfactual platforms 6 months after the release of ChatGPT. We estimate the average effect across the 6 months to be 15 % ⁠, reflecting a lagged kick-in and gradual adoption of ChatGPT. We interpret the 25 % figure as a lower bound of the total impact of ChatGPT on Stack Overflow, as LLMs likely had some impact on even the counterfactual platforms. Additional evidence from the 2023 Stack Overflow Developer Survey supports the hypothesis that ChatGPT users are less likely to post on Stack Overflow and to visit the platform regularly. Second, we investigate whether ChatGPT is simply displacing lower-quality posts on Stack Overflow. To do so, we use data on up- and downvotes, simple forms of social feedback provided by other users to rate posts. We observe little change in the votes posts received on Stack Overflow since the release of ChatGPT. In addition, we find significant declines in posting by users of all experience levels, from novice to expert. These results suggest that ChatGPT is displacing various Stack Overflow posts, including high-quality content. Third, we study the heterogeneity of ChatGPT’s impact across different programming languages discussed on Stack Overflow. We test for these heterogeneities using an event study design. We observe that posting activity in some languages, like Python and Javascript, has decreased significantly more than the platform’s average. Using data on programming language popularity on GitHub, we find that the most widely used languages (and, hence, languages with richer data for training ChatGPT) tend to have larger relative declines in posting activity. Our analysis points to several significant implications for the sustainability of the current AI ecosystem. The first is that the decreased production of open data will limit the training of future models (21). LLM-generated content itself is likely an ineffective substitute for training data generated by humans for the purpose of training new models (22–24). One analogy is that training an LLM on LLM-generated content is like making a photocopy of a photocopy, providing successively less satisfying results (25). While human feedback to LLMs may facilitate continued learning, data generated by interactions with privately owned LLMs belong to the owners of these LLMs. This leads to the second issue: the initial advantage of the first mover, in this case OpenAI with its ChatGPT, compounds if the LLM effectively learns from interactions with users while crowding out the generation of new open data that competitors could use to improve their models. While it is well-known that increasing returns to users and data in the digital sector can lead to winner-take-all dynamics and technological lock-in (26, 27), the transformation of the online commons into a private database presents a novel risk to consumer welfare. More broadly, a shift from open data to a more closed web will likely have significant second-order impacts on the ever-growing digital economy (28) and how we access, share, and evaluate information. These potential consequences have been overlooked in previous risk taxonomies of LLMs (29). The rest of the article is organized as follows. We introduce our empirical set-up, including the data and models used in our analysis, in Data and methods section. Results section presents our results. In Discussion section, we discuss their implications. We argue that our findings of a significant decline in activity on Stack Overflow following the release of ChatGPT have important implications for the training of future models, competition in the AI sector, the provision of digital public goods, and how humans seek and share information. Data and methods Stack exchange and Segmentfault data To measure the effect ChatGPT can have on digital public goods, we compare the change in Stack Overflow’s activity with the activity on a set of similar platforms. These platforms are similar to Stack Overflow in that they are technical Q&A platforms but are less prone to substitution by ChatGPT given their focus or target group. Specifically, we study the Stack Exchange platforms: Mathematics and Math Overflow and the Russian-language version of Stack Overflow. We also examine a Chinese-language Q&A platform on computer programming called Segmentfault. Mathematics and Math Overflow focus on university- and research-level mathematics questions, respectively. We consider these sites to be less susceptible to replacement by ChatGPT given that, during our study’s period of observation, the free-tier version of ChatGPT performed poorly (0–20th percentile) on advanced high-school mathematics exams (6) and was therefore unlikely to serve as a suitable alternative to these platforms. The Russian Stack Overflow and the Chinese Segmentfault have similar scope as Stack Overflow, but target users located in Russia and China, respectively. We consider these platforms to be less affected by ChatGPT given that ChatGPT is officially unavailable in the Russian Federation, Belarus, Russian-occupied Ukrainian territory, and the People’s Republic of China. Although people in these places can and do access ChatGPT via VPNs, such barriers still represent a hurdle to widespread fast adoption (30). We extract all posts (questions and answers) on Stack Overflow, Mathematics, Math Overflow, and Russian Stack Overflow from their launch to early June 2023 using https://archive.org/details/stackexchange. We scraped the data from Segmentfault directly. Our initial dataset comprises 58 million posts on Stack Overflow, over 900 thousand posts for the Russian-language version of Stack Overflow, 3.5 million posts on Mathematics Stack Exchange, 300 thousand posts for Math Overflow, and about 300 thousand for Segmentfault. We focus our analysis on data from January 2022 to the end of May 2023, noting that our findings are robust to alternative time windows. For each post in the Stack Exchange sites, we additionally extract the post’s type (question or answer), the number of votes (up—positive feedback, or down—negative feedback) the post received, and the tags assigned to the post, where tags are predefined labels that summarize the content of the post, for instance, an associated programming language. In addition, we also extract the experience of the post’s author (i.e. number of previous posts). Using this information, we classify posts into those from “New”, “Inexperienced”, “Experienced”, and “Expert” users depending on whether the author had 0, 1–10, 11–100, or more than 100 posts, respectively at the time the post was published.a For more details on the data from Q&A platforms we used, we refer the reader to section. Finally, we also investigated data from the 2023 Stack Overflow Developer Survey, conducted in mid-May 2023. It includes 89,184 responses from software developers living in 185 countries. We focus on user responses to the prompt “Which AI-powered tools did you use regularly over the past year?”, for which ChatGPT was an option to tick. We use this information to provide further suggestive evidence for the relationship between the adoption of ChatGPT and Stack Overflow activity at the individual programmer’s level while controlling for a rich set of characteristics, such as professional status, education, experience, and preferred programming language. Models Difference-in-differences We estimate the effect of ChatGPT for posting activity on Stack Overflow using a difference-in-differences method with four counterfactual platforms. We aggregate posting data at platform- and week-level and fit a regression model using ordinary least squares (OLS):b Log ( Posts 𝑝 , 𝑡 ) = 𝛼 𝑝 + 𝜆 𝑡 + 𝛽 × Treated 𝑝 , 𝑡 + 𝜖 𝑝 , 𝑡 , (1) where Posts 𝑝 , 𝑡 is the number of posts on platform p in a week t, which we log-transform. 𝛼 𝑝 are platform fixed effects, 𝜆 𝑡 are time (week) fixed effects, and 𝜖 𝑝 , 𝑡 is the error term. The coefficient of interest is β, which captures the estimated effect of ChatGPT on posting activity on Stack Overflow relative to the less affected platforms: Treated equals one for weeks after the release of ChatGPT (starting with the week of 2022 November 27) when the platform p is Stack Overflow and zero otherwise. We report standard errors clustered at the monthly level to account for month-specific shocks common to all platforms. We note that β defines an estimate of the effect of ChatGPT on Stack Overflow relative to the counterfactuals averaged across the entire 6-month post-treatment period of our data. We focus our difference-in-differences estimations on the period between January 2022 and May 2023, covering 48 weeks before the release of ChatGPT and 25 weeks after it.c However, to show that our results are not specific to the selected time window, we also repeat the estimations using a wider time period starting from January 2019. The validity of the difference-in-differences approach relies on the assumption of parallel trends. While Fig. 1b, illustrates that posting activity on Stack Overflow and the counterfactual platforms had developed in a similar way prior to the ChatGPT shock, we conduct several formal checks. First, we add platform-specific time trends that represent an interaction between a linear time trend and the average change in the number of posts on a platform between 2018 and pre-GPT. This allows us to check if the results are robust to the inclusion of differential time trends (32). Second, we estimate a generalized difference-in-differences model. Specifically, we employ a similar specification, but instead of 𝛽 × Treated 𝑝 , 𝑡 ⁠, we use ∑ 𝑡 𝛽 𝑡 × 𝐼 ( week = 𝑡 ) × 𝐼 ( platform = StackOverflow ) ⁠. We standardize the effects to 0 in the week before the public release of ChatGPT by dropping the indicator for that week from the regression. This model allows us to examine possible pretrends in our data. By estimating separate coefficients for the weeks before the release, we can check if posts on Stack Overflow had evolved similarly to the activity on counterfactual platforms prior to the release of ChatGPT. This specification also allows us to investigate the dynamics of the ChatGPT effect over time. Separate coefficients for 25 weeks following the release of ChatGPT show how the effects of ChatGPT realized over time as more users adopted the technology. Fig. 1. a) Time series of weekly posts to Stack Overflow since early 2016. The number of weekly posts decreases at a rate of about 7,000 posts each year from 2016 to 2022. In the 6 months after the release of ChatGPT, the weekly posting rate decreases by around 20,000 posts. b) Comparing posts to Stack Overflow, its Russian- and Chinese-language counterparts, and mathematics Q&A platforms since early 2022. Post counts are standardized by the average and standard deviation of post counts within each platform prior to the release of ChatGPT. Posting activity on Stack Overflow falls significantly more relative to activity on other platforms. Open in new tabDownload slide The advantage of the difference-in-differences method compared to a simple event study with Stack Overflow data only is that we estimate ChatGPT effects net of possible weekly shocks that are common across the technical Q&A platforms. For the interpretation of the coefficient, we note that we estimate relative change in posting activity on Stack Overflow compared to activity on other platforms before vs. after the release of ChatGPT. To the extent that ChatGPT also affected activity on the counterfactual platforms, our estimates will be downward biased in the magnitude of the effect. To investigate whether the decrease in posting was driven mainly by a decrease in the number of posts authored by new or inexperienced users, we run the same regression as in Eq. 1 separately for weekly posts made by users with different levels of prior experience. We assign each post the number of previous posts the user had made and differentiate between four groups of posts: posts by “new” users who have not posted before, posts by “inexperienced” users who posted between 1 and 10 times, posts by “experienced” users with between 11 and 100 prior posts, and posts by “expert” users who posted more than 100 times previously. Event study When analyzing the effect of ChatGPT on activity across programming languages, we can no longer compare data from Stack Overflow with the counterfactual platforms. This is because the tags annotating posts are different between Stack Exchange platforms. Therefore, we study ChatGPT’s heterogeneous effects using an event-study specification. For each programming language i (identified by a tag), we model the standardized number of posts in a week t on Stack Overflow by fitting a simple linear time trend with seasonal effects: Posts ¯ 𝑖 , 𝑡 = 𝛽 0 + 𝛽 1 𝑡 + 𝛽 2 ChatGPT + 𝛽 3 ( 𝑡 × ChatGPT ) + 𝜂 + 𝜖 𝑖 , 𝑡 , (2) where Posts 𝑖 , 𝑡 ¯ stands for the standardized number of posts associated with a programming language i in a week t. We standardize the dependent variable in order to be better able to compare effects across programming languages with different numbers of posts.d 𝛽 1 ( 𝑡 ) captures the linear time trend and η are seasonal (month of year) fixed effects. ChatGPT equals one if the week t is after the release of ChatGPT and zero otherwise. Coefficient 𝛽 2 captures the change in the intercept, while coefficient 𝛽 3 reflects the change in the slope of the time trend following the release of ChatGPT. We report HAC standard errors. Additional regression analysis with the Stack Overflow 2023 survey We run an additional model using Stack Overflow survey data to corroborate our findings. We compute the association between self-reported individual activity on Stack Overflow and the adoption of ChatGPT by estimating the following logistic regression: log ( Activity 𝑑 , 𝑖 ( 1 − Activity 𝑑 , 𝑖 ) ) = 𝛽 0 + 𝛽 1 ( Use ChatGPT 𝑑 ) + 𝛽 2 ( 𝑋 𝑑 ) + Age 𝑑 + 𝐶 𝑑 + 𝐼 𝑑 + Lang 𝑖 + Type 𝑑 + 𝜖 𝑑 , 𝑖 , (3) where d stands for the developer and i denotes a programming language used by the developer. Activity 𝑑 , 𝑖 corresponds to the probability of being a frequent Stack Overflow visitor/contributor. 𝑋 𝑑 comprises a set of controls at the developer’s level: a dummy of whether the developer is a professional software engineer, education level, employment status, working mode (remote, hybrid, or in-person), and years of coding. We also add age (Age), country (C), industry (I), programming language (Lang), and developer type (Type) (e.g. researcher, front-end, back-end, full-stack, QA, etc.) fixed effects. Because most developers report using more than one programming language, we expand the dataset to the developer × language level. We apply weights (1/number of languages) to avoid double counts.e We cluster standard errors at the programming language level to allow for common shocks. While the cross-sectional nature of the survey data does not allow us to interpret the results as causal, we try to reduce the endogeneity by controlling for a rich set of the above individual characteristics that are likely to influence both the adoption of ChatGPT and Stack Overflow contributions. In this way, we are comparing how the contributions to Stack Overflow vary between ChatGPT adopters and nonadopters, who are otherwise very similar to each other. Results Decrease in posting activity Figure 1a shows the evolution of activity on Stack Overflow from January 2016 to June 2023. Up to 2022 there was a gradual decrease in activity from roughly 110,000 to 60,000 posts per week, that is roughly 7,000k posts less per week each year. However, after the release of ChatGPT (2022 November 30) posting activity decreased sharply, with the weekly average falling from around 60,000 posts to 40,000 within 6 months. Compared to the pre-ChatGPT trend, this decrease represents more than 5 years worth of deceleration in just half a year. The decrease in activity on Stack Overflow is larger than for similar platforms for which we expect ChatGPT to be a less viable substitute. Figure 1b shows the standardized posting activity on Stack Overflow, the Russian- and Chinese-language counterparts of Stack Overflow, and two mathematics Q&A platforms. We standardize posting activity by the average and standard deviation of post counts within each platform prior to the release of ChatGPT. Figure 1b highlights that Stack Overflow activity deviates markedly from activity on the other platforms after the release of ChatGPT. The plot visualizes the standardized posting activity within each platform since early 2022. Smoothed weekly activity varies between plus and minus two standard deviations for all platforms for most of 2022. Events, such as the Chinese New Year and other holidays and the start of the Russian invasion of Ukraine, are visible. Following the release of ChatGPT, we observe a significant and persistent decline in activity on Stack Overflow. Our difference-in-differences model reveals that Stack Overflow activity significantly declined after the release of ChatGPT, and that this effect became more pronounced over time. Table 1 reports our estimates, the first column indicates that ChatGPT decreased posting activity on Stack Overflow by 15% (⁠ 1 − 𝑒 − 0.163 ⁠). Note that this is a measure of the average effect across the 6 months of post-ChatGPT data we consider. If ChatGPT adoption is gradual, we expect that the effect observed at the end of the data will be larger than at the beginning. Table 1. Open in new tab Results of a difference-in-differences model, estimating the change in activity observed weekly on stack overflow following the release of ChatGPT, relative to activity on four other platforms less likely to have been impacted.(1) (2) (3)Number of posts Number of questions Weekday posts VariablesStack Overflow × Post-GPT− 0.163 * − 0.105 + − 0.151 *(0.0584) (0.0597) (0.0613) Observations 370 370 370 𝑅 2 -within 0.0458 0.0189 0.0294(1) (2) (3)Number of posts Number of questions Weekday posts VariablesStack Overflow × Post-GPT− 0.163 * − 0.105 + − 0.151 *(0.0584) (0.0597) (0.0613) Observations 370 370 370 𝑅 2 -within 0.0458 0.0189 0.0294 All regressions comprise platform fixed effects and week fixed effects. The standard error of the estimate clustered on month is reported in parentheses. 𝑅 2 (within) is derived after differencing out week and platform fixed effects. Significance codes: ***: 𝑃 < 0.001 ⁠, **: 𝑃 < 0.01 ⁠, *: 𝑃 < 0.05 ⁠, + ⁠: 𝑃 < 0.1 ⁠. Table 1. Open in new tab Results of a difference-in-differences model, estimating the change in activity observed weekly on stack overflow following the release of ChatGPT, relative to activity on four other platforms less likely to have been impacted.(1) (2) (3)Number of posts Number of questions Weekday posts VariablesStack Overflow × Post-GPT− 0.163 * − 0.105 + − 0.151 *(0.0584) (0.0597) (0.0613) Observations 370 370 370 𝑅 2 -within 0.0458 0.0189 0.0294(1) (2) (3)Number of posts Number of questions Weekday posts VariablesStack Overflow × Post-GPT− 0.163 * − 0.105 + − 0.151 *(0.0584) (0.0597) (0.0613) Observations 370 370 370 𝑅 2 -within 0.0458 0.0189 0.0294 All regressions comprise platform fixed effects and week fixed effects. The standard error of the estimate clustered on month is reported in parentheses. 𝑅 2 (within) is derived after differencing out week and platform fixed effects. Significance codes: ***: 𝑃 < 0.001 ⁠, **: 𝑃 < 0.01 ⁠, *: 𝑃 < 0.05 ⁠, + ⁠: 𝑃 < 0.1 ⁠. Indeed, our second specification observes exactly this trend. We visualize the weekly estimates of the relative change in the Stack Overflow activity in Fig. 2. This figure shows the impact of ChatGPT is increasing over time and is greater in magnitude than the average post-ChatGPT effect estimated in Table 1 by the end of our study period. By the end of April 2023, coinciding with a peak in traffic to ChatGPT, f the estimated decrease in activity stabilizes at around 25%. Fig. 2. Difference-in-differences analysis for posting activities. The dashed line marks the week of 2022 November 27—the release week of ChatGPT. Eight weeks after its introduction, we observe a steady decline in the activity of Stack Overflow. The plotted coefficients correspond to the interaction between a weekly dummy and posting on Stack Overflow. We normalize the effects to 0 in the week before the public release of ChatGPT by dropping the indicator for that week from the regression. The reported CIs are at 95%. The regression comprises platform fixed effects and week fixed effects. Open in new tabDownload slide We also tested for heterogeneity in subsets of the data, considering only questions (rather than counting both questions and answers) and posts on weekdays. In both subsets, our estimates did not deviate significantly from the main result: we estimate a 10% relative decrease in questions and 14% relative decrease in posts on weekdays (see the second and third column of Table 1). Our results are robust to using alternative transformations of the outcome (Tables S1 and S2), adding platform-specific trends (Table S3), and extending the time window of the analysis (Table S4). The decrease in Stack Overflow activity is consistent with the individual-level evidence from the 2023 Stack Overflow Developer Survey. In particular, we estimate the relationship between self-reported ChatGPT usage and Stack Overflow visit and contribution frequency using specification Eq. 3. We consider several binary variables as the outcomes: Contribute to Stack Overflow ever (weekly or more) is equal to one if a developer has contributed to Stack Overflow at least once (weekly or more often) and zero otherwise, Visit Stack Overflow daily is equal to one if a developer reports visiting the platform once or more times per day. As participants were recruited through Stack Overflow and related platforms, they represent a selected group of engaged Stack Overflow users, and, therefore, their levels of activity are high: about 75% of all respondents have contributed to the platform at least once, and 42% visit it daily. We report the results in Table S7. The coefficients represent changes in log odds of the outcomes, and we also compute the average marginal effects of ChatGPT adoption on the likelihood of frequent contributions/visits. We find that ChatGPT adopters are less likely to contribute to Stack Overflow and to visit the platform frequently compared to nonadopters of the same age, experience, education, employment status, working mode, industry, and programming language used. Moreover, even when we limit the sample to the most active respondents (i.e. those who have contributed at least once to Stack Overflow), we can still detect statistically significant differences in the probability of both contributing weekly and visiting daily between otherwise similar ChatGPT adopters and nonadopters. The magnitude of the effect is not very high. For instance, the average marginal effect of ChatGPT on the likelihood of contributing to Stack Overflow weekly or more is about 0.8 percentage points (or 2.7% lower probability). However, these results are likely to be downward biased because of the selection into survey participation: those who use Stack Overflow less frequently (including those who have reduced their activity because of ChatGPT) were less likely to respond. Post and user heterogeneities A decrease in overall activity on Stack Overflow is not an issue if it is rather the less interesting questions that are outsourced to ChatGPT. We use a post’s score (i.e. difference between upvotes and downvotes) observed 5 weeks after its creation as a proxy of its value—good (bad) posts have a positive (negative) score, while neutral questions have a score of zero. If ChatGPT is displacing bad questions, we would expect that after its release there would be a downward trend in the share of bad questions. However, as Fig. 3a shows, while there was a slight uptick in the fraction of good questions, these were mostly replacing neutral questions and the trend of bad questions was flat. The short-lived increase in the fraction of good questions may be a result of ChatGPT inducing interest in novel topics, such as large language models, which usually results in good questions (see our Discussion section below on the increase in interest in CUDA). With respect to answers, there was no change in the trends of good, bad, and neutral answers. In general, there is a remarkable stability in the proportions post-ChatGPT. We confirm these results by estimating a difference-in-differences specification where the outcome is the number of up(down) votes that posts published in a given week receive over the first 5 weeks, normalized to the total number of posts from this week (Table S5 and Fig. S1). Unlike our previous results on posts, we do not detect any effect of ChatGPT. Fig. 3. a) The weekly time series of the fraction of neutral, good, and bad questions and answers. Good (bad) post are those with a positive (negative) score after 5 weeks of its creation, while neutral questions have a zero score. The horizontal axis indicates the week of the post and the dashed line the week of the release of ChatGPT. We observe no major change in trends in bad posts since the release of ChatGPT. b) Weekly counts of questions and answers, respectively, by users, binned by experience level at the time of posting. We observe larger decreases in posting by users with previous experience post-ChatGPT. Open in new tabDownload slide Votes do not capture all aspects of quality or more generally the ways in which ChatGPT may have influenced content on Stack Overflow. For example, users with different levels of experience contribute different kinds of content to the platform. New users tend to ask more basic questions, which ChatGPT may answer better. In contrast, experienced users may ask more sophisticated questions beyond the abilities of ChatGPT. A heterogeneous effect of ChatGPT on participation on Stack Overflow by users stratified by experience would have significant implications for content. Table 2 reports changes in activity estimated in a difference-in-differences specification, decomposed by prior user experience at the time of posting. Our estimates show that, while posts made by first-time users on Stack Overflow decreased only slightly relative to the control platforms, inexperienced, experienced, and expert users made significantly fewer posts on average after the release of ChatGPT.g The point estimates (a reduction of about 21% relative to the counterfactual platforms) for both inexperienced and experienced users are almost identical, suggesting no significant difference in the decrease in activity. Table S8 estimates separate effects of ChatGPT on questions and answers. Interestingly, while inexperienced users reduce the number of their questions and answers to a similar extent, the effects for experienced and expert users are more pronounced for posting answers. We could link the latter result to lower incentives to contribute to Stack Overflow: as fewer developers are using the platform, the visibility “premium” that could be earned by answering questions becomes lower. Table 2. Open in new tab Results of a difference-in-differences model, estimating the change in activity observed weekly on stack overflow following the release of ChatGPT by user group, relative to activity on three other platforms (we exclude segment fault as we do not have access to user experience data) less likely to have been impacted.(1) (2) (3) (4)Number of posts Number of posts Number of posts Number of posts VARIABLES NewUser InexperiencedUser ExperiencedUser ExpertUser Stack Overflow × Post-GPT− 0.0833 * − 0.245 * * * − 0.254 * * * − 0.168 * * *(0.0376) (0.0529) (0.0424) (0.0292) Observations 296 296 296 296 𝑅 2 -within 0.0259 0.198 0.235 0.104(1) (2) (3) (4)Number of posts Number of posts Number of posts Number of posts VARIABLES NewUser InexperiencedUser ExperiencedUser ExpertUser Stack Overflow × Post-GPT− 0.0833 * − 0.245 * * * − 0.254 * * * − 0.168 * * *(0.0376) (0.0529) (0.0424) (0.0292) Observations 296 296 296 296 𝑅 2 -within 0.0259 0.198 0.235 0.104 Posts by new users are posts by users with no previous posts at the time of posting. Inexperienced users have posted 1–10 times before, experienced users 11–100, and experts more than 100 times. All regressions comprise platform fixed effects and week fixed effects. The standard error of the estimate clustered on month is reported in parentheses. 𝑅 2 (within) is derived after differencing out week and platform fixed effects. Significance codes: ***: 𝑃 < 0.001 ⁠, **: 𝑃 < 0.01 ⁠, *: 𝑃 < 0.05 ⁠, + ⁠: 𝑃 < 0.1 ⁠. Table 2. Open in new tab Results of a difference-in-differences model, estimating the change in activity observed weekly on stack overflow following the release of ChatGPT by user group, relative to activity on three other platforms (we exclude segment fault as we do not have access to user experience data) less likely to have been impacted.(1) (2) (3) (4)Number of posts Number of posts Number of posts Number of posts VARIABLES NewUser InexperiencedUser ExperiencedUser ExpertUser Stack Overflow × Post-GPT− 0.0833 * − 0.245 * * * − 0.254 * * * − 0.168 * * *(0.0376) (0.0529) (0.0424) (0.0292) Observations 296 296 296 296 𝑅 2 -within 0.0259 0.198 0.235 0.104(1) (2) (3) (4)Number of posts Number of posts Number of posts Number of posts VARIABLES NewUser InexperiencedUser ExperiencedUser ExpertUser Stack Overflow × Post-GPT− 0.0833 * − 0.245 * * * − 0.254 * * * − 0.168 * * *(0.0376) (0.0529) (0.0424) (0.0292) Observations 296 296 296 296 𝑅 2 -within 0.0259 0.198 0.235 0.104 Posts by new users are posts by users with no previous posts at the time of posting. Inexperienced users have posted 1–10 times before, experienced users 11–100, and experts more than 100 times. All regressions comprise platform fixed effects and week fixed effects. The standard error of the estimate clustered on month is reported in parentheses. 𝑅 2 (within) is derived after differencing out week and platform fixed effects. Significance codes: ***: 𝑃 < 0.001 ⁠, **: 𝑃 < 0.01 ⁠, *: 𝑃 < 0.05 ⁠, + ⁠: 𝑃 < 0.1 ⁠. Overall, our analysis shows little evidence that ChatGPT tends to replace low-quality posts and no evidence that it replaced posts by inexperienced users relative to experts and experienced users. Heterogeneities across programming languages Next, we investigated differences in the impact of ChatGPT on posts about different programming languages, finding significant heterogeneities. In Facet A of Fig. 4, we plot the estimated effects (slope changes in the linear time trend after the introduction of ChatGPT) for those 69 tags that we connected to a programming language on GitHub. We estimate a negative effect of ChatGPT for most tags, but the estimates range between a 0.25 standard deviation decrease in slope (i.e. change per week following the ChatGPT release) to a 0.03 standard deviation increase. We observe that some of the widely used languages like Python and Javascript are the most impacted by ChatGPT. Interestingly, the model estimates that posts about CUDA have increased (though not significantly) after ChatGPT was released. CUDA is an application programming interface created by Nvidia, a graphics card manufacturer, that facilitates the use of graphics cards for computational tasks, in particular for machine learning and AI. This exception again demonstrates the impact of ChatGPT on the world of computer programming: people are increasingly interested in software relating to AI. Fig. 4. a) The event study estimates of the effect of ChatGPT’s release on activity on a selection of tags on Stack Overflow. We report HAC-corrected 95% CIs. b) The relationship between estimated effects and salary data from the Stack Overflow Developer Survey. We find no significant relationship. c) The relationship between the number of GitHub repositories using a tag and the estimated effect of ChatGPT on that tag. In both b) and c), we plot a linear fit with bootstrapped 95% CIs. The dashed line in b) indicates that the correlation is not significant. Open in new tabDownload slide Given that previous research suggests that high-wage jobs are more exposed to ChatGPT (33), we test whether the impact of ChatGPT is more predominant among better-paid languages. We source salary data from the 2022 Stack Overflow Developer Survey, focusing on US-based developers and calculating medians of reported salaries. In Fig. 4b, we compare the estimated impact of ChatGPT on different languages against the salary data of developers using those languages. We observe no clear relationship between the estimated labor market value of a specific language and changes in posting behavior in that language post-ChatGPT. To better understand the relationship between the size of the user base of a programming language and how it is impacted by ChatGPT, we compare our estimates with data from GitHub, the largest online platform for collaborative software development. Among other sources, ChatGPT was trained on data from GitHub. Because training data were collected up to September 2021, we use data on language use on GitHub up to June 2021. In Facet C of Fig. 4, we visualize the relationship between the number of GitHub repositories (coding projects) in a specific language and the estimated impact of ChatGPT on that language. We observe that languages with more GitHub repositories tend to be more significantly impacted by the release of ChatGPT in terms of associated activity on Stack Overflow (Pearson’s 𝜌 = − 0.45 , 𝑃 < 0.001 ⁠). This result is confirmed by estimating a difference-in-differences specification that compares the change in posting following the release of ChatGPT between more and less popular programming languages as measured by the number of GitHub commits attributed to a given language as of 2021 (Table S6). Subsequent dynamics Using the Stack Exchange Data Explorer, we extended the timeseries of weekly posts to Stack Overflow to Spring 2024. We visualize this data in Fig. 5. We observe a continued, if slower, decrease in weekly posting activity after the end of our statistical analyses. In raw terms, the number of weekly posts to Stack Overflow has fallen from 60,000 to 30,000 from May 2022 to May 2024, with much of that change happening in the 6 months following the release of ChatGPT. Again, this suggests that the 25% estimate of the effect of ChatGPT on Stack Overflow should be interpreted as a lower bound effect, which is likely still growing. Fig. 5. An extended timeseries of the weekly posts to Stack Overflow. We highlight the release of ChatGPT and the conclusion of the data we use in the statistical analyses, respectively. After May 2023, the decline in posting activity continues, albeit at a slower rate. Open in new tabDownload slide An extension of the difference-in-differences analysis would not yield reliable estimates of the relative impact of LLMs of Stack Overflow for several reasons. First, the subsequent proliferation of ChatGPT or-better quality LLMs, including open source models and models available in Russia and China mean that the reference timeseries are no longer valid counterfactuals. Moreover, advances in LLM capabilities have significantly improved their performance in mathematical tasks. Thus, we do not extend our difference-in-differences analyses. Discussion The rate at which people have adopted ChatGPT is one of the fastest in the history of technology (7). It is essential that we better understand what activities this new technology displaces and what second-order effects this substitution may have (34, 35). This article shows that after the introduction of ChatGPT there was a sharp decrease in human content creation on Stack Overflow. We compare the decrease in activity on Stack Overflow with other Stack Exchange platforms where current LLMs are less likely to be used. Using a difference-in-differences model, we estimates a 25 % decline to posts on Stack Overflow relative to the counterfactual platforms within 6 months of ChatGPT’s release. We interpret this as a lower bound as ChatGPT is likely to have had small but growing impact on the counterfactual platforms as well. The Stack Overflow Developer Survey confirms that people using ChatGPT were less likely to post questions or answers on Stack Overflow. We observe no large change in social feedback on posts, measured using votes, nor in the experience composition of posting users following ChatGPT’s release. These results suggest that average post quality has not changed, nor has ChatGPT replaced only the new and inexperienced users. Posting activity related to more popular programming languages decreased more on average than that for more niche languages. Given that LLMs performance depends on the quantity of training data, this finding suggests that users are more likely to substitute Stack Overflow with ChatGPT with respect to languages LLMs are more knowledgeable about. Consequently, the widespread adoption of LLMs will likely decrease the provision of digital public goods including open data previously generated by interactions on the web. Two of our results offer some limited reasons for optimism. While posting activity on Stack Overflow decreased among inexperienced, experienced, and expert users relative to the control platforms, content created by new users remained relatively stable. New users are known to be essential to the long-run health of online communities (36). However, this optimism should be nuanced given that, if new users start behaving as inexperienced users did, then new users will also be more to likely reduce their activity in Stack Overflow. The second is that the impact of ChatGPT was less on more niche languages used by fewer people, suggesting that online conversations around such languages and the valuable information they generate will continue. Recent work by Burtch et al. (37) studying the evolution of activity on Stack Overflow and Reddit found similar results to ours. Using a synthetic control method to adjust for seasonality, the authors report a roughly 20% decrease in posting activity on Stack Overflow within 15 weeks of the release of ChatGPT, and find similar heterogeneities among programming languages. These findings complement ours, which are derived from a more conservative analysis using counterfactual platforms. One difference in our findings is that their method finds a sharp decrease in posts by new users, while we observe fewer posts by more experienced users on Stack Overflow compared to the counterfactual platforms. It would be valuable for future work to resolve this ambiguity given the importance of new users to platform health discussed above. Our results and data have some shortcomings that point to other open questions about the use and impact of LLMs. First, while we can present strong evidence that ChatGPT decreased the posting activity in Stack Overflow, we can only partially assess quality of posting activity using data on upvotes and downvotes. Users may be posting more challenging questions, ones that LLMs cannot (yet) address, to Stack Overflow. Future work should examine whether continued activity on Stack Overflow is more complex or sophisticated on average than posts from prior to ChatGPT release. Similarly, ChatGPT may have reduced the volume of duplicate questions about simple topics, though this is unlikely to impact our main results as duplicates are estimated to account for only 3 % of posts (38), and we do not observe significant changes in voting outcomes. A second limitation of our work is that we cannot observe the extent to which Russian- and Chinese-language users of the corresponding Q&A platforms are actually hindered from accessing ChatGPT; indeed recent work has shown a spike in VPN and Tor activity following the blocking of ChatGPT in Italy (30). While our results are robust to excluding the Chinese and the Russian counterfactuals, given the potential economic importance of ChatGPT and similar LLMs, it is essential that we better understand how such bans and blocks impact the accessibility of these tools (39, 40). Finally, we do not address the issue that ChatGPT may be used to generate Stack Overflow content. Stack Overflow policy effectively banned posts authored by ChatGPT within a week of its release. In any case, a significant amount of ChatGPT generated content on Stack Overflow would mean that our measures underestimate the magnitude of the ChatGPT effect. Despite these shortcomings, our results have important implications for the future of digital public goods. Before the introduction of ChatGPT, more human-generated content was posted to Stack Overflow, forming a collective digital public good due to their nonrivalrous and nonexclusionary nature—anyone with internet access can view, absorb, and extend this information, without diminishing the value of the knowledge. Now, part of this information is rather fed into privately owned LLMs like ChatGPT. This represents a significant shift of knowledge from public to private domains. This observed substitution effect also poses several issues for the future of AI. The first is that if language models crowd out open data creation, they will be limiting their own future training data and effectiveness. The second is that owners of the current leading models have exclusive access to user inputs and feedback, which, with a relatively smaller pool of open data, gives them a significant advantage against new competitors in training future models. Third, the decline of public resources on the web would reverse progress made by the web toward democratizing access to knowledge and information. Finally, the consolidation of humans searching for information around one or a few language models could narrow our explorations and focus our attention on mainstream topics. We briefly elaborate on these points, then conclude with a wider appeal for more research on the political economy of open data and AI, and how we can incentivize continued contributions to digital public goods. Training future models Our findings suggest that the widespread adoption of ChatGPT may ironically make it difficult to train future models (41). Though researchers have already expressed concerns about running out of data for training AI models (21), our results show that the use of LLMs can slow down the creation of new (open) data. Given the growing evidence that data generated by LLMs are unlikely to effectively train new LLMs (22, 23), modelers face the real problem of running out of useful data. While research on using synthetic data and mixed data to train LLMs is still ongoing, current results show that use of synthetic training data can degrade performance (24) and may even amplify biases in models (42). Human input and guidance can mitigate these issues to some extent, but in general it is still unclear if synthetic data can power continued advances in LLM capabilities. If ChatGPT truly is a “blurry JPEG” of the web (25), then in the long run, it cannot effectively replace its most important input: data derived from human activity. Indeed, OpenAI’s recent strategic partnerships with Stack Overflow and Reddit demonstrate the value of this kind of data for the continued training of LLMs.h The proliferation of LLMs has already impacted other forms of data creation: many Amazon Mechanical Turk workers now generate content (i.e. respond to surveys, evaluate texts) using ChatGPT (43). And though watermarks may help humans and models identify data creators (44), the general problem of determining whether, for example, a text is written by a human or LLM is difficult at scale (45). Competition in the AI sector A firm’s early advantage in technological innovation often leads to significant market share via various mechanisms of path dependence (46). There are increasing returns to using ChatGPT as more people use it, as it can learn from user feedback (26). Our results indicate that ChatGPT is simultaneously decreasing the amount of open training data that competitors could use to build competing models while it captures user data for itself, which may lead to technological lock-in (27). Unlike synthetic data, data on user interactions with LLMs can be used to significantly improve and tune their performance (47). We suggest that besides increasing returns to scale from network effects, the transformation of public data commons into private databases presents another mechanism by which the tech sector can become even more concentrated. Lost economic value Digital public goods generate value in many ways besides feeding LLMs and other algorithms. For instance, Wikipedia is an important source of information worldwide, but in developing countries, readers are more often motivated by intrinsic learning goals and tend to read articles in greater detail (3). Unequal access to AI may also compound inequalities in growth and innovation between countries (40). Digital public goods also provide direct value to the many websites that extract data from open data to complement their core services with extra information (4). For instance, there is substantial interdependence between sites like Wikipedia, Reddit, and Stack Overflow and the search engines that use them to enrich responses to user queries via infoboxes (17, 48), sometimes referred to as the “paradox of re-use” (18). In the case of search engines, putting links to knowledge sources within infoboxes has mitigated the issue to some degree (49), but LLMs like ChatGPT are substituting for search engines and are much less likely to link to sources. Their widespread adoption presents a significant threat to the overall sustainability of the web (50). Creators of digital public goods may also lose out. Contributors to Stack Overflow or Open Source Software (OSS) often enjoy indirect benefits (51). For instance, while OSS itself provides significant value in the global economy (52), OSS contributions are valuable signals of a firm’s capabilities to investors (53). Individual contributions to Stack Overflow are used to signal ability on the labor market (54). Any general tendency of ChatGPT to crowd out contributions to digital public goods, may limit these valuable signals that reduce economic frictions. On the other hand, such signaling activity may serve as a powerful incentive to keep people contributing. Narrowing of information seeking The substitution effect we report likely has important second-order effects on how people search for information and their exposure to new ideas. LLMs likely favor well-established perspectives and due to their efficiency decrease the need for users to forage for information. These features of LLMs may reinforce a trend observed earlier in the context of the web. Specifically, internet search engines are thought to have pushed science toward consensus and narrower topics by improving efficiency of information search and improving the visibility of mainstream information (55). LLMs may also disincentivize the use of new or niche tools because they most amplify our productivity with those tools for which it has much training data. For instance, ChatGPT may not be able to help users of a new programming language that is has not seen many examples of. Given that LLMs are poised to change how we do research (56), present a strong competitor to search engines (57), and will likely influence our news consumption (58), we need to understand what LLM efficiency implies for our contact with diverse sources of information and incentives to try new things. More generally, models like ChatGPT are going to generate political and economic winners and losers like many previous breakthrough technologies. While early evidence shows that these models enhance productivity especially among new and inexperienced workers (12, 14), there are other ways in which they may contribute to inequality between people and firms (59), for instance via potential negative side effects of automation (33, 60). Our results suggest that the economics of data creation and ownership will become more salient: as data become more valuable, there will be growing interest in how creators of data can capture some of that value (61). These multifaceted aspects of the impact of LLMs suggest that the political economy of data and AI will be especially important in the next years (58, 62, 63). In this context, our work highlights the specific issue that valuable digital public goods may be under-produced as a result of the proliferation of AI. A natural follow-up question is how we can incentivize the creation of such goods. While unemployment shocks are known to increase the provision of digital public goods (64), it would be an unsatisfying solution to suggest that people put out of work by automation will fill this gap. In the case of platforms like Stack Overflow, active users are often motivated by social feedback and gamification (65), but the continual onboarding of new users is what keeps these platforms relevant in the long run (36). For the sake of a sustainable open web and an AI ecosystem that draws on its data, we should think about how to keep people exchanging information and knowledge online. Materials Stack Exchange platform sites The raw dataset obtained from https://archive.org/details/stackexchange contains nearly all posting activity on the question and answer platforms hosted on the Stack Exchange network from its launch in 2008 to early June 2023. These include Stack Overflow, its Russian language version, and Math Overflow and Math Stack Exchange. Stack Overflow is the largest online Q&A platform for topics relating to computer programming and software development. It provides a community-curated discussion of issues programmers face (65). Questions have multiple answers, and users debate the relative merits of solutions and alternatives in comments. A track record on Stack Overflow has value on the labor market as a signal of an individual’s skills (54). The data contain over 58 million posts, including both questions and answers. Posts are linked to their posting users, from which we infer poster previous activity and can identify posts made by new users. Questions are annotated with tags indicating the topic of the post including programming languages used. Users can give posts upvotes or downvotes, providing posting users with social feedback and reputation points. The Russian language version of Stack Overflow (over 900 thousand posts) and the mathematics-oriented platforms Math Stack Exchange (over 3.5 million posts) and Math Overflow (over 300 thousand posts) have identically structured data dumps hosted in the same location. Registered users can upvotes and downvote posts made on Stack Exchange platforms. These votes provide a valuable signal of the value of posts (65, 66). They are the primary way users earn reputation points and status on Stack Exchange platforms. Votes also influence the ranking of posts in user feeds and search engine results, facilitating information filtering. Downvotes are used to moderate. The Stack Exchange data dump contains data on every vote cast, including the corresponding post, the date the vote was made, and whether it was an upvote or downvote. Segmentfault Segmentfault is a Chinese language platform with a Q&A platform for developers that has many similarities with the Stack Exchange sites. Users post questions on programming language topics and other users post answers. Questions are tagged by relevant languages and technologies, and there are similar gamification elements on the platform. We scraped data on all posts as of early June 2023, gathering over 300 thousand in total. We were careful to follow best practices when collecting this data, limiting strain on the host platform’s servers and retaining only anonymized data and metadata rather than content of posts (67). Selection of tags Stack Overflow posts are annotated by tags which describe the concepts and technologies used in the post. For example, many tags indicate programming languages, web frameworks, database technologies, or programming concepts like functions or algorithms. Stack Overflow reconciles tags referring to the same things via a centralized synonym dictionary. We selected the 1,000 most used tags up to early June 2023 and focused on those 69 which could be directly linked to language statistics reported by GitHub, described next. GitHub data on programming language use We use data from the June 2021 GHTorrent data dump (68) as a proxy measure for the amount of open data available for each programming language. The dataset reports which languages are used in each project or repository on GitHub. We simply count the number of repositories mentioning each language. We then link the languages with tags on Stack Overflow. As an alternative, we count the number of commits, elemental code contributions to repositories, to each repository, hence language. In the main article, we visualize the estimated effects of ChatGPT on specific tags that we can link to GitHub languages. We exclude some tags which refer to file formats or plain text, specifically: yaml, json, text, svg, markdown, and xml. Stack Overflow Developer Survey The 2023 Stack Overflow Developer Survey was conducted from 2023 May 8 to May 19 and captured responses from 89,184 software developers across 185 countries. Respondents were recruited primarily through channels owned by Stack Overflow, therefore users that are highly engaged on Stack Overflow were more likely to notice the prompts to take the survey over the duration of the collection promotion.i This survey includes self-disclosed information about respondents professional status, academic qualifications, employment type, remote work status, and years of coding experience. Moreover the survey asked participants ’Which AI powered tools did you use regularly over the past year’ and included ChatGPT as an option to tick. Notes a There is no standard classification of user experience based on the number of posts. We chose a log-binned classification since activity on Stack Overflow is heavy-tailed (31), and log base 10 is a commonly used base. b For robustness, we test an OLS specification with standardized outcomes and a specification with the raw count of posts that we fit using the Poisson pseudo-maximum likelihood method. c In this way, we do not include Covid-induced positive shock in 2020 and then the reversion to the trend in 2021. d We standardize the number of posts within each tag by subtracting the mean and dividing by the standard deviation. Both statistics are calculated using data up to the release of ChatGPT. e For example, if a developer reports using three languages, there will be three entries (one for each language) in our dataset for this developer, each with a weight of 1/3. f https://www.similarweb.com/blog/insights/ai-news/chatgpt-bard/ g Prior to the release of ChatGPT, new users contributed 9.5 thousand posts per week; inexperienced users—almost 20 thousand; experienced users—about 17.5 thousand, and expert users—16 thousand. h See: https://www.theverge.com/2024/5/6/24150341/openai-stack-overflow-partner-api-coding-assistance and https://time.com/6979197/reddit-openai-partnership-chatgpt/ i Additional information about the survey and its methodology is available here: https://survey.stackoverflow.co/2023/ Acknowledgments We thank Frank Neffke, Gergő Tóth, Christoffer Koch, Sándor Juhász, Martin Allen, Manran Zhu, Karl Wachs, László Czaller, Todd Davies, Thomas Fackler, César Hidalgo, Florian Englmaier, Helene Strandt, and James Evans for helpful comments and discussions. Supplementary Material Supplementary material is available at PNAS Nexus online. Funding R.M.D.R.C. acknowledges funding from James S. McDonnell Foundation. J.W. acknowledges support from the Hungarian National Scientific Fund (OTKA FK 145960) and use of the HUN-REN Cloud (69) in the “Geographies of Creation, Learning and Use in Software” project. N.L. acknowledges support from CRC TRR 190 Rationality & Competition. Author Contributions R.M.D.R.C., N.L., and J.W. together conceived the idea, collected the data, created the models, analyzed the results, and wrote the manuscript. Preprints A preprint of this article is published at https://arxiv.org/abs/2307.07367. Data Availability Data and code to reproduce our analyses are available on Zenodo: https://zenodo.org/records/12670482. The Stack Overflow data dump is available here: https://archive.org/details/stackexchange. References 1 Henzinger M , Lawrence S . 2004 . Extracting knowledge from the world wide web . Proc Natl Acad Sci U S A . 101 : 5186 – 5191 . Google Scholar Crossref Search ADS PubMed 2 Hess C , Ostrom E . 2003 . Ideas, artifacts, and facilities: information as a common-pool resource . Law Contemp Probl . 66 ( 1/2 ): 111 – 145 . Google Scholar OpenURL Placeholder Text 3 Lemmerich F , Sáez-Trumper D , West R , Zia L . 2019 . Why the world reads Wikipedia: beyond English speakers. In: Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining; Melbourne. ACM. p. 618–626 . 4 Piccardi T , Redi M , Colavizza G , West R . 2021 . On the value of Wikipedia as a gateway to the web. In: Proceedings of the Web Conference 2021; IW3C2, Ljubljana. p. 249–260 . 5 Naveed H , et al. 2023 . A comprehensive overview of large language models, arXiv, arXiv:2307.06435, preprint: not peer reviewed. 10.48550/arXiv.2307.06435 6 OpenAI . 2023 . GPT-4 Technical Report . 7 Teubner T , Flath CM , Weinhardt C , van der Aalst W , Hinz O . 2023 . Welcome to the era of ChatGPT et al. the prospects of large language models . Bus Inf Syst Eng . 65 ( 2 ): 95 – 101 . Google Scholar Crossref Search ADS 8 Gu H , Schreyer M , Moffitt K , Vasarhelyi MA . 2023 . Artificial intelligence co-piloted auditing. Available at SSRN 4444763 . 9 Smith MJ , Geach JE . 2023 . Astronomia ex machina: a history, primer and outlook on neural networks in astronomy . R Soc Open Sci . 10 ( 5 ): 221454 . Google Scholar Crossref Search ADS PubMed 10 Kanjee Z , Crowe B , Rodman A . 2023 . Accuracy of a generative artificial intelligence model in a complex diagnostic challenge . JAMA . 330 ( 1 ): 78 – 80 . Google Scholar Crossref Search ADS PubMed 11 Guo T , et al. 2023 . What indeed can GPT models do in chemistry? A comprehensive benchmark on eight tasks, arXiv, arXiv:2305.18365, preprint: not peer reviewed. 10.48550/arXiv.2305.18365 12 Brynjolfsson E , Li D , Raymond LR . 2023 . Generative AI at work. Technical Report, National Bureau of Economic Research . 13 Dell’Acqua F , et al. 2023 . Navigating the jagged technological frontier: field experimental evidence of the effects of ai on knowledge worker productivity and quality. Harvard Business School Technology & Operations Mgt. Unit Working Paper (24-013) . 14 Noy S , Zhang W . 2023 . Experimental evidence on the productivity effects of generative artificial intelligence . Science . 381 ( 6654 ): 187 – 192 . Google Scholar Crossref Search ADS PubMed 15 Peng S , Kalliamvakou E , Cihon P , Demirer M . 2023 . The impact of AI on developer productivity: evidence from GitHub Copilot, arXiv, arXiv:2302.06590, preprint: not peer reviewed. 10.48550/arXiv.2302.06590 16 Wiles E , Munyikwa ZT , Horton JJ . 2023 . Algorithmic writing assistance on Jobseekers’ resumes increases hires. Technical Report. National Bureau of Economic Research . 17 McMahon C , Johnson I , Hecht B . 2017 . The substantial interdependence of Wikipedia and Google: a case study on the relationship between peer production communities and information technologies. In: Proceedings of the International AAAI Conference on Web and Social Media; Montreal. AAAI. vol. 11, p. 142–151 . 18 Taraborelli D . 2015 . The sum of all human knowledge in the age of machines: a new research agenda for Wikimedia. In: ICWSM-15 Workshop on Wikipedia; Oxford. AAAI. 19 Delile Z , et al. 2023 . Evaluating privacy questions from stack overflow: can ChatGPT compete?, arXiv, arXiv:2306.11174, preprint: not peer reviewed. 10.48550/arXiv.2306.11174 20 Widjojo P , Treude C . 2023 . Addressing compiler errors: stack overflow or large language models?, arXiv, arXiv:2307.10793, preprint: not peer reviewed. 10.48550/arXiv.2307.10793 21 Villalobos P , et al. 2022 . Will we run out of data? An analysis of the limits of scaling datasets in Machine Learning, arXiv, arXiv:2211.04325, preprint: not peer reviewed. 10.48550/arXiv.2211.04325 22 Alemohammad S , et al. 2023 . Self-consuming generative models go mad , arXiv, arXiv:2307.01850 , preprint. 23 Gudibande A , et al. 2023 . The false promise of imitating proprietary LLMs, arXiv, arXiv:2305.15717, preprint: not peer reviewed . 10.48550/arXiv.2305.15717 24 Shumailov I , et al. 2024 . Ai models collapse when trained on recursively generated data . Nature . 631 ( 8022 ): 755 – 759 . Google Scholar Crossref Search ADS PubMed 25 Chiang T . 2023 . ChatGPT is a blurry JPEG of the web . The New Yorker . Google Scholar OpenURL Placeholder Text 26 Arthur WB . 1989 . Competing technologies, increasing returns, and lock-in by historical events . Econ J . 99 ( 394 ): 116 – 131 . Google Scholar OpenURL Placeholder Text 27 David PA . 1985 . Clio and the economics of QWERTY . Am Econ Rev . 75 ( 2 ): 332 – 337 . Google Scholar OpenURL Placeholder Text 28 Stojkoski V , Koch P , Coll E , Hidalgo CA . 2024 . Estimating digital product trade through corporate revenue data . Nat Commun . 15 ( 1 ): 5262 . Google Scholar Crossref Search ADS PubMed 29 Weidinger L . 2022 . Taxonomy of risks posed by language models. In: Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency; Seoul. ACM. p. 214–229 . 30 Kreitmeir DH , Raschky PA . 2023 . The Unintended Consequences of Censoring Digital Technology–Evidence from Italy’s ChatGPT Ban, arXiv, arXiv:2304.09339, preprint: not peer reviewed . 10.48550/arXiv.2304.09339 31 Upadhyay U , Valera I , Gomez-Rodriguez M . 2017 . Uncovering the dynamics of crowdlearning and the value of knowledge. In: Proceedings of the Tenth ACM International Conference on Web Search and Data Mining. p. 61–70; Cambridge (UK). ACM. 32 Bilinski A , Hatfield LA . 2018 . Nothing to see here? Non-inferiority approaches to parallel trends and other model assumptions, arXiv, arXiv:1805.03273, preprint: not peer reviewed . 10.48550/arXiv.1805.03273 33 Eloundou T , Manning S , Mishkin P , Rock D . 2024 . GPTs are GPTs: labor market impact potential of LLMs . Science . 384 ( 6702 ): 1306 – 1308 . Google Scholar Crossref Search ADS PubMed 34 Aghion P , Howitt P . 1992 . A model of growth through creative destruction . Econometrica . 60 ( 2 ): 323 – 351 . Google Scholar Crossref Search ADS 35 Schumpeter JA . 1942 . Capitalism, socialism, and democracy . New York : Routledge . Google Scholar OpenURL Placeholder Text 36 Danescu-Niculescu-Mizil C , West R , Jurafsky D , Leskovec J , Potts C . 2013 . No country for old members: user lifecycle and linguistic change in online communities. In: Proceedings of the 22nd international conference on World Wide Web; Rio, ACM. p. 307–318 . 37 Burtch G , Lee D , Chen Z . 2024 . The consequences of generative AI for online knowledge communities . Sci Rep . 14 ( 1 ): 10413 . Google Scholar Crossref Search ADS PubMed 38 Correa D , Sureka A . 2013 . Fit or unfit: analysis and prediction of ’closed questions’ on Stack Overflow. In: Proceedings of the First ACM conference on Online Social Networks; Boston. ACM. p. 201–212 . 39 Bao H , Sun M , Teplitskiy M . 2024 . Where there’s a will there’s a way: ChatGPT is used more for science in countries where it is prohibited . 40 Gaessler F , Piezunka H . 2023 . Training with AI: evidence from chess computers . Strat Manag J . 44 ( 11 ): 2724 – 2750 . Google Scholar Crossref Search ADS 41 Taleb NN . 2012 . Antifragile: how to live in a world we don’t understand . vol. 3 . London : Allen Lane . Google Scholar OpenURL Placeholder Text 42 Wyllie S , Shumailov I , Papernot N . 2024 . Fairness feedback loops: training on synthetic data amplifies bias, arXiv, arXiv:2403.07857, preprint: not peer reviewed . 10.48550/arXiv.2403.07857 43 Veselovsky V , Ribeiro MH , West R . 2023 . Artificial artificial artificial intelligence: crowd workers widely use large language models for text production tasks, arXiv, arXiv:2306.07899, preprint: not peer reviewed . 10.48550/arXiv.2306.07899 44 Tian-Zheng Wei J , Wang RY , Jia R . 2024 . Proving membership in LLM pretraining data via data watermarks, arXiv, arXiv:2402.10892, preprint: not peer reviewed. 10.48550/arXiv.2402.10892 45 Tang R , Chuang Y-N , Hu X . 2024 . The science of detecting LLM-generated text . Commun ACM . 67 ( 4 ): 50 – 59 . Google Scholar Crossref Search ADS 46 Page SE . 2006 . Path dependence . Quart J Polit Sci . 1 ( 1 ): 87 – 115 . Google Scholar Crossref Search ADS 47 Köpf A . 2024 . Openassistant conversations-democratizing large language model alignment . Adv Neural Inf Process Syst . 36 : 47669 – 47681 . Google Scholar OpenURL Placeholder Text 48 Vincent N , Johnson I , Hecht B . 2018 . Examining Wikipedia with a broader lens: quantifying the value of Wikipedia’s relationships with other large-scale online communities. In: Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems; Quebec. ACM. p. 1–13 . 49 Vincent N , Hecht B . 2021 . A deeper investigation of the importance of Wikipedia links to search engine results . Proc ACM Hum-Comput Inter . 5 ( CSCW1 ): 1 – 15 . Google Scholar OpenURL Placeholder Text 50 Vincent N . 2022 . The paradox of reuse, language models edition. Data leverage . 51 Lerner J , Tirole J . 2002 . Some simple economics of open source . J Ind Econ . 50 ( 2 ): 197 – 234 . Google Scholar Crossref Search ADS 52 Greenstein S , Nagle F . 2014 . Digital dark matter and the economic contribution of Apache . Res Policy . 43 ( 4 ): 623 – 631 . Google Scholar Crossref Search ADS 53 Conti A , Peukert C , Roche MP . 2021 . Beefing IT up for your investor? Open sourcing and startup funding: evidence from GitHub. HBS Working Paper 22-001 . 54 Xu L , Nian T , Cabral L . 2020 . What makes geeks tick? A study of stack overflow careers . Manage Sci . 66 ( 2 ): 587 – 604 . Google Scholar Crossref Search ADS 55 Evans JA . 2008 . Electronic publication and the narrowing of science and scholarship . Science . 321 ( 5887 ): 395 – 399 . Google Scholar Crossref Search ADS PubMed 56 Grossmann I , et al. 2023 . AI and the transformation of social science research . Science . 380 ( 6650 ): 1108 – 1109 . Google Scholar Crossref Search ADS PubMed 57 Xu R , Feng Y , Chen H . 2023 . ChatGPT vs. Google: a comparative study of search performance and user experience, arXiv, arXiv:2307.01135, preprint: not peer reviewed . 10.48550/arXiv.2307.01135 58 Sandrini L , Somogyi R . 2023 . Generative ai and deceptive news consumption . Econ Lett . 232 : 111317 . Google Scholar Crossref Search ADS 59 Rock D . 2019 . Engineering value: the returns to technological talent and investments in artificial intelligence. Available at SSRN 3427412 . 60 Acemoglu D , Restrepo P . 2019 . Automation and new tasks: how technology displaces and reinstates labor . J Econ Perspect . 33 ( 2 ): 3 – 30 . Google Scholar Crossref Search ADS 61 Li H , Vincent N , Chancellor S , Hecht B . 2023 . The dimensions of data labor: a road map for researchers, activists, and policymakers to empower data producers. In: Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency; Chicago. ACM. p. 1151–1161 . 62 Johnson S , Acemoglu D . 2023 . Power and progress: our thousand-year struggle over technology and prosperity . UK : Hachette . Google Scholar OpenURL Placeholder Text 63 Lehdonvirta V . 2022 . Cloud empires: how digital platforms are overtaking the state and how we can regain control . Cambridge : MIT Press . Google Scholar Crossref Search ADS 64 Kummer M , Slivko O , Zhang X . 2020 . Unemployment and digital public goods contribution . Inform Syst Res . 31 ( 3 ): 801 – 819 . Google Scholar Crossref Search ADS 65 Anderson A , Huttenlocher D , Kleinberg J , Leskovec J . 2012 . Discovering value from community activity on focused question answering sites: a case study of Stack Overflow. In: Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining; Beijing. ACM. p. 850–858 . 66 Mamykina L , Manoim B , Mittal M , Hripcsak G , Hartmann B . 2011 . Design lessons from the fastest Q&A site in the west. In: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems; Vancouver. ACM. p. 2857–2866 . 67 Franzke AS , Bechmann A , Zimmer M , Ess C , The Association of Internet Researchers . 2020 . Internet research: ethical guidelines 3.0. Technical Report, Association of Internet Researchers . 68 Gousios G , Spinellis D . 2012 . GHTorrent: GitHub’s data from a firehose. In: 2012 9th IEEE Working Conference on Mining Software Repositories (MSR); Zurich. IEEE. p. 12–21 . 69 Héder M , et al. 2022 . The past, present and future of the ELKH cloud . Inform Társadalom . 22 ( 2 ): 128 . Google Scholar Crossref Search ADS Author notes Competing Interest: The authors declare no competing interests. © The Author(s) 2024. Published by Oxford University Press on behalf of National Academy of Sciences. This is an Open Access article distributed under the terms of the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited. Subject Computer Sciences Economic Sciences Editor: Matjaz Perc Matjaz Perc Editor Search for other works by this author on: Oxford Academic Google Scholar Download all slides Supplementary data Supplementary data pgae400_Supplementary_Data - pdf file Advertisement Citations Views 1,916 Altmetric More metrics information Metrics Total Views 1,916 1,549 Pageviews 367 PDF Downloads Since 9/1/2024 Month: Total Views: September 2024 1,140 October 2024 776 Citations Powered by Dimensions Altmetrics × Email alerts Article activity alert Advance article alerts New issue alert In progress issue alert Subject alert Receive exclusive offers and updates from Oxford Academic Citing articles via Google Scholar Latest Most Read Most Cited Modeling human activity comprehension at human scale: Prediction, segmentation, and categorization Engineering a cleaved, prefusion-stabilized influenza B virus hemagglutinin by identification and locking of all six pH-switches A complete classification of evolutionary games with environmental feedback Spatial multiomic insights into acute cocaine exposure Neuron-specific deficiency of autophagy increases neuronal loss in traumatic brain injury More from Oxford Academic Medicine and Health Science and Mathematics Social Sciences Books Journals Advertisement Advertisement close advertisement Advertisement About PNAS Nexus Editorial Board Author guidelines X (formerly Twitter) LinkedIn Facebook Researcher Contact PNAS Nexus Advertising and Corporate Services Journals Career Network Online ISSN 2752-6542 Copyright © 2024 National Academy of Sciences About Oxford Academic Publish journals with us University press partners What we publish New features Authoring Open access Purchasing Institutional account management Rights and permissions Get help with access Accessibility Contact us Advertising Media enquiries Oxford University Press News Oxford Languages University of Oxford Oxford University Press is a department of the University of Oxford. It furthers the University's objective of excellence in research, scholarship, and education by publishing worldwide Copyright © 2024 Oxford University Press Cookie settings Cookie policy Privacy policy Legal notice Close Close This Feature Is Available To Subscribers Only Sign In or Create an Account Close This PDF is available to Subscribers Only View Article Abstract & Purchase Options For full access to this pdf, sign in to an existing account, or purchase an annual subscription. Close",
    "commentLink": "https://news.ycombinator.com/item?id=41827043",
    "commentBody": "Large language models reduce public knowledge sharing on online Q&A platforms (oup.com)253 points by croes 7 hours agohidepastfavorite220 comments melenaboija 5 hours agoIt’s been a relief to find a platform where I can ask questions without the fear of being humiliated Half joking, but I am pretty tired of SO pedantry. reply PhilipRoman 4 hours agoparentI haven't really found stackoverflow to be that humiliating (compared to some IRC rooms or forums), basic questions get asked and answered all the time. But the worst part is when you want to do something off the beaten path. Q: how do I do thing X in C? A: Why do you need to know this? The C standard doesn't say anything about X. The answer will depend on your compiler and platform. Are you sure you want to do X instead Y? What version of Ubuntu are you running? reply haolez 4 hours agorootparentThe first time that I asked a question on #cpp @Freenode was a unique experience for my younger self. My message contained greetings and the question in the same message. I was banned immediately and the response from the mods was: - do not greet; we don't have time for that bullshit - do not use natural language questions; submit a test case and we will understand what you mean through your code - do not abbreviate words (you have abbreviated \"you\" as \"u\"); if you do not have time to type the words, we do not have time to read them The ban lasted for a week! :D reply jancsika 1 hour agorootparentThis being HN, I'd love to hear from one of the many IRC channel mods who literally typed (I'd guess copy/pasted) this kind of text into their chat room topics and auto-responders. If you're out there-- how does it feel to know that what you meant as a efficient course-correction for newcomers was instead a social shaming that cut so deep that the message you wrote is still burned verbatim into their memory after all these years? To be clear, I'm taking OP's experience as a common case of IRC newbies at that time on many channels. I certainly experienced something like it (though I can't remember the exact text), and I've read many others post on HN about the same behavior from the IRC days. Edit: clarifications reply HPsquared 52 minutes agorootparentI think a lot of unpaid online discussion forum moderation volunteers get psychic profit from power tripping. reply hinkley 34 minutes agorootparentGive a man a little power. reply CogitoCogito 56 minutes agorootparentprev> was instead a social shaming that cut so deep that the message you wrote is still burned verbatim into their memory after all these years? Maybe that was the point? reply bqmjjx0kac 4 hours agorootparentprev> do not use natural language questions That is really absurd! AFAIK, it is not possible to pose a question to a human in C++. This level of dogmatism and ignorance of human communication reminds me of a TL I worked with once who believed that their project's C codebase was \"self-documenting\". They would categorically reject PRs that contained comments, even \"why\" comments that were legitimately informative. It was a very frustrating experience, but at least I have some anecdotes now that are funny in retrospect. reply ravenstine 1 hour agorootparentSelf-documenting code is one of the worst ideas in programming. Like you, I've had to work with teams where my PRs would be blocked until I removed my comments. I'm not talking pointless comments like \"# loop through the array\" but JSdoc style comments describing why a function was needed. I will no longer work anywhere that has this kind of culture. reply wccrawford 4 hours agorootparentprevA one week ban on the first message is clearly gatekeeping. What a bunch of jerks. A 1 hour ban would have been a lot more appropriate, and escalate from there if the person can't follow the rules. Don't even get me started about how dumb rule 2 is, though. And rule 3 doesn't even work for normal English as many things are abbreviated, e.g. this example. And of course, you didn't greet and wait, you just put a pleasantry in the same message. Jeez. I'm 100% sure I'd never have gone back after that rude ban. reply GeoAtreides 4 hours agorootparent\"I'm 100% sure I'd never have gone back after that rude ban.\" upon saying this, the young apprentice was enlightened reply luckylion 2 hours agorootparentprev> And of course, you didn't greet and wait, you just put a pleasantry in the same message. Jeez. I'm pretty sure that \"rule\" was more aimed towards \"just ask your question\" rather than \"greet, make smalltalk, then ask your question\". I have similar rules, though I don't communicate them as aggressively, and don't ban people for breaking them, I just don't reply to greetings coming from people I know aren't looking to talk to me to ask me how I've been. It's a lot easier if you send the question you have instead of sending \"Hi, how are you?\" and then wait for 3 minutes to type out your question. reply SunlitCat 3 hours agorootparentprevThat contradiction is funny, tho: > - do not greet; we don't have time for that bullshit and > do not abbreviate words (you have abbreviated \"you\" as \"u\"); if you do not have time to type the words, we do not have time to read them So they have apparently enough time to read full words, it seems! reply sokoloff 3 hours agorootparentI think reading “u” takes longer than reading “you”. With “u”, I have to pause for a moment and think “that’s not a normal word; I wonder if they meant to type ‘i’ instead (and just hit a little left of target)?” and then maybe read the passage twice to see which is more likely. I don’t think it’s quite as much a contradiction. (It still could be more gruff than needed.) reply beeboobaa3 3 hours agorootparentprevYh u gtta b c00l w abbrvs reply 6510 2 hours agorootparentThe noobs don't got how we get where we get? edit: I remember how some communities changed into: The help isn't good enough, you should help harder, I want you to help me by these conventions. Then they leave after getting their answer and no one has seen them ever again rather than join the help desk. reply tinco 2 hours agorootparentprevI learned a bunch of programming languages on IRC, and the C and C++ communities on freenode were by far the most toxic I've encountered. Now that Rust is succesfully assimilating those communities, I have noticed the same toxicity on less well moderated forums, like the subreddit. The Discord luckily is still great. It's probably really important to separate the curmudgeons from the fresh initiates to provide an enjoyable and positive experience for both groups. Discord makes that really easy. In the Ruby IRC channel curmudgeons would simply be shot down instantly with MINASWAN style arguments. In the Haskell IRC channel I guess it was basically accepted that everyone was learning new things all the time, and there was always someone willing to teach at the level you were trying to learn. reply betaby 2 hours agorootparentNot my experience. IRC was 'toxic' since forever, but that't not a toxicity, that's inability to read emotion through transactional plan text. Once one account that in the mental model IRC is just fine. reply d0mine 7 minutes agorootparentprevThe major misunderstanding is that SO exists to help the question author first. It is not an IRC. The most value comes from googling a topic and getting existing answers on SO. In other words, perhaps in your very specific case, your question is not XY problem but for the vast majority of visitors from google it won't be so. https://en.wikipedia.org/wiki/XY_problem Personally, I always answered SO from at least two perspectives: how the question looks for someone coming from google and how the author might interpret it. reply elicksaur 2 hours agorootparentprevOn the other hand, I find it to be a fatal flaw that LLMs can’t say, “Hey you probably don’t actually want to do it that way.” reply Ekaros 58 minutes agorootparentI always wonder about that. Very often it seems that you need to be able to LLM that they are wrong. And then they happily correct themselves. But if you do not know that the answer is wrong how can you get correct answer? reply o11c 15 minutes agorootparentWorse: if you think the LLM is wrong and try to correct it, it will happily invent something completely different (and actually wrong this time). reply stemlord 2 hours agorootparentprevSure but most common LLMs aren't going to be patronizing and presumptuous while they say so reply chii 1 hour agorootparentprev> Q: how do I do thing X in C? SO does suck, but i've found that if you clarify in the question what you want, and pre-empt the Y instead of X type answers, you will get some results. reply PhilipRoman 52 minutes agorootparentI wish... Some commenters follow up with \"Why do you think Y won't work for you?\" reply mhh__ 4 hours agorootparentprevI find that this is mainly a problem in languages that attract \"practical\"/\"best tool for the job\" Philistines. Not going to name names right now but I had never really experienced this until I started using languages from a certain Washington based software company. reply appendix-rock 4 hours agorootparentGod. Yeah. I’ve always hated #IAMPolicies on Freenode :) reply ElFitz 1 hour agorootparentprevIt’s also quite fun when you ask niche questions that haven’t been asked or answered yet (\"How do I do X with Y?\"), and just get downvoted for some reason. That’s when I stopped investing any effort into that community. Turned out it, counter-intuitively, was impossible. And not documented anywhere. reply hanniabu 3 hours agorootparentprevMy questions always get closed and marked as a duplicate with a comment linking to a question that's unrelated reply skywhopper 1 hour agorootparentprevI mean, those all sound like good questions. You might be a super genius, but most people who ask how to do X actually want to do Y. And if they DO want X, then those other questions about compiler and OS version really matter. The fact that you didn’t include them in your question shows you aren’t really respecting the time of the experts on the platform. If you know you are doing something unusual, then you need to provide a lot more context. reply bayindirh 1 hour agorootparentprevYes, immature people are everywhere, but SO took it to a new level before they had to implement a code of conduct. I remember asking questions and getting \"this is is common misconception, maybe you're looking for X instead\" type of actually helpful and kind answers. After some point it came to a point that if you're not asking a complete problem which can't be modeled as a logic statement, you're labeled as stupid for not knowing better. The thing is, if I knew better or already found the answer, I'd not be asking to SO in the first place. After a couple of incidents, I left the place for the better. I can do my own research, and share my knowledge elsewhere. Now they're training their and others models with that corpus, I'll never add a single dot to their dataset. reply tlogan 4 hours agoparentprevThe main issue with Stack Overflow (and similar public Q&A platforms) is that many contributors do not know what they do not know, leading to inaccurate answers. Additionally, these platforms tend to attract a fair amount of spam (self promotion etc) which can make it very hard to find high-quality responses. reply delichon 4 hours agorootparentI've gotten answers from OpenAI that were technically correct but quite horrible in the longer term. I've gotten the same kinds of answers on Stack Overflow, but there other people are eager to add the necessary feedback. I got the same feedback from an LLM but only because in that case I knew enough to ask for it. Maybe we can get this multi-headed advantage back from LLMs by applying a team of divergent AIs to the same problem. I've had other occasions when OpenAI gave me crap that Claude corrected, and visa versa. reply zmgsabst 3 hours agorootparentYou can usually even ask the same LLM: - do a task - criticize your job on that task - redo that task based on criticism I find giving the LLM a process greatly improves the results. reply iSnow 1 hour agorootparentThat's a smart idea I didn't think of. I've been arguing with Copilot back and forth where it gave me a half-working solution that seemed overly complicated but since I was new to the tech used, I couldn't say what exactly was wrong. After a couple of hours, I googled the background and trust my instinct and was able to simplify the code. At that situation, where I iteratively improved the solution by telling Copilot things seem to complicated and this or that isn't working. That led the LLM to actually come back with better ideas. I kept asking myself why something like you propose isn't baked into the system. reply drawnwren 1 hour agorootparentprevThe papers I've read have shown LLM critics to be quite bad at their work. If you give an LLM a few known good and bad results, I think you'll see the LLM is just as likely to make good results bad as it is to make bad results good. reply roughly 2 hours agorootparentprevWhat’s fun is that you can skip step 1. The LLM will happily critique its own nonexistent output. reply blazing234 3 hours agorootparentprevHow do you know the second result is correct? Or the third? Or the fourth? reply phil-martin 1 hour agorootparentI approach it the same way as the things I build myself - testing and measuring. Although if I’m truly honest with myself, even after many years of developing, the true cycle of me writing code is: over confidence, then shock it didn’t work 100% the first time, wondering if there is a bug in the compiler, and then reality setting in that of course the compiler is fine and I just made my 15th off-by-one error of the day :) reply cjauvin 4 hours agorootparentprevI find that LLMs are precisely that: marvelous engines to explore \"what you don't know that you don't know\", about anything. reply internet101010 3 hours agorootparentprevMedium is even worse about this. It's more self-promotion than it is common help. reply bee_rider 59 minutes agorootparentQA platforms and blogging platforms both seem to have finite lifespans. QA forums (Stack Overflow, Quora, Yahoo answers) do seem to last longer, need to be moderated pretty aggressively unless they are going to turn into homework help platforms. Blogging platforms are the worst though. Medium looked pretty OK when it first came out. But now it is just a platform for self-promotion. Substack is like 75% of the way through that transition IMO. People who do interesting things spend most of their time doing the thing. So, non-practicing bloggers and other influencers will naturally overwhelm the people who actually have anything interesting to report. reply n_ary 2 hours agorootparentprevBegin rant. I don’t want to be that guy saying this, but 99% of the top results on google from Medium related to anything technical is literally the reworded/reframed version of the official quick start guide. There are some very rare gems, but it is hard to find those among the above mentioned ocean of reworded quick starts disguised as “how to X”, “fixing Y”. Almost reminds me of the SEO junks when you search “how to restart iPhone” and find answers that dance around letting it die from battery drain and then charge, install this software, take it to the apple repair shop, go to settings and traverse many steps while not saying that if you are between these models use the power+volume up button trick. End of rant. reply bee_rider 58 minutes agorootparentSomebody who just summarizes tutorials can write like 10 medium posts, in the time it takes an actual practitioner to do something legitimately interesting. reply mrkramer 3 hours agorootparentprev>The main issue with Stack Overflow (and similar public Q&A platforms) is that many contributors do not know what they do not know, leading to inaccurate answers. The best Q&A platform would be the one where experts and scientists answer questions but sites like Wikipedia and Reddit showed that broad range of audience can also be pretty good at providing useful information and moderating it. reply TacticalCoder 1 hour agorootparentprevWhat you mention has been serious from day one indeed. But to me the worst issue is it's now \"Dead Overflow\": most answers are completely, totally and utterly outdated. And seen that they made the mistake of having the concept of an \"accepted answer\" (which should never have existed), it only makes the issue worse. If it's a question about things that don't change often, like algorithms, then it's OK. But for anything \"tech\", technical rot is a very real thing. To me SO has both outdated and inaccurate answers. reply LeadB 4 hours agoparentprevFor the major programming languages, it must be a pretty esoteric question if it does not have an answer yet. Increasingly, the free products of experts are stolen from them with the pretext that \"users need to be protected\". Entire open source projects are stolen by corporations and the experts are removed using the CoC wedge. Now SO answers are stolen because the experts are not trained like hotel receptionists (while being short of time and unpaid). I'm sure that the corporations who steal are very polite and CoC compliant, and when they fire all developers once an AGI is developed, the firing notices will be in business speak, polite, express regret and wish you all the best in your future endeavors! reply stemlord 1 hour agorootparentHm fair point. Rudeness is actually a sign of humanity. Like that one black mirror episode reply appendix-rock 4 hours agorootparentprevI’m sorry that you ran afoul of a CoC or whatever, but this sounds like a real ‘airing dirty laundry’ tangent. reply lrpanb 2 hours agorootparentOne man's tangent is another man's big picture. It may be the case of course that some people guilty of CoC overreach are shaking in their boots right now because they went further than their corporations wanted them to go. reply herval 3 hours agoparentprevThe flipside to this is you can’t get answers to anything _recent_, since the models are trained years behind in content. My feelig is it’s getting increasingly difficult to figure out issues on the latest version of libraries & tools, as the only options are private Discords (which aren’t even googleable) reply Vegenoid 2 hours agorootparentI think that knowledge hoarding may come back with a vengeance with the threat people feel from LLMs and offshoring. reply chairmansteve 1 hour agorootparentYep. For SO, the incentive was a high reputation. But now an LLM is stealing your work, what's the point? reply yieldcrv 2 hours agorootparentprevThe models come out fast enough Doesn’t seem to be a great strategy to always need these things retrained, but OpenAI’s o1 has things from early 2024 Don’t ask about knowledge cutoffs anymore, that’s not how these things are trained these days. They don’t know their names or the date. reply herval 2 hours agorootparentNot my daily experience. It’s been impossible to get relevant answers to questions on multiple languages and frameworks, no matter the model. O1 frequently generates code using deprecated libraries (and is unable to fix it with iteration). Not to mention there will be no data for the model to learn the new stuff anyway, since places like SO will get zero responses with the new stuff for the model to crawl reply yieldcrv 1 hour agorootparentYes I encounter that too but for things in just the last few months with o1 It is really difficult if you need project flags and configurations to make things work, instead of just code Github issues gets crawled, where many of these frameworks have their community reply jneagu 1 hour agoparentprevI am very curious to see how this is going to impact STEM education. Such a big part of an engineer's education happens informally by asking peers, teachers, and strangers questions. Different groups are more or less likely to do that consistently (e.g. https://journals.asm.org/doi/10.1128/jmbe.00100-21), and it can impact their progress. I've learned most from publicly asking \"dumb\" questions. reply ocular-rockular 1 hour agorootparentIt won't. If you look at advanced engineering/mathematics material online it is abysmal in quality of actually \"explaining\" the content. Most of the learning and understanding of intricacies happens via dialogue with professors/mentors/colleagues/etc. That said, when that is not available, LLMs do an excellent job or rubber ducky-ing complicated topics. reply jneagu 1 hour agorootparentTo your latter point - that’s where I think most of the value of LLMs in education is. They can explain code beyond the educational content that’s already available out there. They are pretty decent at finding and explaining code errors. Someone who’s ramping up their coding skills can make a lot of progress with those two features alone. reply Aurornis 2 hours agoparentprevMany of the forums I enjoyed in the past have become heavily burdened by rules, processes, and expectations. They are frequented by people who spend hours every day reading everything and calling out any misstep. Some of them are so overburdened that navigating all of the rules and expectations becomes a skill in itself. A single innocent misstep turns simple questions into lectures about how you’ve violated the rules. One Slack I joined has created a Slackbot to enforce these rules. It became a game in itself for people to add new rules to the bot. Now it triggers on a large dictionary of problematic words such as “blind” (potentially offensive to people with vision impairments. Don’t bother discussing poker.). It gives a stern warning if anyone accidentally says “crazy” (offensive to those with mental health problems) or “you guys” (how dare you be so sexist). They even created a rule that you have to make sure someone wants advice about a situation before offering it, because a group of people decided it was too presumptuous and potentially sexist (I don’t know how) for people to give advice when the other person may have only wanted to vent. This creates the weirdest situations where someone posts a question in channels named “Help and advice” and then lurkers wait to jump on anyone who offers advice if the question wasn’t explicitly phrased in a way that unequivocally requested advice. It’s all so very tiresome to navigate. Some people appear to thrive in this environment where there are rules for everything. People who memorize and enforce all of the rules on others get to operate a tiny little power trip while opening an opportunity to lecture internet strangers all day. It’s honestly refreshing to go from that to asking an LLM that you know isn’t going to turn your question into a lecture on social issues because you used a secretly problematic word or broke rule #73 on the ever growing list of community rules. reply abraae 1 hour agorootparent> Some people appear to thrive in this environment where there are rules for everything. People who memorize and enforce all of the rules on others get to operate a tiny little power trip while opening an opportunity to lecture internet strangers all day. Toddlers go through this sometimes around ages 2 or 3. They discover the \"rules\" for the first time and delight in brandishing them. reply Ferret7446 1 hour agorootparentprevThe reason those rules are created is because at some point something happened that necessitated that rule. (Not always of course, there are dictatorial mods.) The fundamental problem is that communities/forums (in the general sense, e.g., market squares) don't scale, period. Because moderation and (transmission and error correction of) social mores don't scale. reply Aurornis 1 hour agorootparent> The reason those rules are created is because at some point something happened that necessitated that rule. (Not always of course, there are dictatorial mods.) Maybe initially, but in the community I’m talking about rules are introduced to prevent situations that might offend someone. For example, the rule warning against using the word “blind” was introduced by someone who thought it was a good thing to do in case a person with vision issues maybe got offended by it at some point in the future. It’s a small group of people introducing the rules. Introducing a new rule brings a lot of celebration for the person’s thoughtfulness and earns a lot of praise and thanks for making the community safer. It’s turned into a meta-game in itself, much like how I feel when I navigate Stack Overflow reply lynx23 2 hours agoparentprevFull ACK. It has been liberating to be able to chat about a topic I always wanted to catch up on. And, even though I read a lot of apologies, at least nobody is telling me \"Thats not what you actually want.\" reply Vegenoid 2 hours agoparentprevYeah, Stackoverflow kinda dug their own grave by making their platform and community very unpleasant to engage with. reply lynx23 1 hour agorootparentWell, I believe the underlying problem of platforms like StackOverflow, ticketing systems (in-house and public) and even CRMs is not really solvable. The problem is, the quality of an answer is actually not easy to determine. All the mechanisms we have are hacks, and better solutions would need more resources... which leads to skewed incentives, and ultimately to a \"knwoledge\" db thats actually not very good. People are incentiviszed to collect karma points, or whatever it is. But these metrics are not really resembling the quality of their work... Crowdsourcing this mechanisms via upvotes or whatever does also not really work, because quantity is not quality... As said, I believe this is a problem we can not solve. reply beeboobaa3 3 hours agoparentprevI'm sorry but the funny thing is, the only people I've ever seen complain about SO are people who don't know how to search. reply wokwokwok 3 hours agorootparentEveryone has a pet theory about what’s wrong with SO; but here’s the truth: Whatever they’re doing, it isn’t working. Blame mods. Blame AI. Blame askers… whatever man. That is a sinking ship. If you don’t see people complain about SO, it’s because they aren’t using it, not because they’re using the search. Pretty hard to argue at this point that the problem is with the users being too shit to use the platform. That’s some high level BS. reply Ferret7446 1 hour agorootparentI submit that what SO is doing is working; it's just that SO is not what some people want it to be. SO is not a pure Q&A site. It is essentially a wiki where the contents are formatted as Q&As, and asking questions is merely a method to contribute toward this wiki. This is why, e.g., duplicates are aggressively culled. reply grogenaut 3 hours agorootparentprevI get good answers all the time on SO or used to. My problem is that I've been down voted several times for \"stupid question\" and also been down voted for not knowing what I was talking about in an area I'm an expert in. I had one question that was a bit odd and went against testing dogma that I had a friend post. He pulled it 30 minutes later as he was already down 30 votes. It was a thing that's not best practice in most cases but also in certain situations the only way to do it. Like when you're testing apis you don't control. In some sections people also want textbook or better quality answers from random strangers on the internet. The final part is that you at least used to have to build up a lot of karma to be able to post effectively or at all in some sections or be seen. Which is very catch 22. So it can be both very useful and very sh*t. reply fabian2k 2 hours agorootparent-30 votes would be extremely unusual on SO. That amount of votes even including upvotes in such a short time would be almost impossible. The only way you get that kind of massive voting is either if the question hits the \"Hot Network Questions\" or if an external site like HN with a high population of SO users links to it and drives lots of traffic. Questions with a negative score won't hit the hot network questions, so it seems very unlikely to me that it could be voted on that much. reply o11c 3 minutes agorootparentI don't think I've ever seen anything, no matter how bad, go below -5, and most don't go below -1. Once a question is downvoted: - it's less likely that the question even gets shown - it's less likely that people will even click on it - it's less likely that people who think it's bad will bother to vote on it, since the votes are already doing the right thing - if it's really bad, it will be marked for deletion before it gets that many downvotes anyway SO has its problems but I don't even recognize half the things people complain about. wizzwizz4 8 minutes agorootparentprevYou can get +30 from the HNQ list, but -30 is much harder, because the association bonus only gives you 101 rep, and the threshold for downvoting is 125. reply wwweston 3 hours agorootparentprevI get useful info from SO all the time, so often that these days it’s rare I have to ask a question. When I do, the issue seems to be it’s likely niche enough that an answer could take days or weeks, which is too bad, but fair enough. It’s also rare I can add an answer these days but I’m glad when I can. reply barbecue_sauce 3 hours agorootparentprevBut what problem is there with it? Most of the important questions have been answered already. reply wholinator2 3 hours agorootparentprevWe're talking about stackoverflow right? The website is a veritable gold mine of carefully answered queries. Sure, some people are shit, but how often are you unable to get at least some progress on a question from it? I find it useful in 90-95% of queries, i find the answers useful in 99% of queries that match my question. The thing is amazing! I Google search a problem, and there's 5 threads of people with comparable issues, even if no one has my exact error, the debugging and advice around the related errors is almost always enough to get me over the hump. Why all the hate? AI answers can suck, definitely. Stackoverflow literally holds the modern industry up. Next time you have a technical problem or error you don't understand go ahead and avoid the easy answers given on the platform and see how much better the web is without it. I don't understand, what kind questions do you have? reply mvdtnz 57 minutes agorootparentNobody is criticising the content that is on the site. The problem is an incredibly hostile user base that will berate you if you don't ask your question in the exact right way, or if you ask a question that implies a violation of some kind of best practice (for which you don't provide context because it's irrelevant to the question). As for the AI, it can only erode the quality of the content on SO. reply insane_dreamer 3 hours agoprevThe problem is eventually what are LLMs going’s to draw from? They’re not creating new information, just regurgitating and combining existing info. That’s why they perform so poorly on code for which there aren’t many many publicly available samples, SO/reddit answers etc. reply neither_color 1 hour agoparentI find that it sloppily goes back and forth between old and new methods, and as your LLM spaghetti code grows it becomes incapable of precision adding functions without breaking existing logic. All those tech demos of it instantly creating a whole app with one or a few prompts are junk. If you don't know what you're doing then as you keep adding features it WILL constantly switch up the way you make api calls(here's a file with 3 native fetch functions, let's install and use axios for no reason), the way you handle state, change your css library, etc. {/* rest of your functions here*} - DELETED After a while it's only safe for doing tedious things like loops and switches. So I guess our jobs are safe for a little while longer reply n_ary 2 hours agoparentprevLLMs show their limits as you try to ask something new(introduced in last 6-12 months) being not used. I was asking Claude and GPT4o about a new feature of go, it just gave me some old stuff from go docs. Then I went to go docs(official) and found what I was looking for anyways, the feature was released 2 major versions back, but somehow neither GPT4o nor claude know about this. reply SunlitCat 2 hours agorootparentWith GPT 4o I had some success pointing it to the current documentation of projects I needed and had it giving me current and actual answers. Like \"Help me to do this and that and use this list of internet resources to answer my questions\" reply finolex1 2 hours agoparentprevThere is still publicly available code and documentation to draw from. As models get smarter and bootstrapped on top of older models, they should need less and less training data. In theory, just providing the grammar for a new programming language should be enough for a sufficiently smart LLM to answer problems in that language. Unlike freeform writing tasks, coding also has a strong feedback loop (i.e. does the code compile, run successfully, and output a result?), which means it is probably easier to generate synthetic training data for models. reply layer8 2 hours agorootparent> In theory, just providing the grammar for a new programming language should be enough for a sufficiently smart LLM to answer problems in that language. I doubt it. Take a language like Rust or Haskell or even modern Java or Python. Without prolonged experience with the language, you have no idea how the various features interact in practice, what the best practices and typical pitfalls are, what common patterns and habits have been established by its practitioners, and so on. At best, the system would have to simulate building a number of nontrivial systems using the language in order to discover that knowledge, and in the end it would still be like someone locked in a room without knowledge of how the language is actually applied in the real world. reply stickfigure 2 hours agoparentprev> The problem is eventually what are LLMs going’s to draw from? Published documentation. I'm going to make up a number but I'll defend it: 90% of the information content of stackoverflow is regurgitated from some manual somewhere. The problem is that the specific information you're looking for in the relevant documentation is often hard to find, and even when found is often hard to read. LLMs are fantastic at reading and understanding documentation. reply Const-me 2 hours agorootparentThat is only true for trivial questions. I've answered dozens of questions on stackoverflow.com with tags like SIMD, SSE, AVX, NEON. Only a minority of these asked for a single SIMD instruction which does something specific. Usually people ask how to use the complete instruction set to accomplish something higher level. Documentation alone doesn't answer questions like that, you need an expert who actually used that stuff. reply irunmyownemail 2 hours agorootparentprevPublished documentation has been and can be wrong. In the late 1990's and early 2000's when I still did a mix of Microsoft technologies and Java, I found several bad non-obvious errors in MSDN documentation. AI today would likely regurgitate it in a soft but seemingly mild but arguably authoritative sounding way. At least when discussing with real people after the arrows fly and the dust settles, we can figure out the truth. reply Ferret7446 56 minutes agorootparentEverything (and everyone for that matter) can be and has been wrong. What matter is if it is useful. And AI as it is now is pretty decent at finding (\"regurgitating\") information in large bodies of data much faster than humans and with enough accuracy to be \"good enough\" for most uses. Nothing will ever replace your own critical thinking and judgment. > At least when discussing with real people after the arrows fly and the dust settles, we can figure out the truth. You can actually do that with AI now. I have been able to correct AI many times via a Socratic approach (where I didn't know the correct answer, but I knew the answer the AI gave me was wrong). reply roughly 2 hours agorootparentprevYeah, this is wildly optimistic. From personal experience, I'm skeptical of the quantity and especially quality of published documentation available, the completeness of that documentation, the degree to which it both recognizes and covers all the relevant edge cases, etc. Even Apple, which used to be quite good at that kind of thing, has increasingly effectively referred developers to their WWDC videos. I'm also skeptical of the ability of the LLMs to ingest and properly synthesize that documentation - I'm willing to bet the answers from SO and Reddit are doing more heavy lifting on shaping the LLM's \"answers\" than you're hoping here. There is nothing in my couple decades of programming or experience with LLMs that suggests to me that published documentation is going to be sufficient to let an LLM produce sufficient quality output without human synthesis somehwere in the loop. reply elicksaur 2 hours agorootparentprevFollowing the article’s conclusion farther, humans would stop producing new documentation with new concepts. reply mycall 3 hours agoparentprevI thought synthetic data is what is partially training the new multimodal large models, i.e. AlphaGeometry, o1, etc. reply y7 2 hours agorootparentSynthetic data can never contain more information than the statistical model from which it is derived: it is simply the evaluation of a non-deterministic function on the model parameters. And the model parameters are simply a function of the training data. I don't see how you can \"bootstrap a smarter model\" based on synthetic data from a previous-gen model this way. You may as well well just train your new model on the original training data. reply antisthenes 2 hours agorootparentprevSynthetic data without some kind of external validation is garbage. E.g. you can't just synthetically generate code, something or someone needs to run it and see if it performs the functions you actually asked of it. You need to feed the LLM output into some kind of formal verification system, and only then add it back to the synthetic training dataset. Here, for example - dumb recursive training causes model collapse: https://www.nature.com/articles/s41586-024-07566-y reply HPsquared 12 minutes agorootparentThere are definitely a lot of wrong ways to do it. Doesn't mean the basic idea is unsound. reply jneagu 2 hours agorootparentprevAnecdotally, synthetic data can get good if the generation involves a nugget of human labels/feedback that gets scaled up w/ a generative process. reply jneagu 2 hours agorootparentprevYeah, There was a reference in a paywalled article a year ago (https://www.theinformation.com/articles/openai-made-an-ai-br...): \"Sutskever's breakthrough allowed OpenAI to overcome limitations on obtaining high-quality data to train new models, according to the person with knowledge, a major obstacle for developing next-generation models. The research involved using computer-generated, rather than real-world, data like text or images pulled from the internet to train new models.\" I suspect most foundational models are now knowingly trained on at least some synthetic data. reply epgui 2 hours agoparentprevIn a very real sense, that’s also how human brains work. reply elicksaur 2 hours agorootparentThis argument always conflates simple processes with complex ones. Humans can work with abstract concepts at a level LLMs currently can’t and don’t seem likely capable of. “True” and “False” are the best examples. reply epgui 2 hours agorootparentIt doesn’t conflate anything though. It points to exactly that as a main difference (along with comparative functional neuroanatomy). It’s helpful to realize the ways in which we do work the same way as AI, because it gives us perspective unto ourselves. (I don’t follow regarding your true and false statement, and I don’t share your apparent pessimism about the fundamental limits of AI.) reply empath75 2 hours agoparentprevAI companies are already paying humans to produce new data to train on and will continue to do that. There's also additional modalities -- they've already added text, video, and audio, and there's probably more possible. Right now almost all the content being fed into these AIs is stuff that humans can sense and understand, but why does it have to limit itself to that? There's probably all kinds of data types it could train on that could give it more knowledge about the world. Even limiting yourself to code generation, there are going to be a lot of software developers employed to write or generate code examples and documentation just for AIs to ingest. I think eventually AIs will begin coding in programming languages that are designed for AI to understand and work with and not for people to understand. reply imoverclocked 2 hours agorootparent> AI companies are already paying humans to produce new data to train on and will continue to do that. The sheer difference in scale between the domain of “here are all the people in the world that have shared data publicly until now” and “here is the relatively tiny population of people being paid to add new information to an LLM” dooms the LLM to become outdated in an information hoarding society. So, the question in my mind is, “Why will people keep producing public information just for it to be devalued into LLMs?” reply jneagu 3 hours agoparentprevEdit: OP had actually qualified their statement to refer to only underrepresented coding languages. That's 100% true - LLM coding performance is super biased in favor of well-represented languages, esp. in public repos. Interesting - I actually think they perform quite well on code, considering that code has a set of correct answers (unlike most other tasks we use LLMs for on a daily basis). GitHub Copilot had a 30%+ acceptance rate (https://github.blog/news-insights/research/research-quantify...). How often does one accept the first answer that ChatGPT returns? To answer your first question: new content is still being created in an LLM-assisted way, and a lot of it can be quite good. The rate of that happening is a lot lower than that of LLM-generated spam - this is the concerning part. reply generic92034 2 hours agorootparentThe OP has qualified \"code\" with bad availability of samples online. My experience with LLMs on a proprietary language with little online presence confirms their statement. It is not even worth trying, in many cases. reply jneagu 2 hours agorootparentFair point - I actually had parsed OP's sentence differently. I'll edit my comment. I agree, LLMs performance for coding tasks is super biased in favor of well-represented languages. I think this is what GitHub is trying to solve with custom private models for Copilot, but I expect that to be enterprise only. reply okoma 4 hours agoprevThe authors claim that LLM are reducing public knowledge sharing and that the effect is not merely displacing duplicate, low-quality, or beginner-level content. However their claim is weak and the effect is not quite as sensational as they make it sound. First, they only present Figure 3 and not regression results for their suggested tests of LLMs being substitutes of bad quality posts. In contrast, they report tests for their random qualification by user experience (where someone is experienced if they posted 10 times). Now, why would they omit tests by post quality but show results by a random bucketing of user “experience”? Second, their own Figure 3 “shows” a change in trends for good and neutral questions. Good questions were downtrending and now they are flat, and neutral questions (arguably the noise) went from an uptrend to flat. Bad question continue to go down, no visible change in the trend. This suggests the opposite, ie that LLMs are in fact substituting bad quality content. I feel the conclusion needed a stronger statement and research doesn’t reward meticulous but unsurprising results. Hence the sensational title and the somewhat redacted results. reply BolexNOLA 3 hours agoparentWhile this article doesn’t really seem to be hitting what I am about to say, I think someone on HN a while back described a related phenomenon (which leads to the same issue) really well. The Internet is Balkanizing. This is hardly a new concept but they were drilling down specifically into online communities. People are electing to not freely share information on public forums like they used to. They are retreating into discord and other services where they can put down motes and raise the draw bridges. And who can blame them? So many forums and social media sites and forums are engaging in increasingly hostile design and monetization processes, AI/LLM’s are crawling everywhere vacuuming up everything then putting them behind paywalls and ruining the original sources’ abilities to be found in search, algorithms designed to create engagement foster vitriol and controversy, the list goes on. HN is a rare exception these days. So what happens? A bunch of people with niche interests or knowledge sets congregate into private communities and only talk to each other. Which makes it harder for new people to join. It’s a sad state of affairs if you ask me. reply Simran-B 2 hours agorootparentYes, it's sad. On the other hand, I think it's a good thing that people share knowledge less, publicly and free of charge on the web, because there is so much exploitation going on. Big corporations obviously capitalize on the good will of people with their LLMs, but there are also others who take advantage of the ones who want to help. A lot of users seemingly expect others to solve their problems for free and don't even put any effort into asking their questions. It's a massive drain for energy and enthusiasm, some even suffer from burnout (I assume more in open-source projects than on SO but still). I rather want it to be harder to connect with people sharing the same passion \"in private\" than having outsider who don't contribute anything profit off of activities happening in the open. This frustratingly appears to become the main reason for corporate open source these days. reply verdverm 4 hours agoprevFor me, many of my questions about open source projects have moved to GitHub and Discord, so there is platform migration besides LLMs. I also tend to start with Gemini for more general programming things, because it will (1) answer in the terms of my problem instead of me having to visit multiple pages to piece it together, or (2) what it's wrong, I often get better jump off points when searching. Either way, LLMs save me time instead of having to click through to SO multiple times because the title is close but the content as an important difference reply joshdavham 4 hours agoparent> many of my questions about open source projects have moved to GitHub and Discord Exact same experience here. Plus, being able to talk to maintainers directly has been great! reply klabb3 1 hour agorootparentNo doubt that discord has struck a good balance. Much better than GitHub imo. Both for maintainers to get a soft understanding of their users, and equally beneficial for users who can interact casually without being shamed for filing an issue the wrong way. There’s some weird blind spot with techies who are unable to see the appeal. UX matters in a “the medium is the message”-kind of way. Also, GitHub is only marginally more open than discord. It’s indexable at the moment, yes, but would not surprise me at all if MS is gonna make an offensive move to protect “their” (read our) data from AI competitors. reply verdverm 1 hour agorootparentChat is an important medium, especially as new generations of developers enter the field (they are more chat native). It certainly offers a more comfortable, or appropriate place, to ask beginner questions, or have quick back-n-forths, than GitHub issues/discussions offers. I've always wondered why GH didn't incorporate chat, seems like a big missed opportunity. reply baq 3 hours agoparentprev2022: Discord is not indexed by search engines, it sucks 2024: Discord is not indexed by AI slop generators, it's great reply throwaway918299 23 minutes agorootparentDiscord stores trillions of messages. If they haven’t figured out how to make a slop generator out of it yet, I’m sure it’s coming soon. reply verdverm 1 hour agorootparentprevIt's more that Discord is replacing Slack as the place where community happens. Less about about indexing, which still sucks even in Discord search. Slack/Salesforce threw a lot of small projects under the bus, post-acquisition, with the reductions to history from message count to 90 days reply rkncland 5 hours agoprevOf course people reduce their free contributions to Stackoverflow. Stackoverflow is selling then out with the OpenAI API agreement and countless \"AI\" hype blog posts. reply jeremyjh 5 hours agoparentI think this is more about a drop in questions, than a drop in answers. reply bryanrasmussen 4 hours agorootparentI mean part of the reason to not ask about stuff on SO, there are several types of questions that one might like to ask - such as: I don't know the first thing about this thing, help me get to where I know the first thing. This is not allowed any more. I want to know the pros and cons of various things compared. this is not allowed. I have quality questions regarding an approach that I know how to do, but I want to know better ways. This is generally not allowed but you might slip through if you ask it just right. I pretty much know really well what I'm doing but having some difficulty finding the right documentation on some little thing,help me - this is allowe Something does not work as per the documentation, help me, this is allowed I think I have done everything right but it is not working, this is allowed and is generally a typo or something that you have put in the wrong order because you're tired. At any rate, the ones that are not allowed are the only questions that are worth asking. The last two that is allowed I generally find gets answered in the asking - I'm pretty good in the field I'm asking in, the rigor of making something match SO question requirements leads me to the answer. If I ask one of the interesting disallowed questions and get shit on then I will probably go through a period of screw it, I will just look extra hard for the documentation before I bother with that site again. reply jakub_g 3 hours agorootparentI can see how frustrating it might be, but the overall idea of SO is \"no duplicates\". They don't want to have 1000 questions which are exactly the same but with slightly different phrasing. It can be problematic for total newcomers, but at the same time it makes it more useful for professionals: instead of having 1000 questions how to X with 1 reply, you have one canonical question with 20 replies sorted by upvotes and you can quickly see which one is likely the best. FWIW, I found LLMs to be actually really good at those basic questions where I'm at expert at language X and I ask how to do similar thing in Y, using Y's terms (which might be named differently in X). I believe this actually would work well: - extra basic things, or things that depend on opinion etc: ask LLMs and let they infer and steer you - advanced / off the beaten path questions that LLMs hallucinate on: ask on SO reply noirscape 3 hours agorootparentThe problem SO tends to run into is when you have a question that seems like it answers another question on the surface (ie. because the question title is bad) and then a very different question is closed with the dupe reason pointing to that question because the close titles are similar. Since there's no way to appeal duplicate close votes on SO until you have a pretty large amount of rep, this kinda creates a problem where there's a \"silent mass\" of duplicate questions that aren't really duplicates. A basic example is this question: https://stackoverflow.com/q/27957454 , which is about disabling PRs on GitHub on the surface. The body text however reveals that the poster is instead asking how they can set up branch permissions and get certain accounts to bypass them. I can already assure you that just by basic searching, this question will pop up first when you look up disabling PRs, and the accepted answer answers the question body (which means that it's almost certain a different question has been closed as a duplicate of this one), rather than the question title. You could give a more informative answer (which kinda happened here), but this is technically off-topic to the question being closed. That's where SO gets it's bad rep for inaccurate duplicate closing from. reply bryanrasmussen 3 hours agorootparentprev>I can see how frustrating it might be It's certainly not frustrating for me, I ask a question maybe once a year on SO, most of their content is, in my chosen disciplines, not technically interesting, it is no better than looking up code snippets in documentation (which most of the time is what it really, really is) I suppose it's frustrating for SO that people no longer find it worthwhile to ask questions there. >advanced / off the beaten path show me an advanced and off the beaten path question that SO has answered well, that is just not worth the effort to try to get an answer - if you have an advanced and off the beaten path question that you can't answer then you ask it on SO just \"in case\" but really you will find the answer somewhere else or not at all in my experience. reply Izkata 1 hour agorootparentprev> I don't know the first thing about this thing, help me get to where I know the first thing. This is not allowed any more. This may have been allowed in like the first year while figuring out what kind of moderation worked, but it hasn't been as least since I started using it in like 2011. They just kept slipping through the cracks because so many questions are constantly being posted. reply Ferret7446 48 minutes agorootparentprevThe problem is that SO is not a Q&A site although it calls itself that (which is admittedly misleading). It is a community edited knowledgebase, basically a wiki, where the content is Q&As. It just so happens that one method of contributing to the site is by writing questions for other people to write answers to. If you ask a question (i.e., add content to the wiki) that is not in scope, then of course it will get removed. reply SoftTalker 4 hours agorootparentprevThe first one especially is not interesting except to the person asking the question, who wants to be spoon-fed answers instead of making any effort of his own to acquire foundational knowledge. Often these are students asking for someone to solve their homework problems. Pro/Con questions are too likely to involve opinion and degenerate into flamewars. Some could be answered factually, but mostly are not. Others have no clear answers. reply bryanrasmussen 3 hours agorootparentthank you for bringing the default SO reasons why these are not the case, but first off >Often these are students asking for someone to solve their homework problems. I don't think I've been in any class since elementary school in which I did not have foundational knowledge, I'm talking \"I just realized there must be a technical discipline that handles this issue and I can't google my way to it level of questions.\" If I'm a student, I have a textbook and the ability to read. I'm not asking textbook readable or relevant literature readable in the thing I am studying questions because I, being in a class on the subject I would \"know the first thing\" to quote my earlier post, that first thing being how to get more good and relevant knowledge on the thing I am in a class in. I'm talking about things you don't even know what questions to ask to get that foundational knowledge which is among the most interesting questions to ask - the problem with SO is it only wants me to ask questions in a field in which I am already fairly expert but I have just hit a temporary stumbling block for some reason. I remember when I was working on a big government security project and there was a Java guy who was an expert in a field that I knew nothing about and he would laugh and say you can't go to SO and ask about how do I ... long bit of technical jargon outside my field that I sort of understood hung together, maybe eigenvectors came up (this was in 2013) Second thing, yes I know SO does not want people to ask non-factual questions, and it does not want me to ask questions in fields in which I am uninformed, so it follows it wants me to ask questions that I can probably find out myself one way or another, only SO is more convenient. I gave some reasons why I do not find SO particularly convenient or useful given their constraints implying this is probably the same for others, you said two of my reasons were no good, but I notice you did not have any input on the underlying question of - why are people not asking as many questions on SO as they once did? reply Ferret7446 44 minutes agorootparent> I don't think I've been in any class since elementary school in which I did not have foundational knowledge > If I'm a student, I have a textbook and the ability to read You are such an outlier that I don't think you have the awareness to make any useful observations on this topic. Quite a lot of students in the US are now starting to lack the ability to read, horrifyingly (and it was never 100%), and using ChatGPT to do homework is common. reply SoftTalker 3 hours agorootparentprevSO is what it is, they have made the choices they made as to what questions are appropriate on their platform. I don't know why SO questions are declining -- perhaps people find SO frustrating, as you seem to, and they give up. I myself have never posted a question on SO as I generally have found that my questions had already been asked and answered. And lately, perhaps LLMs are providing better avenues for the sorts of questions you describe. That seems very plausible to me. reply fforflo 2 hours agoprevWell, we know we'll have reached AGI when LLM says \"this chat has been marked as duplicate\" reply p0w3n3d 52 minutes agoprevThat's what I've been predicting and scared of: LLMs learn from online Q&A platforms, but people already stop posting questions and receiving answers. The sole knowledge sources will get poisoned with inaccurate LLM generated data, and therefore the entropy available to LLMs will become damped by the LLMs itselves (in a negative feedback loop) reply Havoc 4 hours agoprevI’d imagine they also narrow the range of knowledge and discourse in general. A bit like if you ask an LLM to tell you a joke they all tend to go with the same one reply MASNeo 5 hours agoprevWondering about wider implications. If technical interactions reduce online, how about RL and how do we rate a human competence against an AI once society gets a habit from asking an AI first? Will we start to constantly question human advice or responses and what does that do to the human condition. I am active in a few specialized fields and already I have to defined my advice against poorly crafted prompt responses. reply VancouverMan 1 hour agoparent> Will we start to constantly question human advice or responses and what does that do to the human condition. I'm surprised when people don't already engage in questioning like that. I've had to be doing it for decades at this point. Much of the worst advice and information I've ever received has come from expensive human so-called \"professionals\" and \"experts\" like doctors, accountants, lawyers, financial advisors, professors, journalists, mechanics, and so on. I now assume that anything such \"experts\" tell me is wrong, and too often that ends up being true. Sourcing information and advice from a larger pool of online knowledge, even if the sources may be deemed \"amateur\" or \"hobbyist\" or \"unreliable\", has generally provided me with far better results and outcomes. If an LLM is built upon a wide base of source information, I'm inclined to trust what it generates more than what a single human \"professional\" or \"expert\" says. reply bloomingkales 5 hours agoprevGuess we need an Agent that logs and re-contributes to Stackoverflow (for example) automatically. Then also have agents that automatically give upvotes for used solutions. Weird world. I’m just imagining the precogs talking to each other in Minority Report if that makes sense. reply Abecid 52 minutes agoprevI think this is just the future though. Why ask other people if LLMs can just retrieve, read, and train on official documentations reply jetsetk 24 minutes agoparentOfficial documentations are not always complete. It depends on the diligence of who wrote them and how good they are at writing. Customers and users will always send mails or open tickets to ask this and that about the docs afterwards. Can't rely on just learning or retrieving from the docs. Clarifications by some dev or someone who found a solution/workaround will always be required. reply atomic128 4 hours agoprevEventually, large language models will be the end of open source. That's ok, just accept it. Large language models are used to aggregate and interpolate intellectual property. This is performed with no acknowledgement of authorship or lineage, with no attribution or citation. In effect, the intellectual property used to train such models becomes anonymous common property. The social rewards (e.g., credit, respect) that often motivate open source work are undermined. That's how it ends. reply zmgsabst 3 hours agoparentWhy wouldn’t you use LLMs to write even more open source? The cost of contributions falls dramatically, eg, $100 is 200M tokens of GPT-3.5; so you’re talking enough to spend 10,000 tokens developing each line of a 20kloc project (amortized). That’s a moderate project for a single donation and an afternoon of managing a workflow framework. reply atomic128 3 hours agorootparentWhat you're describing is \"open slop\", and yes, there will be a lot of it. Open source as we know it today, not so much. reply yapyap 4 hours agoparentprevno it won’t, it’ll just make it more niche than it already is. reply atomic128 5 minutes agorootparentLLM users are feeding their precious entropy into the model, and paying for that privilege. LLM users are producing the new anonymous common property. Quoting David Cronenberg's movie Videodrome: \"Death to Videodrome! Long live the new flesh!\" reply optimiz3 5 hours agoprevIf a site aims to commoditize shared expertise, royalties should be paid. Why would anyone willingly reduce their earning power, let alone hand away the right for someone else to profit from selling their knowledge, unattributed no less. Best bet is to book publish, and require a license from anyone that wants to train on it. reply afh1 5 hours agoparentWhy open source anything, let alone with permissive licensing, right? reply optimiz3 5 hours agorootparentTo a degree, yes. I only open source work where I expect reciprocal value from other contributions. reply johannes1234321 4 hours agorootparentThere is a lot of indirect hardly measurable value one can gain. Going back to the original source: By giving an answer to somebody on a Q&A site, they might be a kid learning and then building solutions I benefit from later, again. Similar with software. And I also consider the total gain of knowledge for our society at large a gain. While my marginal cost form many things is low. And often lower than a cost-benefit calculation. And some Q&A questions strike a nerve and are interesting to me to answer (be it in thinking about the problem or in trying to boiling it down to a good answer), similar to open source. Some programming tasks as fun problems to solve, that's a gain, and then sharing the result cost me nothing. reply benfortuna 5 hours agorootparentprevI think that is antithetical to the idea of Open Source. If you expect contributions then pay a bounty, don't pretend. reply optimiz3 5 hours agorootparentThe bounty is you getting to use my work (shared in good faith no less). Appreciate the charity and don't be a freeloader or you'll get less in the future. reply andrepd 5 hours agorootparentprevGPL is antithetical to open source? Odd take reply verdverm 4 hours agorootparentThere is a permissionless (MIT) vs permissioned (GPL) difference that is at the heart of the debate of what society thinks open source should mean reply Y_Y 5 hours agorootparentprevSee also: BSD vs. GPL reply jncfhnb 5 hours agoparentprevBecause it’s a marginal effect on your earning power and it’s a nice thing to do. reply optimiz3 5 hours agorootparentThe management of these walled gardens will keep saying that to your face as they sell your contributions. Meanwhile your family gets nothing. reply jncfhnb 3 hours agorootparentDid your family get anything from you sharing this opinion? If not, why did you share it? Are you suggesting that your personal motivations for posting this cynicism are reasonable but that similar motivations that are altruistic for helping someone are not? reply optimiz3 2 hours agorootparentSharing this opinion doesn't sacrifice my primary economic utility, and in fact disseminates a sentiment that if more widespread would empower everyone to realize more of the value they offer. Please do train an LLM to inform people to seek licensing arrangements for the expertise they provide. reply jncfhnb 2 hours agorootparentThat’s just dumb, man. You’re not sacrificing anything by giving someone a helpful answer. reply AlexandrB 5 hours agorootparentprev\"It's a nice thing to do\" never seems to sway online platforms to treat their users better. This kind of asymmetry seems to only ever go one way. reply falcor84 4 hours agorootparentAs a mid-core SO user (4 digit reputation), I never felt like I needed them to treat me better. I always feel that while I'm contributing a bit, I get so much more value out of SO than what I've put in, and am grateful for it being there. It might also have something to do with me being old enough to remember the original expertsexchange, as well as those MSDN support documentation CDs. I'm much happier now. reply wwweston 3 hours agoparentprevWhen the jobs side of SO was active, it effectively did this. Strong answers and scoring were compensated with prospective employer attention. For a few years, this was actually where the majority of my new job leads came from. It was a pretty rewarding ecosystem, though not without its problems. Not sure why they shut down jobs; they recently brought back a poorer version of it. reply malicka 4 hours agoparentprevWhile there is a thing to be said about the unethical business practices of Quora/StackOverflow, I reject the framing of “reducing your earning power.” Not everything is about transactions or self-benefit, especially when it comes to knowledge; it’s about contributing and collaboration. There is immense intrinsic value to that. I’m glad we don’t live in your world, where libre software is a pipe-dream and hackers hoard their knowledge like sickly dragons. reply simonw 4 hours agoparentprev... you just shared your expertise here on Hacker News in the form of this comment without any expectation of royalties. How is posting on StackOverflow different? reply krtalc 4 hours agorootparentOne could answer that question to people whose salary does not depend upon not understanding the answer. reply rq1 4 hours agoprevPeople should just share their conversations with the LLMs online no? This would blogging 5.0. Or web 7.0. reply SunlitCat 2 hours agoparentWell, I just asked ChatGPT to answer my \"How to print hello world in c++\" with a typical stack overflow answer. Lo and behold, the answer is very polite, explanative and even lists common mistakes. It even added two very helpful user comments! I asked it again how this answer would look in 2024 and it just updated the answer to the latest c++ standard! Then! I asked it what a moderator would say when they chime in. Of course the moderator reminded everyone to stay on focus regarding the question, avoid opinions and back their answer by documentation or standards. In the end the mod thanked for everyone's contribution and keeping the discussion constructive! Ah! What a wonderful world ChatGPT is living at! I want to be there too! reply qntmfred 4 hours agoparentprevThat's pretty much what my youtube channel is turning into. just me talking to myself with chatgpt as co-host eg https://www.youtube.com/watch?v=kB59Bz-F04E reply joshdavham 4 hours agoprevWith that being said, I imagine the quality of the questions have also improved quite a bit. I definitely don’t condone the rude behaviour on SO, but I also understand that the site used to be bombarded constantly with low quality questions that now thankfully LLMs can handle. reply vitiral 5 hours agoprevWe need to refine our tech stack to create a new one which is understandable by humans, before LLMs pollute our current stack to the point it's impossible to understand or modify. That's what I'm doing at https://lua.civboot.org reply mrcino 5 hours agoprevBy Public knowledge sharing, do they mean bazillions of StackOverflow duplicates? reply knotimpressed 5 hours agoparentThe article mentions that all kinds of posts were reduced, not just duplicates or even simple questions. reply gigatexal 2 hours agoprevBecause toxic but well meaning mods at stack overflow made us not want to use them anymore. reply kajaktum 4 hours agoprevI have no idea where to ask questions nowadays. Stackoverflow is way \"too slow\" (Go to website, write a nice well formatted thread, wait for answers). But there's way faster solutions now, namely from message groups. For example, I was wondering if its okay to move my home directory to a different filesystem altogether and create a symlink from /home/. Where do I ask such questions? The freaking ZFS mailing list? SO? It was just a passerby question, and what I wanted more than the answer is the sense of community. The only place that I know that have a wide enough range of interest, with many people that each know some of these stuff quite deep, is public, is easily accessible is unironically 4chan /g/. I would rather go there then Discord where humanity's knowledge will be piped to /dev/null. reply CoastalCoder 3 hours agoparentI guess I'm out of the loop. What does \"/g/\" mean? reply aezart 3 hours agorootparentIt's the technology message board on 4chan, each board has a name like that. /a/ for anime, /v/ for video games, etc. reply scotty79 6 hours agoprevDon't they just reduce the Q part of Q&A? And since the Q was A-d by AI doesn't that mean that A was there already and people just couldn't find it but AI did? reply lordgrenville 6 hours agoparentThe answer by humans is a) publicly accessible b) hallucination-free (although it still may not be correct) c) subject to a voting process which gives a good signal of how much we should trust it. Which makes me think, maybe a good move for Stack Overflow (which does not allow the submission of LLM-generated answers, wisely imo) would be to add an AI agent that would suggest an answer for each question, that people could vote on. That way you can elicit human and machine answers, and still have the verification process. reply david-gpu 5 hours agorootparentAs a user, why would I care whether an answer is \"incorrect\" or \"hallucinated\"? Neither one is going to solve the problem I have at hand. It sounds like a distinction without a difference. reply lordgrenville 5 hours agorootparentOne relevant difference is that a better-quality human answer is correlated with certain \"tells\": correct formatting and grammar, longer answers, higher reputation. An incorrect LLM answer looks (from the outside) exactly the same as a correct answer. reply mikepurvis 5 hours agorootparentprevObviously there are exceptions but human-wrong answers tend to be more subtly wrong whereas hallucinated answers are just baffling and nonsensical. reply Davidzheng 5 hours agorootparentprevI don't think human mistakes are distinguishable from hallucinations. reply Y_Y 5 hours agorootparentLet's train a discriminator and see! reply intended 5 hours agorootparentprevWhy a vote? Voting != Verification. reply TeMPOraL 6 hours agorootparentprevLLMs are much better experience on the \"Q side\". Sure, there's the occasional hallucination here and there, but QnA sites are not all StackOverflow. Most of them are just content farms for SEO and advertising purposes - meaning, the veracity of the content doesn't matter, as long as it's driving clicks. At this moment, this makes LLMs much more trustworthy. reply scotty79 6 hours agorootparentprevIt's a good idea but probably not easy to implement. SO answers are usually quite neat, like an email. Solving a problem with ChatGPT is more like ... chat. It's hard to turn it into something googlable and Google is how SO gets most of its traffic and utility. reply jmyeet 6 hours agoprevIt's a losing battle to try and maintain walled gardens for these corpuses of human-generated text that have become valuable to train LLMs. The horse has probably already bolted. I see this as a temporary problem however because LLMs are transitional. At some point it won't be necessary to train an LLM on the entirety of Reddit plus everything else ever written because there are obvious limits to statistical models like this and, as a counter point, that's not how humans learn. You may have read hundres of books in your life, maybe even thousands. You haven't read a million. You don't need to. I find it interesting that this issue (which is theft, to be clear) is being framed as theft from the site or company that \"owns\" that data, rather than theft from the users who created it. All these user-generated content (\"UGC\") sites are doomed to eventually fail because their motivations diverge from their users and the endless quest to increase profits inevitably drives users away. Another issue is how much IP consumption constitutes theft? If an LLM watches every movie ever made, that's probably theft. But how many is too many? Like Apocalypse Now was loosely based on or at least inspired by Heart of Darkness (the novel). Yet you can't accuse a human of \"theft\" by reading Heart of Darkness. All art is derivative, as they say. reply vlovich123 5 hours agoparent> At some point it won't be necessary to train an LLM on the entirety of Reddit plus everything else ever written because there are obvious limits to statistical models like this and, as a counter point, that's not how humans learn. You may have read hundres of books in your life, maybe even thousands. You haven't read a million. You don't need to. I agree but I think it may be privileging the human intelligence mechanism a bit too much. These LLMs are polymaths that can spit out content at a super human rate. It can generate poetry and literature similarly to code and answers about physics and car repair. It’s very rare for a human to be able to do that especially these days. So I agree they’re transitional but only in the sense that our brains are transitional from the basal ganglia to the neocortex. In that sense I think LLMs will probably be a part of a future GAI brain with other things tracked on, but it’s not clear it will necessarily evolve to work like a human’s brain does. reply jprete 5 hours agorootparentI think the actual reason people can't do it is that we avoid situations with high risk and no apparent reward. And we aren't sufficiently supportive of other people doing surprising things (so there's no reward for trying). I.e. it's a modern culture problem, not a human brain problem. reply jmyeet 5 hours agorootparentprev> These LLMs are polymaths that can spit out content at a super human rate. Do you mean in theory or currently? Because currently, LLMs make simple errors (eg [1]) and are more capable of spitting out, well, nonsense. I think it's safe to say we're a long way from LLMs producing anything creatively good. I'll put it this way: you won't be getting The Godfather from LLMs anytime soon but you can probably get an industrial film with generic music that tells you how to safely handle solvents, maybe. Computers are generally good at doing math but LLMs generally aren't [2] and that really demonstrates the weaknesses in this statistical approach. ChatGPT (as one example) doesn't understand what numbers are or how to multiply them. It relies seeing similar answers to derive a likely answer so it often gets the first and large digits of the answer correct but not the middle. You can't keep scaling the input data to have it see every possible math question. That's just not practical. Now multiplying two large numbers is a solvable problem. Counting Rs in strawberry is a solvable problem. But statistical LLMs are going to have a massive long tail of these problems. It's really going to take the next generational change to make progress. [1]: https://www.inc.com/kit-eaton/how-many-rs-in-strawberry-this... [2]: https://www.reachcapital.com/2024/07/16/why-llms-are-bad-at-... reply simonw 4 hours agorootparentBoth the \"count the Rs in strawberry\" and the \"multiply two large numbers\" things have been solved for over a year now by the tool usage pattern: give an LLM the ability to delegate to a code execution environment for things it's inherently bad at and train it how to identify when to use that option. reply shagie 1 hour agorootparent(I did an earlier attempt at this with a \"ok, longer conversation\" ... and then did a \"well, what if I just asked it directly?\") https://chatgpt.com/share/670bfdbd-8624-8011-bc31-2ba66eab3e... I didn't realize that it had come that far with the delegating of those problems to the code writing and executing part of itself. reply vlovich123 4 hours agorootparentprevI think the point is that playing whack a mole is an effective practical strategy to shore up individual weaknesses (or even classes of weaknesses) but that doesn’t get you to general reasoning unless you think that intelligence evolved this way. Given the adaptability of intelligence across the animal kingdom to novel environments never seen before, I don’t think that can be anything other than a short term strategy for AGI. reply simonw 2 hours agorootparentSure, LLMs won't ever get to general reasoning (for pick your definition of \"reasoning\") unassisted. I think that adding different forms of assistance remains the most interesting pattern right now. reply vlovich123 5 hours agorootparentprevI think we’re in agreement. It’s going to take next generation architecture to address the flaws where the LLM often can’t even correct its mistake when it’s pointed out as with the strawberry example. I still think transformers and LLMs will likely remain as some component within that next gen architecture vs something completely alien. reply 0x1ceb00da 5 hours agoparentprev> You may have read hundres of books in your life, maybe even thousands. You haven't read a million. You don't need to. Sometimes online forums are the only place where you can find solutions for niche situations and edge cases. Tricks which would have been very difficult to figure out on your own. LLMs can train on the official documentation of tools l/libraries but they can't experiment and figure out solutions to weird problems which are unfortunately very common in tech industry. If people stop sharing such solutions with others, it might become a big problem. reply simonw 4 hours agorootparent\"LLMs can train on the official documentation of tools l/libraries but they can't experiment and figure out solutions to weird problems\" LLMs train on way more than just the official documentation: they train on the code itself, the unit tests for that code (which, for well written projects, cover all sorts of undocumented edge-based) and - for popular projects - thousands of examples of that library being used (and unit tested) \"in the wild\". This is why LLMs are so effective at helping figure out edge-cases for widely used libraries. The best coding LLMs are also trained on additional custom examples written by humans who were paid to build proprietary training data for those LLMs. I suspect they are increasingly trained on artificially created examples which have been validated (to a certain extent) through executing that code before adding it to the training data. That's a unique advantage for code - it's a lot harder to \"verify\" non-code generated prose since you can't execute that and see if you get an error. reply 0x1ceb00da 4 hours agorootparent> they train on the code itself, the unit tests for that code If understanding the code was enough, we wouldn't have any bugs or counterintuitive behaviors. > and - for popular projects - thousands of examples of that library being used (and unit tested) \"in the wild\". If people stopped contributing to forums, we won't have any such data for new things that are being made. reply simonw 2 hours agorootparentThe examples I'm talking about come from openly licensed code in sources like GitHub, not from StackOverflow. I would argue that code in GitHub is much more useful, because it's presented in the context of a larger application and is also more likely to work. reply skydhash 4 hours agorootparentprev> Sometimes online forums are the only place where you can find solutions for niche situations and edge cases. That's the most valuable aspect of it. When you find yourself in these niches situations, it's nice when you see someone has encountered it and has done the legwork to solve it, saving you hours and days. And that's why Wikis like the Arch Wiki are important. You need people to document the system, not just individual components. reply falcor84 4 hours agoparentprev> that's not how humans learn I've been thinking about this a lot lately. Could we train an AI, e.g. using RL and GAN, where it gets an IT task to perform based on a body of documentation, such that its fitness would then be measured based on both direct success on the task, and on the creation of new (distilled and better written) documentation that would allow an otherwise context-less copy of itself to do well on the task? reply jumping_frog 4 hours agoparentprevJust to add to your point, consider a book like \"Finite and Infinite\" games. I think I can \"recreate\" the knowledge and main thesis in the book by my readings from other areas. 'Listening to different spiritual gurus saying the same thing using different words' is like 'watching the same coloured glass pieces getting rearranged to form new patterns in kaleidoscope' reply szundi 5 hours agoparentprevOnly half true as maybe reasoning and actual understanding is not the strength of LLMs but it is fascinating that they actually can produce good info from everything they have read - unlike me who only read a fraction of that. Maybe dumb, but good memory. So I think future AI has to read also everything if it is used like ChatGPT these days by average people to ask for advice about almost anything. reply airstrike 5 hours agoparentprev> (which is theft, to be clear) > Another issue is how much IP consumption constitutes theft? If an LLM watches every movie ever made, that's probably theft. It's hard to reconcile those two views, and I don't think theft is defined by \"how much\" is being stolen. reply Artgor 6 hours agoprevWell, if the users ask frequent/common questions to ChatGPT and get acceptable answers, is this even a problem? If the volume of duplicate questions decreases, there should be no bad influence on the training data, right? reply jeremyjh 5 hours agoparentThey spoke to this point in the abstract. They observe a similar drop in less common and more advanced questions. reply timhh 6 hours agoprev [–] Stackoverflow mods and power users being arseholes reduces the use of Stackoverflow. ChatGPT is just the first viable alternative. reply tomrod 5 hours agoparentWhile not exactly the same wording, this was my also first thought. There have been two places that I remember where arrogance of the esoterati drive two feedback cycles: 1. People leave after seeking help for an issue they believed needed the input of masters. 2. Because of gruff treatment, the masters receive complaints and indignation, triggering a backfire effect feedback loop, often under the guise of said masters not wanting to be overwhelmed by common problems and issues. There is a few practical things that can help with this (clear guides to point to, etc.), but the missing element is kindness / non-judgmental responsiveness. reply croes 5 hours agoparentprevHow can it be an alternative if it needs the data from Stackoverflow? reply OgsyedIE 5 hours agorootparentBecause consumers in every market develop models of reality (and make purchasing decisions) on the basis of their best attempts to derive accuracy from their own inevitably flawed perceptions, instead of having perfect information about every aspect of the world? reply manojlds 5 hours agoparentprevEasy to keep saying this, but SO was useful because it wasn't wild west. reply weinzierl 4 hours agorootparentIt was useful and not the wild west as long as a very small group of intelligent and highly motivated individuals moderated it. First and foremost Jeff Atwood used to do a lot of moderation himself - not unlike dang on HN. When that stopped, the site (and to some degree its growing number of sister sites) continued on its ballistic curve, slowly but continuously descending into the abyss. My primary take away is that we have not found a way to scale moderation. SO was doomed anyway, LLMs have just sped up that process. reply timhh 2 hours agorootparentprevI disagree. It was useful because the UI was (and is!) great. Easy to use markdown input, lists of answers sorted by votes, very limited ads, etc. The gamification was also well done. Compared to anything before it (endless phpBB forums, expertsexchange, etc.) it was just light years ahead. Even today compared the SO UI with Quora. It's still 10x better. reply waynecochran 3 hours agorootparentprevI have definitely noticed a large drop in responses on SO. I am old enough to have seen the death of these platforms. First to go was usenet when AOL and its ilk became a thing and every channel turned into spam. reply hifromwork 5 hours agoparentprev>Stackoverflow mods and power users being arseholes reduces the use of Stackoverflow While they are certainly not perfect, they willingly spend their own spare time to help other peoples for free. I disagree with calling them arseholes. reply timhh 59 minutes agorootparentThe people I am referring to are not helping. At this point they are making SO worse. The problems are two-fold: 1. Any community with volunteer moderators attracts the kind of people you don't want to be moderators. They enjoy rigidly enforcing the rules even if it makes no sense. 2. There are two ways to find questions and answer them: random new questions from the review queue, and from Google when you're searching for a problem you have. SO encourages the former, and unfortunately the vast majority of questions are awful. If you go and review questions like this you will go \"downvote close, downvote close, downvote close\". You're going to correctly close a load of trash questions that nobody cares about and a load of good questions you just don't understand. I've started recording a list of questions I've asked that get idiotic downvotes or closed, so I can write a proper rant about it with concrete examples. Otherwise you get people dismissing the problem as imaginary. These mods now hold SO hostage. SO is definitely aware of the problem but they can't instigate proper changes to fix it because the mods like this situation and they revolt if SO tries to remedy it. reply tomrod 4 hours agorootparentprevA lot of people comment on online forums for free and are arseholes there too. Not in this thread so far that I've read, to be clear, but it certainly happens. How would you qualify the difference? reply miohtama 5 hours agoparentprevIt's an interesting question. The world has had 30 years to come up with a StackOverflow alternative with friendly mods. It hasn't. So the question is that has someone tried hard enough or can it be done it the first place. I am Stack overflow mod, dealing with other mods. There is definitely unnecessary hostility there, but most of question closes and downvotes Go 90% to low quality questiond which lack proper professionalism to warrant anyone's time. It is remaining 10% that turns off people. We can also take analogs from the death of Usenet. reply jprete 5 hours agorootparentI think the problem isn't specific to SO. Text-based communication with strangers lacks two crucial emotional filters. Before speaking, a person anticipates the listener's reaction and adjusts what they say accordingly. After speaking, they pay attention to the listener's reaction to update their understanding for the future. Without seeing faces, people just don't do this very well. reply shagie 2 hours agorootparentprevThe model of QA that Stack Overflow and its various forks follow the same approach struggle with the 90/9/1 problem ( https://en.wikipedia.org/wiki/1%25_rule ). Q&A was designed to handle the social explosion problem and the eternal September problems by having a larger percent of the username take an interest in the community over time and continue to maintain that ideal. Things like comments and discussions being difficult is part of the design to make it so that you don't get protracted discussions that in turn needs more moderation resources. The fraction of the people doing the curation and moderation of the site overall has dropped. The reasons for that drop are manyfold. I believe that much of it falls squarely upon Stack Overflow corporate without considering second order effects of engaging and managing the community of people who are interested in the success of the site as they envision. Ultimately, Stack Overflow has become too successful and the people looking to it now have a different vision for what it should be that comes into conflict with both the design of the site and the vision of the core group. While Stack Overflow can thrive with a smaller number of people asking \"good\" (yes, very subjective) questions it has difficulty when it strays into questions that need discussion (which its design comes into conflict with) or too many questions for the committed core group to maintain. Smaller sites can (and do) have a larger fraction of the user base committed to the goals of the site and in turn are able to provide more individual guidance - while Stack Overflow has long gone past that point. --- Stack Overflow and its Q&A format that has been often copied works for certain sized user bases. It needs enough people to keep it interesting, but it fails to scale when too many people participate who have a different idea of what questions should be there. There is a lack of moderation tools for the core user base to be able to manage it at scale (you will note the history of Stack Overflow has been removing and restricting moderation tools until it gets \"too\" bad - see also removal of 20k users helping with flag handling and the continued rescoping of close reasons). Until someone comes up with a fundamentally different approach that is able to handle moderation at scale or sufficient barriers for new accounts (to handle the Eternal September problem), we are going to continue to see Stack Overflow clones spout and die on the vine along with a continued balkanization of knowledge in smaller areas that are able handle vision and moderation at a smaller scale. --- Every attempt at a site I've seen since (and I include things like Lemmy in this which did a \"copy reddit\" and then worry (or not) about moderation) have started from a \"get popular, then work on the moderation problem\" which is ultimately too late to really solve the problem. The tools for moderation need to be baked into the design from the start. reply rkncland 5 hours agoparentprevChatGPT plagiarizes the anwers of those whom you call \"arseholes\". How is using Stackoverflow in read-only mode different from using ChatGPT? Except of course that reading Stackoverflow directly has better retention rates, better explanations and more in-depth discussions. (My view is that moderators can be annoying but the issue is overblown.) reply verdverm 4 hours agorootparentPlagiarizing means violating copyright, loosely speaking. When you, as a human, use SO, you assign your rights to the content to SO. That company is licensing the content to 3rd parties, including those who want to train their LLMs. What I find is that the LLMs are not spitting out SO text word for word, as one would when plagiarizing. Rather, the LLM uses the context and words of my question when answering, making the response specific and cohesive (by piecing together answers from across questions). reply tomrod 4 hours agorootparentI thought plagarizing was producing new work substantially copied from prior work, regardless who owns the copyright? I thought this because self-plagarizing exists. reply verdverm 4 hours agorootparentWell, if we could not reproduce with changes, what others have written and we have learned, it is unlikely we could make real progress. There are many more concepts, like fair use, meaningful changes, and other legalese; as well as how people use the term \"plagiarize\" differently. I never heard of this \"self-plagarizing\" concept, it seems like something fringe that would not be enforceable other than in the court of public opinion or the classroom via grades reply tomrod 3 hours agorootparentYou're one of today's lucky 10,000! https://xkcd.com/1053/ It's a core issue in academia and other areas where the output is heavily the written word. [0] https://en.wikipedia.org/wiki/Plagiarism#Self-plagiarism [1] https://ori.hhs.gov/self-plagiarism [2] https://www.aje.com/arc/self-plagiarism-how-to-define-it-and... reply verdverm 1 hour agorootparentReproducing sections is useful in academic publishing. I saw it while reading 100s of papers during my PhD. (1) If you are reading your entrypoint into an area of research, or a group, it is useful context on first encounter (2) If you are not, then you can easily skip it (3) Citing, instead of reproducing sections like background work, means you have to go look up other papers, meaning a paper can no longer stand on its own. Self-plagiarism is an opinion among a subset of academics, not something widely discussed or debated. Are there bad apples, sure. Is there a systemic issue, I don't think so. reply romeros 5 hours agoparentprev [–] thats just cope. I stopped using stackoverflow because I get everything from chatpgt/claude. Just a case of having better tech. Sure the mods were arseholes etc.. but before gpt never minded using it . reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The introduction of ChatGPT, a large language model, has resulted in a 25% reduction in activity on Stack Overflow, a major platform for programmers.- This decline indicates that LLMs are replacing traditional human-generated content, which could affect the quality of training data for future AI models.- The study underscores the importance of understanding the broader effects of LLM adoption on online knowledge sharing and the sustainability of digital public goods."
    ],
    "commentSummary": [
      "Large language models (LLMs) are causing a decline in public knowledge sharing on platforms like Stack Overflow, as users prefer LLMs for quick answers.- The shift is influenced by unfriendly environments on some forums, where users encounter harsh moderation and unwelcoming responses.- This trend raises concerns about the future of open-source knowledge and the quality of information available for training LLMs."
    ],
    "points": 253,
    "commentCount": 220,
    "retryCount": 0,
    "time": 1728818802
  },
  {
    "id": 41826082,
    "title": "WordPress.org's latest move involves taking control of a WP Engine plugin",
    "originLink": "https://www.theverge.com/2024/10/12/24268637/wordpress-org-matt-mullenweg-acf-fork-secure-custom-fields-wp-engine",
    "originBody": "Web/ Tech/ Business WordPress.org’s latest move involves taking control of a WP Engine plugin WordPress.org’s latest move involves taking control of a WP Engine plugin / WordPress co-founder Matt Mullenweg calls it “a rare and unusual situation” resulting from WP Engine’s legal moves. By Wes Davis, a weekend editor who covers the latest in tech and entertainment. He has written news, reviews, and more as a tech journalist since 2020. Oct 12, 2024, 9:10 PM UTC Share this story Image: Cath Virginia / The Verge WordPress.org has taken over a popular WP Engine plugin in order “to remove commercial upsells and fix a security problem,” WordPress cofounder and Automattic CEO Matt Mullenweg announced today. This “minimal” update, which he labels a fork of the Advanced Custom Fields (ACF) plugin, is now called “Secure Custom Fields.” It’s not clear what security problem Mullenweg is referring to in the post. He writes that he’s “invoking point 18 of the plugin directory guidelines,” in which the WordPress team reserves several rights, including removing a plugin, or changing it “without developer consent.” Mullenweg explains that the move has to do with WP Engine’s recently-filed lawsuit against him and Automattic. Similar situations have happened before, but not at this scale. This is a rare and unusual situation brought on by WP Engine’s legal attacks, we do not anticipate this happening for other plugins. Related The ‘WordPress’ fight is now a lawsuit Matt Mullenweg: ‘WordPress.org just belongs to me’ WP Engine’s ACF team claimed on X that WordPress has never “unilaterally and forcibly” taken a plugin “from its creator without consent.” It later wrote that those who aren’t WP Engine, Flywheel, or ACF Pro customers will need to go to the ACF site and follow steps it published earlier to “perform a 1-time download of the genuine 6.3.8 version” to keep getting updates. As its name implies, the ACF plugin allows website creators to use custom fields when existing generic ones won’t do — something ACF’s overview of the plugin says is already a native, but “not very user friendly,” feature of WordPress. The Verge has reached out to Automattic, WordPress.org, and WP Engine for comment. Update October 12th: Adjusted to add clarity about Mullenweg’s use of the “fork” label. Most Popular Most Popular Marques Brownlee says ‘we failed on the price’ with Panels Tesla’s Robovan is the surprise of the night The Tesla Cybercab is a cool-looking prototype that needed to be much more than that Steam now says the ‘game’ you’re buying is really just a license Tesla’s Optimus bot makes a scene at the robotaxi event Verge Deals / Sign up for Verge Deals to get deals on products we've tested sent to your inbox weekly. Email (required)Sign up By submitting your email, you agree to our Terms and Privacy Notice. This site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply. From our sponsor Advertiser Content From",
    "commentLink": "https://news.ycombinator.com/item?id=41826082",
    "commentBody": "WordPress.org's latest move involves taking control of a WP Engine plugin (theverge.com)251 points by lsaferite 10 hours agohidepastfavorite185 comments pluc 6 hours agoGot a call today from a client I hadn't heard from in 5 years. They use ACF, they use another plugin that WPEngine acquired to move their uploaded files to S3. They're freaking out at all this and worried about how the next thing might impact their business... so they want off WordPress. Thanks for the work, Matt! reply RobotToaster 9 hours agoprevWordpress banned forks from the plugin directory a while ago, so they're doing what they ban everyone else from doing. https://make.wordpress.org/plugins/2021/02/16/reminder-forke... reply asddubs 8 hours agoparentThat post seems to be about forking open source plugins which cost money. From what I can tell they did fork this plugin but are not offering its premium features for free. Just to preempt people arguing with me, please note that this comment isn't meant to be an ethical defense of them one way or the other, but just a factual correction. reply luckylion 3 hours agorootparentIt's a freemium plugin though, the advertisements for the pro-version and the update-funnel are how you pay for it. If WP had only fixed the error but not removed all of those things & and hadn't claimed authorship themselves (\"By Wordpress.org\", and putting themselves at the top of the contributors while they're certainly the one with the fewest contributions), that argument would be different, I believe. Would wordpress.org allow WooCommerce to be forked, uploaded with no modifications beyond the all references to WooCommerce being changed to YouCommerce and all mentions of Automattic being wiped? I don't see that being allowed. reply supermatt 8 hours agoparentprevIt’s not a premium plugin. reply sccxy 8 hours agorootparentWordpress did not fork the plugin. They just took ownership of the plugin. Wordpress removed all premium features after the hijacking of the plugin. https://plugins.trac.wordpress.org/changeset?new=3167679%40a... reply RobotToaster 8 hours agorootparentprevIt has premium features, it may be a grey area, but that makes it a premium plugin IMO. reply vintagedave 10 hours agoprev> It’s not clear what security problem Mullenweg is referring to in the post. Where is the CVE? What risk is there continuing to use the original plugin? No details at all. This results in fear: we don't know if the original is safe to use. > Going forward, Secure Custom Fields is now a non-commercial plugin Does this imply that Wordpress is potentially going after a revenue stream from WPEngine? If the plugin had Pro options* then are those closed source and so not available to Wordpress in their fork of the codebase? It's not clear. * https://www.advancedcustomfields.com/pro/ reply maxloh 10 hours agoparent> Where is the CVE? What risk is there continuing to use the original plugin? Here’s the diff showing what has changed: https://plugins.trac.wordpress.org/changeset?new=3167679%40a... reply jeroenhd 9 hours agorootparentI do see various security fixes in that patch, but most of the changes are removing references and code for a \"pro\" version of the plugin. I'm guessing the WP security team has been pentesting any WPEngine code they could get their hands on to find an excuse to make all of these changes. The security issues do look bad (once again proving that WordPress' worst vulnerabilities come from the plugins they install) but I think the branding removal is pretty wild. A quick skim through the plugin development guidelines does seem to indicate that trialware isn't allowed, and the plugin seems to be doing all kinds of other stuff that isn't really permitted by the guidelines. I don't know if WordPress is as strict in enforcing those as they are with this plugin, but the changes do seem to be based on them. With WPEngine recommending people to install their (vulnerable) version, I once again feel like there's no right side in this conflict. What a mess. reply maxloh 9 hours agorootparentWP Engine fixed the vulnerability about a week ago [0]. The WP security team may have just backported the fix, even using the same line in their changelog [1]. [0]: https://www.advancedcustomfields.com/changelog/ [1]: https://plugins.trac.wordpress.org/changeset?new=3164480%40a... reply Svip 9 hours agorootparentprev- public $version = '6.3.6'; + public $version = '6.3.6.2'; Why would you break the version standard? Maybe there are scripts expecting a specific format or some such. Also, given the majority of this change, I think 6.4.0 or even 7.0.0 is more in order, but at least 6.3.7. Edit: To make matters worse, elsewhere in the code, it's referenced as 6.3.8. Very confusing. reply Genbox 9 hours agorootparentprevFor those reviewing the changeset: There are two places where they read a value directly from $POST into an $args array. There is no validation applied, which means an attacker can inject whatever value they wish. reply talkin 7 hours agorootparentAnd 2 problems jn the fix: - It’s a specific symptom fix: The same problem could occur with $_COOKIE or $_REQUEST always being available - The cleanup is not done in a finally{}, so random missing vars when an exception occurs. Exec summary: Horrible code as always in WP. reply sschueller 9 hours agorootparentprevIn 2024, wtf. How can anyone especially on software with this kind of reach still do such absolute amateur things? reply echoangle 8 hours agorootparentI can't speak for WP Engine devs specifically but for Wordpress customization developers, the ones I've worked with were just absolute amateurs. The people doing this are mostly untrained people hacking together stuff layer upon layer until it kind of worked. Don't even ask about using version control. I don't want to say this applies to every Webdev but it attracts a group of people that aren't too much into IT but want to quickly learn programming to have a job. reply criley2 6 hours agorootparentprev> want free software to run website > want free plugins to add functionality to site > absolutely will not hire developers or pay for software or plugins > how dare the free plugin for my free software not be coded to the highest standards reply luckylion 3 hours agorootparentThat code is identical to the commercial version ACF Pro which currently costs $49/yr for a single website. reply grayhatter 9 hours agorootparentprevwow, they deleted 300 lines, many giving credit to others, just to replace it with > Security - ACF defined Post Type and Taxonomy metabox callbacks no longer have access to $_POST data. (Thanks to the Automattic Security Team for the disclosure) If I was on that security team, I would be livid they used my team's name on this behavior. If this was done by that security team, their ethics are disgusting, and likely non-salvageable... Still looking for the security exploit worthy of a plugin takeover though. edit; best I can figure tonight is it's some concern over CSRF, but they don't even sanitize $_GET nor $_SESSION, only _POST and _REQUEST... so either it's more complicated than it looks on the surface, or this \"fix\" is partial at best, and wasn't written by someone from security. (It's also possible or likely that I'm missing some context, it's been a long time since I've had to work on php) reply maxloh 9 hours agorootparentWieldy enough, that line likely came from the original developer (ACF/WP Engine) [0]. I believe WordPress.org backported the change and named it v6.3.6.1 at that time [1], before rebranding ACF in a later version (v6.3.6.2). [0]: https://www.advancedcustomfields.com/changelog/ [1]: https://plugins.trac.wordpress.org/changeset?new=3164480%40a... reply Svip 9 hours agorootparentAh, that would explain why those fixes are noted as \"since 6.3.8\". reply luckylion 9 hours agorootparentprevIt's extra funny when you consider all the recent acts where some plugin developer sells his plugin to some shady company, they go and add \"functionality\" to it that fundamentally changes the plugin, adds forced widgets to users' websites to promote their services etc, and the WP security team is like \"they're allowed to do that, that's perfectly fine\". I don't know how much overlap there is between the Automattic and WP security teams but I assume there's some like with most things in WP. reply rmbyrro 9 hours agoparentprev> This results in fear: we don't know if the original is safe to use. The exact result intended by Matt, I presume. He wants to scare WPEngine's customers away from their services. At this point, this looks more like a war between personalities. reply closewith 8 hours agorootparent> At this point, this looks more like a war between personalities. There only seems to be one personality at play to me. reply skywhopper 7 hours agorootparentprevWho’s the other “personality”? reply aeurielesn 10 hours agoparentprevMy understanding is ACF injected notices for their personal legal gains on everyone's dashboards. I wouldn't be surprised if such thing is a breach. reply ImPostingOnHN 4 hours agorootparentMy understanding is that it was actually matt who did that, and he did it so he could denounce WPEngine to their customers, due to his personal vendetta against WPEngine, because WPEngine refused his shakedown attempt for money. This is cited in WPEngine's lawsuit. Like you said, I wouldn't be surprised if this behavior of matt's was a breach. reply maxloh 10 hours agoparentprev> If the plugin had Pro options* then are those closed source and so not available to Wordpress in their fork of the codebase? It's not clear. Because WordPress is licensed under the GPL, all plugins must be licensed under GPL-compatible licenses. This applies to ACF Pro as well. reply mimsee 9 hours agorootparentAs far as I've understood, the copyleftness of the Wordpress's GPL tainting themes and plugins is or at least has been controversial (I'm not in the WP community, but have read stuff regarding this drama). Wordpress itself uses React and other non-GPL licensed software in the core that yes, may be GPL-compatible but doesn't enforce everything to be GPL'd. When it comes to themes and plugins, I'd consider that userspace, akin to installing Spotify on Ubuntu doesn't make Spotify suddenly GPL'd. reply ThatPlayer 8 hours agorootparentIn Linux, not even in user space: ZFS is developed as a kernel module rather than mainline Linux kernel because of its incompatible with GPL license. Or Nvidia's Linux driver aren't even open sourced. Well AMD has proprietary Linux drivers too, but they have an open sourced one also. reply no92 8 hours agorootparentNot detracting from the point you're making, but the kernel side for Nvidia drivers for Linux is published on GitHub, by Nvidia. The userspace side is still delivered as pre-compiled binaries, tough. reply ThatPlayer 8 hours agorootparentYou're right. But that's a somewhat recent (2 years) release and not compatible with as many GPUs as their closed source kernel side ones drivers, which are still on version parity. My 1080Ti isn't compatible with those drivers, so I do forget about the open sourced ones. reply luckylion 9 hours agorootparentprevFundamentally yes, though it hasn't really been tested in court and I'm not sure that plugins really are obviously derivative works - you can create a plugin that doesn't depend on WP but can be used with WP, but I'm not a lawyer. Matt and WP's position however includes that this applies to the PHP code but not to accompanying assets like images, CSS, JS (because those can obviously be used without WP). Those only need to be GPL if you want to host it on wordpress.org which the commercial versions of those freemium-plugins are not. reply thih9 9 hours agoprevThe appropriate twist here would be to have WPEngine find a trusted third party (one or more), start a foundation together and successfully fork wordpress. reply lordofgibbons 9 hours agoparentDidn't this whole thing start because Automattic was complaining that WPEngine wasn't contributing much to the Wordpress development but competing with them for free? reply mrweasel 7 hours agorootparentThat was my impression too, but now we learn that they have made plugins available, so they are contributing to the Wordpress community, but they're doing it wrong? I feel like Wordpress is going down the same path as Elastic and other companies have done, make something, open source it and then make a fuss when other companies \"misuse\" their code? If you want that tight control over how others use your code, then maybe consider not picking an open source license? reply slyall 8 hours agorootparentprevThat one of Matts claims. Personally I suspect the actual trigger was WPE removing Automattic's affiliate key from WooCommerce and costing them $m/y of revenue. reply ThatPlayer 8 hours agorootparentThat's also one of Matt's claims. According to WPE's court filings, they claim it was an alternate/fork of WooCommerce's Stripe integration they provided with additional features, and makes them less than ~2000$/mo. If that claim is false, that's perjury. reply batuhanicoz 8 hours agorootparentprevOur attempts at resolving trademark issues with WPE predate our discovery of the attribution removal. reply closewith 8 hours agorootparentWho is our in this context? If you are a party to this dispute, why on Earth would you comment publicly? reply Kye 5 hours agorootparentIt really is strange Automattic's lawyers haven't clamped down on any public comment. Have any of WP Engine's people been as free with commentary as Automattic employees have been? If I were an employee at a company being sued I wouldn't say anything related to it even without an order from legal because I wouldn't want to have to risk answering for it in the trial. Why dangle yourself out there as a target for the opposition's lawyers? reply thih9 8 hours agorootparentprevWhat do you mean by “trademark issues”? Is this about the WP in the WPEngine’s name? If yes, what about other companies in a similar position? reply kgeist 8 hours agorootparentThe abbreviation \"WP\" was initially permitted for use before they retroactively changed it a few days ago. Saying \"We host WordPress\" on WPEngine's site constitutes as much trademark violation as a random shoe shop listing Nike shoes in their catalog. It seems this is more about attempting to undermine a competitor, with trademark issues serving merely as an excuse. reply Kye 5 hours agorootparentprevThe claim is they spent over a year trying to convince WP Engine to pay up. But based on what I've seen, it was probably increasingly pouty complaints and demands leading up to the threat to go nuclear at WCUS 2024. I wouldn't take how this played out as evidence that WP Engine was opposed to making more significant contributions like sponsoring a real, independent foundation or even putting a significant portion of its revenue toward the project. reply bravetraveler 9 hours agoparentprevIf not for Microsoft I'd suggest calling it \"Word\", no need for the Press. reply nopakos 9 hours agorootparentWhat about \"Press\"? reply bravetraveler 9 hours agorootparentI like it, a lot of room to play with things like 'imPress' reply slyall 8 hours agorootparentConsidering this dispute already invoves trademrk confustion allegations I'd avoid the words \"word\" , \"press\" , WP\" and \"auto\" and \"matic\" completely. reply thih9 5 hours agorootparentHow about ProseWeb? Moving from WP to PW seems particularly petty; perhaps fitting in a dispute like this. reply Ey7NFZ3P0nzAe 8 hours agorootparentprevI think PressWord sounds nice reply mrweasel 7 hours agorootparentprevJust call it TextPress. reply teruakohatu 9 hours agoparentprevI think Matt would be quite happy with that. His issue is WP Engine not contributing to WordPress. If they decide to maintain a fork and infrastructure, they won't be freeloading anymore. Edit: That attracted a lot of downvotes. I was giving my option in response to the parent comment. In my option Automattic would be happy if they forked it. reply closewith 9 hours agorootparentOn one hand, the claim is that WP Engine contributes nothing to the WordPress ecosystem. On the other hand, the necessity for Automattic to fork a plugin developed by WPE suggests that WPE has been contributing something significant—so significant that its continuation is essential for the community. reply thih9 9 hours agorootparentprevIf the new project offers a more stable platform, one that cannot be controlled by a single person, the community might like that more and might move; then there will be little left to freeload. Is that an option btw? I.e. is it possible to offer hosting with seamless migration from wordpress.org? reply serial_dev 9 hours agorootparentprevWhere is it in the license that you need to contribute to the project if you make big bucks? We have open source licenses for a reason, contributing back is never a requirement. If you believe that is his issue, I have a bridge to sell you. reply saaaaaam 8 hours agorootparentMullenweg literally said that this was his issue. That’s what kicked off the whole debacle. reply serial_dev 8 hours agorootparentBill Clinton said he didn’t have relations with that woman. At some point in life, people usually stop believing everything that others “literally said”. reply saaaaaam 6 hours agorootparentRight, sorry, I see what you mean. You mean “if you believe that the issue is truly as he describes it publicly and not in fact something else” rather than the literal interpretation of “if you believe that this is his issue”. My answer was based on the fact that I’ve seen several people who are unaware of what Mullen’s said that precipitated the row. reply ImPostingOnHN 4 hours agorootparentmatt's actions which precipitated the row were to demand that WPEngine pay 8% of their revenue to Automattic, a private-equity-associated, for-profit corporation matt runs. Money, more and more money for matt, seems to be the matt's driving motivation here. He seems to be projecting his own greed onto others. reply teruakohatu 8 hours agorootparentprev> Where is it in the license that you need to contribute to the project if you make big bucks? We have open source licenses for a reason, contributing back is never a requirement. If you believe that is his issue, I have a bridge to sell you. I never said that the license forced them to contribute. reply immibis 9 hours agorootparentprevWPE contributes to WordPress; Matt is just full of shit on this. It's an excuse, not a reason. reply ryukoposting 9 hours agorootparentWhat does WPE contribute? I ask from a place of curiosity. Code, money? reply saaaaaam 8 hours agorootparentBeyond code and money, both of which I believe they contribute - though apparently not enough to satisfy Matt Mullenweg - you could argue that they offer a very straight forward distribution/adoption channel in terms of straightforward infrastructure. I hate WP Engine - having had first hand dealings with them after they acquired a company I’d spent tens of thousands of dollar with - but nonetheless I fee like there’s probably a pretty strong argument that simple straightforward Wordpress hosting means more people use Wordpress which strengthens the overall ecosystem. When I first started using Wordpress there was a steep learning curve to get it configured correctly on hosting where stuff wouldn’t break. I used to pay someone to do that for me, and when they gave up doing that I moved to Flywheel (which was later acquired by WP Engine) and suddenly all of my previous problems with Wordpress from a hosting perspective vanished. It just worked and the Flyhweel support team was amazing and would even unofficially support the wider implementation of Wordpress (“I changed this and now that thing has broken… I know it’s not your fault but any suggestions?” “Hey, here’s the problem, plus we have fixed it for you!”) That made me stick with Wordpress for years and build out more sites in Wordpress and recommend it to friends and clients. Most small businesses (which represents the majority of Wordpress users) don’t want to have to think about hosting: in the same way they expect their mobile phone service to just work, and email to just work, they need Wordpress to just work. Whatever issues I have with WP Engine they offer a very straightforward “forget about it” service for running Wordpress which ultimately means more people are likely to use it. reply mst 9 hours agorootparentprevOn code contributions, I believe the accusation was that the amount of developer time they fund is tiny relative to the size of the company. I've not seen numbers comparing other significant WP users to them, though, only comparing them to Automattic itself, which seems a bit apples to aardvarks to me. reply underdown 8 hours agorootparentprevAt a minimum a plugin that Wordpress was forced to “fork”. reply odo1242 3 hours agorootparentprevThey have an equivalent of 1-2 full time people working on it, they sponsor the WordPress conference, and they make a couple really important plugins almost everyone uses reply jeltz 9 hours agorootparentprevBoth. Matt just does not think they contribute enough. Either that or he is angry that they are more successful than his own company. reply stefanos82 9 hours agoprevEven if they find a middle ground to this mess that Matt has created, I very much fear that the damage to the community is done and it's only a matter of time before the popularity of the once famous platform that almost everyone uses in one way or another collapses. reply Etheryte 9 hours agoparentFor better or worse, I would optimistically say that that might be a good thing. WordPress has given a lot to the internet over the years, but a very large portion of its giving has been pwned sites and security issues. If this leads to either the birth or popularization of a tool that's modern and secure, that would be a net win for everyone involved. reply seydor 7 hours agorootparentI can only imagine how worse it will be if wordpress is hastily replaced with some javascript contraption reply arend321 8 hours agoparentprevIt sounds a bit similar to when Drupal faded to obscurity during the Larry Garfield sage. Of course there is a long run up that finally explodes with a high profile issue, but the writing could be on the wall for WordPress. reply gjvc 8 hours agoparentprevGood. WordPress is merely the tallest dwarf. reply switch007 8 hours agoparentprevDepends how good their PR is. By all accounts, their current PR is not very good reply saaaaaam 8 hours agorootparentI think their current PR seems to be Matt Mullenweg himself. And the more I hear him speak - having never really concerned myself with anything he said - the more loopy and disconnected from reality he seems. Post-economic! Sometimes it’s great to have a slightly crazy visionary with utopian ideals who is prepared to say whatever they think will effect change. But also: Elon. reply withinboredom 10 hours agoprevThey either have some of the best or worst legal counsel; or they just ignore the legal counsel. reply threeseed 10 hours agoparentThey also have the worst social media team I have ever seen: https://x.com/WordPress/status/1845121130207535524 reply docdeek 10 hours agorootparentThis is wild. Surely Matt isn't tweeting from the official account - marketing would not allow that, right? reply Hamuko 9 hours agorootparentYou mean his subordinate after he gave six months worth of pay for everyone not on his side to leave the company? reply pluc 6 hours agorootparent\"You don't agree with what I'm about to post? Here's a severance check.\" reply closewith 8 hours agorootparentprevSeems to be unavailable to me now, but Archive link here: https://archive.is/WKx4a reply srmarm 8 hours agorootparentprevWhat's that in response to? I don't have an account so don't seem to see anything beyond 'Sorry, who are you?' reply echoangle 7 hours agorootparentAccording to https://bullenweg.com/#matt-uses-wordpress-to-dismiss-founde... , \"Laura Elizabeth, founder of WordPress Plugin Client Portal\" posted \"Guess I can’t contribute to WordPress \" reply miragecraft 9 hours agorootparentprevI thought I was reading a tweet from Wendy’s. reply pwdisswordfishz 9 hours agorootparentprevWho is she, though? reply InsideOutSanta 9 hours agorootparent1. It doesn't matter. 2. It takes one click to find out that she's the \"founder of http://Client-Portal.io, a WP plugin for freelancers to use with their clients to keep track of all the deliverables in a centralized portal\" reply ablation 9 hours agorootparentprevMullengweg taking a leaf out of Musk PR playbook, clearly. reply sureIy 10 hours agoparentprevSome people just think they're above the law. This Matt guy has gone mental. reply withinboredom 10 hours agorootparentYeah, but pretty sure some of the employees/volunteers are in on it or \"just following orders.\" reply speedgoose 10 hours agorootparentYes, no one stopped the co-funder and it's unlikely he did the fork and the change of ownership only by himself. Other people at Automattic are responsible too. reply InsideOutSanta 9 hours agorootparentAll the sane people left when Matt offered them $30,000 or six months of salary to quit. reply sureIy 9 hours agorootparentprevConsidering how many people stayed, it's clear that they've been brainwashed to think what he's doing is ok. reply slyall 8 hours agorootparentOr they didn't feel confident about quickly getting a new job in this market. Especially since Automattic is mostly/entirely WFH. It's pretty unlikely the company will go completely broke any time soon so their jobs are probably fairly safe. reply labster 9 hours agoparentprevOne of Automattic’s statements was issued by Neal Katyal, who was Acting Solicitor General of the United States. I would tend towards thinking the client himself may be the problem. reply raverbashing 10 hours agoparentprevWhile the situation is much less problematic, I think WordPress management could listen to their lawyers more reply sccxy 10 hours agoprevMullenweg is hijacking existing users with supply chain attack. reply shprd 10 hours agoparent> supply chain attack. Where's the \"attack\" part? I thought that was a crucial part in the definition reply crote 9 hours agorootparentThe author of a library has lost all control over the codebase, and a third party is now making changes to it. That's pretty much the textbook definition of stage one of a supply chain attack. Considering what Matt has already done, it wouldn't even remotely come as a surprise if a future ACF update would, say, brick all WP installations using ACF on a WP Engine host. reply mimsee 9 hours agorootparent> brick all WP installations using ACF on a WP Engine host That tactic would work, if WP Engine had access to the update server hosted at wordpress.org. reply shprd 9 hours agorootparentprevIt's like claiming going to the bank is stage one in a robbery. So if you go to the bank you're a thief. WordPress have the rights, just like the responsibility and possible liability of everything distrubted on their platform. reply chucky123 9 hours agorootparentIt's more like gaining backdoor access to the bank's server. At this stage no attack has happened(but can happen) reply shprd 9 hours agorootparentThey didn't gain access anywhere, it's their platform. reply immibis 9 hours agorootparentIf the bank starts fiddling with the numbers in your account: \"I'm not being attacked, it's their database\" reply shprd 9 hours agorootparent> bank starts fiddling with the numbers in your account If a bank messes with your money, you ask for your money when that happens. Not defame the bank based that they updated their database, business as usual, but you liked the old one. how exactly did they mess with your stuff? where's the attack you're speaking about? where's physical harm? reply immibis 3 hours agorootparentThe database says you have zero money, in fact you are not even a customer and never were, good day sir. reply shprd 2 hours agorootparentThe paid version of AFC is not affected, so I'm not sure what are you talking about? What money? who did you pay? for what? reply DonnieBurger 6 hours agorootparentprevThis is how users will unknowingly update from ACF to Secure Custom Fields: https://x.com/Brugman/status/1845195750550143424 https://archive.is/u6ZbY reply shprd 5 hours agorootparentAs user how were you affected? Are there any features you can no longer access? reply rbanffy 9 hours agorootparentprevInjecting code that creates misleading or malicious dashboard warnings is a supply chain attack, even if it’s the intent of the supplier and not a malicious third party interfering with the supply chain. reply shprd 9 hours agorootparent> misleading or malicious dashboard warnings Who did that? WP Engine was the one making these before the change reply msephton 9 hours agoprevWow, the WordPress name is being ploughed into the ground with this sort of behaviour. reply josephg 8 hours agoparentWell, they banned Wpengine from updating their package in the repository. Then the package maintainers found security problems - which they can’t fix with an update because they’re locked out. It makes a weird sort of sense. Ie, wp.org backed themselves into a corner where they needed to close the security hole. And to do that, they needed to patch it themselves, which in turn requires them taking over the package. It’s shocking, yeah. But it would probably be worse if they just left the (known, publicised) security vulnerabilities in. reply srmarm 8 hours agorootparentThey could have backed down and allowed WPEngine to publish an update or even published it on their behalf. Instead they've doubled down and taken this ridiculous action that undermines the whole wp/plugin eco system. None of this is necessary. reply pluc 5 hours agorootparentThis was engineered and deliberate, they were never gonna let WPEngine have their space back. We called this a week ago: https://news.ycombinator.com/item?id=41751856 reply LeoPanthera 10 hours agoprevPreviously on Hacker News. https://news.ycombinator.com/item?id=41821400 reply r721 9 hours agoparentAlso https://news.ycombinator.com/item?id=41821336 (165 comments, including 5 by photomatt) https://news.ycombinator.com/item?id=41824852 (63 comments) reply Havoc 9 hours agoprevThis is starting to be just vanilla sad. Pity that there isn’t a comparable eco system that is less…mercurial reply itfossil 7 hours agoprevWhat the hell is Mullenweg smoking? Seriously. This dude has completely lost what little was left of his mind. reply recursivegirth 4 hours agoprevJust finished submitting our companies RFP to 4 vendors for our 2025 website redesign. We are moving from WP to a React enterprise CMS. Thanks for the job security Matt :). reply nelblu 10 hours agoprevYesterday's link https://news.ycombinator.com/item?id=41821400 reply spiderfarmer 9 hours agoprevI just cancelled my ACF subscription as it's up for renewal in 30 days. I'll wait and see how the dust settles. reply labster 9 hours agoparentMy advice would be to make plans to move away from WordPress entirely. While I think that the “supply chain attack” is hyperbolic, if technically true, it’s indicative of an organization that cares about winning more than ensuring any form of stability whatsoever to their users and clients. Beware. reply DoubleGlazing 7 hours agorootparentI agree, If I were an IT manager this sort of stuff would make me start looking at alternatives. If an app/pluging/package is maintrained and published by X, I want to make sure no one else can interefere with it - even if they have good intentions. What Automattic should have done is removed the plugin from distribution and told WP Engine to fix the problem. By doing what they did they have breached the trust of their users. reply gtvwill 7 hours agorootparentThis is gonna be a hard move for some of the IT companies I've seen who have based their entire business around WordPress websites. They literally don't have the skill to use anything else as WP is all they know. Rip. reply spiderfarmer 9 hours agorootparentprevGood advice, but it's a subscription I'm paying on behalf of a customer for a website I made 5 years ago. It's one of the few Wordpress websites I made as I hate the development process and the bloat of it all. reply tiffanyh 6 hours agoprevDidn’t WPE change ACF to pull updates from WPE (on Oct 2nd), essentially at Matt’s request … because one of Matt issues with WPE, is that they were putting undue infrastructure load/cost on Wordpress.org And then days after WPE makes this update, Matt then hijacks their plugin. Am I understanding this correctly? https://www.advancedcustomfields.com/changelog/ reply shdon 10 hours agoprevMullenweg calls it a fork. I could see that being somewhat okay if it's indeed for security fixes, but removing the upsells is petty at the very least. But a fork doesn't take control of the original, so I wonder what they did there? Perhaps a redirect from the ACF entry to SCF? To be honest, none of this makes WordPress look good... It just seems like a douche move. reply k1kingy 8 hours agoparentIt can hardly be called a fork when you wipe every mention of the original dev team in the codebase, and start a fresh changelog with 'Patched security fixes' whilst thanking his team. It's an absolute takeover not a fork. reply mananaysiempre 7 hours agorootparentThe thanks come from the upstream release notes for 6.3.8, FWIW. reply ChrisArchitect 7 hours agoprev[dupe] discussion: https://news.ycombinator.com/item?id=41821400 https://news.ycombinator.com/item?id=41821336 reply justinclift 8 hours agoprevWell, I hope he's going to pay people to maintain that plugin now. Because the original author (or team?) is probably motivated to move on to other endeavours instead. reply dustingetz 8 hours agoprevcan someone please explain the feud for those of us who are out of the loop? reply sccxy 7 hours agoparentMatt Mullenweg - owner of wordpress.org (open-source project) and wordpress.com (paid WordPress hosting provider) He is angry that WPEngine makes money with Wordpress hosting. He thinks wordpress.com should be the only paid WordPress hosting provider. He demanded 8% of revenue of WP Engine or he would embark on a “scorched earth nuclear approach” to WP Engine. https://bullenweg.com reply chii 6 hours agorootparentwordpress itself is GPL, so anyone, not just WPEngine, should be allowed to freely provide hosting. This is exactly what elastic search faced, which necessitated their changing of their license, and subsequently, caused AWS to fork elastic search. Elastic search cannot, and do not, have the right to demand payment from AWS. reply 2-3-7-43-1807 9 hours agoprevNow who's the heel and who the face? reply chrishare 8 hours agoparentNot a WordPress client but seems like this Matt dude has go-away heat reply nurettin 10 hours agoprevYou make an opensource project, provide hosting services, then others take your project, modify it for their needs, cut into your hosting market share and then you try to get rid of them. ...What was the end game plan? reply mirzap 8 hours agoparentAutomattic was an early investor in WPE in 2011, but then they made a mistake and sold their shares. In addition, the GPL allows you to do what you mentioned. You can take any GPL-licensed code and use it for commercial purposes without having to contribute back. The license is designed to protect you in this way, and it aligns with the spirit of GPL. Expecting something in return for open-sourcing code is not in line with the spirit of GPL and open-source software. If you're not comfortable with other people making money from your code, it's best not to open-source it. reply gorbachev 9 hours agoparentprevAt this point I wouldn't be surprised if he'd had secured funding for a new CMS platform startup and is secretly working on it. He seems absolutely hellbent on assuring nobody should use Wordpress. reply mimsee 9 hours agorootparentAfter this, who would trust his second try? reply gorbachev 9 hours agorootparentI certainly wouldn't, true. reply maccard 8 hours agoparentprevI don’t think this is a reasonable argument. The landscape when Wordpress was released was wildly wildly different to how it is today, and we shouldn’t begrudge someone for not foreseeing how this would unfold from the landscape in the mid 2000’s. We should judge for how it’s being handled now. reply weird-eye-issue 7 hours agoparentprevWhat a terrible take WordPress.com is only successful in the first place because WordPress was open source and had so many hosting options Also what do you mean it was modified? WPE didn't fork or modify WordPress any more than other hosts reply RobotToaster 8 hours agoparentprevHe didn't make shit, wordpress is a fork of b2/cafelog. reply bagels 8 hours agoprevTo me, WordPress used to be the thing you used if you wanted a website that was easy to put together but was full of third party php spaghetti code and security holes. Now, it's completely radioactive. reply everforward 7 hours agoparentI wouldn't even really call it that. I only think Wordpress is easier if one of two things is true: 1. You are actually building a dynamic site; eg WooCommerce is much easier than building your own storefront. 2. Your users refuse to use Markdown, and are paying you enough to double the overhead and put it all on you. It gives me a good chuckle when I see posts on here like \"We use Wordpress and then scrape the static assets and serve it as a static site from S3\". I won't denigrate those people; I'm sure there are good contextual reasons to do that. I just think it's a pretty damning indictment for it to be downgraded from \"the software that runs the website\" to \"a web-hosted WYSIWIG editor for people who can't/won't do Markdown\". reply cj 7 hours agorootparentIn my experience, Markdown isn't the barrier. It's the: - Plugin ecosystem. Marketing people want to use specific plugins for SEO, automatic internal linking, etc. Those plugin only work with wordpress. - Marketing people want to deploy to production. They hate waiting for dev to do anything (which brings us back to the importance of the plugin ecosystem, to add functionality without developers). - It's a familiar system that doesn't need to be \"learned\" by end users (the same way VS Code, VIM, or whatever is your preferred code editor) If it weren't for the first 2 barriers, I think the 3rd (learning markdown) is the easiest to overcome. Especially with side-by-side realtime markdown rendering, which itself is a form a WYSIWYG. Edit: FWIW, we moved to Webflow at my company, used to be on Wordpress, and before that used to have a Markdown-compiled site, help docs, and blog. Markdown-compiled was my favorite as a developer (and also the most performant), but it was everyone else's least favorite because it required me to deploy and make code changes, and they weren't patient enough to put a ticket in for every change request. They also understandably didn't want to login Github to make updates to markdown files. reply theyknowitsxmas 1 hour agorootparentNot really. Weird syntax is intimidating, GitHub for CMS is intimidating, people want nice goowies. reply cj 25 minutes agorootparentAsking your dev team to prioritize deploying updates to your markdown site is also intimidating. In most companies, everyone outside of engineering has a view of engineering that “Developers hate to be interrupted and need everything scheduled in a sprint 2 weeks in advance” - when marketing teams decide between Wordpress and a markdown system managed by the dev team, the choice is obvious (avoid anything that requires dev getting involved) Easy GUI for writing and publishing to production, with no-code installable plugins that extend functionality, is exactly what Wordpress offers that markdown does not. Webflow has a very similar value prop. reply saaaaaam 8 hours agoparentprevAbsolutely. I moved a big revenue generating content site away from Wordpress about 18 months ago. With this absurd debacle going on, I’m so glad I made that decision. reply fHr 7 hours agoparentprevWordpress made me quit web developement and my job at one point, custom plugins in php 5 running on wordpress I think version 4 were a nightmare to maintain. reply mikl 7 hours agoprev1. Release your code as open source. 2. Make a fortune. 3. Complain that people are freeloading. 4. Abuse your power as project founder to punish them, torching the community trust you’ve built up over decades. 5. Profit? Whatever Mullenweg hoped to gain by undermining WPEngine can’t possibly be worth the damage he’s done to WP and his own company. reply huskyr 7 hours agoparentI think the only reasonable course of action for Automattic would be for Mullenweg to step down and for them to make a mea culpa, but i doubt that will happen. Given that half of the web runs on WordPress, i wonder how many people will actually move away from WP as a CMS. Maybe if there is a succesful community fork. reply bdcravens 7 hours agorootparentThey could also buy WP Engine. Or vice versa. I doubt it will result in a mass migration. Many wouldn't move to a fork. We are very engaged in this kind of news, but the kinds of users who use WordPress often aren't. Our small company actually uses WP Engine, and I asked our owner (who also handles content, marketing, etc, and who I report to) if he had heard of what was going on, and he hadn't. reply that_guy_iain 6 hours agorootparentThey actually sold their stake in WP Engine... Maybe they realised that was a bad move. reply karel-3d 7 hours agoparentprevIf I understand it correctly, the core of the dispute is trademark and that WPEngine somehow implies WordPress affiliation. I can see both sides of the story here, but the scorched earth strategy doesn't seem to be very effective for building trust reply prox 7 hours agorootparentThe wording you use “effective for building trust” is a misnomer. As someone who uses ACF, I am suddenly an outcast in the world of Matt/Wordpress.org? And people who I know make money from plugins, are they next in line for the hostile take over treatment? This IS a violent breach of consumer/user trust. Whatever you thought before of this takeover/stealing, this is what trust gone looks like. reply that_guy_iain 7 hours agorootparentprev>If I understand it correctly, the core of the dispute is trademark and that WPEngine somehow implies WordPress affiliation. No, the core of the dispute is Matt wants either money from WP Engine or for them to contribute to WordPress. He's using the trademark as leverage. However, their usage of WordPress does not imply affiliation instead saying stuff \"We bring WordPress to the masses\". The use of WP in their name was them actually following WordPress' trademark policy where they asked people not to use WordPress in their names but WP. reply skywhopper 7 hours agorootparentprevI think the trademark issue is weak in this case anyway. WP Engine has been around for 14 years with that name, and has been part of the Wordpress community. But absolutely nothing about a trademark dispute excuses the lies, unethical behavior, and just downright personal animus Mullenweg is pouring into this takedown. reply DonnieBurger 7 hours agorootparentSpeaking of potential trademark issues, it looks like \"Secure Custom Fields\" retains some ACF branding: https://x.com/TDKibru/status/1845178985308881146/ https://archive.is/sjuHl Ironically, Automattic is actively taking legal action against a premium plugin reseller for trademark infringement on modified WooCommerce plugins: https://www.reddit.com/r/Wordpress/comments/1fqw2eh/automatt... reply lexicality 7 hours agoparentprevHe has certainly gained a lot of personal attention. Maybe he was feeling lonely? reply dangsux 10 hours agoprevThis seems blown out of proportion. Do you think Mrs Bakewell and her cooking blog set up by her 14yo son are gonna care about this drama? No. Neither are the larger companies using it for microsites. reply mananaysiempre 6 hours agoparentBoth Mrs Bakewell’s fourteen-year-old son and the couple of guys setting up the large company’s microsite are both downstream of what’s in among the larger web dev community, they’re just lagging behind it by several years (possibly; I wouldn’t be so sure about the fourteen-year-old). There are of course platforms (it’s always platforms) that have survived losing their in-ness and are still in widespread use (even PHP, arguably), but those are generally described as proven, reliable, stable, well-supported, and so on. I’m not convinced that at this point Wordpress still has that road open to it, instead of only the usual slow decay of widely-deployed software, but either way I don’t think that a transition to a legacy status is a nonevent. It just takes some time to show up in the deployment statistics. reply rafaelmn 9 hours agoprevHonestly I don't get the backlash Mullenweg is getting - a corp is trying to freeload on OSS community, GPL let's them have the code - but I'm all in favor of kicking then off of everything else. reply maccard 8 hours agoparentIt’s a bit two faced. He’s clearly abusing his position, annd every time this topic comes up (see elasticsearch, terraform, redis from recent memory) HN has clearly been on the side of the open source license. But, that’s not why he’s getting backlash. He’s getting backlash because he does what the “other side” did in all of these scenarios - invent reasons that are unrelated to the license dispute to cause the split. In this case, delisting their plugins from the central marketplace and implying trademark violations - while claiming that the problem is that they don’t contribute back to the ecosystem. The real problem is that automattic want wpengine to “contribute their share” - by development or rev share, and they’re using dirty tricks and smear tactics to do so. reply josephg 8 hours agorootparentYep. And they’ve gone 0-to-100 within days. I think he has a fair point - I’d be totally comfortable with a standing expectation in a community like Wordpress for companies to pay their way (or be looked down on & excluded from taking part in community events). But the expectations have to be a fair & transparent, and ideally communicated from the start, so people and companies can make informed choices about how they want to be involved. Not suddenly enforced with no lead time, with demands of money amounts seemingly made up on the spot, and “scorched earth” tactics when the demands aren’t met. As a rule of thumb, bikes can start and stop fast. Cars should be more predictable on the roads. And trucks should accelerate and turn more slowly still. If you’re big and powerful (government, standards body, maintainer of a huge opensource project, etc) you need to telegraph your moves and act slowly and predictably so other people can react to you. This is not how you do that. reply maccard 5 hours agorootparent> I think he has a fair point - I’d be totally comfortable with a standing expectation in a community like Wordpress for companies to pay their way (or be looked down on & excluded from taking part in community events). I agree with you, but this is a well trodden topic in the development community, particularly on HN - see Terraform for example [0]. One of the problems is that the world WP came to exist in and came to dominate the web in doesn't exist in the same form. There's been a few attempts at thisand they've caused large fractures - nobody has really got it right yet. On one hand you have a very vocal, and powerful group of people who believe that the freedom of the software is far more important than anything else, and those groups are often backed by large organisations that have the people-power to continue that effort (see: TF -> OpenTofu with Spacelift and co, Redis -> Valkey backed by AWS, and the OG split with Elastic). On the other hand you have a less vocal group who are more concerned with the functionality of the software rather than the original agreement that was signed (fair warning I fall in this category - I'm trying to remain Swiss in this comment), and accept that the terms of the deal have changed in that being Jeff'ed. (My reading of) The value of OSS in this group is the ability to for community to improve things, and to not end up locked into a problem that you have no solution to. The advantage here is that this group can just hitch their wagon to whichever solution appears to win out. I don't see an easy path out of this that satisfies both camps, unfortunately. [0] https://news.ycombinator.com/item?id=37081306 reply chii 6 hours agorootparentprev> ideally communicated from the start if only a minority could grow big and sufficiently high revenue generating to be capable of paying anything, then having this concept of \"paying back\" from the start would've been a chilling effect on the adoption in the first place. wordpress ecosystem is big, but it is the network effect, rather than the software itself. This network effect require lots of individual participants to kick start it at the beginning. Those participants will benefit from the software being free. If it was known at the start, that if you grew to a certain size, you'd have to start paying a royalty of some sort (which is the \"community expectation of pay or contribution\"), then you may not even start using the software in the first place - or at least, consider alternatives. This makes the ecosystem small. They don't call it bait and switch for nothing. The expectation of contribution will never be transparent from the beginning. reply rafaelmn 3 hours agorootparentprev> The real problem is that automattic want wpengine to “contribute their share” - by development or rev share, and they’re using dirty tricks and smear tactics to do so. Again I don't see what's wrong with this ? Clearly they have no recourse through GPL but they have trademarks and infrastructure. I don't see anything unreasonable about his behavior. reply kmlx 8 hours agoparentprev> a corp is trying to freeload on OSS community automattic is worth a few billion. what are talking about? reply rafaelmn 3 hours agorootparentThey are the single largest contributor to the project ? reply ThatPlayer 8 hours agoparentprevBecause I disagree with this definition of freeloading. They've offer a free, open source, very popular plugin for Wordpress, according to the Wordpress.org's plugin page, with 2+ million installs. Wordpress forking and taking over the plugin seems like they've accepted the code/contribution. reply rafaelmn 3 hours agorootparentNot contributing back to the project you're basing your business (eg. the smallest example being infrastructure they are cut off from). Honestly zero sympathy for WP engine, and I don't really see a better way to force them to pay. reply xd1936 5 hours agoparentprevThere's history. https://bullenweg.com/ reply Crazyontap 9 hours agoprevSo, ACF injected notices into everyone's dashboards to push their own legal agenda. It’s a move that reeks of self-interest more than community benefit. While everyone’s ready to grab their pitchforks at Matt, this actually sounds somewhat reasonable. Still, given its impact, this could easily be seen as a breach of trust. Definitely a move that's going to stir the pot. reply DonnieBurger 7 hours agoparentI believe you may have the story confused. Please correct me if otherwise. WordPress was the one who injected notices into everyone's dashboard. This started because the WP dashboard shows the blogs from wordpress.org, and then they published this post: https://wordpress.org/news/2024/09/wp-engine/ The result was WP Engine removing the widget that shows wordpress.org blogs on their installs. reply aimazon 9 hours agoparentprevWhere does this claim come from? The article doesn't include it. Matt hasn't made this claim either. reply kioleanu 9 hours agoparentprevThey should do that for all plugins that do it then, if it’s reasonable, right? Which is a lot of plugins. PS: isn’t WooCommerce doing the exact same? reply echoangle 7 hours agoparentprevAssuming the notices are real and a problem, Automattic should cite that instead of making it about security issues. As it stands, that's just a claim. reply beezlewax 9 hours agoparentprevWhat notices were these? reply that_guy_iain 6 hours agoparentprevJust like WordPress does, right? I assume you work for automattic, right? I've noticed when it comes to comments like this, normally they're made by automattic employees. reply RobotToaster 8 hours agoparentprevHonestly, everyone involved in this situation seems terrible. With my own experience of the WP \"community\", that isn't a surprise. reply danielovichdk 9 hours agoprevI like this whole so-called debate because it mostly shows that if 40% of Websites can be run on top WP then there is clear telling that we either haven't gone very far as an industry or that people that make websites couldn't give less fucks about who owns what plugin and what the fuck else people are yapping about. Hats of to Matt for at least showing some personality and showing a bit of faith. Now, go build another CMS. Use Rust or Go perhaps and make sure it can scale wildly reply 2024user 6 hours agoprev [–] The whole thing is confuinsing. It turns out WP Engine is not part of WordPress (despite the name) and has never contributed to WordPress. WordPress are trying to take more control of their name/trademark. reply chii 6 hours agoparent [–] There's more to the story, as recounted in other threads here. It isn't as one sided as you would imply. reply 2024user 3 hours agorootparent [–] Explain it then? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "WordPress.org has taken control of the WP Engine plugin, creating a fork called \"Secure Custom Fields,\" to remove commercial upsells and address a security issue.",
      "This action follows a lawsuit by WP Engine against WordPress co-founder Matt Mullenweg and Automattic, marking a rare instance of WordPress forcibly taking a plugin without consent.",
      "Users not associated with WP Engine, Flywheel, or ACF Pro are advised to download the genuine version from the ACF site for updates."
    ],
    "commentSummary": [
      "WordPress.org has assumed control of a WP Engine plugin, leading to user concerns and sparking a debate over the appropriateness of WordPress's actions.",
      "The controversy has raised discussions about WordPress's influence over plugins and its implications for the broader community, with some viewing the move as hypocritical and others as a security necessity.",
      "This situation underscores tensions between WordPress and WP Engine, with allegations of unfair practices and worries about the future of WordPress's ecosystem."
    ],
    "points": 251,
    "commentCount": 185,
    "retryCount": 0,
    "time": 1728806707
  },
  {
    "id": 41824852,
    "title": "ACF has been hijacked",
    "originLink": "https://anderegg.ca/2024/10/13/acf-has-been-hijacked",
    "originBody": "ACF has been hijacked October 13, 2024 It’s super late at night on Thanksgiving weekend in Canada. I shouldn’t be thinking about weird internet drama, but here we are. Since I last wrote about the ongoing WordPress drama: Matt Mullenweg promoted a “fork” of WordPress that wasn’t actually a fork. He then hijacked one of the most prominent plugins in the WordPress development world. The first point is pretty minor, but highlights the depths of strangeness at play. Vinny Green created a project called FreeWP that… actually, I don’t actually know what it’s about. You need to read the site for yourself. It seems to be an announcement of an organization that includes a news site, a class action lawsuit, and some other things. It’s not really clear to me, and it seems like it might be an elaborate troll. This normally wouldn’t be news, except Matt Mullenweg made it so. It’s uncertain if Mullenweg even understood what Green was building, but Green was quick to point out that it wasn’t a fork of WordPress. Mullenweg then amended his post to include AspirePress, and noted a spelling error. The whole thing seems strange, but I’m assuming that Mullenweg wrote the blog post to make fun of potential WordPress forks. The bigger issue happened on Saturday when Automattic hijacked the Advanced Custom Fields (ACF) plugin. As I’ve written in the past, ACF is a major plugin in the WordPress development world, and a requirement for many custom websites. It’s also owned by WP Engine, the company Mullenweg is beefing with. In the previous link, I had guessed that Mullenweg intended to kick them out of the WordPress plugin directory. Turns out, they went one further. In a post titled “Secure Custom Fields”, and in the category “Security”, Mullenweg posted that he was invoking “point 18” of the plugin directory guidelines to hijack the ACF plugin. ACF is ostensibly offered under the GPL because it interfaces with WordPress. That said, it doesn’t have an explicit license listed on its GitHub page. Still, it’s almost certainly legal and reasonable for Automattic to fork it and do whatever it wants with the source code. But that’s not the point. ACF is something that WordPress users trust and expect to come from a specific source. That Automattic would unilaterally decide to hijack such a popular plugin is completely insane. I’m not sure how this differs from a supply-chain attack. As I’ve written, the reason for this is invented and brought on by Automattic’s blocking of WP Engine employees from WordPress.org. Automattic just happened to find a mild vulnerability in ACF, and is now using the block Automattic imposed as a reason to take control of the plugin because the ACF team can’t update the plugin while the block is in place. This is some Grade-A 100% bullshit. The ACF site has been updated with a notice about the takeover, but most users likely won’t see this. The team behind the WordPress plugin directory could now update ACF to make any changes they’d like. If they’re willing to do this, I wouldn’t trust any plugins hosted on WordPress.org. I really don’t know what to say at this point. I assumed that ACF would be removed from the WordPress plugin directory, but I never would have guessed it would be hijacked. It seems like Mullenweg has lost the plot completely. If you use WordPress for a living, I recommend strongly that you consider changing platforms.",
    "commentLink": "https://news.ycombinator.com/item?id=41824852",
    "commentBody": "[dupe] ACF has been hijacked (anderegg.ca)221 points by GavinAnderegg 15 hours agohidepastfavorite66 comments ChrisArchitect 12 hours ago[dupe] Lots more discussion: https://news.ycombinator.com/item?id=41821400 https://news.ycombinator.com/item?id=41821336 reply whalesalad 13 hours agoprevI was heavily involved with Wordpress from about 2006 to 2012. I made it do things it was never designed to do before a lot of plugins like this existed. It was garbage then and it’s still garbage now. I stopped using it primarily because I saw what a cluster fuck the internals were and how out of control the plugin upsell ecosystem became. There were inklings of this behavior from the supreme leader too, like believing theme sales were antithetical to the entire point of WP. So I jumped ship with a real bad taste in my mouth and never looked back. I’ve tried it a handful of times over the year and it still looks like the same turd with a few more layers of polish. Still won’t scale out of the box without caching plugins. The irony of this entire situation is Matt didn’t even make Wordpress. It was forked from a blogging engine called b2. How’s that expression go? You either die a hero, or live long enough to see yourself become the villain. reply yard2010 12 hours agoparent// (so much for) code is poetry reply cranium 13 hours agoprevWhat an ego trip... now I'll definitely stop considering WordPress, even if it perfectly fills the use-case (mine or client's). I know it was frustrating for Automattic to see WPEngine as a leecher, but to be this hostile and volatile does not inspire confidence. What if you had a WP instance hosted by Automattic and said something the leadership does not approve? Will you get banned with no way of recovering your website? (Ghost had a similar story.) reply navigate8310 13 hours agoparentSlightly tangential, ACF support forum https://wordpress.org/support/topic/if-this-is-the-fork-wher... had many users calling out Matt regarding this unethical takeover; all comments are now purged and thread locked. reply kcrwfrd_ 13 hours agoparentprevWhat’s the story with Ghost? reply ookblah 13 hours agoprevhe must be having a legit mental breakdown. i do not understand any of these decisions done so haphazardly with no regard to users or their current situation, even if that was the direction they were moving. basically, telegraphing that he will personally go out and fuck up your day if you cross him. pettiness to the nth degree right here. reply Analemma_ 13 hours agoparentAt first we were saying it as a joke, but I am increasingly seriously wondering just how many famous people in the Valley are in various stages of stimulant psychosis, considering how widespread the joking-not-joking talk is about liberally using Adderall etc. to maximize \"the grind\". reply purple-leafy 13 hours agoparentprevDon’t casually suggest “mental breakdown” for people and situations you do not know or have first party insight to. First, blaming things on “mental breakdowns” is incredibly lazy and shallow and belittles the struggle that people with mental illness have. Did you ever stop to think that maybe this guy is just greedy, or an incompetent CEO? reply Ey7NFZ3P0nzAe 8 hours agorootparentWell, an essential part of psychiatric diagnosis is often to notice the presence of a noticeable before/after change. Psychosis, mania, are valid hypothesis that would make a CEO take surprising decision. I don't see how that belittles the struggle of patients. Having and company and being bipolar is far from life on easy mode. Greed and incompetence are also valid hypothesis, although don't necessarily need an abrupt change in behavior. reply ookblah 13 hours agorootparentprevhe could be all of them? i'm basing this off the fact that he was able to run and build it up to what it is today, then suddenly going off the rails. more of me grasping at an explanation than a declaration of truth heh. reply baggy_trough 13 hours agorootparentprevYes but I think it’s more likely he is having a mental breakdown. reply williamstein 5 hours agorootparentMatt said in his keynote that he had a kidney stone a few weeks ago, which is evidently extremely painful. Perhaps that physical trauma triggered something. reply gwerbret 13 hours agoprevAside: each and every post about Wordpress on HN over the past couple of days has been downweighted basically to oblivion (I expect this one to vanish from anywhere near the front page very soon). Is there a reason for this? The topic is rapidly evolving and is relevant to the HN community. reply awb 13 hours agoparentCheck out “how are stories ranked” in the FAQ: https://news.ycombinator.com/newsfaq.html Overheated discussions get demoted. I think the idea is that the comments should support discussion of the content, but not usurp it. reply suzzer99 12 hours agorootparent> How are stories ranked? > The basic algorithm divides points by a power of the time since a story was submitted. Comments in threads are ranked the same way. > Other factors affecting rank include user flags, anti-abuse software, software which demotes overheated discussions, account or site weighting, and moderator action. It could also be moderator action. My most viral submission suddenly dropped from the top story to page 8, despite having far more points than anything else on that page, and only being a few hours old. I suspect this happened because it was a negative post about Amazon. The comments were not overheated. Most posters agreed with my sentiment. reply ChrisArchitect 12 hours agoparentprevMultiple submissions seen by many and lots of discussion. Stuff moves fast. https://news.ycombinator.com/item?id=41821336 https://news.ycombinator.com/item?id=41821400 And that's just on this development. Each stage of this crazy story has had plenty of views and discussion here. reply akrotkov 13 hours agoparentprevI believe the comment-to-upvote ratio is triggering an automated down-weighing on most of them. reply yellow_lead 12 hours agoparentprevWhen the comment to upvote ratio is too high, posts are down weighted to prevent flamewars, apparently. reply hyperbrainer 13 hours agoparentprevDo note that there is barely any comments on any. So, maybe that is a factor. reply ars 13 hours agoparentprevThere's no such thing as downvoting a post on HN, only a comment. There's flagging a post, but that would show up next to the post - do you have any examples? reply gwerbret 13 hours agorootparentModerators can downweight posts to drop their rankings. Here are 3 examples: https://news.social-protocols.org/stats?id=41791369 https://news.social-protocols.org/stats?id=41815614 https://news.social-protocols.org/stats?id=41821336 Note the orange line indicating rank, which in every case shows a very sudden and precipitous drop in the rank of each post. reply Fej 12 hours agorootparentThose threads appear to be stoking the drama more than anything. HN's stated goal is to satisfy intellectual curiosity, and even if the post topic itself is of interest, if the discussion isn't substantive then the system is probably working as intended (regardless of whether it's the flamewar detector or a manual downweight). reply suzzer99 12 hours agorootparentprevI guess these only go back so far? I was hoping to see the graph on my post, which dropped like a stone after a few hours. (https://news.ycombinator.com/item?id=40992654) reply Zak 13 hours agorootparentprevModerators can reduce the position of a post on the front page. reply binary_slinger 13 hours agoprev> If you use WordPress for a living, I recommend strongly that you consider changing platforms. I initially thought this as well. There are alternatives but unless those alternatives are 100% API compatible with WP plugins and themes nothing is going to happen. Wordpress users and devs will continue to use WP. business as usual. Matt knows this. reply cwalv 13 hours agoparentI don't know much about WordPress, but it's pretty amazing to me how much staying power it's had. It seemed crusty, bloated and not long for this world 10 years ago to me. reply gnabgib 15 hours agoprevOngoing discussion (289 points, 8 hours ago, 125 comments) https://news.ycombinator.com/item?id=41821400 reply perihelions 13 hours agoparentAnd another one https://news.ycombinator.com/item?id=41821336 (\"Secure Custom Fields by WordPress.org (wordpress.org)\"; 11 hours ago, 153 comments) reply butterfly42069 14 hours agoprevEvery day that goes by I'm more satisfied with my decision a week a go to migrate everything I have/am building off of WordPress. Matt, if you read this... :( reply input_sh 13 hours agoparentFrom WordPress to what? reply butterfly42069 12 hours agorootparentOnly a week in, but at the moment I'm building out things with HUGO and experiments are going very well. Decided to seek out the absolute antithesis of WordPress after this experience, and don't wish to be dependent on peoples whims so much anymore. I recognise the limitations of SSGs, but I think these are overcomeable, and the benefits (Speed, CI) seem massive. I am open to hearing other suggestions people may have though. reply SansGuidon 10 hours agorootparentIt's maybe an issue with me but I've been on blogotext where I would post stuff, then on Hugo but the tooling was taking most of my energy and the version upgrade path was a blocker for my themes, plugins, etc. I was clueless how to solve those pains without coding and opening issues. Then I tried zola but it was buggy, and I had to learn Rust to fix one basic issue which took days of rewriting code review after review. And having yet to setup a pipeline and fight to make that work, just too much for me. Then I went to WordPress and didn't had to mess with trying to make the blogging system adapt to my needs with code, it was just flexible enough with a nice WYSIWYG editor and admin panel and plugins. No mess with ci/cd build times, manual upgrades and reading language specs and opening issues to make things work. Those things were not needed to just blog. Today I'm still on WordPress and none of the SSG feel simple enough to me. Git, markdown, build pipelines... Code editor. It's all fun for work and collaboration with devs but just out of interest for blogging. Also they mostly generate invalid HTML and lack features or have custom templates. And next upgrade could break everything. I prefer something that is helping me focus on blogging for long term without upgrade maintenance cost and without fearing platform dies. But yeah WordPress is not perfect and I'm considering maybe to glue a few tools together in the long run and make my edits in pure txt or HTML for which no existing SSG or WordPress are needed. reply parkcedar 12 hours agorootparentprevInterested to hear your experience with HUGO- I’ve done a lot of development in Go and keen to give this a crack. reply butterfly42069 12 hours agorootparentFlawless, it's really easy to wrap your head around (especially if you grok Go). I would recommend spinning up the most basic site from scratch to give it a try, takes minutes tops and its got a built in dev server to see your site. It pretty much all rapidly clicked into place from there. The idea of adding content as markdown is so easy and appealing, and the flow is so logical. The build times make me smile. Everything feels so rapid and under my control. reply mastazi 13 hours agoprevI hope Matt can get better but in the meantime, the community needs to fork. In the same way that LibreOffice forked from OpenOffice. Otherwise the blogosphere is just going to adopt one of the competing platforms and many of them (at least many of the \"user friendly\" ones) are not open source. reply navigate8310 13 hours agoparenthttps://www.classicpress.net reply yellow_lead 12 hours agoparentprevAs much as the community may want a fork, I suspect it's not going to pick up much momentum unless it's created by a larger company with skin in the game, i.e WPEngine. reply thih9 12 hours agorootparentWPEngine starting a foundation and successfully forking Wordpress would have been an appropriate plot twist. reply mastazi 12 hours agorootparentprevI agree and that is also my fear. That would mean that people will just move to something proprietary like Squarespace or Wix. This type of shift has happened many times before in tech so I consider it likely. reply tasuki 12 hours agoparentprevWordPress is pretty terrible. Perhaps the community can start using something better? Drupal or something. Something slightly less spaghetti... Perhaps this is all for the good of humanity. reply mastazi 11 hours agorootparentI get where you're coming from but I find it more likely that most people will just move to Squarespace, Wix etc. - away from open source and towards proprietary platforms. reply hyperbrainer 13 hours agoprevWhat kind of lawyer would let this happen in the middle of a lawsuit? I know lawyers do not control their clients, but this is ineffable. Even common sense should know better. reply yellow_lead 12 hours agoparent\"What lawyer? I'm the CEO, I can do anything I want.\" reply bigiain 13 hours agoprev\"If they’re willing to do this, I wouldn’t trust any plugins hosted on WordPress.org.\" Yep yep yep. Jesus Fuck Matt, put down the crackpipe and open the window. You are _totally_ out of control here. I am 100% going to start another much more urgent discussion at work on Monday about how we remove all risk of relying on anything from Automattic, wordpress.org, or The WordPress foundation. This will include opening a discussion with WPEngine (where we host about two dozen internal and customer sites) about what their short/medium/longterm plans are and what sort of guarantee they are planning to provide about updates and security fixes to the plugins and themes we rely on. It will include an internal discussion of whether we own it to all our clients running WP to inform then of this stupid stupid drama and the risks in represents and what we are doing to mitigate them. It will also include a very serious discussion about a million dollar government RFQ we submitted last month for a project that has a plan to use WP for the public facing website component. reply outsomnia 12 hours agoparentYou have been and continue to trust Automattic for the core code. If for example, Automattic instead had said they will bundle the plugin functionality with the core, there are many historical cases of that, unpleasant as it is for the third party usually... results are identical, right? reply bigiain 12 hours agorootparent> continue to trust Automattic for the core code That is absolutely no longer true. Which is very very sad. reply outsomnia 10 hours agorootparentThis plugin can only operate on top of the core code, whoever distributes the plugin to you. It means you have to decide to either bin the whole ecosystem, or use the core and plugin from the same people. It's also open to the plugin people to distribute the core themselves, but since they don't have a history of working on it, why would you imagine for core maintenance, you can trust a smaller private equity-funded group that historically leeches on the core project, more than the originating project for the core? reply ds 13 hours agoprevI talked at length with theo about this here if anyone wants a catch up from the very start https://youtu.be/u-KCKEWMt-Q?t=774 Cliffnotes- This is a absolutely insane situation but matt has come out looking insanely bad imo. reply hakanderyal 12 hours agoprevAs the saying goes, half the internet runs on Wordpress. Aside from a nuclear incident like an auto upgrade that permanently breaks all of the sites, it'll continue to be used. Maybe Matt is counting on this? reply butterfly42069 12 hours agoparentI think he's massively underestimated the ingenuity of developers who wish to not have work undone on the whims of a tyrant. If there's one thing we don't like it's FUD on the future of something we want to have completed/easily maintainable. reply balls187 12 hours agoprevI’m sure was covered in a comment on another thread—how is Mullenwag’s behavior different than other OSS projects wanting compensation when their work is monetized, especially from large well funded companies? reply CiPHPerCoder 12 hours agoprevI'd been staying out of this conflict, partly because I'm not really in the know on WP Engine's behavior behind-the-scenes and, as weird as Mullenweg's plays have been, I don't like to comment on things I'm not fully read into. But, this touches on a particular hobby horse of mine. It involves some old conflicts too, but I don't want to ruminate on them. From about 2016 to 2019, I was heavily involved with trying to remedy what I considered an existential threat to the Internet: WordPress's auto-updater. https://core.trac.wordpress.org/ticket/25052 + https://core.trac.wordpress.org/ticket/39309 If that sounds alarming, consider the enormity of WordPress's market share. Millions of websites. W3Techs estimates it powers about 43% of websites whose server-side stack is detectable. At the time, it was a mere 33%. https://w3techs.com/technologies/overview/content_management For the longest time, the auto-updater would pull an update file from WordPress.org, and then install it. There was no code-signing of any form until I got involved. So if you pop one server, you get access to potentially millions. Now imagine all of those webservers conscripted into a DDoS botnet. Thus, existential threat to the Internet. Eventually, we solved the immediate risk and then got into discussing the long tail of getting theme and plugin updates signed too. https://paragonie.com/blog/2019/05/wordpress-5-2-mitigating-... https://core.trac.wordpress.org/ticket/49200 You can read my ideas to solve this problem for WordPress (and the PHP ecosystem at large) here: https://gossamer.tools Here's the part that delves into old drama: Mullenweg was so uncooperative that I wrote a critical piece called #StopMullware (a pun on \"malware\") due to his resistance to even commit to solving the damn problem. On my end, I reimplemented all of libsodium in pure PHP (and supported all the way back to 5.2.4 just to cater to WordPress's obsession with backwards compatibility to the lowest common denominator), and just needed them to be willing to review and accept patches. Even though I was shouldering as much of the work as I logically could, that wasn't enough for Matt. After he responded to my criticism, I took it down, since he committed in writing to actually solving the problem. (You can read his response at https://medium.com/@photomatt/wordpress-and-update-signing-5... if you care to.) The reason I'm bringing this old conflict up isn't to reopen old wounds. It's that this specific tactic that Mullenweg employed would have been mitigated by solving the supply chain risk that I was so incandescent about in 2016. (If you read my proposals from that era, you'll notice that I cared a lot about the developers controlling their keys, not WordPress.) I don't keep up-to-date on Internet drama, so maybe someone already raised this point elsewhere. I just find it remarkable that the unappreciated work for WordPress/PHP I did over the years is relevant to Mullenweg's current clusterfuck. Incredible. Since my knowledge on the background noise that preceded this public conflict is pretty much nil, I have no reason to believe WP Engine hold any sort of moral high ground. And I don't really care either way. Rather, I'd like to extend an open invitation: If anyone is serious about leading the community to fork off WordPress, as I've heard in recent weeks, I'm happy to talk at length about my ideas for security enhancements and technical debt collection. If nothing else comes of this, I'd like to minimize the amount of pain experienced by the community built around WordPress, even if its leadership is frustrating and selfish. reply rafark 2 hours agoparentVery interesting. I’ve been writing code for a while but if I’m honest I have no idea how code signing works. Any good resource on how it works especially in php? reply benatkin 12 hours agoprev> If they’re willing to do this, I wouldn’t trust any plugins hosted on WordPress.org. I wouldn't be surprised if the original author of ACF trusts WordPress more. His last commit was more than 3 years ago and he hasn't shown up on X to defend WP Engine. https://github.com/AdvancedCustomFields/acf/commits?author=e... reply niobe 13 hours agoprevAnd we get yet another case study in how ego destroys value reply outsomnia 12 hours agoparentGPL does not make any representations about private equity being able to extract value from the work. reply analcryptok 13 hours agoprevCurrently, there are lots of applications that bring winnings in the form of prizes, so always be careful, sometimes applications like that should not be installed immediately. reply outsomnia 13 hours agoprev [–] Sorry, this is a GPL plugin to stuff already maintained by Automattic? It's not like users aren't already updating to whatever Automattic want to give them, in the core, if that's the case? Automattic producing the same plugin and delivering it the same as the core doesn't sound like much of change, since users already trusted Automattic for the core either way... reply usea 12 hours agoparentIf the delivery service that transported my vendor's goods to me, suddenly started substituting their own product instead, I would sue them. I think my vendor would be pissed too, especially if the main difference is that their monetization was torn out. This behavior would land people in jail in a more serious industry. reply outsomnia 9 hours agorootparentNo... core volunteers who provide work to you for free, which you have been consuming successfully, have now extended the domain of their works to also encompass something on top you previously got from elsewhere. The plugin you previously used was always completely dependent on the work of the core volunteers; you were always consuming their work and nothing changed about that. It just also already includes the optional plugin now. Why would anyone end up in jail when everything is GPL2+? reply prettymuchnoone 13 hours agoparentprev [–] Well yes, but it's like going to buy a bottle of Coke and finding out it's now Koke (but actually Pepsi inside)...it's iffy reply outsomnia 12 hours agorootparentUsers of the plugin already have a trust and consumption relationship with Automattic for the core. It's more like mcdonalds replacing Coke with McCola with your mcdonalds meal - you were already trusting mcdonalds for the food. But even that is a stretch since both are GPL2 and there's no current sign the plugin Automattic provide differs from the WP Engine one. GPL is on both sides, nothing stops WP Engine doing the same and providing their own flavour of core with their plugin, if that's what people want. Of course that costs more than private equity just using Automattic's core for free. reply dabinat 12 hours agorootparentI feel like the dodgy part isn’t the forking. Any open source project can be forked at any time by anyone. The dodgy part is them automatically switching existing users to their fork. To use your McDonald’s analogy, it’s like specifically ordering a Coke and McD’s secretly switching it to a McCoke without you noticing. reply outsomnia 10 hours agorootparentAs I wrote elsewhere, this is no different from a project deciding to incorporate a third party's functionality into the core. Either way whoever provides the plugin, you trust the provider to provide the core, if you now think they are going to do bad things, there is nothing they can do in the plugin that they couldn't do in the core without all this drama. It seems the \"perceptual framing\" that is being engineered about this, that Automattic and its leader should be cancelled, is not about technical issues. reply pavlov 11 hours agorootparentprev [–] If you were buying Coke at a store owned by Pepsi, it almost seems inevitable. I’m not saying it’s right, but it’s just the kind of thing that one expects from American corporations. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Automattic took control of the Advanced Custom Fields (ACF) plugin, a significant tool for WordPress developers, which was owned by WP Engine.",
      "The takeover was justified by a minor vulnerability and a restriction on WP Engine employees, leading to concerns about trust and control over WordPress plugins.",
      "Users are encouraged to reassess their dependency on WordPress in light of these events."
    ],
    "commentSummary": [
      "WordPress is facing criticism over internal issues and its plugin ecosystem, with recent actions by Automattic's CEO, Matt Mullenweg, causing controversy.- Concerns about trust, control, and the future of WordPress have led to discussions about a potential community fork, similar to other open-source projects.- The situation raises broader questions about leadership and decision-making within the tech industry."
    ],
    "points": 221,
    "commentCount": 66,
    "retryCount": 0,
    "time": 1728788734
  },
  {
    "id": 41824390,
    "title": "FLUX is fast and it's open source",
    "originLink": "https://replicate.com/blog/flux-is-fast-and-open-source",
    "originBody": "FLUX is fast and it's open source Posted October 10, 2024 by @bfirsh FLUX is now much faster on Replicate, and we’ve made our optimizations open-source so you can see exactly how they work and build upon them. Here are the end-to-end speeds: FLUX.1 [schnell] at 512x512 and 4 steps: 0.29 seconds (P90: 0.49 seconds) FLUX.1 [schnell] at 1024x1024 and 4 steps: 0.72 seconds (P90: 0.95 seconds) FLUX.1 [dev] at 1024x1024 and 28 steps: 3.03 seconds (P90: 3.90 seconds) This is from the west coast of the US using the Python client. Here’s a demo of FLUX.1 [schnell]. (It’s live, just start typing!) Here’s the full app, and source code, if you’d like to check it out. How did we do it? Most of the models on Replicate are contributed by our community, but we maintain the FLUX models in collaboration with Black Forest Labs. We’ve done two main things to make FLUX faster: We optimized the model. We used Alex Redden’s flux-fp8-api as a starting point, then optimized it with torch.compile and used fast CuDNN attention kernels in the nightly Torch builds. We added a new synchronous HTTP API that makes all image models much faster on Replicate. The quantization in flux-fp8-api slightly changes the output of the model, but we have found it has little impact on the quality. We’ve created a tool that compares the output of thousands of prompts on FLUX.1 [schnell] and FLUX.1 [dev]. We’re not cherry picking. Take a look for yourself. You can disable this by setting the go_fast input on the model to false. We want to be open with you about how we’re optimizing the models. It’s notoriously hard to compare output between models and providers, and it’s often unclear whether providers are doing things that impact the quality of the model. We’re just going to tell you how we did it and let you disable any optimizations. That means you’re not wondering whether the output you’re getting is the best quality it can be. Most importantly, the code is open-source, so you can see exactly how it works: github.com/replicate/cog-flux Open-source should be fast too Open-source models are often slow out of the box. Model providers then optimize these models to make them fast and release them behind proprietary APIs, without contributing the improvements back to the community. We want to change that. We think open-source should be fast too. We’re open-sourcing all the improvements we make to FLUX. We’re also collaborating with the AI Compiler Study Group and other AI researchers to make an open-source fast version of FLUX. Making the FLUX optimizations open-source is not just the right thing to do, it also means all the experts in the world can collaborate together to make it the fastest. Pull requests welcome. It’s going to get faster New techniques are coming out all the time to make models faster, and by collaborating with the community, you can be sure that they’re going to be on Replicate as fast as possible. Stay tuned. Do more with FLUX You can do more than just run FLUX on Replicate. You can: Fine-tune FLUX on your own data (training and running trained models is going to much faster soon too!) Edit the code and deploy a custom version, if you’re doing something advanced Try out the models and compare outputs on our new playground Follow us on X to keep up to speed.",
    "commentLink": "https://news.ycombinator.com/item?id=41824390",
    "commentBody": "FLUX is fast and it's open source (replicate.com)221 points by smusamashah 17 hours agohidepastfavorite109 comments sorenjan 15 hours agoText to image models feels inefficient to me. I wonder if it would be possible and better to do it in separate steps, like text to scene graph, scene graph to semantically segmented image, segmented image to final image. That way each step could be trained separately and be modular, and the image would be easier to edit instead of completely replace it with the output of a new prompt. That way it should be much easier to generate stuff like \"object x next to object y, with the text foo on it\", and the art style or level of realism would depend on the final rendering model which would be separate from the prompt adherence. Kind of like those video2video (or img2img on each frame I guess) models where they enhance the image outputs from video games: https://www.theverge.com/2021/5/12/22432945/intel-gta-v-real... https://www.reddit.com/r/aivideo/comments/1fx6zdr/gta_iv_wit... reply miki123211 8 hours agoparentIn general, it has been shown time and time again that this approach fails for neural network based models. If you can train a neural network that goes from a to b and a network that goes from b to c, you can usually replace that combination with a simpler network that goes from a to c directly. This makes sense, as there might be information in a that we lose by a conversion to b. A single neural network will ensure that all relevant information from a that we need to generate c will be passed to the upper layers. reply sorenjan 7 hours agorootparentYes this is true, you do lose some information between the layers, and this increased expressibility is the big benefit of using ML instead of classic feature engineering. However, I think the gain would be worth it for some use cases. You could for instance take an existing image, run that through a semantic segmentation model, and then edit the underlying image description. You could add a yellow hat to a person without regenerating any other part of the image, you could edit existing text, change a person's pose, you could probably more easily convert images to 3D, etc. It's probably not a viable idea, I just wish for more composable modules that lets us understand the models' representation better and change certain aspects of them, instead of these massive black boxes that mix all these tasks into one. I would also like to add that the text2image models already have multiple interfaces between different parts. There's the text encoder, the latent to pixel space VAE decoder, controlnets, and sometimes there a separate img2imgstyle transfer at the end. Transformers already process images patchwise, but why does those patches have to be even square patches instead of semantically coherent areas? reply smrtinsert 2 hours agorootparentprevIt's my understanding an a-c will usually be bigger parameter wize and more costly to train reply kqr 12 hours agoparentprevIsn't this essemtially the approach to image recognition etc. that failed for ages until we brute forced it with bigger and deeper matrices? It seems sensible to extract features and reason about things the way a human would, but it turns out its easier to scale pattern matching purely done by computer. reply WithinReason 12 hours agorootparentThis is Sutton's Bitter Lesson: https://www.cs.utexas.edu/~eunsol/courses/data/bitter_lesson... reply selvan 10 hours agorootparentFrom the PDF - \"One thing that should be learned from the bitter lesson is the great power of general purpose methods, of methods that continue to scale with increased computation even as the available computation becomes very great. The two methods that seem to scale arbitrarily in this way are \"search\" and \"learning\". The second general point to be learned from the bitter lesson is that the actual contents of minds are tremendously, irredeemably complex; we should stop trying to find simple ways to think about the contents of minds, such as simple ways to think about space, objects, multiple agents, or symmetries. All these are part of the arbitrary, intrinsically-complex, outside world. They are not what should be built in, as their complexity is endless; instead we should build in only the meta-methods that can find and capture this arbitrary complexity. Essential to these methods is that they can find good approximations, but the search for them should be by our methods, not by us. We want AI agents that can discover like we can, not which contain what we have discovered. Building in our discoveries only makes it harder to see how the discovering process can be done.\" reply nuancebydefault 8 hours agorootparentprevIf I would take the Lesson literally, we should not even study text to image. We should study how a machine with limitless cpu cycles would make our eyes see something we are currently thinking of. My point being, optimization or splitting up int subs, before handing over the problem to the machine, makes sense. reply stoniejohnson 2 hours agorootparentI think the bitter lesson implies that if we could study/implement \"how a machine with limitless cpu cycles would make our eyes see something we are currently thinking of\" then it would likely lead to a better result than us using hominid heuristics to split things into sub-problems that we hand over to the machine. reply nuancebydefault 2 hours agorootparentThe technology to probe brains and visual related neurons exists today. With limitless cpu cycles we would for sure be able to do make us see whatever we think about. reply stoniejohnson 1 hour agorootparentI'm not really familiar with that technology space, but if you take that as true, is your argument something like: - We don't have limitless CPU cycles - Thus we need to split things into sub-problems If so that might still be amenable to the bitter lesson, where Sutton is saying human heuristics will always lose out to computational methods at scale. Meaning something like: - We split up the thought to vision problem into N sub-problems based on some heuristic. - We develop a method which works with our CPU cycle constraint (it isn't some probe -> CPU interface). Perhaps it uses our voice or something as a proxy for our thoughts, and some composition of models. Sutton would say: Yeah that's fine, but if we had the limitless CPU cycles/adequate technology, the solution of probe -> CPU would be better than what we develop. reply nuancebydefault 1 hour agorootparentI think Sutton is right that if we had limitless cpu, any human split up would be inferior. So indeed since we are far away from limitless cpu, we divide and compose. But i think we're onto something! Voice to image indeed might give better results than text to image, since voice has some vibe to it (intonation, tone, color, stress on certain words, speed and probably even traits we don't know yet) that will color or even drastically influence the image output. reply nuancebydefault 8 hours agorootparentprevA problem with image recognition i can think of, is that any rude categorization of the image, which is millions of pixels will make it less accurate. With image generation on the other hand, which starts from a handful of words, we can first do some text processing into categories, such as objects vs people, color vs brightness, environment vs main object, etc. reply nerdponx 6 hours agorootparentprevYou could imagine doing it with 2 specialized NNs, but then you have to figure out a huge labeled dataset of scene graphs. The problem fundamentally is that any \"manual\" feature engineering is not going to be supervised and fitted on a huge corpus, the way the self-learned features are. reply spencerchubb 15 hours agoparentprevThat's essentially what diffusion does, except it doesn't have clear boundaries between \"scene graph\" and \"full image\". It starts out noisy and adds more detail gradually reply WithinReason 12 hours agorootparentThat's true, the inefficiency is from using pixel-to-pixel attention at each stage. It the beginning low resolution would be enough, even at the end high resolution is only needed at the pixel's neighborhood reply ZoomZoomZoom 14 hours agoparentprevThe issue with this is there's a false assumption that an image is a collection of objects. It's not (necessarily). I want a picture of frozen cyan peach fuzz. reply llm_trw 13 hours agorootparenthttps://imgur.com/ayAWSKr Prompt: frozen cyan peach fuzz, with default settings on a first generation SD model. People _seriously_ do not understand how good these tools have been for nearly two years already. reply ZoomZoomZoom 8 hours agorootparentIf by people you mean me, then I wasn't clear enough in my comment. The example given implied an image without any objects the GP was talking about, just a uniform texture. reply sorenjan 7 hours agorootparentprevRunning that image through Segment Anything you get this: https://imgur.com/a/XzCanxx Imagine if instead of generating the RGB image directly the model would generate something like that, but with richer descriptive embeddings on each segment, and then having a separate model generating the final RGB image. Then it would be easy to change the background, rotate the peach, change color, add other fruits, etc, by editing this semantic representation of the image instead of wrestling with the prompt to try to do small changes without regenerating the entire image from scratch. reply thomashop 12 hours agorootparentprevYou can do this dynamically with Pollinations URLs too: https://pollinations.ai/p/frozen_cyan_peach_fuzz?seed=1 https://pollinations.ai/p/frozen_cyan_peach_fuzz?seed=2 https://pollinations.ai/p/frozen_cyan_peach_fuzz?seed=3 Disclaimer: I'm behind Pollinations.AI reply corn13read2 9 hours agorootparentcan do this with any image generation model. Disclaimer: I'm not behind any reply Zambyte 6 hours agoparentprevYou seem to be describing ComfyUI to me. You can definitely do this kind of workflow with ComfyUI. reply teh_infallible 12 hours agoparentprevI am hoping that AI art tends towards a modular approach, where generating a character, setting, style, and camera movement each happens in its own step. It doesn’t make sense to describe everything at once and hope you like what you get. reply portaouflop 1 hour agorootparentYou can already do that with comfyui - it’s just not easy to set up reply sorenjan 7 hours agorootparentprevDefinitely, that would make much more sense seeing how content is produced by people. Adjust the technology to how people want to use it instead of forcing artists becoming prompt engineers and settling for something close enough what they want. At the very least image generators should output layers, I think the style component is already possible with the img2img models. reply seydor 11 hours agoparentprevNeural networks will gradually be compressed to their minimum optimal size (once we know how to do that) reply trickstra 12 hours agoprevNon-commercial is not open-source, because if the original copyright holder stops maintaining it, nobody else can continue (or has to work like a slave for free). Open-source is about what happens if the original author stops working on it. Open-source gives everyone the license to continue developing it, which obviously means also the ability to get paid. Don't call it open-source if this aspect is missing. Only the FLUX.1 [schnell] is open-source (Apache2), FLUX.1 [dev] is non-commercial. reply uxhacker 6 hours agoparentThere is OpenFLUX.1 which is a fine tune of the FLUX.1-schnell model that has had the distillation trained out of it. OpenFLUX.1 is licensed Apache 2.0. https://huggingface.co/ostris/OpenFLUX.1/ reply starfezzy 10 hours agoparentprevDoesn’t open source mean the source is viewable/inspectable? I don’t know any closed source apps that let you view the source. reply dredmorbius 57 minutes agorootparent\"Open Source\" has a specific definition, created by the Open Source Initiative:Certain usages may be covered by trademark protection, as an \"OSI Approved License\":It's based on the Debian Free Software Guidelines (DFSG), which were adopted by the Debian Project to determine what software does, and does not, qualify to be incorporated into the core distribution. (There is a non-free section, it is not considered part of the core distribution.)Both definitions owe much to the Free Software Foundation's \"Free Software\" definition and the four freedoms protected by the GNU GPL: - the freedom to use the software for any purpose, - the freedom to change the software to suit your needs, - the freedom to share the software with your friends and neighbors, and - the freedom to share the changes you make. reply miki123211 8 hours agorootparentprev> Doesn’t open source mean the source is viewable/inspectable? According to the OSI definition, you also need a right to modify the source and/or distribute patches. > I don’t know any closed source apps that let you view the source. A lot of them do, especially in the open-core space. THe model is called source-available. If you're selling to enterprises and not gamers, that model makes sense. What stops large enterprises from pirating software is their own lawyers, not DRM. This is why you can put a lot of strange provisions into enterprise software licenses, even if you have little to no way to enforce these provisions on a purely technical level. reply havaker 9 hours agorootparentprevOpen source usually means that you are able to modify and redistribute the software in question freely. However between open and closed, there is another class - source-available software. From its wikipedia page: > Any software is source-available in the broad sense as long its source code is distributed along with it, even if the user has no legal rights to use, share, modify or even compile it. reply aqme28 8 hours agorootparentprevWebsite frontends are always source viewable, but that is not OSS. reply thomashop 12 hours agoprevIf you want to play with FLUX.schnell easily, type the prompt into a Pollinations URL: https://pollinations.ai/p/a_donkey_holding_a_sign_with_flux_... https://pollinations.ai/p/a_donkey_holding_a_sign_with_flux_... https://pollinations.ai/p/Minimalist%20and%20conceptual%20ar... It's incredible how fast it is. We generate 8000 images every 30 minutes for our users using only three L40S GPUs. Disclaimer: I'm behind Pollinations reply peterpans01 10 hours agoparentThe \"only\" word sounds quite expensive for most of us. reply Kiboneu 6 hours agorootparentHe started a whole business to help pay the installments. reply FridgeSeal 8 hours agorootparentprev“I have successfully destabilised many countries with only a few tanks”. reply jsemrau 14 hours agoprevMy favorite thing to do with Flux is create images with a white background for my substack[1] because the text following is amazing and I can communicate something visually through the artwork as well. [1]https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_... reply ruthmarx 13 hours agoparentThat example you gave is a good reason why artists get pissed off IMO. The LLM is clearly aping some artists specific style, and now missing out on paid work as a result. Not sure I have an opinion on that, technology marches on etc, but it is interesting. reply jsemrau 12 hours agorootparentI understand your point, but in 0% of all cases would I hire an artist to create imagery for my personal blog. Therefore, I would think that market doesn't exist. reply earthnail 10 hours agorootparentHowever, the blogs or newspapers or print outlets that used to hire them hired them because you couldn’t- it was a differentiator. That differentiator is gone, and as such won’t pay for it anymore. They’ll just use the same AI as you. This destroys the existing market of the artist. To be clear, my comment isn’t meant as a judgment, just as market analysis. reply jsemrau 9 hours agorootparentI think it does not take into consideration how much thought and expertise goes into design work. Have a look at the recent controversy about the live-action shooter \"concord\" that failed spectacularly mainly due to bad character design. Here are two videos that explain that well. I don't think I would ever be capable of designing with that degree of purpose given a generative AI tool. [1] https://www.youtube.com/watch?v=mVyXUMJLzE0 [2] https://www.youtube.com/watch?v=5eymH15AfAU reply ionwake 4 hours agorootparentThanks for the links Im glad there are people who are experts at character design. For my untrained eyes it just looks like all of the characters are muddy coloured ( washed out greens brown etc ) AND they are pretty much all incredibly ugly. I think I saw one that atleast looked fashionable, the black sniper female. The older I get the more concerned I get that the larger the team that makes decisions the worse the decisions are, whats the word for this? Is there any escape? Teamfortress 2, took years and teams to build, but it was just perfect. I heard they had a flat structure which is even more confusing as to how they attained such an excellent product. reply hansvm 3 hours agorootparentBureaucracy and hierarchy are much more damaging to good products than a large team. The flat structure and long timelines are how they overcame the limitations of a large team. reply smrtinsert 2 hours agorootparentprevThis is about as realistic as replacing coders with ai tools today. High level content organizations demand creative precision that even models like Flux can ape but not replace. Maybe to a non-artist it would be comparable, but to a creative team its not close. reply ruthmarx 11 hours agorootparentprevYeah, I get that completely, I'm the same way. I just think it's interesting. It's kind of the same argument as piracy, since most people wouldn't pay for what they download if it wasn't free. reply ilkke 11 hours agorootparentWhat is different in this case is that large companies are very likely looking to replace artists with ai, which is a huge potential impact. Piracy never had such risks reply jsemrau 10 hours agorootparentI think this will only happen if you could selectively replace parts within an image selectively and reliably. There are still major problems even in Photoshops genaI application. For example, it is not possible select the head of a person on a picture and then type \"smile\" to make the face smile. We might get there eventually. reply jsemrau 11 hours agorootparentprevI'd rather think it's the same argument as open-source and public domain. Currently, I am researching an agent that ReAct's through a game of TicTacToe. I am using a derivative of the open-source transformer's prompt reply ruthmarx 2 hours agorootparent> I'd rather think it's the same argument as open-source and public domain. In the context of the point I made, it's definitely more similar to piracy, since the point was about taking advantage of something that if not free people would not pay for. reply pajeets 11 hours agorootparentprevDont care about artists opinion on rest of using AI tools instead of not paying them because I couldnt and wouldnt so theres no demand in the first place. All I wanna know is the prompt that was used to generate the art speaking of which i wanna know how to create cartoony images like that OP reply slig 7 hours agoparentprevCould you share the prompt? Thanks. reply jsemrau 2 hours agorootparentThe prompt is actually not that interesting. \"A hand-drawing of a scientific middle-aged man in front of a white background. The man is wearing jeans and a t-shirt. He is thinking a bubble stating \"What's in a ReAct JSON prompt?\" In the style of European comic book artists of the 1970s and 1980s.\" Finding the right seed and model configuration is the more difficult part. reply pajeets 11 minutes agorootparentjust tried it out and it struggled with the bubble caption and adopting other drawing styles but oh god yes this is awesome because an image like this would take forever for me to do and if even arranging someone to commission it is expensive starving artists are going to famish now, not sure how to feel about it reply vunderba 15 hours agoprevFlux is the leading contender for a locally hosted generative systems in terms of prompt adherence, but the omnipresent shallow depth of field is irritatingly hard to get rid of. reply cranium 13 hours agoparentI guess it's optimized for artsy images? reply AuryGlenz 12 hours agorootparentThey almost certainly did DPO it, so that would have an effect. It was also probably just trained more on professional photography than cell phone pics. I’ve found it odd how there’s a segment of the population that hates a shallow depth of field now, as they’re so used to their phone pictures. I got in an argument on Reddit (sigh) with someone who insisted that the somewhat shallow depth of field that SDXL liked to do by default was “fake.” As in, he was only ever exposed to it through portrait mode and the like on phones and didn’t comprehend that larger sensors simply looked like that. The images he was posting that looked “fake” to him looked to be about a 50mm lens at f/4 on a full frame camera at a normal portrait distance, so nothing super shallow either. reply Adverblessly 9 hours agorootparentAs a DoF \"hater\", my problem with it is that DoF is just the result of a sensor limitation (when not used artistically etc.), not some requirement of generating images. If I can get around that limitation, there's very little motivation to maintain that flaw. In the real world, if I see a person at the beach, I can look at the person and see them in perfect focus, I can then look at the ocean behind them and it is also in perfect focus. If you are an AI generating an image for me, I certainly don't need you to tell me on which parts of that image I'm allowed to focus, just let me see both the person and the ocean (unless I tell you to give me something artsy :)). reply tcrenshaw 5 hours agorootparentWhile you could look at DoF as a sensor limitation, most photographers use it as an artistic choice. Sure, I could take a pic at f/16 and have everything within the frame in focus, but maybe the background is distracting and takes away from the subject. I can choose how much background separation I want; maybe just a touch at f/8, maybe full on blue at f/1.2 reply vunderba 4 hours agorootparentprevThat's pretty funny. It reminds me of if you grew up watching movies with the standard 24 fps - trying to watch films at 60fps later felt unnatural and fake. I'll say I'm okay with DOF - it just feels (subjectively to me) like its incredibly exaggerated in Flux. The workarounds have mostly been prompt based adding everything from \"gopro capture\" to \"on flickr in 2007\" but this approach feels like borderline alchemy in terms of how reliable it is. reply llm_trw 13 hours agorootparentprevGive it another month and it will be porn, just like sdxl. reply Zopieux 7 hours agorootparentWhat are you talking about, the model is months old, it's already all porn - and that's okay. reply marginalia_nu 1 hour agoprevGiven the HN exposure, feels like a huge missed opportunity to write anywhere in the article what FLUX even is and what it's for. A single sentence would help so much. The way it's written, you can read the entire thing and still have no clue. reply CosmicShadow 15 hours agoprevI just cancelled my Midjourney subscription, it feels like it's fallen too far behind for the stuff I'd like to do. Spent a lot of time considering using Replicate as well as Ideogram. reply simonjgreen 13 hours agoparentI have been questioning the value beyond novelty as well recently. I’m curious if you replaced it with another tool or simply don’t derive value from those things? reply pajeets 11 hours agoparentprevnever used midjourney because it had that signature look and bad with hands, feet, letters crazy not even a year has past since Emad's downfall a local open source and superior model drops which just shows how little moat these companies have and are just lighting cash on fire which we benefit from reply rolux 9 hours agorootparent> crazy not even a year has past since Emad's downfall a local open source and superior model drops > which just shows how little moat these companies have Flux was developed by the same people that made Stable Diffusion. reply aqme28 10 hours agorootparentprevFlux has a signature look too, it’s just a different one. reply keiferski 10 hours agorootparentprevIt’s very easy to turn off the default Midjourney look. reply thierryzoller 9 hours agoprevThey point to their comparison page to claim similar quality. First off it's very clear that the details are way less, but worse, look at the example \"Three-quarters front view of a yellow 2017 Corvette coming around a curve in a mountain road and looking over a green valley on a cloudy day.\" The Original model shows the FRONT, the speed version shows the BACK of the corvette. It's a completely different picture. This is not similar but strikingly different. https://flux-quality-comparison.vercel.app/ reply 112233 12 hours agoprevDoes someone know what FLUX 1.1 has been trained on? I generated almost hundred images on the pro model using \"camera filename + simple word\" two word prompts, and it all looks like photos from someones phone. Like, unless it has text I would not even stop to consider any of these images AI. They sometimes look cropped. A lot of food pictures, messy tables and appartments etc. Did they scrape public facebook posts? Snapchat? Vkontakte? Buy private images from onedrive/dropbox? If I put as the second word a female name, it almost always triggers nsfw filter. So I assume images in the training set are quite private. See for yourself (autoplay music warning): people: https://vm.tiktok.com/ZGdeXEhMg/ food and stuff: https://vm.tiktok.com/ZGdeXEBDK/ signs: https://vm.tiktok.com/ZGdeXoAgy/ [edit] Looking at these images feels uneasy, like I am looking at someones private photos. There is not enough \"guidance\" in a prompt like \"IMG00012.JPG forbid\" to account for these images, so it must all come from the training data. I do not believe FLUX 1.1 pro has radically different training set than these previous open models, even if it is more prone to such generation. It feels really off, so, again, is there any info on training data used for these models? reply smusamashah 11 hours agoparentIt's not just flux, you can do the same with other models including Stable Diffusion. These two reddit threads [1][2] explore this convention a bit. DSC_0001-9999.JPG - Nikon Default DSCF0001-9999.JPG - Fujifilm Default IMG_0001-9999.JPG - Generic Image P0001-9999.JPG - Panasonic Default CIMG0001-9999.JPG - Casio Default PICT0001-9999.JPG - Sony Default Photo_0001-9999.JPG - Android Photo VID_0001-9999.mp4 - Generic Video Edit: Also created a version for 3D Software Filenames (all of them tested, only a few had some effects) Autodesk Filmbox (FBX): my_model0001-9999.fbx Stereolithography (STL): Model0001-9999.stl 3ds Max: 3ds_Scene0001-9999.max Cinema 4D: Project0001-9999.c4d Maya (ASCII): Animation0001-9999.ma SketchUp: SketchUp0001-9999.skp [1]: https://www.reddit.com/r/StableDiffusion/comments/1fxkt3p/co... [2]: https://www.reddit.com/r/StableDiffusion/comments/1fxdm1n/i_... reply 112233 10 hours agorootparentThank you, this is good and horrific to know. The hair of my hair are standing on their end. Of all the models exibiting this behaviour, has anyone published, what are the training data sources? Like, honest list, not the PR-boilerplate. reply pajeets 11 hours agorootparentprevwow this is wild! https://i.postimg.cc/vT6SV7pq/replicate-prediction-6ap8z1jv5... https://i.postimg.cc/vZzMTM71/replicate-prediction-7r4b4p6sj... https://i.postimg.cc/rs6wM5LJ/replicate-prediction-d8s4c93v5... I DEMAND TO KNOW HOW RUN LOCAL SAAR reply jncfhnb 3 hours agorootparentI’m not sure what saar means here but these images are fairly standard and a drop in the bucket compared to the hideous number of porn fine tunes published daily on civit ai if that’s what you’re looking for reply pajeets 22 minutes agorootparentwait you think these images are pornographic? reply jncfhnb 3 hours agoparentprevI highly doubt it’s a product of the raw training dataset because I had the opposite problem. The token for “background” introduced intense blur on the whole image almost regardless of how it was used in the prompt, which is interesting because their prompt interpretation is much better. It seems likely that they did heavy calibration of text as well as a lot of tuning efforts to make the model prefer images that are “flux-y”. Whatever process they’re following, they’ve inadvertently made the model overly sensitive to certain terms to the point at which their mere inclusion is stronger than a Lora. The photos you’re showing aren’t especially noteworthy in the scheme of things. It doesn’t take a lot of effort to “escape” the basic image formatting and get something hyper realistic. Personally I don’t think they’re trying to hide the hyper realism so much as trying to default to imagery that people want. reply pajeets 11 hours agoparentprevI experienced the same thing, it was so weird i got good results in the beginning and then it \"craps out\" dont know why all the critical comments about flux are being downvoted or flag sure is weird reply jncfhnb 5 hours agoprevDoes this translate to gains on local with comfy reply ionwake 4 hours agoprevHow long does flux take to generate an image if it runs on an m1 macbook pro? Can anyone estimate? reply chmaynard 1 hour agoprevTastes great, too! reply swyx 15 hours agoprev> We added a new synchronous HTTP API that makes all image models much faster on Replicate. ooh why is synchronous fast? i click thru to https://replicate.com/changelog/2024-10-09-synchronous-api > Our client libraries and API are now much faster at running models, particularly if a file is being returned. ... thanks? just sharing my frustration as a developer. try to explain things a little better if you'd like it to stick/for us to become your advocates. reply weird-eye-issue 15 hours agoparentI mean it literally explains why in the second paragraph. It returns the actual file data in the response rather than a URL where you have to make a second request to get the file data reply swyx 13 hours agorootparentthats not \"making the image models much faster\", thats just making getting the image back slightly faster reply weird-eye-issue 6 hours agorootparentIn all practical senses it is the same thing reply popalchemist 12 hours agorootparentprevThe \"making the image models much faster\" part is model optimizations that are also explained in the post. reply ErikBjare 11 hours agorootparentWhere? I don't see any explanation of model optimizations in the linked post. reply LeicaLatte 15 hours agoprevFlux is awesome and improving all the time. reply swyx 15 hours agoprevthis comparison for the quantization effect is very nice https://flux-quality-comparison.vercel.app/ however i do have to ask.. ~2x faster for fp16->fp8 is expected right? its still not as good as the \"realtime\" or \"lightning\" options that basically have to be 5-10x faster. whats the ideal product usecase for just ~2x faster? reply sroussey 12 hours agoparentFunny, sometime I like the fast one better. reply dvrp 15 hours agoprevi think we (krea) are faster at the time of writing this comment (but i’ll have to double-check on our infra) reply lolinder 16 hours agoprevnext [15 more] I know naming things is hard, but... https://justgetflux.com/ https://flux11pro.com/ (Maybe the same thing? Unclear.) https://github.com/flux-framework/flux-core https://github.com/facebookarchive/flux/tree/main (apparently archived now, but this was the first thing I thought of) https://www.flux.ai/ https://fluxcd.io/ https://runonflux.io/ https://fluxml.ai/ reply dang 15 hours agoparent\"Please don't complain about tangential annoyances—e.g. article or website formats, name collisions, or back-button breakage. They're too common to be interesting.\" https://news.ycombinator.com/newsguidelines.html reply lolinder 15 hours agorootparentI just posted a reply to another person quoting this guideline: > In general I think that's true and agree that minor name collision commentary is uninteresting, but in this case we're talking about 11 collisions (and counting) in tech alone, 3 of those in AI/ML and 1 of those specifically in image generation. > When it's that bad I think that the frequency of collisions for this name is an interesting topic in its own right. I'll respect your judgement on this and not push it further, but this is my thought process here. reply CGamesPlay 16 hours agoparentprevWell, the word refers to \"continuous change\", so I guess it's pretty appropriate. reply achrono 16 hours agorootparentthis name flux reply dig1 15 hours agoparentprevAlso https://github.com/influxdata/flux - \"a lightweight scripting language for querying databases and working with data\" reply Conscat 16 hours agoparentprevThe first thing that comes to mind when I think \"flux\" is none of the above too . There's an extremely cool alternative iterator library for C++20 by Tristan Brindle named flux. reply bigiain 15 hours agorootparent/me glances across my desk to see my soldering station... reply roenxi 16 hours agoparentprevAnd then you can branch out of AI - https://en.wikipedia.org/wiki/The_Flux_Foundation works on public art. reply swyx 15 hours agoparentprevthere are just some names that technology brothers gravitate to like moths to a flame. Orion, Voltron, Galactus... reply worstspotgain 15 hours agoparentprevhttps://podcasts.apple.com/us/podcast/what-the-flux/id149389... reply artificialLimbs 16 hours agoparentprevDon't forget Caleb Porzio's new Laravel UI kit. https://fluxui.dev/ reply Vt71fcAqt7 15 hours agoparentprev>Please don't complain about tangential annoyances—e.g. article or website formats, name collisions, or back-button breakage. They're too common to be interesting. reply lolinder 15 hours agorootparentIn general I think that's true and agree that minor name collision commentary is uninteresting, but in this case we're talking about 11 collisions (and counting) in tech alone, 3 of those in AI/ML and 1 of those specifically in image generation. When it's that bad I think that the frequency of collisions for this name is an interesting topic in its own right. reply Scrapemist 12 minutes agorootparentNo reply pajeets 14 hours agoprevnext [2 more] [flagged] Fauntleroy 14 hours agoparentWhat prompt did you give it? Is it capable of animation? reply Palmik 4 hours agoprev [–] Every time there's a thread about models from Meta, there's a flood of comments clarifying that they aren't really open source. So let's also set the record straight for FLUX: only one of the models released is open source -- FLUX schnell -- it's a distillation from the proprietary model that's much harder to work with. Meta's Llama models have ironically much more permissive license for all practical intents and purposes and they are also incredibly easy to fine tune (using Meta's own open source framework, or several third party ones), while FLUX schnell isn't. I think the open source community should rally behind OpenFLUX or a similar project, which tries to fix the artificial limitations of Schnell: https://huggingface.co/ostris/OpenFLUX.1 reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "FLUX has been optimized for faster performance on Replicate, with significant speed improvements at various resolutions, such as 0.29 seconds for 512x512 and 0.72 seconds for 1024x1024.",
      "The enhancements, developed in collaboration with Black Forest Labs, include model optimization and a new synchronous HTTP API, and the code is open-source for community collaboration.",
      "FLUX aims to address the common issue of slow open-source models by sharing these optimizations, allowing users to fine-tune, edit, and deploy custom versions."
    ],
    "commentSummary": [
      "FLUX is an open-source text-to-image model available on replicate.com, sparking discussions on the efficiency of modular versus single neural network approaches.",
      "The debate highlights the complexity of neural networks and the potential benefits of modular methods for greater control and editability.",
      "The conversation also addresses the challenges of naming conventions in technology and the implications of AI on creative industries."
    ],
    "points": 221,
    "commentCount": 110,
    "retryCount": 0,
    "time": 1728782881
  },
  {
    "id": 41826449,
    "title": "Eating less can lead to a longer life: study in mice shows why",
    "originLink": "https://www.nature.com/articles/d41586-024-03277-6",
    "originBody": "NEWS 09 October 2024 Eating less can lead to a longer life: massive study in mice shows why Weight loss and metabolic improvements do not explain the longevity benefits of severe dietary restrictions. By Elie Dolgin Twitter Facebook Email Fat cells (artificially coloured). Restrictive diets cause fat loss and lengthen life, but the two effects are not necessarily linked.Credit: Steve Gschmeissner/SPL Cutting calorie intake can lead to a leaner body — and a longer life, an effect often chalked up to the weight loss and metabolic changes caused by consuming less food. Now, one of the biggest studies1 of dietary restrictions ever conducted in laboratory animals challenges the conventional wisdom about how dietary restriction boosts longevity. The study, involving nearly 1,000 mice fed low-calorie diets or subjected to regular bouts of fasting, found that such regimens do indeed cause weight loss and related metabolic changes. But other factors — including immune health, genetics and physiological indicators of resiliency — seem to better explain the link between cutting calories and increased lifespan. “The metabolic changes are important,” says Gary Churchill, a mouse geneticist at the Jackson Laboratory in Bar Harbor, Maine, who co-led the study. “But they don’t lead to lifespan extension.” To outside investigators, the results drive home the intricate and individualized nature of the body’s reaction to caloric restriction. “It’s revelatory about the complexity of this intervention,” says James Nelson, a biogerontologist at the University of Texas Health Science Center in San Antonio. The study was published today in Nature by Churchill and his co-authors, including scientists at Calico Life Sciences in South San Francisco, California, the anti-ageing focused biotech company that funded the study. Counting calories Scientists have long known that caloric restriction, a regimen of long-term limits on food intake, lengthens lifespan in laboratory animals2. Some studies3,4 have shown that intermittent fasting, which involves short bouts of food deprivation, can also increase longevity. To learn more about how such diets work, the researchers monitored the health and longevity of 960 mice, each a genetically distinct individual drawn from a diverse population that mirrors the genetic variability found in humans. Some mice were placed on calorie-limited diets, another group followed intermittent fasting regimens, and others were allowed to eat freely. Life expectancy rise in rich countries slows down: why discovery took 30 years to prove Cutting calories by 40% yielded the longest longevity bump, but intermittent fasting and less severe calorie restriction also increased average lifespan. The dieting mice also displayed favourable metabolic changes, such as reductions in body fat and blood sugar levels. However, the effects of dietary restriction on metabolism and lifespan didn’t always change in lockstep. To the authors’ surprise, the mice that lost the most weight on a calorie-limited diet tended to die younger than did animals that lost relatively modest amounts. This suggests that processes beyond simple metabolic regulation drive how the body responds to limited-calorie regimes. What mattered most for lengthening lifespan were traits related to immune health and red-blood-cell function. Also key was overall resilience, presumably encoded in the animals’ genes, to the stress of reduced food intake. “The intervention is a stressor,” Churchill explains. The most-resilient animals lost the least weight, maintained immune function and lived longer. Leanness for longevity The findings could reshape how scientists think about studies of dietary restriction in humans. In one of the most comprehensive clinical trials of a low-calorie diet in healthy, non-obese individuals, researchers found5 that the intervention helped to dial down metabolic rates — a short-term effect thought to signal longer-term benefits for lifespan. But the mouse data from Churchill’s team suggest that metabolic measurements might reflect ‘healthspan’ — the period of life spent free from chronic disease and disability — but that other metrics are needed to say whether such ‘anti-ageing’ strategies can truly extend life. Daniel Belsky, an epidemiologist who studies ageing at the Columbia University Mailman School of Public Health in New York City, cautions against over-extrapolating from mice to humans. But he also acknowledges that the study “adds to the growing understanding we have that healthspan and lifespan are not the same thing”. doi: https://doi.org/10.1038/d41586-024-03277-6 Read the related News & Views, ‘Genetics matters more than diet for lifespan’. References Di Francesco, A. et al. Nature https://doi.org/10.1038/s41586-024-08026-3 (2024). Article Google Scholar Fontana, L., Partridge, L. & Longo, V. D. Science 328, 321–326 (2010). Article PubMed Google Scholar Mitchell, S. et al. Cell Metab. 29, 221–228.e3 (2019). Article PubMed Google Scholar Duregon, E. et al. Cell Metab. 35, 1179–1194.e5 (2023). Article PubMed Google Scholar Ravussin, E. et al. J. Gerentol. Ser. A 70, 1097–1104 (2015). Article Google Scholar Download references Reprints and permissions Latest on: Ageing Metabolism Genetics Jobs The 5th Capital Medical University International Young Scholars Forum Announcement High-level talents Beijing (CN) Capital Medical University Professor/Associate Professor/Assistant Professor/Senior Lecturer/Lecturer The School of Science and Engineering (SSE) at The Chinese University of Hong Kong, Shenzhen (CUHK-Shenzhen) sincerely invites applications for mul... Shenzhen, China The Chinese University of Hong Kong, Shenzhen (CUHK Shenzhen) Professor/Associate Professor/Assistant Professor/Senior Lecturer/Lecturer The School of Science and Engineering (SSE) at The Chinese University of Hong Kong, Shenzhen (CUHK-Shenzhen) sincerely invites applications for mul... Shenzhen, China The Chinese University of Hong Kong, Shenzhen (CUHK Shenzhen) Postdoctoral Fellowships at West China Hospital/West China School of Medicine of Sichuan University Open to PhD students, PhD, Post-Doc and residents. Chengdu, Sichuan, China West China School of Medicine/West China Hospital Welcome Global Talents to West China Hospital/West China School of Medicine of Sichuan University Top Talents; Leading Talents; Excellent Overseas Young Talents on National level; Overseas Young Talents Chengdu, Sichuan, China West China School of Medicine/West China Hospital",
    "commentLink": "https://news.ycombinator.com/item?id=41826449",
    "commentBody": "Eating less can lead to a longer life: study in mice shows why (nature.com)186 points by XzetaU8 9 hours agohidepastfavorite120 comments tomp 7 hours ago“Intermittent fasting and/or caloric restriction extends mice lifespan.” is quite old news. The reason mice are used in labs isn’t because they’re a good model of humans, but because they’re easy to bread and have short lifespans. Studies of caloric restriction in monkeys didn’t cause lifespan extension, casting doubt on its effect in humans. Peter Attia (best source of longevity science) wrote about this 5 years ago. https://peterattiamd.com/calorie-restriction-part-iia-monkey... reply throw8932899 2 hours agoparentMonkeys (more precisely chimpanzees) do not have fasting mechanisms. They never start fasting and go into ketosis, they just die from starvation after week or two. In tropics they have food available all year. They never venture far from food sources. Humans (and other non-tropic animals like mice) have adaptation for seasonal food availability, and can go through extended fasting. reply sigmoid10 1 hour agorootparentThis should be higher up. Humans split from primates (including chimpanzees) roughly 5 million years ago. Early hominids only evolved the ability to store large amounts of fat and fast for extended periods of time in the Pleistocene, less than 2 million years ago. Any study done on calorie restriction using monkeys has a serious caveat. Mice offer a much better starting point for comparing metabolic activity. reply gweinberg 1 hour agorootparentYou're kidding, right? I can see saying neither one tells you much about human beings, but saying monkey studies aren't relevant to human but mouse studies are just sounds bizarre. reply meowkit 53 minutes agorootparentWhen you frame it like that of course it sounds bizarre. You just ignored the context of ketogenic metabolism being more similar in humans and mice than humans and chimpanzees. reply onlyrealcuzzo 1 hour agoparentprevHaven't almost 1Bn people intermittently fasted during Ramadan for centuries? Shouldn't there be enough evidence to get an idea if there's possibly a correlation? reply sieste 34 minutes agorootparentThe benefits can be negated by what you eat outside the fasting window. Anecdotally, a few guys I knew feasted on baclava and super sweet tea first chance they got. also not drinking any water, especially in hot climates might be net unhealthy, independent of any fasting benefits. I don't know any studies in that though, happy to stand corrected... reply himinlomax 24 minutes agorootparentprevThey also abstain from drinking water. Medical science has clearly shown that this is bad for your kidneys. Possibly catastrophically bad. Who should I believe, science or medieval superstitions? reply vlovich123 6 hours agoparentprevI thought so too but this is newer research on monkeys based on a long running study since 1989 linked directly in the first sentence of the article: https://www.nature.com/articles/nature.2014.14963. That being said the Wisconsin study does have a fair amount of criticism in that they cut back on a diet composed 30% sugar from an all you can eat buffet so it’s unclear if it’s purely just stopping being overweight / reducing risk of diabetes or if there’s any other benefits. reply appplication 6 hours agorootparentFWIW as much as it’s fair to criticize the study design, an all you can eat buffet composed of 30% sugar is more or less par for the average American. It’s not like we’re out here in the real world with a healthy and nutritionally balanced baseline. Most of us eat quite a bit of trash, so in the context of the real world the findings seem valid enough. reply vlovich123 5 hours agorootparentThere’s a difference between “caloric intake reduces obesity related mortality causes like diabetes” and “caloric intake extends lifespan”. Even though they’re related, the latter is claiming everyone would benefit from it including those that aren’t obese and is fighting some kind of fundamental aging mechanism caused by eating while the former is just saying how to fix the American diet to fix obesity and diabetes. reply dimal 4 hours agoparentprevAre you reading that right? > The investigators were comparing median survival between the CR and control (CON) groups, not longevity or lifespan per se. As I read that, it wasn’t that the study showed that caloric restriction didn’t extent lifespan. It’s that they weren’t investigating the effect of caloric restriction on lifespan, so no conclusions can be made from the study. reply tomp 4 hours agorootparentNo, read the article I posted. TL;DR: there are 2 monkey studies, with unclear/conflicting results, both had pretty shitty diets, the calorie restrictions were severe, the benefits minimal at best. reply dimal 2 hours agorootparentThe quote I provided was from the article. I’ve read the whole thing and I’m still parsing it (and your statement about both being fed shitty diets) as going against your original assertion: “Studies of caloric restriction in monkeys didn’t cause lifespan extension, casting doubt on its effect in humans.” According to Attia, the studies weren’t specifically studying life extension, and all the diets were non-typical for the monkeys being studied. So, to me, this argues not that caloric restriction doesn’t cause lifespan extension in monkeys, but that we cannot know from these studies whether caloric restriction with a healthy diet causes lifespan extension in monkeys, which is what we really want to know. I wouldn’t expect to extend lifespan by eating hamburgers and fries but less of them, and that seems analogous to what the studies did. To me, this simply leaves the question of caloric restriction in humans unanswered instead of casting doubt on it. Big difference. reply nradov 10 minutes agorootparentHamburgers aren't necessarily unhealthy. Plain ground beef is an excellent nutrient source. It depends what else you put on them. reply ineedaj0b 34 minutes agoparentprevand thrown out with the bathwater is we've bred these mice to be very similar to us in some cases. if you wanted these studies done on humans you'd be dead before the results came back positive or negative. reply 8b16380d 6 hours agoparentprevMmmmmmm mouse bread reply doubled112 4 hours agorootparentI read it more like a mouse sandwich. I don’t think there’s much meat on a mouse though, especially when the experiment is to have them eat less. reply aulin 3 hours agoparentprevWait, isn't Attia a big advocate of periodic fasting? reply nradov 2 hours agorootparentNot anymore. More recent research has failed to find any real benefits to intermittent fasting, at least for people who are already at a healthy weight. And it seems to have some negative effects for body composition. https://peterattiamd.com/ep-300-exercise-nutrition-fasting-m... reply aulin 2 hours agorootparentNote I used periodic instead of intermittent for a reason. I seem to recall he was recently for something like a day a day and a half fast per month. Don't follow him that closely, did he move away from that too? reply scotty79 6 hours agoparentprevInteresting question is why it didn't work for monkeys. It might have been a difference in protocol. Studies on monkey's are not easy to repeat. Somebody should test caloric restrictions on mice with explicit goal of making it not work. reply pc86 4 hours agorootparentWhy? It's much more likely caloric restriction extends the lifespan of mice and doesn't similarly extend the lifespan of primates. What does making some bad scientific study that tells you something incorrect about mice help? reply lcnPylGDnU4H9OF 3 hours agorootparent> that tells you something incorrect I know I’m just nitpicking here but I believe you mean “inconclusive”. If we knew the findings specifically to be incorrect then we’d be learning something. reply scotty79 3 hours agorootparentprevI'm not advocating a bad scientific study. I'm advocating a good one for detecting what might be another cause of it not working for monkeys other than 'duh! monkeys aren't mice!\" It's not about finding something inconclusive or incorrect. It's about finding all the ways you can make caloric restriction not cause extended lifespan in mice without harming them in some obvious ways that are hard to apply accidentally in research setting. For example, maybe caloric restriction doesn't work if you feed mice badly inappropriate diet. reply austinjp 8 hours agoprevI can't see a clear conclusion, unfortunately. 'Further study required' as ever. It also seems that 'resilience' to the stress of caloric restriction is key to gaining most benefit but it's not clear exactly what resilience is, and whether it's manipulable. From the article: Weight loss and metabolic improvements do not explain the longevity benefits of severe dietary restrictions. Cutting calories by 40% yielded the longest longevity bump, but intermittent fasting and less severe calorie restriction also increased average lifespan. The dieting mice also displayed favourable metabolic changes, such as reductions in body fat and blood sugar levels. However, the effects of dietary restriction on metabolism and lifespan didn’t always change in lockstep. To the authors’ surprise, the mice that lost the most weight on a calorie-limited diet tended to die younger than did animals that lost relatively modest amounts. This suggests that processes beyond simple metabolic regulation drive how the body responds to limited-calorie regimes. What mattered most for lengthening lifespan were traits related to immune health and red-blood-cell function. Also key was overall resilience [...] The most-resilient animals lost the least weight, maintained immune function and lived longer. In one of the most comprehensive clinical trials of a low-calorie diet in healthy, non-obese individuals, researchers found that the intervention helped to dial down metabolic rates — a short-term effect thought to signal longer-term benefits for lifespan. But the mouse data from Churchill’s team suggest that metabolic measurements might reflect ‘healthspan’ — the period of life spent free from chronic disease and disability — but that other metrics are needed to say whether such ‘anti-ageing’ strategies can truly extend life. reply CraftingLinks 8 hours agoparentMice are very different from humans in that they are prey and we are predator. The reason that this is significant is that evolutionary, mice life history is optimalized for more offspring and shorter life, whereas predators are optimized for long life and few offspring. We know since 1993 with the discovery of the daf-2 gene (a nematode insulin receptor) that genetics can drastically affect longevity. However, and this is my own professional hypothesis, in humans we have tuned the genetic dial to max lifespan, whereas in mice and worms it is not. In these animals there is plenty of flexibility left to tune up lifespan, not so in humans. We may see moderate effects due to increased health though. reply james-revisoai 5 hours agorootparentPlease can you tell me more about the daf-2 gene and it's role in lifespan for nematodes? Any explainers on this? How does insulin relate to it - the metabolic aspect? reply CraftingLinks 2 hours agorootparentI can point you to the abstract of my PhD: https://biblio.ugent.be/publication/3075889. Mutations in the daf-2 gene tend to result in huge shifts in physiological and metabolic makeup. The challenge is discriminating those changes that are relevant for longevity from those that are not. reply thaumasiotes 8 hours agorootparentprev> We know since 1993 with the discovery of the daf-2 gene (a nematode insulin receptor) that genetics can drastically affect longevity. We've known that since we observed that humans can easily live more than 70 years while cats tend not to go past 20. And presumably well before then. reply FollowingTheDao 7 hours agoparentprevIMHO, it is all about lowering oxidative stress. Eating creates oxidative stress in the body because to eat means to create ATP and that means electrons are increased in the mitochondria. More electrons = more oxidative stress in the form of superoxides. I will say that Calorie Restriction with Optimal Nutrition (or CRON) is the healthiest diet but should be geared towards ones heritage or genetics (which is why I think all of the studies on humans have such variable and confounding outcomes.). http://optimal.org/voss/cron_overview.html reply nradov 5 hours agorootparentThat doesn't seem plausible. Hard exercise increases oxidative stress and yet we have pretty good evidence that frequent hard exercise is correlated with longer lifespan. Calorie restriction like the nonsense you're promoting would leave me too weak to do my favorite activities. reply HKH2 3 hours agorootparent> we have pretty good evidence that frequent hard exercise is correlated with longer lifespan. Like bodybuilding? reply nradov 3 hours agorootparentMore like resistance training than bodybuilding (bodybuilding is more about aesthetics than strength, although there is a significant linkage). Falls are a leading cause of death and disability in the elderly. Old people who fall and break a hip seldom fully recover, lose mobility, and are often dead within a couple years. Being strong enough to stabilize your joints and catch yourself before hitting the ground is extremely important (although this is difficult to study through randomized controlled trials). https://peterattiamd.com/avoiding-injury-part-i-eccentric-st... By the way, this is one reason why mouse studies on longevity don't translate well to humans. Lab animals live in safe, flat cages where there's little risk of musculoskeletal injuries. The real world where humans live is far more hazardous. reply cmclaughlin 4 hours agorootparentprevYou’d be surprised… weight lifting while intermittent fasting is quite effective reply nradov 3 hours agorootparentEffective for what? reply FollowingTheDao 3 hours agorootparentprevExercise reduces weight. Extra weight is just past eating we carry on our body to eat later. reply jebarker 3 hours agoprevIf you do intermittent fasting you won't necessarily live longer, but it will feel much longer. reply KolmogorovComp 13 minutes agoparentThank you for this, the only result that does not need more studies to be proven. reply aherforth 3 hours agoparentprevI laughed at this reply penguin_booze 1 hour agoparentprevOnly initially. After a while, hunger and craving feel less of an emergency, and blends into the background, like breathing is--you tend to notice it's happening occasionally, but then go meh. reply mmooss 48 minutes agoprevThe study itself is here: https://www.nature.com/articles/s41586-024-08026-3 It adds this interesting point: Our findings indicate that improving health and extending lifespan are not synonymous and raise questions about which end points are the most relevant for evaluating aging interventions in preclinical models and clinical trials. reply tlogan 3 hours agoprevSome studies on humans suggest that ‘moderate obesity’ can reduce lifespan by about 3 years [1] I’m curious if there are any specific studies comparing the longevity of individuals with a BMI lower than 20 versus those with a BMI closer to 30. [1] https://www.ox.ac.uk/news/2009-03-18-moderate-obesity-takes-... reply sukhavati 7 hours agoprevCaloric restriction (CR) leading to longer lifespan doesn’t generalize, including rhesus monkeys [1]. Moreover, across various species there seems to be negative correlation between average lifespan and effectiveness of CR and lifespan extension [2]. [1] https://x.com/cremieuxrecueil/status/1844282914521096501 [2] https://x.com/powerfultakes/status/1844310815819931661 reply MrBuddyCasino 6 hours agoparentCrémieux is fantastic, highly recommended. reply thenerdhead 7 hours agoprevWe are slowly learning that our immune system is the key to longevity. Many articles these last few years on ways to renew it, ways to treat dysfunction, and the common bad guys we really need to remove from our bodies as they are directly linked to currently incurable diseases like T1D, MS, Alzheimer’s, Parkinson’s, ME/CFS, Long COVID, and more. It seems very realistic we may cure/reverse disease course for a handful of these within the next decade at this rate. Many novel clinical trials are happening to probe at the causes and if proven, we could have new drugs quickly due to the pandemic forcing supply chains to be able to meet antigen targets. reply voisin 6 hours agoparent> the common bad guys we really need to remove from our bodies Can you elaborate? reply thenerdhead 6 hours agorootparentOver the last four years we have made tremendous progress in identifying pathogenic antigens as the main culprit in these diseases. Some may be caused by undetected low grade chronic infections and some may be lingering antigens causing constant immune system activation / dysfunction. reply aashu_dwivedi 6 hours agoparentprevcan you elaborate what are the medical trials which are going on for this? reply thenerdhead 4 hours agorootparenthttps://www.nature.com/articles/s41392-024-01911-3 https://www.nature.com/articles/s41531-024-00716-z https://www.nature.com/articles/d41586-024-00470-5 https://www.science.org/content/article/long-covid-trials-ai... https://videocast.nih.gov/watch=54970 https://www.nature.com/articles/d41586-024-00871-6 https://www.nature.com/articles/d41586-024-01274-3 https://www.fda.gov/drugs/news-events-human-drugs/fda-approv... Immunotherapy is entering the golden age and we're about to find out what and what doesn't work. reply DJBunnies 6 hours agorootparentprevI’m skeptical it has basis in medicine, but I’m here to see too. reply FollowingTheDao 5 hours agoparentprevBut what controls the immune system? IMO it is oxidative imbalance that dis-regulates the immune system, leading to a hyper or hypo inflammatory state. That is, autoimmunity or immunodeficiency. reply thenerdhead 4 hours agorootparentIt's more nuanced than that. genetic predisposition (immune/HLA genes working or not), infections (potential for antigen persistence), diet, environmental factors, stress, etc all also contribute heavily. reply rKarpinski 1 hour agoprev> Weight loss and metabolic improvements do not explain the longevity benefits of severe dietary restrictions. The overlooked word is \"Severe\". These studies either restrict a lot of calories (40% for one study in the article), or are fasting for long periods of time for the animal; Mice start dying without food after 2 days - you aren't going to see the same effect in humans by skipping breakfast. reply belter 7 hours agoprevThe money shot: \"...But other factors — including immune health, genetics and physiological indicators of resiliency — seem to better explain the link between cutting calories and increased lifespan...\" reply SoftTalker 4 hours agoparentAnecdotally, every chronically sick person (always getting colds, coughs, sinus infections, headaches, UTIs, other symptoms) I know is overweight. This doesn't show causation, maybe having a weak immune system contributes to being hungry and overeating but all the normal weight or lean people I know seem to rarely be sick. reply makeitdouble 3 hours agorootparentKnowing nothing about your surroundings, overweight people tend to avoid medical care if they can bare with the symptoms, which make them more subject to longer colds, infections, headaches etc. It's a well known effect: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4381543/ reply SoftTalker 3 hours agorootparentI wonder if the same is/was true for smokers? I can imagine people not wanting to hear \"you need to lose weight\" every time they visit the doctor. What about \"you need to stop smoking\" or \"you need to stop drinking\" or admonishments to stop any other self-destructive behaviors? reply makeitdouble 3 hours agorootparentYes, there's probably a similar effect, with a key difference being that smoking and drinking can be masked a bit easier if it's not an emergency (e.g. sobering for a day before going in) where there's no lying about one's weight. reply nradov 2 hours agorootparentprevThat doesn't make any sense. Medical care can't really reduce the length of common cold symptoms. It mostly comes down to the effectiveness of your own immune system. Obesity has a number of negative effects on the immune system. reply mikesabbagh 3 hours agoprevthere are new reportsin humans that intermittent fasting doubles your rate of heart disease. I am curious to know if it decreases mortality at same time. https://newsroom.heart.org/news/8-hour-time-restricted-eatin... I think that what is important is to give time for your body everyday to recover from the food intake (which can be an inflammatory insult) and give it time to repair. Maybe some people may reach ketosis in 8h, which is an injury itself. It is like the need to sleep and the need to rest after exercise or the need for vacation after long work days. Every insult needs to be compensated for and repaired. Also the idea of having to eat 3 meals a day, does not exist in nature or in our past history. This developed with industrilization when hard working men needed energy at work. But it is clear that evolution never needed to limit excessive food intake. It mainly dealt with famines and low food intake. We never aquired the necessary breaks to stop eating. we need to keep this in our thoughts daily to actively stop ourselves from eating till our death reply elwebmaster 5 hours agoprevThere is just one “little” problem which remains unsolved: how to make money while “not selling” calories? It’s easy to make money by selling more calories: you add sugars, artificial compounds leading to mental addiction, you invest in heavy marketing and expansive distribution channels. All of this makes your product available and desirable, a money printing machine, at the cost of society’s health and wellbeing. So until someone can answer the question of how to do the same without the side effect to society, unfortunately the majority will continue to suffer for the benefit of the very few. reply kelipso 4 hours agoparentThere is no particular need for society to have an industry that sells unhealthy food. It can go the way of horses, would only be a net positive for society. reply Balgair 4 hours agoparentprevThe old adage here applies: \"How do you sell ice to Eskimos? You call it French ice.\" Same kinda thing. You call the net-negative calories: French calories or some other crap like that. reply wildrhythms 5 hours agoparentprevWho is 'you'? reply elwebmaster 5 hours agorootparentYou is the processed food industry. reply apwell23 4 hours agoparentprevIts a way for us to winnow out the weak willed reply amanaplanacanal 4 hours agorootparentIt would be just as true but much funnier if you said \"sinners\". reply simion314 2 hours agoparentprevSorry, but this is such a bad conspiracy, like flat earth level. You could look at history, at communist countries where there were not evil capitalist with evil ads and people still were eating 3 or more times a day and eating meat, eggs and other calories. Or you are aware of some tribe that eats one every week and lives 150+ years or something like that ? reply begueradj 1 hour agoprevThe human body knows how to cope with food scarcity: that's what it's used to since we appeared as a species on Earth. But it does not know how to cope with the abundance of food and energy. reply bjornsing 8 hours agoprevDoes the study show why (as the title says)? Can’t say I got it… reply austinjp 8 hours agoparentNope. The 'why' in the title is misleading. They suspect some unknown factors but aren't sure what and whether they can be targeted for anti-ageing interventions. reply mrangle 3 hours agoprevI've been following this research for two decades. In my opinion, the most plausible explanation for the genesis of these life-extending adaptations is the methionine and branch chain amino acid restriction that is incidental to calorie restriction (that would be only cyclical in humans). reply utopicwork 1 hour agoprevI'm someone who is naturally fat. I have spent my whole life trying to lose weight. Many of you here have excluded fat people from your lives so you won't hear this from someone else: you are the weight you are due to factors outside of your control. You can slightly change your weight temporarily but your body will always reset or give you stress signals to reset it yourself. I know, I have starved myself, done incremental weight loss, worked out almost year round as a football player, it doesn't stay off and it barely comes off (10-20 pounds total is insignificant). Fat people shouldn't have to starve themselves to meet societal standards and personally I won't. PS. The reason fat people don't like going to the doctor is because they quite literally will not treat us for our issues in most cases. I've been trying to get treatment for my crippling back pain for years and have been told by physical therapists not to do physical therapy until I'm on pain medication but even with this info doctors just tell me to exercise. I'd love to, in fact I'd love to take walks often but I physically cant. reply macrolime 7 hours agoprevCould be the reason is aging is programmed and when food is scarce the program changes to delay aging, such that the organism has a better chance to survive longer and reproduce once food is less scarce and at that point turn up the rate of aging again. reply exceptione 4 hours agoparentThat is almost how Hoeijmakers explains it, but not quite, see the link in my other comment for pointers. He thinks aging is not programmed per se, but what is programmed is the switch of modes. Aging happens automatically as faults built up in dna over time. If an organism switches from \"organism growth mode\" to \"repair dna mode\", the chance of survival increases. Now, how to conveniently trick the body into the latter mode is the million dollar question. reply 0x1ceb00da 4 hours agoparentprevOr that extra foods puts more stress on your organs. Liver, kidneys, pancrease, etc have to do more work and for some reason the body isn't able to repair that and they wear out quicker. reply exceptione 4 hours agorootparentAll tissue in organs are under stress and cells die of. That is normal, also for young people. The body produces new cell by making copies of dna. What is problematic however is that after some time, there is a built up of errors in the dna. If those errors do not get corrected, that is when you see a decline in organ quality, what we call aging. If interested, see my other posts for more pointers. reply 0x1ceb00da 4 hours agorootparentprevSome googling suggests this is what causes type 2 diabetes. Pancreas wears out and stops producing insulin. reply Zamicol 3 hours agoprevThe title is misleading. This is not true in more complex animals like humans. https://x.com/cremieuxrecueil/status/1844282914521096501 reply avazhi 3 hours agoprevI love how neither this Nature article nor the actual study shed any new light on this area. We've known about this for decades; we've also known that it isn't just down to metabolic rate changes for... decades. reply himinlomax 26 minutes agoprevGnawing on wood is good for your teeth, study in mice shows why. reply exceptione 7 hours agoprevThere is an interview [0] with Hoeijmakers, who has championed the idea that the single root cause for both aging and cancer are faults in dna. Faults in dna prevent rna copies, aka _transcription stress_. One of his theses was that if faults in dna happen randomly, we would see aging the most in the largest genes. [1] In his experiments [2] he was able to turn the knob of dna repair, also in humans, from \"organism growth\" to \"organism survival\". Survival mode can be induced when confronted with food scarcity. If we can unravel the underlying mechanism, we should be able to enlarge our lifespan. ___ 0. https://www.nrc.nl/nieuws/2024/10/11/de-geneticus-die-gezond... 1. https://www.cell.com/action/showPdf?pii=S0168-9525%2824%2900... 2. https://www.nature.com/articles/nature19329 reply zagrebian 5 hours agoparentThere was a Veritasium video about these two modes. Survival mode is triggered not just by food scarcity, but also by extreme heat, extreme cold, and one other thing (I forgot). reply FollowingTheDao 5 hours agoparentprevThe single greatest contributor to DNA damage is oxidative stress. For example: https://www.mdpi.com/1422-0067/23/19/11664 reply exceptione 4 hours agorootparentHoeijmakers mentions that. Fasting activates anti-oxidants. For example, in the Netherlands hospitals are starting to experiment with fasting before a surgery. During surgery arteries arteries are opened, bringing blood in contact with oxygen. With pre activated anti-oxidants damage will be minimized, helping with recovery. reply bubblesnort 8 hours agoprevA TV documentary in 2002 said they studied 2 groups of rats. The control group had a steady supply of food and could eat whenever they want. The other group was on a diet. After 2 years, the control group all sat in a corner of their cage being fat and lazy. The other group continued powering their ferris wheel generators and were not being lazy at all. They hypothesized that too much food made the body more prone to free radicals and the damage was more difficult to repair in the control group. Effectively making them age faster than the diet group. reply amanaplanacanal 4 hours agoparentI'm pretty sure the free radical theory of aging is not mainstream any more. Too many studies of antioxidants have shown that they don't help, out even make things worse reply Plankaluel 8 hours agoparentprevNot sure a TV documentary from 22 years ago is the source to get scientific explanations from ... reply bubblesnort 7 hours agorootparentTV documentaries today are arguably worse. Dumbed down to \"here's someone who knows science stuff\" and \"here's a startup that does science stuff\" over and over again. The point I was trying to make is that many \"news\" articles aren't really new. It's not unheard of that the publisher just wants your attention so they'll keep rehasing old stuff. reply rafaelmn 3 hours agoprevI thought this research didn't reproduce with primates ? reply HumblyTossed 2 hours agoprevI'm not a mouse. reply okr 7 hours agoprevBut do not stop eating. My advice. reply ck2 6 hours agoprevEvery single longevity (life extension) supplement or drug or technique that I've ever looked at involves pushing the person DOWN to a baseline Which means even if they work to give you more years, you will lose fitness. It is physically impossible to be held down to baseline and yet achieve stress-adaptation from exercise. Everything from NAD+ supplementation to metformin to fasting, they all push your metabolism and other bodily function to an idle state. It is useless, you'll live longer but you will be feeble. reply FollowingTheDao 3 hours agoparentYes, there is a balance. IF you think of your body like a car it can make sense. The more you drive your car the faster it will wear down. Not driving your car can help it last a long time but it is useless. But driving it to much and too fast will wear it down before its' time. Drive your car when you need to, but not too fast. And make sure you give it good gas and take care of the fluids and maintenance. And live knowing that everything is impermanent and one day your car will be gone. reply smolder 1 hour agorootparentThank you for continuing the long tradition of using car analogies to explain things on message boards. This is a pretty good one. reply bratwurst3000 5 hours agoparentprevi thought about this also but then there is this recent science that the body of a office worker uses the same calories as someone doing body work. saw it on kurzgesagt. so by this theory humans should be very energetic even with only the baseline reply ck2 5 hours agorootparentI only burn 80 calories when running a 7 minute mile but the average person uses a lot more if they are untrained. The office-worker uses the same calories as a workout because they have no fitness and their heartrate is sky high even at idle. reply squidgedcricket 3 hours agorootparentIs heart rate directly proportional to calories burned? reply scotty79 6 hours agoprevI'm doing 24h breaks between 24h eating periods because I was slightly overweight. I lost 10lbs in less than 2 months without any additional restrictions or exercise. I noticed no side effects. It's also super easy to keep up esp. when compared with my previous attempt at restricting sugary snacks. 'When' is so much easier to manage than 'what'. If I feel hungry I just need to distract myself for few hours and it's fine. reply kadomony 4 hours agoparentWait, so you're just not eating every other day? reply pjs_ 2 hours agorootparentAbsolutely fine thing to do IMO. Takes a tiny amount of resilience but works great. reply scotty79 2 hours agorootparentprevOne day I'm eating only till 4PM and the other only after 4PM. And so on ... So the breaks are often a bit longer than 24h because I rarely have my last meal right before 4PM. But not as long as if I was eating every other day. Because then it would be two nights and one day between the periods of eating and I think that could be a bit much for me. reply wileydragonfly 3 hours agorootparentprevAnorexia works! reply smolder 1 hour agorootparentAnorexia is when people starve themselves to an unhealthy degree based on mental issues, body dysmorphia, something like that. Someone intermittent fasting to lose excess weight and having success at it isn't the same. reply scotty79 2 hours agorootparentprevWow. Losing weight by not eating apparently is not the correct, moral way to do it either. That's a kind of shaming that even Ozempic patients don't get. Apparently the only moral way is to struggle with terrible exertion and discomfort and mostly fail but sometimes succeed. Only then you earned joining the club of people with healthy, normal weight. reply formerly_proven 1 hour agorootparentprevThis is probably the dumbest comment I've seen on HN regarding weight loss so far, and that's really saying something considering the recent Ozempic thread. reply komali2 8 hours agoprev> \"The intervention is a stressor,” Churchill explains. The most-resilient animals lost the least weight, maintained immune function and lived longer. Very happy to hear that my struggles with weight loss are because of my resilience and a strong indicator that I will live a long life ;) reply bamboozled 8 hours agoprevDidn't we start living longer as we had access to more and better food...and ate it? reply meindnoch 7 hours agoparentWe did. But we have overshot a bit. reply FollowingTheDao 7 hours agoparentprevYou are thinking of Antibiotics. reply bamboozled 5 hours agorootparentNo I'm not... reply FollowingTheDao 5 hours agorootparentLet me rephrase that less sarcastically. More food did not lead to longer lifespan in modern humans. Antibiotics are responsible for increasing human lifespan and estimated 20 years. https://www.antibioticresearch.org.uk/happen-antibiotics-sto... reply amanaplanacanal 4 hours agorootparentI suspect they are taking about the time during which our ancestors evolved into modern humans. reply Horffupolde 6 hours agoprevFasting is widespread in religions. Can’t be a coïncidence. reply spcebar 5 hours agoparentI think religious fasting is more about the symbolic significance of fasting as an act of self denial and self cleansing, but there are plenty of religious dietary restrictions in religions that probably stem from ideas about health and food safety. reply scotty79 6 hours agoparentprevIt's more likely because hunger was a common human experience for entire history so doing it on purpose seems like a quirky, hip thing to do and that's what religions flock to for the purposes of attracting followers. reply swayvil 5 hours agoparentprevFasting gets you high. Bundles well with meditation, sleep-deprivation, etc. (which leads to \"religious visions\" etc) reply senectus1 5 hours agoparentprevSo is wearing funny hats, and performing acts of violence on opposing believers and unbelievers (not to mention their own believers). I wouldn't put too much \"faith\" in religions authority on whats correct or not. Also, absolutely it can be a coincidance. Your standards for truth are very poor. reply xenospn 8 hours agoprev [–] Refuted by every Jewish grandmother ever. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A comprehensive study on nearly 1,000 mice indicates that reduced calorie intake can extend lifespan, but not primarily due to weight loss or metabolic changes.- The research found that factors such as immune health, genetics, and stress resilience are more crucial in determining lifespan extension than metabolic improvements.- This study challenges the traditional belief that metabolic changes directly correlate with longer life, emphasizing the complexity of dietary restriction's impact on longevity."
    ],
    "commentSummary": [
      "A study indicates that caloric restriction can extend lifespan, but this conclusion is mainly derived from research conducted on mice.",
      "The effects of caloric restriction on lifespan have not been consistently replicated in primates, raising questions about its relevance to humans.",
      "The study suggests that factors such as immune health and resilience might contribute to the benefits of caloric restriction, but further research is necessary to determine its impact on human longevity."
    ],
    "points": 186,
    "commentCount": 120,
    "retryCount": 0,
    "time": 1728811505
  },
  {
    "id": 41824138,
    "title": "The 1/8th Sleep",
    "originLink": "https://near.blog/the-1-8th-sleep/",
    "originBody": "The 1/8th Sleep Posted on October 12, 2024 by near The eight sleep bed is a popular self-cooling bed. Sleeping on a colder surface not only feels great but often improves sleep as well. This post discusses a cheaper homemade option that I use instead. A 12℉ difference between the bed surface and the surrounding air. Some ice was used in the cooler prior to the image as measuring surface temperature is challenging with the pictured thermometer. Why I didn’t Buy an Eight Sleep Although many of my friends are happy eight sleep customers, I was put off by the product due to the now-mandatory paired subscription ($200/yr with purchase, annually paired only, can cancel after), the excessive marketing featuring futuristic animations with terms like ‘intelligence’ and ‘autopilot’, constant paid referral links and name dropping of Huberman et al, and the high price points of $3,000 total and $366 for the addition of two pillows and a bed sheet. I already have a mattress that I like, so I don’t feel the need to replace it. I also inherently dislike the thought of paying a subscription to sleep, and the proprietary nature of the data and cloud app certainly don’t help the case (although I found one user who reversed it). Popular Alternatives Here are a few categories of products in the space: Eight Sleep: $2400 to $7600 depending on options (full mattress) Chilipad: $574 to $1300 depending on options (mattress topper only) BedJet 3: $429 to $949 (mattress topper only, pretty loud) Random Chinese Amazon items: $140-$300 (topper + evaporative cooler, generally not good products) AquaPad: I haven’t tried this and I wonder how good it is. If you try it and have feedback and data on temperature cooling, please share! Cooling gel mattress pads: $90 (does not actually cool – just a high-ventilation material) Current Solution Here is my current setup, inspired fully by this tweet: Low-quality cooling system with a topper: Adamson B10 Gray Bed Cooling System ($140) Replace the above low-quality evaporative cooler with a Poafamx Fish Tank Cooler ($180) A stronger pump ($15) and small extension tubes ($9) A small cooler which allows me to get the temperature extra low before bed ($18) A Smart plug in order to turn the system on and off from my phone ($20) The total came out to $382 (without counting 5% off via Amazon) although you could reduce this by at least 15% by purchasing from AliExpress (or much more if you properly source from Alibaba). It’s also possible to save on step one by finding only the mattress topper, but I didn’t want to look through Alibaba for this. This setup is 1/8th the cost of an eight sleep so I named it the 1/8th sleep. How It Works At first glance it may seem complicated to buy six items, but it’s actually a simple setup. The first item comes with a mattress topper which you can run water through (the low-quality cooling unit it came with was not used). It was placed under the first layer of sheets, making it difficult to notice and aesthetically nonmodifying. Next you need a way to continuously pump water through the mattress topper, which the pump and cooler are used for (you could just have a pump but the cooler gives you a water reservoir which simplifies the process and allows for more control). Lastly you need to cool the water, which the fish tank cooler is for (water flows into it and comes out cooler). An image of the cooler used – I keep it set to as low as it will go which will usually be around 12℉ lower than the air temperature. The extension tubes were purchased so that I could move the setup out of my bedroom entirely both to reduce noise (which was already low – the equivalent of a desktop PC fan) and to ensure any heat output went elsewhere. I placed the tubes near a baseboard and put them under the corner of a door, making it hard to notice I’ve modified my bed at all. The smart plug allows me to perform a single tap on my phone to turn the system on or off. As an added bonus you can put ice (or something even colder like dry ice) in the cooler if you’d like to sleep on an extra-cold surface. The eight sleep bed comes with a sleep tracking app, although I have no reason to believe it is better than a whoop, oura, or apple watch. I use an apple watch to track my sleep which has the benefits of requiring no subscription and of allowing me to export and control my data (Claude wrote me a full script to parse my apple health data with just minutes of work Tradeoffs Made All systems come with trade-offs. Here are some for the 1/8th sleep: Pros: Significantly cheaper Ability to excessively lower temperature via ice/dry ice Ability to further modify the system, e.g. move the cooler out of the room, upgrade only the cooler, cover an arbitrary part of the bed Can be paired with any mattress – users may keep their existing beds You don’t have to pay a subscription to sleep Cons: Less aesthetic (This can be improved with a bit of effort – I may 3d-print an optimally-sized encasement) No ability to heat the mattress No fine-grained temperature controls via a phone app (I always set it as low as it will cool regardless) No built-in sleep tracking (I use an Apple watch) Easier to incorrectly set up: if you don’t tighten tube connectors you could cause a water leak. If you attempt a setup like this it must be thoroughly tested before applying it to your bed! The mattress topper linked for this setup is for a twin bed, although you could purchase two or find a larger one Eight sleep loses thousands of dollars in potential revenue (contested – many argue this is a pro) Conclusion This setup is experimental and has only been used for one week. While I’m happy with the results thus far, I wouldn’t suggest it to anyone without an experimental/DIY mindset. This post was made not because the setup is optimal but simply because it seems better to post this than to post nothing at all. This post contains zero referral links. Special thanks to everyone who responded to the initial tweet on the topic, especially this response which inspired this setup. Read another random post / Back to my homepage This entry was posted in Uncategorized and tagged biology by near. Bookmark the permalink.",
    "commentLink": "https://news.ycombinator.com/item?id=41824138",
    "commentBody": "The 1/8th Sleep (near.blog)169 points by asats 18 hours agohidepastfavorite44 comments ninjin 13 hours agoInteresting. Living in Japan there is still a strong culture to try to avoid using air conditioning at night (in particular among the elderly), so there is a whole market for both staying warm in bed during winter and cool in bed during the hot and humid summers. My own family has primarily been using mattress protectors that conduct heat more efficiently [1], towel blankets (タオルケット) [2], and gel pads than are stored in the fridge and then put under the pillow cover just before you go to sleep. There are plenty more on the market, but I have solely by using the conducting mattress protectors been able to sleep with the air conditioning several degrees higher than I would have been able to in the past. [1]: https://www.nitori-net.jp/ec/product/7567051s [2]: https://www.nitori-net.jp/ec/cat/Shingu/Blanket/BlanketTowel... reply trogdor 6 hours agoparent> in Japan there is still a strong culture to try to avoid using air conditioning at night Why? reply smithcoin 5 hours agorootparentSuperstition reply gboss 4 hours agorootparentIt’s not just superstition. Air conditioned air can be bad for you if you’re not maintaining it. It is also drying so it can bother your throat, eyes, and skin. It might make it easier to transmit COVID though that might just be in door air. I definitely prefer comfortable outdoor air to air conditioned air. https://www.webmd.com/a-to-z-guides/ss/slideshow-what-ac-doe... https://time.com/3942050/air-conditioner-healthy/ reply nomel 5 hours agorootparentprevThere’s a little subtlety to it. In some (all?) parts of Asia it’s intentional misinformation to reduce power usage. reply JackMorgan 15 hours agoprevI was just about to buy almost this exact setup! Excellent to see it's effective. I thought I was mad looking into fish tank coolers. I was thinking though skipping the evaporative cooler and going with a compressor model. Quite a bit more expensive but I think it'll be more effective in the humid North East. I have realized over the years that I need to be seriously cold to sleep well. My health watch always registers a great sleep and recharged \"body battery\" when I've been almost shivering all night. My partner likes to joke that my body needs \"suffering to get fully rested\". These days, I only ever have nightmares when I'm too hot. It's a challenge to stay cool enough to not get them. I did recently switch mattresses to this Airweave Futon: https://airweave.com/products/futon It's an amazing product, it helps keep me cool, and is nice and firm, which I need to have a restful sleep without back pain. By far my favorite mattress I've ever used. The Airweave has reduced my hot nights significantly, and I'm thinking if I can put a cool pad underneath it, I'll be set! I'm going to have to try this DIY project for sure now! reply user_7832 5 hours agoparent> My health watch always registers a great sleep and recharged \"body battery\" when I've been almost shivering all night. Would you mind sharing which watch (model) offers this? I guess a Garmin of some sort? reply asats 3 hours agorootparentI think most Garmins have it. I have a Venu 3 and am very happy with it, the killer feature for me is it getting over a week of battery life on a single charge vs the apple watch with it's 20 hours. reply JackMorgan 4 hours agorootparentprevI see the same results on both the Vivosmart 4 and the original Instinct. Both Garmin. reply alwayslikethis 15 hours agoparentprev> Compressor I would think these would be too loud for a comfortable sleep environment. reply kijin 14 hours agorootparentYou could put a small compressor outside and connect a well-insulated hose to pump the refrigerant inside, just like a split system air conditioner. Then the noise in your bedroom will be no worse than the occasional hum of a fridge in the other room. As someone who lives in a humid climate, I wouldn't even think of using an evaporative cooler anywhere in my home, for any purpose, period. It's either compressors or nothing. reply alwayslikethis 5 hours agorootparentDoes the water need to be that cold? I would think the cooling can be accomplished just by having a large enough water tank and possibly a radiator and fan so that the thermal mass would prevent your body from heating it up for the 8 hours or so you spend on bed. reply sagarm 12 hours agorootparentprevI believe the Eight sleep uses a Peltier cooler. Not terribly efficient, but the ΔT and the load are low (a human body puts out[…] I was put off by the product due to the now-mandatory paired subscription ($200/yr with purchase, annually paired only, can cancel after) […] It is both confusing and fascinating how some companies manage to put out a product with a subscription, the existence of which defies logic and consumer expectation, and yet they manage to find a group of people who tolerate it. reply avidiax 10 hours agoparentThis is the new venture capital model. Everything must be (or come with) a subscription. It's a way to take an area that has low technological innovation (little reason to buy next year's cooling mattress, which is the same as last year's mattress) and turn it into recurring income. As a bonus, all the smart features that you bundle into the subscription are also the personal data of people with high disposable income. You can now make money twice. reply spencerchubb 5 hours agorootparentNot just venture capital. Established businesses too Investors like recurring revenue because it's simple to understand. You can just slap a multiple on it to figure out the valuation of a business reply noelwelsh 11 hours agoprevI feel that the vast majority of people should not need an elaborate setup to sleep well. If you need to spends hundreds or thousands of currency units on a setup beyond a basic bed, there is something wrong that could probably be fixed in a simpler way. reply coreyh14444 10 hours agoparentConsider Northern Europe. We live in Copenhagen and approximately zero percent of apartments have air conditioning. Personally, I sleep like crap for 3-4 weeks out of the summer and I'm totally going to try this next Spring. reply switch007 11 hours agoparentprevRight. If people are overheating at night during relatively normal temperatures, it’s probably a medical/diet issue or something in the bed is making you over heat, such as stupid foam. reply crooked-v 10 hours agorootparentI think there's a simpler explanation for a lot of people: they like sleeping under a big fluffy comforter or a heavy quilt or so have you for weight/texture reasons, but don't want to cool down the entire house just to make that comfortable. reply amarcheschi 9 hours agorootparentprevi've bought a moderately expensive, very comfy foam bed which is however much hotter than my previous absolutely cheap 20yr old spring mattress... I'm apalled by the fact that in 20 years we have regressed rather than improved on the temperature aspect of mattresses reply kenjackson 4 hours agoparentprevFor myself I can sleep at any reasonable temperature. But between 60-65 F, I sleep extremely well. I do it with a relatively simple set up called an air conditioner, but I have fantasized about having a pillow that is always cool to the touch. reply flippyhead 5 hours agoprevGood! I'm glad for any alternatives to 8sleep. I have two of their beds, and love them, but bought them before they starting requiring a yearly subscription, which I HATE. Unfortunately, they are the best product available as far as I can tell. Though definitely expensive. reply stavros 2 hours agoparentThey require a subscription? Why does a mattress require subscription? reply chairmansteve 13 hours agoprevGreat hack! I do a cold shower before bed. Often helps. You can start the shower warm and then slowly reduce the temperature. reply Xfx7028 2 hours agoparentI also do this. I lived most of my life in the Mediterranean country and lived in a apartment on the top floor with just a concrete roof where the room temperature was 34°C even at night. The solution I found was to take frequent cold showers. Stay under the shower for some time, like 5-10 mins, until you get cold, and then without drying yourself, lie in bed naked. I also felt so cool that I even put a blanket over me. It takes a while until you get warm again, so it's enough time to fall asleep. For more extreme hot situations you can put a wet cloth/t-shirt on your body, but I think that might be unhealthy. reply tasuki 12 hours agoparentprevWow. I do cold showers too, but never before bed. It'd wake me up! reply at_a_remove 7 hours agorootparentThe method works, and I am going to elaborate on it a little. As the poster said, start warm, work your way down to cool, then cold. Very cold. But what really seems to kick one off to dreamland is not drying your hair, instead putting a towel down on your pillow. As your hair dries, you'll get some more evaporative cooling strictly for your head, and that helps even more reply d0mine 5 hours agorootparentHave you tried a hot shower instead just before the bed. The trick is that it cools down the core when you leave bathroom (by bringing blood to the surface). Extremities should be warm, the core(CBT)—cold, to fall asleep easier. reply kijin 13 hours agoprevYour bed needs to be breathable. That's the single most important thing. A cool bed without proper ventilation will attract condensation and mold, and make you feel damp. A good spring mattress sitting on top of slats will never get uncomfortably hot or cold in any particular spot, because air moves freely in and out, powered by your own body movements. The mattress should also be as firm as you can tolerate. A firm mattress leaves breathable space between some parts of your body and the surface of the bed, instead of allowing your body to sink into the foam and become insulated on all sides. Again, airflow is key. Regardless of what heating or cooling solution you have, airflow will multiply its effect. reply switch007 11 hours agoparentTrouble is the spring mattress manufacturers kept reducing quality to achieve a price point and then people needed the extra comfort they took away, for cheap, so hot, foam toppers became the norm. If you want an all-natural, chemical-free, traditional hand-made pocket-sprung mattress you’re looking at £3-6k here in the UK. But I agree they are the pinnacle of mattresses, ticking every box reply globular-toast 11 hours agoparentprevI'll add to this I find a wool blanket/duvet to be so much better than polyester/plastic. When I use plastic duvets in hotels etc it feels like I'm wrapped in a plastic bag. reply kijin 10 hours agorootparentCompletely agreed. Natural fibers like wool and cotton help moderate humidity by quickly absorbing moisture and slowly letting it evaporate. Synthetic fibers can be designed to channel moisture away from your skin instead of trapping it, but they can't absorb any. Besides, I doubt that hotels use sportswear-grade stuff in their duvets. reply taneq 16 hours agoprevA cooler mattress is better to sleep on? That sounds like the opposite of my experience, unless the weather is uncomfortably warm and there’s no air con. reply drilbo 14 hours agoparentA lot of research seems to suggest that cold (something like 62F-68F) is indeed better for sleep. It makes sense if you think about it \"evolutionarily\" or whatever. Basically, your body feels warmth and it assumes the sun is what's heating you up, so suppresses sleep hormones and promotes wakefulness. Anecdotally seems true for me, but I also have poor circulation to my extremities so I just suffer one way or the other. reply CalRobert 12 hours agoparentprev“ unless the weather is uncomfortably warm and there’s no air con.” I think that’s what this is for? I love a cool bed on a warm evening reply foolswisdom 15 hours agoparentprevIn my experience, I might like a cool bed if I was hot in the evening (say, I was physically active). Otherwise, I prefer a cool room with a nice blanket. reply cm2012 16 hours agoprev [–] Classic hacker mentality! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The 1/8th Sleep is a DIY project that serves as a cost-effective alternative to the expensive Eight Sleep self-cooling mattress.- The homemade cooling system costs $382 and includes a mattress topper, fish tank cooler, pump, and smart plug, offering customization options like adding ice for extra cooling.- While it lacks the aesthetics, heating capability, and built-in sleep tracking of the Eight Sleep, the author uses an Apple Watch for sleep tracking, making it suitable for those with a DIY mindset."
    ],
    "commentSummary": [
      "In Japan, there is a cultural preference, particularly among the elderly, to avoid using air conditioning at night, leading to a demand for bed temperature regulation products.- Products such as heat-conducting mattress protectors, towel blankets, and gel pads are popular for maintaining cooler sleeping conditions.- The trend of subscription models for cooling mattresses is emerging, though some consumers find them unnecessary, highlighting a focus on non-air conditioning solutions for comfortable sleep."
    ],
    "points": 169,
    "commentCount": 44,
    "retryCount": 0,
    "time": 1728780181
  },
  {
    "id": 41828601,
    "title": "The Quiet Art of Attention",
    "originLink": "https://billwear.github.io/art-of-attention.html",
    "originBody": "the quiet art of attention There comes a moment in life, often in the quietest of hours, when one realizes that the world will continue on its wayward course, indifferent to our desires or frustrations. And it is then, perhaps, that a subtle truth begins to emerge: the only thing we truly possess, the only thing we might, with enough care, exert some mastery over, is our mind. It is not a realization of resignation, but rather of liberation. For if the mind can be ordered, if it can be made still in the midst of this restless life, then we have already discovered the key to a deeper kind of freedom. But how does one begin? It is not with grand declarations or bold, sweeping changes. That would miss the point entirely. Rather, it is with a gentle attention to the present, a deliberate shift in the way we move through the world. We begin by paying attention to what our mind does—its wanderings, its anxieties, its compulsions. It is a garden untended, overgrown with concerns that may not even be our own. And the first step is simply to watch, to observe how the mind moves, without judgment, without rush. In this quiet observation, we begin to see patterns. The mind leaps from one thing to another, rarely resting. It is caught in a web of habits, most of which we never consciously chose. But, once we notice this, a door opens. There is space, however small, between the thoughts. And in that space, if we are patient, we can decide how to respond rather than being dragged along by every impulse or fear. This is not about control in the traditional sense, but about clarity. To act, not from reflex, but from intent. It is a simple beginning, but one of great consequence. For when we reclaim our attention, even in this small way, we are no longer mere passengers on the journey. We become, in a sense, our own guides. As we grow in this practice of attention, something else becomes clear: much of what occupies our thoughts is unnecessary. The mind is cluttered, filled with concerns that seem urgent but, on closer inspection, do little to serve our deeper well-being. Simplification is not just a matter of decluttering our physical surroundings—it is a way of thinking, of living. As we quiet the noise within, we see more clearly what truly matters. We focus, not on everything, but on the essentials. We pare down, not by force, but by choice. This process of simplification is not an escape from complexity. It is, in fact, a way of engaging with it more meaningfully. There are things in life that are intricate, yes, but not everything needs our attention at once. What truly requires our effort can be approached in small steps, in manageable pieces. The mind works best when it is focused on one thing at a time, when it is allowed to give itself fully to the task at hand. In this way, the most complex of undertakings becomes simple, not because it is easy, but because we have allowed it to unfold naturally, one step after the other. It is tempting, in moments of ambition, to think that we must change everything all at once, that the path to mastery or peace requires a sudden, dramatic shift. But this is rarely the case. In truth, most lasting changes come from small, deliberate actions. It is in the repetition of these small actions, over time, that we build strength, that we build the habits of mind that lead to deeper clarity. Just as a mountain is climbed not in great leaps but in steady, measured steps, so too is the mind brought into alignment by daily, patient attention to the way we think. But in this process, we must remember something important: life is not meant to be rushed through. It is not a race, nor is it a problem to be solved. It is an experience to be lived, and living well requires presence. To focus on one thing deeply, to give it your full attention, is to experience it fully. And when we do this, something remarkable happens. Time, which so often feels like it is slipping through our fingers, begins to slow. Moments become rich, textured. Even the simplest of tasks takes on a new significance when approached with care, with attention. This is the quiet art of living well. It does not demand that we abandon the world, but that we engage with it more mindfully. It asks that we slow down, that we look more closely, that we listen more carefully. For in doing so, we discover that much of what we seek—clarity, peace, even strength—was always within reach. It was simply waiting for us to stop, to pay attention, and to begin again with intention. The mind, like a garden, requires tending. It needs patience, a steady hand, and, above all, consistency. There will be days when it seems unruly, when old habits return, and when focus feels elusive. But these days, too, are part of the process. Each small effort, each moment of renewed attention, builds upon the last. Over time, these moments accumulate, and what was once difficult becomes second nature. And so, the journey to mastery of the mind begins not with grand gestures but with the simplest of practices: the practice of paying attention. Attention to the present, attention to what truly matters, and attention to the quiet spaces in between. In this way, step by step, thought by thought, we move closer to that elusive state of clarity, of peace, and of freedom.",
    "commentLink": "https://news.ycombinator.com/item?id=41828601",
    "commentBody": "The Quiet Art of Attention (billwear.github.io)137 points by billwear 3 hours agohidepastfavorite62 comments vunderba 13 minutes agoIt's a bit heavy on the purple prose (though I was guilty of a very similar writing style in my 20s, and as I got on in life the purity of the idea became more important than its surrounding ornamental structures). The gist of the article reminds me of a quote from the famous pianist Clara Schumann who would admonish her more virtuosic students for striving to play through passages as rapidly as possible. \"Why hurry over beautiful things? Why not linger and enjoy them?\" reply mzajc 2 hours agoprevWell written! I can relate to most of the article. However, I find that > To focus on one thing deeply, to give it your full attention, is to experience it fully. And when we do this, something remarkable happens. Time, which so often feels like it is slipping through our fingers, begins to slow. doesn't really apply to me, or to many people I know and have worked with - it is when I focus on one task that \"time flies\", and it's distractions that end up throwing men out of the zone. reply smith7018 47 minutes agoparentWhat you're describing is a state of flow which is good for things like work but the article seems to be talking about time metaphorically. For example, imagine you're going to your daughter's piano recital and spend the whole time thinking about work. You would be missing out on the experience of watching her perform and grow. If you become mindful of these habits and say \"My mind is focusing on something that I cannot change right now, I should be present\" then you'll be able to fully experience a moment in your child's life. So rather than feeling like life is passing you by, you're able to experience it in the moment. The surrounding sentences of the line you quoted don't read like the author's describing time like you are: \"But in this process, we must remember something important: life is not meant to be rushed through. It is not a race, nor is it a problem to be solved. It is an experience to be lived, and living well requires presence. ... Moments become rich, textured. Even the simplest of tasks takes on a new significance when approached with care, with attention.\" reply yapyap 33 minutes agoparentprevWhat I think he meant is that time slows down for him in the way that time around him speeds up while he can stay focused on one thing. Now of course I’m not the author so I’m not sure but yeah the way you’re describing it (real time flying when you’re locked in on something) is how I feel it goes for most people reply billwear 2 hours agoparentprevagree that the \"clock of life\" is a strange beast, when compared to the clock on the wall. i try to quit paying too much attention to the latter, and time becomes more nuanced and textured. reply gchamonlive 1 hour agoparentprevYeah, it's more like time taking a backseat than slowing down. reply romesmoke 5 minutes agoprevFor a more elaborate, complete take on the value of attention, I can't recommend the work of Sir Iain McGilchrist enough. reply dmje 1 hour agoprevThis is a really great description of why a meditative practice is worth taking time on and also why it’s worth railing against todays constant attention deficit and lack of empty, quiet spaces, both mental and physical. Excellent writing! reply bansuian 33 minutes agoprevThe first paragraph reminds me of the following from The Joy Luck Club. It started to rain again, just a light rain. The people from downstairs called up to me once again to hurry. And my thoughts became more urgent, more strange. I asked myself, what is true about a person? Would I change in the same way the river changes color but still be the same person? And then I saw the curtains blowing wildly, and outside rain was falling harder, causing everyone to scurry and shout. I smiled. And then I realized it was the first time I could see the power of the wind. I couldn’t see the wind itself, but I could see it carried the water that filled the rivers and shaped the countryside. It caused men to yelp and dance. I wiped my eyes and looked in the mirror. I was surprised at what I saw. I had on a beautiful red dress, but what I saw was even more valuable. I was strong. I was pure. I had genuine thoughts inside that no one could see, that no one could ever take away from me. I was like the wind. I threw my head back and smiled proudly to myself. And then I draped the large embroidered red scarf over my face and covered these thoughts up. But underneath the scarf I still knew who I was. I made a promise to myself: I would always remember my parents’ wishes, but I would never forget myself. reply d4mi3n 2 hours agoprevWhile I like the premise of this piece of writing, I quite strongly disagree with the title and this line: > the only thing we truly possess, the only thing we might, with enough care, exert some mastery over, is our mind. Anyone with ADHD, clinical depression, bipolar disorder, and many other conditions simply do not and cannot have full control of their minds without medical intervention. That said, there is a lot to be said for learning how to recognize and compensate for one’s foibles. Meditation and therapy can be helpful for ADHD and some other conditions. It’s not surprising to me that these same things can help people from all walks of life feel more centered and empowered over their own destinies. reply greggyb 1 hour agoparentI will note that you are not responding to what you have quoted, but to an extreme re-interpretation. The OP says \"exert some mastery over\", which is a far cry from the \"full control\" you say some people cannot have. reply afro88 9 minutes agoparentprevYou're right, but it's a bit of an uncharitable take on the post. Nowhere does the it say that medical intervention is unnecessary for people with conditions requiring it. The title, and quoted passage, are fully applicable to those with the listed conditions and without. The advice from the post supplements medical intervention for folks requiring it. reply hu3 32 minutes agoparentprevYou're getting disproportionately criticised and having uncharitable replies but, you're right. Any serious psychiatrist will confirm that medication is immensely helpful to the majority of ADHD cases if not all. Our brains are just different, chemistry-wise. I don't know why people get so offended by this notion. reply keybored 1 hour agoparentprev> Anyone with ADHD, clinical depression, bipolar disorder, and many other conditions simply do not and cannot have full control of their minds without medical intervention. Who are you and how are you privy to what I can and cannot do without intervention? Where do you get off? reply d4mi3n 1 hour agorootparentI'm speaking for myself (ADHD) and anecdotal experience from people in my life with these conditions (clinical depression, bipolar disorder, ADHD). I don't claim to speak for anyone. Your experience may be different, and that is fine and valid. The point I'm trying to make (and that you're also making) is that things that are fundamental truths for some are not always applicable or valid from the context of another person's lived experience. reply keybored 1 hour agorootparent> I'm speaking for myself (ADHD) and anecdotal experience from people in my life with these conditions (clinical depression, bipolar disorder, ADHD). I don't claim to speak for anyone. Now you say that. But you made a very clear, absolute statement that these people “cannot have full control of their minds without medical intervention”. And everyone’s lived experience is eventually respected with some back and forth in these exchanges. But making absolute statements about what people can or cannot do cuts both ways. So it’s best to make your vantage point clear from the start.[1] I’m personally much more offended when someone says that my “type” cannot do something. Compared to assuming that I can. Thanks for the clarification. [1] For all we knew you could have been a medical researcher. reply soulofmischief 20 minutes agorootparentAs someone with severe, often debilitating ADHD, I understand not wanting to depend on medication. It was forced upon me under threat of punishment as a child and heavily exacerbated my OCD and tic syndrome, which led to further punishment anyway. Learning to be okay with medication has taken a long time. But the last couple decades of research have made a few things clear. Importantly, ADHD has been shown to be a genetic disorder, wherein your brain simply doesn't produce the same amount of dopamine receptors as a normal person. This has a profound impact on your mood, executive functioning skills, motor function and more. Drugs which increase the dopamine available in your system can have negative effects (some extra dopamine gets shunted to your motor cortex and causes motor dysfunction/aggravates tics) but when you consider that 60% of ADHD sufferers are also diagnosed with depression, or in my case a large comorbidity with OCD and bipolar disorder, it becomes clear how valuable medicine can be. ADHD is beginning to be understood as a reward-deficiency syndrome [0] and in this light, meditation/mindfulness and good habits are only coping mechanisms for an underlying condition which is ultimately genetic and massively aided by dopaminergic drugs. The result is literally night and day for many people, especially those who did not get diagnosed until adulthood and never developed coping mechanisms. > But making absolute statements about what people can or cannot do cuts both ways. I just lost one of my best friends last year because I moved in with him and experienced incredible prejudice around my disorder, which he was convinced was made up and not real. He would wax on and on about mindfulness, and constantly get defensive and aggressive at the slightest, most inconsequential manifestations of my disorder, and it rapidly deteriorated my mental health at a time where I was already in dire need of a safe space. His bias and increasingly erratic response to my disorder made me feel unsafe until I had no choice but to leave. The entire experience was very traumatic and reminded me of all the times as a child that my disorders lead to punishment and physical abuse. Some people have mild ADHD and it might be a slight convenience for them, but in my case it has been a major defining aspect of my life with a long list of consequences over the years. [0] https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2626918/ reply scubbo 18 minutes agoparentprev> Anyone with ADHD, clinical depression, bipolar disorder, and many other conditions simply do not and cannot have full control of their minds without medical intervention. Right - hence \"might\", not \"do\"; and \"some\", not \"full\". reply krzat 59 minutes agoparentprevHonestly the bigger problem is probably that creating lasting habits is hard. Everyone knows that exercise is important but how many people maintain a consistent routine? With mindfulness it's even worse because there is no way to track how strong your equanimity is. You can't know if you are making progress or just deluding yourself. reply keybored 48 minutes agorootparentMy meditation book (The Mind Illuminated) divides everything into stages. And the strength of one’s mindfulness is discussed. reply abc-1 2 hours agoparentprevYou know every time someone mentions walking, they’re not obligated to mention some people cannot walk right? Can we stop moralizing and grandstanding everything, it gets so tiresome. reply jnovek 29 minutes agorootparentGee, I'm sorry that my disability is so \"tiresome\" for you. It's tiresome for me, too. I spend every day struggling to get through my tasks. It's massive mental and emotional overhead. All day, every day. Often I pass out for the day after work because I'm so exhausted from it. Did you know that the issues associated with ADHD my average life expectancy vs a neurotypical person? Or that I am almost 3x as likely to be unemployed? Just like anyone else I'm going to talk about something that affects every single aspect of my life every day. reply tikhonj 1 hour agorootparentprevPeople largely understand that folks who can't walk can't walk. There is still a lot of moralizing around mental health and treatment. \"ADHD is not a real thing, they just need to stop being lazy\"/etc/etc. The two situations are simply not comparable. reply d4mi3n 1 hour agorootparentprevMy intent was to point out that personal experiences differ and that things the article points out as given are not so given for everyone. My intent was not to moralize and I'm uncertain to what part of my original comment could be interpreted as a moral stance. reply aroman 1 hour agorootparentprevI read no moralizing or grandstanding in GP’s comment. It’s a valid point - most humans on planet earth will experience some form of mental disorder in their lifetime.[0] [0] https://hms.harvard.edu/news/half-worlds-population-will-exp... reply eterpstra 1 hour agorootparentprevYou know, the entire point of this website is to comment on articles with our own thoughts, experiences, and opinions - even if it's moralizing and/or grandstanding. Can we stop discouraging comments in the comment section? It gets so tiresome. reply oxqbldpxo 31 minutes agoprevI'm grateful for this advice. Will put into practice immediately. Death being a certainty, it is difficult to determine what time is actually worth using for. From an individual perspective, religion and knowledge are part of a conceptual world that will end with death. If there's actually anything post-death, it has to do with what it is here being called attention. Something beyond memory and thinking. reply vonnik 2 hours agoprevCognitive control is one of the most important issues of our era, IMHO: https://vonnik.substack.com/p/how-to-take-your-brain-back There are many techniques to increase our CC. The ADHD community is a trailblazer in this respect. reply YossarianFrPrez 1 hour agoparentI used to think that once I started browsing the internet, it was time for a break. One technique I've implemented, which I haven't seen mentioned is \"walking pomodoro technique\": for every 25 minutes of work, I get up and take a brisk 5-min walk (or lately, a jog) and come back. One of the most surprising things is that by mandating my own breaks, I browse the internet a lot less. reply greggyb 1 hour agorootparentI suffer from RSI and definitely do not move enough when I am invested in some piece of work (personal or professional). I recently installed workrave[0] and have noticed marked improvements in just a couple weeks of actually taking breaks when it indicates. I take a 25 second break every 5 minutes, and use this time to do one hand and wrist exercise (I keep some resistance bands and hand exercise balls at the desk). I take a 5 minute break every 25 minutes. I will either do some stretches, or a quick chore (e.g. vacuum one room). https://workrave.org/ reply keybored 1 hour agoparentprevtl;dr: more talk about “dopamine” and addictive devices. I prefer the submission. reply connectsnk 1 hour agoprevTo the author : I find your article really insightful. I want to read more but I realize this article is not on your homepage. There might be more stuff that you have written and is unlisted. How can i find it all. reply scubbo 20 minutes agoparenthttps://billwear.github.io/ reply connectsnk 11 minutes agorootparentThis article is not listed on the homepage. That’s what I said. reply greggyb 1 hour agoparentprevI can recommend the Waking Up app. It has a well paced and well done introduction to mindfulness in a very similar style to the linked article. If interested, I can share a link for a free 30-day trial. I have no affiliation and get no kickback. It has simply been quite useful for me. reply dmje 58 minutes agoparentprevI was reminded of my all-time favourite book on meditation / mindfulness: Mindfulness in Plain English by Henepola Gunaratana. The article had a similar style. Gunaratana’s book is full of humour and beautiful writing and I’d recommend it as a brilliant guide to anyone interested in this stuff. reply pkilgore 1 hour agoparentprevOP is in large degree one articulation of foundational mindfulness concepts that have been written about for centuries. What about the OP specifically appeals? Happy to point you at some other things if I can. reply billwear 47 minutes agoparentprevi usually post here first, and then add to the homepage later. i'll fix that next couple of days. reply keybored 1 hour agoparentprevThe Mind Illuminated is a book with a similar style. reply tolerance 1 hour agoprevThe mind is not the locus of peace and contentment. It is the heart. reply dirtyhippiefree 1 hour agoprevThe power of Being (simply existing mindfully) as opposed to the Doing we feel compelled into. When you Are who you are, you will Do what you do, and likely find greater success because it comes from who you are, not what someone is telling you to do… reply loa_in_ 1 hour agoparentIt's a very hard topic to write or even talk about but what you wrote rings true. reply akomtu 41 minutes agoprevAds Industry would rather not you to have any control over your attention. Indeed, if ads can't distract you, can't steal your attention, then those ads can't make money. reply andsoitis 3 hours agoprev\"As we grow in this practice of attention, something else becomes clear: much of what occupies our thoughts is unnecessary. The mind is cluttered, filled with concerns that seem urgent but, on closer inspection, do little to serve our deeper well-being. Simplification is not just a matter of decluttering our physical surroundings—it is a way of thinking, of living. As we quiet the noise within, we see more clearly what truly matters. We focus, not on everything, but on the essentials. We pare down, not by force, but by choice.\" Our information-technology driven culture does not help; the algorithms and shiny objects they push undermine our attention-ability. reply 8n4vidtmkvmk 1 hour agoparentThis is why I don't wear a smart watch. My phone is always on silent. Sometimes i Ieave it in a different room. There's nothing good on here. 99% of notifications are just trying to sell me something or bad news. Go do something you enjoy and put down the devices, the notifications will still be there in the morning. That is to say, dedicate a block of time for that stuff if you must, but otherwise tune out the Internet. reply abound 2 hours agoparentprevThis may be the culture, but one can largely choose to not participate in it. E.g. not having social media accounts and curating your news sources with a focus on unsensational, fact-based reporting. reply rfonseca 2 hours agorootparent\"curating your news sources with a focus on unsensational, fact-based reporting\" -> curious how you do this! reply abound 2 hours agorootparentNews Minimalist [1] is one way, where it aggregates stories across outlets and uses LLMs to remove clickbait from titles. It also assigns loose 'scores' to each story to approximate how 'important' it is, which I've found to be directionally useful. Ultimately, it comes down to why one consumes news at all. If it's to have something to discuss around the water cooler or dinner table, that's a very different use case than someone trying to pattern match on world events for trading stocks or selling their wares. [1] https://www.newsminimalist.com/ reply dleink 2 hours agorootparentprevI’m working on this as a personal project so I’m interested in other’s solutions! I’m not working on it toooo hard. This is something I think some AI software tool might swoop in and solve before I can build something I’m happy with. reply billwear 2 hours agorootparentprevyes. reply dirtyhippiefree 1 hour agoparentprevThe “attention economy,” defined. It’s why sensory isolation is valuable. Shut out the world and hear what’s being drowned out by the mad scramble to control our attention… reply joseferben 2 hours agoprevexcellent article! one of my favorite books about this that i can not recommend enough is “the miracle of mindfulness” by thich nhat hanh. reply mistermann 1 hour agoprevAs much as I love the sentiment, these sorts of pieces (written or verbal) always contain contradictions, usually important ones relative to the claims. reply keybored 1 hour agoparentThese contradictions (?) may be rooted in the reader’s assumptions about the world: the writer says A and B, but to the reader B implies (not A) because of their world view. In short they might not be actual contradictions. This might seem very vague but a discussion on something so first-person as the mind is ripe for that kind of thing. Which is resolved with dialogue. If the contradictions are brought up. reply hall0ween 2 hours agoprevI like the message of the article overall. And I am skeptical when “liberation” and “freedom” are used when not clearly defined because where I am from (US) these words are thrown around flippantly. If I follow this approach of quietness and attention, will I be free from hunger (ie have food)? Free from fear (eg security from violence)? No, clearly not. There are other freedoms. Also, a left out item that we have direct access to sense, manipulation, cultivation: our bodies. reply joseferben 2 hours agoparentmy read on it is that it’s liberation and freedom in a very buddhist sense. you won’t be free from hunger, but it may reframe our relationship with food so there is less compulsion and mindless consumption. it won’t take away fear in the face of imminent danger (that’s a good thing, we have to survive) but it may reduce background anxiety that’s present in our daily lives. reply keybored 1 hour agoparentprevIf I am free from a shackle, does that mean that I am free from hunger? Clearly not. Hmph, then why say that I am “free” from a shackle. reply billwear 2 hours agoparentprevgood points. the first could have been a better choice, or a better explanation. the second? that's a much longer piece about the three brains and how we integrate them. reply bl0rg 2 hours agoprev [–] tl;dr? reply keybored 1 hour agoparentYou can train your attention. reply dirtyhippiefree 1 hour agoparentprevWhat billwear said first is true. Also, what is the endpoint of oversimplification? It’s okay if you’re waiting for the comic book edition, but I don’t think it’s on the horizon. reply billwear 2 hours agoparentprev [–] that's the point: sometimes there is no tl;dr. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The concept of attention emphasizes controlling the mind by focusing on the present and observing thoughts without judgment, which leads to intentional responses rather than impulsive reactions.",
      "Lasting change is achieved through small, deliberate actions rather than dramatic shifts, promoting clarity, peace, and freedom.",
      "Mindful engagement, akin to tending a garden, enriches experiences and slows the perception of time, encouraging a focus on what truly matters."
    ],
    "commentSummary": [
      "The article \"The Quiet Art of Attention\" highlights the significance of deep focus on a single task to enhance the experience and make time feel slower, contributing to a more meaningful life.",
      "It contrasts this approach with the modern habit of rushing and being easily distracted, advocating for mindfulness and presence.",
      "The discussion incorporates diverse viewpoints on attention, mindfulness, and mental health, with readers sharing personal experiences and suggesting additional resources on mindfulness practices."
    ],
    "points": 138,
    "commentCount": 62,
    "retryCount": 0,
    "time": 1728831708
  },
  {
    "id": 41824171,
    "title": "Omni SenseVoice: High-Speed Speech Recognition with Words Timestamps",
    "originLink": "https://github.com/lifeiteng/OmniSenseVoice",
    "originBody": "Omni SenseVoice 🚀 The Ultimate Speech Recognition Solution Built on SenseVoice, Omni SenseVoice is optimized for lightning-fast inference and precise timestamps—giving you a smarter, faster way to handle audio transcription! Install pip install . Usage omnisense transcribe [OPTIONS] AUDIO_PATH Key Options: --language: Automatically detect the language or specify (auto, zh, en, yue, ja, ko). --textnorm: Choose whether to apply inverse text normalization (withitn for inverse normalized or woitn for raw). --device-id: Run on a specific GPU (default: -1 for CPU). --quantize: Use a quantized model for faster processing. --help: Display detailed help information. Benchmark omnisense benchmark -s -d --num-workers 2 --device-id 0 --batch-size 10 --textnorm woitn --language en benchmark/data/manifests/libritts/libritts_cuts_dev-clean.jsonl Optimize GPU WER ⬇ RTF ⬇ Speed Up 🔥 baseline(onnx) NVIDIA L4 GPU 4.47% 0.1200 1x torch NVIDIA L4 GPU 5.02% 0.0022 50x With Omni SenseVoice, experience up to 50x faster processing without sacrificing accuracy. # LibriTTS DIR=benchmark/data lhotse download libritts -p dev-clean benchmark/dataLibriTTS lhotse prepare libritts -p dev-clean benchmark/data/LibriTTS/LibriTTS benchmark/data/manifests/libritts lhotse cut simple --force-eager -r benchmark/data/manifests/libritts/libritts_recordings_dev-clean.jsonl.gz \\ -s benchmark/data/manifests/libritts/libritts_supervisions_dev-clean.jsonl.gz \\ benchmark/data/manifests/libritts/libritts_cuts_dev-clean.jsonl omnisense benchmark -s -d --num-workers 2 --device-id 0 --batch-size 10 - -textnorm woitn --language en benchmark/data/manifests/libritts/libritts_cuts_dev-clean.jsonl omnisense benchmark -s --num-workers 4 --device-id 0 --batch-size 16 --textnorm woitn --language en benchmark/data/manifests/libritts/libritts_cuts_dev-clean.jsonl Contributing 🙌 Step 1: Code Formatting Set up pre-commit hooks: pip install pre-commit==3.6.0 pre-commit install Step 2: Pull Request Submit your awesome improvements through a PR. 😊",
    "commentLink": "https://news.ycombinator.com/item?id=41824171",
    "commentBody": "Omni SenseVoice: High-Speed Speech Recognition with Words Timestamps (github.com/lifeiteng)135 points by ringer007 18 hours agohidepastfavorite24 comments modeless 16 hours agoLooks cool! Combine this with this new TTS that released today that looks really good and an LLM and you'd have a pretty good all-local voice assistant! https://github.com/SWivid/F5-TTS reply throwaway2016a 3 hours agoprevThis looks really nice. What I find interesting is that it seems to advertise itself for the transcription use case but if it is \"lightning fast\" I wonder if there are better uses cases for it. I use AWS Transcribe[1] primarily. It costs me $0.024 per minute of video and also provides timestamps. It's unclear to me without running the numbers if using this model I could do any better than that seeing as it needs a GPU to run. With that said, I always love to see these things in the Open Source domain. Competition drives innovation. Edit: Doing some math, with spot instances on EC2 or serverless GPU on some other platforms it could be relatively price competitive with AWS Transcribe if the performance is even slightly fast (2 hours of transcription per hour to break even). Of course the devops work for running your own model is higher. [1] https://aws.amazon.com/transcribe/ reply staticautomatic 15 hours agoprevI’ve been building a production app on top of ASR and find the range of models kind of bewildering compared to LLMs and video. The commercial offerings seem to be custom or built on top of Whisper or maybe nvidia canary/parakeet and then you have stuff like speechbrain that seems to run on top of lots of different open models for different tasks. Sometimes it’s genuinely hard to tell what’s a foundation model and what isn’t. Separately, I wonder if this is the model Speechmatics uses. reply woodson 12 hours agoparentThere’s just not a single one-size-fits-all model/pipeline. You choose the right one for the job, depending on whether you need streaming (i.e., low latency; words output right when they’re spoken), run on device (e.g. phone) or server, what languages/dialects, conversational or more “produced” like a news broadcast or podcast, etc. Best way is to benchmark with data in your target domain. reply staticautomatic 12 hours agorootparentSure, you're just going to try lots of things and see what works best, but it's confusing to be comparing things at such different levels of abstraction where a lot of the time you don't even know what you're comparing and it's impossible to do apples-to-apples even on your own test data. If your need is \"speaker identification\", you're going to end up comparing commercial black boxes like Speechmatics (probably custom) vs commercial translucent boxes like Gladia (some custom blend of whisper + pyannote + etc) vs [asr_api]/[some_specific_sepformer_model]. Like, I can observe that products I know to be built on top of whisper don't seem to handle overlapping speaker diarization that well, but I don't actually have any way of knowing if that's got anything to do with whisper. reply leetharris 14 hours agoparentprevWe released a new SOTA ASR as open source just a couple of weeks ago. https://www.rev.com/blog/speech-to-text-technology/introduci... Take a look. We'll be open sourcing more models very soon! reply mkl 14 hours agorootparent> These models are accessible under a non-commercial license. That is not open source. reply threeseed 12 hours agorootparentExactly. It is source available but not open source: https://opensource.org/osd reply yalok 14 hours agorootparentprevthat's great to hear! amazing performance of the model! for voice chat bots, however, shorter input utterances are a norm (anywhere from 1-10 sec), with lots of silence in between, so this limitation is a bit sad: > On the Gigaspeech test suite, Rev’s research model is worse than other open-source models. The average segment length of this corpus is 5.7 seconds; these short segments are not a good match for the design of Rev’s model. These results demonstrate that despite its strong performance on long-form tests, Rev is not the best candidate for short-form recognition applications like voice search. reply staticautomatic 14 hours agorootparentprevI'll check it out. FWIW, in terms of benchmarking, I'm more interested in benchmarks against Gladia, Deepgram, Pyannote, and Speechmatics than whatever is built into the hyperscaler platforms. But I end up doing my own anyway so whatevs. Also, you guys need any training data? I have >10K hrs of conversational iso-audio :) reply unshavedyak 3 hours agoprevCan't wait for a bundle of something like this with screen capture. I'd love to pipe my convos/habits/apps/etc to a local index for search. Seems we're getting close reply jbellis 3 hours agoprevOOMs even in quantized mode on a 3090. What's a better option for personal use? > torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 43.71 GiB. GPU 0 has a total capacity of 24.00 GiB of which 20.74 GiB is free. reply yellow_lead 2 hours agoparentNot sure if you mean in general, or options for this particular project, but Whisper should work for you. reply steinvakt 12 hours agoprevHow does the accuracy compare to Whisper? reply Etheryte 9 hours agoparentThis uses SenseVoice under the hood, which claims to have better accuracy than Whisper. Not sure how accurate that statement is though, since I haven't seen a third party comparison, in this space it's very easy to toot your own horn. [0] https://github.com/FunAudioLLM/SenseVoice reply jmward01 4 hours agorootparentThis uses SenseVoice small under the hood. They claim their large model is better than Whisper large v3, not the small version. This small version is definitely worse than Whisper large v3 but still usable and the extra annotation it does is interesting. reply pferdone 7 hours agorootparentprevI mean they make a bold statement up top just to paddle back a little bit further down with: \"[…] In terms of Chinese and Cantonese recognition, the SenseVoice-Small model has advantages.\" It feels dishonest to me. [0] https://github.com/FunAudioLLM/SenseVoice?tab=readme-ov-file... reply ks2048 2 hours agoparentprevI've been doing some things with Whisper and find the accuracy very good, BUT I've found the timestamps to be pretty bad. For example, using the timestamps directly to clip words or phrases often clips off the end of word (even simple cases where is followed by silence). Since this emphases word timestamps, I may give it a try. reply satvikpendem 14 hours agoprevCan it diarize? reply staticautomatic 13 hours agoparentApparently not. See https://github.com/lifeiteng/OmniSenseVoice/blob/main/src/om.... See also FunASR running SenseVoice but using Kaldi for speaker identification https://github.com/modelscope/FunASR/blob/cd684580991661b9a0... reply mrkramer 7 hours agoprevWith timestamps?! I gotta try this. reply deegles 13 hours agoprevDoes it do diarization? reply staticautomatic 13 hours agoparentApparently not. See my reply to satvikpendem. reply frozencell 9 hours agoprev [–] Does it work with chorus? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Omni SenseVoice is an advanced speech recognition tool designed for rapid transcription and accurate timestamps, installable via Python's package manager, pip.",
      "It offers features such as language detection, text normalization, device selection, and model quantization, enhancing its versatility and performance.",
      "Users can achieve up to 50 times faster processing speeds without compromising accuracy, making it a significant tool for efficient audio transcription."
    ],
    "commentSummary": [
      "Omni SenseVoice is a high-speed speech recognition tool that provides word timestamps, sparking discussions about its use as a local voice assistant when paired with new Text-to-Speech (TTS) and Large Language Model (LLM) technologies.",
      "Users compare Omni SenseVoice to AWS Transcribe, weighing cost and performance, and discuss the variety of Automatic Speech Recognition (ASR) models, highlighting the difficulty in selecting the right one for specific tasks.",
      "There is excitement about new open-source ASR models, though some are not fully open source, and interest in features like diarization and accuracy, especially in comparison to Whisper."
    ],
    "points": 135,
    "commentCount": 24,
    "retryCount": 0,
    "time": 1728780505
  },
  {
    "id": 41822321,
    "title": "First Greenhouse Gas Plumes Detected with NASA-Designed Instrument",
    "originLink": "https://www.jpl.nasa.gov/news/first-greenhouse-gas-plumes-detected-with-nasa-designed-instrument/",
    "originBody": "CLIMATE CHANGE . 3 MIN READ First Greenhouse Gas Plumes Detected With NASA-Designed Instrument Oct. 10, 2024 The imaging spectrometer aboard the Carbon Mapper Coalition’s Tanager-1 satellite identified methane and carbon dioxide plumes in the United States and internationally. Using data from an instrument designed by NASA’s Jet Propulsion Laboratory in Southern California, the nonprofit Carbon Mapper has released the first methane and carbon dioxide detections from the Tanager-1 satellite. The detections highlight methane plumes in Pakistan and Texas, as well as a carbon dioxide plume in South Africa. The data contributes to Carbon Mapper’s goal to identify and measure greenhouse gas point-source emissions on a global scale and make that information accessible and actionable. Get the Latest JPL News SUBSCRIBE TO THE NEWSLETTER Enabled by Carbon Mapper and built by Planet Labs PBC, Tanager-1 launched from Vandenberg Space Force Base in California on Aug. 16 and has been collecting data to verify that its imaging spectrometer, which is based on technology developed at NASA JPL, is functioning properly. Both Planet Labs PBC and JPL are members of the philanthropically funded Carbon Mapper Coalition. “The first greenhouse gas images from Tanager-1 are exciting and are a compelling sign of things to come,” said James Graf, director for Earth Science and Technology at JPL. “The satellite plays a crucial role in detecting and measuring methane and carbon dioxide emissions. The mission is a giant step forward in addressing greenhouse gas emissions.” The data used to produce the Pakistan image was collected over the city of Karachi on Sept. 19 and shows a roughly 2.5-mile-long (4-kilometer-long) methane plume emanating from a landfill. Carbon Mapper’s preliminary estimate of the source emissions rate is more than 2,600 pounds (1,200 kilograms) of methane released per hour. The image collected that same day over Kendal, South Africa, displays a nearly 2-mile-long (3-kilometer-long) carbon dioxide plume coming from a coal-fired power plant. Carbon Mapper’s preliminary estimate of the source emissions rate is roughly 1.3 million pounds (600,000 kilograms) of carbon dioxide per hour. The Texas image, collected on Sept. 24, reveals a methane plume to the south of the city of Midland, in the Permian Basin, one of the largest oilfields in the world. Carbon Mapper’s preliminary estimate of the source emissions rate is nearly 900 pounds (400 kilograms) of methane per hour. In the 1980s, JPL helped pioneer the development of imaging spectrometers with AVIRIS (Airborne Visible/Infrared Imaging Spectrometer), and in 2022, NASA installed the imaging spectrometer EMIT (Earth Surface Mineral Dust Source Investigation), developed at JPL, aboard the International Space Station. A descendant of those instruments, the imaging spectrometer aboard Tanager-1 can measure hundreds of wavelengths of light reflected from Earth’s surface. Each chemical compound on the ground and in the atmosphere reflects and absorbs different combinations of wavelengths, which give it a “spectral fingerprint” that researchers can identify. Using this approach, Tanager-1 will help researchers detect and measure emissions down to the facility level. Once in full operation, the spacecraft will scan about 116,000 square miles (300,000 square kilometers) of Earth’s surface per day. Methane and carbon dioxide measurements collected by Tanager-1 will be publicly available on the Carbon Mapper data portal. More About Carbon Mapper Carbon Mapper is a nonprofit organization focused on facilitating timely action to mitigate greenhouse gas emissions. Its mission is to fill gaps in the emerging global ecosystem of methane and carbon dioxide monitoring systems by delivering data at facility scale that is precise, timely, and accessible to empower science-based decision making and action. The organization is leading the development of the Carbon Mapper constellation of satellites supported by a public-private partnership composed of Planet Labs PBC, JPL, the California Air Resources Board, Arizona State University, and RMI, with funding from High Tide Foundation, Bloomberg Philanthropies, Grantham Foundation for the Protection of the Environment, and other philanthropic donors. News Media Contact Andrew Wang / Jane J. Lee Jet Propulsion Laboratory, Pasadena, Calif. 626-379-6874 / 818-354-0307 andrew.wang@jpl.nasa.gov / jane.j.lee@jpl.nasa.gov 2024-136 RELATED NEWS EARTH . NASA-Funded Study Assesses Pollution Near Los Angeles-Area Warehouses CLIMATE CHANGE . NASA Analysis Shows Irreversible Sea Level Rise for Pacific Islands CLIMATE CHANGE . NASA Helps Build New Federal Sea Level Rise Website EARTH . NASA Mission Gets Its First Snapshot of Polar Heat Emissions TECHNOLOGY . NASA JPL Developing Underwater Robots to Venture Deep Below Polar Ice CLIMATE CHANGE . New NASA Study Tallies Carbon Emissions From Massive Canadian Fires EARTH . NASA-Designed Greenhouse Gas-Detection Instrument Launches CLIMATE CHANGE . NASA-Funded Studies Explain How Climate Is Changing Earth’s Rotation EARTH . NASA’s ECOSTRESS Maps Burn Risk Across Phoenix Streets EARTH . NASA-Led Mission to Map Air Pollution Over Both US Coasts EXPLORE MORE IMAGE . Simulated View of Thermal Emissions from Europa IMAGE . Tanager-1 First Methane and Carbon Dioxide Plume Detections IMAGE . Particulate Pollution and Warehouses in the Los Angeles Region IMAGE . Particulate Pollution and Warehouses in the Los Angeles Region IMAGE . NASA's OPERA Project Generates Radar Image of Hurricane Helene IMAGE . Asteroid 2024 PT5's Orbit Around the Sun (Animation) IMAGE . Araxa Mine, Brazil IMAGE . NASA's AIRS Instrument Captures Hurricane Helene IMAGE . NASA's AIRS Instrument Captures Hurricane Helene IMAGE . Tanager-1 First Light",
    "commentLink": "https://news.ycombinator.com/item?id=41822321",
    "commentBody": "First Greenhouse Gas Plumes Detected with NASA-Designed Instrument (nasa.gov)118 points by mywacaday 22 hours agohidepastfavorite36 comments ldoughty 19 hours agoAnd this is exactly why _they_ don't want NASA pointing back to Earth... And why they want them to only have enough funding for the megaprojects that redirect federal money into red states. \"There's nothing to see here! Move along!\" But really, I'm glad they managed to get this out there despite the political shenanigans. It looks like they manage their own shenanigans by providing information / assistance, while not actually doing the leg work on building it or deploying it... It will help to eventually find the hotspots (which, if in the US, are likely businesses skirting laws for profit... Or poor monitoring by the business). In either case, we can have more information on where to act. reply mturmon 18 hours agoparentI see where you’re coming from, but the reality is not as stark as you have put it. The forces you point to are real, but it’s more complex because many people have seen the value in learning about Earth. A huge reason we have a NASA Earth observing system (and not just weather satellites) is the studies that grew out of the CFC damage to the ozone layer [1]. Ground stations and aircraft and balloons turned out not to be enough to assess it, so the measurements moved to space and use spectroscopy now. (The current fleet: https://science.nasa.gov/earth-science/missions/) This has led to comprehensive CO2 observation from space, and sea surface temperature, and many other climate-related measurements including methane. All this goes back many decades at this point. It’s not a few people who managed to launch one satellite! [1] For short, https://en.wikipedia.org/wiki/Earth_Observing_System#History..., but see also: https://muse.jhu.edu/book/3472 reply whazor 10 hours agoparentprevIt’s likely that countries will get CO2 satellites to point fingers at each other. Ending up with everyone monitoring everyone. Also, CO2 import taxes will be introduced to protect local industries like steel, fertiliser, and cement. For businesses it is actually in the best interest to get clarity on the rules early on. If they stay behind on technology to reduce emissions, they might eventually lose out. reply blackeyeblitzar 19 hours agoparentprevWhat specifically are the “political shenanigans”? Do you have any evidence/links? I personally don’t think studying earth is really what I want out of NASA. I would expect that funding to go towards study of space. For Earth, I would view that as more the job of the EPA or some other agency, and think it’s more appropriate for them to set aside funds for a satellite or whatever they need. But yes it’s interesting to see where there are unexpected plumes. I suspect a lot of those are in third world countries where regulations and rule of law is worse, and it may be hard to address those effectively (except by subsidizing them). Sure there are some examples in the US, but the ones in the article seem much smaller in magnitude. reply mturmon 17 hours agorootparentYou are getting a well-deserved correction to your ignorance of political forces that might result from perceived threats to the oil and gas industry. I want to comment on a separate thing, about responsibility for the measurement, that’s in your second paragraph. Developing the spectroscopic measurement of methane from space took a lot of time and engineering skill. (I happened to observe some of this work as part of $dayjob.) In the US, the responsibility for developing novel space measurement technology has historically gone to NASA. It is proven in space (raised to “TRL 9” in the jargon) by NASA, and then transitioned to other sectors, like NOAA or USGS, for operational use. The prioritization and maturation is very well developed at this point. Part of the reason it rests with NASA is that systems for spectroscopic measurement of gases (in this example) are also used for other space missions, e.g. planetary and astrophysics. For instance, some of the team of this methane instrument overlaps with the MISE team that will use spectroscopy for Europa https://europa.nasa.gov/spacecraft/instruments/mise/). Besides sharing personnel and knowledge, I believe some of the sensor hardware for MISE and for that in OP was fabricated at the same facility. reply blackeyeblitzar 16 hours agorootparent> You are getting a well-deserved correction to your ignorance of political forces that might result from perceived threats to the oil and gas industry. What I am getting from your comment is an ad hominem attack just because I asked for evidence around claims that I can only assume were false. Instead of admitting they were false, you’re calling me “ignorant”. Nice. reply cwalv 15 hours agorootparentNothing wrong with ignorance .. asking for a source, as you did, is implicitly saying you recognize that you may be ignorant. The only shameful ignorance is ignorance of ignorance. That said, I assumed the \"You are getting a well-deserved correction\" part you're replying to had a typo, or was referring to some other post. I didn't parse it the same way. To me, if a correction of ignorance is \"well deserved,\" that's a vague compliment. reply bloomingeek 14 hours agorootparentWhen someone points to an area of my ignorance, I research the hell out of it to become educated on the matter. If I were to become offended by this being pointed out, I would have succumbed to stupidity. reply dylan604 19 hours agorootparentprevDepending on who wins the election, but one party has made publicly known that they will specifically defund or worse the agencies that will report this kind of information. What other examples of political shenanigans do you need? The same party that when they were in office removed the ability of these same agencies from making these type of releases to the point that they created \"rogue\" social media accounts. reply blackeyeblitzar 19 hours agorootparent> one party has made publicly known that they will specifically defund or worse the agencies that will report this kind of information Evidence on the specific claim you’re making? Defunding government agencies for savings or efficiency is not the same as trying to defund things specifically to hide certain scientific data with some sinister goal. > What other examples of political shenanigans do you need? I’m looking for evidence that supports the GP’s claim. Are there political acts that redirected money from NASA into red states, with an intent to funnel funding based on that political leaning? Is there evidence that they tried (and succeeded?) interfering with specific projects on the grounds that it would show pollution sources that are politically problematic (as opposed to simply defining NASA’s mission as studying things outside of earth)? reply defrost 18 hours agorootparentThe US has been through exactly this before: (2016): On space issues, a senior Trump advisor, former Pennsylvania Rep. Bob Walker, has called for ending NASA earth science research, including work related to climate change. Walker contends that NASA’s proper role is deep-space research and exploration, not “politically correct environmental monitoring.” ~ https://theconversation.com/eyes-in-the-sky-cutting-nasa-ear... (2018) Trump White House quietly cancels NASA research verifying greenhouse gas cuts ~ https://www.science.org/content/article/trump-white-house-qu... And, again, NASA's mission is assisted by testing instrument designs from space on the only planet we have complete surface access to .. otherwise it's just diddling about with remote guesswork sans ground truthing. reply startupsfail 18 hours agorootparentThe tragedy of MAGA is that this is the tragedy of the commons in an adversarial setting, where some members are actively trying to destroy the commons to gain political power. Add to that foreign power interference to finance helpful idiots and it’s not surprising that even obvious common goods, like triaging where accidental greenhouse gas leakage happens are treated adversely. reply dylan604 17 hours agorootparentI imagine the foreign agencies running these interference campaigns as caricatures sipping champagne/cognac/etc, cigar smoking, fully belly laughing that is occurring in all of these places at how little effort is being expelled on their part causing so much chaos on our part. reply dylan604 18 hours agorootparentprevhttps://en.wikipedia.org/wiki/Project_2025 Just defunding agencies from reporting science is bad enough regardless of where they move the money. Denying the part because it's not the whole of what you're looking for says more about you than anything else. reply defrost 18 hours agorootparentprev> I would expect that funding to go towards study of space. Space itself is pretty dull .. it's mostly a near perfect vacuum after all. Planets and stars, galaxies and clusters are more interesting. How does NASA perfect the remote scanning of a planet from an orbital platform if not working out the designs close to home first? The EPA provides ground-truthing, surface level observations across the planet, these are used to calibrate and test results from orbiting instrument packages with more challenging transfer functions ( the path from what an instrument actually produces to an interpretation of what that signal \"means\" ). reply HankB99 19 hours agorootparentprevFunding from \"Carbon Mapper, a new nonprofit organization, and its partners – the State of California, NASA’s Jet Propulsion Laboratory (NASA JPL), Planet, the University of Arizona, Arizona State University (ASU), High Tide Foundation and RMI\" From https://carbonmapper.org/articles/carbon-mapper-launches-sat... I don't know enough about the EPA's charter to know if this would be within their realm or not. Certainly launching and managing satellites is within the purview of NASA. reply noiv 19 hours agoprevHere's the interactive map with nearly 6.5k plumes detected: https://data.carbonmapper.org/#1.04/31.9/19.2 reply perihelions 17 hours agoparentThis is an unusual one: one of California's largest methane plumes (689 kg/hr) is a cheese factory, https://data.carbonmapper.org/?details=CH4_other_250m_-120.8... What exactly is going on there? Are they dumping some sort of nutrient waste into a, wastewater pond? reply abdullahkhalids 13 hours agoparentprevThe link says there are 17379 Plumes and 6418 Sources globally. reply ajoseps 4 hours agoprevIt will be interesting to take the data and normalize it by population density. It won’t be a perfect metric since I’m guessing a lot of these plumes will have their source material sourced from elsewhere e.g. landfills, recycling centers, manufacturing reply tagami 14 hours agoprevThe project is funded by 501(c)(3) non-profit Carbon Mapper. Tanager-1 was built by Public Benefit Corp and publicly traded Planet Labs. I understand that the funding raised from the non-profit paid for the licensing of the technology developed at JPL, NASA's only federally funded R&D center managed by CalTech. reply Sandbag5802 19 hours agoprevI wonder with this data, it'll push communities to do something about these plumes if they are close to their home. reply leptons 19 hours agoprevWow, Texas. But, I'm not sure if the data is complete across the globe yet. There doesn't seem to be much data over China yet. reply dylan604 19 hours agoparentAnd even if there were data from China, there's very little that could be done about it. It's not like the US could shame the plank in their eye. reply mturmon 18 hours agorootparentOne goal is to do a fine grained global census, just to see what we are confronting: https://acp.copernicus.org/articles/22/9617/2022/ Previously, a high quality census was done of California and it turned up a nice separation into source categories (e.g., oil/gas extraction, gas transport, dairies, landfills): https://www.nature.com/articles/s41586-019-1720-3 This was very impactful and influenced legislation and further measurement. The lead author is the CEO of the nonprofit that coordinated launch of this satellite. He’s been involved in the fundamentals of this measurement for many years, including his time as a lead system engineer at JPL. reply throwup238 19 hours agorootparentprevJust because we can’t do anything about it doesn’t mean that China won’t. They haven’t been completely unresponsive to internal pressure on air pollution and armed with public data, the government might want to take action. Most countries in the world don’t have the resources to run these kinds of satellites anyway so they depend on NASA/ESA to make this kind of data available to decision makers. reply dylan604 19 hours agorootparentIt would make much more sense for the country that NASA is part of to use the data to make changes within itself. Let's focus on the changes we can control not the changes we wish others to make. The problem is that within the US this is believed to be junk science, fake news, and whatever other nonsense labels used. reply mturmon 18 hours agorootparentWhy not both? That is, a global measurement and a regional measurement? (Regional, with airborne and in situ for validation of the large scale measurement.) And appropriate global and regional studies? They are highly synergistic! Maybe something integrated like: https://above.nasa.gov/about.html?#questions reply dylan604 17 hours agorootparent> Why not both? Without the \"moral high ground\", there's no way anyone would listen to someone pulling a \"pot calling the kettle black\", \"do as I say not as a I do\", or whatever other phrase you like. Attempting to influence someone like that does nothing positive for your image reply mistrial9 18 hours agorootparentprevnot very many years ago, the public news in the USA said flatly that climate change was a \"hoax\" to \"get money\".. some people still say things like that in public. Meanwhile, the clock is obviously and plainly ticking into irreversible changes. Throw any and all science information into the public now and see what sticks IMHO reply akamaka 15 hours agorootparentprevThey certainly can do something about methane emissions in other countries, such as this proposal (which is similar to the EU carbon adjustment which came into force this year): https://juliabrownley.house.gov/brownley-introduces-legislat... reply ordu 17 hours agorootparentprevI think this is unfair. China is leading the transition to green energy, China does a lot. The data from the satellite is useful for China also, and China can benefit from it, because it means that China doesn't need to launch their own monitoring satellite. And if USA politely point to China that China missed something, I'm sure China will do something about it. Not instantly, and probably with impolite responses like \"get off my lawn\" like they like, but still China will hear. reply daft_pink 21 hours agoprevnext [2 more] [flagged] Isamu 19 hours agoparentThat would be spectacular! Please alert HN when you have a fart seen from space, I am sure we will all clamor to be the first post to comment! reply nascamallai 16 hours agoprev [–] Here's an approximately 1,000,000-lb plume of the most significant greenhouse gas in the atmosphere: a typical cumulus cloud. Nothing anthropogenic comes remotely close; which fact makes this NASA-designed instrument a waste of time. reply _moof 15 hours agoparentWater vapor, which contributes between 41% and 67% of the Earth's greenhouse effect, has been part of the atmosphere since Earth first formed. Liquid water - clouds - contributes between 25% and 31%, and has likewise been part of the atmosphere since before the existence of humans. In other words, the effect of atmospheric water has had billions of years to reach equilibrium. Atmospheric water, at a global scale, is also not significantly affected by human activity. The remainder of the greenhouse effect is caused by gases like CO2 and methane, which are affected by human activity. They contribute significantly - CO2 is a mere 0.04% of the atmosphere but is responsible for between 18% and 26% of the Earth's greenhouse effect, methane 6% - and we have significantly altered the concentration of these gases since the Industrial Revolution. The amount of CO2 has nearly doubled, and the amount of methane has more than doubled. So no, this mission is not a waste of time. reply throwaway4220 15 hours agoparentprev [–] (Ignoring obvious denialism here) Knowing where methane is literally pouring out for the taking is useful if you want to burn it for energy. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "NASA's Tanager-1 satellite, launched in August 2024, is part of the Carbon Mapper Coalition, aiming to track global greenhouse gas emissions using advanced imaging technology.",
      "The satellite has successfully detected methane emissions in Pakistan and Texas, and a carbon dioxide plume in South Africa, contributing valuable data for emission reduction efforts.",
      "This initiative is a collaboration between Planet Labs, NASA's Jet Propulsion Laboratory (JPL), and other partners, with funding from philanthropic organizations, providing public data to support global environmental goals."
    ],
    "commentSummary": [
      "NASA's new instrument has successfully detected greenhouse gas plumes, leading to discussions about its role in Earth studies versus space exploration.",
      "The project, funded by Carbon Mapper and partners, aims to globally identify methane sources, potentially encouraging communities to address local emissions.",
      "There is ongoing debate about the effectiveness of this data in influencing major emitters like China and the balance between NASA's focus on space and Earth observation."
    ],
    "points": 118,
    "commentCount": 36,
    "retryCount": 0,
    "time": 1728765324
  },
  {
    "id": 41821545,
    "title": "If you were rewriting Emacs from scratch, what would you do differently?",
    "originLink": "https://news.ycombinator.com/item?id=41821545",
    "originBody": "Don&#x27;t get me wrong, I&#x27;m not planning on creating an Emacs killer, nor suggest anyone do that. But, hypothetically, what are some fundamental pitfalls of this foundational application?",
    "commentLink": "https://news.ycombinator.com/item?id=41821545",
    "commentBody": "If you were rewriting Emacs from scratch, what would you do differently?104 points by volemo 23 hours agohidepastfavorite209 comments Don't get me wrong, I'm not planning on creating an Emacs killer, nor suggest anyone do that. But, hypothetically, what are some fundamental pitfalls of this foundational application? lr0 16 hours agoAlmost all the comments here are about things that really possible to do in current Emacs i.e. 'feature-requests' and not \"fundamental pitfalls of this foundational application\". For me, it would be just: - Redesigned with concurrency in mind. - Common Lisp, Scheme, or anything else other than Elisp. [Just the same way Neovim adapted Lua instead of VimScript] - More sane defaults for new users. reply az09mugen 9 hours agoparentSomeone created lem [0] which is basically an Emacs in Common Lisp. I don't know for concurrency but I hope it is designed to do so. Also there isn't org-mode. [0] : https://github.com/lem-project/lem reply mark_l_watson 7 hours agorootparentI enjoy using Lem sometimes, but mostly use Emacs. Lem is a very cool idea, and with Roswell it is easy to install and manage. All that said, it seems like Emacs has been improving rapidly the last few years: native compilation of Elisp, LSP support, good integrations with LLMs. reply anthk 9 hours agorootparentprevMore than org-mode, it needs a Texinfo viewer. Once you can access SBCL's documentation (and internal docs), now you have an almost complete Common Lisp editor with the basic features from Emacs. reply throwaway17_17 15 hours agoparentprevHard agree that these are true ‘fundamental pitfalls’ of Emacs’ design. However I may go further on your third point. I would like to see a dedicated new user/learner mode that in addition to saner defaults would be pretty fully configured and pre-loaded with the most common extensions/plug-ins/etc. This could then lead to development of dedicated documentation/tutorials that assist the new user in becoming proficient in the common style/habits of Emacs usage. This could provide a much simpler full featured ‘base’ state for the app, that can then be customized from that point or can be stripped back by removing the more common add-ons and customized from the ground up. It would in some ways necessitate a strongly opinionated set of defaults and an additional focus on documentation, but I think in the long run the app would benefit from higher retention. reply appplication 13 hours agorootparentI’m on the strong opinion that most highly configurable software should come with strongly opinionated defaults. “It’s infinitely configurable so you can do anything you want” is just… so annoying. You could implement an LLM using punchcards if you were mad enough. The thing that makes a tool effective is the fact that it constrains and specializes. reply randmeerkat 15 hours agorootparentprev> I would like to see a dedicated new user/learner mode “To run the tutorial, start Emacs and type C-h t, that is, Ctrl-h followed by t.”[1] [1] https://www.gnu.org/software/emacs/tour/ reply anthk 8 hours agorootparentThat can be just put in the startup screen. reply umanwizard 5 hours agorootparentThere is already a link to the tutorial on the startup screen. reply jolmg 15 hours agorootparentprev> in addition to saner defaults Isn't that cua-mode? https://www.gnu.org/software/emacs/manual/html_node/emacs/CU... > would be pretty fully configured and pre-loaded with the most common extensions/plug-ins/etc. Doesn't it already do this? org-mode comes pre-installed for instance. reply orra 11 hours agorootparentcua-mode, yes, but the point is it's not the default. Similarly, recentf-mode exists but is not the default. Or has that changed? reply anthk 8 hours agorootparentprevI'm against that. Some random user would use Org-Mode and it wouldn't need any IDE related extensions. Also, another programmer might just use Org-Babel and code in Common Lisp with org-babel and a REPL doing literary programming. Another one would use Emacs for IRC, Email and math with M-x calc and/or iMaxima and AucTex. reply mrob 21 hours agoprevI'd make cursors be positioned within the document instead of on the screen. Currently, Emacs does not support off-screen cursors. If you attempt to scroll a cursor off screen, it will move within the document to stay on screen. This behavior is contrary to all modern text editors, and there is no good workaround. I once made a serious effort to start using Emacs, but ultimately stopped because of the annoying cursor behavior. (There were other annoyances, but none so fundamental and unfixable.) reply akira2501 19 hours agoparentIt (kinda) has multiple cursors per document if you split the frame and display the buffer twice. For longer source files I find myself splitting horizontally, editing in the left pane, and using the right pane for secondary movement and reference. I have a buffer manager that can switch the displayed buffer quickly, when I switch to a different buffer and back again, it retains it's \"secondary\" cursor position that is separate from the other view. reply yoavm 20 hours agoparentprevVim has the same problem: https://github.com/neovim/neovim/issues/989 reply throwaway17_17 15 hours agoparentprevThis is a really interesting case of Emacs being so old that its default (and only in this case) behavior is just unable to comprehend current computer technology and usage. I think I am correct in saying that the lack of capabilities to have horizontal scrolling of a screen was an absolute hard limit of the teletype and terminal era. It should not be difficult to add such capability, but more than the obviously connected portions of Emacs’ code could implicitly rely on the assumptions that come from the impossibility of horizontally extending text. reply mojifwisi 7 hours agoparentprevThis might be a dumb question, but how would an off-screen cursor work in a terminal? According to my understanding of the way terminal emulators/ncurses work, the cursor must be positioned somewhere in the screen, even if it isn't visible. reply umanwizard 5 hours agorootparentI'd be happy to break terminal mode (which I almost never use) in exchange for getting this feature. reply knome 4 hours agorootparentwhereas I use emacs exclusively in terminal mode, and would be rather upset to it no longer supported :) reply umanwizard 2 hours agorootparentFair enough, but a straightforward solution would be to just not allow off-screen cursors in terminal mode, and allow them in GUI mode. reply mrob 5 hours agorootparentprevncurses lets you move the cursor to arbitrary positions, and also lets you hide it. And you can disable echo, so you can type without the cursor having any influence on what you see. This means you can treat the terminal cursor as though it's a software-rendered cursor in a GUI app and apply all the same rules. But nobody considered doing this when Emacs was first written (maybe it wasn't possible then), so the assumption that the cursor is always on screen is difficult to change. reply arghnoname 20 hours agoparentprevI'll try to use the more common Windows/MacOS terms for it, but in emacs I often have the same file opened in two different panes within one window or two separate windows. I do this when I want to be looking at one part of a file while editing another. Markers are used to mark a different point in the file and one can pop their location to a previous mark. reply mrob 20 hours agorootparentPutting cursors within the document does not preclude supporting different cursors for different windows. reply tom_ 20 hours agoparentprevThis still annoys me slightly after nearly 20 years of using Emacs. In response to keypresses, it doesn't bother me too much, as I'm used to the Windows-style behaviour of PgUp/PgDn/etc. moving the caret, much as the Mac behaviour of not doing that is sometimes useful. But for mouse wheel scrolling, which I do a lot - precisely because on Windows this typically does not move the caret! - having point follow along has never felt right. reply taeric 16 hours agorootparentConversely, I curse heavily at Mac programs where I can page down or scroll for a while, only to have an arrow press jump way the heck back in the document. reply entropie 18 hours agorootparentprev> But for mouse wheel scrolling, which I do a lot - precisely because on Windows this typically does not move the caret! - having point follow along has never felt right. Iam pretty sure you can customzize that if you want. > scroll-preserve-screen-position is a variable defined in ‘C source code’. > Its value is ‘keep’ > Original value was nil > Controls if scroll commands move point to keep its screen position unchanged. > A value of nil means point does not keep its screen position except > at the scroll margin or window boundary respectively. > A value of t means point keeps its screen position if the scroll > command moved it vertically out of the window, e.g. when scrolling > by full screens. If point is within ‘next-screen-context-lines’ lines > from the edges of the window, point will typically not keep its screen > position when doing commands like ‘scroll-up-command’/‘scroll-down-command’ > and the like. reply hibbelig 10 hours agorootparentYou’re misunderstanding. In those other apps the behavior is this: you open a document. You put the cursor on line 3 column 3. You use the mouse wheel or trackpad to scroll down to line 600. You hit cursor right. The cursor is now on line 3 column 4. In emacs the cursor would be somewhere around line 600 instead. reply celeritascelery 15 hours agoprevI have actually done some work on a rewrite of the Emacs core in rust[1], so I have thought a lot about this question. Most of the things you would want to change could be fairly easily added to the existing Emacs. The biggest ones that are hard to do with the current core is concurrency and collaborative editing. My particular core is focused on concurrency[2]. Unlike a lot of commenters here, I am trying to stay backwards compatible with elisp. It is not the best, but huge body of existing code is one of the strengths of Emacs. Unlike vimscript, elisp isn’t painful enough to be worth replacing. [1] https://coredumped.dev/2021/10/21/building-an-emacs-lisp-vm-... [2] https://coredumped.dev/2022/05/19/a-vision-of-a-multi-thread... reply dargscisyhp 12 hours agoparentHave never tried it personally, but this talk made me think collaborative editing is a solved or close-to-solved problem in Emacs: https://emacsconf.org/2023/talks/collab/ reply rbc 18 hours agoprevOne thing about Emacs, is it's not really just an editor anymore. Comparing it to other editors kind of misses the mark. It's more like an integrated Lisp development and runtime environment. It reminds me of Smalltalk environments, say Squeak or Pharo, albeit in a very text oriented way. The world could probably make room for an integrated Lisp development environment that makes GUI programming more of a first class citizen. Maybe something like Medley Interlisp? reply anthk 8 hours agoparentEmacs runs on terminals too. If you want GUI's, you can choose Common Lisp with Lem and MCCLIM, or that newish web oriented GUI with a similar environment. reply musicale 20 hours agoprevI don't care that much about the implementation, but I would like to see an emacs environment with: - instant startup - blazingly fast scrolling - minimal keypress-to-display latency I have written a lot of elisp (and had to deal with with buffer variables, dynamic scope, etc.), but aligning with modern scheme or lisp (if it can be kept compact and efficient) probably makes sense at this point. (Current emacs should provide backward compatibility as needed.) Since so many people use emacs as an IDE, I think having an official emacs co-project/subproject focusing on a standard, extensible IDE framework (perhaps for other apps as well) would make sense. It still seems like a pain to display graphics in emacs in various terminal apps. This should be easy, fast, and standardized. As others have noted, supply chain attacks against open source are rampant, so vetting and sandboxing packages seems to be more important now. reply lysace 20 hours agoparentI'm confused. On the modern devices I've recently used emacs on (including very low-powered raspberry pi devices), all of your three criteria are already true. What kind of HW are you running emacs on where this isn't the case? reply musicale 20 hours agorootparentThis is from 2015, but emacs still has higher typing latency than vim in 2024. https://pavelfatin.com/typing-with-pleasure/ If you have better numbers and comparisons for emacs keypress-to-pixel latency, etc. I'd be interested. Note classic vi started up faster than vim. Device makes little to no difference: Raspberry Pi 5, MacBook Pro M1, ThinkPad P1, etc. - emacs is clunky on all of them. I like emacs and it's my daily driver. But it isn't fast. see also: \"Computer latency: 1997-2017\" https://danluu.com/input-lag/ \"Measuring keyboard-to-photon latency with a light sensor\" https://thume.ca/2020/05/20/making-a-latency-tester/ reply znpy 9 hours agorootparentI have often see some fellow coworkers start up their fairly complex vim/neovim/other-vim setups and quite frankly… they ended up recreating a poor emacs implementation. Yeah it’s faster at starting up but it’s worse in pretty much everything else. Oh and they end up typing Ctrl-this Ctrl-that anyway. Might as well use gnu emacs. reply arghnoname 20 hours agorootparentprevEmacs pauses are almost always due to some operation blocking in the main thread. It's pretty annoying and is mostly a consequence to a lot of things effectively being single-threaded. Stock emacs doesn't do this very often, but third party packages that might do expensive tasks often do reply lysace 20 hours agorootparentAh. A case of \"don't do that, then\". reply hollerith 18 hours agorootparentprevReceiving output from a process, as is done for example by shell mode and (I guess, but have not verified) compilation mode, is slow in Emacs because all output is run through comint-carriage-motion and ansi-color-filter-region (which does nothing but throw color information away IIRC) which are written in Elisp. If you don't run the output through those 2 functions, then non-printing characters remain in the output that severely undermine legibility. C programmers tend to develop the ability to glean information from the voluminous output of build processes as it goes whizzing by on a terminal, so they find compilation mode frustratingly slow. Or at least that is my guess as to what happens. reply wging 19 hours agoparentprevFor instant startup I recommend daemon mode and emacsclient. Start emacs via emacs --daemon, open windows (in the terminal or otherwise) via `emacsclient` / `emacsclient -nc` (I use aliases for these). reply stackghost 14 hours agorootparentThe solution to \"this text editor takes too long to start up\" is poorly addressed by the solution to \"just leave it running in the background all the time\". reply dokyun 12 hours agorootparentIt really is not poorly addressed, as it's a reasonable solution to the issue which also brings about it's own benefits. If you have your system configured such that the Emacs server starts up when you log in, by the time you actually start Emacs it will long have already loaded completely. If you actually use your Emacs for several things throughout the day, including but not limited to reading and editing text and documents, then it is likely you will have it always running during that time with several files open, and will find it useful to be using Emacs in the same state it was in the last time you opened it. As an aside, having it running in the background was actually the way the original EMACS that ran on ITS was intended to be used: When you C-x C-c out of it, it does not kill the program, it simply puts it in the background, and the next time you invoke EMACS it brings it back up. This is similar in effect to hitting C-z, then using 'fg' to bring it up again, but this only really works in a terminal. The Emacs server achieves the intended effect better, especially on a graphical display. reply perihelions 19 hours agorootparentprevI wrap my slow .emacs loads inside #'run-with-idle-timer so they run when I'm not looking, and not likely to notice. reply andbberger 16 hours agorootparentwhat is this point of this, when would you not be using emacs? reply perihelions 2 hours agorootparentOnly needs a few seconds of idle time to load libraries. reply musicale 19 hours agorootparentprevYep, warm start is the only way to go unfortunately. But I want a fast start from cold boot like classic vi. reply rbc 18 hours agorootparentI think running daemon mode has significant advantages. You can have frames running on multiple virtual desktops for different uses. You can also step away from a buffer, and return to it, maybe from some other location on a different computer. reply nabla9 23 hours agoprev- Use Common Lisp instead of Emacs Lisp. - Design a more modular architecture to make it easier to extend and maintain different components. - Design a more robust plugin system for development and distribution of extensions. - Implement better sandboxing and security measures for extensions. - Better APIs for extension developers. - better multi-threading support baked into the editor. reply az09mugen 22 hours agoparentFor the Common Lisp part, someone created lem [0], but for the other points I can't tell. I know there is an extension manager being developped, but am not able to judge the robustness. Also that there isn't org-mode. [0] : https://github.com/lem-project/lem reply dokyun 12 hours agoparentprev> - Implement better sandboxing and security measures for extensions. Why, pray tell, should this be of any concern at all? Sandboxing is used when the host is concerned about running programs that he doesn't trust. There is no reason that an Emacs package would require security measures around it, unless it were knowingly potentially malware. The only reality in which I could see this is if people were using proprietary Emacs extensions, in which case I would entirely understand it, because then people would be willingly running malware inside their editor. Perhaps this the stance VS Code users like to take towards extensions? reply jrootabega 2 hours agorootparentA lot emacs usage today consists of running programs you shouldn't trust. And the rest of it is hard because you are either reviewing other people's libraries a lot, or simply not using other people's libraries, or updating infrequently. reply tmtvl 9 hours agorootparentprevWell, Emacs does ship with a browser (because of course it does, that kind of thing is what makes Emacs so amazing) and we all remember the XZ Utils near-backdoor, so I think that security measures would be useful for people who decide to use a less trustworthy archive like MELPA or who install extensions with package-vc. reply anthk 3 hours agorootparentJust create another user account and test your new stuff there. reply jrootabega 2 hours agorootparentHow does one just test for malicious code? reply shprd 22 hours agoparentprev> Use Common Lisp instead of Emacs Lisp. Interesting, why not a scheme? Is it because the popularity in the industry? I don't know much about Emacs or lisps and looking to understand better reply nabla9 22 hours agorootparentScheme is smaller, has more static and less interactive philosophy. CL has most things you need straight out of the box. It's more like operating system than programming language. Exactly what Emacs wants to be. reply aidenn0 16 hours agorootparentprevelisp is more like CL than it is like Scheme; RMS was aware of CL when he was working on emacs, but was suspicious of some of its new features (like lexical binding). reply anthk 8 hours agorootparentEmacs' Elisp it's pretty close to Common Lisp, any CL user can learn Elisp in days (and the opposite it's true too). There's even a port of PAIP exercises into Elisp: https://github.com/yfuna/paip-el reply hollerith 23 hours agoprevSupport prettier typography (if the user is not interacting with Emacs through a terminal, in which case of course the typography is up to the terminal-emulation app). If text in Emacs looked as pretty as text on the web does, it would be less of a struggle for me to stay focused on the Emacs text. (Text on the web was already much above average in pleasantness to look at and to read in the 1990s.) Get rid of any keybinding or UI convention that is there because that is the way they did it the AI Lab in 1967. Make the UI as familiar to the average computer user as possible (but keep the general design of a large rectangle of text) by using mainstream conventions (which come mainly from the Mac and Windows) for how to respond to this or that keypress or to clicking or dragging with this or that mouse button. Inside Emacs is a cross-platform toolkit (where the platforms are MacOS, other Unix derivatives, Windows and the terminal) I would split Emacs into 2 projects: a toolkit and an app that uses the toolkit. That way, if someone wants to create an \"standalone\" org-mode app, Magit app or Gemini browser designed to appeal to people who do not want to spend any time learning to use Emacs the app or \"Emacs the generalized interface to information\", they have a straightforward way to do so. (These \"standalone\" apps that are as easy to learn as any other GUI app will I hope help popularize the Emacs ecosystem.) One thing I definitely would not change is I would not make Emacs dependent on or closely integrated with a browser engine. reply tazjin 22 hours agoparent> If text in Emacs looked as pretty as text on the web does Do you have an example of this? I can't tell any difference for the fonts that I use (with emacs-pgtk). I believe Emacs uses Harfbuzz (same as Chrom{e|ium}). reply hollerith 20 hours agorootparentMost of the text on the web for example is in a proportional-pitch typeface. Does your Emacs usually use a proportional-pitch typeface? If so and you're on Linux, I'll install the font you are using. I've tried using proportional typefaces in Emacs (on Mac), but there was something off, so I went back to monospaced. I could try again now that I have a Linux machine. The text in my Emacs looks almost exactly like the text in my Gnome Terminal. (A slight difference in size is the only thing I notice. To be painfully precise, (window-system) evals to 'pgtk on my Emacs.) The text in Gnome Terminal is not terrible, for sure, but text on the web is a nicer in my experience. reply chrchr 17 hours agorootparentPardon me if you're ahead of me on this, but it sounds like you might be using a proportional typeface as the default or fixed-pitch face. You should get nice-looking proportional type if you set the variable-pitch face to your desired typeface and enter variable-pitch-mode in the buffer. E.g., (custom-set-faces '(variable-pitch ((t :family \"Verdana\" :height 180)))) And then in the buffer you wish to view with proportional type, M-x variable-pitch-mode. reply jasomill 16 hours agorootparentI use IBM Plex Sans (proportional) as my default Emacs font, with variable-pitch as a no-op (i.e., defined as \"(variable-pitch ((t nil)))\" in custom.el), and use IBM Plex Mono as fixed-pitch. Tips for using a variable pitch font as the default: 0. Choose default fixed and variable pitch fonts with identical baseline-to-baseline heights for a given size; this makes everything described below work better (e.g., this is true for all fonts in the IBM Plex family across all platforms I regularly run GUI Emacs on [Linux, Mac, Windows]). 1. Define a fixed-pitch-mode by copy-pasting the built-in variable-pitch-mode and making the obvious changes (both are trivial applications of buffer-face-mode). 2. Add fixed-pitch-mode to hooks for modes that don't play nicely with variable-pitch fonts (calc, dired, hexl, magit, terminal and shell modes, etc.), or where you just prefer fixed-pitch modes (hint: define your fixed-pitch-mode in a package so you can use use-package's \":hook ((foo-mode bar-mode … baz-mode) . function)\" syntax to manage this). 3. Some modes that pop up windows (frames in Emacs parlance) within editing buffers require extensions (e.g., company-posframe-mode for company-mode) to work properly in variable pitch buffers. 4. Last, but certainly not least: assign a convenient key binding to toggle fixed-pitch-mode. I can't emphasize this enough! In fact, I've found that variable pitch is fantastic for coding in most languages if and only if fixed pitch can be quickly toggled on and off with a keystroke, iff this setting is per file rather than global (and iff both fonts have identical line heights, but this is a feature of font families rather than editors). For this reason alone, I'd argue that Emacs supports variable pitch fonts better than most text editors. reply skydhash 16 hours agorootparentI use IBM Plex across the board and I was trying to understand the font issue because I do not have it. I default to fixed-pitch mode for everything, and use variable pitch for UI elements. reply hollerith 17 hours agorootparentprevThanks for the info, but the result of that is not any better than the result of what I had already done. (I wrote a command that put an overlay on the buffer to change the typeface.) It's not as good text on the web IMHO. Typography is very complicated, and I think the people who did the typographical details of Chrome and Firefox were very skilled, is my guess. reply tazjin 20 hours agorootparentprevI used to use proportional pitch fonts for telega.el and certain document buffers, but I stopped because I find that with Jetbrains Mono (for me personally) there isn't any benefit even for longer text. I'd rather have everything be uniform. Emacs is perfectly capable of rendering other fonts, too, though. reply hollerith 19 hours agorootparentYes, I remember now that having everything be uniform was one reason I stopped with the proportional faces in Emacs. reply hollerith 21 hours agorootparentprevHuh. I use pgtk Emacs, too, and am surprised to find someone who doesn't find my statement obvious. reply tazjin 21 hours agorootparentWell, if there's no further info then I'm going to speculate you've misconfigured something ;) reply actionfromafar 21 hours agorootparentOk then, \"make it harder to hold it wrong\". :) reply hollerith 16 hours agorootparentI'm not holding it wrong. reply actionfromafar 8 hours agorootparentThat's the joke, people weren't holding the iPhone 4 wrong, either. :) reply abdullahkhalids 21 hours agorootparentprevI have never figured out how to get a good font and rendering going for text in Urdu/arabic script. reply GregDavidson 19 hours agoprev1. I would like higher-level datatypes for key abstractions, such as (1.1) Marks -> StableRegions - reference specified text within a buffer - continue to reference same text despite insertions or deletions (1.2) Strings & characters -> StringBuffers - lightweight immutable buffers (no branches or versions) - able to hold any content a subset of a buffer can hold - could be StableRegions of an Arena Buffer (1.3) AbstractBuffers - immutable buffers + a tree of deltas - some special delta types for, e.g. indentation - AbstractBuffers support transactions and versioning - can support collaborative editing - specific versions can be written to textfiles - all versions can be stored in git or in relational database 2. Use WebAssembly instead of a specific programming language. - This was the vision for Guile. - Scheme one of several languages. - ELisp supported but Emacs port efforts keep failing! - The Racket ecosystem has captured this pretty well - if only it supported ELisp! 3. Prefer languages at least as simple as Scheme, but with monotonic semantics! Non-mutable operations would appear as transactions appearing as branches/versions. An editing session would automatically follow the latest transaction in the current branch. Concurrent edits of the same \"file\" create different branches as with git, et al. 4. Separate monolithic Emacs process into SessionProcesses, DisplayProcesses and WorkerProcesses. Multiple DisplayProcesses would allow for tightly-coupled collaborative editing. A WorkerProcess would interface buffers with processes, files, git repositories, etc. on a specific account@host giving functionality like Tramp. A user would start with one DisplayProcess connected to a SessionProcess. A SessionProcess would provide the interface between DisplayProcesses, WorkerProcesses and any co-SessionProcesses of collaborators. WorkerProcesses could be scripted without any other overhead. reply celeritascelery 15 hours agoparentAre all these “higher-level datatypes for key abstractions” trying to achieve first-class collaborative editing? That sounds a lot like Zed. reply GregDavidson 3 hours agorootparentI want ALL of what Emacs provides, much more than any other \"editor\" - actually text-centric programmable productivity platform. And I want more than what Emacs provides, e.g. non-destructive editing and collaboration. So yes, there are other interesting editors out there such as Zed. And no, it's not about adding 1 important feature. I'm greedy! reply susam 22 hours agoprevInstead of using ctrl and meta modifiers, use a leader key like escape or semicolon or comma or some such thing as the prefix key for key bindings. In fact, this desire for leader-key-based, non-modal text editing led me to write devil-mode for Emacs: . reply smokel 22 hours agoparentMake CapsLock an additional Ctrl. On many old keyboards that is where the Ctrl key was positioned [1]. [1] https://en.m.wikipedia.org/wiki/Caps_Lock#Placement reply susam 21 hours agorootparentI know many people like to remap Caps Lock to function as Ctrl. However that setup does not quite work for me. There is only one Caps Lock key on the left side of the keyboard. I need Ctrl on both sides of the keyboard, so that I can use the left Ctrl key while typing Ctrl+P but the right one while typing Ctrl+A. There are other options as well, like remapping the Enter key to act as Ctrl when chorded or using sticky modifiers. I think using an ergonomic keyboard with two large Ctrl keys on both sides of the keyboard is probably the best solution. I've discussed some of these alternatives in more detail . By the way, there are some vendors that still make Unix layout keyboards with the Ctrl key positioned where Caps Lock key usually is: . reply tom_ 20 hours agorootparentI just leave Ctrl where it is and press it with the knuckle of my little finger. I do it just like the guy in the pic on this page, pressing the control key with his little finger: http://xahlee.info/kbd/how_to_press_control_key.html - because that pic is of my hand. (Xah might not recommend doing this all the time, but I've been doing this for nearly 20 years now and I've had no problems.) reply asciimov 18 hours agorootparentprevFor the past 3 years I’ve done the following remap Caps -> lctrl/esc, enter -> rctrl/enter, tab -> lalt/tab, backslash -> ralt/ backslash. Also have the physical right alt mapped to multi key/dead greek. It works really well, and makes emacs much more comfortable to type in. Downside is this keeps me locked to X11 and fighting the occasional app that reads the key codes directly. reply imiric 21 hours agorootparentprevInstead of making it an additional Ctrl key, you can also make it a new separate modifier key with XKB[1]. I've found this very useful over the years for WM-related key bindings, leaving the other modifiers for applications. Tangentially, I really loathe how Wayland has no alternative to this. I'm expected to configure keyboard layouts in every DE or WM I use, which is a much worse UX. [1]: https://vincent.bernat.ch/en/extending-xkb#attaching-symbols... reply petepete 12 hours agorootparentprevI use a HHKB because of this. On the Acorn Archimedes I learnt to type on, it wasn't an extra Ctrl, it was the Ctrl. Caps lock was rightfully relegated to bottom left. reply macintux 20 hours agorootparentprevI was very disappointed that Apple gave up that fight. They also at some point joined the PC world in moving the nubs on their keyboard from \"d\" and \"k\", where you were more likely to notice if your fingers are not in their proper place on the home row. Now if your right hand is offset slightly to the right, you won't feel anything, which is less immediately noticeable than if the nub were under the wrong finger. reply fsckboy 21 hours agoparentprevemacs is not its keybindings. you can bind your emacs keyboard to do what you are asking for; as you said, you wrote a mode for emacs that works the way you want, and it wasn't necessary to rewrite Emacs. reply susam 20 hours agorootparent> emacs is not its keybindings ... and it wasn't necessary to rewrite Emacs That's indeed true! But the premise of this question explores the scenario: What if we did rewrite Emacs from scratch? reply wging 19 hours agoprevI'd really prefer for emacs' implementation language to have been a lisp-1 rather than a lisp-2. It's annoying to have to do different things to treat a function as a value (put it into a list, assign it to a variable, etc.), as opposed to all other kinds of data. Any benefit you get from allowing name collisions (i.e. function named f, related but distinct variable also named f) seems very small in all elisp code I've seen and promotes confusion more than it enables desirable programming patterns. I'd make lexical scope the default and dynamic scope opt-in on a per-variable basis. This one is probably less controversial. I think the devs are moving in that direction (e.g. by changing emacs core code to use lexical scope and adding warnings for code that doesn't opt into it), but I don't see how they will actually be able to change the default without breaking a whole bunch of user code. reply tmtvl 10 hours agoparentMy introduction to Lisp was through SICP and Scheme so I used to be in favour of Lisp-1, but having used Common Lisp for a while now I've changed my mind. Treating a function as a value is easy enough: (mapcar #'list list) It's like how if you want to treat a symbol as data you have to quote it: (let ((x 2)) (list 1 x 'x)) reply khazhoux 20 hours agoprevEmacs is the shittiest tool I’ve been using since 1992 and will use till I die. reply eointierney 18 hours agoparentIt's the finest manure to fertilize thought we can turn over with only a fork reply ironmagma 11 hours agoparentprevIt may be shit, but it's freedom-flavored shit. The best kind. reply eointierney 22 hours agoprevThe core should be in rust, verfied in lean, and the runtime should be in guile. Literate programming in org-mode should be a hard requirement. Package management should require patch algebra. Macros should be submitted to a leaderboard in a blockchain and yield flair in EUDC. M-x measure-beard-length should require 10000 hours of logged usage and unlock the major mode infinity-categorization. Tongue-no-longer-in-cheek: I reckon the C core of Emacs is some of the most battle-hardened code out there. Verification, a-la SEL4, is probably irrelevant but still nice. Guile is modern and performant but Elisp is still its own little joy. Literate programming is always nice until it gets in the way. Straight is good enough for me now. Macros are always cool and a leaderboard would be fun, but patch algebra is really nice, see jujutsu nowadays. And beard length is gendered and so only partially admissable. Infinity categories are way out there and always good for a reference. reply fsckboy 21 hours agoparent>beard length is gendered you insensitive clod https://www.gocomics.com/calvinandhobbes/1986/02/14 reply eointierney 18 hours agorootparentYes reply denotational 22 hours agoparentprevOk, you had me for the first three sentences. reply neilv 21 hours agorootparentImplemented mostly in Guile, with just some native primitives/kernel bits in Rust, makes a lot of sense, for a programmer's application platform in the spirit of Emacs. reply sshine 21 hours agoparentprev> The core should be in rust, verfied in lean, and the runtime should be in guile. Literate programming in org-mode should be a hard requirement. Package management should require patch algebra. Since I agreed with all of these, I’ll add some more non-ironic, idealistic wishes: Plugins must be written in sandboxed WebAssembly so you can know what a plugin is capable of without reading the source code. The runtime must be portable so it can run in wasm32-wasi. reply mdaniel 18 hours agoparentprevrelevant: https://github.com/remacs/remacs#rust-heart-emacs although as of this message it says they've given up in favor of https://github.com/emacs-ng/emacs-ng#contributing which does mention Rust but seems to be ... a lot different from Emacs so I guess they're really going all-in on the \"NG\" part for the non-tongue-in-cheek, also relevant: https://www.emacswiki.org/emacs/GuileEmacs#h5o-2 reply mech422 16 hours agoparentprev>> see jujutsu nowadays I'm looking at pijul.. https://pijul.org/ reply eddieh 16 hours agoparentprevWhen I saw “should be in rust” my instinct was to flame like nobody’s business, glad I kept reading. Lmao. Good slow burn. Though, not to put too fine a point on it, I know it was in jest, but the core implementation language is the least of my concerns. As long as it is extremely portable, compiles fast, and runs on virtually anything, then whatever the core is, doesn’t matter much to me. reply mikewarot 20 hours agoprevI'd start with the core TECO editor I've written in Free Pascal[1]. Free Pascal does gigabyte strings you don't even have to allocate. Then I'd read the Emacs manual, and start writing code and tweaking TECO to make it all impedance match better. But I'm old and weird, so maybe not the best way to get there. [1] https://github.com/mikewarot/teco reply ristos 6 hours agoprevI wish it were built with a very small, minimal core, that was just the TUI with the lisp machine like functionality, scriptable using R7RS scheme instead of elisp. And then everything else is built on top in a more modular fashion, with \"battery packs\" that install many modular components into one higher level functionality, like an IDE. I'd like to eventually write something like this, there are a lot of things it would enable that you just can't do with emacs. It would be an emacs killer if implemented well, but it would also open up a whole new set of possibilities. I don't know how far I'd get just working on it on my free time, which is competing for time with other projects. I'm looking for funding for these sorts of projects. If anyone reading this is an investor and interested in funding this sort of thing, please reach out. reply keyle 19 hours agoprevI like emacs for its flexibility and its ability to be a platform for people to build just about any extension. So for this you'd need a solid scripting language. I was never sold on Lisp but it's fine. I'd prefer to see lua; it just makes more sense. The repo-reference stuff works pretty well all things considered. If it were python it would be hell. Crazy side of me would like to see it fully written in a safe(r) language like rust, swift or zig. Basically your config would be a recompiled subsystem, loaded at runtime, or you'd recompile the whole editor. It wouldn't hurt if the config had a bit more structure. Forcing people to set fonts etc. BEFORE loading other things; essentially enforcing an order in which things get loaded. It can get hairy after years of working in emacs. I love that it's keyboard driven and I'd keep that for sure. The config should automatically be a git repository and any change should generate a meaningful commit (thinking out loud). Better key handling and none c-x nonsense. Switching between keyboard combos and straight up shortcut should be a defined choice not overwritten by x. Every 'plugins' should expose their shortcuts override clearly. Have sane defaults that aren't 30 years of cruft. This means you'd have to consider running on mac just as well as windows, and in and out of the terminal makes this extremely tricky. Better plugin system and discovery. Often the best and only way to find how to solve a problem in emacs was by finding some random gray-beard post on some forum by sheer luck. reply inkyoto 16 hours agoparent> Crazy side of me would like to see it fully written in a safe(r) language like rust, swift or zig. Emacs Lisp is a very safe language. Even though the Emacs core is written in C, that is immaterial and impalpable to Emacs users as they only interact with Emacs Lisp. What benefits would Rust / Swift / Zig bring to an average Emacs user? > Basically your config would be a recompiled subsystem, loaded at runtime, or you'd recompile the whole editor. elisp -> bytecode compilation in Emacs… predates humanity? https://www.gnu.org/software/emacs/manual/html_node/elisp/By... The entire stash of Emacs packages compiles into the byte code at the package installation time, anyway. Personally, I even byte-compile-file my ~/.emacs to eke out an imperceptible speed improvement at the startup time, which is slightless less than entirely useless but it warms my heart that I can do it. Recent versions of Emacs can also compile Elisp into the native code: https://www.gnu.org/software/emacs/manual/html_node/elisp/Na... > Better key handling and none c-x nonsense. Perhaps you do not need Emacs then. reply kazinator 11 hours agorootparent> elisp -> bytecode compilation in Emacs… predates humanity? Actually, I believe it was developed by HackerNews hater Jamie Zawinsky. reply inkyoto 7 hours agorootparentjwz also predates humanity. reply binary132 15 hours agorootparentprevThe pseudo-JIT provided by gccjit is really quite good. I’ve been using emacs for a pretty long time and it is definitely the most noticeable improvement they’ve added so far (and it even works seamlessly out of the box on windows!) reply inkyoto 15 hours agorootparentIt is exceptionally good, indeed. Emacs now starts up in an instant ( If you were rewriting Emacs from scratch, what would you do differently? UI: Electron, of course. Json to represent the edit buffer in RAM. Each utf8 code point base64 encoded, in a json array, it itself, as a blob, base64 encoded. Now, before you complain that that is gonna blow up the data too much, don’t forget that 1. “Ram is cheap” and 2. “gzipped base64 is about the same size as binary”. So, of course, we’ll gzip the data in RAM. Plugins should be JavaScript, as should be self-evident. And you’ll need a few installations of python (both 2 and 3) and node.js (each in its own docker container, obviously) to glue it all together and provide reproduceability. With some care and work, it’ll run even on a modest machine taking up merely 60GB of disk, 32GB of RAM, a 4090ti GPU, and 8 CPU cores. Every key press should be passed through an LLM, to add some intelligence to the editor. The user will, of course, supply a ChatGPT api key when they register for their mandatory myNewEmacs.ai account that they’ll need to subscribe to the editor for only the cost of a few lattes a month. It is 2024, after all. One must use modern tools and technologies. reply Crosseye_Jack 20 hours agoparentLGTM: Ship it! reply maxk42 21 hours agoparentprevPrefer Tauri to Electron. It is 2024, after all. reply aardvark179 20 hours agoparentprevThanks, I hate it. reply dannyfreeman 17 hours agoprevStarting with some kind of namespacing solution for emacs lisp would be nice. reply aidenn0 16 hours agoprevUse Common Lisp instead of making its own mac-lisp like dialect. reply intellectronica 21 hours agoprevScheme instead of ELISP. Concurrency (Async IO would do). reply abe-101 15 hours agoprevI wouldn't. I just used vim reply otabdeveloper4 22 hours agoprevEmacs is fine, but buggy as hell. Their version of Lisp is clearly not suited for any large-scale development. (This trickles down hard into user experience, i.e., lack of parallelism or multithreading.) reply keyle 19 hours agoparentEmacs is fine, but buggy as hell. That kind of took 180 turn on that one. Ship is fine, but leaks as hell. reply buescher 19 hours agorootparentAs long as I can keep this ship, I will keep bailing. reply JadeNB 21 hours agoparentprev> Emacs is fine, but buggy as hell. Is this so obvious as to go without examples? I'm no Emacs power user, nor even really an Emacs user, but it certainly conflicts with my understanding of core Emacs. reply the_clarence 21 hours agoprevI would probably just implement vscode but for the terminal. Emacs shortcuts already work by default in vscode for the most part. reply umanwizard 19 hours agoparentNone of the main selling points of emacs have anything to do with its shortcuts or using it in the terminal. Plenty of emacs users (including me) rarely or never use it in the terminal. It’s a GUI editor just like vscode is. I think this misconception comes from the fact that (1) people often compare emacs and vim, and (2) vim is usually used in the terminal. But emacs and vim are really categorically different things so I think the “emacs vs. vim” meme kinda doesn’t make sense. reply sshine 21 hours agoparentprevWhile Emacs is recognisable for its shortcuts, it is hardly a defining feature. Example: Doom Emacs adds Vim shortcuts, and it is still distinctly Emacs. I think of VSCode as “Emacs, but JavaScript instead of Elisp.” That’s one thing I would not choose, in spite of the good things VSCode brings to the table. reply skydhash 16 hours agorootparentVSCode is notepad with plugins, built upon a web engine. Comparing it to Emacs is a huge disservice to Emacs. reply umanwizard 19 hours agorootparentprevCan I hit one key combination to edit the JavaScript corresponding to any vscode command, debug it and possibly modify it however I want? If not, it’s not really comparable to emacs IMO. reply Decabytes 13 hours agoprevHonestly I think you could go a long way with default evil mode, and updated documentation with evil keybindings in mind. reply znpy 9 hours agoprevI’d keep the possibility to use the ide both in the console as well as in a graphical environment for sure. I’m not sure i’d change the scripting language, despite all of its shortcomings it has proven itself way more than enough. Maybe better concurrency? reply worthless-trash 10 hours agoprevI'd ask the lem guys. reply azram 21 hours agoprevNothing reply I_complete_me 20 hours agoparentI too use vim. reply randmeerkat 15 hours agoprevRealize that VI was the superior editor, give up on the endeavor, print out the code I had already written for emacs, shred it, then set the shredder unceremoniously on fire. For those that downvote me, worth it. reply kunley 4 hours agoparentStrategically thinking, after setting the shredder on fire it will be harder to shred the code of the future emacs clones! :D reply randmeerkat 54 minutes agorootparent> Strategically thinking, after setting the shredder on fire it will be harder to shred the code of the future emacs clones! :D The most realistic future is that emacs begins shipping with a self-shredding feature. C-h s reply kQq9oHeAz6wLLS 14 hours agoparentprev> For those that downvote me, worth it. Nay, quite the opposite; I scrolled down specifically looking for comments like these, because I knew they'd be here. Kind of comforting, really. reply randmeerkat 14 hours agorootparent<3 reply clojureyoureyes 16 hours agoprevI'd rewrite in a Clojure-like language reply geocrasher 13 hours agoprevI'd give it a good editor. reply bitwize 22 hours agoprevBuild it on top of Guile. reply lysace 21 hours agoprevUse python instead of lisp. reply ssivark 19 hours agoparentThere's the Leo editor, if Python's what you want: https://leo-editor.github.io/leo-editor/ reply m463 20 hours agoparentprevI agree. Maybe it's just personal preference, since I think it's easier for me to think in python over lisp (which I've known for longer, but I still fumble through) I do think python would make emacs more accessible to a wider audience. reply sno129 22 hours agoprevWrite Vim instead. /s reply peter-m80 22 hours agoprevNot using lisp reply vincent-manis 1 hour agoparentIf I were rewriting Emacs, I would not replace its language with Python, Lua, or Cobol. I don't like those languages, and I have just as much right to say that as the people who don't like Lisp. reply m463 20 hours agoparentprevwhy the downvotes? This is a reasonable point of view. I sometimes think using lisp for a language is a little like trying to implement comments within json data. reply sva_ 23 hours agoprevNot using Lisp would be helpful reply volemo 23 hours agoparentActually, I love Emacs for its Lisp! Yes, Emacs Lisp is not the best Lisp out there, however, IMHO, it's miles ahead of VimScript. If I were really to rewrite Emacs, I'd use some modern Scheme. reply samatman 19 hours agorootparentFennel is a better Lisp than elisp. Neovim is extensible, and to a large degree written in, Lua, which the target language of Fennel. Most developers do not like writing Lisp. That's just a fact, slamming the downvote button won't change it. I am not among those developers, I like writing Lisp, but most, flatly put, do not. So by choosing Scheme you are competing with a remarkable number of little-used editors which can be extended in Scheme or Common Lisp, as well as Emacs, far and away the top dog in the extensible-in-Lisp-editor niche. Neovim has achieved the best of both worlds, because it can be extended in a rather nice Lisp, and also in Lua, which, while some find the quirks of the language annoying, is at least Algolic in structure, matching the mode of thinking and writing used by the vast majority of devs. reply ac130kz 14 hours agoprevIf I leapfrog over my generally cold attitude towards Lisp-like languages and my habit of Vim keys, the main problem is Emacs's sluggishness, even if it is natively compiled. Neovim fully packed with plugins, LSPs, settings and etc without lazy loading (!) is so much faster. reply EasyMark 17 hours agoprevI’d get rid of the text editor and swap in neovim or hx code as a mode, I’d chuck emacs lisp and use Common Lisp instead, I’d definitely keep org mode and ditch old stuff like mail and news reader as core and let them be add ons written in CL . Rewrite it all in rust. That’s probably enough for today. reply celeritascelery 15 hours agoparentWhat advantage would you get by rewriting it in Rust? reply anthk 3 hours agorootparentAgainst CL, maybe speed, but SBCL has native compilation. Also, with Lem you don't have to care about Rust. Just keep coding CL to achieve anything with a REPL. You don't have to wait weeks to recompile. reply dvh 21 hours agoprev [–] Don't use lisp. Normal people don't like it, it looks weird. I wonder how many projects failed because they were lisp. Normal people: Visits a project page. Sees it's a lisp. Closes page. reply iLemming 1 hour agoparentDon't use Math. Normal people don't like it, it looks weird. I wonder how many projects failed because they wanted to use a lot of math notation. Normal people: Visits a project website. Sees it has equations and formulas. Closes page immediately. That's how it sounds to me. Dismissing Lisp solely based on its syntax (that you're unfamiliar with), is equally irrational as rejecting projects that incorporate mathematical notation. reply vincent-manis 1 hour agoparentprevI am proudly not-normal, in that case. reply neilv 21 hours agoparentprevParentheses scare away anyone who shouts \"bro!\" and fist-bumps each other, before they can insist \"the first thing Emacs needs is a package manager, to hide code as much as possible from casual users\" (missing half the point of Emacs). reply shawn_w 19 hours agorootparentEmacs has a package manager... reply neilv 19 hours agorootparentAnd consequently a lot more friction to users becoming extenders. reply iLemming 1 hour agorootparent\"A friction\" you say? You can run Emacs, open a scratch buffer and extend it right away. You don't even have to save the damn code, you can try it out immediately. Folks complaining about Emacs being hard without even trying to understand any Lisp, is on the same level of whining about how web-development is so much harder compared to building shit in Squarespace (or something), only because you can't figure out HTML, CSS and Javascript. reply neilv 8 minutes agorootparentCompare to when you'd go to use an add-on, and it would be one text file, right on your screen, and you would think: . o O ( hey, I get some of this, and I can just start tweaking it here... ) reply tightbookkeeper 19 hours agoparentprev [–] And how would that make emacs better? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Suggestions for rewriting Emacs include designing with concurrency and using languages like Common Lisp or Scheme instead of Elisp for improved functionality.- Emphasizing a modular architecture, robust plugin systems, and enhanced security measures are key considerations.- Proposals include using modern languages like Rust or Lua for better performance, and separating Emacs into toolkit and app projects for easier standalone app creation."
    ],
    "points": 104,
    "commentCount": 209,
    "retryCount": 0,
    "time": 1728759858
  },
  {
    "id": 41822303,
    "title": "AOO – C++ library for real-time audio streaming and messaging",
    "originLink": "https://aoo.iem.sh/",
    "originBody": "AOO is a lightweight and flexible peer-to-peer audio streaming and messaging solution. It allows to send audio and messages in real-time and on demand between network endpoints - not only in local networks, but also over the public internet!The C&#x2F;C++ library can be easily embedded in host applications or plugins. It even runs on embedded devices, such as the ESP32. In addition, the project contains a Pure Data external and a SuperCollider extension.The following article provides a high level introduction and demonstrates some use cases: https:&#x2F;&#x2F;www.soundingfuture.com&#x2F;en&#x2F;article&#x2F;aoo-low-latency-pe...AOO has been in development since 2020 and it has been used in several art projects since. Now it has finally reached a stage where I feel comfortable presenting it to a larger public.This is still a pre-release and I want to get more feedback before making a final release, so tell me what you think!",
    "commentLink": "https://news.ycombinator.com/item?id=41822303",
    "commentBody": "AOO – C++ library for real-time audio streaming and messaging (iem.sh)97 points by spacechild1 22 hours agohidepastfavorite18 comments AOO is a lightweight and flexible peer-to-peer audio streaming and messaging solution. It allows to send audio and messages in real-time and on demand between network endpoints - not only in local networks, but also over the public internet! The C/C++ library can be easily embedded in host applications or plugins. It even runs on embedded devices, such as the ESP32. In addition, the project contains a Pure Data external and a SuperCollider extension. The following article provides a high level introduction and demonstrates some use cases: https://www.soundingfuture.com/en/article/aoo-low-latency-pe... AOO has been in development since 2020 and it has been used in several art projects since. Now it has finally reached a stage where I feel comfortable presenting it to a larger public. This is still a pre-release and I want to get more feedback before making a final release, so tell me what you think! Zopieux 17 hours ago* How does this compare to Roc, if you've looked at it? https://github.com/roc-streaming/roc-toolkit * Would it be possible to have pulse/pipewire sink/source examples to quickly get started on desktop? reply spacechild1 14 hours agoparent> How does this compare to Roc, if you've looked at it? Strangely, I haven't heard of Roc, thanks! Looks neat, albeit a bit minimal. AOO seems to have more features. Most importantly, it has an (optional) connection server to facilitate peer-to-peer streaming over the internet, but there are also many little things like stream metadata, embedded stream messages, event handling, etc. Some of these are very useful or even essential for online jamming applications, for example. (https://sonobus.net/ uses AOO under the hood.) > Would it be possible to have pulse/pipewire sink/source examples to quickly get started on desktop? ATM, there are only portaudio examples (https://git.iem.at/aoo/aoo/-/tree/master/examples/cpp), but they should translate easily to pulse/pipewire. reply bsaul 9 hours agoprevDoes jamming over the internet requires special techniques on the musician side ? Whenever i played with my DAW, any kind of hardware latency made it impossible to play correctly. I can’t imagine how that would work with two or more people over the net. reply atmanactive 8 hours agoparentJamming online in real-time is possible only between participants in the same city or district. It all boils down to internet-induced latency. More than 20ms and no real-time is possible. For long distance (high latency) online jamming, there is Ninjam [1], developed by Cockos, creators of Reaper DAW, where latency is actually expanded to match a musical division (bar), so everyone can jam in non-realtime but still musically correct. For long distance collaboration and recording (not jamming), I'm currently working on developing a Reaper + Sonobus (which uses AOO) solution which I named ReaConnect [0]. [0] https://github.com/AtmanActive/ReaConnect [1] https://www.cockos.com/ninjam/ reply spacechild1 7 hours agoparentprevWith good hardware and a stable system you should be able to get the harware latency down to 2-5 ms. Then everybody needs a very stable and fast internet connection with low jitter, so you get average ping times of 20 ms or less. On top of that you have a jitter buffer latency of maybe 20 ms or less. (If you're very far apart geographically, you also need to consider the speed of light for the network transmission.) Under ideal circumstances you may end up with a total end-to-end latency of 30 ms. This is the same latency you get when standing 10 meters apart on a large stage. You probably won't be able to play bebop or technical death metal, but pop ballads or stoner rock should be feasable :) reply jononor 10 hours agoprevCool to see that it works even on ESP32 devices. That will open up many possibilities for affordable speakers/microphones with this integrated. reply stargrazer 5 hours agoprevSo given the ESP32 aspect, is there also Android support? Such that it could then be used for an interactive door bell? reply spacechild1 4 hours agoparentAOO works fine on Linux ARM64 devices, so Android shouldn't be a problem. Personally, I haven't tried it yet. I definitely want to add an Android example because mobile devices are a great use case. reply jarmitage 19 hours agoprevHi Christof! Really interesting project! I have used your VSTPlugin before (and probably other stuff..). Have you tried AOO on embedded platforms e.g. Bela, RPi? Would you consider supporting bindings to other langs, e.g. Python? At iil.is we have an OSC package called iipyper and I'm curious what we could do with AOO in the Python ecosystem https://github.com/Intelligent-Instruments-Lab/iipyper reply spacechild1 19 hours agoparentHi, nice to see you here :) > Have you tried AOO on embedded platforms e.g. Bela, RPi? Yes, AOO also works on embedded platforms! I managed to run AOO on the Olimex ESP32-ADF board (https://www.olimex.com/Products/IoT/ESP32/ESP32-ADF/open-sou...) to build low-cost wireless speakers that can be played directly from Pd or SC. With two cores @ 240 MHz, the ESP32 is not exactly a powerful chip :) Bela or RPi is no problem at all. > Would you consider supporting bindings to other langs, e.g. Python? Actually, that has been on my mind. C# and Java might also be worthwhile, in particular for mobile devices. I don't think I will have the time to do it myself in the near future, but if someone's interested in creating langauge bindings, I'm happy to assist! Since AOO also has a plain C interface, it shouldn't be a big deal. EDIT: the IIL looks amazing btw! reply epcoa 19 hours agoparentprevThey claim it will work on an ESP32. If it fits and runs decently there it will be nothing an RPi or Bela. reply spacechild1 19 hours agorootparentIt actually does run on an ESP32 :) And yes, it's really nothing for an RPi or Bela. reply pier25 4 hours agoprevThis is very cool. Is there like a GUI server and client? reply spacechild1 3 hours agoparentIf you count Pd and SC, then yes :-D In the future, I want to add a client plugin and standalone, probably with JUCE. I'm not sure how useful a GUI would be for the server, though, because it typically runs as a service/daemon. reply pier25 2 hours agorootparentI was thinking about a free replacement for the Listento service. https://audiomovers.com/listento/ I do sound design work for media composers and very often I demo the work in realtime during a video call. The problem is that Google Meet, Skype, Zoom, etc don't have good enough audio quality for this. So in parallel with the call I stream high quality audio from the DAW to a web UI the service offers. reply fenesiistvan 3 hours agoprevVoIP reinvented? reply taneq 20 hours agoprev [–] This looks really interesting! Is there anything to stop it being used for video frames as well as audio? Any war stories or interesting projects using it? reply spacechild1 20 hours agoparent [–] Thanks! > Is there anything to stop it being used for video frames as well as audio? Generally, the library is aimed at audio applications and follows the typical model of audio plugins: there is a process() function that takes an array of audio buffers and is called by the host application in the audio callback. That being said, you could abuse the so-called \"stream message\" feature to embed images resp. video frames in the audio stream, but I'm not sure how practical that would be... Someone should try it :) > Any war stories or interesting projects using it? Check out the section \"Use cases\" in my article: https://www.soundingfuture.com/en/article/aoo-low-latency-pe... reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "AOO is a lightweight, flexible peer-to-peer audio streaming and messaging solution designed for real-time and on-demand communication over local and public networks.- The C/C++ library can be embedded in host applications or plugins and is compatible with devices like the ESP32, including integrations with Pure Data and SuperCollider.- Developed since 2020 and utilized in art projects, AOO is now seeking broader public feedback before its final release."
    ],
    "commentSummary": [
      "AOO is a C++ library designed for real-time audio streaming and messaging, compatible with both local and internet networks, and can be integrated into applications or plugins.",
      "The project, in development since 2020, is now seeking broader feedback and offers more features than similar tools, such as Roc, including a connection server for internet streaming.",
      "AOO supports platforms like Linux ARM64, ESP32, and potentially Android, with plans for Python language bindings, and is compatible with embedded platforms like Bela and Raspberry Pi."
    ],
    "points": 97,
    "commentCount": 18,
    "retryCount": 0,
    "time": 1728765167
  },
  {
    "id": 41828923,
    "title": "Ward Christensen (of BBS and XMODEM fame) has died",
    "originLink": "https://en.wikipedia.org/wiki/Ward_Christensen",
    "originBody": "Toggle the table of contents Ward Christensen 5 languages العربية Català Kapampangan Português 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Special pages Permanent link Page information Cite this page Get shortened URL Download QR code Wikidata item Print/export Download as PDF Printable version In other projects Appearance move to sidebar hide From Wikipedia, the free encyclopedia Ward Christensen Ward Christensen and the First BBS Born October 23, 1945 West Bend, Wisconsin, United States Died October 11, 2024 (aged 78) Rolling Meadows, Illinois, United States Known for first bulletin board system (BBS) XMODEM Protocol Ward Christensen (born 1945 in West Bend, Wisconsin, United States) was the co-founder of the CBBS bulletin board, the first bulletin board system (BBS) ever brought online.[1] Christensen, along with partner Randy Suess,[2] members of the Chicago Area Computer Hobbyists' Exchange (CACHE), started development during a blizzard in Chicago, Illinois, and officially established CBBS four weeks later, on February 16, 1978. CACHE members frequently shared programs and had long been discussing some form of file transfer, and the two used the downtime during the blizzard to implement it.[3][4][5] Christensen was noted for building software tools for his needs. He wrote a cassette-based operating system before floppies and hard disks were common. When he lost track of the source code for some programs, he wrote ReSource, an iterative disassembler for the Intel 8080, to help him regenerate the source code. When he needed to send files to Randy Suess, he wrote XMODEM. Jerry Pournelle wrote in 1983 of a collection of CP/M public-domain software that \"probably 50 percent of the really good programs were written by Ward Christensen, a public benefactor.\"[6] Christensen received two 1992 Dvorak Awards for Excellence in Telecommunications, one with Randy Suess for developing the first BBS, and a lifetime achievement award \"for outstanding contributions to PC telecommunications.\"[7] In 1993, he received the Pioneer Award from the Electronic Frontier Foundation.[8] Christensen worked at IBM from 1968[9] until his retirement in 2012. His last position with IBM was field technical sales specialist. In May 2005, Christensen and Suess were both featured in BBS: The Documentary.[10] References [edit] ^ Zelchenko, Peter (30 October 1998). \"Jack Rickard, editor of Boardwatch magazine, saw it coming\". Chicago Tribune. Retrieved 8 October 2022. ^ Metz, Cade (2019-12-20). \"Randy Suess, Computer Bulletin Board Inventor, Dies at 74\". The New York Times. ISSN 0362-4331. Retrieved 2021-10-03. ^ Barry, Rey. \"The Origin of Computer Bulletin Boards\". Freeware Hall of Fame. ^ Goodwins, Rupert. \"Online communities turn twenty-five\". ^ \"Ward Christensen\". Smart Computing Encyclopedia. Archived from the original on June 7, 2011. ^ Pournelle, Jerry (July 1983). \"Interstellar Drives, Osborne Accessories, DEDICATE/32, and Death Valley\". BYTE. p. 323. Retrieved 28 August 2016. ^ \"Dvorak Awards for Excellence in Telecommunications\". citivu. Archived from the original on 2016-03-06. ^ \"Second Annual EFF Pioneer Awards\". Electronic Frontier Foundation. ^ re: R/1ST BBS QUESTIONS (Msg 46394) from Ward Christensen to Steve Culver, July 31, 1993. ^ \"BBS: TheDocumentary\". BBS: The Documentary. Retrieved 15 September 2022. External links [edit] Ward Christensen on Twitter vte Bulletin board systems List of bulletin board systemsList of bulletin board system software CultureANSI artChat roomFile sharing ProtocolsTimelineMUDSysopVirtual community TechnologiesANSI escape codeDoorInternet outdialRemote Imaging ProtocolSkypix NetworksFidoNetRelayNetWWIVnet Media coverageBoardwatchComputer Shoppertextfiles.comBBS: The Documentary PeopleWard ChristensenRandy SuessChuck ForsbergTom JenningsSteve PunterJason Scott Retrieved from \"https://en.wikipedia.org/w/index.php?title=Ward_Christensen&oldid=1250720023\" Categories: American computer programmers IBM employees Milton College alumni People from West Bend, Wisconsin Living people 1945 births Hidden categories: Articles with short description Short description matches Wikidata Articles with hCards",
    "commentLink": "https://news.ycombinator.com/item?id=41828923",
    "commentBody": "Ward Christensen (of BBS and XMODEM fame) has died (wikipedia.org)91 points by DamonHD 3 hours agohidepastfavorite17 comments xorcist 1 hour agoBBSes was such a huge part of being into computers in the 80s and 90s. I really wish this culture could be understood by future generations. Yes, we have the BBS Documentary movie but we need so much more. Everything non-US is underdocumented, and all the subcultures such as the eLiTe scene, the demo scene, the vision impaired stuff, all of that risks being forgotten with time. reply axpvms 9 minutes agoparentI found myself reading through textfiles.com just recently,a really good archive of BBS-era text files. reply jlundberg 37 minutes agoparentprevThis is a relevant reflection and I have contemplated collecting BBS memories from my network and strangers. Will be doable once my kids are a bit older and work is a bit leds intense. Let us stay in touch! 2:206/149 or about in my profile and you’ll find me :) reply glimshe 1 hour agoprevBBSs were a huge part of my life in the 90s. I wanted teenagers of today to be able to feel the same thrill of socializing like we did back then. BBSs are not as good as the Internet, obviously, but there are no full fidelity replacements for BBSs nowadays - if you were there, you get it. reply axpvms 5 minutes agoprevI remember riding my bicycle over to the local sysop's place to pay $5 cash for my BBS account as a young teen. Looking back this was probably ill advised and risky. Turned out the sysop was only a couple of years older than me. reply shrubble 5 minutes agoprevA friend who works in embedded systems pointed out that XMODEM protocol communication is used everywhere in embedded; it may be that the protocol is more widely shipped now than it has been in the past! Many Cisco, Adtran, Juniper etc switches and routers have it in their firmware also. reply sokoloff 2 hours agoprevRIP. My first paid programming gig ($20) was implementing the XMODEM checksum in 6502 assembly for a BBS sysop who had bought an early 1200 baud modem, only to find that his Atari BASIC BBS software was computing the checksum so slowly that it still created slowdowns in file transfers and needed a USR() that could compute it faster. I learned a lot about protocols and algorithms from that exercise (now trivially simple, but wasn't for me at the time). reply relistan 1 hour agoprevBBSes were a very big part of my early computer days. I learned real programming in high school teaching myself and hacking on BBS source code in Pascal. Not knowing that I would soon be on the Internet, one of the reasons I went to university in a city was so that there would be local BBSes. All of that had huge impact on my life and I’m just one small example. I and many others owe huge thanks to Ward Christensen and all those who carried on what he started. reply bane 17 minutes agoprev@dang this would seem to be worthy of a black banner day? reply rexreed 26 minutes agoprevAn amazing guy, and not much of an attention seeker. That he stayed at IBM all his productive work life (1968-2012) says something, especially as \"His last position with IBM was field technical sales specialist.\" reply michaelcampbell 3 hours agoprevOh, darn. I had an online text chat with him on Compuserve back in the 80's; he was surprised anyone knew who he was. Nice guy. reply LVB 1 hour agoprevThat’s a name I remember from my youth. I became pretty interested in all these curiously named file transfer protocols (Xmodem, Zmodem, Kermit, bimodem, etc.) and learned what I could from pouring over microfiche archives of magazines and papers and the local library. reply Scramblejams 19 minutes agoparent(poring*) Yep, it’s one of those names I can’t think of without seeing it emblazoned in green on a black background on my old Apple Monitor III screen. reply greenthrow 22 minutes agoprevBBSes were a big part of my pre-teen years, before dial up internet access became available in my area. Really difficult to explain to younger folks what it was like. XMODEM was the file transfer protocol for more than a decade, as I recall. reply DamonHD 3 hours agoprevAlso see: https://mastodon.laurenweinstein.org/@lauren/113300835222615... reply wnoise 1 hour agoprev [2 more] Dying of BBS and XMODEM fame sounds painful. Can dang or another mod move the parenthetical modifier? reply DamonHD 1 hour agoparent [–] Fixed, thanks. That was my third attempt at being concise and informative... reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Ward Christensen co-founded the first bulletin board system (BBS), CBBS, in 1978 with Randy Suess, marking a significant milestone in the history of online communication.",
      "He is known for developing XMODEM, a protocol for file transfer, and has been recognized with awards like the 1992 Dvorak Awards and the 1993 EFF Pioneer Award for his contributions to PC telecommunications.",
      "Christensen's career at IBM spanned from 1968 to 2012, and he was featured in \"BBS: The Documentary\" in 2005, highlighting his impact on the tech industry."
    ],
    "commentSummary": [
      "Ward Christensen, a pioneer known for creating Bulletin Board Systems (BBS) and the XMODEM protocol, has passed away, marking the end of an era in computer history.",
      "BBSes were crucial in the 1980s and 1990s, serving as early online communities, and Christensen's work remains influential, particularly in embedded systems.",
      "Many individuals have shared personal anecdotes about the impact of BBSes on their lives, expressing appreciation for Christensen's significant contributions to technology and culture."
    ],
    "points": 91,
    "commentCount": 17,
    "retryCount": 0,
    "time": 1728833963
  }
]
