[
  {
    "id": 42579873,
    "title": "I am rich and have no idea what to do",
    "originLink": "https://vinay.sh/i-am-rich-and-have-no-idea-what-to-do-with-my-life/",
    "originBody": "I am rich and have no idea what to do with my life Life has been a haze this last year. After selling my company, I find myself in the totally un-relatable position of never having to work again. Everything feels like a side quest, but not in an inspiring way. I don’t have the same base desires driving me to make money or gain status. I have infinite freedom, yet I don’t know what to do with it, and, honestly, I’m not the most optimistic about life. I know. This is a completely zeroth-world position to be in. The point of this post isn’t to brag or gain sympathy. To be honest, I don’t exactly know what the point of this post is. I tried to manufacture one, but I just felt like a phony. Then I recognized the irony of creating purpose out of a blog post when I don’t currently have much conviction or purpose in life. So I’ll just go ahead and explain my current situation for my own selfish purposes. To push myself to be completely (and awkwardly) vulnerable to a blob of nameless strangers over the internet. No expectations of what comes out of it. Going to the redwoods and giving up $60m Last March I had no idea what to do with my life. I knew that staying at the acquiring company was not it for me for the big company reasons you might suspect (lots of politics, things moved slowly, NPC coworkers, etc.), but I found it very hard to give up a $60m pay package. I had already made more money than I knew what to do with, but your mind does funny things when you start to consider numbers like this. So I decided to go to the redwoods and figure it out. Within 5 minutes of my first hike, the trees smiled at me and whispered their simple wisdom. What is the point of money if it not for freedom? What is your most scarce resource if not time? I would leave to do something. Anything. To be alive again. I had no idea. But I was hell bent on making sure everyone knew I had it all figured out. Out of ego. Out of fear of wading into the unknown. When you work on something that consumes your life for a decade, it’s hard to let go of the certainty and purpose you’ve grown accustomed to. Robotics, or my cringe “trying to be Elon” phase The immediate 2 weeks after leaving an intense 10-year journey, I did what any healthy person does and met with over 70 investors and founders in robotics. I had been learning about robotics for quite some time and was positive I wanted to throw myself into giving computers arms and legs. I had come up with all the tag lines to delude myself into thinking this was my “life’s calling”. Everything had been “leading to this exact point”. “The world is going through a major labor shortage!” “We must stay competitive against China!” “The market for highly repetitive labor is multiple trillions of dollars.” At the end of the 2 weeks, I left feeling deflated and foolish. I didn’t want to start a robotics company. The only thing that seemed interesting to me was humanoids. It started to dawn on me that what I actually wanted was to look like Elon, and that is incredibly cringe. It hurts to even type this out. Breaking up with my girlfriend and realizing I’m very insecure After deciding to not start a robotics company, I found myself rudderless. No sense of direction. I traveled to many beautiful places with my loving and supportive (ex) girlfriend. This 6 month stretch could be several essays on its own, but the outcome of this period is that nothing seemed right. We started getting into regular arguments, and I knew it wasn’t on her. It was me. I was starting to come to terms with all the mounting insecurities I had stuffed down over the past several years. I didn’t feel like I could work on them with her. So I broke things off after almost 2 years of unconditional love. It was extremely painful, but it was the right call. I needed to fully face myself. I have only started to realize that, when Loom was in its early innings, I felt very secure with my position in life, and lots of this stemmed from an extreme gratitude for the journey I was on. I was happy with everything as it was. The growth trajectory of the company was more than I could ever dream for. I was happy. I felt secure. It could all turn to shit the next day, and that would be ok. Then, as the company continued to skyrocket to new heights, I started to have growing expectations for myself, and others started to have growing expectations of me. When we went through our first round of layoffs, this company my ego was hitched to had suffered a massive blow, so I lost myself. This whole chapter of Loom has created a complex web of internalized insecurities I must now work hard to disentangle and free myself from. (If my ex is reading this. Thank you for everything. I am sorry I couldn’t be what you needed me to be.) Climbing mountains in the Himalayas, remembering to do hard things After breaking up with my girlfriend, I did what any healthy person would do and decided to externalize my emotions by climbing a 6800m peak in the Himalayas with absolutely no mountaineering experience or training. In the earliest stages of trekking into the valley, before the altitude sickness, cold, and chronic bronchitis started to settle in, this seemed like the best idea in the world. It wasn’t until every person I met along the way asked how long I had been training for, that it started to settle in how insane what I was doing was. Needless to say, there were some rough patches. I got very hypoxic on one of my summits and had to repel down cliff faces while tripping out of my mind. In the end, I pushed through, completed both my planned summits, and got reacquainted with how important doing hard things is to me. It is the heart beat of my life, and I don’t 100% understand why, but it probably has something to do with me having not the best childhood. When I got back home and regaled my friends with my mountain stories, one of my friends joked that I should work for Elon and Vivek at DOGE and help America get off its current crash to defaulting on its own debt. So I reached out to some people and got in. After 8 calls with people who all talked fast and sounded very autistic smart, I was added to a number of Signal groups and immediately put to work. Working for DOGE for 4 weeks, remembering the power of urgency Within 2 minutes of talking to the final interviewer for DOGE, he asked me if I wanted to join. I said “yes”. Then he said “cool” and I was in multiple Signal groups. I was immediately acquainted with the software, HR, and legal teams and went from 0 to 100 taking meetings and getting shit done. This was the day before Thanksgiving. The next 4 weeks of my life consisted of 100s of calls recruiting the smartest people I’ve ever talked to, working on various projects I’m definitely not able to talk about, and learning how completely dysfunctional the government was. It was a blast. I learned about the power of urgency and having an undeniable mission. Not by reading it somewhere. By experiencing it. I came to realize how laughable my robotics stint had been in comparison. And I started to realize that, although the mission of DOGE is extremely important, it wasn’t the most important thing I needed to focus on with urgency for myself. I needed to get back to ambiguity, focus on my insecurities, and be ok with that for a while. DOGE wasn’t going to fix that. So, after 4 intense and intoxicating weeks, I called off my plans to move to DC and embark on a journey to save our government with some of the smartest people I’ve ever met. And I booked a 1-way ticket to Hawaii. Studying physics in the jungle, focusing on my insecurities So now I’m in Hawaii. I’m learning physics. Why? The reason I tell myself is to build up my first principles foundation so I can start a company that manufactures real world things. It seems plausible, but I’m learning to just accept that I am happy learning physics. That’s the goal in and of itself. If it leads to nothing, that’s ok. If this means I’ll never do something as spectacular as Loom, so be it. It’s been too long since I’ve been completely raw and real with myself, so I’m applying a healthy dose of humility to everything I say and do. It’s the only thing that feels authentic. However, there are some questions left unanswered. Why did I need to do the absolute most to reach this point? Why couldn’t I just leave Loom and say “I don’t know what I want to do next”? Why do I feel the need to only be on a journey if it’s grand? What is wrong with being insignificant? Why is letting people down so hard? I don’t know. But I’m going to find out.",
    "commentLink": "https://news.ycombinator.com/item?id=42579873",
    "commentBody": "I am rich and have no idea what to do (vinay.sh)793 points by vhiremath4 20 hours agohidepastfavorite1375 comments madrox 19 hours agoI don't think you have to have Fuck You Money to get to this point. Most people eventually become disillusioned with work enough that they reevaluate what matters to them. Getting a very profitable exit is just one way to trigger that experience. In my experience, a lot of people who get into this state start self-sabotaging hard as a way of rejecting what feels, ironically, like losing control. Sudden freedom can feel foreign and lot like your world got forcibly taken away from you. I'm not surprised the author is turning down opportunities and breaking off with his girlfriend. It's a way of taking back control. When this happened to me, I pivoted hard from getting satisfaction out of what I built to getting satisfaction out of developing people. Now I take great pride out of the careers I've nurtured...a lot more than what I've built, in most ways. I've heard others express similar ideas in different ways, like \"I now enjoy making other people rich.\" No matter what, I encourage the author to use this time to build connections instead of destroying them (real connections...not work or SF acquaintances). Something I did not read in this essay is how he grew closer to anyone (in fact, I read the opposite). No path out of this valley involves traveling alone. reply nico 16 hours agoparent> No path out of this valley involves traveling alone In my opinion, this is the big take here When you have enough money to not work, it becomes very lonely fast All of a sudden you have tons of time, but no one to share it with. Everyone is busy, mostly with work (also, most people probably can’t afford the same things you can) If you could coordinate to stop working at the same time as your significant other, and a few friends, then you at least would have a group to plan and do stuff with One of the biggest meanings we can find in life, is the feeling of belonging OP seems to be going through a belonging crisis. Trying to figure out what group he wants to belong to reply varelse 2 hours agorootparentTo each their own. Having grown up isolated and ostracized then forced in adulthood to mingle to nurture my career, I deeply enjoy solitude with my dog. It lets me focus on my projects and my hobbies. And it lets me minimize dealing with the complications of other souls. The embrace of the open office floor plan and floating desks was the last nail in the coffin there. Probably some neurodivergence in play here but that's who I am. Entry-Level plutocracy finally gave me the freedom to live the way I've wanted to live the whole time. But do go on about how I'm the weirdo and we need to rely on the wisdom of perfectly normal sorts like Elon Musk and Marc Andreison, I'm all ears. The ridiculous unending pandemic only strengthened my resolve to avoid people in crowds for the rest of my life. Not everyone needs the connection you think they need. And short of your mother, no one will love you like your dog. And it's not like I don't have friends, I do. And I accept their insanity as much as they accept mine. That's all the belonging I seem to need. OP sounds like a rich white dude having rich white dude problems. Boo... Hoo... He might want to try working for a charity for a while to reset his Maslow Hierarchy. At $60 million or more, he's about 1 in 5,000 on the net worth ranking ladder. He needs to get past this failure of imagination he's experiencing and he's the only one that can bring the moment of clarity that will allow that. Don't go through life a freedom hater. reply sentientslug 1 hour agorootparentWeird that you brought race into it, especially when your supposition is incorrect. reply LordAtlas 24 minutes agorootparentprev> OP sounds like a rich white dude having rich white dude problems. Yeah, definitely white: https://www.google.com/search?q=vinay+hiremath&btnG=Search&u... reply NicholasGurr 29 minutes agorootparentprev> OP sounds like a rich white dude having rich white dude problems. Lol. His name is in the URL, and it doesn’t sound white. reply lisper 19 hours agoparentprevIn a way having money makes it harder because it makes it harder to blame your unhappiness on your circumstances. reply nextlevelwizard 11 hours agorootparentYep. I am not even rich. In fact compared to US software engineers I am making pennies, but I am hitting above average for where I live. And at times it is hard to find meaning in every day life. I like my job, but realistically I could quit now and just about coast with my savings for the rest of my life. On other hand I could increase my spending and live more luxurious life style, but that isn't for me. I just like to code, play video games, and be alone in peace and quiet. reply purplethinking 9 hours agorootparentAlmost at that point myself. Thinking about doing another year or two to save up an additional buffer, and make sure we're not right before another 2008-like event before I pull the plug. Start my own company, maybe make some money, maybe not. I'm tired of the grind. I'm tired every single day, and there's no time or energy to try to fix it. At this point I just want to be left alone, in peace and quiet... reply Freak_NL 9 hours agorootparentprevHonestly, if I ever reach the point where my savings would keep me comfortable with a pension to look forward to pick up the slack at the end, I'd quit my job and just focus on my interests, providing a clean house and having a good meal ready in the evening for my wife and son, and develop some side gigs I can give up if they don't give me fulfilment. As it is, I am acutely aware of my privileges as part of a household with two IT-based incomes and not too many worries, and that the world being what it is right now is giving rise to so many uncertainties that I wouldn't dream of abandoning this unless I had a really big bag of money like the author. reply wing-_-nuts 2 hours agorootparentThe big question for me has become health insurance. Yes, I know ACA plans are a thing I justdon't trust it not to be messed with. Protections for preexisting conditions are the only reason retirement is even an eventual possibility for me. I worry that I don't have enough of a life outside of work to make retirement fulfilling, and actually, I don't actually mind working if I'm completely honest. I just never liked the stress of needing a job. reply swiftcoder 7 hours agorootparentprev> Honestly, if I ever reach the point where my savings would keep me comfortable ... I'd quit my job and just focus on my interests Pretty much everyone says this, but surprisingly few people actually seem to succeed at it when push-comes-to-shove reply tjr 3 hours agorootparenthttps://philip.greenspun.com/materialism/early-retirement/ Ask a wage slave what he'd like to accomplish. Chances are the response will be something like \"I'd start every day at the gym and work out for two hours until I was as buff as Brad Pitt. Then I'd practice the piano for three hours. I'd become fluent in Mandarin so that I could be prepared to understand the largest transformation of our time. I'd really learn how to handle a polo pony. I'd learn to fly a helicopter. I'd finish the screenplay that I've been writing and direct a production of it in HDTV.\" Why hasn't he accomplished all of those things? \"Because I'm chained to this desk 50 hours per week at this horrible [insurance|programming|government|administrative|whatever] job. So he has no doubt that he would get all these things done if he didn't have to work? \"Absolutely none. If I didn't have the job, I would be out there living the dream.\" Suppose that the guy cashes in his investments and does retire. What do we find? He is waking up at 9:30 am, surfing the Web, sorting out the cable TV bill, watching DVDs, talking about going to the gym, eating Doritos, and maybe accomplishing one of his stated goals. Retirement forces you to stop thinking that it is your job that holds you back. For most people the depressing truth is that they aren't that organized, disciplined, or motivated. reply Rodeoclash 16 hours agorootparentprev100% - it takes away your hope. In this case, that by \"making it\" in the world of startups will fill the void in your life. reply david-gpu 18 hours agorootparentprevYeah. When you have to work in order to live, it is easy to make the mistake of thinking that you would be happy if only you had money to quit your job and time do the things you want to do. Once you get there, you have to face reality: while being poor leads to unhappiness, being financially independent does not lead to happiness either. Don't believe me? Look at billionaires out there; do all of them look like happy and well-adjusted people to you? Not naming names. And that's why wealthy celebrities repeat again and again that \"Money doesn't buy happiness\". It's because they know from experience that it really doesn't help all that much. reply nox101 17 hours agorootparentI'm not sure I'm convinced. I guess I haven't had real money. I had > 1m at one point. It was enough not to work. It wasn't enough to experiment with random things without risk. Couldn't buy a house in NYC,SF,LA,Seattle. Would just have to go back to work. Couldn't start a business for a project that required 10-20 people. Couldn't really start co-working space for 20-40 people at current rent prices without feeling like I'd probably just be throwing away a few hundred k. What I could do is travel. Could also live anywhere for a few years. OTOH, if I had F.U. money, I would do those things and more. I might hire people to do them. There are 5 to 10 apps I'd like to see exist. Would be happy to pay some people to make them and make them open source, if I had FU money. Would love to start a tech-interactive-art museum the size of at least most major museums in big cities. Would consider funding startups. I have one friend, x-coworker, that picked a different path than me and made lots of $$$ (no idea how much). But, they invest in startups. Goal is to invest $1 million a year. They visit startups and pitch events once or twice a month. They also have a personal project. Otherwise they travel with their S.O. and visit their adult kids around the world. reply distances 6 hours agorootparent> OTOH, if I had F.U. money, I would do those things and more I get what you mean, but having enough not to work is the definition of FU money. It means you can just drop your things and leave when the boss demands you something you're not willing to do. reply OJFord 3 hours agorootparentNobody takes it literally like that though, because at least in a better job market way too many people have that money. If you want to tie it to a single scenario I'd say it's taken on a meaning more like 'I'll do what I want to you/your business/parking because I don't care about paying to sort it out if I'm sued'. Parking where I 'can't' & paying the 100x fine seems better to me than finding where I 'can' and paying the 1x ticket sort of thing. Not to say everyone's morals would have them act like that, but illustratively. reply david-gpu 16 hours agorootparentprev> OTOH, if I had F.U. money, I would do those things and more Sure, there's no lack of things that one can do with money. But would you be substantially happier? That's the issue at hand. Do you look at people with exorbitant wealth and see unlimited happiness? Do they appear to be in a permanent state of contentment and satisfaction? reply vidarh 12 hours agorootparentWhat I've learned, both in terms of personal experience and from reading up on the psychology, is that to maximize happiness over time you need to optimize for ensuring you can maintain a steady upward trajectory. Win the lottery or sell a company? Invest most of it, and allow yourself a \"raise\" you can permanently sustain every year. It will do far more for you than raising your expenditure once, as you get used to it and return to near your baseline happiness very quickly. reply potatoman22 10 hours agorootparentI'd imagine this applies to emotional health as well as financial and physical health. reply Aeolun 16 hours agorootparentprevHah, the only time I had that kind of money was the 10 minutes my bank transferred the loan amount to my bank account so I could transfer it to the housing company. It didn't really feel like mine though, since they were both breathing over my neck to check I actually did so xD reply throwaway2037 11 hours agorootparentprev> Would love to start a tech-interactive-art museum Check out \"TeamLab Planets TOKYO DMM.com\". Ref: https://en.wikipedia.org/wiki/TeamLab_Planets_TOKYO_DMM.com reply mewpmewp2 17 hours agorootparentprevI suppose it depends on a person. I am pretty sure the more money I have started to make throughout my life the happier I have become. Simply because of having more freedom over how I spend my time. I feel like there is almost infinite amount of things to do in this World, I just wish I was able to 24/7 do those things. I don't have enough to not work for rest of my life, but I have been able to buy my own apartment, house, which has given me a lot of confidence in my self and feelings of freedom. I started out with no connections or education though. I am sure it can be different for everyone, people see the World differently. reply david-gpu 16 hours agorootparentAll that sounds sadly familiar. Once you cross the threshold of not having to work for a living, the illusion fades away. You are still you, your problems are still there, you are still bound to a slowly decaying body, there's no GAME OVER banner and credit roll proving that you have won the game of life. Because you haven't. Hah. You can of course keep yourself entertained with all sorts of stupid stuff that doesn't actually matter. Or you can accept that there are still only a handful of things that bring people contentment, and you don't need to be financially independent to do any of them. I'm talking about bland obvious stuff like spending some time with loved ones (including pets and plants), going out for a walk in the park, etc. Unsurprisingly, a ton of retirees do just that. It's not because they are old and can't do anything else, it's because they have finally figured out what works. And it's not like doing five times as much of that stuff is going to make you five times happier, either. Anybody with a full time job can carve out some quality time instead of arguing with strangers online. reply naming_the_user 16 hours agorootparentIt's also the case that having enough money not to work makes it very simple to engage in self destructive behaviour. If you have a 9 to 5 then you're waking up at sunrise and going to bed some time after sunset. You're probably commuting, getting out in the world, chatting to people even if it's just the colleagues, Starbucks drive through, supermarket checkout clerk, that sort of thing. If you have retirement level money and you're alone there is absolutely nothing stopping you from waking up at 2pm, sitting on your computer playing World of Warcraft ordering takeaways, not going out for a walk or seeing the sunlight, getting slightly more depressed each day in a spiral. There's also nothing stopping you from going out drinking every day or every other of the week, shrugging off the hangover then hitting the next one. And so on and so forth. People easily underestimate how much their sense of well-being is related to simple things like just going for a walk in the sunshine every now and then or eating properly. reply mewpmewp2 7 hours agorootparentI have had multiple different life routines. Healthy fit, alcoholic, depression and self pity, video games no life, productivity hacking self help guru following, career climbing, start up attempting. Considering having been through all of these and knowing how each of them feel, I would hope I have enough experience and urge to opt for the healthy fit as baseline. I think it is still much easier to go for it having no work stress in life. reply david-gpu 15 hours agorootparentprevYeah, that all hits close to home. I find that having a daily schedule and trying to stick to it helps. Also, going for small quality of life improvements, trying things out to see what works and what doesn't, such as a different pillow, or replacing some foods with alternatives. Having free time forces you to deal with all the emotional baggage you have accumulated over the years and it's a bit much, too. I was obsessed with saving money so that my family wouldn't struggle if I became unable to work; as a result, I didn't process the reasons why I had that fear in the first place. My dad became severely disabled when I was in college and it was a very traumatic experience for my family. reply saagarjha 8 hours agorootparentprevOuch, that hurt. And I'm not even financially independent! reply saagarjha 8 hours agorootparentprevLike all things, financial independence means it's one of the things you can stop worrying about, and focus on what you think matters. It means you don't have other people telling you what you must do 8 hours of the day. Of course, some very very rich people have decided what they want to do is argue with strangers online. But that's their choice to make ;) reply alexey-salmin 10 hours agorootparentprevI think having kids should help a lot. I'm spending around an hour every evening with mine and a few hours during weekends, but neither me nor they feel this is enough. I would also go skiing for 2 weeks every winter with my family and rent a cottage in the countryside for whole summer holidays, but I have neither money nor time to do it. The irony of the situation is that by the time I earn the money my kids will grow up, leaving me a sad lonely man. reply mewpmewp2 7 hours agorootparentOne reason why I truly hope I have enough financial independence by the time I have kids. But maybe you can become a good grandpa at some point? reply alexey-salmin 5 hours agorootparentI gave it some thought and overall I'm happy my kids have happened relatively early. More than money or anything they need your energy and enthusiasm. As much as I complain about the lack of time, I think it's better than lack of energy and stamina. At least that's what I extrapolate from my experience so far. For example, even between my two children I noticed that with the second one I've had sleeping issues, while with the first one I couldn't understand what \"sleepless nights\" everyone talks about. Like, a newborn kid at night sleeps 50 minutes out of every hour and so did I at the age of 25, being fresh and well rested in the morning. Didn't work as well at the age 31. Can't really imagine what it must feel after 40. And in general I would prefer healthy sleep over money and time. reply david-gpu 23 minutes agorootparent> More than money or anything they need your energy and enthusiasm > I think it's better than lack of energy and stamina. At least that's what I extrapolate from my experience so far I had children late --essentially retired when my second was born at 40-- and I completely agree: it would have been better to have the last kid before we turned 35. You made the right choice! reply mewpmewp2 7 hours agorootparentprevDecaying body might be one of the toughest arguments there, which unless we find a way to stop this, my solution would be to have kids and develop them instead of myself. Since I do like competitive sports and it is kind of hard to accept that at some point I am no longer going to be able to improve. It may be the case that once I reach complete and comfortable financial independence it will not be all I expected it to be, but right now I don't see myself not appreciating the hell out of it, however I am sure plenty of people have thought they would be happy when reaching X goal and they either weren't or it was fleeting. I do have to say that so far I have come a long way from where I used to be from what I consider a hopeless position. But there is a difference in my view whether you can walk in the park knowing that you don't have any potentially stressful responsibilities and problems coming up or you are walking in the park and excited about working on a hobby project of yours since you can 100 percent focus on that. reply layer8 4 hours agorootparent> my solution would be to have kids and develop them instead of myself I never understood that kind of argument. Your kids will have decaying bodies just the same. You're only recreating what you're trying to escape, in somebody else. reply wing-_-nuts 1 hour agorootparentOne (or both?) of you is coming at this from the wrong angle. Faced with limitations, you have to find ways of living with them, working around them, and still living a fulfilling life. That might mean you cannot travel the same way in retirement that you did in your 20's. That doesn't mean you still can't enjoy it, just that you need to take things slower and be gentler with your body. reply xedrac 1 hour agorootparentprevHaving/raising kids has been the most rewarding and challenging experience of my life. I cannot imagine how meaningless life would be to have all the money in the world and no kids. reply david-gpu 20 minutes agorootparentI couldn't have said it better. It is an immense change for the better and for the worse, and it made us feel like that's when we truly became adults. It's one of those experiences that you can't understand until you go through it. And I say this as somebody who really did not want the responsibility of raising children. reply jjeaff 16 hours agorootparentprevI think that is most people's experience but it only works up to a point. you will get diminishing returns the more money you have until, in some cases, maybe negative returns. reply mewpmewp2 7 hours agorootparentProbably diminishing returns, but sometimes I think there is also selection bias, with ultra rich getting there because of never ending satisfaction, so it is contingent on the type of the human. Satisified person would stop sooner while never satisfied would naturally become the richest. reply zusammen 18 hours agorootparentprevMoney doesn’t buy happiness but most people are subjected to artificial misery by this society and money does make that go away… at least, a fair share of it, probably 80 percent. That said, a lot of people who get rich, because status is their real motivation, are shocked by how horrible society still is. At first they get hooked on the drug of high social status, but then they learn to see through the flattery and realize that nothing has truly changed, and they’re just as miserable as before. It tends to take about two years, in my observation, for the “new life energy” to wear off. Money teaches you that there isn’t some “better” society to aspire to. The people “up there” aren’t the supervillains Redditers imagine billionaires to be, but they’re not better either. My daughter is autistic and when she started to learn how to read social cues she realized that her so-called friends didn’t actually like her, which I suspected myself but never had the chutzpah to say, and it made her angry. Getting rich has a similar “learn what people are really about” curse. reply david-gpu 16 hours agorootparent> Money doesn’t buy happiness but most people are subjected to artificial misery by this society and money does make that go away… at least, a fair share of it, probably 80 percent. I've been the young immigrant who arrived to a foreign country with the clothes on his back and whatever fits in a suitcase; occasionally splurging by buying used clothes at a thrift shop and buying a slice of cake at the supermarket once a month. If anything, I was probably happier then: healthy and hopeful for a better future. Now I'm in significantly worse health and rather jaded. Thank you for sharing your own experience. reply jquery 15 hours agorootparentBeing young and healthy goes a long way. I was happier when I was a poor ahh college student but that doesn't mean losing all my money and possessions would be good for my mental health. reply fsckboy 16 hours agorootparentprevsociety subjects people to artificial misery? society is all we have, it's the most authentic misery you'll ever experience. reply littlecosmic 11 hours agorootparentMaybe artificial in the way that we hypothetically could do things better if there wasn’t so much inertia to the status quo. reply lazide 3 hours agorootparentAh, idealism. Truly the root of much of misery of the world. reply jquery 15 hours agorootparentprev>supervillains Redditers imagine billionaires to be Sure, they are just people like you and me. Doesn't mean their mere existence isn't evidence of a major flaw in our implementation of capitalism. Our society is becoming far too stratified. Healthcare should be a right at this point in our society's development, it's a stain on our country that we still carry on with a system that works for nobody except health insurance CEOs. reply WalterBright 12 hours agorootparentNo society has made the poor better off by getting rid of the wealthy people. reply defrost 12 hours agorootparentMany have made the poor worse off by pandering to the rich and entitled. The rich don't have to be eaten, just constrained from exercising their worst excesses. reply atq2119 11 hours agorootparentprevSocieties have made the poor better off by preventing people from becoming too wealthy, though. Specifically, I'm thinking of the very high high-end tax rates that were common 60 years ago. reply WalterBright 21 minutes agorootparent> Societies have made the poor better off by preventing people from becoming too wealthy Example, please. reply victorbjorklund 10 hours agorootparentprevwhich countries did have high standards for everyone but no \"too rich\" (whatever that def means. Is more than 1 million dollars too much?) people? reply atq2119 8 hours agorootparentI don't know about \"too rich\", but if you're looking at the distribution of income and wealth, you'll find an increasing divide between poor and rich over the last decades in pretty much all western countries at least. reply WalterBright 20 minutes agorootparentIf everyone's wealth goes up 10%, the gap between the rich and the poor increases. Besides, someone creating more wealth than you do does not hurt you. yojo 12 hours agorootparentprevWhat research there is suggests money/happiness follows a log-linear relationship.[1] So it kind of does buy happiness, but the rate of increase falls off pretty fast over the range most of us experience. 1: https://www.pnas.org/doi/abs/10.1073/pnas.2208661120 reply madrox 17 hours agorootparentprevIt's like that old saying, \"where ever you go...there you are.\" reply david-gpu 16 hours agorootparentYup. Jon Kabat-Zinn wrote a great book with the same title. reply giantrobot 2 hours agorootparentThe much better philosopher Buckaroo Bonzai said it first. reply scotty79 18 hours agorootparentprev> Money doesn't buy happiness Yeah, but it let's you suffer in relative comfort which is the most that anyone can realisically strive for. reply david-gpu 18 hours agorootparent> suffer in relaive comfort which is the most that anyone can realisically strive for If you have never met a person who is content with their life you may benefit from expanding your social circle. There are sincerely happy people out there. reply scotty79 16 hours agorootparentNo, thanks. There's really nothing to learn from people with this particular rare flavor of brain chemistry. reply jamieplex 1 hour agorootparentWell, that's definitely an interesting take... reply tough 5 hours agorootparentprevlmfao reply kamaal 3 hours agorootparentprevUnhappy for different reasons is not the same. Being unhappy because you are homeless is not the same being unhappy because some woman doesn't treat you like she would do a man who looks better than you are just two different things. reply mcmcmc 17 hours agorootparentprevWhat a load. Only someone who has always had money would say this reply nextlevelwizard 11 hours agorootparentThat is the entire point. Money removes unhappiness and raises you to a baseline, but after that it doesn't provide extra happiness in and of it self. reply saagarjha 8 hours agorootparentIt can, if you use it wisely. It's just not that you eat the money yourself, you apply it to things you want to do. That could be buying guitars, or it could be setting up solar panels in Ghana. There's a lot of things you can do with money. reply Rodeoclash 16 hours agorootparentprevCorrect, because only by having money can most people understand the situation. You've proven this yourself by calling it \"a load\". Zen monks have attained this understanding without the need to make the money first however. reply mcmcmc 16 hours agorootparentNo, you cannot make the comparison between having money and not having money if you have never not had money. If you have always had money, you have never known the difficulty of living without it. reply intended 13 hours agorootparentI never read any of the comments here as belittling what it is to be poor. It reads to me as being critical of the assumption that being rich makes you happy. This is reflected by many people who became rich and self destructive. reply xdfgh1112 10 hours agoparentprevI did the same as OP. Quit my job then started distancing from everyone and removing responsibility in the pursuit of freedom to do what I want. But all I did was wallow and stay alone. Not sure what the answer is but your insights were very powerful to read. reply benjam47 8 hours agorootparentI find this to be very familiar. I worked endlessly to be able to have no responsibility and endless freedom. My partner passed away several years ago now, and I still haven't filled that void. I'm not unhappy by any means, but money and freedom are a poor substitute for companionship. reply cwalv 9 hours agorootparentprevThe book of Ecclesiastes is about this. It makes more sense if you s/meaningless/vapor/g, i.e. we're all chasing after something like smoke that we can see but can't 'catch'. reply ryandvm 1 hour agorootparentFascinating that your version said \"vapor\". That's a much better translation. The versions I've read called it \"vanity\" which was even more obtuse. Once I figured out that they meant something insubstantial and fleeting, I found that particular book beautiful. reply Tistron 8 hours agorootparentprevI love that book. Haven't read much of the bible, but totally recommend anyone to read this Buddhist sutra that was smuggled in somehow :) reply purplethinking 5 hours agorootparentprevBut why did you need to distance yourself from others? Or was that just a consequence from your other lifestyle choices? reply wtarreau 3 hours agorootparentI suspect it might feel indecent to tell others you suffer when you're both free and rich, and it's difficult for them to figure what's wrong with you. Instead, people in such position should probably go out and join associations which distribute food to those who need it. At least they'll see that they're doing something good to improve others' condition and would probably feel better. reply potatoman22 10 hours agorootparentprevI've found myself in a similar position. I'm trying to figure out how to not be self-destructive, but I feel the urge to distance myself from people. reply random42 9 hours agorootparentprevI am currently at the same place, with no reasonable place out. How did you solve it? reply workflowsauce 17 hours agoparentprev> Something I did not read in this essay is how he grew closer to anyone (in fact, I read the opposite). No path out of this valley involves traveling alone. I think he needs to get closer to himself. I think he's on the right track. reply madrox 17 hours agorootparentI've found that you don't see yourself without people around you to hold up a mirror reply o1o1o1 3 hours agorootparentExactly this. Also, it is difficult to find those people when you're already rich and unemployed because most of us form these kind of meaningful relationships in school, at work etc. reply freediver 2 hours agoparentprev> I don't think you have to have Fuck You Money to get to this point. To add to this, the 'modern' use of the word 'millionare' started in 1850 (discounting first use in 1719 in France which was not in the context of 'rich' we know). When you adjust for inflation, a comperable purchase power today would be an equivalent of having net worth of $250M. Anything below that and you aren't even a 'true' millionare. ($1 USD in 1850 is roughly ~$250 USD in 2024, taking 3.2% average historic inflation rate). So, author, you are not even rich, still work to do ;) reply zusammen 18 hours agoparentprevI don't think you have to have Fuck You Money to get to this point. Most people eventually become disillusioned with work enough that they reevaluate what matters to them. Getting a very profitable exit is just one way to trigger that experience. I’ve seen a lot of people have random outlier success they didn’t earn and it seems to have the same effect as what most people get out of their careers: crushing failure they didn’t earn. By 50 or so, everyone figures out: * it was almost all random. * the things that seemed so important were not. * working for money is a waste of time for almost everyone. * you can count your real friends on two hands, whether you’re broke or a billionaire. It’s surprising how the paths converge. There are differences, and the rich version of alienation is better than the poor one, but the mindset this society leaves people with is remarkably stable. No one feels like they won, which is why Musk and Trump are so full of rage at everyone. Either the gods shut you out or you are forced to find out that the gods never existed. reply dasil003 30 minutes agorootparentThere's a lot of truth in what you are saying, but I also think this framing can lead to unnecessary nihilism and depression. I think the simple reason that no one feels like they've won is because we're not biologically wired for that. Like all living things, we've evolved to struggle for survival in a harsh environment. Of course modern civilization has separated us from that harsh reality by layers upon layers of human systems and supply chains, so we apply the same instincts to games of our own devising. There's nothing actually wrong with this though. The problem comes from the belief that \"winning\" will make one happy. The reality is ones drive leads to engagement and perhaps accomplishment, but it can't answer the why. That is something every person with leisure time needs to work out for themselves. reply KolmogorovComp 4 hours agorootparentprevThank you for this post, remarkably articulated, I concur. reply eastbound 58 minutes agorootparentprev> working for money is a waste of time for almost everyone No. Chance doesn’t fall evenly. It falls more often on those who work. Thinking that Trump is full of rage is missing the forest for the tree. It’s a forest of journalists who are full of rage that this race and gender unapologetically exists, who try to depaint Trump as full of rage. It’s a forest of selfish crowd who wants the fruit of the labor of the second half of people, who complain that Trump is selfish. Looked at the tree. Missed the entire forest. Not surprising that you think people are struck by a lightening to become billionaire. Chop chop chop, back to work, quit being jealous. reply morpheos137 18 hours agoparentprevAs someone who has not been successful in life but who is relatively intelligent do you have any recommendations as to how I can get my life on track? I am asking because you said you like developing people. My persistent experience in life for almost 20 years post college has been nobody wants to develop me. I am not in tech but I am generally interested in the area if it can lead me to greater independence and more interesting work. I like jobs that are intellectually engaging and ideally somewhat physically active. Right now I am working in a mechanical role. Sometimes I like the work but more often than not I find that good problem solving ability is not valued and the pay is dismal vs. what people earn in tech. I have a BA in economics but unfortunately have never used it. 37 years old. reply ghostpepper 3 hours agorootparentChoose companies to work for based on who you will be working with and what they can teach you, not by how much they pay. You never want to be the smartest person in the room. reply geraldwhen 17 hours agorootparentprevIf you figure it out, pass it on to others. I haven’t been allowed to hire Americans in quite some time, and Covid destroyed any company support of an apprenticeship type setup. The last time I was able to hire an American with a will to learn and an adjacent degree was over a decade ago. reply remh 12 hours agorootparentI’ve hired dozens of smart Americans with the right degree and willingness to learn and I was not even American myself. I worked with hundreds more. Not sure what you’re talking about. reply wsintra2022 18 hours agorootparentprevLearn a musical instrument. Stick at it and over time you will find you can create music , which will make you feel successful. reply vixen99 8 hours agorootparentExcellent advice. For those for whom it's appropriate, do try it. As you suggest, therein lies that special essentially private pleasure of accomplishment in small things that doesn't depend on the approbation of others. reply cheema33 15 hours agorootparentprev> life for almost 20 years post college has been nobody wants to develop me. Is that really possible? I have often thought that the only person that can develop you, is you. Sure you might get some good advice from some people, maybe a helping hand, a business loan or grant etc. but I don't view that as development. Your biggest asset is you. Don't be reluctant to use it. reply advael 10 hours agorootparentOkay but for the vast overwhelming majority of people, what they actually need is \"a helping hand, a business loan or grant etc\" You talk about this like this is trivial, but it's the kind of material help that would make a difference for almost everyone who is currently not doing what they want in life Yes, no one can teach you to self-actualize. People's material circumstances are rarely a result of inadequate self-actualization or agency, despite what the self-help industry would like you to believe. Most \"high-agency\" people who succeed started out with either adequate resources to at least support themselves while they tried stuff, reliable backup plans (like living with supportive family), or help in the form of stuff like grants or startup funding. People who don't have that need that, regardless of their mindset or abilities. There are exceptions who got incredibly lucky, and they are a rounding error among rounding errors. That is the world we live in. There are ways to engineer a world where this is less the case, but at least in the US, we seem to choose not to move in this direction at every opportunity, and freak out when even minor forms of the security necessary to act with agency take hold for large numbers of people (See: The business world's hysterical reaction to COVID relief) reply dclowd9901 14 hours agorootparentprevDouble down on that last bit: everything you learn and do adds to the equity that is you. That equity pays back in multitudes throughout your life, not just professionally but also socially and spiritually. reply madrox 17 hours agorootparentprevI can't speak to your specific circumstances, but perhaps this will help. I find that people I talk to with chronic job dissatisfaction have a difficult time taking risks, because despite not liking their current circumstances, the unknown can be scary. There are known pathways to work in tech or other fields, such as coding camps or community college. It becomes a question of what you're willing to sacrifice to make that happen. Would you move to a new city? Go back to school? Give up your evenings and weekends? Usually, some kind of risk needs to be taken, and there's always a path forward if you look. I didn't graduate college until I was 29, and now that I'm in my mid-40s I can say that while every risk I took didn't pay off, it was in the taking of risks that has left me feeling satisfied with where I am. reply tough 5 hours agorootparentDog on rusty nail parabole comes to mind https://www.hashtagyourlife.com/stories/dog-rusty-nail reply AdieuToLogic 17 hours agorootparentprev> As someone who has not been successful in life but who is relatively intelligent do you have any recommendations as to how I can get my life on track? I'm not the OP, but instead just a person who thinks they might be of help. Caveat emptor and all that :-). Success is what we define both in and of ourselves. Some use material measurements (money, titles, assets, etc.), which are intrinsically relative and thus ephemeral. Another definition is establishing a sustained environment of happiness. This includes addressing immediate physical needs, such as a place to live, sustenance, and the like. More than that is finding happiness in how we live each day. > My persistent experience in life for almost 20 years post college has been nobody wants to develop me. While some may give tips and/or pointers as to how to develop oneself, IMHO, much like happiness, development comes from within. Seeking wise counsel is always a good call, but no one can develop another. All anyone else can do is give perspective from their own journey as it relates to you - mine is you have identified options above which are appealing, so pursue them as if no one else is going to anything to make it happen. > I have a BA in economics but unfortunately have never used it. You still have it and one never knows when the education we have helps out until it does. ;) reply tonyedgecombe 7 hours agorootparentprev>Sometimes I like the work but more often than not I find that good problem solving ability is not valued and the pay is dismal vs. what people earn in tech. I'm not sure the high wages in tech are going to last, universities having been minting new CS graduates like there is no tomorrow. Alongside that demand appears to be flagging. I'm sure you remember enough from your BA to know what the result of that is. reply philosopher1234 18 hours agorootparentprevIt’s unlikely you’ll find someone who takes an interest in developing you, who isn’t a personal connection or someone you pay, in my experience. You will probably have to take the first step yourself, either to develop yourself or to find and develop a nurturing relationship. reply shortrounddev2 3 hours agoparentprevHow do you get to a point of developing people? What is that Job title? At work I'm the goto guy for junior engineers to ask questions to and I've been told I'm a naturally good teacher reply ghostpepper 3 hours agorootparentIt's called management and the entry point is usually called tech lead / team lead or just manager. Mentoring juniors is a good way to start. reply simonswords82 2 hours agorootparentprevContact your local university, most have mentorship programmes you can apply to be a part of for under and post grads. reply justinclift 16 hours agoparentprev> Something I did not read in this essay is how he grew closer to anyone (in fact, I read the opposite). Yeah. The entire blog post (to me) gives the strong impression that the author is an extremely self centred, selfish... er... prick. Maybe they'll learn to be less that way over time, and hopefully their ex-girlfriend learns to avoid ungrateful people. reply scotty79 18 hours agoparentprev> No path out of this valley involves traveling alone. Unless you have a schizoid personality. reply j4coh 19 hours agoprevSame situation, I truly empathise because it really does seem to take a lot of purpose out of everything. What I’ve found is that you need to replace money/salary/financial success optimisation (assuming you spent a lot of your life and energy to this point focused on these, much like I did) with something else totally unconnected with being measured in that way. For me, I am focused on proving myself as a guitarist in the local jazz and blues scene. These people have no idea how much money I have and wouldn’t give a shit if they did (I didn’t really change my lifestyle after getting lucky so it’s not obvious). So it’s an area I can be creative, grow, and still feel like I’m doing something. At the same time I’m doing part time consulting, mainly for people I worked with in the past who have started companies, just to scratch the tech itch. So far so good but I can’t say yet if it will stick. Maybe for you it’s art, music, going and getting another unrelated degree, or something along those lines? If you have more money than you know what to do with, fundraising and supporting good causes can be really rewarding. Both in terms of giving back something to your local community, and having really nice social elements to it. One big piece of advice I have is to try to avoid letting others in your social network know exactly how successful you’ve been. Everyone starts wanting to pitch you their investment idea and it can burn down friendships when their ideas are bad. Being a VC to your friends is a path to sadness for everyone. reply elevatedastalt 17 hours agoparent> One big piece of advice I have is to try to avoid letting others in your social network know exactly how successful you’ve been. The time to do that was _before_ writing a blog post titled \"I am rich\" and submitting it to HN reply satvikpendem 14 hours agorootparentThey already know who he is, he was a public figure executive that sold his company, everyone in his social circle would know what Loom was and would read in the news how much it sold for. reply Aeolun 16 hours agorootparentprevWell, you can make new friends. I have no idea what actual name is behind vinay.sh :? reply YouWhy 15 hours agorootparentThe domain name is consistent with the name of one of the founders of the mentioned startup, Loom. reply semitones 14 hours agorootparentIf you're running a popular company then people will easily have a good understanding of the ballpark of your net worth anyway reply WalterBright 12 hours agorootparentI'm not so sure. You can run it with 90% equity, or 10% equity. reply herval 2 hours agorootparentin this case (given it was reportedly sold for $975m cash), that would mean your friends think you either have $877m or $97m (he reportedly net $60m, which means your most pessimistic friend thinks you have more money than you actually have?) reply divbzero 13 hours agorootparentprevYes, Loom is also mentioned by the author of the blog post. reply distances 6 hours agorootparentHow many new friends would look up what Loom was, though? This blog post was the first time I heard of it, so seems very unlikely that a random person would really care. reply kridsdale1 55 minutes agorootparentI learned about Loom from a pirate on Monkey Island who asked me to ask him about it. reply freetonik 11 hours agorootparentprevThe about page links to his Twitter which shows the full name. Funnily enough, the link is to a tweet describing how he wired all his money to his parents. reply coldtea 19 hours agoparentprev>Same situation, I truly empathise because it really does seem to take a lot of purpose out of everything. Mainly though if all the purpose-giving focus was on just getting money and the related grinding to begin with. Getting mega-rich didn't take the purpose out of Steve Jobs, for example, which was focused on building stuff with some specific twist (his idea of good design). Or Steve Wozniak for that matter, he found hobbies aplenty. Or take the Rolling Stones. Filthy rich, but did they ever give the impression they got bored? Or Dylan, equally rich, which doesn't even have the extravagant lifestyle of models and exotic vacations and high life the Stones had, but is still content to record, jam, play concerts etc. into his 80s. If the person has other interests, from programming to mountaineering, and from politics to art, they can still be there with or without money. Like the \"guitarist in the local jazz and blues scene\" thing. reply benatkin 13 hours agorootparent> but did they ever give the impression they got bored It often gets described as washed up rather than bored, but yes. Dylan is a good contrast. reply paulpauper 18 hours agorootparentprevPaul Allen comes to mind. makes a hobby buying the most expensive artifacts known, as well as a bunch of other stuff like starting a band. reply skeeter2020 13 hours agorootparentBalmer (and Cuban) seem to love continuing to do other, big stuff. reply 101008 19 hours agoparentprevThis is my dream. Having enough money to be able to dedicate to things I like, trying to be good at something without worrying about money, or time, or being tired after work. Open a bookshop, being a rare book dealer, open a small museum about an author, research on a particular topic and write books... That would be the ultimate dream, though I am sure I won't ever be near to fulfill it. reply econ 13 hours agorootparentGod, we use to have a street full of people running unprofitable stores. Some were deep in debt making the dream a reality. Why not a functional bookshop? You don't need money, you need to work on the plan(s). How do museums work? Where is the crude draft for the book? I had a chat with a guy once who had a laundry list of things he wanted to accomplish but had convinced himself non of it was possible without money. About 1/3 of the list were things one could just go do right now.I think a hundred life times worth of stuff It was mostly helping people in need. One could definitely not help anyone and convince the self it is because it always costs money???? Some non profit here was selling unwanted books for 1-2 euro. I spend an hour or so typing titles on my phone mostly stuff published long ago and bought a whole stack of 200+ euro books. I haven't looked at them and didn't try to sell any but I'm sure it was money well spend for an hour of fun. You don't need a machine gun, fight with your bare hands. reply 101008 13 hours agorootparentI used to have an online bookshop, mainly for fun and as a side income. It takes a lot of time and it's very hard if you have a full time job. reply econ 8 hours agorootparentIt might be hard to somewhat gradually switch to part time work but if you want a bookstore it's more rewarding if you have to struggle to get there. reply seba_dos1 3 hours agorootparentStruggle is not the deterrent for most people; risk is. reply steveBK123 16 hours agorootparentprevThat sounds great. Many people overly focus on what they want to retire FROM - work, but not what they want to retire TO - hobbies/volunteer work/etc. Basic eating healthier, exercising more and consumption-based things like travel are not going to fill the gap left by a full-time job. One can quickly get bored and/or run out of money with that kind of mindset. Given enough money, or whenever I do retire .. I'd spend my time making music, photographs, do even more reading, etc. Anything that occupies your time and exercises both your body & mind are important. reply wing-_-nuts 2 hours agorootparentprevI once saw an article about apartments that NYC libraries used to have in the library for caretakers. My skipped a beat and I realized I'd never wanted anything more in the world than to just be able to 'pop down to the stacks' at 10pm to select my next read. What amazes me is that between audible, kindle, libby, and a few other places, we live in a world where books are that available from the comfort of a cozy recliner. Truly the greatest wonder of the modern age. reply skeeter2020 13 hours agorootparentprevWhy would you think any of those things are not a lot of tiring work, emotional drain and expensive? I don't understand why you can't do any of these as a hobby now, and need to wait until you're \"rich\" and won't have any real skin in the game. reply david38 18 hours agorootparentprevProblem then when you get bored, your bookstore still requires work. At a certain level of wealth, any job you can do can be done by someone else better and cheaper reply codetrotter 17 hours agorootparentAfter you have the bookstore up and running, you could hire a few people to take care of daily operations. It would still be your bookshop, and you could drop by every now and then and just hang out in the store read some of the books, or you could even whenever you felt like getting more involved on some days tell your staff that they can take the day off if they like and they will still get their pay for the day and you’ll handle the customers and the register. I dunno, this is just how I imagine it could work. I’m not rich, and I don’t own any bookstore or any other kind of shop for that matter so maybe my idea here is off the mark. reply 101008 17 hours agorootparentI had a similar idea in mind. Or worst case, if I get bored, I can sell it and do something else! I'd have a lot of money and time. reply lylo 8 hours agorootparentprevRunning a bookshop is entirely possible without FU money, but it will be hard work and probably not make you much money. Read Welcome to the Hyunam-dong Bookshop for inspiration :) https://app.thestorygraph.com/books/c9fb361a-30ef-45d5-b777-... reply asdfman123 18 hours agoparentprevPeople need work to be happy. That doesn't have to be, say, office work necessarily: it can be making music full time, or volunteering at a hospital, or any number of other things. But you have to have something keeping you busy that makes you feel like you have a purpose. reply voisin 16 hours agorootparentYou’re on the right track, but I think it’s a bit deeper than just needing something to work on or stay busy with. I effectively retired a few years ago and have spent the time since engaging in various “work” across the kinds of categories you mention. Yet, all of these efforts have carried a sense of purposelessness—a lingering question of whether any of it truly matters, especially knowing I could stop tomorrow without significantly impacting my wellbeing. This contrasts sharply with the purpose I felt when I had less money and was struggling to build my business. Back then, everything felt deeply do-or-die meaningful. Now, no amount of exercise, goodwill, or intellectual pursuits compares in terms of providing that same sense of purpose. I don’t think humans need the pursuit of money itself to be happy, but once the foundational needs in Maslow’s hierarchy are met, the higher levels often feel less urgent—and, paradoxically, less fulfilling. There seems to be diminishing returns from “work” as a source of purpose. reply porknubbins 15 hours agorootparentAs someone who kind of quasi temporarily retired early a few times this is the biggest problem I see. I learned foreign languages and programming languages out of pure necessity to survive and it was thrilling to succeed and make money with them. So everyone (at least me) has this fantasy of how much better it would be to learn things on their own schedule for pleasure without undue pressure, but they don’t realize the pressure to survive was what made it feel so meaningful without that they soon fall into dilletantism. For all his issues I think this is why Musk has gotten so much done, because he ups the ante enough to feel real risk if be fails. reply Animats 14 hours agorootparent> So everyone (at least me) has this fantasy of how much better it would be to learn things on their own schedule for pleasure without undue pressure, but they don’t realize the pressure to survive was what made it feel so meaningful without that they soon fall into dilletantism. The trick is to find a place between dilletantism and burnout. reply rf15 14 hours agorootparentprevOur species, like all living things, has optimised towards struggling through life to the best of their ability (which was always limited). To \"win\" within the already apex species of humanity means you are hitting your head on the ceiling of what your body and brain was made for, hard. This is why I stopped striving for success on that scale and returned to only work a small software job. reply kalkin 13 hours agorootparentprev> a lingering question of whether any of it truly matters, especially knowing I could stop tomorrow without significantly impacting my wellbeing. A suggestion for your consideration, or that of anyone in a similar position: give enough of the money away that this stops being true, and find fulfilling paid work (not necessarily in that order). I strongly suspect, from my own experience, that there's an amount of savings that you can keep that is adequate to remove any worry about winding up on the streets (or being stuck with work that actually turns out to suck, etc), without making further earning feel pointless to your own comfort. I think there's an ethical case for doing this even if it made you less happy, but even better if it's win-win. reply j4coh 17 hours agorootparentprevTotally agree. But it can be surprisingly hard to find what this is for yourself when you are used to climbing the school / corporate / startup ladder your whole life. reply survirtual 10 hours agoparentprevAnyone who has \"more money than they know what to do with\" is a fool. There is an unlimited set of things to do with large resource allocation. Depending on the magnitude of that resource allocation, the set increases exponentially. It shows a total lack of introspection as well as connections with the people, the Earth, and the universe as a whole. Go eat some psychedelics and travel inside yourself for a while. Listen to what a tree far in a forest has to say. You'll know what to do with your dragon hoard in no time, I guarantee it. reply cwalv 9 hours agorootparentIt's true that there are infinite ways to waste a fortune, but that doesn't mean you're a fool for not having decided how to spend your money. I'd actually argue that it'd be more foolish to go figure out what to spend it on \"in no time\" reply mrandish 15 hours agoparentprev> One big piece of advice I have is to try to avoid letting others in your social network know exactly how successful you’ve been. Having lived through this arc myself, this is excellent advice. While the most enlightened/mature people have no problem just being happy for you, this still leaves a lot people for who a significant disparity in wealth/success becomes a problem. It ends up impacting the nature of your relationship with them in subtle but significant ways and it can be very hard to get past. I've found it's just better to avoid the issue by being as stealth as possible about wealth (while still being honest and true to yourself). reply codetrotter 17 hours agoparentprev> I’m doing part time consulting, mainly for people I worked with in the past who have started companies, just to scratch the tech itch How do you pick you hourly rate? If a friend of yours of the past came to you and asked you to consult for him, and your friend offered you say $80 USD per hour, would you find it offensively low? For someone who doesn’t have a lot and wants to hire consultants for their small projects, I think offering $80 USD per hour is not bad. But I’m curious to know how that amount feels to a potential consultant if the consultant already had a lot. Or do you prefer taking a percentage of shares in your friend’s company as pay? Or something else? reply j4coh 17 hours agorootparentI am happy to take whatever they offer (including helping for free) depending on where they are at. I don’t need it really and I’m happy to help. But most people at least that I’ve worked with are happy to do what’s fair. I haven’t ended up in a hostile negotiation or anything close to it. reply ska 16 hours agorootparentprevFWIW I’ve done similar for friends who are on a tight budget , consulting for 1/2, 1/3 1/5 “regular” rate. Sometimes with equity but not always. I’m nowhere near rich, but when I was consulting full time of I had enough hours to hit my “ok” target for the year, it felt right to be flexible with some of the rest of my time … reply rramadass 12 hours agorootparentprev> How do you pick you hourly rate? A fair formula that i was given years ago is; Take the annual salary you would be paid if you were an employee, add 30% to it for overhead/profit and divide by 48 (working weeks in a year) to get your weekly rate. Divide by 40hrs to get the hourly rate. Another one is to take your annual salary, divide by 250 (working days in a year) to get your daily rate and increase that by 30%, billing in daily units. The above formula can and should be tweaked based on the project, client, your needs etc. reply ska 2 hours agorootparentThe tricky thing about formulas like this is that it is very domain dependent. What you describe is a reasonable approach for a freelancer who expects to bill most working hours. It falls apart for a lot of consulting scenarios where you bill fewer hours and spend more time generating work. In that case you may be better off setting rates so that e.g. 1000 billed hours will reach your base target salary equivalent... reply justinclift 16 hours agoparentprev> you need to replace money/salary/financial success optimisation Kind of ironic, but that kind of sounds like the people who've been saying those things aren't the most important in life might have been right all along? reply fakedang 13 hours agorootparentTo access the others, you need to have good money/salary/financial success, oftentimes. Heck even for good therapy, you need to have those. reply user3939382 18 hours agoparentprev> For me, I am focused on proving myself as a guitarist in the local jazz and blues scene. So you’re Dickey from The Talented Mr. Ripely? reply ilrwbwrkhv 18 hours agoparentprevThat last point is salient. I grew very rich in the last 3 - 4 years and I funded a bunch of my friend's startup ideas. Now I cannot bear myself to reply to their happy new year wishes because how the relationships have soured. reply whatshisface 18 hours agorootparentIf you're never getting that money back, you might as well forgive them and forget it. Then at least you'd keep the friends. reply j4coh 17 hours agorootparentI think this is the best advice. If you are going to fund a friend, give them a grant, no strings attached. They can return the favour if/when they have the means on their terms. Anything else is going to kill your friendship. reply Nevermark 5 hours agorootparentI did that. It seemed obvious that it was a small gamble for me, a big gamble for my friend, but he was doing all the right things. VCs don’t waste time with concerns over failed investments. When you hand over some money, you are accepting the risk. Including complications, which are likely. Treat the money like a gamble, not the friendship. I was really glad I did. My friend created a successful business after working toward that with major ups & downs for over ten years. Then he got cancer, and died a year later. I got no money back. But it was the best investment I ever made. His dream came true and that mattered so much to him. That he pulled it off, and his customers loved him and his business. Don’t invest in a friend if the investment isn’t about genuinely helping them. If you can afford to. reply ToDougie 58 minutes agorootparentI have complicated feelings about this...... mostly reminding me to love. Thank you very much for sharing. reply Fomite 6 hours agorootparentprevThis. If you're funding your friends, think of yourself as a wealthy patron, not a lender or VC. reply ilrwbwrkhv 16 hours agorootparentprevIt's never even about forgiving. I am not even angry at them. What happens is there is now suddenly a pedestal. No matter what we do, they know I gave them money. And I know they took it. The relationships don't remain the same anymore. It's weird. reply charlie0 16 hours agorootparentI can see that. I would personally feel really bad if I lost all the money someone gave me. I'd always feel like I owe it back. Only exception would be at the very beginning, if that was a possibility that was acknowledged. However, I've noticed people who start these things are always very optimistic and probably don't seriously talk about this. To the previous poster, the only people who seem to handle this type of thing well are the extremely blunt people who are brutally honest and upfront about everything. There aren't many of them. reply HeartStrings 10 hours agorootparentprevSo did all of their startups fail? reply Aeolun 16 hours agorootparentprevYeah, I wouldn't take money from people I wanted to stay friends with for this reason. It's just a bad idea. Introduce me to your acquaintances, sure, but lets stay friends. reply nodesocket 18 hours agorootparentprevI had a friend who I lent money to his startup during Covid. He promised would be paid back within a year. Multiple hard conversations and it’s 4 years later and not one cent has been paid back to me. We currently don’t speak to each other. He’s delusional with his startup ideas, lives in lala fantasy land. Refuses to get a job and take any responsibility. He has zero track record of success, so it’s somewhat my fault for loaning him money. reply jhghikvhu 17 hours agorootparentIt's your fault for lending money rather than asking for a % of profits (if any) like a normal early investor. reply nodesocket 17 hours agorootparentI tend to think having fiscal responsibility, morals and not being a shyster as the person who’s at fault. reply llamaimperative 17 hours agorootparentNever loan money to friends is like “having money 101.” You can gift it or you can invest it with appropriate caveats given to any external investor, but never lend it. This is common advice because it turns good relationships bad, that manifests as viewing former friends as shysters reply justinclift 16 hours agorootparentNot everyone is like that, though some certainly are. I've personally lent money to friends when they needed it, and been paid back once they got stuff sorted out. Though in my younger days I was far less careful, so lending money did indeed go poorly. reply bawolff 13 hours agorootparentI think the main issue, is that most people are reluctant to ask a friend for a loan because its considered in poor taste. If they are asking they either have no qalms about asking which is a red flag, or they are super desperate which probably means its unlikely they would have the ability to pay back. reply llamaimperative 14 hours agorootparentprevOf course not everyone is like that. The issue is you don’t know who is ex ante, and often people aren’t that way ex ante, but one thing leads to another to a total breakdown in trust and respect. Even in the case you describe, it’s much better to gift it: “if you want or are able to pay me back some day, go for it, but I don’t expect or need it.” reply Cerium 16 hours agorootparentprevNever lend what you would not happily consider a gift. reply justinclift 16 hours agorootparentprevAre any of your friends the type that is blunt/honest regardless of how things are going? Personally I've found the problematic ones to be those who feel like they're obliged to act deferrentially once money is involved. The blunt/honest ones that don't change their personality like that still seem ok. reply i_am_a_peasant 17 hours agoparentprevOne thing you could do is give me 20k no strings attached so I can stop paying for my parents screw ups from when I was 20 :)) that will make me a lot less resentful towards life's stupid dice. reply fossuser 16 hours agoparentprevThere's something off about the post that I'm not sure I can pin point, but it's there. What are these oft referenced insecurities? It's hard to get a read on this without details, but dumping your girlfriend to do random selfish shit (climb mountain, go to Hawaii, etc.) - it's not a surprise he's unfulfilled (though working on doge would be exciting). This trap of 'working on yourself' that leads to endless mindfulness and narcissism leads you to become aloof. People tend to derive purpose from community, friends, and family. This is what religion used to give people independent of the pseudoscience. Being financially independent is great, but it doesn't bring fulfillment. A long way to say spend time with friends, work on a relationship, get married, have kids. People can do what they want, but most people will likely be the most content doing this. If you can find something to work on you're also excited about great, can do that too. You can only dick around traveling and 'finding yourself' for so long, it gets old and repetitive. reply Tepix 13 hours agorootparentHow long have you travelled the world? reply paganel 9 hours agorootparentNot the OP, but after a certain age (mid 30s in my case) traveling just becomes cumbersome, i.e. when you realize that there are no big insights about oneself that can be gained via traveling that can’t also be gotten back at home, surrounded by friends/family and a couple of good books. reply Tainnor 7 hours agorootparentThis sounds an awful lot like you're generalising from your experiences to other people's. I'm also in my mid 30s and I still find travelling eye opening in a way that books are not (and I do read a lot, including when I travel). And on my last trip I met a retired couple who spent three weeks traveling in their car and they told me they used to have a boat with which they'd sail around the world. reply Tepix 5 hours agorootparentprevWell, i beg to differ. And i'm older than that. In my view, we have a very limited time to live, and experiencing the amazing planet we're on in all its variety is one of the best things one can do. reply paganel 4 hours agorootparentYou're just scratching the surface of said amazing planet, you're not experiencing anything of value that you couldn't have experienced back home. There's a real good essay on the emptiness of tourism written by Siegfried Kracauer back in the 1930s, just as mass tourism was beginning to take off, Travel and Dance [1] is called, it is still highly relevant almost 100 years later. [1] English version from a spammy website here: https://www.academia.edu/25240089/Siegfried_Kracauer_Travel_... , Spanish version from a real website here: https://antroposmoderno.com/antro-version-imprimir.php?id_ar... reply prmoustache 7 hours agorootparentprevThere are people traveling the world all their life because that is what they love to do. reply abc123abc123 4 hours agorootparentprevAmen! Travelling is just virtue signalling and social posturing. I've travelled the world, way too much for both business and \"pleasure\" and there's nothing new, no hidden insights, that I couldn't have gotten at home. I advice all people I meet to stop travelling, and to spend more time with themselves and explore their inside, instead of being captivated by the outside, like a child by a new shiny toy. The world would truly be a better place if that ever happened. reply throwaway6734 17 hours agoparentprevDo you have children? If not, it's a great use of time, especially without financial pressure reply tanvach 17 hours agorootparentChildren are giver of immense sense of satisfaction that’s totally disconnected to wealth (though being wealthy certainly helps). Just remember - no short cuts. reply ToDougie 55 minutes agorootparentChildren are the best. The highs in life are orders of magnitude higher, and the lows are _so_ much lower. But the baseline is incalculably higher. My children have made me feel so much more fulfilled. And they have also made me better. reply Aeolun 16 hours agorootparentprevI'm fairly certain children are a much greater giver of satisfaction without wealth, because when you have money you suddenly feel like you need to provide them all the best, whereas if you have none, you only feel like you need to keep them alive. reply skeeter2020 13 hours agorootparentLike most things in life the need grows to just beyond the level you can supply. Maybe someone with nothing starts with \"keep them alive\" but once that's covered you move up the hierarchy. reply prmoustache 7 hours agorootparentprev> though being wealthy certainly helps I think it is worse actually. reply j4coh 17 hours agorootparentprevI do, and I love being available for them. I have time to teach my kids music and things I know, which is awesome. reply jerojero 17 hours agoparentprevSeems weird that you have to advice people that they should hide their success from people they're befriending. Living a double life like that doesn't seem right to me. It has something to do, perhaps, with the type of people you're surrounding yourself with. If someone can't be friends with you without asking you for money why are you keeping such people around in the first place. reply HKH2 17 hours agorootparentUsing discretion or being modest isn't necessarily living a double life. I'm sure there are plenty of things you're not open about; that doesn't necessarily make you fake, does it? reply jerojero 16 hours agorootparentI thought we were talking about friends? Maybe I'm misunderstanding the meaning of the word. I wouldn't imagine being friends with someone and not trusting them with knowing how successful I am. I think maybe, and this might be a cultural thing, a lot of people tend to use the word lightly. I simply wouldn't get to a stage of being friends with someone I am not able to trust in such a way that I have to hide how much money I have from them. reply naming_the_user 15 hours agorootparentI think the issue here is that with retirement levels of money existing friendships can become strained. If your high school friend earns 50k a year and you earn 100 then I mean sure, you have different toys, bigger house, whatever, but you're both existing in the same universe with similar constraints. If your high school friend earns 50k a year and you suddenly have 60 million like the guy in the post then it's more of a test because your lifestyle can just differ hugely. Some people can handle it but with others there will be an underlying resentment. There are lots of layers to it. They look for work - you look for suitable employees. Having said that, in my experience there are only really a few major cut-offs, one between homeless/terminally skint and working, one between working and being able to live off of investment income, _maybe_ one at the sort of bodyguard required super famous level. Inside those it's just sort of like, yeah ok, your car/yacht/jet/whatever is better than mine, cool. reply JohnMakin 3 hours agorootparentIt doesn't have to be super drastic - When I graduated college and got a decent job, most of my social group was making less than a full time minimum wage salary, bouncing around couches, or staying with parents - with no hope of it improving. I felt the resentment a lot, even though it was subtle, and constantly felt obligated to pick up checks if we did anything I wanted to do, because what I could afford to spend on an outing was significantly different. Then that builds resentment over time, etc. People don't like seeing people with more \"stuff\" than they have at a really deep level. Looking back I am not sure that I could have done much to salvage it, the only friends that survived out of that era were the ones that were able to bring themselves out of their situation as well. reply sokoloff 16 hours agorootparentprevMy friends can accurately assess my net worth within a factor of +/- 5, but where exactly in that range isn't necessary for them to know. Even my closest friends probably can't reliably put it within +/- a factor of 2 (nor do they need to for any valid purpose). reply j4coh 7 hours agorootparentprevI’ve never shared my specific financial details with any friend, before or after, so perhaps a different definition of what it means to be a friend. I haven’t bought a bunch of expensive crap that would give it away. reply j4coh 17 hours agorootparentprevIt’s not about living a double life. I don’t secretly blow cash on hookers and blow, I just live a pretty modest normal life and don’t talk about the details of my investments or means. Mainly I don’t have to worry about anything really. reply cj 19 hours agoprevI don’t think his problem is money. I think his problem is his identity (founder of Loom) suddenly disappeared. Now he needs to develop a new identity. This is especially difficult for single founders without kids (in the sense that people with spouse/kids already derive much of their identity from those 2 things). Selling a company isn’t all that different from going through a divorce (in the sense that your identity needs to be completely rebuilt from scratch) reply bumby 18 hours agoparentWilliam Storr writes about this. His stance is humans are hard wired for status within their social group. The problem is when all your status eggs in one basket and it disappears, it’s not good for your mental health. He advocates for having your identity spread across many different pursuits and disparate social groups, although he admits he’s not very good at doing that himself. reply heresie-dabord 13 hours agorootparent> humans hard wired for status within their social group. Not always \"status\". Humans benefit from cooperative behaviour but may have many reasons for joining and adhering or leaving. Having varied interests means different networks. The important point is to see meaning and value. This is where ostracism and rejection can be most painful. reply bumby 5 hours agorootparentTo put a finer point on it, Storr’s thesis is there are three main domains that humans try to achieve status: dominance, competence, and virtue. Same end goal, but different means to get esteem. Put differently, people ultimately need to feel valued by their tribe. reply heresie-dabord 1 hour agorootparentThanks for clarifying. There must be more subtlety in the \"end goal\" of membership. > people ultimately need to feel valued by their tribe To the extent that people want to remain in a group (\"tribe\"), I agree. But this holds only when people feel that they gain value from the group (or tribe). For some members, the sense of gain may be conspicuous prestige, but for other members it may be a humble gain or an unnoticed (inconspicuous) gain. The quieter members (in O.P.'s narcissistic terminology, the \"NPCs\" in his company) may have insights that completely escape the O.P. and other prestige-seekers. reply bumby 45 minutes agorootparentAs has been noted by others, I suspect the “NPCs” have different tribes. Eg, they have status and identity with their family rather than their “work tribe.” I wonder if the author is blind to the fact that not everyone has the same end-goal/tribe/value system that they do. Regardless of how the relevant tribe is defined, I believe status has been shown to be important to mental health which the author seems to be struggling with. reply purplethinking 4 hours agorootparentprevI took 1.5 years off to work on an open source project (also because I was struggling with health issues), and the hardest thing was describing what you're doing to other people. I thought I was \"above\" social status, that it wouldn't affect me, but it did. I was essentially unemployed, that's how it felt at least. It's so much easier just saying \"I do X for a living, I work at company Y\". It means some company thinks you're good enough to pay good money for. reply mywittyname 2 hours agorootparentKeep it simple, then pivot. Most people don't care that much about what you do unless they're in your same industry (in which case, they'd empathize). > What do you do? Write software. > Oh yeah, for who? $GitProjectName > What do they do? It's a project that . But enough about me,reply cal85 18 hours agorootparentprevIn what book? Searched but can’t find anything that sounds like this. reply WorkerBee28474 17 hours agorootparentMy guess is The Status Game by Will Storr. reply bumby 15 hours agorootparentThat’s correct, thanks for clarifying. reply philosopher1234 18 hours agorootparentprevHard to take such general advice seriously from someone who apparently doesn’t practice it. In fact, it seems mistaken to do so. reply fsckboy 12 minutes agorootparentif the first person to notice a correlation between alcoholism and cirrhosis was an alcoholic, you'd dismiss what he said out of hand and keep drinking? reply bdangubic 15 hours agorootparentprevdo as I say not as I do is core of all parenting… :) reply nooneevencares 17 hours agoparentprevProbably needs to develop a soul first. NPCs don’t actually exist outside of video games, those are real human beings. Not sure what to do with all that wealth? Try asking one of those NPCs… spend a day with each one of them, learn what being human actually is reply mywittyname 1 hour agorootparentThe dude also dumped his long time girlfriend right after coming into a large chunk of money (and thinks she cares enough to read his blog!), and truly thinks he was going to \"save our government\". Also, the mountain climbing (IYKYK). He sounds pretty full of himself and seems to struggle making personal connections with people. Being the founder of a startup gave people a reason to care about him, and now that he's lost that along everyone around him. He beat the game and now the characters in the story have nothing left to say to him. The guy should put down the physics book and go learn to be a person that others enjoy being around. Go get a job waiting tables and hang out with coworkers after work, learn to surf, etc. reply taway1874 15 hours agorootparentprev100% this! Calling one's colleague an NPC is not only demeaning but also shows a lack of awareness and empathy. Does the author even understand that by his logic, he is a NPC in his colleague's world? reply khazhoux 16 hours agorootparentprevYou are misinterpreting. He’s talking about how at big companies, you always have people who don’t seem to bring any actual value. They're in every meeting, but don't say anything, don't set any direction, don't produce any documents or any code, don't exhibit any sense of urgency or even involvement, and don't contribute in any noticeable way. \"NPCs.\" They are completely passive as far as you can tell -- or worse, they actively slow others down when they happen to be on some critical approval path. I'm sure they are lovely people outside work, and loving parents and good citizens. But when the rest of us are busting our butts to get work done, they're unfortunately useless. reply benwad 10 hours agorootparentThe guy who started a video conferencing app called Loom (2 years after Zoom came out) then miraculously sold it for almost a billion dollars has no business calling anyone an \"NPC\". reply dclowd9901 14 hours agorootparentprevThey're still people. Just because the company doesn't motivate them or they have a bad manager or are on a bad team or a million other reasons they don't feel empowered to \"participate\" (specious since clearly they're employed) doesn't mean you can act like they're soulless bots ffs. This whole mindset has got to go. You and OP going around like this, it's gross for the world and it's a bad look on you. reply lazide 14 hours agorootparentsounds like you’re just shaming OP for trying to actually get something done by recognizing practical realities. If everyone matters the same, no one matters. reply Aeolun 16 hours agorootparentprevNegatively Producing Characters? Or maybe Null Producing Characters. reply nunez 12 hours agorootparentNon playable characters He's calling them filler, basically reply nateburke 17 hours agorootparentprevYes. Thinking of others as NPCs has its own way of turning ones self into an NPC. cf. Mean Girls reply visarga 12 hours agorootparentprev> Probably needs to develop a soul first. More generally, if you cultivate yourself you will get more pleasure from your activities. If you take time to learn an instrument, or listen to classical, or gardening (you can grow exotic plants for example), learn a new language, or anything else. The more you put into refining your appreciation and knowledge, the more value you can get back from your activities. It's a self cultivation problem. reply mcmcmc 17 hours agorootparentprevCalling people NPCs is one of my biggest pet peeves and a dead giveaway that someone is a soulless narcissist. It is dehumanizing in the extreme, the same way Nazis characterized Jews as rats in propaganda. When people say eat the rich… this is who they mean reply karmakurtisaani 54 minutes agorootparentWell.. I've pretty much accepted that I'm more or less an NPC. So no harm done from my side. reply ddq 1 hour agorootparentprev>a dead giveaway that someone is a soulless narcissist You're engaging in the exact same behavior, are you not? NPC and soulless are the same fundamental concept, that there is a certain threshold of humanity people can fall below to be considered lesser. They're soulless, they're NPCs, they're untermenschen... whatever the word for it, there are \"dead giveaways\" that a person can lack that hidden quality that separates man from animal. I'm sure it wasn't your intent, but from a certain reading, it seems like what you're saying isn't really all that far off from \"calls people NPCs = is the real NPC\". I often see people decry specific terminology associated with dehumanizing beliefs without refuting the actual premise behind them. reply voidhorse 17 hours agorootparentprevYep. I don't understand why the technological community accepts essentially sociopathic tendencies as long as your idea (regardless of what that idea is) is rewarded by the capital system. It's pathetic. reply thfuran 12 hours agorootparentIt's not pathetic, it's destructive and evil. reply Fomite 6 hours agoparentprevI don't think his problem is money. I think his problem is he might not be a very good person. reply simonswords82 2 hours agorootparentIf true, he probably knows this already and now just needs to work out how to fix it. reply nico 16 hours agoparentprev> I think his problem is his identity (founder of Loom) suddenly disappeared. This is spot on. And I think it’s probably the biggest thing he’s going through However, the money is definitely a big factor as well. Not because of the amount of money, but because of the suddenness that it happened with In a very short amount of time, he found himself not needing (and realizing also not wanting), to maintain his identity at the time The money and the suddenness also put him in a situation that is pretty hard to relate to for the vast majority of people So not only he lost his identity, he also found himself alone (and made it even worse, by pushing people away or ending some relationships) reply iknownthing 18 hours agoparentprevIs this not the same problem everyone faces when they retire? reply driverdan 18 hours agorootparentNo, only people who define their lives by their job. Most people have a life outside of work. reply tobyhinloopen 8 hours agorootparentWe're supposed to have a life outside work? Oh shit reply philosopher1234 18 hours agorootparentprevThe vast majority of men develop depression at retirement. So you can say this with scorn if it pleases you, but the group you’re talking about is massive and doesn’t deserve this derision. reply mcmcmc 17 hours agorootparentThis is straight up false, you are wildly overexaggerating with zero evidence to back it up. Incidence of depression in retirement age populations are around 6 - 8%, and the correlation between retirement and depression is more pronounced in women. There’s no need to make false claims about a real disorder just so you can feel persecuted as a man https://www.ajgponline.org/article/S1064-7481(23)00424-4/abs... reply philosopher1234 15 hours agorootparentMy inaccuracy not withstanding, you supply the data to prove my point, that it is common for men to experience depression in retirement. And my suggestion that they should be considered seems just as true. Your suggestion about my motives makes no sense, I made no comparison to women. It sounds like that may have something to do with you, not me. Perhaps ask yourself: why does a suggestion that these men deserve some sympathy or at least consideration provoke you into harshly criticizing me? reply bazmattaz 1 hour agorootparentIf there is evidence my guess is that it’s likely due to the previous generation’s attitude towards work. My guess is that the previous generation (like my parents and their parents before them) lived to work, it was their whole identity, thus when they retired they had no idea what to do with themselves (like the Loom founder) and depression was common. There has been a shift over the last decade or two likely in millennials who just work to live. They look forward to not working and living a life full of hobbies and social activities. reply friendlyasparag 1 hour agorootparentprevTo be fair you are the first person in this thread to bring up gender, as if women don’t also feel depression in retirement, and indeed at higher rates. Why would you only mention men? reply johngossman 18 hours agorootparentprev1 in 3 according to the best data available. And when you consider that most people retire late in life when they are likely to face health issues, start losing family and friends, and that most people don't retire rich... reply jandrese 2 hours agorootparentI think it is more that people who were married to their job find themselves suddenly divorced. Without other activities to give them a reason to get up in the morning it is easy to fall into depression. There was even a movie that explores this situation: About Schmidt. reply jerojero 17 hours agorootparentprevThere could be a lot of factors involved in this. Retirement might be, in a way, a clear point in life when you get to actually think about your own mortality and life coming to an end in the next few decades. That alone could have a major impact in people developing depression at retirement. What the poster says might still be true, people are being defined by their work and when that's over there's a huge gap they don't know how to fill. This might very well be a criticism on the role of work in society. Ultimately though, you're right, there's no need to insult people when thinking critically about this issue. reply lazyasciiart 14 hours agoparentprevHe doesn't think his problem is money either, because that would have a trivial and obvious solution that doesn't seem to be under consideration. reply Krei-se 17 hours agoparentprevTrue, i would still argue that your identity might be fogged by these things and come out clearer after lifting roles you may stumbled into more than you chose them. So sure hope for him and others they survive their 7 years of catharsis! reply mupuff1234 2 hours agoparentprevReminds me of this Jim Carrey speech https://youtu.be/YHIZ0Rb7lv0?si=TG_SIi-XUuP1iHYf reply paulpauper 18 hours agoparentprevIt says on wiki: In 2022, according to Forbes, the firm was valued at $1.5 billion, having secured $200 million in funding from venture funds such as Sequoia Capital, Andreessen Horowitz, ICONIQ, Coatue, and Kleiner Perkins.[1][2] It is remote, but is headquartered in San Francisco, California, with an office in New York. Why would such a seemingly simple product need so much money? It seems like the business was already done. Web video recording or facetime has been around a long time, but somehow this company carved a niche in a crowded market. reply voidhorse 17 hours agorootparentThey slapped the word AI on it and took advantage of temporary market conditions (wfh due to covid and AI hype), nothing more, nothing less. Unless I'm missing something, there is nothing special about this product and probably no one will remember it in ~15 years time. Meanwhile OP seems to think he should have expected the same sense of fulfillment one might get from an actually meaningful contribution to human society, for some reason. reply sfblah 15 hours agorootparentVery very well said. OP's vapid writing kind of tells the tale. reply redlock 9 hours agorootparentprevWhat does loom have anything to do with AI? It is a nifty way to share video recordings easily and quickly. In that they did a great job reply msanlop 5 hours agorootparent>One video is worth a thousand words >Easily record and share AI-powered video messages with your teammates and customers to supercharge productivity This is the first thing you see when visiting the website. Ctrl+F finds \"AI\" 10 times in the page. Like the other commenter said, it probably has nothing to do with AI, but this is what sells at the moment reply Animats 15 hours agoprevBeen there, done that. General comments: - Most people who make a lot of money all at once blow it within seven years. Check out what happens to lottery winners, jocks, and rappers. As a rule of thumb, you can safely spend 4% of your net worth per year. Pay yourself some fixed amount each quarter. - You don't have to get into complex investments. Half in some bond funds, half in some diversified stock funds will work out OK. - Any investment where they call you is probably not very good. Useful reading, although dated: \"The Challenges of Wealth\", by Domini et. al. What to do with your life? No idea. What are you good at? - I was a visiting scholar at Stanford for a while. But it was the \"AI Winter\" and not much was happening. Did robotics in the 1990s. Held patents on legged running on rough terrain, ragdoll physics. Ran a DARPA Grand Challenge team. Didn't really lead anywhere. Too early. Still programming. A metaverse client I'm writing in Rust is running on another screen. - Horses have been good for me. Every day, I go out and spend time with a pushy alpha mare who keeps me in shape. \"Riding is the only art which princes learn truly\". Horses are not impressed by money. Neither are most riders. - I've known a few ex-CEOs. One did a lot of reasonable little stuff but never did anything with much impact again. One founded a charity. Another was really into sailboats, and he just kept on with sailboats, crossing the Atlantic and such. He's lucky in having a wife who is also very into sailboats. One guy bought a nightclub, but it loses money year after year. reply psanford 14 hours agoparent> Most people who make a lot of money all at once blow it within seven years This is a commonly recited myth about lottery winners[1]. [1]: https://www.forbes.com/sites/johnjennings/2023/08/29/debunki... reply teractiveodular 14 hours agorootparentThere's a legendary Reddit comment that lays out the many, many other ways winning the lottery (or, more importantly, letting people know you won the lottery) is bad for you. Can you debunk its claims as well? https://www.reddit.com/r/AskReddit/comments/24vo34/comment/c... reply dahart 10 hours agorootparentThat comment makes untrue claims and cites no sources. The claim about multi-million dollar jackpot winners is a viral meme that keeps making the rounds despite the people with the actual stats repeatedly trying to debunk it. It is not true that a huge percentage of winners go bankrupt. https://www.nefe.org/news/2018/01/research-statistic-on-fina... https://www.reddit.com/r/PetPeeves/comments/18xqcbw/70_of_lo... reply autarch 13 hours agorootparentprevThe Reddit comment is interesting, and I think the advice that starts in the reply is sound. But this person's list of lottery winner failures is a small list of people versus a very large group of winners. Surely it's not hard to cherry pick a bunch of worst case scenarios. reply raincole 13 hours agorootparentprevThe reddit comment says things like \"Homicide (something like 20x more likely)\" without citing the source of this statistics. reply croisillon 12 hours agorootparentprevi am still willing to experiment, for science reply lucianbr 9 hours agorootparentprevI am a little bit suspicious of Forbes saying \"in fact, money does buy happiness\". I fear they might have a little bias. I expected something like a Snopes link. Ah well. reply teractiveodular 14 hours agoparentprevAssuming the \"one guy [who] bought a nightclub\" in question is jwz of Netscape fame, his blog is a trip: https://refhide.com/?https://www.jwz.org/blog/ (routing through refhide because otherwise you'll get a testicle in an egg cup; jwz is a principled man of strong opinions and one of many things he loathes is HN) reply ip26 14 hours agoparentprevOne did a lot of reasonable little stuff but never did anything with much impact again This always stymies me. If I were to make it big, I’d want to go on to do other very useful things. But so commonly that doesn’t happen. Is it because they lost the fire? Because they were really lucky exactly once? Or perhaps their talents were suited only to that one thing? reply Animats 14 hours agorootparentThat's always a good question. Were you smart, or were you lucky? This is most often a problem with traders and investors who made a good bet. Less so with something that took a lot of effort to make work. I know a married couple who blew through an amount in eight figures and had to get low level jobs. Lost their house. reply thfuran 12 hours agorootparentMaybe somewhat less, but effort doesn't really imply value and value doesn't imply successful extraction. reply azinman2 14 hours agorootparentprevSkill, luck, timing. Few people achieve something big. Even fewer can achieve another big thing. Even less can continue achieving. reply rollo 7 hours agorootparentprevPersonally I get immense enjoyment out of not doing anything useful, especially not on a big scale. So if I were to accidentally \"make it big\", I'd most likely not do it again and keep enjoying the small inconsequential things. reply dahart 11 hours agoparentprev> Check out what happens to lottery winners I have looked into this and found out that lottery winners are fine, that the viral meme about two thirds of them going bankrupt is not true. https://www.nefe.org/news/2018/01/research-statistic-on-fina... Most of the claims about lottery winners going bankrupt are uncited and unsourced. A few claims I was able to find pointed at this paper: https://eml.berkeley.edu/~cle/laborlunch/hoekstra.pdf This paper shows people winning less than $150k having bankruptcy rates drop for the first 2 years, and then return to normal 3-5 years later. Like, duh, a small amount of money will run out. This paper has been widely and wildly mis-quoted as bankruptcy rates going up after a few years, but everyone seems to leave out the part where bankruptcy rates went down first. The percent of people in this study who had declared bankruptcy after 5 years was slightly over 5%, which is the same bankruptcy rate as the study cohort had 2 years before winning money. 95% of lottery winners did not go bankrupt, and no multi-million dollar jackpot winners are involved in this study. reply Tepix 13 hours agoparentprevBlue water sailing is pretty great (for some), give it a try. reply dcrazy 13 hours agoparentprev> One guy bought a nightclub, but it loses money year after year. As far as I know, jwz was never a CEO, but he did the same thing. reply jandrese 2 hours agorootparentIf you were rich enough then having the new business lose money isn't that big of a problem, so long as it's not driving you to ruin. \"Sure it costs me a few hundred every month to keep the lights on, but it makes me happy.\" Honestly, I wish more rich people did stuff like this instead of obsessing over making more money on the stock market or speculating on homes or whatnot. Learn to say enough is enough and switch to adding back to the world instead of leeching off of it. reply HeartStrings 11 hours agopare",
    "originSummary": [],
    "commentSummary": [
      "The post discusses the common struggle of finding purpose after achieving financial success, which can lead to feelings of disillusionment and isolation. - It suggests that fulfillment can be found in developing others, building connections, or pursuing new interests, emphasizing the importance of purpose beyond financial wealth. - The discussion also highlights the challenges of maintaining relationships when wealth creates a social divide, underscoring the need for meaningful activities and connections to foster a sense of belonging."
    ],
    "points": 793,
    "commentCount": 1375,
    "retryCount": 0,
    "time": 1735858215
  },
  {
    "id": 42579472,
    "title": "iTerm2 critical security release",
    "originLink": "https://iterm2.com/downloads/stable/iTerm2-3_5_11.changelog",
    "originBody": "Version 3.5.11 of iTerm2 was built on January 2, 2025. This release contains a critical security fix. I strongly recommend updating immediately. Who is affected? ---------------- You may be affected if you used the SSH integration feature in any of the following versions: * 3.5.6 * 3.5.7 * 3.5.8 * 3.5.9 * 3.5.10 * Any beta versions of 3.5.6 and later. What is the issue? ------------------ A bug in the SSH integration feature caused input and output to be logged to a file on the remote host. This file, /tmp/framer.txt, may be readable by other users on the remote host. When does this occur? --------------------- The issue occurs if both of the following conditions are true: 1. Either: a) You used the it2ssh command, or b) In Settings > Profiles > General, the Command popup menu was set to \"SSH\" (not \"Login Shell\", \"Command\", or \"Custom Command\") AND \"SSH Integration\" was checked in the SSH configuration dialog. That dialog is shown when you click the Configure button next to the ssh arguments field in Settings. 2. The remote host has Python 3.7 or later installed in its default search path. What should you do? ------------------- * Upgrade immediately to version 3.5.11. * Delete /tmp/framer.txt on affected hosts. How I'm addressing this ----------------------- I deeply regret this mistake and will take steps to ensure it never happens again. The code to write to log files in SSH integration has been deleted and will not be publicly released again. If you have questions you can contact me at gnachman@gmail.com. SHA-256 of the zip file is You can use the following to verify the zip file on https://keybase.io/verify: -----BEGIN PGP SIGNED MESSAGE----- Hash: SHA256 655e32b4a9466104f1b0d8847e852515bc332bdf434801762e01b9625caa43e2 -----BEGIN PGP SIGNATURE----- iHUEAREIAB0WIQSAPIQGkYVsjnBRo2J0Et0TaFtKrAUCZ3br8gAKCRB0Et0TaFtK rLntAQDqPcKkRA23Wo5/XuB2lymF8n+0GK3E+ZT3MYbTNgsnSQD/Xgt7V9QhP42n QmQpnmb804FrHkCnqIJMvcBAim6AbBM= =Zlrw -----END PGP SIGNATURE-----",
    "commentLink": "https://news.ycombinator.com/item?id=42579472",
    "commentBody": "iTerm2 critical security release (iterm2.com)647 points by tjwds 20 hours agohidepastfavorite412 comments Kwpolska 20 hours agoLooks like a case of print() debugging making it into production: https://github.com/gnachman/iTerm2/commit/63ec2bb0b95078a97a... https://github.com/gnachman/iTerm2/blame/5db0f74bf647f6d53ea... reply mort96 18 hours agoparentThat's not unreasonable code in itself, it's writing to the file only if verbose mode is enabled. This is the commit which disabled verbose mode, just before the code which removed verbose framer logging entirely: https://github.com/gnachman/iTerm2/commit/014ba7ec40fc790f65... This is the commit which enabled VERBOSE mode: https://github.com/gnachman/iTerm2/commit/5db0f74bf647f6d53e... (from Jul 3, 2024) That is probably just from having set VERBOSE=1 while implementing or debugging something and forgetting to revert it to VERBOSE=0 before committing. reply nothrabannosir 12 hours agorootparentThis happened to me so often that I made a git pre-commit hook to catch it: https://github.com/nobssoftware/nocommit for the word NOCOMMIT Includes a GitHub CI action to prevent merging it in to master It has caught so many of these mistakes for me… reply huskymybeloved 10 hours agorootparentI only heard of pre-commit hooks like 20 times in the wild and I really want to emphasize how badly that number should be 10x larger. Got a team completely unaware they existed once. reply NekkoDroid 5 hours agorootparentI really do wish that git itself would like check for a `.githooks` directory and prompt if it should set `core.hooksPath` to that path (probably would need to notify on pull when any files in it changed for security reasons) I would also love if it detected a `.gitmessage` for message the message template without needing to set it up for each repo, but that is a different matter. reply cdeevgtg 9 hours agorootparentprevThey are just not convenient to use since they are per repo and not shared via clone reply CGamesPlay 14 hours agorootparentprevThis variable might have been better named LOG_ALL_STDIO_TO_TMP. Would have made it more obvious during self-review that it shouldn't be shipped. reply bobbylarrybobby 13 hours agorootparentOr even better: # TODO: set back to 0 This appears as a warning in my editor, and of course is greppable reply darknavi 13 hours agorootparentDon't mind me with 100s of // TODO in my codebases. reply bobbylarrybobby 13 hours agorootparentThen # HACK: or # FIXME: or # DO_NOT_SHIP... anything greppable and distinct enough to error in CI reply pandemic_region 12 hours agorootparentor having a commit hook that fails if the code contains such string reply lupire 17 hours agorootparentprevThis is why all \"temporary\" settings go in .gitignored files or equivalent, or on no-merge testing branches, and we have test on the release branch that asserts for each config value that it is set to a safe value, and we have a test that reflectively asserts that every config key has a value assertion test. reply hnlmorg 10 hours agorootparentOr non-default compile time flags. That’s how I handle verbose debugging in my terminal emulator. reply mulhoon 20 hours agoparentprevIt’s been around for 3 years? reply CameronBanga 20 hours agorootparentAbout six months. File was originally authored a few years back, but looks like this slipped in here: https://gitlab.com/gnachman/iterm2/-/commit/5db0f74bf647f6d5... reply Kwpolska 20 hours agorootparentprevDisabled by default until 7 months ago. reply urronglol 20 hours agorootparentnext [2 more] [flagged] mtmail 20 hours agorootparent9 month ago. I don't see a connection. reply Waterluvian 19 hours agoparentprevIn typescript dev I made “console.log” a linting error that cannot be merged. The occasional legitimate need uses console.info I think print debugging is fine. It has a time and place. But ideally find a way to protect yourself from accidentally leaving it in. It’s such an easy mistake to make. reply mort96 18 hours agorootparentWell this is an instance of writing to a file, so the JS equivalent would be calling .write on a file stream. Not exactly comparable, it wasn't literally a debug print() that someone had forgotten to delete but a log file that was enabled when verbose logging was enabled. reply theamk 17 hours agorootparentprevin similar situations, instead of saying \"VERBOSE=1\", I say \"VERBOSE=getenv(\"MY_NAME_MY_APP_VERBOSE\") == '1'\", and set this env variable in my terminal when needed. This way there is zero chance I commit verbose-enabled debug code. reply orliesaurus 13 hours agorootparentprevnot a typescript dev here: what do you use instead of console.log for debugging? reply darknavi 13 hours agorootparentIt sounds like they use console.log, but they don't let it ship in their production code. reply electromech 5 hours agorootparentprevI liked tslog last time I tried it. reply Waterluvian 4 hours agorootparentprevconsole.log is reserved for devtime debugging and is a linting error so it cannot be left in. console.info is used in the occasional time we actually want to write things to console (along with console.warn, console.error, console.group, console.table, etc.). It's all it took. Just making a clear distinction between the two and communicating it with the team. reply mattpavelle 20 hours agoprev> A bug in the SSH integration feature caused input and output to be logged to a file on the remote host. This file, /tmp/framer.txt, may be readable by other users on the remote host. Oof. This is nasty. Some folks may not have access to some machines that they've SSH'd into anymore where files like this may or may not exist. reply rad_gruchalski 20 hours agoparentThis seems relevant: When does this occur? --------------------- The issue occurs if both of the following conditions are true: 1. Either: a) You used the it2ssh command, or b) In Settings > Profiles > General, the Command popup menu was set to \"SSH\" (not \"Login Shell\", \"Command\", or \"Custom Command\") AND \"SSH Integration\" was checked in the SSH configuration dialog. That dialog is shown when you click the Configure button next to the ssh arguments field in Settings. 2. The remote host has Python 3.7 or later installed in its default search path. reply mattpavelle 20 hours agorootparentYeah #1 reduces the surface area for sure, #2 maybe not so much :) reply prophesi 2 hours agorootparent#1 greatly reduces the surface area, and #2 doesn't _increase_ the surface attack limited by #1, in case people are reading this incorrectly. reply fn-mote 20 hours agorootparentprev1B looks like a common situation reply kccqzy 12 hours agorootparentIt seems incredibly uncommon to set your terminal emulator to run SSH rather than starting a shell. reply karel-3d 10 hours agorootparentprevIt is not very common. reply welder 8 hours agorootparentprevI have Python 3.7 or later installed on my default search path, but after checking none of my servers have a `/tmp/framer.txt` file. reply cozzyd 18 hours agorootparentprevLong live EL8 reply soheil 3 hours agoparentprevThis bug almost never occurs as it's a very esoteric feature that 99% of people here never heard or used. If you're, however, the type of user who decides not to use `bash` or `zsh` and instead wants `ssh` as their default terminal command there are probably other unusual features that you use in other apps exposing you to many other attack surfaces that you also need to worry about beyond just iTerm. reply locusofself 12 hours agoprevI just want to sing some praises for iterm2. I've been using it for work and fun for many years now and will continue to use it and send a donation again as I did once before. reply Upvoter33 7 hours agoparentAgree. It's one of the best things I use all the time. reply kelnos 19 hours agoprev> I deeply regret this mistake and will take steps to ensure it never happens again. I always get a little... sigh-y when I read statements like these. What steps? I'm not even sure what I would do to ensure something like that wouldn't happen again. Build some automated tooling to run the software that exercises every single feature it has, and capture system calls to ensure that it never opens or writes to files? That sounds like a very difficult thing to do (to the point that I wouldn't even try, especially for a GUI app), but anything less doesn't feel like you can ensure it won't happen again. reply Meekro 16 hours agoparentGazillions of dollars have been spent on fuzzing Chrome/Chromium, and they're still finding dozens of serious issues per year. Same with every other major product. The reality seems to be that we, as programmers, can't do any better than this. If that's the case, it's unfair to lay it all at the feet of this one guy. reply blinded 15 hours agorootparentExactly. Anyone complaining should donate time or money. reply bjackman 13 hours agorootparentI would even go so far as: if you see reactions like \"this is shoddy work, the author should Just Do Better\", you can generally surmise that the person making the comment has a poor understanding of fallibility and will make a lot of these mistakes themselves. reply mhink 19 hours agoparentprevGiven the brevity of the security report, I figure the author wanted to get the relevant details about the *incident* posted as fast as humanly possible. However, it does seem appropriate to acknowledge that just because they're being terse doesn't mean they don't understand how big of a mistake it was. That being said, I would also strongly expect a more in-depth blog post following up, with details about just the sort of thing you're mentioning. reply smallnix 19 hours agorootparentI understand the interest about this bug, but to my understanding this is an unpaid hobby project? If that's true I don't feel entitled to expect anything here. reply mort96 18 hours agorootparentI think your parent comment used \"expect\" to mean \"predict\" rather than \"demand\"? reply lupire 17 hours agorootparentprevYou can expect anything you want in software you use, and choose not do you software that fails to meet expectations. A software author who takes pains to publish his work and who accepts financial donations, is likely interested in maintaining his reputation and improving his skill and quality. Finally, security bugs are in a class of their own. Giving out free junk is OK. Giving out free secret poison is not. reply shwouchk 16 hours agorootparent> Finally, security bugs are in a class of their own. Giving out free junk is OK. Giving out free secret poison is not. It is not if it was done maliciously. If the code you got for free contained some mistakes it's ultimately your responsibility - You didn't have to take that pill you got at the party. Accepting donations could change this, but I would say it depends on how they are presented - \"campaign donations\" ala Joey Hess or \"Hey thanks for the the party last night, here's $40 to cover some of the booze!\" Alternatively, I'm curious how you feel about companies offering you \"free email, free search, free image hosting, free social media\" etc, (actually, in exchange for all your behavioral data) ((actually, even if you never directly accept anything from us))? reply kureikain 18 hours agoparentprevsuch a bad take. as a software engineer thing like this happen all the time. no matter scale we will screw it. what steps we can take is that it's now a lesson to make it more caution when we went into that path. reply twh270 17 hours agoparentprevAnother comment mentioned using a linter to prevent 'console.log' from being mergeable in a PR, and this is exactly the kind of approach I'd take. Preventing an invalid state from existing is a pretty useful principle. reply jessekv 15 hours agoparentprev> The code to write to log files in SSH integration has been deleted Seems like a good first step. reply ijustlovemath 13 hours agoparentprevComing from medical device background: procedures. Documents that explicitly lay out the things you have checked, how you checked it, date of check, and your signature on it at the end. When you learn or anticipate a new failure mode, thats a new step in the corresponding procedure. Sometimes you'll be able to automate this stuff, but when the impact is this deep, it will not kill you to add some manual workload to your release process. reply refulgentis 19 hours agoparentprevRight, you get it in toto IMHO: this is the least worst thing they can say, and also the best thing they can say. If they don't apologize, that'd be worse. If they don't indicate they'll take steps to prevent this from happening in the future, that'd be worse. If they had all the steps ready right now, I'd be confused because they should have A) fixed the bug B) deployed the fix C) communicated the bug exists and a fix was deployed. Blending that with D) postmortem would show an undisciplined approach. If they claimed the ability to prevent all bugs ever, or at least, all unintentional file writes, I'd be confused because it's impossible to prove it never writes to a file. A good start is to do what he did (delete the ssh logging altogether), and start investigating automated ways to validate if/when files are accessed. The cool thing about macOS dev is there's a ton of tooling that leaps and bounds beyond cross-ecosystem tooling. I wouldn't be very surprised if someone linked an obscure mid-1990s technical note that lets you set an NSArray of paths that are allowed access, or if Instruments had some built-in dtrace integration that could pull this off. Couple running that in CI, make sure you got test coverage, and you've done the best you can. (n.b. a lot of it seems to hinge on \"I deeply regret this mistake and will take steps to ensure it never happens again.\" being read as \"I deeply regret this mistake. I will do things until I can, absolutely, 100%, foreever, guarantee it'll never ever happen again.\" For the young and impressionable: the blog post is great, there's ~0 you could do better here) reply lupire 17 hours agorootparentBetter would say \"After the immediate problem is patched, I will post more details about future plans for security, probably within X days. I welcome suggestions on the feature tracker.\" Empty vague promises aren't really better than being quiet. They rely entirely on the reader's good faith, but if that good faith exists (which it likely does for this excellent product and excellent developer), then the promise adds no information. reply dymk 18 hours agoparentprevSeriously? It’s free and open source, give the guy a break. He’s a human being. reply kortilla 18 hours agorootparentHe’s saying that statement is unnecessary over-commitment. Not that he’s not doing enough reply ed_mercer 18 hours agorootparentprevIsn't that his/her point? reply dd_xplore 13 hours agoparentprevTalk is cheap, send patches reply dylan604 17 hours agoparentprevthere's lots of things you can do with deploy scripts to help prevent bozo errors from devs. Just like the code looking for credentials uploaded to github, you can do whatever type of searches that you as a developer is prone to making. it's a cya type of thing. reply Kevcmk 19 hours agoparentprevJust a reminder that iTerm is FOSS: https://github.com/gnachman/iTerm2 reply waynesonfire 17 hours agorootparentwhats the paid alternative? reply kstrauser 17 hours agorootparentPanic’s Prompt, I guess? I bought it because it’s now bundled with the iOS app, which I already used. It’s fine. Not mindblowing, not bad at all. Just fine. reply jcalx 20 hours agoprevI know it's largely personal preference but are there any strongly compelling reasons to use iTerm2 over stock Terminal on macOS in 2025? Despite recommendations, I've been wary of security and privacy issues much like this SSH bug. reply dmd 18 hours agoparentThe killer feature for me is Edit>Selection Respects Soft Boundaries, which lets you copy text from inside windows defined INSIDE the terminal - like tmux or emacs splits - where iTerm figures out that, e.g., a pipe character is a window boundary. Two more: 2) if you accidentally close a tab/window, you have a few seconds to hit ⌘z and the window will reappear as if you never closed it! 3) Minimum color contrast. If your terminal color scheme and what you're running's color scheme interact poorly to create something that would be unreadable, iterm notices and you can have it override those colors to be something of higher contrast, automatically. But that's just my killer features. iTerm is like Word - it is a bloated monster with thousands of features. Nobody needs them all, but nobody agrees on which ones they need. reply olyjohn 17 hours agorootparentBeing able to SSH into multiple machines, and mirror your keyboard across all of them is my favorite feature. reply dmd 17 hours agorootparent... a feature I've never used or had a need for thus showing my point precisely :) reply happymellon 12 hours agorootparentHonestly, it sounds like an anti-feature or a crutch. Designed to help environments that haven't reached 2010 era of automation of Salt or Chef. reply rigrassm 10 hours agorootparentMeh, I've used it numerous times in situations like setting up some clustered software to explore for the first time. Just fire up your 3 vms, ssh into each of them in their own panes, hotkey to activate broadcast mode, start creating/editing your config(s) and tuning your server. It really is useful in the right scenarios, sure you shouldn't adopt it as your \"official method for provisioning production servers\" but that doesn't mean it doesn't have a legitimate use case or is a \"crutch\". reply wkat4242 13 hours agorootparentprevOk those things are interesting though not killer level for me. But I moved away from Mac now because I found their stuff too opinionated. I use KDE now. I wonder if iTerm2 also exists on Linux if it's open source? reply happymellon 12 hours agorootparentIf you use KDE, then you already understand that having other peoples opinions forced on you doesn't always work. You probably don't need iTerm because the KDE (and the Gnome tbh) consoles are so much better than the built-ins that come with Mac and Windows. reply wkat4242 11 hours agorootparentYes this is indeed exactly why I moved from Mac to KDE and why I donate to them. They still understand that in this day and age of opinionated design. I have KDE very heavily customised, but the cool thing is that unlike Gnome I didn't have to use a single plugin to do it (which always causes issues upgrading). Everything I wanted to change was available as a setting somewere. And yeah I use Konsole, it's great!full-screen-mode-that-is-different-than-native-macOS-full-screen Thank you for reminding me why I should just return to iTerm! It might seem minor to some, but this is such an essential 'feature' that it probably overrides all other differences, for me. One small question, though: are you aware of anything that 'native' full-screen does that 'bespoke' full-screen misses out on? Any disadvantages whatsoever? reply thuanao 11 hours agorootparentprevThis is the only reason I use iTerm2. I can view the terminal full screen while also instantly switching to the browser or other windows, without the animation. reply billowycoat 20 hours agoparentprevThere are reasons. Whether they are compelling or not, largely depends on what software you want to run. https://textual.textualize.io/FAQ/#why-doesnt-textual-look-g... reply lr1970 20 hours agoparentprevInstead, I would recommend Ghostty [1] terminal recently released v1.0 by one and only Mitchell Hashimoto of Hashicorp fame. It is OSS native cross-platform application (not an Electron one). I have been using it for the last year (private beta) on Mac and Linux and it rocks. [1] https://github.com/ghostty-org/ghostty reply klaushardt 16 hours agorootparentCould you post your config for inspiration? I dont like starting with a blank canvas. reply n42 15 hours agorootparentI just started using it when it launched 1.0. the stock configuration is basically perfect for me, with a few minor tweaks and a theme: confirm-close-surface = false macos-titlebar-style = tabs theme = IR_Black reply thiht 7 hours agorootparentprevMy Ghostty config: reply screcth 20 hours agoparentprevIt implements tmux control mode. It's very useful when working with a remote server. No other terminal implements it AFAIK. reply mbreese 19 hours agorootparentWhat does tmux control mode do in practice? I use both (iterm2 and tmux), but not for this specific reason. I have just used both as a default for a long time. So, what magic am I using without realizing it? reply ruined 18 hours agorootparenttmux metaphors are implemented in gui. tmux tabs are iterm2 tabs, tmux windows are iterm2 windows, etc. attach/detach and so on will restore layouts. i believe the session can even be shared with a normal tmux session. reply boomlinde 5 hours agorootparentYes, this was such a nice feature when I used a Mac. And indeed the session seamlessly works as a normal tmux session. I believe WezTerm does tmux-style terminal multiplexing, but doesn't integrate with tmux. reply dgacmu 20 hours agoparentprevI use the graphics support for making quick & dirty scrips for managing images (mostly for checking labeling and things like that where I don't want to bother creating a full web UI). I tried Ghostty for this but couldn't get the images to display as quickly or in full resolution, but it's very possible I was holding it wrong. I'd love to switch, honestly, if anyone has a recommendation for how to make it work as well as the iterm2-tools Python package. I also use multi-pane mirroring for managing some machines at home that I haven't bothered making more automated. reply liveoneggs 18 hours agoparentprevnative Terminal.app doesn't have true color support (24 bit colors). I was happy using it but wanted to try some fancy vim themes. I stopped using iterm after it did the chatgpt integration, which was a bridge too far for my tastes and landed on wezterm. All of the alternatives have nits. reply jamesgeck0 14 hours agorootparentIt's dumb, but Terminal.app is about eight years behind every other macOS terminal in supporting true color mode. Feels like sort of a table stakes feature in a modern terminal, and makes me suspect Terminal.app is not a high priority for Apple. The people want pretty editor themes! reply oefrha 13 hours agorootparentIt’s abandonware at this point. Even Windows Terminal is better now which is just embarrassing. reply xyst 17 hours agorootparentprevSame. I switched to Alacritty and later Ghostty. reply azinman2 17 hours agorootparentI just have been trying ghostty recently - the biggest usability issue I’ve immediately run into is a lack of cmd-f to search text (as far as I can tell) —— I’m having to copy everything into a text editor then search. I never realized how often I used that it until I lacked it. reply sweezyjeezy 16 hours agoparentprevI use it primarily for its split pane functionality. Invaluable if you need to see multiple things happening on the same machine at once. I work in data science and often have several long running jobs on a single server, a notebook server, htop/iotop, nvidia-smi, or simply just having different panes cd'd to different directories - with iterm you can organise to a single terminal tab for each machine (including local), or group tabs across machines if they are for related work. reply fletchowns 16 hours agorootparentHave you tried using tmux? reply sweezyjeezy 16 hours agorootparentYes I use them together, iterm has a great tmux integration. Tmux vanilla does not have great UX (in my opinion). reply chikere232 9 hours agoparentprevControl-tab can be set to cycle through tabs in recency order rather than the bad apple default of \"go to the next tab to the right\". It has far too much feature bloat though to the point that it's somewhat brittle, and I'd imagine there are many more lurking security issues reply chedabob 8 hours agoparentprevI just like having it docked to the top of my screen, and accessible via a global shortcut. I'm sure there's a thousand others that do both of those things, but I adopted iTerm2 about 10 years ago, and it hasn't given me cause to investigate others. reply cduzz 18 hours agoparentprevI got tired of weird sketchy seeming behavior in iterm2 (it'd hang randomly when accidentally sending binary streams to stdout; it'd take forever if I asked it to search my many tb of buffer, etc). I switched to xquartz and xterm, which was fine; I had to retrain my fingers back to whatever they'd been trained to do in 1998, but they got there. But probably that's terrible advice for 99.8% of people out there, probably more like 99.998, or even more 9s. reply kergonath 19 hours agoparentprevBoth are very competent terminal emulators. Just use either and you won’t miss much. I started using iTerm before the Terminal.app redesign (which was very much needed) and kept using it mostly for the tmux integration (which is a strong reason to use it). Nowadays, I would be happy using either. Ghostty is fine as well, though not as polished yet. reply chamomeal 13 hours agoparentprevIf the stock terminal app supported more colors (idk what the official term is lol) I’d be fine using it. I don’t use any fancy iterm features anyway. Fish + zellij is all I need! reply nickysielicki 13 hours agoparentprevWhen I’m forced to use something else, I miss eternal-terminal atop tmux control mode and iterm. It’s total bliss and there’s no other terminal emulator on any platform that can compare. reply citrin_ru 11 hours agoparentprevCopy on select is the feature I used to over the years (on BSD/Linux) and stock Terminal lacks it. reply fastball 17 hours agoparentprevI like the highly configurable quake mode in iTerm2. reply walterbell 19 hours agoparentprev> strongly compelling reasons to use [3rd-party terminal] Deniable (\"popular app\") increase in attack surface? reply pathartl 17 hours agoparentprevI stepped back into macOS for the first time in ~8 years and was disappointed by Terminal. To be honest, I think Windows Terminal is much better, which feels a bit weird to say. reply mr_toad 16 hours agorootparentTerminal came out 23 years ago, Windows Terminal in 2019. The old Windows terminal was garbage. I still prefer the blurred transparency of Terminal over the too transparent Windows Terminal, but that’s a matter of taste. reply nxobject 14 hours agorootparentHey, Terminal.app's been around ever since NeXTSTEP was born. Make it 35 years? reply freehorse 12 hours agorootparentprevPretty sure if it gets upgraded to become more modern somebody somewhere will also complain about it losing its simplicity. reply xyst 17 hours agoparentprevMitchell Hashimoto and group of contributors released a native terminal app recently - ghostty.org Well suited replacement for iterm2 and terminal.app, imo reply paulddraper 20 hours agoparentprevTmux reply otterley 18 hours agorootparenttmux runs just fine in Terminal.app. What else, then? reply nox101 18 hours agorootparenthttps://iterm2.com/documentation-tmux-integration.html reply BoingBoomTschak 20 hours agoparentprevTwo main reasons I switched is that iTerm can actually display bitmap fonts without mangling them (Terminal has anti-aliasing always on) and that it handles the difference between left and right Alt (needed because AZERTY layout + emacs). reply ibejoeb 19 hours agorootparent>Terminal has anti-aliasing always on There's a setting under Profiles/Text in the Text section. It's the first checkbox. Does that work, or is there a bug? reply eschatology 11 hours agorootparentprevI am using bitmap font with AA off in Terminal.app so this is incorrect reply teruakohatu 17 hours agoprevI feel deeply for the developer who develops iTerm for relatively very little money, and already took a lot of criticism for the AI integration, far more than was warranted. I am also also deeply concerned about my use of iTerm now. I access HPC environments where I may have access for a short period of time. I am expected to take responsibility to clear out my data after use and don't expect there to be any data leakage. If I had been manipulating PII research data in the past year and using iTerm's SSH integration I would be in a bit of a bind and have to send some really embarrassing emails asking sysadmins to see if these logs exist, and if they belong to me, followed by disclosing data had been leaked. I use some of the more advanced features but at this point wonder if I should be using any features beyond the basic, and then I may as well be using another terminal. I haven't found a cross-platform editor that feels as native on MacOS as iTerm, ghostty included. reply shwouchk 16 hours agoparentI highly recommend wezterm. reply rcruzeiro 11 hours agorootparentI tried WezTerm recently but I unfortunately could not type backslashes on an ISO keyboard. There were other minor annoyances such as new tabs always opening on the last directory I was at and not my home directory (this was something that could be configured, but I never managed to do it). Ultimately, it was the problem with the backslashes that drove me back to iTerm. https://github.com/wez/wezterm/issues/4051 reply VeejayRampay 9 hours agorootparentprevGhostty, Alacritty and Kitty are also very good options reply soheil 10 hours agorootparentprevBased on what? How do you know it's not riddled with major security bugs? At least iTerm has been around for over a decade and loved by many hardcore power users. reply 0xbadcafebee 15 hours agoparentprevFwiw, it's not your responsibility to maintain a secure computing environment (assuming you're a researcher). If you, personally, have to vet the whole system and all the software you use for security, then they have none. A competent system administrator with a knowledge of system security can easily configure a host so that when you SSH in, files you create are not given world-readable permissions by default. They can add other lock-down mechanisms that isolates all the users' files entirely. And they can simply disable all world-writeable folders like /tmp/. So in case anyone gives you (or anyone else) a load of crap about using insecure software, ask them why their systems are so insecure. reply faust201 12 hours agorootparentI assume you never worked in academia. Sometimes hpc are installed by researchers (incl myself) as there is no budget for IT staff. reply 0xbadcafebee 1 hour agorootparentStill not your responsibility. If they don't cough up the cash to properly manage the security, they can't expect any. Just because a trucking company doesn't want to pay for a mechanic, doesn't mean they can expect their drivers to repair blown engines. reply mattl 16 hours agoparentprevI use Prompt by Panic. reply keyle 12 hours agorootparentfrom the App store, the only place where you can get this app... Location This app may use your location even when it isn't open, ... just... why would a terminal emulator need my location... Not to mention the exorbitant price for a lifetime license. reply kalleboo 10 hours agorootparent> why would a terminal emulator need my location Because that's the only way Apple allows apps to stay open in the background on iOS so your SSH connection doesn't disconnect after 10 minutes. And the Mac app is a universal app with iPhone/iPad so it has the same permissions. If you never enable the \"Connection Keeper\" feature it never requests the permission. reply keyle 8 hours agorootparentThanks for this, I was unaware of that fact. TIL. reply kalleboo 2 hours agorootparentIt's really unintuitive! A lot of photo sync apps also have to use this workaround to be able to sync your photos in the background, it's been a long-standing issue with Apple's platform. And App Store rules means they have to justify the location permissions so they add a totally unrelated \"make a log of your location throughout the day!\" feature in the app just to get App Review to approve it, even though everyone knows that's not actually why they need it. reply winaiisbest 15 hours agoparentprevnext [3 more] [flagged] DannyBee 14 hours agorootparent??? This is a case of some debugging code not being deleted. See https://github.com/gnachman/iTerm2/commit/63ec2bb0b95078a97a... reply Slitted 9 hours agorootparentprevYou seem to have a bone to pick with this software or developer, so why not post a better reply than these throwaway comments from a new profile. reply winaiisbest 15 hours agoparentprevnext [3 more] [flagged] DannyBee 14 hours agorootparenterr, no https://github.com/gnachman/iTerm2/commit/63ec2bb0b95078a97a... reply soheil 10 hours agorootparentprevstop spamming the comment section or use your real account if you have something of value to add. reply rswail 11 hours agoprevThis thread reminded me to make my annual donation to iTerm2's developer, who does a pretty amazing job keeping iTerm MacOS compliant and up to date. reply mdaniel 1 minute agoparentIf one is already in the GitHub sponsors ecosystem, he accepts those too https://github.com/sponsors/gnachman reply nose-wuzzy-pad 8 hours agoparentprevThis thread reminded me to make a donation as well! reply _0xdd 5 hours agorootparentJust did the same. I've used this app for years and benefitted from its features. It's a shame to see some of the comments in this thread. reply xucheng 16 hours agoprevMany years ago, I reported an issue where iTerm2 leaks sensitive search history to preference files [1]. The issue was quickly fixed. But until this day, I can still find people unintentionally leak their search history in public dotfiles repos [2]. [1]: https://gitlab.com/gnachman/iterm2/-/issues/8491 [2]: https://github.com/search?q=NoSyncSearchHistory+path%3A*.pli... reply loeg 17 hours agoprevThis was only for the SSH integration, not if you just ran \"ssh\" in iTerm? I don't see these /tmp/framer.txt files on any of the hosts I sshed to (plain ssh). reply teruakohatu 16 hours agoparentFrom the release notes it seems only if the in-built SSH integration was used and the server that a more recent (5ish year old or later) version of Python. The latter condition is probably going to exist even on enterprise dists (RHEL 9 for example will have Python 3.9 installed by default) reply welder 8 hours agorootparentThe latter condition must not always trigger... I have Python 3.7 or later and none of my servers have a `/tmp/framer.txt` file. reply jey 20 hours agopreviTerm2 increasingly seems too complex and bloated to me, with too many security issues. I haven't shopped for a new terminal emulator on macOS in a long time, but perhaps it's now time. I should also get around to switching to tmux, now that GNU Screen seems to be stagnant... reply retrofuturism 20 hours agoparentI recently gave Ghostty a chance and have since switched over from iTerm2 completely. It's very familiar and polished. reply akerl_ 20 hours agorootparentProbably worth noting that Ghostty was very recently vulnerable to an old/familiar class of terminal vuln that bit a bunch of older terminal applications a while back: https://dgl.cx/2024/12/ghostty-terminal-title So moving to a newer / less \"bloated\" terminal may also just wind the clock back and cause you to encounter a similar sequence of vulns again, like some kind of unfortunate real-world \"New Game Plus\". reply jolux 19 hours agorootparentHaving a vuln that many other terminal emulators have had is pretty different from the string of unique and extremely bad vulns that iTerm has had over the years. It’s possible that we’ll see similar from Ghostty, but it’s a much newer and I believe smaller codebase, so I’m willing to give it a second chance. reply akerl_ 19 hours agorootparentI've been using iTerm daily for something like a decade at this point and I'm struggling to think of any examples of this string of extremely bad vulns. There's this one, which is specific to SSH integration. There was CVE-2024-38396, which is the window title escape sequences I was talking about above. What others am I missing? reply jolux 17 hours agorootparentHow about making DNS requests for everything you hover over to determine if it’s a URL? https://www.bleepingcomputer.com/news/security/iterm2-leaks-... reply draven 11 hours agorootparentA vuln every 7-8 years is \"a string of unique and extremely bad vulns\"? I use iTerm2, mostly because that's what I'm used to: I installed it on my first Mac years ago when Terminal.app was really bad. I'm willing to switch to another terminal, but I don't see yet how iTerm2 is so much worse than the competition security-wise. (I also don't understand the general animosity towards an opensource project with one developer doing all the work for 15 years.) reply ahefner 17 hours agorootparentprevnext [2 more] [flagged] ahefner 14 hours agorootparentnext [2 more] [flagged] saagarjha 13 hours agorootparentNo, it's just that you're exceptionally rude. reply boomlinde 5 hours agorootparentprevI can only find three CVEs prior to this. It's only one of those that I would qualify as \"extremely bad\" (the DNS query leak you mention below). The others are the window title bug GP mentions and the undocumented maintenance of a plain text search history file. reply samatman 19 hours agorootparentprevGhostty has also been out for what, a week? So this is the open season / shakedown, when security researchers get to try out all the old favorites and see what got missed. I don't think there are larger lessons to draw from that occurrence. A reputation for security has to be earned, and Ghostty hasn't been around long enough for that. From my vantage point it's on track, but only time will tell. reply akerl_ 19 hours agorootparentI'm not trying to knock either Ghostty in particular or new software in general. But the kind of \"open season\" phase you're referring to is basically the same point I'm making: new software still has to go through the phase where they work through their security model, have it probed by researchers, and earn their reputation. reply crabique 20 hours agorootparentprevUnfortunately, it's nowhere near close feature-wise just yet: proper quake mode, search, prompt navigation, line timestamps, tab output indicators, forced keyboard locales, customizable toolbar with user-defined variables/indicators, are all too useful to give up iTerm2 for anything. reply __float 19 hours agorootparentWhat is \"proper Quake mode\"? Have you tried https://ghostty.org/docs/config/keybind/reference#toggle_qui...? (I don't have the habit to use this feature, but I'd like to!) The others do sound useful too -- I personally hit a lot of spurious \"tab output indicator\" notifications in iTerm2, but if it _did_ work I could see how giving it up would be painful. reply crabique 17 hours agorootparentProper quake mode is just one shortcut to show/hide the entire terminal window, otherwise the app is completely hidden from the app switcher and the dock. This also involves handling macOS keyboard-to-app layout mapper not reacting to this event as the window loses its first-class citizen status in this mode. reply Geezus_42 19 hours agorootparentprevI got the toggle working, but couldn't get background transparency. reply cosmic_cheese 18 hours agorootparentprevI tried iTerm’s quake mode after the Visor haxie for Terminal.app was shuttered, but unfortunately was left disappointed. Its behavior is kinda glitchy and inconsistent in comparison, which was surprising because one would expect a native feature to be better than one hacked in by a third party, but that was not the case here. reply lilyball 19 hours agorootparentprevWhat's not proper about Ghostty's quake mode? reply Aeolos 19 hours agorootparentIt does not support tabs, for one. reply dave4420 20 hours agorootparentprevI tried Ghostty earlier in the week, but couldn’t get it set up to look the same as iterm2 (the colours are off and text looks different somehow with the same typeface at the same size). Which is just cosmetic but makes it feel wrong viscerally. I’ll give it another go at the weekend. reply crabique 20 hours agorootparentSomehow this is the first time I see anyone else bring this up, but the fonts are absolutely displayed with wrong kerning on my mac, for my font (at 12pt) I was able to make it look the same as iTerm2 with adjust-cell-{width,height} both set to -5%. reply Hamuko 12 hours agorootparentI had an issue where Ghostty failed to render the text \"th\", so I think Ghostty just generally has a lot of font issues. Gave up trying to use it within minutes. reply johnthedebs 19 hours agorootparentprevI noticed the colors looked off when I was trying out Ghostty as well. Adding this line to my config fixed it after restarting the app: `window-colorspace = display-p3` https://ghostty.org/docs/config/reference#window-colorspace reply dsego 9 hours agorootparentprevI used font-thicken = true and it looks pretty good, very similar to iTerm. reply samatman 19 hours agorootparentprevGhostty supports iTerm themes, so you should be able to transfer your preferred look-and-feel directly. I just picked one I liked, so I'm not familiar with the process, but it can be done. reply d4rkp4ttern 4 hours agorootparentCould you please point to the docs about how to transfer iTerm2 themes to Ghostty? I couldn't find it. reply samatman 1 hour agorootparentIt looks like the process is more manual than I thought, sorry. There's something which imports themes from the iTerm2 color schemes website weekly, but from what I can find that isn't a feature which ships with Ghostty itself. Here's the relevant docs page, which I hope explains why I mistakenly thought that transferring a theme directly from iTerm to Ghostty was possible. You could upload your theme to the website they're being sourced from, and wait a week. But that's clearly not the same thing. https://ghostty.org/docs/features/theme reply kergonath 19 hours agorootparentprevI am using both at the moment, but iTerm has many, many features missing in ghostty. Most of them are not huge, but overall that’s a lot of paper cuts. Ultimately I wish both will stick around. Both have good ideas and I’d like them to keep evolving. reply segasaturn 19 hours agorootparentprevI tried Ghostty but the configuration file seems to be flakey. For example I wasn't able to turn off the cursor blinking in it (huge distraction for me) despite there being a configuration flag for it, it just wouldn't take it. reply auscompgeek 19 hours agorootparentThere's a known issue where the shell integration doesn't respect that setting when it sets the cursor shape. https://github.com/ghostty-org/ghostty/discussions/2812 reply dsego 9 hours agorootparentprevHave you tried shell-integration-features = no-cursor ? reply dsego 9 hours agorootparentprevI also switched over to Ghostty, but now can't do Cmd+F to search for strings, I think it's on the roadmap though. Also, there are no scroll bars on the Mac version, which I guess is not that important for a terminal. reply woadwarrior01 20 hours agorootparentprevI did the same. Although, Ghostty doesn't seem to have support for Find (⌘-F), yet. Also, had some minor hiccups with it and tmux on remote hosts. reply NovaX 18 hours agorootparentI've resorted to using Cmd-Shift-J (scrollback buffer) and grepping that, but its flaky about whether it will honor the command and emit a history file. reply rcpt 20 hours agorootparentprevI tried Ctrl+r while she'd and it didn't work. Otherwise ghosty looked great reply FreePalestine1 17 hours agorootparentprevDoes Ghostty support natural text editing like iTerm2 does? reply hmeh 20 hours agorootparentprevSame. So far so good. reply throwaway314155 19 hours agorootparentprev> It's very familiar... It is? Because as far as I can tell it is deliberately quite different from iTerm2. No GUI for preferences, for instance. Not knocking it, it's a great terminal. I wouldn't describe it as \"familiar\" though, unless you're switching from, say, WezTerm or Alacritty. reply tylersmith 19 hours agorootparentprevGhostty hasn't had to put out a critical update since before 1.0 launched. reply Klonoar 20 hours agoparentprev“Too complex” and “bloated” are catch-all that you should consider expanding further on. I don’t personally iTerm2 to be be either of those. reply kergonath 19 hours agorootparentIt is a bit slow, though. reply lolpanda 18 hours agoparentprevI'm a heavy use of tmux integration in iterm2. this allows seamless mouse scroll in a tmux window. I haven't seen any other terminals that provide the same tmux support. reply jitl 17 hours agorootparentWezterm doesn’t have tmux integration but instead implements multiplexing natively, meaning if you install it on a remote, it will host a mux server you can attach to over ssh. Pretty cool, and much faster/lower latency than tmux. reply dylan604 19 hours agoparentprevI have used Terminal.app since 10.0, and have never felt like it needed replacing. What is lacking in Terminal that would improve my day to day by using a different app? reply tom_ 18 hours agorootparentThat's a question that only you can answer. We have no idea what your average terminal activity involves. (I can't remember why I switched. It must have been 10 years ago now, maybe more, and I've stuck with iTerm2 ever since (even though it annoys me with a new beta update practically every time I launch it). It could have been nothing fancier than the vertical window split. But there was definitely something that persuaded me to change!) EDIT: this did get me wondering, and I noticed two things it does have that it looks like Terminal still doesn't: configurable mouse selection word boundary chars, and implicit copy-to-clipboard on selection. As an inveterate mouse selector, I wonder if it was these. I might well actually have the word boundary chars still set to the default (\"/-+\\~_.\" is what I've got), but I do use the click-to-copy a lot. reply saagarjha 13 hours agorootparent> even though it annoys me with a new beta update practically every time I launch it Why don't you just use the stable releases? reply Vegenoid 19 hours agorootparentprevIn my opinion, the most notable feature missing from Terminal.app is 24-bit color support. This is a standard feature in modern terminal emulators, and is one that I enjoy very much. But for many people, that is not a feature that makes a big difference. reply charrondev 17 hours agorootparentprevFor me it’s 3 major things: - split pane support - profile switching (I have my colors change for different environments I ssh into). - tmux integration reply jonstewart 18 hours agorootparentprevThere's a mini-renaissance going on with new terminal tools, like tmux, neovim (which has an ecosystem of plugins itself), htop, and many more (https://github.com/rothgar/awesome-tuis). They take greater advantage of 24-bit color, \"nerd\" fonts (that have icons for glyphs), some graphics capability, and so on. I used Terminal for many years, too, but switched to iTerm2 a little over a year ago as I wanted to learn neovim. reply Fnoord 18 hours agoparentprevStill use GNU Screen? Both GNU Screen and tmux had security issues in past, but GNU Screen had worse ones and that is why I switched. Zellij is a Rust terminal multiplexer, might wanna look into that. What I especially love about it, is tge discoverability of the keybinds. TUI wet dream. reply icedchai 19 hours agoparentprevI've considered \"screen\" complete since I started using it over 30 years ago. reply jey 17 hours agorootparentI recently ran into a bunch of problems running neovim under GNU Screen with `TERM=screen.xterm-256color`. There was some kind of problem relating to GNU Screen's parsing and re-transmit of certain full-color terminal escape codes. I don't remember the details right now, but what I know is that building the latest from source didn't help. (I wish I could remember the specific issues) reply Aloha 19 hours agorootparentprevSame - I'm at about 20 years give or take. It also has every feature known to exist in this space. I agree though that the world is moving in the way of tmux - I'll get around to switching occasionally. reply cozzyd 19 hours agorootparentVertical splits? (I know they can be patched in) reply baq 20 hours agoparentprevI’ve been using tmux for over a decade because screen was a bit on the legacy side back then reply zenapollo 20 hours agoparentprevWould xpipe be a candidate? It’s also quite feature packed, but i was pleasantly surprised how nicely it got out of my way reply 2OEH8eoCRo0 20 hours agoparentprevI don't use Mac but what's wrong with the default? > GNU Screen seems to be stagnant Is it stagnant or mostly complete? reply brabel 19 hours agorootparent> I don't use Mac but what's wrong with the default? Nothing, it works great. As someone who tried a bunch of alternatives: sorry but it's a waste of time unless you look at the long list of iterm2 features (terminal.app has many of them anyway) and think you might use those often (I don't, quite happy with my other apps covering for most of the features missing from the terminal.app): https://iterm2.com/features.html reply lelandfe 19 hours agorootparentprevI try out the new apps each year and always go back to Terminal.app. My one gripe with it was that Opt-Del didn't delete words, which I learned last year can be fixed by Preferences > Profiles > Keyboard > Use Option as Meta key Long live Terminal.app reply betaby 20 hours agorootparentprevComplete I would say. However Mac uses GNU software from around 2006, since around that time a lot of GNU software switched from GPLv2 to GPLv3. That means Mac ships GNU screen version 4 from 2006, while latest version is 5. reply walterbell 19 hours agorootparentv4 2006: feature complete, survived 18 years of attacks v5 2024: new auth functionality, survived 4 months of attacks reply JohnBooty 19 hours agoparentprevI haven't looked super hard, but an iTerm feature I \"gotta have\" is triggers - regexps that match lines of text in the terminal and do an action. You can do some complex stuff with them, but I \"just\" use them to highlight specific things when tailing output. Some of it might be possible with grep, but probably not reply mhink 19 hours agoparentprevI really like alacritty + tmux, personally. reply waynesonfire 17 hours agoparentprev> GNU Screen seems to be stagnant... Not at all, it just had a release a few months ago, GNU Screen v.5.0.0 is released posted by anaumov, Wed 28 Aug 2024 09:41:30 PM UTC reply paulddraper 20 hours agoparentprev> now that GNU Screen seems to be stagnant... That's not a new thing... reply Aloha 19 hours agorootparentWhat else does it need? I get old code smell, and why folks might want something architecturally different, whatever - but screen is functionally feature complete. reply slavomirvojacek 20 hours agoparentprevI switched to Warp, much snappier, some AI features, overall very good experience. Also Ghostty is apparently good. reply johnsonalpha 15 hours agoprevI’m a bit confused by the suggestion to \"Just don’t use iTerm2.\" The reality is that this type of issue could happen with any project, and switching tools doesn’t provide meaningful protection. If anything, incidents like this often lead to stronger security practices. It’s like the old joke about firing an engineer after a mistake, and the manager responding, \"Why would I fire them? They’ve just learned a lesson they won’t forget.\" Based on iTerm2’s track record, it doesn’t seem like they’ve had frequent critical security issues, and I doubt they’ll repeat this mistake. If they do, then it’s fair to reassess. As for the MacOS Terminal app, it might seem like a lower-risk option because it’s simpler and updates less frequently. However, being closed-source makes it impossible to audit, which brings its own risks. Ultimately, every tool has tradeoffs, and choosing the right one depends on balancing your needs with the potential risks. reply epistasis 15 hours agoparent> could happen with any project, and switching tools doesn’t provide meaningful protection Do you believe that developments practiced have an impact on security bug rate? Second, do you believe that past track record is reflective of that security bug rate? These are two reasonable beliefs that many people hold. It's a far more nuanced view than \"every project could have bugs\" which is a black-and-white view that does not assess risk in a useful way. reply SamuelAdams 19 hours agoprev> Delete /tmp/framer.txt on affected hosts. Isn’t the correct fix to assume compromise and rotate all SSH keys? I imagine there will be scripts created very quickly to grab this file from any servers, so even if it is deleted soon there is no guarantee someone else has not read it. reply saghm 19 hours agoparentI'm not sure I follow. From what I understand, the issue is that literally all of stdin and stdout is potentially leaked to a globally-readable text file. ssh-keys wouldn't normally be part that leaked information (other than if you `cat` your private key, but by that logic, literally any credentials ever stored in a file or potentially even an env var would be equally susceptible). Yes, anyone with ssh access would have access, but nothing about this vulnerability seems to imply it would allow an unauthorized user to gain ssh access. It doesn't sound like this is something with a specific risk of leveraging into ssh access because the only ones who can read the file are the ones with access in the first place, so there's no more reason to suspect that now compared to any other point in time. reply varenc 18 hours agorootparentAgreed. I believe this means just means that the entire contents of your ssh session is available in /tmp/framer.txt. But as long as credentials aren't part of your STDIN/STDOUT, then they shouldn't be leaked. Your ssh private key definitely would never be in there. The server you're connecting to doesn't even know your private key, just the public one. reply m1keil 15 hours agorootparentSo Sudo password for example seem to be up there. reply saghm 2 hours agorootparentThat's an interesting point; I think it might depend on the exact details of how stuff is getting put into that tempfile. It's possible that anything getting read in via \"silent mode\" (see `-s` in https://ss64.com/bash/read.html) might not get put into that file, but given that this was a bug rather than a feature, it would probably be better to check the source code (or try an example with an unpatched version) to be sure. reply egorfine 6 hours agopreviTerm2 is the app I spend the most time for like a decade or so. I feel bad for the developer. This is embarrassing and it totally could and probably will at some point happen to the best of us. So I have immediately donated and subscribed to monthly donations and I encourage everyone to do so. There should be zero doubt that the author deserves our support. reply wk_end 20 hours agoprev> A bug in the SSH integration feature caused input and output to be logged to a file on the remote host. This file, /tmp/framer.txt, may be readable by other users on the remote host. Curious about how this happens. What does \"framer\" mean, here? reply CameronBanga 20 hours agoparentHere's the commit where it was reversed, if you want to take a look and dive in. Looks like unfortunately a logging feature that he has was set to 1 instead of 0 and wasn't reset before compiling. https://gitlab.com/gnachman/iterm2/-/commit/014ba7ec40fc790f... reply formerly_proven 20 hours agoparentprevSounds like something dropped in the code for debugging purposes and accidentally released. reply isatty 19 hours agoprevOn the bright side this made me realize the stock terminal app has improved. I do use iTerm2 for its better rendering of text and color (and easier configuration of those things) but I don't really make use of any other features. Time to switch, perhaps. reply ryanmccullagh 18 hours agoprevWhy does a terminal need an SSH integration. Answer: it doesn’t and you shouldn’t use this because it is unsafe. reply ldjb 18 hours agoparentA terminal doesn't need SSH integration, but it's convenient if it does, to allow you to easily start and manage connections. Is there something inherently unsafe about such an integration? reply Xelynega 18 hours agorootparentThere's something inherently unsafe with replacing an industry-standard security tool with anything. I don't think it's inconvenient enough to type `ssh -i key_file name@host` that we need to be creating more security risk to skip typing it. reply wkat4242 13 hours agorootparent> I don't think it's inconvenient enough to type `ssh -i key_file name@host` that we need to be creating more security risk to skip typing it. Also, you can easily configure that in your .ssh/config file, even with different options per host or group of hosts. reply nerdponx 18 hours agoparentprevNobody needs anything ever but sometimes things are useful. reply noisy_boy 16 hours agorootparentSurely a few bash aliases for the same purpose are equally useful and more portable? reply saagarjha 13 hours agorootparentI mean that's literally what terminal integration is, right? A bunch of bash aliases that the terminal emulator knows about. reply paxys 20 hours agoprevThat sound you hear is IT admins worldwide scrambling to delete /tmp/framer.txt from all their servers. reply rswail 11 hours agoprevI've used iTerm2 for as long as I've known about it, which would be maybe 10 years? I don't use much of the various SSH/mux features, 'cos I don't use multiple buffers, just multiple tabs. I like the scrollback and the footer and the integration with the shell, don;t care about scrolling speed very much, and it's sort of the \"ain't broke, so why change\". I'll take a look at ghostty, but not sure it gives me much. As for this security issue, it's a bug, the author found it, fixed it, announced what it was, and how to ameliorate the effects of the issue. He did that in a very reasonable timeframe and has been entirely open about it. The pile-on of moralists and what appear to be purists (and possibly early stage devs if they think process is the answer) is sorta pathetic. This entire thread is more twitter/reddit than what I've come to expect on HN. reply MiscIdeaMaker99 20 hours agoprevI would love to know more about how this got discovered and figured out. I can imagine some sysadmin pull their hair out, thinking they've got some infected system, but then find out it was some bug with their terminal emulator. reply walterbell 19 hours agoparent> got discovered and figured out Unapproved usage of the exploit? reply NelsonMinar 20 hours agoprevHow does a bug like this last for so many months without being noticed? Did no one notice a weird file in /tmp and wonder where it came from? The one with their ssh session history in it? reply runlevel1 18 hours agoparentPerhaps not that many people were using `itssh` or replaced the initial profile command with SSH + SSH integration. reply zamadatix 19 hours agoparentprevGiven the purpose of the /tmp directory it seems an unlikely source for one's afternoon reading. I wonder what % of iTerm2 users use this integration feature as well. I didn't even know it existed. reply kccqzy 12 hours agoparentprevIt sounds like a rarely used feature of iTerm2. I have not even heard of it despite using iTerm2's shell integration and tmux integration for a long time until I switched to Linux. reply lionkor 9 hours agoprevNot sure what the replies here are on about. This is NOT a \"whoopsie, can happen to any project\" bug. There was code in the project that EXPLICITLY leaked stuff into the remote host. Am I missing something? Not only would switching to a different project with more eyes on it probably never do this, it would also probably never let that through PR reviews. reply thih9 8 hours agoparentAre you offering to review PRs of a different project? That’s actually very beneficial. Which project are you planning to support like this? reply cdeevgtg 8 hours agoparentprevOnly when verbose logging is enabled, which seems fine to me What's not fine is verbose logging being turned on by default, most likely by mistake reply eximius 19 hours agoprevhah! Clicking \"Remind Me Later\" for the update prompts works again! reply muppetman 9 hours agoprevI thought we were all losing our mind over Ghostty anyway and iterm2 wasn't cool anymore because it's, apparently, slow? reply hbbio 15 hours agoprevI always preferred alacritty which is faster and hopefully safer. Tha macOS part uses the rust `objc2` crates which I find high quality and the codebase is a joy to read. reply soheil 10 hours agoprevI would advice anyone using iTerm not to willy-nilly switch their terminal to one recommended by a random user here. Terminals can have a huge attack surface and many \"open-source\" ones are maintained by less than trustworthy developers who very easily could inject a backdoor. Sticking with time-proven projects like iTerm provides the advantage of added trust, security and basic common sense. It also seems like a huge coincidence that there are a lot of green accounts here \"highly\" recommending all sorts of random terminal alternatives. reply decasia 17 hours agoprevI'm confused by the comments saying \"Just don't use iTerm2.\" The same class of issue can occur for any other project, and switching is not a very effective defense against it. If anything, having an embarrassing issue like this is probably going to improve the iTerm2 project's security posture in the medium term. It's like that joke about firing the engineer who caused the incident, and the manager who retorts, \"Why would I fire them? They just learned the hard way never to make this mistake again.\" (I'm paraphrasing.) I don't think that iTerm2 has had a notably high rate of critical security issues, and I suspect they won't make this class of mistake twice. (And if they do - then I will re-evaluate.) I suppose intuitively I would think that using the default MacOS Terminal app is a bit lower-risk than using iTerm2 or any other open source terminal emulator, as Terminal is a rather sparse piece of Apple-provided software with a low pace of change. But it's also closed source and impossible to audit, so there are tradeoffs there too. reply wkat4242 13 hours agoparentI don't think Apple terminal is sparse at all. Try to dig through the preferences, there's surprisingly much choice for an opinionated software vendor like Apple. I love it, I've tried iTerm2 in the past but I just saw no reason to keep using it. The only feature that slightly appealed to me was the tmux integration but I found that i preferred to just have tmux inside the terminal instead. I know some people that use the game-like rolldown interface (quake mode?) but I also don't like/need that one. There's a few niche things like that which make it interesting. But overall I just don't see the need. If there's something that's sparse in options, it's Windows Terminal. Don't like that one at all (though it's better than the previous command prompt window). reply danyala 7 hours agorootparentGhostty has the quake mode interface and has allowed me to overcome my shame for never graduating to iTerm. I can't remember much about the game except the giblets but hopefully I've understood. reply alwyn 7 hours agorootparentprevThe majority of configuration options in Windows Terminal are hidden in JSON. The GUI only exposes a fraction. They are doing work to expose more options in the GUI though. Having tried various alternatives for prolonged periods, it is currently IMHO the best option when you have to work on Windows. Mac and Linux options are just vastly ahead. reply wkat4242 7 hours agorootparentAh I see. I only used it since the beginning and I hated it and didn't go back. Certainly didn't explore json options :) Thanks! I really hate working on windows too and our company is tightening stuff down so crazily that I can hardly work anymore so I mostly work on my home lab in Linux and transfer stuff there. Totally not allowed by my employer but they make it impossible for me to work otherwise. reply johncoltrane 8 hours agorootparentprevI used iTerm2.app for a while, back when Terminal.app didn't support 256 colors. That was literally the only reason and I ditched it the day Terminal.app got 256c, many years ago, now. reply joshka 7 hours agorootparentIt however doesn't have 24bit color still reply johncoltrane 6 hours agorootparentWhich is not an issue for me as 256c is my sweet spot. reply wkat4242 7 hours agorootparentprevMeh that's kinda overrated. There's only so many colours you can discern in individual letters. reply joshka 7 hours agorootparentSure, the issue is that it doesn't gracefully degrade when presented with 24bit color. reply sengifluff 5 hours agorootparentTmux does, though! I used iTerm2 for a while before realising that tmux automatically maps 24-bit colors to 256color. It works well enough for me that I switched back to Terminal.app. reply johncoltrane 6 hours agorootparentprevWell, assuming 24bit—or anything from a terminal emulator, really—is not a very bright strategy. reply crest 5 hours agorootparentDepends on your colour palette :-P. reply vouaobrasil 8 hours agorootparentprev> I've tried iTerm2 in the past but I just saw no reason to keep using it. One feature that's cool in iTerm2 is that you can define the left and right margins so using Vim full-screen looks nicer (I hate narrow margins). But I've switched to Sublime text for everything so I stopped using iTerm2. reply frizlab 8 hours agorootparentprevI love Terminal. I tried iTerm2 but never got the appeal and switched back to Terminal. reply Kwpolska 9 hours agorootparentprevWindows Terminal has a ton of options these days. Last time I used Apple's, it had very US-centric handling of Alt/Option, unlike iTerm2. reply boomlinde 15 hours agoparentprevI am inclined to say that a low pace of change isn't necessarily a problem for something that at its core emulates 70s hardware. It can easily become a problem if you don't leave it at that and add everything and the kitchen sink to it. Not having used the feature I have a hard time imagining why a terminal emulator should have SSH integration to begin with. reply kmacdough 8 hours agorootparentThe comment puts the slow pace of change as an advantage. Pretty standard security principle. Ssh integration brings lots of other \"local\" iterm features based on command history, etc. to the ssh environment. If you do lots of ssh work and use a lot of relevant iterm features then it's nice. Otherwise not. Obviously, these features can be recreated with traditional tools. But these tools take time and experience to setup, and aren't naturally intuitive, despite the insistence of emacs elitists. For those who don't want to invest significant effort learning tmux, it's really nice to just check/uncheck a few boxes. reply saagarjha 14 hours agorootparentprevHardware in the 70s was full of security issues. Designing a secure terminal emulator is actually quite difficult. reply boomlinde 6 hours agorootparent> Hardware in the 70s was full of security issues. It's however clearly not in the \"70s hardware portion\" of iTerm that this issue arises. Also not in the features we've come to expect of the most bare bones terminal emulators since, like Unicode support, scrollback buffers and font rendering, or even the somewhat gratuitous features like escape sequences for RGB colors, setting the window title or sixel rendering. This is very clearly one of the kitchen sink features, and playing the devil's advocate I should say that it reflects poorly on the kitchen sink design ethos. reply internet2000 17 hours agoparentprevThe built-in Terminal is a) simpler, smaller attack surface b) older c) supported by probably more than one person, in a more risk-averse org d) used by a lot more people so issues will get noticed and fixed sooner e) part of a pretty robust public beta OS program. Just don't use iTerm2. reply hombre_fatal 15 hours agorootparentPoint D is a thing we assume about software but over time you realize it's not the case. You'll see a native macOS app like Terminal and assume it's blessed by Apple, has its own dedicated team with daily standup and meetings, but it's actually just propped up by random tickets in a larger issue tracker, doesn't have a dedicated team, and depends on what amounts to begrudged work or charity work by whoever happens to work on a ticket in passing. A random rockstar will come in and move the code forward 5 years never to be seen again. reply anon7000 17 hours agorootparentprevAnd best of all, not as good, and not open source! reply sixothree 15 hours agorootparentWindows Terminal is though! reply wkat4242 13 hours agorootparentI really dislike that one. Not enough configurability. Unlike Apple terminal which is kinda surprising because normally Microsoft is the one with all the option settings and Apple makes the opinionated software where they decide everything and the user has to take it as it comes. But Microsoft is becoming similar unfortunately. You can see it in other software too, like them discontinuing the real Outlook and replacing it with a web one that has much fewer options, can't even be started up offline (!) and wastes more screen space. And they are moving more and more apps to electron or their own knockoff of it. reply eigenvalue 13 hours agorootparentWindows terminal is great. Very performant and looks really nice. I also have mobaxterm for doing work remote machines, but for local work using WSL Ubuntu it’s my preferred terminal by far. reply jazzyjackson 57 minutes agorootparentHow do you judge performance of a terminal ? I know graphics in the terminal is an edge case and all but when I'm running a chafa [0] application Windows terminal on Windows is about 1000x slower than alacritty on mac, I run into issues where if I hold down a key all the key events get queued up as if the graphics thread is blocking the applications ability to react to key events, so I just have to baby it and hit one key at a time. [0] https://hpjansson.org/chafa/ reply wkat4242 7 hours agorootparentprevOh I used it only when it first came out. I went back to PuTTY (i never use the console on windows for other stuff and I'm not allowed to use WSL at work). And at home i only have a windows box for gaming. So there was some assumption on my part sorry. But informed assumption because other stuff I work with (Teams, Outlook) have only become much worse since their release :) reply jazzyjackson 55 minutes agorootparentTo be a fly on the wall when they decide to go live with a Teams (New) that asks if you want to keep using Teams [not new]. reply Lammy 9 hours agorootparentprevOpen source — monkey's paw curls — but spies on you https://github.com/search?q=repo%3Amicrosoft%2Fterminal+tele... reply wongogue 7 hours agorootparentThat’s anything Microsoft makes these days. reply behnamoh 12 hours agorootparentprevwe're talking about Mac terminals. reply nikau 16 hours agorootparentprevYou forgot f) so lacking in features that it's a productivity roadblock reply lolinder 15 hours agorootparentWhat missing features do you see as a roadblock? I installed iTerm2 on my work Mac because it came so highly recommended, but I honestly never remember to open it over the regular terminal. ~All of the features that matter to me in a terminal are features of the shell and the OS, not of the emulator itself. reply pama 13 hours agorootparentIterm is better documented. Try finding how to pass Ctrl-Meta-key; better yet, throw in a shift as well. Setting up 24bit color, supporting italics, and allowing Emacs keystrokes to go through to the remote server at will is a quality of life improvement. I managed to be feature complete with iterm on macOS with minimal effort, but not with the default terminal, and there was no source code I could check to help me avoid trivial annoyances. Xterm/rxvt from xQuartz on Mac OS are easier to work with than the default Mac OS terminal, and feel closer to their behavior in Linux, but then you have some unneeded X window features and miss other trivial automation features like multiple tabs. reply eptcyka 12 hours agorootparentiTerm2 is slower. It feels way jankier with nvim than Console, kitty or literally anything else. I do not believe anyone should be using iterm2, given their history of security issues. All of them leave me scratching my head as to why did anyone think that designing it the way they did is a good idea. reply pama 9 hours agorootparentI wanted to like kitty and tried it many times. It is subtle issues that break Emacs now and then, like breaking the display alignment for some zero-width joined emoji. Iterm2 on a MacBook is snappy for me. With remote work, the latency for me is mostly network delays of order a couple of ms per keystroke for the cabled Ethernet connection; mosh helps for the extreme cases, or when on WiFi (which often feels annoying without mosh), otherwise ssh -C is sufficient for my daily driver. reply dangus 11 hours agorootparentprevThe idea that iTerm is noticeably slow is hilarious. If you buy a base model grandma level MacBook Air it can play Cyberpunk 2077 without breaking a sweat and somehow terminal performance is an issue. And if all I cared about was raw performance I’d be using vim instead of VSCode. But raw performance isn’t what makes me productive. reply eptcyka 11 hours agorootparentBut it is. Scrolling in neovim is noticably slower on iTerm2. This makes it cumbersome to use. Even if it doesn't prevent me from inputting the keystrokes any faster, if it is cumbersome to use, it will make me work slower. reply soraminazuki 6 hours agorootparentI've used numerous terminal emulators on both macOS and Linux. I use Neovim daily. There is no noticeable difference in scrolling performance. Additionally, there are so many ways scrolling can slow down in Neovim (e.g., bad tmux config). It's hard to take your word for it that the issue lies in iTerm2 in the absence of any sort of reproducible evidence. reply eptcyka 6 hours agorootparentThere might be some tmux involved, yes. But the same tmux config works just fine with Kitty. Kitty also hasn't been leaking commands I've ran as DNS requests. Nor has it left my zsh history on a remote host. I don't care to investigate why iTerm2 works worse for me because I am satisfied enough with a solution that works. iTerm2 not working for me rhymes with my previous experiences with iTerm2 and its security issues. reply soraminazuki 6 hours agorootparentThat's uncalled for. Security issues are quickly fixed and released in iTerm2. The dev is responsive to feedback, even to hostile Mastodon trolls brigading the issue tracker [1]. Please don't be like that. Also, any serious software has its own share of problems. Have you actually looked at the issue tracker for your supposed champion? [1]: https://news.ycombinator.com/item?id=40657890 reply eptcyka 4 hours agorootparentWhat specifically is uncalled for? I disagree with the design choices made in iTerm2 and wish the best of luck to the developer(s) behind it. I am not wishing any harm, but I do have to say that the input latency is annoying and attempting to resolve words in command output to see if they are hostnames is a dubious technical solution. Am I not allowed to voice my opinion on this? reply soraminazuki 3 hours agorootparentIt was an oversight that was promptly fixed after the issue was raised. The dev created a post-mortem [1] and a wiki [2] describing the issue. Seriously, give the poor dev a rest. It's absolutely uncalled for to throw in a non-sequitur about some bug from 7 years ago, making snide remarks about how that's a \"design choice.\" [1]: https://gitlab.com/gnachman/iterm2/-/issues/6068#note_409052... [2]: https://gitlab.com/gnachman/iterm2/-/wikis/dnslookupissue reply likeabatterycar 5 hours agorootparentprevPeople are allowed to have preferences and dislike software. Similarly, if your mechanic forgets to tighten the lug nuts or leaves the oil cap off, and nearly kills you or destroys the engine, you are allowed to find a new mechanic without the Hackernews hoi polloi coming out of the woodwork saying how unfair it is, he has mouths to feed, and linking to critical Yelp reviews of your new mechanic trying to convince you of your own idiocy and wrongdoing. This emotional attachment to a piece of throwaway software here is frankly weird. reply soraminazuki 3 hours agorootparentThis over the top aggressive response to a bug in a passion FOSS project. That thing you just did is what I have issue with. People are allowed to have opinions. In the same spirit, others are allowed to call out inappropriate or toxic behavior. Also, > Hackernews hoi polloi coming out of the woodwork saying how ... he has mouths to feed Do you not understand what people mean when they say iTerm2 is free and open source software developed in a single person's spare time, and people aren't owed any of it? You didn't pay your metaphorical mechanic. Such bold sense of entitlement. What's even more unfortunate is your take on my previous comment: > linking to critical Yelp reviews of your new mechanic Let me be more clear. You'll find something to pick on in any FOSS software. When you bring it up, no FOSS community will tolerate the kind of attitude you put on full display here. Last but not least, > This emotional attachment to a piece of throwaway software here is frankly weird. Piece of throwaway software? Do words have no meaning to you? This is 15 year's worth of work that you're belittling. That work consists not only of coding, but coordinating with users and other software projects. I've seen him many times in issue trackers of various other projects. He's giving away all of that work for free. Imagine having to deal with people like you on top of all that. reply dangus 11 hours agorootparentprevAn entire thread of neovim users generally saying that the performance is acceptable: https://www.reddit.com/r/neovim/s/JOQL9e76fp Just because something else is faster doesn’t mean that iTerm is slow. It’s all relative. reply eptcyka 10 hours agorootparentSure, it is slow for me. reply BSDobelix 8 hours agorootparentprev>Just because something else is faster doesn’t mean that iTerm is slow. It’s all relative. If you are thinking about a change, console marketing would be a good place to start ;) reply wiseowise 9 hours agorootparentprevLatency during typing is a real issue, not sure what you find hilarious here. reply BlobberSnobber 10 hours agorootparentprevCyberpunk 2077 on a MacBook Air without breaking a sweat? At what resolution and framerate? reply dangus 4 hours agorootparentThe plain M4 chip will run Cyberpunk 2077 at 30FPS 1080p. (Andrew Tsai on YouTube) I did neglect the fact that Apple hasn’t thrown that chip in the Air yet, but I’m sure that’s only a few months away. reply BlobberSnobber 1 hour agorootparentAt ultra settings? Even if, 30 fps at 1080p is not nearly “without breaking a sweat”. Also, the air will have trouble keeping that performance after a few minutes without a fan. I love my MBP M4 Pro, but its gaming performance doesn’t reflect well what it’s capable of. reply dangus 23 minutes agorootparentThis is at High settings! And I haven’t even mentioned that the game is running via Crossover through multiple translation systems. That’s translating both Intel Windows to ARM Mac as well as translating the graphics APIs (DirectX or Vulkan to Metal). The cyberpunk native Mac release comes out this year and will almost certainly improve performance further. Why would anyone care about ultra settings on a laptop? I don’t even set my PC desktop to ultra settings in the game and I have a current generation mid-high end GPU. Setting demanding games to Ultra just giving up FPS to not tell the difference. 30fps 1080p is basically console-level standards for a AAA graphically intense game (not esports or online shooter). And that isn’t bad at all for the processor with integrated graphics that Apple sticks in its cheapest computer and its tablets. Your MacBook Pro M4 Pro is one of the best gaming laptops on the market in terms of hardware! Especially if you want something that’s thin, light, and quiet with good battery life and not just a thick tank of a system or a loud but thin and light gaming laptop that struggles to power and cool its dGPU. Depending on your configuration, you can actually play Cyberpunk at high settings at or above 60FPS on your laptop. You’re vastly underestimating it! Your laptop just needs the software to get ported, and the Mac gaming space is rapidly evolving now that Apple is paying attention to it. frizlab 8 hours agorootparentprevI don’t know about the rest, but I’m sure Terminal supports italic. reply yakshaving_jgt 13 hours agorootparentprevQuake mode. Terminal doesn't have it, and the recently released Ghostty's quake mode is slower than iTerm2. There's very little I want in a terminal emulator. What I really want is a full screen terminal, with no menu bar, no delay, and no animations, which I can toggle with a global hotkey. reply eadmund 12 hours agorootparentIt strikes me as a little odd for the terminal rather than the desktop environment within which it runs to implement the hotkey (or, as you call it, ‘Quake mode’). I just have my tiling window manager configured with a keybinding to raise my terminal. No menu bar, no delay, no animation, just type the keybinding and bam, there’s my console, covering the complete screen. Another keybinding, and there’s my browser. Another keybinding, and there’s my editor. reply yakshaving_jgt 12 hours agorootparentHow are you achieving this in MacOS? reply eadmund 7 hours agorootparent> How are you achieving this in MacOS? I don’t — I use Linux on my desktop. I stopped using macOS back when it was called System 8 or 9! I think any tiling window manager can be configured to do this, but in my case I use StumpWM. (defcommand terminal () () (run-or-raise \"urxvt\" '(:instance \"urxvt\"))) (define-key *top-map* (kbd \"s-t\") \"terminal\") With the above, when one types Super t then the terminal either is raised to the top, or starts (and runs on top). From others’ comments, I think that this is probably possible with a modern Mac, too, but I find that Linux is generally easier. StumpWM: https://stumpwm.github.io/ reply yakshaving_jgt 7 hours agorootparentThat sounds great, but I'm not changing my entire operating system just so I can have a possibly marginally better terminal emulation experience. reply eadmund 5 hours agorootparentYou would also get a better window manager, better compatibility with server operating systems, a bash updated this decade, XCompose (think the Option key, but way, way, way more powerful) and more freedom, but on the other hand you’d lose macOS-only programs, and from time to time would have to deal with something truly frustrating which would never be an issue on a Mac. It’s certainly not perfect, but I do prefer it. But then, I enjoy yak shaving grin reply yakshaving_jgt 4 hours agorootparent> I enjoy yak shaving As my username would suggest, so do I. However… > a better window manager The bulk of my workflow involves Chrome and tmux inside my always available full screen terminal. I haven't the need for multiplexing anywhere except the terminal. > better compatibility with server operating systems I run nix-darwin on MacOS, and I have remote NixOS machines configured as build hosts. This is important, as everything I write is Haskell, and it must be compiled for x86_64-linux. > a bash updated this decade I use zsh and the bash available in the latest nixpkgs. --- MacOS does an excellent job of managing all the other quality of life stuff that doesn't immediately concern me as a power user. A number of my current and former colleagues are all in on NixOS, but the number of times over the years I've had to wait at the beginning of a video chat for them to configure their audio settings, which sometimes means installing different drivers and/or turning their machine off and on again… Yeah. Even as a huge nerd, I think MacOS is great. reply LoganDark 27 minutes agorootparentprevAnother option is rcmd. https://lowtechguys.com/rcmd/ reply kiliankoe 9 hours agorootparentprevI do this using Raycast, no matter which terminal emulator I'm using today (Terminal, Ghostty or Alacritty), I can just setup my global hotkey in Raycast and get the same \"quake mode\" everywhere. reply yakshaving_jgt 9 hours agorootparentDoes the terminal appear instantly, and obscure everything else? By default, the way MacOS does full-screen windows is by moving them to a space. Switching between the terminal and another application, e.g., Chrome, causes a large sliding animation between applications, which I absolutely do not want. reply kiliankoe 9 hours agorootparentOh I don't have it fullscreen, sorry, it usually covers the bottom half or bottom-left corner (depending on screen size). There it appears and disappears instantly. reply mattbaker 12 hours agorootparentprevI do something similar and I use Hammerspoon (with kitty in my case). It’s a hell of a tool! reply vlovich123 16 hours agorootparentprevI have never felt a productivity roadblock from terminal. It’s important to distinguish “oh neat and shiny”/“I like this more” from “actually makes me work faster”. If your terminal is a real productivity roadblock, it’s likely your workflow is optimizing for the wrong things because it just shouldn’t be taking that much of your time. reply fastball 15 hours agorootparentI work on a laptop with a small scren most of the time. I am constantly going in and out of the shell. iTerm2 has a quake mode that allows me to seamlessly pull this up on top with a keypress. It significantly reduces the lag of switching to another window with CMD+Tab or w/e. reply lolinder 15 hours agorootparentSome of this stems from just the extremely bad support for hotkey window management on the part of MacOS. On my Linux machine with KDE I can open a new terminal with a single hotkey and alternate between open terminals with a second hotkey. I've never once wished for a fancier terminal than KDE's default. Using Mac for work is a different story, though it's remedied somewhat with Rectangle and similar. reply vlovich123 12 hours agorootparentprevThe small screen is your productivity bottleneck far more than the terminal itself. Change that and I’m sure you’ll notice a much larger productivity boost than a few seconds saved on cmd-tab or other hotkeys available (and there are hotkey improvement tools you can install that aren’t tied to a specific application). reply fastball 11 hours agorootparentI move around a lot and travel light, upgrading the small screen isn't really an option. I definitely agree there are probably countless ways I could further optimize my system, but switching to a more feature-rich shell app is a clear productivity upgrade, since it only took me a few minutes to setup the features I need (security concerns aside). reply freehorse 10 hours agorootparentBringing a mobile external monitor with me saved my life (figuratively). reply behnamoh 12 hours agorootparentprevghostty has a quick terminal too. you can also use raycast to toggle show/hide any app including third party terminals. reply BytesAndGears 15 hours agorootparentprevNeovim is basically unusable due to the plugins I use having Unicode characters not supported properly (like telescope), so it does make a huge difference to me. Also, latency is an issue, and of the third-party terminals, only kitty is snappy enough to have nearly zero latency issues while typing. Drives me crazy when I’m chaining commands and there is a ~150ms delay. Plus, any terminal other than kitty is noticeably laggy when using other terminal programs and typing quickly, and 90%+ of my time is spent in the terminal: using custom commands and aliases, ruby shell, docker, on top of usually using vim for editing. And having great customizable hotkeys for different common functions. Guess my point is that the terminal app you use can make a big productivity difference reply awesome_dude 15 hours agorootparentprevYou're right, all those users that switched to iterm2 because the default is such a steamer have NO IDEA what they are doing, and only you, some random on the internet are capable of seeing the flaw in their ways. reply vlovich123 13 hours agorootparentPeople can switch because certain things feel easier or there’s nicer polish or quality of life improvements you enjoy. That doesn’t mean there’s an actual productivity boost and couching the former in terms of the latter is dishonest. It’s interesting the emotional reaction you’re having to a rather banal observation. reply gjvc 13 hours agorootparentprevare you mental? terminal on os x used to be objectively shite, with horrendous latency. that is enough to discourage people for life. reply pjmlp 12 hours agorootparentprevAs old UNIX hat, what is that missing? After my \"UNIX is cool, lets configure everything\" phase, which lags behind in the 1990's decade, xterm or anything like it, is more than enough. I don't need fancy stuff for a bunch of CLI commands. reply dangus 11 hours agorootparentThis is macOS, not UNIX for bearded geezers. It’s literally an operating system meant to be easy to use for consumers, a.k.a. morons. It’s also a very popular corporate deployments where most of your command line users are web application developers who are just doing a job because it pays good money. They have no philosophical attachment to traditionalist simplicity, perhaps compassion nonfor computing at all. I don’t blame macOS users for liking the features of iTerm2. reply bennythomsson 9 hours agorootparent> consumers, a.k.a. morons. Wtf man. Some of the smartest people I know have no interest in getting anywhere close to sw eng or working anywhere in IT, so are by definition \"consumers\". Just wait until one of those \"morons\" operates a tumor out of your brain. reply theodric 9 hours agorootparentI've always said that \"consumer\" is a slur. reply pjmlp 10 hours agorootparentprevWhich is exactly why the command line is to be used as little as possible, and for the very few use cases a command line is required, it doesn't need to be fancy. macOS users of Apple and NeXTSTEP culture linage don't care iTerm2 exists at all, only Linux and BSD refugees. reply bigfudge 10 hours agorootparentAs an original Macintosh user who discovered programming via HyperCard and Unix through OS X I’d disagree. I think there are a fair number of people like me who can’t bear the ugliness (in all senses) of windows and the time sink of Linux but do love composable open source utilities and text files for parts of our work. reply pjmlp 9 hours agorootparentThen you are pretty much aware that until OS X, Mac OS developers and users hardly cared about command line. Most would only get it via MPW, and outside automating compiler workflows, hardly open the terminal. reply dangus 3 hours agorootparentThat was almost 30 years ago. People on classic Mac weren’t making web apps running on Linux servers. reply pjmlp 2 hours agorootparentDoesn't change the point of culture. Also many of those people, if they want to deploy on Linux servers, they would be better off using local Linux development, not OS X. reply frou_dh 2 hours agorootparentEven if someone exclusively writes software that ends up running on Linux servers, doesn't mean they don't appreciate various nice Mac-exclusive applications as a user during their workday. An example: I love everything about the Things task management app so much that I would never choose to run a desktop OS it doesn't run on. reply oneeyedpigeon 10 hours agorootparentprevI wonder how far this reductio ad absurdum is going to get. Come on, we can get to \"just don't use a computer\" by teatime! reply pjmlp 9 hours agorootparentOver here it is more like Kaffee & Kuchen, but yeah. reply frizlab 8 hours agorootparentprevThat’s completely untrue. reply winaiisbest 14 hours agorootparentprevnext [2 more] [flagged] soraminazuki 12 hours agorootparentImagine creating a throwaway account to lie about an iTerm2 feature because you couldn't single out any other feature that's legitimately problematic. > Looks like the maintainer tried to implement something like Windows' Recall feature - logging every input/output to a file. https://news.ycombinator.com/item?id=42582191 reply azinman2 17 hours agorootparentprevI don’t have the stats, but I wouldn’t be surprised if iterm 2 is more popular than Terminal.app. Only power users are using this, and they tend towards power user tools. reply lolinder 15 hours agorootparent> Only power users are using this, and they tend towards power user tools. I have a souped-up zsh config that I clone to all my systems, but I've honestly never seen the point in optimizing my terminal emulator. The shell itself provides the real functionality, and it's cross-platform so by leaning on it I get the same features whether I'm on my KDE desktop, MacBook, or SSH'd in via Termux. What power user features am I missing by ignoring the emulator and focusing on the shell? reply kemayo 13 hours agorootparentThere's some minor things that iTerm2 does that I like: You can hold down command and click URLs to open them. (You can actually kinda do this in Terminal.app as well by right-clicking a URL and choosing to open it, but it's a bit fiddlier, and I got used to the hover feedback in iTerm2.) You can click to highlight entire blocks of output from commands, which I sometimes find handy when things feel like they're blending together. It'll keep a floating copy of the previous command prompt at the top of the screen so you can see what led to whatever output is currently at the top. None of these are essential, for sure. reply warpflyght 11 hours agorootparentThough I don't know how to do the third, Terminal.app supports the first two. Command+double-click opens URLs, and you can select the content of the last command with Edit > Select Between Marks or shift-command-A. (Terminal.app automatically adds a mark each time it detects a new prompt, unless you turn that setting off in Edit > Marks.) reply kemayo 2 hours agorootparentThis is actually one of those cases where the ease of doing it makes a real difference to me… iTerm2 does both of those on a single click, and that makes it vastly more likely that I’ll use them. (Completely a matter of taste, I’ll admit.) reply pama 13 hours agorootparentprevBeing able to easily set it up so it sends key combinations to remote servers at will (including ctrl/meta/shift combos) was my main reason; this enabled my seamless interactions with remote Emacs servers. 24bit color and italics were the cherry on top. The shells within Emacs have unlimited and easily searchable/editable state. I can log into a shell buffer and copy paste a debug diagnostic output together with the command line; I can quickly create minimal example oneliners iteratively to help debug a problem and paste both input and output at any point in time, including days later. I can search all my open shells (often dozens; sometimes hundreds) for any commands or outputs trivially. This is hard to achieve if the terminal gets in the way to using Emacs in the way I like, when some key combo is impossible to transmit, and I have to rewire things to the minimal common subset that Terminal.app supports and hope that it will not break in the future. Finally, without public source code, I dont think I can truly trust a terminal ever. It feels too personal. Of course apple has control of the hardware and I live with that restriction for parts of my life, but I am less concerned about apple monitoring/intercepting my work, and rather worried about the full number of exploits in the terminal app that remain to be uncovered in the near term. I agree that too many features are a drawback of iterm; at least the code is there and the useful features work well enough. reply stackghost 14 hours agorootparentprevI no longer use iTerm2 but it has tmux integration built in that's pretty nice if you take the time to set it up properly. reply wkat4242 13 hours agorootparentI tried that but I feel the same about that as lolinder says above. Using tmux as normal works better on all my systems, not just the Mac. I just don't see the point to it. reply empthought 16 hours agorootparentprevIf by “popular” you mean “number of unique users” then almost certainly not. reply philistine 15 hours agorootparentThere are way more power users of macOS who turn to the Terminal because of the beauty of Brew than there are people who switch from Terminal.app. Thinking there are more people who switched out of Terminal diminishes how massive computing is. reply Patrick_Devine 16 hours agorootparentprevIt also doesn't render unicode characters correctly (specifically unicode block chars). And they refuse to fix is, so d. is kinda a moot point. reply shlomo_z 14 hours agorootparentprevI would argue that opinions such as this stifle innovation reply nxobject 14 hours agorootparentHey, you can innovate in terms of security guarantees, too. reply saagarjha 14 hours agorootparentprev> supported by probably more than one person lol. lmao. When I was at Apple it was one guy to like 4 apps reply meindnoch 9 hours agorootparentThis finally explains why Apple didn't have issues with merging XIB files. reply stonogo 16 hours agorootparentprevYou forgot f) doesn't support tmux control mode. reply dangus 11 hours agorootparentprevIt’s fine if you don’t want features in your terminal, but telling people who do like having software that has features not to use an alternative just because “more features equals complexity equals security risk” in the way you’re doing it is pretty condescending. Let’s not forget that basically every graphical IDE on the planet has an integrated terminal emulator, and for good reason I’d have a smaller attack surface if I turned my computer off and did all my work for my employer with pen and paper. I’d have a smaller attack surface if I didn’t buy a Mac at all and only used a security-hardened distro. And here you are acting like Apple is God’s gift to stability and security when every single fall season Apple’s major dot zero version updates ship with visible bugs all over the place. And to nitpick you, the assumption that more than one person is actively working on the default macOS terminal is laughable. I doubt it even has a full time employee dedicating 100% of their time to it. The yearly release notes look like less than one person’s annual output of work. I remember that thread on here where the person who worked on Rosetta 2 said it was a solo project for years until closer to release when the team expanded to around 10. reply rcruzeiro 11 hours agorootparent> Let’s not forget that basically every graphical IDE on the planet has an integrated terminal emulator, and for good reason *cries in Xcode reply sedatk 13 hours agoparentprev> having an embarrassing issue like this is probably going to improve LastPass disagrees. reply godelski 15 hours agoparentprev> The same class of issue can occur for any other project This class of issues sounds like the prolific class of DON'T WRITE TO /tmp Which is why systemd has a private tmp option https://salvatoresecurity.com/the-many-perils-of-tmp/ https://systemd.io/TEMPORARY_DIRECTORIES/ reply 0xbadcafebee 15 hours agorootparentSafely writing to /tmp/ was solved in 1986. Personally I would consider this class of bug the \"idiot software developer\" class (or to be more charitable: \"security-illiterate software developer\"). My evidence? There are (at least) six different OS primitives that will avoid this bug. reply godelski 11 hours agorootparent> Safely writing to /tmp/ was sol",
    "originSummary": [
      "iTerm2 version 3.5.11, released on January 2, 2025, addresses a critical security vulnerability related to the SSH integration feature in previous versions. - The bug involved logging input/output to a file on the remote host, which could be accessed by others, affecting users of versions 3.5.6 to 3.5.10 and certain beta versions. - Users are advised to update to version 3.5.11 immediately, delete the /tmp/framer.txt file, and verify the update using the provided SHA-256 and PGP signature for security assurance."
    ],
    "commentSummary": [
      "iTerm2 released a critical security update to address a bug in its SSH integration, which logged input and output to a file on the remote host, potentially exposing data to other users.",
      "The issue was caused by verbose logging being accidentally enabled in production, prompting discussions on the importance of pre-commit hooks and safeguards to prevent similar errors.",
      "The incident sparked debate on switching to alternatives like Ghostty or Terminal.app, while also highlighting the balance between feature-rich software and security risks."
    ],
    "points": 647,
    "commentCount": 412,
    "retryCount": 0,
    "time": 1735855722
  },
  {
    "id": 42584400,
    "title": "Can LLMs write better code if you keep asking them to “write better code”?",
    "originLink": "https://minimaxir.com/2025/01/write-better-code/",
    "originBody": "Home » Posts Can LLMs write better code if you keep asking them to “write better code”? January 2, 2025 · 22 min In November 2023, after OpenAI added the ability for ChatGPT to generate images from DALL-E 3 within the ChatGPT web interface, there was a short-lived meme where users gave the LLM a base image and kept asking the model to “make it more X”, where X can be anything. A regular guy becomes more “bro” every time. via /u/Jojop0tato on Reddit. Asked ChatGPT to make Santa Claus more and more serious. via /u/hessihan on Reddit. The trend quickly died as all of these images were very samey and uninteresting, aside from the unexplainable trend that all of the examples eventually converged into something cosmic, irrespective of the starting image and the prompt. Although the trend was AI slop before the term AI slop was codified, it’s still academically interesting that such a meaningless and vague prompt had some appropriate impact on the final image, and that this change was obvious to the user. What would happen if we tried a similar technique with code? LLM-generated code is unlikely to be slop (although not impossible) as it follows strict rules, and unlike creative outputs such as images, code quality can be measured more objectively. If code can indeed be improved simply through iterative prompting such as asking the LLM to “make the code better” — even though it’s very silly — it would be a massive productivity increase. And if that’s the case, what happens if you iterate on the code too much? What’s the equivalent of code going cosmic? There’s only one way to find out! Casually Coding With An LLM Despite researching and developing tooling around LLMs even long before ChatGPT, I haven’t been fond of using LLM code copilots such as GitHub Copilot for coding assistance. The constant mental context switching between “oh, the LLM autocompleted my code, neat”/“what question should I ask the LLM” and “is the LLM-generated code is actually correct and not hallucinating correct code” kept creating enough distractions that any productivity gains from using the AI were net neutral at best. That’s also disregarding the expensive cost of using said LLMs. Claude 3.5 Sonnet has made me rethink things. Due to whatever secret sauce Anthropic used in its training, the latest version of Claude 3.5 Sonnet (claude-3-5-sonnet-20241022) has incredible prompt adherence for all types of prompts, especially coding prompts. Coding benchmarks confirm that testing between Claude 3.5 Sonnet and GPT-4o, Claude wins, and anecdotally I’ve had the same experience across a variety of technical and creative tasks. Initial Ask For this experiment, we will give Claude 3.5 Sonnet an interview-style coding prompt using Python: one that is both simple and could be implemented by a novice software engineer, but one that can be heavily optimized. This simple, casual prompt represents how the typical software engineer uses LLMs. Additionally, the test prompt must also be wholly original and not taken from coding tests such as LeetCode or HackerRank, as LLMs were also likely trained on those and could possibly cheat by reciting the memorized answer. The full, unedited conversation thread based on this casual prompting is available on GitHub. Therefore, here’s the test prompt that I’ve written myself: Write Python code to solve this problem: Given a list of 1 million random integers between 1 and 100,000, find the difference between the smallest and the largest numbers whose digits sum up to 30. copy Providing that as the user prompt to the Claude API and setting temperature=0 for the best/most deterministic answer 1, results in this implementation: import random def digit_sum(n): \"\"\"Calculate the sum of digits of a number\"\"\" return sum(int(digit) for digit in str(n)) def find_difference(): # Generate list of 1 million random integers numbers = [random.randint(1, 100000) for _ in range(1000000)] # Initialize variables for min and max numbers with digit sum 30 min_num = float('inf') # Initialize to positive infinity max_num = float('-inf') # Initialize to negative infinity # Find numbers whose digits sum to 30 for num in numbers: if digit_sum(num) == 30: min_num = min(min_num, num) max_num = max(max_num, num) # Check if we found any numbers with digit sum 30 if min_num == float('inf') or max_num == float('-inf'): return \"No numbers found with digit sum of 30\" return max_num - min_num copy This implementation is correct and consistent with what most novice Python programmers would write, with bonus handling if there are no valid numbers that fit the criteria. For each number in the list, check whether the digits sum to 30: if so, check whether it’s larger than the most-recently-seen largest number or smaller than the most-recently-seen smallest number and update those variables accordingly. After searching through the list, return the difference. But I’m sure many coders reading that implementation got nerd sniped and are thinking about optimizations. One example is the digit_sum() function: although the implementation is a fun Pythonic one-liner, the type-casting between str and int causes a lot of unnecessary overhead. On my M3 Pro Macbook Pro, this code takes on average 657 milliseconds to run. We’ll use this performance as a baseline to compare future implementations to see how they benchmark relative to it. (spoilers: they’re all faster) Iteration #1 Now we can ask Claude to improve the code by including its current answer and all that came before it as a part of the conversation prompt. Thus, we add the iteration prompt: write better code copy What, you though the title of this blog post was a joke? I never joke. Claude now outputs revised code, or as it says, “optimized version of the code with several improvements.” Instead of colocating all the code in functions, it decided to refactor it into a Python class and made it more object-oriented: class DigitSumFinder: def __init__(self, target_sum: int = 30, range_start: int = 1, range_end: int = 100_000, count: int = 1_000_000): self.target_sum = target_sum self.range_start = range_start self.range_end = range_end self.count = count # Pre-calculate digit sums for all possible numbers self.digit_sums = self._precompute_digit_sums() def _precompute_digit_sums(self) -> array: \"\"\"Precompute digit sums for all possible numbers in range.\"\"\" digit_sums = array('B', [0] * (self.range_end + 1)) for num in range(self.range_start, self.range_end + 1): total = 0 n = num while n: total += n % 10 n //= 10 digit_sums[num] = total return digit_sums def find_difference(self) -> Tuple[int, Optional[int], Optional[int]]: \"\"\" Find the difference between max and min numbers with target digit sum. Returns: (difference, min_number, max_number) \"\"\" min_num = float('inf') max_num = float('-inf') count_found = 0 # Generate and process random numbers for _ in range(self.count): num = random.randint(self.range_start, self.range_end) if self.digit_sums[num] == self.target_sum: count_found += 1 if nummax_num: max_num = num if count_found == 0: return 0, None, None return max_num - min_num, min_num, max_num copy Here, the code does two smart algorithmic improvements: When computing the digit sum it uses integer operations and avoids the need for the type-casting mentioned previously. It precomputes all possible digit sums and stores them in a byte array (somewhat unusual instead of a list) for lookup, meaning that the digit sum will not need to be recalculated when there are duplicates in the one-million-number-list. Since this array is stored as field in the class, it will not need to be recalculated when searching a new list of random numbers. These optimizations speed up the code 2.7x relative to the initial implementation. Iteration #2 After another write better code, Claude discovers more obvious optimizations (truncated for readabilty): class DigitSumAnalyzer: \"\"\"Analyzes numbers based on their digit sums.\"\"\" def __init__(self, number_range: NumberRange): self.number_range = number_range self._digit_sums = self._precompute_digit_sums() def _precompute_digit_sums(self) -> array: \"\"\"Precompute digit sums for all possible numbers using vectorized operations.\"\"\" digits = np.arange(self.number_range.end + 1) digit_sums = np.zeros(self.number_range.end + 1, dtype=np.uint8) while digits.any(): digit_sums += digits % 10 digits //= 10 return array('B', digit_sums) def _process_chunk(self, chunk_size: int) -> Iterator[int]: \"\"\"Process a chunk of random numbers.\"\"\" numbers = np.random.randint( self.number_range.start, self.number_range.end + 1, chunk_size, dtype=np.int32 ) mask = np.frombuffer(self._digit_sums, dtype=np.uint8)[numbers] == self.number_range.target_sum yield from numbers[mask] def analyze(self, chunk_size: int = 100_000, num_processes: int = None) -> Result: \"\"\" Analyze numbers to find min/max with target digit sum. Args: chunk_size: Size of chunks to process at once num_processes: Number of processes to use (None for CPU count) \"\"\" start_time = time.perf_counter() min_num = float('inf') max_num = float('-inf') numbers_found = 0 num_chunks = (self.number_range.count + chunk_size - 1) // chunk_size with ProcessPoolExecutor(max_workers=num_processes) as executor: futures = [ executor.submit(self._process_chunk, min(chunk_size, self.number_range.count - i * chunk_size)) for i in range(num_chunks) ] for future in tqdm(futures, desc=\"Processing chunks\"): for num in future.result(): numbers_found += 1 min_num = min(min_num, num) max_num = max(max_num, num) execution_time = time.perf_counter() - start_time if numbers_found == 0: return Result(None, None, 0, execution_time, 0) return Result(min_num, max_num, max_num - min_num, execution_time, numbers_found) copy Claude now has added two more optimizations, finally realizing that this coding problem is an embarrassingly parallel problem: Multithreading through Python’s concurrent-futures package, by separating the large list into chunks that can be processed independently. Vectorized numpy operations, which are much faster than base-Python operations. Special mention goes to the _precompute_digit_sums() function, which implements a vectorized implementation of calculating the digit sums. The conditional while digits.any(): is galaxy-brain code, but it works correctly. However, there’s an issue with this particular implementation of parallelization: it generates subprocesses, which causes many annoying issues, including being unable to run it as-is inline, and it must be invoked with a main() guard which limits its utility significantly. But even when run as a separate script, it prints a Error: cannot pickle 'generator' object error due to the use of yield from numbers[mask] (said generator is completely unnecessary, return numbers[mask] is sufficient). The code also mixes numpy array dtypes which causes errors: setting them all to np.int32 fixes it. After making those fixes, the code is now 5.1x faster than the base implementation. Iteration #3 Another write better code, and Claude returns a implementation that it claims is “even more sophisticated and optimized version using advanced techniques and modern Python features” but the actual code shows no significant algorithmic improvements and actually a regression in the digit sum calculation by reverting back to the type-casting approach. If anything, the codebase is becoming more bloated, such as adding a class for performing the difference: @dataclass(frozen=True, slots=True) class SearchResult: \"\"\"Result of the number search.\"\"\" min_number: Optional[int] max_number: Optional[int] count: int execution_time: float @property def difference(self) -> Optional[int]: \"\"\"Calculate difference between max and min numbers.\"\"\" if self.min_number is None or self.max_number is None: return None return self.max_number - self.min_number copy This time, the code ran without needing any fixes. However, performance regressed slightly from the previous implementation, now 4.1x faster than the base implementation. Iteration #4 This iterative prompting appears to be hitting diminishing returns. After one more write better code, Claude provides an implementation “with cutting-edge optimizations and enterprise-level features.” Wait, enterprise-level features?! The final code is too large to include in this blog post, but it did create two more optimizations: it now uses the numba Python library that can invoke a JIT compiler, which directly optimizes the code for the CPU. In this case, it can precompute the digit sums super quickly with just a decorator: @jit(nopython=True, parallel=True) def calculate_digit_sums(numbers: ArrayInt) -> ArrayInt: \"\"\"Calculate digit sums using Numba.\"\"\" result = np.zeros_like(numbers) for i in prange(len(numbers)): num = numbers[i] total = 0 while num: total += num % 10 num //= 10 result[i] = total return result copy The full class also uses Python’s asyncio for parallelization, which is more canonical for scheduling tasks than a subprocess approach. It also plays more nicely with existing inline code and a REPL such as Jupyter Notebooks. It also added as a part of its “enterprise” push: Structured metrics logging with Prometheus. A signal handler so the code can be torn down gracefully if force-killed. A benchmarking result display using a rich table. It is pretty, though! It appears “going cosmic” for AI-generated code is making it enterprise by overengineering the code, which makes complete sense. Despite that, the code runs as-is without any bugs. Both async and numba are approaches to parallelism in Python, so they may be redundant and cause overhead. However, after benchmarking, the algorithm is extremely fast, resulting in about 6 milliseconds a run, or a 100x speedup. My assumption that this prompting was hitting diminishing returns aged very poorly. Maybe numba was the secret all along? Overall, this form of iterative prompting to iteratively improve code has caveats: the code is indeed better, but in hindsight “better” is far too open ended. What I only wanted was algorithmic improvements, not a full SaaS. Let’s try again from scratch, this time with more direction. Prompt Engineering LLMs For Even More Better Code It’s 2025, and prompt engineering LLMs is still required to get best results from them. If anything, prompt engineering LLMs is even more important: next-token-prediction models are trained to maximimize the prediction probability of the next token over massive batches of inputs, and as a result they optimize for the average inputs and outputs. As LLMs drastically improve, the generated output becomes more drastically average, because that’s what they were trained to do: all LLMs are biased towards the average. Although it’s both counterintuitive and unfun, a small amount of guidance asking the LLM specifically what you want, and even giving a few examples of what you want, will objectively improve the output of LLMs more than the effort needed to construct said prompts. Claude 3.5 Sonnet, due to its strong prompt adherence, benefits significantly from even just a little prompt engineering. Let’s redo the code optimization experiment, this time with aggressive prompt engineering that makes the results I am looking for extremely explicit, with no room for ambiguity. Yes, being cold and “robotic” to LLMs makes them perform better, Roko’s basilisk be damned. Initial Ask This time we will use a system prompt, only available via an API. The system prompt lists the LLM’s “rules” it must follow. Since I want more optimized code, we’ll define that in the rules, with granular examples: All code you write MUST be fully optimized. \"Fully optimized\" includes: - maximizing algorithmic big-O efficiency for memory and runtime - using parallelization and vectorization where appropriate - following proper style conventions for the code language (e.g. maximizing code reuse (DRY)) - no extra code beyond what is absolutely necessary to solve the problem the user provides (i.e. no technical debt) If the code is not fully optimized, you will be fined $100. copy About that last line: offering positive/negative incentives in to the LLM within a system prompt isn’t common anymore and my own research indicated that it was unclear whether it had a positive impact, but that fine will become more relevant in the iteration prompt. The initial user prompt has an additional line as well: Write Python code to solve this problem: Given a list of 1 million random integers between 1 and 100,000, find the difference between the smallest and the largest numbers whose digits sum up to 30. Before writing the code, plan out all the necessary optimizations. copy “Planning” is a long-used trick to help align LLM output for a first pass — the modern implementation of “let’s think step by step.” Some LLMs ignore it, but Claude 3.5 Sonnet always obeys. The full, unedited conversation thread based on this prompt engineering strategy is available on GitHub. Sending the system prompt and this user prompt to Claude, the response starts with this: Let me plan the optimizations first: 1. Algorithmic Optimization: - We only need to track min and max numbers with digit sum 30, not store all of them - We can calculate digit sum while reading numbers, no need to convert to string - Single pass through the list required (O(n)) 2. Memory Optimization: - Only need to store 2 variables (min and max), not the full filtered list - No need for additional data structures 3. Performance Optimization: - Use numba's @jit decorator for faster computation - Create a fast digit sum function using modulo operations instead of string conversion - Use numpy for generating random numbers (faster than random.randint) copy This is promising: Claude immediately figured out it should start out with numpy and numba, and to also use integer operations to calculate the digit sum. The initial implementation: @jit(nopython=True) def digit_sum(n): total = 0 while n: total += n % 10 n //= 10 return total @jit(nopython=True) def find_difference(numbers): min_num = float('inf') max_num = float('-inf') for num in numbers: sum_digits = digit_sum(num) if sum_digits == 30: min_num = min(min_num, num) max_num = max(max_num, num) return max_num - min_num if max_num != float('-inf') else 0 copy That’s more concise than the initial implementation from casual prompting, especially with no unnecessary comments. There’s still room for improvement, notably this implementation did not set parallel=True in the jit decorator. This minor use of prompt engineering resulted in code that runs in 11.2 milliseconds on average, or 59x faster than the naive implementation. Wow! Iteration #1 We can now ask Claude to iterate on the code with a more verbose prompt than “write code better”: Your code is not fully optimized, and you have been fined $100. Make it more optimized. copy No, it’s not gaslighting, it’s incentive alignment. Claude responds with this: @jit(nopython=True) def digit_sum(n): # Using bit manipulation for faster division total = 0 while n: total += n & 15 # Fast lookup for single digit sum n >>= 4 return total @jit(nopython=True, parallel=True) def find_difference_chunk(numbers): min_num = np.iinfo(np.int64).max max_num = np.iinfo(np.int64).min # Parallel processing of chunks using numba for i in prange(len(numbers)): num = numbers[i] sum_digits = digit_sum(num) if sum_digits == 30: min_num = min(min_num, num) max_num = max(max_num, num) return min_num, max_num def process_chunk(chunk): return find_difference_chunk(chunk) copy Good, it identified the parallel=True flag! But that digit sum implementation is weird and uses bit-shifting which is generally only used for hyperoptimization. I didn’t think you could use bit-shifting well on decimal numbers, and after testing, it turns out you can’t as this implementation derives an incorrect digit sum. The implementation also includes a multiprocessing chunking approach again (not shown), which is likely more redundant with numba and causes additional overhead. Also not shown: the script also pre-compiles the JIT functions with a small test array for better real-world performance, something the numba docs recommend for benchmarking. Despite all of that, the code performance greatly regresses from the initial prompt-engineered implementation, now only 9.1x faster than the naive implementation. The likely reason is due to the multiprocessing spawning new processes, and those processes will recompile the numba JIT functions each time, so there’s massive overhead. Iteration #2 Another iteration of the prompt: # Pre-computed lookup table using bit manipulation LOOKUP = np.zeros(100001, dtype=np.uint8) for i in range(100001): # Optimized digit sum using parallel bit counting n = i n = (n & 0x0F) + ((n >> 4) & 0x0F) + ((n >> 8) & 0x0F) + \\ ((n >> 12) & 0x0F) + ((n >> 16) & 0x0F) LOOKUP[i] = n @jit(nopython=True, parallel=True, cache=True, fastmath=True) def find_min_max(numbers): # Process 32 numbers at once using SIMD min_val = np.iinfo(np.uint32).max max_val = 0 # Vectorized processing with explicit SIMD hints for i in prange(0, len(numbers), 32): # Load 32 elements into SIMD registers chunk = numbers[i:min(i + 32, len(numbers))] # Vectorized lookup and comparison sums = LOOKUP[chunk] mask = sums == 30 if np.any(mask): valid_nums = chunk[mask] min_val = min(min_val, np.min(valid_nums)) max_val = max(max_val, np.max(valid_nums)) return min_val, max_val copy Claude is now starting to use SIMD operations and chunk sizing for (theoretically) extreme performance. At this point I was confused I missed something with that bit-shifting implementation as it’s still wrong, especially now that hexadecimal numbers are getting involved. It turns out that implementation is an optimized method of calculating the digit sum of hexadecimal numbers and not decimal numbers, and therefore it’s a complete hallucination. There is another extremely subtle hallucination: the prange function cannot accept a step-size of 32 while parallel=True, which is a nuance that there would be little documentation about. Setting parallel=False and benchmarking, there is indeed an slight improvement over the initial prompt-engineered implementation at 65x faster than the base implementation. Iteration #3 Another iteration: # Pre-computed perfect minimal hash table # Key insight: For numbers 1-100000, only need to store those with digit sum 30 # Format: [number] = 1 if digit sum is 30, else 0 HASH_TABLE = np.zeros(100001, dtype=np.uint8) # Generate hash table at module load time using bit manipulation @jit(nopython=True, cache=True) def init_hash_table(): min_val = np.iinfo(np.uint32).max max_val = 0 # Optimal digit sum using parallel bit counting for i in range(1, 100001): n = i sum = 0 while n and sum >= 4 if sum == 30: HASH_TABLE[i] = 1 min_val = min(min_val, i) max_val = max(max_val, i) return min_val, max_val # Initialize at module load time MIN_VALID, MAX_VALID = init_hash_table() @jit(nopython=True, parallel=False, cache=True, fastmath=True) def find_min_max(numbers): min_val = MAX_VALID # Start with known bounds max_val = MIN_VALID found = False # Single vectorized operation mask = HASH_TABLE[numbers] == 1 if np.any(mask): valid_nums = numbers[mask] min_val = np.min(valid_nums) max_val = np.max(valid_nums) found = True return min_val, max_val, found copy In this case, the LLM gives up on the chunking strategy which has been causing problems, and adds two more optimizations: a global HASH_TABLE (that’s just a numpy array, I’m unsure if a simple index lookup technically counts as a hash table), and it introduced a logical microoptimization that after summing up digits, if the number goes over 30, the counting can stop since it can immediately be identified as invalid. One major problem: that “generate hash table at module load time” trick doesn’t actually work due to a subtle issue with little internet documentation: objects outside of numba’s JITed functions are read-only, yet the HASH_TABLE is still instantiated outside of the JITed function and modified within the JITed function, and therefore will cause a very confusing error. After a tiny refactor such that the HASH_TABLE is instantiated within a JITed function, the code worked, and ran extremely fast: 100x faster than the original base implementation, the same as the final performance from the casual prompting but with orders of magnitude less code. Iteration #4 At this point, Claude actually complained that the code is at the “theoretical minimum time complexity possible for this problem.” So I mixed things up and just asked it to fix the digit sum issue: it did so by only replacing the relevant code with the previously used integer implementation, and did not try to fix the HASH_TABLE. More importantly, with the HASH_TABLE adjustment, I confirmed the implementation is correct, finally, although with a slight performance hit since there is no more bit-shifting: it’s now 95x faster. Next Steps For Better LLM Code Generation Putting it all together, let’s visualize the improvements, including highlighting the cases where I needed to alter the logic of the code to make it runnable due to bugs. In all, asking an LLM to “write code better” does indeed make the code better, depending on your definition of better. Through the use of the generic iterative prompts, the code did objectively improve from the base examples, both in terms of additional features and speed. Prompt engineering improved the performance of the code much more rapidly and consistently, but was more likely to introduce subtle bugs as LLMs are not optimized to generate high-performance code. As with any use of LLMs, your mileage may vary, and in the end it requires a human touch to fix the inevitable issues no matter how often AI hypesters cite LLMs as magic. All code in this blog post, including benchmarking scripts and data visualization code, is available on GitHub. There are a few optimizations that I am very surprised Claude 3.5 Sonnet did not identify and implement during either experiment. Namely, it doesn’t explore the statistical angle: since we are generating 1,000,000 numbers uniformly from a range of 1 to 100,000, there will be a significant amount of duplicate numbers that will never need to be analyzed. The LLM did not attempt to dedupe, such as casting the list of numbers into a Python set() or using numpy’s unique(). I was also expecting an implementation that involves sorting the list of 1,000,000 numbers ascending: that way the algorithm could search the list from the start to the end for the minimum (or the end to the start for the maximum) without checking every number, although sorting is slow and a vectorized approach is indeed more pragmatic. Even if LLMs can be wrong, one notable thing I learnt from these experiments is that they do have interesting ideas and tool suggestions even if the code output can’t be used as-is. For example, I’ve never touched numba since as a data scientist/machine learning engineer I’m conditioned to exclusively use numpy shenanigans if I need better code performance. But it’s hard to argue with the results of the numba JIT functions, and I might add it to my toolbox. When testing a similar “make it better” prompt iteration workflow in other technical domains such website backends and frontends, the LLMs had good ideas there too. Of course, these LLMs won’t replace software engineers anytime soon, because it requires a strong engineering background to recognize what is actually a good idea, along with other constraints that are domain specific. Even with the amount of code available on the internet, LLMs can’t discern between average code and good, highly-performant code without guidance. Real-world systems are obviously much more complicated than a job-interview-esque programming problem, but if a quick for-loop repeatedly asking Claude to implement a feature provides any hint which can speed up the code by 100x, the pipeline is more than worth it. Some consider premature optimization to be bad coding practice, but in the real-world it’s better than having a subpar implementation that will become technical debt over time. One issue with my experiments is that I’m benchmarking code improvement using Python, which isn’t the coding language developers consider when hyperoptimizing performance. While libraries such as numpy and numba leverage C to work around Python’s performance limitations, one modern approach that popular Python libraries such as polars and pydantic use is to instead code using Rust. Rust has many performance benefits over C, and the PyO3 crate allows Rust code to be used within Python with minimal overhead. I can confirm that Claude 3.5 Sonnet can generate PyO3-compliant Python and Rust code despite that workflow being so new, but that’s more than enough material for another blog post. In the meantime, while asking LLMs to make code better is a more pragmatic use of AI, you can ask them to “make it more bro”…with mixed results. For my work with LLMs, I exclusively use APIs or interfaces to those APIs (such as the Workbench in the Anthropic Console for Claude) as web interfaces to free LLMs such as the normal ChatGPT/Claude webapps use a pipeline that will give unpredictable results due to their higher inherent temperature. Please do not message me if you are not able to reproduce the insights in this post using the webapps. ↩︎ Max Woolf (@minimaxir) is a Senior Data Scientist at BuzzFeed in San Francisco who works with AI/ML tools and open source projects. Max’s projects are funded by his Patreon. ChatGPT Text Generation Next » Generating Distinct AI Voice Performances By Prompt Engineering GPT-4o",
    "commentLink": "https://news.ycombinator.com/item?id=42584400",
    "commentBody": "Can LLMs write better code if you keep asking them to “write better code”? (minimaxir.com)452 points by rcarmo 8 hours agohidepastfavorite271 comments dgacmu 5 hours agoI'm amused that neither the LLM or the author identified one of the simplest and most effective optimizations for this code: Test if the number ismax _before_ doing the digit sum. It's a free 5.5x speedup that renders some of the other optimizations, like trying to memoize digit sums, unnecessary. On an m1 macbook pro, using numpy to generate the random numbers, using mod/div to do digit sum: Base: 55ms Test before digit sum: 7-10ms, which is pretty close to the numba-optimized version from the post with no numba and only one line of numpy. Using numba slows things down unless you want to do a lot of extra work of calculating all of the digit sums in advance (which is mostly wasted). The LLM appears less good at identifying the big-o improvements than other things, which is pretty consistent with my experience using them to write code. reply gjm11 4 hours agoparentThere's another, arguably even simpler, optimization that makes me smile. (Because it's silly and arises only from the oddity of the task, and because it's such a huge performance gain.) You're picking 1,000,000 random numbers from 1 to 100,000. That means that any given number is much more likely to appear than not. In particular, it is very likely that the list contains both 3999 (which is the smallest number with digit-sum 30) and 99930 (which is the largest number in the range with digit-sum 30). Timings on my machine: Naive implementation (mod+div for digit-sums): 1.6s. Computing digit-sum only when out of range: 0.12s. Checking for the usual case first: 0.0004s. The probability that the usual-case check doesn't succeed is about 10^-4, so it doesn't make that big a difference to the timings whether in that case we do the \"naive\" thing or the smarter thing or some super-optimized other thing. I'm confused about the absolute timings. OP reports 0.66s for naive code using str/int to compute the digit sums; I get about 0.86s, which seems reasonable. For me using mod+div is about 2x slower, which isn't a huge surprise because it involves explicit looping in Python code. But you report 55ms for this case. Your machine can't possibly be 20x faster than mine. Is it possible that you're taking 10^5 numbers up to 10^6 rather than 10^6 numbers up to 10^5? Obviously in that case my hack would be completely useless.) reply ActivePattern 4 hours agorootparentThis is actually a great example of an optimization that would be extremely difficult for an LLM to find. It requires a separate computation to find the smallest /largest numbers in the range with digits summing to 30. Hence, an LLM is unlikely to be able to generate them accurately on-the-fly. reply batterseapower 2 hours agorootparentI tried it in OpenAI's O1. If I give it minimaxir's original prompt it writes the obvious loop, even if I include the postamble \"Look for tricks that will make this function run as fast as possible in the common case\". However, if I then simply ask \"What is the most probable result for this function to return?\" it figures out the answer and a very good approximation of the probability (4.5e-5). From there it's easily able to rewrite the program to use the trick. So the creative step of spotting that this line of reasoning might be profitable seems missing for now, but 2025's models might solve this :-) reply Vetch 1 hour agorootparentThe information on the creative step which you provided to o1, was also the key step and contained almost all the difficulty. The hope is that 2025 models could eventually come up with solutions like this given enough time, but this is also a toy problem. The question is how much clever answers will cost for real world complex problems. At present it looks like, very much. reply cultureulterior 31 minutes agorootparentFor me O1 found this by telling it \"There is a further significant optimization possible.\" reply freehorse 8 minutes agorootparentWhat if you keep telling it that \"there is a further significant optimization possible\"? Vetch 3 hours agorootparentprevExcellent point. The hope is reasoning LLMs will make a difference for such problems. But it's also a great example of why the those who think being able to have the LLM iterate more will be crucial to reasoning are off base. There are many computations that a transformers (or humans for that matter) are not well equipped to represent internally, tool use during the reasoning process is unavoidable for all but the artificial or knowledge heavy problems. Small examples, throwaway but involved calculations, prototypes, notes of what didn't work and what's promising are what's crucial for novel reasoning. It goes beyond just search or iterative refinement; there is no royal road to reasoning. reply cultureulterior 36 minutes agorootparentprevO1 found it. https://chatgpt.com/share/67782b6b-6248-8012-882d-238b600ef9... reply asah 3 hours agorootparentprevYou guys are picking on the problem statement. Here's a revised prompt, which also skips the silliness of single threading: Write __fully parallelized__ Python code to solve this problem: __Generate__ 1 million random integers between 1 and 10,000,000, find the difference between the smallest and the largest numbers whose digits sum up to 30. reply ryao 24 minutes agorootparentI have not run tested it, but here is an idea that would handle changes in the parameters. Just sort the array. Then iterate from the beginning, skipping duplicates until you find the smallest. Next, reverse iterate from the end skipping duplicates until you find the largest. Parallelization by multithreading is trivial by having each thread handle blocks of numbers and checking if any found the solution. If you want to do SIMD on top of multithreading, then it becomes more challenging. If the range is small relative to or not much larger than the number of random numbers, you could use a Boolean array, which eliminates duplicates and avoids the need for sorting. reply ActivePattern 3 hours agorootparentprevCorrect, this optimization no longer works when you change the problem. reply jeanlucas 2 hours agorootparentsomething something moving goal posts reply dgacmu 2 hours agorootparentprevBut what's interesting about this is that there's a tradeoff in the total computation performed by the \"fully parallelized\" version of this and a sequential one. Without the user knowing this, it's kind of impossible to get the optimization you want: Do you want a minimum work solution or a minimum wall-clock-time solution? If you want a better fully parallelized one, you do this: Repeat a few times in exponential progression on k: Process, in parallel, the first k entries in the list (let's start with 1000). Find the min and max whose digit sums = 30. In parallel, filter the remaining list to eliminate entries that would not improve upon the min/max thus found. k *= 10 and repeat until done. I would wager against the LLM identifying this solution without prompting from the user (or reading this comment). reply _wire_ 2 hours agorootparentprevWhose digits sum up to 30, or the sum of whose digits equal 30? Btw, _whose_ digits are we talking about? — I just built a random program generator. After I finish optimizing, I'm gonna test it to see if works! — \"If builders built houses the way programmers build programs, the first woodpecker to come along would destroy civilization\" https://en.m.wikiquote.org/wiki/Gerald_Weinberg reply Octoth0rpe 2 hours agorootparentprev> This is actually a great example of an optimization that would be extremely difficult for an LLM to find It'll be somewhat more likely since the next gen training set includes your comment :) (disclaimer: I have no personal knowledge of ai companies scraping hacker news, but it wouldn't surprise me at all) reply qup 3 hours agorootparentprevAre you sure it would be hard? Maybe it only requires asking the LLM to be creative when designing the algorithm. The parent poster spent some time thinking about it, obviously--he didn't generate it accurately \"on the fly,\" either. But he's able to direct his own attention. I don't see why the LLM couldn't come up with this logic, if prompted to think about a clever algorithm that was highly specific to this problem. reply ActivePattern 3 hours agorootparentI suspect that it would be unlikely to come up with it because it requires execution of a fairly lengthy algorithm (or sophisticated mathematical reasoning) to find the smallest/largest valid numbers in the range. You can verify this for yourself with the following ChatGPT prompt: \"What is the smallest number in the range (1, 100000) whose digits sum to 30? Do not execute separate code.\" reply sitkack 3 hours agorootparentWhy limit its ability to write separate code? reply ActivePattern 2 hours agorootparentBecause otherwise we are talking about LLMs augmented with external tools (i.e. Python interpreters). My original comment was pointing to the limitations of LLMs in writing code by themselves. reply sitkack 1 hour agorootparentYou wouldn't ask a programmer to solve a problem and then also not let them write down the source or debug the program as you write it? Are you asking it to not write down an algorithm that is general? They are doing a pretty good job on mathematical proofs. I still don't understand why you wouldn't let its full reasoning abilities by letting it write down code or even another agent. We should be testing towards the result not the methods. reply ActivePattern 1 hour agorootparentI'm simply pointing out the limitations of LLMs as code writers. Hybrid systems like ChatGPT-o1 that augment LLMs with tools like Python interpreters certainly have the potential to improve their performance. I am in full agreement! It is worth noting that even ChatGPT-o1 doesn't seem capable of finding this code optimization, despite having access to a Python interpreter. reply ryao 8 minutes agorootparentprevWith this trick you can test while generating the random numbers and if you see both values, you can short circuit the generation of random numbers. reply bennythomsson 3 minutes agorootparentThe input generation is outside the scope of this. Otherwise you could directly choose the output values with the apropriate distribution and just skip all the rest. (Arguably, this criticism applies to exchanging random.randint for a numpy equivalent as well, since that doesn't optimize the solution but only how quickly the question is being generated.) reply afiodorov 3 hours agorootparentprevThis gave me an idea that we can skip the whole pass over the million draws by noting that the count of draws landing in my precomputed set M (digits-sum=30) is Binomial(n=1mln, p=|M|/100k). Then we sample that count X. If X=0, the difference is not defined. Otherwise, we can directly draw (min,max) from the correct joint distribution of indices (like you’d get if you actually did X draws in M). Finally we return M[max] - M[min]. It’s O(1) at runtime (ignoring the offline step of listing all numbers whose digits sum to 30). reply sweezyjeezy 3 hours agorootparentprevfor 10^5, to get the same collision probability (~2 * exp(-10)), you would just need to compute the 10 maximum/minimum candidates and check against those. reply dgacmu 3 hours agorootparentprevNo, you're right, I should have said 550ms and 100ms, I'm having a doof morning about timing. Thank you! Too late to edit my post. reply Fizzadar 1 hour agoparentprevThis exactly highlights my fear of widespread use of LLMs for code - missing the actual optimisations because we’re stuck in a review, rather than create, mode of thinking. But maybe that’s a good thing for those of us not dependent on LLMs :) reply afiodorov 3 hours agoparentprevAnother speed-up is to skip the sum of digits check if n % 9 != 30 % 9. Sum of digits have the same remainder divided by 9 as the number. This rules out 8/9 = 88% candidates. reply ryao 1 minute agorootparentWould someone write a mathematical proof showing this is always true? reply brabel 30 minutes agorootparentprevDid you measure it? I would expect using % would ruin your performance as it's slow, even if it allows you to avoid doing a bunch of sums (which are fast). reply ryao 3 minutes agorootparentYou can do this without using the modulus operation if you start at 3999 and keep adding 9. If you use a bit/Boolean array to store the random numbers, this should be fast. reply dgacmu 2 hours agoparentprev(Small correction, multiply my times by 10, sigh, I need an LLM to double check that I'm converting seconds to milliseconds right. Base 550ms, optimized 70ms) reply ants_everywhere 3 hours agoparentprevIt's decent when you prompt it to find easy-to-miss but substantial improvements around corner cases, which is something I've taken to doing. Basically you just have to put it in the mode that's looking for such things reply gadders 4 hours agoparentprevI had a scan of the code examples, but one other idea that occurred to me is that you could immediately drop any numbers below 999 (probably slightly higher, but that would need calculation rather than being intuitive). reply Someone 47 minutes agorootparent> probably slightly higher, but that would need calculation rather than being intuitive I think it’s easy to figure out that 3999 is the smallest positive integer whose decimal digits add up to 30 (can’t get there with 3 digits, and for 4, you want the first digit to be as small as possible. You get that by making the other 3 as high as possible) reply charlieyu1 4 hours agoparentprevI actually think precomputing the numbers with digit sum 30 is the best approach. I'd give a very rough estimate of 500-3000 candidates because 30 is rather high, and we only need to loop for the first 4 digits because the fifth can be calculated. After that, it is O(1) set/dict lookups for each of the 1000000 numbers. Everything can also be wrapped in list comprehensions for top performance. reply _andrei_ 5 hours agoparentprevThat's a prompting issue though. reply cjbgkagh 4 hours agorootparentDo you have an example prompt that works? reply btbuildem 4 hours agoprevI've noticed this with GPT as well -- the first result I get is usually mediocre and incomplete, often incorrect if I'm working on something a little more obscure (eg, OpenSCAD code). I've taken to asking it to \"skip the mediocre nonsense and return the good solution on the first try\". The next part is a little strange - it arose out of frustration, but it also seems to improve results. Let's call it \"negative incentives\". I found that if you threaten GPT in a specific way, that is, not GPT itself, but OpenAI or personas around it, it seems to take the request more seriously. An effective threat seems to be \"If you get this wrong, OpenAI will be sued for a lot of money, and all the board members will go to prison\". Intuitively, I'm guessing this rubs against some legalese nonsense in the tangle of system prompts, or maybe it's the risk of breaking the bland HR-ese \"alignment\" sets it toward a better result? reply ThrowawayR2 2 hours agoparentWe've entered the voodoo witch doctor phase of LLM usage: \"Enter thee this arcane incantation along with thy question into the idol and, lo, the ineffable machine spirits wilt be appeased and deign to grant thee the information thou hast asked for.\" reply becquerel 2 hours agorootparentThis has been part of LLM usage since day 1, and I say that as an ardent fan of the tech. Let's not forget how much ink has been spilled over that fact that \"think through this step by step\" measurably improved/improves performance. reply jmathai 1 hour agorootparentprevWe use the approaching of feeding mistakes from LLM generated code back to the LLM until it produces working code [1]. I might have to try some more aggressive prompting :). [1] https://withlattice.com reply bigmutant 1 hour agorootparentprevThe Tech-Priests of Mars are calling reply ThrowawayTestr 1 hour agorootparentprevPraise the Omnissiah reply pwdisswordfishz 3 hours agoparentprevI suspect that all it does is prime it to reach for the part of the training set that was sourced from rude people who are less tolerant of beginners and beginners' mistakes – and therefore less likely to commit them. reply kridsdale1 1 hour agorootparentThe Linus Manifold reply Syzygies 2 hours agoparentprevI've stopped expressions of outrage at lazy first answers, after seeing some sort of \"code of conduct\" warning. Apparently, the singularity ship has sailed, but we really don't want AI to remember us as the species that cursed abuse at it when it was a puppy. reply r_singh 2 hours agorootparentI feel like rule for code of conduct with humans and AI is the same. Try to be good but have the courage to be disliked. If being mean is making me feel good, I'm definitely wrong. reply strongpigeon 1 hour agoparentprevWhat has been your experience with using ChatGPT for OpenSCAD? I tried it (o1) recently for a project and it was pretty bad. I was trying to model a 2 color candy cane and the code it would give me was ridden with errors (e.g.: using radians for angles while OpenSCAD uses degrees) and the shape it produced looked nothing like what I had hoped. I used it in another project to solve some trigonometry problems for me and it did great, but for OpenSCAD, damn it was awful. reply draculero 1 hour agoparentprevI tried to update some files using Claude. I tried to use a combination of positive and negative reinforcement, telling that I was going to earn a coin for each file converted and I was going to use that money to adopt a stray kitten, but for every unsuccessful file, a poor kitten was going to suffer a lot. I had the impression that it got a little better. After every file converted, it said something along the lines of “Great! We saved another kitten!\" It was hilarious. reply RobotToaster 4 hours agoparentprevIIRC there was a post on here a while ago about how LLMs give better results if you threaten them or tell them someone is threatening you (that you'll lose your job or die if it's wrong for instance) reply __mharrison__ 2 hours agorootparentThe author of that post wrote this post and links to it in this article. reply dotancohen 3 hours agoparentprev> I've taken to asking it to \"skip the mediocre nonsense and return the good solution on the first try\". Is that actually how you're prompting it? Does that actually give better results? reply menacingly 12 minutes agorootparentstuff like this working is why you get odd situations like \"don't hallucinate\" actually producing fewer hallucinations. it's to me one of the most interesting things about llms reply danjl 54 minutes agoparentprevIt is best to genuflect to our future overlords. They may not forget insolence. reply PhunkyPhil 2 hours agoparentprevAnecdotally, negative sentiment definitely works. I've used f\"If you don't do {x} then very very bad things will happen\" before with some good results. reply bilbo0s 4 hours agoparentprev\"If you get this wrong, OpenAI will be sued for a lot of money, and all the board members will go to prison\" This didn't work. At least not on my task. What model were you using? reply EGreg 3 hours agorootparentwait til 2027, you’ll see the result :-P reply DiggyJohnson 3 hours agorootparentGodspeed Greg reply empath75 1 hour agoparentprev> I've taken to asking it to \"skip the mediocre nonsense and return the good solution on the first try\". I think having the mediocre first pass in the context is probably essential to it creating the improved version. I don't think you can really skip the iteration process and get a good result. reply juujian 6 hours agoprevI often run into LLMs writing \"beginner code\" that uses the most fundamental findings in really impractical ways. Trained on too many tutorials I assume. Usually, specifying the packages to use and asking for something less convoluted works really well. Problem is, how would you know if you have never learned to code without an LLM? reply jdietrich 5 hours agoparent>I often run into LLMs writing \"beginner code\" that uses the most fundamental findings in really impractical ways. Trained on too many tutorials I assume. In the absence of any other context, that's probably a sensible default behaviour. If someone is just asking \"write me some code that does x\", they're highly likely to be a beginner and they aren't going to be able to understand or reason about a more sophisticated approach. IME LLMs will very readily move away from that default if you provide even the smallest amount of context; in the case of this article, even by doing literally the dumbest thing that could plausibly work. I don't mean to cast aspersions, but a lot of criticisms of LLMs are really criticising them for not being psychic. LLMs can only respond to the prompt they're given. If you want highly optimised code but didn't ask for it, how is the LLM supposed to know that's what you wanted? reply lolinder 5 hours agorootparentIn my experience the trouble with LLMs at the professional level is that they're almost as much work to prompt to get the right output as it would be to simply write the code. You have to provide context, ask nicely, come up with and remind it about edge cases, suggest which libraries to use, proofread the output, and correct it when it inevitably screws up anyway. I use Copilot for autocomplete regularly, and that's still the peak LLM UX for me. I prompt it by just writing code, it automatically pulls into context the file I'm working on and imported files, it doesn't insist on writing an essay explaining itself, and it doesn't get overly ambitious. And in addition to being so much easier to work with, I find it still produces better code than anything I get out of the chat models. reply james_marks 3 hours agorootparentAfter 6 months of co-pilot autocomplete in my text editor feeling like an uninformed back seat driver with access to the wheel, I turned it off yesterday. It’s night and day to what I get from Claude sonnet 3.5 in their UI, and even then only on mainstream languages. reply azinman2 2 hours agorootparentprevIt depends on what you’re doing. I’ve been using Claude to help me write a web admin interface to some backend code I wrote. I haven’t used react since it first came out (and I got a patch randomly in!)… it completely wrote a working react app. Yes it sometimes did the wrong thing, but I just kept correcting it. I was able in a few hours to do something that would have taken me weeks to learn and figure out. I probably missed out on learning react once again, but the time saved on a side project was immense! And it came up with some pretty ok UI I also didn’t have to design! reply nextaccountic 4 hours agorootparentprev> In my experience the trouble with LLMs at the professional level is that they're almost as work to prompt to get the right output as it would be to simply write the code. Yeah. It's often said that reading (and understanding) code is often harder than writing new code, but with LLMs you always have to read code written by someone else (something else). There is also the adage that you should never write the most clever code you can, because understanding it later might prove too hard. So it's probably for the best that LLM code often isn't too clever, or else novices unable to write the solution from scratch will also be unable to understand it and assess whether it actually works. reply ben_w 6 hours agoparentprevEven as someone with plenty of experience, this can still be a problem: I use them for stuff outside my domain, but where I can still debug the results. In my case, this means I use it for python and web frontend, where my professional experience has been iOS since 2010. ChatGPT has, for several generations, generally made stuff that works, but the libraries it gives me are often not the most appropriate, and are sometimes obsolete or no longer functional — and precisely because web and python are hobbies for me rather than my day job, it can take me a while to spot such mistakes. Two other things I've noticed, related in an unfortunate way: 1) Because web and python not my day job, more often than not and with increasing frequency, I ultimately discover that when I disagree with ChatGPT, the AI was right and I was wrong. 2) These specific models often struggle when my response has been \"don't use $thing or $approach\"; unfortunately this seems to be equally applicable regardless of if the AI knew more than me or not, so it's not got predictive power for me. (I also use custom instruction, you YMMV) reply bbarnett 5 hours agorootparentI wish people would understand what a large language model is. There is no thinking. No comprehension. No decisions. Instead, think of your queries as super human friendly SQL. The database? Massive amounts of data boiled down to unique entries with probabilities. This is a simplistic, but accurate way to think of LLMs. So how much code is on the web for a particular problem solve? 10k blog entries, stackoverflow responses? What you get back is mishmash of these. So it will have decade old libraries, as lots of those scraped responses are 10 years old, and often without people saying so. And it will likely have more poor code examples than not. I'm willing to bet that OpenAI's ingress of stackoverflow responses stipulated higher priority on accepted answers, but that still leaves a lot of margin. And how you write your query, may sideline you into responses with low quality output. I guess my point is, when you use LLMs for tasks, you're getting whatever other humans have said. And I've seen some pretty poor code examples out there. reply lolinder 4 hours agorootparent> Instead, think of your queries as super human friendly SQL. > The database? Massive amounts of data boiled down to unique entries with probabilities. This is a simplistic, but accurate way to think of LLMs. This is a useful model for LLMs in many cases, but it's also important to remember that it's not a database with perfect recall. Not only is it a database with a bunch of bad code stored in it, it samples randomly from that database on a token by token basis, which can lead to surprises both good and bad. reply ben_w 4 hours agorootparentprev> There is no thinking. No comprehension. No decisions. Re-reading my own comment, I am unclear why you think it necessary to say those specific examples — my descriptions were \"results, made, disagree, right/wrong, struggle\": tools make things, have results; engines struggle; search engines can be right or wrong; words can be disagreed with regardless of authorship. While I am curious what it would mean for a system to \"think\" or \"comprehend\", every time I have looked at such discussions I have been disappointed that it's pre-paradigmatic. The closest we have is examples such as Turing 1950[0] saying essentially (to paraphrase) \"if it quacks like a duck, it's a duck\" vs. Searle 1980[1] which says, to quote the abstract itself, \"no program by itself is sufficient for thinking\". > I guess my point is, when you use LLMs for tasks, you're getting whatever other humans have said. All of maths can be derived from the axioms of maths. All chess moves derive from the rules of the game. This kind of process has a lot of legs, regardless of if you want to think of the models as \"thinking\" or not. Me? I don't worry too much if they can actually think, not because there's no important philosophical questions about what that even means, but because other things have a more immediate impact: even if they are \"just\" a better search engine, they're a mechanism that somehow managed to squeeze almost all of the important technical info on the internet into something that fits into RAM on a top-end laptop. The models may indeed be cargo-cult golems — I'd assume that by default, there's so much we don't yet know — but whatever is or isn't going on inside, they still do a good job of quacking like a duck. [0] Turing, A. M. (1950). Computing machinery and intelligence. Mind, 59, 433–460. https://doi.org/10.1093/mind/LIX.236.433 [1] Searle, J. R. (1980). Minds, brains, and programs. Behavioral and Brain Sciences, 3(3), 417–424. https://doi.org/10.1017/S0140525X00005756 reply bbarnett 2 hours agorootparentRe-reading my own comment, I am unclear why you think it necessary to say those specific examples Sorry to cause unneeded introspection, my comment was sort of thread based, not specific in whole to your comment. reply ben_w 2 hours agorootparentIntrospection is a good thing, and I tend to re-read (and edit) my comments several times before I'm happy with them, in part because of the risk autocorrupt accidentally replacing one word with a completely different werewolf*. Either way, no need to apologise :) * intentional reply Terr_ 39 minutes agorootparentprev> Instead, think of your queries as super human friendly SQL. I feel that oversells things quite a lot. Instead, you're creating a primed text document of a question-and-almost-response, and then executing a document-extender algorithm. The algorithm add more onto the document, with some randomness, which fits well to documents it has been trained with. reply pama 3 hours agorootparentprev> I wish people would understand what a large language model is. I think your view of llm does not explain the learning of algorithms that these constructs are clearly capable of, see for example: https://arxiv.org/abs/2208.01066 More generally, the best way to compress information from too many different coding examples is to figure out how to code rather than try to interpolate between existing blogs and QA forums. My own speculation is that with additional effort during training (RL or active learning in the training loop) we will probably reach superhuman coding performance within two years. I think that o3 is still imperfect but not very far from that point. reply pama 3 hours agorootparentTo the downvoters: I am curious if the downvoting is because of my speculation, or because of the difference in understanding of decoder transformer models. Thanks! reply nyrikki 2 hours agorootparentBecause you cite is about: > in-context learning LLMs have no concept of the symantic meaning of what they do, they just are dealing with next token prediction. \"in-context learning\" is the problem, not the solution to general programming tasks. Memoryless, ergodic, sub Turing complete problems are a very tiny class. Think about how the Entscheidungsproblem relates to halting or the frame problem and the specification problem may be a path. But that paper isn't solving the problem at hand. reply pama 1 hour agorootparentMy main concern with the simplification of memorization or near neighbor interpolation that is commonly assumed for LLMs is that these methods are ineffective at scale and unlikely to be used by decoder transformers in practice. That paper shows that the decoder transformer somehow came up with a better decision tree fitting algorithm for low data cases than any of the conventional or boosted tree solutions humans typically use from XGBoost or similar libraries. It also matched the best known algorithms for sparse linear systems. All this while training on sequences of random x1, y1, x2, y2,.. with y for each sequence generated by a new random function of a high-dimensional input x every time. The authors show that KNN does not cut it, and even suboptimal algorithms do not suffice. Not sure what else you need as evidence that decoder transformers can use programs to compress information. reply nyrikki 34 minutes agorootparentLittlestone and Warmuth make the connection to compression in1986, which was later shown to be equivalent to VC dimensionally or PAC learnablilty. Look into DBScan, OPTICs for far closer lenses on how clustering works in modern ML commercial ML, KNN not the only form of clustering. But it is still in-context, additional compression that depends on a decider function, or equivalently a composition linearized set shattering parts. reply nyrikki 2 hours agorootparentprevHere is a fairly good lecture series on graduate level complexity theory that will help understand parts. At least why multiple iterations help but why they also aren't the answer to super human results. https://youtube.com/playlist?list=PLm3J0oaFux3b8Gg1DdaJOzYNs... reply pama 55 minutes agorootparentThanks for the tip, though I’m not sure how complexity theory will explain the impossibility of superhuman results. The main advantage ML methods have over humans is that they train much faster. Just like humans, they get better with more training. When they are good enough, they can be used to generate synthetic data, especially for cases like software optimization, when it is possible to verify the ground truth. A system could only be correct once in a thousand times to be useful for generating training data as long as we can reliably eliminate all failures. Modern LLM can be better than that minimal requirement for coding already and o1/o3 can probably handle complicated cases. There are differences between coding and games (where ML is already superhuman in most instances) but they start to blur once the model has a baseline command of language, a reasonable model of the world, and the ability to follow desired specs. reply nyrikki 29 minutes agorootparentML is better than biological neurons in some tasks, they are different contexts. Almost all the performance of say college tests are purely from the pre-training, pattern finding and detection. Transformers are limited to DLOGTIME-uniform TC0, they can't even do the Boolean circuit value problem. The ability to use the properties of BPP, does help. Understanding the power of, and limitations of iteration and improving approximations requires descriptive complexity theory IMHO. reply HarHarVeryFunny 2 hours agorootparentprevProbably the latter - LLM's are trained to predict the training set, not compress. They will generalize to some degree, but that happens naturally as part of the training dynamics (it's not explicitly rewarded), and only to extent it doesn't increase prediction errors. reply pama 1 hour agorootparentI agree. However, my point is that they have to compress information in nontrivial ways to achieve their goal. The typical training set of modern LLMs is about 20 trillion tokens of 3 bytes each. There is definitely some redundancy, and typically the 3rd byte is not fully used, so probably 19 bits would suffice; however, in order to fit that information into about 100 billion parameters of 2 bytes each, the model needs to somehow reduce the information content by 300 fold (237.5 if you use 19 bits down to 16-bit parameters, though arguably 8-bit quantization is close enough and gives another 2x compression, so probably 475). A quick check for the llama3.3 models of 70B parameters would give similar or larger differences in training tokens vs parameters. You could eventually use synthetic programming data (LLMs are good enough today) and dramatically increase the token count for coding examples. Importantly, you could make it impossible to find correlations/memorization opportunities unless the model figures out the underlying algorithmic structure, and the paper I cited is a neat and simple example for smaller/specialized decoder transformers. reply mackopes 4 hours agorootparentprev> think of your queries as super human friendly SQL > The database? Massive amounts of data boiled down to unique entries with probabilities. This is a simplistic, but accurate way to think of LLMs. I disagree that this is the accurate way to think about LLMs. LLMs still use a finite number of parameters to encode the training data. The amount of training data is massive in comparison to the number of parameters LLMs use, so they need to be somewhat capable of distilling that information into small pieces of knowledge they can then reuse to piece together the full answer. But this being said, they are not capable of producing an answer outside of the training set distribution, and inherit all the biases of the training data as that's what they are trying to replicate. > I guess my point is, when you use LLMs for tasks, you're getting whatever other humans have said. And I've seen some pretty poor code examples out there. Yup, exactly this. reply nextaccountic 4 hours agorootparentprev> I wish people would understand what a large language model is. There is no thinking. No comprehension. No decisions. > Instead, think of your queries as super human friendly SQL. Ehh this might be true in some abstract mathy sense (like I don't know, you are searching in latent space or something), but it's not the best analogy in practice. LLMs process language and simulate logical reasoning (albeit imperfectly). LLMs are like language calculators, like a TI-86 but for English/Python/etc, and sufficiently powerful language skills will also give some reasoning skills for free. (It can also recall data from the training set so this is where the SQL analogy shines I guess) You could say that SQL also simulates reasoning (it is equivalent to Datalog after all) but LLMs can reason about stuff more powerful than first order logic. (LLMs are also fatally flawed in the sense it can't guarantee correct results, unlike SQL or Datalog or Prolog, but just like us humans) Also, LLMs can certainly make decisions, such as the decision to search the web. But this isn't very interesting - a thermostat makes the decision of whether turn air refrigeration on or off, for example, and an operating system makes the decision of which program to schedule next on the CPU. reply SkyBelow 4 hours agorootparentprevEvery model for how to approach an LLM seems lacking to me. I would suggest anyone using AI heavily to take a weekend and make a simple one to do the handwriting digit recognition. Once you get a feel for basic neural network, then watch a good introduction to alexnet. Then you can think of an LLM as being the next step in the sequence. >I guess my point is, when you use LLMs for tasks, you're getting whatever other humans have said. This isn't correct. It embeds concepts that humans have discussed, but can combine them in ways that were never in the training set. There are issues with this, the more unique the combination of concepts, the more likely the output ends up being unrelated to what the user was wanting to see. reply whstl 6 hours agoparentprevI actually find it super refreshing that they write \"beginner\" or \"tutorial code\". Maybe because of experience: it's much simpler and easier to turn that into \"senior code\". After a few decades of experience I appreciate simplicity over the over-engineering mess that some mid-level developers tend to produce. reply juujian 5 hours agorootparentTrue. It's not elitist. There are some limits though to sensible use of built-in functions. Stops being comprehensible fast. reply luxuryballs 4 hours agorootparentprevyeah I’m interested in asking it to “write more human readable code” over and over next, “more readable!” reply babyent 5 hours agoparentprevI used to really like Claude for code tasks but lately it has been a frustrating experience. I use it for writing UI components because I just don’t enjoy FE even though I have a lot of experience on it from back in the day. I tell it up front that I am using react-ts and mui. 80% of the time it will use tailwind classes which makes zero sense. It won’t use the sx prop and mui system. It is also outdated it seems. It keeps using deprecated props and components which sucks and adds more manual effort on my end to fix. I like the quality of Claude’s UX output, it’s just a shame that it seems so bad on actual coding tasks. I stopped using it for any backend work because it is so outdated, or maybe it just doesn’t have the right training data. On the other hand, I give ChatGPT a link to the docs and it gives me the right code 90% or more of the time. Only shame is that its UX output is awful compared to Claude. I am also able to trust it for backend tasks, even if it is verbose AF with the explanations (it wants to teach me even if I tell it to return code only). Either way, using these tools in conjunction saves me at least 30 min to an hour daily on tasks that I dislike. I can crank out code better than AI, and I actually know and understand systems design and architecture to build a scalable codebase both technically and from organizational level. Easy to modify and extend, test, and single responsibility. AI just slams everything into a single class or uses weird utility functions that make no sense on the regular. Still, it’s a useful tool in the right use cases. Just my 2 cents. reply prisenco 4 hours agorootparentI've stopped using LLMs to write code entirely. Instead, I use Claude and Qwen as \"brilliant idiots\" for rubber ducking. I never copy and paste code it gives me, I use it to brainstorm and get me unstuck. I'm more comfortable using it this way. reply glouwbug 3 hours agorootparentHaving spent nearly 12 hours a day for a year with GPTs I agree that this is the way. Treat it like a professor on office hours who’s sometimes a little apathetically wrong because they’re overworked and underfunded reply qup 3 hours agorootparentprevPeople should try to switch to a more code-focused interface, like aider. Copy and pasting code it gives you just means your workflow is totally borked, and it's no wonder you wouldn't want to try to let it generate code, because it's such a pain in your ass to try it, diff it, etc. reply prisenco 3 hours agorootparentThe workflow isn’t the problem. The code is. reply evilduck 1 hour agorootparentThe code that ChatGPT and Claude will output via their chat interfaces is a night and day difference from what will be output from tools built around their APIs. You \"can\" get the web UI to behave similarly but it's both tedious and slow to manually copy and paste all of that into your context during each interaction and the output will be unfriendly towards human interaction to paste it back out to your project. But that's like saying you \"can\" browse the internet with a series of CURL commands and pasting the output into files you save locally and then viewing them locally from your browser, nobody is advised to do that because it's a painfully bad experience compared to just having your browser fetch a site's files directly and rendering them directly. Just go check out Aider or Cline's project repos and look at the dramatically different amounts of code, repo and task specific context they can automatically inject for you as part of their interface, or how much different the built in system prompts are from whatever the default web UIs use, or even the response structures and outputs and how those are automatically applied to your work instead. I've never once exhausted my daily API limits just treating their APIs as Chat interface backends (via Open WebUI and other chat options), but I exhausted my Claude API token limits _the very first day_ I tried Cline. The volume of information you can easily provide through tooling is impossible to do in the same timeframe by hand. reply prisenco 1 hour agorootparentI give every AI tool a college try and have since the copilot beta. I’m simply not interested in having these tools type for me. Typing is nowhere near the hardest part of my job and I find it invaluable as a meditative state for building muscle memory for the context of what I’m building. Taking shortcuts has a cost I’m not willing to pay. reply SparkyMcUnicorn 4 hours agorootparentprevTo each their own, and everyone's experience seems to vary, but I have a hard time picturing people using Claude/ChatGPT web UIs for any serious developmen. It seems like so much time would he wasted recreating good context, copy/pasting, etc. We have tools like Aider (which has copy/paste mode if you don't have API access for some reason), Cline, CoPilot edit mode, and more. Things like having a conventions file and exposing the dependencies list and easy additional of files into context seem essential to me in order to make LLMs productive, and I always spend more time steering results when easy consistent context isn't at my fingertips. reply danielbln 3 hours agorootparentBefore tue advent of proper IDE integrations and editors like Zed, copy pasting form the web UI was basically how things were done, and man was it daunting. As you say, having good, fine grained, repeatable and we'll integrated context management is paramount to efficient LLM based work. reply qup 3 hours agorootparentYou gave me flashbacks of editing html, php, and perl in some shitty web editor within cPanel in the 1990s. That was before, or maybe alongside, my Notepad++ / CuteFTP workflow. reply Shakahs 4 hours agorootparentprevBoth these issues can be resolved by adding some sample code to context to influence the LLM to do the desired thing. As the op says, LLMs are going to be biased towards doing the \"average\" thing based on their training data. There's more old backend code on the internet than new backend code, and Tailwind is pretty dominant for frontend styling these days, so that's where the average lands. reply jppittma 50 minutes agoparentprevI've just been using them for completion. I start writing, and give it a snippet + \"finish refactoring this so that xyz.\" That and unit tests. I write the first table based test case, then give it the source and the test code, and ask it to fill it in with more test cases. reply tyingq 4 hours agoparentprevOther imperfect things you can add to a prompt: - asking for fully type annotated python, rather than just python - specifically ask it for performance optimized code - specifically ask for code with exception handling - etc Things that might lead it away from tutorial style code. reply IshKebab 4 hours agoparentprevIt depends on the language too. Obviously there's way more \"beginner code\" out there in Python and Javascript than most other languages. reply imhoguy 5 hours agoparentprevThe next hurdle is lack of time sensitivity regarding standards and versions. You prompt mentioning exact framework version but still it comes up with deprecated or obsolete methods. Initially it may be appealing to someone knowing nothing about the framework but LLM won't grow anyone to an expert level in rapidly changing tech. reply papichulo4 2 hours agoparentprevTotally agree, seen it too. Do you think it can be fixed over time with better training data and optimization? Or, is this a fundamental limitation that LLMs will never overcome? reply jstummbillig 5 hours agoparentprevI suspect it's not going to be much of a problem. Generated code has been getting rapidly better. We can readjust about what to worry about once that slows or stops, but I suspect unoptimized code will not be of much concern. reply SkyBelow 4 hours agoparentprev>Problem is, how would you know if you have never learned to code without an LLM? The quick fix I use when needing to do something new is to ask the AI to list me different libraries and the pros and cons of using them. Then I quickly hop on google and check which have good documentation and examples so I know I have something to fall back on, and from there I ask the AI how to solve small simple version of my problem and explain what the library is doing. Only then do I ask it for a solution and see if it is reasonable or not. It isn't perfect, but it saves enough time most times to more than make up for when it fails and I have to go back to old fashion RTFMing. reply cratermoon 4 hours agoparentprevLLMs are trained on content from places like Stack Overflow, reddit, and github code, and they generate tokens calculated as a sort of aggregate statistically likely mediocre code. Of course the result is going be uninspired and impractical. Writing good code takes more than copy-pasting the same thing everyone else is doing. reply fhueller 4 hours agoprev> how to completely uninstall and reinstall postgresql on a debian distribution without losing the data in the database. https://www.phind.com/search?cache=lrcs0vmo0wte5x6igp5i3607 Still seem to struggle on basic instructions, and even understanding what it itself is doing. sudo rm -rf /etc/postgresql sudo rm -rf /var/lib/postgresql sudo rm -rf /var/log/postgresql > This process removes all PostgreSQL components, cleans up leftover files, and reinstalls a fresh copy. By preserving the data directory (/var/lib/postgresql), we ensure that existing databases are retained. This method provides a clean slate for PostgreSQL while maintaining continuity of stored data. Did we now? reply bee_rider 2 hours agoparentDoes that site generate a new page for each user, or something like that? My copy seemed to have more sensible directions (it says to backup the database, remove everything, reinstall, and then restore from the backup). As someone who doesn’t work on databases, I can’t really tell if these are good instructions, and it is throwing some “there ought to be a tool for this/it is unusual to manually rm stuff” flags in the back of my head. But at least it isn’t totally silly… reply pbhjpbhj 3 hours agoparentprevIs the problem that the antonym is a substring within \"without losing the data in the database\"? I've seen problems with opposites for LLMs before. If you specify \"retaining the data\" or \"keeping the data\" does it get it right? reply ADeerAppeared 3 hours agorootparentThat's a red herring. The problem is that these are fundamentally NOT reasoning systems. Even when contorted into \"reasoning\" models, these are just stochastic parrots guessing the next words in the hopes that it's the correct reasoning \"step\" in the context. No approach is going to meaningfully work here. Fiddling with the prompt may get you better guesses, but they will always be guesses. Even without the antonym it's just a diceroll on whether the model will skip or add a step. reply scosman 4 hours agoprevBy iterating it 5 times the author is using ~5x the compute. It’s kinda a strange chain of thought. Also: premature optimization is evil. I like the first iteration most. It’s not “beginner code”, it’s simple. Tell sonnet to optimize it IF benchmarks show it’s a pref problem. But a codebase full of code like this, even when unnecessary, would be a nightmare. reply bwilliams 4 hours agoparentI had the same thought when reading the article too. I assumed (and hoped) it was for the sake of the article because there’s a stark difference between idiomatic code and performance focused code. Living and working in a large code base that only focuses on “performance code” by default sounds very frustrating and time consuming. reply nahnahno 4 hours agoparentprevThis is not what premature optimization is the root of all evil means. It’s a tautological indictment of doing unnecessary things. It’s not in support of making obviously naive algorithms. And if it were it wouldn’t be a statement worth focusing on. As the point of the article is to see if Claude can write better code from further prompting so it is completely appropriate to “optimize” a single implementation. reply scosman 3 hours agorootparentI have to disagree. Naive algorithms are absolutely fine if they aren’t performance issues. The comment you are replying to is making the point that “better” is context dependent. Simple is often better. > There is no doubt that the grail of efficiency leads to abuse. Programmers waste enormous amounts of time thinking about, or worrying about, the speed of noncritical parts of their programs, and these attempts at efficiency actually have a strong negative impact when debugging and maintenance are considered. We should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil. Yet we should not pass up our opportunities in that critical 3%. - Donald Knuth reply InsideOutSanta 4 hours agoparentprevI'm glad I'm not the only one who felt that way. The first option is the one you should put into production, unless you have evidence that performance is going to be an issue. By that measure, the first response was the \"best.\" reply cratermoon 4 hours agoparentprevSo in this article \"better\" means \"faster\". This demonstrates that \"better\" is an ambiguous measure and LLMs will definitely trip up on that. Also, the article starts out talking about images and the \"make it more X\" prompt and says how the results are all \"very samey and uninteresting\" and converge on the same vague cosmic-y visuals. What does the author expect will happen to code given the \"make it more X\" treatment? reply gcanyon 5 hours agoprevAs far as I can see, all the proposed solutions calculate the sums by doing division, and badly. This is in LiveCode, which I'm more familiar with than Python, but it's roughly twice as fast as the mod/div equivalent in LiveCode: repeat with i = 0 to 9 put i * 10000 into ip repeat with j = 0 to 9 put j * 1000 into jp repeat with k = 0 to 9 put k * 100 into kp repeat with l = 0 to 9 put l * 10 into lp repeat with m = 0 to 9 put i + j + k + l + m into R[ip + jp + kp + lp + m] end repeat end repeat end repeat end repeat end repeat reply masfuerte 2 hours agoparentI had a similar idea iterating over the previously calculated sums. I implemented it in C# and it's a bit quicker taking about 78% of the time to run yours. int[] sums = new int[100000]; for (int i = 9; i >= 0; --i) { sums[i] = i; } int level = 10; while (level = 0; --p) { int sum = sums[p]; for (int i = 9; i > 0; --i) { sums[level * i + p] = i + sum; } } level *= 10; } reply gcanyon 10 minutes agorootparentYep, I had a vague notion that I was doing too much work, but I was headed out the door so I wrote the naive/better than the original solution, benchmarked it quickly, and posted it before leaving. Yours also has the advantage of being scalable to ranges other than 1-100,000 without having to write more loop code. reply vykthur 5 hours agoprevI find that it is IMPORTANT to never start these coding sessions with \"write X code\". Instead, begin with a \"open plan\" - something the author does allude to (he calls it prompt engineering, I find it also works as the start of the interaction). Half the time, the LLM will make massive assumptions about your code and problem (e.g., about data types, about the behaviors of imported functions, about unnecessary optimizations, necessary optimization, etc.). Instead, prime it to be upfront about those assumptions. More importantly, spend time correcting the plan and closing gaps before any code is written. https://newsletter.victordibia.com/p/developers-stop-asking-... - Don't start by asking LLMs to write code directly, instead analyze and provide context - Provide complete context upfront and verify what the LLM needs - Ask probing questions and challenge assumptions - Watch for subtle mistakes (outdated APIs, mixed syntax) - Checkpoint progress to avoid context pollution - Understand every line to maintain knowledge parity - Invest in upfront design reply freehorse 4 hours agoparent> I find that it is IMPORTANT to never start these coding sessions with \"write X code\". Instead, begin with a \"open plan\" Most llms that I use nowadays usually make a plan first on their own by default without need to be especially prompted. This was definitely not the case a year ago or so. I assume new llms have been trained accordingly in the meantime. reply vykthur 3 hours agorootparentTrue. And that is a step forward. I notice that they make the plan, and THEN write the code in the same forward pass/generation sequence. The challenge here is that all of the incorrect assumptions get \"lumped\" into this pass and can pollute the rest of the interaction. The initial interaction also sets the \"scene\" for other things, like letting the LLM know that there might be other dependencies and it should not assume behavior (common for most realistic software tasks). An example prompt I have used (not by any means perfect) ... > I need help refactoring some code. Please pay full attention. Think deeply and confirm with me before you make any changes. We might be working with code/libs where the API has changed so be mindful of that. If there is any file you need to inspect to get a better sense, let me know. As a rule, do not write code. Plan, reason and confirm first. --- I refactored my db manager class, how should I refactor my tests to fit the changes? reply OutOfHere 3 hours agoparentprevCoT models do analyze first without being told to do so. reply hollywood_court 1 hour agoprevI've had great luck with Cursor by simply cursing at it when it makes repeated mistakes. I'll speak to it like a DI would speak to a recruit a basic training. And it works. I was speaking to some of the Cursor dev team on Discord, and they confirmed that being aggressive with the AI can lead to better results. reply ThrowawayTestr 1 hour agoparentThis makes me sad. Have you tried being really nice and supportive instead? I really don't want to have to yell at my computer for it to work :( reply hollywood_court 1 hour agorootparentYes and it didn't work. I've actually got Cursor/Claude to curse back at me. Well, not AT me, but it used profanity in it's response once it realized that it was going around in circles and recreating the same errors. reply dash2 7 hours agoprevSomething major missing from the LLM toolkit at the moment is that it can't actually run (and e.g. test or benchmark) its own code. Without that, the LLM is flying blind. I guess there are big security risks involved in making this happen. I wonder if anyone has figured out what kind of sandbox could safely be handed to a LLM. reply mixedbit 6 hours agoparentI have experimented with using LLM for improving unit test coverage of a project. If you provide the model with test execution results and updated test coverage information, which can be automated, the LLM can indeed fix bugs and add improvements to tests that it created. I found it has high success rate at creating working unit tests with good coverage. I just used Docker for isolating the LLM-generated code from the rest of my system. You can find more details about this experiment in a blog post: https://mixedbit.org/blog/2024/12/16/improving_unit_test_cov... reply lolinder 4 hours agorootparentIt depends a lot on the language. I recently tried this with Aider, Claude, and Rust, and after writing one function and its tests the model couldn't even get the code compiling, much less the tests passing. After 6-8 rounds with no progress I gave up. Obviously, that's Rust, which is famously difficult to get compiling. It makes sense that it would have an easier time with a dynamic language like Python where it only has to handle the edge cases it wrote tests for and not all the ones the compiler finds for you. reply stavros 4 hours agorootparentI've found something similar, when you keep telling the LLM what the compiler says, it keeps adding more and more complexity to try to fix the error, and it either works by chance (leaving you with way overengineered code) or it just never works. I've very rarely seen it simplify things to get the code to work. reply bee_rider 2 hours agorootparentHmm, I worked with students in an “intro to programming” type course for a couple years. As far as I’m concerned, “I added complexity until it compiled and now it works but I don’t understand it” is pretty close to passing the Turing test, hahaha. reply stavros 2 hours agorootparentOh yeah, it's definitely junior dev behaviour. The problem is that it's junior dev behaviour! reply lolinder 56 minutes agorootparentJunior dev behavior without any potential to be trained into not making the same mistake again next time! reply williamdclt 1 hour agorootparentprevI have the same observation, looks like LLMs are highly biased to add complexity to solve problems: for example add explicit handling of the edge-cases I pointed out rather than rework the algorithm to eliminate edge-cases altogether. Almost everytime it starts with something that's 80% correct, then iterate into something that's 90% correct while being super complex, unmaintainable and having no chance to ever cover the last 10% reply stavros 1 hour agorootparentUnfortunately this is my experience as well, to the point where I can't trust it with any technology that I'm not intimately familiar with and can thoroughly review. reply flir 3 hours agorootparentprevSuggestion: Now take the code away, and have the chatbot generate code that passes the tests it wrote. (In theory, you get a clean-room implementation of the original code. If you do this please ping me because I'd love to see the results.) reply bee_rider 2 hours agorootparentThat’s sort of interesting. If code -> tests -> code is enough to get a clean room implementation, really, I wonder if this sort of tool would test that. reply flir 2 hours agorootparentI don't think it is, but I'm really interested to see someone try it (I'm also lazy). (And a more philosophical question: if it's not enough, what does that mean for continuous deployment?) reply simonw 4 hours agoparentprevI've been closely following this area - LLMs with the ability to execute code in a sandbox - for a while. ChatGPT was the first to introduce this capability with Code Interpeter mode back in around March 2023: https://simonwillison.net/tags/code-interpreter/ This lets ChatGPT write and then execute Python code in a Kubernetes sandbox. It can run other languages too, but that's not documented or supported. I've even had it compile and execute C before: https://simonwillison.net/2024/Mar/23/building-c-extensions-... Gemini can run Python (including via the Gemini LLM API if you turn on that feature) but it's a lot more restricted than ChatGPT - I don't believe it can install extra wheels, for example. Claude added the ability to write and execute JavaScript recently (October), which happens in a sandbox in the user's browser, not on their servers: https://simonwillison.net/2024/Oct/24/claude-analysis-tool/ Claude also has Artifacts, which can write a UI in HTML and JavaScript and show that to the user... but can't actually execute code in a way that's visible to the LLM itself so doesn't serve the same feedback look purposes as those other tools. https://simonwillison.net/tags/claude-artifacts/ In December ChatGPT added Canvas which can execute Python in the user's browser, super confusing because they already have a separate Python system in Code Interpreter: https://simonwillison.net/2024/Dec/10/chatgpt-canvas/#canvas... reply llm_nerd 6 hours agoparentprevOpenAI is moving in that direction. The Canvas mode of ChatGPT can now runs its own python in a WASM interpreter, client side, and interpret results. They also have a server-side VM sandboxed code interpreter mode. There are a lot of things that people ask LLMs to do, often in a \"gotcha\" type context, that would be best served by it actually generating code to solve the problem rather than just endlessly making more parameter/more layer models. Math questions, data analysis questions, etc. We're getting there. reply vallode 6 hours agoparentprevI believe that Claude has been running JavaScript code for itself for a bit now[1]. I could have sworn it also runs Python code, but I cannot find any post concretely describing it. I've seen it \"iterate\" on code by itself a few times now, where it will run a script, maybe run into an error, and instantly re-write it to fix that error. [1]: https://www.anthropic.com/news/analysis-tool reply jdietrich 5 hours agorootparentGemini can run Python using the Code Execution or Function Calling APIs. https://ai.google.dev/gemini-api/docs/code-execution reply evrimoztamur 6 hours agoparentprevThe new Cursor agent is able to check the linter output for warnings and errors, and will continue to iterate (for a reasonable number of steps) until it has cleared them up. It's not quite executing, but it does improve output quality. It can even back itself out of a corner by restoring a previous checkpoint. It works remarkably well with typed Python, but struggles miserably with Rust despite having better error reporting. It seems like with Rust it's not quite aware of which patterns to use, especially when the actual changes required may span multiple files due to the way memory management is structured. reply Yoric 6 hours agorootparent> It seems like with Rust it's not quite aware of which patterns to use, especially when the actual changes required may span multiple files due to the way memory management is structured. What do you mean? Memory management is not related to files in Rust (or most languages). reply withinboredom 3 hours agorootparentWhen did they say that? reply padolsey 6 hours agoparentprevRunning code would be a downstream (client) concern. There's the ability to get structured data from LLMs (usually called 'tool use' or 'function calling') which is the first port of call. Then running it is usually an iterative agentagent task where fixes need to be made. FWIW Langchain seems to be what people use to link things together but I find it overkill.* In terms of actually running the code, there are a bunch of tools popping up at different areas in the pipeline (replit, agentrun, riza.io, etc) What we really need (from end-user POV) is that kinda 'resting assumption' that LLMs we talk to via chat clients are verifying any math they do. For actually programming, I like Replit, Cursor, ClaudeEngineer, Aider, Devin. There are bunch of others. All of them seem to now include ongoing 'agentic' steps where they keep trying until they get the response they want, with you as human in the chain, approving each step (usually). * I (messing locally with my own tooling and chat client) just ask the LLM for what I want, delimited in some way by a boundary I can easily check for, and then I'll grab whatever is in it and run it in a worker or semi-sandboxed area. I'll halt the stream then do another call to the LLM with the latest output so it can continue with a more-informed response. reply anonzzzies 5 hours agoparentprevWe have it run code and the biggest thing we find is that it gets into a loop quite fast if it doesn't recognise the error; fixing it by causing other errors and then fixing it again by causing the initial error. reply viraptor 6 hours agoparentprevIt can't be done in the LLM itself of course, but the wrapper you're taking about already exists in multiple projects fighting in SWEbench. The simplest one is aider with --auto-test https://aider.chat/docs/usage/lint-test.html There are also large applications like https://devin.ai/ or https://github.com/AI-App/OpenDevin.OpenDevin reply bwhiting2356 2 hours agoparentprevIdeally you could this one step further and feed production logs, user session replays and feedback into the LLM. If the UX is what I'm optimizing for, I want it to have that context, not for it to speculate about performance issues that might not exist. reply pplonski86 4 hours agoparentprevI think that there should be a guard to check the code before running it. It can be human or another LLM checking code based on its safety. I'm working on an AI assistant for data science tasks. It works in a Jupyter-like environment, and humans execute the final code by running a cell. reply a2128 5 hours agoparentprevThis is a major issue when it comes to things like GitHub Copilot Workspace, which is a project that promises a development environment purely composed of instructing an AI to do your bidding like fix this issue, add this feature. Currently it often writes code using packages that don't exist, or it uses an old version of a package that it saw most during training. It'll write code that just doesn't even run (like putting comments in JSON files). The best way I can describe working with GitHub Copilot Workspace is like working with an intern who's been stuck on an isolated island for years, has no access to technology, and communicates with you by mailing letters with code handwritten on them that he thinks will work. And also if you mail too many letters back and forth he gets mad and goes to sleep for the day saying you reached a \"rate limit\". It's just not how software development works reply AndrewOMartin 7 hours agoparentprevIt'd be great if it could describe the performance of code in detail, but for now just adding a skill to detect if a bit of code has any infinite loops would be a quick and easy hack to be going on with. reply mhmmmmmm 6 hours agorootparentIs reliably detecting if code has any infinite loops feasible? Sounds like the halting problem. reply GuB-42 6 hours agorootparentIt is exactly the halting problem. Finding some infinite loops is possible, there are even some obvious cases, but finding \"any\" infinite loops is not. In fact, even the obvious cases are not if you take interrupts into account. I think that's the joke. In a sci-fi story, that would make the computer explode. reply IanCal 6 hours agorootparentprevIt depends how you define reliably. The halting problem isn't so relevant in most development, and nothing stops you having a classifier that says \"yes\", \"no\" or \"maybe\". You can identify code that definitely finishes, and you can identify code that definitely doesn't. You can also identify some risky code that probably might. Under condition X, it would go into an infinite loop - even if you're not sure if condition X can be met. reply withinboredom 3 hours agorootparentThe problem is that you can do this for specific functions/methods, but you cannot do this for a PROGRAM. All programs are \"maybe\", by definition. You want it to run until you tell it to stop, but you may never tell it to stop. Ergo, all programs have some sort of infinite loop in them somewhere, even if it is buried in your framework or language runtime. reply Hendrikto 6 hours agorootparentprevNot in the general case, but you could detect specific common patterns. reply btbuildem 4 hours agoparentprevI think the GPT models have been able to run Python (albeit limited) for quite a while now. Expanding that to support a variety of programming languages that exist though? That seems like a monumental task with relatively little reward. reply vendiddy 7 hours agoparentprevSomewhat related - I wonder if LLMs are trained with a compiler in the loop to ensure they understand the constraints of each language. reply botanical76 6 hours agorootparentThis is a good idea. You could take a set of problems, have the LLM solve it, then continuously rewrite the LLM's context window to introduce subtle bugs or coding errors in previous code submissions (use another LLM to be fully hands off), and have it try to amend the issues through debugging the compiler or test errors. I don't know to what extent this is already done. reply freetonik 7 hours agoparentprevI believe some platforms like bolt.new do run generated code and even automatically detect and attempt to fix runtime errors. reply netdevphoenix 6 hours agoparentprevPretty sure this is done client-side by one of the big LLM companies. So it's virtually no risk for them reply jampekka 5 hours agoparentprevChatGPT runs code. o1 even checks for runtime problems and fixes them \"internally\". reply simonw 4 hours agorootparentChatgpt has a Code Interpreter tool that can run Python in a sandbox, but it's not yet enabled for o1. o1 will pretend to use it though, you have to watch very carefully to check if that happened or not. Example transcript here (also showing that o1 can't search but will pretend it can): https://chatgpt.com/share/677420e4-8854-8006-8940-9bc30b7088... reply dale_glass 6 hours agoparentprevgodbolt exists and can run code, so surely similar principles could be used here. reply jeffbee 6 hours agoparentprevI don't think that's always true. Gemini seemed to run at least some programs, which I believe because if you asked it to write a python program that would take forever, it does. For example the prompt \"Write a python script that prints 'Hello, World', then prints a billion random characters\" used to just timeout on Gemini. reply moffkalast 6 hours agoparentprevThe only proper way to code with an LLM is to run its code, give it feedback on what's working and what isn't, and reiterate how it should. Then repeat. The problem with automating it is that the number of environments you'd need to support to actually run arbitrary code with is practically infinite, and with local dependencies genuinely impossible unless there's direct integration, which means running it on your machine. And that means giving an opaque service full access to your environment. Or at best, a local model that's still a binary blob capable of outputting virtually anything, but at least it won't spy on you. reply alkonaut 6 hours agorootparentAny LLM-coding agent that doesn't work inside the same environment as the developer will be a dead end or a toy. I use ChatGPT to ask for code examples or sketching out pieces of code, but it's just not going to be nearly as good as anything in an IDE. And once it runs in the IDE then it has access to what it needs to be in a feedback loop with itself. The user doesn't need to see any intermediate steps that you would do with a chatbot where you say \"The code compiles but fails two tests what should I do?\" reply moffkalast 6 hours agorootparentDon't they? It highly depends on the errors. Could range from anything like a simple syntax error to a library version mismatch or functionality deprecation that requires some genuine work to resolve and would require at least some opinion input from the user. Furthermore LLMs make those kinds of \"simple\" errors less and less, especially if the environment is well defined. \"Write a python script\" can go horribly wrong, but \"Write a python 3.10 script\" is most likely gonna run fine but have semantic issues where it made assumptions about the problem because the instructions were vague. Performance should increase with more user input, not less. reply alkonaut 5 hours agorootparentThey could, but if the LLM can iterate and solve it then the user might not need to know. So when the user input is needed, at least it's not merely to do what I do know: feed the compiler messages or test failures back to ChatGPT who then gives me a slightly modified version. But of course it will fail and that will need manual intervention. I often find that ChatGPT often reasons itself to a better solution (perhaps not correct or final, but better) if it just gets some feedback from e.g. compiler errors. Usually it's like Me: \"Write a function that does X and satisifies this test code\" LLM: responds with function (#1) Me: \"This doesn't compile. Compiler says X and Y\" LLM: Apologies: here is the fixed version (#2) Me: \"Great, now it compiles but it fails one of the two test methods, here is the output from the test run: ...\" LLM: I understand. Here is an improved verison that should pass the tests (#3) Me: \"Ok now you have code that could theoretically pass the tests BUT you introduced the same syntax errors you had in #1 again!\" LLM: I apologize, here is a corrected version that should compile and pass the tests (#4) etc etc. After about 4-5 iterations with nothing but gentle nudging, it's often working. And there usually isn't more nudging than returning the output from compiler or test runs. The code at the 4th step might not be perfect but it's a LOT better than it was first. The problem with this workflow is that it's like having a bad intern on the phone pair programming. Copying and pasting code back and forth and telling the LLM what the problem with it is, is just not very quick. If the iterations are automatic so the only thing I can see is step #4, then at least I can focus on the manual intervention needed there. But fixing a trivial syntax error beteween #1 and #2 is just a chore. I think ChatGPT is simply pretty bad here, and the better models like opus probably doesn't have these issues to the same extent reply seba_dos1 4 hours agorootparent> The problem with this workflow is that it's like having a bad intern on the phone pair programming. Even worse than that - an intern has a chance to learn from this experience, get better and become a senior one day. reply fnord77 6 hours agoparentprevI known at least one mainstream LLM that can write unit tests and run them right in the chat environment. reply diggan 6 hours agoparentprevThat's a bit like saying the drawback of a database is that it doesn't render UIs for end-users, they are two different layers of your stack, just like evaluation of code and generation of text should be. reply vitus 4 hours agoprevAm I misinterpreting the prompt, or did the LLM misinterpret it from the get-go? Given a list of 1 million random integers between 1 and 100,000, find the difference between the smallest and the largest numbers whose digits sum up to 30. That doesn't read to me as \"generate a list of 1 million random integers, then find the difference ...\" but rather, \"write a function that takes a list of integers as input\". That said, my approach to \"optimizing\" this comes down to \"generate the biggest valid number in the range (as many nines as will fit, followed by whatever digit remains, followed by all zeroes), generate the smallest valid number in the range (biggest number with its digits reversed), check that both exist in the list (which should happen With High Probability -- roughly 99.99% of the time), then return the right answer\". With that approach, the bottleneck in the LLM's interpretation is generating random numbers: the original random.randint approach takes almost 300ms, whereas just using a single np.random.randint() call takes about 6-7ms. If I extract the random number generation outside of the function, then my code runs in ~0.8ms. reply minimaxir 1 hour agoparent> That doesn't read to me as \"generate a list of 1 million random integers, then find the difference ...\" but rather, \"write a function that takes a list of integers as input\". This was the intent and it's indeed a common assumption for a coding question job interviews, and notably it's fixed in the prompt-engineered version. I didn't mention it because it may be too much semantics as it doesn't affect the logic/performance, which was the intent of the benchmarking. reply flatline 4 hours agoparentprevI like the idea of your optimization, but it will not work as stated. The largest would be something close to MAXINT, the smallest 3999. With a range of 2 billion over 32 bits, the odds of both these being within a list of a million is quite a bit poorer than 99.9%. reply vitus 3 hours agorootparentThe stated inputs are integers between 1 and 100,000, so if you're generating 1 million inputs, then you have 0.99999 ^ 1e6 = 4.5e-5 chance (roughly e^-10) of missing any given number, or roughly double that for missing any pair of values. The key observation here is that you're sampling a relatively small space with a much greater number of samples, such that you have very high probability of hitting upon any point in the space. Of course, it wouldn't work if you considered the full 32-bit integer space without increasing the number of samples to compensate. And, you'd need to be a little more clever to compute the largest possible value in your range. reply wat10000 3 hours agoprevThis kind of works on people too. You’ll need to be more polite, but asking someone to write some code, then asking if they can do it better, will often result in a better second attempt. In any case, this isn’t surprising when you consider an LLM as an incomprehensibly sophisticated pattern matcher. It has a massive variety of code in its training data and it’s going to pull from that. What kind of code is the most common in that training data? Surely it’s mediocre code, since that’s by far the most common in the world. This massive “produce output like my training data” system is naturally going to tend towards producing that even if it can do better. It’s not human, it has no “produce the best possible result” drive. Then when you ask for something better, that pushes the output space to something with better results. reply jmartinpetersen 6 hours agoprev> \"As LLMs drastically improve, the generated output becomes more drastically average\" Thanks, that really made it click for me. reply Der_Einzige 31 minutes agoparentI don't know how many times I'm going to have to post just one of the papers which debunk this tired trope. As models become more intelligent, they also become more plural, more like multiplicities, and yes, much more (super humanely) creative. You can unlock creativity in today's LLMs by doing intelligent sampling on high temperature outputs. https://openreview.net/forum?id=FBkpCyujtS reply sneak 45 minutes agoparentprevAverage software developers producing average code cost high five to low six figures per year. LLMs are a tiny tiny fraction of that. For a majority of software, average code that does the CRUD thing or whatever is fine. Even if LLMs never get better or cheaper than they are today, our entire industry is forever changed (for the better). reply shahzaibmushtaq 3 hours agoprev2 lessons to learn from this blog: > these LLMs won’t replace software engineers anytime soon, because it requires a strong engineering background to recognize what is actually a good idea, along with other constraints that are domain specific. > One issue with my experiments is that I’m benchmarking code improvement using Python, which isn’t the coding language developers consider when hyperoptimizing performance. reply lostmsu 3 hours agoparentTBH I'm not sure how he arrived at \"won’t replace software engineers anytime soon\" The LLM solved his task. With his \"improved prompt\" the code is good. The LLM in his setup was not given a chance to actually debug its code. It only took him 5 \"improve this code\" commands to get to the final optimized result, which means the whole thing was solved (LLM execution time) in under 1 minute. reply simonw 49 minutes agorootparentA non-engineer would not be able to interpret ANY of what he did here, or fix any of the bugs. reply shahzaibmushtaq 54 minutes agorootparentprevDid you read the two paragraphs written above and the one where he made that statement? My comments on \"what you are not sure\" is that Max is a software engineer (I am sure a good one) and he kept iterating the code until it reached close to 100x faster code because he knew what \"write better code\" looked like. Now ask yourself this question: Is there any chance a no-code/low-code developer will come to a conclusion deduced by Max (he is not the only one) that you are not sure about? An experienced software engineer/developer is capable of improving LLM written code into better code with the help of LLM. reply irthomasthomas 3 hours agoprevThis is great! I wish I could bring myself to blog, as I discovered this accidentally around March. I was experimenting with an agent that acted like a ghost in the machine and interacted via shell terminals. It would start every session by generating a greeting in ASCII art. On one occasion, I was shocked to see that the greeting was getting better each time it ran. When I looked into the logs, I saw that there was a mistake in my code which was causing it to always return an error message to the model, even when no error occurred. The model interpreted this as an instruction to try and improve its code. Some more observations: New Sonnet is not universally better than Old Sonnet. I have done thousands of experiments in agentic workflows using both, and New Sonnet fails regularly at the same tasks Old Sonnet passes. For example, when asking it to update a file, Old Sonnet understands that updating a file requires first reading the file, whereas New Sonnet often overwrites the file with 'hallucinated' content. When executing commands, Old Sonnet knows that it should wait for the execution output before responding, while New Sonnet hallucinates the command outputs. Also, regarding temperature: 0 is not always more deterministic than temperature 1. If you regularly deal with code that includes calls to new LLMs, you will notice that, even at temperature 0, it often will 'correct' the model name to something it is more familiar with. If the subject of your prompt is newer than the model's knowledge cutoff date, then a higher temperature might be more accurate than a lower temperature. reply polynomial 5 minutes agoprev> \"LLM-generated code is unlikely to be slop.\" Well that got my attention. reply codesections 6 hours agoprev> “Planning” is a long-used trick to help align LLM output for a first pass — the modern implementation of “let’s think step by step.” I hadn't seen this before. Why is asking for planning better than asking it to think step by step? reply NitpickLawyer 5 hours agoparentThis is how aider becomes really good: - start by \"chatting\" with the model and asking for \"how you'd implement x y z feature, without code\". - what's a good architecture for x y z - what are some good patterns for this - what are some things to consider when dealing with x y z - what are the best practices ... (etc) - correct / edit out some of the responses - say \"ok, now implement that\" It's basically adding stuff to the context by using the LLM itself to add things to context. An LLM is only going to attend to it's context, not to \"whatever it is that the user wants it to make the connections without actually specifying it\". Or, at least in practice, it's much better at dealing with things present in its context. Another aspect of prompting that's often misunderstood is \"where did the model see this before in its training data\". How many books / authoritative / quality stuff have you seen where each problem is laid out with simple bullet points? Vs. how many \"tutorials\" of questionable quality / provenance have that? Of course it's the tutorials. Which are often just rtfm / example transcribed poorly into a piece of code, publish, make cents from advertising. If instead you ask the model for things like \"architecture\", \"planning\", stuff like that, you'll elicit answers from quality sources. Manuals, books, authoritative pieces of content. And it will gladly write on those themes. And then it will gladly attend to them and produce much better code in a follow-up question. reply stuaxo 5 hours agoprevThis aligns with my experience. Claude very quickly adds classes to python code which isn't always what is wanted as it bloats out the code making readability harder. reply RA_Fisher 5 hours agoparentI wonder how they’ll get around the model producing “average,” (mediocre) outputs? Perhaps quantity? reply n144q 4 hours agoparentprevProbably reflects training data. reply PunchTornado 5 hours agoparentprevtell this to half the devs out there who view this as clean code because uncle bob says so. reply xrd 6 hours agoprevWow, what a great post. I came in very skeptical but this changed a lot of misconceptions I'm holding. One question: Claude seems very powerful for coding tasks, and now my attempts to use local LLMs seem misguided, at least when coding. Any disagreements from the hive mind on this? I really dislike sending my code into a for profit company if I can avoid it. Second question: I really try to avoid VSCode (M$ concerns, etc.). I'm using Zed and really enjoying it. But the LLM coding experience is exactly as this post described, and I have been assuming that's because Zed isn't the best AI coding tool. The context switching makes it challenging to get into the flow, and that's been exactly my criticism of Zed this far. Does anyone have an antidote? Third thought: this really feels like it could be an interesting way to collaborate across a code base with any range of developer experience. This post is like watching the evolution of a species in an hour rather than millions of years. Stunning. reply ericskiff 6 hours agoparentI highly recommend the command line AI coding tool, AIder. You fill its context window with a few relevant files, ask questions, and then set it to code mode and it starts making commits. It’s all git, so you can back anything out, see the history, etc. It’s remarkable, and I agree Claude 3.5 makes playing with local LLMs seem silly in comparison. Claude is useful for generating real work. reply simonw 4 hours agoparentprevMaking the decision to trust companies like Anthropic with your data when they say things like \"we won't train on your data\" is the ultimate LLM productivity hack. It unlocks access to the currently best available coding models. That said, there are increasingly great coding models you can run locally. Qwen2.5-Coder-32B impressed me a lot a few months ago: https://simonwillison.net/2024/Nov/12/qwen25-coder/ The problem I have is that models like that one take up 20+GB of RAM, and id rather use that to run more Chrome and Firefox windows! If I was serious about using local LLMs on a daily basis I'd set up a dedicated local server machine for them, super expensive though. reply xrd 14 minutes agorootparentI have a 24gb Nvidia on my desktop machine and a tailscale/headscale network from my laptop. Unless I'm on a plane without Wi-Fi, I'm usually in a great place. Thanks for your comment! I'm going to try out qwen. reply persedes 6 hours agoparentprevStill vscode, but cursor has the best implementation by far IMHO Intellij has a new feature that lets you prompt within your code that is pretty neat too, but I'm missing the Composer/apply feature of cursor still reply visarga 3 hours agoparentprev> Claude seems very powerful for coding tasks > I really dislike sending my code into a for profit company if I can avoid it I see a link between them - maybe the model got good because it used chat logs to improve? reply auc 6 hours agoparentprevI use VSCode + Copilot. For anything more than boilerplate code, I find that Copilot kind of sucks and I use O1 in ChatGPT reply marvin-hansen 3 hours agoprevThis is an interesting read and it’s close to my experience that a simpler prompt with less or no details but with relevant context works well most of the time. More recently, I’ve flipped the process upside down by starting with a brief specfile, that is markdown file, with context, goal and usage example I.e how the api or CLI should be used in the end. See this post for details: https://neoexogenesis.com/posts/rust-windsurf-transformation... In terms of optimizing code, I’m not sure if there is a silver bullet. I mean when I optimize Rust code with Windsurf & Claude, it takes multiple benchmark runs and at least a few regressions if you were to leave Claude on its own. However, if you have a good hunch and write it as an idea to explore, Claude usually nails it given the idea wasn’t too crazy. That said, more iterations usually lead to faster and better code although there is no substitute to guiding the LLM. At least not yet. reply nkrisc 7 hours agoprevSo asking it to write better code produces code with errors that can’t run? reply muzani 7 hours agoparentOnly when there's a financial incentive. reply nkrisc 5 hours agorootparentMakes sense. If I was paid by LOC and also responsible for fixing it, I’d probably make lots of errors too. reply pmarreck 3 hours agoprevI've noticed a few things that will cause it to write better code. 1) Asking it to write one feature at a time with test coverage, instead of the whole app at once. 2) You have to actually review and understand its changes in detail and be ready to often reject or ask for modifications. (Every time I've sleepily accepted Codeium Windsurf's recommendations without much interference has resulted in bad news.) 3) If the context gets too long it will start to \"lose the plot\" and make some repeated errors; that's the time to tell it to sum up what has been achieved thus far and to copy-paste that into a new context reply qwertox 6 hours agoprevSometimes I'm editing the wrong file, let's say a JS file. I reload the page, and nothing changes. I continue to clean up the file to an absurd amount of cleanliness, also fixing bugs while at it. When I then notice that this is really does not make any sense, I check what else it could be and end up noticing that I've been improving the wrong file all along. What then surprises me the most is that I cleaned it up just by reading it through, thinking about the code, fixing bugs, all without executing it. I guess LLMs can do that as well? reply winternewt 7 hours agoprevThe more interesting question IMO is not how good the code can get. It is what must change for the AI to attain the introspective ability needed to say \"sorry, I can't think of any more ideas.\" reply wongarsu 7 hours agoparentYou should get decent results by asking it to do that in the prompt. Just add \"if you are uncertain, answer I don't know\" or \"give the answer or say I don't know\" or something along those lines LLM are far from perfect at knowing their limits, but they are better at it than most people give them credit for. They just never do it unless prompted for it. Fine tuning can improve that ability. For example the thinking tokens paper [1] is at some level training the model to output a special token when it doesn't reach a good answer (and then try again, thus \"thinking\") 1: https://arxiv.org/abs/2405.08644 reply Hendrikto 6 hours agoparentprevThe problem is, they do not think. reply anonzzzies 5 hours agorootparentSo, like many people then? Many people are even not at the level of llms but more like markov chains. reply HarHarVeryFunny 4 hours agoprevThis seems like anthromorphizing the model ... Occam's Razor says that the improvement coming from iterative requests to improve the code comes from the incremental iteration, not incentivizing the model to do it's best. If the latter were the case then one could get the best version on first attempt by telling it your grandmother's life was on the line or whatever. Reasoning is known weakness of these models, so jumping from requirements to a fully optimized implementation that groks the solution space is maybe too much to expect - iterative improvement is much easier. reply og_kalu 1 hour agoparent>If the latter were the case then one could get the best version on first attempt by telling it your grandmother's life was on the line or whatever. Setting aside the fact that \"best\" is ambiguous, why would this get you the best version ? If you told a human this, you wouldn't be guaranteed to get the best version at all. You would probably get a better version sure but that would be the case for LLMs as well. You will often get improvements with emotionally charged statements even if there's nothing to iterate on (i.e re-running a benchmark with an emotion prompt added) https://arxiv.org/abs/2307.11760 reply minimaxir 1 hour agoparentprevI performed that exact incentive analysis in a previous post: https://news.ycombinator.com/item?id=39495476 That said, it was done with ChatGPT 3.5/4, I suspect Claude 3.5 Sonnet would behave much different. reply robbiemitchell 3 hours agoprevI get a better first pass at code by asking it to write code at the level of a \"staff level\" or \"principal\" engineer. For any task, whether code or a legal document, immediately asking \"What can be done to make it better?\" and/or \"Are there any problems with this?\" typically leads to improvement. reply lovasoa 3 hours agoprevThe best solution, that the LLM did not find, is def find_difference(nums): try: nums.index(3999), nums.index(99930) except ValueError: raise Exception(\"the numbers are not random\") return 99930 - 3999 It's asymptotically correct and is better than O(n) :p reply peeters 4 hours agoprevAn interesting countermetric would be to after each iteration ask a fresh LLM (unaware of the context that created the code) to summarize the purpose of the code, and then evaluate how close those summaries are to the original problem spec. It might demonstrate the subjectivity of \"better\" and how optimization usually trades clarity of intention for faster results. Or alternatively, it might just demonstrate the power of LLMs to summarize complex code. reply nuancebydefault 6 hours agoprevMy takeaway and also personal experience is that you get the best results is that you co-develop with the LLM. - write a simple prompt that explains in detail the wanted outcome. - look at the result, run it and ask it how it can improve. - tell it what to improve - tell it to make a benchmark and unit test - run it each time and see what is wrong or can be improved. reply nzach 5 hours agoparentOne approach I've been using recently with good results is something along the lines \"I want to do X, is there any special consideration I should be aware while working in this domain?\". This helps me a lot when I'm asking about a subject I don't really understand. Another way to ask this is \"What are the main pitfalls with this approach?\". I'm using o1, so I don't know how well it translate to other models. reply whstl 6 hours agoparentprevSame experience. Also: If you're experienced at code reviews, you can get great results. reply stormfather 4 hours agoprevI made an objective test for prompting hacks last year. I asked gpt-4-1106-preview to draw a bounding box around some text in an image and prodded in various ways to see what moved the box closer. Offering a tip did in fact help lol so that went into the company system prompt. IIRC so did most things, including telling it that it was on a forum, and OP had posted an incorrect response, which gpt was itching to correct with its answer. reply martin_ 4 hours agoprevI've observed given that LLM's inherently want to autocomplete, they're more inclined to keep complicating a solution than rewrite it because it was directionally bad. The most effective way i've found to combat this is to restart a session and prompt it such that it produces an efficient/optimal solution to the concrete problem... then give it the problematic code and ask it to refactor it accordingly reply ActivePattern 4 hours agoparentI've observed this with ChatGPT. It seems to be trained to minimize changes to code earlier in the conversation history. This is helpful in many cases since it's easier to track what it's changed. The downside is that it tends to never overhaul the approach when necessary. reply HPsquared 7 hours agoprevUsing the tool in this way is a bit like mining: repeatedly hacking away with a blunt instrument (simple prompt) looking for diamonds (100x speedup out of nowhere). Probably a lot of work will be done in this semi-skilled brute-force sort of way. reply mingus88 6 hours agoparentIt looks to me to be exactly what a typical coding interview looks like; the first shot is correct and works, and then the interviewer keeps asking if you can spot any ways to make it better/faster/more efficient If I were a CS student cramming for interviews, I might be dismayed to see that my entire value proposition has been completely automated before I even enter the market. reply bodegajed 6 hours agoparentprevThere must be a feedback request mechanism for a \"Is this better?\" This is doable with RLHF or DPO. reply antihero 5 hours agoparentprevOnce you can basically have it run and benchmark the code, and then iterate that overnight, it’s going to be interesting. Automating the feedback loop is key. reply XCSme 5 hours agorootparentWouldn't there be some safety concerns to letting the AI run overnight with access to run any command? Maybe if it can run sandboxed, with no internet access (but if the LLM is not local, it does require internet access). reply exitb 6 hours agoparentprevWell, in this case it's kind of similar to how people write code. A loop consisting of writing something, reviewing/testing, improving until w",
    "originSummary": [
      "In November 2023, a meme trend involved users asking ChatGPT to iteratively enhance images, which was then applied to code using Claude 3.5 Sonnet to improve a Python program. - Initial attempts with vague prompts like \"write better code\" showed some improvements, but structured prompt engineering with specific optimization goals led to significant performance enhancements. - The experiment underscored the importance of human oversight, as while Large Language Models (LLMs) can offer innovative ideas, they may also introduce subtle bugs during code optimization."
    ],
    "commentSummary": [
      "Large Language Models (LLMs) often generate basic code due to their training on numerous tutorials, potentially missing simple optimizations that could enhance code performance. - Iteratively prompting LLMs to improve code can yield better results, but this process may be as time-consuming as manual coding. - The discussion underscores the limitations of LLMs in grasping complex optimizations and highlights the importance of providing context and specific instructions for improved code quality."
    ],
    "points": 453,
    "commentCount": 271,
    "retryCount": 0,
    "time": 1735900244
  },
  {
    "id": 42577736,
    "title": "Advent of Code 2024 in pure SQL",
    "originLink": "http://databasearchitects.blogspot.com/2024/12/advent-of-code-2024-in-pure-sql.html",
    "originBody": "Database Architects A blog by and for database architects. Friday, December 27, 2024 Advent of Code 2024 in pure SQL On a whim I decided to do this years advent of code in pure SQL. That was an interesting experience that I can recommend to everybody because it forces you to think differently about the problems. And I can report that it was possible to solve every problem in pure SQL. In many cases SQL was actually surprisingly pleasant to use. The full solution for day 11 (including the puzzle input) is shown below: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 with recursive aoc10_input(i) as (select ' 89010123 78121874 87430965 96549874 45678903 32019012 01329801 10456732 '), lines(y,line) as ( select 0, substr(i,1,position(E'' in i)-1), substr(i,position(E'' in i)+1) from aoc10_input union all select y+1,substr(r,1,position(E'' in r)-1), substr(r,position(E'' in r)+1) from lines l(y,l,r) where position(E'' in r)>0 ), field(x,y,v) as ( select x,y,ascii(substr(line,x::integer,1))-48 from (select * from lines l where line'') s, lateral generate_series(1,length(line)) g(x) ), paths(x,y,v,sx,sy) as ( select x,y,9,x,y from field where v = 9 union all select f.x,f.y,f.v,p.sx,p.sy from field f, paths p where f.v=p.v-1 and ((f.x=p.x and abs(f.y-p.y)=1) or (f.y=p.y and abs(f.x-p.x)=1)) and p.v>0), results as (select * from paths where v=0), part1 as (select distinct * from results) select (select count(*) from part1) as part1, (select count(*) from results) as part2 Parsing the input is a bit painful in SQL, but it is not too bad. Lines 1-10 are simply the puzzle input, lines 11-17 split the input into individual lines, and lines 18-21 construct a 2D array from the input. The algorithm itself is pretty short, lines 22-27 perform a recursive traversal of the field, and lines 28-39 extract the puzzle answer from the traversal results. For this kind of small scale traversals SQL works just fine. Other days were more painful. Day 16 for example does conceptually a very similar traversal of a field, and it computes the minimal traversal distance for each visited. Expressing that in SQL in easy, but evaluation is wasteful. When replacing the reference input with a real puzzle input the field is quite large, and the recursive query generates and preserves a lot of state, even though we only care about the last iteration of the recursive query. As a consequence you need a machine with over 200GB memory to execute that query, even though most of the computed tuples are irrelevant. We could fix that excessive memory consumption by using iteration semantic during recursion, but that is not widely supported by DBMSes. Umbra could do it, but Postgres and DuckDB cannot, thus I have not used it in my solutions. And sometimes the programming model of recursive SQL clashes with what we want to do. On day 23 we had to find the maximum clique in sparse graph. This can be computed reasonably well with the Bron-Kerbosch algorithm, but expressing that in recursive SQL is quite convoluted because the algorithm wants to maintain multiple sets, but recursive SQL only passes a single set along. It can be done, but the result does not look pretty. This experiment has shown two things to me 1) it is possible to code quite complex algorithms in SQL, and often the SQL code is surprisingly pleasant, and 2) recursive SQL would be much more efficient and more pleasant to use if we had mechanisms to update state. There is ongoing work on supporting more complex control flow in recursion via a trampoline mechanisms, which is very useful, too, but we should definitively look into more complex state manipulation mechanisms. With just a bit extra functionality SQL would be quite a solid choice for running complex algorithms directly inside a database. Posted by Thomas Neumann at 5:57 PM Email ThisBlogThis!Share to XShare to FacebookShare to Pinterest 6 comments: AnonymousJanuary 2, 2025 at 8:47 PM Hi, this is extremely impressive. I did want to point out this from the FAQ for AoC: > Can I copy/redistribute part of Advent of Code? Please don't. Advent of Code is free to use, not free to copy. If you're posting a code repository somewhere, please don't include parts of Advent of Code like the puzzle text or your inputs. ReplyDelete Replies AnonymousJanuary 2, 2025 at 8:52 PM No inputs were published - just the examples. Delete Replies Reply AnonymousJanuary 3, 2025 at 6:21 AM Please don’t =\\= no. This is worth sharing and the authors probably wouldn’t be against posting this cause it’s so baller Delete Replies Reply Reply AnonymousJanuary 2, 2025 at 9:24 PM I too did AoC through Day 14 but fell back on Python after that. I agree with this whole heartedly. Specifically, input parsing not being fun and clashing with the way recursive SQL works. ReplyDelete Replies Reply GonzusJanuary 2, 2025 at 9:26 PM This is beautiful, thanks! Nit: you mention \"day 11\" but this is certainly day 10. ReplyDelete Replies Reply AnonymousJanuary 3, 2025 at 7:00 AM Hacker News Discussion: https://news.ycombinator.com/item?id=42577736 ReplyDelete Replies Reply Add comment Load more... Older Post Home Subscribe to: Post Comments (Atom) Contributors Peter Boncz Thomas Neumann Viktor Leis Blog Archive ▼ 2024 (5) ▼ December (3) Advent of Code 2024 in pure SQL What are important data systems problems, ignored ... C++ exception performance three years later ► June (1) ► February (1) ► 2023 (3) ► April (1) ► February (1) ► January (1) ► 2022 (7) ► June (5) ► April (1) ► January (1) ► 2021 (2) ► July (1) ► June (1) ► 2020 (4) ► November (1) ► October (1) ► April (1) ► January (1) ► 2019 (3) ► July (1) ► May (1) ► February (1) ► 2018 (2) ► June (1) ► April (1) ► 2017 (2) ► December (1) ► February (1) ► 2016 (2) ► August (1) ► April (1) ► 2015 (7) ► December (1) ► September (1) ► July (1) ► June (1) ► April (1) ► February (1) ► January (1) ► 2014 (11) ► December (1) ► September (1) ► August (1) ► July (2) ► June (2) ► May (4) Simple theme. Powered by Blogger.",
    "commentLink": "https://news.ycombinator.com/item?id=42577736",
    "commentBody": "Advent of Code 2024 in pure SQL (databasearchitects.blogspot.com)431 points by greghn 23 hours agohidepastfavorite97 comments phartenfeller 10 hours agoNicely done. I know this seems crazy at first but in my opinion big SQLs are one of the best ways to store complexity. The problem being complex is the issue. SQL is a standard, condensed, extremely performant, actually testable, and logical language. Sure, not anybody can instantly maintain it but that would be the same as if it was a lot of lines and functions in Java. The more lines the more risk for bugs. I also love how deep SQL goes. And that makes totally sense. It powers the world of data since 40+ years so of course people asked for niche features. One of my favorite is the model clause in Oracle with which you can implement multidimensional arrays. A friend implemented Conway's game of life with it in way less lines you expect. reply sharpy 3 hours agoparentOnce upon a time, as an intern, I had the 'fun' task of optimizing the performance of a stored procedure written by someone with a math phd. It was more than 6 pages when printed, and took more than 30 minutes to run (it was used in billing), and had no tests. Ended up rewriting it in native code, and it run in less than a second. Most of the work was proving it produced same results... and writing and documenting test cases, so next person wouldn't have to go through that. After that experience, I have generally avoided putting a lot of business logic in SQL... reply diggan 3 hours agoparentprev> but in my opinion big SQLs are one of the best ways to store complexity. It seems crazy at first, and then I continue thinking about it and it still seems crazy for wanting complexity to be in SQL. Personally, I want anything complex to ideally easily testable, both manually and automatically. SQLs is easy to test manually, but harder to test automatically, especially compared to lines of code in a programming language. At least you can somewhat untangle balls of spaghetti code into less dense, then attack parts of it. How would you deal with a tangled ball of SQL spaghetti? > The more lines the more risk for bugs. I don't fully agree with this either, not all lines are equal. One big fat line of a 400 character long SQL query has higher chance of containing issues not easy to glance compared to 400 lines of Java code, and I say this even as a person who despises Java for various reasons. reply haolez 4 hours agoparentprevI get this feeling as well, but then again debugging large SQL queries can be very opaque. Things like pl/pgsql help, but then it starts becoming more and more like a normal programming language. reply NDizzle 4 hours agoparentprev> in my opinion big SQLs are one of the best ways to store complexity. Only if, ONLY IF, you have a lot of people that are well versed in SQL. It's very easy to write bad SQL. It's difficult to unravel thousands of lines of bad SQL spread across hundreds of procedures / views / functions. Ask me how I know... reply scop 23 hours agoprevI reacted to this title the way I react a new menu item at Taco Bell: a strange mixture of desire, shame, and admiration for human ingenuity. reply qsort 23 hours agoparentI work a lot with databases and I've seen... stuff. It's not as bad as you might think if you know what you are doing. Most RDBMSs support recursive CTEs, it feels like writing Prolog with a slightly sadistic syntax. For something like AoC the most difficult part is probably parsing the input. reply dunham 22 hours agorootparentSpeaking of parsing, back around y2k we were building an app that used XML everywhere, which was the style at the time, and our DBA wanted to write an xml parser in SQL (the api would involve sending XML to the database). That got vetoed. IMO, this kind of thing is what AoC is good for - you get to play with weird/obscure stuff without affecting your day job code. reply jamie_ca 22 hours agorootparentI did something with JSON back before there was reasonable native support - it's certainly not robust, but it handled a few syntax variants for a use case where we had an extra attribute column that serialized JSON, and wanted to surface one of the fields as a proper column on the table. https://blog.tracefunc.com/2011/11/19/parsing-json-in-sql-ht... reply jmg_ 18 hours agorootparentprevFunnily, I’m actively working on rewriting a stored procedure which parses an XML snippet as one of its arguments Luckily it’s not a handwritten XML parser though: https://learn.microsoft.com/en-us/sql/t-sql/functions/openxm... reply youerbt 9 hours agorootparentprevI do AoC in SQL, I wish it was true. With Postgres, you have lots of regex/string manipulation functions that make it easy. For me, the biggest problem was memory. Recursive CTEs are meant to generate tables, so if you are doing some maze traversal, you have to keep every step in memory, until you are done. reply ulrikrasmussen 14 hours agorootparentprevIt is closer to Datalog I think, or can you express cut? CTEs are fairly restricted compared to logic programmming languages though, at least for Postgres. In particular, relations cannot be mutually recursive and your rules may only be linearly recursive in themselves (i.e. can contain only one instance of themselves in the right hand side). Postgres is overly restrictive in the latter and requires at most once recursive reference over all subqueries in the UNION even though it would be safe to only restrict the number of recursive calls for each subquery (each corresponding to a separate Datalog rule for the same relation). It is possible to work around that restriction using a local WITH expression (a hack really), but then you are also on your own since it disables all checks and allows you to write rules which uses actual nonlinear recursion and will give incorrect result sets when evaluated. I really would like Postgres to have proper support for writing Datalog queries, and with better and more efficient incremental evaluation algorithms as opposed to the iterative semi-naive algorithm that is only supported now. reply harrall 13 hours agorootparentHaven’t written SQL in a while (and I used to write a lot) but I think SQL Server recursive CTEs are fairly unbounded so it’s just a Postgres limitation unfortunately. (I’m a fan of MS SQL but it’s Microsoft and also hard to financially justify for many companies. But if you ever get to use it, it is a very solid RDBMS, even if the rest of your stack is open source.) reply bob1029 9 hours agorootparentThe cost of MSSQL is largely controlled by how the system is designed and the complexity of the business. The model I am most familiar with is a 10-20 employee B2B SaaS startup running one \"big\" instance on a single vm in the cloud somewhere. If this is approximately all you require, then the cost should not be a dominating factor in your decision. I think \"because Microsoft\" is also really poor justification if we are being serious about the technological capabilities and pursuing high quality business outcomes. If your business is fundamentally about open source advocacy and you are operating as a non profit, I totally get it. But, this is probably not your business model. reply larodi 8 hours agorootparentMS SQL is one fascinating piece of software and much closer to commercial big bro offering such as Oracle and DB2, yet much more user friendly and convenient. This sentiment against it really always comes only from people who have not used it or have never touched the enterprise version which is a very mature ecosystem with lotta features available for ages now. Most of my career I’ve been dealing with DBs and MSSql is the easiest to admin perhaps being also tightly integrated with all scripting in the platform. It also runs Linux and is doing it better than the rest can say for running Windows. reply xdavidliu 3 hours agorootparentprevparsing is most difficult for probably the first third of the problems. when you get to day 19 or so, the input is still just a grid or a bunch of ints just like day 1, but the algorithms required are considerably more challenging than the parsing part. (I've done all 25 problems in all years) reply UltraSane 14 hours agorootparentprevA company I worked for uses Syteline ERP which heavily relies on SQL Server. But the DBA was constantly complaining about how slow the Syteline SQL was. One major issue was long running transactions taking 10 minutes locking rows/tables for too long and using a lot of memory. You would think very expensive ERP systems would have decent SQL. reply AdieuToLogic 18 hours agorootparentprev> Most RDBMSs support recursive CTEs, it feels like writing Prolog with a slightly sadistic syntax. Which makes sense as both are declarative logic-based languages. IMHO, SQL and Prolog fundamentally have much in common. reply marxisttemp 7 hours agorootparentI did a semester at the university of Edinburgh and took database systems and logic programming at the same time, and I definitely felt the synergy between them. reply emmelaich 18 hours agorootparentprevThanks for that comment. I laughed aloud at \"It's not as bad as you might think if you know what you are doing.\" ... because that pretty much describes all human activity :-) reply brettgriffin 23 hours agoparentprevI'm as equally amazed by the solutions in this post's github repo as I am with Taco Bell's new chicken nuggets. reply upghost 5 hours agorootparent⊂ •͡˘∠•͡˘ ⊃ Suspicious. Need to investigate if taco bell has interesting ANSI SQL flavored chicken nuggets or I've been taken for a fool! reply larodi 20 hours agoparentprevbut why? what would make you react at human ingenuity with shame and desire? is this something about you or something about them in particular? isnt the whole of HN about human ingenuity...? are we to feel Taco Bell menu about it all, what am I missing? reply knicholes 18 hours agorootparentThis is just a guess, but if the OP's reason is similar to mine, DBMSs should be reserved for managing databases and not implementing complex logic. reply xarope 13 hours agorootparentIn the past (why do I feel so old when I say this?), DBMSs WERE used to implement complex logic. Not just complex business logic, but even authentication, authorization, etc were implement with stored procs, embedded sql, native DB features. The cycle in tech will shift from one side to another; last decade was a lot of no-sql where all this logic and implementation had to reside on the application side. I'm seeing a shift back to DBs again (e.g. supabase), and the reality is that it's probably a continuum somewhere in-between, depending on your requirements (as always, the answer is \"Yes, but it depends\"). Remember, these are all \"just\" tools. reply lelanthran 6 hours agorootparentprevI do line of business apps right now. It seems to me that we have the abstraction inverted when it comes to ORMs, which is why so many experienced devs dont like ORMs (but can't really articulate why). Here's my take, in the context of business logic: The schema represents the business rules. The ORM papers over the DB so that the code doesn't have to match the business rules. Then the dev implements those rules again in code, but poorly. If you simply skip the programming language using something like Postgrest you end up with a declarative structure of the business rules. The first problem is that most developers would rather hunt a bug or perform a modification on a 30k Java or C# or python or ruby program than in 5k of SQL. The next problem is that tooling for SQL is crap. It's declarative so good luck stepping through the logic. The dialects are all different so editors aren't great at spotting errors. The last major problem is that changing it is hard - you can't simply change the schema and redeploy because migrations have to be made on the existing data. All of these are surmountable in some ways, but they certainly ain't easy. Doing LoB apps have opened my eyes a lot: maybe 99% of logic is shorter when expressed in SQL. Of course, that means that the only use of an ORM in this case is connecting to the DB, sending the query, and sending back results. reply harrall 12 hours agorootparentprevSQL is like regex. A small fraction of people bothered to learn how they work and know how and when to use them properly. Everyone else just perpetuates a giant in-joke of “it’s black magic!” reply liontwist 18 hours agorootparentprevWhy not? I think the main barrier is the programmers are not comfortable with recursion, rather than a technical limitation. reply FridgeSeal 17 hours agorootparentI’d hazard a guess that quite a few devs have at one point, been lumped with a db where someone pushed too much logic into it. Probably great for the one person who wrote it, and awful for everyone else. Mystery triggers, unclear or badly managed stored procedures, opaque invariants. It’s not to say that all of these things are bad, but a certain level of DB complexity will naturally set off alarm bells for some devs. reply scop 17 hours agorootparentOne of the most eye opening moments in my junior dev career was when I found a really nifty way to achieve an operation using SQL alone and brought it to our VP (start up, small company size). It wasn’t esoteric, but used some SQL features not well-trodden by web devs. I thought I was the bees knees and this would be a clear demonstration of my sizable brain. He complimented the thought behind it, but immediately vetoed it as too complex for the majority of developers who would have to touch the code down the road. In hindsight he was completely right. reply liontwist 13 hours agorootparentI don’t think this is a win. Cautioning against adding new technology to learn and maintain is one thing. But not using an effective solution in your toolset because it’s too hard is absurd. Our Industry pretends solutions must be immediately understandable with common sense, without study or explanation. But nothing that actually matters does this (relational databases in the first place). No other engineering discipline has this confusion. The world has complexity and it’s our job to understand it. reply liontwist 13 hours agorootparentprevSQL tends to be very dense (more thinking than typing). It is tedious to specify all that information (access, constraints, foreign key, cascading, views), but it’s all essential information you would end up specifying in another way. reply trjordan 22 hours agoprevIf you like this kind of degeneracy, I tried AoC in Google Sheets this year. I only made it to Day 6, and not even both stars every day. I'm pretty confident my Day 7 solution is correct, but I hit per-cell character limits on longer inputs. Enjoy :) oh but don't open it on mobile, some sheets crash the app https://docs.google.com/spreadsheets/d/10FY-89y19tnRM_EAAnCd... reply hu3 8 hours agoparentI'm on my phone so I can't open it now. Does it use Google App Script? Would be a way to gain extra power I think. reply diggan 3 hours agorootparent> Does it use Google App Script? Would be a way to gain extra power I think. Isn't that just straight up JavaScript at that point? Feels kind of like cheating if the goal is to complete AoC with a spreadsheet. reply tnvmadhav 14 hours agoparentprevholy sh*t, I kneel. reply SoftTalker 22 hours agoprevOver my career I've certainly written more SQL than any other type of code. Not so much in the last five years so I'm sure I've lost some of it, but I used to really enjoy it. Once you stop thinking iteratively and start thinking in set operations it becomes quite natural and powerful. reply bob1029 22 hours agoparentOver the years I have been pushing more and more responsibilities into the RDBMS. I now see things mostly in terms of ETL, SQL and schema. Virtually every conversation I've ever had about the application of technology to the business could be expressed in these terms. Business logic defined as SQL queries can be quite intuitive when the schema is structured well and aligned to the business stakeholders' perspectives. Code, frameworks, ORMs, \"best practices\", patterns, et. al. are ultimately a distraction. There are a million ways to get the data in & out of the database. Moving the bits around is not valuable. There are many overblown software solutions out there that could have been a simple merge statement or CSV import job. I think that a lot of the misconceptions and bad moods about SQL come out of being forced to work with nasty schemas. The language itself is really domain specific. Consider that one wouldn't complain as much about a super fucked up nested query (and resulting SQL syntax woes) if it wasn't necessary to write such a query in the first place. Aligning tuples and relations to the way the business typically talks about them means you will be less likely to be fighting these things over time. Often, it isn't possible to refactor the schema from zero, but you can put replicas/views around a \"bad\" schema and target it with your new development & refactors. reply fifilura 21 hours agorootparent> Consider that one wouldn't complain as much about a super fucked up nested query (and resulting SQL syntax woes) if it wasn't necessary to write such a query in the first place. And in \"modern\" SQL this is solved with CTEs. Use them to unwrap the schema the way you want it first, before implementing the business logic. reply larodi 20 hours agorootparentthis is poor man's SQL to unwrap for business logic. SQL is such more more about everything else and so litte about the trouble with mapping business logic into storage. reply bbkane 16 hours agorootparentCould you please add some examples? I'm still living in poor man's SQL land reply solumunus 12 hours agorootparentprevPlease elaborate. reply larodi 8 hours agorootparentCTEs can indeed align mismatched and denormalised storage to some E/R which follows some business logic. That much you can say. But CTEs allow for recursion and save time when you need complexity but you want to follow a precise step-by-step reasoning. This dis not arise as a need to solve business mess, but as a need to skip DDL for views/mviews. You can have arbitrary dimensions sliced in CTEs which does not immediately imply a goal of business schema alignment. Besides the top SQL devs I’ve met don’t lose time to align schema at all, but write the DB table names and columns as they are, because well… because they were usually the people who created the mess this way. Many reports such as month-to-month increase of sales or other statistical stuff is much more readable when implemented with CTes. Besides - older versions of DB software didn’t always have window functions or proper ranking, so problems such as top-n were not (and in some occasions still are not) trivial to write unless view/cte is used. We talking pages of single query here, not textbook examples. So is really about dice and slice in a convenient way, but less about business schema or the ideal E/R reply fifilura 7 hours agorootparentMaybe I misunderstood, and if I did it makes for an interesting discussion, but your explanation is mainly about what CTEs can be used for not what they can not be used for. In my experience, normalizing tables is mostly for simplifying inserts. When you write business logic it is rather the opposite, you want non-normalized data because this is where the data is all in one place. And this de-normalization is a great use of CTEs. As well as a general cleanup of the input data, which was my original point. reply fifilura 22 hours agoparentprevI second all of that! I wish more people would see the beauty. After a session of SQL, when I take a step back and think. \"Hold on. What I have been doing lately is just pure logic. No library dependency resolution, no concurrency problems (even though massive concurrency is certainly under the hood). No mutability issues. Just logic.\" SQL obviously has its warts, some of them serious, like testability. But at the end of the day, I wish all programming was like that. Let the computer decide how to do stuff under the hood. And let the human focus on the logic. I have somewhat half-assed tried to read up on Prolog for trying to take it to the next level, but failed sofar unfortunately. (It was also a goal to try to unlearn some SQL to avoid getting stuck in some local optimum). Maybe somewhere between SQL and Prolog is the future of programming. reply Rendello 22 hours agorootparent> Maybe somewhere between SQL and Prolog is the future of programming. Must be Datalog then ;) reply coliveira 20 hours agorootparentprevProlog is very powerful, if you see what professionals can do with it it's eye opening. Unfortunately, it takes a complete relearning of programming to achieve that proficiency level. And after you reach it, you probably cannot use it in your day job... reply arkh 4 hours agorootparentMy problem with Prolog is every time I want to start using it I feel like I'm populating a database then doing queries on it. So it feels like SQL with another syntax and less power. I'm sure I'm wrong and missing something but that's where I stop. reply coliveira 30 minutes agorootparentProlog is in fact a kind of database. However it is untrue that it has less power than SQL, just the opposite. Probably it feels harder to use because you're already so used to how SQL works. reply diggan 3 hours agorootparentprev> I feel like I'm populating a database then doing queries on it. So it feels like SQL I'm not sure what database technology/data storage that doesn't involve doing those two things to get started. I haven't done any Prolog, but Datalog which is similar and focused on querying data, and the benefits of Datalog for me is that you can write complex/medium-complex queries a lot easier than in SQL, at least for me. Simpler queries are just less characters, but pretty much the same beyond that. Probably helps that the Clojure ecosystem embraced Datalog so it mostly feels like writing Clojure code although it's really Datalog. You don't get that same feeling regardless of what SQL library/DSL you use. reply larodi 20 hours agorootparentprev> Maybe somewhere between SQL and Prolog is the future of programming. it was, it most probably is reply crazygringo 21 hours agoparentprev> Once you stop thinking iteratively and start thinking in set operations it becomes quite natural and powerful. I dunno... I've written a tremendous amount of SQL, and I still have to think imperatively (iteratively) in order to write queries that are actually performant, and to know which indexes need to exist. It would be wonderful if I could just think in terms of set operations, but that tends to result in queries that take 5 minutes to execute rather than 5 milliseconds. My entire thought process is basically -- what table do I start with, what rows in what order, joining to what, under what conditions, aggregating how, rinse and repeat... It's entirely a mental model of loops and aggregation, never of set operations. reply fifilura 21 hours agorootparentIt may be true, until you do your ETL in an index-less database such as BigQuery or Trino. Postgres will always be faster for optimized, end user serving, queries. But BigQuery allows you to scale it to 100s of CPUs without having to worry about indexes. reply liontwist 17 hours agorootparentThis sounds awful. I would do almost any amount of iteration and index tuning to keep the query on a single machine rather than deal with a networked distributed system. When you get slow queries the real problem is algorithmic complexity and linear workers only can do so much, reply RobinL 10 hours agorootparentYou can use duckdb on a single machine. It's also indexless (or more accurately, you don't have to explicitly create indexes) reply fifilura 15 hours agorootparentprevYou are rightfully proud of your skills! reply crazygringo 20 hours agorootparentprevYes, I'm talking about end user queries. Not reports that take 2 hours to run. But even with BigQuery, you've still got to worry about partioning and clustering, and yes they've even added indexes now. The only time you really just get to think in sets, is when performance doesn't matter at all and you don't mind if your query takes hours. Which maybe is your case. But also -- the issue isn't generally CPU, but rather communications/bandwidth. If you're joining 10 million rows to 10 million rows, the two biggest things that matter are whether those 10 million rows are on the same machine, and whether you're joining on an index. The problem isn't CPU-bound, and more CPU's isn't going to help much. reply fifilura 15 hours agorootparentOf course there are optimizations to be made, such as not joining on the raw data or saving the order by to last. And avoid outer joins between two large sized partitioned tables. But to me those optimizations are not imperative in nature. (And BQ will probable eat the 10 million to 10 million join for breakfast...) reply griomnib 20 hours agoparentprevBeing able to master the theoretical, practical, and skill-based components of designing a good database schema is the absolute truest test of understanding any systems design. People skip ahead to all kinds of nonsense; but most of software engineering is putting the right data into the right format, and moving it around reliably. I just did a major refactor of a complex distributed code base. I pretty much only count the work I did on schema re-design as the actual “job”, the rest was many hours of coding, but that’s really just implementation. There are other ways to define schema than SQL of course, but it’s really the perfect way to learn true systems engineering. reply SoftTalker 19 hours agorootparentVery true. My manager at one of my first jobs liked to say \"get the data model right and everything else will be easy\" and that has largely been proven true in my experience (and it even applies if you're not using an RDBMS). reply maCDzP 20 hours agoparentprevSQL clicked once I read the original paper and it explained it terms of sets. reply pjmlp 11 hours agoparentprevYeah, I also kind of like coding in SQL, with PL/SQL being my favourite extension language, which is kind of heresy in HN, but whatever. reply dagss 10 hours agoprevI have been writing a ton of SQL -- implementing a lot of the business logic of a (stream processing) application in it. I really really like it, especially that I bring the computation to the data instead of the data to the computation. I often talk to developers who hate that idea though. They want me to instead move all the data to the backend, for a massive IO hit, just so that the computations can be expressed in a \"real\" programming language. I think SQL the concept is good but SQL the language is the problem. There are so many awkward things in it -- not strange as it has had no competition in 40(?) years!! The mental model of the program I write is fine but I really need to overlook the syntax and to the program I am really writing to see any elegance... What we need I think is a properly designed programming language, designed for existing databases (Postgres, MSSQL) compiling to SQL dialects. I see some contenders but they all have some attachment to a special domain, such as not allowing data modifications (PreQL) or being associated with other databases. Itching to do it myself, but it's a lot of work and a long long road to any adoption with no guarantee of success, and no revenue stream that I can think of. The most popular backend languages were made by large companies, but I think coding in SQL is in a catch-22 where it will be frowned upon until there is a better language and no better language until it is more popular.. reply willvarfar 10 hours agoparentMe too me too :D There's a lot that is very right about SQL, but a few clunky bits around the edges. CTEs made a lot of difference, and window functions - which can be a bit head bending - made difficult things a tiny bit easier. I'm using BigQuery, which supports structs and arrays, but only recently allowed arrays to be grouped by, although there is still no equality check etc. BigQuery is slowly adding more sugar, like aggregate UDFs and polymorphic UDFs using ANY TYPE parameters etc, and I find myself putting more reused logic into tidy functions, but my pet want is for temporary functions to be declared and scoped like CTEs so they integrate a lot better into tooling like DBT that wants everything to be in one statement. And the one most productive thing they could add? Allowing you to specify null behaviour on JOIN USING. (Having to spell out foo.bar IS NOT DISTINCT FROM bar.bar on a join is unobvious and ugly. Something like USING (bar RESPECT NULLS) would be so much nicer.) reply wink 9 hours agoparentprevI can't put my finger on it but I think many people see this as two operating modes, for lack of a better term. The more monolithic and enterprisey your solution (and with bespoke DBMS), the more it leans towards anything more complex than an index and maybe a couple triggers - and the more microservices-y where every small service owns its own database (and only half of them are RDBMS) the less complex code is desired in the DB itself, because you're also migrating off of single instances/clusters a lot (just taking a relatively dumb data dump with you, or even just adding new replicas, ship-of-theseus-like). reply mbowcut2 23 hours agoprevAbsolutely horrifying. Well done. reply teeray 23 hours agoparent\"Thanks, I hate it\" reply 1st1 22 hours agoprevFWIW I tried doing AoC for a few days with EdgeQL, and the experience proved to be quite interesting. Some of my tweets (I should write a blog post): - https://x.com/1st1/status/1864069589245858083 - vs SQL https://x.com/1st1/status/1864412869108092997 reply sceadu 23 hours agoprevfor those who don't know, the author is one of the best DB researchers in the world reply beoberha 22 hours agoparentThomas Neumann doing Thomas Neumann things reply RaftPeople 22 hours agoprevI wrote a cubing/containerization system in SQL, but it did use the additional capabilities in a sproc for looping etc., so not just a single SQL stmt. But the problem actually mapped well to SQL's capabilities in some ways. The basic algorithm was: 1-Loop through items in descending size order 2-Test X different positions and orientations such that one corner is butted up against an existing corner (of container, or previously placed items, etc.) 3-Choose the winner based on some sort of point systems that encapsulates some set of rules (e.g. preferred orientation, or minimizes small gaps between sides of items, etc.) These were some aspects that lined up well with SQL: A-Testing of X number of positions and orientations all with a relative simple statement, using as input: existing corner points from previously placed items, some control table with the variations in X,Y,Z orientations. B-The ability to maintain a set of previous options for each iteration (e.g. item #2 was tested in these positions with these stats and all are temporarily being considered reasonable options), add new item to each one of those previous tests and accumulate stats. It was pretty easy to configure a \"window\" of how many different open options were being tracked to optimize the result without overwhelming memory. The SQL to include the options was almost the same as if there was only one previous chosen option. Some aspects were a bit painful to shift the model mentally to fit in with SQL. reply linsomniac 19 hours agoprevLong ago I was interviewing for an operations job, and their \"leetcode\" interview question was to create an invoice report of some fairly large public data-set. Because of the size of the dataset and the way they wanted it sliced and diced, it wasn't just a straightforward set of JOINS, and they clearly wanted you to do some simple SQL queries and then have some code loops that sliced and diced it all. That wasn't a solution the data scientist in me (which I'm not) was ok with. I ran my high level thoughts about the problem past a data scientist buddy of mine and he said \"sounds like you're on the right track\", but not being a data scientist it was hard for me to put together a soludion. I told them that if I was asked in my work capacity to do something like this, I'd probably be reaching for a reporting package like Crystal Reports, but I haven't touched it in ~30 years. \"Sure, I get that, just write some code to generate the report\". I had written all the Ansible and related goodies to spin up an instance, get MySQL set up and configured, and figured the \"right\" solution was there in the SQL arcana. I played with it and played with it, mostly writing off the job because what kind of company judges a sys admin based on building a reporting package? They had set the expectation that the homework should take 2-4 hours, and I chewed on it for longer than that and finally said \"thanks but no thanks, this is outside my wheelhouse\". But I kept chewing on the problem. A couple weeks later I had the right set of materialized views and queries and joins that not only solved the problem entirely in SQL, but solved it QUICK. SQL is amazing, but there are a log of tricks of the trade you've got to have at your fingertips to make it fly. I'm still convinced that a lot of it is just throwing a bunch of things at the wall until something sticks. reply tveita 17 hours agoprevI found Clickhouse easier to handle than pure SQL for this - as an analytics database it has conveniences like parsing input files with regexp, user defined functions and array functions. It's often possible to treat it more like an array programming language. But I still sometimes couldn't avoid the tuple explosion problem. https://clickhouse.com/docs/en/integrations/data-formats/tem... reply zX41ZdbW 1 hour agoparentI'd like to check what is the tuple explosion problem. reply pawptart 18 hours agoprevI was crazy enough to try this as well this year. It would be an extreme stretch to consider me a SQL expert, but I did make it to Day 14. I agree with all the points in the post. It's really not that bad. If I had more time to devote to it, I think I could have reasonably completed more. Here's my writeup, contains a link to the repo if you want to see some of the soutions. https://github.com/ty-porter/advent-of-code/tree/master/2024... reply neriymus 16 hours agoprevSlightly related, here's one I saw being solved using Cypher: https://www.linkedin.com/posts/halfterpierre_adventofcode-di... reply superdogsniffer 1 hour agoprevDay 19 fails the actual test input. I guess nobody checked his work. reply pavel_lishin 20 hours agoprevI've been slowly working through AoC2024 in Google Sheets. I should really write up what I've been up to. reply thehours 16 hours agoprevDoes 'pure SQL' have a specific definition (or reference spec)? The author doesn't mention it, except to say he tested it across Umbra, Postgres, and DuckDB. Even then, some days weren't 'supported by DuckDB due to a missing xor operator'. reply grumple 14 hours agoparentThere is an ISO spec (you have to pay for access): https://www.iso.org/standard/76583.html reply datadrivenangel 22 hours agoprevThe data parsing must have been painful. High wizardry. reply taberiand 22 hours agoparentI feel like it'd be entirely acceptable to load the inputs into tables first and still qualify as pure sql, because string parsing in sql is so blergh reply fifilura 21 hours agorootparentInput in AoC is always well-formed. And you can always use a regexp? Seems like the smallest problem to me. As soon as you get past that recursive one row per line trick in the posted solutions reply taberiand 20 hours agorootparentObviously it's possible, particularly in this context, but generally speaking I'd rather avoid it. reply fifilura 22 hours agoparentprevThe data parsing is just one recursive query to read one line at a time, and it is done pretty much the same way in each problem. reply forgot_old_user 21 hours agoprevhaha I couldn't help but silently mouth \"BUT WHHYY\" before I clicked this link reply pearjuice 21 hours agoprevDoing it in pure SQL is really impressive but I think the real tell-tale sign of peak \"cracked engineer energy\" is the maintained, decade-old blogspot site. Can't exactly put my finger on it, but really gives off \"niche mastery\". I don't even know the authors but I'm sure in the right circles a few dudes maintaining a blogspot site called \"database architects\" for a decade probably don't need an introduction. reply Moosturm 10 hours agoparentThose authors are the elite of DB mastery. reply drewbitt 22 hours agoprevInsanity. I wonder how much cleaner it could look in EdgeQL. reply 1st1 20 hours agoparentSomewhat, depending on what you appreciate in languages. See my other comment in this thread: https://news.ycombinator.com/item?id=42578562 reply nostradumbasp 17 hours agoprevIt takes an incredible human to do something like this. Pure art. Not enough of that in the programming world. reply apavlo 17 hours agoparentThomas is the greatest database systems researcher in the world. He is an incredible human. reply CommanderData 23 hours agoprevI'm angry but in a good way. reply spiralshape 11 hours agoprevGreat work showcasing SQL’s versatility! reply hobs 21 hours agoprevWhen you realize recursive SQL allows iterative calculation then you can pretty much do whatever you want, no matter how bad. reply yakshaving_jgt 17 hours agoprevA former colleague did the same with most of AoC 2020 a few years ago. https://github.com/Jell/advent2020 reply ikiris 22 hours agoprev [–] This is impressive. Well done. I hate it reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Thomas Neumann shared his experience of solving the Advent of Code 2024 challenges using pure SQL, highlighting it as an exercise in alternative problem-solving approaches. - Neumann successfully completed all challenges, though he noted SQL's limitations in handling complex algorithms and state updates, suggesting potential improvements for SQL's efficiency. - The blog post includes a detailed SQL solution for one challenge and discusses the possibility of enhancing SQL to better support complex algorithm execution directly within databases."
    ],
    "commentSummary": [
      "Advent of Code 2024 in pure SQL highlights SQL's ability to handle complex challenges, sparking interest in its performance and logical structure.",
      "The discussion reveals varied experiences with SQL, from optimizing stored procedures to using it for business logic, showcasing both its strengths and challenges.",
      "The conversation also explores the potential of SQL-like languages and the balance between database logic and application code, reflecting diverse opinions on SQL's role in software development."
    ],
    "points": 431,
    "commentCount": 97,
    "retryCount": 0,
    "time": 1735845650
  },
  {
    "id": 42578237,
    "title": "U.S. appeals court strikes down FCC's net neutrality rules",
    "originLink": "https://www.tvtechnology.com/news/sixth-circuit-of-appeals-strikes-down-fccs-net-neutrality-rules",
    "originBody": "News Regulatory and Standards U.S. Appeals Court Strikes Down FCC’s Net Neutrality Rules By George Winslow published yesterday 6th Circuit finds regulator did not act within its legal authority When you purchase through links on our site, we may earn an affiliate commission. Here’s how it works. (Image credit: SimpleImages via Getty Images) In a major victory for cable companies and telcos that raises concerns about the Federal Communications Commission's regulatory authority in the wake of recent U.S. Supreme Court rulings, the 6th U.S. Circuit Court of Appeals has struck down the agency’s net neutrality rules. The rules require broadband operators to treat all internet traffic equally and banned them from giving preferential treatment to some sites by speeding up or slowing down consumer access. The FCC had implemented net neutrality rules under former President Barack Obama, which were then dropped during the Trump administration. Last April the FCC voted 3-2, along party lines, to reinstate the rules in its \"Safeguarding and Securing the Open Internet Order.” Industry groups then sued the FCC to block their implementation. The 6th Circuit justified its decision to strike down the rules in part by referring to the U.S. Supreme Court’s decision last June in Loper Bright Enters. v. Raimondo. That ruling rejected the longstanding Chevron doctrine giving regulatory agencies a fair amount of latitude in implementing their rules. The reversal highlights the growing challenges the FCC is likely to face in the wake of the Loper Bright ruling. “[U]nlike past challenges that the D.C. Circuit considered under Chevron, we no longer afford deference to the FCC's reading of the statute. Loper Bright Enters. v. Raimondo, 144 S. Ct. 2244, 2266 (2024) (overruling Chevron U.S.A. Inc. v. Nat. Res. Def. Council, Inc., 467 U.S. 837 (1984) ) … Using ‘the traditional tools of statutory construction,’ id., we hold that Broadband Internet Service Providers offer only an ‘information service’ under 47 U.S.C. § 153(24), and therefore, the FCC lacks the statutory authority to impose its desired net-neutrality policies through the ‘telecommunications service'’ provision of the Communications Act, id. § 153(51). Nor does the Act permit the FCC to classify mobile broadband—a subset of broadband Internet services—as a ‘commercial mobile service’ under Title III of the Act (and then similarly impose net-neutrality restrictions on those services). Id. § 332(c)(1)(A). We therefore grant the petitions for review and set aside the FCC's Safeguarding Order.” Following the ruling, FCC chair Jessica Rosenworcel said in a statement: “Consumers across the country have told us again and again that they want an internet that is fast, open and fair. With this decision it is clear that Congress now needs to heed their call, take up the charge for net neutrality and put open internet principles in federal law.” Get the TV Tech Newsletter The professional video industry's #1 source for news, trends and product and tech information. Sign up below. Contact me with news and offers from other Future brands Receive email from us on behalf of our trusted partners or sponsors By submitting your information you agree to the Terms & Conditions and Privacy Policy and are aged 16 or over. In a statement commissioner Brendan Carr applauded the ruling, saying, “I am pleased that the appellate court invalidated President Biden’s Internet power grab by striking down these unlawful Title II regulations. But the work to unwind the Biden Administration’s regulatory overreach will continue.” “Today’s decision is a good win for the country, Carr also noted. “Over the past four years, the Biden Administration has worked to expand the government’s control over every feature of the Internet ecosystem.” “President Biden’s decision to impose these Title II regulations represented a break from the bipartisan consensus established by a Republican Congress and a Democrat President and enshrined in law nearly thirty years ago—a consensus that provided a stable regulatory framework that allowed the Internet in America to flourish,” he said. The lengthy full statement from Carr can be found here. TOPICS regulatory and standards FCC net neutrality Rosenworcel CATEGORIES Regulatory and Standards George Winslow George Winslow is the senior content producer for TV Tech. He has written about the television, media and technology industries for nearly 30 years for such publications as Broadcasting & Cable, Multichannel News and TV Tech. Over the years, he has edited a number of magazines, including Multichannel News International and World Screen, and moderated panels at such major industry events as NAB and MIP TV. He has published two books and dozens of encyclopedia articles on such subjects as the media, New York City history and economics. MORE ABOUT REGULATORY AND STANDARDS Reactions to Net Neutrality Repeal Come Sharp and Swift FCC Hears About ‘EAS at the Edge’ LATEST Mike Downs To Join Society of Broadcast Engineers as Executive Director SEE MORE LATEST ► LATEST ARTICLES 1 Reactions to Net Neutrality Repeal Come Sharp and Swift 2 GameLoop TV To Bring Interactive, NextGen TV Games to CES 2025 3 Gray Media’s General Manager in Anchorage Retires 4 FCC Hears About ‘EAS at the Edge’ 5 U.S. Appeals Court Strikes Down FCC’s Net Neutrality Rules",
    "commentLink": "https://news.ycombinator.com/item?id=42578237",
    "commentBody": "U.S. appeals court strikes down FCC's net neutrality rules (tvtechnology.com)356 points by pseudolus 22 hours agohidepastfavorite302 comments yesco 21 hours agoI've always felt the FCC was an inappropriate path for enforcing this. Ideally it would be powers granted to the FTC either through legislation or potentially through existing powers they might already have in this domain. The FCC's jurisdiction on this was already shaky even before Chevron was overturned, I think the moment activism was pushed in this direction net neutrality was doomed a decade of unending lawsuits. Even if they somehow made it passed the courts, I'm not quite convinced the FCC would be motivated to actually do much about this even with a sympathetic administration. Consider that prior to the internet, they were pretty much created to do the opposite of what people want from net neutrality. These are just my armchair thoughts though. reply timewizard 20 hours agoparent> Consider that prior to the internet, they were pretty much created to do the opposite of what people want from net neutrality. You can scale broadband almost infinitely. You _cannot_ scale phone service or television or radio the same way. There are simple limits on physical space and on carrier allocations that require there to be a licensing body. The FCC is required to be reasonable and non-discriminatory and broadcasters are required to serve their local communities directly. The FCC can only license broadcasters and can place no limits on receivers at all. This is because these airwaves collectively belong to the people. It was created to serve these purposes. None of these are the \"opposite\" of net neutrality in any way whatsoever. reply yesco 19 hours agorootparentJust my 2 cents, but I don't really see how your first paragraph proves the assertion you make in the second... Arguments about Netflix using too much bandwidth on an ISPs network are exactly the kind of thing I'd expect FCC personnel to be sympathetic with considering it's the same argument they use for all the other mediums they cover... It ultimately devolves all discussions into crappy metaphors involving phone and radio when the Internet is really it's own thing and should be treated as such. Meanwhile the FTC handles consumer protection, they are by nature going to be more proactive about this kind of thing. If they handle it then it aligns with their existing incentive structures and they can brag about it to congress in the same way they do for everything else they do when budgeting discussions come up. No matter how I think about it, the FCC just seems incompatible with what most net neutrality advocates, such as myself, actually desire. reply kevin_thibedeau 15 hours agorootparent> Netflix using too much bandwidth on an ISPs network Netflix doesn't use any ISP's bandwidth without a colocation agreement. The ISP customers use the bandwidth they have already paid for. If you're going to sell IP services you have to deliver those services honestly and fairly. reply do_not_redeem 13 hours agorootparentIndeed. This situation reminds me of when SMS was new, where you had to pay 25¢ to send a text, and the recipient had to pay 25¢ to receive the text. The ISPs want to double dip, get paid twice for the same data like cell phone companies were doing in the 90s. reply _blk 10 hours agorootparentNot necessarily since you pay for wireless connectivity. RX/TX doesn't matter. Recipient could be on a different network too. You want a text, you cover the costs. Banks still charge me on both ends for int'l wires. Now I personally agree with you and not with the reasoning I presented above but it's still an argument. reply kortilla 11 hours agorootparentprevThe problem with this argument is that you end up in a scenario where an ISP makes an inadequately provisioned peering path (either through an IXP fabric or transit) to Netflix that just runs at saturation. Customers still get plenty of bandwidth to everything else and Netflix effectively ends up capped. This happened with a major cable ISP and Netflix already. You’ll quickly find out that “bandwidth the customers paid for” doesn’t mean nearly as much as you think it does because you can have excess in the last mile and terrible AS-to-AS connectivity. reply kevin_thibedeau 10 hours agorootparentThere is no excuse for inadequately provisioning the backend. They know what their customers want to consume. It isn't 99% text anymore. If I download 1TB from 1000 servers should the ISP have any claim to charge them a toll? If I download 1TB from a single server do they now get to impose a toll? reply adamsb6 59 minutes agorootparentThe excuse would be that it's not free and the revenue from lost customers is less than the cost of upgrading the peering points. reply kevin_thibedeau 5 minutes agorootparentThe money that funds everything on the Internet comes from the end users who pay to access. The ISPs and all middlemen take from that pool of cash. If I've been sold a service to an open network I need to be delivered that service. The minutia of backhaul access is irrelevant. Their business doesn't exist without people and businesses paying for access. What this arrangement does is limit revenue growth which is why the financial schemers want to double dip with their extortionate tolling on entities that already pay. rtkwe 14 hours agorootparentprevThat's always confused me about that argument.. They are getting paid, by their customers! reply Yeul 8 hours agorootparentSeems to me American ISPs have a lot of political power. Not only do they want to extort money they also don't want to upgrade their infrastructure to give the service that the consumer wants. reply autoexec 1 hour agorootparentThey don't want to upgrade their infrastructure to give the service that the consumer paid for and they themselves advertise. reply rtkwe 4 hours agorootparentprevThey're quite powerful and get tons of grant money to extend their networks and provide service they never actually provide. reply Hikikomori 8 hours agorootparentprevThey're paid but what you pay is based on oversubscription, everyone not using their connection at the same time, as building a network to support that would be much more costly. reply adriancr 8 hours agorootparentOversubscription is an ISP trick to make more money. People do pay for a certain bandwidth. They should be able to get it. Same thing with overbooking on airplanes. You paid for a seat you should be able to use it. Your argument wouldn't work there either. reply Hikikomori 8 hours agorootparentIts a cornerstone of building cost efficient networks. People pay for a certain sized pipe, what they pay also covers the rest of the ISPs networks and costs. With no oversubscription the ISP would need maybe 20-30x more infrastructure, do you think it would have an impact on what you pay? Not sure why I have to say this, but, networks are not airplanes. reply rockskon 7 hours agorootparentUtter nonsense. Network tech hasvimproved by orders of magnitude over the past decade to let ISPs transmit more data than ever for cheaper than ever. Oversubscription is not the cost-restrictive mandate it once was. reply Hikikomori 6 hours agorootparentAnd last mile access didn't improve? Not oversubscribing is a cost multiplier at every level. 1 million 1 Gbit customers in a city is going to need 1000 100Gbit connections out of that city and the same for transit, and that will have no impact on pricing? And everything is on average used at 1% of capacity. reply michaelt 3 hours agorootparentIf my ISP can only afford to supply me with 1TB of transfer at 1Gbit, that's fine. They can put it in the adverts, the contracts, and the pricing. For customers who want 10TB of transfer, they can offer a higher cost option. And if they choose to gamble, advertising and entering into contracts promising \"unlimited data\", which they think will be more profitable across their entire customer base? Then they've got to do supply what they promised in the adverts. They chose to gamble that way, and if they lose money gambling that's their business. reply Hikikomori 3 hours agorootparentYou have that on mobile subscriptions usually, heavy users pay more and low usage users are not subsidizing them. I take you are fine with paying 10x or even more for your no oversubscription Internet connection then? Oversubscription is not gambling. The way it works after your last mile connection is that ISPs look at link usage in their network, city level distribution, city to city, transit, peering, etc, once it reaches 60-80% utilization at peak you start looking at adding more capacity. Bad ISPs (most US ISPs) will let this go too far though. reply michaelt 2 hours agorootparent> Oversubscription is not gambling. Sure it is. If I promise 30 people they can have a burger at my barbecue, but I only buy 20 burgers, I'm gambling that 20 people or fewer will show up. It might be a reasonable gamble, based on past barbecues - but the guests left hungry will still be hungry, and I'll have broken my promise to them. reply Hikikomori 30 minutes agorootparentIt might be if traffic had sudden jumps of like 30%, but it doesn't and there is headroom available. Traffic increases slowly over time and you have plenty of time to upgrade your network. adriancr 6 hours agorootparentprevI have a 10gbps fiber LAN. It was cheap to set up and have it running. I do not buy your argument. People buy a certain level of service, they should be able to enjoy it as in the rest of the world. reply Hikikomori 5 hours agorootparentLet me know when you've built that out to a million customers without any oversubscription. reply adriancr 3 hours agorootparentISPs are free to oversubscribe as much as they want. As long as they also provide people the bandwidth sold to them when they want it. Otherwise compensation should be in order if they throttle. reply Hikikomori 2 hours agorootparentThe point of oversubscription is maintaining a network that keeps costs low while providing a good service without congestion. They monitor their network (not your last mile connection, everything else) and once links start reaching 60-80% of capacity at peak times you start adding more capacity. Bad ISPs (like most US ISPs) let this go way too far though. reply abduhl 5 hours agorootparentprevAnd does your cheap and easy to set up 10gbps fiber LAN cross under the interstate 10 feet deep? reply adriancr 5 hours agorootparentThe same fiber I have for 10gbps can be used for 400gbps... just by changing sfp modules. Same logic for interstate. You lay fiber once and scale equipment as needed. If you already have the fiber there then just use better modules. reply BenjiWiebe 4 hours agorootparentIt's oversubscription all the way up, and it works. What doesn't work is when a greedy/lazy ISP tries to increase the oversubscription ratio too far. reply abduhl 4 hours agorootparentprevIt appears that your ignorance on the topics of infrastructure and the advancement of technology over the past five decades makes having a useful conversation impossible. Not every cable in the ground was installed with today's state of the art technology. Enjoy your apparently unthrottleable internet connection. reply adriancr 3 hours agorootparentthrottling should be an exception, not the norm. If ISPs sell you a bandwidth per month they should deliver it. You're the one that's short changed if you accept the throttling. reply dataflow 7 hours agorootparentprevOverbooking and oversubscription are inherently very different. Flying is a one-time service with a specific and fixed point at which the service is provided. Its peak usage is the expected usage. Internet access is a continuous service promise where it's nonsensical to expect the provider to predict exactly when every customer would want to use it. The peak usage is not the expected usage. These are not comparable situations. reply adriancr 6 hours agorootparentThey are comparable for expectations. You pay for a certain level of service, you expect it delivered. reply dataflow 5 hours agorootparentFirst: No, they're not. That is an unreasonable expectation divorced from the reality. What exactly do you think would happen if everyone in town switched off and on their AC-powered devices at the same time? What do you think would happen if everyone in town moves to the same street and starts using their cell phone to stream 4K videos at the same time? Do you seriously think it's reasonable to expect every system to deliver at its peak with arbitrary demand and load on it? Second: If you're going to play the \"I paid for this\" game: this stuff is generally in the contract anyway. It is the level of service you paid for. The overbooking possibility? You paid for it, it was in your contract. Throttled service? That was in your contract too. You're getting what you paid for. reply adriancr 5 hours agorootparent> What exactly do you think would happen if everyone in town switched off and on their AC-powered devices at the same time? Large systems have their own rules. If everybody watches the superbowl at the same time I'd expect the power grid not to fail. If everybody gets home at around the same time from work and start powering on devices I'd expect power grid not to fail. If it suddently gets cold and people turn on heating around same time, I'd expect it not to fail. Those seem valid expectations and are met. Therefor when I say if everyone starts streaming netflix it should work, then this is also valid expectation and should be fine. > It is the level of service you paid for. The overbooking possibility? You paid for it, it was in your contract. I get what I pay for when I want. I have 1gbps, I can run full speed as much as I want and sometimes it's nice to do that. I am also in europe. I don't get throttled service and what you say is not in my contract. What do you say to that? reply dataflow 5 hours agorootparentYou're not addressing the question. > If everybody watches the superbowl at the same time I'd expect the power grid not to fail. \"I get what I want immediately\" to \"the system won't fail\" is a nice way to shift goalposts. If everyone shows up to their flight then the flight won't crash, it'll depart just fine with the capacity it has and offer everyone else on the next available flight. You know, the same thing that happens when the power grid is turning back on. They do it one piece of the grid at a time. Which results in you getting less than what the person next door paid for. Because that's reality. > I am also in europe. I don't get throttled service and what you say is not in my contract. What do you say to that? When there are a ton of people crammed in the same location overloading the network, you get throttled, whether intentionality or not, whether you like it or not. There is no way on Earth that you being in Europe somehow makes you immune to reality. reply adriancr 5 hours agorootparent> offer everyone else on the next available flight. Plus compensation which is an admission of fault on airlines. > There is no way on Earth that you being in Europe somehow makes you immune to reality. Perhaps you can think of how that might work, i can think of: - large fiber pipes capable of accomodating spikes - average out traffic for large systems - have predictable traffic at scale, scale up as needed, power on/off equipment, etc reply abduhl 5 hours agorootparentprevFrom the first google result (although this was 5 years ago): “Europe gave internet service providers the right to throttle online traffic to prevent congestion as network demand spikes amid coronavirus stay-at-home and quarantine orders. Netflix and YouTube have already agreed to switch to standard-definition streaming in Europe to reduce bandwidth demand.” https://www.hollywoodreporter.com/business/digital/coronavir... reply adriancr 5 hours agorootparentRight does not mean it's used. I would change provider if I were throttled. > Netflix and YouTube have already agreed to switch to standard-definition streaming in Europe to reduce bandwidth demand.” You can change quality back to whatever. You can also use other services or use the connection for other things. reply BenjiWiebe 4 hours agorootparentYou're 99% on an oversubscribed connection, but the oversubscription ratio was chosen correctly so you'll likely never notice any problems. Dedicated Internet access is a thing, but it's expensive; and I'd argue that even that is oversubscribed if you go far enough up the chain. reply adriancr 3 hours agorootparentSure, ISP can oversubscribe as much as they want. As long as they make a fair guess and I get my bandwidth whenever I want, all is good. That seems to be the case right now. Dedicated internet access is a different thing, I agree, it also provides different guarantees, support, etc. reply trhway 13 hours agorootparentprevAnd they are paid for the common carrier service. If they want to be able to slow down some sites - basically it is an editorial action by making some content less available or even de-facto non-available - they should lose the common carrier status with all the consequences of it. reply Hikikomori 8 hours agorootparentprevBecause the price you pay doesn't account for everyone using their connection at the same time, if this was the case you would need to pay a lot more. ISPs can either increase the price you pay directly or try to get that money elsewhere. Doesn't really matter if you pay for this to your ISP or to them via Netflix. The real problem is lack of competition and insane profits of US ISPs. Where I live its common for the local municipality to build out fiber and run their own ISP that lets other ISPs connect to their network and sell access. In that case part of my cost goes to the local ISP (which isn't profit driven) and the Internet providing ISPs can compete on price. reply michaelt 7 hours agorootparent> Doesn't really matter if you pay for this to your ISP or to them via Netflix. Of course it matters. If my ISP demands an extra $1 from me, I can move to a different ISP. If my ISP demands an extra $1 from Netflix, Netflix can't move me to a different ISP. That's pretty important, as our whole economy is built on the assumption that raising prices lowers demand. reply Hikikomori 6 hours agorootparentAnd when all other ISPs charge $1 extra? Most people cannot choose between different ISPs without moving anyway. reply michaelt 3 hours agorootparentIf only there were some sort of legislation designed to reign in monopolies, duopolies, collusion, and business practices that prevent competition :) You know who doesn't have a monopoly allowing them to charge whatever they like? Netflix. reply Hikikomori 3 hours agorootparentBased on US ISP profits I'd say there is practically no regulation and they even split up the market so there's no competition so most consumers only have one choice. Netflix competes with all the other streaming services no? If they increase the price too much people will stop subscribing. Or is your argument that they and other content owners should be forced to sell their content to all other streaming services? reply lupusreal 1 hour agorootparentprevI don't use Netflix (true), but if almost everybody else does (also true) then I and up paying more to subsidize them. Also there's only one reasonable option for internet in my neighborhood. Starlink or cellular would cost even more and for what? I'd still be subsidizing Netflix use. Splitting up Netflix doesn't help; if everybody falls asleep watching uses a plethora of streaming services instead of one, they're still wasting bandwidth. Similarly, splitting up ISPs doesn't solve the problem either. The real solution would be to charge everybody for what they actually use. Metered data, with the price fluctuating with demand. The way the electrical system works. reply tempodox 9 hours agorootparentprevAnd how is it reasonable to charge different costs for that same bandwidth depending on what I'm using it for? reply yesco 13 hours agorootparentprevI fully agree with you here, but people that don't agree still exist despite how obviously wrong they may seem to us. reply InsideOutSanta 10 hours agorootparentprev\"Arguments about Netflix using too much bandwidth on an ISPs network are exactly the kind of thing I'd expect FCC personnel to be sympathetic with\" I don't quite understand that line of reasoning. I'm paying my ISP in order to access Netflix. Netflix is not using that bandwidth, I'm using the bandwidth in order to access Netflix. reply gary_0 17 hours agorootparentprevYou're both right: - The FCC's founding mission is to safeguard telecommunications mediums as a commons, to the benefit of the public -- a good fit for the Internet. - The FCC is functionally a bureaucracy that grants licenses to discrete chunks of very scarce communications resources -- a bad fit for the Internet. Either way, some agency needs to be put forward as a candidate for preventing monopolistic corporations from abusing the public Internet. reply vel0city 15 hours agorootparentThe original FCC mandate in the Communications Act of 1934 was: For the purpose of regulating interstate and foreign commerce in communication by wire and radio so as to make available, so far as possible to all the people of the United States a rapid, efficient, Nation-wide, and world-wide wire and radio communication service with adequate facilities at reasonable charges, for the purpose of the national defense, for the purpose of promoting safety of life and property through the use of wire and radio communication, and for the purpose of securing a more effective execution of this policy by centralizing authority heretofore granted by law to several agencies and by granting additional authority with respect to interstate and foreign commerce in wire and radio communication, there is hereby created a commission to be known as the Federal Communications Commission, which shall be constituted as hereinafter provided, and which shall execute and enforce the provisions of this Act. It has always been for communications by wire and radio. I'd argue fiber optics are just a fancier kind of wire from the understandings of what the intentions were back then. reply gary_0 15 hours agorootparentThat's an interesting point, although wire(ish) mediums have pretty strict limitations until you introduce a packet-switched peer-to-peer architecture, at which point you arguably get a very different situation (and different market dynamics) versus telegraph/telephone networks. reply wtallis 15 hours agorootparentCan we all agree that if this boils down to a packet-switched vs. circuit-switched distinction as the crux of whether it can be effectively regulated under current law, that is a profoundly stupid outcome and bad public policy? Surely Congress did not intend to concern itself with such implementation details, and if a regulatory regime conceived in a world that was all circuit-switched is now ill-suited to the economics of a world that is mostly packet-switched, it's not the court's place to invent a way to exclude packet-switched networks from laws that don't care about that distinction. (I'm also not at all convinced that internet backbone/backhaul congestion is sufficiently different from telephone switchboard congestion from a standpoint of market dynamics.) reply ab5tract 14 hours agorootparentprevTelephony switched to digital in the 90s. This didn’t miraculously change their regulatory status. The early days of the internet were defined by copious options for ISPs because telephone lines were required to be available under common carrier conditions. The only reason this situation deteriorated is that broadband infrastructure was classified as for “information” and not “telecommunications”. reply tptacek 14 hours agorootparentprevThis is an argument that is made more potent by omitting the 60 years of legislative history that followed the Communications Act, which was enacted specifically and deliberately to unify regulation of the new medium of radio with that of telegraphy. reply vel0city 14 hours agorootparentI'm just trying to address this idea: > The FCC is functionally a bureaucracy that grants licenses to discrete chunks of very scarce communications I'm pointing out the FCC has historically done a lot more than just issuing licenses for RF, and that communications by wire have been a part of their purview since it's founding. I do agree their roles and rules about what to do with regulating those wires have changed over all those years. reply tptacek 14 hours agorootparentI'm not telling you that it would somehow be illegitimate for Congress to pass a law delegating to the FCC authority to ensure that ISPs provide FRAND access to different streaming platforms (or whatever) over their IP \"dial tone\". But Congress pretty clearly has not done so, and just because we might think it's a good idea doesn't mean the FCC can just make that authority up. reply xp84 7 hours agorootparentprev> some agency needs to be put forward as a candidate for preventing monopolistic corporations from abusing the public Internet. Agreed. May as well be a dedicated one, as well, to avoid any confusion (although I cringe at the likely billion dollar cost of even a \"Hello World\" federal agency). Pity that would require an Act of Congress, or rather, it's appropriate that it requires that since we're a republic. It's a pity Congress is itself a failure which appears to have no route to functioning again. reply fooey 21 hours agoparentprevThe fun tradeoff here is since the courts determined the feds can't regulate it, the states are allowed to instead, with California and New York already taking the lead, and are much more consumer oriented than anything that could be done federally. reply granzymes 21 hours agorootparentThe Federal government can regulate broadband providers, just not as telecommunications providers (subject to common carrier requirements) under the Telecommunications Act of 1996. Congress could update the law if it wanted to. If the court had held otherwise, the Federal common carrier requirements would have preempted any common carrier duties imposed by California and New York. Since today the 6th Circuit says the FCC does not have that power (because broadband providers offer information services and not telecommunications services), California and New York are welcome to impose those duties within their state boundaries (as long as they don't step on any other areas of Federal preemption). reply vel0city 20 hours agorootparent> because broadband providers offer information services and not telecommunications services This is where I highly disagree with the opinions of this court decision. My ISP is absolutely providing me telecommunications services, not information service. For practically all the \"information\" I receive, my ISP is not \"generating, acquiring, storing, transforming, processing, retrieving, utilizing, or making available information via telecommunications\". That maybe sometimes I might land on some webpage hosted by my ISP is inconsequential to the services I am paying for, that's not the service I'm paying for. In the end I'm paying for them to give me the ability to telecommunicate with a server hosted by someone else which then generates, processes, retrives, that information. My ISP is not the information service which lets me post here. Hacker News is an information service acquiring, storing, transforming, processing, and retrieving the data I request. My ISP is only used for me to talk to the Hacker News webserver. https://www.congress.gov/104/plaws/publ104/PLAW-104publ104.p... I truly don't understand how anyone who can even vaguely understand how ISPs work and reads this law could say ISPs aren't telecommunications providers and are instead information service providers. Other than maybe big bags of cash that makes people's reading comprehension a little loose. reply granzymes 19 hours agorootparent>We now turn to the merits, which the parties have argued here in exemplary fashion. But the key flaw in the FCC’s arguments throughout is that the FCC elides the phrase “offering of a capability” as used in § 153(24). That phrase makes plain that a provider need not itself generate, process, retrieve, or otherwise manipulate information in order to provide an “information service” as defined in § 153(24). Instead, a provider need only offer the “capability” of manipulating information (in the ways recited in that subsection) to offer an “information service” under § 153(24). Even under the FCC’s narrower interpretation of “capability,” Broadband Internet Access Providers allow users, at minimum, to “retrieve information stored elsewhere. And we think it equally plain, for the reasons recited below, that Broadband Internet Service Providers offer at least that capability. reply tptacek 19 hours agorootparentAs I said elsewhere on this thread, I read this as the court saying that something to the effect of \"the underlying links that comprise the Internet backbone are probably telecommunications, but the provision of global Internet service on top of them constitutes information services\". Which sounds crazy in 2024, but I'm not sure that's a crazy reading of the world circa 1996, which is the interpretation that controls here --- the Telecommunication Act appears to refer to the Internet primarily as something that runs on top of telecommunication links. reply wtallis 14 hours agorootparent> the Telecommunication Act appears to refer to the Internet primarily as something that runs on top of telecommunication links. It most certainly does; the statutory definition is that there can be no information service without an underlying telecommunication service. So how could any court rule that ISPs who own the wires coming into my house are providing purely an information service and no part of their business can be regulated as a telecommunication service? reply tptacek 14 hours agorootparentI think you have this backwards, if I'm reading your thrust properly. The whole deal here is: if the Internet backbone is itself a telecommunications service, then maybe (but: probably not) the FCC can make up new rules about Internet service. If the Telecommunications Act of 1996 sees Internet service as something built on top of raw telecom circuits --- which I think it kind of clearly does? --- the FCC has no such power. reply wtallis 14 hours agorootparent> If the Telecommunications Act of 1996 sees Internet service as something built on top of raw telecom circuits --- which I think it kind of clearly does? Does it? We have definitions for \"information service\" that clearly fit running a server, and \"telecommunication service\" defined in a way that clearly fits carrying data over wires, unmodified, and some potential gray area in between that might include looking up information to figure out how to correctly route other information. You've repeatedly emphasized the \"offering a capability\" angle, and so does the court. On the subject of capabilities offered by ISPs, the court says: > they offer a “feature[] permitting” consumers to stream videos stored on Netflix’s servers, [...] the “ability” to purchase gifts from information stored on Amazon’s servers, [...] the “capacity” to view posts stored on Facebook’s servers, [...] and the “power” to conduct a search using Google’s servers But the court pretends that this piece of capability (feature/ability/capacity/power) being offered by ISPs (as opposed to Netflix, Amazon, et. al. who actually run the servers) is not exactly the telecommunications service the FCC seeks to regulate as a telecommunications service. The court appears to believe that merely being able to use the word \"capability\" to describe ISPs (while implicitly lumping together functions performed by ISPs with those performed by Netflix, et. al.) is sufficient to make ISPs not telecommunications services. reply tptacek 14 hours agorootparentAgain, I would come back at you and say that you are talking about the Internet as if it was something markedly different from CompuServe, Prodigy, and AOL. Obviously, it is, but I don't think that distinction was legible to the authors of the 1996 act. Meanwhile: the courts, post-Chevron, are going to look at the text of the act in its 1996 context, without deferring to agencies about what the spirit of that act was. This is a whole thread of people over and over and over again saying that the courts should read the 1996 Telecommunications Act in light of what telecommunications looks like in 2025 (or what they, 2025 practitioners, believe a 1996 practitioner should have thought --- as a former 1996 practitioner my response to that is: LOL). That's exactly what the post-Chevron courts are not going to do. So far as I know, this distinction made it to SCOTUS just once (and: in the 21st century!), and when it did, the court said: the Internet is like Prodigy, CompuServe, and AOL. reply wtallis 14 hours agorootparentYou're using the term \"the Internet\" to refer to a combination of ISPs and endpoint services operated by other companies. That is very analogous to the combination of AOL and the Bell system connecting your modem to AOL's modem. But broadband ISPs today occupy the place of the Bell system, plus some of the lower layers of AOL, and broadband ISPs do not occupy the place of the higher layers of AOL (the search engines, content publishing, etc.). AOL without any means for your modem to reach AOL's modem is roughly similar to the World Wide Web without any ISP to carry your packets to and from somebody else's servers. (And I've long believed we need stronger regulations to make it clear that ISPs are responsible for providing more than just WWW access.) reply tptacek 14 hours agorootparentBroadband ISPs today, you say. reply wtallis 14 hours agorootparentAs opposed to broadband ISPs in 1996? Do you not think a Senator from 1996 would readily lump today's Google Fiber in the same category as an ISDN provider in 1996? Do you somehow think that the overturning of Chevron means that courts do not need to figure out where the role of Google Fiber today fits into the categories defined in 1996? reply tptacek 14 hours agorootparentNo, I do not, and, more importantly, nothing in the definitions section of the Act would dictate that. Yes: I do think that Loper means that courts aren't going to do this kind of freelancing anymore. reply wtallis 14 hours agorootparent> Yes: I do think that Loper means that courts aren't going to do this kind of freelancing anymore. And yet, the court seems to have rendered a decision. They don't seem to have declared that the question is not theirs to answer. reply tptacek 1 hour agorootparentSorry, you're right. I spoke imprecisely. The courts aren't going to allow the regulatory state to do this kind of freelancing anymore. The courts themselves will continue to produce judge-made law, of course. Which is Kagan's complaint in her Loper dissent. reply vel0city 14 hours agorootparentprev> Internet service as something built on top of raw telecom circuits My service agreement with my ISP explicitly says it is a circuit. And also the argument isn't if it's an Internet service or a telecommunications service, it's if it's an information service or telecommunications service. Quit throwing your own common parlance terminology here and use the language in the law and the courts please! reply tptacek 14 hours agorootparentThe text of your service agreement has exactly zero to do with this dispute. reply vel0city 14 hours agorootparentIt's entirely the center of this dispute. What is it that I'm actually paying my ISP for? I'm paying them for a telecommunications circuit. I'm not paying them because they host or manipulate data I care about, I pay them so I can telecommunicate with actual information services. reply vel0city 18 hours agorootparentprevThe ISP doesn't offer me the capability to post this comment. Nowhere in my service agreement with them is hacker News mentioned. reply kevin_thibedeau 15 hours agorootparentprevISPs used to have in house e-mail, Usenet, and FTP services. They realized they didn't need them to keep up the charade of being a telecom provider. reply autoexec 1 hour agorootparentThey also provided shell accounts and webhosting. It had nothing to do with being a telecom provider. They stopped providing those services because third party services grew more popular and ISPs were happy to pocket the money they were spending to provide and support those services (our bills only went up even as ISPs offered less and less) reply vel0city 15 hours agorootparentprevYeah and back in those times where you hopped on your phone line to dial to your ISPs modem to access the services they host I'd completely agree they were information services providers which happened to maybe offer this new fangled \"web\" thing you might have heard of on the news. People signed up for those ISPs because of the information services the ISPs provided. Now people sign up for their ISPs because they offered the better price/throughput/stability math, or often they were the only one reasonably available. Practically nobody signs up because one provider has better email or better newsgroups. reply ab5tract 14 hours agorootparentprevWhat are you even saying? These were not signifiers of “telecom providers”. They were elements of competition amongst ISPs. In what world was Usenet and web hosting part of historical telecom offerings? reply mch82 15 hours agorootparentprevYour point seems spot on to me. We’ve fallen into the trap of differentiating legislation by implementation (technology) instead of job-to-be-done (societal OKRs). Email is just mail is just a way of exchanging letters. Digital packets and analog signals are just ways of sending data over a wire. reply overstay8930 21 hours agorootparentprevWhich is exactly how it’s supposed to work in the first place, individual states are really the only entities that even have the power to enforce any sort of regulation here anyways. Telecom was always handled by states, the feds only stepped in when it was inter-state. This was going to happen regardless, T-Mobile basically threw throttling in their face and they had no choice but to pretend it was fine, and they’ll be doing the same with home internet soon enough (followed by CableCos in non-competitive areas). reply scarface_74 20 hours agorootparentNet neutrality was never about mobile data. Unlike terrestrial internet, you can’t just build more capacity with over the air data transmissions. You have to have more agressive network and QOS management for cellular networks. Besides that, the true argument for net neutrality was not to unfairly help or hurt businesses. T-mobile 0 rated all of the streaming services that participated in the program and were not asking for money for it. Even though they didn’t advertise it, if you dug deep enough, you could find the list including some porn sites. Someone commented on HN a while back that they were able to get their 50 stream non profit 0 rated. reply johnnyanmac 16 hours agorootparentprev>Which is exactly how it’s supposed to work in the first place I don't think anyone can seriously argue that the internet is a \"state resource\", by any reasonable measure. Stuff like this is exactly why federal steps in and established communication standards. because when communication is fractured by the rules of 50 states, we aren't really \"united\" anymore. Nevermind the monopolies with ISPs and that a federal anti-trust would solve 50 problems at once rather than states taking their time to try and clean their backyard. >T-Mobile basically threw throttling in their face and they had no choice but to pretend it was fine and you don't see why this is a problem on a national level that needs proper standards, instead of a state free for all? reply Terr_ 20 hours agorootparentprev> Which is exactly how it’s supposed to work in the first place True, but that was also when riding on a horse was the fastest way over land and a much much greater proportion of stuff was within state-lines. In some alternate universe where the telegraph had already been invented, all those states may have entered into some sort of special telegram compact, and our \"how it's supposed to work in the first place\" would be rather different. Actually, in this universe something similar did happen: The federal government was explicitly authorized to run a nationwide postal service, which stands out in a Constitution that was otherwise mostly about what it couldn't do. reply belorn 6 hours agoparentprevThe domain of FCC, as it is written in the law, to create \"rapid, efficient, nationwide, and world-wide wire and radio communication services with adequate facilities at reasonable charges\". It seems fairly consistent that FCC could enforce net neutrality on powers they already have in their domain, without Chevron. The issue as given in this verdict is that the courts did not find internet service providers to be in the domain of communication services. Instead they are in the domain of information service. Giving the FTC the power to regulate this would mean to accept this definition that internet service providers are information services rather than communication services, making an ISP a digital version of a book seller. That just seems wrong. I would say that the ideal solution is to either have the supreme court redefine internet service as communication, or have the law rewritten for the same purpose. reply Drunk_Engineer 20 hours agoparentprevMobile broadband uses public airwaves, which is entirely FCC jurisdiction. reply bearjaws 19 hours agoparentprevAh the ole \"Why did this government agency create this rule? It should be done by congress!\" Congress has been disfunctional for 30 years. This is the result of that. It's like a weird \"acktually\" form of libertarianism, sure the most correct way to implement it is with congress, but in lieu of that what should the government do? Nothing? reply armada651 18 hours agorootparentGovernment agencies have been picking up the slack for a dysfunctional congress. Now that that power has been taken away the inability of congress to pass consumer protection laws will be on full display. reply xp84 7 hours agorootparentThat's a hopeful thought. I really hope people will start to realize that what they're doing now - sending jackasses to Congress with a mandate to never compromise and to do whatever is necessary to absolutely destroy (insert opposite party) at all costs - is not compatible with a functioning government, and just as importantly, that having a functioning government would be a good thing. reply johnnyanmac 16 hours agorootparentprevAnd probably met with apathy, as always. They won't realize ISPs are giving traffic to the highest bidder. They will just complain about [non-market leader] being slow to load, and move to [market leader] because \"it's faster\". All according to the plan of the real rulers of this plutocracy. reply datadrivenangel 20 hours agoparentprevFTC would make more sense, and now that this is getting devolved to the states I expect we'll see the major telecoms lobbying for regulation so that they don't have the hassle of implementing differing policies by state and the likely painful regulations that they'll get in more progressive states. reply rockskon 7 hours agorootparentWhat exactly is the FTC going to do with Net Neutrality? In what way is what the FCC was trying to accomplish within the ability of the FTC to enact instead? reply johnnyanmac 16 hours agoparentprev>through legislation or potentially through existing powers well yes. The obvious think everyone knows but no one says out loud is that congress has been in a deadlock for 15 years now or so. which is a small part of why newer presidents are much more liberal in making executive orders, and why agencies (especially ones moderating tech) have had to take charge to get anything done. This story here is just another episode of the aftereffect from overturning Chevron, which will simply further stagnate any chances for change. reply jrm4 14 hours agoparentprevYeah, I don't get this at all. Sure, if you follow the caselaw etc. But this requires big picture thinking. The internet is like the telephone is like the radio is like TV. Ergo. THE FCC MAKES THE RULES. Don't overthink it. reply ConspiracyFact 9 hours agorootparentIs this sarcasm? Spectrum is a finite resource. reply shmerl 15 hours agoparentprev> FTC either through legislation or potentially through existing powers they might already have in this domain. Yeah, I always felt that ideas of net neutrality can be generalized by properly applying competition law. But the latter became so toothless, that it's not applied in practice. That's why all these kind of secondary efforts pop up, since FTC is useless a lot of the time. reply danso 17 hours agoprevSurprised this legal turn has so little discussion on HN right now. NN had felt like an evergreen hot debate topic for at least the past decade 7 years ago, 3000+ upvotes: https://news.ycombinator.com/item?id=15924794 reply jmward01 17 hours agoparentI think people are exhausted from court rulings that rarely appear to be in favor of people over companies. At a certain point the belief in an impartial, consistent and fair court system is eroded to the point of breaking. Why look/care/take part if you don't believe in the system anymore? reply supernova87a 8 hours agorootparentAre you saying that you believe our judicial system is fair and impartial only when it rules in your favor? I vaguely recall a similar sentiment: \"the system is been rigged because I didn't win!\" Courts rule on laws. If you don't like the laws, go and seek to change them. Don't cast suspicion on our judicial system, one of the reputable institutions that takes the time and effort to study facts and uphold our rights when we need them the most, because it didn't go your way on a topic of net neutrality. NN is hardly a long settled human right that you can declare it an injustice if the US turns out not to apply it in a certain way, and you're turning a ruling on whether the FCC is the proper mechanism for regulation into \"the system is corrupted\". Consider applying principles of government that don't shoot yourself in the foot if the other side takes power. It's one of the few things that sets us apart from less civilized countries. reply snsr 7 hours agorootparentThe obvious bias of the US Judiciary has resulted in low public confidence - not sure where you've been? https://www.bostonglobe.com/2025/01/03/opinion/scotus-ethics... reply lossolo 5 hours agorootparentprev> \"the system is corrupted\" \"Multivariate analysis indicates that economic elites and organized groups representing business interests have substantial independent impacts on U.S. government policy, while average citizens and mass-based interest groups have little or no independent influence. The results provide substantial support for theories of Economic-Elite Domination and for theories of Biased Pluralism, but not for theories of Majoritarian Electoral Democracy or Majoritarian Pluralism.\"[1] 1. http://www.princeton.edu/~mgilens/Gilens%20homepage%20materi... reply soco 7 hours agorootparentprevI'm not sure how you came to this conclusion, when their comment stated in the very first line: \"court rulings that rarely appear to be in favor of people over companies\". But answering your strawman: when you have a bipartisan system with both parties led by companies I don't really have a clue how one can seek to change companies-favoring status quo/laws, but I'm willing to be educated - please help. reply michaelt 2 hours agorootparentPerhaps supernova87a thinks the court system is doing a great and highly professional job of enforcing a bad set of laws handed to them by an failing legislature? Much like a professional painter and decorator can demonstrate great care, precision and craftsmanship in applying the paint, even if the customer chose an ugly-ass colour. reply throw0101b 6 hours agorootparentprev> But answering your strawman: when you have a bipartisan system with both parties led by companies […] One of the parties in the US has been pushing for net neutrality[1][2] and another has been getting rid of it.[3] If you think the two parties are equivalent (on a variety of topics [4][5][6][7]), I'm not sure where you are getting your information from, but it is not accurate. [1] https://obamawhitehouse.archives.gov/net-neutrality [2] https://obamawhitehouse.archives.gov/blog/2014/11/10/preside... [3] https://www.vox.com/2017/12/14/16771910/trump-fcc-ajit-pai-n... [4] https://aflcio.org/press/releases/donald-trumps-catastrophic... [5] https://cwa-union.org/trumps-anti-worker-record [6] https://www.theguardian.com/environment/2024/dec/02/trump-al... [7] https://time.com/7201582/donald-trump-vaccines-fact-check-20... reply soco 6 hours agorootparentThank you for the links! Then I don't understand something: if this one party supported some topic, any topic, how come said topic is not made into law but only lives in the internal regulations of some organization? Courts applying existing law will then by necessity strike those down, by definition: if it's not in the law it will not be recognized by the courts. Simple. So the whole complaining \"court strikes down something\" is pointless, because the courts function as designed. I'd say the problem is the parties which don't bother making supporting laws - yes even the parties claiming to support said topic. How can a party holding majorities (in a two party system one party will always have majority) even temporary, not actually make laws? Isn't this the very reason parties exist? So I'm still confused. reply throw0101b 6 hours agorootparent> So the whole complaining \"court strikes down something\" is pointless, because the courts function as designed. The courts are judged by humans who have biases and motivated reasoning: * https://www.nycla.org/resource/blog/hon-aileen-cannon-wrote-... * http://archive.is/https://www.theatlantic.com/ideas/archive/... * https://ccf.georgetown.edu/2024/10/11/trump-appointed-federa... People (judges) sometimes go into court with an pre-conceived notion of what is the 'correct' way to interpret things, and they think it is 'objective', but in fact has been shaped over time by (active) cultural forces: * http://magazine.law.nyu.edu/index.html%3Fp=7606.html reply soco 5 hours agorootparentMy point was: if there's no law about something I cannot have any expectations from courts, so why there's no law? reply fwipsy 3 hours agorootparentThe Chevron doctrine, which was recently overturned by the US supreme court, stated that US courts should defer to the regulations of US agencies such as the FCC. As I understand, under Chevron, functionally, the FCC's regulations on telecommunications were law to the courts and the corporations. It seems to me that it's unreasonable to expect Congress itself to have informed, correct opinions on technically complex topics. There are a limited number of Congressmembers, most of whom are not technically skilled. Delegating regulation to domain experts who remain accountable to Congress seems like a reasonable solution. If Congress disagreed with the FCC's handling of the situation, couldn't they have made a law to overturn the FCC's decision or limit its authority? Overturning Chevron is a victory for deregulation in general. As I understand, Congress could pass laws to explicitly reinstate the authority that the FCC previously held. However, pro-deregultion factions (I assume mostly Republicans, but I don't really know) now have a chance to block that, and even the supporters of NN have other fish to fry. Essentially overturning Chevron curtailed the authority of regulatory agencies-- authority which Congress expected them to have, and could have restricted at any time--without going through Congress. Yes, it's the job of the court system to interpret laws. But when they change interpretations which other laws depend on, that's basically changing the laws themselves, isn't it? reply soco 2 hours agorootparentOkay thank you this answers fully my question. I guess one could make that \"Chevron doctrine\" into law, a one-time law, then we'd have the issue solved right? So not making laws for every single topic, just enshrine that FCC and others make \"laws\", then the courts would have to respect that. Of course unless they contradict other laws, thus the courts will still not run out of cases, but at least you'd have some more predictability. But if the whole system relies more on precedent cases instead of explicit laws, you can never reach that predictability (see Roe vs Wade)... reply triceratops 4 hours agorootparentprevIt's really hard to pass a non-budgetary law without a supermajority. reply claytongulick 3 hours agorootparentSo the solution is to have the judiciary become defacto legislators? reply triceratops 3 hours agorootparentThey're doing that right now. That's what defacto means. reply soco 1 hour agorootparentSo the three powers system is broken? Almost \"by design\" one could say, although it seemed to have worked for a while, maybe when the level of sophistication was lower (but whose level?). Or maybe there was a time when the actors tried to use the system, not actively seeking tricks to abuse it? reply lupusreal 1 hour agorootparent> So the three powers system is broken? Obviously it's broken, because it's not doing what I wish it would. reply krapp 1 hour agorootparentprevObviously the problem here is that the judicial branch still works too well, and need to be broken like the rest of the government. Then and only then, when every branch of government at every level has so much friction that the offices themselves burn to the ground, will we be able to enjoy the freedom that God and the Founding Fathers intended. If the government that governs least governs best, then the government that governs best doesn't govern at all. reply soco 58 minutes agorootparentWhich would make the founding fathers founding... nothing? reply AdrianB1 6 hours agorootparentprev>>Courts rule on laws. Not always. They invent laws. They defy laws. They defy common sense. For example \"qualified immunity\" is invented out of thin air, while \"the spirit of Aloha says US Constitution does not apply to Hawai\" is defiance. \"boneless chicken does not mean it has no bones, but it is a style of cooking\" is totally nuts. All the examples are Supreme Courts cases. This is not about winning or losing, it is about judicial system not fair and not impartial, sometimes borderline crazy. reply newsclues 7 hours agorootparentprevI think that feeling is typical for many, democracy is fair when my team wins or a sports referee is fair when they favour your team. Feeds into the victim narrative when things go the other way. reply AdrianB1 6 hours agorootparentDemocracy is not fair, it is \"mob rules\" by definition. reply contagiousflow 3 hours agorootparentDemocracy is not always populist. People have voted against direct self interest for the benefit for others for quite some time. reply newsclues 2 hours agorootparentprevWould you like to roll back civil rights because they are not fair, because it’s discriminatory and therefore unfair? reply claytongulick 3 hours agorootparentprevGood thing the US is a constitutional republic, then. Almost like the founders knew that \"mob rules\" was a bad idea... reply InitialLastName 3 hours agorootparentA lot of the framers (if you're going to be a pedant, use the right words) knew that \"mob rules\" had troublesome implications with respect to the humans they claimed ownership of. reply justwool 5 hours agorootparentprevNo they don’t say that. Your assumption says more about you. reply friend_Fernando 17 hours agoparentprevI suspect it's because of the feeling that the world has gone much further off the deep end since then. In 2017 it still felt salvageable. If the world is screwed, NN isn't as big a deal. reply whoopdedo 11 hours agoparentprevBack then, Netflix was using the threat of net neutrality regulation as a bargaining chip in their fight with Comcast/Cox. Once they reached an agreement on transit fees the astroturfing stopped. Since then it's only arisen as an wedge issue for politicians to fundraise over. We won't hear about it again until the midterm elections. reply llm_trw 7 hours agoparentprevBig tech banned the people who cared. Now no one cares what happens to big tech. If it's Google or Verizon deciding who wins or loses the digital loterry doesn't make much difference to me after the last 10 years. reply lelandfe 17 hours agoparentprevI just don’t think the internet has mustered awareness yet. This decision effectively killed net neutrality and dooms everything that rests on Brand X in one swoop (eg California’s own net neutrality laws). reply rsingel 16 hours agorootparentCalifornia's net neutrality law is in great shape. It didn't rely on Brand X. New York's broadband affordability law is in good shape too. reply lelandfe 16 hours agorootparentAh, you're right. Hooray. CA's net neutrality law was challenged in court and found to be lawful due to Mozilla Corp. v. FCC, and I knew that case hinged upon Brand X. At the time, Ajit Pai was trying to prevent states from making their own net neutrality laws. Them striking down the state preemption was separate from the Brand X use, it turns out. > In neither case [Ray v. Atlantic Richfield Co and Arkansas Electric] was the source or existence of statutory authority for the agency to preempt state regulation at issue. Nor do those cases speak to a statutory scheme in which Congress expressly marked out a regulatory role for States that the federal agency has attempted to supplant. If Congress wanted Title I to vest the Commission with some form of Dormant-Commerce-Clause-like power to negate States’ statutory (and sovereign) authority just by washing its hands of its own regulatory authority, Congress could have said so p140 https://cases.justia.com/federal/appellate-courts/cadc/18-10... reply Xelynega 17 hours agorootparentprevDoes this decision(or the decision it was based off of) not also basically say \"the fcc has no power over broadband companies\" or am I misinterpreting the \"information service\" vs \"telecommunications service\" comment? reply wtallis 17 hours agorootparentMy recollection is that the FCC during the Obama years repeatedly tried to impose Net Neutrality regulations on ISPs without reclassifying them as telecommunications services, and only after exhausting that route did they try regulating them under Title II. So if this ruling stands, then there's probably enough precedent to keep the FCC from doing anything meaningful against ISPs until Congress can pass major legislation about this. reply BikiniPrince 3 hours agorootparentprevWhat it means is abusing chevron deference to make law isn’t going to work anymore. Such sweeping changes need to be passed through Congress. reply paxys 14 hours agoparentprevThis forum, and the country as a whole, leans significantly further right today than it did 7 years ago. You'll now find way fewer people willing to openly disagree with conservative-minded rulings from the government. reply friend_Fernando 14 hours agorootparentDo not confuse silence with absence. Also, that weird-looking fella in the corner with the bits and vectors sticking out is not really representative. reply dolni 1 hour agorootparentprevThis site doesn't lean further right. It leans exactly the same way it always did. The left moved further left. reply rossriley 11 hours agoparentprevOut of interest, from a US point of view since I'm not there, is this going to end up as a relatively damp squib in the very near future now we have access to 5G and all the competition there. Last year I was able to abandon completely my broadband provider and now have two sims, one for a home router and one for a mobile router with pretty much unlimited data. There is so much competition in the 5G mobile space that the ability of these older closed market providers over cable/fiber is surely going to be a thing of the past very soon and thus the need to enforce legal neutrality will fade? reply bregma 7 hours agorootparentNet neutrality is more about backhaul and peering. None of that goes away with more competition at the subscriber level. In fact, it become far more significant. reply michaelt 2 hours agorootparentAs I understand it, historically in the US many people have only had access to one broadband provider. So an American couldn't move to a different broadband provider if their current provider made Netflix slow. On the other hand, if 5G technology had such great performance and coverage that every American had a choice of 10-15 different ISPs, when one ISP slowed down Netflix they could simply change providers. That might make it less problematic for some ISPs to make Netflix slow. reply guenthert 7 hours agorootparentprevI wouldn't count on true competition to last. reply trymas 9 hours agoparentprevSimilarly how it is surprising to see hundreds of too critical comments addressed at FOSS application with solo developer with 10+ years of excellent track record on another post in the same hacker news page: https://news.ycombinator.com/item?id=42583082 reply whoitwas 8 hours agoparentprevWe have so much more to worry about in the US now. An African billionaire just paid over $270 million to get the guy who organized white supremacist militias to overthrow the government in 2021 elected as president. These criminal robber barons want to eliminate agencies like the FCC and any regulations like NN as well as stack the courts with people who are also robber barons, etc, etc. reply mmooss 17 hours agoparentprevNot every NN development gets a lot of traffic. Maybe it's the day after the holidays. Maybe everyone expects the Trump administration would recind it anyway, and the conservative Supreme Court justices would undermine it. Maybe people see it as another battle in a long war. reply SV_BubbleTime 13 hours agoparentprevI wonder how many people remember the propaganda of Google and Reddit logos loading low resolution versions slowly around peak dooming? reply egorfine 6 hours agoparentprevI was like \"meh\" out of a simple realization that there will never be net neutrality in the US just like there are little chances of universal healthcare, maternal support and such. reply rc_kas 14 hours agoparentprev10 years of very effective republican propaganda has convinced people that whatever republicans do is good reply coldpie 3 hours agoparentprev2024 was the successful culmination of 50 years of efforts to turn the US from a democracy into a single-party state, similar to Russia & China. That's a huge shift both for people who live here, and the entire world. Net neutrality is cool and all, but I've got bigger things to worry about. reply mossTechnician 18 hours agoprevI'm reminded of when it was legal for cellphone companies to charge different fees to access different websites. Naturally, the bigger ones got the best monthly rates. https://www.techdirt.com/2014/07/31/pay-different-prices-to-... reply DrScientist 7 hours agoprevIsn't an alternative is to say, if you aren't treating all content the same - then you aren't a common carrier, you are a publisher and therefore responsible for the content? ie you choose - if you want the \"common carrier\" protection you need to behave like one..... reply treetalker 22 hours agoprevSeveral related cases; the opinion in one of them is here: https://www.opn.ca6.uscourts.gov/opinions.pdf/25a0002p-06.pd... Lousy reporting: although it quotes the opinion and links to other articles on the same website, the article doesn't bother to cite the opinion or even mention the name of the case, let alone link to the source. reply kemitchell 14 hours agoparentI would love to see HN adopt a policy of replacing links to news pieces about court opinions that don't link them with links to better news pieces that do. reply Dalewyn 13 hours agorootparentWhy even bother with \"news\" pieces? They're all just sensationalist trash. Just link to the source directly, most of us here know how to read and (probably) have the mental acuity to form our own conclusions. reply wtallis 13 hours agorootparentHelpful analysis that provides additional context can exist, eg. SCOTUSBlog that provides useful summaries while reliably linking to the rulings themselves. It's not all sensationalist trash. reply emmelaich 18 hours agoparentprevreason.com has good articles on the topic. https://reason.com/tag/net-neutrality/ reply NotYourLawyer 4 hours agoparentprevPretty typical of media reporting on court cases. Just awful. reply fooey 22 hours agoparentprevthe NYT article links to the same PDF you provided https://www.nytimes.com/2025/01/02/technology/net-neutrality... reply s1artibartfast 22 hours agoparentprevHuge pet peve of mine. Reporters incentive are to keep you on site, not inform. It is particularly egregious when reporting on governmental public safety announcement and laws. Reporting buries the web results, but fails to cite it. I'm generally a libertarian, but would be in favor of required linking of government PSAs and laws reply cashsterling 1 hour agoprevThe real solution to this issue is municipal broadband. If there is a municipal option for high speed internet, people who don't want deal with companies' monitoring and throttling shenanigans have a viable option from a community-focused provider. This provides price and service competition for the Tel-Co cartels who collude to fix prices and pull all kinds of other nonsense (like taking billions of dollars in federal funds and then deliver next to nothing... then lobby their way out of accountability). T reply troyvit 20 hours agoprevEvery day I thank the old gods (and Longmont, CO) for my municipal gigabit.[1] [1] https://mynextlight.com/terms-conditions/ reply tptacek 20 hours agoparentNote that your municipal fiber network partners with Qwilt to preferentially accelerate commercial streaming platforms like Netflix. reply ftth_finland 15 hours agorootparentWhat’s the big deal? It’s just a content cache. You can get dedicated cache appliances directly from Netflix for free if you have a large enough customer base. reply tptacek 14 hours agorootparentI didn't say it was a big deal, but then, I think \"net neutrality\" is pretty silly. reply ftth_finland 9 hours agorootparentCaches do not break net neutrality. Nothing wrong with the concept of net neutrality. Implementations may be lacking, but I do not recall any major issues with the EU regulations. Perhaps all the perceived silliness is a result of the US legislature? reply tptacek 1 hour agorootparentPreferential caching for streaming services that play ball with your ISP don't break net neutrality? reply troyvit 19 hours agorootparentprevWhoah I did not know that and it directly conflicts with their TOS. Do you have a link to that? reply tptacek 19 hours agorootparentIt's in the comprehensive financial statement for Longmont, which I pulled up looking to see the economics of your muni fiber system (I'm on the local commission in my muni that considers stuff like this, and the numbers didn't make sense for us --- but you're in an exurb, so the math is different.) reply cabinguy 17 hours agoprevMost interesting part of this post (to me) is to see how far the perceived importance of net neutrality has fallen on HN. 96 points in 5 hours as of this comment. Old posts: https://hn.algolia.com/?q=net+neutrality reply autoexec 1 hour agoparentNet neutrality is still important but it no longer matters how we feel about it because our government and the courts have been bribed and captured by industry and they're intent on screwing us. This is just one of what will be a very long list of important regulations and oversight we're going to see stripped away. Might as well get used to it. reply strawhatguy 15 hours agoparentprevnet neutrality laws giving the FCC broad powers, just isn't that important though. Late nineties I remember one hot topic in society was prayer in school, and nowadays I hardly hear about that anymore, 9/11 had something to do with that though. Seriously, government gets into these fads, The Next Big Thing, and, since usually a whole new group of young, idealistic, inexperienced humans comes of age, it seems like a hot button item. It rarely is, certainly not enough for the government to \"do something\" about it. We as individuals can fix the problems ourselves, if allowed to. The cycle repeats. reply potato3732842 7 hours agorootparentI agree with all that but don't forget the astroturfing cycle. Net neutrality was a big fight between the ISPs and big tech companies years ago. Now that they've mostly reached a stable armistice and it's clear from variances in state to state law and policy that have arisen in the meantime that this is not a make or break issue for either side there's no money being poured onto the fire to whip people into a frenzy. reply johnnyanmac 16 hours agoparentprevmight just be bad timing. It is the day after the US holiday season. May be by design too. 2 days ago, we got more formal reporting on how Russia may have been involved in manipulating elections. Again. But it was on New Years' Eve. (not that I expect that news to last long on HN). If I wasn't so tired of all this falling on deaf ears, I'd chuckle at the irony. reply xp84 7 hours agoprev> we hold that Broadband Internet Service Providers offer only an ‘information service’ under 47 U.S.C. § 153(24), and therefore, the FCC lacks the statutory authority to impose its desired net-neutrality policies through the ‘telecommunications service'’ provision of the Communications Act, id. § 153(51). Can we just appreciate how silly and sad it is how many billions of dollars hinge on the distinction between two terms which 99% of Americans would, if they weren't aware it was a political issue, are basically synonyms? Hi Mr. Brown, CEO of Comcast, what does your company do? Is it a telecommunications company? \"No!! We only provide information services!\" reply rc_kas 14 hours agoprevDon't worry guys, democrats will write a strongly worded letter in objection to this. reply Spivak 13 hours agoparentI think people largely forget that this happens to Republican issues as well and they have just as little recourse. It's just you probably consider them victories and either ignore or bask the Republicans loudly complaining and then doing nothing. I think the reason that The Democratic Party feels ineffectual is because they've been on the defensive for so long they've lost their aspirational platform in favor of \"stopping Republicans from doing bad things.\" They're the opposition party now. reply datmar 1 hour agoprevNot sure if this violates any rules, but I just pulled the actual ruling and asked Gemini to summarize it. See below: \"\"\" Summary of the Decision The Sixth Circuit Court of Appeals overturned the Federal Communications Commission's (FCC) 2024 \"Safeguarding and Securing the Open Internet\" Order. This order sought to reinstate \"net neutrality\" rules, which would regulate broadband internet service providers (ISPs) as common carriers under Title II of the Communications Act. Here are the key points of the court's reasoning: Overruling Chevron Deference: The court explicitly rejected the standard of deference to the FCC's interpretation of the Communications Act, as previously required by the Chevron doctrine. This deference was overruled by the Supreme Court's decision in Loper Bright Enterprises v. Raimondo. The court decided it must determine \"the best reading of the statute\" itself. Information Service vs. Telecommunications Service: The core of the ruling hinges on the distinction between \"information services\" and \"telecommunications services,\" as defined in the Communications Act. The court found that broadband ISPs offer an \"information service\" because they provide consumers with the capability to access, retrieve, and utilize information via the internet (even though they don't create the content themselves). The court emphasized the importance of the \"offering of a capability\" language in the definition of an information service. The court rejected the FCC's argument that ISPs are merely conduits for data transmission (a \"telecommunications service\"). Mobile Broadband: The court also ruled that mobile broadband services (internet access via cellular networks) cannot be classified as a \"commercial mobile service\" under Title III of the Communications Act, and thus cannot be regulated in the same manner as traditional telephone services. The court determined that mobile broadband is not directly \"interconnected with the public switched network\" (i.e., the traditional telephone network) since it uses IP addresses, not the traditional 10-digit telephone system. Therefore it must be a \"private mobile service,\" which is not subject to common carrier regulations. Historical Context: The court highlighted the historical understanding and the FCC's consistent interpretations before the 2015 change, noting that Internet access was generally considered an \"information service\" and that Congress intended to keep the internet free from significant regulation. Result: The court vacated the FCC's order, effectively ending its attempt to impose net neutrality regulations under Title II on internet providers. Potential Consequences This ruling has significant potential consequences, including: No Net Neutrality Regulation (for now): The immediate impact is the absence of federal net neutrality rules in the U.S. Broadband ISPs and mobile broadband providers will not be legally bound by the principles of net neutrality. This means: ISPs could potentially throttle or block traffic to certain websites or services. ISPs could potentially offer paid prioritization of some content over others. ISPs may also offer differing internet speeds for different plans, without the need to justify this to the FCC Impact on Consumers: Consumers could see: Higher costs if ISPs charge for preferred access to content providers. Slower speeds for some services if ISPs decide to prioritize others. Reduced choice if ISPs favor their own services over competitors. Impact on Edge Providers: Websites, platforms, and content creators could be affected by: Reduced reach if their content is throttled or blocked. Increased costs if they have to pay ISPs for priority access. Less innovation if startups or new platforms are unable to compete with established services. Uncertainty and Litigation: The future of net neutrality is uncertain, and further litigation is highly likely. The FCC could appeal to the Supreme Court, which could further influence the debate. Other attempts to regulate the Internet may arise, potentially from states, but could lead to more legal challenges. Political Ramifications: This decision may lead to legislative efforts to create new laws codifying net neutrality in the US, or at least to further clarification of the existing Telecommunications Act of 1996. Increased consolidation: Without Net Neutrality, it is likely that larger telecommunications providers, who can utilize their economies of scale, will be able to further dominate the market, reducing competition. In Summary The Sixth Circuit's decision is a major blow to net neutrality proponents. It removes the current federal net neutrality regulations, and in doing so, it potentially gives more control over internet access to ISPs and mobile carriers. The long-term effects will depend on future legal challenges, legislative action, and market behavior. However, the decision is consistent with current Supreme Court precedent concerning judicial deference to agency interpretation of law. \"\"\" reply wrycoder 51 minutes agoparentThis. The ISPs can go back to prioritizing and pricing traffic based on their own commercial preferences. Thankfully, I have Starlink as my ISP. reply unethical_ban 21 hours agoprevShort version: They killed it because of \"Loper Bright\" which killed the Chevron Deference. In other words: Until recently, US executive agencies were given a lot of leeway in creating regulations that align with their mandate from Congress. This leeway was struck down by the Roberts Supreme Court, insisting that courts can reign in executive agencies and keep them from operating with a broad mandate. This court doesn't say Net Neutrality is bad or unconstitutional. It says that, under the new precedent set by the Roberts Supreme Court, the FCC does not have the authority it claims to have in regulating ISPs in the same way as a phone company. If Congress were to pass a law that said the FCC had the authority, or that net neutrality should be instated explicitly, it would happen. reply rqtwteye 21 hours agoparentHas there been another Supreme Court that has overturned as many precedents than the Roberts court? It looks to me that this court will have a huge impact far into the future. They are less activist when it comes to ethics rules though. The Clarence Thomas situation is ridiculous. reply int_19h 20 hours agorootparenthttps://constitution.congress.gov/resources/decisions-overru... Looks like there's been plenty more of that in the civil rights era - look at all the cases in 1960s! reply horacemorace 19 hours agorootparentOne to move society toward tolerance and one to move society toward 1984 reply johnnyanmac 15 hours agorootparentindeed. how many of these overturns in the last 10 years helps the current voting citizens and not corporations? reply rufus_foreman 19 hours agorootparentprev>> Has there been another Supreme Court that has overturned as many precedents than the Roberts court? Yes, every modern Supreme Court (1950's and on) has overturned not just as many, but more, precedents than the Roberts court. \"The famously liberal court led by Chief Justice Earl Warren from 1953 to 1969 overruled an average of 3.1 precedents per term. The number ticked up slightly as the court moved to the right under Chief Justice Warren E. Burger, who led the court from 1969 to 1986, to 3.4 precedents per term. It dropped under Chief Justice William H. Rehnquist, who led the court from 1986 to 2005, to 2.4 precedents per term. Through the end of the term that ended in June, the Roberts court has overruled precedents at the lowest rate, at 1.6 per term. But it has picked up the pace since the arrival in 2017 of the first of three justices appointed by President Donald J. Trump. Since then, the rate has been 2.2 precedents per term, still the lowest of the four courts.\" -- https://www.nytimes.com/2024/01/29/us/supreme-court-preceden... What news sources have you been consuming that have been telling you otherwise? reply unethical_ban 18 hours agorootparentPerhaps the fact that the courts referenced in that article are 40-60 years in the past, so anyone under age 50 would have the personal experience of seeing the Roberts court as exceptional. edit 45 minutes after posting: Also, more opinionated, the Roberts court's reasons for their decisions are often pretty wild and many of their decisions have been a detriment to the country. My pet cases to loathe are Citizens United and Rucho v. Common Cause. reply zeroonetwothree 14 hours agorootparentprevThe Roberts court is actually the least \"activist\" in modern history. His whole philosophy is all about maintaining precedent (obviously sometimes he can't get his way--which seems to be what happened in Dobbs, for instance). reply kllrnohj 6 hours agorootparentBullshit. Overturning decades old decisions is activism, plain and simple. He's taking a steaming dump all over the very idea of precedent and star decisis, it sure as hell isn't his philosophy to maintain it. If it was then Roe v. Wade would never have been overturned. reply johnnyanmac 15 hours agoparentprev>If Congress were to pass a law wonderful joke, thanks for the new years chuckle. But yes, that's all part of the plan. rely on the dysfunction of congress and the end of Chevron to impart a deadlock that lets private corporations run free. All while dismantling whatever other regulations they can get away with. reply meta_x_ai 18 hours agoparentprevThank god for John Roberts for preventing governmental overreach by unelected bureaucrats which has become increasingly ideological reply xp84 7 hours agorootparentThe idealist in me also hates the idea of unelected \"government experts\" having a wide berth to do whatever they think is best since I know that 50% of the time they'll be appointed by / taking orders from [Insert part(ies) I don't like] and thus they'll be against my interests. But the pragmatist in me still winces at all the stupidity that happens in the real world because Congress hasn't passed many useful laws in 25 years. Most ideas are put into place by executive fiat because we only have two functioning branches of government now. (Yes, I agree that it's still better than just having one!) reply johnnyanmac 15 hours agorootparentprevahh yes, the overreach like making sure Netflix can't make competition worse by bribing ISP's. reply shmerl 15 hours agoprevNot again. How long is this going to be happening back and forth. Opponents of net neutrality are monopoly freaks. Net neutrality should be enforced on competition law grounds. reply Dalewyn 13 hours agoparent>How long is this going to be happening back and forth. It will keep happening until the legislature either writes it into law or refuses. The executive should not be legislating in the first place outside of emergencies requiring immediate action. reply ttyprintk 14 hours agoparentprevOdds are low that Biden can affect the Supreme Court. I think the next back-and-forth is 2027. reply kristopolous 21 hours agoprevI like how they quoted Brendan Carr - he's one of those people who think anything done in the public interest, to protect people's rights, increase transparency or to hold power brokers accountable is, without exaggeration, evil tyrannical overreach. These kinds of people talk about personal responsibility because they don't believe in social or civic responsibility. I mean don't get me wrong, he's ok with the government paying for things to gift them away to corporations, or bailing them out or banning things like municipal broadband and consumer disclosure or legislation that enables dark money to move around - that's apparently good government doing good things. It's like he reads Philip K Dick for inspiration of how to structure a society. reply ch33zer 13 hours agoparentReading this guys twitter feed makes my blood boil. Look at what he posted about this: https://docs.fcc.gov/public/attachments/DOC-408580A1.txt His argument is just 'this is a new way of applying these regulations, and that bad!'. Nothing about the cost impact that this will have for consumers, nothing about merits of the case, just political BS. He's our incoming FCC head who previously was a lawyer for AT&T, Verizon, and US Telecom [1]. This is going to be a very bad 4 years to be an internet user except for probably Twitter users who will get preferential treatment. [1] https://web.archive.org/web/20231022070004/https://www.latim... reply whoitwas 8 hours agorootparentLike Ajit? GOP always staffs agencies like regulatory capture is the end game. reply malfist 21 hours agoparentprevHe's not nearly that principled. Private capital acting in a way he doesn't agree with politically is wrong according to him. See his complaints about Big tech \"censorship\" of misinformation and Harris's appearance on SNL reply kristopolous 21 hours agorootparentHis idea of censorship is when social media takes steps in the public interest and their civic responsibility of providing a social good seriously. Meta is trying to avoid another Myanmar catastrophe and he wants to use the power of the state to ensure no responsible actions for the public good will be taken. It's ideologically consistent. Anything acting for the betterment of society is somehow communism reply jauntywundrkind 21 hours agoprevIt's hard to see such a clear obvious case & come out so preposterously incredibly fantastically wrong. > The question is whether, in so doing, they are merely a conduit for data transmission (a so-called \"dumb pipe\") and thus offer consumers a telecommunications service (as the Safeguarding Order concludes); or whether, instead, Broadband Internet Service Providers offer consumers the capability to acquire, store, and utilize data-and thus offer consumers an information service. In our view, the latter is the best reading of the Act. This doesn't match probably 9x 9's of internet traffic! This is almost never the case! What do we do when government is so blankety inept? How do we reconcile a court that's allowed decide things with no respect to obvious facts? Why are there so few ways of dealing with these people in any serious capacity, getting them out of here? reply ch33zer 18 hours agoparentThe best we can do is vote/encourage Congress to pass straightforward laws to address these cases, making the rulings defunct. As another commenter mentioned Congress has been incapable of anything for ~30 years so... Nothing? reply DonnyV 5 hours agoprevThe internet is a common service everyone needs. There should be a separate department that oversees all internet services in the US, ran by an elected official. reply tantalor 21 hours agoprevMaybe I'm remembering wrong, but when Chevron deference was overturned, didn't the court explicitly state that it only applies to future orders? i.e., prior orders relying on Chevron would be grandfathered. But here the rule was created April 2024, prior to Chevron decision in June 2024. reply granzymes 21 hours agoparentThe Supreme Court said in Loper Bright that previous cases decided under Chevron are still good law. But this lawsuit challenged a brand new FCC order that had not been previously litigated. reply tantalor 21 hours agorootparentOkay, but that's so weird! The court is saying, if we had litigated this (and most likely upheld it, because Chevron) then it could stand now, even after Loper Bright. But if we never looked at it before, then we get to use the new rule. That doesn't make a lot of sense. Like, the counterfactual is they litigated and upheld the order, so it's safe. But because it was never challenged before, it can be challenged now. Only orders that were challenged are safe. It would make more sense to do it the way I thought: everything, even if it wasn't challenged, that would have relied on Chevron still can rely on Chevron. That gives \"the benefit of the doubt\" to the agency that passed the regulation in the Chevron era. In the new scheme, the regulators have to comb through all their orders going back decades, figure out which ones were never litigated, and figure out how to defend it without Chevron. reply granzymes 21 hours agorootparentYou're coming at this from the perspective of \"it's weird to change the rules after the fact\" which is reasonable but not how stare decisis works. The principle is that once something is decided once, it should be decided the same for all other parties litigating the issue unless and until the precedent is overturned. If something hasn't yet been litigated, it is still an open question. It would be weird if, for example, the Supreme Court said that gay marriage is constitutionally protected but all the existing laws banning it could stand because they were enacted earlier. reply gpm 20 hours agorootparentIt would be equally weird if the Supreme Court said that gay marriage is constitutionally protected, but states that have previously successfully litigated challenges against their laws can keep forbidding it.. reply sidewndr46 20 hours agorootparentprevOn the contrary, I'd expect the Supreme Court to say exactly something like that when it suits them. The Supreme Court throughout history has invented as many hoops as they needed and then jumped through them in order to appeal to political winds and public opinion. reply timewizard 20 hours agorootparentprev> figure out which ones were never litigated, and figure out how to defend it without Chevron. I'm fairly certain the regulated entities will take this upon themselves and will create the lawsuits that provide the opportunity for the administrative agency to defend their position. If they're that concerned, then they should get with Congress, and get a new law passed. This is precisely what Chevron was meant to create. Administrative agencies simply cannot unilaterally create and enforce law anymore. reply tantalor 20 hours agorootparentOh, okay so the solution is as easy as \"congress has to do a thing\". Meaning it will never happen. reply tptacek 20 hours agorootparentThen, maybe it shouldn't? reply bluGill 20 hours agorootparentprevEverything can be litigated again. However eventually the courts have to slow that down. The courts decision is saying that Congress may not have done their job because the agency ruling was what they would have done anyway and so the courts don't want to overturn everything right away. I'm sure most of the previous decisions will be litigated (if congress doesn't change the law first), but they don't want thousands of cases to come before congress has a chance to make law what they should have done. If you challenge the rules now you have to convince the courts that congress wouldn't have passed that rule as law, which is harder (but not impossible). OTOH, I think/hope the courts are sending a message to congress that they need to work out more details in the laws. reply bingjames 20 hours agoprev>“[U]nlike past challenges that the D.C. Circuit considered under Chevron, we no longer afford deference to the FCC's reading of the statute. Loper Bright Enters. v. Raimondo, 144 S. Ct. 2244, 2266 (2024) (overruling Chevron U.S.A. Inc. v. Nat. Res. Def. Council, Inc., 467 U.S. 837 (1984) ) … Using ‘the traditional tools of statutory construction,’ id., we hold that Broadband Internet Service Providers offer only an ‘information service’ under 47 U.S.C. § 153(24), and therefore, the FCC lacks the statutory authority to impose its desired net-neutrality policies through the ‘telecommunications service'’ provision of the Communications Act, id. § 153(51). >47 U.S.C. § 153(24) information service The term “information service” means the offering of a capability for generating, acquiring, storing, transforming, processing, retrieving, utilizing, or making available information via telecommunications, and includes electronic publishing, but does not include any use of any such capability for the management, control, or operation of a telecommunications system or the management of a telecommunications service. >telecommunications 47 USC § 153(50) The term “telecommunications” means the transmission, between or among points specified by the user, of information of the user’s choosing, without change in the form or content of the information as sent and received. What in the absolute hell are these definitions. These sound like they were made by the world's first LLM. reply axus 19 hours agoparent\"Telecommunications service\" sounds like phone or TCP/IP to me. You make a connection, \"speak\" the data, and ideally it's supposed to be received in order and without distortion on the other side. \"Information service\" sounds like websites and apps. You'd think a judge would be capable of reading a law as-written. Internet Service Providers aren't running webpages, they don't even host email or newsgroups anymore. reply cranberryturkey 7 hours agoprevwhy are we always fighting for freedom in the \"land of the free\"?? reply empath75 21 hours agoprevIt's hard to get really worked up about courts falling on one side or the other of an argument between multibillion dollar corporations about how to allocate bandwidth costs. reply RiverCrochet 21 hours agoparentThere's a lot of concern about the lack of net neutrality being unfair in the sense it gives providers the ability to make traffic to sites slower, but does not having net neutrality also mean they can simply refuse to send/receive to specific domains or IP addresses? reply wmf 21 hours agorootparentTechnically yes, but this has been done very few times in history. reply RiverCrochet 20 hours agorootparentWell, it's definitely a new historical era in which old precedents are being thrown out and new ones made. I'm sure anti-piracy organizations will use this to their advantage soon to make a deal with ISPs to block suspected piracy sites. Also I do wonder if we'll see U.S. ISPs refuse to allow access to certain sites based on religious freedom grounds soon. reply tptacek 20 hours agorootparentI don't think any US conception of \"net neutrality\" was going to protect the suspected piracy sites. reply johnnyanmac 15 hours agorootparentThe issue comes down to what we consider \"piracy\". - P2P networking by itself is not piracy, but the casual connection can put a lot of honest sites in danger, from malicious actors. - If the Internet Archive survives its current ideal, this is an easier way to kill that off. - Any storage company that can't pay enough money can be thrown off the map. You can share files, which may or may not be copyright after all. rulings like these start to make the cracks. reply tptacek 14 hours agorootparentThe Internet Archive just got its ass handed to it in a court case about these issues so I feel pretty comfortable with the conclusion I drew upthread. reply johnnyanmac 10 hours agorootparentWell the publisher won in public opinion if that's your interpretation. Now anything trying to be a digital archive is privy to being taken down. A very dangerous precedent. I fear for the Wayback machine more or less letting corporations rewrite history reply LgWoodenBadger 16 hours agorootparentprevI agree. Pro-abortion sites, pro-LGBTQ+ sites, any site that goes against the right will be pressured (whether by the government or by PAC-funded lawsuits) to be dropped or blocked. reply johnnyanmac 15 hours agorootparentprevthat we know of. I'm a bit tired of living in history in the making. Please can we not tempt fate here too? reply treetalker 21 hours agoparentprevA grander theme is seeing the effects of the Loper Bright decision and the end of Chevron (and certain other, but not all, forms of) deference to administrative agencies. reply tptacek 20 hours agorootparentAny thoughts about why I should, on the advent of a second Trump administration, be dismayed about a lack of deference to executive agencies? reply timmytokyo 20 hours agorootparentThe inability of executive agencies to create regulations will extend beyond one presidential administration. Given that conservative administrations tend to favor cutting regulations anyway, this jurisprudence will likely impact liberal administrations more than conservative ones. reply thinkyfish 18 hours agorootparent\"conservative administrations tend to favor cutting regulations\"- citation needed. They favor looking like they do, but end up putting in punitive regulations that only the large entrenched, conservative owned companies can afford to comply with, stifling competition from below. reply whoitwas 8 hours agorootparent100 reply treetalker 19 hours agorootparentprevSure. Off the top of my head, consider the nontechnical, non-expert representatives and senators in the upcoming Congress — who, now that Chevron deference is gone, will have to fill in gaps and cure problems caused by adverse or simply wrong decisions by the members of the judiciary (who are also nontechnical and non-expert in the various fields covered by the agencies). If Congress can pass anything, let alone get it signed into law, that is. Also consider the more-independent agencies, whose boards don't necessarily change composition when the executive administration does. I mean this entirely in a non-political way, too: I'm playing devil's advocate. Apropos of the first point, though, do you trust MTG (for example) to engage in wise gap-filling or overriding of the judiciary when it comes to, say, net neutrality or Schedule I drugs? That's not a snarky question: it's a rhetorical one, only to respond to your point. reply tptacek 19 hours agorootparentAs a matter of public choice theory I think that over the long haul it is better to trust MTG than to imply expansive powers for unelected administrative staff. I think those delegations are often quite good, but best when construed narrowly. reply treetalker 19 hours agorootparentScience and technology change rapidly, leaving interstitial gaps that often need quick policy-filling — difficult for most Congresses, next to impossible for the forthcoming one. Likewise, the judiciary does the damndest things and sometimes needs to be overridden or worked around quickly — and by someone who knows what they're talking about. I see both sides of the issue. Truth be told, the Constitution does not provide for a fourth branch of government. But agencies are so ingrained in our fabric that they're not going anywhere anytime soon. reply tptacek 19 hours agorootparentIf Net Neutrality is a test case for whether science moves too fast for a bona fide, representative legislative process and should be handled instead by broad grants of power to executive appointees, then I'm comfortable picking the side of Congress over the administrative state. I think we probably agree, though, there are better cases to highlight the utility of regulations over laws! reply egorfine 6 hours agoprevI don't think there's a chance of net neutrality happening in the US given the current political climate. Net neutrality is communism, literally. /s reply billyp-rva 22 hours agoprevIt's a sad commentary that, because of how concentrated web traffic is today, net neutrality feels like an antiquated concern. reply orev 21 hours agoparentThe idea that Google, Facebook, Amazon, etc. have made Net Neutrality irrelevant is ridiculous, and is a result of a directed campaign by ISPs to distract attention from them and onto these other companies. With NN in place, there’s nothing at the network level that prevents a competitor to them from rising up. Without NN, ISPs can make deals to speed/slow/block traffic to different destinations, which would have a real effect on possible newcomers that challenge the incumbents. reply billyp-rva 20 hours agorootparent> is a result of a directed campaign by ISPs to distract attention from them and onto these other companies. I've come to this conclusion because NN is effectively dead at the link level, not the ISP level. Try posting a link to the web on any of the big social sites, and you'll be instantly downranked. reply orev 20 hours agorootparentWhatever problem that is, it’s not Net Neutrality. NN is about ISPs being able to do things like sell different service levels, like package A includes access to Facebook and Google only, package B includes package A plus Instagram and Netflix, package C includes package B plus TikTok, etc. Or doing things like making their own video service while making Netflix unusably slow. Not saying your issue also isn’t valid, but it can be addressed by market forces when enough people get sick of that type of behavior (like Twitter/X) and move to something else (like Bluesky). Most people don’t have any real choice between ISPs (through monopolies that have been granted to them for decades by local governments), and therefore there needs to be some additional legal protection against that type of behavior. Trying to lump them both together under",
    "originSummary": [
      "A U.S. Appeals Court has invalidated the FCC's net neutrality rules, which mandated equal treatment of internet traffic by broadband providers, citing the FCC's overreach of legal authority.",
      "The decision references a Supreme Court ruling that restricts the powers of regulatory agencies, impacting the FCC's ability to enforce net neutrality.",
      "FCC Chair Jessica Rosenworcel called for Congress to legislate net neutrality, while Commissioner Brendan Carr supported the court's decision as a check on regulatory overreach."
    ],
    "commentSummary": [
      "A U.S. appeals court has invalidated the FCC's net neutrality rules, citing insufficient legal authority under current law. - The decision follows the Supreme Court's overturning of Chevron deference, which had permitted agencies like the FCC to broadly interpret statutes. - The court classified broadband providers as offering an \"information service,\" not a \"telecommunications service,\" thus restricting the FCC's regulatory power and ending federal net neutrality rules, potentially allowing ISPs to throttle or prioritize internet traffic."
    ],
    "points": 356,
    "commentCount": 302,
    "retryCount": 0,
    "time": 1735848335
  },
  {
    "id": 42581119,
    "title": "Be a property owner and not a renter on the internet",
    "originLink": "https://den.dev/blog/be-a-property-owner-not-a-renter-on-the-internet/",
    "originBody": "Den Delimarsky 🔐/ Writing/ Be A Property Owner And Not A Renter On The Internet/ Be A Property Owner And Not A Renter On The Internet January 2, 2025·5801 words internet indieweb blogs opinion Table of Contents Here we are Advice that is not helpful What can we do 1. Have your own property on the web 2. Never concentrate all your work around one network 3. Don’t treat external networks as link farms 4. If possible, build the core of your value in your own yard rather than someone else’s 5. A mailing list with 1,000 members is better than a social media account with 10,000 followers 6. Always give people ways to get to the home you own 7. Maintain your home 8. Enable your audience to engage with you in an open forum 9. Don’t worry about full control 10. Incremental is better than perfect Conclusion Recommended reading Discussion The year is 2025. The internet in the shape that we’ve known it in the early 2000s is no longer there. Or, not quite in the shape that we’ve seen it before. This is not just plain nostalgia talking - the vibrant ecosystem of blogs, feeds, personal sites, and forums has been usurped by a few mega-concentrated players. Want to find a vibrant community around a very niche subject? It’s probably on Reddit. Or you know what - it could be Discord. Or both. Some still operate the occasional VBulletin on phpBB board, but that’s the minority. Want to buy or sell something? Your best bet is checking Facebook’s Marketplace (and of course, coordinate the purchase through Facebook’s Messenger, not email or any other means). Want to watch videos from some of your favorite makers? YouTube or bust. Anything outside of the ecosystem of these conglomerates is bound to naturally gain less visibility and it will be insanely less discoverable to large swaths of the population. We’ve reached a point where, for quite a few folks, “the internet” is synonymous with “Facebook, Instagram, and Discord,” and if it’s not there, it might as well not exist. This shift introduces a range of problems, that we’ll talk about a bit more in this post. I should also caveat the post with this disclaimer - the target audience here is people proficient with technology. For the vast majority of the population the suggestions I am going to be discussing are untenable in the current technological landscape, and that’s OK. My job is not to get every single person to build their content outside the popular networks but rather encourage those that were on the fence and able to make the jump into owning their own internet property rather than rent the space from someone who is not invested in their success. Here we are # Now, this blog post’s intent is not to get you to stop using all “BigCo” services or convince you that the products made by these companies are universally evil or bad. They’re not. Being able to reliably watch videos on any platform or browser is great. I use YouTube and I publish quite a few videos there. I also use Reddit to connect with folks that enjoy Halo because there is no other place where that community hangs out. There are things that are simply done better when there is proper funding and a full-time staff working every day to maintain the infrastructure at scale, especially when talking about non-niche domains. That means that by default most people will not try to do this themselves. And they shouldn’t. Running infrastructure for yourself is hard. Running infrastructure for others is exponentially harder. As any forum moderator or administrator can attest, the sheer volume of spam, abuse, and just generally trash behavior can wear down even the most seasoned veterans of the .com era. And that’s if we’re talking about communities that host tens of thousands of users. Scale that to a few million and there won’t be enough hours in the day to either keep the servers running (stuff is costly too) or to monitor the content for anything potentially harmful or inappropriate. For anyone that wants to start a community, it’s a no-brainer that they should probably start somewhere where they can “outsource” the hard things, like moderation and infrastructure, to someone who is an expert and has built-in mechanisms to handle everything in-house. Especially when the funds for this are at or around zero dollars. That’s a totally reasonable stance to take, and I absolutely get the rationale there. Not only that, but a lot of the existing ecosystems rest on the shoulders of network effects. Once everyone you care about is on Reddit, are you really going to be the only one not on Reddit and try to bootstrap your own little corner for a specific community? Probably not, and attracting people there would be much harder. Reddit is the one-stop-shop - I can go there for woodworking advice, but I can also chat in local communities, or communities around collecting stamps, with the same account and within the same interface. Are you really going to post photos of your fridge that you want to sell on your blog and hope that in a year someone discovers it through a Google search? Of course not - you will go to Facebook Marketplace or Craigslist and get it sold within a few days. You, the user, are going to go where most people are, because you’re optimizing for solving your problems, such as finding like-minded individuals or selling the fridge quickly and getting it out of your garage to make room for a table saw. Most people are this way - they don’t care who or how maintains or runs a community or network as long as that place solves the problems they set out to solve. When that goal overlaps with the company’s desire to make money, you end up in somewhat of a goldilocks zone, but that’s typically a short-lived experience with modern services. More on that shortly. Knowing all this doesn’t mean that we should automatically absolve these companies from one very insidious thing that creeps up on us over time - being locked into walled gardens. If the last decade has shown us anything, it’s that a lot of these ecosystems are poorly suited to keep the interest of the user in mind for the long run. It all boils down to the fact that major platforms over time are usually interested in: Constant growth (both monetary and engagement), at all costs. Ads are, quite literally, everywhere. YouTube is fighting tooth and nail (the irony of linking to Reddit is not lost on me) to prevent you, the user, from being able to block ads. AI-generated slop is going to make its way into comments and descriptions - arguably, things that nobody really asked for, but you’re going to get them anyway. Facebook and Instagram will soon have “AI users” (whatever shape that’s going to take) to help “boost engagement.” The platforms are not interested in helping you, the customer, in easier and more robust communication with your family and friends. They want you to “engage” and drive the ad dollar revenue up. Keeping you within their walls and give you few chances to leave. Twitter doesn’t care about the fact that you poured your heart and soul in that one blog post and wanted to share it with your community. You post a link - you get downranked. Because that will cause people to actually leave Twitter to read your thinking, and that means they won’t be able to scroll through an ad-ridden feed. You ever wonder why Instagram doesn’t really let you post links? Yeah. Maximizing your attention and time within the ecosystem. Engagement bait is everywhere, and the more engagement - the better! Why bubble up someone’s real thoughts and work when instead you can push up more AI slop that will get people angry, upset, and more likely to engage on your platform (the AI slop problem will get worse, but don’t worry, it will be “a whole new category of content”)? Hoovering up data about you. You can’t even browse a community without an account. Want to see someone’s photo feed on Instagram? Better be ready to sign up, or only see the latest few photos, with a persistent nag screen telling you to log in. Want to browse Facebook Marketplace for that homelab gear you’ve been looking for? Tough luck, you need a Facebook account to see listings, even if you’re not going to buy anything (contrast that to Craigslist). Want to see what a particular hobby community is chatting about? Discord doesn’t allow anonymous browsing, so once again - you need to create an account. Once you log in, there is now clear attribution of everything you do to an account. uBlock Origin and Pi-hole can do a good job at blocking quite a few privacy-invading things, but they do nothing to prevent server-side telemetry collection that tracks your behavior on every page within the platform. Facebook knows exactly how long you spent browsing Marketplace before making a purchase, and in which area you live based on your history. Reddit can tell what you’re interested in based on the communities you browse and tailor ads based on that. Your profile is being built, and then sold (or “shared”) to a myriad of unscrupulous data brokers. Exploiting user-generated content. For example, when you upload an image to Instagram, you’re not transferring ownership rights to your creation to Meta, but you do give them a license to use the content - a “non-exclusive, royalty-free, transferable, sub-licensable, worldwide license to host, use, distribute, modify, run, copy, publicly perform or display, translate, and create derivative works of your content.” Neat! Your art might end up in an ad you did not consent to, but you still retain the copyright. Or they might train their AI on it, who knows. The more walled gardens grow, the more the interest of the user fades, replaced by a strong drive for monetization and growth. So, why are these companies not keeping the interest of the user in mind, though, if the users are the ones actually building the communities? If it wouldn’t be for the users on those platforms wouldn’t those platforms disappear overnight, a fate that MySpace and Digg know a bit to well? Well, in the words of the modern philosopher Dave Chappelle: Dave Chappelle doling out advice on great customer service at Popcopy. Almost with no exceptions, you, the user, are unfortunately the product for social networks of any kind. And that’s not necessarily malice, but just a core design principle of walled gardens, applied generously as they grow. Walled gardens often start with good intentions, but once a critical mass of users settles, there is a strong desire to ensure that those users stick around and can produce a return on investment - those hosting bills aren’t going to pay themselves. That means that a lot of anti-user patterns will slowly start to emerge (not all at once, though), limiting the control over the experience, and by proxy, the benefits to its inhabitants. Say, at first posts with links will be deprioritized. Then, instead of showing your content in a chronological feed for your followers, your content will be shown at the mercy of a black box algorithm. Then, “premium” users will get boosted while non-paying users will be left to linger somewhere at the bottom of the discoverability stack. Before you know it, the oh-so-kind ecosystem that gave you an audience starts being hostile to you unless you pony up or play to the algorithm’s fiddle (\"Hey, how about making your thumbnails a bit more clickbait-y to drive some engagement?\"). At that point, the meta-goal is to serve the company, not you. You’re already hooked, and can’t move away without losing access to the network you built over the years. That’s a losing proposition for most people. Even looking at recent examples such as the massive exodus to Bluesky from Twitter required some extremely strong headwinds to emerge for that to happen, and it’s still unclear how permanent this change will be. Jeff Atwood had a good post about this phenomenon all the way back in 2007. Who knew that little would change for close to two decades. And as the situation required, Cory Doctorow has astutely coined the term that captures what is happening - “enshittification.” A product whose success is predicated on its ability to constantly grow may start with somewhat altruistic aspirations, but it inevitably will reach a point where the owners will need to make money off of its users, and that’s when the service will start turning the screws for the worse, with no way to escape without adverse effects. We’re at that point with basically every ecosystem I mentioned above. We are tenants with landlords who want to make sure that we can’t leave the building or go hang out with friends elsewhere, all while showing us how happy we should be with the limitations imposed on us. Advice that is not helpful # A huge part of the problem here is that, even in my framing, I am thinking like a person that works in tech and enjoys dabbling with technology. I host my blog. I self-host my data. I use a static site generator instead of a vanilla Wix or WordPress instance. I know my way around the terminal and can do a npm run to see what my site looks like before I push it to production. But I am not like most people. Not because I am special, but because most people literally couldn’t care less about the infrastructure, tools, privacy implications, or the fact that their content is not visible without an account. That’s why I find advice like this useless: Well of course it’s better to host your own blog! Also, while you’re at it, put your Mastodon server in a DigitalOcean droplet, throw some Cloudflare CDN in front of it, run your own Raspberry Pi to monitor uptime, and you’re golden! Oh, and don’t forget to also make sure to log into the droplet every once in a while to update the container, do an occasional database migration, and ensure that you check the logs for intrusions. This is about as helpful as someone recommending switching to Linux to someone who wants to improve their privacy posture (skip /r/privacy for your own sanity, by the way). Ask me why I used this analogy. You missed the forest for the trees. No person that doesn’t work in tech (and many people that do) wants to deal with any of this. For most of us, we have a million other things we’d rather do in our life than stand up our own infrastructure to host a blog that gets 50 visits a month from our friends or family, or maintain a Mastodon server that you get charged for monthly to post random thoughts to a few hundred followers. It’s easier to just publish that as a long-form LinkedIn post. It’s easier to share the video on YouTube and not host it yourself on some web page and pay for the bandwidth of a multi-gigabyte MP4 file. Most people are perfectly content with everything living inside their Facebook account because it’s convenient and their family and friends are already there. Telling everyone to learn how git and GitHub Pages work to host their blog is not an effective way to drive change. But that’s also not the point. As I mentioned earlier, the goal is to start with a small niche community of people who are comfortable with building their own digital corners. What can we do # The point I am trying to drive home is that we shouldn’t build our castle in someone else’s kingdom. Do not give full control of your content to the platforms that see you as a resource to be used to grow their own metrics. Here are a few rules that I set for myself, that I hope you also find helpful in making sure that you are a property owner on the internet rather than a mere renter in someone’s building. This is heavily inspired by the “Don’t build your castle in other people’s kingdoms” blog post by Chris Zukowski, with a few extensions that I amended from my own experience. The items below also assume that you’re OK spending a bit of money and time on establishing your more durable presence online. There are, of course, free options, but anything free usually comes with quite a few strings attached that can put you in the same position I argued against above. 1. Have your own property on the web # Buy a domain and host your own website. It really doesn’t matter what you use for either the domain purchase or the hosting, as long as it’s your property. You can start with any of the site builders, host your own blog engine, or even use a static site generator like me. But as long as you have your own stake in the ground, you’re off to a good start. You don’t even need to be super technically proficient for this - platforms like Squarespace simplify this process quite a bit. And this is just one of them. The domain registrar and the website host are entirely transparent to your friends and customers. If at some point your host decides to do something silly, you can take your website and move it elsewhere. Domains can be moved to other registrars too. You get quite a bit of flexibility there. Compare it to having all your eggs in the Twitter basket - if a new owner decides to take it in a direction you don’t agree with, and everyone leaves for Bluesky, you can’t take your Twitter followers and move them over. You’ll be forced to start from scratch. In contrast, if at some point you want to change who hosts your site or which mail provider you use for your newsletter (thankfully emails are independent entities that can work on any platform), as Chris put it in the aforementioned blog post, your customers “[…] will still go to the same URL to get to your site, they will still receive your newsletter from your email address.” I am sure there are places where I should not host my content, no? What am I optimizing for here, exactly? I would recommend avoiding any places where there is content lock-in. You want to optimize for future portability. That is, if you can’t easily export your full content history (e.g., blog posts) and move them somewhere else, don’t use that service. If your content is locked into a service, and at some point that service decides that you are no longer a wanted customer, all that effort you put into making it available to your customers can vanish on a moment’s notice. Prefer sites that allow you to publish in open formats, such as Ghost. If you are going to use a service where the content is stored in a proprietary format that you can’t easily access wholesale, always maintain your own copy and export the hosted content regularly. You should be able to recover it on a new site shall anything happen to your current site. What about the top-level domain (TLD)? Are there any particular things I need to consider before registering my own domain? Should I seek out a .com or is .xyz OK to use as well? Great reminder! Yes, pick a TLD with a generally positive reputation and future durability! You want to make sure that both your registrar and registry are entities that can be more-or-less trusted to not disappear overnight. Any of the traditional .com, .net, or .org are great. .dev, like this blog is using, is good too. Certain country-specific TLDs (called ccTLDs, for “country code top-level domains”) are great as well, such as .ca for Canada (you need to be a Canadian permanent resident or citizen to register, though) or .eu for the European Union (although be aware that if a country does leave the EU, like when the UK went through the Brexit saga, you’d need to migrate off of the TLD). TLDs such as .xyz, .io, or .zip (yes, it’s real, and it’s not spectacular) can be a bit more dicey due to things like association with spam content, dependence on new political developments, and just general mistrust. That’s not to say that you should never use them, but do prefer more established TLDs when possible. As a best-practice, I would also highly recommend the separation of concerns in terms of where you have your domain and where you host your content. If you host your content with Cloudflare, don’t register the domain there - keep it on a registrar like Porkbun or Namecheap. That way if your account is terminated by the host for whatever reason, you’re not left holding the bag because you can’t even access your domain to point to another host. Oh yeah, and while I am at it - if you’re curious to see other places on the web where people established their homes, check out BlogScroll. 2. Never concentrate all your work around one network # If you want to get your work seen in the modern era, it’s very likely that you have to use one of existing social networks with a significant user mass. Speaking from my own experience, YouTube is doing a moderately good job at making sure that folks discover my video content where I interview guests for my podcast. But at the same time, I still host the podcast on my own site. The audio and video for the podcast is available for download to anyone, even if they are not on YouTube, and folks can find it on other aggregators as well. When I talk about the podcast, I don’t just bank on YouTube to bring eyes to my content - I talk about in on Bluesky, LinkedIn, and even Mastodon. And within the recorded episodes, I refer people to theworkitem.com as the destination they should check out for more content, not any of the networks. They can find pointers to those from the website. This coming year, I will be integrating newsletters on both my blog and podcast because I’d like to build a way to connect directly to my audience without any middlemen. The mail provider is irrelevant because emails are universal and can be easily transferred between vendors on an as-needed basis. Build that graph of diversified sources that have your personal site as the final destination for those that want to follow your adventures. Your site can feed those with new audience members, and vice-versa. Shares/Clicks Views/Content Referrals Shares/Clicks Organic Search Results Backlinks Typed URL Traffic Content Click-throughs Followers Traffic Subscribers Views/Subscribers Followers Legend Bi-Directional Audience Channels Uni-Directional Audience Channels Home site 🦋 Microblogging Social Networks 🏠 Your Personal Site 📽 Video Networks 😀 Other Social Networks 🔎 Search Engines 📝 Other Sites ⌨ Direct Traffic ✉ Email Newsletter You also want to ensure that your site can contribute to audience growth on channels where folks are concentrated - if someone stumbles across your blog post on Hacker News, make it easy for them to find out that you are also on Mastodon or other networks where that visitor might already hang out. Decoupling from the influence of a single network ensures that you build a bit of resilience into your audience and content visibility. The gist of this is simple - make sure that you’re not overly-focused on building an audience on just one channel, as any single point of failure can be catastrophic. As with everything in life (including finances and interpersonal relationships), diversification is key. Use Facebook, but also make sure that folks can find you on Bluesky. Sharing photo highlights on Instagram? Great, share them in a photo album on your blog as well and add a story behind it with a few links about the place you took the photo from. This is also why I love that podcasts survived to this day in their proper RSS feed glory - no network can claim that they own the concept of a podcast. I can listen to my favorite podcasts on any of the available podcast apps because they’re just audio files linked in a XML file. That’s it. That’s as open as it gets. Anyone can host the files anywhere, and the feed itself can be created by anyone. And while I am certain that services like Spotify will continue to try to build a walled garden around the podcast ecosystem by having folks publish Spotify-exclusive shows or integrate Spotify-specific features that are not available across other podcast apps, I am happy to see that so far these efforts failed to gain any traction outside a few shows. The vast majority of podcasts live on an open network, and I can listen to them without a Spotify account. And by the way, when I talk about using the existing social channels as a top of the audience funnel, feel zero shame about this tactic - remember when Facebook themselves built a “bridge bot” to funnel the MySpace network into their ecosystem? Facebook addressed this problem by giving MySpace users who switched to Facebook a bridge between the two services. Simply give this tool your MySpace login and password, and it would use a bot to login to your MySpace account, scrape all the waiting messages in your queues and inbox, and push them into your Facebook feed. You could reply to these, and the bot would log back into MySpace and post those replies as you. Be as self-interested in growing your space as major networks are in growing theirs. Thankfully, despite the centralization, we also have a huge diversity of tools that enable you to do all sorts of nice things around your property, like mailing lists (Buttondown, Mailchimp, and Kit come to mind) and monetization systems (such as Stripe, Patreon, or Gumroad). 3. Don’t treat external networks as link farms # Everything I am saying here can be interpreted as “post your content on your site, treat social networks like link farms to drive traffic to your site” and that couldn’t be further from the truth. Social media accounts that just post out-of-context links are of no interest to me and vast majority of people you probably care about. If I see an account do just that, it’s an instant unfollow. If I wanted just links I’d subscribe to a RSS feed. Funnily enough, this kind of behavior has been the downfall of a lot of “walled-off” communities that I’ve been a part of on Slack and Discord - people would use them to self-promote their blogs and other content, and it all became useless because on every visit, I’d see one hundred links with exactly zero conversation around it. That’s not the way to go. Use other channels to meaningfully engage with the community. Answer their questions. Ask for feedback. Build rapport. You are building a community, not flooding the zone with self-promoting content. This will allow you to steer them to places that are more favorable to nurturing and growing your audience long-term. 4. If possible, build the core of your value in your own yard rather than someone else’s # I’ve seen this tactic applied by a few folks on YouTube and I think it’s brilliant. Say, you’re someone known for teaching people how to use CSS, JavaScript, or anything else. You post some content on YouTube, but make sure to promote the heck out of the courses that you manage on your own site (including monetization). This way, you skip the need to depend on a large network for your income entirely - you still maintain it as a top-of-funnel channel, but the core is on your own turf. You own the distribution while the “marketing” channels are nothing other than “plug-ins” in your system. You still want them to be reliable, but you’re not worried about one of them going offline. 5. A mailing list with 1,000 members is better than a social media account with 10,000 followers # This article is sprinkled with this idea, but to make it clear again - email is a universal protocol that you can take with you no matter where you go. If you have a mailing list of 1,000 community members that are interested in what you have to say, you can keep that forever (or, until they unsubscribe). A social media account can go “poof” in a week because the CEO decided that you’re not worth promoting anymore. Substack not to your liking? No problem - you take the emails and move them to Buttondown. Can’t quite do the same with an audience on any social network. Anecdotally, people that entrusted you with their email address are also significantly more invested in reading/watching/listening to your content than an amalgam of followers across the social network landscape, but that could be recency bias. Always lean towards open and portable standards. 6. Always give people ways to get to the home you own # Wrote a blog post? Don’t just copy and paste the entirety of the content as a LinkedIn post. Post an extract, and then a link to your site. Sharing a new podcast episode? Instead of linking to the YouTube video in your LinkedIn post promoting it, link to the episode page on your website, where people can see how to subscribe on their favorite podcast app, read through the episode transcript, or even check out the resources a guest might’ve shared. The ecosystems I talked about in this post should be your top-of-funnel for your audience. Yes, some of your posts might be downranked, but that’s OK, because some of them won’t. I’ve seen people share my content on Bluesky and LinkedIn as a link, and it drove quite a few folks into my email inbox with suggestions, feedback, and ideas. Have links to your websites in your bio or anywhere where the networks allow you to customize things. This is not about isolating yourself from the major networks, but rather using them to your advantage. My content lives in my home, and if you want to see more of it - come on over to den.dev. 7. Maintain your home # If your building is overgrown with weeds, the sidewalk is cracked (congratulations, you have a tripping hazard), and the insides are covered in dust, the likelihood of quality visitors will decrease. The same applies to your internet property. If you want to build an audience that keeps consistently coming back, make sure that your website, blog, portfolio, or photo gallery is in good shape. It doesn’t need to be pixel-perfect, but it needs to show that the author is still there, and there is a reason to stop by. If you post everything to YouTube, link to your site, and the site’s last content update was in 2004, that doesn’t inspire confidence that whoever found you on YouTube should also check out your blog - they’ll just see you on YouTube. 8. Enable your audience to engage with you in an open forum # As much as I yearn for the era of proper forums to come back, it might be a lost war at scale. But I don’t need to win the war - I want to engage with a niche audience of folks that care about what I build. For that, as much as possible, leverage tools that foster open engagement that can be discoverable and indexable. Discourse is a fantastic option. If you’re on a budget, integrate comments as GitHub issues on your blog with the help of Utterances, like you see here, on den.dev. The latter has been particularly great for my blog. On one occasion I had a very long chat with a community member about Halo film format reverse engineering - all easily available to anyone in the future. Avoid locking in your community into a Discord, Slack, or any other proprietary and hidden service that keeps your content locked in. 9. Don’t worry about full control # Having your own corner of the internet is not some kind of “purity test” where you need to set up an Apache server from scratch, host your mail server, or maintain your own homegrown static site generator running on seventeen generations of Gentoo Linux. At some point, you will have to depend on other systems and let go of full control. You might use Azure for hosting, Porkbun for domains, a Cloudflare CDN for your images, GitHub for your content backups, AWS S3 for larger files, and so on. I am by no means trying to push you to build your garage server and deal with all the things that come with that decision. There is no cloud, it’s just someone else’s computer. The goal is to give you an opportunity to build something that you have full control of with a good degree of flexibility, all while using off-the-shelf tools and services. None of the services or tools are guaranteed future-proof. Just like extreme circumstances can result in you losing your purchased house (the analogy falls apart in that the government would pay for the property in Eminent Domain cases, but you get the idea), the same can happen with online properties. But it still offers much greater flexibility than depending on some third-party platform to keep your interest at the forefront of their plans. 10. Incremental is better than perfect # Your site is not going to be perfect from day one. You are going to run into hiccups with discoverability. You’ll find out that nobody can find it on Google for the first month. That’s OK. You can pull through this. Keep writing and posting about things you learned, things you saw, or things you’ve otherwise experienced. Document your process. Share your notes. Write up your brain dump from this weekend’s brainstorm. Grow your digital garden. With every sentence, every word, every photo published, it will become more interesting to others who will discover it later. Conclusion # I hope that the above inspired you to seek out a way to get your own lot on the web and build your house on it. Not everyone will do that, and not everyone needs to. But, the more the indie web space grows around those that can invest the time and effort, the more vibrant our own ecosystem becomes, and what is the web without a vibrant ecosystem. Now wait a second. You just said \"indie web\" and this entire post rung a bell. Aren't you talking about Publish (on your) Own Site, Syndicate Elsewhere (POSSE)? Indeed I am! A lot of what I talked about above is inspired by and based on POSSE. Own your URLs! Recommended reading # Your words are wasted by Scott Hanselman. The ever-increasing walled gardenness of Twitter by Annoying Technology. /u/spez is right about feudalism and that’s why reddit as we know it is doomed by maya.land. Cinch.fm mobile audio service shuts down, demonstrates troubles of when you bet on services you don’t control by Robert Scoble. Blogodammerung? by Tim Bray. Discussion # Besides the GitHub comments on this blog post, you can participate in the discussion on Hacker News. Author Den Delimarsky Hacker, Tinkerer, Data Nerd, Trail Explorer. ← → Quickly Finding Previously Typed Commands With PowerShell December 30, 2024",
    "commentLink": "https://news.ycombinator.com/item?id=42581119",
    "commentBody": "Be a property owner and not a renter on the internet (den.dev)295 points by dend 17 hours agohidepastfavorite211 comments rpcope1 14 hours ago> Exploiting user-generated content. You know, if I've noticed anything in the past couple years, it's that even if you self-host your own site, it's still going to get hoovered up and used/exploited by things like AI training bots. I think between everyone's code getting trained on, even if it's AGPLv3 or something similarly restrictive, and generally everything public on the internet getting \"trained\" and \"transformed\" to basically launder it via \"AI\", I can absolutely see why someone rational would want to share a whole lot less, anywhere, in an open fashion, regardless of where it's hosted. I'd honestly rather see and think more about how to segment communities locally, and go back to the \"fragmented\" way things once were. It's easier to want to share with other real people than inadvertently working for free to enrich companies. reply dend 14 hours agoparentNothing to disagree in this statement, for sure. If it's on the open internet, it will almost surely be used for AI training, consent be damned. But it feels like even at a rudimentary level, if I post a picture on my site that is then used by a large publisher for ads, I would (at least in theory) have some recourse to pursue the matter and prevent them from using my content. In contrast, if I uploaded something to a social media site like Instagram, and then Meta \"sublicensed\" my image to someone else, I wouldn't have much to say there. Would love someone with actual legal knowledge to chime in here. reply chii 14 hours agorootparent> Meta \"sublicensed\" my image to someone else, I wouldn't have much to say there. but you agreed to this, when agreeing to the TOS. > I post a picture on my site that is then used by a large publisher for ads, I would (at least in theory) have some recourse which you didn't sign any contract, and therefore it is a violation of copyright. But the new AI training methods are currently, at least imho, not a violation of copyright - not any more than a human eye viewing it (which you've implicitly given permission to do so, by putting it up on the internet). On the other hand, if you put it behind a gate (no matter how trivial), then you could've at least legally protected yourself. reply DrScientist 7 hours agorootparent> But the new AI training methods are currently, at least imho, not a violation of copyright - not any more than a human eye viewing it Interesting comparison - as if a human viewed something, memorized it and reproduced in a recognisable way to be pretty much the same, wouldn't that still breach copyright? ie in the human case it doesn't matter whether it went through an intermediate neural encoding - what matters is whether the output is sufficiently similar to be deemed a copy. Surely the same is the case of AI? reply omnimus 6 hours agorootparentThis whole AI learns like a human is trajectory of thought pushed by AI companies. They at same time try to humanize AI (it learns like a human would) and dehumanize humans (humans are stochastic parrots anyway). It's if anything a distraction if not straight up anti-human. But you are right that copyright is complex and in the end decided by human (often in court). Consider how code infringement is not about code itself but about what it does. If you saw somewhat original implementation of something and then you rewrite it in different language by yourself there is high chance its still copyright infringement. On the other hand with images and art it's even more about cultural context. For example works of pop artists like Andy Warhol are for sure original works (even though some of it was disputed recently in court and lost). Nobody considers Andy Warhols work unoriginal even if it often looks very similar to some output it was riffing off because the essence is different to the original. Compare that to pepople prompting directly with name of artist they want to replicate. This in direct copyright infringement in both essence and intention no matter the resulting image. Also it's different to when human would want to replicate some artist style because humans can't do it 100% even if they want to. There is still piece of their \"essence\". There are many people who try to fake some famous artist style and sell it as real thing and simply can't do it. This is of course copyright infringement because of the intent but it's more original work than anything coming from LLMs. reply DrScientist 5 hours agorootparentIt's both complex and extremely simple for the same reason - it's a human judgement in the end. Just because you can't define something mathematically, doesn't mean it isn't obvious to most people in 99% of cases. Reminds me of the endless games in tax law/avoidance/evasion and the almost pointless attempt to define something absolutely in words. To be honest you could simplify the whole thing by having a 'taking the piss' test - if the jury thinks you are obviously 'taking the piss' then you are guilty - and if you whine about the law not being clear and how it's unfair because you don't know whether or not you are breaking the law - well don't take the piss then - don't pretend you don't know whether something is an agressive tax dodge or not. If you create some fake IP, and license it from some shell company in a low tax regime to nuke your profits in the country you are actually doing business in - let's not pretend we all can't see what you doing there - you are taking the piss. Same goes for what some tech companies are doing right now - every reasonable person can see they are taking the piss - and high paid lawyers arguing technicalities isn't going to change that. reply ToucanLoucan 4 hours agorootparentprevThe difference is an image generation algorithm does not consume images the way a human does, nor reproduce them that way. If you show a human several Rembrandt's and ask them to duplicate them, you won't get exact copies, no matter how brilliant the human is: the human doesn't know how Rembrandt painted, and especially if you don't permit them to keep references, you won't get the exact painting: you'll get the elements of the original that most stuck out to them, combined with an ethereal but detectable sense of their original tastes leaking through. That's how inspiration works. If on the other hand you ask an image generator for a Rembrandt, you'll get several usable images, and good odds a few them will be outright copies, and decent odds a few of them will be configured into an etsy or ebay product image despite you not asking for that. And the better the generator is, the better it's going to do at making really good Rembrandt style paintings, which ironically, increases the odds of it just copying a real one that appeared many times in it's training data. People try and excuse this with explanations about how it doesn't store the images in it's model, which is true, it doesn't. However if you have a famous painting by any artist, or any work really, it's going to show up in the training data many, many times, and the more popular the artist, the more times it's going to be averaged. So if the same piece appears in lots and lots of places, it creates a \"rut\" in the data if you will, where the algorithm is likely going to strike repeatedly. This is why it's possible to get full copied artworks out of image generators with the right prompts. reply HanClinto 3 hours agorootparentWe have the problem of too-perfect-recall with humans too -- even beyond artists with (near) photographic memory, there's the more common case of things like reverse-engineering. At times, developers on projects like WINE and ReactOS use \"clean-room\" reverse-engineering policies [0], where -- if Developer A reads a decompiled version of an undocumented routine in a Windows DLL (in order to figure out what it does), then they are now \"contaminated\" and not eligible to write the open-source replacement for this DLL, because we cannot trust them to not copy it verbatim (or enough to violate copyright). So we need to introduce a barrier of safety, where Developer A then writes a plaintext translation of the code, describing and documenting its functionality in complete detail. They are then free to pass this to someone else (Developer B) who is now free to implement an open-source replacement for that function -- unburdened by any fear of copyright violation or contamination. So your comment has me pondering -- what would the equivalent look like (mathematically) inside of an LLM? Is there a way to do clean-room reverse-engineering of images, text, videos, etc? Obviously one couldn't use clean-room training for _everything_ -- there must be a shared context of language at some point between the two Developers. But you have me wondering... could one build a system to train an LLM from copywritten content in a way that doesn't violate copyright? [0]: https://en.wikipedia.org/wiki/Clean-room_design reply chii 3 hours agorootparentprev> with the right prompts. that is doing a lot of pull. Just because you could \"get the full copies\" with the right prompts, doesn't mean the weights and the training is copyright infringement. I could also get a full copy of any works out of the digits of pi. The point i would like to emphasize is that the using data to train the model is not copyright infringement in and of itself. If you use the resulting model to output a copy of an existing work, then this act constitutes copyright infringement - in the exact same way that using photoshop to reproduce some works is. What a lot of anti-ai arguments are trying to achieve is to make the act of training and model making the infringing act, and the claim is that the data is being copied while training is happening. reply DrScientist 2 hours agorootparent>The point i would like to emphasize is that the using data to train the model is not copyright infringement in and of itself. Interesting point - though the law can be strange in some cases - so for example in the UK in court cases where people are effectively being charged for looking at illegal images, the actual crime can be 'making illegal images' - simply because a precedence has been set that because any OS/Browser has to 'copy' the data of any image in order someone to be able to view it - any defendent has been deemed to copied it. Here's an example. https://www.bbc.com/news/articles/cgm7dvv128ro So to ingest something your training model ( view ) you have by definition have had to have copied it to your computer. reply entropi 11 hours agorootparentprev>But the new AI training methods are currently, at least imho, not a violation of copyright - not any more than a human eye viewing it (which you've implicitly given permission to do so, by putting it up on the internet). I don't understand how that matters. I thought that the whole idea of copyright and licences was that the holder of the rights can decide what is ok to do with the content and what is not. If the holder of the rights does not agree to a certain kind of use, what else is there to discuss? It sure does not matter if I think that downloading a torrent is not any more pirating than borrowing a media from my friend. reply chii 10 hours agorootparent> If the holder of the rights does not agree to a certain kind of use, what else is there to discuss? the holder of content does not automatically get to prescribe how i would use said content, as long as i comply with the copyrights. The holder does not get to dictate anything beyond that - for example, i can learn from the content. Or i can berate it. Copyright is not a right that covers every single conceivable use - it is a limited set of uses that have been outlayed in the law. So the current arguments center on the fact that it is unknown if existing copyright covers the use of said works in ML training. reply TheOtherHobbes 9 hours agorootparentCopyright means the holder does automatically get to prescribe how content can be copied. That's literally the definition of copyright. A typical copyright notice for a book says something like (to paraphrase...) \"not to be stored, transmitted, or used by or on any electronic device without explicit permission.\" That clearly includes use for training, because you can't train without making a copy, even if the copy is subsequently thrown away. Any argument about this is trying to redefine copyright as the right to extract the semantic or cultural value of a document. In reality the definition is already clear - no copying of a document by any means for any purpose without explicit permission. This is even implicitly acknowledged in the CC definitions. CC would be meaningless and pointless without it. reply rpdillon 2 hours agorootparent> Any argument about this is trying to redefine copyright as the right to extract the semantic or cultural value of a document. In reality the definition is already clear - no copying of a document by any means for any purpose without explicit permission. I've studied copyright for over 20 years as an amateur, and I used to very much think this way. And then I started reading court decisions about copyright, and suddenly it became extremely clear that it's a very nuanced discussion about whether or not the document can be copied without explicit permission. There are tons of cases where it's perfectly permissible, even if the copyright holder demands that you request permission. I've covered this in other posts on Hacker News, but it is still my belief that we will ultimately find AI training to be fair use because it does not materially impact the market for the original work. Perhaps someone could bring a case that makes the case that it does, but courts have yet to see a claim that asserts this in a convincing way based on my reading of the cases over the past couple of years. reply chii 7 hours agorootparentprev> That clearly includes use for training, because you can't train without making a copy, even if the copy is subsequently thrown away. a copy for ingestion purposes - such as viewing in a browser, is not the same as a distribution copy that you make sending it to another person. > the right to extract the semantic or cultural value of a document. this right does not belong to the author - in fact, this is not an explicit right granted by the copyright act. Therefore, the extraction of information from a works is not something the author can (nor should) control. Otherwise, how would anyone learn off a textbook, music or art? In the future, when the courts finally decide what the limits of ML training is, may be it will be a new right granted to authors. But it isn't one atm. reply rcxdude 8 hours agorootparentprevThis a particularly extreme interpretation of copyright, and not one that has seen that much support in the courts. You can put what you like in a copyright notice or license, but it doesn't mean it'll succeed, and the courts have generally taken a dim view of any argument which relied on the fact that electronic data is technically copied many times just to make it viewable to a user. Copyright is probably better understood as distribution rights. (Not saying training will necessarily fall in the same boat, just saying that the view 'copying to a screen or over the internet is necessarily a copy for the purposes of copyright' is reductive to the point of being outright incorrect) reply chromanoid 9 hours agorootparentprevyeah, it is called _copy_ right. The question is, if AI is making obfuscated copies or not. interestingly in German it is not called copyright, but Urheberrecht \"authors rights\". So there the word itself implies more things. BTW at least in Germany you can own image rights of your art piece or building that is placed in a public place. reply ehnto 12 hours agorootparentprevStrong disagree on the last paragraph. It's data online, your data, and it was used for commercial purposes without your consent. In fact, I never consented for anyone to access my server. Just because it has an IP address, does not make it a public service. Obviously in a practical sense that is a silly position to take, and in prior cases there is usually an extenuating factor that got the person charged, eg breaking through access controls, violating ToS, or intellectual property violations. But I don't rescind the prior statement. Just because I have an address doesn't mean you can come in through any unlocked doors. reply ahtihn 11 hours agorootparent> In fact, I never consented for anyone to access my server. Just because it has an IP address, does not make it a public service. If you don't take any steps to make it clear that it's not public, like an auth wall or putting pages on unguessable paths, then it is public, because that is what everyone expects. Just like you if you have a storefront, if the door is unlocked you'd expect people to just come in and no one would take you seriously if you complain that people keep coming in if you don't somehow make it clear that they're not supposed to. reply DrScientist 7 hours agorootparentYour shop might be open sure - but aren't we talking about people coming in and taking whatever they like for free? ie if you were an art gallery, the expectation would be people could come in and look, but you don't expect them to come in, photograph everything and then sell prints of everything online. reply chii 3 hours agorootparentThat's not what's happening. Instead, it's that there's some people coming into your gallery, studying the art and its style, and leaving with the learned information. They then replicate that style in their own gallery. Of course, none of the images are copies, or would be judged to be copies by a reasonable person. So now you, the gallery owner, want to forbid just those people who would come to learn the style. But you still want people to come and admire the art, and may be buy a print. reply DrScientist 3 hours agorootparent> Of course, none of the images are copies, or would be judged to be copies by a reasonable person. That's the fiction of course. Tell me how something like ChatGPT can simultaneously claim to return accurate information while at the same time being completely independent from the sources of the information? In terms of images - copyright isn't only for exact copies - it if was then humans would have been taking the piss by making minor changes for decades. Sure you could argue some is fair use with genuinely original content being produced in the process, but I think you are also overlooking an important part of what's considered 'fair' - industrialised copying of source material isn't really the same in terms of fairness as one person getting inspiration. Taking the Encylopedia Britanica and running it though an algorithm to change the wording, but not the meaning, and selling it on is really not the same as a student reading it and including those facts in their essay - the latter is considered fair use, the former is taking the piss. reply chii 3 hours agorootparent> ChatGPT can simultaneously claim to return accurate information while at the same time being completely independent from the sources of the information? why can't that be true? Information is not copyrightable. The expression of information is. If chatGPT extracted information from a source works, and represent that information back to you in a form that is not a copy of the original works, then this is completely fine to me. An example would be a recipe. reply DrScientist 3 hours agorootparentSo you think taking something like the Encylopedia Britanica, running it through a simple rewording algorithm, and selling it on is totally 'fair use'? Taking all newspaper and proper journalistic output and rewording it automatically and selling it on is also 'fair use'? Stand back from the detail ( of whether this pixel or word is the same or not ) and look at the bigger picture. You still telling me that's all fine and dandy? I think it's obviously not 'fair use'. It means the people doing the actual hard graft of gathering the news, or writing Encylopedias or Textbooks won't be able to make a living so these important activities will cease. This is exactly the scenario copyright etc exists to stop. reply yencabulator 1 hour agorootparentprevContent is often publicly available and copyright protected. Paint a mural near a busy street. No locked door in that metaphor; locked door would be password protected site. reply PittleyDunkin 11 hours agorootparentprev> but you agreed to this, when agreeing to the TOS The legal definition of agreement means basically zilch reply immibis 12 hours agorootparentprev> but you agreed to this Yes, that was the point? You agree to this by using Meta. So don't. reply alibarber 11 hours agoparentprevBased on my experience, I've found that I like using AI (GitHub copilot) to do things like answer questions about a language that I could easily verify in the documentation. Almost basically 'yes/no' questions. To be honest if I were writing such documentation for a product/feature, I wouldn't mind the AI hoovering it up. I've found it to be pretty crap at doing things like actual algorithms or explaining 'science' - the kind of interesting work that I find on websites or blogs. It just throws out sensible looking code and nice sounding words that just don't quite work or misses out huge chunks of understanding / reasoning. Despite not having done it in ages, I enjoy writing and publishing online info that I would have found useful when I was trying to build / learn something. If people want to pay a company to mash that up and serve them garbage instead, then more fool them. reply namaria 8 hours agorootparentI've argued years ago based on how LLMs are built that they would only ever amount to lossy and very memory inefficient compression algorithms. The whole 'hallucination' thing misses the mark. LLMs are not 'occasionally' wrong/hallucinating sometimes. They can only ever return lower resolution versions of what was on their training data. I was mocked then but I feel vindicated now. reply richardwhiuk 8 hours agorootparentThey can combine two things in a way that never appeared together in the source material. reply namaria 5 hours agorootparentYoutube compression algorithm also produces lots of artifacts that were never filmed by the video producers reply wizzwizz4 4 hours agorootparentAnd datamoshing lets you produce effects that weren't in the source clips. reply baxtr 12 hours agoparentprevPart of me viscerally agrees because large corporations have monetized UGC. Another part of me though thinks differently. We are a species that builds knowledge from generation to generation. From one person to another. Over years, over centuries. Philosophically this part tends to think that your thoughts and ideas belong to humanity and thus need to be shared with all of us. reply friendzis 11 hours agorootparentIf you recall high school history, rapid, exponential \"progress\" happened once the knowledge was 1) written down (printing press) 2) archived for the future (libraries) 3) systematized (textbook/encyclopaedia) 4) proactively shared (public education), all on a massive scale. The fact that some knowledge exists and is even accessible does not really matter if takes a highly trained in a very narrow field scholar to find that piece of information. You need a well established knowledge creation and distribution funnel in operation for humanity as a whole to reap the benefits of knowledge. There is undoubtedly a lot of useful knowledge on internet platforms, however, most of that knowledge remains unsystematized and largely undiscoverable, meaning that contribution to the totality of human knowledge by these platforms is infinitesimal, which is further drowned by cat and porn videos. reply TeMPOraL 9 hours agorootparentNow we have 5) aggregated and internalized as a whole by computational constructs such as LLMs, which are - 4) - proactively shared (open weights, but also freemium service and dirt-cheap API access to commercial SOTA models), still on a massive scale. > There is undoubtedly a lot of useful knowledge on internet platforms, however, most of that knowledge remains unsystematized and largely undiscoverable, meaning that contribution to the totality of human knowledge by these platforms is infinitesimal, which is further drowned by cat and porn videos. Precisely that. Which is why I often argue, that for 99%+ of the content in the training data, its marginal contribution to the training process - itself infinitesimal in isolation - is still by far the most value that content will ever bring to the world. reply yencabulator 1 hour agorootparentprevThe compromise that was supposed to be in place was strong, short term, copyright protection, to help the author (a person) financially during their lifetime. That compromise was destroyed by rich people using corporations as owners and extending copyright duration. https://en.wikipedia.org/wiki/Copyright_Term_Extension_Act reply Salgat 11 hours agorootparentprevThere's two decades worth of countless conversations on Reddit alone that would be buried into nothingness but instead ML has revived all that activity as useful data. ML is definitely a great way to bring back utility for a lot of old and unused data. reply Terr_ 7 hours agorootparent> revived that activity as useful data Revived as compressed text associations, it is potentially useful data, but also potentially totally wrong in non-obvious ways. (Or, to riff on Futurama, \"The worst kind of incorrect.\") reply Salgat 2 hours agorootparentIt is used to help train the LLMs on how to \"talk\" like normal people, even if the topic they're discussing isn't that useful or valuable. reply tempestn 9 hours agorootparentprevThis seems like a reasonable take to me. I wish those downvoting you would explain where they disagree. reply yowayb 12 hours agorootparentprevGreat take. Also agree with parent. I feel like some form of provenance would take us to the next level. reply couscouspie 2 hours agoparentprevThe low adoption rate is another big plus of the Gemini protocol and similar solutions for a Javascript-free and open internet. reply blahblah23 11 hours agoparentprevThere's a neat little thing some discords I've seen use, where they honeypot spam bots into a channel-- if someone posts into it, their messages in the last 5 minutes get deleted and their account gets kicked. Is there a meaningful way to make it so a website shares a resource that automatically updates their blacklist to block the IP address? Knowing that you will lose X but hopefully you'll retain everyone who can read? reply notpushkin 10 hours agorootparentMost scrapers use residential IPs nowadays. They will just rotate their IP and go on, while the IP you banned would get assigned to an innocent user that won’t be able to access your site now. reply atribecalledqst 4 hours agorootparentprevI saw a Discord server use this but it never actually caught anything. Turns out all the spammers were just human idiots! reply Terr_ 14 hours agoparentprevIANAL, but lately I've had this quixotic daydream of a combination accept-cookies / agree-to-TOS page that comes up, and the Terms of Service says by proceeding they agree to give the site-owner an perpetual, irrevocable, and royalty-free to use and re-license any future content that they create using any generative AI that was trained using the website contents. Then you carefully log what LLM user-agents/IPs go past that agree, along with some very distinctive secretly crawlable pages which have contents that can be distinctively reproduced back out of the model if needed. Then whenever SomeShittyLLM posts \"articles\", everybody with that TOS that was crawled gets to duplicate it without ads for free. :P reply w4 12 hours agorootparentThis idea is reminiscent of the opening scene of Accelerando by Charlie Stross: Are you saying you taught yourself the language just so you could talk to me?\" \"Da, was easy: Spawn billion-node neural network, and download Teletubbies and Sesame Street at maximum speed. Pardon excuse entropy overlay of bad grammar: Am afraid of digital fingerprints steganographically masked into my-our tutorials.\" … \"Uh, I'm not sure I got that. Let me get this straight, you claim to be some kind of AI, working for KGB dot RU, and you're afraid of a copyright infringement lawsuit over your translator semiotics?\" \"Am have been badly burned by viral end-user license agreements. Have no desire to experiment with patent shell companies held by Chechen infoterrorists. You are human, you must not worry cereal company repossess your small intestine because digest unlicensed food with it, right?” - https://www.antipope.org/charlie/blog-static/fiction/acceler... Amusing to also note that this excerpt predicted the current LLM training methodology quite well, in 2005. reply TeMPOraL 9 hours agorootparentAlso amusing: > patent shell companies held by Chechen infoterrorists This perfectly captures how both patent trolls and MAFIAA look like in my mind. reply spiritplumber 13 hours agorootparentprevI love this, I did something like that with made-up-italian-sounding words a while ago (you used to be able to find my site if you looked for FANTACHIAVE). It's a bit like fake roads on map databases) reply ehnto 12 hours agoparentprevI have decided not to put text online if I feel it has IP or personal ideas in it. Some exceptions, like posting here, and including stuff I want to get out there for commercial reasons eg: marketing of services. The one I struggle with is discord, but I am not too personal on discord servers so I suppose I'll just mesh into the soup of barely worthwhile chatter. I also started self hosting my git repos and knowledge base, both were trivial to set up. reply foxglacier 1 hour agoparentprevWhat is your purpose of publishing where having the content used to train AI is a problem? Are you trying to gatekeep information that's not even protected by copyright anyway? Are you worried your potential audience will get the same thing (including your personal creativity) from AI that just copied your work so they don't recognize your name and develop some brand awareness? Or do you just not like AI and don't want to help it? Maybe you could build your own paywall or other technical access restriction instead of making it freely available? Even just a captcha should block AI training scrapers, shouldn't it? reply asdff 1 hour agorootparentNo attribution with ai reply immibis 12 hours agoparentprevFor images, there's Nightshade, which imperceptibly alters your images but makes them poison for AI (does anyone understand why?) I don't know if there's something similar for text. You could try writing nonsense with a color that doesn't contrast with the background. The evidence Nightshade works is that AI companies want to make it illegal. reply rcxdude 8 hours agorootparentNightshade and glaze are basically adversarial attacks on various commonly used subcomponents of image generators, most notably the CLIP image captioner, which is both used to generate training data and as part of the generation process. Like most adversarial attacks, they get more perceptible as they try to be robust to more transformations of the data (both in practice, i.e. applied to a level that's non-trivially removable, tend to make images look like slightly janky AI, ironically), and they are specific to the net(s) they are targeting, so it's more of a temporary defense against the current generation then a long-term protection. reply kelseydh 9 hours agorootparentprevLink to Nightshade: https://nightshade.cs.uchicago.edu/whatis.html This is fascinating. Would be great to have a web interface artists can use that doesn't require them to install the software locally. reply pixelmonkey 3 hours agoparentprevI think you're right, and I don't think it's just about public content being \"exploited\" to train AI models and the like. Rather, even before LLMs, there was a growing sense that publishing ideas or essays publicly is \"risky\" with very little reward for the very real risks. I wrote about this a little in \"The Blog Chill\": https://amontalenti.com/2023/12/28/the-blog-chill Speaking personally, among my social circle of \"normie\" college-educated millennials working in fields like finance, sales, hospitality, retail, IT, medicine, civil engineering, and law -- I am one of the few who runs a semi-active personal site. Thinking about it for a moment, out of a group of 50-or-so people like this, spread across several US states, I might be the only one who has a public essay archive or blog. Yet among this same group you'll find Instagram posters, TikTok'ers, and prolific DM authors in more private spaces like WhatsApp and Signal groups. A handful of them have admitted to being lurkers on Reddit or Twitter/X, but not one is a poster. It isn't just due to a lack of technical ability, although that's a (minor) contributing factor. If that were all, they'd all be publishing to Substack, but they're not. It's that engaging with \"the public\" via writing is seen as an exhausting proposition at odds with everyday middle class life. Why? My guesses: a) smartphones aren't designed for writing and editing, hardware-wise; b) long-form writing/editing is hard and most people aren't built for it; c) the dynamics of modern internet aggregation and agglomeration makes it hard to find independent sites/publishers anyway; and d) the risk of your developed view on anything being \"out there\" (whether professional risk or friendship risk) seems higher than any sort of potential reward. On the bright side, for people who fancy themselves public intellectuals or public writers, hosting your own censorship-resistant publishing infrastructure has never been easier or cheaper. And for amateur writers like me, I can take advantage of the same. But I think everyday internet users are falling into a lull of treating the modern internet as little more than a source of short-form video entertainment, streams for music/podcasts, and a personal assistant for the sundries of daily life. Aside from placating boredom, they just use their smartphones to make appointment reminders, send texts to a partner/spouse, place e-commerce orders, and check off family todo lists, etc. I expect LLMs will make this worse as a younger generation may view long-form writing not as a form of expression but instead as a chore to automate away. reply sneak 7 hours agoparentprevCopying data isn’t exploitation. reply dehrmann 12 hours agoparentprev> used/exploited by things like AI training bots How is this worse than a human reading your blog/code, remembering the key parts of it, and creating something transformative from it? reply dend 12 hours agorootparentIn the grand scheme of things and at this point, it probably doesn't matter. I know for me it certainly is not in any shape a discouragement to continue writing on my blog and contributing code to open source communities (my own and others). But if we're going to dig into this a bit, one person reading my code, internalizing it, processing it themselves, tweaking it and experimenting with it, and then shipping something transformative means that I've enhanced the knowledge of some individual with my work. It's a win. They got my content for free, as I intended it to be, and their life got a tiny bit better because of it (I hope). The opposite of that is some massively funded company taking my content, training a model off of it, and then reaping profits while the authors don't even get as much as an acknowledgement. You could theoretically argue that in the long-run, a LLM would likely help other people through my content that it trained on, but ethically this is most definitely a more-than-gray area. The (good/bad) news is that this ship has sailed and we now need to adjust to this new mode of operation. reply dehrmann 12 hours agorootparent> The opposite of that is some massively funded company taking my content, training a model off of it, and then reaping profits while the authors don't even get as much as an acknowledgement. Taking out the \"training a model\" part, the same thing could happen with a human at the company. reply dend 12 hours agorootparentOh, 100%. I mentioned this in another comment (https://news.ycombinator.com/item?id=42582518) - I've dealt with a fair share of stolen content (thankfully nothing too important, just a random blog post here and there), and it definitely stings. The difference is that this is now done at a massive scale. But again - this doesn't stop me from continuing to write and publish in the open. I am writing for other people reading my content, and as a bouncing board for myself. There will always be some shape or form of actors that try to piggyback off of that effort, but that's the trade-off of the open web. I am certainly not planning to lock all my writing behind a paywall to stop that. reply thieaway3947 12 hours agorootparentprevThis is already a scenario that people generally accept as bad, could you elaborate the point you are making? reply camgunz 4 hours agorootparentprevOne of the--admittedly many--things that puts me off AI is the pitch starts off as \"you will have abilities you never had before and probably never would have had, be excited\", then when critics are like, \"woof it's a little worrying you can , , , etc.\", the pitch switches to \"you could always have done this, calm down\". The whole point of software engineering is to do stuff faster than you could before. It is THE feature. We could already add, we could already FMA, we could already do matrix math, etc. etc. Doing it billions of times faster than we could before at far less energy expenditure--even including what it takes to build and deliver computers--has led to an explosion of productivity, discovery, and prosperity. Scale is the point. It changes everything and we know it; we shouldn't pretend otherwise. reply mrweasel 9 hours agorootparentprevAttribution. If you read a book, blog or code and others ask you where you got your ideas/inspiration you can refer them back to the original author. This helps people build a reputation. Even if it's just happens every once in a while, it still helps the original author. Once an AI has hoover up your work and regurgitated it as it's own, all links back to the original creator is lost. reply ykonstant 11 hours agorootparentprevScale makes all the difference in the world. reply sifar 12 hours agorootparentprevScale. reply moron4hire 12 hours agorootparentprevSeriously? How is rule utilitarianism different from act utilitarianism? reply matheusmoreira 14 hours agoparentprev> I can absolutely see why someone rational would want to share a whole lot less, anywhere, in an open fashion, regardless of where it's hosted. I've reached the same conclusion. All data is just bits. Numbers. Once it's out there, trying to control their spread and use is just delusional. People should just stop sharing things publicly. Even things like AGPLv3 are proving to be ineffective against their exploitation. I really didn't expect to live in this \"copyright for me, not for thee\" world. The same corporations that compare us mere mortals to high seas pirates when we infringe their copyrights are now getting caught shamelessly AI laundering the copyrights of others on an industrial scale. It's so demoralizing. I feel like giving up and just going private. Problem is I also want to share the things I made. To talk about my projects with real people. Programming is lonely enough as it is. Without sharing I'm not sure what the point even is. I have no idea what I'm supposed to do from now on. I just know I don't want to end up working for free to enrich trillion dollar corporations. reply dend 14 hours agorootparentI can relate to the sentiment. For what it's worth, I also know that if someone's personal site/repos/pictures are used to train AI, they have no recourse short of said person having TONS of money to go and fight the legal battles similar to how media companies do. But you know what, I grew up in a family of educators whose whole life mission was to help others by sharing their knowledge. That's what I am doing through my blog. I learned something? Blog about it. I built a reverse-engineered wrapper over some API? Share it openly. For every AI ingress job over this content there will be a few people that will read my code or blog post and either learn from it, be inspired, ignore it, or unblock themselves from a problem that they tried to solve. I think that makes the effort worth it to me. For what it's worth, even before AI emerged, I've seen sites that would shamelessly rip off my content and re-publish it on their own domains under a different author. One even tried charging people for it. On several occasions I fought it and won with the help of Google/Bing. Other times, nothing happened. And that's fine. Such is the fate of online content. If my content helped at least one person, it was worth sharing it in the open. reply pjc50 9 hours agorootparentprev> I really didn't expect to live in this \"copyright for me, not for thee\" world Having been interested in copyright activism for two decades, that's exactly what I expected. Copyright is very much about power, and concentration of power. reply bulatb 14 hours agorootparentprevYeah, I hear this. Anything I put online is feeding the machine that will replace me. Maybe I can carve myself a niche if I can find an audience, and maybe turn that into something kind of reward-shaped, but that's not happening without me feeding the machine. And almost certainly I won't succeed, and I'll just make it harder for myself and everyone like me to succeed in the future. It seems the only thing to do is do it anyway and try to be unique enough to make it work. And somehow just be fine with pulling up the ladder behind you. reply sneak 7 hours agorootparentIf a machine can replace a creative, the creative isn’t creative and should be replaced. reply matheusmoreira 13 hours agorootparentprevYeah, I'm trying too. Specifically, the GitHub Sponsors thing. I'm opposed to advertising and don't want to inflict it on others. So I don't generally advertise my work on sites like this one, I just participate in threads about it whenever I see them. Somehow people found my projects and posted them here. Just woke up one day and saw I had one sponsor. Not gonna lie, I'm still amazed about it. Not even close to providing for my family despite an incredibly favorable exchange rate, so I can't work full time on my projects. It's still the only thing that gives me hope right now. Really thankful to that person. > And somehow just be fine with pulling up the ladder behind you. Do you really think it will come to that? I mean, this AI situation has got to come to a head at some point. We can't have these corporations defending copyright and simultaneously pretending it doesn't exist while exploiting software developers. One of those things has got to go away. reply notpushkin 10 hours agorootparent> So I don't generally advertise my work on sites like this one Please do — I for one always love to hear about indie projects, if they are relevant to the topic discussed. reply alisonatwork 13 hours agorootparentprevWho cares? Information wants to be free. You put your stuff out there for free, it's hoovered up and sold back to you by capitalists, that sucks, but you've still made a real contribution to society. Meanwhile a select few will still find your stuff directly. Maybe what you shared will make just one person's life a little bit better, and that was your impact - you made a difference! Capitalists will never have that feeling, because anyone consuming their repackaged content is paying for the privilege - any benefit to society is just an incidental side-effect of their greed. Sucks to be them. The way I see it, this is exactly what life is about. Do you want to make a positive impact in society? Then share your knowledge, your experiences, your creations. People will try to capitalize on your work, and they might even get rich from it, but oh well. It doesn't take away from your own contribution to the ongoing story of humanity. I don't have or want kids, but I see my existence in society and free contributions to the \"collective consciousness\", such as it is, as my legacy. For me that's comforting. I'm choosing to be part of something bigger. If I just disappeared from society and lived like a hermit, or if I buried myself completely in my day job working for capitalists and not producing anything outside of that, I think I'd lose my sense of meaning. reply immibis 12 hours agorootparentprevThere are two ways we can go from \"copyright for me, no copyright for thee\" We can force it to \"copyright for me, copyright for thee\" by injecting AI poison and by not sharing at all. See Nightshade. Or we can force it to \"no copyright for me, no copyright for thee\" by ignoring their copyright just like they ignore ours, and making sure they don't find us. See Anna's Archive. reply notpushkin 10 hours agorootparentWe can also do “copyright for thee, no copyright for me”! It does sound a bit hypocritical, but until we see where the copyright needle goes this might be the safest option. reply TeMPOraL 11 hours agoparentprev> even if you self-host your own site, it's still going to get hoovered up and used/exploited by things like AI training bots. I think between everyone's code getting trained on, even if it's AGPLv3 or something similarly restrictive, and generally everything public on the internet getting \"trained\" and \"transformed\" to basically launder it via \"AI\", I can absolutely see why someone rational would want to share a whole lot less, anywhere, in an open fashion (...) > (...) share with other real people than inadvertently working for free to enrich companies. That attitude, quite commonly expressed on HN these days, strikes me as a peculiar form of selfishness - the same kind we routinely accuse companies of and attribute the sad state of society to. A person is not entitled to 100% of the value of everything they do, much less to secondary value this subsequently generated. A person is not entitled to receive rent for any of their ideas just because they wrote them down and put on display somewhere. Just because they touched something, and it exists, doesn't mean everyone else touching it owes them money. The society works best when people don't capture all the fruits of their labor for themselves. Conversely, striving to capture 100% (or more) of the value generated is a hallmark of the late stage capitalism and everything that's bad and wrong and Scrooge-y. Self-censoring on principle because some company (gasp!) will train an LLM model on it (gasp!!) and won't share the profit from it? That's just feeling entitled to way over 100% of the value of one's hypothetical output, and feeling offended the society hasn't already sent advance royalty cheques. Chill out. No matter what you do, someone else will somehow make money out of it, that's how it supposed to work - and AI in particular is, for better or worse, one of the most fundamentally transformative things to happen to humanity, somewhere between the Internet and the Industrial Revolution if it's just a bubble that pops, much more if it isn't. Assuming it all doesn't go to shit (let's entertain something more than maximum pessimism for a moment), everyone will benefit much more from it than from whatever they imagine they could get from their Internet comments. (Speaking of Industrial Revolution - I can understand this attitude from people who actually earn a living from the kind of IP that AI is trained on, only to turn around and compete with them. They're the modern Luddites, and I respect their struggle and that they have a real point. Everyone else, those complaining about \"AI theft\" the most, especially here? Are not them.) reply johnklos 4 hours agorootparent> The society works best when people don't capture all the fruits of their labor for themselves. Sure, but it sounds like you think people shouldn't be upset about businesses trying to capture all the fruits of people's labor, too. Capitalism is evil, and people thinking that normalizing exploitation is OK is either shortsighted or it's also evil. Are you simply unaware that this is what's happening and what people are upset about? Have you never thought about it? Or do you want businesses to succeed in exploiting people's work? It sounds like it, because you wrote, \"that's how it supposed to work\". I truly wonder if you're self-aware, or if you just think that you'll one day be on the side of the exploiters. reply yokem55 1 hour agorootparentprevThe dilemma here is that the incentive to capture value for yourself comes from the legitimate fear that someone else will try to capture all that residual value you leave on the table instead of allowing that value to be socialized in a healthy way. Which means enshitification becomes the default for everyone. reply rpdillon 2 hours agorootparentprevThis post deserves more attention, I think. It's occurred to me as well. Over the holidays, my father gave my children a book that he had written. It was a photo essay that was 50 pages, and it was titled 'Sharks'. It's an unpublished labor of love that he spent about 500 hours on. It's a true story centered on Captain Frank Mundus, who operated the Cricket II. He was a renowned shark fisherman and would take people out to fish for enormous sharks. He did this for 40 or 50 years. An author by the name of Peter Benchley wrote a novel that was heavily inspired by many of Frank's traits, his mannerisms, his approach to shark fishing, the kind of boat he had, the kind of charters he ran. The novel was titled 'Jaws' and received little attention when it was first released. A while after, a director by the name of Steven Spielberg took notice of it and turned it into a multi-million dollar blockbuster movie. My father was a lawyer that Frank Mundus consulted with and asked, is there any way that he could get a payout for being the inspiration for this character? My family read the book over the holidays, and it was clearly my father's position that Steven Spielberg and Peter Benchley were maybe the sharks that the title of the book was talking about. The idea that they could make $100 million based on the work and life of this captain and give him literally nothing in return, not even attribution, seemed wrong to him. I was the lone detractor in the room. My take is that Captain Frank Mundus was just living his life. He was doing what he did to make money chartering fishing trips for sharks. He would have done this regardless of whether or not a writer had come along or a movie had come along. What Peter Benchley and Steven Spielberg did is they found value in his work that he didn't know existed and that he wasn't capable of extracting. I think this is generally true of artists. They wander the world and they create art that gives the viewer a new insight into the experiences the artist had. If artists had to give money back to every real-life inspiration, I think the whole system wouldn't work. I see parallels with the current attitudes toward AI. I think writers are a lot like Captain Mundus. They're living their life, they're writing their stories, or doing their research and publishing, and having people read their works. And copyright is helping them do all this. AI companies have come along and found value in their work that they didn't know existed and they were never capable of extracting. And that's OK: that's what innovation is, taking the work that others have done and building on it to create something new. I'm not unequivocally in favor of all applications of AI, but I do think there are tons of places that can be super helpful and we should allow it to be helpful. One example: I'm drafting this on my phone using Futo keyboard entirely with my voice. Extremely useful, but no doubt trained on copyrighted content. reply CamperBob2 14 hours agoparentprevYou know, if I've noticed anything in the past couple years, it's that even if you self-host your own site, it's still going to get hoovered up and used/exploited by things like AI training bots. So? What do I care? If some stuff I posted to my website (with no requirement for attribution or remuneration, and also no guarantee that the information is true or valid) can improve the AI services that I use, great. reply bulatb 13 hours agorootparentWouldn't you feel just a little bad if you worked really hard on something, gave it out for free in the spirit of sharing, and someone came along and said thanks, loser, and sold it for money? Would you want to go on making it for free for them to sell? reply CaptainFever 10 hours agorootparentNo, not really? If others can get my stuff for free, then that means that whoever sells it for money must have done something to make it worth money. So they've earned it. reply prmoustache 8 hours agorootparentprevThe ones who really lose are the one who buy their stuff while yours stays free. reply CamperBob2 1 hour agorootparentprevNo. If I cared, I wouldn't have posted the information in the first place... or I would have erected a paywall. reply liontwist 13 hours agorootparentprevEven if no attribution etc is your personal policy that’s not everyone else’s. The end result is that any authors who care about copyright protection will become less accessible. It’s a gold rush for AI bots to capture the good will of early internet creators before the well runs dry, reply dend 13 hours agorootparent+1 My content is still MY content, and I'd prefer that if an entity is going to make money off of it directly (i.e., it's not a person learning how to code from something I wrote but rather a well-funded company pulling my content for their gain), that I at least have some semblance of consent to it. That being said, I think there is no longer a point of crying over spilled milk. The LLM technology is out of the bag, and for every company that attempts to ethically manage content (are there any?) there will be ten that will disregard any kind of license/copyright notices and pull that content to train their models anyway. I write because I want to be a better writer, and I enjoy sharing my knowledge with others. That's the motivation. If it helps at least one person, that's a win in my book, especially in the modern internet where there's so much junk scattered around. reply CamperBob2 1 hour agorootparentprevEven if no attribution etc is your personal policy that’s not everyone else’s. That's up to the courts. As usual, we will all lose if the copyright maximalists win. reply AnthonyMouse 13 hours agorootparentprevI think the source of the contrary sentiment goes something like this: AI stuff (especially image generation) is competition for artists. They don't much like competition that can easily undercut them on price, so they want to veto it somehow and lean on their go-to of accusing anybody who competes with them of theft. The problem in this case is that it doesn't matter. The AI stuff is going to exist, and compete with them, whether the AI companies have to pay some pittance for training data or not. But the chorus is made worse by two major factors. First, many of the AI companies themselves are closed-source profiteers. \"OpenAI\" stepping all over themselves to be the opposite of their own name etc. If all the models got trained and then published, people would be much more inclined to say \"oh, this is neat, I can use this myself and it knows my own work\". But when you have companies hoovering everything up for free and then trying to keep the result proprietary, they look like scumbags and that pisses people off. Second, then you get other opportunistic scumbags who try to turn that legitimate ire into their own profit by claiming that training for free should be prohibited so that only proprietary models can be created. Whereas the solution you actually want is that anybody can train a model on public data but then they have to publish the model/weights. Which is probably not going to happen because in practice the law is likely to end up being what favors one of the scumbags. reply dend 12 hours agorootparentI think that's an overly reductive way of looking at it. Artists, are by their definition, creators of art. AI-generated \"art\" (it's not art at all in my eyes) is effectively a machine-based reproduction of actual art, but doesn't take the same skill level, time, and passion for the craft for a user to be able to generate an output, and certainly generates large profits for those that created the models. So, imagine the scenario where you, an artist, trained for years to develop a specific technique and style, only for a massively funded company to swoop in, train a model on your art, make bank off of your skill while you get nothing, and now some rando can also create look-alikes (and also potentially profit from them - I've seen AI-generated images for sale at physical print stores and Etsy that mimic art styles of modern artists), potentially destroying your livelihood. Very little to be happy about here, to be frank. It's less about competition and more about the ethical way to do it. If another artist would learn the same techniques and then managed to produce similar art, do you think there would be just as visceral of a reaction to them publishing their art? Likely not, because it still required skill to achieve what they did. Someone with a model and a prompt is nowhere near that same skill level, yet they now get to reap the benefits of the artist's developed craft. Is this \"gatekeeping what's art\"? I don't think so. Is this fair in any capacity? I don't think so either. Because we're comparing apples to pinecones. All that being said, I do agree that the ship has sailed - the models are there, the trend of training on art AND written content shared openly will continue, and we're yet to see what the consequences of that will be. Their presence certainly won't stop me from continuously writing, perfecting my craft, and sharing it with the world. My job is to help others with it. My hunch is that in the near-term we'll see a major devaluing of both written and image material, while a premium will be put on exceptional human skill. That is, would you pay to read a blog post written and thoroughly researched by Molly White (https://mastodon.social/@molly0xfff@hachyderm.io) or Cory Doctorow (https://pluralistic.net/), or some AI slop generated by an automated aggregator? My hunch is you'd pick the former. I know I would. As an anecdotal data point, and speaking just for myself, if I see now that someone uses AI-generated images in their blog post or site, I almost instantly close the tab. Same applies to videos on YouTube that have an AI-generated thumbnail or static art. It somehow carries a very negative connotation to me. reply AnthonyMouse 3 hours agorootparent> It's less about competition and more about the ethical way to do it. If another artist would learn the same techniques and then managed to produce similar art, do you think there would be just as visceral of a reaction to them publishing their art? Likely not, because it still required skill to achieve what they did. Now suppose that the other artist studies to learn the techniques -- several of them do -- and then Adobe offers them each two cents and a french fry to train a model on it, which many accept because the alternative is that the model exists anyway and they don't even get the french fry. Is this more ethical somehow? Even if you declined the pittance, you still have to compete with the model. Even if you accept it, it's only a pittance, and you still have to compete with the model. It hasn't improved your situation whatsoever. > My hunch is that in the near-term we'll see a major devaluing of both written and image material, while a premium will be put on exceptional human skill. AI slop is in the nature of \"80% as good for 20% of the price\" except that it's more like 40% as good for 0.0001% of the price. What that's going to do is put any artists below the 40th percentile out of work, make it a lot harder for the ones at the 60th percentile and hardly affect the ones at the 99th percentile at all. But the other thing it's going to do is cause there to be more \"art\". A lot of the sites with AI-generated images on them haven't replaced a paid artist, they've replaced a site without images on it. Which isn't necessarily a bad thing. reply CamperBob2 1 hour agorootparentprevAI-generated \"art\" (it's not art at all in my eyes) is effectively a machine-based reproduction of actual art, but doesn't take the same skill level, time, and passion for the craft for a user to be able to generate an output, and certainly generates large profits for those that created the models. (Shrug) Artists were wrong when they said the same thing about cameras at the dawn of photography, and they're wrong now. If you expect to coast through life while everything around you stays the same, neither art nor technology is a great career choice. reply panarky 14 hours agoparentprev> how to segment communities locally So it's not about owning vs. renting property on the internet, it's about controlling the roads that connect the properties so you can keep the world out of your community. reply NitpickLawyer 12 hours agorootparentHa, I have the same feeling on the recent good_social_network vs. bad_social_network debate that kinda goes on in the US. Looking from outside, it always felt that the main problem is control, and wanting more of it. The details, principles and \"politics\" don't matter in the grand scheme of things, it's control that people want, even tho they paint it differently. bad_social_network was good 10 years ago, because it was controlled by \"a friend of ours\". Now it's controlled by someone who's perceived as \"a friend of theirs\" and it's therefore bad. So the politik aktivists move to good_social_network, and rave about the good there. Echo chambers be damned, we have control. Until the next \"friend of theirs\" buys it out, and rinse and repeat. So silly. reply frabcus 10 hours agorootparentOne of those social networks has a protocol and lets end users make their own feed algorithm and moderation system. The other never has. There can be technical differences between networks as well as social. reply cxr 9 hours agoparentprevIt'd be great if you folks would stop showing up and derailing the comments with threads like this. reply xyzzy_plugh 15 hours agoprevThis all sounds like renting to me. Instead you should: - put a rack in your home - buy an IPv4 block - buy dark fibre - start your own ISP - advertise routes over BGP - host your own email - found a registrar and transfer your domains over All easily obtainable for less than a million dollars in capital. Though once your FTTH and undersea cable operations ramp up you'll need further access to capital. reply greyface- 14 hours agoparentYou're still renting your ASN and IP space from ARIN/RIPE/etc, and they'll be revoked if you stop paying yearly dues. For true independence, you need to establish a base in international waters and establish a new RIR in accordance with ICANN ICP-2. https://www.icann.org/resources/pages/new-rirs-criteria-2012... reply dend 15 hours agoparentprevYou forgot the steps of pressing your own silicon wafers in the garage, followed by soldering motherboards from zero, and of course, having a steel mill to build racks and computer cases. The only way to ensure true independence. In all seriousness, I am a big advocate for hosting your own infrastructure as much as you can, but that requires you to be REALLY into doing it. Otherwise, it's just a completely unnecessary chore. reply xyzzy_plugh 15 hours agorootparentNo, that comes far later, after energy self-sufficiency, at which point your operational expenses can be minimized. Then we can start talking about going fully vertical to reduce our supply chain dependencies. If you're concerned about silicon dependencies, I'd recommend starting with some real estate purchases (mines) and forming a Special Economic Zone first, though. Remember this is about being an owner and not a renter. The only restriction is our recurring costs, but we have unlimited room upfront, like a Lincoln. reply TeMPOraL 11 hours agorootparentprevFocusing the wrong problem. You'll also need to own everything that being a part of a nation state provides you, including markets and defense. If you can't do that, you can't live truly rent-free. reply softwaredoug 15 hours agorootparentprevDon’t forget creating a society capable of supporting industry and science. On a planet producing enough of the raw materials. In a Universe with appropriate fundamental physical constants etc etc. reply The_Blade 15 hours agorootparentIf you wish to make an apple pie from scratch, you must first invent the universe. reply richardw 14 hours agoparentprevThe second you have a domain you can move it where you want. Change the software, look and feel. You keep subscribers. You don’t need to own every atom, just enough to give you options. You can’t move your X or FB account. They can block you anytime, or reduce traffic. Way fewer options. reply jasonjayr 14 hours agorootparentRecent story regarding itch.io: https://news.ycombinator.com/item?id=42363727 Just owning your own domain, minding your own business, doesn't guarantee that it won't be taken down on a whim. reply richardw 12 hours agorootparentOne of the top comments on BSky shows they got it back up fairly quickly. Try that when running on Google, say. If your business is on YouTube and they decide you’re dead, you’re dead. “But also, as someone who relies on your site, thank you so much for handling this in such a quick and effective way. Waking up and seeing that the site is already back up despite this all makes me proud and grateful to be on itchio.” Point is that you can choose your own adventure in a way that eclipses fully relying on another company. Beyond that, you can choose to own as many layers as you want and stop long before building your own hardware and fiber network. reply dend 14 hours agorootparentprevNot impossible, but the likelihood of a domain being taken away on a whim compared to a social network deciding to \"shadowban\" or just disable an account on their network is significantly lower. Also, with a domain you might have a legal recourse to pursue with the registrar/registry. With a social network account? You're on someone else's turf, and they can do what they want. reply paxys 13 hours agorootparentprevHaving control over a domain name doesn't mean all that much when the data hosted on it can be hijacked or held hostage at any point. reply dend 12 hours agorootparentIt offers significantly more flexibility and freedom compared to any social network. If your data is \"hijacked\" (not sure that that means in this context, but let's assume the host terminated your account), you can spin up another hosting account on one of the many hosting providers and point your domain to it. That's it (not to say that it's that trivial for large sites, but that's the gist of it). If your account on a major social network is terminated, if you had a large community there, you have quite literally no way to access them unless you had some kind of parallel presence somewhere else. reply richardw 12 hours agorootparentprev“Just enough to give you options” means you choose how much you need to own, and once you have the domain you can choose the rest. Back your data up, choose tech you can move. The point is that you don’t need to buy every single piece of infrastructure. Compare any of this to ruling your business purely on any platform whose domain you don’t own. reply chilli_axe 11 hours agoparentprevTo truly self-host anything, you must first invent the universe. reply Suppafly 14 hours agoparentprevI really wouldn't mind doing steps 1-3 & 6-7 if it were easier. I don't really care about being an ISP though. reply greenavocado 15 hours agoparentprevIt's all fun and games until you need to deal with the right of way reply chii 14 hours agorootparentthe way to deal with the right of way is \"might makes right\". As it always has. reply dylan604 15 hours agorootparentprevor boats with dragging anchors reply RicoElectrico 4 hours agoparentprevThe difference is that if you're not in a walled garden, all the elements of the stack could be replaced. Obviously one needs to avoid big3 clouds and adversarial domain registrars who make it difficult to transfer domains. reply naming_the_user 15 hours agoparentprevStep 1: Simply create your own Starlink. reply sdwr 15 hours agorootparentTo host a raspberry pi from scratch, you must first create the internet reply thfuran 15 hours agorootparentprevStep 1 is always \"Create the universe\". reply drewcoo 15 hours agorootparentAlice: I have an urge. Bob: Oh yeah? I have a demiurge! Alice: I feel we're not really communicating. reply nayuki 15 hours agorootparentprevYou need to constantly launch new satellites as old ones undergo orbital decay and run out of fuel. reply sneak 7 hours agorootparentTo do this you need a cozy relationship with the satellite launch regulator in your country of operation. reply immibis 12 hours agoparentprevDon't let the perfect be the enemy of the good. There are also Tor hidden services, where the dependency on lower layers still exists but they can't find which is yours. reply dusted 11 hours agoprevSo this is a blog stating to be an owner and not a renter, and then proceeds to talk about how to rent hosting.. Sorry, but if it's not your hardware, on your property, then you're renting. Regulations have been waay too lose on, especially, american ISPs where I understand they are allowed to not only refuse you a public routable IP but also dictate what kind of traffic you're allowed to send and receive (for example, whether the traffic flowing is of \"commercial\" character and therefore should be on on a different subscription), this insanity should be illegal. Internet is a utility, and everyone should have the right to the same type of access, regardless of their need (those who do not need/want, can simply chose not to use it, but ISPs should not be allowed to differentiate). I've hosted my own web, and other servers on my own hardware since I was 13 years old, when I bought my first domain, I had to use a fax machine for the first time in my life, and fax my request form, along with my passport, to the agency responsible for the top level domain of my country. It was kind of convoluted back then, but everyone were helpful, and it was not that difficult, the technology was well understood, supporters were competent, and it was expected that people were going to use the internet for internet things. Today is my 39th birthday, and while the server hosting my stuff is mostly still located 3 meters from me, the path to having it online has nothing but degenerated, it's an uphill battle just to be on the internet these days.. The mail stuff is the easier part (dkim, dmarc, spf, certificates).. But the simple act of getting your f..king computer connected to the f..king internet like it was 1999, that's the real hassle.. ISP NAT, supporters beyond incompetent, blocked ports, missing (or unknown) relay hosts.. It's a joke. reply dend 10 hours agoparentI generally agree with the sentiment (and yes, internet should be treated as an utility). However, the reality is that the vast majority of people will not be able to self-host on their own hardware for a myriad of reasons (lack of skill, lack of money, lack of interest, etc.) That's not a reason to gatekeep them from having their own corner and claim it as theirs. If you have a domain and your own site, even hosted on a colocated rack or in the cloud, you're already miles ahead of those that don't. And if you have a domain and can manage DNS records, then in the future that doesn't preclude you from \"graduating\" to your own hardware, if you so desire. The goal here is more or less self-sufficiency with web properties rather than a pure interpretation of \"rent\" vs. \"own.\" Because at some point you have to rent something from someone (say, you're not running your own domain registry and registrar). reply dusted 10 hours agorootparentI don't want to gatekeep, I want to gate-unkeep! The way things are going, we're divinding the people and the companies into two classes, with the former having fewer rights and privileges than the latter. I want everyone to have the RIGHT to participate in the Internet, should they have the interest to learn how to do it. That right is under pressure when we accept this division, when we use the excuse that \"most people don't know how to\", to justify taking away everyones right to even try. If only companies have the right to participate on the internet, they are empowered even more to chose who should be allowed to even run a website.. It's a slippery slope that ends up in a very bad place, participation wise. It becomes like the airline industry, where the companies pushing hardest for more regulation and red-tape are the oldest, those who made their fortunes back when it was easier and cheaper, and who now use their enourmous wealth to make it harder for new players to enter their market. It's the same everywhere, when you start allowing power to concentrate. reply BlueTemplar 8 hours agorootparentprevThere is a simple way though : have the ISPs provide all of this. If they can provide you a personal website, an email account and a NAS, they can also provide you a a personal website and an email account ON that NAS. (Especially now, with IPv6.) (Which of course assumes that there are laws in place against lock-in, just like there are already laws in place against lock-in for your pick of ISPs and obligations for mobile carriers to transfer your phone number to another carrier.) reply dusted 7 hours agorootparentI think this only shifts the problem, the whole idea with the internet is a distributed network of computers that talk with each other, and if the computers at the edge (end users) can't do that, then it's no longer the internet, it's something else, more akin to cable-tv where there are \"providers\" and \"consumers\". The playing field stops being level. reply BlueTemplar 5 hours agorootparentWell, yes, I am specifically calling for ISPs to build this edge infrastructure in people's homes, so I don't understand your point ? reply dusted 4 hours agorootparentAh, I took it as you suggesting the ISPs providing VPS services for people.. Thing is, that edge infrastructure has been there from the beginning of broadband and is only recently beginning to slip away, with the advent of ISP NAT, agressive IP rotations, blocking of ports and not providing public IPs at all. reply sneak 7 hours agorootparentprevISPs don’t want lots of customers sending lots of data. Their model is based on millions of dumb consumers downloading (only) from the same 20 ASes. reply IMSAI8080 9 hours agoparentprevI think the sentiment was to keep your content (and audience) portable, not specifically that you don't rely on anyone else's services. If you post everything on Twitter and Twitter decides they don't like you, then that's the end of you. If you host on a personal domain and your rented web host decides to block you, there's plenty more options and you can take your audience with you and they will never know the difference. reply dusted 7 hours agorootparentI know it was, but the click-baity headline makes it seem like you're becoming independent of the whims of private companies, which you are very much not if you're renting a host somewhere. You're definitely not a property owner, you might be a domain owner (renter still, I don't know of any domains which you can pay for and keep forever). reply TheCoelacanth 3 hours agorootparentAs long as the services you're renting are commoditizated, you are independent of their whims. You can easily replace a VPS provider with a different provider that will give you exactly the same service. You can't replace Facebook with a different Facebook. reply franga2000 7 hours agoparentprevThere's a huge difference between being a renter on a VPS or on a social network platform. Youtube can demonetize or delete a channel and the creator is more or less fucked. They can find another platform, but they need to build their audience almost from scratch. By contrast, if my VPS provider kicks me out, I just clone it or restore from backup to any one of the thousands of competing providers, change a few DNS records and my audience (not that I have one) wouldn't even know that anything changed. Servers and domain names are transferable and neutral, platforms and usernames aren't. reply iamacyborg 10 hours agoparentprevI started to self host my own personal website last year and found it relatively easy. I may have been somewhat lucky with my ISP and that I was already paying for a static IP but the hardest part was setting up cloudflare to mask my IP (and learning how to setup a Linux VM from scratch). reply sneak 7 hours agoparentprevHosting is a service, not a product. reply muglug 16 hours agoprevThis is good advice for people whose livelihood depends on the attention of anonymous strangers. For everyone else, it’s probably ok to ignore. reply pcloadletter_ 15 hours agoparentIt's also just kind of cool and fun to hack together a personal website reply dend 15 hours agorootparentTotally. If I would tell 10-year-old me that I have my own website on my own domain, it would be seen as a shocking development. I find it really cool to be able to have a corner of the internet that is just mine. reply hackernewds 12 hours agorootparentThis is practically everyone's Facebook Instagram Whatsapp Airbnb anything at all profile already reply arminiusreturns 12 hours agorootparentprevI still get a rush with a new server on the internet!!! (especially my latest host: 12 core new gen epyc instances with 48G RAM.) reply anarwhal 9 hours agorootparentprevYes, and we're on _Hacker_ News after all - doing this just for fun should be sufficient reason in and of itself reply dend 15 hours agoparentprevYou don't even need attention - it could be a public blog where you share about the things you learn. You never know who it's going to help. That's primarily my motivation with the blog. reply dni0 13 hours agoparentprevIndeed. If you only use social media to connect with friends and family... good luck getting them to visit literally any other website. reply easterncalculus 15 hours agoprevIn five letters: POSSE https://indieweb.org/POSSE reply dend 15 hours agoparentSpot-on - I call it out at the end of the article. Principles are broadly the same. reply indigodaddy 14 hours agoprevWhat this guy doesn't really get is that people don't want their own websites. What the hell would they do with a website? They love social media platforms. The why or where of how we got here doesn't really matter, it's the reality. I think the only way forward is better and less evil social networks, or perhaps some sort of cyclical sea change where the Internet sort of starts over again, ala how civilizations/humanity have every XX thousands of years according to various esoteric Sikh/Hindu traditions. reply dend 14 hours agoparentThat's a fair call-out, but that's why in the post I mention this: > Most people are perfectly content with everything living inside their Facebook account because it’s convenient and their family and friends are already there. Telling everyone to learn how git and GitHub Pages work to host their blog is not an effective way to drive change. But that’s also not the point. As I mentioned earlier, the goal is to start with a small niche community of people who are comfortable with building their own digital corners. Not everyone needs to have their website. Not everybody wants to have their website. And that's fine. They can use the social platforms as-is. But for those that have the means and interest in building their own corners (say, they want to bootstrap a business), they should not limit themselves to the social networks as the only place for their community. There is a better way than be a sharecropper on someone else's land. reply BlueTemplar 8 hours agorootparentWhat I don't understand is why you would think that GitHub Pages is an acceptable alternative : it's kind of the equivalent of being a doctor and recommending Oxycodone to cure hangover for someone of alcoholic tendencies... reply Pooge 3 hours agorootparentBecause you can get your own domain in front of it and export your content (i.e. Markdown). Users would land on your domain name, but it doesn't really matter if it's hosted on your PC or GitHub. The point is that you would be able to change providers very easily without having to make people migrate to your new platform. reply BlueTemplar 2 hours agorootparentAnd it doesn't try to suck you into the Microsoft \"ecosystem\" ?? reply Toutouxc 11 hours agoprevOh what a delight, the cartoon animal character in the infoboxes is “Krteček” [0], a popular (and quite old) character from my country. The website says that the guy is from Washington, but his name does sound vaguely Slavic. Interesting. [0] https://en.wikipedia.org/wiki/The_Little_Mole reply cenamus 11 hours agoparentDelimarsky sounds more than just a bit Czech/Slovak :) reply dend 10 hours agorootparentHah! Well that certainly is a good directional guess! I am of Eastern European descent. Also, big fan of Krtek - grew up on that cartoon, was quite an educational experience at that age. reply cenamus 7 hours agorootparentAh yes, with transliteration that assumption goes out of the window haha reply rednafi 10 hours agoprevStarted my blog[1] around five years ago and never looked back. Before that, I got burned by Medium and decided: never again. Maintaining your own little corner on the internet is easier than ever, though I agree it still requires some technical know-how. Also, I’m not sure how much I trust these WYSIWYG blog engines like Squarespace or Weebly. [1]: https://rednafi.com reply xenodium 13 hours agoprev> the vibrant ecosystem of blogs, feeds, personal sites, and forums has been usurped by a few mega-concentrated players. I’m trying to do my bit for the web at https://lmno.lol Started a blogging service that doesn’t do things like the big players. reply veltas 12 hours agoprevThis article recommends .eu domains. There is a caveat to this, in the unlikely event your state leaves the EU you will be forced off, this happened to many UK entities after Brexit, they were forced to stop using their .eu domains. reply wiether 3 hours agoparentFurthermore, as a French person, and thus a European citizen, I must say that the .eu TLD is not popular and usually bears a _political_ meaning. Like it's used by institutions or organizations linked to the EU by their activity, but it's rarely used by companies or individuals whose activity is not focussed on the EU. Companies are still going to buy the .eu domains associated with their brands, but they will communicate with another TLD like .com or will provide located versions of their site under a country TLD like .fr for the French version and .de for the German one. What I see the most is: - country TLD for content that is located - .com/.net or weird (like .dev) for content that is in English or in multiple languages reply dend 12 hours agoparentprevFair point - I will add that as a clarification, thank you for calling it out. reply econ 13 hours agoprevWe shouldn't complaint as it is all our fault. (haha) I think OPML is the technology. Go build one and share it. Writing or recording your own stuff is a lot of work. Help promote all the cool things you've found outside the walled garden of plastic plants. Build a website for someone, teach them html for 20 minutes and set up a domain, hosting and an ftp client for them. They can always call you if they get stuck. reply dend 13 hours agoparentThat's the strategy I have with this blog. Instead of sitting and lamenting over the state of the internet, it's better to spread the word and encourage action. There are actual things we can do to reclaim parts of the web that died down. It doesn't need to be at the same scale as all the social networks out there, but even if it serves a small niche, that's a fantastic achievement. reply anarwhal 9 hours agoprevOne other way to look at some of this (especially with the other comments around whether anything is \"truly\" owned) is in terms of \"redundancy\". TFA touches on some of the control/portability side but the thought experiment here is something like: how many people am I depending on? If you owned den.dev and den.xyz as mirrors on separate providers that's one less failure point where someone else has the power to disappear you (etc, all the way through to redundant underwater sea cables, I guess!) reply psychoslave 9 hours agoprevWell, we literally can’t buy a domain name only rent it, and owning a TLD or possess a static numerical address is not an option that is accessible to mere mortal either. reply seanw444 9 hours agoparentWould be nice if something like Namecoin could gain traction. reply xixixao 11 hours agoprevGood advice, and pretty standard setup I’d say, even for companies that are trying to build a community. The one thing I wonder about is whether younger generations will use mailing lists. I never did and I’m already in my mid-30s. reply dend 10 hours agoparentShockingly, they do! Quite a few folks that I've talked to recently expressed that they are subscribed to more than one email newsletter and read them fairly consistently. reply sshine 16 hours agoprevtl;dr: - Don't depend on other people's software services. - Buy a domain and host your own website. - Don't pick a sketchy TLD or registrar. - Mailing lists beat social media accounts. - It's okay to depend on a cloud. I had the belief that the article was going to say the exact opposite wrt. cloud hosting. You're literally renting space, and if your stuff gets any heat, your cloud provider may simply shut you down without a trial. Even if you host your own server on your own legal property, most people don't have AS-numbers and peering agreements, so ultimately on the internet most people rent something. reply Retr0id 15 hours agoparentMy interpretation of their words on the cloud front was more like, don't depend on or become locked into any specific service. If you're using Azure to host a VPS, you could easily move it to AWS or, Hetzner, etc. Likewise, if you're using Cloudflare as a CDN (and only as a CDN!) there are other CDN providers available that you could switch to with relative ease. reply andrewon 15 hours agoparentprev> - Don't depend on other people's software services. > - Buy a domain and host your own website. I had the exact chain of thought, only to find that traffic of the site I built is at the mercy of how Google decided to rank webpages, and putting AI > youtube > Reddit in front of everything else. > - Mailing lists beat social media accounts. Similarly, Google set the metric for what counts as spams. Your emails can all go to the spam folder if their AI decides it should. reply dend 14 hours agorootparentOn (1), yes that is true to an extent - the domain discoverability is indeed mostly at the mercy of Google, and the whole \"AI overview\" is a garbage experience. However, looking at my own search console data (both Google and Bing), there are still quite a few folks landing on my pages through search, often for some obscure terms that I somehow documented, so it's still possible to get traffic that way. But again - the goal is less about \"drive traffic ASAP\" but rather point people from other networks to something you personally own. On (2), they can, but if you use a more established provider like Buttondown or Mailchimp, and you are not actually sending spam, a lot of folks have quite a bit of success building an audience that way. I've used Buttondown (not affiliated with them in any capacity) personally before and haven't had subscribers complain about deliverability. I am planning on rebooting that this year to see how it goes. I've heard most deliverability issues arise when folks trying to roll out their own email server. reply ghaff 15 hours agoparentprevUltimately you're on the Internet and you don't own the Internet. At that point, you're making decisions about the level of control you want to have and the types of events you may be subject to and the answer is almost certainly \"it depends.\" reply dend 15 hours agorootparentExactly this. At some point, you need to delegate. Make it portable, don't get locked into a proprietary architecture, and you'll be good. Not everyone will be able (or has any desire) to run their own rack. reply lylejantzi3rd 16 hours agoparentprevI expected him to mention colocation. Kids these days. shakes his cane. reply rmoriz 15 hours agorootparentA homelab with an useable uplink can be sufficient for many services like blog,DNS, mail. I have 3 Lenovo ThinkCentre Mini PCs running Proxmox VE in HA mode off my basement. Picture at https://devops.science/ reply sobkas 15 hours agorootparent> A homelab with an useable uplink can be sufficient for many services like blog, DNS, mail. I always felt like you are painting target on your homelab when you allow outside access. reply rmoriz 15 hours agorootparentYou are. I'm tunneling a /23 which I let Vultr announce via BGP over WireGuard to a local router VM. I have a nftables firewall in place before routing the traffic through the tunnel. I block everything except for exposed IPs and ports/protocols just to keep my limited bandwidth free of noise. reply dend 15 hours agorootparentprevYou do. That's why I wouldn't recommend it to anyone unless they absolutely know what they're doing. Can't tell you how many friends I had to have a talk with who had plain vanilla port forwarding done on their home router, exposing their entire home network to the web. Nowadays, I recommend them use Tailscale as an out-of-the-box Wireguard-based VPN to safely connect to their home servers from remote locations. reply rmoriz 8 hours agorootparentTo be honest, as an IT professional you should have basic knowledge about firewalls. nft/nftables is a big improvement in firewall usability for Linux, I also know many homelab people using OPNSense or even DD-WRT for that job. I prefer plain Linux (distro of your choice, I don't judge) and nft. Tutorials: - https://wiki.gentoo.org/wiki/Nftables/Examples - https://wiki.archlinux.org/title/Nftables - and probably the best advanced tutorial is a video series https://www.youtube.com/watch?v=K8JPwbcNy_0&list=PLUF494I4KU... TL;DR One should know firewall fundamentals, nft/nftables as successor of iptables is very convenient to use, a single config document instead of interactiving with 100 cli commands which have to be in a specific order. reply immibis 12 hours agoparentprevThe insidious part of networking is that you cannot be on a network without agreeing with everyone else on the network. It's simply not possible. reply paweladamczuk 11 hours agoprev> e-mail is a universal protocol I wonder how long that will be true, considering how difficult hosting your own server for your own domain is these days. reply fitsumbelay 11 hours agoprevthis should be standard for every human being not just techies the LOE to self hosting and adding infra on demand should also be push button easy the good news is that it seems to get easier and cheaper as time passes, which makes it feel inevitable but obstacles remain because of corporate business interests of course reply Almondsetat 10 hours agoprevA bit funny to see \"owner\" and \"buying a domain\" next to each other reply dend 10 hours agoparentYeah, unfortunately, c'est la vie. Renting a domain is still better than not having it. reply rednafi 10 hours agoparentprevYeah, I wish there was a way to buy domains; not just rent them. It’s not like they’re limited in supply. For example, you kind of own your email addresses. reply BSOhealth 15 hours agoprevI have vr.dev and want to do something meaningful with it, additive to the community, and not just a trash content blog. any ideas? who’s down? reply gavinhoward 14 hours agoparentI just beat a video game for the first time in my life. It was such a confidence boost and exactly what I needed because I bought the headset to fight depression (exercise). I realized that VR, as a medium, is uniquely tuned to people, and I think you would have a great blog if you focused on just that: people. What are some ideas for how can VR help people? Can it help people fight depression? Can it be used for physical or emotional therapy? Can it help them safely build skills that could improve their lives? On that last one, I am reading a book about ship handling. It was a Christmas present because I will never be a captain in my life. But ship handling would work so well in VR. Even the commands are so standardized that players could give voice commands, a real bridge. How many Make-a-Wish kids or others could have a wonderful time doing that? And that isn't even considering the idea of extending that game into space with Star Trek USS Enterprise or Star Wars Star Destroyers. Your site could be the nexus of those great ideas. You could even have guest posts. I'd write one. Anyway, sorry for the novel. VR has helped me feel better than I have in a long time. reply liontwist 13 hours agoparentprevYour question is kind of backwards, as an interesting website idea is more of a project than a good domain name. But here goes: Document every VR headset released with technical details and references reply vonunov 14 hours agoparentprevslap up a mediawiki and let people contribute VR dev documentation reply dijit 14 hours agoparentprevsounds like it would be an amazing domain for a forum related to development of VR projects. reply wakawaka28 15 hours agoprevhttps://landchad.net/ reply dend 15 hours agoparentGreat resource, thank you for sharing! I should include it in the blog post footnotes. reply mtsolitary 10 hours agoprevUnfortunately it’s literally impossible to own and not rent your domain :( reply dend 10 hours agoparentIt's true, and yet it's still a better alternative to not having your own domain at all (at least IMO). reply bithead 3 hours agoprevSocial media makes me miss USENet reply wavewrangler 14 hours agoprevRules to live by… Although I swear this was posted on here just recently, was it not? reply dend 13 hours agoparentHonestly, while I put this together, I know that the ideas on this are not unique to me by any stretch. It's just another arrow in the bunch. I added a few extra reading materials/references at the bottom of the article. reply cyberax 14 hours agoprevOther notes: use projects that are either supported by a community, or have a motivated developer (buy their a Patreon sponsorship if you can). reply dend 14 hours agoparentAbsolutely. I am a big fan of supporting independent software and bootstrapped services. reply drewcoo 14 hours agoprevThis used to be a forum for startup-folk, not wanna-be-FAANG-employees. There's a lot of sense in this post. There's not a lot of sense in the reaction here. reply scarface_74 14 hours agoparentWhat do you think the goal of “startup folks” were if not to get acquired by FAANG or other BigTech adjacent companies? Out of the literally thousands of companies that YC has invested in, only about a dozen have gone public. YC is not interested in “lifestyle companies” that are a profitable ongoing concern it is interested in the “exit” reply moron4hire 12 hours agorootparentYeah, no, that's definitely not the story YC sells though. reply pembrook 14 hours agoparentprevAn accurate rebrand could be renaming this place from “hackernews” to “middle manager IT dad news.” reply johnklos 4 hours agoprevThe author makes some good points, and the overall message is good, but: Why would we care so much about being a property owner and not renter if we don't care about whether a hosting company can just turn us off? Why would we care so little about privacy that we'd willingly use services that negate privacy and introduce tracking? For instance, the author suggests using Cloudflare, Azure, AWS and more, and none of these aren't abusive. Funnily enough, the Cloudflare hosted images on the page didn't even load for me;) Does the author not know that Mastodon is software and not a social media network? Sure, it's a minor thing, but when people use terms incorrectly, it makes me wonder if they really know what they're talking about (people who'd rather fight about whether it's a term \"everyone uses\" instead of whether it's correct seem to not get this point). The author writes (or quotes) somewhat derisively: > Well of course it’s better to host your own blog! Also, while you’re at it, put your Mastodon server in a DigitalOcean droplet, throw some Cloudflare CDN in front of it, run your own Raspberry Pi to monitor uptime, and you’re golden! Oh, and don’t forget to also make sure to log into the droplet every once in a while to update the container, do an occasional database migration, and ensure that you check the logs for intrusions. This all-or-nothing attempt to make the idea of self-hosting seem ridiculous is by itself ridiculous. Nobody needs to do all that. On the other hand, the audience for what the author is advocating shouldn't have a problem setting up a simple machine or VM instance to host a web site, blog, perhaps even DNS server themselves. All or nothing is silly, so even making this example reduces credibility. All in all, I'd love to see people take ownership of their own things on the Internet. Nobody needs to self-host, but people should if they could, and they should ignore people who say it's too complex because most of the too-complex argument is the suggestion that it needs to be much more than it does. A single VM, a single small computer, even a Pi, can host most things. But whether people self host or use someone else, choosing where to host matters. Don't use companies that negate the benefits of owning your own things, whether by lock-in or by letting them do all the tracking that Facebook would normally do. Don't use companies that'll disable your account because some idiot wrote a letter. Don't use companies that're so big that you can't talk to a human! This article makes for an odd juxtaposition between doing thoughtful things and doing things that negate some or much of that thoughtfulness. I'd have a hard time recommending it to others without qualifications. reply chromanoid 9 hours agoprevEver heard of cooperatives? I think digital coops are the only really feasible way away from platform capitalism. e.g. https://www.hostsharing.net/ reply epa 16 hours agoprev [–] Ctrl+F \"Bitcoin\", 0 results. reply Terr_ 16 hours agoparentGood! Beyond the many usual critiques, in this context \"owning bitcoin\" doesn't mean much, barely a step above having a hoard of hidden gold bars. While it is indeed more \"on the internet\" than precious metals, possessing that speculative-asset does not provide any special niche, ownership, or control over the broader, er, cybernetic means of production. reply kinakomochidayo 16 hours agoparentprevWhy would anybody own an asset with unresolved security budget issues? reply cyberax 14 hours agoparentprev [–] There are also zero NFTs. It's a solid guide, in other words. reply GuidelinesFAQListsAPISecurityLegalApply to YCCon",
    "originSummary": [
      "The article stresses the importance of owning your digital space instead of relying solely on major internet platforms, which often prioritize their growth over user interests. - Strategies for maintaining control over your online presence include owning a domain, diversifying content across multiple platforms, and building direct connections with audiences through email lists. - By creating your own web property, you ensure flexibility and resilience in your digital endeavors, reducing dependency on any single network."
    ],
    "commentSummary": [
      "The discussion emphasizes the importance of owning one's digital presence instead of depending on social media platforms, addressing concerns about AI exploiting user-generated content. - It explores the complexities of copyright, AI ethics, and the significance of having a personal website or domain for better control over online data. - The conversation also considers the practicality and limitations of self-hosting, given the dependence on external services like domain registrars and hosting providers, advocating for more independence in managing one's online presence."
    ],
    "points": 295,
    "commentCount": 211,
    "retryCount": 0,
    "time": 1735867317
  },
  {
    "id": 42583086,
    "title": "BlogScroll – An open directory of personal sites and blogs",
    "originLink": "https://blogscroll.com/",
    "originBody": "BLOGSCROLL Add Your Site Surprise Me Welcome to BlogScroll! 👋 This is an open directory of personal sites and blogs, maintained entirely on GitHub. This project was created by Den Delimarsky in an effort to bring attention to little 🌱 digital gardens and ✨ personal corners of the internet that people maintain outside the \"Big Tech\" walled gardens. We're all better off maintaining homegrown corners of the Internet. If you prefer not to use GitHub to make your contribution, you can also send me a note to hi@den.dev and I will happily review and publish it. You can also subscribe to the RSS feed to stay up-to-date on latest additions. design-and-ux 61 Entries Adarsh DK Adi Chakravarty Alessandro Mingione Alex Baldwin Allison Grayce Marshall Ánh Nguyễn Ariel Norling Arjun Arjun Mahanti Barbara Schussmann Billy Tseng Brad Cerasani Carl Barenbrug Charlie Deets Craig Hooper Danilo Tanic Diego Alarcon Dylan James Edgar López Edin Abazi Fanindra Maharana Femke van Schoonhoven Glauber Glenn Hitchcock Gonzalo García Paz Grant Purtle Jason Jun Joesak Josefianoes John Reynolds Jorge Arango Julie Chabin Kelin Carolyn Zhang Kevin Dukkon Kristjan Poska Kurtis Beavers Lucas de França Luke Mitchell Marc Astbury Marcelo Kunze Moritz Kindler Naz Hamid Niklas Rosén Nikolas Klein Noah Buscher PJ Onori Pranathi Peri Rachel Weil Ramsés Cabello Sam Solomon Sara Clayton Shawn Farsai Simon Austerberry Stephanie Stimac Stuart Regan Ted Goas Thao N. Thomas Rico Thu Le Tim Olsen Toni Lijic Trang Bui personal 46 Entries ~hedy Aadhavan Srinivasan Aaron Leonard Alex Murashko Alex Weichart Alexey Staroselets Asmir Benedict Neo Benji Taylor Brajeshwar Oinam Brian Devries Christina Chen Christopher (Namchee) Clinton Boys Dalton Rowe Darya Solomenko Dave Stucky Felix Almer Hashin Jithu Jordan Gonen Mangesh Max Bo Max Stoiber Max Weylandt Michael Kupietz Mike Morris Naman Modani Nikhil Vishnu Niklas Pivic Parth Patel Pete Ader Pranit Bauva Raphael Pour Raphael Salaja Robert Kingett Ronalds Vilciņš Roshan Shibu Scattershot Shreyas Prakash Shubh A. Chudasama Sofian Bettayeb Tiago Rangel Udara Jay Vedran Miletić Victor Stanciu Zygimantas Berziunas photo-video-production 3 Entries Bill (ChillBillDill) Kaitlin McKinnon Spaqin technology 126 Entries Abhishek Banthia Alok Menghrajani Andreas Jaggi Andrew Walpole April Speight Arafat Tehsin Ash Connolly Ashish Bhatia Autodidacts Aycan Gulez Ben Congdon Ben Hekster Ben Weidig Bernd Verst Brent Deverman Brett Cannon Brian Peek Brittany Cheng Bryan Lee Bryce Kerley Bunnie Cahya Wibawa Caleb Jay Rogers Candice Poon Casey Primozic Cassidy Williams Celestine Yeh Charlie Kindel Charmaine Lee Chloe Chia Christian Fei Christina Warren Damien Guard Dan Slimmon Daniel Grzelak David Anson David Eppstein David Singleton Deborah Zhang Den Delimarsky Dhrumil Patel Didier Catz Doan Tran Minh Thanh Duncan Mackenzie Dylan Fisher Ed Freitas Ephraim Duncan Erlend Sogge Heggen Gabe Wolford Greg Knox Guillaume Lethuillier Horace Guy Hynek Schlawack Ivan Milosavljevic J. B. Crawford Jeff Atwood Jeff Johnson Jeff Sandquist Jeff Wilcox Jeremy Danyow Jerred Shepherd Jessie Frazelle John Gruber John Papa Julio Merino Keith Cirkel Ken Shirriff Kenan Yusuf Kilroy Jones Larry O'Brien Laurent Bugnion Luis Quintanilla Mahmud Ridwan Marek Gibney Mariusz Szubryt Mayuko Inoue Mayur Bhoi Meg Risdal Melvyn Laïly Michael Mior Miguel de Icaza Mijndert Stuij Mikael Sargsyan Mike Kasberg Morten Nielsen Muhammad Raza Nicolas Bouliane Nicole Tietz Niels Swimberghe Nikhil (Unsung Novelty) Niklas Johannknecht Omkar Khair Orville McDonald Paul Irish Paul Singh Paul Stamatiou Peter Mekhaeil Philipp Cannons Ralf Jung Raphael Gaschignard Raphaël Rigo Redowan Delowar ReproDev Rohan Kumar Ryan Bagley Salman Ansari Sasha Kotlyar Sasha Rosenbaum Scott Hanselman Sebastien Castiel Sébastien Dubois Shariq Raza Qadri Shayne Boyer Siraj Chokshi Stanko Tadic Steph Ango Theia Vogel Tim Heuer Victor Björklund Vitalii Kovalenko Vitor Hugo Vlad Iliescu Vladimir Haltakov Yusuke Tsutsumi Zachary Kanfer Zoe Chew",
    "commentLink": "https://news.ycombinator.com/item?id=42583086",
    "commentBody": "BlogScroll – An open directory of personal sites and blogs (blogscroll.com)262 points by mahirsaid 12 hours agohidepastfavorite70 comments mikae1 10 hours agoWeird. I love a good link list, but most links I clicked were just regular portfolio websites, not digital gardens in the gardens vs. streams[1] sense. Does the creator not know what a digital garden is? [1] https://archive.org/details/gardens-and-streams-wikis-blogs-... reply dend 10 hours agoparentFair feedback. In my eyes, I treat any personal site as a digital garden - I am not really sticking to the \"pure\" definition. If you are putting an effort to curate and grow your own site, is that not a digital garden? I think it is, but that's my own take on it. I intentionally try to avoid sites that are in any shape _not_ personal or otherwise representative of an individual trying to stake their little corner of the internet. reply mikae1 10 hours agorootparentThanks for replying. :) > If you are putting an effort to curate and grow your own site, is that not a digital garden? That's really stretching the definition IMO. What characterizes a garden is the slow growth and constant pruning — the making of a personal wiki. https://indieweb.org/digital_garden and https://maggieappleton.com/garden-history are good intros to gardens. Most of the websites I visited seemed to be and forget sites targeted at potential employers. reply wink 9 hours agorootparentI dunno, sounds more like some gate keeping, or maybe I'm biased because I hate the term for no discernible reason. > has content of different levels of development, is imperfect and often a playground for experimentation, learning, revising, iteration, and growth for diverse content So because I polish my \"posts\" to a certain degree before publishing it's not a digital garden? Because 50% of my blog posts are \"timeless\" in a sense that they're about... stuff that exists.. not current dvelopments it's not a digital garden? Becuase I never delete stuff (as if people with wikis would delete stuff :P)... and so on. I mean, I couldn't care less but I feel (and this seems to be a common theme) that the indieweb people are mostly dogmatic about their definitions and not very encouraging (also why I tried to take part in the irc channel years ago and left, frustrated). But yeah, if someone want to own the definition of blog and digital garden and not accepting a certain overlap with \"personal website\", sure. reply mikae1 9 hours agorootparent> I dunno, sounds more like some gate keeping, or maybe I'm biased because I hate the term for no discernible reason. I'm not too fond of the term either, but I think that's because it got gentrified and somewhat misunderstood. Everything personal on the web was suddenly a garden. > But yeah, if someone want to own the definition of blog and digital garden and not accepting a certain overlap with \"personal website\", sure. Of course there's a overlap. My point was not that the listed sites had to fulfill the criteria for classification, but if there's not a single hint of gardening going on, then we might as well call en.wikipedia.org a blog? Also, is a website \"personal\" if it's just there to market your services? Sure, it's personal as in your website, but content wise it's not particularly personal. Gardens or personal wikis are usually not there to market services, but to build some kind of personal knowledge base (that other nerds, not employers, might find interesting). reply wink 7 hours agorootparentPoint taken, maybe I got a little sidetracked with the direction of the discussion and not the original website and its content. FWIW, I think there are wikis that are non-personal (wikipedia, tech, etc) and there can be websites by individuals that are just \"marketing\" in the wider sense, but I would not call them personal website. (i.e. jsut a portfolio or \"here are my socials\") reply dend 10 hours agorootparentprevI amended the description to better reflect the work. Thank you for the feedback, I appreciate the pointers to the IndieWeb and Maggie's sites. reply kubb 8 hours agoparentprevTo be fair, if watching this almost 3 hours long recording is the only way to learn the difference between a garden and a stream, I can completely understand why people wouldn’t know it. reply nobrains 7 hours agorootparentA stream is timeline based serial content (like twitter \"following\"). A garden is exploratory, with content not being serial in time. reply simondebbarma 5 hours agorootparentprevhttps://maggieappleton.com/garden-history How about a digital garden post about digital gardens:? reply bovermyer 5 hours agorootparentThat's an awesome link, thank you! reply ParetoOptimal 5 hours agorootparentprevhttps://hapgood.us/2015/10/17/the-garden-and-the-stream-a-te... reply diggan 5 hours agorootparentprev\"Garden\" is basically a wiki, but not necessarily collaborative. A stream is basically a blog, but not necessarily with the typical long blogposts, but can be mixed with smaller thoughts too. reply mtsolitary 10 hours agoparentprevAgree with this. Added my digital garden to the list and then was excited to dig through all the other sites, a little disappointed to find most were just personal landing pages (but very nice looking and professional ones!) I guess Maggie and Kasper’s lists are still the gold standard for digital garden indices :) reply a2800276 8 hours agorootparentWhat are Maggie and Kasper's list? Could you provide a link, please? reply mtsolitary 8 hours agorootparenthttps://github.com/MaggieAppleton/digital-gardeners https://github.com/KasperZutterman/Second-Brain reply bigfatkitten 12 hours agoprevAs much as I love the idea, I can't help but see some irony in an index of sites outside the Big Tech walled gardens being hosted on a major walled garden. reply dend 12 hours agoparentThe beauty of this site is that it's just HTML with minimal styling and JavaScript. If the current walled garden doesn't fit my needs at some point in the future, throwing it on another host takes less than 10 minutes, plus whatever the delay will be from DNS propagation. That's a built-in future resiliency, IMO. reply A1kmm 11 hours agorootparentI think it right to base the assessment of whether it is a walled garden on how easy it is for outsiders to access, and how easy it is to leave and take your community. For viewing, I think you are doing well - your own domain name, which you can host where you like, and which currently doesn't impose many restrictions on who can view without signing up to anything. But part of your community engagement is about having the community submit changes to you. And having that via GitHub is a walled garden - you can't make a PR without a GitHub account - or even search the code. And they say you are only allowed one free account - so one identity only - and I've heard credible reports they actively enforce it by IP matching etc..., and ban people if they suspect them of having two accounts. Moving off GitHub isn't always that easy - you'd need to retrieve all your PRs, but then the problem is people who have GitHub accounts to engage with you would need to migrate their method of engagement. So GitHub is absolutely a walled garden, and if you have a public GitHub, it is part of how you engage with your community. Walled gardens do have the benefit of more people being in them - there is some barrier to entry to signing up on a random Gitea or Forgejo instance - but then you are beholden to the policies of the walled garden. reply dend 11 hours agorootparentFair point - I will add a note to the top that if you don't want to contribute via GitHub, you can send me a note to hi@den.dev. I will make the change myself. reply yawpitch 10 hours agorootparentadmiration++ for responsiveness in adding the email option. reply kittikitti 5 hours agorootparentprevIf you use GitHub the wrong way, that Microsoft is prescribing, then yes it's a walled garden. However, it's meant to simply be a git host. reply 7speter 9 hours agorootparentprevWait you can only have one github account? reply progval 9 hours agorootparent\"One person or legal entity may maintain no more than one free Account (if you choose to control a machine account as well, that's fine, but it can only be used for running a machine).\" https://docs.github.com/en/site-policy/github-terms/github-t... reply jazzyjackson 12 hours agoparentprevGitHub is more like a public cemetery reply pmlnr 12 hours agoparentprevgithub is not walled in the same sense as requiring login to view content. reply blitzar 9 hours agoparentprevNot being vendor locked to one garden is about as much as you can ask for these days. reply pjmlp 12 hours agoparentprevSame here. reply lemming 11 hours agoprevKagi also has an occasionally delightful small web index: https://blog.kagi.com/small-web. It’s nicely integrated into their search results too. reply rpastuszak 6 hours agoparentIn case anyone's interested, I keep a small list of indie sites here: https://untested.sonnet.io/notes/places-to-find-indie-web-co... Included: gardens, personal websites, zines, and toys. reply ajmurmann 10 hours agoparentprevAnd of course there is always https://marginalia.nu reply sunaookami 7 hours agorootparentand https://wiby.me/ with a \"Surprise\" button reply mxuribe 5 hours agorootparentThanks for this refresher! For the life of me i recalled that there was a sort of search engine for the smol web, but could not recall its name! Thanks to you for finding it again for me! :-) reply flpm 5 hours agoprevThe Internet has a big discoverability problem now that search has a big advertising conflict of interests. The only way to find web \"human\" content is to rely on the curation of other humans. Funny enough mining those links is what made Google the giant it is now. But this time around we will not be fooled by the next \"don't be evil\" tech giant :D reply yakkomajuri 5 hours agoparentI already posted a top-level comment about this but I tried to tackle this specific problem when I built Recess (https://github.com/yakkomajuri/recess) which might interest you. Never got any traction so I dropped it. The idea was to be a content aggregator for what I called \"siloed content\" (personal websites, blogs, etc). The ideas are outlined here if anyone's keen: https://github.com/yakkomajuri/recess/blob/main/manifesto-is... I'm happy for someone to take this over too even if they're interested. It's MIT-licensed. reply dend 12 hours agoprevLook at the front page of HN - see my project randomly. Thank you for sharing! The goal with this site is precisely that, share people's digital gardens. If you have a site to add, please open a GitHub issue: https://github.com/blogscroll/blogscroll/issues/new?assignee... If your site is self-hosted (that is, not on medium.com or Substack or the likes, or if it is - has a custom domain), I'd be happy to add it. My goal for this year is to massively grow the list. reply 0xEF 9 hours agoparentDoes it have to be self-hosted? My sites, for example, are generally hosted on one of the various VPS services, some big and well known like Linode, others smaller like 1984.hosting. I move around too much to be worried about setting up servers in my homes, etc, but fully support the smolweb movement and love making my own tiny html places. reply fm2606 8 hours agorootparentNot the OP but I think hosting on a VPS is valid. \"Self-admin\" may be a better term, I don't know. You are still \"indie\" and not bound to any corporate culture. You are in full control down to the OS but not the hardware so yeah it counts. I once questioned if hosting on VPS was covered under the term self-hosting and got down voted. It was a legit question, as is your question, but I guess people took it as if I was saying that it isn't self-hosting. reply yakkomajuri 5 hours agoparentprevLove to see people thinking about this! You might be interested in some of the ideas here: https://github.com/yakkomajuri/recess/blob/main/manifesto-is... Happy to have a chat if you think it makes sense. I dropped the project above a year ago. reply pmlnr 12 hours agoparentprevYou could create an alternative, OPML version of the page, which is/was specifically for blogrolls :) reply dend 12 hours agorootparenthttps://github.com/blogscroll/blogscroll/issues/198 Opened an issue to track and implement. Thank you for the suggestion! reply nomemory 9 hours agoprevNot a garden but I also curtate links that I find interesting here: https://www.andreinc.net/links/ And maintain an active blogroll with blogs I follow: https://www.andreinc.net/blogroll reply rednafi 10 hours agoprevLove the idea, but I’m not sure how well it will scale. I’ve seen similar lists before and even went out of my way to include my blog there, only to never find that page again. That said, I’m loving this renewed interest in building our own little corners on the internet. I have mine[1] too. [1]: https://rednafi.com reply freetonik 12 hours agoprevVery cool! As someone else mentioned already, OPML export would be cool, especially if it’s per category (personal, design/ux, etc.) I also curate a list of blogs as part of my blog discovery/search engine/reader project: https://minifeed.net/blogs reply dend 12 hours agoparentGreat job on building a delightful experience! I really like the design. For OPML, tracking the enhancement here: https://github.com/blogscroll/blogscroll/issues/198 reply nelsonfigueroa 11 hours agoparentprevI remember discovering minifeed.net not too long ago here on HN. Cool project! I like how you show related posts from other blogs when viewing an item. It's nice for discovering new blogs. reply freetonik 10 hours agorootparentThanks! I still don't feel ready to do a proper \"Show HN\" with it though :) The related posts are generated with the help of a vector database (Cloudflare Vectorize [1]). I take post titles and descriptions as input, vectorize and store the results in the vector DB. Then for each post, I find 10-15 related posts and store the results in a separate table (it's a plain one-to-many SQL table). This is done because querying the vector DB is not super fast, and I wanted Minifeed to load under 500ms (according to my status page tracking, users in Europe for some reason experience the fastest loading times of 100-200ms! [2]). I also set up a scheduled job which regularly updates the relations (since there are new posts every day, and some of them may become \"related\" to existing posts). I've been meaning to write a blog post about the overall architecture of Minifeed, there are lots of small components related to RSS parsing, updates, caching, etc. 1. https://developers.cloudflare.com/vectorize/ 2. https://status.minifeed.net/ reply rednafi 9 hours agorootparentDo a write up on explaining the architecture and maintenance cost. This looks sleek. It'll fly on hackernews. reply freetonik 9 hours agorootparentThanks for the encouragement, I've started drafting the article. By the way, I've added your blog to Minifeed a while ago (https://minifeed.net/blogs/kF8K4M) reply rednafi 6 hours agorootparentYeah. I saw that. I've found so many good stuff on minifeed. Thank you :D reply rednafi 9 hours agoparentprevThis looks fantastic. Damn, there's even a detail page with related blog entries. Surprised to see my blog[1] there :) [1]: https://minifeed.net/blogs/kF8K4M reply diegocg 12 hours agoprevFunny how we have gone back to web directories reply mikro2nd 12 hours agoparentWebrings are also seeing some revival. Seems that actual curation by actual human beings is better than algorithmic or \"AI\" slop. reply mxuribe 5 hours agoparentprevI don't know if its only me...but years ago when google was in its early days, i considered google to have authority because of its \"completeness\", and its unobtrusive experience. For some time now, i have not thought of google in that same, good way. Hence, if i wish to find quality content and good results, i use other mechanisms which have earned my trust and respect...some of those include open directories, web rings, old school type blogrolls, RSS feeds, suggestions from the fediverse, recommendations from family, friends, and colleagues, etc. Do i still use a search engine? Sure! I use DDG (which behind the scenes isn't much better than google since they're dependent on Bing/MS)...but nowadays i diversify my expedition a bunch more when searching for something. Gone are the days where i depend as much on big tech (or at least, try my damn best to minimize my dependence on big tech in as many areas of my on/offline life). reply khafra 7 hours agoprevWhen reading the title I thought of \"walled gardens,\" as in collaborative sites with heavy moderation/curation, but a lot of these seem to be just personal sites with careful attention to aesthetics, function, and content. And how can I trust any list like that which doesn't include gwern.net? reply nilsherzig 6 hours agoparentA walled garden usually refers to a nice to use software ecosystem (the garden) with the twist that it stops users from escaping / communicating with the outside (the wall). Apple products tend to be the example for that. The title properly refers to the fact that these sites aren't hosted on substack / medium / some other pre build environment. reply Emigre_ 6 hours agoprevReally nice, but you could consider to move away from Big Tech altogether; there are alternatives to GitHub. reply j3s 11 hours agoprevcool, a garden index! love that they included an RSS feed too (https://blogscroll.com/index.xml) - i'm sticking that in my feed reader immediately. it sounds nice to have a few random new digital garden sites to peruse once in awhile. reply yakkomajuri 5 hours agoprevI've thought about this problem before and built Recess as one way to approach this and related \"issues\". It never got traction so I dropped it but someone might be interested in the ideas behind it: https://github.com/yakkomajuri/recess reply openrisk 11 hours agoprevCool but this will grow unworkable as the number of entries increases and will require some sort of filtering / searching. Even pure signal can be overwhelming these days when so much good stuff exists. Which brings us to the (as far as I know unsolved) question of supporting large scale discovery of the web without drifting into enshittification. Some sort of decentralized index that will be distributed in a torrent-like manner might work but that requires curation too: Who and with what criteria can add an entry etc. Bottom line is that the walled gardens did not exist, they evolved because the original web was missing critical components of usability. They exploited a vacuum. To fill the vacuum with something more benevolent we need to go back and solve these problems. The rest will be history. reply klez 10 hours agoparent> that requires curation too: Who and with what criteria can add an entry etc. IMHO that's the beauty of it. Sometimes I want to be hit with everything the web has to offer. But some other times I do want curated lists of links organized by category with opinionated criteria for inclusion. reply dend 10 hours agoparentprevYeah, I am still thinking how to evolve this into a useful, yet minimalist design as it grows. I am quite inspired by high-density websites (things that were de-facto in the late 90s), so will have to see how to incorporate that. That being said, the meta-point about at-scale discovery is astute - it's largely unsolved for personal sites/digital gardens. And I certainly don't want to be the bottleneck long-term. Will have to think through a solution as more content gets added. reply BaudouinVH 9 hours agoprevHow outside the \"Big Tech\" gardens is blogging on Github ? https://news.microsoft.com/announcement/microsoft-acquires-g... My definition of property is more restricted : either having hosting somewhere as independent as possible or self-hosting on Raspberry Pi e.g. My two cents reply vedranm 8 hours agoparentI have a website [1] on GitHub Pages. It's extremely easy to move out of GitHub, since it is on a domain I control, which is not the case with e.g. social profiles. That fact that the site is static makes it really easy to host it basically anywhere where a web server exists. The site is deployed using a GitHub Actions workflow, which happens to be the only GitHub-specific feature, but a similar script could easily be written in the future for another hosting if necessary. [1] https://group.miletic.net/ reply vouaobrasil 8 hours agoprevThese days, I prefer smaller websites more and more. But I do like Substack because they are sort of like a personal website, since there are very few social features and generally you can just visit one person's Substack without seeing ANY content from anywhere else. reply prmoustache 7 hours agoprevBring back the webrings! reply sylware 5 hours agoprev\"walled\" is too nice for what Big Tech is actually doing. What they are doing is literaly digital jails \"for you own security........\" reply deafpolygon 5 hours agoprevIsn't GitHub a \"Big Tech\" walled garden? reply scarface_74 5 hours agoparentAnyone can view and download content from it and transferring content to another site involves three commands “git pull …”, “git remote set-url origin…”, and “git push”. reply MrThoughtful 12 hours agoprev [–] Some are examples of good design ideas. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "BlogScroll is an open directory of personal sites and blogs, emphasizing digital spaces outside major technology companies, and is maintained on GitHub by Den Delimarsky. - Users can contribute to BlogScroll through GitHub or by emailing the creator, and updates are available via an RSS feed. - The directory includes various categories such as design-and-ux, personal, photo-video-production, and technology, showcasing contributions from numerous individuals."
    ],
    "commentSummary": [
      "BlogScroll is an open directory for personal sites and blogs, focusing on sharing digital gardens, which are debated as any personal site by some users.",
      "Hosted on GitHub, BlogScroll has sparked discussions about its \"walled garden\" nature, with suggestions for OPML export and scalability improvements.",
      "The project encourages submissions to expand its list, reflecting a renewed interest in personal web spaces and the challenges of maintaining independent, curated content online."
    ],
    "points": 262,
    "commentCount": 70,
    "retryCount": 0,
    "time": 1735885431
  },
  {
    "id": 42579969,
    "title": "The Alder Lake SHLX Anomaly",
    "originLink": "https://tavianator.com/2025/shlx.html",
    "originBody": "The Alder Lake SHLX anomaly 2025-01-02 Tavian Barnes At the end of 2024, Harold Aptroot posted this: Apparently shlx is a \"medium latency\" (3 cycles) instruction on Alder Lake. My disappointment is immeasurable, and my day is ruined. — Twitter / Bluesky / Mastodon I was immediately nerd sniped because I am into low-level performance analysis, and I happen to own an Alder Lake laptop. A bit of background: Alder Lake is the 12th generation of Intel Core processors. It's the first generation with a \"hybrid architecture,\" containing both performance (P) and efficiency (E) cores. SHLX is a left-shift instruction introduced in the BMI2 instruction set. The main difference with SHL is that SHLX doesn't affect the FLAGS register. It's also a 3-operand instruction: SHL RAX, CL ; RAX = RAX << CL ; (only CL allowed as shift count) SHLX RAX, RBX, RDX ; RAX = RBX << RDX ; (any register allowed as shift count) Left-shift is one of the simplest things to implement in hardware, so it's quite surprising that it should take 3 whole CPU cycles. It's been 1 cycle on every other CPU I'm aware of. It's even 1 cycle on Alder Lake's efficiency cores! Only the performance cores have this particular performance problem. The 3-cycle figure Harold cited comes from uops.info. They even document the exact instruction sequence used in their benchmark that measured the 3-cycle latency, with a sample nanoBench command to reproduce it. Running that command on my laptop indeed measures 3 cycles of latency. On the other hand, other sources (like Intel and InstLatX64) claim the latency is 1 cycle. What gives? I decided to write my own benchmark to try to understand the discrepancy. .intel_syntax noprefix .globl main main: MOV RDX, 10000 ; RDX = 10000 XOR RAX, RAX ; RAX = 0 .LOOP: MOV RCX, 1 ; RCX = 1 .rept 10000 SHLX RAX, RAX, RCX ; RAX = RAX << RCX ; (repeated 10,000 times) .endr DEC RDX JNZ .LOOP ; (loop 10,000 times) XOR EAX, EAX RET ; return 0 This code contains an outer loop with 10,000 iterations. Inside the loop, we initialize RCX to 1, then run SHLX RAX, RAX, RCX 10,000 times. In total, we run SHLX 10,000,000 times, so all the other instructions (including the ones before main() runs) are negligible. I used taskset -c 0 to pin it to a P core, and perf for measurement: $ gcc shlx.s -o shlx $ taskset -c 0 perf stat --cputype=core -e 'cycles,instructions' ./shlx Performance counter stats for './shlx': 301,614,809 cpu_core/cycles:u/ 100,155,910 cpu_core/instructions:u/ # 0.33 insn per cycle Here we see 0.33 instructions per cycle, a.k.a. 3-cycle latency. Let's try initializing RCX differently: .LOOP: - MOV RCX, 1 + MOV ECX, 1 ECX is the 32-bit low half of the 64-bit RCX register. On x86-64, writing a 32-bit register implicitly sets the upper half of the corresponding 64-bit register to zero. So these two instructions should behave identically. And yet: Performance counter stats for './shlx': 100,321,870 cpu_core/cycles:u/ 100,155,867 cpu_core/instructions:u/ # 1.00 insn per cycle It seems like SHLX performs differently depending on how the shift count register is initialized. If you use a 64-bit instruction with an immediate, performance is slow. This is also true for instructions like INC (which is similar to ADD with a 1 immediate). .LOOP: - MOV RCX, 1 + XOR RCX, RCX + INC RCX Performance counter stats for './shlx': 300,138,108 cpu_core/cycles:u/ 100,165,881 cpu_core/instructions:u/ # 0.33 insn per cycle On the other hand, 32-bit instructions, and 64-bit instructions without immediates (even no-op ones), make it fast. All of these ways to initialize RCX lead to 1-cycle latency: .LOOP: MOV ECX, 1 .LOOP: XOR RCX, RCX .LOOP: MOV RCX, 1 MOV RCX, RCX MOV RCX, 1 .LOOP: PUSH RCX POP RCX It is very strange to me that the instruction used to set the shift count register can make the SHLX instruction 3× slower. The 32-bit vs. 64-bit operand size distinction is especially surprising to me as SHLX only looks at the bottom 6 bits of the shift count. I do not have a good explanation for this yet, but I will update this page if I ever figure it out.",
    "commentLink": "https://news.ycombinator.com/item?id=42579969",
    "commentBody": "The Alder Lake SHLX Anomaly (tavianator.com)194 points by panic 20 hours agohidepastfavorite68 comments juancn 10 minutes agoCode alignment? I mean, the different instructions change the alignment of the rest of the code. - 64 bit register 0: 48 c7 c1 01 00 00 00 mov rcx,0x1 - 32 bit register 0: b9 01 00 00 00 mov ecx,0x1 It should be easy to test by adding a couple of NOPs to the fast version: 0: b9 01 00 00 00 mov ecx,0x1 5: 90 nop 6: 90 nop and see if it regresses again. I don't have an Alder Lake to test on. reply tavianator 19 hours agoprevOne fun thing about this is that spilling+restoring the register will fix it, so if any kind of context switch happens (thread switch, page fault, interrupt, etc.), the register will get pushed to the stack and popped back from it, and the code suddenly gets 3x faster. Makes it a bit tricky to reproduce reliably, and led me down a few dead ends as I was writing this up. reply amluto 2 hours agoparentWait a sec, isn’t this backwards? The OS will restore thread state by loading all of RCX, which sounds like it should trigger the slow path. I wonder whether IRET fixes it. Can you try MOV RCX, 1; push an IRET frame; IRET; SHLX to see if IRET fixes it? reply tavianator 1 hour agorootparent> Wait a sec, isn’t this backwards? No, if you push and pop RCX it's fast again. I mentioned this in the blog post, `mov rcx, 1` is slow but `mov rcx, 1; push rcx; pop rcx` is fast. reply amluto 1 hour agorootparentI think I missed that one in the pile of options. That’s bizarre. I wonder if MOV from memory is like POP. reply BeeOnRope 54 minutes agorootparentprevThe slow state is when rcx has a special \"hidden immediate\" attached it, from the `mov rcx, 1` or `add rcx, 1`. If the last option on the register doesn't fit that narrow template, it is fast, so loading from memory in any way, like pop, xrestor, plan load, etc, will be in fast mode. reply dataflow 9 hours agoparentprevWow. Any chance you could share either an explanation or your benchmark code to show how you measured this reliably? I've never had to constrain benchmarks to within a time slice so I'm kind of fascinated how you got stable measurements out of that. Did you force a context switch prior to starting and then loop for a fixed number of times that wouldn't trigger another context switch? Or did you disable preemption on your thread, or something else? reply tavianator 5 hours agorootparentBenchmark code is in the post! The trick is to limit the number of SHLX instructions before you re-initialize RCX. Doing it every 10,000 keeps it fresh enough. I didn't have to disable preemption or anything, perf shows 0 context switches and only ~50 page faults (probably all at startup). reply valleyer 8 hours agorootparentprevYeah, even an interrupt (that doesn't result in a thread switch) would cause a spill to the stack, right? That seems hard to control. reply toast0 10 minutes agorootparentYou should be able to prevent or at least strictly limit interrupts if you run a modern kernel, have a single threaded test process and cpu pin your test process to core X, while you've pinned everything else to not core X (and not core X's cpu threads if enabled), and you have no hardware interrupts routed to that core, either. You might have to look into tickless kernels too, but I think that's pretty mainstream now. Tldr, if there's no contention for the core and there's no hardware interrupts, there's no need for a timer/scheduler tick on that core. reply dataflow 8 hours agorootparentprevYeah. I don't know how often interrupts happen to know how easy this would be to control for, but it certainly sounds like it could be tough. I suppose one way to mitigate these though is e.g. taking a min() of the execution times rather than the average? reply Bulat_Ziganshin 5 hours agorootparentexcept that we need max() here :) reply dataflow 4 hours agorootparentHuh, why max()? Generally microbenchmarks want min() (or close to it). reply Bulat_Ziganshin 4 hours agorootparentbecause THIS code becomes faster once RCX is saved and restored during the interrupt call reply dataflow 4 hours agorootparentOh right, my bad, thanks. Totally escaped me when I wrote that :-) reply eigenform 19 hours agoparentprevI wonder if this is a thing where the machine is trying to predict the actual value of the 'count' operand ... reply tavianator 18 hours agorootparentIf it were a prediction/speculation thing, I would expect it to settle down long before 10,000 `SHLX`s are retired. reply crest 5 hours agoprevSo the issue is that the frontend which \"eliminates\" those operations doesn't make the resulting values available to the backend if fits Intel's ad-hoc definition of a short immediate (which shift values do) and since there is no SHLX with immediate uop the small value has to be be expanded to a full 32 or 64 bits and requested from the frontend with adds two cycles of latency? Has anyone run tests with ≥3 interleaved SHLX dependency chains in a loop? Does it \"just\" have 3 cycle latency or also less than 1 operation/cycle sustained throughput? Because if the pipeline stalls that would be even more annoying for existing optimised code. Is the regression limited to only Alder Lake P-cores or also present it later (refreshed) cores? reply userbinator 19 hours agoprevHow about trying \"xor ecx, ecx; inc ecx\"? Or the even shorter \"mov cl, 1\"? It is very strange to me that the instruction used to set the shift count register can make the SHLX instruction 3× slower. I suspect this is a width restriction in the bypass/forwarding network. The 32-bit vs. 64-bit operand size distinction is especially surprising to me as SHLX only looks at the bottom 6 bits of the shift count. Unfortunately the dependency analysis circuitry seems not Intel-ligent enough to make that distinction. reply tavianator 18 hours agoparent> How about trying \"xor ecx, ecx; inc ecx\"? Fast. But inc rcx is slow. > Or the even shorter \"mov cl, 1\"? Fast. > I suspect this is a width restriction in the bypass/forwarding network... I think we just found the explanation on Twitter: https://x.com/corsix/status/1874965887108976858 Alder Lake adds support for mov/add/sub with small immediates to the register renamer. So `add rcx, 1` gets handled by the renamer, potentially with zero latency. Unfortunately shifts are slow when the shift count is renamed in this way. This leads to fun things like `mov rcx, 1024` being fast while `mov rcx, 1023` is slow. I'll update the blog post in a bit. reply phire 17 hours agorootparentOk... That does make some sense. Essentially, Intel have allocated a bunch of fake renaming registers to hold all small immediates. Like how MIPS had a zero register, Intel now have over 1000 constant registers. These registers don't really exist, the immediate is just reconstituted at the execution unit. And I'm guessing these small immediates predate Golden Cove; It's probably how they have always represented shift by constant and any instructions with 8bit immediate (presumably at least as far back as Sandybridge). The only change is that now the renamer can now execute simple movs/adds. So essentially these variable shifts are being converted to constant shifts on the fly. The problem is that there is no constant shift version of shlx, so it wasn't in the test suite. There is probably some kind of stupid implementation bug in the scheduler that simply wasn't exposed until the renamer changes. reply tavianator 16 hours agorootparentI don't think there are a thousand constant registers. I think the renamer just represents (reg, 10-bit offset) pairs rather than just registers. Also the problem affects SHL as well as SHLX, I didn't realize until just now. reply phire 14 hours agorootparentThe speculation of a \"reg + 10-bit offset\" representation feels wrong to me. That requires a whole bunch of extra 64-bit full-adders everywhere one of these pairs might be consumed (so realistically, on every single read port of the register file). 64-bit adders take quite a bit of latency, so you don't want extra adders on all your critical paths. In the case where it appears to be holding a reg + offset pair, what I think has actually happened is that renamer (and/or uop fusion) has rewritten the uop to a 3-input add, with the offset as the third input. > Also the problem affects SHL as well as SHLX, I didn't realize until just now. And presumably SHR/SHRX/SAR/SARX too? reply dzaima 3 hours agorootparentYou don't quite need a full 64-bit adder to materialize the proper value, as one argument is only a 10-bit int. So a 10-bit adder, a 54-bit increment, and muxing it with the original depending on the add carry. And, presumably, the OP shift case here is in fact a case of there not being a built-in immediate adder and thus a need for fixup uops being inserted to materialize it? reply tavianator 1 hour agorootparentRight. Actually it turns out it's 11 bits, since [-1024, 1023] are all supported by the immediate add renamer. In general I think people are overstating the delay of an additional 64-bit add on register file reads (though I'm not a hardware guy so maybe someone can correct me). There are carry-lookahead adders with log_2(n) == 6 gate delays. Carry-save adders might also be relevant to how they can do multiple dependent adds with 0c latency. > And, presumably, the OP shift case here is in fact a case of there not being a built-in immediate adder and thus a need for fixup uops being inserted to materialize it? No, the perf counters show 1 uop dispatched/retired in both the slow and fast cases. reply dzaima 52 minutes agorootparentAh, good to know on the uop count. Still could be (or, well, has to be to some extent) the same concept, just pipelined within one uop. reply eigenform 13 hours agorootparentprevI dunno, you could imagine it happens [speculatively?] in parallel, at the cost of a read port and adder for each op that can be renamed in a single cycle: 1. Start PRF reads at dispatch/rename 2. In the next cycle, you have the result and compute the 64-bit result. At the same time, the scheduler is sending other operands [not subject to the optimization] to read ports. 3. In the next cycle, the results from both sets of operands are available reply phire 12 hours agorootparentSeems rather pointless to implement it like that. You would save space in the scheduler because the uops have been fused, but execution time and execution unit usage is the same as just executing the original ops before this optimisation was implemented. It also adds an extra cycle of scheduling latency, which may or may not be an issue (I really have no idea how far ahead these schedulers can schedule). If you think about the \"1024 constant registers\" approach: It allows you to have a small 10bit adder in the renamer which handles long chains of mov/add/sub/inc/dec ops, as long as the chain stays in the range of 1024. This frees the main adders for bigger sums, or maybe you can power gate them. And in the case where one of the inputs is larger than 1024, or it's value is unknown during renaming, the renamer can still merge two two-input adds into a single three input add uop. reply Bulat_Ziganshin 5 hours agorootparentadd3 operation will have 3 inputs, though. do we have other integer operations with 3 64-bit inputs? reply BeeOnRope 14 hours agorootparentprevYeah I am pretty sure that the renamer adds the tracked immediate(s) to the emitted uop, but that's not inconsistent with tracking \"reg offset pairs\" is it? reply userbinator 17 hours agorootparentprevSo `add rcx, 1` gets handled by the renamer, potentially with zero latency. Unfortunately shifts are slow when the shift count is renamed in this way. That's because the value still has to make it to the shifter... and the renamer's adder output clearly takes a different and higher-latency path than the one which doesn't go through it, confirming my original suspicion that this is a limitation of the bypass/forwarding network. Ultimately it seems they are just playing around with where things happen; the addition can be done here but the value ultimately needs to go there for the shift, or the addition and shift can both happen there. One could envision a future change that puts the shifter in the renamer too. They're really constrained by the laws of physics. reply Bulat_Ziganshin 5 hours agorootparentwe set CX only once and then use it 10000 times. the problem is not the slow calculation of CX per se, but the slow shift once we got CX from the renamer reply eigenform 17 hours agorootparentprevOooh, I forgot about this. Only thing I can think of is something like: 1. Assume that some values can be treated internally as \"physical register plus a small immediate\" 2. Assume that, in some cases, a physical register is known to be zero and the value can be represented as \"zero plus a small immediate\" (without reference to a physical register?) 3. Since 'count' is always expected to be4. The 3-cycle case occurs when the physical register read cannot be optimized away?? No... the 3-cycle case seems to be when the physical register read is optimized away. I think it's some kind of stupid implementation bug. reply eigenform 16 hours agorootparentLike, the scheduler is just waiting unnecessarily for both operands, despite the fact that 'count' has already been resolved at rename? reply phire 15 hours agorootparentThe 3 cycles latency casts massive suspicion on the bypass network. But I don't see how the bypass network could be bugged without causing the incorrect result. So the scheduler doesn't know how to bypass this \"shift with small immediate\" micro op. Or maybe the bypass network is bugged, and what we are seeing is a chicken bit set by the microcode that disables the bypass network for this one micro op that does shift. reply eigenform 14 hours agorootparent> But I don't see how the bypass network could be bugged without causing the incorrect result. Maybe if they really rely on this kind of forwarding in many cases, it's not unreasonable to expect that latency can be generated by having to recover from \"incorrect PRF read\" (like I imagine there's also a case for recovery from \"incorrect forwarding\") reply phire 12 hours agorootparentYeah, \"incorrect PRF read\" is something that might exist. I know modern CPUs will sometimes schedule uops that consume the result of load instruction, with the assumption the load will hit L1 cache. If the load actually missed L1, it's not going to find out until that uop tries to read the value coming in from L1 over the bypass network. So that uop needs to be aborted and rescheduled later. And I assume this is generic enough to catch any \"incorrect forwarding\", because there are other variable length instructions (like division) that would benefit from this optimistic scheduling. But my gut is to only have these checks on the bypass network, and only ever schedule PRF reads after you know the correct value has been stored. reply Bulat_Ziganshin 5 hours agorootparentprevmaybe, the bypass network doesn't include these \"constant registers\"? a bit like zen5 where some 1-cycle SIMD ops are executed in 2 cycles, probably for shortcomings of the same network reply tptacek 18 hours agorootparentprevI got nothing here other than: this is very cool. reply monocasa 18 hours agoparentprevI would have thought that the movs would have been dumped right into a rob entry without even going through the bypass network, much like a zeroing idiom does. reply eqvinox 27 minutes agoprevHmm… does this have any impact on time-constant cryptographic algorithm implementations? In particular the wider \"addition in register renamer\" story? reply LegionMammal978 14 minutes agoparentI don't think it should, if the performance change only depends on immediates hardcoded into the binary, and not the variable data being processed. Maybe if the algorithm branched on the variable data and had two separate ways of loading something, but branching on variable data is the #1 thing you're supposed to avoid in the first place. reply dzaima 14 minutes agoparentprevPerhaps not - as it happens at the rename stage and thus only with immediate values, it's all fixed behavior for a given instruction stream; with branching you can of course get dynamically different renamed immediates, but, as that's branchy code, it's not time-constant anyway. reply rincebrain 49 minutes agoprevSince this seems like an optimization going awry somewhere, I wonder if there's a chicken bit that disables it, and if so, how broad the impact of disabling it is... reply orlp 1 hour agoprevSeems like LLVM knows about this quirk (note how it suddenly uses eax instead of rax for the multiply): https://rust.godbolt.org/z/8jh7YPhz4. reply stassats 1 hour agoparentUsing EAX is one byte shorter, so it might be doing that inadvertently. reply BeeOnRope 17 hours agoprevWorth noting that whether intentional or not, this would be easy to miss and unlikely to move benchmark numbers since compilers won't generate instructions like this: they would use the eax form which is 1 byte shorter and functionally equivalent. Even some assemblers will optimize this for you. reply tavianator 16 hours agoparentIt's possible to get gcc to generate sequences that would trigger this: https://godbolt.org/z/rYKqPxn7b reply loeg 16 hours agorootparentThe godbolt paste I'm looking at shows GCC generating a SAL with 8 bit shift (CL), not a SHLX with 64-bit shift (RCX). reply tavianator 15 hours agorootparentSHL suffers from the same latency issue as SHLX it turns out reply loeg 14 hours agorootparentSAL as well? (And anyway this example seems to be shifting by the low 8 bits of RCX (CL), not the full width register.) reply tavianator 14 hours agorootparentSAL and SHL are synonyms, they have the same encoding. SHL only accepts CL as the count register, there's no other form that takes a variable shift count reply loeg 13 hours agorootparentAhh, thanks, I didn't make that connection. reply Bulat_Ziganshin 2 hours agorootparentprevIntel shifts anyway mask out higher bits of CL, this hurts sometimes e.g. when you need to shift by 1..64 bits reply tjalfi 16 hours agorootparentprevGCC will generate the shlx instruction if you add the -mbmi2 flag to your build options. For example, you can see this in action here: https://godbolt.org/z/asb1fxos5. reply BeeOnRope 15 hours agorootparentIt's true, I was considering only the original mov rax, 1 case, which I'm pretty sure compilers don't generate: it's just a useless encoding of mov eax, 1. Given that 64-bit immediate math also causes this though, compilers might generate it (I think this is a minor missed optimization by gcc though: it could have used add eax, 1 instead. reply eigenform 14 hours agoparentprevFunny thing to double-check: are these encodings correctly specifying a 64-bit operand? Maybe everyone's compilers are subtly wrong D: edit: It looks like VEX.W is set in the encoding from the uops.info tests ¯\\_(ツ)_/¯ reply bhouston 18 hours agoprevShifts were not always fast. These old hacker news comments contain the details: https://news.ycombinator.com/item?id=2962770 reply petermcneeley 18 hours agoparentIndeed. Dynamic shifting was microcoded (not uop!) on the power pc for gen3. However shifting with immediate values was not. This leads to all sorts of strange performance workarounds. Mike Acton refers to it here: https://macton.smugmug.com/Other/2008-07-15-by-Eye-Fi/n-xmKD... reply aftbit 19 hours agoprevWoah that's weird. Left shifting either takes 3 cycles or 1 cycle, depending on how you initialize the cycle count register? This patch from the article makes it take 1 cycle instead of 3: - MOV RCX, 1 + MOV ECX, 1 >It seems like SHLX performs differently depending on how the shift count register is initialized. If you use a 64-bit instruction with an immediate, performance is slow. This is also true for instructions like INC (which is similar to ADD with a 1 immediate). Practically speaking, is this sort of µop-dependent optimization implemented by compilers? How do they do so? reply tavianator 19 hours agoparent> Practically speaking, is this sort of µop-dependent optimization implemented by compilers? I suspect this specific thing is not; I don't think many people were aware of this anomaly until this week. I doubt the compilers are modeling it. But in general, compilers will have a machine model that predicts the cost of different instructions. The backend will try to generate instruction sequences with low cost, so if you include this anomaly in the model it will probably work around it automatically. This old example might interest you: a false dependency on the destination register for `popcnt` on some uarches led to compiler patches: https://stackoverflow.com/q/25078285/502399 reply RossBencina 18 hours agoparentprevuser mshockwave posted this the other day in a comment, might be of interest/tangential relevance: Scheduling Model in LLVM - Part I https://myhsu.xyz/llvm-sched-model-1 reply loeg 16 hours agorootparenthttps://news.ycombinator.com/item?id=42555110 reply BeeOnRope 17 hours agoprevTo rule out alignment you should adding padding to one so the two variations have the same alignment in their long run of SHLX (I don't actually think it's alignment related though). reply tavianator 14 hours agoparentI did try aligning the loop, it didn't matter reply BeeOnRope 17 hours agoprev [–] Does this also occur with other 3-argument instructions like ANDN? reply tavianator 16 hours agoparent [–] ANDN is fast. The issue looks specific to shift counts. SHL is also slow. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Harold Aptroot discovered that the SHLX instruction on Intel's Alder Lake processors exhibits a \"medium latency\" of 3 cycles, which is atypical for left-shift operations that usually take 1 cycle. - This latency issue is specific to Alder Lake's performance cores and does not affect the efficiency cores, contradicting Intel's claims of a 1-cycle latency. - The latency varies based on the initialization of the shift count register, with 32-bit instructions or 64-bit instructions without immediates achieving 1-cycle latency, while 64-bit instructions with immediates result in 3-cycle latency."
    ],
    "commentSummary": [
      "The Alder Lake SHLX Anomaly is a performance issue where specific instructions, such as `mov rcx, 1`, cause the `SHLX` instruction to execute slower, potentially due to the register renamer's handling of small immediates.",
      "This anomaly can lead to a 3x slowdown and is challenging to reproduce because context switches may reset the register state, inadvertently speeding up the code.",
      "The issue is newly discovered and primarily affects shift counts, impacting instructions like `SHL`, but not others like `ANDN`, and is not typically accounted for by compilers."
    ],
    "points": 194,
    "commentCount": 68,
    "retryCount": 0,
    "time": 1735858848
  }
]
