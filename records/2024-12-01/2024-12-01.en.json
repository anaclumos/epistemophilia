[
  {
    "id": 42287231,
    "title": "Advent of Code 2024",
    "originLink": "https://adventofcode.com/2024/about",
    "originBody": "Hi! I'm Eric Wastl. I make Advent of Code. I hope you like it! I also make lots of other things. I'm on Bluesky, Mastodon, GitHub, and Twitter. Advent of Code is an Advent calendar of small programming puzzles for a variety of skill levels that can be solved in any programming language you like. People use them as interview prep, company training, university coursework, practice problems, a speed contest, or to challenge each other. You don't need a computer science background to participate - just a little programming knowledge and some problem solving skills will get you pretty far. Nor do you need a fancy computer; every problem has a solution that completes in at most 15 seconds on ten-year-old hardware. If you'd like to support Advent of Code, you can do so indirectly by helping to [Shareon Bluesky Twitter Mastodon] it with others or directly via AoC++. --- General Tips --- If you get stuck, try your solution against the examples given in the puzzle; you should get the same answers. If not, re-read the description. Did you misunderstand something? Is your program doing something you don't expect? After the examples work, if your answer still isn't correct, build some test cases for which you can verify the answer by hand and see if those work with your program. Make sure you have the entire puzzle input. If you're still stuck, maybe ask a friend for help, or come back to the puzzle later. You can also ask for hints in the subreddit. --- Frequently Asked Questions --- Is there an easy way to select entire code blocks? You should be able to triple-click code blocks to select them. You'll need JavaScript enabled. #!/usr/bin/env perl use warnings; use strict; print \"You can test it out by \"; print \"triple-clicking this code.\"; How does authentication work? Advent of Code uses OAuth to confirm your identity through other services. When you log in, you only ever give your credentials to that service - never to Advent of Code. Then, the service you use tells the Advent of Code servers that you're really you. In general, this reveals no information about you beyond what is already public; here are examples from Reddit and GitHub. Advent of Code will remember your unique ID, names, URL, and image from the service you use to authenticate. Should I compete on the global leaderboard? Maybe. Solving puzzles is hard enough on its own, but competing for a spot on the global leaderboard also requires many additional skills and a lot of practice. Depending on the puzzle, that day's leaderboard usually fills up anywhere between two minutes and an hour. If that sounds interesting, go for it! However, you should do Advent of Code in a way that is useful to you, and so it is completely fine to choose an approach that meets your goals and ignore the leaderboard entirely. Why was this puzzle so easy / hard? The difficulty and subject matter varies throughout each event. Very generally, the puzzles get more difficult over time, but your specific skillset will make each puzzle significantly easier or harder for you than someone else. Making puzzles is tricky. Why do the puzzles unlock at midnight EST/UTC-5? Because that's when I can consistently be available to make sure everything is working. I also have a family, a day job, and even need sleep occasionally. If you can't participate at midnight, that's not a problem; many people use private leaderboards to compete with people in their area. I find the text on the site hard to read. Is there a high contrast mode? There is a high contrast alternate stylesheet. Firefox supports these by default (View -> Page Style -> High Contrast). I have a puzzle idea! Can I send it to you? Please don't. Because of legal issues like copyright and attribution, I don't accept puzzle ideas, and I won't even read your email if it looks like one just in case I use parts of it by accident. Did I find a bug with a puzzle? Once a puzzle has been out for even an hour, many people have already solved it; after that point, bugs are very unlikely. Start by asking on the subreddit. Can I stream my solution? Please try to avoid giving away the solution while people are competing. If a puzzle's global daily leaderboard isn't full yet and you're likely to get points, please wait to stream/post your solution until after that leaderboard is full. If you are unlikely to get points or the daily leaderboard is already full for the puzzle you're working on, streaming is fine. Can I use AI to get on the global leaderboard? Please don't use AI / LLMs (like GPT) to automatically solve a day's puzzles until that day's global leaderboards are full. By \"automatically\", I mean using AI to do most or all of the puzzle solving, like handing the puzzle text directly to an LLM. The leaderboards are for human competitors; if you want to compare the speed of your AI solver with others, please do so elsewhere. (If you want to use AI to help you solve puzzles, I can't really stop you, but I feel like it's harder to get better at programming if you ask an AI to do the programming for you.) Can I copy/redistribute part of Advent of Code? Please don't. Advent of Code is free to use, not free to copy. If you're posting a code repository somewhere, please don't include parts of Advent of Code like the puzzle text or your inputs. If you're making a website, please don't make it look like Advent of Code or name it something similar. --- Credits --- Puzzles, Code, & Design: Eric Wastl Beta Testing: Tim Giannetti Ben Lucek JP Burke Aneurysm9 Andrew Skalski Community Managers: Danielle Lucek and Aneurysm9 Playing: You! --- Legal --- Advent of Code is a registered trademark in the United States. The design elements, language, styles, and concept of Advent of Code are all the sole property of Advent of Code and may not be replicated or used by any other person or entity without express written consent of Advent of Code. Copyright 2015-2024 Advent of Code. All rights reserved. You may link to or reference puzzles from Advent of Code in discussions, classes, source code, printed material, etc., even in commercial contexts. Advent of Code does not claim ownership or copyright over your solution implementation.",
    "commentLink": "https://news.ycombinator.com/item?id=42287231",
    "commentBody": "Advent of Code 2024 (adventofcode.com)725 points by thinkingemote 9 hours agohidepastfavorite277 comments the_duke 3 hours agoThis years challenge for me: write it in C without the standard library or an allocator. Has to be runnable on an STM32 with 32kb of SRAM. I tried doing it in Assembly two years ago, ended up spending hours and hours writing an Assembly standard library, then gave up and switched to Rust... reply badgersnake 39 minutes agoparentLast year I tried C on a real Amiga 1200 (using DICE, Matt Dillon’s compiler / runtime). I didn’t get very far, lack of memory protection makes things really hard. This year the Amiga has an 060 upgrade with an MMU, so perhaps I can figure out how to use that and have another go. reply sebtron 1 hour agoparentprevYour restrictions sound quite challenging, good luck! Last year I solved all the problems in C without external libraries [1] and I enjoyed it a lot. It forced me implement some low-level stuff that I had forgotten how to do (e.g. a heap) and to write some numerical routines myself (easier than you'd think!). [1] https://github.com/sebastianotronto/aoc/tree/master/2023 reply BoingBoomTschak 2 hours agoparentprevGood luck! Personally, I'm still going with CL but decided to try it in all the languages I \"know\" for the first day. Including C which doesn't have hash tables (inb4 hsearch)... what a pain, let me tell you. https://git.sr.ht/~q3cpma/aoc2024/tree/master/item/01 If you could post a repo link so I can look at some of the progress, I'd be grateful. reply nickstinemates 36 minutes agorootparentI just want to note that Quake 3 CPMA is one of the best games ever made. No idea if the handle is in reference to that, but thanks for the :) reply wodenokoto 2 hours agoprevPeople here are doing it in Common Lisp and C standard library, meanwhile I’m just sitting here trying to get a curl call to download the data file. reply yoyohello13 1 hour agoparentCopy your session token into .token then: curl \"https://adventofcode.com/2024/day/$DAY/input\" --header \"Cookie: $(cat .token)\" > input.txt reply hoistbypetard 1 hour agoparentprevThe command line tool included with this python library does just what you want once you paste your session token into an environment variable: https://github.com/wimglenn/advent-of-code-data reply sva_ 1 hour agoparentprevYou could open the dev console in the browser and copy the curl command off the network tab reply eddsolves 1 hour agoparentprevI just copy and paste into a local text file, but I bet there’s a tool if you want to do it programmatically already (unless curl’ing is your idea of fun!) reply codr7 4 hours agoprevI usually do AoC in Common Lisp, but this year I'm giving Swift a chance. It's not half bad at this kind of twiddling for being a statically typed mainstream language. https://github.com/codr7/aoc24/tree/main/swift/Sources/aoc This year is a tiny bit weird, I was just getting ramped up organizing the event at a new job; because I think it's very useful for devs to learn some real problem solving, as opposed to stitching frameworks. And then I had to leave because my new boss turned out to be someone I couldn't imagine working with. Guess it'll be just me and Emacs as usual. reply audiodude 18 minutes agoprevI know a lot of people use AoC to try out/learn a new language. I tried that with Go in like 2018. However, I found it too frustrating (especially since AoC requires a lot of string parsing/regex which are difficult/verbose in Go). The past few years I've decided to stick to the same principle I've used in all of my side projects recently. Either I do something in a new language, or I get it done correctly before I get bored. I've found I can't have both. reply atribecalledqst 6 hours agoprevLast year I got stuck on Day 12 for a full week, and thinking about how to solve it consumed my every waking moment. I think this year, I'm going to be kind to myself and not participate so I can really enjoy the winter break from work. reply rak1507 1 hour agoparentWhat's so hard about Day 12? It's just +/'{x:\".\",x;H:(-1+;1+i-)@'+|\\m*i:!#m:x=\\:\"#.\" R:(x=x)({[h;d;x;y;z](z#0),+\\(((-z)_~\"#\"=x)&z_d>z)*(*y),(-z+1)_y-0^y h}. H)[x]/y (*|R)-R@*|0,&1_*+m}.''1({(\"?\"/:5#,x;,/5#,y)}.')\\@[;1;.:]'\" \"\\:'0:`:i/12.txt reply meisel 49 minutes agorootparentThis is what programming languages looked like to me before I learned programming reply Jorge1o1 1 hour agorootparentprevIs this Q? or k? reply rak1507 1 hour agorootparentIt's k4, the underlying language behind Q. With slight modifications it also runs in ngn/k which is FOSS. https://ngn.codeberg.page/k/#eJxVjsFugzAMhu95iiyWik1CIGPtIVG... reply icen 1 hour agorootparentprevLooks like K to me! reply SketchySeaBeast 4 hours agoparentprevIt ate my life for a few years in a row, I even managed to finish on Christmas eve twice. Now I don't even look, it turns from fun to stress rather quickly. reply ljm 2 hours agorootparentI re-read the intro and the fact it mentions leetcode and the like was enough for me to decide that it's an ultimately pointless endeavour for me. I have no interest at all in competitive programming or maths; I spend 40+ hours a week doing programming for work, I want games and challenges that pull me away from that so I continue to have a life outside of my job. reply ipdashc 2 hours agorootparentFor what it's worth, I hate leetcode with a burning passion, have no real interest in math, and yet I personally find Advent of Code quite fun and enjoyable. reply dunham 50 minutes agorootparentI have found AoC fun, but on some of the later days time constraints make it a little stressful (full time job + kids constrain my time). I've done it (and completed it) the last five years. I used it to try out a few languages (Haskell, Idris, Lean) and did it in python one year I was feeling lazy. I've got a project going now, and I probably should do that instead. However, that project is a programming language, so this is a way to test practicality. But solving problems and fixing shortcomings in the underlying language at the same time may be a bit too much. (It's a dependent typed language, so there is a lot of subtlety to deal with.) reply sureglymop 1 hour agorootparentprevHow about something creative that is at the same time relaxing? Some time ago I started creating mods for the game stardew valley. It still involves some programming but mainly drawing, creating animations and composing music! It's an absolute blast and so relaxing (like the game itself). I think by now I could even start working on my own game but I don't yet have a desire to. reply whoodle 24 minutes agorootparentNot OP, but that’s really neat. What’s your process for that? What IDE for the coding? Is C# required? What software for the art? reply ehsankia 1 hour agorootparentprevI find it useful for trying new languages. The first 10 days usually start very easily and progress quite gradually. The 2nd half definitely gets more brutal, but if you do have the self-control, you can stop whenever it stops being productive. reply pjot 1 hour agorootparentI love it for these meta-type use cases that aren’t necessarily about solving the puzzles. For instance I’m using it this year to dial in new neovim configs. Last year was to get comfortable with a split keyboard. reply moomin 4 hours agorootparentprevOnly year I ever completed it it took eight months. I’m not seeing my name on a leaderboard any time soon. reply SketchySeaBeast 4 hours agorootparentOh yeah, me neither. The competition is far too intense. What takes me hours takes the competitors minutes. reply orra 6 hours agoparentprevSounds sensible. It's important to set boundaries, and enjoy time off. For me Advent of Code is a slippery slope. The difficulty ramps up so at first it's easy, then it's rewardingly difficult. But then before I know it, it takes wayyyyy too much time. The danger is being emotionally invested by then. reply tails4e 2 hours agorootparentI usually make it Monday 18 or 19 and then I loose the will as the time taken is excessive, and obsessing about it is not good. I've finished a 5 years, but all after the fact. Some I am not sure I'll ever finish.... reply matsemann 5 hours agorootparentprevThere's a graph here of the \"hardness\" of each day/task, based on how long the 100th place on the leaderboard used. https://aoc.xhyrom.dev/ So it's not linear, and also based on your own knowledge. So perfectly fine to skip some days and still it's possible to solve some of the next ones! reply schneems 4 hours agorootparent> So perfectly fine to skip some days and still it's possible to solve some of the next ones! You try telling that to my brain. That guy doesn’t listen to me. reply tg180 3 hours agorootparentThis is why I usually do the simpler ones and then stop... Otherwise, I just can’t skip, it’s stronger than me. reply roncesvalles 2 hours agorootparentprevThere are some \"filter\" days for sure, usually those are when the solution needs a major leap in your approach such as concurrency, dynamic programming, or geometry equations. reply abound 2 hours agorootparentIt's usually not concurrency FWIW, it's almost always algorithmic in nature. On a modern machine, even highly concurrent* code would only execute 10-20x as fast and you could just wait a bit. One common AoC trick is that you can brute-force part one (e.g. O(n^2) complexity or worse), but part two scales up `n` to make that intractable. *ignore my sloppy conflating of concurrency and parallelism reply Jtsummers 1 hour agorootparentI agree with you. I don't think either concurrency or parallelism have ever been necessary. In 2019 with Intcode, it was the simplest approach (use multiple threads, one per VM) for a couple days, but it was never actually necessary. You could do the same thing with purely sequential code, but you had to juggle the state of multiple running systems yourself then. Threads were much easier (or coroutines, go routines, processes, etc.; some concurrency system). And by design, every problem is solvable on decade old computers in a reasonable amount of time (seconds) so parallelism is great if you're looking to minimize the runtime, but it's never necessary. reply mvdtnz 40 minutes agorootparentprevFor those just glancing at the chart do note the scale is logarithmic, so the difficulty increases exponentially. reply ruuda 4 hours agorootparentprevCool, it would be nice if that one aligned the days which were on weekends, as those tend to have harder problems. reply SketchySeaBeast 3 hours agorootparentIf I understand correctly, and I'm not sure if that graph proves that, they try to put longer puzzles on weekends. reply torginus 3 hours agorootparentprevUgh, I hate log scales used arbirarily, every silicon valley nerd things it makes them cool like their EE profs teaching actual science or Ray Kurzweil singularity whatever nonsense. reply dahart 47 minutes agorootparentRay Kurzweil is indeed full of crap (I have a specific bone to pick with his intentional mis-use of life expectancy among other things), but what makes you think in this case the use of log is arbitrary? Did you look at the data with a linear axis? I just tried it, and the vertical space is dominated by 4 or 5 outliers, and on top of that you can’t see the trend as well when it’s linear and all the data is smooshed at the bottom. Log plots are great when your values span many orders of magnitude. That’s true in this case, and the log plot both uses less vertical space, which is nice, and it more clearly shows the trend and wastes less empty space on the small minority of outliers. reply pvinis 5 hours agoparentprevA friend recently shared this with me. I think you'll like it. https://eli.li/december-adventure reply JackMorgan 4 hours agoparentprevAll things in moderation! Now if I can just make it through the winter without installing the new Factorio DLC... reply Tyr42 44 minutes agorootparentI've been playing half an hour each morning before the rest of the family wakes up. Just finally made it to the volcano world reply mrwww 4 hours agorootparentprevthis comment actually prompted me to finally download and install the factorio demo, been hearing so many things about how it takes over lives! :) reply JackMorgan 3 hours agorootparentIt really is such a great game! I got the base game, beat it a few times, then my partner and I played a few mods, currently in an Angels+Bobs playthrough. So much replay value reply hollerith 4 hours agorootparentprevSo, you want it to take over your life? reply adventofshmode 2 hours agoparentprevI got stuck on the graph-cut puzzle for FOUR MONTHS. I had to write a force-directed graphing engine to find the longest three edges to cut. After I solved it I looked at other people's solutions and they used Meta's proposition solver in about 10 lines. Seemed like a massive cheat to me. reply moustachehedron 8 minutes agorootparentOh man, this is my best memory of last year's AoC. After uselessly noodling for a while, I used Graphviz to draw the graph to an SVG file. It drew two messy balls of yarn neatly connected by three edges. My script still says \"TODO: find a real solution\". Good times. reply brightball 3 hours agoparentprevWe run a private board for Advent of Code for the Carolina Code Conference. Eligibility for prizes starts after earning only 10 of 50 possible stars precisely for this reason. reply flanbiscuit 3 hours agorootparentOh cool. I live in WNC and had just missed your last conference in August. Is it possible to join multiple private boards? I usually do one with my coworkers as well reply brightball 45 minutes agorootparentYep. I’m running a little behind but the announcement will come out later today on our site/email list. https://carolina.codes reply sokoloff 2 hours agorootparentprevYou can only “own” one board, but you can join many. reply oweiler 5 hours agoparentprevI will probably solve the first 7 puzzles like every year and then just stop. reply ryukoposting 5 hours agoparentprevLikewise. I did it one year in college and it became a life-consuming thing almost immediately. Not AoC's fault - part of it was depression, part of it was the Minnesota winter. Now that I have a full-time job and a wife, I'm trying to be more careful with those things that I know will suck me in. reply globular-toast 1 hour agoparentprevThe year I did it I got lucky and solved them all within a reasonable amount of time until there was one that suddenly involved a lot of nontrivial linear algebra and I immediately spotted that this wouldn't be fun and noped out. Noticed the number of people solving dropped off a cliff on that day. I think as nerds we need to be quite careful not to get too drawn into this kind of thing. Sometimes it's like a superpower, but other times it just pointlessly consumes your life. Kinda makes me think of gambling addiction: \"when the fun stops, you stop\". reply smcl 6 hours agoparentprevAh yeah I've been there! Having done it a few years now, I've found that the approach that works for me is: if it starts looking like I'll be stuck on one for more than a few hours, I'll skip it and move on. Otherwise I'll accumulate an insurmountable backlog that becomes more of a depressing chore to think about, than a fun little christmas tradition. I'd rather have a mostly-complete set of problems by the end of the year that I can come back and clean up when I feel like it. That said, if you'd have a better holiday season by just stepping back from the computer and relaxing then that sounds great too. Either way - enjoy! reply chucksmash 3 hours agorootparentMy main complaint the last time I did this (2022) was the havoc it wreaked on my sleep schedule. Advent of Code is not kind to East Coast participants. Every year except for one has been kind of the same pattern for me: Day 1: this year, I'm just going to solve the problems. No futzing around. Day 3: but it would be kind of neat to turn the solutions into a reusable AoC library. Just something minimal. Day 5: and I should really add a CLI harness for retrieving the problems and parsing the input files. Day 6: and testing of course. Day 7: maybe I'll skip today's problem (just for today) and keep improving the framework. Day 358: oh neat, Advent of Code is coming up. reply smcl 3 hours agorootparentI'm in CET so time-wise it can be ok - problems open at 6am meaning if I get up I have about an hour around before I need to walk my walk my dog and get ready for work. But switching on at that time is really hard, the amount of stupid off-by-one errors, or referring to since-renamed-but-still-present functions in my Jupyter Notebook is not even funny. But I luckily managed to avoid the \"reusable AoC library\" problem around 2019 when a week beforehand I wrote down the sort of functions I wanted to have at my disposal (usually things around representing 2D/3D grids of unknown size and pathfinding/debugging therein, but a few other bits and pieces) and made a simple library that I will sometimes add things to after I'm done with the problem for the day. I was tempted to some functions (similar to those your CLI harness provided) for retrieving test data and submitting answers but I managed to stop myself short of that! But I am sure you're far from the only one to end up down that road. reply Kwpolska 48 minutes agorootparentI'm in CET too, and 6:00 is not an hour where I’m awake, and if I were, my brain functions would definitely not be at a level where I would be capable of coding. Midnight would be much more acceptable. reply beacon294 2 hours agorootparentprevWhat if you don't get up? reply danielovichdk 5 hours agoparentprevHahaha...I love this comment. I have just been stuck for a week doing edge puzzles and backstepping recursion, keeping myself awake aye night because it bothered me I couldn't \"just\" solve it. reply bmitc 2 hours agoparentprevOne reason I didn't enjoy it was that I felt the days don't build on each other well. So you get little code reuse. It was continually changing requirements, so it was especially like work. reply Jtsummers 2 hours agorootparentIn 2019 he built up about 12 challenges using a VM, for Intcode, you had to construct. It was poorly received because without a working version (developed over the first few Intcode challenges), you couldn't solve the rest of them. He hasn't done anything like that since, though I thought it was probably the more interesting series of challenges. The problem with continuity across days is that the later days can be blocked by the earlier ones, as they were in 2019. That partly defeats the purpose (or structure) of the challenge, where you can mostly pick any day and try it without regard to earlier days or prior years. reply hoistbypetard 1 hour agorootparentI agree. Intcode was fun, but completely destroyed the promise of skipping a day and still having fun with later puzzles. I didn't come to enjoy it until much later. reply smrq 16 minutes agorootparentprevThere's a lot of potential code reuse between years; whether that's good or bad is up to you, I think. (I would personally prefer if my Chinese remainder theorem solving function got less use, but it seems to be called for every year or two.) reply tmtvl 3 hours agoprevI still haven't made my way through all of the 2015 problems yet. But I don't play the game correctly any way: instead of trying to solve the problems as fast as I can, I try to write well-documented easily maintained code which runs fast. Balancing 'easily maintained' and 'runs fast' takes a little more time than 'just solve it' _and_ I'm planning on working my way through the problems chronologically, so I doubt I'll get to the 2024 problems any time this decade. reply zach_miller 2 hours agoparentThat’s one of the many ways to do Advent of Code. reply empath75 3 hours agoparentprevI try to do everything in a single pure functional expression no matter how dumb and unreadable that makes the code look. reply amw-zero 3 hours agoparentprevnext [7 more] [flagged] release-object 3 hours agorootparentWhy not? It’s supposed to be fun. As long as you’re not cheating the global leader board - which this approach doesn’t - why not play however you want. I like go to overboard writing parsers for each day. reply mytec 2 hours agorootparentprevFrom the about page: \"... However, you should do Advent of Code in a way that is useful to you, and so it is completely fine to choose an approach that meets your goals and ignore the leaderboard entirely.\" reply sokoloff 3 hours agorootparentprevIs there really a wrong way to do it? reply nikanj 1 hour agorootparentYes, with ChatGPT or similar: \" Can I use AI to get on the global leaderboard? Please don't use AI / LLMs (like GPT) to automatically solve a day's puzzles until that day's global leaderboards are full.\" https://adventofcode.com/about reply tmtvl 2 hours agorootparentprevHa! Yeah, imagine me streaming my working on AoC, basically anyone tuning in the stream would say 'you're doing it wrong, stop having fun!' and leave. reply xdavidliu 1 hour agorootparentprevmy guess is that the creator would almost certainly disagree with you reply ducdetronquito 7 hours agoprevI will try to do it with F# and Gleam this year, but like every year I won’t have time (and brain) to do more than 10/12 days ^^ For the pythonists around here, give F# a try: it can feels very close to scripting and it has a wonderful REPL too :) reply thaliaarchi 5 hours agoparentI'm continuing my tradition of doing AoC in Whitespace[0]. The first year I did it, it was motivation to build out a standard library so things wouldn't be so tedious. Now, I find myself wishing I had finished better tooling. I debug with wsjq[1], a CLI debugger like gdb written in jq, but it's slow. [0]: https://github.com/thaliaarchi/ws-challenges [1]: https://github.com/thaliaarchi/wsjq reply axpy906 6 hours agoparentprevI am doing it in bash. Will see how far I can go! reply andrelaszlo 6 hours agoparentprevHow's linux support? :) reply davidgl 5 hours agorootparentPerfect, run loads of f# and c# on Linux, and have for years reply neonsunset 6 hours agorootparentprevIf almost 9 years were not enough to make this question seem silly, perhaps DuckDuckGo/Kagi/Perplexity could answer this? reply plagiarist 5 hours agorootparentSure those can answer it. That person is perhaps trying to be social on a social network by inviting comments instead? reply neonsunset 5 hours agorootparentThis specific question posed in this specific way usually implies a negative interaction. I have seen maybe one exception here so far. And after providing detailed answers for so many times, I don't think it's worth it. reply samatman 36 minutes agorootparentYou guaranteed it would be a negative interaction when you negatively interacted with the post. You're allowed to skip the ones you don't want to reply to. reply matejn 1 hour agoprevI completed last year's in Scryer Prolog and it was a joy. Some problems were almost impossible due to the lack of mutation (Karger's algorithm comes to mind), but file parsing was a breeze and I find Prolog programs generally beautiful. My favourite syntactical feature is the full stop at the end of clauses. reply heelix 4 hours agoprevWoohoo! This is my favorite time of the year. As the year rolls into vacation'ish days and on call rotations, this is where I hunker down and use this to code something other than power point. Last year, solved via my primary language (Java), then ported to Rust. This year, I'm starting with Rust. I had not realized it was the first. reply tgv 5 hours agoprevGoing to use it to learn a bit of Ada. I've always been curious about it. It's not a popular language, and it has some serious documentation problems. Sure, there are guides for \"hello, world\" and other basics, but how to use a generic integer vector or even how to read lines with two numbers from stdin or a file? That was a bit of a puzzle. I saw a solution that allocates an array of 99999 elements, just to track the number of occurrences of each number in the input. Ada took me somewhere between 90-120 minutes, whereas I had the first problem done in JavaScript in about 30s-60s, just for verification. reply Jtsummers 3 hours agoparentAda.Integer_Text_IO with Get will happily read across all whitespace, including new lines, to find the next integer. This is true for most (all?) instances of Get, though that may not always be what you want. with Ada.Integer_Text_IO; use Ada.Integer_Text_IO; procedure main is Left : Integer; Right : Integer; begin Get(Left); Get(Right); Put(Left); Put(Right); end main; If you give it any of these pairs it'll work as expected, put it in a loop and you'll get all of them: 1 2 3 4 5 6 Sometimes thinking about lines is a red herring in AoC, the lines often don't matter, only getting each value into the appropriate collection (a pair of vectors in this case since you don't know the size). For the counts, you can use a hashed map, they're built into the standard library. If you learn to use them now that'll help you out in later days, they're a commonly used collection (for me) in these challenges. reply ctur 9 hours agoprevWoohoo, one of the highlights of this time of year. I had to do mine from an eastbound flight over the pacific. This has become a fun tradition not just for me personally but for many friends, colleagues, and fellow HNers. Big props once again to wastl and his helper elves for making this! I encourage anyone who gets value from this to donate to support it if they can. It is a passion project but nonetheless comes with real costs. reply mteoharov 6 hours agoparent> I encourage anyone who gets value from this to donate to support it if they can. It is a passion project but nonetheless comes with real costs. With the sheer amount of sponsors and AoC++ users I do believe that this is not quite a small 'passion project' struggling to pay the monthly subscription to a VPS. That being said, adventofcode is absolutely great and people should support it if they can. But I do think the author is doing quite well with the amount of support he is currently receiving. reply paulb73 3 hours agoprevI can hear the elves calling me... But no, last year I lost all momentum on my side projects and my gf thought she'd lost me to the elves. Completing it on time was rewarding but I can't go back. But I can hear those elven bells a jingling... reply HeavyStorm 2 hours agoprevI believe that this page should be mobile friendly. I won't be coding on mobile, but reading about it on my portable device would be great. reply dchuk 1 hour agoparentAt least on iOS, Reader mode in safari fixes it right up reply NoelJacob 6 hours agoprevOther challenges (in advent and not in advent) like advent of code: https://github.com/NoelJacob/advent-and-other-calandars Compiled by myself. reply smcl 6 hours agoparentNot a challenge, but it may be of interest anyway - Sergey Tihon's F# Advent Calendar: https://sergeytihon.com/2024/10/26/f-advent-calendar-in-engl... I think there's a few of these for different languages/tech. I think they may be good for HN'ers seeking some kind of little daily advent-y fix without the potential emotional/mental investment of AoC. reply MortyWaves 2 hours agorootparentThat’s an impressively broken website on mobile. Really, I haven’t seen a site I wasn’t able to scroll horizontally before. reply SleepyMyroslav 5 hours agoprevAm I alone in thinking that measuring time to get answer is the worst possible metric? I have not participated because of that (yet). If there is a community for those who use other rules to compare actual solutions instead of answers I would be interested to hear about it. I am coming from low level C++ gamedev side so I understand that most people here use different tools to solve different problems. reply Jcampuzano2 4 hours agoparentThe vast majority of people are doing it for fun or learning purposes and not to compete on the leaderboard, and it wouldn't quite be fair to compare much else in a competitive setting but time of completion and correctness, since you'd be at a disadvantage just by language choice in many other metrics. Unless you are someone with experience competing in competitive programming you almost certainly won't make the leaderboard anyway. If your goal is to compare solutions, lots of that happens on the subreddit for it where people post solutions in their language of choice on the daily threads. I just do it for fun. When I was younger I'd actually do them at release (11pm in my timezone), now I don't even bother and just used them as sort of a brain teaser to start my days and compare with coworkers who also do it, a lot of us in different languages. reply dunham 25 minutes agoparentprevI try to get it done within a day, so they don't back up, but I've never tried to compete. I am PST which opens at 9PM. I try to get through part 1 then, so I can sleep on part 2. The subreddit /r/adventofcode contains discussions of solutions with lots of different skill levels. I've done AoC for five years to learn new languages and try solve all of them myself during the month of December. (Dunno if I'll run the whole thing this year - I have another project.) Others try to get on the leaderboard, and some will implement solutions that they've seen sketched on reddit. Last year a few people used Z3 for one of the problems, and I went back and tried that to get some experience with Z3. And I've occasionally gone back and tried another approach or new trick that I saw on the subreddit. (In the years that I've used Lean, I've sometimes gone back and added proofs for termination or array indices, too.) reply dingensundso 5 hours agoparentprevI think most people don't participate for the competition. I did it several times to increase my coding skills, have fun or get more practice in a new programming language. reply SleepyMyroslav 5 hours agorootparentIt is normal for coding to seek feedback from others to your solution. Even if it is automated. Looking at 'competing' solutions after you spent time on yours can teach how others think. Improving code after you learned new facts is huge part of coding fun at least for me. People mentioned private leaderboards in the topic few times. I just don't think time to submit an answer is relevant. reply mhitza 1 hour agoparentprev> If there is a community for those who use other rules to compare actual solutions instead of answers I would be interested to hear about it. Generally you have the main community on reddit (memes, questions, daily thread for sharing solutions), then the language specific subreddits or hosted forums where you will see solutions discussed and shared, plus a couple of new users asking questions. Also, within the daily main community thread you will see the niche sub community of people posting their code-golfing attempts. reply sebtron 1 hour agoparentprevI also don't like it, last year we had a private leaderboard at work and I realized being crazy enough to wake up at 5:50 every and solve at least part 1 would give me an edge. But the \"wake up at 5:50\" part is what I enjoyed the least. However, there are other ways to rank yourself against others. You can order your private leaderboard by number of stars, or make your own leaderboard using their APIs. reply Levitz 5 hours agoparentprevCan you propose other metrics that don't involve executing stuff in a whole lot of languages? The point is to let people work in whatever they want, as only the solution matters. If only the solution matters I don't really see other options beyond time. reply nikanj 1 hour agoparentprevThat's like saying you don't want to go for a run, because professional marathon runners are so focused on time reply kibwen 4 hours agoparentprevHere's a website that appears to be about competing on the performance of each solution: https://codspeed.io/advent reply SleepyMyroslav 4 hours agorootparent>Participants have 36 hours Looks like it is for young people who have dedicated time for it everyday. Personally I would like to do anything like this with no time limit and probably no monetary prizes. I think the only value of those puzzles is to fire up rarely used neurons that hopefully are still there after another year of shipping corporate products xD. I might appreciate fresh point of view from young people and new programming languages though. reply _se 3 hours agorootparentBecause only young people can make time for things. There are plenty of professionals with jobs and families making time for AOC because they enjoy it. Doing the problems at the same time as everyone else is a VERY different experience from doing them whenever you'd like. If you don't want to make the time for it, power to you. I'd recommend most people to drop off after the first 10ish days. But don't delude yourself by ascribing this as the domain of \"young people\" or those without responsibilities. You're making a decision. Own it. reply SleepyMyroslav 3 hours agorootparentI appreciate your perspective and it is correct. I should have phrased it differently. Imho: I worked with code that has long history for my entire career. If the goal is to look at some objective quality of solution then I do not believe in time limits. The longer I work the more things getting patches/updates/remasters and value of better code goes up and value of arriving at any kind of solution overnight goes down. reply _se 3 hours agorootparentFor software that's meant to be maintained for long periods, especially by others, I agree with you. The thing about AOC is that it's really less about the code that you generate, and more about the process of solving the problem. The challenge is really what you make of it. Some people will golf it, some will go for speed, other for performance, etc. That's why it's so different to solve the problems in \"real time\". There's a huge community of people solving the same problem that you can interact with and bounce ideas off of. Even just a few days after the problem is released, most of that active discussion has dried up, so you can no longer participate in that discourse. So, again, I don't think there's anything wrong at all with what you're saying, but there are other elements to consider beyond maintainable code and pristine solutions. reply criddell 3 hours agorootparentprev> Doing the problems at the same time as everyone else is a VERY different experience from doing them whenever you'd like. I agree and I happen to think the experience of doing it later than everybody else is significantly better. If I search for “AoC 2024 day 12 hint”, I’ll get better results on Jan 12 than Dec 12. reply criddell 3 hours agoparentprevI wouldn’t use an AI to solve the problems, but I wonder if an AI could give good feedback on a solution I cook up? reply yoyohello13 2 hours agorootparentThat's what I've been doing with some older AOC puzzles. I solve it, then paste my solution into Claude and ask for tips on making it more idiomatic. It's been pretty nice so far. I learned about Haskell Arrows which I would probably have never come across otherwise. reply clbrmbr 3 hours agoparentprevIt could be interesting to find the shortest solution. But the site is not set up for verifying solutions in various languages. reply dchuk 21 minutes agoprevWhile I agree that just dumping the puzzle instructions into an llm and getting the right answer at least doesn’t align with the spirit of Advent of Code (though it does show how crazy good LLMs are getting), I’m using this as an opportunity to try out the new Windsurf AI IDE and am driving the AI code editing in it to help me write the code to solve each puzzle (I’m using Ruby). My goal is to develop the muscle memory for the tool so I can write code quickly. I’m still generally thinking through the puzzles, but being able to just write out plain English logic, get code generated, ask for it to be well documented, quickly refactor things to be generally reusable, etc, is just fantastic and how all software development should be done in this day and age frankly. Such an accelerator to problem solving. reply setopt 6 hours agoprevThe tiny thin gray font on black background is impossible to read on a mobile device :( reply 0xEF 5 hours agoparentColor aside they also ignore a number of solutions for font resizing that follow the users accessibility settings. I can think of at least three easy ways to do this with html/css alone, and yet hear we are; a site that is unreadable to me. I love what Advent of Code does, but when your site is all text, there's just no excuse to not let the user resize it by default. reply sebtron 1 hour agoparentprevIt is terrible for me on my desktop too :( reply scrivna 2 hours agoparentprevAoC day one challenge… make website responsive reply MattRix 5 hours agoparentprevOn iOS you can switch to Reader view, and I imagine Android has something similar. reply technofiend 2 hours agorootparentYes on Android it's already readable in Firefox, but that may be due to ad-blocking plugin. Even so, Firefox reader mode makes it even better. reply goodlinks 1 hour agorootparentprev*browser not os feature :) reply NamTaf 7 hours agoprevI've never done AoC but I've done other programming-related challenges before. I come from a non-IT background (mech eng), and I'm currently away for work for the first several days with only their locked-down laptop. Normally I'd break out Python for this, but given the constraints maybe I should try to see how far I can get through this in Excel. It'd be a fun little challenge :) reply andai 7 hours agoparentMy approach here would be to make an index.html file with a script tag and drag it onto whatever browser is available. Then again I have made peace with JavaScript! (I think you could even use typescript with this method with the on the fly babel transpiler (you just include a script tag) but I haven't tried that.) reply cube00 4 hours agorootparentYou'll lose the benefit of the TypeScript compiler picking up mismatched types if you only use a transpiler. reply Cruncharoo 4 hours agoparentprevI use Excel for the first few as well! I actually find it to be easier than using Python/Javascript, at least early on. reply ksymph 4 hours agoprevThis is the first time I've seen Firefox's alternate stylesheets mentioned in the wild. Interesting. reply prophesi 1 hour agoprevFor anyone interested in using this to learn Elixir or improve your skills with it, I saw this video[0] a while back and their AoC starter repo[1] is super slick. [0] https://www.youtube.com/watch?v=gpaV4bgEG-g [1] https://github.com/ChristianAlexander/advent-generator reply msmitha 36 minutes agoparentThere are a few Elixir solutions fo Day 1 here, including mine: https://elixirforum.com/t/advent-of-code-2024-day-1/67786. I'm using it as a motivator for actually learning the language/libs. reply kazinator 1 hour agoprevI did some of it in TXR Lisp a bunch of years ago. https://www.kylheku.com/cgit/advent/tree/2021 Give it a try. The structure and recurring themes in these solutions could be used as a source of ideas for how to get started. reply azhenley 5 hours agoprevI’m attempting to make my own language for solving the puzzles as I go along. reply lolinder 4 hours agoparentI'd pretty much decided I wasn't going to participate because it would take up too much time, but now you've both made it way more complicated and way more appealing... reply nemo1618 2 hours agoparentprevI did this, and it's been very satisfying! Here's where I'm at now: https://www.youtube.com/watch?v=r99-nzGDapg reply signa11 5 hours agoprevif you want to use Zig, here are some tips : https://kristoff.it/blog/advent-of-code-zig/ reply barkingcat 2 hours agoprevI use it to learn new languages - this year it's Ada - https://blog.adacore.com/announcing-advent-of-ada-2024-codin... reply seba_dos1 25 minutes agoprevAoC is in this weird place where it's too easy to be fulfilling on its own, but too bothersome to just do it for leisure. I did it once (using Python with no imports for some mild challenge), waking up super early to actually start on time, then golfing my answers if I felt like it. It was a fun thing to do... once. I don't feel the need to repeat that and I don't find it engaging enough to do without time pressure, so I don't. Perhaps if you aimed at global leaderboards it would be different, but that's neither my league nor I see any fun in that - getting there requires serious effort and preparation in things that aren't directly related to solving intelectual puzzles. reply 110jawefopiwa 20 minutes agoparentIt's best for me when I do something that I ordinarily don't do for AoC. I find no particular pleasure in using an everyday language like Python for it, because as you said it's too easy. I have used Haskell, Racket, and in some easier cases APL and it's been fun. Treating it more like a puzzle than an actual programming assignment. When learning new languages, it's best to do something that actually makes you think in a different shape. If you know Python, don't do Ruby. If you know Java, don't do C#. reply cjauvin 1 hour agoprevBecause we can assume that these problems necessarily cannot be in the training set of any current LLM (and are hard enough, i.e. they should be more than mere variations on existing problems), I'm wondering if they can be a good benchmark to get a better sense of how good they really are at problem solving? reply bufferoverflow 1 hour agoparentModern LLMs are very good at solving programming puzzles like that. As soon as it recognizes which algorithm needs to apply, it's just massaging the data to the right format. reply cjauvin 1 hour agorootparentYour answer somehow suggests that solving such problems is merely \"recognizing which algorithms to apply\".. which is another way of saying that they are more \"pattern matchers\" than \"true reasoners\". I would think, on the contrary, that these problems (at least the tougher ones that are coming in about two weeks) require more than pattern matching, but I'm not sure exactly what are my thoughts on that. reply gavinhoward 4 hours agoprevThis is the first year that I will do AoC. I have never wanted to, but I have a working language of my own now. https://gavinhoward.com/2024/11/advent-of-code-2024-getting-... reply criddell 6 hours agoprev> You don't need a computer science background to participate - just a little programming knowledge and some problem solving skills will get you pretty far. The use of “pretty far” gives them a bit of an out, but I think this statement is a little disingenuous. Last year, at least, a bunch of the problems needed fairly sophisticated algorithms to find the solution in a reasonable amount of time. To me, a little programming knowledge is what somebody who is six weeks into their introduction to programming class has. They know variables, loops, lists, and maybe associative arrays. reply kadoban 4 hours agoparent\"Participate\" doesn't mean \"win\". You can look things up as well. I haven't done a ton of AoC, but the harder ones from it I've seen were not very hard to search/research for the correct algorithm (even avoiding \"what's the answer for day X\" searches). reply wenderen 5 hours agoparentprevAnecdote to support your comment: The Chinese Remainder Theorem has featured in Advent of Code at least twice IIRC. Not an algorithm the average programmer (average is a very fuzzy term, yeah) would know. reply Jtsummers 3 hours agorootparent> The Chinese Remainder Theorem has featured in Advent of Code at least twice IIRC Fortunately it's never been needed. Every time it's come up the problem has been solvable with high school algebra level math skills (you need to know what the lcm is and that's covered in middle school in many places). If you knew the CRT you could jump straight to a solution, but a solution was easily derived using algebra and a couple loops. reply benmccann 2 hours agoprevSvelte is doing its own Advent of Code challenge with a twist this year. Instead of challenging users, the maintainers will be challenging themselves to launch one feature a day. Today's feature is error boundaries! https://bsky.app/profile/svelte.dev/post/3lcavobxn7c2k reply orra 6 hours agoprevI would love it if the first star of the day was required, but the second was a bonus. I love Advent of Code, but I don't have the time to get 50 stars. reply andix 6 hours agoparentNo stars are required at all. You can also skip some days if you don't want to do them. Or only to the first stars. However you like. First/silver stars will also increase your score in the leader boards. reply 4hg4ufxhy 5 hours agorootparentI think you need all stars to do the final puzzle. reply lancebeet 4 hours agorootparentI might be mistaken but I believe the final day has only one puzzle, but the 50th star (a.k.a., the second star of December 25th) is given for having solved everything else. I don't think any puzzles are \"locked\". reply SketchySeaBeast 3 hours agorootparentYou're right. The second star is a freebie for completing the rest. reply andix 4 hours agorootparentprevThat might be true, I've only completed it once a few years ago. But if you don't want to the more complicated parts, then you want to skip the last one for sure. reply linhns 4 hours agoparentprevSame. Especially when it gets really hard later on. Despite studying CS, I still struggle with it. reply ArcHound 7 hours agoprevI love AoC. I made a CLI client in python for me (and a lib for common tasks, especially the 2d grid maps). Repo: https://github.com/ArcHound/advent_of_code Writeup: https://blog.miloslavhomer.cz/p/advent-of-code-cli-client-an... Good luck to y'all in 2024 and enjoy! reply lyxell 4 hours agoprevI’m using sqlite this year. Hoping that there won’t be any computational geometry or trie problems. Kind of hoping for a graph problem solvable with recursive CTEs, that would be cool. reply tmountain 9 hours agoprevIt feels like AI will shape a lot of the submissions for these types of coding “traditions”. But, I guess that’s just the world we live in. reply uludag 8 hours agoparentI'm actually pleasantly surprised by the results. I like to think that despite problem 1 being easily solvable by LLMs, just about everyone (sans qianxyz) read the FAQ, and decided that they would forego a leaderboard spot for the sake of this coding tradition. Either that, or there were hundreds of people trying and none were able to get it working despite the basic problem. I like to imagine most people reading the rules and being a good sport. reply Retr0id 9 hours agoparentprevEven before AI I was never fast enough for a respectable global rank, but it's still fun to compare solutions with friends. reply Barrin92 6 hours agoparentprevThe global leaderboard is so fast that any AI assistance would literally slow them down, here's one of the guys who tends to score highly solving today's puzzle. (https://youtu.be/ym1ae-vBy6g), and on the more complicated days that's even more pronounced because anyone who is even somewhat decent doesn't need to ask chatgpt how to write Dijkstra. Obviously if you're doing it recreationally you can cheat with AI but then again that's no different than copying a solution from reddit and you're only fooling yourself. I don't see it having an impact. reply Kiro 6 hours agorootparentThe \"winner\" solved it in 9 seconds using AI so I don't understand how you can claim that. https://adventofcode.com/2024/leaderboard/day/1 reply anonu 4 hours agorootparentprevPython looks excruciatingly slow to me. If you want fast I believe you need to think and write in vector languages like kdb+/q. I am not a kdb+ expert by any means and my code can probably use more q primitives, but here was my solution in ~2 minutes: i1:(\"I I\";\" \")0: `:1.txt; sum {abs last deltas x }each flip asc each i1 / answer 1 sum {x * sum x = i1[1]}each i1[0] / answer 2 reply eddsolves 1 hour agorootparentThat’s cool, but horribly ugly! What does production kdb+ look like? Is it similarly terse, maybe Perl-ish, or was this purely for speed? What does each flip asc do? reply anonu 23 minutes agorootparenti1 is Nx2. asc each i1 sorts each N in ascending order. flip transposes to 2xN. reply MattRix 5 hours agorootparentprevThe thing is that the AI can read a puzzle faster than a human can. If someone put any effort towards an AI-based setup, it would easily beat human competitiors (well, up until the point the puzzles got too difficult for it to solve). reply petercooper 4 hours agorootparentI've always done AoC \"properly\" but this year I've decided to actually use it as a learning experience for working with LLMs (and I don't get up early so will never sully the leaderboard) and trying some experiments along the way. I think the strategy for the harder puzzles is to still \"do\" them yourself (i.e. read the challenge and understand it) but write the solution in English pseudocode and then have an LLM take it from there. Doing this has yielded perfect results (but less than perfect implementations) in several languages for me so far and I've learnt a few interesting things about how they perform and the \"tells\" that an LLM was involved. reply wiseowise 8 hours agoparentprevAll the people I know already participate on private boards. AI enshitification will only speed up the process of moving there. reply andix 5 hours agorootparentIt also doesn't make any sense for most of the people to compete with the geniuses on the public leader board. It's like signing up for the Olympics as an amateur athlete. reply danielbln 7 hours agorootparentprevAre we using enshittification for everything we don't like these days? We invented calculators, those really enshittified manual arithmetic puzzles. Private boards for this stuff makes sense anyway, it's the Internet afterall. reply davidcbc 1 hour agorootparentJust the shitty things like using LLMs to spoil a fun competition reply wiseowise 5 hours agorootparentprevUsing LLM for this kind of thing is like using cheats or aim assist for online games. So yes, this is prime example of enshifittication. reply ikesau 3 hours agorootparentenshittification isn't \"things become worse\" - it's the specific process of how services worsen in 3 stages: > Here is how platforms die: first, they are good to their users; then they abuse their users to make things better for their business customers; finally, they abuse those business customers to claw back all the value for themselves. Then, they die. I call this enshittification, and it is a seemingly inevitable consequence arising from the combination of the ease of changing how a platform allocates value, combined with the nature of a \"two-sided market\", where a platform sits between buyers and sellers, hold each hostage to the other, raking off an ever-larger share of the value that passes between them. reply petercooper 4 hours agorootparentprevI think the intent is important. Using LLMs to do well on the public leaderboard is like using cheats/aim assist. But learning how to use LLMs to solve complex puzzles independent of any sense of \"competition\" is more like when people train neural networks to drive a car in GTA or something - it's not hurting anyone and it can be a real learning experience that leads to other interesting byproducts. But, yeah, don't use LLMs to try and get 9 second solve times on the public leaderboard, it's not in the spirit of the thing and is more like taking a dictionary to a spelling bee. reply ktaqhg 5 hours agorootparentprevNo, we do not. Calculators are a whole different issue from LLMs, which plagiarize and spoonfeed whole paragraphs of thought. Enshittification occurs when previously good or excellent things are replaced by mediocre things that are good enough for those susceptible for advertising and group think. Examples are McDonalds vs. real restaurants, Disney theme parks vs. Paris, the interior of modern cars, search engine decline, software bloat etc. reply fleeno 4 hours agoprevI usually get stuck about halfway through and give up. Doing it in a Lisp I’ve been writing in Ruby will have me giving up even sooner, but it will be fun! reply wizrrd 5 hours agoprevYup. Advent of Code is a fun, yearly alternative to LeetCode with a Christmas theme, and it's approachable for novice developers. reply anonzzzies 5 hours agoprevOne of the things I look forward to. It is a lot of fun. reply rubyfan 3 hours agoprevI don’t code for a living anymore so this looks fun. reply akkad33 2 hours agoparentWhat do you do then? reply mvdtnz 34 minutes agoprevI haven't completed one before and I have a question for those who have. Can I treat the entire month as one code based which evolves over time, with an evolving set of unit and integration tests? Or do some days require starting from scratch? Lots of people here are talking about completing it in exotic languages but I'm more interested in using it to sharpen enterprise coding techniques. reply Jtsummers 19 minutes agoparentEvery day is mostly independent of the others. I generally don't reuse code because most of the common code is easy enough to rewrite, sometimes I copy/paste earlier code in to a new solution. The main exception is 2019's Intcode VM/interpreter which was used over about a dozen days (I mentioned it elsewhere, he hasn't done anything like that since). Occasionally, beyond basic algorithms there is some continuity between 2-3 days but that's not terribly common, I don't think there were any pairs of puzzles like that last year or the year before. If you want to make one code base, you'll still have 25 or so separate solutions. You might have some continuity with common algorithms (like A*) that get used a fair bit in search problems or abstracting out the input handling when you identify common structures for parsing. reply markus_zhang 2 hours agoprevDoes anyone know what is the theme of this year? If it's VM or anything low level then it's fun. reply MortyWaves 2 hours agoparentIt’s only ever elves finding and losing things reply ruuda 4 hours agoprevThis has been a good driver for me to add features to https://rcl-lang.org/. I just added List.sort for today’s problem. reply SushiHippie 2 hours agoparentInteresting, do you publish your AoC solutions in rcl somewhere? EDIT: nevermind, found it! https://github.com/ruuda/adventofcode/tree/master reply neonsunset 7 hours agoprevI can't explain why but doing AoC is always interesting, and doing LeetCode which is supposed to be similar is always very depressing. Wishing everyone a fun challenge. This year I will be practicing F# and hope some of you will give it a try too :) https://github.com/neon-sunset/AOC24/blob/master/day1.fsx reply BitterAmethyst 4 hours agoparentI’m also doing them in F# (again). Hoping to best my past attempts, never gone beyond day 12 before! I’m never gonna do it fast but I enjoy using fparsec to get the input text in whatever format I need each time, even if writing and debugging the parsers sometimes takes up way too much of my time! reply sailorganymede 6 hours agoparentprevAoC would be more stressful if you had to solve a problem in 30 minutes for a highly competitive job. reply acomjean 4 hours agorootparentthose exist. except I think the one I did was an hour(? it was ten years ago). I got the interview, but that didn't go well, so I went to work elsewhere. they have part of the site that has lessons for developers: https://app.codility.com/programmers/ I haven't used it for a long time, but they also have contests with some small prizes: https://app.codility.com/programmers/challenges/ reply jacamera 2 hours agoparentprevHave you ever tried the LeetCode live competitions? I found those to be really fun with a great community. Just grinding problems in isolation can definitely be depressing. reply kadoban 4 hours agoparentprevThe community is most of the difference. That and the problems aren't very similar to leetcode. reply foooorsyth 6 hours agoparentprevThe Christmas theme is fun and it’s nice to do one problem per day with a large community. With leetcode you’re off in the woods by yourself. Stuck on a problem? Here’s a cold write up. Finished a problem? Do another, monkey. reply andai 7 hours agoparentprevThat's actually a very important question I think, and sorely neglected by most educators! (and book authors, etc) reply ucefkh 1 hour agoprevGetting the vibe of Google code jam Pretty good tbh reply bradley13 5 hours agoprevI never have made it to the end of a season, just due to lack of time. It's great fun, though - already solved today's \"warm up\" puzzles... reply dwayne_dibley 4 hours agoprevIs this any good for beginners/kids? reply michaelfm1211 3 hours agoparentSure, but it starts to get hard a few days in so beginners won't be able to finish. However, I think that makes it an amazing learning opportunity. There's plenty of write-ups on solutions on the internet. reply matsemann 8 hours agoprevI love AoC. You don't have to care about the AI bots solving it or people waking up earlier than you, just solve it for your own fun. Either because you like the challenges, or to try it in a new language etc. I like to do them in a functional style in Kotlin as far as possible, as that's different from what I do at work. Edit: Here's mine from today, with my utils it's not exactly plain kotlin, but part of the fun is building a library of sorts with cool functions https://github.com/Matsemann/algorithm-problems/blob/main/ad... reply c-fe 6 hours agoparentNice one, i like your transpose function, as python-numpy user in past years, but kotlin this year, i really missed the transpose function the most for this day one. My code: https://github.com/charelF/AdventOfCode/blob/main/kt/src/y20... reply matsemann 2 hours agorootparentUnzip was nice, will remember that one! This is also what I like, reading other's solutions and learning new stuff. I browse the subreddit after solving it myself to see all kinds of cool approaches. reply ucefkh 1 hour agoprevGetting the vibe of Google code jam, pretty good reply tslater2006 4 hours agoprevI maintain a joke domain which I've found sadly applicable through the years. https://adventofrealizingicantread.com. I try to keep it updated pointing to the current day throughout the month I've found especially as the month progresses it's just as much Advent of Reading Comprehension as it is coding :) reply librasteve 8 hours agoprevother advents are available… https://raku-advent.blog/2024/12/01/day-8-rendering-down-for... https://news.ycombinator.com/item?id=42286954 reply Enk1du 8 hours agoparentand more advents... https://pdl.perl.org/advent/index.html reply sundarurfriend 7 hours agorootparentI was looking for a puzzle in the first link until I saw the home page: \"One article on Raku per day, until Christmas.\" I vaguely remember these from my Perl days (a decade ago) now. AoC usually loses my interest around day 6 or so, but a PDL journey for the advent sounds a lot more appealing. Time to dust off my Perl skills and see if I find it as fun today as I did back then. reply librasteve 7 hours agorootparentprevyes perl was the original advent of code iirc reply ranger_danger 2 hours agoprevI read the entire page and I don't see anywhere that it actually lists what the puzzles are, I'm so confused. reply auselen 2 hours agoparent- https://adventofcode.com/2024/day/1 - https://adventofcode.com/ reply arjvik 9 hours agoprev9 seconds to get both stars is absolutely insane - there had to be some AI assistance here. Come to think of it, a pipeline that feeds the problem text into an LLM to generate a solution and automatically runs it on the input and attempts to submit the solution, doing this N times in parallel, could certainly solve the first few days' problem in 9 seconds. reply emadb 8 hours agoparentI participate almost every year but I don't care about the leaderboard. The timezone play a crucial role in being able to be ready at the right time, so actually who cares? I prefer to build private leaderboards with my friends and colleagues. reply fuglede_ 9 hours agoparentprevYep, there was; they even wrote so in their commit message before removing it: https://old.reddit.com/r/adventofcode/comments/1h3w7mc/2024_... reply Almondsetat 8 hours agoparentprevCaring about the leaderboards is the problem. Are you (impersonal) seriously doing AoC for clout or something? reply aithrowawaycomm 3 hours agorootparentThe primary reason to not care about AoC leaderboards is that that it penalizes people for being in the wrong time zone. That said, the top 100 or so contributors clearly do care about these things and using an LLM is cheating. In particular the LLM cheating isn’t just by conjuring a solution: humans don’t get ASCII characters pumped directly into their brain, we have to slowly read problem descriptions with our eyes. It takes humans more than 9 seconds to solve AoC #1 purely because of unavoidable latency. reply thinkingemote 5 hours agorootparentprevIf people are concerned about leaderboards there are private leaderboards: https://adventofcode.com/2024/leaderboard/private Personally I don't do it to compete, I just like puzzles. reply nicce 8 hours agorootparentprevI did it without AI last year and planning to do it again, for fun. reply zwnow 8 hours agorootparentprevTrue. Leaderboards are and always have been full of cheaters. reply nneonneo 9 hours agoparentprevIt was, of course, an AI-generated solution; they posted it here: https://web.archive.org/web/20241201052156/https://github.co... Later, after being called out on it, they posted an apology to their GitHub profile (https://web.archive.org/web/20241201064816/https://github.co...): \"If you are here from the AoC leaderboard, I apologize for not reading the FAQ. Won't happen again.\" Both the repo and that message are now gone. reply dsissitka 9 hours agoparentprevFor comparison, here's how long it took in past years: 2015 - 10:55 2016 - 7:01 2017 - 1:16 2018 - 1:48 2019 - 1:39 2020 - 7:11 2021 - 1:07 2022 - 0:53 2023 - 2:24 And this year's second place was 0:54. reply fuglede_ 8 hours agorootparentNote that regarding the outliers, in 2015 and 2016 the puzzles weren't as widely known, and in 2020, AWS' load balancers crashed and the puzzle was unavailable to most people for 6 minutes, then solved in a few minutes. https://adventofcode.com/2020/leaderboard/day/1 -- postmortem: https://old.reddit.com/r/adventofcode/comments/k9lt09/postmo... reply isoprophlex 9 hours agoparentprevAI coding assistants ruined the global leaderboard experience. AoC might as well nerf it by discarding the quickest x percent of submissions, or something... reply fuglede_ 9 hours agorootparentIt's not that bad. I'm sure there are more LLM'ers in there than the one, but you can tell that the majority of the day 1 leaderboard is made up of people who have historically performed well, even before LLMs were a thing. Compare https://adventofcode.com/2024/leaderboard/day/1 to e.g. https://fuglede.github.io/aoc-full-leaderboard/ There was also at least one instance of people working together where you would have 15 people from the same company submit solutions at the same time, which can be a bit frustrating but again, not a huge issue. reply gorgoiler 8 hours agorootparentprevI have a rule in life: no summary statistics without showing the distribution. Usually this goes for any median which might be in a sneaky bimodal distribution of, say, AI models vs humans. I guess it applies to leaderboards too though. reply exitb 8 hours agorootparentprevPotentially the challenge just doesn’t make as much sense anymore? There apparently are „mental calculations” competitions and I’m sure their participants have fun. Yet I can hardly imagine doing arithmetic in ones head is any fun for an average mathematician. The challenge just shifted elsewhere over time. reply WithinReason 9 hours agorootparentprevThey should check that LLMs can't solve the problems in 9 seconds and come up with appropriate problems. Or just allow AI assistants, they are now as much part of the programmer's toolkit as syntax highlighting or autocomplete or Stack Overflow, and pretending otherwise is not useful. reply martin-t 8 hours agorootparentNot gonna happen. AoC always starts with beginner level problems. That's why it's so commonly used for learning the basics of new languages. A problem that wouldn't be immediately solvable by LLMs would either be too advanced or simply too large to be fun. This is probably where programming as a whole is going. Many of the things that make programming fun for me, like deeply understanding a small but non-trivial problem and finding a good solution, are gonna be performed much faster by LLMs. After all most of what we do has been done before, just in a slightly different content or a different language. Either LLMs will peak out at the current level and be often useful but very error prone and not-quite-there. Or they'll get better and we'll be just checking their output and designing the general architecture. reply Retr0id 9 hours agorootparentprevThe first few days are supposed to be beginner-accessible, it's practically impossible to have something beginner accessible but GPT-inaccessible. reply zwirbl 7 hours agorootparentprevAnd then allow aimbots for counterstrike, stockfish at chess tournaments and Epo on the tour de France. The leader board is intended for people to compete against each other, one could make a separate leaderboard for LLM, kind of similar to the chess AI leaderboards. reply falcor84 7 hours agorootparent> allow aimbots for counterstrike I'm not played counterstrike in over a decade, so you got me wondering - are there matches where everyone uses aimbots? What does the game look like then? I suppose there's a new mix of strategies evolving, with a higher focus on the macro movement planning? reply worthless-trash 7 hours agorootparent> are there matches where everyone uses aimbots? Yes > What does the game look like then? I have only observed the games, it requires a lot of hiding. Most of the time the winning method is to act at the very last second and hope the other player is distracted. reply WithinReason 7 hours agorootparentprevFalse equivalence. The sole reason for counterstrike and chess to exist is competition. Programming is about solving a problem. If you want to turn programming into a competition you shouldn't take away tools from the programmer. reply jbjbjbjb 7 hours agorootparentYou’re saying programming isn’t not equivalent to chess here because programming isn’t a competition, but the Advent of Code leaderboard very much is a competition. reply matsemann 8 hours agorootparentprevThat's like going out for a run and taking an electrical scooter around the park instead. The point isn't finishing, the point is doing the activity. reply WithinReason 8 hours agorootparentThen why have a leaderboard? reply matsemann 8 hours agorootparentBecause someone likes to compete? There are 5k races as well, which people enjoy to do even though vehicles exist. And people would rightfully be upset if they got beaten by someone not running themselves. reply hmottestad 9 hours agoparentprevOpenAI did something similar with their o1 model. Ran a coding problem through o1 thousands or maybe millions of times and then checked if the solution was correct. I can imagine a great pipeline for performance optimization: 1. have an AI generate millions of tests for your existing code 2. have another AI generate faster code that still makes the tests pass So I guess all I want for Christmas is a massive compute cluster and infinite OpenAI credits :P reply exitb 8 hours agoparentprevI wonder how many seconds could be won by the organizers if the challenge included a blant prompt injection breaking the result. reply rich_sasha 7 hours agoparentprevWithout condoning cheating, I am impressed with the automation aspect of it. 9 seconds sounds more or less like the inference time of the LLM, so this must have been automated. Login at midnight + lots of C&P may not have done it. Perhaps there is a scope for an alternative AoC type competition aimed at AI submissions... ...though of course that would be experimenting to get us all out of work. Hmm. reply zwnow 7 hours agorootparentIf real life problems were as easy and defined as AoC problems we might be able to be replaced at some point. I highly doubt you can replace software devs otherwise. Who else is going to take the blame for software issues? reply rich_sasha 6 hours agorootparent\"Write me a snippet that does X\" is a step behind \"figure out how to log into this page, download the data, write a snippet that gives the right answer to the sample data, then run it on the real thing and submit the output to the text box\". reply tags2k 7 hours agoparentprevAs with real life, the speed generally doesn't matter as long as you get a working solution and you find it fun. If you find \"copy and paste into an LLM and then copy and paste the answer back out\" fun, then I suppose you do you. I didn't realise it was be timed, which is good because I casually set up a new rig to give future puzzles some kind of rig. I used C# which, although probably more wordy than other solutions, did the job and LINQ made light work of the list operations. Ended up with about 6.5 minutes for each one but most of that was refactoring out of pedantry. reply rvz 8 hours agoparentprevAt this point, it just shows that Advent of Code is completely worthless given the ease and accessibility of AI-assisted tools to solve these problems. RIP Advent of Code. reply abenga 8 hours agorootparentWhy? Solving interesting problems to learn is a worthwhile goal. Why should it matter to you that others are \"cheating\"? reply zwirbl 7 hours agorootparentBecause competing against other people is the fun part for some people. Why not allow everyone to use stockfish at chess tournaments? reply graynk 7 hours agorootparent“For _some_ people” is at odds with “_completely_ worthless”, don’t you think? reply dakiol 3 hours agorootparentTo be fair, I read it as \"_completely_ worthless for _some_ people\" reply zwnow 7 hours agorootparentprevThis is stupid though. Advent of Code Leaderbords were always full of cheaters. At least since 2020 when I first started. If you want competitive programming, AoC is not the place for that. reply wiseowise 8 hours agorootparentprevSarcasm? reply peanut-walrus 7 hours agoprevI love AoC, but I hate the timing. December is always the busiest month of the year, so I really almost never find the time for it :( reply criddell 7 hours agoparentYou don’t have to do it in December. I was still working on last years in March. reply bernds74 9 hours agoprevNice to see that traditions are upheld, such as the unreadable font and colors. reply simonjgreen 9 hours agoparentAnd the timeless tradition of HN commentators cynicism reply MortyWaves 8 hours agorootparentAnd the timeless tradition of criticising perfectly valid concerns. Not everyone wants to be a member of the dark mode lobby. reply arp242 8 hours agorootparentprevI had to fiddle with the colours in the web inspector to even read the page. So yeah... Of all the cynical comments I see on HN, this ranks very very low. reply matsemann 8 hours agorootparentReader mode or \"View -> Page Style -> High contrast\" in your browser works perfectly fine. reply block_dagger 8 hours agorootparentprevAnd the timeless tradition of HN commentators contempt reply moffkalast 8 hours agorootparentWouldn't have it any other way :) reply delta_p_delta_x 8 hours agoparentprevReposting a comment below: I used Stylus[1]. There are already some considerably more readable themes by the community. [1]: https://github.com/openstyles/stylus reply lpapez 7 hours agoparentprevIt's a simple webpage. If you don't like it, you can customize the CSS with a few custom rules in your browser. reply xPaw 5 hours agoparentprevYeah, I made a userscript for myself to fix that up: https://gist.github.com/xPaw/4fffaf776fe14d15c9602991f51dbfa... reply bberrry 8 hours agoparentprevPersonally I love the esthetic and hope they never change it. reply delta_p_delta_x 9 hours agoprevIs there a way to override the CSS (on Chromium)? The body font and weight, Source Code Pro is far too thin and far too wide and gives me a headache (and has regularly turned me off AoC). I'd like to change it to `sans-serif`. reply delta_p_delta_x 8 hours agoparentSelf-response: Stylus is useful here[1]. There is a list of already more readable styles than the default that the community has prepared. [1]: https://github.com/openstyles/stylus reply 9029 8 hours agoparentprevI have used Stylus for CSS tweaks: https://addons.mozilla.org/firefox/addon/styl-us/ reply babo 7 hours agoparentprevThere are CLI tools to fetch and submit solutions. At least one of them allows you to download the puzzle description as markup: https://github.com/scarvalhojr/aoc-cli reply GCUMstlyHarmls 7 hours agoparentprevYou can hit the \"Reader\" button on Firefox (ctrl-alt-r), I think Chrome also has that? Or an extension that does the same thing would probably work. Bit simpler than writing an override or fetching the HTML. reply riffraff 8 hours agoparentprevOn thing you can do is fetch the html and extract the main content to display it in your favorite way, it's been stable for years, and you can use the (stable) session_id cookie, it's how many people approach the challenge. reply mcintyre1994 7 hours agoparentprevI'd just create a bookmarklet or run JS in devtools: `document.body.style.fontFamily = \"sans-serif\";` etc reply devit 5 hours agoprevThe first day problem is trivial and also solved in a single attempt by o1-preview. It would be nice if they actually proposed interesting problems and of course checked that LLMs can't solve them. reply criddell 28 minutes agoparentUsing an AI to solve AoC is like taking an art class and complaining that the still life exercises aren’t very interesting because your camera can capture the image quickly. reply awkwardpotato 3 hours agoparentprevThis is already answered right on the about page[0]. AoC is meant to be an educational experience, feeding it directly into a LLM isn't that. > The leaderboards are for human competitors; if you want to compare the speed of your AI solver with others, please do so elsewhere. (If you want to use AI to help you solve puzzles, I can't really stop you, but I feel like it's harder to get better at programming if you ask an AI to do the programming for you.) [0] https://adventofcode.com/2024/about reply hshshshshsh 3 hours agoprev [–] Work on a side project this December instead of doing this. Solving advent of code just keeps you in your comfort zone. Creates a false sense of accomplishment. Redirect all the positive energy to something that will make you proud when you are old or help with an earlier retirement. This won't. reply mattbaker 3 hours agoparentAdvent of Code is fun, I like having fun! I have no shortage of challenges and things to be proud of, but when I’m old I don’t want to look back and realize I didn’t take the time to find little moments of joy along the way. Everyone’s different, and it sounds like AoC isn’t for you, but remember not everyone is you :) reply moomin 2 hours agorootparentI used it to spend time in Haskell. You won’t win any prizes for speed, but it’s a good way to learn a bunch of new stuff. reply hshshshshsh 2 hours agorootparentprevThat's fair. Just sending the message out to folks who are in similiar space and might do a reconsideration. reply grayhatter 2 hours agoparentprev> Redirect all the positive energy to something that will make you proud when you are old or help with an earlier retirement. This won't. So if some activity [work] doesn't contribute to your ability to never have to work again [retirement], you shouldn't do it? What if I've retired, am I allowed to do AOC then? Or then does the first rule of, only do something the future you would be proud of apply? What if all of the things I'm proud of are just a false sense of accomplishments? How do I know when I'm actually allowed to be proud of it, or if it's just a false sense of pride? reply kissiel 3 hours agoparentprevOr use it to practice a language that you just wanted to give a spin. reply hombre_fatal 1 hour agoparentprevYet you're on HN instead of working on your sideproject. reply yoyohello13 2 hours agoparentprev [–] Learn to let go of the constant need to be productive. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Advent of Code, created by Eric Wastl, is an Advent calendar featuring programming puzzles suitable for all skill levels, which can be solved in any programming language.",
      "The puzzles are designed to run efficiently on older hardware and are intended for practice, interviews, or entertainment, without requiring a computer science background.",
      "Participants are encouraged to share their experiences, test solutions with examples, and seek help if needed, while adhering to guidelines such as not using AI for solving puzzles and respecting trademark rules."
    ],
    "commentSummary": [
      "Advent of Code 2024 has begun, with participants engaging in creative problem-solving and sharing their experiences.- Some participants are experimenting with unique approaches, such as using C without standard libraries or exploring new programming languages like Swift or Ada.- While there are concerns about AI's impact on the leaderboard, the event primarily fosters personal enjoyment, learning, and community interaction."
    ],
    "points": 726,
    "commentCount": 277,
    "retryCount": 0,
    "time": 1733044175
  },
  {
    "id": 42284202,
    "title": "A Brazilian CA trusted only by Microsoft has issued a certificate for google.com",
    "originLink": "https://follow.agwa.name/notice/AoZSMI38xcA3TrN1sm",
    "originBody": "Andrew Ayer in the Fediverse{\"\\/api\\/pleroma\\/frontend_configurations\":\"eyJwbGVyb21hX2ZlIjp7ImJhY2tncm91bmQiOiJcL2ltYWdlc1wvYmFja2dyb3VuZC5qcGciLCJyZWRpcmVjdFJvb3ROb0xvZ2luIjoiXC9tYWluXC9wdWJsaWMiLCJzaG93RmVhdHVyZXNQYW5lbCI6ZmFsc2UsInRoZW1lIjoiYnJlZXp5LWRhcmsifX0=\",\"\\/api\\/v1\\/instance\":\"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\",\"\\/instance\\/panel.html\":\"Ilx1MDAzQ2RpdiBzdHlsZT1cIm1hcmdpbi1sZWZ0OjEycHg7IG1hcmdpbi1yaWdodDoxMnB4XCI+XG5cdTAwM0NwPldlbGNvbWUgdG8gXHUwMDNDYSBocmVmPVwiaHR0cHM6XC9cL3BsZXJvbWEuc29jaWFsXCIgdGFyZ2V0PVwiX2JsYW5rXCI+UGxlcm9tYSFcdTAwM0NcL2E+XHUwMDNDXC9wPlxuXHUwMDNDcD5cdTAwM0NhIGhyZWY9XCJcL21haW5cL2FsbFwiPlBsZXJvbWEgRkVcdTAwM0NcL2E+XHUwMDNDXC9wPlxuXHUwMDNDXC9kaXY+XG5cbiI=\",\"\\/nodeinfo\\/2.0.json\":\"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\"}To use Pleroma, please enable JavaScript.",
    "commentLink": "https://news.ycombinator.com/item?id=42284202",
    "commentBody": "A Brazilian CA trusted only by Microsoft has issued a certificate for google.com (agwa.name)438 points by sanqui 21 hours agohidepastfavorite181 comments leonidasv 17 hours agoICP-Brasil officially stopped emitting public-facing SSL/TLS certificates in October: https://www.gov.br/iti/pt-br/assuntos/noticias/indice-de-not... This is pretty bad. Someone circunvented the ban on emitting public certificates but also disrespected Google's CAA rules. Hope this CA gets banned on Microsoft OSes for good. reply TheRealPomax 16 hours agoparentnext [8 more] [flagged] semitones 16 hours agorootparentDo you actually understand what's going here? reply raincole 14 hours agorootparentAs someone who doesn't understand what's actually going on: could someone ELI5? reply saagarjha 13 hours agorootparentCAs are in the business of being a trusted third party that, among other things, verifies the identity of things. In this case someone seems to have scammed/hacked/whatever the CA into issuing a certificate for google.com, which is clearly bogus. So the result is that we should not trust this CA anymore. reply raincole 13 hours agorootparentBut why would someone hacked a CA to just... issue a certificate for google.com? How does it benifit them? I'd imagine they issue a certficate for some phishing sites or something. reply tsimionescu 11 hours agorootparentIn short, whoever has that certificate can now come between your PC and the real Google, and tell your PC \"here is the real google.com\" while serving you malware, in a way that Windows will trust. You typed in google.com in the address bar, your browser got the attacker's IP, and then the attacker has a certificate that says they're the real google.com, so your browser will go \"all right, all good\". Basically the way certificates work is that whoever has a certificate for a domain name will be able to serve anything they want and browsers will accept this is the real domain. To turn this into an attack, they just need to trick your DNS into pointing your to their machine, or to intercept your traffic even while you're accessing the real server. Getting a certificate for \"mytotallyrealnotascamwinkwink.phishing.com\" is not useful in any way for an attacker: the whole idea is to have the user think they are on a trusted site like google.com, while in reality looking at the attacker's site. reply cmeacham98 13 hours agorootparentprevIt's entirely possible this certificate is being used to mitm attack and phish people right now. reply salawat 11 hours agorootparentprevSo... Think of it like this. All of us have tried to shield users from having to develop or maintain their own trust networks. It's a hell of a lot of work. And it isn't sexy. These CA's are basically that implementation. They have been programmed in by default on most devices to be trusted, and changing that status after the fact is very hard. The reason someone would want to to be able to issue one of these certs is it essentially allows them to eavesdrop on normally unreadable connection data because the device thinks the system in the middle is actually a trustworthy endpoint, and not a malicious TLS terminating proxy. No one whose devices are by default trusting that CA now have any guarantee of confidentiality on any connection to a system presenting those issued certs. reply cjalmeida 17 hours agoprevIt gets worse. ICP-Brasil, the AC mentioned in the bug reports, the the government run agency responsible for all things related to digital signatures. Digitally signing a contract, a deed, accessing tax returns… reply justinclift 11 hours agoparentSo you're saying it's only a matter of time until they issue a cert for x.com as well? :) reply layer8 16 hours agoparentprevUnlike web browsers, digital signature use cases should perform revocation checks, so revoking the google.com certificate should solve that. reply lxgr 15 hours agorootparentThe problem here isn't really that one mis-issued certificate, but rather the general problematic behavior of that CA reported in TFA. If a CA can be convinced to issue a server certificate for google.com, would you feel very comfortable trusting their contract/deed/... signing certificates? reply Muromec 8 hours agorootparentIf the government says you need to use their CA, you may feel the feelings, but you will still use them reply perching_aix 16 hours agorootparentprevI think the current \"meta\" is CAA records? https://blog.cloudflare.com/why-certificate-pinning-is-outda... reply 8organicbits 15 hours agorootparentCorrect, which Google is using: https://www.nslookup.io/domains/google.com/dns-records/caa/ reply danpalmer 18 hours agoprevThis is a bad look. I expected the result would be Chrome and Firefox dropping trust for this CA, but they already don't trust this CA. Arguably, Microsoft/Windows trusting a CA that the other big players choose not to trust is an even worse look for Microsoft. reply justinclift 11 hours agoparent> an even worse look for Microsoft. Microsoft have a terrible reputation for security, which they've earned through doing stuff like this. It's not likely to get any better any time soon either, as their trajectory is still pointed downwards. reply danpalmer 10 hours agorootparentI don’t know enough to comment on that reputation, but this surprises me. They’re known for being great at serving and selling to the enterprise, frequently at the expense of end users, and big enterprises/govts care a lot about security usually. Even if much of that caring is box ticking rather than actually looking into the security (hello ISO27001), you’d expect it to result in generally a security conscious culture. reply outworlder 10 hours agorootparentIt's hit and miss. They have one of the largest cyber security operations worldwide and regularly track and dismantle criminal operations. There's some great people working there. Then there's Azure. Which is used by large organizations and you would expect it to have the utmost care when it comes to security. But it often does badly, in several instances it allowed different tenants to access information from one another, something unheard of on AWS. For example: https://www.securityweek.com/microsoft-patches-azure-cross-t... or https://www.theregister.com/2024/06/05/tenable_azure_flaw/ or https://borncity.com/win/2023/08/03/microsoft-as-a-security-... There are so many cross tenant vulnerabilities that there could be some overlap in those URLs, and it's a bit late at night for me to read those carefully, but you get the idea. They do get the most flak about Windows, which used to be a non networked, single user OS. reply justinclift 8 hours agorootparentprev> but this surprises me Unfortunately, it's true. People used to relying on Microsoft understandably don't want it to be so, so they're in for a rough time trying to figure out actually workable alternatives. :( This has been an ongoing problem for years, and every time some new problem is found Microsoft just trots out the PR promises that they'll do better. Without then doing any better. • https://arstechnica.com/information-technology/2022/10/how-a... (2022) • https://arstechnica.com/security/2023/08/microsoft-cloud-sec... (2023) • https://arstechnica.com/information-technology/2024/04/micro... (2024) For the US government's official perspective on Microsoft's security competence, there's the federal Cyber Safety Review Board report released in April this year: • https://www.cisa.gov/sites/default/files/2024-04/CSRB_Review... (2024) \"Throughout this review, the board identified a series of Microsoft operational and strategic decisions that collectively points to a corporate culture that deprioritized both enterprise security investments and rigorous risk management,\" the report reads. And so on. Note that the problems didn't start in 2022, that's just the earliest I could be bothered looking with minimal effort. ;) reply Muromec 8 hours agorootparentprevThat's the problem, the only security culture it produces is thinking of security as annoying box ticking. reply cassianoleal 10 hours agorootparentprev> Even if much of that caring is box ticking rather than actually looking into the security (hello ISO27001), you’d expect it to result in generally a security conscious culture. If the whole value is in ticking the box, why would that develop a culture that values anything more than the tick? reply antonvs 9 hours agorootparentThe cycle usually goes something like box ticking, complacency, security scare, remediation, rinse and repeat. reply jajko 8 hours agorootparentprevCompany pushing constant snooping of all activity of users even on professional/enterprise variants of their OS can't be taken seriously re security, so absolutely no idea where this rumor 'They’re known for being great at serving and selling to the enterprise' comes from. They may be good when luring in customers, but once thats done, they don't give a fuck about anything but their current cash flow. And the fact that ultra-big players can ask them for customized OS distribution that has this turned off (just like my own mega corporation) doesn't change anything on statements above. reply jsheard 17 hours agoparentprevWhat is even the point of a web CA that isn't trusted by all of the major players? Is there one? reply tialaramex 17 hours agorootparentThese are generally government CAs, so, typically the situation is Microsoft sold the government Windows, and as part of that deal (at least tacitly) agreed to the CA being trusted, and so every system that's trusting these certificates is a Windows PC anyway, running Edge because the whole point was the government will only use Windows and pays Microsoft $$$. Why bake it into everybody else's Windows? If you make say a Brazil Government-only Windows which trusts this CA instead, I guarantee somebody crucial in Brazil will buy a 3rd party Windows laptop independently and it doesn't work with this CA's certificates and that ends up as Microsoft's problem to fix, so, easier to just have every Windows device trust the CA. They'll have an assurance from the CA that it won't do this sort of crap, and that's enough, plausible deniability. Microsoft will say they take this \"very seriously\" and do nothing and it'll blow over. After all this stuff happened before and it'll happen again, and Windows will remain very popular. reply efitz 15 hours agorootparentWindows CA program is governed by requirements like any other CA. Microsoft has ways to provision machines with enterprise CA roots so there is no advantage, and highly visible disadvantage, to adding a noncompliant CA to your trust store. I think that the theory that Microsoft will included it to sweeten a sale has no merit, unless you have evidence. Most certificate trust stores have some certs in them that are sketchy, eg a bunch of university certs from all over Europe. These are slowly dropping off, presumably because it costs quite a bit to operate a CA in a compliant fashion and get it professionally audited. Issuing a fake cert is grounds for removal from every certificate trust program I’m aware of, if it can’t be demonstrated that they found what went wrong and have fixed it so it can never happen again. reply lokar 15 hours agorootparentIMO, issuing a fake CA for one of the top (and highest risk) domains even once should be the end of that CA (and any other CAs managed by that org) reply amluto 15 hours agorootparentprevThe solution seems straightforward: limit the trust in the CA to .BR domains. [domain name typo fixed] reply kelnos 15 hours agorootparentIIRC name constraints is very poorly supported by client software, so there are likely lots of clients out there that wouldn't even parse that restriction out of the cert, and happy accept anything singed by the CA. reply 8organicbits 12 hours agorootparentI think support for name constraints is much better now, but I think someone needs to correctly audit it. We need near universal adoption for it to be considered a usable tool. I researched the issue a little here: https://alexsci.com/blog/name-non-constraint/ reply amluto 15 hours agorootparentprevI’m not talking about a name constraint — that would need to be part of the root certificate. I’m suggesting that MS add a feature to its root store to constrain the usage of the certificates in the store. IIRC Google’s root store has features like this. reply tsimionescu 11 hours agorootparentThe Windows trust store doesn't offer a verification API, I believe it simply lists the trusted certificates so that they can be looked up by verification software. That is, OpenSSL doesn't ask windows \"hey, is this certificate with this chain trusted for google.com?\" it asks Windows \"hey, do you have a cert in the trusted root CAs with this ID? If so give it to me\", and then OpenSSL will use that root cert to check if this is the real google.com. Chrome, which is both the cert store and the client on certain OSs, might implement this limited trust. But Windows can't, except maybe for its own internal services. Either way, this makes little sense overall. If a CA is trustable, it can be trusted to sign a certificate for any domain. And if it's not trustable, then you can't trust it for any domain. Brazilian companies wishing to use a local CA can own .com domain names, so you'd be preventing a completely legitimate use case. Google almost certainly has a google.br domain, so if the Brazil CA is untrustworthy, they can still be used to attack Google even if you only trust them for .br domain. reply nordsieck 4 hours agorootparent> Either way, this makes little sense overall. If a CA is trustable, it can be trusted to sign a certificate for any domain. And if it's not trustable, then you can't trust it for any domain. That's a silly position to take. When I lived with roommates, I trusted them. But I also locked my bedroom when I went out. Because there's no good reason to rely on trust when you don't have to. reply tsimionescu 3 hours agorootparentIt is given the design of the PKI and DNS. There's no relation between CA and the TLDs on the certificate being signed. reply bitwize 15 hours agorootparentprev.bz is the TLD for Belize. Brazil is .br. reply awinter-py 16 hours agorootparentprevwhat's the state's interest in having their CA built into windows? reply tptacek 16 hours agorootparentStates are themselves extraordinarily large IT enterprises, they generally want control of traffic and its transparency or protection, and they are large enough to get arrangements for that, though usually not this particular arrangement. Large enterprises in the US generally have the same capability, but not loaded into operating systems by default (that is: Walmart's ability to do this on its own network in no way impacts you, who have never worked on that network). reply adra 15 hours agorootparentIf you're a large enterprise, then it's trivial to add yourself your own custom CA and save the cost/hassle of needing to deal with outside companies. The tradeoff being you need to manage it yourself vs basically paying this third party company to survive? reply tptacek 14 hours agorootparentThat's true, but in the bad-old-days of the antidiluvian WebPKI it was somewhat routine to sell big companies CA=YES certs simply to allow them to do this universally without pushing out updates to all their endpoints. It was a terrible, bad practice, and so far as I know it's completely dead now --- except for Microsoft, I guess. reply hulitu 8 hours agorootparentprev> If you're a large enterprise, then it's trivial to add yourself your own custom CA The big CA have their own \"Boy club\". See Ahmed used cars and certificates. reply mnau 16 hours agorootparentprevE.g. identity verification. My state has a \"qualified\" certificate that can be used to sign contracts and basically everything else you can do in-person. When you can transfer you home with that, there are higher requirements on checking the identity of a person who gets the certificate. That CA is not used for much else and is basically confined to our state. But it has to be in Windows, otherwise no other software could verify the signatures. See eIDAS and other similar schemes. reply tsimionescu 11 hours agorootparentWhy would you want to mix identity verification with the WebPKI? This makes no sense at all. Just because a CA is trusted for web verification doesn't mean it's trusted for identity verification, machine enrollment, or any other purpose. And vice-versa: a CA for identity verification is not in any way trusted for web verification. reply Muromec 4 hours agorootparentI think the idea was to use client certs for strong authentication on the government web services, which didn't rally took off, except maybe in Estonia. reply Muromec 4 hours agorootparentprevYou don't really need your CA doing eIDAS in the system root. This scheme works as a closed system where you need eIDAS app to produce the artifact and another eIDAS app to verify it, when both have their own non-system root. Ukraine for example successfully operates their own eIDAS-like scheme where everything is based on DSTU+GOST algos not supported by any operating systems a major libraries, the certs are signed by the government root and it doesn't leak into web pki. reply efitz 15 hours agorootparentprevGetting your CA into a trust store means that every machine using that trust store will accept your certs. It’s not really necessary for a government or corporation to have a public CA in anyone’s trust store unless they want to issue certificates that everyone trusts. If they just need their own machines to trust their certificates, they can use the management utilities that come with Windows and with AD to distribute an “enterprise root”, which only their machines will trust. This is how most large companies and governments do it. reply csomar 16 hours agorootparentprevSo when they issue their certificates, you don't get that huge red banner? I belong to a small developing country and even with its tech illiteracy it has a CA. Now, of course, because that CA is not trusted by anyone, all government websites are red. reply tsimionescu 11 hours agorootparentprevSo that they don't depend on anyone else to have proper TLS for their state sites and for companies operating in their state. Imagine if you don't have a state CA, and your relationship with the USA goes sour, and the USA prohibits all of their major CAs from doing business with your country, including Let's Encrypt. People in your country still use the internet and you still want to protect them from scammers pretending to be local businesses online. So it's important that you as the state can provide CA services and sign those certificates yourself. Of course, in this scenario you wouldn't want to be relying on Microsoft to help. But the general principle is that any state who can afford it has a strategic interest in having fully self-sufficient Internet infrastructure, including DNS, CAs, IP allocation etc. reply withinboredom 7 hours agorootparentThis seems like a matter of signing a certificate signed by an actual CA with your own CA as well. If the relationship sours, you still have your own CA to vouch for it. reply Onavo 16 hours agorootparentprevSo they can mitm their own employees without annoying TLS warnings. reply throwaway2037 14 hours agorootparentTo be clear, this is bog standard in all mega-corps now. They have a vendor product that provides HTTP Internet proxy, then they perform MitM to decrypt HTTPS traffic and re-sign/encrypt with in-house issued cert. Then, this cert is auto-trusted as part of all base OS installations. To be honest, how else can mega-corps spy on HTTPS traffic without this MitM tactic? I don't know any other way. reply echoangle 9 hours agorootparentYes, but normally this is done by making your own CA and installing it into your client devices, not by getting it into every device globally by working with Microsoft. reply hulitu 8 hours agorootparent> Yes, but normally this is done by making your own CA and installing it into your client devices, not by getting it into every device globally by working with Microsoft. Google, Facebook, Microsoft, Apple, Cloudfare, Godaddy, Lets encrypt. They all \"work with Microsoft\". reply echoangle 8 hours agorootparentDoes any employer get a certificate from any of the CAs you listed to MITM their internal networks? reply 3np 6 hours agorootparentThe listed companies are employers. I think they all have self-managed CAs. reply echoangle 4 hours agorootparentYes, but surely the listed companies don't use their public and globally trusted CAs to MITM their internal networks. I hope they have another internal CA to allow them to MITM their internal Network. reply tsimionescu 11 hours agorootparentprevYou don't need a publicly trusted CA for that. You just run an internal CA and install its root certificate on your employees' machines, just like you install VPN software or whatever else. reply lazide 16 hours agorootparentprevLegitimate, or illegitimate? reply sneak 17 hours agorootparentprevWindows is less popular every year. reply saghm 16 hours agorootparentI feel confident in guessing that any net changes in Windows popularity have close to no relation to Microsoft's policies around trusted CA. The number of users who are worried about sketchy certificates being trusted by default are dwarfed by the number of users who don't have any idea what a \"trusted CA\" is but care about more \"visible\" things like UI changes, performance, and how hard Windows is pushing Edge and other things they don't want. reply l33t7332273 14 hours agorootparentIt’s not becoming the users that are the decision makers. A few CTOs could make decisions based on this reply saghm 14 hours agorootparentIf the rationale in the parent comment for this behavior is correct, it sounds like a lot of people making the decision to use Windows are doing it _because_ of behavior like this, not in spite of it. reply notimetorelax 17 hours agorootparentprevI looked at the graphs at Statista. I don’t think it’s so clear cut. Mobile OSs have pushed it down, but it seem to dominate PC market. Do you have a graph that shows its decline on computers, not mobile phones? Or in absolute unit counts? reply lysace 9 hours agorootparenthttps://gs.statcounter.com/os-market-share/desktop/worldwide... There's a clear but slow trend on desktop. Jan 2009: 95.4% Windows Jan 2016: 85.2% Windows Jan 2024: 73.0% Windows In e.g. US it's going down faster, desktop market share now at 62%: https://gs.statcounter.com/os-market-share/desktop/united-st... reply flir 9 hours agorootparentprevI think that might be a bit of an unfair caveat. People do real work on mobile OSes. They shop and communicate on mobile OSes, and occasionally organise revolutions. (Although I'm not sure why \"Netraft confirms, Windows is dying\" is a useful comment here anyway. Windows is a behemoth.) reply n144q 15 hours agorootparentprevYou need to show statistics to prove that, not just throw the statement out there, possibly only based on the vibes on HN. reply beeflet 17 hours agorootparentprevI suppose it allows you to enable third party control and censorship. If you look at microsoft's censorship of bing in china for example, they are more than willing to bend the knee if it means they can get ahead. reply alganet 16 hours agorootparentAs a brazillian, I find this very unlikely. In 2013, when the same party was in power, SERPRO was tasked with replacing Microsoft in key aspects, such as government email (which was handled by Outlook Server at that time) and operating systems. The main reason was fear of espionage. So, in reality, we are more afraid of the US spying on us than random internet dissidents. reply serial_dev 14 hours agorootparentAs a non Brazilian, sometimes when a government says a company is spying on its citizens, they mean that they want access, too, to the spying and censoring apparatus. reply alganet 13 hours agorootparentI see your point. Maybe if I was in government I would think the same. Catch criminals before they act, stuff like that (I'm just being the devil's advocate here). This is a dillema, and the worst kind. The kind citizens know nothing about, so the only possible way to talk about it is to speculate. I am, however, too old to speculate about these things anymore. reply move-on-by 17 hours agoparentprevAlso being issued on a major US holiday- when many are on PTO- does not help with the look. reply alganet 16 hours agorootparentDuring carnival we brazillians often take 3 or 4 days leave. Would it be fair during that time if I asked you to hold your PRs, bug tickets and work in general because we're on paid leave? On-call rotation exists for those reasons. Otherwise, all countries would need to respect all other countries holidays. In fact, we're not even aware of most US holidays. It is likely to be a coincidence. reply lmm 15 hours agorootparent> Would it be fair during that time if I asked you to hold your PRs, bug tickets and work in general because we're on paid leave? Yes. That's completely normal for companies that do business with Brazil. reply alganet 14 hours agorootparentSorry, my example was bad. In fact, your example is perfect. We're not talking about business. CAs are different. In security and infrastructure, there's always someone working on holidays. The larger the organization, higher are the chances that some kind of rotation exists. reply move-on-by 14 hours agorootparentprevMy comment is not about how all work should stop during US holidays. What I’m attempting to refer to, is that _if_ this was done with malicious intent, then maybe the hope was that doing it during a holiday would reduce response time or allow it to fly under the radar. Of course, as you say, just because it was a holiday does not inherently mean it’s malicious, it has plausible deniability. reply alganet 13 hours agorootparentWhat I actually said is that I believe that the notion of a holiday \"hiding\" these activities is naive. I don't think it makes any difference. I don't know if there's a rotation or another system. I think there are probably multiple across different parties responsible for maintaining CA trust. reply noirbot 14 hours agorootparentprevFor as big a country as Brazil? Totally. I've worked at companies that had minor code freezes for all sorts of holidays in countries we had a big client presence in, specifically to avoid releasing changes to client that wouldn't have engineers in-office to adapt to them. reply JumpCrisscross 14 hours agorootparentprev> we're not even aware of most US holidays You’re not. Someone above you should be. Otherwise that’s incompetence. reply bogota 15 hours agorootparentprevHave you never worked at a multinational company? reply alganet 14 hours agorootparentI did, multiple times with multiple countries. All of them had some sort of call rotation. Someone was always at the helm, _specially_ in infrastructure and security. There are whole startups designed to solve this, like PagerDuty. I am now very curious to understand where your question comes from. There must be some misunderstanding here. You never went on-call or seen a friend do it? reply JumpCrisscross 14 hours agorootparent> You never went on-call or seen a friend do it? Red herring [1]. OP said it’s malicious or incompetent to release this on a U.S. holiday weekend. You asked if similar consideration would be given to Brazil. Multiple people chimed in that it would. You’re now pivoting to on-call capacity. Any amount of on-call capacity can be saturated. That’s why competent multinationals avoid releasing while markets they’re likely to impact are sleeping or drunk. This is a high-level scheduling operation, however, so it’s reasonable for those lower in the organisation to be unaware why an update is being pushed next Tuesday instead of this. [1] https://en.m.wikipedia.org/wiki/Red_herring reply alganet 14 hours agorootparentYou can totally ignore the red herring and focus on the first part. In the end I was just paraphrasing the comment I replied to. Rotations exist, specially in large organizations, or when there's shared responsibility. Now we're talking nonsense about \"you said, he said\", this conversation makes no sense. I am much less invested in this than you think. reply JumpCrisscross 14 hours agorootparent> Rotations exist Straw man [1]. Nobody claimed otherwise. Rotation or always-on isn’t a substitute for being aware of your customers. Good culture permeate this throughout the organisation. Competent ones have someone at the top ensuring controls are followed. [1] https://en.m.wikipedia.org/wiki/Straw_man reply alganet 14 hours agorootparentSorry, I lost the track. Can you explain the point you made precisely, in the context of the original subject? reply jowea 8 hours agoparentprevFunny thing is this is just the latest issue around this CA. For a long time you had to manually add it to certificate store because it was not trusted by default but the Brazilian government insisted in using it on official websites. reply beeflet 17 hours agoparentprevIt's not just a bad look, it's bad period. reply lokar 15 hours agoparentprevMicrosoft is all about bad looks reply raincole 14 hours agoparentprevHow bad is it? (Genuine question from me who lacks cybersecurity knowledge) reply retrodaredevil 12 hours agorootparentLet's assume that some malicious third party has control of the certificate that was created by this fishy CA. The main attack that they could carry out is a man in the middle (MitM) attack. This attack requires this malicious third party to be able to intercept and change the contents of requests being sent to google.com and someone's web browser. A MitM attack can be easily carried out by someone in control of an ISP, or someone in control of a WiFi network. So, if you trust your ISP and your WiFi network, realistically you have nothing to worry about. The reason that this issued certificate could allow an attack like this to happen is because all websites nowadays use HTTPS connections, and certificate authorities are the entities that tell your web browser that certain certificates are legit. They confirm that a website is actually that website. If you visit some website and someone tries to do a MitM attack between your web browser and that website, the web page should fail to load because if they try to change the certificate, your web browser should reject it because it is invalid. reply 8organicbits 15 hours agoprevMicrosoft seems to be casual about trusting CAs, isn't transparent in their inclusion decisions, and their trust store is quite large. Any reasonable website would only use a certificate trusted by a quorum of browsers (especially Chrome), so the benefit of the extraneous CAs seems low. I'm not a Windows user, but I have to wonder if there's a way to use the Chrome trust store on Windows/Edge. I can't imagine trusting Microsoft's list. reply lokar 14 hours agoparentThey are not transparent because it is based on enabling sales. reply throwaway2037 13 hours agoparentprev> Microsoft seems to be casual about trusting CAs Woah, that is a bold statement. Classic HN overreach. I am not here to shill for MSFT, but, in terms of OS sales to gov'ts, no one else has nearly the same level of experience. I am sure that MSFT carefully vets all CA additions. Are you aware of the big hack on Netherlands govt-approved CA? Read about: DigiNotar. My point: That was a widely trusted CA that was hacked after the root CA cert was added to most browsers / OSes trust stores. So would you say that MSFT was \"casual\" about trusting DigiNotar root CA? How about Mozilla Firefox? I doubt it. reply 8organicbits 13 hours agorootparentI'm very aware of DigiNotar, I wrote a blog post last year that discusses DigiNotar and even mentions Brazil/ITI [1]. A challenge for Microsoft is that they aren't transparent in their inclusion decisions, so we can only speculate why they chose to trust this CA. What gives you confidence that Microsoft is doing careful vetting? In stark contrast, Mozilla publicly and extensively documented why they didn't trust this CA [2]. [1] https://alexsci.com/blog/ca-trust/ [2] https://bugzilla.mozilla.org/show_bug.cgi?id=438825 reply eschatology 11 hours agorootparentThat bugzilla thread was quite a read! Thank you for sharing reply cookiengineer 13 hours agorootparentprevYou are comparing a non publicly available trust chain (Microsoft's) with a public and transparent one (Mozilla's/Linux Foundation's) [1] I don't see any reproducible builds for Microsoft Edge. Therefore, your statement is an assumption and nothing more. We can not trust Microsoft more because they are more proprietary. [1] https://www.ccadb.org/ reply tialaramex 7 hours agorootparentprev> I am sure that MSFT carefully vets all CA additions. Are you? Why? For Mozilla the vetting process takes place in public, that's one purpose of m.d.s.policy so we can see what is or is not done and draw our own conclusions. Each of the proprietary trust stores has an opaque process which unless you're a CA applicant you don't even know what they're asking for, much less what (if anything) they do with it. These are for-profit companies, and this is a cost centre. The cheapest possible thing they could do is piggy back entirely on the public Mozilla process (which of course for this CA would mean rejecting) The next cheapest option would be to allow senior management to override Mozilla's decisions for, you know, commercial reasons. And yes, it would certainly be possible for them to have their own teams every bit as effective as the public process but entirely made up of employees and contractors. Weirdly though, although it's easy to run into people who worked for say, the Windows OS team, or XBox team, or Azure team, you don't run into ex-Microsoft opaque CA process people. One reason might be that they're all career professionals, never leave, never get downsized, maybe there are dozens of them. But the more likely reason is they do not exist. reply lelandbatey 11 hours agorootparentprev> ... In terms of OS sales to gov'ts, no one else has nearly the same level of experience. I am sure that MSFT carefully vets all CA additions. I don't think those two things have anything to do with each other. Living in Redmond for my entire life has mostly shown me that MS owns one of the best and most lucrative sales orgs and sales channels in the world. That sales channel means they can sell to governments better than nearly anyone one the planet, no matter what their security practices are like. reply anothernewdude 13 hours agorootparentprev> I am sure that MSFT carefully vets all CA additions. I'm sure that Microsoft carefully ensure they're paid for all CA additions. Given their monopoly there is no incentive for vetting. reply tialaramex 6 hours agorootparentI'm pretty sure there isn't a fee. Somebody from ISRG (the people who brought you Let's Encrypt) might be able to state categorically that there was no fee charged by Microsoft, obviously it's not free in practice to spin up a decent Certificate Authority, but that's not the same thing as Microsoft charging a fee. For these government CAs my expectation is that they're a sort of quid pro quo and (wrongly) not seen as a security problem. reply MattPalmer1086 8 hours agoprevThings like this make me wonder why certificates are not also signed by the certificate owner. Right now, a CA can issue a certificate for any public key and domain they like. A rogue trusted CA can intercept all traffic. If a certificate also included a signature by the owner of the public key signed by the CA (using their private key, signed over the CA signature), then a CA would no longer have this ability. What am I missing? reply rhplus 7 hours agoparent> What am I missing? The chain of trust for all the certificates in your example is established by trusting the rogue CA root certificate. The CA (or a bad actor who misled the CA through real-world fraud) could be the “owner” of the key pair you’re trusting for the second signature. reply MattPalmer1086 3 hours agorootparentGood point. reply 3np 8 hours agoparentprev> What am I missing? Infrastructure and processes for key distribution and revocation. Reusing the existing PKI infrastructure used for CA trust roots won't handle it. Perhaps public keys/certs could be distributed over DNS, like for DANE (or maybe even using DANE)? Not saying it can't be done, just to point out how it's not trivial and requires buy-in from incumbents across the ecosystem. https://en.wikipedia.org/wiki/DNS-based_Authentication_of_Na... I like your general idea of improving the status quo by adding decentralized/self-managed trust on top of/alongside the existing centralized PKI. Could be a stepping stone towards something more systematically resilient. reply MattPalmer1086 4 hours agorootparentOh sure, any change to X.509 certs would require a lot of change. I'm not sure it would make much difference to most of the existing PKI infrastructure though. CAs wouldn't see any difference. For example, currently this is what happens: 1. Owner: generate CSR and send to CA 2. CA: validates owner identity, signs cert and returns cert to owner. All we would then add is: 3. Owner: signs cert with own private key and uses it. As far as I can see, the only other changes required would be to clients (so they could reject non owner signed certs), and maybe some revocation stuff. reply mattfields 12 hours agoprevSpeculative guess, but it sounds like intentional collusion/coercion between government and big corporations. ie: Brazilian government demands Microsoft to grant them MITM access from Windows machines, in order for the right to do business in the country. reply knowitnone 15 hours agoprev\"Windows users deserve better!\" As if Microsoft cares about their users. But this is clearly negligent behavior and open to lawsuits..hopefully. reply noitpmeder 17 hours agoprevNot clear (to me) in the original post -- was this done accidentally or intentionally? reply tptacek 16 hours agoparentThe certificate was registered in CT, so a reasonable assumption would be that this was accidental, because it was guaranteed to be noticed and to generate drama that would threaten the capability they arranged, presumably at some significant expense. reply px43 13 hours agorootparentWhat is CT here? Central Time? Connecticut? Maybe Certificate Transparency? I guess that last one might make the most sense. Abbreviations are hard. reply bart__ 10 hours agorootparentCertificate Transparency, all CA's log their issued certificates to central log servers, managed by Cloudflare, google etc. If this is not done, the certificate will not be seen as trusted by Browsers. It was designed to have a publicly auditable source of issued certificates, exactly so we can notice rogue google.com certs. reply probstal 4 hours agorootparentActually, it won't be trusted by most browsers. As of today, Firefox hasn't implemented it yet [0] [0] https://bugzilla.mozilla.org/show_bug.cgi?id=1281469 reply tialaramex 6 hours agorootparentprevTechnically you don't have to log certificates during issuance, and actually doing so is slightly more trouble (because of a chicken & egg problem, you want the log proof in the certificate, so you must log special \"poisoned\" certificates to get that proof and then fasten that proof to the certificate. A customer can take an unlogged cert, log it themselves, and then use the certificate and the separate proof of logging they received and use that just fine. Google have some services which do this. One clever thing this enables is you can buy the cert secret-product-name.example, unlogged, build the web site, check everything works, and log the certificate seconds before the product launch event, so snoops can't tell your new product is secret-product-name until the moment you announce it, yet the site works immediately. I have very rarely seen this done but it's possible. When there's an ordinary White House transition process both plausible transition site certs get logged, even though in practice one of those sites is never published. Since Trump I have no idea if this process is so smooth any more. A CA can choose whether to have this \"issue unlogged certs\" process as something they offer, it's a niche thing, but it could make sense. They need to keep adequate records of every certificate they issue (that's required) and logging is a very easy way to satisfy that requirement, but it's not the only way. In practice, the logged certificates are the easy consumer option, like selling ready-to-eat food in a deli. Some customers might be prepared to buy ingredients and go away to make food, but, many customers probably want to eat food immediately so for extra money you sell products that can just be eaten immediately. So, yes, the vast majority of certificates issued every day are indeed logged immediately so as to provide the product people want. reply FergusArgyll 10 hours agorootparentprevComputed Tomography? reply woodson 16 hours agoparentprevAs a CA, how does one accidentally issue a certificate for google.com? I mean, is there a scenario that isn't malicious? reply tptacek 16 hours agorootparentYes, if the interception system involved was meant only for resources within Brazil’s own agency networks. reply 8organicbits 12 hours agorootparentNote that this scenario happened for ANSSI and MCS Holdings, so there would be precedence. I'm eager to see what Google concludes this time. https://security.googleblog.com/2013/12/further-improving-di... https://security.googleblog.com/2015/03/maintaining-digital-... reply lxgr 15 hours agorootparentprevBut that's not allowed for publicly trusted roots under any circumstances, right? Not sure if that would qualify as an accident. reply foota 15 hours agorootparentI think the parent is saying that if they meant to use the cert only internally (e.g., to monitor employees) then that would arguably not be malicious. reply lxgr 15 hours agorootparentNot malicious, but also not exactly purely accidental, i.e. as part of some otherwise totally legitimate activity. reply foota 56 minutes agorootparentI think the accidental part would be in the scope. I'm not an expert on these things, but they could have intended to create a self signed cert only valid within the scope of their IT, but accidentally created one from their CA. reply tptacek 14 hours agorootparentprevIt would not be malicious. I don't think there's a serious argument here (bearing in mind that in the airless vacuum of a message we can, of course, argue anything). I don't know that's what happened here, though; there are malicious possible explanations! reply foota 53 minutes agorootparentI largely agree, although I think there's some part of a slippery slope specifically when it comes to government, since you could argue that a government monitoring its citizens is also not malicious since (in a democratic society) the government derives its mandate from the people. This isn't too different from the argument that (I believe reasonably) applies for how a company has the right to monitor employees, but I think many people are opposed to even democratic governments monitoring people and would consider such use malicious. So a government monitoring its employees is one step closer even than a company, since it's the same organization in this case (though again, I think it's largely reasonable for a government to monitor their employees). reply JumpCrisscross 14 hours agorootparentprev> if they meant to use the cert only internally (e.g., to monitor employees) Or to redirect to an internal, no doubt pitched as more secure, search engine. reply grayhatter 15 hours agorootparentprev> (e.g., to monitor employees) then that would arguably not be malicious. If only there was a way to monitor company equipment without issuing a cert for a public 3rd party. reply switch007 2 hours agorootparentAI screen monitoring right reply tialaramex 6 hours agorootparentprevMost Certificate Authorities have manual issuance†, at least as an option. There's a UI where an authorized employee can issue whatever they want, the UI may be fairly crude or something quite polished used in ordinary business processes. So an employee can type in google.com and check any boxes about did you verify this is the correct name and it's OK to issue, and then they hit issue and the certificate is minted, just like that. Why google.com? Well, if you're testing something, say a web browser, what web site comes to mind? Maybe google.com? Doesn't work. Oh - the cable is unplugged. Doesn't work. Wait, this checkbox isn't checked, try again. Aha, now it works... Oops we issued a certificate for google.com This is a \"Never\" event, there should be countless things in place to ensure it doesn't happen. In practice, just like safety guards on dangerous machinery, too many people just can't be bothered with safety, it's a cultural issue. † Let's Encrypt famously does not. As part of the Mozilla application process they need to show their certificates expire properly, usually people either manually issue a back-dated certificate which has expired already, or they manually issue one with a deliberately short lifetime to expire. Since they can't issue manually Let's Encrypt obtained an ordinary certificate from their own service and then waited ninety days for it to expire like a fucking boss. reply Thaxll 14 hours agorootparentprevYou know testing stuff like example.com ... reply fguerraz 17 hours agoparentprevCarelessly is the answer reply ruined 17 hours agoparentprevdoes that matter? reply altairprime 17 hours agorootparentYes; malice is indefensible no matter the circumstances, mistakes may be defensible under certain circumstances or with certain responses by the mistakee. reply sabbaticaldev 17 hours agorootparentas a brazilian i’m not sure if I’d prefer it to be malice or incompetence reply altairprime 15 hours agorootparentIncompetence: operating a CA is difficult enough that sometimes people fuck up, but if the CA is corrupted, then that’s much worse. reply griomnib 16 hours agorootparentprevAs an American…why not both? reply lazide 16 hours agorootparentprevThere is also the option of malicious incompetence, of course. reply resters 17 hours agoprevThe simple solution would be to have independent entities offer trust assertions about CAs and to allow users to consider multiple entities' views in their decision about whether to trust. It's surprising this doesn't exist yet when the attack vector is so clear. reply tptacek 17 hours agoparentThis is something more akin to a client software bug than a WebPKI issue. Any alternative PKI scheme you could come up with would still be subject to Microsoft cutting deals. reply 8organicbits 14 hours agorootparentCan you explain? I think the parent is suggesting that users should be able to tune their trust stores. I'd imagine that trusting only the CAs that are in all the major trust stores (Google, Microsoft, Mozilla, and Apple) would be a reasonable policy. Few websites would choose a CA that falls outside that group. reply tptacek 14 hours agorootparentUsers can tune their own trust stores. reply 8organicbits 14 hours agorootparentIs there a way to do it that isn't tedious? I'm not familiar with tooling beyond the UI browsers offer, which doesn't match the experience I was trying to describe. reply salawat 9 hours agorootparentI mean... It's as easy as getting SSL certs and importing them into a trust store/adding them to a directory. The hard part is getting the people you want to establish a trust relationship with to give you a copy of their key. Web of Trust was the answer to logistical key distribution problem. The idea being there would be an organization that would vet people and vouchsafe their cryptographic material for everyone else. The problem of course, is that the more invisible this is to users, and the more unintuitive the actual mechanics, the more valuable cracking the CA's becomes for hostile actors because of the ensuing blast radius compared to the boast radius that would result from theoretically getting the practice of key exchange in the public, and getting them to internalize the act of creating their own trust networks. Of course, if you have dreams or fantasies of being able to control people, none of the work that goes into educating the populace is ever going to be endorsed, because once everyone realizes that they can at least assure their own safety by not delegating their cryptography, the entire idea of eacesdropping as a third-party by tapping the line is unmade. Which is not a popular state of affairs universally. reply 8organicbits 4 hours agorootparentWeb of trust is way more ambitious than what I'm talking about. Key distribution for the Apple, Microsoft, Google, and Mozilla trust stores is already a solved problem and works well at scale already. However, if you don't trust the inclusive nature of Microsoft's trust store and prefer Chrome's, there should be a tool to swap out trust stores. I don't think such a tool exists yet. reply silotis 14 hours agorootparentprevWith DNSSEC + DANE Brazil would not have needed to make any deal with MS to be able to issue certs for .br domains and they would not have been able to issue a cert for google.com. Admittedly DNSSEC has issues to put it mildly, but it does serve as a counterexample to your claim. reply will4274 1 hour agoparentprevIt'd be a simple enough browser plugin to build - a tool that checks multiple trust stores when rendering a page. Probably it already exists. The problem is between the keyboard and the chair. Users struggle to understand SSL already. Browsers decided that the distinctions between EV, DV, and OV were too complex and hid them. What will your grandmother think when she opens up her bank and your browser plugin shows a greenish yellow trust indicator because the cert is trusted by Google, Apple, and Microsoft, but not Mozilla? Unfortunately, trust is binary. Your grandmother click on the bank bookmark and either sees her banking websites or sees a scary warning. reply b800h 10 hours agoprevCan anyone tell me which CA is used by Open Banking in Brazil? The infrastructure is heavily based on PKI. I assume it's not this one? reply meiraleal 8 hours agoparentYes, ICP-Brasil is responsible for that too. reply notorandit 10 hours agoprevIt's not Microsoft being careless about CAs. That's been made on purpose by them to comply with some request in order to keep a slice of their market. reply coretx 14 hours agoprevDoes anyone have a list of state ( associated ) CA's so that I can ditch them all ? reply 8organicbits 3 hours agoparentI built a partial list last year [1]. It's challenging to decide which CAs are government operated or controlled: the names don't always make it clear, private companies may operate under government direction, law may require CAs to follow government requests. These were all very clearly government/military organizations running CAs. The Brazilian CA referenced here is number two on the list. [1] https://alexsci.com/blog/ca-trust/#government-control-of-cas reply II2II 14 hours agoprevTangentially related: The system is deeply flawed, which is something I realized fifteen years ago when I was put into a situation where I had to use online banking. (Had to being the nearest branch of any bank was an hour long flight away, though there was an ice road you could use in the winter.) One of my first questions of the bank was: who issued their certificate. They didn't have a clue what I was talking about. I suppose I could have pushed the question until I found someone who did know, but I also realized that a random person asking about security would be flagged as suspicious. The whole process was based upon blind trust. Not just trust in the browser vendors to limit themselves to reputable CA, but of the CAs themselves and their procedures/policies, and who knows what else. reply JumpCrisscross 14 hours agoparent> One of my first questions of the bank was: who issued their certificate …what did the certificate say? > whole process was based upon blind trust If I offer someone a ride and they start quizzing me on what differential I’m driving, I’m going to ignore them. That isn’t requiring blind trust, it’s just the wrong place and way to get the information you’re asking for. reply salawat 11 hours agorootparentThe problem with that analogy is that the cert issuer isn't a mere component of the car, but the entire car in this instance. That cert being trustworthy is the entire point. When I was in schooling getting filled in on Web of Trust, I about ground that particular day's class to a halt because I couldn't imagine the world was that cavalier on such a thing. Lo and behold, I realized shortly afterward it absolutely was the case, and there was nada I could do to change it except figure out how to get normal people universally fluent and invested in basic cryptography so they could manage their own trust networks. You can imagine how well that's gone. reply JumpCrisscross 8 hours agorootparent> problem with that analogy is that the cert issuer isn't a mere component of the car, but the entire car in this instance I'm critising OP for castiglating a bank employee for not knowing who their CA is. That's not something a line employee needs to know. And that's not the appropriate way to ask that. If I want to know who issued HN's certificate, I don't e-mail a YC associate. I look at my browser and see it's Let's Encrypt. reply echoangle 9 hours agoparentprevHow does knowing the issuer of the certificate tell you anything if any CA can make certificates for your bank domain? If the answer was „sure, we use GlobalSign“, is that good or bad? If the Brazilian CA is malicious, they can still MITM you, right? (Assuming certificate pinning doesn’t exist, which was the case 10 years ago and is true now, too) reply throwaway2037 13 hours agoparentprevFirst, you don't tell us the location. Are we talking about a CA in Syria or Canada? It makes a big difference. Second, yeah, I'm sure banking regulators say nothing to commercial banks about using a reputable CA. reply ikekkdcjkfke 11 hours agoprevHow do i remove this CA from windows and Edge? reply motbus3 8 hours agoprevYou care about google? Look at those links, they are loaded with critical government stuff. Omg reply sabbaticaldev 17 hours agoprevCan someone explain what could be done with that and by whom? reply woofcat 17 hours agoparentWhomever has this fake certificate can run a server and say it's google.com and windows will say \"yep you are\" with the little green lock. reply baobabKoodaa 9 hours agorootparentWhere? In Edge you mean? reply echoangle 9 hours agorootparentAnd everything that uses the OS trust store, when run on Windows, I assume. reply bufferoverflow 16 hours agorootparentprevThe certificate is for a specific IP address, no? And without DNS pointing google.com to that IP address, it's pretty useless. reply zer0x4d 16 hours agorootparentNope, certificates are issued for CNs(Common Name), also known as FQDNs (Fully qualified domain names). Something such as *.google.com, not IP addresses. If they were issued for IP addresses they would have to reissue the certificate every time they spun up a new server. Also it's why if you spin up another server and make DNS point google.com to that server, it would not pass verification since the certificate you will be using on that server is not issued to *.google.com, but rather some other domain you own. The IP address plays no role in certificates. reply colanderman 16 hours agorootparentNit: a CN (stored in the Subject field of a cert) is not an FQDN, though historically web browsers treated them as such. This practice is now deprecated. Modern practice is for the domain name(s) to be placed in the Subject Alternative Name (SAN) field. The Subject field is not consulted so long as the SAN field is present, and can in theory be any X.500 Distinguished Name, of which Common Name is one possible attribute, which may be any freeform string of a limited length (though it is typically set to the primary domain the cert is issued for, for easy identification). reply buzer 16 hours agorootparentprevCertificates can be issued to IP addresses (at least on SAN level, not sure if they are allowed in CN in CA/B baseline requirements), like https://crt.sh/?id=15492507462 reply Arrowmaster 15 hours agorootparentThat is different in context to what was being asked though. reply echoangle 9 hours agorootparentprev> And without DNS pointing google.com to that IP address, it's pretty useless. On the internet itself maybe, but you can still MITM people on some network, right? reply tptacek 17 hours agoparentprevMicrosoft appears to have arranged with the government of Brazil for one of their national CAs to have the ability to mint arbitrary certificates. Only Microsoft's own WebPKI software cares; Chrome, Safari, and Firefox don't trust this CA. reply brianpan 14 hours agoparentprevIt's not entirely about this particular certificate (although this is bad, too). This is about a certificate authority giving someone who is NOT Google, a certificate that can be used to \"prove\" a server is Google. Accidental or not, this should not happen. The \"blast radius\" is limited to Microsoft since they are the only ones that trust this particular certificate authority. Your non-Microsoft browser won't trust these certs. Your non-Microsoft OS, Java program, etc. etc. won't trust these certs. reply xcrunner529 12 hours agorootparentChrome uses the Windows trust store on Windows, IIRC. reply 77pt77 17 hours agoparentprevWhoever has the private certificate can pretend to be google.com to people using windows. The brower (possibly only edge) and system would show the connection as being secure. reply ThePowerOfFuet 9 hours agoprev@dang Can we update the link to the original source? https://bugzilla.mozilla.org/show_bug.cgi?id=1934361 reply xyst 15 hours agoprevSo an incompetent CA is trusted by an even more incompetent company, Microsoft? Is anybody else surprised at this point? reply ed_mercer 15 hours agoparentMicrosoft is many things but not incompetent. reply cookiengineer 13 hours agorootparentFrom a security standpoint that's debatable. Multiple RCEs and critical CVEs cannot be fixed because Microsoft \"lost\" the source code. So they disclosed those RCEs but without any solution or fix. (Not kidding, sadly, look it up, there also have been occasional binary patches because of the same reason) [1] https://msrc.microsoft.com/update-guide reply echoangle 8 hours agorootparentDo you have a link for the „lost sourcecode so we won’t patch“ claim? The link you gave just gives me a long list of patches. reply cookiengineer 8 hours agorootparentCVE-2017-11882 and the NTLM relay attack come to mind, for example. Down the line they weren't actually fixed, and are continuously being used by a lot of ransomware / malware campaigns. I remember some Windows Fax Service related CVEs and some Wi-Fi drivers that couldn't be fixed directly, too, but don't remember the CVE or whether that was related to the Broadcom driver/module sideloading fuckup. > The link you gave just gives me a long list of patches. The link I gave you is the only disclosure/advisory page that Microsoft offers, don't blame me for them not offering a better UI. Ask them to do better. - https://nvd.nist.gov/vuln/detail/CVE-2017-11882 - https://blog.0patch.com/2017/11/did-microsoft-just-manually-... - https://cert.europa.eu/publications/security-advisories/2022... reply will4274 1 hour agorootparent> CVE-2017-11882 and the NTLM relay attack come to mind, for example. Down the line they weren't actually fixed, and are continuously being used by a lot of ransomware / malware campaigns. Your own sources indicate CVE-2017-11882 was fixed in November of 2017. The title of the blob.0patch.com article is > Did Microsoft Just Manually Patch Their Equation Editor Executable? Why Yes, Yes They Did. (CVE-2017-11882) clearly indicating that Microsoft fixed the issue, contrary to your statement that they 'weren't actually fixed\". The body content is consistent. > NTLM relay attack NTLM is bad, no question. It's based on a bad threat model - it assumes network admins can secure their corporate networks. Microsoft also fixed most of the issues in NTLM with NTLMv2 back in the Windows Vista and Windows 7 era. And Microsoft announced they will disable all NTLM versions by default within the Win11 lifetime. The biggest problem (unsurprisingly) is non-Microsoft software which has hardcoded the use of NTLM. It's fair to criticize Microsoft here for making available a technology that required so much from corporate network admins and leaving it available (and with use in Microsoft products) for so many years. At the same time, it's misleading to characterize these problems as \"weren't actually fixed\" - concrete issues with NTLM within its security model _were_ fixed and new technologies were created with better security models. - https://techcommunity.microsoft.com/blog/windows-itpro-blog/... > The link I gave you is the only disclosure/advisory page that Microsoft offers, don't blame me for them not offering a better UI. Ask them to do better. You're mistaken. Microsoft has deep links for each CVE. - https://msrc.microsoft.com/update-guide/vulnerability/CVE-20... reply meiraleal 8 hours agorootparentprev> From a security standpoint that's debatable. still not incompetence if what they gain from it is bigger than what their customers lose, unfortunately. reply tylerchilds 13 hours agorootparentprevwealthy and strategic can cover up a lot of incompetence reply ikekkdcjkfke 12 hours agorootparentprevThen willfully negligant reply connor11528 15 hours agoprev [–] this is an issue with companies being too big reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "A Brazilian Certificate Authority (CA), trusted solely by Microsoft, issued a certificate for google.com, breaching Google's Certificate Authority Authorization (CAA) rules.",
      "This incident raises concerns about the CA's reliability and Microsoft's decision to trust it, as other major browsers do not.",
      "The certificate could facilitate man-in-the-middle attacks, allowing attackers to impersonate Google and distribute malware, underscoring the need for stricter oversight and transparency in CA trust decisions."
    ],
    "points": 438,
    "commentCount": 181,
    "retryCount": 0,
    "time": 1733002503
  },
  {
    "id": 42284412,
    "title": "Open-source private home security camera system (end-to-end encryption)",
    "originLink": "https://github.com/privastead/privastead",
    "originBody": "I needed a security camera inside my house, one that would send motion notifications to my smartphone and would allow me to livestream remotely. However, I could not find one that I could trust due to privacy concerns. Many of them upload the plaintext of videos to their servers and none is fully open-source as far as I know. Therefore, I decided to use my spare time to build one from scratch. Called Privastead (as in Private Homestead), it uses OpenMLS for end-to-end encryption (between the camera local hub and the smartphone) and is mostly implemented in Rust (except for part of the Android app that is implemented in Kotlin). The system is functional now and I&#x27;ve been using it in my own house for the past couple of weeks.Based on some of the discussions I&#x27;ve seen online, it seems like there are other users who are also concerned with the privacy implications of home security cameras. Therefore, I decided to open source my solution for everyone to use. If you need a privacy-preserving home security camera, please give it a try and provide feedback. Note that trying out the system requires you to have a supported IP camera, a local machine connected to the IP camera, a server, and an Android smartphone. I have put together detailed instructions on setting up the system, which I hope makes it easier for others to get the system up and running.In addition, consider contributing to the project. The prototype currently has a lot of limitations: mainly that it has only been tested with one IP camera, only allows the use of one camera, and only supports Android. I&#x27;ll continue to improve the prototype as time permits, but progress will be much faster if there are other contributors as well.",
    "commentLink": "https://news.ycombinator.com/item?id=42284412",
    "commentBody": "Open-source private home security camera system (end-to-end encryption) (github.com/privastead)384 points by arrdalan 20 hours agohidepastfavorite109 comments I needed a security camera inside my house, one that would send motion notifications to my smartphone and would allow me to livestream remotely. However, I could not find one that I could trust due to privacy concerns. Many of them upload the plaintext of videos to their servers and none is fully open-source as far as I know. Therefore, I decided to use my spare time to build one from scratch. Called Privastead (as in Private Homestead), it uses OpenMLS for end-to-end encryption (between the camera local hub and the smartphone) and is mostly implemented in Rust (except for part of the Android app that is implemented in Kotlin). The system is functional now and I've been using it in my own house for the past couple of weeks. Based on some of the discussions I've seen online, it seems like there are other users who are also concerned with the privacy implications of home security cameras. Therefore, I decided to open source my solution for everyone to use. If you need a privacy-preserving home security camera, please give it a try and provide feedback. Note that trying out the system requires you to have a supported IP camera, a local machine connected to the IP camera, a server, and an Android smartphone. I have put together detailed instructions on setting up the system, which I hope makes it easier for others to get the system up and running. In addition, consider contributing to the project. The prototype currently has a lot of limitations: mainly that it has only been tested with one IP camera, only allows the use of one camera, and only supports Android. I'll continue to improve the prototype as time permits, but progress will be much faster if there are other contributors as well. tills13 1 hour agoIf you're like me and you want something that Just Works -- ubiquiti's offering is fully local and has all the features you'd want including alerts, ml detection (on some first party cameras), app, timeline, etc. They recently started allowing third party cameras, too, so all you need is a Cloud Gateway (USG Ultra is $150) and you're good to go. reply tuchsen 27 minutes agoparentThe last time I upgraded my networking setup (just as the pandemic started) I went with Ubiquiti, shortly after (before?) they started default forcing you to sign up for their cloud service to use the router, you can switch it to a locally operated mode after you sign up but they bury it in the options. Their networking equipment works great, don't get me wrong, but they don't open source anything. I keep waiting for the full rug pull when sales start to slow because everything they release is rock solid and everything I have is \"good enough.\" I don't feel the need to upgrade for as long as the rug pull doesn't happen. I am super grateful people are starting to work on open source solutions to this stuff :). reply KetoManx64 17 hours agoprevWow!! Very cool project. The reasons you mentioned above are the exact reason that I have not setup security cameras at my house, but this makes me reconsider. Combined with the OSS firmware: https://github.com/openmiko/openmiko this would make a killer combo for privacy conscious individuals. reply arrdalan 17 hours agoparentGlad to hear it. And thanks for the pointer on OpenMiko. It would indeed be nice to port Privastead's camera hub to run directly in the camera firmware. This will make the setup a lot easier: no need for a machine to act as a hub. reply ranger_danger 17 hours agoparentprevThe best time to have security cameras is yesterday. Same for a dash cam. Please protect yourself and your loved ones. reply ndsipa_pomu 1 hour agorootparentDashcams (including bikecams and helmetcams) are a great idea and AFAIK there's not much of a privacy concern with them as they record to local storage and usually in a common video standard (e.g. mp4). Hopefully as more people use them, there should be more chance of video footage being available from other vehicles if you're involved in a RTC and don't have one yourself. There's also the protective effect of people uploading bad driving to the police where the police are interested in road crime - hopefully providing the police with a very cheap way of having visibility in lots of places. reply arrdalan 17 hours agorootparentprevI completely agree that security cameras can be a disaster for one's privacy. That's why I decided to build Privastead for my own use in the first place. It's fully open source, so we know what we're getting and what we're trusting. It also uses strong end-to-end encryption provided by Messaging Layer Security (MLS): https://datatracker.ietf.org/doc/rfc9420/ reply chgs 2 hours agorootparentYou should do a ShowHN on it! reply LoganDark 17 hours agorootparentprevI got hit by a truck a few months ago and a dash cam (well, really, scooter cam) could have shown beyond a doubt whose fault it was. I wish I didn't have to be hit by a truck in order to realize this. If something ever happens you're going to want to have had a recording of it. reply EthicalSimilar 17 hours agoprevDid you look into Scrypted? It is OSS. With HomeKit (which is how I use it) it is also E2EE. I’m not sure if their app is E2EE though, I only use HomeKit. It’s pretty good and has an extensive ecosystem. The dev can be a bit… feisty though. It’s very performant and easy to setup. I don’t use the NVR features as I already have an NVR, I essentially just use it for HomeKit integration of my cameras + doorbell. https://github.com/koush/scrypted reply arrdalan 15 hours agoparentI'm not familiar with Scrypted unfortunately. But thanks for the pointer. I'll have to study it to understand its security and privacy implications. I skimmed the docs very quickly but couldn't find info on their use of encryption. About HomeKit: yes, HomeKit uses iCloud end-to-end encryption (https://support.apple.com/en-us/102651) and is certainly superior to those systems that don't use encryption at all or just use encryption between the device and their servers. But Privastead has two advantages: 1) Privastead uses MLS for end-to-end encryption, which provides forward secrecy and post-compromise security. iCloud's end-to-end encryption does not. So what does that mean? This is from the link I included earlier: \"If you lose access to your account, only you can recover this data, using your device passcode or password, recovery contact, or recovery key.\" If an attacker manages to access your password, recovery key, etc., they'll be able to decrypt all your videos (assuming they have recorded all your encrypted videos). Such an attack is not possible in MLS. Similar to the Signal protocol, MLS uses double ratchet and there is not a single password, recovery code, key, etc. that can decrypt everything. 2) The HomeKit framework and iCloud end-to-end encryption are not fully open source as far as I know. Therefore, we simply have to trust what Apple says about their security and privacy implications. That might be okay for some users, but not others. Privastead is and intends to remain fully open source. IMO, being open source is a critical component of any security/privacy solution that would like to gain users' trust. reply cvwright 1 hour agorootparentI’m not sure that 1) is such a win for MLS here. If I lose my phone, I want to get all my security footage back by entering my passphrase etc on my new phone. In other words, I want the “encrypted cloud storage” security model from [1], not the encrypted messaging security model from Signal etc. [1] https://eprint.iacr.org/2024/989 reply amluto 15 hours agoparentprev> It is OSS. Barely. A lot of functionality is gated behind the NVR plugin, which is closed-source and fairly expensive. > easy to setup In my experience, it’s easy to set up. But it’s extremely configurable in all the wrong ways and quite difficult to configure in the ways that one might actually want to configure. And the front end is not fantastic IMO: event filtering is extremely weak and scrubbing is bizarre. reply corytheboyd 2 hours agorootparent> But it’s extremely configurable in all the wrong ways Damn that feels exactly like my experience with Zoneminder. I’m sure it is decent software under the hood, but the UX is downright hostile to anyone who just wants to get IP cameras to do very basic motion detection to record some full resolution footage, which feels like the major, obvious use case that should be optimized for. I am thankful for OSS existing in this realm, but why do so many solutions make the same mistakes? Am I crazy in thinking that a good out of the box experience is important? Is some critical part of the formula locked behind private walls? Something else? Genuinely curious. reply h317 12 hours agoprevIf you need an open-source hardware+fw design for a camera with a motion sensor: https://github.com/maxlab-io/tokay-lite-pcb or can be purchased at: https://www.mouser.ca/ProductDetail/Maxlab/TOKAY-LITE-01?qs=... reply SahAssar 6 hours agoprevI'd assume that end-to-end encryption would mean the traffic is encrypted between the camera and the app, which it isn't. That would require an app in the camera to support the system (which could be done on many cameras). reply arrdalan 4 hours agoparentThe traffic is encrypted between the hub and the app. The camera is connected to the hub. reply SahAssar 4 hours agorootparentRight, but the \"ends\" in end-to-end encryption are typically the device producing the content and the device consuming the content. In this case that is the camera and the app. reply mattlondon 11 hours agoprevHaving had cameras and a couple of breakin attempts for a while now my biggest must-have for these sort of things is: - reliable human detection (not just motion) - integration via some sort of API or MQTT etc. This project appears to have no \"smarts\" in terms of human detection nor APIs which is a shame. Just pure motion detection on its own is ok, but you're going to pick up all sorts of things that you don't care about. E.g. I have foxes that run around my garden all night. I don't care about those. But the second a human steps into my garden and starts approaching the house in the night, all the perimeter floodlights are activated, in-house lights turn on, a fake-yet-loud barking dog MP3 starts playing and I get pushover notifications on my phone that won't stop until I ACK them etc (to wake me up). So far, from personal experience, the most reliable way to stop an attempted burglary in-progress is for lights to switch on from inside as they are attempting to force entry. reply xyst 2 hours agoparent> But the second a human steps into my garden and starts approaching the house in the night, all the perimeter floodlights are activated, in-house lights turn on, a fake-yet-loud barking dog MP3 starts playing and I get pushover notifications on my phone that won't stop until I ACK them etc (to wake me up). Wow this setup seems like overkill for a residential setup. I have lived in “bad” neighborhoods before and have never felt the need to get this kind of setup. The most I have is a single camera pointed at primary ingress/egress point which is configured to upload to Apple iCloud via “HomeKit Secure Video”. reply bobbob1921 1 hour agoparentprevThe best human detection for cctv cameras (in contrast to motion only detection) is an open source project called frigate. You should definitely look it up. It does this better than any paid or free solution I’ve ever come across. reply wkat4242 10 hours agoparentprevFor me the human detection isn't an issue. I only have cameras indoors anyway, i have one outside but it's equipped with a highly zoomed lens so it can see the faces of the people in front of my door. It won't trigger on animals because they don't get up that high. But in your case I could recommend Frigate, it's got decent detection that can be accelerated with a Google coral TPU. It also integrates really well with home assistant, you could use it for the light triggering you mention. I personally use Shinobi but it's not really great and constantly asking for money. Especially if you want to use the app. reply mattlondon 8 hours agorootparentFor me I use Nest cameras with Home Assistant. I went through various brands of cameras and the Nest ones were the only ones that I could actually rely on. I didn't spend Megabucks but even mid-range local IP cams were very unreliable for RTSP (randomly going offline, super-sketchy partially translated apps for set up/config, frozen streams etc) I am not concerned about Google's use of my data. I'm not important enough for anyone to want to spy on me specifically. Of all the cloud solutions/providers, I probably trust Google to have the technical know-how but also the public scrutiny pressure to do a general decent job (way more than some random rebadged Tuya service or something \"in-house\" thing). By that I mean things like e2e encrypted, there won't be employees with just random read access, decent engineering practices, proper security, SREs for uptime, and won't have default passwords/no passwords on some random world-readable S3 bucket etc. And even if someone did manage infiltrate Google, they'd very likely target someone high-value before me - I doubt anyone would be able to exfiltrate all the data for practical reasons of where to put it but also someone at Google noticing additional exabytes of suddenly outbound data before getting shutdown. I'm working on the (perhaps flawed) assumption that a successful & undetected Google hack would be short-lived. You'd go for someone rich and powerful first, not video clips of me taking the rubbish out. reply wkat4242 5 hours agorootparentHmm I don't trust Google at all to be honest. I don't even use a Google account on my android phone anymore. I do have some ring cameras but they're only looking at my front door and I've cut the microphones out of them (they're incredibly sensitive, I could literally hear myself word for word two rooms away). Most of my cams are TP-Link used in local mode with internet access blocked. reply michaelt 5 hours agorootparentprev> And even if someone did manage infiltrate Google, they'd very likely target someone high-value before me The risk isn't just an evil insider infiltrating Google. It's also that a lot of home security companies are in the business of having a friendly relationship with the authorities. So if the cops were investigating a fender-bender down the street and they ask Google for your videos? Better hope that living room camera doesn't show anything you wouldn't want the cops to see, because it's getting handed over. reply wkat4242 5 hours agorootparentYeah ring had a dirty habit of doing this too. But they've stopped apparently: https://www.wired.com/story/ring-police-rfa-tool-shut-down/ Still, I've disabled the microphones in mine and they only see my front door anyway. reply arrdalan 4 hours agoparentprevPrivastead relies on event detection by the camera itself. The Amcrest camera that I've used so far only does motion detection (which is all I needed on my end). There are IP cameras, however, that do human detection. I've purchased one of those and plan to integrate it, which would then allow Privastead to provide human detection too. reply unit149 9 hours agoprev>It guarantees that only the hub and the mobile app have access to unecrypted videos. Interesting project! As a Rust implementation for Open MLS, secure end-to-end archival, and a TLS vector, having the DIY home camera setup connect to the internet via a Privastead hub eliminates the need for secure tunneling. I could see this incorporating facial recognition technology and real-time monitoring. If you've ever seen eigenfaces, they look like early hominids. One method is principal component analysis (PCA) which isolates the major characteristics of a human face in correlating underlying noise to the most essential features of a face. reply nubinetwork 7 hours agoprevI was thinking about using Pi's for some cameras, but your readme says something about using a Google firebase for passing messages... would email to sms make more sense (if your provider still supports it)? Also, if I were to set this up behind a firewall, would I need to port forward something so I can access it on the road? reply arrdalan 4 hours agoparentUsing Pi's to run the hub is technically feasible (and I do plan on trying it on my end). Port forwarding is not needed. The Pi only needs to be able to initiate a connection to the server. Google FCM is used for delivering notifications to the app. The server asks FCM to do so when needed. reply INTPenis 10 hours agoprevSeeing all this focus on security I thought you might be interested in the fact that there are cameras out there with Secure Boot, Axis is one manufacturer I know of that focuses on this feature. reply goodpoint 10 hours agoparentWhat's a realistic use-case for secure boot on a camera? It's such a corner case... reply stragies 6 hours agorootparentOne use-case I see (for the Vendor) is avoiding the possibility of users to extend the useful life of their device by loading an aftermarket Firmware like OpenIPCam, OpenMiko, or OpenWrt when the Vendor decides it want's the user to buy a new device instead of continuing to use the existing device for more years. Of course, Axis will say, that they do this to protect the world from terrorism, CP, and human trafficking. reply INTPenis 3 hours agorootparentprevA friend discovered it when he was wanting to monitor his datacenter and considering local security. A situation where someone has access to your DC and could theoretically erase images of them being there since they also have local access to your cameras. reply genericacct 9 hours agoprevI've had some success with Motion (in that i have pictures of a dude ski mask wearing dude looking around the house), which is also open source C (++? no idea). I will look into this but my impression is that work on the edge is done already and what is really needed is some sort of central processor that runs YOLO on several streams reply nzeid 17 hours agoprevI'm deeply interested in this and it just so happens I only need one hardened IP camera for my home. Can you point out how your project provides a better security layout than say, Home Assistant? It does appear that there is little concern for encryption between cameras and local networking hubs, but the assumption seems to be that if the device is over a wire it's isolated to the local network, and if it's Wi-Fi it's subject to WPA. So there isn't much desire for additional encryption in that context. Otherwise, Home Assistant's server/portal is more than capable of encryption beyond the camera's network interface. reply arrdalan 14 hours agoparentIf you use Home Assistant in an isolated local network with no Internet access, you will have good privacy. However, I needed to be able to use my camera remotely from my phone. That's when using strong end-to-end encryption becomes critical. Home Assistant seems to provide two options for remote access: https://www.home-assistant.io/docs/configuration/securing/ One option is to expose your instance to the Internet and using TLS/SSL to connect to it remotely. To me, that's risky since any machine exposed to the Internet is going to see more attack attempts and is harder to secure. The second option is to use their cloud. Here, they explain how remote access through their cloud works: https://www.nabucasa.com/config/remote/#security They also mention its weaknesses: \"Before we talk about weaknesses, know that we will never abuse any weakness unless forced by a government entity. Our approach has one single weakness that is unavoidable: since we own the domain that hosts the remote connection, we are able to issue our own certificate and man-in-the-middle attack (MITM) remote connections. This would allow us to see all data passing through, including authentication tokens.\" Privastead's use of MLS provides strong end-to-end encryption. And Privastead is not vulnerable to MITM attacks due to its secure pairing solution. I plan to write more about the security of Privastead including its pairing solution. reply hathawsh 14 hours agorootparentA third option is to set up a Wireguard service and expose the web UI only through the VPN. I use the Wireguard app to connect my phone to my VPN. This solution seems very safe to me. Are there risks I'm not thinking of? reply arrdalan 12 hours agorootparentThat's definitely better, if done right. I still prefer to avoid port forwarding on my router if possible. The fewer attack surfaces I have to worry about, the better. reply euroderf 1 hour agorootparentprevNo mention of Tailscale ? It's dead simple. reply ww520 9 hours agoprevI don't want to come off as negative, but here's an off-the-shelf alternative. My home network has VPN set up that let me access the home video feed securely and privately. The NVR software doing the video recording can process the videos to detect abnormal activities. It will send a push notification to my phone. I then connect to the VPN and view the videos as if I'm on my home's local network, totally private and secure. reply globalnode 17 hours agoprevYou are a champion, thank you for this. Will have a look at it when time permits. I've had an irrational and random dislike for anything Rust for some reason... probably because big business seems to like it, but will have to choke on that for this project :) reply arrdalan 16 hours agoparentThe reason I chose Rust is that it eliminates memory safety vulnerabilities, which have been a major headache is software systems. Using Rust provides higher security assurance for the hub and the native library used in the Android app. Besides, OpenMLS is implemented in Rust, making it easy to integrate it with Privastead. reply gauge_field 12 hours agoparentprevRegarding big business, I kind of have the opposite reaction that it has been more community maintained and been the one of the reasons I kind of admired it as a software project. They also tried to make this choice clear by making the tools, documents as accessible as possible. If you look at the history of Rust, the crates, cargo-team, etc, it is mostly community maintained. An example of such community-oriented large project is Bevy. It was only when libraries, rust itself was mature enough, the big tech companies started get involved (apart from Mozilla of course). Whenever the project gets big and important enough, it will get attraction from large companies. The same is true for C++, as well. reply globalnode 6 hours agorootparentYour comment persuaded me to have a little look at Rust. And I kinda like what I see. Every time I say to myself \"I'll never use that tech\", I end up using/learning it. reply EGreg 2 hours agoprevA few years ago I wanted to build a self-sovereign home security system for entire communities and HOAs. We spoke to IBM engineers about scanning video at the edge with machine learning models. I bought cameras that use RTMP and RTSP, and sent them to my developers. From there, it is not a hard task to use WebRTC (which has end-to-end encryption) to stream somewhere. But my use case involved retaining encrypted videos (every minute and every camera would have a different key) and have a well defined protocol for decryption. To me, the issue of security is not just on one end (recording the crimes) but the other end (mass surveillance, aka who watches the watchers?) Here is a longer write-up I did: https://community.qbix.com/t/balancing-privacy-and-accountab... If anyone is interested in working on a startup with me and selling to homeowners and gated communities, reach out to greg at the domain qbix.com reply amluto 15 hours agoprevNifty. Have you considered trying to make the server be able to run as something like a Cloudflare worker? reply arrdalan 14 hours agoparentNot yet. The server is currently fairly simple and doesn't do much other than relaying ciphertexts (and MLS group welcome messages). It should be okay serving one user and several cameras, but is certainly not scalable. I have plans to improve its scalability, but that is not at the top of the todo list. I'll check out Cloudflare workers for sure. Thanks. reply amluto 6 hours agorootparentI wasn’t actually thinking of scalability. I was thinking that Cloudflare’s Tunnel is a handy, and often free, way to make a LAN service available from outside, and the combination of Tunnel and Workers could make for a nice, easy to deploy, solution. reply kkfx 8 hours agoprevZoneMinder? reply gregoriol 7 hours agoparentUsing it! reply hackermeows 14 hours agoprevDo self hosted options qualify for insurance discounts ? reply arrdalan 13 hours agoparentGood question. I'm not really familiar with insurance requirements. Privastead simply records videos from an IP camera and sends it to your phone. It does not modify the videos and hopefully that should be adequate for insurance companies. reply jokowueu 6 hours agoprevFantastic , thanks reply moritonal 10 hours agoprevThere is also https://shinobi.video/, as another alternative. reply scotty79 17 hours agoprevDoes the hub possess ability to delete recording from the server? I understand that hub being compromised is not a part of your threat model but I'm curious if you can preserve the recordings in case it was taken over. reply arrdalan 16 hours agoparentThe hub keeps a video until it is received by the app. The hub has a component called the delivery monitor, which processes acks from the app and keeps track of the delivery status of each video. It resends the videos when needed to ensure delivery. Once the app acks receipt of a video, the hub deletes it. The app then keeps all the videos until the user explicitly deletes them. (I've been thinking of a feature that automatically deletes all videos in the app that are older than a threshold, but haven't implemented it yet.) The server is untrusted and does not have access to the plaintext of videos. It only receives the ciphertext (encrypted video) and notifies the app to download them. When the app fetches a video, the server deletes the ciphertext (in order to be memory efficient). Yes, the hub is part of the Trusted Computing Base (TCB). That means that we trust it. If the attacker compromises it and takes full control of the hub, then the attacker has unfettered access to the IP camera. That being said, if the attacker only manages to take a snapshot of the keys used by the hub for encryption (and nothing more), then the forward secrecy and post-compromise security aspects of MLS comes to our rescue. They ensure that the attacker can only decrypt some of the messages (at most a full video depending on what keys are compromised), but nothing before and after that. reply eth0up 9 hours agoprevI appreciate this and will look into it eventually. Years ago, I used Motion[0] on a pi, laptop and desktop, configured to sftp the motion-activated images to a remote server. I could check the files upon alert and view them in rapid succession as 'video'. It was a clunky setup but worked, aside from the vulnerability of the computers being physically accessible to unwanted visitors - but for this I'd use nohup & physlock to at least prevent quick access. This relatively free setup had limitations, mostly due to my lack of skills and resources, but it provided significant consolation and I'd bring it with me while traveling, eg using the laptop setup for hotel rooms, etc. The fact that the great Bezosauron et al wasn't involved, did please me. 0. https://motion-project.github.io/ reply hereme888 17 hours agoprevDude good job. Thanks for this. reply 0points 7 hours agoprevCertainly interesting project, but I do encourage people to do more thorough research before starting a new project and claiming they needed to fill a void. > However, I could not find one that I could trust due to privacy concerns. > Many of them upload the plaintext of videos to their servers and none is fully open-source as far as I know. There's Frigate [1] which is off-cloud, opensource and self-hosted and it does person detection using GPU or Google Coral. It relays camera streams over WebRTC using ssl and works well with Home Assistant [2], which is your self-hosted off-cloud smart home solution. Then there's moonfire-nvr [3] which is written in rust and less feature complete than frigate. And then there's sentryshot [4], another option written in rust. Interestingly, your project got more stars since you posted it on HN than sentryshot has in total, and they have had a product out since 2021 while yours only support a single camera model and has no object detection. I suppose this reflects how unknown these option are, which is kind of unexpected since googling \"rust nvr\" lists them all in the first page. 1: https://frigate.video/ 2: https://www.home-assistant.io/ 3: https://github.com/scottlamb/moonfire-nvr 4: https://github.com/SentryShot/sentryshot > The prototype currently has a lot of limitations: mainly that it has only been tested with one IP camera Since you use rust, you should maybe have a look at retina: https://github.com/scottlamb/retina Personally, I use frigate with Home Assistant linked to HomeKit, so I get snapshots & video from frigate straight into my iPhone lock screen as soon as it detects moving people or cars outside my house. reply acidburnNSA 7 hours agoparentI've got frigate running at 3 sites and use wireguard to log into them and then monitor/review through the browser. Truly awesome technology. Self-hosting object detection and getting alerts is very effective and satisfying. reply thelittleone 6 hours agorootparentI'm looking at a similar setup. Mind sharing which cameras you went with? reply acidburnNSA 6 hours agorootparentI have a bunch of Amcrest brand cams, indoor and outdoor, plus one Reolink combination floodlight/camera. I like these PoE outdoor ones best: IP5M-T1179EW Indoor PTZ ones: IP4M-1041W Old wifi outdoor one: IP4M-1026E I got some with the plastic dome around them but they do not work well at night, especially in weather. I also have a ridiculous industrial PTZ camera that I put on my roof to watch birds, planes, and boats, and this thing is an absolute beast. I love it. https://www.alibaba.com/product-detail/4k-action-ptz-camera-... reply smitelli 5 hours agorootparentThose Amcrest cameras are rebranded Dahua products with the logos changed. Right down to the firmware bugs and confusing UI. They’re actually great cameras for the money, but man do I loathe their quirks sometimes. reply 0points 6 hours agorootparentprevI use Reolink fwiw. Cheap china cameras which stream in https & rtsp in h264 and h265. reply arrdalan 3 hours agoparentprevThanks for the pointers. Frigate, Home Assistant, and HomeKit were mentioned in other comments and we had some discussions there. They're certainly good setups. A key design decision for me was using a strong end-to-end encryption scheme between the camera/hub and the app. MLS fit this need perfectly and therefore I came up with the design that is now Privastead. I was also less concerned with supporting many cameras since all I needed was one camera inside my house. Rather, I wanted a lean solution that I could fully understand and reason about. I will however look more carefully into the NVRs you listed to see if I can borrow some ideas to support more cameras more easily in Privastead. reply VTimofeenko 16 hours agoprevI've been a happy user of frigate (https://frigate.video) with fully local isolated setup: multiple poe cameras on a dedicated network, coral to supplement on-camera recognition, HA+Prometheus for alerts and smarts. Excited to see another project, especially in Rust(not for the memes; python env management has bit me a few times when hacking). One major gripe with frigate that I have is the way it treats detection events as pointers to video files. This makes replicating events off site a major pain. reply stevenpetryk 15 hours agoparentHave you found cameras you’re happy with? That’s the main thing holding me back is not knowing a good PoE camera to use. reply hughesjj 14 hours agorootparentHikvision and reolink are highly rated onvif ones that support poe, but as long as the camera support ONVIF from there just compare as normal reply hipitihop 13 hours agorootparentI can second Frigate and welcome any work in this space, so nice work OP will have a look. For others asking, I have found so far Hikvision POE PTZ domes like DS-2DE2A404IW-DE3 have been reliable, depends on your budget. I have cams themselves fully locked down from internet and on a separate subnet on the local net. OPNSense is also a friend here. I would love some OSS firmware for these cams. For remote, I find ZeroTier to Frigate & Home Assistant machines is all I need. I get why others WireGuard too. YMMV reply hunter2_ 13 hours agorootparentprevSearch for \"Hikvision compromised\" though. The results are damning. I suppose there's not much of a concern if using a proper firewall on a managed network, however. reply nonrandomstring 9 hours agorootparentprevAren't HikVision the ones banned by many governments because they have dodgy \"call home to China\" firmware? Persoanlly I go with the Eyeball Entities Omnicam with an Omniscient Systems Xcam-REAM control centre [0]. [0] https://cybershow.uk/media/ads/cctv-ads.mp3 reply paranoidrobot 9 hours agorootparentEven if it does have phone-home functions, why does the camera itself need internet access? IMO, audit the hardware for wireless radios (PoE cameras shouldn't have them), and put them on a camera-only VLAN that can only talk to the video server. Maybe not suitable for super secure TS locations, but in general should be fine for 99% of situations. reply nonrandomstring 30 minutes agorootparentI think if you've got total control over them, not a problem compartmentalising. Problem is, they're like loitering munitions, next person comes along and connectes them up to a router or misconfigures a firewall... Much as I hate e-waste sadly best place is in the bin. They're cheap but, easy come easy go. reply VTimofeenko 14 hours agorootparentprevReolink have not disappointed me so far. I am using a duo 2 cam watching over the front of the property and a collection of their bullet cams. There's a lot of recommendations on frigate github discussions. Some models indeed have quirks. On top of decent cameras they also have pretty well-documented API. reply arrdalan 14 hours agoparentprevA fully isolated setup on a dedicated network can certainly provide good privacy. A key benefit of Privastead is that it can send the videos to your smartphone remotely using a strong (MLS-based) end-to-end encryption. And I share your excitement about Rust. :-) reply VTimofeenko 13 hours agorootparentThe remote access part can be solved by wireguard or tailscale for less hands-on approach. Ntfy.sh/telegram/whatever cool kids use these days for notifications. This and my previous comment probably amount to a full Dropbox comment now :) Can you elaborate on the sending video part -- do you transfer the whole file or a link to the file on the server? Curious about the replication piece I mentioned reply arrdalan 13 hours agorootparentPrivastead uses end-to-end encryption between the hub and the app. The hub encrypts the whole video file and sends it (through an untrusted server) to the app. The app will have a local copy of the video. The hub deletes the video upon delivery to the app. The server never has access to the unecrypted video. reply tehlike 12 hours agorootparentWebRTC might provide a good way to handle this. reply arrdalan 12 hours agorootparentYes, indeed. Using WebRTC and MLS together should provide good performance and privacy. In fact, Discord recently did that: https://discord.com/blog/meet-dave-e2ee-for-audio-video It's something I plan to look into at some point. reply windexh8er 13 hours agorootparentprevI've been using locally hosted NVRs going on two decades now and so I definitely appreciate another local NVR option. But given the current lack of common requirements (e.g. more than 1 camera), curious if you're seeing any usage outside of your own? As others have mentioned Frigate is filling a nice gap for many, currently. I can only connect to it via a VPN I control and I can easily get notifications via Home Assistant. I feel as though I have protection of getting the feeds to my phone securely, already. And while I'd love to be able to trust exposing Frigate and making it more accessible, I really don't have a need nor would I as the barrier to entry is already low enough to not impose much friction. My intent isn't to dissuade you (obviously you wrote it for yourself) but I'd be curious if implementing a more secure transport in an existing OSS NVR that has traction was something you considered? reply arrdalan 12 hours agorootparentYour local setup plus VPN/wireguard is a good design. For me, port forwarding on my router was a no-go due to the increased attack surface. Therefore, I wanted to use the cloud to route the videos and hence I decided to use a strong end-to-end encryption scheme (MLS). Regarding the limitation you mentioned: I plan to address them. The next task on my plate is to support multiple cameras. :-) reply hamandcheese 10 hours agorootparentprevFWIW, e2ee is a feature that mostly matters to me when I don't own the infrastructure, and my bytes are flowing over someone else's platform. If I do own all the infra, it seems like a lot of extra effort and complexity in the application which could be farmed out to other solutions (I.e. wireguard). reply arrdalan 4 hours agorootparentThat's right. My assumption is that the server is untrusted and that's why I use end-to-end encryption. This allows me to use any server, possibly even one operated by a third-party service in the future. reply goodpoint 9 hours agorootparentprevHow is video streamed to the apps? Push notification services are not meant for real time video streaming. reply arrdalan 4 hours agorootparentPush notifications are for notifying the apps of events (e.g., motion). The actual encrypted video (either motion-triggered video or livestreams) are transferred through the server. reply discordance 8 hours agorootparentprevRTSP reply StayTrue 16 hours agoprevMy DIY setup uses cameras on an isolated VLAN that prohibits outbound internet access, RTSP data collected by Frigate[0] with Coral TPU recognition, and alerts via Pushover[1] notifications. My push notifications include an unencrypted thumbnail with a link to Frigate that only works via VPN connection. Seems like this project offers a lot of what I had to cobble together. Nice work. I like that it offers encrypted notifications although I don’t use Android so not ready for me yet. I don’t like the Google dependency. [0] https://frigate.video/ [1] https://pushover.net/ reply bobbob1921 1 hour agoparentExcellent set up, and in fact exactly what I’m using / how I’m set up. In addition for a few IOT type sensors and things like thermostats around my house, I have another vlan as well as a dedicated SSID tagged for that vlan, that only allows Internet access and everything else is dropped for that vlan. (Ie a firewall rule on the forward chain that drops anything in on that IOT vlan, that has a destination address of the private ip ranges: ie dst= 10.x or 192.168.x or 172…. anything locally that needs to access those sensors does so through the cloud/Internet, and for the one or two device devices that need to access them locally I have a fw rule allowing just that devices IP reply arrdalan 13 hours agoparentprevA fully isolated setup can certainly provide good privacy. A key benefit of Privastead is that it can send the videos to your smartphone remotely using a strong (MLS-based) end-to-end encryption. Privastead does not currently send thumbnails as part of the notifications. Performing proper end-to-end encryption on data in push notifications is a bit tricky and I haven't solved that yet. I do have some ideas on how to make that happen though. Once I do, I plan to add thumbnails to push notifications as well. And thanks for the pointer on Pushover. I'll check it out to see if it can be a replacement for Google FCM. reply modeless 11 hours agoprevI've been wishing for a self hosted alternative to cloud cameras. My house was recently robbed and it was incredibly frustrating to know that criminals were literally in my house but not be able to see them due to the general crappiness of modern big tech software. Seems silly to pay to upload all my video of my own house to who knows where and struggle to download it back with absurdly sluggish proprietary software when I have perfectly good computers here already. I should be able to check my cameras without waiting 10-30 seconds for loading spinners and I should be able to scrub through time instantly instead of waiting for interminable loading every time I touch the seek control. What camera hardware are people using for custom setups? Is there anything out there that is wireless but with high quality/security firmware and reasonably priced? reply mattlondon 11 hours agoparentUbiquiti cameras store the data on-prem. Not open source, and not the cheapest (but not absurd either). App is acceptable. reply Tempest1981 10 hours agorootparentI read elsewhere that people are upset since Ubiquiti now requires using Unifi hub, at least for initial configuration. Maybe others do too? reply mattlondon 7 hours agorootparentI think the hub thing is where the local storage is? I don't know if they are just open RTSP things or not that you could use with any random NVR? I have unifi WiFi APs so have all the rest of the hub/controller anyway and trialled one camera but ultimately opted for Nest cameras instead. reply wkat4242 9 hours agoparentprevI absolutely hate cloud solutions but in this case there's a big benefit in that the robbers can't steal or smash your servers. They can do that to your networking gear though but by then hopefully their image has already made it to the cloud. reply modeless 9 hours agorootparentIn general I wouldn't expect random street criminals to be able to specifically locate and identify and properly disable a nondescript server in a closet in the middle of a break-in. They're frantically looking for cash and jewelry and portable commodity electronics and firearms. Stuff they can pocket and scram and easily use or fence later. reply 9dev 10 hours agoparentprevSo you did have a cloud camera system in place but it didn’t record them, or you don’t have one because all available options are shitty? reply modeless 10 hours agorootparentI do (Nest), and it did record them, but the shitty app made it really hard to see what was happening. Takes forever for the app to load, and it can't show all the cameras at once, and tapping into each camera shows another loading spinner, and then seeking through time is frustratingly slow and imprecise. Then there's the split between old cameras in one app and new cameras in another because Google can't be bothered to update old cameras to work with the new app (which is not any better than the old one anyway, worse if anything). It took minutes longer than necessary to get the full picture of what was happening, and when criminals are rifling through your stuff every second matters. Then after the fact it was a huge pain to get the recordings in a form I could archive and share, and there's time pressure because the recordings disappear after just a few days unless manually archived. OTOH our alarm system (Ring) performed very well. reply wkat4242 9 hours agorootparentYeah my ring alarm worked well too when my place was broken into. It scared them off, they broke down the door but didn't actually come inside. Unfortunately that prompted to police to be less than helpful (they considered it vandalism instead of a burglary and didn't even take prints). And because they didn't enter the house my cameras didn't see their faces so I couldn't try to track them down on my own either. But at least my stuff was ok. reply 9x39 9 hours agoparentprevIf you insist on 'secure' firmware, you're fighting uphill and probably going to be disappointed with the ecosystem out there. Tunnel back to your home network to access cams if that worries you. I recommend either Ubiquiti or Reolink cameras, both are power-over-Ethernet. Both offer an app that allows connecting back to your equipment. UI will require their Protect software running on one of their local platforms in your house with storage on 1 or more disk drives, while Reolink offers direct storage on microSD on the camera itself, capped at 256GB. reply modeless 9 hours agorootparentUnfortunately my house is not wired for Ethernet and it's a rental so I can't install it. Wireless is a must and security is obviously important for wireless. But I think you are correct that I will be disappointed by what's out there. reply 9x39 9 hours agorootparentUnfortunate. Reolink even has wireless cameras, but they will require a DC input, so you'd need an outdoor electrical outlet if you can't drill any holes and sneak either a power or UTP Ethernet cable. Given the strict physical requirements, you might be looking at battery-backed wireless cameras of the type often found on Amazon, but afaik they will never offer the local storage or control you want. I will say that surprising things can be accomplished with small drilled holes and a fishtape. Your risk tolerance and how nice the rental are obviously very important factors. reply kdmtctl 8 hours agorootparentprevI used a Reolink Argus with a battery and Wi-Fi connection. The one I used is probably EOL now, but a quick look at the site shows even solar chargeable options. So, it is possible. reply ryandrake 5 hours agorootparentI've got a Reolink Argus 2E which look like it's been replaced by the 3E, but it works great without any wired connection to the house. I've got it solar powered and strapped to a tree, unprotected by the elements. 2 years and still working. reply dylan604 16 hours agoprev [–] \"Privastead uses the Google Firebase Cloud Messaging (FCM)\" I'm am totally unfamiliar with this, but what level of concern that theGoogs pulls the plug on this service about the time you gain traction? reply arrdalan 16 hours agoparent [–] I'm not too concerned with that. As far as I know, almost all Android apps that need to send notifications to the device use FCM. If Google pulls the plug on it, many apps will stop working as intended. reply SushiHippie 6 hours agorootparent [–] The open source/degoogled alternative to that is UnifiedPush, did you take a look at this? https://unifiedpush.org/ https://ntfy.sh/ reply arrdalan 4 hours agorootparent [–] I did not, but I will. I would just point out that Privastead assumes the the push notification infrastructure is untrusted anyway. reply dylan604 4 hours agorootparent [–] it's not about trusted/untrusted as in what will they do with the data. it's entirely about will the service be available long term or if theGoogs will deprecate it in the future. I have no skin in the game of push notification infrastructure, but we've seen plenty of ShowHNs where a critical part of the Show is a total dependency on a 3rd party shim. While this isn't something like a dependency on a Twitter API that suddenly got yanked, it is still something that devs should really consider. Especially given theGoogs' track record. Maybe this isn't the case it is a library deeply embedded in the Android ecosystem, but anytime a ShowHN mentions a dependency on theGoog it sets of the SpideySenses reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author developed a home security camera system called Privastead, focusing on privacy by using end-to-end encryption with OpenMLS and implementing it mostly in Rust.- Privastead is open-sourced for those with privacy concerns, requiring a compatible IP camera, a local machine, a server, and an Android smartphone for setup.- Currently, the system supports only one IP camera and Android, but the author seeks community feedback and contributions to enhance its capabilities."
    ],
    "commentSummary": [
      "Privastead is an open-source home security camera system developed to address privacy concerns with existing solutions, utilizing end-to-end encryption via OpenMLS.- The system is primarily implemented in Rust and requires a supported IP camera, a local machine, a server, and an Android smartphone for operation.- The project is available on GitHub, inviting feedback and contributions from the community to enhance its functionality."
    ],
    "points": 386,
    "commentCount": 109,
    "retryCount": 0,
    "time": 1733004829
  },
  {
    "id": 42283950,
    "title": "Ntfs2btrfs does in-place conversion of NTFS filesystem to the open-source Btrfs",
    "originLink": "https://github.com/maharmstone/ntfs2btrfs",
    "originBody": "Ntfs2btrfs Ntfs2btrfs is a tool which does in-place conversion of Microsoft's NTFS filesystem to the open-source filesystem Btrfs, much as btrfs-convert does for ext2. The original image is saved as a reflink copy at image/ntfs.img, and if you want to keep the conversion you can delete this to free up space. Although I believe this tool to be stable, please note that I take no responsibility if something goes awry! You're probably also interested in WinBtrfs, which is a Btrfs filesystem driver for Windows. Thanks to Eric Biggers, who successfully reverse-engineered Windows 10's \"WOF compressed data\", and whose code I've used here. Usage On Windows, from an Administrator command prompt: ntfs2btrfs.exe D:\\ Bear in mind that it won't work with your boot drive or a drive containing a pagefile that's currently in use. If you are using WinBtrfs, you will need to clear the readonly flag on the image subvolume before you can delete it. On Linux, as root: ntfs2btrfs /dev/sda1 Installation On Windows, go to the Releases page and download the latest Zip file, or use Scoop. For Linux: Arch Fedora (thanks to Conan-Kudo) Gentoo - available as sys-fs/ntfs2btrfs in the guru repository Debian (thanks to alexmyczko) Ubuntu (thanks to alexmyczko) openSUSE (thanks to David Sterba) For other distributions or operating systems, you will need to compile it yourself - see below. Changelog 20240115 Fixed compilation on GCC 14 (-Werror=incompatible-pointer-types now enabled by default) 20230501 Fixed inline extent items being written out of order (not diagnosed by btrfs check) Fixed metadata items being written with wrong level value (not diagnosed by btrfs check) ADSes with overly-long names now get skipped 20220812 Added --no-datasum option, to skip calculating checksums LXSS / WSL metadata is now preserved Fixed lowercase drive letters not being recognized Fixed crash due to iterator invalidation (thanks to nyanpasu64) Fixed corruption when NTFS places file in last megabyte of disk 20210923 Added (Btrfs) compression support (zlib, lzo, and zstd) Added support for other hash algorithms: xxhash, sha256, and blake2 Added support for rolling back to NTFS Added support for NT4-style security descriptors Increased conversion speed for volume with many inodes Fixed bug when fragmented file was in superblock location Fixed buffer overflow when reading security descriptors Fixed bug where filesystems would be corrupted in a way that btrfs check doesn't pick up 20210523 Improved handling of large compressed files 20210402 (source code only release) Fixes for compilation on non-amd64 architectures 20210105 Added support for NTFS compression Added support for \"WOF compressed data\" Fixed problems caused by sparse files Miscellaneous bug fixes 20201108 Improved error handling Added better message if NTFS is corrupted or unclean Better handling of relocations 20200330 Initial release Compilation On Windows, open the source directory in a recent version of MSVC, right-click on CMakeLists.txt, and click Compile. On Linux: mkdir build cd build cmake .. make You'll also need libfmt installed - it should be in your package manager. Compression support requires zlib, lzo, and/or zstd - again, they will be in your package manager. See also the cmake options WITH_ZLIB, WITH_LZO, and WITH_ZSTD, if you want to disable this. What works Files Directories Symlinks Other reparse points Security descriptors Alternate data streams DOS attributes (hidden, system, etc.) Rollback to original NTFS image Preservation of LXSS metadata What doesn't work Windows' old extended attributes (you're not using these) Large (i.e >16KB) ADSes (you're not using these either) Preservation of the case-sensitivity flag Unusual cluster sizes (i.e. not 4 KB) Encrypted files Can I boot Windows from Btrfs with this? Yes, if the stars are right. See Quibble.",
    "commentLink": "https://news.ycombinator.com/item?id=42283950",
    "commentBody": "Ntfs2btrfs does in-place conversion of NTFS filesystem to the open-source Btrfs (github.com/maharmstone)366 points by Sami_Lehtinen 22 hours agohidepastfavorite133 comments SomeoneOnTheWeb 13 minutes agoI would have needed that like 2 months ago, when I had to format a hard drive with more than 10TB of data into from NTFS... ^^ Nic project! reply ComputerGuru 1 hour agoprevI found the link to Quibble, an open and extensible reverse engineering of the Windows kernel bootloader to be much more intriguing: https://github.com/maharmstone/quibble reply oDot 1 hour agoprevIs anyone here using BTRFS and can comment on its current-day stability? I used to read horror stories about it reply badsectoracula 2 minutes agoparentI've been using it for a few years now on my main PC (has a couple SSDs and a large HDD) and my laptop, it was the default of openSUSE and just used that. Then i realized that snapshots are a feature i didn't knew i wanted :-P. Never had a problem, though it is annoying that whatever BTRFS thinks is free space and what the rest of the OS thinks is free space do not always align. It has rarely been a problem in practice though. reply jcalvinowens 24 minutes agoparentprevI've used BTRFS exclusively for over a decade now on all my personal laptops, servers, and embedded devices. I've never had a single problem. It's the flagship Linux filesystem: outside of database workloads, I don't understand why anybody uses anything else. reply einsteinx2 38 minutes agoparentprevI’ve been using it for a few years on my NAS for the all the data drives (with Snapraid for parity and data validation), and as the boot drive on a few SBCs that run various services. Also use it as the boot drive for my Linux desktop PC. So far no problems at all and I make heavy use of snapshots, I have also had various things like power outages that have shut down the various machines multiple times. I’ve never used BTRFS raid so can’t speak to that, but in my personal experience I’ve found BTRFS and the snapshot system to be reliable. Seems like most (all?) stories I hear about corruption and other problems are all from years ago when it was less stable (years before I started using it). Or maybe I just got lucky ¯\\_(ツ)_/¯ reply quotemstr 1 hour agoparentprevI've used it for my personal machines and backups (via btrbk) for years without any issues reply cryptonector 17 hours agoprevThinking of how I'd do this for ZFS... I think I'd do something like: add a layer that can read other filesystem types and synthesize ZFS block pointers, then ZFS could read other filesystems, and as it writes it could rewrite the whole thing slowly. If ZFS had block pointer rewrite (and I've explained here before why it does not and cannot have BP rewrite caoabilities, not being a proper CAS filesystem), one could just make it rewrite the whole thing to finish the conversion. reply npn 11 hours agoprevI tried this one before, resulted in a read-only disk. Hope it improves since then. reply Dwedit 20 hours agoprevI would be very surprised if it supported files that are under LZX compression. (Not to be confused with Windows 2000-era file compression, this is something you need to activate with \"compact.exe /C /EXE:LZX (filename)\") reply ruined 19 hours agoparentit seems to contain code that handles LZX, among other formats https://github.com/search?q=repo%3Amaharmstone%2Fntfs2btrfs%... reply the_hoser 21 hours agoprevThe degree of hold-my-beer here is off the charts. reply koverstreet 21 hours agoparentIt's not quite as dangerous as you'd think. The standard technique is to reserve a big file on the old filesystem for the new filesystem metadata, and then walk all files on the old filesystem and use fiemap() to create new extents that point to the existing data - only writing to the space you reserved. You only overwrite the superblock at the very end, and you can verify that the old and new filesystems have the same contents before you do. reply jeroenhd 21 hours agorootparentI believe that is also the method [btrfs-convert](https://btrfs.readthedocs.io/en/latest/Convert.html) uses. A cool trick that tool uses is to keep the ext4 structures on disk (as a subvolume), which allows reverting to ext4 if the conversion didn't go as planned (as long as you don't do anything to mess with the ext4 extents, such as defragmenting or balancing the filesystem, and you can't revert after deleting the subvolume of course). reply doublepg23 13 hours agorootparentI tried that on a system in 2020 and it just corrupted my new FS. Cool idea though. reply jeroenhd 11 minutes agorootparentI've never had the conversion corrupt a filesystem for me (plenty of segfaults halfway through, though). It's a neat trick for when you want to convert a filesystem that doesn't have much on it, but I wouldn't use it for anything critical. Better to format the drive and copy files back from a backup, and you probably want that anyway if you're planning on using filesystem features like snapshots. Windows used to feature a similar tool to transition from FAT32 to NTFS. I'd have the same reservations about that tool, though. Apple also did something like this with an even weirder conversion step (source and target filesystem didn't have the same handling for case sensitivity!) and I've only read one or two articles about people losing data because of it. It can definitely be done safely, if given enough attention, but I don't think anyone cares enough to write a conversion tool with production grade quality. reply jorvi 8 hours agorootparentprevMy conversion went fine, but there were so many misaligned sectors and constant strange checksum errors (on files written after the conversion). With the cherry on top being that if there’s more than X% of checksum errors, btrfs refuses to mount and you have to do multiple arcane incantations to get it to clear all its errors. Real fun if you need your laptop for a high priority problem to solve. Lesson learned: despite whatever “hard” promises a conversion tool (and its creators) make, just backup, check the backup, then format and create your new filesystem. reply ahartmetz 10 hours agorootparentprevYou don't understand. You did get a btrfs that worked normally. /s reply Joe_Cool 19 hours agorootparentprevI believe you are right. You can only convert back to the metadata from before. So any new or changed (different extents) files will be lost or corrupted. So best to only mount ro when considering to rollback. Otherwise it's pretty risky. reply heftig 19 hours agorootparentNo, it also covers the data. As long as you don't delete the rollback subvolume, all the original data should still be there, uncorrupted. Even if you disable copy-on-write, as long as the rollback subvolume is there to lay claim to the old data, it's considered immutable and any modification will still have to copy it. reply bongodongobob 12 hours agorootparentprevThis is a weird level of pedantry induced by holding many beers tonight, but I've always thought of \"Hold my beer\" as in \"Holy shit the sonofabitch actually pulled it off, brilliant\". I think it's perfectly fitting. Jumping a riding lawnmower over a car with a beer in hand but they actually did the math first. I love it. reply Gigachad 12 hours agorootparentIt’s referring to a comment a drunk person would make before doing something extremely risky. They need someone to hold the beer so it isn’t spilled during what’s coming next. reply bongodongobob 8 hours agorootparentRight, but in those situations they succeed, kind of like \"the cameraman never dies\". reply bmacho 4 hours agorootparent{\\off I think they used \"hold my beer\" correctly. It can be used for any weird idea, that a drunk person would actually try (usually with a stretch), regardless if they succeed or not. I don't think that \"the SOAB actually pulled it off\" is part of the usage.} reply boricj 21 hours agoparentprevA couple of years ago it was more like juggling chainsaws: https://github.com/maharmstone/ntfs2btrfs/issues/9 I tracked down a couple of nasty bugs at that time playing around with it, hopefully it's more stable now. reply tgma 19 hours agoparentprevApple did something like this with a billion live OS X/iOS deployments (HFS+ -> APFS). It can be done methodically at scale as other commenters point out, but obviously needs care). reply ComputerGuru 1 hour agorootparentYou don’t need to look that far. Many of us here lived through the introduction of NTFS and did live migrations from FAT32 to NTFS in the days of Windows 2000 and Windows XP. reply SanjayMehta 16 hours agorootparentprevWhen this first showed up I took 3 backups: two on networked drives and one on an external drive which was then disconnected from the system. The second time I just went “meh” and let it run. reply tgma 15 hours agorootparentCraig Federighi on some podcast once said they conducted dry-runs of the process in previous iOS updates (presumably building the new APFS filesystem metadata in a file without promoting it to the superblock) and checking its integrity and submitting telemetry data to ensure success. reply 3eb7988a1663 14 hours agorootparentYou can do all the testing in the world, but clicking deploy on that update must have been nerve wracking. reply Gigachad 12 hours agorootparentApple doesn’t just deploy to the whole world in an instant though. First it goes to the private beta users, then the public beta users, and then it slowly rolls out globally. Presumably they could slow down the roll out even more for a risky change to monitor it. reply tgma 11 hours agorootparentSure, but still whoever wrote the patch had his ass on the line even shipping to a batch of beta users. Remember this is Apple not Google where the dude likely got promoted and left the team right after pressing click :) reply chasil 20 hours agoparentprevNote this is not the Linux btrfs: \"WinBtrfs is a Windows driver for the next-generation Linux filesystem Btrfs. A reimplementation from scratch, it contains no code from the Linux kernel, and should work on any version from Windows XP onwards. It is also included as part of the free operating system ReactOS.\" This is from the ntfs2btrfs maintainer's page. https://github.com/maharmstone/btrfs reply chungy 20 hours agorootparentIt's the same file system, with two different drivers for two different operating systems. reply chasil 20 hours agorootparentThe metadata is adjusted for Windows in a way that is foreign to Linux. Do Linux NTFS drivers deal with alternate streams? \"Getting and setting of Access Control Lists (ACLs), using the xattr security.NTACL\" \"Alternate Data Streams (e.g. :Zone.Identifier is stored as the xattr user.Zone.Identifier)\" reply biorach 20 hours agorootparentNot sure what point you're making here. WinBtrfs is a driver for the same btrfs filesystem that Linux uses. It's most common use case is reading the Linux partitions in Windows on machines that dual-boot both operating systems reply dark-star 19 hours agorootparentprevWhat? Why would you need a Linux NTFS driver to read a btrfs filesystem? that makes no sense. Storing Windows ACLs in xattrs is also pretty common (Samba does the same) reply chasil 19 hours agorootparentI'd delete my comment if I could at this point. reply cwillu 20 hours agorootparentprevYes it is? reply MisterTea 20 hours agoparentprevAs someone who has witnessed Windows explode twice from in-place upgrades I would just buy a new disk or computer and start over. I get that this is different but the time that went into that data is worth way more than a new disk. It's just not worth the risk IMO. Maybe if you don't care about the data or have good backups and wish to help shake bugs out - go for it I guess. reply rini17 12 hours agorootparentIf only it had a native filesystem with snapshotting capability... reply ssl-3 4 hours agorootparentWhich \"it\"? Both btrfs and NTFS have snapshot capabilities. reply rini17 2 hours agorootparentIt's so well hidden from users it might not as well exist. And you can't snapshot only a part of the ntfs filesystem. reply Joker_vD 19 hours agorootparentprevAnd the new disk is also likely to have more longevity left in it, doesn't it? reply casey2 12 hours agoprevVery cool, but nobody will hear about this until at least a week after they format their ntfs drives that they have been putting off formatting for 2 years reply johnea 21 hours agoprevnext [78 more] [flagged] snapplebobapple 21 hours agoparentbtrfs isn't that terrible for desktop use right now. I mean, I wouldn't personally use it, I lost data on it a couple times four plus years ago, but it's come a long way since then. (my preference is keep everything I care about on a fileserver like truenas running zfs with proper snapshotting, replication and backup and live dangerously on the desktop testing out bcachefs, but I recognize not everyone can live my life and some people just want a laptop with a reasonable filesystem resistant to bit rot. reply jeroenhd 21 hours agorootparentI recently found out Fedora defaults to btrfs with zstd compression enabled by default. Seems to work well enough. On my personal devices I prefer BTRFS' snapshotting ability over the risk of having to restore from backup at some point. reply emacsen 19 hours agorootparentIs there decent encryption support, or are we stuck using full disk encryption at the block level? reply jeroenhd 17 hours agorootparentI don't know, I just use FDE because I don't trust filesystem level encryption to protect against the many side channel attacks one can inject into a running system. LUKS is good enough for me. It looks like there's slow but steady progress on FSCRYPT support on BTRFS: https://bkhome.org/news/202403/linux-kernel-btrfs-supports-f... No idea what kind of state it is in currently. reply RussianCow 17 hours agorootparentprevThe latter at this point. reply arjvik 21 hours agorootparentprevHow is bcachefs for personal use these days? reply chasil 20 hours agorootparentThe political situation for bcachefs is far from good, with pressure from Linus and a CoC violation. The net effect will likely delay stability. https://www.phoronix.com/news/Bcachefs-Fixes-Two-Choices https://www.phoronix.com/news/Linux-CoC-Bcachefs-6.13 reply koverstreet 20 hours agorootparentHonestly the political situation will probably be a /good/ thing for long term stability, because I get a few months without any stupid arguments with upstream and finally get to write code in peace :) It sucks for users though, because now you have to get my tree if you want the latest fixes, and there's some stuff that should be backported for forwards compatibility with the scalability improvements coming soon [1]. [1]: https://www.patreon.com/posts/more-expensive-116975457 reply MrDrMcCoy 38 minutes agorootparentHey Kent, I just wanted to thank you for all your hard work. For those of us that would like to use the code in your tree to get the latest fixes and help with testing, I was wondering how hard it would be to set up OBS to automatically build and package fresh kernels and userland tools for OpenSUSE/Debian/Arch/etc, nightly and/or for each tag. I think it would help adoption as well as comfort, knowing that your improvements will arrive as soon as they are available. reply wkat4242 5 hours agorootparentprevI have to say I do see Linus' point though. Mainline is for stuff that's production-ready and well tested. Having said that it's a great initiative and I hope it becomes ready for prime time. I think btrfs is taking too long and is infused with too much big tech interest. reply chasil 19 hours agorootparentprevI'm hoping to use your filesystem when it's ready. Everyone wishes that this were easier for you. reply homebrewer 19 hours agorootparentprevDon't worry about the users, we'll manage somehow, it's such a tiny burden compared to the actual development. I'm just really happy to see you're not discouraged by the petty political mess and keep pushing through. Thank you! reply Herbstluft 7 hours agorootparentprevI've been running bcachefs on one of my laptops since it hit linux stable. Just wanted to say thank you for all your constant work on it. reply BlackLotus89 19 hours agorootparentprevI personally stopped compiling your code in my personal repo when bcachefs was upstreamed. It often was a pain to rebase against the latest hardened code and I'm happier since it's upstream. I use your fs for 7-8 years now and I hope your latest changes to the disk format will actually improve mount performance (yes I'm one of the silent \"victims\" you were talking about). I hope nothing breaks... Anyway thank you for your work and I wish you all the best on the lkml and your work. reply mappu 19 hours agorootparentprevEchoing the sibling comment Kent, bcachefs is a really wonderful and important project. The whole world wants your filesystem to become the de-facto standard Linux filesystem for the next decade. One more month of LKML drama is a small price for that (at LKML prices). reply gkmcd 18 hours agorootparentprevI switched back to the arch default kernel for my 32TB home media server, would you recommend going back to compiling your kernel for the time being? reply koverstreet 17 hours agorootparentNot unless you've been hitting a bug you need the fix for reply pimeys 19 hours agorootparentprevBeen using it since 6.7 on my root partition. Around 6.9 there were issues that needed fsck. Now on 6.12 it is pretty stable already. And fast, it is easy to run thousands of Postgres tests on it. Not something zfs or btrfs really could do without tuning... So if you're a cowboy, now it's a good time to test. If not, wait one more year. reply snapplebobapple 19 hours agorootparentprevI haven't lost any data yet. It did something stupid on my laptop that looked like it was about to repeat btrfs's treatment of my data a few months ago but 15 minutes of googling on my phone and I figured out the right commands to get it to fix whatever was broken and get to a bootable state. I'm a decade away from considering it for a file server holding data I actually care about but as my main desktop and my laptop file system (with dotfiles backed up to my git instance via yadm and everything I care about nfs mounted in from my fileservers) it's totally fine. reply biorach 20 hours agorootparentprevI believe the main bcachefs mantainer does not advocate production use yet reply koverstreet 19 hours agorootparentWe're still six months or so from taking the experimental label off, yeah. Getting close, though: filesystem-is-offline bugs have slowed to a trickle, and it's starting to be performance issues that people are complaining about. Hoping to get online fsck and erasure coding finished before taking off experimental, and I want to see us scaling to petabyte sized filesystems as well. reply biorach 18 hours agorootparentOh wow... That's actually really fast progress all things considered. Well done! I really hope all the... umm... misunderstandings get worked out because you're doing great work. reply koverstreet 17 hours agorootparentIt's all stuff that's been in the pipeline for a long time. (And we'll see when online fsck and erasure coding actually land, I keep getting distracted by more immediate issues). Really, the bigger news right now is probably all the self healing work that's been going on. We're able to repair all kinds of damage without an explicit fsck now, without any user intervention: some things online, other things will cause us to go emergency read only and be repaired on the next mount (e.g. toasted btree nodes). reply qhwudbebd 9 hours agorootparentOne of the choices you've made that I really like is sharing the kernel and userspace filesystem code so directly in the form of libbcachefs. I get the impression this means the kernel can do practically everything userspace can, and vice versa. (I think the only exception is initialising devices by writing a superblock, although the kernel can take over the initialisation of the rest of the filesystem from that point onwards? And maybe turning passphrases into keys for encrypted-fs support which does an scrypt thing?) As well as giving you really powerful userspace tools for manipulating filesystems, this also suggests that a stripped down busybox module for bcachefs could consist of superblock writing and pretty much nothing else? Maybe a few ioctls to trigger various operations. \"Just leave it all to the kernel.\" reply tombert 19 hours agorootparentprevAfter a bit of peer pressure from a friend, I decided ended up using btrfs with my laptop about three months ago. It’s been fine thus far. reply aucisson_masque 19 hours agorootparentEmphasize on ‘this far’… To me that kind of experiment is the equivalent of changing your car brake for something ‘open source’, maybe better, maybe not. But when you need them you’re going to want to make sure they’re working. reply kccqzy 17 hours agorootparentHow much of an assurance do you want? I've been using btrfs on my desktop for 3+ years. And I've also experienced no issues thus far. reply kalleboo 4 hours agorootparentThe btrfs volume on my Synology NAS is 6 years old now, with 24/7 operation (including hosting web sites with constant read/write database activity) and going through several volume resize operations. No issues. reply do_not_redeem 21 hours agoparentprevWhat filesystem would you suggest that has data checksums, efficient snapshots, and doesn't require compiling an out of tree kernel module? reply badgersnake 19 hours agorootparentIf you artificially tailor your criteria such that the only answer you your question is btrfs then that is the answer you will get. reply homebrewer 19 hours agorootparentThere's nothing \"artificial\" in his requirements, data checksums and efficient snapshots are required for some workloads (for example, we use them for end-to-end testing on copies of the real production database that are created and thrown away in seconds), and building your own kernel modules is a stupid idea in many cases outside of two extremes of the home desktop or a well-funded behemoth like Facebook. reply arp242 8 hours agorootparentData checksums in particular is 90% of the reason I want to use a newer filesystem than XFS or Ext4. This is useful for almost any usage. Snapshots a bit less so, but even for my laptop this would be useful, mostly for backups (that is: create snapshot, backup that, and then delete the snapshot – this is how I did things on FreeBSD back in the day). A second use case would be safer system updates and easier rollbacks – not useful that often, but when you need it, it's pretty handy. reply ComputerGuru 18 hours agorootparentprevZFS… using a BSD kernel :) reply toast0 20 hours agorootparentprevZfs is in tree if you use a different kernel. :p reply johnny22 19 hours agorootparentno, that is not in in tree. it's in build. big difference. reply chasil 20 hours agorootparentprevThere is an OpenZFS port to Windows, but I'm not sure how to find it from here: https://github.com/openzfsonwindows/ZFSin There is also Microsoft's own ReFS: https://en.m.wikipedia.org/wiki/ReFS reply mappu 16 hours agorootparentprevNILFS2 is upstream, stores checksums for all data, and has the most efficient snapshot system bar none. Although I don't think i'd push the recommendation, solely because there are fewer eyes on it. reply ahartmetz 9 hours agorootparentHeh, NILFS2. The first flash-friendly filesystem in Linux. When Flash was still expensive, I used it on an SD card to keep my most important source trees to speed up builds and things like svn (yes yes!) diff. I thought it had languished due to better resourced and more sophisticated efforts like F2FS. reply freeone3000 21 hours agorootparentprevzfs is licensed too freely to be in-tree, but it’s still an excellent choice. reply riffraff 20 hours agorootparentI may be wrong but I don't think it's just \"excessive freeness\", the CDDL also has restrictions the GPL does not have (stuff about patents), it's mutual incompatibility. Apache v2 and GPLv3 were made explicitly compatible while providing different kinds of freedom. reply boomboomsubban 19 hours agorootparentThe CDDL is more permissive, it's a weak copyleft license while the GPL is strong copyleft, and that makes the two incompatible. Calling it \"excessive freeness\" is inflammatory, but they're broadly correct. reply starspangled 18 hours agorootparent> The CDDL is more permissive, it's a weak copyleft license while the GPL is strong copyleft, and that makes the two incompatible. Calling it \"excessive freeness\" is inflammatory, but they're broadly correct. It's not really. Many aspects of the license are free-er, but that's not what causes the incompatibility. The GPL does not have any kind of clause saying code that is distributed under too permissive of a license may not be incorporated into a derived/combined work. It's not that it's weak copyleft, it is that it contains particular restrictions that makes it incompatible with GPL's restrictions. BSD licenses do not have that incompatible restriction (= are freer than CDDL, in that aspect) and can be compatible with the GPL. reply arp242 8 hours agorootparentThe basic problem is that both licenses have a \"you must redistribute under this license, and you can't impose additional restrictions\"-type clause. There are some other differences between the licences, but that's what the incompatibility is about. Some have argued that this may not actually constitute an incompatibility, but not many are keen to \"fuck around and find out\" with Oracle's lawyers. So here we are. reply ChocolateGod 7 hours agorootparent> but not many are keen to \"fuck around and find out\" with Oracle's lawyers Oracle could spend 10 minutes and clear this up, but the fact they don't should be fear enough about any large company shipping with OpenZFS code. See Oracle vs Google reply wkat4242 3 hours agorootparentprevI think the whole license stuff is controlling software too much. All that legal blah has nothing to do with software and is only good for feeding overpriced lawyers. When I publish code I don't pick any license. It's just free for anyone to use for whatever. I don't like the GPL for this, it's way too complicated. I don't want to deal with that. It's a shame some legal BS like that is holding back the deployment of a great filesystem like ZFS. reply mort96 8 hours agorootparentprevThis is a complete misunderstanding. People take MIT code and put it in the kernel all the time. The issue is that the CDDL is not GPL-compatible because it has restrictions which the GPL doesn't have. reply chasil 20 hours agorootparentprevIn the Linux tree. In Windows, Satya would need to write Larry a check. It would probably be hefty. Edit: there was a time that this was planned for MacOS. https://arstechnica.com/gadgets/2016/06/zfs-the-other-new-ap... reply gjvc 19 hours agorootparentEdit: there was a time that this was planned for MacOS. That was a joyous prospect. A single volume manager/filesystem across all UNIX platforms would be wonderful. We had the UNIX wars of the 1990s. Since Linux won, they have been replaced by the filesystem wars. reply mort96 8 hours agorootparentIt wouldn't have been a candidate for \"the standard UNIX filesystem\" even if it was in macOS because SUN made an intentionally GPL-incompatible license for it. reply chasil 15 hours agorootparentprevFor what it's worth, I upvoted you. I see no reason for the down votes. reply do_not_redeem 20 hours agorootparentprevYeah the situation is unfortunate. There's a decent chance I'd be using ZFS if not for the licensing issues, but as a practical matter I'm getting too old to be futzing with kernel modules on my daily driver. reply telgareith 20 hours agorootparentDKMS solved these \"licensing issues.\" Dell is mum on official motivation- but it provides a licensing demarcation point, and a way for kernels to update without breaking modules- so it's easier for companies to develop for Linux. _Windows Drivers work the same way and nobody huffs and puffs about that_ I'd love to have an intelligent discussion on how one person's opinion on licensing issues stacks up against the legal teams of half the fortune 50's. Licensing doesn't work on \"well, I didn't mean it THAT way.\" reply do_not_redeem 19 hours agorootparentI admit I'm not fully up to date on whether it's actually \"license issues\" or something else. I'm not a lawyer. As a layman here's what I know. I go to the Arch wiki (https://wiki.archlinux.org/title/ZFS) and I see this warning under the DKMS section (as you advised): > Warning: Occasionally, the dkms package might not compile against the newest kernel packages in Arch. Using the linux-lts kernel may provide better compatibility with out-of-tree kernel modules, otherwise zfs-dkms-staging-gitAUR backports compatibility patches and fixes for the latest kernel package in Arch on top of the stable zfs branch So... my system might fail to boot after updates. If I use linux-lts, it might break less often. Or I can use zfs-dkms-staging-git, and my system might break even less often... or more often, because it looks like that's installing kernel modules directly from the master branch of some repo. As a practical matter I could care less if my system fails to boot because of \"license issues\" or some other reason, I just want the lawyers to sort their shit out so I don't have to risk my system becoming unbootable at some random inopportune time. Until then, I've never hit a btrfs bug, so I'm going to keep on using it for every new build. reply MrDrMcCoy 29 minutes agorootparentI've been bitten by kernel module incompatibility making my data unavailable enough times that I no longer consider ZFS to be viable under Linux. Using an LTS kernel only delays the issue until the next release is LTS. I really hope that bcachefs goes stable soon. reply arp242 8 hours agorootparentprevI used ZFS with DKMS on CentOS back in the day, and I found it a pain. It took a long time to compile, and I had some issues with upgrades as well (it's been a few years, so I have forgotten what the exact issues were). When it comes to filesystems, I very much appreciate the \"it just works\" experience – not having a working filesystem is not having a working system and it a pain to solve. Again, all of this has been a while. Maybe it's better now and I'm not opposed to trying, but I consider \"having to use DKMS\" to be a downside of ZFS. reply asyx 9 hours agorootparentprevI think ZFS on Debian is pretty safe. Debian is conservative enough that I don’t expect the DKMS build to fail on me. reply mort96 8 hours agorootparentI would never use a DKMS filesystem for / though. reply JackSlateur 20 hours agorootparentprevXfs ? Bcachefs ? Whatever you like because those features may not be implemented at the filesystem layer ? reply do_not_redeem 20 hours agorootparentxfs doesn't have data checksumming. > Note: Unlike Btrfs and ZFS, the CRC32 checksum only applies to the metadata and not actual data. https://wiki.archlinux.org/title/XFS --- bcachefs isn't stable enough to daily drive. > Nobody sane uses bcachefs and expects it to be stable —Linus Torvalds (2024) https://lore.kernel.org/lkml/CAHk-%3Dwj1Oo9-g-yuwWuHQZU8v%3D... reply mappu 19 hours agorootparentYou can get data checksumming for any filesystem with dm-integrity. reply kasabali 10 hours agorootparentIt has its own journaling mode just to really keep its integrity. Some things are just better solved at the filesystem level. reply koverstreet 19 hours agorootparentprevLinus is not a filesystem guy reply mort96 8 hours agorootparentBcachefs is literally labeled as experimental. reply starspangled 16 hours agorootparentprevYou don't have to be to see bug reports, bug fix patches, or test it yourself, of course. \"Filesystem guys\" also tend to wear rose colored glasses when it comes to their filesystem, at times (ext2, xfs, btrfs, etc.) reply koverstreet 16 hours agorootparentLinus absolutely does jump in on VFS level bugs when necessary (and major respect for that; he doesn't rest on laurels and he's always got his priorities in the right place) - but there's only so much he can keep track of, and the complexity of a modern filesystem tends to dwarf other subsystems. The people at the top explicitly don't and can't keep track of everything, there's a lot of stuff (e.g. testing) that they leave to other people - and I do fault them for that a bit; we badly need to get a bit more organized on test infrastructure. And I wouldn't say that filesystem people in general wear rose colored glasses; I would categorize Dave Chinner and Ted T'so more in the hard nosed realistic category, and myself as well. I'd say it's just the btrfs folks who've had that fault in the past, and I think Josef has had more than enough experience at this point to learn that lesson. reply starspangled 15 hours agorootparent> Linus absolutely does jump in on VFS level bugs when necessary (and major respect for that; he doesn't rest on laurels and he's always got his priorities in the right place) - but there's only so much he can keep track of, and the complexity of a modern filesystem tends to dwarf other subsystems. The point is he doesn't have to understand details of the code to see bug reports and code churn and bug fix commits and have a reasonable idea of whether it's stable enough for end users. I would trust him to make that call more than a \"filesystem guy\", in fact. > The people at the top explicitly don't and can't keep track of everything, there's a lot of stuff (e.g. testing) that they leave to other people - and I do fault them for that a bit; we badly need to get a bit more organized on test infrastructure. Absolutely not on the top nodes. Testing has to be distributed and pushed down to end nodes where development happens or even below otherwise it does not scale. > And I wouldn't say that filesystem people in general wear rose colored glasses; Perhaps that's what you see through your rose colored glasses? (sorry, just a cheeky dig). > I would categorize Dave Chinner and Ted T'so more in the hard nosed realistic category, and myself as well. I'd say it's just the btrfs folks who've had that fault in the past, and I think Josef has had more than enough experience at this point to learn that lesson. Dave Chinner for example would insist the file-of-zeroes problem of XFS is really not a problem. Not because he was flat wrong or consciously being biased for XFS I'm sure, but because according to the filesystem design and the system call interface and the big customers they talked to at SGI, it was operating completely as per specification. I'm not singling out filesystem developers or any one person or even software development specifically. All complex projects need advocates and input from outside stakeholders (users, other software, etc) for this exact reason, is that those deep in the guts of it usually don't understand all perspectives. reply koverstreet 15 hours agorootparent> The point is he doesn't have to understand details of the code to see bug reports and code churn and bug fix commits and have a reasonable idea of whether it's stable enough for end users. I would trust him to make that call more than a \"filesystem guy\", in fact. No, that's not enough, and I would not call that kind of slagging good communication to users. Seeing bugfixes go by doesn't tell you that much, and it definitely doesn't tell you which filesystem to recommend to users because other filesystems simply may not be fixing critical bugs. Based on (a great many) user reports that I've seen, I actually have every reason to believe that your data is much safer on bcachefs than btrfs. I'm not shouting about that while I still have hardening to do, and my goal isn't just to beat btrfs, it's to beat ext4 and xfs as well: but given what I see I have to view Linus's communications as irresponsible. > Absolutely not on the top nodes. Testing has to be distributed and pushed down to end nodes where development happens or even below otherwise it does not scale. No, our testing situation is crap, and we need leadership that says more than \"not my problem\". > Dave Chinner for example would insist the file-of-zeroes problem of XFS is really not a problem. Not because he was flat wrong or consciously being biased for XFS I'm sure, but because according to the filesystem design and the system call interface and the big customers they talked to at SGI, it was operating completely as per specification. Well, he had a point, and you don't want to be artificially injecting fsyncs because for applications that don't need them that gets really expensive. Fsync is really expensive, and it impacts the whole system. Now, it turned out there is a clever and more practical solution to this (which I stole from ext4), but you simply cannot expect any one person to know the perfect solution to every problem. By way of example, I was in an argument with Linus a month or so ago where he was talking about filesystems that \"don't need fsck\" (which is blatently impossible), and making \"2GB should be enough for anyone\" arguments. No one is right all the time, no one has all the answers - but if you go into a conversation assuming the domain experts aren't actually the experts, that's not a recipe for a productive conversation. reply starspangled 14 hours agorootparent> No, that's not enough, and I would not call that kind of slagging good communication to users. It is enough. Users need to be told when something is not stable or good enough. > Seeing bugfixes go by doesn't tell you that much, and it definitely doesn't tell you which filesystem to recommend to users because other filesystems simply may not be fixing critical bugs. Cherry picking what I wrote. Bugfixes, code churn, and bug reports from users. It certainly tells someone like Linus a great deal without ever reading a single line of code. > Based on (a great many) user reports that I've seen, I actually have every reason to believe that your data is much safer on bcachefs than btrfs. I'm not shouting about that while I still have hardening to do, and my goal isn't just to beat btrfs, it's to beat ext4 and xfs as well: but given what I see I have to view Linus's communications as irresponsible. Being risk adverse with my data, I think Linus's comment is a helpful and responsible one to balance other opinions. > No, our testing situation is crap, and we need leadership that says more than \"not my problem\". No. Testing is crap because developers and employers don't put enough time into testing. They know what has to be done, leadership has told them what has to be done, common sense says what has to be done. They refuse to do it. When code gets to a pull request for Linus it should have had enough testing (including integration testing via linux-next) that it is ready to be taken up by early user testers via Linus' tree. Distros and ISVs and IHVs and so on need to be testing there if not linux-next. > Well, he had a point, and you don't want to be artificially injecting fsyncs because for applications that don't need them that gets really expensive. Fsync is really expensive, and it impacts the whole system. No it was never about fsync, it was about data writes that extend a file hitting persistent storage before inode length metadata write does. By careful reading of posix it may be allowed, as a quality of implementation for actual users (aside from administrator-intensive high end file servers and databases etc from SGI), it is the wrong thing to do. ext3 for example solved it with \"ordered\" journal mode (not fsync). You can accept it is poor quality but decide you will do it anyway, but you can't just say it's not a problem because you language-lawyered POSIX and found out its okay, when you have application developers and users complaining about it. > By way of example, I was in an argument with Linus a month or so ago where he was talking about filesystems that \"don't need fsck\" (which is blatently impossible), and making \"2GB should be enough for anyone\" arguments. No one is right all the time, no one has all the answers - but if you go into a conversation assuming the domain experts aren't actually the experts, that's not a recipe for a productive conversation. I didn't see that so I can't really comment. It does not seem like it provides a counter example to what I wrote. I did not say Linus is never wrong. I have got into many flame wars with him so I would be the last to say he is always right. Domain experts are frequently wrong about their field of expertise too, especially in places where it interacts with things outside their field of expertise. reply koverstreet 14 hours agorootparent> I didn't see that so I can't really comment. It does not seem like it provides a counter example to what I wrote. I did not say Linus is never wrong. I have got into many flame wars with him so I would be the last to say he is always right. Domain experts are frequently wrong about their field of expertise too, especially in places where it interacts with things outside their field of expertise. You came in with an argument to authority, and now you're saying you disagree with that authority yourself, but you trust that authority more than domain experts? I don't think you've fully thought this through... Everyone believes what they read in the news, until they see it reporting on something they know about - and then they forget about it a week later and go back to trusting the news. reply JackSlateur 20 hours agorootparentprev(read my comment again) reply fragmede 21 hours agoparentprevhonestly I think btrfs isn't bloated enough for today's VM-enabled world. ext4 and xfs and hell, exfat haven't gone anywhere, and if those fulfill your needs, just use those. but if you need more advanced features that btrfs or zfs bring, those added features are quite welcome. imo, btrfs could use the benefits of being a cluster filesystem on top of everything it already does because having a VM be able to access a disk that is currently mounted by the host or another VM would useful. imagine if the disk exported to the VM could be mounted by another VM, either locally or remote simultaneously. arguably ceph fills this need, but having a btrfs-native solution for that would be useful. reply ChocolateGod 21 hours agorootparentRunning VMs (and database servers) on btrfs performs really bad so you have to disable CoW for them. Otherwise you'll get situations where your 100GB VM image will use over a TB of physical disk space. It's a shame really that this still isn't solved. reply jeroenhd 17 hours agorootparentCoW won't necessarily make the VM image bloated. In fact, as I've foolishly found out, BTRFS can be quite useful for deduplicating very similar VMs at the block level, at the cost of needing to re-allocate new disk space on writes. In my VM archive, six 50 GiB virtual machines took up 52 GiB rather than 300 GiB and that was quite impressive. Many downsides to CoW are also present with many common alternatives (i.e. thin LVM2 snapshots). Best to leave all of that off if you're using spinning rust or native compression features, though. reply ComputerGuru 18 hours agorootparentprevWhat’s the underlying issue? I used VMs with ZFS for storage for well over a decade with no issue. reply dmm 17 hours agorootparentZFS performs much better than btrfs with the many small writes that VMs produce. Why exactly is a great question. Maybe it has to do with the optimizations around the ZIL, the temporary area where sync writes are accumulated before they are written to the long-term spot. reply Joe_Cool 19 hours agorootparentprevI don't think thin provisioning btrfs makes a lot of sense. Before disabling CoW I'd rather use a different filesystem. Are you sure your TRIM is working and the VM disk image is compacting properly? It's working for me but not really great for fragmentation. reply irusensei 20 hours agorootparentprevChecksum self healing on ZFS and BTRFS saved my data from janky custom NAS setups more times that I can count. Compression is also nice but the thing I like most is the possibility of creating many partition-like sub volumes without needing to allocate or manage space. reply tronicdude 9 hours agoprevnext [2 more] [flagged] mort96 9 hours agoparent\"Endless errors\"? Are you talking about disk errors? Or are you referring to that one time long ago that it had a bug wrt a specific uncommon RAID setup? APFS and ZFS aren't very interesting to me honestly, because neither are, or can be, in the Linux kernel. I also don't understand why APFS is in the same conversation as ZFS and BTRFS. reply rurban 13 hours agoprev [–] Why would someone do that? NTFS is stable, faster than btrfs and has all the same features. reply WhyNotHugo 1 hour agoparentFor fun? To prove that it is possible? As a learning activity? There are millions of reasons to write software other than \"faster\" or \"more features\". I can imagine this being convenient (albeit risky) when migrating Windows to Linux if you really can't afford a spare disk to backup all your data. reply rnd0 11 hours agoparentprevThe only reason I can think of is so that they can use the same FS in both windows and linux -but with ntfs, they already can. Mind you, with openzfs (https://openzfsonwindows.org/) you get windows (flakey), freebsd, netbsd and linux but -as I said; I'm not sure zfs is super reliable on windows at this point. Mind you, I just stick with ntfs -linux can see it, windows can see it and if there's extra features btrfs provides they're not ones I am missing. reply ComputerGuru 1 hour agorootparentI’m a die-hard ZFS fan and heavy user since the Solaris days (and counting) but I believe the WinBtrfs project is in better (more useable) shape than the OpenZFS for Windows project. reply mistaken 9 hours agorootparentprevWith ntfs you have to create a separate partition though. With btrfs you could create a subvolume and just have one big partition for both linux and windows. reply paines 5 hours agoparentprev [–] what ?!?! NTFS has no case sensitivity no compression. And I guess a couple of more things I do not want to miss. reply ComputerGuru 1 hour agorootparentNTFS has per-folder case sensitivity flag. You could set it online at anytime prior to Windows 11, but as of 11 you can now only change it on an empty folder (probably due to latent bugs they didn’t want to fix). NTFS had mediocre compression support from the very start that could be enabled on a volume or directory basis, but gained modern LZ-based compression (that could be extended to whatever algorithm you wanted) in Windows 10, but it’s unfortunately a per-file process that must be done post-write. reply cesarb 4 hours agorootparentprev> what ?!?! NTFS has no case sensitivity no compression. As the sibling comment mentioned, NTFS does have a case-sensitive mode, for instance for the POSIX subsystem (which no longer exists, but it existed back when NTFS was new); I think it's also used for WSL1. And NTFS does have per-file compression, I've used it myself back in the early 2000s (as it was a good way to free a bit of space on the small disks from back then); there was even a setting you could enable on Windows Explorer which made compressed files in its listing blue-colored. reply rurban 5 hours agorootparentprev [–] NTFS does have case-sensitivity, just nobody dares to activate it. Compression is big, but I thought I've read winbtrfs neither reply inetknght 3 hours agorootparent [–] I activated it back in mid 2010 or so. I had the most amazing pikachuface when random things stopped working because it could no longer find that file it wanted to load with an all-lowercase-string even though the project builds it with CapitalCase. Sigh... reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Ntfs2btrfs is a tool designed to convert NTFS file systems to Btrfs in-place, preserving the original image as a reflink copy for potential space recovery.",
      "The tool is available for both Windows and Linux, with specific installation instructions and dependencies for each platform, and it includes features like compression support and rollback to NTFS.",
      "Limitations include the lack of support for older extended attributes, large alternate data streams (ADS), and encrypted files, with specific conditions required for booting Windows from a Btrfs file system."
    ],
    "commentSummary": [
      "Ntfs2btrfs is a tool that enables in-place conversion from NTFS (New Technology File System) to Btrfs (B-tree File System), sparking discussions on its utility and timing for large data transfers.",
      "Users share mixed experiences with Btrfs, appreciating features like snapshots but recalling past stability issues, while also discussing other filesystems like ZFS and bcachefs.",
      "The conversation underscores the complexities and risks of filesystem management, with some users advocating for backups and fresh formats over conversion tools."
    ],
    "points": 367,
    "commentCount": 133,
    "retryCount": 0,
    "time": 1732999808
  },
  {
    "id": 42283933,
    "title": "AMD Disables Zen 4's Loop Buffer",
    "originLink": "https://chipsandcheese.com/p/amd-disables-zen-4s-loop-buffer",
    "originBody": "Share this post Chips and Cheese AMD Disables Zen 4's Loop Buffer Copy link Facebook Email Notes More AMD Disables Zen 4's Loop Buffer Chester Lam Nov 30, 2024 16 Share this post Chips and Cheese AMD Disables Zen 4's Loop Buffer Copy link Facebook Email Notes More 3 Share A loop buffer sits at a CPU's frontend, where it holds a small number of previously fetched instructions. Small loops can be contained within the loop buffer, after which they can be executed with some frontend stages shut off. That saves power, and can improve performance by bypassing any limitations present in prior frontend stages. It's an old but popular technique that has seen use by Intel, Arm, and AMD cores. Arm's Cortex A15 contains a 32 entry loop buffer As far as I know, Zen 4 is the only high performance AMD core with a loop buffer. Zen 4's Processor Programming Reference mentions it as a micro-op dispatch source, alongside the op cache and decoder. Experimenting with performance counters suggests the loop buffer has 144 entries when the core is running on one thread, and is statically partitioned to give each thread 72 entries when two SMT threads are active. Calls and returns within a loop will prevent it from being captured by Zen 4's loop buffer. AMD's Zen 4 optimization guide makes no mention of the loop buffer and only suggests keeping hot code regions within the op cache's capacity. Zen 4's frontend can dispatch micro-ops from three sources From side discussions with AMD employees at Hot Chips 2024, the loop buffer was primarily a power optimization. I never heard AMD discuss the loop buffer elsewhere. That's likely because the loop buffer isn't an important feature from a performance perspective. The op cache can already deliver more bandwidth than the core's downstream rename/allocate stage can consume. After I updated my ASRock B650 PG Lightning to BIOS version 3.10, hardware performance monitoring indicated the frontend no longer dispatched any micro-ops from the loop buffer. Reverting to BIOS version 1.21 showed the loop buffer was active again. AMD must have disabled the loop buffer somewhere between BIOS 1.21 (AGESA version 1.0.0.6) and BIOS 3.10 (AGESA version 1.2.0.2a). They did so without any announcement or fanfare. SPEC CPU2017: Looking for Differences SPEC CPU2017 scores suggest no notable difference with the loop buffer on and off. Total scores for the integer and floating point suites differ by less than 1%. SMT performance gains are also not affected. None of that comes as a surprise. Again, the op cache provides more than enough bandwidth to feed the renamer. Moreover, performance counters indicate the loop buffer only delivers a small minority of micro-ops, even when enabled. Zen 4 largely feeds itself using the op cache. 523.xalanbmk sees a significant minority of its instruction stream covered by the loop buffer. However, even the subscore changes for that test land within margin of error. It scored 9.48 on the new BIOS, versus 9.44 before. Much of the same applies across SPEC CPU2017's floating point suite. 544.nab has nearly a quarter of its micro-ops delivered from the loop buffer, yet its score actually increased by 1.7% with the loop buffer off (11.7 on the new BIOS, 11.5 before). That could be run to run variance, but overall it's clear turning off the loop buffer didn't cause performance loss. Going over the same performance counter data on the new BIOS shows Zen 4's op cache picking up the slack. The op cache handles an even larger majority of the instruction stream. SPEC CPU2017's floating point suite sees a similar pattern. 507.cactuBSSN sees op cache coverage dip somewhat, causing the decoders to deliver about a quarter of total micro-ops. I'm not sure what's going on with that, but it's worth remembering performance counters typically give a general idea of what's going on rather than being 100% accurate. Ops dispatched from the frontend is also a speculative event, and can be influenced for example by incorrectly fetched instructions past a mispredicted branch. Other cases like 544.nab or 508.namd behave as you'd expect. The loop buffer used to cover 20-something percent of the incoming micro-op stream, but now the op cache does nearly everything. In summary, disabling the loop buffer causes Zen 4 to move from running the vast majority of code from the op cache, to running the overwhelming majority of code from the op cache. It's not very interesting from a performance point of view. Count Masking But the loop buffer's primary goal is not to increase performance. Instead, it tries to let the core opportunistically shut off much of the frontend, including the op cache. AMD (and Intel) have excellent hardware performance monitoring facilities that include count masking capabilities. Count mask field as described in Zen 4's Processor Programming Reference That is, a performance counter can be programmed to increment by one when event count exceeds a threshold. Setting the threshold to 1 lets me count how many cycles each micro-op delivery source was actively supplying micro-ops. That in turn gives me an estimate of how often the core can power off its frontend with the loop buffer enabled. How often each op delivery source is active aligns pretty well with the percentage of micro-ops they deliver. In addition, this data also shows the frontend can spend a good percentage of cycles delivering nothing at all. 502.gcc and 520.omnetpp for example are heavily bound by backend memory latency. Often the out-of-order execution engine can't keep enough instructions in flight to hide that latency. That in turn means the frontend has to idle, because it can't send any more instructions down to the backend until some get cleared out. In SPEC CPU2017's floating point suite, 544.nab and 508.namd see the loop buffer active for a decent percentage of core cycles. 508.namd may be particularly interesting because it's a high IPC workload (3.64 IPC), which means the frontend has to sustain high throughput. 508.namd is also friendly to the loop buffer, so it's a good opportunity to save power by shutting off the op cache. With the loop buffer disabled, Zen 4's op cache feeds the core over more cycles. But in most tests, the difference isn't that big. 523.xalanbmk is an exception. There, the op cache has to be active for an extra 12% of core cycles without the loop buffer. Other workloads see far less difference. 548.exchange2 is a high IPC workload in the integer suite, averaging an incredible 4.31 IPC. The frontend has to deliver a lot of throughput, but the loop buffer is basically absent even when enabled. Evidently 548.exchange2 doesn't spend a lot of time in tiny loops. The op cache is busy for over 85% of core cycles even with the loop buffer enabled. Another way of looking at it, showing the difference in how often either the op cache or decoder were delivering ops, with the loop buffer enabled or disabled 508.namd was an interesting example from the floating point suite. Indeed, disabling the loop buffer leads to the op cache being active over 75.1% of core cycles, compared to 56.67% with the loop buffer enabled. But 508.namd is an exception in my opinion. Differences are minimal elsewhere. A 144 entry loop buffer just isn't big enough to contain much of the instruction stream. The loop buffer only has potential to make a notable impact when a program spends much of its runtime in tiny loops, and is also not bound by backend throughput or latency. From this look at SPEC CPU2017 with frontend performance monitoring data, I think disabling the loop buffer is inconsequential. Even from a power savings perspective, the loop buffer only lets the core shut off the op cache over a tiny percentage of cycles. Cyberpunk 2077 Cyberpunk 2077 is a game where you can sneak and hold tab while looking at enemies. It features a built-in benchmark, letting me conveniently check on whether disabling the loop buffer might impact gaming performance. Why did I mention the op cache was enabled? Surely I wouldn't disable that big beautiful op cache for some other experiment. Right? Because I expect negligible performance differences with the loop buffer disabled, I ran the benchmark with an unusual setup to maximize consistency. I disabled Core Performance Boost on the Ryzen 9 7950X3D by setting bit 25 of the Hardware Configuration register (MSR 0xC0010015). That limits all cores to 4.2 GHz. I also capped my RX 6900 XT to 2 GHz. For benchmark settings, I'm using the medium preset at 1080P with no upscaling. Disabling the loop buffer basically doesn't affect performance with the game pinned to the VCache die. Strangely, the game sees a 5% performance loss with the loop buffer disabled when pinned to the non-VCache die. I have no explanation for this, and I've re-run the benchmark half a dozen times. Cyberpunk 2077 is unexpectedly friendly to the loop buffer, which covers about 22% of the instruction stream on average. Disabling the loop buffer causes the op cache to deliver 82% of micro-ops, up from 62% before. There's a lot of action in Cyberpunk 2077, but most of it doesn't happen at the CPU's frontend. Disabling the loop buffer of course doesn't change that. But because the loop buffer covers a significant minority of the instruction stream, turning it off does mean the op cache works harder. Again, it's not a big difference. With an average IPC of 0.89 with the loop buffer disabled, or 1.02 with the loop buffer enabled, Cyberpunk 2077 is not a high IPC workload. That means frontend bandwidth isn't a big consideration. Perhaps the game is more backend bound, or bound by branch predictor delays. Still, the Cyberpunk 2077 data bothers me. Performance counters also indicate higher average IPC with the loop buffer enabled when the game is running on the VCache die. Specifically, it averages 1.25 IPC with the loop buffer on, and 1.07 IPC with the loop buffer disabled. And, there is a tiny performance dip on the new BIOS. Perhaps I'm pushing closer to a GPU-side bottleneck at 155 FPS. But I've already spent enough free time on what I thought would be a quick article. Perhaps some more mainstream tech outlets will figure out AMD disabled the loop buffer at some point, and do testing that I personally lack the time and resources to carry out. Attempt at Checking Power Draw I also tried to look at Zen 4's core power counters to see whether running from the loop buffer improved power efficiency. To do this, I had to modify my instruction bandwidth benchmark to not use calls or returns in the test section. Apparently, calls or returns cause Zen 4 to not use the loop buffer. This should let me focus on core power, I think. I also pinned the test to one core and read the Core Energy Status MSR before and after jumping to my test array, letting me calculate average power draw over the test duration. For consistency, I disabled Core Performance Boost because power readings would vary wildly with boost active. Testing with 4B NOPs. Core averages 11-12 IPC when fetching from the op cache or loop buffer thanks to instruction fusion. It averages 4 IPC when using the decoders Results make no sense. On the old BIOS, the Core Energy Status MSR tells me the core averaged 6W of power draw when fetching NOPs from the op cache, and much lower power when doing the same from the loop buffer. Next, I increased the test array size until performance counters showed op cache coverage dropping to under 1%. By that time, the array size had gone well into L2 capacity (128 KB). But even though exercising the decoders and L2 fetch path should increase power draw, the Core Energy Status MSR showed just 1.5W of average core power. Updating to the new BIOS gave 1.68W of average core power when testing the op cache, and nearly the same power when feeding the decoders mostly from L2. That means the core is achieving better efficiency when running code from the op cache, and makes sense. Of course, I can't test the loop buffer on the new BIOS because it's disabled. To make things even more confusing, AMD's power monitoring facilities may be modeling power draw instead of measuring it1. There's a distinct possibility AMD modeled the power draw wrong, or changed the power modeling methodology between the two BIOS versions. I don't have power measuring hardware to follow up on this. I feel like I don't understand the power draw situation any more than when I started, and a few hours have gone to waste. Final Words I don't know why AMD disabled Zen 4's loop buffer. Sometimes CPU features get disabled because there's a hardware bug. Intel's Skylake saw its loop buffer (LSD) disabled due to a bug related to partial register access in short loops with both SMT threads active. Zen 4 is AMD's first attempt at putting a loop buffer into a high performance CPU. Validation is always difficult, especially when implementing a feature for the first time. It's not crazy to imagine that AMD internally discovered a bug that no one else hit, and decided to turn off the loop buffer out of an abundance of caution. I can't think of any other reason AMD would mess with Zen 4's frontend this far into the core's lifecycle. Turning off the loop buffer should have little to no impact on performance because the op cache has more than enough bandwidth to feed the subsequent rename/allocate stage. Impact on power consumption is an unknown factor, but I suspect it's also minor, and may be very difficult to evaluate even when using expensive hardware to measure CPU power draw at the 12V EPS connector. The only place AMD ever documented the loop buffer AMD's move to disable Zen 4's loop buffer is interesting, but should go largely unnoticed. AMD never advertised or documented the feature beyond dropping a line in the Processor Programming Reference. It's a clear contrast to Intel, which often documents its loop buffer and encourages developers to optimize their code to take advantage of it. Advice from Intel's software optimization guide, suggesting developers take advantage of Ice Lake's LSD (loop stream detector, or loop buffer). Note similar limitations to AMD's loop buffer, like no CALL/RET Combine that with what looks like minimal impact on performance, and I doubt anyone will ever know that AMD turned the loop buffer off. It was a limited feature in the first place, with low capacity and restrictions like no function calls that prevent it from being as useful as an op cache. Perhaps the best way of looking at Zen 4's loop buffer is that it signals the company has engineering bandwidth to go try things. Maybe it didn't go anywhere this time. But letting engineers experiment with a low risk, low impact feature is a great way to build confidence. I look forward to seeing more of that confidence in the future. If you like our articles and journalism, and you want to support us in our endeavors, then consider heading over to our Patreon or our PayPal if you want to toss a few bucks our way. If you would like to talk with the Chips and Cheese staff and the people behind the scenes, then consider joining our Discord. References Robert Schöne et al, Energy Efficiency Aspects of the AMD Zen 2 Architecture Appendix Since AMD never offered optimization advice related to the loop buffer, I'll do it. On Zen 4 running an old BIOS version, consider sizing loops to have less than 144 micro-ops, or half that if threads share a physical core. Consider inlining a function called within a small loop to avoid CALL/RET instructions. Do this, and your reward will most likely be absolutely nothing. Have fun. Restrictions like no CALL/RET could indicate Zen 4 shuts off certain parts of the branch predictor in addition to the op cache and decoder. That could add to power Subscribe to Chips and Cheese Launched a year ago The Devil is in the Details! Deep dives into computer hardware and software and the wider industry... Subscribe Error 16 Share this post Chips and Cheese AMD Disables Zen 4's Loop Buffer Copy link Facebook Email Notes More 3 Share Previous",
    "commentLink": "https://news.ycombinator.com/item?id=42283933",
    "commentBody": "AMD Disables Zen 4's Loop Buffer (chipsandcheese.com)271 points by luyu_wu 22 hours agohidepastfavorite105 comments shantara 19 hours agoThis is a wild guess, but could this feature be disabled in an attempt at preventing some publicly undisclosed hardware vulnerability? reply baq 2 hours agoparentIndeed, it might be the case that there’s more than that disabled, since numbers are somewhat surprising: > Still, the Cyberpunk 2077 data bothers me. Performance counters also indicate higher average IPC with the loop buffer enabled when the game is running on the VCache die. Specifically, it averages 1.25 IPC with the loop buffer on, and 1.07 IPC with the loop buffer disabled. And, there is a tiny performance dip on the new BIOS. Smells of microcode mitigations if you ask me, but naturally let’s wait for the CVE. reply BartjeD 10 hours agoparentprevQuitely disabling it is also a big risk. Because you're signalling that in all probablity you were aware of the severity of the issue; Enough so that you took steps to patch it. If you don't disclose the vulnerability then affected parties cannot start taking countermeasures, except out of sheer paranoia. Disclosing a vulnerability is a way shift liability onto the end user. You didn't update? Then don't complain. Only rarely do disclosures lead to product liability. I don't remember this (liability) happening with Meltdown and Spectre either. So wouldn't assume this is AMD being secretive. reply immibis 3 hours agorootparentThe countermeasure is to disable the loop buffer. Everyone who wants to protect themselves from the unknown vulnerability should disable the loop buffer. Once everyone's done that or had a reasonable opportunity to do that, it can be safely published. reply jdiff 2 hours agorootparentThere's no real impetus except paranoia if the change is unannounced. You don't have to detail the vulnerability, just inform people that somewhere, one exists, and that this is in fact a countermeasure. Without doing that, you don't shift liability, you don't actually get people out of harm's way, you don't really benefit at all. reply wtallis 3 hours agorootparentprevPlease don't post duplicate comments like this. Your first comment (https://news.ycombinator.com/item?id=42287118) was fine but spamming a thread with copy-and-pasted comments just hurts the signal to noise ratio. reply alexnewman 33 minutes agorootparentIm confused would you be ok if he's addressing the same point in the forum with a slightly different sentence? reply throw_away_x1y2 19 hours agoparentprevBingo. I can't say more. :( reply pdimitar 18 hours agorootparentHave we learned nothing from Spectre and Meltdown?... :( reply rincebrain 17 hours agorootparentThe problem is that we're more or less stuck with this class of problem unless we end up with something that looks like a Xeon Phi without shared resources and run calculations on many, many truly independent cores, or we accept that the worst and best case performance cases are identical (which I don't foresee anyone really agreeing to). Or, framed differently, if Intel or AMD announced a new gamer CPU tomorrow that was 3x faster in most games but utterly unsafe against all Meltdown/Spectre-class vulns, how fast do you think they'd sell out? reply thechao 17 hours agorootparentLarabee was fun to program, but I think it'd have an even worse time hardening memory sideband effects: the barrel processor (which was necessary to have anything like reasonable performance) was humorously easy to use for cross-process exfiltration. Like... it was so easy, we actually used it as an IPC mechanism. reply wheybags 17 hours agorootparent> it was so easy, we actually used it as an IPC mechanism. Can you elaborate on that? It sounds interesting reply thechao 15 hours agorootparentNow you’re asking me technical details from more than a decade ago. My recollection is that you could map one of the caches between cores — there were uncached-write-through instructions. By reverse engineering the cache’s hash, you could write to a specific cache-line; the uc-write would push it up into the correct line and the “other core” could snoop that line from its side with a lazy read-and-clear. The whole thing was janky-AF, but way the hell faster than sending a message around the ring. (My recollection was that the three interlocking rings could make the longest-range message take hundreds of cycles.) reply rincebrain 17 hours agorootparentprevSure, absolutely, there's large numbers of additional classes of side effects you would need to harden against if you wanted to eliminate everything, I was mostly thinking specifically of something with an enormous number of cores without the 4-way SMT as a high-level description. I was always morbidly curious about programming those, but never to the point of actually buying one, and I always had more things to do in the day than time in past life when we had a few of the cards in my office. reply jorvi 3 hours agorootparentprevAlso, a good chunk of these vulnerabilities (Retbleed, Downfall, Rowhammer, there's probably a few I'm forgetting) are either theoretical, lab-only or spear exploits that require a lot of setup. And then the leaking info from something like Retbleed mostly applies to shared machines like in cloud infrastructure. Which makes it kind of terrible that the kernel has these mitigations turned on by default, stealing somewhere in the neighborhood of 20-60% of performance on older gen hardware, just because the kernel has to roll with \"one size fits all\" defaults. reply nine_k 54 minutes agorootparentIf you know what you're doing, you do something like this: https://gist.github.com/jfeilbach/f06bb8408626383a083f68276f... and make Linux fast again (c). If you don't know what kernel parameters are and what do they affect, it's likely safer to go with all the mitigations enabled by default :-| reply DSingularity 37 minutes agorootparentprevI don’t think you are thinking of this right. One bit of leakage makes it half as hard to break encryption via brute force. It’s a serious problem. The defaults are justified. I think things will only shift once we have systems they ship with fully sandboxes that are minimally optimized and fully isolated. Until then we are forced to assume the worst. reply lukan 7 hours agorootparentprev\"if Intel or AMD announced a new gamer CPU tomorrow that was 3x faster in most games but utterly unsafe against all Meltdown/Spectre-class vulns, how fast do you think they'd sell out\" Well, many people have gaming computers, they won't use for anything serious. So I would also buy it. And in restricted gaming consoles, I suppose the risk is not too high? reply formerly_proven 5 hours agorootparentConsoles are hardened very well to prevent homebrew, cracking, cheating etc. reply dcow 5 hours agorootparentBut this class of vuln is about data leaking between users in a multi-user system. reply alexvitkov 1 hour agorootparentThey're a pain in the ass all around. Spectre allowed you to read everything paged in (including kernel memory) from JS in the browser. To mitigate it browsers did a bunch of hacks, including nerfing precision on all timer APIs and disabling shared memory, because you need an accurate timer for the exploit - to this day performance.now() rounds to 1MS on firefox and 0.1MS on Chrome. This 1MS rounding funnily is a headache for me right as we speak. On a say 240Hz monitor, for video games you need to render a frame every ~4.16ms -- 1ms precision is not enough for accurate ticker -- even if you render your frames on time, the result can't be perfectly smooth as the browser doesn't give an accurate enough timer by which to advance your physics every frame. reply wongarsu 4 hours agorootparentprevIsn't it rather about data leaks between any two processes? Whether those two processes belong to different users is a detail of the threat model and the OS's security model. In a console it could well be about data leaks between a game with code-injection vulnerability and the OS or DRM system. reply sim7c00 6 hours agorootparentprevyou mean those consoles that can attack the rest of your devices and your neighbours via its wireless chips? reply ggu7hgfk8j 6 hours agorootparentSpeculation attacks enables code running on the machine to access data it shouldn't. I don't see how that relates to your scenario. reply magicalhippo 8 hours agorootparentprevWe already have heterogeneous cores these days, with E and P, and we have a ton of them as they take little space on the die relative to cache. The solution, it seems to me, is to have most cores go brrrrrr and a few that are secure. Given that we have effectively two browser platforms (Chromium and Firefox) and two operating systems to contend with (Linux and Windows), it seems entirely tractable to get the security sensitive threads scheduled to the \"S cores\". reply ggu7hgfk8j 6 hours agorootparentThe main security boundary a modern computer upholds is web vs everything else, including protecting one webpage from another. So I think it should be the javascript that should run on these hypothetical cores. Though perhaps a few other operations might choose to use them as well. reply nine_k 40 minutes agorootparentAlso all the TLS, SSH, Wireguard and other encryption, anything with long-persisted secret information. Everything else, even secret (like displayed OTP codes) is likely too fleeting for a snooping attack to be able to find and exfiltrate it, even if an exfiltration channel remains. Until a better exfiltration method is found, of course :-( I think we're headed towards the future of many highly insulated computing nodes that share little if anything. Maybe they'd have a faster way to communicate, e.g. by remapping fast cache-like memory between cores, but that memory would never be uncontrollably shared the way cache lines are now. reply astrange 6 hours agorootparentprevThat's a secure enclave aka secure element aka TPM. Once you start wanting security you usually think up enough other features (voltage glitching prevention, memory encryption) that it's worth moving it off the CPU. reply Dylan16807 3 hours agorootparentThat's a wildly different type of security. I just want to sandbox some code, not treat the entire world as hostile. reply nottorp 5 hours agorootparentprev> how fast do you think they'd sell out? 10-20 min, depending on how many they make :) reply sweetjuly 18 hours agorootparentprevI imagine this is more of a functional issue. i.e., the loop buffer caused corruption of the instruction stream under some weird specific circumstances. Spectre and Meltdown are not functional issues but rather just side channel issues. This should be fun, however, for someone with enough time to chase down and try and find the bug. Depending on the consequences of the bug and the conditions under which it hits, maybe you could even write an exploit (either going from JavaScript to the browser or from user mode to the kernel) with it :) Though, I strongly suspect that reverse engineering and weaponizing the bug without any insider knowledge will be exceedingly difficult. And, anyways, there's also a decent chance this issue just leads to a hang/livelock/MCE which would make it pointless to exploit. reply RobotToaster 3 hours agorootparentprevWe should have learnt from the fdiv bug[0] that processor manufacturers need to be mandated to recall faulty hardware. [0] https://en.wikipedia.org/wiki/Pentium_FDIV_bug reply gpderetta 18 hours agorootparentprevComplex systems are complex? reply pdimitar 18 hours agorootparentSadly you're right. And obviously we're not about to give up on high IPC. I get it and I'm not judging -- it's just a bit saddening. reply StressedDev 18 hours agorootparentprevA lot has been learned. Unfortunately, people still make mistakes and hardware will continue to have security vulnerabilities. reply PittleyDunkin 4 hours agorootparentprevI'm still not convinced most of the computers in my home need to care about leaking data this way. I'm open to being persuaded, though. reply pdimitar 4 hours agorootparentI am not convinced either but I am willing to bet some software is adversarial and will try to exfiltrate data. F.ex. many people look suspiciously at Zoom and Chrome. So as long as stuff is not perfectly isolated from each other then there's always a room for a bad actor to snoop on stuff. reply api 1 hour agorootparentprevFor most of these vulnerabilities the risk is low, but keep in mind that your web browser runs random untrusted code from all over the Internet in a VM with a JIT compiler. This means you can't rule out the possibility that someone will figure out a way to exploit this over the web reliably, which would be catastrophic. \"Attacks only get better.\" reply Am4TIfIsER0ppos 17 hours agorootparentprevWe learned that processor manufacturers love \"bugs\" that get solved by making them or your code slower giving you incentive to buy a newer one to restore the performance. reply shepherdjerred 16 hours agorootparentThis seems unnecessarily cynical. Are you saying Intel/AMD are intentionally crippling CPUs? reply bobmcnamara 2 hours agorootparentI'm not saying Intel intentionally limited CPUs, just that they have intentionally limited a lot of things and lied about it in the past. https://www.ftc.gov/news-events/news/press-releases/2010/08/... reply tedunangst 16 hours agorootparentprevI was told the lesson is to avoid Intel and only buy AMD because they don't make mistakes. reply UberFly 16 hours agorootparentNo one said to buy AMD because they don't make mistakes. AMD just currently makes a better product overall. reply Dylan16807 3 hours agorootparentprevI do not think you are accurately recounting what people said. reply bell-cot 18 hours agoparentprevThe Article more-or-less speculates that: > Zen 4 is AMD's first attempt at putting a loop buffer into a high performance CPU. Validation is always difficult, especially when implementing a feature for the first time. It's not crazy to imagine that AMD internally discovered a bug that no one else hit, and decided to turn off the loop buffer out of an abundance of caution. I can't think of any other reason AMD would mess with Zen 4's frontend this far into the core's lifecycle. reply bhouston 18 hours agoparentprevYeah, my first thoughts too. reply londons_explore 20 hours agoprevThe article seems to suggest that the loop buffer provides no performance benefit and no power benefit. If so, it might be a classic case of \"Team of engineers spent months working on new shiny feature which turned out to not actually have any benefit, but was shipped anyway, possibly so someone could save face\". I see this in software teams when someone suggests it's time to rewrite the codebase to get rid of legacy bloat and increase performance. Yet, when the project is done, there are more lines of code and performance is worse. In both cases, the project shouldn't have shipped. reply akira2501 20 hours agoparent> but was shipped anyway, possibly so someone could save face Was shipped anyway because it can be disabled with a firmware update and because drastically altering physical hardware layouts mid design was likely to have worse impacts. reply eek2121 17 hours agorootparentWell that and changing a chip can take years due to redesigning, putting through validation, RTM, and time to create. Building chips is a multiyear process and most folks don’t understand this. reply usrusr 9 hours agorootparentprevWhat you describe would be shipped physically but disabled, and that certainly happens a lot. For exactly those reasons. What GP described was shipped not only physically present but also not even disabled, because politics. That would be a very different thing. reply readyplayernull 18 hours agorootparentprevThat bathroom with a door to the kitchen. reply adgjlsfhk1 20 hours agoparentprev> but was shipped anyway, possibly so someone could save face no. once the core has it and you realize it doesn't help much, it absolutely is a risk to remove it. reply glzone1 20 hours agorootparentNo kidding. I was adjacent to a tape out w some last minute tweaks - ugh. The problem is the current cycle time is very slow and costly and u spend as much time validating things as you do designing. It’s not programming. reply magicalhippo 19 hours agorootparentOnce interviewed at a place which made sensors that was used a lot in the oil industry. Once you put a sensor on the bottom of the ocean 100+ meters (300+ feet) down, they're not getting serviced any time soon. They showed me the facilities, and the vast majority was taken up by testing and validation rigs. The sensors would go through many stages, taking several weeks. The final stage had an adjacent room with a viewing window and a nice couch, so a representative for the client could watch the final tests before bringing the sensors back. Quite the opposite to the \"just publish a patch\" mentality that's so prevalent these days. reply hajile 19 hours agorootparentprevIf you work on a critical piece of software (especially one you can't update later), you absolutely can spend way more time validating than you do writing code. The ease of pushing updates encourages lazy coding. reply chefandy 18 hours agorootparent> The ease of pushing updates encourages lazy coding. Certainly in some cases, but in others, it just shifts the economics: Obviously, fault tolerance can be laborious and time consuming, and that time and labor is taken from something else. When the natures of your dev and distribution pipelines render faults less disruptive, and you have a good foundational codebase and code review process that pay attention to security and core stability, quickly creating 3 working features can be much, much more valuable than making sure 1 working feature will never ever generate a support ticket. reply oefrha 11 hours agorootparentprev> It’s not programming. Even for software it’s often risky to remove code once it’s in there. Lots of software products are shipped with tons of unused code and assets because no one’s got time to validate nothing’s gonna go wrong when you remove them. Check out some game teardowns, they often have dead assets from years ago, sometimes even completely unrelated things from the studio’s past projects. Of course it’s 100x worse for hardware projects. reply gtirloni 5 hours agorootparentAnd that's another reason for tackling technical debt early on because once it compounds, no one is ever touching that thing. reply sweetjuly 17 hours agoparentprevThe article also mentions they had trouble measuring power usage in general so we can't necessarily (and, really, shouldn't) conclude that it has no impact whatsoever. I highly doubt that AMD's engineering teams are so unprincipled as to allow people to add HW features with no value (why would you dedicate area and power to a feature which doesn't do anything?), and so I'm inclined to give them the benefit of the doubt here and assume that Chips 'n Cheese simply couldn't measure the impact. reply clamchowder 11 hours agorootparentNote - I saw the article through from start to finish. For power measurements I modified my memory bandwidth test to read AMD's core energy status MSR, and modified the instruction bandwidth testing part to create a loop within the test array. (https://github.com/clamchowder/Microbenchmarks/commit/6942ab...) Remember most of the technical analysis on Chips and Cheese is a one person effort, and I simply don't have infinite free time or equipment to dig deeper into power. That's why I wrote \"Perhaps some more mainstream tech outlets will figure out AMD disabled the loop buffer at some point, and do testing that I personally lack the time and resources to carry out.\" reply weinzierl 7 hours agoparentprev\"The article seems to suggest that the loop buffer provides no performance benefit and no power benefit.\" It tests the performance benefit hypothesis in different scenarios and does not find evidence that supports it. It makes one best effort attempt to test the power benefit hypothesis and concludes it with: \"Results make no sense.\" I think the real take-away is that performance measurements without considering power tell only half the story. We came a long way when it comes to the performance measurement half but power measurement is still hard. We should work on that. reply iforgotpassword 11 hours agoparentprevWell the other possibility is that the power benchmarks are accurate: the buffer did save power, but then they figured out an even better optimization on the microcodes level that would make the regular path save even more power, so the buffer actually became a power hog. reply EVa5I7bHFq9mnYK 11 hours agoparentprev>> when the project is done, there are more lines of code and performance is worse There is an added benefit though - that the new programmers now are fluent in the code base. That benefit might be worth more than LOCs or performance. reply 01100011 11 hours agoparentprevWorking at.. a very popular HW company.. I'll say that we(the SW folks) are currently obsessed with 'doing something' even if the thing we're doing hasn't fully been proven to have benefits outside of some narrow use cases or targeted benchmarks. It's very frustrating, but no one wants to put the time in to do the research up front. It's easier to just move forward with a new project because upper management stays happy and doesn't ask questions. reply usrusr 8 hours agorootparentIs it that expectation of major updates coming in at a fixed cycle? Not only expected by upper management but also by end users? That's a difficult trap to get out of. I wonder if that will be the key benefit of Google's switch to two \"major\" Android releases each year: it will get people used to nothing newsworthy happening within a version increment. And I also wonder if that's intentional, and my guess is not the tiniest bit. reply markus_zhang 6 hours agorootparentprevDo you have new software managers/directors who are encouraging such behavior? From my experience new leaders tend to lean on this tactics to grab power. reply ksaj 18 hours agoparentprev\"the project shouldn't have shipped.\" Tell that to the share holders. As a public company, they can very quickly lose enormous amounts of money by being behind or below on just about anything. reply saagarjha 14 hours agoparentprevOnly on Hacker News will you get CPU validation fanfiction. reply Loic 10 hours agoprevFor me the most interesting paragraph in the article is: > Perhaps the best way of looking at Zen 4's loop buffer is that it signals the company has engineering bandwidth to go try things. Maybe it didn't go anywhere this time. But letting engineers experiment with a low risk, low impact feature is a great way to build confidence. I look forward to seeing more of that confidence in the future. reply eqvinox 20 hours agoprev> Strangely, the game sees a 5% performance loss with the loop buffer disabled when pinned to the non-VCache die. I have no explanation for this, […] With more detailed power measurements, it could be possible to determine if this is thermal/power budget related? It does sound like the feature was intended to conserve power… reply eek2121 16 hours agoparentHe didn’t provide enough detail here. The second CCD on a Ryzen chip is not as well binned as the first one even on. non-X3D chips. Also, EVERY chip is different. Most of the cores on CCD0 of my non-X3D chip hit 5.6-5.75ghz. CCD 1 has cores topping out at 5.4-5.5ghz. V-Cache chips for Zen 4 have a huge clock penalty, however the Cache more than makes up for it. Did he test CCD1 on the same chip with both the feature disabled and enabled? Did he attempt to isolate other changes like security fixes as well? He admitted “no” in his article. The only proper way to test would be to find a way to disable the feature on a bios that has it enabled and test both scenarios across the same chip, and even then the result may still not be accurate due to other possible branch conditions. A full performance profile could bring accuracy, but I suspect only an AMD engineer could do that… reply clamchowder 12 hours agorootparentYes, I tested on CCD1 (the non-vcache CCD) on both BIOS versions. reply ryao 11 hours agoparentprevHe mentioned that it was disabled somewhere between the two UEFI versions he tested. Presumably there are other changes included, so his measurements are not strict A/B testing. reply eek2121 20 hours agoprevIt sounds to me like it was too small to make any real difference except in very specific scenarios and a larger one would have been too expensive to implement compared to the benefit. That being said, some workloads will see a small regression, however AMD has made some small performance improvements since launch. They should have just made it a BIOS option for Zen 4. The fact they do not appear to have done so does indicate the possibility of a bug or security issue. reply crest 18 hours agoparentThem *quietly* disabling a feature that few users will notice yet complicates the frontend suggests they pulled this chicken bit because they wanted to avoid or delay disclosing a hardware bug to the general public, but already push the mitigation. Fucking vendors! Will they ever learn? sigh reply BartjeD 10 hours agorootparentQuitely disabling it is also a big risk. Because you're signalling that in all probablity you were aware of the severity of the issue; Enough so that you took steps to patch it. If you don't disclose the vulnerability then affected parties cannot start taking countermeasures, except out of sheer paranoia. Disclosing a vulnerability is a way shift liability onto the end user. You didn't update? Then don't complain. Only rarely do disclosures lead to product liability. I don't remember this (liability) happening with Meltdown and Spectre either. So wouldn't assume this is AMD being secretive. reply dannyw 13 hours agorootparentprevEvery modern CPU has dozens of hardware bugs that aren’t disclosed and quietly patched away or not mentioned. reply whaleofatw2022 13 hours agorootparentprevDevils advocate... if this is being actively exploited or is easily exploitable, the delay in announcement can prevent other actions. reply fulafel 11 hours agoprevInteresting that in the Cortex-A15 this is a \"key design feature\". Are there any numbers about its effect other chips? I guess this could also be used as an optimization target at least on devices that are more long lived designs (eg consoles). reply syntaxing 21 hours agoprevInteresting read, one thing I don’t understand is how much space does loop buffer take on the die? I’m curious with it removed, on future chips could you use the space for something more useful like a bigger L2 cache? reply akira2501 20 hours agoparentI think most modern chips are routing constrained and not floorspace constrained. You can build tons of features but getting them all power and normalized signals is an absolute chore. reply atq2119 19 hours agoparentprevJudging from the diagrams, the loop buffer is using the same storage as the micro-op queue that's there anyway. If that is accurate (and it does seem plausible), then the area cost is just some additional control logic. I suspect the most expensive part is detecting a loop in the first place, but that's probably quite small compared to the size of the queue. reply Remnant44 21 hours agoparentprevMy understanding is that it's a pretty small optimization on the front end. It doesn't have a lot of entries to begin with (144) so the amount of space saved is probably negligible. Theoretically, the loop buffer would let you save power or improve performance in a tight loop. In practice, it doesn't seem to do either, and AMD removed it completely for Zen 5. reply progbits 21 hours agoparentprevIt says 144 micro-op entries per core. Not sure how many bytes that is, but L2 caches these days are around 1MB per core, so assuming the loop buffer die space is mostly storage (sounds like it) then it wouldn't make a notable difference. reply londons_explore 20 hours agoprevIn the \"power\" section, it seems the analysis doesn't divide by the number of instructions executed per second. Energy used per instruction is almost certainly the metric that should be considered to see the benefits of this loop buffer, not energy used per second (power, watts). reply eek2121 16 hours agoparentEvery instruction takes a different amount of clock cycles (and this varies between architectures or iterations of an architecture such as Zen 4-Zen 5), so that is not feasible unless running the workload produced the exact same instructions per cycle, which is impossible due to multi threading/tasking. Even order and the contents of RAM matters since both can change everything. While you can somewhat isolate for this by doing hundreds of runs for both on and off, that takes tons of time and still won’t be 100% accurate. Even disabling the feature can cause the code to use a different branch which may shift everything around. I am not specifically familiar with this issue, but I have seen cases where disabling a feature shifted the load from integer units to the FPU or the GPU as an example, or added 2 additional instructions while taking away 5. reply rasz 20 hours agoprevAnecdotally one of very few differences between 1979 68000 and 1982 68010 was addition of \"loop mode\", a 6 byte Loop Buffer :) reply crest 19 hours agoparentMuch more importantly they fixed the MMU support. The original 68000 lost some state required to recover from a page fault the workaround was ugly and expensive: run two CPUs \"time shifted\" by one cycle and inject a recoverable interrupt on the second CPU. Apparently it was still cheaper than the alternatives at the time if you wanted a CPU with MMU, a 32 bit ISA and a 24 bit address bus. Must have been a wild time. reply phire 13 hours agorootparent> run two CPUs \"time shifted\" by one cycle and inject a recoverable interrupt on the second CPU. That's not quite how it was implemented. Instead, the second 68000 was halted and disconnected from the bus until the first 68000 (the executor) trigged a fault. Then the first 68000 would be held in halt, disconnected from the bus and the second 68000 (the fixer) would take over the bus to run the fault handler code. After the fault had been handled, the first 68000 could be released from halt and it would resume execution of the instruction, with all state intact. As for the cost of a second 68000, extra logic and larger PCBs? Well, the of the Motorola 68451 MMU (or equivalent) absolutely dwarfed the cost of everything else, so adding a second CPU really wasn't a big deal. Technically it didn't need to be another 68000, any CPU would do. But it's simpler to use a single ISA. For more details, see Motorola's application note here: http://marc.retronik.fr/motorola/68K/68000/Application%20Not... reply ack_complete 1 hour agoparentprevThe loop buffer on the 68010 was almost useless, because not only was it only 6 bytes, it only held two instructions. One had to be the loop instruction (DBcc), so the loop body had to be a single instruction. Pretty much the only thing it could speed up in practice was an unoptimized memcpy. reply Dylan16807 14 hours agoparentprevThat's neat. For small loop buffers, I quite like the GreenArrays forth core. It has 18 bit words that hold 4 instructions each, and one of the opcodes decrements a loop counter and goes back to the start of the word. And it can run appreciably faster while it's doing that. reply CalChris 15 hours agoprevIf it saved power wouldn’t that lead to less thermal throttling and thus improved performance? That power had to matter in the first place or it wouldn’t have been worth it in the first place. reply kllrnohj 15 hours agoparentNot necessarily. Let's say this optimization can save 0.1w in certain situations. If one of those situations is common when the chip is idle just keeping wifi alive, well hey that's 0.1w in a ~1w total draw scenario, that's 10% that's huge! But when the CPU is pulling 100w under load? Well now we're talking an amount so small it's irrelevant. Maybe with a well calibrated scope you could figure out if it was on or not. Since this is in the micro-op queue in the front end, it's going to be more about that very low total power draw side of things where this comes into play. So this would have been something they were doing to see if it helped for the laptop skus, not for the desktop ones. reply Out_of_Characte 4 hours agorootparentYou're probaly right on the mark with this. Though even desktops and servers can benefit from lower idle power draw. So there is a chance that it might have been moved to a different c-state. reply mleonhard 13 hours agoprevIt looks like they disabled a feature flag. I didn't expect to see such things in CPUs. reply astrange 6 hours agoparentThey have lots of them (called \"chicken bits\"). Some of them have BIOS flags, some don't. It's very very expensive to fix a bug in a CPU, so it's easier to expose control flags or microcode so you can patch it out. reply ksec 16 hours agoprevWondering if Loop Buffer is still there with Zen 5? ( Idly waiting for x86 to try and compete with ARM on efficiency. Unfortunately I dont see Zen 6 or Panther Lake getting close. ) reply monocasa 16 hours agoparentIt is not. reply Pannoniae 20 hours agoprevFrom another article: \"Both the fetch+decode and op cache pipelines can be active at the same time, and both feed into the in-order micro-op queue. Zen 4 could use its micro-op queue as a loop buffer, but Zen 5 does not. I asked why the loop buffer was gone in Zen 5 in side conversations. They quickly pointed out that the loop buffer wasn’t deleted. Rather, Zen 5’s frontend was a new design and the loop buffer never got added back. As to why, they said the loop buffer was primarily a power optimization. It could help IPC in some cases, but the primary goal was to let Zen 4 shut off much of the frontend in small loops. Adding any feature has an engineering cost, which has to be balanced against potential benefits. Just as with having dual decode clusters service a single thread, whether the loop buffer was worth engineer time was apparently “no”.\" reply jb1991 21 hours agoprev [7 more] [flagged] olejorgenb 21 hours agoparentThat was not my takeaway from (admittingly) skimming the article. (that it's \"huge\") reply Shalah 19 hours agorootparentWould people please stop hiding posts they disagree with. I'm a grown-up and can make my own mind up as to the veracity of any stated opinions. reply kergonath 12 hours agorootparentThe problem is not agreeing or not. The problem is posting uninformed opinions without reading the story, which explains in detail that it is both true and not that important (i.e., not \"huge\"). reply umanwizard 17 hours agorootparentprevHN has a “showdead” option in the user preferences that will make it so you can see everything, no matter how downvoted. reply throwuxiytayq 21 hours agoparentprev [–] Neither huge, nor if. reply moffkalast 21 hours agorootparent [–] So just true? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "AMD has disabled the loop buffer in its Zen 4 CPUs, a feature intended for power optimization rather than performance enhancement.- Performance tests indicate negligible differences with the loop buffer disabled, as the op cache compensates with sufficient bandwidth.- The decision to disable the loop buffer, possibly due to a hardware bug, is unlikely to significantly impact performance or power consumption, and the feature was not widely documented by AMD."
    ],
    "commentSummary": [
      "AMD has disabled the loop buffer in its Zen 4 CPUs, likely due to undisclosed hardware vulnerabilities, resulting in a minor performance decrease in certain situations.- The loop buffer was designed to enhance power efficiency and performance, but its removal indicates it may not have offered substantial advantages.- The quiet decision to disable the loop buffer has led to concerns regarding transparency and the management of hardware vulnerabilities."
    ],
    "points": 271,
    "commentCount": 105,
    "retryCount": 0,
    "time": 1732999655
  },
  {
    "id": 42285689,
    "title": "OpenWRT One Released: First Router Designed Specifically for OpenWrt",
    "originLink": "https://sfconservancy.org/news/2024/nov/29/openwrt-one-wireless-router-now-ships-black-friday/",
    "originBody": "Donate Join Vizio Home What we do The Corresponding Source Copyleft Compliance Vizio Lawsuit Member Projects Outreachy Use The Source FOSSY Who we are Sponsors Sustainers Board of Directors Staff Evaluation Committee Outside Counsel, et alia Transparency Contact Learn The Corresponding Source Glossary of Terms Vizio Lawsuit Q&A Our Issues In The News News News Releases Blog Vizio Lawsuit In News Our Issues In News Press Until January 15, the next $185,769 of support we receive will be matched! $19,118 matched! $185,769 to go! Home / News First Router Designed Specifically For OpenWrt Released The New OpenWrt One on sale now for $89 — Ultimate Gift for Right-To-Repair Enthusiasts November 29, 2024 Today, we at SFC, along with our OpenWrt member project, announce the production release of the OpenWrt One. This is the first wireless Internet router designed and built with your software freedom and right to repair in mind. The OpenWrt One will never be locked down and is forever unbrickable. This device services your needs as its owner and user. Everyone deserves control of their computing. The OpenWrt One takes a great first step toward bringing software rights to your home: you can control your own network with the software of your choice, and ensure your right to change, modify, and repair it as you like. The OpenWrt One demonstrates what's possible when hardware designers and manufacturers prioritize your software right to repair; OpenWrt One exuberantly follows these requirements of the copyleft licenses of Linux and other GPL'd programs. This device provides the fully copyleft-compliant source code release from the start. Device owners have all the rights as intended on Day 1; device owners are encouraged to take full advantage of these rights to improve and repair the software on their OpenWrt One. Priced at US$89 for a complete OpenWrt One with case (or US$68.42 for a caseless One's logic board), it's ready for a wide variety of use cases. Manufactured in collaboration with Banana Pi, the OpenWrt One uses the MediaTek MT7981B SoC, with MT7976C wifi, 1 GiB DDR4 RAM, 128 MiB SPI NAND + 4 MiB SPI NOR flash, two Ethernet ports (2.5 GbE and 1 GbE), a USB host port, M.2 2042 for NVMe SSD or similar devices, and mikroBUS expansion header. The OpenWrt offers both PoE (Power over Ethernet) via the 2.5 GbE port , or direct power via the USB-C power port with 12V USB-PD. A convenient USB serial interface is built into the other USB-C port: expert users won't miss any boot messages! This hacker-friendly device is unbrickable, providing a switch to separately flash the NOR and NAND portions of the flash memory. This new product has completed full FCC compliance tests; it's confirmed that OpenWrt met all of the FCC compliance requirements. Industry “conventional wisdom” often argues that FCC requirements somehow conflict with the software right to repair. SFC has long argued that's pure FUD. We at SFC and OpenWrt have now proved copyleft compliance, the software right to repair, and FCC requirements are all attainable in one product! You can order an OpenWrt One now! Since today is the traditional day in the USA when folks buy gifts for love ones, we urge you to invest in a wireless router that can last! We do expect that for orders placed today, sellers will deliver by December 22 in most countries. Everyone can purchase a complete OpenWrt One with case or just the board via those links and other sale outlets, too. Regardless of where you buy from, for every purchase of a new OpenWrt One, a US$10 donation will go to the OpenWrt earmarked fund at Software Freedom Conservancy. Your purchase not only improves your software right to repair, but also helps OpenWrt and SFC continue to improve the important software and software freedom on which we all rely! Connect with Conservancy on Fediverse, X, Facebook, and YouTube. Main PageContactSponsorsPrivacy PolicyRSS Feed Our privacy policy was last updated 22 December 2020. This page, and all contents herein, unless a license is otherwise specified, are licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.",
    "commentLink": "https://news.ycombinator.com/item?id=42285689",
    "commentBody": "OpenWRT One Released: First Router Designed Specifically for OpenWrt (sfconservancy.org)242 points by m463 16 hours agohidepastfavorite108 comments jolan 15 hours agoIf you want a more polished product, GL.iNet's Flint 2 is almost identical, has more ports, and runs stock OpenWrt just fine: https://openwrt.org/toh/gl.inet/gl-mt6000 reply thisislife2 14 hours agoparentBe careful of GL.iNet's products - some of them say they run OpenWRT, but they don't run stock OpenWRT and instead offer a version of OpenWRT supplied by the CPU manufacturer, with binary drivers and no source code. reply mook 13 hours agorootparentYeah, it's probably best to pick something with upstream OpenWRT support and flash it when you get the device. I think that's what grandparent was saying, given that they link to the ToH. reply simonmales 9 hours agorootparentAh shit, I blindly ordered the first Flint thinking OpenWRT would magically work. I just spotted an upstream PR in the works, so it should be supported eventually. reply nickysielicki 9 hours agorootparentSigh, time is a flat circle: https://news.ycombinator.com/item?id=38934013#38936279 reply smashed 14 hours agoparentprevIt's not openwrt even though they pretend it is in their marketing. It's based on openwrt and might be \"compatible\" to some level with other openwrt packages. When asked for full source code they seem transparent about it: https://forum.gl-inet.com/t/source-code-for-gl-firmware-and-... You can't reproduce their images and they don't share the improvements. Of course not GPL compliant but not a concern in China I believe. reply aspenmayer 12 hours agorootparentYou can check if your GL-iNet product supports native OpenWrt here: https://www.gl-inet.com/support/firmware-versions/ You might find some sources here: https://github.com/gl-inet The issues regarding GPL compliance or lack thereof are worth noting, however. I made a point of asking for native OpenWrt firmware for the products I have from them, only to discover after the fact that due to closed source firmware blobs, it will likely never be available in that format, which was somewhat disappointing. Given the fairly low/competitive price point of their hardware, I think it’s worth taking the time to make sure that the device suits your needs in that regard, if it’s important to you. reply nickysielicki 9 hours agorootparentThe bigger deal is the lack of an open source BL2/BL31, but that’s me with my tin foil hat. Sent from my iPhone reply aspenmayer 8 hours agorootparentAre you referring to open source BL2/BL31 for GL-iNet products, and/or for OpenWrt One? I’m not sure it’s possible with either, as haven’t looked into the One in detail yet, as I wasn’t aware it had launched until TFA was posted, though I was aware of it since January or so. I’d like to run fully open source network stack if possible myself, though I’m not sure if that possible without moving the goalposts and virtualizing something or doing it in software, and even then I’d have to figure out some kind of boot attestation ideally, thought I'm not sure how that's going to pan out. Isn't Intel SGX/AMD SEV/ARM CCA required for that? Some links I thought we interesting on that topic, as it's adjacent to the discussion: > A comparison study of intel SGX and AMD memory encryption technology https://dl.acm.org/doi/10.1145/3214292.3214301 > vSGX: Virtualizing SGX Enclaves on AMD SEV https://ieeexplore.ieee.org/document/9833694 What do you suggest? How’s your hat fitting, by the way? Here's the firmware for the OpenWrt One, if that helps you determine whether it does what you want: https://firmware-selector.openwrt.org/?version=SNAPSHOT&targ... If you find the answer to your/our questions, please let me/us know! Sent from my iPhone in Lockdown Mode reply throwaway2037 9 hours agorootparentprev> Of course not GPL compliant but not a concern in China I believe. I don't believe this. There are multiple cases where GPL was enforced by Chinese courts. Example: https://www.ifross.org/?q=node/1676 reply jitl 2 hours agorootparentPosted case does not enforce GPL reply danieldk 11 hours agorootparentprevIt's not really an issue, since OpenWrt has full support for this device. IIRC the support in OpenWrt 23.05 is pretty good, I have been running 24.10 on a Flint 2 that we use as an AP and I've had zero issues. I'd argue that even though by default the Flint 2 has a nicer interface for beginners, vanilla OpenWrt is much better. E.g. their old OpenWrt 21.02 build with the proprietary Mediatek SDK does not support baby jumbo frames, which are used by a bunch of providers that still use PPPoE (to get better performance). reply orra 7 hours agorootparentYeah, you'd definitely want baby jumbo frames if you live in the UK. ISPs here almost universally use PPP. reply anonym29 13 hours agorootparentprev>It's not openwrt This isn't entirely accurate. It absolutely is running a full OpenWRT instance. In addition to that, they have produced their own UI/shell, which is the default that you'll land on, but it's not difficult to get into LuCI. That said, I'm not stating that it's only running OpenWRT, or that the OpenWRT instance it is running is unmodified, or trustworthy. That said, I have struggled to get gigabit wireguard VPN throughput on other devices that support OpenWRT. I love FOSS, I love self-hosting, I love DIY-friendly tinkerer-friendly, and I love high levels of user control, I just wish the ecosystem that prioritized these things had a stronger emphasis on high-end hardware that offers high performance. reply danieldk 11 hours agoparentprevAlso, 2 x 2.5Gbe, so if you have a > 1Gbit &&Maybe they will make a one plus with double the ram and 2x2.5Gbe? I parse this as \"Maybe they will make one that costs more than $89?\" reply bradley13 14 hours agoprevWhy do routers always come with WiFi? Many people need/want to run a separate (mesh) network of APs. Having WiFi in the router, just to turn it off, seems a waste. reply AceJohnny2 13 hours agoparentOpenWRT was born from WiFi... That's the valuable stuff for most of the market. The \"router\" aspect was the bundled part. If you don't want wifi aspect, perhaps look at Mikrotik stuff? reply lxpz 12 hours agoparentprevhttps://openwrt.org/toh/tp-link/er605_v2?s[]=er605v2 This one doesn't have WiFi, just 5 Ethernet ports. It can be flashed openwrt with a very reasonable amount of tweaking. It's actually quite powerful and has 256M of RAM and 128M of flash memory. I have one, it's very cool. reply transpute 10 hours agorootparent> In the default setup, the “Routing/NAT Offloading” for the ER605 is disabled. This may slow down the ER605 enormously (50% slower). Is that because of a binary blob dependency for offloading? reply martin1975 8 hours agoparentprevI never buy routers w wifi. Running PC Engines APU4 with Ubiquiti AP is more rock solid/stable than anything else I ever ran which had wifi+wired in a single router. reply HnUser12 10 hours agoparentprevSeems like they’re starting to do this as mesh is becoming common. In my apartment, my ISP has a permanently installed router without wifi. Then they just send you a wifi router or mesh devices depending on your requirements. reply intsunny 15 hours agoprevAs an aside: Why can't DSL modems be a single USB dongle? Those of us with DSL connections must suffer either an extremely limited selection of DSL modem/routers that can run Linux/OpenWRT, or have to suffer running a Linux/OpenWRT router behind a DSL modem (that often has proprietary and out of date firmware). reply toast0 14 hours agoparentI'm just about to cancel my DSL, but when you run the modem in bridge mode, and run PPPoE on your actual NAT gateway if needed (which is sadly often the case), the modem firmware doesn't matter very much. I put together some stuff so I could transfer PPPoE sessions to a backup system and then I could reboot the NAT boxes for upgrates with minimal downtime. Sometimes, it even worked ;) reply numpad0 14 hours agoparentprevThere are fiber PON/ONU/UT/$JARGON in shape of an SFP module, though most customers don't appreciate such offering and therefore it'll be an upsell. As for why not USB specifically, probably because such a device is inherently much faster and responsive in upload to the Internet than downloads, and therefore it makes less sense. reply ssl-3 11 hours agoparentprevWhen I last used DSL, I was using AT&T uverse (which is/was VDSL with multicast video layered in). I configured the provided gateway/router-widget to provide a \"DMZ Plus\" mode for my router (a custom box running Tomato or OpenWRT or something), and I called to get ports 25 and 80 unblocked. And then, plus-or-minus some completely-surmountable difficulty with making dynamic DNS behave properly it all worked fine. For years. I never connected anything other than my router to the ISP-provided device. There's probably some corner cases where this configuration falls flat, but I never ran into them. What might be some practical advantages of what you suggest? reply consp 11 hours agorootparent> What might be some practical advantages of what you suggest? Using half the power reply ssl-3 5 hours agorootparentHalf? A device runs on electricity and performs a function. It consumes power at a rate of x. You're telling me that a device that performs the same function will consume power at a rate of precisely x/2 simply by virtue of being plugged into USB? reply throwaway2037 9 hours agorootparentprevHow much power difference? reply justsomehnguy 10 hours agorootparentprevYou have some other more demanding problems if the power usage of an additional DSL modem is a concern for you. Well okay, you shelled out $29.99 for a new shiny USB DSL modem. How many years should pass to at least have a ROI? reply bpye 14 hours agoparentprev> As an aside: Why can't DSL modems be a single USB dongle? They definitely existed in the UK for a time and were often supplied by ISPs. IIRC they were only supported in Windows XP, and drivers were never provided for Vista. reply cyfex 14 hours agoparentprevNot USB, but there exist SFP DSL modems, like the Proscend 180-T. They are pricey and difficult to come by, but with something like a Turris Omnia as a host, you _can_ have a single-device modem/router/WiFi AP reply CursedSilicon 15 hours agoparentprevI tried for years to do similar when I lived in Australia. Though with a PCI/PCI-E card ZyXEL if I remember correctly did make an ADSL2+ at the time PCI-E card. Literally just a DSL modem wired to a Realtek 8139 NIC. You could slap it in a Linux (or BSD, or Windows) PC and just use PPPoE to connect to the internet Naturally it was impossible to order the damn thing and I never got to realize my dream of an \"all-in-one\" DSL Linux router. After moving to DOCSIS (Cable) internet I ran into the same confusing problem. \"Thankfully\" with Fiber everything is just ethernet (more or less) now. But it was an infuriating time in the 2010's reply darkest_ruby 10 hours agoprevGl.inet is designed for openwrt is cheap and well tested. reply mattlondon 10 hours agoparent+1 - I have a \"beryl\" one I think and it worked well in a few situations (backup router at home, and replicating home WiFi SSID when travelling to places with random hotel wifi so I didn't need to reprogram all the kid's wifi-connected white noise machines/night-lights/cameras/monitors/etc - yes I realise this is absurd but we find it all useful for a stress-free life with tiny kids) Only complaint is the USB LTE dongle was super unintuitive to setup/use, and I felt like some of the translations were just flat out wrong so it was a bit of trial by error and factory resetting things when inadvertently bricking/locking myself out etc. reply pdimitar 6 hours agoprevNot bad, but call me when they also make 100% OSS switches and also have beefier hardware with fully OSS firmware. If you wanna do the open thing, go all the way. Before that there's not much point. Also 1x1Gbps and 1x2.5Gbps, why?! I get it, the SoC, but could they not get something a touch more powerful? :( reply redundantly 13 hours agoprevI'd like to know if there are any binary blobs required to boot and/or use the Wi-Fi chip. reply Namidairo 10 hours agoparentIn addition to the wifi, I recall the preloader at the start of the boot chain is also a binary blob, which handles some of the chip init and memory calibration for the DDR4. reply blazex344 11 hours agoparentprevalmost certainly. we don't know what's running inside the modem chip reply evanjrowley 15 hours agoprevNice. I'll definitely consider using it. I love that it's got a battery powered RTC. reply throwaway2037 9 hours agoparentWhat is the alternative? No battery, but update via NNTP on reboot? reply evanjrowley 2 hours agorootparentAssuming there's an active uplink that can reach a timeserver, then yes. The batter-powered RTC allows the router to serve more accurate time and maintain working HTTPS in the event it gets rebooted as a troubleshooting step for when the internet goes down, possibly for an extended period. reply mrbuttons454 16 hours agoprevVery cool, but wish it was at least 2x2.5GbE. reply circularfoyers 15 hours agoparentI think it's unfortunate given the audience I imagine will make up most of its purchases. For example, the NBN in Australia just announced earlier this year it's first 2 Gbps residential plans (previously 1 Gbps being the maximum) planned for availability some time next year[1]. [1] https://www.nbnco.com.au/corporate-information/media-centre/... reply throwaway2037 9 hours agoparentprevDo you have a WAN connect greater than 1GbE? reply kalleboo 5 hours agorootparentWhere I live (Japan), 10 Gbit availability is starting to spread, I just got upgraded a few weeks ago, the monthly cost is the same. Ended up building a router out of a used ThinkCentre Tiny. reply acka 9 hours agoprev> Industry “conventional wisdom” often argues that FCC requirements somehow conflict with the software right to repair. SFC has long argued that's pure FUD. We at SFC and OpenWrt have now proved copyleft compliance, the software right to repair, and FCC requirements are all attainable in one product! This is not FUD, regulatory compliance requires that devices which make unlicensed use of radio spectrum (such as Wi-Fi) must contain non-user modifiable software for their radios in order to comply with regulations, and the MT7976C chip used in this device is no exception. While most of the mt76 driver is open source, it does contain a binary blob containing proprietary firmware which must be uploaded to the chip in order for it to function. The claim that all of the software in the OpenWRT One is open source and copyleft is therefore false, it cannot be any other way, FCC regulatory compliance and being fully open source are mutually exclusive by definition. reply Sebb767 8 hours agoparent> regulatory compliance requires that devices which make unlicensed use of radio spectrum (such as Wi-Fi) must contain non-user modifiable software for their radios in order to comply with regulations Couldn't one restrict the chip to only accept signed firmware and then publish the source with reproduceable build instructions? This would prevent user modification while still being open source. reply rgmerk 8 hours agorootparentWhile it might be technically open source, it's very, very much against the spirit of open source. It's also explicitly prohibited under the GPLv3 unless there's no way that the manufacturer can update that firmware. reply madduci 12 hours agoprev> Priced at US$89 for a complete OpenWrt One with case (or US$68.42 for a caseless One's logic board) I see EUR €103 when I open the website. reply fonema 11 hours agoparentPossibly, that's 89 without VAT. reply serialx 12 hours agoparentprevChange the currency to USD reply shadowpho 15 hours agoprevSad that it’s wifi6 and not wifi6E. Can’t wait for wider adoption reply CursedSilicon 15 hours agoparentWorse still, Wi-Fi *7* devices are already out. Qualcomm's QCN9274's running on the \"ath12k\" chipset [1] have been a tempting DIY option. Though that module requires an external 3 amps at 5 Volt (15 watts!) to be soldered to it to operate [1] https://www.524wifi.com/index.php/network-modules-adapters/w... reply SirMaster 14 hours agorootparentAm I the only one who doesn’t even want their router to have wifi? I’d rather use wifi APs connected via PoE to a switch which is connected to the router. I feel like router and wifi should be separate so that I can expand and upgrade the wifi independently of the router. reply baby_souffle 14 hours agorootparent> I feel like router and wifi should be separate so that I can expand and upgrade the wifi independently of the router. This _is_ the objectively better way. It's not practical for a whole host of scenarios, though. reply toast0 14 hours agorootparentprevThis device looks like a candidate for a wifi AP; if I'm reading correctly, it accepts PoE over the 2.5G port, and OpenWRT makes for a decent AP. reply yjftsjthsd-h 12 hours agorootparent> and OpenWRT makes for a decent AP. Decent but sort of annoying to set up; https://openwrt.org/docs/guide-user/network/wifi/wifiextende... appears to have combined what used to be 2 pages with different instructions, but it still seems to have both sets of slightly different steps. IMHO there really should be a single button in LuCI to make the box just an AP without DHCP/DNS/firewall. reply sliken 14 hours agorootparentprevThat's my preference as well. Keep the router simple, don't put a transmitter near the sensitive bits to cause interference with the CPU, ethernet, or USB bits. I want my AP to be dumb and do nothing besides connect ethernet to wifi clients. That way when you need more coverage you add APs, newer WIFI upgrade only the APs, etc. reply SanjayMehta 14 hours agorootparentprevSame here. I’m stuck with the service provider’s (“free”) fibre router. I’ve turned its wifi off, and connected my own setup via the LAN port. reply punnerud 11 hours agorootparentI found that I could connect UniFi directly to the fiber bridge, dropping the “free” router. Believe there is better firewall in UniFi anyway. reply JoshTriplett 10 hours agorootparentprev> Am I the only one who doesn’t even want their router to have wifi? I'd prefer to separate the two as well, but I'd rather not admin them separately. It'd be convenient to manage them all in one (fully local, non-cloud) place. But yeah, my ideal setup would be a 10Gbps Ethernet router the upstream Internet connection, firewall, NAT, and LAN DHCP, and then one or more separate Wifi 7 routers whose only job is to bridge transparently to Ethernet but not handle DHCP or NAT. reply thaumasiotes 12 hours agorootparentprev> I’d rather use wifi APs connected via PoE to a switch which is connected to the router. What's the point of separating the router and the switch? reply accrual 12 hours agorootparentI can think of a couple reasons: - Switches frequently offer PoE by default whereas routers (especially consumer or PC-grade stuff) would need a PoE adapter - Devices connected to the switch can communicate directly without the router, saving a hop - Modern switches may only need to read the first few bytes of the frame before cutting the packet over to the appropriate port, whereas routers tend to need to read the whole packet and apply rules/policy, incurring higher processing costs An ordinary home LAN may not find any noticeable benefits, but an enterprise setup might benefit. reply jauntywundrkind 15 hours agorootparentprevComplex (who I believe does most of Qualcomm's reference designs) has some newer models out, if you are willing to go down market some. Only 2x2 and lower power output, but they are powered off mini-pcie (and I think there are some m.2 models as well). https://compex.com.sg/shop/wifi-module/wle7002e25d-wifi7-11b... Not required, but you can solder a Multi Link Operation wire between multiple cards (within a single system), to enable them to work in concert together (basically as a 4x4 or whatnot). In case you still really miss soldering extra shit onto your cards!! Sponsored write-up, and longer/jargon-y-er than it needs to be, but still enjoyed this post: https://www.cnx-software.com/2024/11/07/compex-systems-wi-fi... reply rubatuga 14 hours agorootparentprevlol, I'm still on 802.11n ! reply mrbluecoat 13 hours agoprevWasn't the BPI-R3 also designed for OpenWRT? reply yellowapple 13 hours agoparentSo was the recent Linksys WRT series. The product description on Amazon (which Linksys presumably provided) explicitly advertises the WRT3200ACM as \"[o]pen source ready with OpenWrt and DD WRT for complete flexibility and customization of networking functions, or to optimize your router for specific use cases\". Likewise for the WRT1900AC: \"For advanced users who want to customize their firmware, the WRT1900AC stays true to its lineage by offering an Open WRT compatible firmware with Open Source ready capabilities that allow advanced users to expand the capabilities of the Router.\". The headline's (and article's) claim of the OpenWRT One being the \"first router designed specifically for OpenWRT\" is misleading, at best. reply aspenmayer 9 hours agorootparent> The headline's (and article's) claim of the OpenWRT One being the \"first router designed specifically for OpenWRT\" is misleading, at best. This is the first “official” OpenWrt first-party developed and supported router. It also is intended to be the initial “blessed by OpenWrt devs” hardware configuration that will hopefully lead to further devices in the future. It’s also intended to help drive awareness and interest in the platform, with a known-good hardware and software platform, help build brand loyalty, and raise money for development of OpenWrt for the One and all other compatible devices. It’s something the community and the project maintainers have wanted to do for many years now. You can read more about the discussion leading up to this on the OpenWrt mailing list and forum. https://lists.openwrt.org/pipermail/openwrt-devel/2024-Janua... https://forum.openwrt.org/t/openwrt-one-celebrating-20-years... https://docs.banana-pi.org/en/OpenWRT-One/BananaPi_OpenWRT-O... https://openwrt.org/toh/openwrt/one reply yellowapple 6 hours agorootparentThat's great, but they should actually go with that claim of being the first router OpenWRT designed themselves, instead of incorrectly claiming that their router is the first designed specifically for OpenWRT. reply aspenmayer 5 hours agorootparentI think there's some confusion, but I'll admit that you're not confused about what you want, but you might be about what correctness and/or a reasonable remedy looks like. TFA is a post, a press release really, by Software Freedom Conservancy, of which the OpenWrt project is a member. That said, the post doesn't have a byline, so we can't really point the finger at anyone. I doubt that the OpenWrt project would even characterize the OpenWrt One in the same manner as does the title of the post in TFA. That said, the title is largely correct to my reading, as other devices that precede the One, like the Turris Omnia, for instance, aren't designed specifically for OpenWrt, but rather a derivative, Turris OS. https://en.wikipedia.org/wiki/Turris_Omnia https://www.turris.com/en/products/omnia/ To my knowledge, the OpenWrt One is the first router designed specifically for mainline/stock OpenWrt, which is what it ships with, and any comparable router meeting your definition must also meet these marks. That rules out every Linksys or GL-iNet router ever sold, because they don't ship with stock OpenWrt, regardless of whether or not they run a derivative out of the box, or whether they may be reflashed to run stock OpenWrt, whereas the OpenWrt One disticntly does. I will admit I could be mistaken on this point or any other made herein, and will happily admit my mistake if so, but as it sits, the burden of proof is on the accuser, which would be you in this case. So if OpenWrt One isn't first, who is? reply Namidairo 9 hours agorootparentprevThe sad thing is, the WRT3200ACM has more or less an unmaintained wifi driver, with 802.11w (and thus WPA3) possibly broken within the radio firmware itself. I believe there are other issues regarding regulatory settings being hardcoded in there too. reply newman314 8 hours agoparentprevI've been eyeing the BPI-R4 as an eventual replacement for my aging ER4. reply StayTrue 16 hours agoprevSeems almost ideal if you don’t use it as an ethernet switch. A third radio for wireless mesh trunking is the only thing I see missing from my OpenWRT setup. reply bpye 15 hours agoparentThere is an M.2 slot, though only PCIe 2.0 x1. Possibly still good enough for reasonable bandwidth. reply vkdelta 16 hours agoprevWould other SoC suppliers ( Qualcomm, Broadcom) embrace it? reply c420 12 hours agoparentDo you mean open wrt? If yes, the devs have serious problems whenever they need to deal with broadcom because of all the proprietary blobs. So the number of supported chipsets is low. reply aspenmayer 9 hours agoprev [–] Development discussion from the OpenWrt mailing list: https://lists.openwrt.org/pipermail/openwrt-devel/2024-Janua... Forum discussion: https://forum.openwrt.org/t/openwrt-one-celebrating-20-years... Manufacturer page, with diagrams and info about PoE module, etc: https://docs.banana-pi.org/en/OpenWRT-One/BananaPi_OpenWRT-O... OpenWrt Table of Hardware page for OpenWrt One: https://openwrt.org/toh/openwrt/one > OpenWrt One - specification, unboxing, web interface preview https://www.youtube.com/watch?v=VZ4itva1Cv8 > Open-Source-Router #OpenWrt One im Test https://www.youtube.com/shorts/STfbQW1rdzY > Open Source Router OpenWrt One in beta testing https://www.heise.de/tests/Open-Source-Router-OpenWrt-One-im... https://archive.is/joUa1 (archive of above link due to registration wall) They were good enough to actually put out some tests and specs! I included the performance numbers below; heise.de also includes physical dimensions and other stats that I didn't include below. > Measured values > WLAN 2.4 GHz (close / 20 m)[1] 341 / 206 Mbit/s > WLAN 5 GHz 809 / 533 Mbit/s > NAT Performance IP/IP (down- / upstream) 941 / 934 Mbit/s > NAT Performance PPPoE 936 / 934 Mbit/s > VPN Performance WireGuard 536 / 564 Mbit/s > SMB throughput (write / read)[2] 151 / 170 MByte/s > Power consumption [3] 5 watts >tested firmware OpenWrt SNAPSHOT r27777-4d81f40d63 > Price [4] 125 € [1] vs Intel BE200 [2] against Samba4 with NVMe SSD [3] 2G5 port, wireless on, without traffic [4] expected Current/latest snapshot firmware for OpenWrt One for reference/comparison: https://firmware-selector.openwrt.org/?version=SNAPSHOT&targ... reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Software Freedom Conservancy (SFC) and OpenWrt have released the OpenWrt One, a wireless router designed to promote software freedom and the right to repair, priced at $89 with a case or $68.42 for the board.",
      "The router is unbrickable, fully copyleft-compliant, and features a MediaTek MT7981B SoC, MT7976C wifi, 1 GiB DDR4 RAM, supporting Power over Ethernet (PoE) and USB-C power, with a USB serial interface for advanced users.",
      "The device is FCC compliant, demonstrating that software right to repair can align with regulatory requirements, and each purchase includes a $10 donation to OpenWrt at the Software Freedom Conservancy."
    ],
    "commentSummary": [
      "OpenWRT One, the first router specifically designed for the OpenWrt open-source firmware, has been released, priced at $89.- It aims to balance copyleft compliance with FCC regulations, though it still includes some proprietary binary blobs for Wi-Fi functionality.- The release has sparked discussions on the benefits of separating router and Wi-Fi functions for greater flexibility in networking hardware."
    ],
    "points": 242,
    "commentCount": 109,
    "retryCount": 0,
    "time": 1733020464
  },
  {
    "id": 42285128,
    "title": "Jeff Dean Responds to EDA Industry about AlphaChip",
    "originLink": "https://twitter.com/JeffDean/status/1858540085794451906",
    "originBody": "There has been unfounded skepticism in the EDA community about whether our AlphaChip method works as claimed in our Nature paper. @annadgoldie, @Azaliamirh, and I wrote a technical response highlighting these issues:That Chip Has Sailed: A Critique of Unfounded Skepticism… https://t.co/EoG7r6ktNm pic.twitter.com/W9X76qZ08B— Jeff Dean (@🏡) (@JeffDean) November 18, 2024",
    "commentLink": "https://news.ycombinator.com/item?id=42285128",
    "commentBody": "Jeff Dean Responds to EDA Industry about AlphaChip (twitter.com/jeffdean)178 points by nsoonhui 18 hours agohidepastfavorite109 comments vighneshiyer 15 hours agoI have published an addendum to an article I wrote about AlphaChip (https://vighneshiyer.com/misc/ml-for-placement/) at the very bottom that addresses this rebuttal from Google and the AlphaChip algorithm in general. In short, I think the Nature authors have made some reasonable criticisms regarding the training methodology employed by the ISPD authors, but the extreme compute cost and runtime of AlphaChip still makes it non-competitive with commercial autofloorplanners and AutoDMP. Regardless, I think the ISPD authors owe the Nature authors an even more rigorous study that addresses all their criticisms. Even if they just try to evaluate the pre-trained checkpoint that Google published, that would be a useful piece of data to add to the debate. reply nemonemo 12 hours agoparentIn the conclusion of the article, you said: \"While I concede that there are things the ISPD authors could have done better, their conclusion is still sound. The Nature authors do not address the fact that CMP and AutoDMP outperform CT with far less runtime and compute requirements.\" One key argument in the rebuttal against the ISPD article is that the resources used in their comparison were significantly smaller. To me, this point alone seems sufficient to question the validity of the ISPD work's conclusions. What are your thoughts on this? Additionally, I noticed that the neutral tone of this comment is quite a departure from the strongly critical tone of your article toward the AlphaChip work (words like \"arrogance\", \"disdain\", \"hyperbole\", \"belittling\", \"hostile\" for AlphaChip authors, as opposed to \"excellent\" for a Synopsys VP.) Could you share where this difference in tone originates? reply vighneshiyer 11 hours agorootparent> One key argument in the rebuttal against the ISPD article is that the resources used in their comparison were significantly smaller. To me, this point alone seems sufficient to question the validity of the ISPD work's conclusions. What are your thoughts on this? I believe this is a fair criticism, and it could be a reason why the ISPD Tensorboard shows divergence during training for some RTL designs. The ISPD authors provide their own justification for their substitution of training time for compute resources in page 11 of their paper (https://arxiv.org/pdf/2302.11014). I do not think it changes the ISPD work's conclusions however since they demonstrate that CMP and AutoDMP outperform CT wrt QoR and runtime even though they use much fewer compute resources. If more compute resources are used and CT becomes competitive wrt QoR, then it will still lag behind in runtime. Furthermore, Google has not produced evidence that AlphaChip, with their substantial compute resources, outperforms commercial placers (or even AutoDMP). In the recent rebuttal from Google (https://arxiv.org/pdf/2411.10053), the only claim on page 8 says Google VLSI engineers preferred RL over humans and commercial placers on a blind study conducted in 2020. Commercial mixed placers, if configured correctly, have become very good over the past 4 years, so perhaps another blind study is warranted. > Additionally, I noticed that the neutral tone of this comment is quite a departure from the strongly critical tone of your article I will openly admit my bias is against the AlphaChip work. I referred to the Nature authors as 'arrogant' and 'disdainful' with respect to their statement that EDA CAD engineers are just being bitter ML-haters when they criticize the AlphaChip work. I referred to Jeff Dean as 'belittling' and 'hostile' and using 'hyperbole' with respect to his statements against Igor Markov, which I think is unbecoming of him. I referred to Shankar as 'excellent' with respect to his shrewd business acumen. reply nemonemo 10 hours agorootparentThank you for your thoughtful response. Acknowledging potential biases openly in a public forum is never easy, and in my view, it adds credibility to your words compared to leaving such matters as implicit insinuations. That said, on page 8, the paper says that 'standard licensing agreements with commercial vendors prohibit public comparison with their offerings.' Given this inherent limitation, what alternative approach could have been taken to enable a more meaningful comparison between CT and CMP? reply vighneshiyer 10 hours agorootparentSo I'm not sure what Google is referring to here. As you can see in the ISPD paper (https://vlsicad.ucsd.edu/Publications/Conferences/396/c396.p...) on page 5, they openly compare Cadence CMP with AutoDMP and other algorithims quantitatively. The only obfuscation is with the proprietary GF12 technology, where they can't provide absolute numbers, but only relative ones. Comparison against commercial tools is actually a common practice in academic EDA CAD papers, although usually the exact tool vendor is obfuscated. CAD tool vendors have actually gotten more permissive about sharing tool data and scripts in public over the past few years. However, PDKs have always been under NDAs and are still very restrictive. Perhaps the Cadence license agreement signed by a corporation is different than the one signed by a university. In such a case, they could partner with a university. But I doubt their license agreement prevents any public comparison. For example, see the AutoDMP paper from NVIDIA (https://d1qx31qr3h6wln.cloudfront.net/publications/AutoDMP.p...) where on page 7 they openly benchmark their tool against Cadence Innovus. My suspicion is they wish to keep details about the TPU blocks they evaluated under tight wraps. reply nemonemo 9 hours agorootparentThe UCSD paper says \"We thank ... colleagues at Cadence and Synopsys for policy changes that permit our methods and results to be reproducible and sharable in the open, toward advancement of research in the field.\" This suggests that there may have been policies restricting publication prior to this work. It would be intriguing to see if future research on AlphaChip could receive a similar endorsement or support from these EDA companies. reply make3 11 hours agorootparentprevnext [4 more] [flagged] BoingBoomTschak 11 hours agorootparent> EQ Using a fantasy concept invented by a science journalist doesn't help your posts, you know. Protip: it's just empathy + regular intelligence. reply HeatrayEnjoyer 5 hours agorootparentEQ is not a fantasy concept. reply make3 6 hours agorootparentprevyes you figured out what I meant good job reply wholehog 14 hours agoparentprevWe're talking 16 GPUs for ~6 hrs for inference, and 48 hrs for pre-training. This is not an exorbitant amount of compute. A GPU costs $1-2/hr on the cloud market. So, ~$100-200 for inference, and ~$800-1600 for pre-training, which amortizes across chips. Cloud prices are an upper bound -- most CS labs will have way more than this available on premises. In an industry context, these costs are completely dwarfed by the rest of the chip design process. (For context, the licensing costs alone for most commercial EDA software are in the millions of dollars.) reply vighneshiyer 12 hours agorootparentYou are correct. For commercial use, the GPUs used for training and fine-tuning aren't a problem financially. However, if we wanted to rigorously benchmark AlphaChip against simulated annealing or other floorplanning algorithms, we have to afford the same compute and runtime budget to each algorithm. With 16 GPUs running for 6 hours, you could explore a huge placement space using any algorithm, and it isn't clear if RL will outperform the other ones. Furthermore, the runtime of AlphaChip as shown in the Nature paper and ISPD was still significantly greater than Cadence's concurrent macro placer (even after pre-training, RL requires several hours of fine-tuning on the target problem instance). Arguably, the runtime could go down with more GPUs, but at this point, it is unclear how much value is coming from the policy network / problem embedding vs the ability to explore many potential placements. reply Jabbles 6 hours agorootparentYou're saying that if the other methods were given the equivalent amount of compute they might be able to perform as well as AlphaChip? Or at least that the comparison would be fairer? Are the other methods scalable in that way? reply pclmulqdq 6 hours agorootparentYes, they are. The other approaches usually look like simulated annealing, which has several hyperparameters that control how much computing is used and improve results with more compute usage. reply bushbaba 13 hours agorootparentprevh100 GPU instances are multiple orders of magnitude more expensive. reply radq 13 hours agorootparentNot true, H100s cost $2-3/GPU/hr on the open market. reply menaerus 6 hours agorootparentYes, they even do at $1/GPU/hr. However, 8xH100 cluster at full utilization is ~8kWh of electricity and costs almost ~0.5M$. 16xH100 cluster is probably 2x of that. How many years before you break-even at ~24$/GPU/day income? reply Jabbles 6 hours agorootparent7 https://www.google.com/search?q=0.5e6%2F8%2F24%2F365 reply menaerus 4 hours agorootparentDid you really not understand rethoric nature of my question and assumed that I can't do 1st grade primary school math? reply YetAnotherNick 12 hours agorootparentprevH100 GPUs are more or less similar in price/performance. It is 2-3x more expensive per hour for 2-3x higher performance. reply segmondy 1 hour agoprevIt's ridiculous how expensive the wrong hire can be https://www.wired.com/story/google-brain-ai-researcher-fired... reply oesa 10 hours agoprevIn the tweet Jeff Dean says that Cheng at al. failed to follow the steps required to replicate the work of the Google researchers. Specifically: > In particular the authors did no pre-training (despite pre-training being mentioned 37 times in our Nature article), robbing our learning-based method of its ability to learn from other chip designs But in the Circuit Training Google repo[1] they specifically say: > Our results training from scratch are comparable or better than the reported results in the paper (on page 22) which used fine-tuning from a pre-trained model. I may be misunderstanding something here, but which one is it? Did they mess up when they did not pre-train or they followed the \"steps\" described in the original repo and tried to get a fair reproduction? Also, the UCSD group had to reverse-engineer several steps to reproduce the results so it seems like the paper's results weren't reproducible by themselves. [1]: https://github.com/google-research/circuit_training/blob/mai... reply cma 6 hours agoparentTraining from scratch could presumably mean including the new design attempts and old designs mixed in. So no contradiction: pretrain on old designs then finetune on new design, vs train on everything mixed together throughout. Finetuning can cause catastrophic forgetting. Both could have better performance than not including old designs. reply _cs2017_ 12 hours agoprevCurious why there's so much emotion and unpleasantness in this dispute? How did it evolve from the boring academic argument about benchmarks, significance, etc to a battle of personal attacks? reply boredatoms 11 hours agoparentA lot of people work on non-AI implementations reply aithrowawaycomm 3 hours agoparentprevThe issue is that Big Tech commercial incentives around AI have polluted the “boring academic” waters with dishonest infomercials masquerading as journal articles or arXiv preprints[1], and as a direct result contemporary AI research has a much worse “replication crisis” than the social sciences, yet with far fewer legitimate excuses. Assuming Google isn’t lying, a lot of controversy would go away if they actually released their benchmark data for independent people to look at. They are still refusing to do so: https://cacm.acm.org/news/updates-spark-uproar/ Google thinks we should simply accept their conclusions by fiat. And don’t forget about this: Madden further pointed out that the “30 to 35%” advantage of RePlAce was consistent with findings reported in a leaked paper by internal Google whistleblower Satrajit Chatterjee, an engineer who Google fired in 2022 when he first tried to publish the paper that discredited the “superhuman” claims Google was making at the time for its AI approach to chip design. It is entirely appropriate to make “personal attacks” against Jeff Dean, because the heart of the criticism is that his personality is dishonest and authoritarian: he publishes suspicious research and fires people who dissent. [1] Jeff Dean hypocritically sneering about the critique being a conference paper is especially galling. What an unbelievable asshole. reply jart 9 hours agoparentprevIf you think this is unpleasant, you should see the environmentalists who try to take a poke at Jeff Dean on Twitter. reply _cs2017_ 5 hours agorootparentWell... I kinda expect some people to be overly emotional. But I just didn't expect this particular group of people to be that. reply RicoElectrico 9 hours agoparentprevMaking extraordinary claims without a way to replicate it. And then running to the press, which will swallow anything. Because \"AI designs AI... umm... I mean chips\" sounds futuristic to a liberal-arts majors (and apparently programmers too, which I'd expect to know better and question everything \"AI\") The whole publication process seems dishonest, starting from publishing in Nature (why not ISCCC or something similar?) reply nsoonhui 17 hours agoprevThe context of Jeff Dean's response: https://news.ycombinator.com/item?id=41673769 https://news.ycombinator.com/item?id=41673808 reply wholehog 13 hours agoprevThe paper: https://arxiv.org/abs/2411.10053 reply puff_pastry 9 hours agoprevThe biggest disappointment is that these discussions are still happening on Twitter/X. Leave that platform already reply xpe 4 hours agoparentSure, we want individuals to act in a way to mitigate collective action problems. But the collective action problem exists (by definition) because individuals are trapped in some variation of a prisoner's dilemma. So, collective action problems are nearly a statistical certainty across a wide variety of situations. And yet we still \"blame\" individuals? We should know better. reply pas 2 hours agorootparentSo you're saying Head of AI of Google of Jeff can't choose a better venue? He's not the first Jeffery with a lot of power who doesn't care. reply xpe 1 hour agorootparent> So you're saying Head of AI of Google of Jeff can't choose a better venue? Phrasing it this way isn't useful. Talking about choice in the abstract doesn't help with a game-theoretic analysis. You need costs and benefits too. There are many people who face something like a prisoner's dilemma (on Twitter, for example). We could assess the cost-benefit of a particular person leaving Twitter. We could even judge them according to some standards (ethical, rational, and so on). But why bother?... ...Think about major collective action failures. How often are they the result of just one person's decisions? How does \"blaming\" or \"judging\" an individual help make a situation better? This effort on blaming could be better spent elsewhere; such as understanding the system and finding leverage points. There are cases where blaming/guilt can help, but only in the prospective sense: if a person knows they will be blamed and face consequences for an action, it will make that action more costly. This might be enough to deter than decision. But do you think this applies in the context of the \"do I leave Twitter?\" decision? I'd say very little, if at all. reply rowanG077 11 hours agoprevI don't get. Why isn't the model open if it works? If it isn't this is just a fart in the wind. If it is the findings should be straightforward to replicate. reply bsder 17 hours agoprevThe fact that the EDA companies are garbage in no way mitigates the fact that Google continues to peddle unsubstantiated snake oil. This is easy to debunk from the Google side: release a tool. If you don't want to release a tool, then it's unsubstantiated and you don't get to publish. Simple. That having been said: 1) None of these \"AI\" tools have yet demonstrated the ability to classify \"This is datapath\", \"This is array logic\", \"This is random logic\". This is the BIG win. And it won't just be a couple of percentage points in area or a couple of days saved when it works--it will be 25%+ in area and months in time. 2) Saving a couple of percentage points in random logic isn't impressive. If I have the compute power to run EDA tools with a couple of different random seeds, at least one run will likely be a couple percentage points better. 3) I really don't understand why they don't do stuff on analog/RF. The patterns are smaller and much better matches to the kind of reinforcement learning that current \"AI\" is suited for. I put this snake oil in the same category as \"financial advice\"--if it worked, they wouldn't be sharing it and would simply be printing money by taking advantage of it. reply xpe 16 hours agoparent> Google continues to peddle unsubstantiated snake oil I read your comment, but I'm not following -- or maybe I disagree with it -- I'm not sure yet. \"Snake oil\" is an emotionally loaded term that raises the temperature of the conversation. That usually makes having a conversation harder. From my point of view, AlphaGo, AlphaZero, AlphaFold were significant achievements. Agree? Are you claiming that AlphaChip is not? Are you claiming they are perpetrating some kind of deception or exaggeration? Your numbered points seem like valid criticisms (I haven't evaluated them closely), but even if true, I don't see how they support your \"snake oil\" claim. reply seanhunter 4 hours agorootparentWell here’s one exaggeration that was pretty obvious to me straight away as a somewhat disinterested observer. In her status on X Anna Goldie says [1] “ AlphaChip was one of the first RL methods deployed to solve a real-world engineering problem”. This seems very clearly untrue- for example here’s a real-world engineering use of reinforcement learning by google AI themselves from 6 years ago [2] which if you use Anna Goldie’s own timeline is 2 years before alphachip. [1] https://x.com/annadgoldie/status/1858531756506558688 [2] https://youtu.be/W4joe3zzglU?si=mFvZq8gEI6LeEQdC reply griomnib 16 hours agorootparentprevThey have literally been caught faking AI demos, they brought distrust on themselves. reply rajup 16 hours agorootparentReally not sure how you’re conflating product demos which are known to be pie in the sky across the industry (not just Google) with peer reviewed research published in journals. Super basic distinction imho. reply stackghost 14 hours agorootparent>peer reviewed research published in journals Peer review doesn't mean as much as Elsevier would like you to believe. Plenty of peer-reviewed research is absolute trash. reply throwaway2037 12 hours agorootparentAll of the highest impact papers authored by DeepMind and Google Brain have appeared in Nature, which is the gold standard for peer-reviewed natural science research. What exactly are you trying to claim about Google's peer-reviewed papers? reply stackghost 11 hours agorootparentNature is just as susceptible to the perverse incentives at play in the academic publishing market as anyone else, and has had their share of controversies over the years including having to retract papers after they were found to be bogus. In and of itself, \"Being published in a peer reviewed journal\" does not place the contents of a paper beyond reproach or criticism. reply nautilius 4 hours agorootparentprevFrom personal experience: in Nature Communications the handling editor and editor in chief absolutely do intervene, in my example to suppress a proper lit review that would have revealed the paper under review as much less innovative than claimed. reply 11101010001100 16 hours agorootparentprevTheir material discovery paper turned out to have negligible significance. reply xpe 16 hours agorootparentIf so, does this qualify as “snake oil”? What do you mean? Snake oil requires exaggeration and deception. Fair? If a paper / experiment is done with intellectual honesty, great! If it doesn’t make a big splash, fine. reply bsder 15 hours agorootparentprev> From my point of view, AlphaGo, AlphaZero, AlphaFold were significant achievements. These things you mentioned had obvious benchmarks that were easily surpassed by the appropriate \"AI\". The evidence that they were better wasn't just significant, it was obvious. This leaves the fact that with what appears to be maximal cooking of the books, the only thing AlphaChip seems to be able to beat is human, manual placement and not anything algorithmic--even from many, many generations ago. Trying to pass that off as a significant \"advance\" in a \"scientific publication\" borders on scientific fraud and should definitely be called out. The problem here is that I am certain that this is wired to the career trajectories of \"Very Important People(tm)\" and the fact that it essentially failed miserably is simply not politically allowed. If they want to lie, they can do that in press releases. If they want published in something reputable, they should have to be able to provide proper evidence for replication. And, if they can't do that, well, that's an answer itself, no? reply xpe 4 hours agorootparent> \"scientific publication\" These air quotes suggests the commenter above doesn't think the paper qualifies a scientific publication. Such a characterization is unfair. When I read the Nature article titled \"Addendum: A graph placement methodology for fast chip design\" [1], I see writing that more than meets the bar for a scientific publication. For example: > Since publication, we have open-sourced a software repository [21] to fully reproduce the methods described in our paper. External researchers can use this repository to pre-train on a variety of chip blocks and then apply the pre-trained model to new blocks, as was done in our original paper. As part of this addendum, we are also releasing a model checkpoint pre-trained on 20 TPU blocks [22]. For best results, however, we continue to recommend that developers pre-train on their own in-distribution blocks [18], and provide a tutorial on how to perform pre-training with our open-source repository [23]. [1]: https://www.nature.com/articles/s41586-024-08032-5 [18]: Yue, S. et al. Scalability and generalization of circuit training for chip floorplanning. In Proc. 2022 International Symposium on Physical Design 65–70 (2022). [21]: Guadarrama, S. et al. Circuit Training: an open-source framework for generating chip floor plans with distributed deep reinforcement learning. GitHub https://github.com/google-research/circuit_training (2021). [23]: Guadarrama, S. et al. Pre-training. GitHub https://github.com/google-research/circuit_training/blob/mai... (2021). reply xpe 4 hours agorootparentprev> Trying to pass that off as a significant \"advance\" in a \"scientific publication\" borders on scientific fraud and should definitely be called out. If true, your stated concerns with the AlphaChip paper -- selective benchmarking and potential overselling of results - reflect poor scientific practice and possible intellectual dishonesty. This does not constitute scientific fraud, which occurs when the underlying method/experiment/rules are faked. If the paper has issues with how it positions and contextualizes its contribution, criticism is warranted, sure. But don't confuse this with \"scientific fraud\". Some context: for as long as benchmark suites have existed, people rightly comment on which benchmarks should be included and how they should be weighted. reply xpe 16 hours agoparentprev> if it worked, they wouldn't be sharing it and would simply be printing money by taking advantage of it. Sure, there are some techniques in financial markets that are only valuable when they are not widely known. But claiming this pattern applies universally is incorrect. Publishing a technique doesn't prove it doesn't work. (Stating it this way makes it fairly obvious.) DeepMind, like many AI research labs, publish important and useful research. One might ask \"is a lab leaving money off the table by publishing?\". Perhaps a better question is \"What 'game' is the lab playing and over what time scale?\". reply joshuamorton 17 hours agoparentprevAs someone who has no skin in the game and is only loosely following this, there is a tool: https://github.com/google-research/circuit_training, the detractors claim to not be able to reproduce Google's results (what Dean is commenting on) with it, Google and 1-2 other companies claim to be using it internally to success (e.g. see the end of this article: https://deepmind.google/discover/blog/how-alphachip-transfor...). reply bsder 16 hours agorootparentThere are benchmarks in this space. You can also bring your chip designs into the open and show what happens with different tools. You can run the algorithm on the placed designs that you sponsor for open source VLSI to show how much better they are. None of this has been done. This is table stakes if you want to talk about your EDA algorithm advancement. If this weren't coming out of Google, everybody would laugh it out of the room (see what happened to a similar publication with similar claims from a Chinese source--everybody dismissed it out of hand--rightfully so even though that paper was MUCH better than anything Google has promulgated). Extraordinary claims require extraordinary evidence. Nothing about AlphaChip even reaches ordinary evidence. If they hadn't gotten a publication in Nature for effectively a failure, this would be way less contentious. reply throwaway2037 12 hours agorootparent> Nothing about AlphaChip even reaches ordinary evidence. You reply is wildly confident and dismissive. If correct, why did Nature choose to publish? reply rowanG077 11 hours agorootparentCan you stop with this pure appeal to authority. Publishing in nature is not proof it works. It's only proof the paper has packaged the claim it works semi well. reply throwaway2037 12 hours agoparentprev> EDA companies are garbage I don't understand this comment. Can you please explain? Are they unethical? Or do they write poor software? reply bsder 12 hours agorootparentYes and yes. EDA companies are gatekeeping monopolies. They absolutely abuse their monopoly position to extract huge chunks of money out of companies, and are pretty much single-handedly responsible for the fact that the hardware startup ecosystem is moribund compared to that of the software startup ecosystem. They have been horrible liars about performance and benchmarketing for decades. They dragged their feet miserably over releasing Linux versions of their software because they were extracting money based upon number of CPU licenses (everything was on Sparc which was vastly inferior). Their software hasn't really improved all that much over decades--mostly they benefited from Moore's Law. They have made a point of stifling attempts at interoperability and open data exchange. They have bought lots of competitors mostly to just shut them down. I can go on and on. The EDA companies aren't quite Oracle--but they're not far off. This is one of the reasons why Google is getting pounded over this--maybe even unfairly. People in the field are super sensitive about bullshit claims from EDA vendors--we've heard them all and been on the receiving end of the stick far too many times. reply teleforce 5 hours agorootparent> The EDA companies aren't quite Oracle--but they're not far off. Agreed with most you mentioned but not about EDA companies are not worst than Oracle, at least Oracle is still supporting popular and useful open source projects namely MySQL, Virtualbox, etc. What open-source design software these EDA companies are supporting currently although most of their software originated from open source EDA software from UC Berkeley, etc? reply alexey-salmin 11 hours agorootparentprev> pretty much single-handedly responsible for the fact that the hardware startup ecosystem is moribund compared to that of the software startup ecosystem. This was the case before EDA companies even appeared. Hardware is hard because it's manufacturing. You can't \"iterate quickly\", every iteration costs millions of dollars and so does every mistake. reply bsder 10 hours agorootparent> Hardware is hard because it's manufacturing. You can't \"iterate quickly\", every iteration costs millions of dollars and so does every mistake. This is true for injection molding and yet we do that all the time in small businesses. A mask set for an older technology can be in the range of $50K-$100K. That's right about the same price as injection molds. The main difference is that Solidworks is about $25K while Cadence, et al, is about a megabuck. reply octoberfranklin 6 hours agorootparentprevand are pretty much single-handedly responsible for the fact that the hardware startup ecosystem is moribund Yes but not single-handedly -- it's them and the foundries, hand-in-hand. No startup can compete with Synopsys because TSMC doesn't give out the true design rules to anybody smaller than Apple for finfet processes. Essentially their DRC+LVS software has become a DRM-encoded version of the design rule manual. reply throwup238 12 hours agoparentprev> if it worked, they wouldn't be sharing it and would simply be printing money by taking advantage of it. This is a fallacious argument. A better chip design process does not eliminate all other risks like product-market fit or the upfront cost of making masks or chronic mismanagement. reply lobochrome 13 hours agoparentprevAgreed, in particular on #2 Given infinite time and compute - maybe the approach is significantly better. But that’s just not practical. So unless you see dramatic shifts - no one is going to throw away proven results on your new approach because of the TTM penalty if it goes wrong. The EDA industry is (has to be) ultra conservative. reply throwaway2037 12 hours agorootparent> The EDA industry is (has to be) ultra conservative. What is special about EDA that requires it to be more conservative? reply achierius 4 hours agorootparentTaping out a chip is an incredibly expensive (7-8 figure) fixed cost. If the chips that come out have too many bugs (say because your PD tools missed up some wiring for 1 in 10,000 blocks) then that money is gone. If you're Intel this is enough to make people doubt the health of your firm; if you're a startup, you're just done. reply raverbashing 12 hours agoparentprevHonestly this does not compute > None of these \"AI\" tools have yet demonstrated the ability to classify \"This is datapath\", \"This is array logic\", \"This is random logic\". Sounds like a good objective, one that could be added to training parameters. Or maybe it isn't needed (AI can 'understand' some concepts without explicitly tagging) > If I have the compute power to run EDA tools with a couple of different random seeds, at least one run will likely be a couple percentage points better. Then do it?! How long does it actually take to run? I know EDA tools creators are bad at some kinds of code optimization (and yes, it's hard) but let's say for a company like Intel, if it takes 10 days to rerun a chip to get 1% better, that sounds like a worthy tradeoff. > I put this snake oil in the same category as \"financial advice\"--if it worked, they wouldn't be sharing it and would simply be printing money by taking advantage of it. Yeah I don't think you understood the problem here. Good financial advice is about balancing risks and returns. reply AtlasBarfed 15 hours agoprevHow the hell would you verify an AI-generated silicon design? Like, for a CPU, you want to be sure it behaves properly for the given inputs. Anyone remember that floating point error in, was it Pentium IIs or Pentium IIIs? I mean, I guess if the chip is designed for AI, and AIs are inherently nonguaranteed output/responses, then the AI chip design being nonguaranteed isn't any difference in nonguarantees. Unless it is... reply quadrature 14 hours agoparent> How the hell would you verify an AI-generated silicon design? I think you're asking a different question, but in the context of the OP researchers are exploring AI for solving deterministic but intractable problems in the field of chip design and not generating designs end to end. Here's an excerpt from the paper. \"The objective is to place a netlist graph of macros (e.g., SRAMs) and standard cells (logic gates, such as NAND, NOR, and XOR) onto a chip canvas, such that power, performance, and area (PPA) are optimized, while adhering to constraints on placement density and routing congestion (described in Sections 3.3.6 and 3.3.5). Despite decades of research on this problem, it is still necessary for human experts to iterate for weeks with the existing placement tools, in order to produce solutions that meet multi-faceted design criteria.\" The hope is that Reinforcement Learning can find solutions to such complex optimization problems. reply throwaway2037 12 hours agorootparent> Despite decades of research on this problem, it is still necessary for human experts to iterate for weeks with the existing placement tools, in order to produce solutions that meet multi-faceted design criteria. Ironically, this sounds a lot like building a bot to play StarCraft, which is exactly what AlphaStar did. I had no idea that EDA layout is still so difficult and manual in 2024. This seems like a very worth area of research. I am not an expert in AI/ML, but is the ultimate goal: Train on as many open source circuit designs as possible to build a base, then try to solve IC layouts problems via reinforcement learning, similar to AlphaStar. Finally, use the trained model to do inference during IC layout? reply lisper 15 hours agoparentprev> How the hell would you verify an AI-generated silicon design? The same way you verify a human-generated one. > Anyone remember that floating point error in, was it Pentium IIs or Pentium IIIs? That was 1994. The industry has come a long way in the intervening 30 years. reply gwervc 15 hours agoparentprevA well working CPU is probably beside the point. What's important now is for researchers to publish papers using or speaking about AI. Then executives and managers to deploy AI in their companies. Then selling AI PC (somehow, we are already at this step). Whatever the results are. Customers issues will be solved by using more AI (think chatbots) until morale improves. reply asveikau 13 hours agoparentprevThe famous FPU issue that I can think of was the original Pentium. reply twothreeone 17 hours agoprev [–] I get why Jeff would be pressed to comment on this, given he's credited on basically all of \"Google Brain\" research output. But saying \"they couldn't replicate it because they're idiots, therefore it's replicable\" is not a rebuttal, just bullying. Sounds like the critics struck a nerve and there's no good way for him to refute the replication problem his research apparently exhibits. reply danpalmer 17 hours agoparent> But saying \"they couldn't replicate it because they're idiots, therefore it's replicable\" is not a rebuttal, just bullying That's not an argument made in the linked tweet. His claim is \"they couldn't replicate it because they didn't follow the steps\", which seems like a very reasonable claim, regardless of the motivation behind making it. reply bayarearefugee 17 hours agorootparentAt the end of the day my question is simply why does anyone care about the drama over this one way or another? Either the research is as much of a breakthrough as is claimed and Google is about to pull way ahead of all these other \"idiots\" who can't replicate their method even when it is described to them in detail, or the research is flawed and overblown and not as effective as claimed. This seems like exactly the sort of question the market will quickly decide over the next couple of years and not worth arguing over. Why do a non-zero amount of people have seemingly religious beliefs about this topic on one side or the other? reply pclmulqdq 16 hours agorootparentThe reason Jeff Dean cares is that his team's improvement compared to standard EDA tools was marginal at best and may have overfitted to a certain class of chips. Thus, he is defending his research because it is not widely accepted. Open source code has been out for years and in that time the EDA companies have largely done their own ML-based approaches that do not match his. He attributes this not to failings in his own research but to the detractors at these companies not giving it a fair chance. The guys at EDA companies care because Google's result makes them look like idiots when you take the paper at face value, and does advance the state of the art a bit. They have been working hard for marginal improvements, and that some team of ML people can come in and make a big splash with something like this is offensive to them. Furthermore, the result is not that impressive and does not generalize enough to be useful to them (and competent teams at these companies absolutely have checked). The fact that the result is so minor is the reason that this is so contentious. reply choppaface 14 hours agorootparentThe result is minor AND Google spent a (relative) lot of money to achieve it (especially in the eyes of the new CFO). Jeff Dean is desperately trying to save the prestige of the research (in a very insular, Google-y way) because he wants to save the 2017-era economically-not-viable blue sky culture where Tensorflow & the TPU flourished and the transformer was born. But the reality is that Google’s core businesses are under attack (anti-trust, Jedi Blue etc), the TPU now has zero chance versus NVidia, and Google is literally no longer growing ads. His financing is about to pop in the next 1-2 years. https://sparktoro.com/blog/is-google-losing-search-market-sh... reply throwaway2037 12 hours agorootparentprevOne constant that I see on HN: they love drama and love to tear down a winner, presumably over jealousy. reply joatmon-snoo 12 hours agorootparentprev> This seems like exactly the sort of question the market will quickly decide over the next couple of years and not worth arguing over. Discussions like this are _how_ the market decides whether or not this achievement is real or not. reply refulgentis 17 hours agorootparentprev> why does anyone care n.b. you're on a social news site > pull way ahead of all these other \"idiots\" Pulling way ahead sounds sufficient, not necessary. Can we prove it's not the case? Let's say someone says that's why Gemini inference is so cheap. Can we show that's wrong? > \"idiots\" ? reply bsder 15 hours agorootparentprev> Why do a non-zero amount of people have seemingly religious beliefs about this topic on one side or the other? Because lots of engineers are being told by managers \"Why aren't we using that tool?\" and a bunch of engineers are stuck saying \"Because it doesn't actually work.\" aka \"Google is lying through their teeth.\" to which the response is \"Oh, so you know better than Google?\" to which the reponse is \"Yeah, actually, I fucking do. Now piss off and let me finish timing closure this goddamn block that is already 6 weeks late.\" Now can you understand why this is a bit contentious? Marketing \"exaggerations\" from authority can cause huge amounts of grief. In my little corner of the world, I had to sit and defend against the lies that a startup with famous designers were putting out about power consumption while we were designing similar chips in the space. I had to go toe to toe with Senior VPs over it and I had to stand my ground and defend my team who analyzed things dead on. All this occurred in spite of the fact that they had no silicon. In addition, I knew the famous designers involved would happily lie straight to your face having worked with them before and having been lied straight to my face and having had to clean up the mess when they left the company. To be fair, it is also the only time I have had a Senior VP remember the kerfuffle and apologize when said startup finally delivered silicon and not only were the real numbers not what they claimed they weren't even close to the ones we were getting. reply btilly 14 hours agorootparentAnd do you believe that that is what's happening in this case? If you have personal experience with Jeff Dean et al that you're willing to share, I'd be interested in hearing about it. From where I'm sitting it looks like, \"Google spent a fortune on deep learning, and got a small but real win. People who don't like Google failed to follow Google's recipe and got a large and easily replicated loss.\" It's not even clear that Google's approach is feasible right now for companies not named Google. It is not clear that it works on other classes of chip. It is not clear that the technique will grow beyond what Google already got. It is really not clear that anyone should be jumping on this. But there is a world of difference between that, and concluding that Google is lying. reply bsder 12 hours agorootparent> From where I'm sitting it looks like, \"Google spent a fortune on deep learning, and got a small but real win. People who don't like Google failed to follow Google's recipe and got a large and easily replicated loss.\" From where I'm sitting it looks like Google cooked the books maximally, barely beat humans let alone state of the art algorithms, published a crappy article in Nature because it would never have passed editorial muster at something like DAC or an IEEE journal and now have to browbeat other people who are calling them out on it. And that's the best interpretation we can cough up. I'll go further, we don't even have any raw data that says that they actually did beat the humans. Some of the humans I know who run P&R are REALLY good at what they do. The data could be completely made up. Given how much scientific fraud has come out lately, I'm amazed at the number of people defending Google on this. Where I'm from, we call what Google is doing both \"lying\" and \"bullying\". Look, Google can easily defuse this in all manner of ways. Publish their raw data. Run things on testbenches and benchmarks that the EDA tools vendors have been running on for years. Run things on the open source VLSI designs that they sponsored. What I suspect happened is that Google's AI group has gotten used to being able to make hyperbolic marketing claims which are difficult to verify. They then poked at place and route, failed, and published an article anyway because someone's promotion is tied to this. They expected that everybody would swallow their glop just like every other time, be mostly ignored and the people involved can get their promotions and move on. Unfortunately, Google is shoveling bullshit around something that has objective answers; real money is at stake; and they're getting rightfully excoriated for it. Whoops. reply btilly 12 hours agorootparentLook, either the follow-up article did pretraining or not. Jeff Dean is claiming that the importance of pretraining was mentioned 37 times and the follow-up didn't do it. That sounds easy to verify. Likewise the importance of spending 20x as much money on the training portion seems easy to verify, and significant. That they would fail to properly test against industry standard workbenches seems reasonable to me. This is a bunch of ML specialists who know nothing about chip design. Their background is beating everyone at Go and setting a new state of the art for protein folding, and not chip design. If you dismiss those particular past accomplishments as hyperbolic marketing, that's your decision. But you aren't going to find a lot of people in these parts who agree with you. If you think that those were real, but that a bunch of more recent accomplishments are BS, I haven't been following closely enough to have an opinion. The stuff that crossed my radar since AlphaFold is mostly done at places like OpenAI, and not Google. Regardless, the truth will out. And what Google is claiming for itself here really isn't all that impressive. reply 1024core 17 hours agoparentprev> they couldn't replicate it because they're idiots If they did not follow the steps to replicate (pre-training, using less compute, etc.) and then failed, so what's wrong with calling out the flaws in their attempted \"replication\"? reply twothreeone 17 hours agorootparentIt's not a value judgement, just doesn't help his case at all. He'd need to counter the replication problem, but apparently that's not an option. Instead, he's making people who were unable to replicate it look bad, which actually strengthens their criticism. reply skybrian 17 hours agorootparentI don't know how you rebut a flawed paper without making its authors look bad? That would be a general-purpose argument against criticizing papers. Actually, people should criticize flawed papers. That's how science works! When you publish scientific papers, you should expect criticism if there's something that doesn't look right. The only way to avoid that is to get critical feedback before publishing the paper, and it's not always possible, so then the scientific debate happens in public. reply twothreeone 16 hours agorootparentThe situation here is different though.. If I'm making an existence claim by demonstrating a constructive argument and then being criticized for it, the most effective response to that critique would be a second, alternative construction, not attacking the critic's argument. After all, I'm the one claiming existence.. the burden of proof is on me, not my critics. reply skybrian 16 hours agorootparentI don't know which argument is more constructive, though? Both teams reported what they did. They got different results. Figuring out why is the next step, and pointing out that they did different things seems useful. Though, the broader question is how useful the results of the original paper are to other people who might do the same thing. reply xpe 16 hours agorootparentprev> But saying \"they couldn't replicate it because they're idiots, therefore it's replicable\" is not a rebuttal, just bullying. > It's not a value judgement, just doesn't help his case at all. Calling it \"bullying\" looks like a value judgment to me. Am I missing something? To me, Dean's response is quite sensible, particularly given his claims the other papers made serious mistakes and have potential conflicts of interest. reply twothreeone 16 hours agorootparentI'm not saying \"Bullying is bad and bullies are bad people\", that would be a value judgement. I'm saying bullying is the strictly worse strategy for strengthening his paper's claims in this scenario. The better strategy would be to foster an environment in which people can easily replicate your claims. reply xpe 16 hours agorootparentI think for most people the word “bullying” has a value judgment built-in. reply xpe 16 hours agorootparentprevAre you suggesting Dean take a different approach in his response? Are you saying it was already too late given the environment? (I’m also not sure I know what you mean by environment here.) reply xpe 16 hours agorootparentprevIn a perfect world, making a paper easier to replicate has advantages, sure. (But it also has costs.) Second, even a healthy environment can be undermined by lack of skills or resources, intellectual dishonesty, or conflicts of interest. reply danielmarkbruce 15 hours agorootparentprevthey have open source code. reply johnfn 16 hours agoparentprev> \"they couldn't replicate it because they're idiots, therefore it's replicable\" Where does it say that? Dean outlines explicit steps that the authors missed in the tweet. reply jsnell 17 hours agoparentprevIt's a good thing that he didn't say that, then. The tweet just says that the reproduction attempt didn't didn't actually follow the original methodology. There is no claim that the authors of the replication attempt were \"idiots\" or anything similar, you just made that up. The obviously fallacious logic in \"they couldn't replicate it ..., therefore it's replicable\" is also a total fabrication on your part. reply twothreeone 16 hours agorootparentA Google Nature Paper has not been replicated for over 3 years, but I'm the one fabricating stuff :D Making a novel claim implies its *_claimed_ replicability. \"You did not follow the steps\" is calling them idiots. The only inference I made is that he's pressed to comment. He could have said nothing.. instead he's lashing out publicly, because other people were unable to replicate it. If there's no problem replicating the work, why hasn't that happend? Any other author would be worried if a publication about their work were saying \"it's not replicable\" and trying their best to help replicate it.. but somehow that doesn't apply to him. reply griomnib 16 hours agorootparent“You can only validate my results if you have an entire Google data center worth of compute available. Since you don’t, you can’t question us.” reply jeffbee 16 hours agorootparentWe're actually talking about the difference between Cheng using 8 GPUs and 2 CPUs while Google used 16 GPUs and 40 CPUs. These are under-your-desk levels of resources. Cheng et al authors are all affiliated with UCSD which owns the Expanse supercomputer which is orders of magnitude larger than what you would need to reproduce the original work. Cheng et al does not explain why they used fewer resources. reply griomnib 16 hours agorootparentThat’s a fair complaint then. reply phonon 14 hours agorootparentNo it's not. They ran it longer instead. reply jeffbee 11 hours agorootparentThe 2022 paper pretty explicitly says that runtime is not a substitute. They say their best result \"can only be achieved in our 8-GPU setup\". reply phonon 6 hours agorootparentI assume you mean Fig. 6 here?[0] But that was explicitly limited to 8 hours for all setups. Do they have another paper that shows that you can't increase the number of hours of a smaller GPU setup to compensate? [0]https://dl.acm.org/doi/pdf/10.1145/3505170.3511478 reply MicolashKyoka 14 hours agoparentprevif they are idiots and couldn't replicate it, it's worth saying it. better that than sugarcoating idiocy until it harms future research. reply griomnib 16 hours agoparentprevI love how he’s claiming bias due to his critic’s employer. As though working for Google has no conflicts? A company that is desperately hyping each and every “me too” AI development to juice the stock price? Jeff drank so much kool aid he forget what water is. reply jeffbee 16 hours agorootparentHe's criticizing Markov for not disclosing the conflict, not for the conflict itself. Hiding your affiliation in a scientific publication is far outside the norms of science, and they should be criticized for that. The publication we are discussing — \"That Chip Has Sailed\" — dismisses Markov in a few paragraph and spends the bulk its arguments on Cheng. reply griomnib 16 hours agorootparentI know the norms of science, I also know norms of present day Google. Nobody from Google should have the gall to accuse others of anything. reply make3 16 hours agoparentprev\"they couldn't replicate it because they're idiots, therefore it's replicable\" That's literally not what he says though. He says, \"they didn't replicate it so their conclusions are invalid\", which is a completely different thing than what you're accusing him of, and is valid. reply iamnotafraid 16 hours agoparentprev [–] Interesting point, I give u this reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Jeff Dean addressed criticisms regarding Google's AlphaChip, noting that critics did not adhere to essential replication steps, like pre-training.- The debate questions whether AlphaChip's performance is on par with existing tools, with some suggesting Google's claims may be exaggerated.- This discussion underscores tensions between Google and the Electronic Design Automation (EDA) industry, involving accusations of bias and concerns about the replicability of Google's research."
    ],
    "points": 178,
    "commentCount": 109,
    "retryCount": 0,
    "time": 1733012884
  }
]
