[
  {
    "id": 42240678,
    "title": "Amazon S3 Adds Put-If-Match (Compare-and-Swap)",
    "originLink": "https://aws.amazon.com/about-aws/whats-new/2024/11/amazon-s3-functionality-conditional-writes/",
    "originBody": "Amazon S3 adds new functionality for conditional writes Posted on: Nov 25, 2024 Amazon S3 can now perform conditional writes that evaluate if an object is unmodified before updating it. This helps you coordinate simultaneous writes to the same object and prevents multiple concurrent writers from unintentionally overwriting the object without knowing the state of its content. You can use this capability by providing the ETag of an object using S3 PutObject or CompleteMultipartUpload API requests in both S3 general purpose and directory buckets. Conditional writes simplify how distributed applications with multiple clients concurrently update data across shared datasets. Similar to using the HTTP if-none-match conditional header to check for the existence of an object before creating it, clients can now perform conditional-write checks on an object’s Etag, which reflects changes to the object, by specifying it via the HTTP if-match header in the API request. S3 then evaluates if the object's ETag matches the value provided in the API request before committing the write and prevents your clients from overwriting the object until the condition is satisfied. This new conditional header can help improve the efficiency of your large-scale analytics, distributed machine learning, and other highly parallelized workloads by reliably offloading compare and swap operations to S3. This new conditional-write functionality is available at no additional charge in all AWS Regions. You can use the AWS SDK, API, or CLI to perform conditional writes. To learn more about conditional writes, visit the S3 User Guide.",
    "commentLink": "https://news.ycombinator.com/item?id=42240678",
    "commentBody": "Amazon S3 Adds Put-If-Match (Compare-and-Swap) (amazon.com)468 points by Sirupsen 20 hours agohidepastfavorite145 comments torginus 11 hours agoAh so its not only me that uses AWS primitives for hackily implementing all sorts of synchronization primitives. My other favorite pattern is implementing a pool of workers by quering ec2 instances with a certain tag in a stopped state and starting them. Starting the instance can succeed only once - that means I managed to snatch the machine. If it fails, I try again, grabbing another one. This is one of those things that I never advertised out of professional shame, but it works, its bulletproof and dead simple and does not require additional infra to work. reply belter 8 hours agoparentIf you use hourly billed machines...Sounds like the world most expensive semaphore :-) reply messe 6 hours agorootparentEC2 bills by the second. reply belter 5 hours agorootparentSome... \"Your Amazon EC2 usage is calculated by either the hour or the second based on the size of the instance, operating system, and the AWS Region where the instances are launched\" - https://repost.aws/knowledge-center/ec2-instance-hour-billin... https://aws.amazon.com/ec2/pricing/on-demand/ reply QuinnyPig 2 hours agorootparentMacOS instances appear to be the sole remaining exception since RHEL got on board. reply redeux 2 hours agorootparentThanks Corey. Always nice to get the TL;DR from an authority on the subject. reply torginus 6 hours agorootparentprevexcept we are actually using them :) reply belter 4 hours agorootparentJust don't call them before the hour and start a different one again.Because otherwise within the hour, you will be billed for hundreds of hours...If they are of the type billed by the hour.... reply williamdclt 7 hours agoparentprevWhat would you say would be the \"clean\" way to implement a pool of workers (using EC2 instances too)? reply Cthulhu_ 4 hours agorootparentAutoscaling and task queue based workloads, if my cloud theory is still relevant. reply twodave 3 hours agorootparentAgreed. Scaling based on the length of the queue, up to some maximum. reply giovannibonetti 3 hours agorootparentEven better, based on queue latency instead of length reply jcrites 51 minutes agorootparentThe single best metric I've found for scaling things like this is the percent of concurrent capacity that's in use. I wrote about this in a previous HN comment: https://news.ycombinator.com/item?id=41277046 Scaling on things like the length of the queue doesn't work very well at all in practice. A queue length of 100 might be horribly long in some workloads and insignificant in others, so scaling on queue length requires a lot of tuning that must be adjusted over time as the workload changes. Scaling based on percent of concurrent capacity can work for most workloads, and tends to remain stable over time even as workloads change. reply torginus 6 hours agorootparentprevnot sure, probably either an eks cluster with a job scheduler pod that creates jobs via the batch api. The scheduler pod might be replaced by a lambda. Another possibility is something cooked up with a lambda creating ec2 instances via cdk and the whole thing is kept track by a dynamodb table. the first one is probably cleaner (though I don't like it, it means that I need the instance to be a kubernetes node, and that comes with a bunch of baggage). reply ndjdjddjsjj 7 hours agorootparentprevetcd? reply _zoltan_ 10 hours agoparentprevthis actually sounds interesting. do you precreate the workers beforehand and then just keep them in a stopped state? reply torginus 10 hours agorootparentyeah. one of the goals was startup time, so It made sense to precreate them. In practice we never ran out of free machines (and if we did, I have a cdk script to make more), and inifnite scaling is a pain in the butt anyways due to having to manage subnets etc. Cost-wise we're only paying for the EBS volumes for the stopped instances which are like 4GB each, so they cost practically nothing, we spend less than a dollar per month for the whole bunch. reply zild3d 8 hours agorootparentWarm pools are a supported feature in AWS on auto scaling groups. Works as you're describing (have a pool of instances in stopped state ready to use, only pay for EBS volume if relevant) https://aws.amazon.com/blogs/compute/scaling-your-applicatio... reply rfoo 5 hours agorootparentprev> we spend less than a dollar per month for the whole bunch This does not change the point, I'm just being pedantic, but: 4GB of gp3 EBS takes $0.32 per month, assuming a 50% discount (not unusual), less than a dollar gives only... 6 instances. reply merb 8 hours agorootparentprevI always thought that stopped instances will cost money as well?! reply torginus 8 hours agorootparentYou're only paying for the hard drive (and the VPC stuff, if you want to be pedantic). The downside is that if you try to start your instance, they might not start if AWS doesn't have the capacity (rare but have seen it happen, particularly with larger, more exotic instances.) reply JoshTriplett 18 hours agoprevIt's also possible to enforce the use of conditional writes: https://aws.amazon.com/about-aws/whats-new/2024/11/amazon-s3... My biggest wishlist item for S3 is the ability to enforce that an object is named with a name that matches its hash. (With a modern hash considered secure, not MD5 or SHA1, though it isn't supported for those either.) That would make it much easier to build content-addressible storage. reply UltraSane 15 hours agoparentS3 has supported SHA-256 as a checksum algo since 2022. You can calculate the hash locally and then specify that hash in the PutObject call. S3 will calculate the hash and compare it with the hash in the PutObject call and reject the Put if they differ. The hash and algo are then stored in the object's metadata. You simply also use the SHA-256 hash as the key for the object. https://aws.amazon.com/blogs/aws/new-additional-checksum-alg... reply thayne 14 hours agorootparentUnfortunately, for a multi-part upload it isn't a hash of the total object, it is a hash of the hashes for each part, which is a lot less useful. Especially if you don't know how the file was partititioned during upload. And even if it was for the whole file, it isn't used for the ETag, so, so it can't be used for conditional PUTs. I had a use case where this looked really promising, then I ran into the multipart upload limitations, and ended up using my own custom metadata for the sha256sum. reply infogulch 1 hour agorootparentIf parts are aligned on a 1024-byte boundary and you know each part's start offset, it should be possible to use the internals of a BLAKE3 tree to get the final hash of all the parts together even as they're uploaded separately. https://github.com/C2SP/C2SP/blob/main/BLAKE3.md#13-tree-has... Edit: This is actually already implemented in the Bao project which exploits the structure of the BLAKE3 merkle tree structure to offer cool features like streaming verification and verifying slices of a file as I described above: https://github.com/oconnor663/bao#verifying-slices reply vdm 11 hours agorootparentprevWays to control etag/Additional Checksums without configuring clients: CopyObject writes a single part object and can read from a multipart object, as long as the parts total less than the 5 gibibyte limit for a single part. For future writes, s3:ObjectCreated:CompleteMultipartUpload event can trigger CopyObject, else defrag to policy size parts. Boto copy() with multipart_chunksize configured is the most convenient implementation, other SDKs lack an equivalent. For past writes, existing multipart objects can be selected from inventory filtering ETag column length greater than 32 characters. Dividing object size by part size might hint if part size is policy. reply vdm 8 hours agorootparent> Dividing object size by part size Correction: and also part quantity (parsed from etag) for comparison reply vdm 11 hours agorootparentprevDon't the SDKs take care of computing the multi-part checksum during upload? > To create a trailing checksum when using an AWS SDK, populate the ChecksumAlgorithm parameter with your preferred algorithm. The SDK uses that algorithm to calculate the checksum for your object (or object parts) and automatically appends it to the end of your upload request. This behavior saves you time because Amazon S3 performs both the verification and upload of your data in a single pass. https://docs.aws.amazon.com/AmazonS3/latest/userguide/checki... reply tedk-42 8 hours agorootparentIt does and has a good default. An issue I've come across though is you have the file locally and you want to check the e-tag value - you'll have to do this locally first and then compare the value to the S3 stored object. reply vdm 8 hours agorootparenthttps://github.com/peak/s3hash It would be nice if this got updated for Additional Checksums. reply josnyder 11 hours agoparentprevWhile it can't be done server-side, this can be done straightforwardly in a signer service, and the signer doesn't need to interact with the payloads being uploaded. In other words, a tiny signer can act as a control plane for massive quantities of uploaded data. The client sends the request headers (including the x-amz-content-sha256 header) to the signer, and the signer responds with a valid S3 PUT request (minus body). The client takes the signer's response, appends its chosen request payload, and uploads it to S3. With such a system, you can implement a signer in a lambda function, and the lambda function enforces the content-addressed invariant. Unfortunately it doesn't work natively with multipart: while SigV4+S3 enables you to enforce the SHA256 of each individual part, you can't enforce the SHA256 of the entire object. If you really want, you can invent your own tree hashing format atop SHA256, and enforce content-addressability on that. I have a blog post [1] that goes into more depth on signers in general. [1] https://josnyder.com/blog/2024/patterns_in_s3_data_access.ht... reply JoshTriplett 3 hours agorootparentThat's incredibly interesting, thank you! That's a really creative approach, and it looks like it might work for me. reply texthompson 17 hours agoparentprevThat's interesting. Would you want it to be something like a bucket setting, like \"any time an object is uploaded, don't let an object write complete unless S3 verifies that a pre-defined hash function (like SHA256) is called to verify that the object's name matches the object's contents?\" reply BikiniPrince 16 hours agorootparentYou can already put with a sha256 hash. If it fails it just returns an error. reply cmeacham98 18 hours agoparentprevIs there any reason you can't enforce that restriction on your side? Or are you saying you want S3 to automatically set the name for you based on the hash? reply JoshTriplett 17 hours agorootparent> Is there any reason you can't enforce that restriction on your side? I'd like to set IAM permissions for a role, so that that role can add objects to the content-addressible store, but only if their name matches the hash of their content. > Or are you saying you want S3 to automatically set the name for you based on the hash? I'm happy to name the files myself, if I can get S3 to enforce that. But sure, if it were easier, I'd be thrilled to have S3 name the files by hash, and/or support retrieving files by hash. reply mdavidn 16 hours agorootparentI think you can presign PutObject calls that validate a particular SHA-256 checksum. An API endpoint, e.g. in a Lambda, can effectively enforce this rule. It unfortunately won’t work on multipart uploads except on individual parts. reply UltraSane 15 hours agorootparentThe hash of multipart uploads is simply the hash of all the part hashes. I've been able to replicate it. reply thayne 14 hours agorootparentprevBut in order to do that you need to already know the contents of the file. I suppose you could have some API to request a signed url for a certain hash, but that starts getting complicated, especially if you need support for multi-part uploads, which you probably do. reply JoshTriplett 11 hours agorootparentprevUnfortunately, last I checked, the list of headers you're allowed to enforce for pre-signing does not include the hash. reply anotheraccount9 17 hours agoparentprevCould you use a meta field from the object and save the hash in it, running a compare from it? reply jiggawatts 18 hours agoparentprevThat will probably never happen because of the fundamental nature of blob storage. Individual objects are split into multiple blocks, each of which can be stored independently on different underlying servers. Each can see its own block, but not any other block. Calculating a hash like SHA256 would require a sequential scan through all blocks. This could be done with a minimum of network traffic if instead of streaming the bytes to a central server to hash, the hash state is forwarded from block server to block server in sequence. Still though, it would be a very slow serial operation that could be fairly chatty too if there are many tiny blocks. What could work would be to use a Merkle tree hash construction where some of subdivision boundaries match the block sizes. reply texthompson 17 hours agorootparentWhy would you PUT an object, then download it again to a central server in the first place? If a service is accepting an upload of the bytes, it is already doing a pass over all the bytes anyway. It doesn't seem like a ton of overhead to calculate SHA256 in the 4092-byte chunks as the upload progresses. I suspect that sort of calculation would happen anyways. reply willglynn 16 hours agorootparentYou're right, and in fact S3 does this with the `ETag:` header… in the simple case. S3 also supports more complicated cases where the entire object may not be visible to any single component while it is being written, and in those cases, `ETag:` works differently. > * Objects created by the PUT Object, POST Object, or Copy operation, or through the AWS Management Console, and are encrypted by SSE-S3 or plaintext, have ETags that are an MD5 digest of their object data. > * Objects created by the PUT Object, POST Object, or Copy operation, or through the AWS Management Console, and are encrypted by SSE-C or SSE-KMS, have ETags that are not an MD5 digest of their object data. > * If an object is created by either the Multipart Upload or Part Copy operation, the ETag is not an MD5 digest, regardless of the method of encryption. If an object is larger than 16 MB, the AWS Management Console will upload or copy that object as a Multipart Upload, and therefore the ETag will not be an MD5 digest. https://docs.aws.amazon.com/AmazonS3/latest/API/API_Object.h... reply danielheath 16 hours agorootparentprevS3 supports multipart uploads which don’t necessarily send all the parts to the same server. reply texthompson 16 hours agorootparentWhy does it matter where the bytes are stored at rest? Isn't everything you need for SHA-256 just the results of the SHA-256 algorithm on every 4096-byte block? I think you could just calculate that as the data is streamed in. reply Dylan16807 10 hours agorootparent> Isn't everything you need for SHA-256 just the results of the SHA-256 algorithm on every 4096-byte block? No, you need the hash of the previous block before you can start processing the next block. reply jiggawatts 16 hours agorootparentprevThe data is not necessarily \"streamed\" in! That's a significant design feature to allow parallel uploads of a single object using many parts (\"blocks\"). See: https://docs.aws.amazon.com/AmazonS3/latest/API/API_CreateMu... reply flakes 16 hours agorootparentprevYou have just re-invented IPFS! https://en.m.wikipedia.org/wiki/InterPlanetary_File_System reply losteric 17 hours agorootparentprevWhy does the architect of blob storage matter? The hash can be calculated as data streams in for the first write, before data gets dispersed into multiple physically stored blocks. reply willglynn 17 hours agorootparentIt is common to use multipart uploads for large objects, since this both increases throughput and decreases latency. Individual part uploads can happen in parallel and complete in any sequence. There's no architectural requirement that an entire object pass through a single system on either S3's side or on the client's side. reply Salgat 16 hours agorootparentprevIsn't that the point of the metadata? Calculate the hash ahead of time and store it in the metadata as part of the atomic commit for the blob (at least for S3). reply Sirupsen 18 hours agoprevTo avoid any dependencies other than object storage, we've been making use of this in our database (turbopuffer.com) for consensus and concurrency control since day one. Been waiting for this since the day we launched on Google Cloud Storage ~1 year ago. Our bet that S3 would get it in a reasonable time-frame worked out! https://turbopuffer.com/blog/turbopuffer reply amazingamazing 16 hours agoparentInteresting that what’s basically an ad is the top comment - it’s not like this is open source or anything - can’t even use it immediately (you have to apply for access). Totally proprietary. At least elasticsearch is APGL, saying nothing of open search which also supports use of S3 reply viraptor 15 hours agorootparentSomeone made an informed technical bet that worked out. Sounds like HN material to me. (Also, is it really a useful ad if you can't easily use the product?) reply amazingamazing 15 hours agorootparentWorked out how? There’s no implementation. It’s just conjecture. reply viraptor 8 hours agorootparentIt's right there: > Our bet that S3 would get it in a reasonable time-frame worked out! reply amazingamazing 7 hours agorootparentHow? This is a technical forum. Unless you’re saying any consumer of S3 can now spam links to their product on this thread with impunity. (Hey maybe they’re using cas). reply richardlblair 6 hours agorootparentOh look, someone is mad on the internet about something silly. reply hedora 15 hours agorootparentprevPretty much all other S3 implementations (including open source ones) support this or equivalent primitives, so this is great for interoperability with existing implementations. reply ramraj07 12 hours agorootparentprevNo one owes anyone open source. If they can make the business case work or if it works in their favor, sure. reply jrochkind1 4 hours agorootparentprevI don't mind hearing another developer's use case for this feature, even if it's commercial proprietary software. It's no longer top comment, which is fine. reply jauntywundrkind 14 hours agorootparentprevhttps://github.com/slatedb/slatedb will, I expect, use this at some point. Object backed DB, which is open source. reply benesch 12 hours agorootparentYes! I’m actively working on it, in fact. We’re waiting on the next release of the Rust `object_store` crate, which will bring support for S3’s native conditional puts. If you want to follow along: https://github.com/slatedb/slatedb/issues/164 reply CobrastanJorji 11 hours agoparentprevI'm glad that bet worked out for you, but what made you think one year ago that S3 would introduce it soon that was untrue for the previous 15 years? reply 1a527dd5 19 hours agoprevBe still my beating heart. I have lived to see this day. Genuinely, we've wanted this for ages and we got half way there with strong consistency. reply ncruces 19 hours agoparentMight finally be possible to do this on S3: https://pkg.go.dev/github.com/ncruces/go-gcp/gmutex reply phrotoma 7 hours agorootparentHuh. Does this mean that the AWS terraform provider could implement state locking without the need for a DDB table the way the GCP provider does? reply arianvanp 7 hours agorootparentCorrect reply paulddraper 19 hours agoparentprevSo....given CAP, which one did they give up reply nimih 1 hour agorootparentBased on my general experience with S3, they jettisoned A years ago (or maybe never had it). reply the_arun 17 hours agorootparentprevI thought they have implemented Optimistic locking now to coordinate concurrent writes. How does it change anything in CAP? reply paulddraper 27 minutes agorootparentThe C stands for Consistency. reply moralestapia 18 hours agorootparentprevA tiny bit of availability, unnoticeable at web scale. reply johnrob 19 hours agorootparentprevI’d wager that the algorithm is slightly eager to throw a consistency error if it’s unable to verify across partitions. Since the caller is naturally ready for this error, it’s likely not a problem. So in short it’s the P :) reply alanyilunli 18 hours agorootparentShouldn't that be the A then? Since the network partition is still there but availability is non-guaranteed. reply johnrob 18 hours agorootparentYes, definitely. Good point (I was knee jerk assuming the A is always chosen and the real “choice” is between C and P). reply btown 17 hours agorootparenthttps://tqdev.com/2024-the-p-in-cap-is-for-performance is a really interesting take on this as a response to https://blog.dtornow.com/the-cap-theorem.-the-bad-the-bad-th... - essentially, the only way to get CA is if you're willing to say that every request will succeed eventually, but it might take an unbounded amount of time for partitions to heal, and you have to be willing to wait indefinitely for that to happen. Which can indeed make sense for asynchronous messaging, but not for real-time applications as we think about them in the modern day. In practice, if you're talking about CAP for high-performance systems, you're choosing either CP or AP. reply rhaen 17 hours agorootparentprevWell, P isn't really much of a choice, I don't think you can opt out of acts of god. reply fwip 17 hours agorootparentYou can design to minimize P, though. For instance, if you have all the services running on the same physical box, and make people enter the room to use it instead of over the Internet, \"partition\" becomes much less likely. (This example is a bit silly.) But you're right, if you take a broad view of P, the choice is really between consistency and availability. reply maglite77 15 hours agoprevNoting that Azure Blob storage supports e-tag / optimistic controls as well (via If-Match conditions)[1], how does this differ? Or is it the same feature? [1]: https://learn.microsoft.com/en-us/azure/storage/blobs/concur... reply simonw 15 hours agoparentIt's the same feature. Google Cloud Storage has it too: https://cloud.google.com/storage/docs/request-preconditions#... reply CubsFan1060 18 hours agoprevI feel dumb for asking this, but can someone explain why this is such a big deal? I’m not quite sure I am grokking it yet. reply lxgr 18 hours agoparentIf my memory of parallel algorithms class serves me right, you can build any synchronization algorithm on top of compare-and-swap as an atomic primitive. As a (horribly inefficient, in case of non-trivial write contention) toy example, you could use S3 as a lock-free concurrent SQLite storage backend: Reads work as expected by fetching the entire database and satisfying the operation locally; writes work like this: - Download the current database copy - Perform your write locally - Upload it back using \"Put-If-Match\" and the pre-edit copy as the matched object. - If you get success, consider the transaction successful. - If you get failure, go back to step 1 and try again. reply CobrastanJorji 11 hours agoparentprevIt is often very important to know, when you write an object, what the previous state was. Say you sold plushies and you had 100 plushies in a warehouse. You create a file \"remainingPlushies.txt\" that stores \"100\". If somebody buys a plushie, you read the file, and if it's bigger than 0, you subtract 1, write the new version of the file, and okay the sale. Without conditional writes, two instances of your application might both read \"100\", both subtract 1, and both write \"99\". If they checked the file afterward, both would think everything was fine. But things aren't find because you've actually sold two. The other cloud storage providers have had these sorts of conditional write features since basically forever, and it's always been really weird that S3 has lacked them. reply Sirupsen 18 hours agoparentprevThe short of it is that building a database on top of object storage has generally required a complicated, distributed system for consensus/metadata. CAS makes it possible to build these big data systems without any other dependencies. This is a win for simplicity and reliability. reply CubsFan1060 18 hours agorootparentThanks! Do they mention when the comparison is done? Is it before, after, or during an upload? (For instance, if I have a 4tb file in a multi part upload, would I only know it would fail as soon as the whole file is uploaded?) reply timmg 16 hours agorootparent(I assume) it will fail if the eTag doesn't match -- the instance it got the header. The main point of it is: I have an object that I want to mutate. I think I have the latest version in memory. So I update in memory and upload it to S3 with the eTag of the version I have and tell it to only commit if that is the latest version. If it \"fails\", I re-download the object, re-apply the mutation, and try again. reply poincaredisk 17 hours agorootparentprevI imagine, for it to make sense, that the comparison is done at the last possible moment, before atomically swapping the file contents. reply lxgr 16 hours agorootparentPractically, they could do both: Do an early reject of a given POST in case the ETag does not match, but re-validate this just before swapping out the objects (and committing to considering the given request as the successful one globally). That said, I'm not sure if common HTTP libraries look at response headers before they're done posting a response body, or if that's even allowed/possible in HTTP? It seems feasible at a first glance with chunked encoding, at least. Edit: Upon looking a bit, it seems that informational response codes, e.g. 100 (Continue) in combination with Expect 100-continue in the requests, could enable just that and avoid an extra GET with If-Match. reply Nevermark 16 hours agorootparentprevI can imagine it might be useful to make this a choice for databases with high frequency small swaps and occasional large ones. 1) default, load-compare-&-swap for small fast load/swaps. 2) optional, compare-load-&-swap to allow a large load to pass its compare, and cut in front of all the fast small swap that would otherwise create an un-hittable moving target during its long loads for its own compare. 3) If the load itself was stable relative to the compare, then it could be pre-loaded and swapped into a holding location, followed by as many fast compare-&-swaps as needed to get it into the right location. reply jayd16 16 hours agoparentprevWhen you upload a change you can know you're not clobbering changes you never saw. reply ramraj07 12 hours agorootparentBrilliant single line that is better than every other description above. Kudos. reply papichulo2023 8 hours agorootparentprevI think is called write after write (WAW) if I remember correctly. reply koolba 20 hours agoprevThis combined with the read-after-write consistency guarantee is a perfect building block (pun intended) for incremental append only storage atop an object store. It solves the biggest problem with coordinating multiple writers to a WAL. reply IgorPartola 19 hours agoparentRename for objects and “directories” also. Atomic. reply ncruces 9 hours agoparentprevBoth this and read-after-write consistency is single object. So coordinating writes to multiple objects still requires… creativity. reply offmycloud 19 hours agoprevIf the default ETag algorithm for non-encrypted, non-multipart uploads in AWS is a plain MD5 hash, is this subject to failure for object data with MD5 collisions? I'm thinking of a situation in which an application assumes that different (possibly adversarial) user-provided data will always generate a different ETag. reply revnode 18 hours agoparentMD5 hash collisions are unlikely to happen at random. The defect was that you can make it happen purposefully, making it useless for security. reply aphantastic 16 hours agorootparentSure, but theoretically you could have a system where a distributed log of user generated content is built via this CAS//MD5 primitive. A malicious actor could craft the data such that entries are dropped. reply CobrastanJorji 11 hours agoparentprevWith Google Cloud Storage, you can solve this by conditionally writing based on the \"generation number\" of the object, which always increases with each new write, so you can know whether the object has been overwritten regardless of its contents. I think Azure also has an equivalent. reply UltraSane 15 hours agoparentprevThe default Etag is used to detect bit errors and and MD5 is fine for that. S3 does support using SHA256 instead. reply spprashant 5 hours agoprevI had no idea people rely on S3 beyond dumb storage. It almost feels like people are trying to build out a distributed OLAP database in the reverse direction. reply amne 3 hours agoparent1. SELECT ... INTO OUTFILE S3 2. glue jobs to partition by some columns reporting uses 3. query with athena 4. ??? 5. profit (celebrate reduced cost) This thing costs couple $ a month for ~500gb of data. Snowflake wanted crazy amounts of money for the same thing. reply ipython 17 hours agoprevI can't wait to see what abomination Cory Quinn can come up with now given this new primitive! (see previous work abusing Route53 as a database: https://www.lastweekinaws.com/blog/route-53-amazons-premier-...) reply amazingamazing 15 hours agoprevIronically with this and lambda you could make a serverless sqlite by mapping pages to objects, using http range reads to read the db and lambda to translate queries to the writes in the appropriate pages via cas. Prior to this it would require a server to handle concurrent writers, making the whole thing a nonstarter for “serverless”. Too bad performance would be terrible without a caching layer (ebs). reply captn3m0 15 hours agoparentFor read heavy workloads, you could cache the results at cloudfront. Maybe we will someday see Wordpress-on-Lambda-to-Sqlite-over-S3. reply m_d_ 15 hours agoprevs3fs's https://github.com/fsspec/s3fs/pull/917 was in response to the IfNoneMatch feature from the summer. How would people imagine this new feature being surfaced in a filesystem abstraction? reply vytautask 12 hours agoprevAn open-source implementation of Amazon S3 - MinIO has had it for almost two years (relevant post: https://blog.min.io/leading-the-way-minios-conditional-write...). Strangely, Amazon is catching up just now. reply aseipp 58 minutes agoparentIt's not surprising at all. The scale of AWS, in particular S3, is nearly unfathomable, and the kind of solutions they need for \"simple\" things are totally different at that size. S3 was doing 1.1million requests a second back in 2013.[1] I wouldn't be surprised if they saw over 100mil/req/sec globally by now. That's 100 million requests a second that need strong read-your-write consistency and atomicity at global scale. The number of pieces they had to move into place for this to happen is probably quite the engineering tale. [1] https://aws.amazon.com/blogs/aws/amazon-s3-two-trillion-obje... reply topspin 12 hours agoparentprevThat's not \"strange\" to me. Object storage has been a long time coming, and it's still being figured out: the entirely typical process of discovering useful and feasible primitives that expand applicability to more sophisticated problems. This is obviously going occur first in smaller and/or younger, more agile implementations, whereas AWS has the problem of implementing this at pretty much the largest conceivable scale with zero risk. The lag is, therefore, entirely unsurprising. reply sillysaurusx 19 hours agoprevFinally. GCP has had this for a long time. Years ago I was surprised S3 didn’t. reply ncruces 19 hours agoparentGCS is just missing x-amz-copy-source-range in my book. Can we have this Google? … Please? reply mannyv 17 hours agoparentprevGCP still doesn't have triggers out of beta last time i checked (which was a while ago). reply BrandonY 11 hours agorootparentWe do have Cloud Run Functions that trigger on Cloud Storage events, as well as Cloud Pub/Sub notifications for the same. Is there a specific bit of functionality you're looking for? reply fragmede 17 hours agorootparentprevGmail was in beta for five years, I don't think that label really means anything. reply UltraSane 15 hours agorootparentIt means that Google doesn't want to offer an SLA reply sitkack 14 hours agorootparentNot that it matters. It just changes the volume and timing of \"I believe I did bob\" reply lttlrck 11 hours agoprevIsn't this compare-and-set rather than compare-and-swap? reply wanderingmind 17 hours agoprevDoes this mean, in theory we will be able to manage multiple concurrent writes/updates to s3 without having to use new solutions like Regatta[1] that was recently launched? https://news.ycombinator.com/item?id=42174204 reply huntaub 7 hours agoparentHere's how I would think about this. Regatta isn't the best way to add synchronization primitives to S3, if you're already using the S3 API and able to change your code. Regatta is most useful when you need a local disk, or a higher performance version of S3. In this case, the addition of these new primitives actually just makes Regatta work better for our customers -- because we get to achieve even stronger consistency. reply gravitronic 19 hours agoprevFirst thing I thought when I saw the headline was \"oh! I should tell Sirupsen\" reply stevefan1999 16 hours agoprevSo...are we closer to getting to use S3 as a...you guessed it...a database? With CAS, we are probably able to get a basic level of atomicity, and S3 itself is pretty durable, now we have to deal with consistency and isolation...although S3 branded itself as \"eventually consistent\"... reply gynther 3 hours agoparentS3 is strongly consistent since 4 years ago. https://aws.amazon.com/blogs/aws/amazon-s3-update-strong-rea... reply mr_toad 16 hours agoparentprevPeople who want all those features use something like Delta Lake on top of object storage. reply User23 16 hours agoparentprevThere was a great deal of interest in gossip protocols, eventual consistency, and such at Amazon in the mid oughts. So much so that they hired a certain Cornell professor along with the better part of his grad students to build out those technologies. reply rrr_oh_man 18 hours agoprevCould anybody explain for the uninitiated? reply msoad 18 hours agoparentIt ensures that when you try to upload (or “put”) a new version of a file, the operation only succeeds if the file on the server still has the exact version (ETag) you specify. If someone else has updated the file in the meantime, your upload is blocked to prevent overwriting their changes. This is especially useful in scenarios where multiple users or processes are working on the same data, as it helps maintain consistency and avoids accidental overwrites. This is using the same mechanism as HTTP's `If-None-Match` header so it's easier to implement/learn reply rrr_oh_man 9 hours agorootparentThank you! That was extremely helpful (and written in a way that is easy to understand)! reply anonymousDan 10 hours agoprevWould be interesting to understand how they've implemented it and they whether there is any perf impact on other API calls. reply vlovich123 17 hours agoprevI implemented that extension in R2 at launch IIRC. Thanks for catching up & helping move distributed storage applications a meaningful step forward. Intended sincerely. I'm sure adding this was non-trivial for a complex legacy codebase like that. reply dvektor 17 hours agoprev[rejected] error: failed to push some refs to remote repository Finally we can have this with s3 :) reply mdaniel 1 hour agoparentRelevant: https://github.com/awslabs/git-remote-s3#readme https://news.ycombinator.com/item?id=41887004 reply tonymet 19 hours agoprevgood example of how a simple feature on the surface (a header comparison) requires tremendous complexity and capacity on the backend. reply akira2501 19 hours agoparentS3 is rated as \"durable\" as opposed to \"best effort.\" It has lots of interesting guarantees as a result. reply tonymet 15 hours agorootparentAlso they are faithful to their consistency commitments reply thayne 14 hours agoprevNow if only you had more control over the ETag, so you could use a sha256 of the total file (even for multi-part uploads), or a version counter, or a global counter from an external system, or a logical hash of the content as opposed to a hash of the bytes. reply juggli 6 hours agoprevfinally reply londons_explore 7 hours agoprevSo we can now implement S3-as-RAM for a worldwide million-core linux VM? reply paulsutter 14 hours agoprevWhat’s amazing is that it took them so long to add these functions reply throwaway314155 20 hours agoprevnext [4 more] [flagged] earth2mars 19 hours agoparentWhat is stopping you not doing it now? I know Q is not good (hallucinates, slow, requires sign in) But it's wise to explain what your gripe is about than saying which you can always do. reply throwaway314155 19 hours agorootparentMy gripe was with the Explainer modal that covers the entire article upon visiting the site. reply ramon156 20 hours agoparentprevHonestly if it was fast and uninvasive, I wouldn't mind it at all reply grahamj 15 hours agoprevbender_neat.gif reply serbrech 14 hours agoprev [–] Why is standard etag support making the frontpage? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Amazon S3 introduces conditional writes, enabling updates only if an object remains unmodified, preventing concurrent overwrites by verifying the object's ETag.- This feature, akin to the HTTP if-none-match header, allows clients to use the if-match header to ensure ETag consistency before writing, enhancing efficiency for distributed applications.- Available at no extra cost across all AWS Regions, implementation can be done using the AWS SDK, API, or CLI, with further details in the S3 User Guide."
    ],
    "commentSummary": [
      "Amazon S3 has launched a new feature called Put-If-Match, which acts like a compare-and-swap operation, allowing conditional object writing based on version matching.- This feature improves synchronization and concurrency control, enabling developers to perform more complex operations, such as building databases on S3 without extra infrastructure.- The update brings Amazon S3 in line with similar features offered by other cloud storage services like Google Cloud Storage and Azure Blob Storage."
    ],
    "points": 468,
    "commentCount": 145,
    "retryCount": 0,
    "time": 1732572691
  },
  {
    "id": 42243500,
    "title": "Lies we tell ourselves to keep using Golang (2022)",
    "originLink": "https://fasterthanli.me/articles/lies-we-tell-ourselves-to-keep-using-golang",
    "originBody": "Home Log in fasterthanlime Apr 29, 2022 28 min #golang · #rant Lies we tell ourselves to keep using Golang 👋 This page was last updated ~3 years ago. Just so you know. In the two years since I've posted I want off Mr Golang's Wild Ride, it's made the rounds time and time again, on Reddit, on Lobste.rs, on HackerNews, and elsewhere. And every time, it elicits the same responses: You talk about Windows: that's not what Go is good at! (Also, who cares?) This is very one-sided: you're not talking about the good sides of Go! You don't understand the compromises Go makes. Large companies use Go, so it can't be that bad! Modelling problems \"correctly\" is too costly, so caring about correctness is moot. Correctness is a spectrum, Go lets you trade some for development speed. Your go-to is Rust, which also has shortcomings, so your argument is invalid. etc. There's also a vocal portion of commenters who wholeheartedly agree with the rant, but let's focus on unpacking the apparent conflict here. I'll first spend a short amount of time pointing out clearly disingenuous arguments, to get them out of the way, and then I'll move on to the fairer comments, addressing them as best I can. The author is a platypus When you don't want to hear something, one easy way to not have to think about it at all is to convince yourself that whoever is saying it is incompetent, or that they have ulterior motives. For example, the top comment on HackerNews right now starts like this: The author fundamentally misunderstands language design. As an impostor syndrome enthusiast, I would normally be sympathetic to such comments. However, it is a lazy and dismissive way to consider any sort of feedback. It doesn't take much skill to notice a problem. In fact, as developers get more and more senior, they tend to ignore more and more problems, because they've gotten so used to it. That's the way it's always been done, and they've learned to live with them, so they've stopped questioning it any more. Junior developers however, get to look at everything again with a fresh pair of eyes: they haven't learned to ignore all the quirks yet, so it feels uncomfortable to them, and they tend to question it (if they're made to feel safe enough to voice their concerns). This alone is an extremely compelling reason to hire junior developers, which I wish more companies would do, instead of banking on the fact that \"seniors can get up-to-speed with our current mess faster\". As it happens, I am not a junior developer, far from it. Some way or another, over the past 12 years, seven different companies have found an excuse to pay me enough money to cover rent and then some. I did, in fact, design a language all the way back in 2009 (when I was a wee programmer baby), focused mainly on syntactic sugar over C. At the time it was deemed interesting enough to warrant an invitation to OSCON (my first time in Portland Oregon, the capital of grunge, coffee, poor weather and whiteness), where I got to meet other young and not-so-young whippersnappers (working on Io, Ioke, Wren, JRuby, Clojure, D, Go, etc.) It was a very interesting conference: I'm still deeply ashamed by the presentation I gave, but I remember fondly the time an audience member asked the Go team \"why did you choose to ignore any research about type systems since the 1970s\"? I didn't fully understand the implications at the time, but I sure do now. I have since thoroughly lost interest in my language, because I've started caring about semantics a lot more than syntax, which is why I also haven't looked at Zig, Nim, Odin, etc: I am no longer interested in \"a better C\". But all of that is completely irrelevant. It doesn't matter who points out that \"maybe we shouldn't hit ourselves in the head with a rake repeatedly\": that feedback ought to be taken under advisement no matter who it comes from. Mom smokes, so it's probably okay One of the least effective way to shop for technologies (which CTOs, VPs of engineering, principals, senior staff and staff engineers need to do regularly) is to look at what other companies are using. It is a great way to discover technologies to evaluate (that or checking ThoughtWorks' Tech Radar), but it's far from enough. A piece from company X on \"how they used technology Y\", will very rarely reflect the true cost of adopting that technology. By the point the engineers behind the post have been bullied into filling out the company's tech blog after months of an uphill battle, the decision has been made, and there's no going back. This kind of blog doesn't lend itself to coming out and admitting that mistakes were made. It's supposed to make the company look good. It's supposed to attract new hires. It's supposed to help us stay relevant. Typically, scathing indictments of technologies come from individuals, who have simply decided that they, as a person, can afford making a lot of people angry. Companies typically cannot. There are some exceptions: Tailscale's blog is refreshingly candid, for example. But when reading articles like netaddr.IP: a new IP address type for Go, or Hey linker, can you spare a meg? you can react in different ways. You can be impressed, that very smart folks are using Go, right now, and that they have gone all the way to Davy Jones' Locker and back to solve complex problems that ultimately helps deliver value to customers. Or you can be horrified, as you realize that those complex problems only exist because Go is being used. Those complex problems would not exist in other languages, not even in C, which I can definitely not be accused of shilling for (and would not recommend as a Go replacement). A lot of the pain in the netaddr.IP article is caused by: Go not having sum types — making it really awkward to have a type that is \"either an IPv4 address or an IPv6 address\" Go choosing which data structures you need — in this case, it's the one-size-fits-all slice, for which you pay 24 bytes on 64-bit machines. Go not letting you do operator overloading, harkening back to the Java days where a == b isn't the same as a.equals(b) Go's lack of support for immutable data — the only way to prevent something from being mutated is to only hand out copies of it, and to be very careful to not mutate it in the code that actually has access to the inner bits. Go's unwillingness to let you make an opaque \"newtype\". The only way to do it is to make a separate package and use interfaces for indirection, which is costly and awkward. Unless you're out for confirmation bias, that whole article is a very compelling argument against using Go for that specific problem. And yet Tailscale is using it. Are they wrong? Not necessarily! Because their team is made up of a bunch of Go experts. As evidenced by the other article, about the Go linker. Because they're Go experts, they know the cost of using Go upfront, and they're equipped to make the decision whether or not it's worth it. They know how Go works deep down (something Go marketing pinky-swears you never need to worry about, why do you ask?), so if they hit edge cases, they can dive into it, fix it, and wait for their fix to be upstreamed (if ever). But chances are, this is not you. This is not your org. You are not Google either, and you cannot afford to build a whole new type system on top of Go just to make your project (Kubernetes) work at all. The good parts But okay - Tailscale's usage of Go is pretty out there still. Just like my 2020 piece about Windows raised an army of \"but that's not what Go is good for\" objections, you could dismiss Tailscale's posts as \"well that's on you for wanting to ship stuff on iOS / doing low-level network stuff\". Fair enough! Okay. Let's talk about what makes Go compelling. Go is a pretty good async runtime, with opinionated defaults, a state-of-the-art garbage collector with two knobs, and tooling that would make C developers jealous, if they bothered looking outside their bubble. This also describes Node.js from the very start (which is essentially libuv + V8), and I believe it also describes \"modern Java\", with APIs like NIO. Although I haven't checked what's happening in Java land too closely, so if you're looking for an easy inaccuracy to ignore this whole article, there you go: that's a freebie. Because the async runtime is core to the language, it comes with tooling that does make Rust developers jealous! I talk about it in Request coalescing in async Rust, for example. Go makes it easy to dump backtraces (stack traces) for all running goroutines in a way tokio doesn't, at this time. It is also able to detect deadlocks, it comes with its own profiler, it seemingly lets you not worry about the color of functions, etc. Go's tooling around package management, refactoring, cross-compiling, etc., is easy to pick up and easy to love — and certainly feels at first like a definite improvement over the many person-hours lost to the whims of pkg-config, autotools, CMake, etc. Until you reach some of the arbitrary limitations that simply do not matter to the Go team, and then you're on your own. All those and more explains why many, including me, were originally enticed by it: enough to write piles and piles of it, until its shortcomings have finally become impossible to ignore, by which point it's too late. You've made your bed, and now you've got to make yourself feel okay about lying in it. But one really good bit does not a platform make. The really convenient async runtime is not the only thing you adopted. You also adopted a very custom toolchain, a build system, a calling convention, a single GC (whether it works for you or not), the set of included batteries, some of which you CAN swap out, but the rest of the ecosystem won't, and most importantly, you adopted a language that happened by accident. I will grant you that caring too much about something is grounds for suspicion. It is no secret that a large part of what comes out of academia is woefully inapplicable in the industry at this time: it is easy to lose oneself in the abstract, and come up with convoluted schemes to solve problems that do not really exist for anyone else. I imagine this is the way some folks feel about Rust. But caring too little about something is dangerous too. Evidently, the Go team didn't want to design a language. What they really liked was their async runtime. And they wanted to be able to implement TCP, and HTTP, and TLS, and HTTP/2, and DNS, etc., on top of it. And then web services on top of all of that. And so they didn't. They didn't design a language. It sorta just \"happened\". Because it needed to be familiar to \"Googlers, fresh out of school, who probably learned some Java/C/C++/Python\" (Rob Pike, Lang NEXT 2014), it borrowed from all of these. Just like C, it doesn't concern itself with error handling at all. Everything is a big furry ball of mutable state, and it's on you to add ifs and elses to VERY CAREFULLY (and very manually) ensure that you do not propagate invalid data. Just like Java, it tries to erase the distinction between \"value\" and \"reference\", and so it's impossible to tell from the callsite if something is getting mutated or not: Go code import \"fmt\" type A struct {Value int } func main() {a := A{Value: 1}a.Change()fmt.Printf(\"a.Value = %d\", a.Value) } Depending on whether the signature for change is this: Go code func (a A) Change() {a.Value = 2 } Or this: Go code func (a *A) Change() {a.Value = 2 } ...the local a in main will either get mutated or not. And since, just like C and Java, you do not get to decide what is mutable and what is immutable (the const keyword in C is essentially advisory, kinda), passing a reference to something (to avoid a costly copy, for example) is fraught with risk, like it getting mutated from under you, or it being held somewhere forever, preventing it from being freed (a lesser, but very real, problem). Go fails to prevent many other classes of errors: it makes it easy to accidentally copy a mutex, rendering it completely ineffective, or leaving struct fields uninitialized (or rather, initialized to their zero value), resulting in countless logic errors. Taken in isolation, each of these and more can be dismissed as \"just a thing to be careful about\". And breaking down an argument to its smallest pieces, rebutting them one by one, is a self-defense tactic used by those who cannot afford to adjust their position in the slightest. Which makes perfect sense, because Go is really hard to move away from. Go is an island Unless you use cgo, (but cgo is not Go), you are living in the Plan 9 cinematic universe. The Go toolchain does not use the assembly language everyone else knows about. It does not use the linkers everyone else knows about. It does not let you use the debuggers everyone knows about, the memory checkers everyone knows about, or the calling conventions everyone else has agreed to suffer, in the interest of interoperability. Go is closer to closed-world languages than it is to C or C++. Even Node.js, Python and Ruby are not as hostile to FFI. To a large extent, this is a feature: being different is the point. And it comes with its benefits. Being able to profile the internals of the TLS and HTTP stacks the same way you do your business logic is fantastic. (Whereas in dynamic languages, the stack trace stops at OpenSSL). And that code takes full advantage of the lack of function coloring: it can let the runtime worry about non-blocking I/O and scheduling. But it comes at a terrible cost, too. There is excellent tooling out there for many things, which you cannot use with Go (you can use it for the cgo parts, but again, you should not use cgo if you want the Real Go Experience). All the \"institutional knowledge\" there is lost, and must be relearned from scratch. It also makes it extremely hard to integrate Go with anything else, whether it's upstream (calling C from Go) or downstream (calling Go from Ruby). Both these scenarios involve cgo, or, if you're unreasonably brave, a terrifying hack. Note: as of Go 1.13, binary-only packages are no longer supported Making Go play nice with another language (any other language) is really hard. Calling C from Go, nevermind the cost of crossing the FFI boundary, involves manual descriptor tracking, so as to not break the GC. (WebAssembly had the same problem before reference types!) Calling Go from anything involves shoving the whole Go runtime (GC included) into whatever you're running: expect a very large static library and all the operational burden of running Go code as a regular executable. After spending years doing those FFI dances in both directions, I've reached the conclusion that the only good boundary with Go is a network boundary. Integrating with Go is relatively painless if you can afford to pay the latency cost of doing RPC over TCP (whether it's a REST-ish HTTP/1 API, something like JSON-RPC, a more complicated scheme like GRPC, etc.). It's also the only way to make sure it doesn't \"infect\" your whole codebase. But even that is costly: you need to maintain invariants on both sides of the boundary. In Rust, one would typically reach for something like serde for that, which, combined with sum types and the lack of zero values, lets you make reasonably sure that what you're holding is what you think you're holding: if a number is zero, it was meant to be zero, it wasn't just missing. (All this goes out the window if you use a serialization format like protobuf, which has all the drawbacks of Go's type system and none of the advantages). That still leaves you with the Go side of things, where unless you use some sort of validation package religiously, you need to be ever vigilant not to let bad data slip in, because the compiler does nothing to help you maintain those invariants. And that brings us to the larger overall problem of the Go culture. All or nothing (so let's do nothing) I've mentioned \"leaving struct fields uninitialized\". This happens easily when you make a code change from something like this: Go code package main import \"log\" type Params struct {a int32 } func work(p Params) {log.Printf(\"Working with a=%v\", p.a) } func main() {work(Params{ a: 47,}) } To something like this: Go code package main import \"log\" type Params struct {a int32b int32 } func work(p Params) {log.Printf(\"Working with a=%v, b=%v\", p.a, p.b) } func main() {work(Params{ a: 47,}) } That second program prints this: 2009/11/10 23:00:00 Working with a=47, b=0 We've essentially changed the function signature, but forgot to update a callsite. This doesn't bother the compiler at all. Oddly enough, if our function was structured like this: Go code package main import \"log\" func work(a int32, b int32) {log.Printf(\"Working with a=%v, b=%v\", p.a, p.b) } func main() {work(47) } Then we'd get a compile error: ./prog.go:6:40: undefined: p ./prog.go:10:7: not enough arguments in call to workhave (number)want (int32, int32) Go build failed. Why does the Go compiler suddenly care if we provide explicit values now? If the language was self-consistent, it would let me omit both parameters, and just default to zero. Because one of the tenets of Go is that zero values are good, actually. See, they let you go fast. If you did mean for b to be zero, you can just not specify it. And sometimes it works fine, because zero values do mean something: Go code package main import \"log\" type Container struct {Items []int32 } func (c *Container) Inspect() {log.Printf(\"We have %v items\", len(c.Items)) } func main() {var c Containerc.Inspect() } 2009/11/10 23:00:00 We have 0 items Program exited. This is fine! Because the []int32 slice is actually a reference type, and its zero value is nil, and len(nil) just returns zero, because \"obviously\", a nil slice is empty. And sometimes it's not fine, because zero values don't mean what you think they mean: Go code package main type Container struct {Items map[string]int32 } func (c *Container) Insert(key string, value int32) {c.Items[key] = value } func main() {var c Containerc.Insert(\"number\", 32) } panic: assignment to entry in nil map goroutine 1 [running]: main.(*Container).Insert(...)/tmp/sandbox115204525/prog.go:8 main.main()/tmp/sandbox115204525/prog.go:13 +0x2e Program exited. In that case, you should've initialized the map first (which is also actually a reference type), with make, or with a map literal. That alone is enough to cause incidents and outages that wake people up at night, but everything gets worse real fast when you consider the Channel Axioms: A send to a nil channel blocks forever A receive from a nil channel blocks forever A send to a closed channel panics A receive from a closed channel returns the zero value immediately Because there had to be a meaning for nil channels, this is what was picked. Good thing there's pprof to find those deadlocks! And because there's no way to \"move\" out of values, there has to be meaning for receiving and sending to closed channels, too, because even after you close them you can still interact with them. (Whereas in a language like Rust, a channel closes when its Sender is dropped, which only happens when nobody can touch it again, ever. The same probably applies to C++ and a bunch of other languages, this is not new stuff). \"Zero values have meaning\" is naive, and clearly untrue when you consider the inputs of, like... almost everything. There's so many situations when values need to be \"one of these known options, and nothing else\", and that's where sum types come in (in Rust, that's enums). And Go's response to that is: just be careful. Just like C's response before it. Just don't access the return value if you haven't checked the error value. Just have a half-dozen people carefully review each trivial code change to make sure you're not accidentally propagating a nil, zero, or empty string way too deep into your system. It's just another thing watch out for. It's not like you can prevent all problems anyway. That is true! There's a ton of things to watch out for, always. Something as simple as downloading a file to disk... isn't! At all! And you can write logic errors in just about every language! And if you try hard enough I'm sure you can drive a train straight into a tree! It's just much easier with a car. The fallacy here is that because it is impossible to solve everything, we shouldn't even attempt to solve some of it. By that same logic, it's always worthless to support any individual financially, because it does nothing to help every other individual who's struggling. And this is another self-defense tactic: to refuse to consider anything but the most extreme version of a position, and point out how ridiculous it is (ignoring the fact that nobody is actually defending that ridiculous, extreme position). So let's talk about that position. \"Rust is perfect and you're all idiots\" I so wish that was how I felt, because it would be so much simpler to explain. That fantasy version of my argument is so easy to defeat, too. \"How come you use Linux then? That's written in C\". \"Unsafe Rust is incredibly hard to write correctly, how do you feel about that?\" The success of Go is due in large part to it having batteries included and opinionated defaults. The success of Rust is due in large part to it being easy to adopt piecemeal and playing nice with others. They are both success stories, just very different ones. If the boogeyman is to be believed, \"Rust shills\" would have everyone immediately throw away everything, and replace it with The Only Good Language Out there. This is so very far from what's happening in the real world, it's tragic. Firefox is largely a C++ codebase, but ships several crucial components in Rust. The Android project recently reimplemented its entire Bluetooth stack in Rust. Rust cryptography code has found its way into Python, Rust HTTP code has found its way into curl (as one of many available backends), and the Linux kernel Rust patches are looking better every round. None of these are without challenges, and none of the people involved are denying said challenges. But all of these are incremental and pragmatic, very progressively porting parts to a safer language where it makes sense. We are very far from a \"throwing the baby out with the bathwater\" approach. The Rust codegen backend literally everyone uses is a mountain of C++ code (LLVM). The alternatives are not competitors by any stretch of the imagination, except maybe for another mountain of C++ code. The most hardcore Rust users are the most vocal about issues like build times, the lack of certain language features (I just want GATs!), and all the other shortcomings everyone else is also talking about. And they're also the first to be on the lookout for other, newer languages, that tackle the same kind of problems, but do it even better. But as with the \"questioning your credentials\" angle, this is all irrelevant. The current trends could be dangerous snake oil and we could have literally no decent alternative, and it would still be worth talking about. No matter who raises the point! Creating false dichotomies isn't going to help resolve any of this. Folks who develop an allergic reaction to \"big balls of mutable state without sum types\" tend to gravitate towards languages that gives them control over mutability, lifetimes, and lets them build abstractions. That those languages happen to often be Go and Rust is immaterial. Sometimes it's C and Haskell. Sometimes it's ECMAScript and Elixir. I can't speak to those, but they do happen. You don't have to choose between \"going fast\" and \"modelling literally every last detail of the problem space\". And you're not stuck doing one or the other if you choose Go or Rust. You can, at great cost, write extremely careful Go code that stays far away from stringly-typed values and constantly checks invariants — you just get no help from the compiler whatsoever. And you can, fairly easily, decide not to care about a whole bunch of cases when writing Rust code. For example, if you're not writing a low-level command-line utility like ls, you can decide to only care about paths that are valid UTF-8 strings by using camino. When handling errors, it is extremely common to list a few options we do care about and want to do special handling for, and shove everything else into an \"Other\" or \"Internal\" or \"Unknown\" variant, which we can flesh out later as needed, when reviewing logs. The \"correct\" way to assume an optional value is set, is to assert that it is, not to use it regardless. That's the difference between calling json.Unmarshal and crossing your fingers, and calling unwrap() on an Option. And it's so much easier to do it correctly when the type system lets you spell out what the options are — even when it's as simple as \"ok\" or \"not ok\". Which brings me to the next argument, by far the most reasonable of the bunch. Go as a prototyping/starter language We've reached the fifth stage of grief: acceptance. Fine. It may well be that Go is not adequate for production services unless your shop is literally made up of Go experts (Tailscale) or you have infinite money to spend on engineering costs (Google). But surely there's still a place for it. After all, Go is an easy language to pick up (because it's so small, right?), and a lot of folks have learned it by now, so it's easy to recruit Go developers, so we can get lots of them on the cheap and just uhhh prototype a few systems? And then later when things get hard (as they always do at scale) we'll either rewrite it to something else, or we'll bring in experts, we'll figure something out. Except there is no such thing as throwaway code. All engineering organizations I've ever seen are EXTREMELY rewrite-averse, and for good reason! They take time, orchestrating a seamless transition is hard, details get lost in the shuffle, you're not shipping new features while you're doing that, you have to retrain your staff to be effective at the new thing, etc. Tons of good, compelling reasons. So very few things eventually end up being rewritten. And as more and more components get written in Go, there's more and more reason to keep doing that: not because it's working particularly well for you, but because interacting with the existing codebases from literally anything else is so painful (except over the network, and even then.. see \"Go is an island\" above). So things essentially never improve. All the Go pitfalls, all the things the language and compiler doesn't help you prevent, are an issue for everyone, fresh or experienced. Linters help some, but can never do quite as much as compiler for languages that took these problems seriously to begin with. And they slow down development, cutting into the \"fast development\" promise. All the complexity that doesn't live in the language now lives in your codebase. All the invariants you don't have to spell out using types, you now have to spell out using code: the signal-to-noise ratio of your (very large) codebases is extremely poor. Because it has been decided that abstractions are for academics and fools, and all you really need is slices and maps and channels and funcs and structs, it becomes extremely hard to follow what any program is doing at a high level, because everywhere you look, you get bogged down in imperative code doing trivial data manipulation or error propagation. Because function signatures don't tell you much of anything (does this mutate data? does it hold onto it? is a zero value there okay? does it start a goroutine? can that channel be nil? what types can I really pass for this interface{} param?), you rely on documentation, which is costly to update, and costlier still not to update, resulting in more and more bugs. The very reason I don't consider Go a language \"suitable for beginners\" is precisely that its compiler accepts so much code that is very clearly wrong. It takes a lot of experience about everything around the language, everything Go willfully leaves as an exercise to the writer, to write semi-decent Go code, and even then, I consider it more effort than it's worth. The \"worse is better\" debate was never about some people wanting to feel superior by adding needless complexity, then mastering it. Quite the contrary, it's an admission that humans suck at maintaining invariants. All of us. But we are capable of building tools that can help us doing that. And focusing our efforts on that has an upfront cost, but that cost is well worth it. I thought we'd moved past the notion that \"programming is typing on a keyboard\" long ago, but when I keep reading \"but it's fast to write lots of Go!\", I'm not so sure. Inherent complexity does not go away if you close your eyes. When you choose not to care about complexity, you're merely pushing it onto other developers in your org, ops people, your customers, someone. Now they have to work around your assumptions to make sure everything keeps running smoothly. And nowadays, I'm often that someone, and I'm tired of it. Because there is a lot to like in Go at first, because it's so easy to pick up, but so hard to move away from, and because the cost of choosing it in the first place reveals itself slowly over time, and compounds, only becoming unbearable when it's much too late, this is not a discussion we can afford to ignore as an industry. Until we demand better of our tools, we are doomed to be woken up in the middle of the night, over and over again, because some nil value slipped in where it never should have. It's the Billion Dollar Mistake all over again. What did we learn? Here's a list of lies we tell ourselves to keep using Golang: Others use it, so it must be good for us too Everyone who has concerns about it is an elitist jerk Its attractive async runtime and GC make up for everything else Every language design flaw is ok in isolation, and ok in aggregate too We can overcome these by \"just being careful\" or adding more linters/eyeballs Because it's easy to write, it's easy to develop production software with Because the language is simple, everything else is, too We can do just a little of it, or just at first, or we can move away from it easily We can always rewrite it later Comment on /r/fasterthanlime Thanks to my sponsors: belzael, Alex Doroshenko, hgranthorner, Ahmad Alhashemi, Joonas Koivunen, Christoph Grabo, pinkhatbeard, Niels Abildgaard, henrique-pinto, Andronik, Nikolai Vincent Vaags, qrpth, Kamran Khan, Scott Sanderson, Sylvie Nightshade, Aleksandre Khokhiashvili, Cole Kurkowski, Jörn Huxhorn, Matthew T, Marky Mark and 234 more My work is sponsored by people like you. Donate now so it can keep going: Continue with GitHub Continue with Patreon Here's another article just for you: Image decay as a service Jul 01, 2020 1 hour 8 min #rust · #web · #tide · #warp Since I write a lot of articles about Rust, I tend to get a lot of questions about specific crates: \"Amos, what do you think of oauth2-simd? Is it better than openid-sse4? I think the latter has a lot of boilerplate.\" And most of the time, I'm not sure what to responds. There's a lot of crates out there. I could probably review one crate a day until I retire! Read more Contents The author is a platypus Mom smokes, so it's probably okay The good parts Go is an island All or nothing (so let's do nothing) \"Rust is perfect and you're all idiots\" Go as a prototyping/starter language About Legal Notice Privacy Policy Terms and Conditions",
    "commentLink": "https://news.ycombinator.com/item?id=42243500",
    "commentBody": "Lies we tell ourselves to keep using Golang (2022) (fasterthanli.me)280 points by reillyse 10 hours agohidepastfavorite366 comments devjab 10 hours agoThis article makes a lot of great points about the shortcomings of Go. I don’t think explicit error handling is one of them however. I’ve previously spoken about my loathing of exception handling because it adds a “magic” layer to things which is way too easy to mess up. From a technical standpoint that isn’t necessarily a good argument, but from a pragmatic standpoint and decades of experience… well I will take explicit error handling which happens exactly where the errors occur every day. You can argue that Rust does it in a more elegant way, and I prefer it for personal projects. For big projects with a lot of developers of various skill level joining and leaving I think Go’s philosophy is one of the sanest approaches to error handling in the modern world. Staying in that lane. In my part of the world Go is seeing adoption that no other “new” language has exactly because of its simplicity. It’s not the best language, but it’s often the best general purpose language because it has a lot of build in opinions which protect you from yourself. reply the_gipsy 10 hours agoparentThere are several shortcomings with go's error handling. The author heavily lies onto rust, so the alternative is not exceptions but a `Result` sum type. No stacktraces and error wrapping forces you to not only invent unique error messages. You must also conceive a unique wrapping message at every call-site so that you can grep the error message and approximate a stacktrace. The weird \"return tuple\" , which obviously just exists for errors because there is not a single other place where you can use tuples in the language, and the awkward variable initialization rules, make it so that you use the wrong `err` var at some point. E.g. if you want to reassign the result to an existing var, suddenly you have to declare `var err error`, and if `err` already exists then you have to reuse it. There should be an enum type in go, or instead of the bizarre \"return tuple\" mechanics exclusive for errors, they should have added a better syntax sugar for errors like rust's `?` sugar. Instead we have something extremely tedious and quite error prone. > it has a lot of build in opinions which protect you from yourself It does have opinions, but too often they seem to be there to protect the language from being criticized. Sadly, this works, as marketing (lying) is an important factor towards making a PL popular in today's market. reply the_gipsy 6 hours agorootparentI forgot: The tenet \"accept interfaces, return structs\" is violated all over by returning the `error` interface. IMO it's okay to make behaviour-exceptions specifically for error handling. Rust for example doesn't really have builtin behaviour exceptions specifically for errors, they're generic to sumtypes and just happen to work well for errors. But then in practice you must resort to thiserror or anyhow helper crates to deal with errors in anything but tiny programs. If you do make behaviour exceptions for error handling, be honest and upfront about it. Don't say \"errors are just values so just use them like regular vars\" in docs, if then there are several huge exceptions (tuple-returns and breaking a core tenet). If you make exceptions then you might as well do them right, instead of half-assing them. I believe zig does it right, but I haven't gotten around to try it. reply Seb-C 5 hours agorootparent\"Do what I say, not what I do\" is almost a design guideline of Go at this point, there are so many inconsistencies like this in the language design. reply 9rx 3 hours agorootparentprev> The tenet \"accept interfaces, return structs\" is violated all over by returning the `error` interface. To be fair, that expression came from a blogger who often wrote about Go. It is not a tenet held by the Go project. In fact, Rob Pike has made clear that he considers it to be misguided advice. It is only violated in the same way returning Result in Rust violates the assertion I made up for this comment: Do not return Result. reply the_gipsy 1 hour agorootparenthttps://go.dev/wiki/CodeReviewComments#interfaces reply masklinn 9 hours agorootparentprev> The weird \"return tuple\" , which obviously just exists for errors because there is not a single other place where you can use tuples in the language MRV, go does not have tuples. Go is not the only language with MRV (as a special case) and they’re not necessarily bad, iirc Common Lisp uses them as auxiliary data channels and has a whole host of functions to manipulate and refit them. Go is singularly incompetent at MRVs though, in the sense that only syntax and builtin functions get access to variable arity (e.g. if you access a map entry you can either get one return which is a value, or two which are the value and whether the key is/was in the map). So MRVs mostly end up being worse tuples infecting everything (e.g. iterators needing Iter and Iter2 because you can’t just yield tuples to for loops). reply pwdisswordfishz 8 hours agorootparent> MRV, go does not have tuples. > MRVs mostly end up being worse tuples I think you noticed yourself that you’re getting too hung up on terminology. Multiple return values are a half-hearted, non-reified version of tuples. reply masklinn 8 hours agorootparentNo, MRVs can actually offer useful properties and features, that is what they do in Common Lisp. That Go does not do that has nothing to do with MRVs. reply biorach 8 hours agorootparentprevWhich is what they said. I'm not sure what point you're making reply packetlost 3 hours agorootparentprevRust and Go's lack of stack traces are basically equivalent in that you need to call an additional function to add the stack context to the error result. For go you use fmt.Errorf, in Rust you use .context from anyhow (bad practice in many contexts IMO) or .inspect_err + log. It's rather unfortunate that neither has an easy way of capturing a line number + file easily and appending it to the context. Go could easily do it, I think. Oh well. I agree that Go should really have an analogue to Rust's `?`, but you can't really do that in a sane way without sum types to represent your conditions. The very multiple-return style error propagation makes it impractical to do. IMO Go should add really basic tagged unions and rework the stdlib to use them, but such a change would probably necessitate a v2. reply pdimitar 3 hours agorootparentRE: Golang v2, they clearly said they will not do it and will double down on backwards compatibility with exceptions powered by env vars and/or CLI switches. reply 9rx 1 minute agorootparentTechnically, Go v2 signified the transition away from Google control to the project being directed by community. That happened several years ago. The stdlib is also at v2 now, (e.g. math/rand/v2). So you must mean the language? They said that a v2 of the language might happen if absolutely necessary, but believe it is unlikely that it will ever be necessary. I don't expect simple tagged unions would require a v2. A v2 (or even v3, perhaps) stdlib would be necessary, but they certainly haven't rejected that. packetlost 3 hours agorootparentprevI'm fully aware of that. reply the_gipsy 1 hour agorootparentprev> Go should really have an analogue to Rust's `?`, but you can't really do that in a sane way without sum types It could be just some simple (hey, that's what go wants to be, right?) macro thing, that just does the everyday `if err!=nil{return ..., err}` for you without having to juggle (and think about) vars. b := g(f()?)? // var b B // { // var a A // var err error // a, err = f() // if err != nil { // return *new(A), *new(B), err // } // var b B // b, err = g(a) // if err != nil { // return *new(A), *new(B), err // } // // no accidental reuse of err // } I mean look how much utterly useless noise this is, and count all the opportunities for mistakes that wouldn't get caught by the compiler. reply yegle 2 hours agorootparentprevhttps://github.com/pkg/errors provides stack traces support. This unfortunately did not get included in the Go1.13 release when error wrapping was introduced. reply 9rx 4 hours agorootparentprev> The weird \"return tuple\" , which obviously just exists for errors because there is not a single other place where you can use tuples in the language Go functions also accept a tuple on input. While theoretically you could pass an error, or a pointer to an error for assignment, it is a stretch to claim it is for errors. reply the_gipsy 3 hours agorootparentYes exactly, that rather useless feature just makes the whole thing even weirder. reply devjab 6 hours agorootparentprevI prefer the structure of Rust errors as it’s fully typed, I don’t like that you can chain them though. It’s a cool feature but it leaves you with some of the same issues exception handling does when the freedom is used “wrong”. reply KingOfCoders 9 hours agorootparentprevExceptions are sum types, they just have different syntactic sugar. reply 9rx 4 hours agorootparentChecked exceptions may be implemented as a sum type. Traditional exceptions are more likely to be a single type that wraps up a context object alongside stack trace information. reply veidelis 6 hours agorootparentprevAnd different control flow, and different or sometimes non-existent types (Java's throws). reply pema99 8 hours agorootparentprevNot really. Exceptions usually imply unwinding the stack, and the ability to catch at any point throughout the callstack. Result types are just 'dead' data. reply zozbot234 8 hours agorootparentThese are fully equivalent in outcome, though often not low-level implementation. You can use try...catch (called panic...recover in Go) to pack a normal and abnormal return case into the equivalent of a Result type. Or just pass an abnormal Result back to the caller to manually unwind a single \"layer\" of the call stack. reply biorach 8 hours agorootparent> These are fully equivalent in outcome They are so different in DX, ergonomics, implementation and traceability that I'm not sure this is true other than in the most abstract sense reply sshine 6 hours agorootparentThere is some DX similarity between checked exceptions and Result types. Because the compiler will fail if you don't explicitly mention each possible exception. But checked exceptions are coming out of style: They're unchecked in C#, and frameworks like Spring Boot in Java catch all checked exceptions and rethrow them as Spring Boot flavored unchecked ones. For unchecked exceptions and Result types: The DX is very different in one critical way: With Results you constantly have to differentiate between error and ok states, before you proceed. With unchecked exceptions you generally assume you're always in an ok state. It's equivalent to wrapping your whole function body in 'try { ... } catch (Exception e)'. And you can get that with Result types in Rust by using '?' and not worry about doing something half-way. Ultimately: Are you a happy-path programmer? reply abtinf 10 hours agoparentprevHaving programmed for over 30 years, including nearly a decade of C#, I would say exceptions are one of the worst ideas in all of programming. They are just horrific gotos that any library can invoke against your code. They are pretty much never, ever handled correctly. And nearly always, after an exception is “handled”, the application is actually in an unknown state and cannot be reasoned about. Even junior engineers have a trivial time debugging most go errors, while even experienced principles struggle with figuring out the true cause of a Java exception. reply mike_hearn 6 hours agorootparentWell, I've also programmed for over thirty years and I wouldn't use a language without exceptions and even wrote a whole essay defending that position: https://blog.plan99.net/what-s-wrong-with-exceptions-nothing... > Even junior engineers have a trivial time debugging most go errors Not my experience at all. I had to do this once. An HTTP request to a production server was yielding a 400 Bad Request with no useful information for what was bad about it. No problem, I'll check the logs and look at the source code. Useless: the server was written in Go and the logs had no information about where the error was originating. It was just getting propagated up via return codes and not logged properly. It ended up being faster to blackbox reverse engineer the server. In a language with exceptions there'd have been a stack trace that pinpointed exactly where the error originated, the story of how it was handled, and the story of how the program got there. Diagnosing errors given stack traces is very easy. I've regularly diagnosed subtle bugs given just logs+stack trace and nothing else. I've also had to do the same for platforms that only have error codes that aren't Go (like Windows). It's much, much harder. reply bob1029 5 hours agorootparent> Diagnosing errors given stack traces is very easy. This is the most important aspect of exceptions in my view. The line that threw the exception isn't even the part of a stack trace that I find most interesting. The part that is most valuable to me when working on complex production systems are all of the call sites leading up to that point. I remember in my junior years I wasn't a big fan of exceptions. A stack trace would make my eyes glaze over. I would try/catch at really deep levels of abstraction and try to suppress errors too early. It took me a solid ~5 years before I was like \"yes, exceptions are good and here's why\". I think a lot of this boils down to experience and suffering the consequences of bad design enough times. reply mike_hearn 3 hours agorootparentException usability is definitely an area that needs work. If you work support for a dev platform for a while, it's a really common experience that people will post questions with a copy/pasted stack trace where the message+stack actually answers their question. You can just copy/paste parts back to them and they're happy. There's too much information and not enough tools to digest/simplify them and people get overwhelmed. Still, better to have too much data than too little. reply 9rx 4 hours agorootparentprev> Useless: the server was written in Go and the logs had no information about where the error was originating. [...] In a language with exceptions there'd have been a stack trace that pinpointed exactly where the error originated Go has support for exceptions, not to mention providing runtime access to stack trace information in general. They are most definitely there if your application requirements necessitate, which it seems yours did. Unfortunately, language support only helps if your developers actually speak the language. Go in particular seems especially prone to attracting developers who are familiar with other languages, who insist on continuing to try to write code in those other languages without consideration for how this one might be different and then blame the technology when things don't work out... reply quotemstr 4 hours agorootparentprevI can't agree with you about C++ exceptions being worse than useless. Exceptional C++ is worth it. Safety isn't that hard with RAII and ScopeGuard.. In your map example, just add a scope guard that removes the just-added element using the returned iterator if the rest of the procedure doesn't succeed. It's no different in Java. reply kaoD 9 hours agorootparentprevMany people against Go's error handling do not advocate for exceptions, but for a combination of an Either/Result type (for recoverable errors) and fully aborting (for unrecoverable errors). reply OtomotO 9 hours agorootparentAbort on the other hand is used WAY to liberally in Rust. How I hate it, that every second function call can break my program when it's clearly not a \"halt the world, it's totally unrecoverable that the user sent us nonsense\" type. Return a Result and get on with your life! reply Chai-T-Rex 8 hours agorootparentIf a Rust function can panic, there's generally a non-panicking alternative. For example, `Vec` indexing has `vec[n]` as the panicking version and `vec.get(n)` as the version that can return `None` when there's nothing at that index. reply mathw 7 hours agorootparentI do wish this is something Rust had done better though - the panicking versions often look more attractive and obvious to developers, and that's the wrong way round. Vec indexing, IMO, should return Option. reply sshine 6 hours agorootparentWhile that is true, there are clippy::indexing_slicing, clippy::string_slice for that: https://github.com/rust-lang/rust-clippy/issues/8184#issuecomment-1003651774 error: indexing may panic --> src/main.rs:100:57100rtmp::header::BasicHeader::ID0 => u32::from(buffer[1]) + 64, |^^^^^^^^^= help: consider using `.get(n)` or `.get_mut(n)` instead = help: for further information visit https://rust-lang.github.io/rust-clippy/master/index.html#indexing_slicing reply quotemstr 4 hours agorootparentprevOne of my dream projects is creating a Rust stdlib based solely on panics for error handling. Sure, it'd be incompatible with everything, but that might be a feature, not a bug. reply andrewshadura 4 hours agorootparentI think someone's already created this. reply OtomotO 5 hours agorootparentprevin the stdlib, yes. In 3rd party crates? Depends! reply rand_r 10 hours agorootparentprev> never handled correctly I’ve seen this argument, but if you look at real golang code and examples, it’s just a bunch of “if errnill” copy pasta on every line. It’s true that handling errors is painstaking, but nothing about golang makes that problem easier. It ends up being a manual, poor-man’s stack-trace with no real advantage over an automatically generated one like in Python. reply swiftcoder 10 hours agorootparentWhich could be solved in one swipe by adding a Result sum type, and a ? operator to the language. This is more a self-inflicted limitation of Go, then a general indictment of explicit error handling. reply pyrale 9 hours agorootparentprevException and explicit on-the-spot handling are not the only two ways to handle failing processes. Optional/result types wrapping the are a clean way to let devs handle errors, for instance, and chaining operations on them without handling errors at every step is pretty ergonomic. reply quotemstr 9 hours agorootparentRust's error handling evolution is hilarious. In the beginning, the language designers threw out exceptions --- mostly, I think, because Go was fashionable at the time. Then, slowly, Rust evolved various forms of syntactic sugar that transformed its explicit error returns into something reminiscent of exceptions. Once every return is a Result, every call a ?, and every error a yeet, what's the difference between your program and one with exceptions except the Result program being syntactically noisy and full of footguns? Better for a language to be exceptional from the start. Most code can fail, so fallibility should be the default. The proper response to failure is usually propagating it up the stack, so that should be the default too. What do you get? Exceptions. reply maleldil 9 hours agorootparent> what's the difference between your program and one with exceptions Because errors as values are explicit. You're not forced to use ? everywhere; you can still process errors however you like, or return them directly to the calling function so they deal with it. They're not separate control flow like exceptions, and they're not a mess like Go's. reply quotemstr 9 hours agorootparentNo, because you end up with a function coloring problem that way. A function that returns something other than Result has to either call only infallible code or panic on error, and since something can go wrong in most code, the whole codebase converges as time goes to infinity on having Result everywhere. Yeah, yeah, you can say it's explicit and you can handle it how you want and so on, but the overall effect is just a noisy spelling of exceptions with more runtime overhead and fewer features. reply dwattttt 8 hours agorootparentI very much care about whether a function can fail or not, and I encourage all the function colouring needed to convey that. reply funcDropShadow 2 hours agorootparentAs almost always, we programmers / software developers / engineers, forget to state our assumptions. In closed-world, system-software, or low-level software you want to have your kind of knowledge about everything you call. Even more: can it block? In open-world, business-software, or high-level software it is often impossible or impractical to know all the ways in which a function or method can fail. What you need then, is a broad classification of errors or exception in the following two dimensions: 1. transient or permanent, 2. domain or technical. Those four categories are most of the time enough to know whether to return a 4xx or 5xx error or to retry in a moment or to write something into a log where a human will find it. Here, unchecked exceptions are hugely beneficial. Coincidentally, that is the domain of most Java software. Of course, these two groups of software systems are not distinct, there is a grey area in the middle. reply sunshowers 1 hour agorootparentprevFunction \"coloring\" is good! It's not a problem here and it's overblown as a problem in general. If something fails recoverably then it should be indicated as such. reply pyrale 8 hours agorootparentprev> A function that returns something other than Result has to either call only infallible code or panic on error ...Or solve the problem. A library function that can be a source of issues and can't fix these issues locally should simply not be returning something that is not a result in that paradigm. > since something can go wrong in most code That is not my experience. Separating e.g. business logic which can be described cleanly and e.g. API calls which don't is a clear improvement of a codebase. > the whole codebase converges as time goes to infinity on having Result everywhere. As I said previously, it is pretty easy to pipe a result value into a function that requires a non-result as input. This means your pure functions don't need to be colored. reply quotemstr 5 hours agorootparent> Or solve the problem. A library function that can be a source of issues and can't fix these issues locally should simply not be returning something that is not a result in that paradigm. People \"solve\" this problem by swallowing errors (if you're lucky, logging them) or by just panicking. It's the same problem that checked exceptions in Java have: the error type being part of the signature constrains implementation flexibility. reply ViewTrick1002 3 hours agorootparentIn my experience an unwrap, expect or panicking function is a direct comment in code review and won’t be merged without a reason explaining why panicking is acceptable. reply chrismorgan 7 hours agorootparentprevOne practical benefit of Rust’s approach that hasn’t been emphasised enough yet is the consequences of Option and Result being just values, same as anything else. It means you can use things like result.map_err(|e| …) to transform an error from one type to another. (Though if there’s a suitable From conversion, and you’re going to return it, you can just write ?.) It means you can use option.ok_or(error) or option.ok_or_else(|| error) to convert a Some(T) into an Ok(T) and a None into an Err(E). It means you can .collect() an iterator of Result into a Vec>, or (one of the more spectacular examples) a Result, E> which is either Ok(items) or Err(first_error). It’s rather like I found expression-orientation, when I came to Rust from Python: at first I thought it a gimmick that didn’t actually change much, just let you omit the `return` keyword or so. But now, I’m always disappointed when I work in Python or JavaScript because statement-orientation is so limiting, and so much worse.¹ Similarly, from the outside you might not see the differences between exceptions and Rust-style Result-and-? handling, but I assure you, if you lean into it, it’s hard to go back. —⁂— ¹ I still kinda like Python, but it really painted itself into a corner, and I’ve become convinced that it chose the wrong corner in various important ways, ways that made total sense at the time, but our understanding of programming and software engineering has improved and no new general-purpose language should make such choices any more. It’s local-maximum sort of stuff. reply quotemstr 5 hours agorootparentExceptions are values in C++, Java, and Python too. They're just values you throw. You can program these values. As usual, I find that opposition to exceptions is rooted in a misunderstanding of what exceptions really are reply chrismorgan 4 hours agorootparentExceptions are values, but normal-value-or-exception (which is what Result is) isn’t a value. Review my remarks about map_err, ok_or, &c. with the understanding that Result is handling both branches of the control flow, and you can work with both together, and you might be able to see it a bit more. Try looking at real code bases using these things, with things like heavily method-chained APIs (popular in JS, but exceptions ruin the entire thing, so such APIs in JS tend to just drop errors!). And try to imagine how the collect() forms I described could work, in an exceptions world: it can’t, elegantly; not in the slightest. Perhaps this also might be clearer: the fuss is not about the errors themselves being values, but about the entire error handling system being just values. reply zozbot234 8 hours agorootparentprevThe '?' operator is the opposite of a footgun. The whole point of it is to be very explicit that the function call can potentially fail, in which case the error is propagated back to the caller. You can always choose to do something different by using Rust's extensive facilities for handling \"Result\" types instead of, or in addition to, using '?'. reply pwdisswordfishz 7 hours agorootparentprevIn most languages with exceptions: • they may propagate automatically from any point in code, potentially breaking atomicity invariants and preventing forward progress, and have to be caught to be transformed or wrapped – Result requires an explicit operator for propagation and enables restoring invariants and transforming the error before it is propagated. • they are an implicit side-channel treated in the type system like an afterthought and at best opt-out (e.g. \"noexcept\") – Result is opt-in, visible in the return type, and a regular type like any other, so improvements to type system machinery apply to Result automatically. • try…catch is a non-expression statement, which means errors often cannot be pinpointed to a particular sub-expression – Result is a value like any other, and can be manipulated by match expressions in the exact place you obtain it. Sure, if you syntactically transform code in an exception-based language into Rust you won’t see a difference – but the point is to avoid structuring the code that way in the first place. reply quotemstr 5 hours agorootparent> they may propagate automatically from any point in code, potentially breaking atomicity invariants and preventing forward progress A failure can propagate in the same circumstances in a Rust program. First, Rust has panics, which are exceptions. Second, if any function you call returns Result, and if propagate any error to your caller with ?, you have the same from-anywhere control flow you're complaining about above. Programmers who can't maintain invariants in exceptional code can't maintain them at all. > try…catch is a non-expression statement, That's a language design choice. Some languages, like Common Lisp, have a catch that's also an expression. So what? > they are an implicit side-channel treated in the type system like an afterthought an Non-specific criticism. If your complaint is that exceptions don't appear in function signatures, you can design a language in which they do. The mechanism is called \"checked exceptions\" Amazing to me that the same people will laud Result because it lifts errors into signatures in Rust but hate checked exceptions because they lift errors into signatures in Java. Besides, in the real world, junior Rust programmers (and some senior ones who should be ashamed of themselves) just .unwrap().unwrap().unwrap(). I can abort on error in C too, LOL. reply mottalli 5 hours agorootparentprevHonest question: syntactically noisy as opposed to what? In the context of this post, which is a critique of Go as a programming language, for me this is orders of magnitude better than the \"if err != nil {\" approach of Go. reply sunshowers 1 hour agorootparentprevThe big difference is API stability, one of the primary focuses of Rust. reply pyrale 8 hours agorootparentprevI'm not sure why you bring up Rust here, plenty of libs/languages use the Result pattern. Your explanation of what bothers you with results seems to be focused on one specific way of handling the result, and not very clear on what the issue is exactly. > what's the difference between your program and one with exceptions Sometimes, in a language where performance matters, you want an error to be handled as an exception, there's nothing wrong with having that option. In other languages (e.g. Elm), using the same Result pattern would not give you that option, and force you to resolve the failure without ending the program, because the language's design goals are different (i.e. avoiding in-browser app crash is more important than performance). > syntactically noisy Yeah setting up semantics to make users aware of the potential failure and giving them options to solve them requires some syntax. In the context of a discussion about golang, which also requires a specific pattern of code to explicitly handle failures, I'm not sure what's your point here. > full of footguns I fail to see where there's a footgun here? Result forces you to acknowledge errors, which Go doesn't. That's the opposite of a footgun. reply Ygg2 8 hours agorootparentprev> Once every return is a Result, every call a ?, and every error a yeet The Try operator (`?`) is just a syntax sugar for return. You are free to ignore it. Just write the nested return. People like it for succinctness. Yeet? I don't understand, do you mean the unstable operator? Rust doesn't have errors either. > what's the difference between your program and one with exceptions except the Result program being syntactically noisy and full of footguns? Exceptions keep the stacktrace, and have to be caught. They behave similar to panics. If panics were heavy and could be caught. Rust errors aren't caught, they must be dealt with in whatever method invokes them. Try operator by being noisy, tells you - \"Hey, you're potentially returning here\". That's a feature. Having many return in method can both be a smell, or it could be fine. I can find what lines potentially return (by searching for `?`). An exception can be mostly ignored, until it bubbles up god knows where. THAT IS A HUGE FOOTGUN. In Java/C# every line in your program becomes a quiet return. You can't find what line returns because EVERY LINE CAN. reply OtomotO 9 hours agorootparentprevBut with Exceptions you can easily implement multiple return types in e.g. Java ;) I shocked my Professor at university with that statement. After I started laughing, he asked me more questions... still went away with a straight A ;D reply Cthulhu_ 8 hours agorootparentAs you should, it shows a deeper insight in the language beyond the base course material and out of the box (and cursed) thinking. reply Yoric 6 hours agorootparentprevIn OCaml, that's actually a common use for exceptions, as they let you unwind extremely quickly and carry a result (helped by the fact that OCaml doesn't have finally or destructors). reply sshine 6 hours agorootparentprevWhen I took the compiler course at university, the professor would have a new coursework theme every year, and the year I took the course, the coursework compiler was exception-oriented. So exceptions were the only control flow mechanism besides function calls. If/else, while, return were all variations of throw. To me this proved that there's nothing inherently wrong about exceptions. It's how you structure your code and the implied assumptions you share with your colleagues. Some people are way too optimistic about their program will actually do. Happy path programming. reply BoiledCabbage 5 hours agorootparentSounds interesting - any link to the control flow implementation online? Or how does one optionally throw and exception without an \"if\" statement? What's the \"base\" exception call? Ie if \"if\" is implemented via exceptions, how do exceptions get triggered? And is \"while\" done via an \"if exception\" and recursion? Or another way? reply Seb-C 5 hours agorootparentprevI agree about implicit exceptions, but I think that there is a sweet spot with explicit exceptions like Swift (and maybe Java): where you cannot not-handle one, it is part of a function's signature, and the syntax is still compact enough that it does not hurt readability. reply bazoom42 6 hours agorootparentprevIs “goto” just used to mean “bad and evil” here? Because exceptions are not a goto anymore than a return is a goto. The problem with goto is it can jump to any arbitrary place in your code. Exceptions will only go to catch-blocks up the call stack, which presumably you have written on purpose. reply vaylian 4 hours agorootparent> Because exceptions are not a goto anymore than a return is a goto Not true at all * goto goes to a specific hard-coded address * return looks up the previous address from the stack and goes there * exceptions are a complex mess that require branching logic to determine where to resume execution reply quotemstr 9 hours agorootparentprevHard disagree. Exceptions are actually good. They make code clear and errors hard to ignore. I've written a ton of code over decades in both exceptional and explicit-error languages and I'll take the former every day. There's no function color problem. No syntactic pollution of logic with repetitive error propagation tokens. Also, exception systems usually come with built in stack trace support, \"this error caused by this other error\" support, debugger integration (\"break the first time something goes wrong\"), and tons of other useful features. (Common Lisp conditions are even better, but you can't have everything.) You can't just wave the word \"goto\" around as if it were self-evident that nonlocal flow control is bad. It isn't. > And nearly always, after an exception is “handled”, the application is actually in an unknown state and cannot be reasoned about. That's not been my experience at all. Writing exception safe code is a matter of using your language's TWR/disposable/RAII/etc. facility. A programmer who can't get this right is going to bungle explicit error handling too. Oh, and sum types? Have you read any real world Rust code? Junior developers just add unwrap() until things compile. The result is not only syntactic clutter, but also a program that just panics, which is throwing an exception, the first time something goes wrong. Many junior developers struggle with error handling in general. They'll ignore error codes. They'll unwrap sum types. They might... well, they'll propagate exceptions non-fat ally, because that's the syntactic default, and that's usually the right thing. We have to design languages with misuse in mind. reply maleldil 9 hours agorootparent> Have you read any real world Rust code? Junior developers just add unwrap() until things compile. If you really don't like unwrap[1], you can enable a linter warning that will let you know about its uses to flag it during code review. You know exactly where they are and when they happen. Exceptions are hidden control flow, so you rely on documentation to know when a function throws. > Writing exception safe code is a matter of using your language's TWR/disposable/RAII/etc. facility. A programmer who can't get this right is going to bungle explicit error handling too. Rust has RAII, so you don't have to worry about clean-up when returning errors. This is a Go problem, not Rust. [1] https://blog.burntsushi.net/unwrap/ reply OtomotO 9 hours agorootparentprevBah, no, I hated that you had to wrap basically every code block in a try/catch in Java, because the underlying lib could change and suddenly throw a Runtime-Exception. At the same time Checked Exceptions were a nightmare as well, because suddenly they were part of the contract, even though maybe wrong later. reply didntcheck 8 hours agorootparent> the underlying lib could change and suddenly throw a Runtime-Exception. And what would you do in that case? Since this is a future change your existing code presumably wouldn't know what else to do but throw its own exception, so why not just let that one propagate? reply OtomotO 1 hour agorootparentAlways depends on which level it happens reply quotemstr 9 hours agorootparentprevChecked exceptions are more trouble than they're worth. That doesn't make exceptions in general bad. reply mathw 7 hours agorootparentNot having checked exceptions is a huge problem, because then you never know when something might throw and what it might through, and in the .NET world the documentation on that is pretty awful and absolutely incomplete. But then over in Java world, your checked exception paradise (which it of course isn't because the syntax and toolkit for managing the things is so clunky) is easily broken by the number of unchecked exceptions which could be thrown from anything at any time and break your code in unexpected and exciting ways, so not only do you have to deal with that system you also don't get any assurance that it's even worth doing. But this doesn't actually mean checked exceptions are a bad idea, it means that Java didn't implement them very well (largely because it also has unchecked exceptions, and NullPointerException is unchecked because otherwise the burden of handling it would be hideous, but that comes down to reference types being nullable by default, which is a whole other barrel of pain they didn't have to do, and oh look, Go did the same thing wooo). reply neonsunset 5 hours agorootparent> in the .NET world the documentation on that is pretty awful and absolutely incomplete. Depends on the area you look at. Language documentation is pretty good and so is documentation for the standard library itself. Documentation for the frameworks can be hit or miss. EF Core is pretty well documented and it’s easy to find what to look for usually. GUI frameworks are more of a learning curve however. FWIW many in Java community consider checked exceptions to be a mistake. While I don’t find writing code that has many failure modes particularly fun with exception handling - Rust perfected the solution to this (and the Go way is visually abrasive, no thanks), I don’t think it’s particularly egregious either - Try pattern is pretty popular and idiomatic to use or implement, and business code often uses its own Result abstractions - switch expressions are pretty good at handling these. Personally, I’d write such code in F# instead which is a recent discovery I can’t believe so few know how good it is. reply Cthulhu_ 8 hours agorootparentprevWhat does make exceptions bad in my opinion (and shared by Go developers?) is a few things: 1. Exceptions are expensive (at least in Java / C#), as they generate a stack trace every time. Which is fine for actually exceptional situations, the equivalent of `panic()` in Go, but: 2. Exceptions are thrown for situations that are not exceptional, e.g. files that don't exist, database rows that don't exist, etc. Those are simple business logic cases. The workaround is defensive coding, check if the file exists first, check if the row exists? that kind of thing. 3. The inconsistency between checked and unchecked exceptions. 4. Distance - but this is developer / implementation specific - between calling a function that can throw an error and handling it. But #2 is the big one I think. Go's error handling is one solution, but if it's about correct code, then more functional languages that use the Either pattern or whatever it's called formally are even better. Go's approach is the more / most pragmatic of the options. reply cesarb 7 hours agorootparent> e.g. files that don't exist, database rows that don't exist, etc. [...] The workaround is defensive coding, check if the file exists first, check if the row exists? Ugh NO. Please don't. You should never \"check if the file exists first\". It can stop existing between your check and your later attempt at opening the file (the same with database rows). That can even lead to security issues. The name for that kind of programming mistake, as a vulnerability class, is TOCTOU (time-of-check to time-of-use). The correct way is always to try to do the operation in a single step, and handle the \"does not exist\" error return, be it a traditional error return (negative result with errno as ENOENT), a sum type (either the result or an error), or an exception. reply OtomotO 7 hours agorootparentTotally agreed, but as the previous poster wrote: an exception is meant for EXCEPTIONAL behavior. So it may be that the file access throws an exception but generally, I wouldn't agree. reply OtomotO 9 hours agorootparentprevAs said, I don't like the wrapping of about everything with try/catch Sure, you can only do it way up the stack, but that's not enough quite often. If you can only do it all the way up, I find it ergonomic. Maybe I should experiment more with catch unwind in Rust. reply biorach 8 hours agorootparentprev> Oh, and sum types? Have you read any real world Rust code? Junior developers just add unwrap() until things compile. Junior developers will write suboptimal code in any language. So I'm not sure what your point is. reply stouset 9 hours agoparentprevWithout fail, every single person I’ve seen rave about go’s error handling compares it only to exceptions as if that’s the only alternative. On the flip side I have yet to find a person who’s familiar with sum types (e.g., Maybe, Option, Result) that finds the golang approach even remotely acceptable. reply OtomotO 9 hours agorootparentHere, you've found me. I don't LIKE it, but it's acceptable. I have been working with Rust since 2015 (albeit not professionally, but a lot of side projects) and love it. But I also dabbled into go the last couple of months and while it has its warts, I see it as another tool in the tool-belt with different trade-offs. Error handling is weird, but it's working, so shrug reply cnity 8 hours agorootparentSorry, balanced opinions are not welcome in discussions about favourite programming languages. reply devjab 6 hours agorootparentprevI think it’s because Go is an alternative to Java and C# more so than an alternative to Rust. It is for me at least. As I said, Rust isn’t seeing any form of real world adoption in my region while Go is. Go isn’t replacing C/C++ or even Python though, it’s replacing Typescript, C# and Java. Now, there are a lot of good reasons as to why Go shouldn’t be doing that, a lot of them listed in the article, but that’s still what is happening. As I pointed out I think Rust does it better with its types error handling. That isn’t too relevant for me though as Rust will probably never seem any form of adoption in my part of the world. I think Zig may have a better chance considering how interoperable it is with C, but around here the C++ folks are simply sticking with C++. reply zozbot234 5 hours agorootparent> Now, there are a lot of good reasons as to why Go shouldn’t be doing that I disagree. Typescript, C# and Java are terrible languages (as are Python/Ruby/etc. in other ways). Golang is bad by OP's standards but there's nothing wrong with it gaining ground on those languages. Besides it's also easier to convert a codebase to Rust from Golang than Typescript or C#/Java. reply devjab 1 hour agorootparentIt was meant more as an observation than my opinion. I would pick Go over Java/C# any day of the week, but it’s not like talented JVM engineers won’t run in circles around you as far as performance goes. I’d frankly pick Python for most things though. It’s a terrible language, everyone knows it’s terrible but it gets things done and everyone can work with it. I view performance issues a little different than most people though. To me hitting the wall where you can no longer “make do” with C/Zig replacements of Python bottlenecks means you’ve made it. The vast majority of software projects will never be successful enough to get there. reply neonsunset 3 hours agorootparentprevRust and C# have far more overlap than Go could ever hope for. Go is limited (and convoluted sometimes due to \"solutions\" devised to cope with it) so it is easily expressible in languages with better type systems and concurrency primitives. reply clausecker 9 hours agorootparentprevI dislike sum-type based error handling. It is annoying syntactically and only really doable with lots of high-level combinators, which in turn hinder debuggability. reply lionkor 5 hours agorootparentHave you tried the approach that Zig has, or the approach that Rust has? They are easy to debug and do not use any crazy stuff, just simple syntax like `try x()` (Zig) or `x()?` (Rust) reply DanielHB 9 hours agorootparentprevYou hit the main gripe I have with Go, its types system is so basic. I get people raving type-correctness of Go when they come from Python but the type system in Go is simply pre-historic by modern day standards. reply masklinn 9 hours agorootparentGo’s type system is not even impressive compared to python’s. reply orwin 9 hours agorootparentDo you have a pydantic equivalent in go? Also modern typing in python is starting to be OK to be honest (well, if you consider typescript typing OK), so it isn't really a knock on Go :) reply Yoric 6 hours agorootparent> Do you have a pydantic equivalent in go? I've been working on one [1]. But gosh, does go make it hard. [1] https://github.com/pasqal-io/godasse reply DanielHB 8 hours agorootparentprevWell I was comparing to python codebases before they added type annotations reply Yoric 6 hours agorootparentWhich, sadly, is still the case of too many dependencies. While I much prefer Python as a language, Go wins against Python by having a fresher ecosystem, with a higher baseline for type safety. Still pretty low with respect to Rust or mypy/pyright with highest settings, but much better than any of the Python frameworks I've had to deal with. reply yyyfb 8 hours agorootparentprevI feel that the future for Python people who want type safety will eventually be TypeScript on nodejs. Go was intended as an alternative to C++. It seems that in reaction to the ungodly complexity of C++, the creators wanted to avoid adding language features as hard as possible. If the user could work around it with a little extra verbosity, it'd be ok. I feel they removed too much and maybe not the right things. reply eska 9 hours agorootparentprevThat’s to be expected since it is marketed towards beginner and casual programmers. reply Cthulhu_ 8 hours agorootparentI don't agree that that's what it's marketed towards, but it was designed with those in mind. That said, experienced developers can enjoy it too, as code is just a means to an end and code complexity or cleverness does not make for good software in the broader sense of the word. It's a Google solution to Google scale problems, e.g. codebases with millions of lines of code worked on by thousands of developers. Problems that few people that have an Opinion on Go will ever encounter. reply euroderf 9 hours agorootparentprevFWIW... WebAssembly has Option and Result, and adapters for Go. reply cnity 8 hours agorootparentWhat do you mean by this? WebAssembly is a low level bytecode which only defines low level types. WebAssembly doesn't \"have\" types any more than x86 \"has\" types right? Or have I missed something? reply euroderf 8 hours agorootparentAh, sorry. Shoulda said: WebAssembly component model. reply cnity 6 hours agorootparentOh, I see. Thanks! reply cowl 8 hours agoparentprevthis part of error handling is pure religion. it goes even against one of the most basic go tenents. that code should be easy to read not write. Try reading and understanding the logic of a particular method where 75% of the lines are error noise and only 25% are the ones you need to understand what the method does. yes it's noise because whenever read a codebase for the first time you are never interested on the the error edge case. first glance readability needs to tell you what you are trying to accomplish and only after what you are doing to make sure that is correct. on this point go's error handling is a massive fail. Notice that I'm not saying explicit error handling is bad. I'm saying the insistence that error handling needs to be implemented inline interleaved with the happy path is the problem. You can have explicit error handling in dedicated error handling sections reply Yoric 6 hours agorootparentDo you have examples for the latter? reply cowl 2 hours agorootparentthe most basic example was the declined proposal https://github.com/golang/proposal/blob/master/design/32437-... Some people didn't like the \"try\" keyword it reminded them too much of exceptions, some people didn't like that they couldnt see a return inline (which was the purpose of the proposal in the first place). it's not that there are no solutions. the main problem is the go team's insistence to have \"one true way\" (tm) of doing something and unfortunately this gap between people who want to see every return inline and people who want to see the clean solution separate from the error handling is not something that can be bridged by technical means. the only solution is to implement both ways and lets see which one wins. reply Yoric 45 minutes agorootparentThis doesn't look meaningfully different from current error handling in Go. It's basically the same syntactic sugar as `try!` in Rust, isn't it? reply monksy 10 hours agoparentprevI would certainly argue against the claim that explicit error handling is far overkill. Where I agree: It forces you to think about all of the possibilities your code might generate. (This is more of a C question than it is with other languages) However, when abstracting blocks of code away, you don't always need to handle the error immediently or you may want to handle it down the stack. You're giving up a lot of readability in order for the language to be particular. reply madeofpalk 8 hours agorootparent> It forces you to think about all of the possibilities your code might generate. Except it doesn't actually. You can totally just ignore it and pretend errors don't exist. Lack of sum types/Result, and pointers as poor mans optional, really hinder's Go's error handling story. reply acdha 5 hours agorootparentprev> It forces you to think about all of the possibilities your code might generate I’ve seen way too much Go code which never even tested the err value to believe that until something like errcheck is built in to the compiler. I do agree that this is a plus for the explicit model but that’s been a better argument for Rust in my experience since there’s better culture and tooling around actually checking errors. I’m sure there are plenty of teams doing a good job here, but it always felt like the one lesson from C they didn’t learn well enough, probably because a language created by experts working at a place with a strong culture of review doesn’t really account for the other 99% of developers. reply eru 9 hours agorootparentprevRust handles this much better. Error handling is still explicit, but it gives you the tools needed to make it less tedious. reply Attummm 9 hours agoparentprevFor me, the issue with error handling is that while errors are explicitly stated, they are often poorly handled. Rarely have I seen the handling of multiple reasons for why an error might occur, along with tailored approaches to handle each case. This is something very common in older languages like Python or Java reply cnity 8 hours agorootparentAs a regular Go user, I agree with this take. Though the tools exist, error wrapping and checking (with errors.Is and so on) is actually pretty rare in my experience. Positive example of good and appropriate usage here: https://github.com/coder/websocket/blob/master/internal/exam... reply Cthulhu_ 8 hours agorootparentprevThis is down to developer style and agreements though; Go has typed errors and a set of utilities to match them [0]. Not using those is a choice, just like how in Java you can just `catch (Exception e)` after calling a dozen methods that might each throw a different exception. [0] https://pkg.go.dev/errors reply Yoric 6 hours agorootparentInterestingly, every time (and I mean _every_ time) that I've tried to use `errors.As` on errors raised by lib code, I found out that the lib just went with \"nah, I'm just going to use `errors.New` or `fmt.Errorf`\", which makes the error impossible to match. So... I'd say that this is a fumble in the design of Go. reply flir 8 hours agoparentprevI see a lot of people say this about exceptions, and I don't have that problem. The exception bubbles up the stack until something catches it. Ok it's a different code path, but it's a very simple one (straight up). So you either catch the exception nearby and do something specific with it, or it bubbles up to a generic \"I'm sorry there was a problem please try again later\" handler. Honestly makes me wonder what I'm missing. Maybe it's because I don't deal with state much? Do the problems start to mount up when you get into writing transaction locks, rollbacks etc? But I don't see why you wouldn't have the same problems with Go's mechanism. Hoping to gain enlightenment here. [copied from a comment below]: They are just horrific gotos that any library can invoke against your code. They are pretty much never, ever handled correctly. And nearly always, after an exception is “handled”, the application is actually in an unknown state and cannot be reasoned about. Maybe this is it? I prefer a \"fail early and often\" approach, and I tend to validate my data before I perform operations on it. I don't want to attempt recovery, I want to spew log messages and quit. reply acdha 4 hours agorootparent> They are pretty much never, ever handled correctly. And nearly always, after an exception is “handled”, the application is actually in an unknown state and cannot be reasoned about. I think that’s misdirected but illustrates the emotional reasons why people develop a negative impression of the concept. Usually it means someone had bad experiences with code written in a poor culture of error handling (e.g. certain Java frameworks) and generalized it to “exceptions are bad” rather than recognizing that error handling isn’t trivial and many programmers don’t take it seriously enough, regardless of the paradigm. As a simple example, C and PHP code have had many, many security and correctness issues caused by _not_ having errors interrupt program execution where the users would have been much better off had the program simply halted on the first unhandled error. If you write complex programs with lots of mutable shared state, yes, it’s hard to reason about error recovery but that’s misattributing the problem to the mechanism which surfaced the error rather than the fact that their program’s architecture makes it hard to rollback or recover. reply pansa2 8 hours agoparentprev> Go is seeing adoption that no other “new” language has exactly because of its simplicity Yes - for me, the simplicity is essential. As a part-time programmer, I don't have months to spend learning C++ or Rust. If my project needs to compile to small(-ish) standalone binaries for multiple platforms (ruling out Python, Ruby, Java, C#, etc) what simple alternative language is there? Plain C? reply acdha 5 hours agorootparentBasic Rust doesn’t take months to learn, especially when you’re not trying to do things like distributing crates to other people. I found the compiler to be enough more helpful than Go’s to make them roughly time-equivalent for the subset of common features, especially for a simple CLI tool. reply martindevans 6 hours agorootparentprevC# can compile standalone binaries for multiple platforms. reply pif 10 hours agoparentprev> I’ve previously spoken about my loathing of exception handling because it adds a “magic” layer to things which is way too easy to mess up. I kind of see your point. In this very moment, it doesn't matter whether I agree. What I don't understand, though, is why (typically) people who abhor exceptions are among the fiercest defenders of garbage collection, which does add a “magic” and uncontrollable layer to object destruction. Personally, having learned to love RAII with C++, I was shocked to discover that other languages discarded it initially and had to add it in later when they realized that their target developers are not as dummy as those choosing Golang. reply Mawr 7 hours agorootparentDifferent kind of magic. Needing to account for every single line of code being able to throw an exception is very mentally taxing, whereas the existence of a GC removes mental load of needing to account for every single allocation. reply bsaul 9 hours agorootparentprevHow does RAII works in concurrent systems ? It seems to me you need to add compile-time object lifetime evaluation (as in rust) which so far incurs a high toll on language complexity. reply maleldil 9 hours agorootparentThe exact same way it works in Rust. C++'s RAII works the same as Rust's Drop trait. The object is released when it goes out of scope, and if it's shared (e.g. Arc), it's released when the last reference to it drops. reply troupo 10 hours agoparentprevIt's actually very rare that it should be the caller who has to handle the errors. Go, however, forces you to spread your error handling over a thousand little pieces with zero overview or control of what's happening. Rust eventually realised this and introduced try! and ? to simplify this reply koito17 10 hours agorootparentMore importantly, Rust has the notion of a result type and it is designed to be both generic and composable. A problem I often face in Go and TypeScript code is code that ignores errors, often unintentionally. For instance, many uses of JSON.parse in TypeScript do not check for the SyntaxError that may be thrown. In Go, it is common to see patterns like _ := foo.Bar() // assume Bar() returns error This pattern exists to tell the reader \"I don't care if this method returns an error\". It allows one to avoid returning an error, but it also stops the caller from ever being handle to the error. Also, the position of the error matters. While the convention in the stdlib is to return errors as the final value, this isn't necessarily followed by third party code. Similarly, errors are just an interface and there is no requirement to actually handle returned errors. Even if one wants to handle errors, it's quite awkward having to use errors.As or errors.Is to look into a (possibly wrapped) chain of errors. The benefit of Rust's Result is that - position doesn't matter - there is strong, static type checking - the language provides operators like ? to effortlessly pass errors up the call stack, and - the language provides pattern matching, so it's easy to exhaustively handle errors in a Result The last two points are extremely important. It's what prevents boilerplate like if err != nil { return nil, err } and it's what allows one to write type-safe code rather than guess whether errors.As() or errors.Is() should be used to handle a returned error. reply DanielHB 9 hours agorootparentI am pretty sure if it were for the Typescript creators they would not allow exceptions in the language, but they had to work within the confines of Javascript. Heck they even refused to make exceptions part of the type-system. It is unfortunate that many of Typescript developers still rely on throwing exceptions around (even in their own typescript code). Result types are totally doable in Typescript and you can always wrap native calls to return result types. reply quotemstr 9 hours agorootparentprevWhy would you \"check\" for TypeError being thrown? Just let exceptions in general propagate until they reach one of the few places in the program that can log, display, crash, or otherwise handle an exception. No need to \"check\" anything at call sites. 90% of the criticism of exceptions I see comes from the bizarre and mistaken idea that every call needs to be wrapped in a try block and every possible error mentioned. reply Yoric 6 hours agorootparentprev> Rust eventually realised this and introduced try! and ? to simplify this That was prototyped around Rust 0.4, so I wouldn't say \"eventually\" :) reply liotier 9 hours agorootparentprevUnsure if this is the right place to ask, but this conversation inspires me this question: Is there in practice a significant difference between try/catch and Go's \"if err\" ? Both seem to achieve the same purpose, though try/catch can cover a whole bunch of logic rather than a single function. Is that the only difference ? reply slau 9 hours agorootparentTry/catch can bubble through multiple layers. You can decide/design where to handle the errors. If you don't `if err` in Golang, the error is skipped/forgotten, with no way to catch it higher up. reply theshrike79 9 hours agorootparentprevYou can decide not to catch a thrown exception, it travels upwards automatically if you don't catch it. I think that's the biggest difference. With Go you need to specifically check the errors and intentionally decide what to do, do you handle it right there or do you bubble it upwards. If you do, what kind of context would the caller want from this piece of code, you can add that too. reply stouset 2 hours agorootparent> With Go you need to specifically check the errors and intentionally decide what to do, do you handle it right there or do you bubble it upwards. Is this really all that interesting or worth the LOC spent on error handling when 99.9999% of the time in practice it’s just bubbled up? And any “context” added is just string wrapping. Approximately nobody types golang errors in a way that lets you programmatically know what went wrong, to be able to fix it in-line. I think I would be more empathetic to the arguments defending golang here if I’d ever worked or seen a project where people actually handled errors instead of spending 2/3 of their time writing code that just punts on any error. reply LinXitoW 7 hours agorootparentprevI'd argue that at least checked exceptions also require a conscious decision from you. You either need to add the Exception type to your throws clause, or your catch clause. Compared to Go, this is actually better because the type system can tell you what kind of errors to expect, instead of just \"error\". reply XorNot 10 hours agoparentprevWhen I started trying to teach myself Rust, the error handling story fell apart on me very quick. Like as soon as I wanted to try and get sensible reporting in their, suddenly we were relieving libraries, adding shims and fighting mismatched types and every article was saying the same thing: haha yeah it's kind of a problem. I'm very, very unsold on explicit error handling compared to exceptions for practical programming. The number of things which can error in a program is far larger then those that can't. reply lkirkwood 10 hours agorootparentI felt the same but after switching to anyhow and thiserror in pretty much every Rust project I work on I find it quite painless. It's not ideal to rely on crates for a core language feature but I never find myself fighting error types anymore. Have you tried those crates? Do you still hold that opinion? reply masklinn 9 hours agorootparentYou don’t need crates for it, anyhow is basically a better Box, if you just want the error signal you can use that. The main thing missing from the stdlib fur this use case is I don’t think there’s anything to easily wrap / contextualise errors built in. reply usrnm 10 hours agorootparentprevThe problems you're describing don't exist in go. There is exactly one standard type that is used by everyone, at least in public API's, you can always just return the error to the caller, if you don't want to handle it in place. The main difference with exceptions in my practice is the fact that it's a lot easier to mess up, since it requires manual typing. This is probably my main problem with everything being as explicit as possible: it requires people to not make mistakes while performing boring manual tasks. What could possibly go wrong? reply Yoric 5 hours agorootparentThe drawback, on the other hand, is that all the Go code I've read (including the stdlib and all the dependencies of my current project) is using `fmt.Errorf` or `errors.New`, which means that you can't use `errors.As`, which means that you generally cannot handle errors at all. reply XorNot 7 hours agorootparentprevI think that sort of nails it: the problem with errors as values is errors become part of the type signature and put under user control, and the user can't really be trusted with that power. Even the simplest functions tend to have error states - i.e. floating point math can always wind up handing back NaN. So where I end up is, the default assumption is every function is effectively of a type MightError(T)...so why even make us type this? Why not just assume it, assume the unwrap handling code, and so you basically wind up back at try-catch exception handling as a syntactic sugar for that whole system. reply sureglymop 9 hours agoprevRust and Go are very different and I feel people want a middle ground that just doesn't exist currently. A garbage collected relatively simple language that compiles into a statically linked binary but has a type system similar to rust, rest types etc. Syntactically, Gleam and Kotlin come somewhat close but not really. I like Rust but I do believe it is too complicated for many people who are capable of creating something but are not CS grads nor working as programmers. If you're only gonna use the language every once in a while you won't remember what a vtable is, how and when things are dropped etc. I understand that \"the perfect language\" doesn't exist but I think both Go and Rust brought amazing things to the table. I can only hope someone takes inspiration from both and creates a widely usable, simple programming language. reply prmph 17 minutes agoparentSo here is my take on this, once again: Start with JavaScript. The basic syntax is delightfully direct, it has massive adoption already, the ecosystem is large, the runtimes are getting better all the time, compilation is here with WASM, Now remove the weird parts (e.g., too much flexibility to redefine things, too much use of the global scope, too much weirdness with numbers, etc.), and add: - Types (including sum/product types, Result, Maybe, decimals, etc.) - More functional features (everything-is-an-expression, pattern matching, currying, etc) - A comprehensive standard library. Already this starts to yield a language that has the best chance to be what a lot want. The other major advance in developer tools that I'm wanting to see is revamping HTML to have proper sophisticated controls built-in, controls that can be easily styled with inline CSS. This will reduce the amount of JS needed on the client. These two things will yield a massive advance in programming productivity, at least as far as web-related development is concerned, IMO reply iamcalledrob 8 hours agoparentprevKotlin is interesting as a middle ground, but I still find it much less productive than Go for most tasks, and unsuitable for tasks where you'd reach for Rust. In practice, Kotlin is extremely complicated, and you end up spending time being clever. There are 1000 ways to do things. Operator overloading. Proxies. Properties. Companion objects. Exceptions AND result types... The build system (assuming you use Gradle) is tantamount to torture for anyone used to \"go build\". The coroutines APIs feel simultaneously more complicated and yet more restrictive than Goroutines. More structured but less flexible and more effort to use. Access control feels awkward. There's no way to make a type package-private -- it's file-private or available to the whole module. This leads to either a larger API surface than desired, or the inability to break up complexity into multiple files. Kotlin/Native and Kotlin/JVM really are two different beasts too. Kotlin/JVM is mature, but then you are running on the JVM, so that cuts out a whole class of use cases you might bust out Rust for. There is a very weak ecosystem for Kotlin/Native, and it's poorly documented. There are some scary bugs in the bug tracker. You can't publish source-only libraries for Kotlin/Native either, so you need a complex CI setup to build binaries for every OS and arch under the sun. Or just don't publish libraries at all, which probably feeds in to the weak ecosystem... reply LinXitoW 6 hours agoparentprevImho, most features of Rust that people would like to see in Go would still fit into the \"concept\" of Go. Just like they added generics, they could add just three things: A generic container for errors (Result), one for saner Nil handling (Optional) and a small sprinkling of syntax sugar to make these comfortable to work with (something like an elvis operator equivalent). Go has the one big advantage that is almost solely responsible for it's success: It was created and directly used by a giant company that could afford to create amazing tooling around it and develop great opensource libraries for it. Already being in use and having libraries feel like the biggest determinants of a languages success. reply ivell 7 hours agoparentprevThere is Crystal and Nim. With especially Nim, there is GC and generates c in the end. reply Daegalus 7 hours agorootparentI love Crystal, but the lack of a proper LSP and tooling makes it hard to just jump in and adopt for bigger projects. reply fasterthanlime 8 hours agoparentprev(author here) in which ways does Gleam come short of that? Because I'm also looking for that middle ground and I was very curious to get a look at Gleam. reply trissi1996 7 hours agorootparentIMHO it's just that it's a beam VM language, which is a fatter runtime/ecosystem than is really needed to achieve the goal stated above can bring it's own bag of problems (but also it's own superpowers). Also to be productive you have to utilize the rest of the erlang ecosystem, so at least some superficial knowledge in elixir & erlang is helpful for for some use-cases. Syntactically I actually don't think it's that for off, but I dunno what GP was thinking, maybe that it leans more into functional patterns & sugar for those whereas rust/go can also be used in a procedural style. (Though at least personally I am using way more functional patterns in rust than I expected) reply tessela 8 hours agoparentprev> A garbage collected relatively simple language that compiles into a statically linked binary but has a type system similar to rust, rest types etc. Swift. reply zapnuk 8 hours agorootparentIf they'd drastically improved their tooling then yes. But sadly it's not that easy to create a statically liked binary in swift. The last time i did it it also included the whole runtime, and the resulting \"hello world\" binary was 50mb large. Annoying at least. For years I wished they got their stuff together, but at this point I'd be very suprised. They probably have too much technical dept already due to the support of XXX edge cases Apple need for iOS/MacOS development. reply Terretta 46 minutes agorootparent> For years I wished they got their stuff together, but at this point I'd be very suprised. This is a thing that's having work done, but still too large depending: https://www.swift.org/documentation/articles/static-linux-ge... See here for sizes of various languages as of August 2024 when I'm commenting, but this is Swift 5.10 for Windows: https://github.com/MichalStrehovsky/sizegame Build actions here: https://github.com/MichalStrehovsky/sizegame/blob/master/.gi... What you want, instead, is Swift 6 for Linux, while e.g. Alpine, as of summer, still too large: https://mko.re/blog/swift-alpine-packaging/#:~:text=Package%.... In this thread on Swift for AWS Lambda, a dev gets a Linux static build to 5.9 megabytes: \"Then, I stripped it and it's now 5.9Mb. That's an impressive 86% reduction. Given that musl libc.a is 2.4Mb and libc++.a is 10Mb, I find that 5.9Mb for an executable that contains both libc and the Swift runtime is not that bad :-)\" https://forums.swift.org/t/aws-lambda-functions-and-the-linu... > They probably have too much technical dept already due to the support of XXX edge cases Apple need for iOS/MacOS development. See also embedded Swift: \"Apple explains that Embedded Swift is a subset of the Swift programming language with a much smaller footprint (the binary can be around 10KB...\" https://www.cnx-software.com/2024/06/13/embedded-swift-esp32... reply pansa2 8 hours agorootparentprev> relatively simple reply Daegalus 7 hours agoparentprevI think Inko (https://inko-lang.org/) has the potential to be that language with some tooling/adoption/maturation reply zozbot234 8 hours agoparentprev> A garbage collected relatively simple language that compiles into a statically linked binary but has a type system similar to rust, rest types etc. You just described Ocaml and ReasonML (which is Ocaml with Go-like syntax). reply glass-z13 7 hours agorootparentLast time i tried to install ocaml on windows few months ago i failed to do so, it's a well known thing that it is not 100% supported on windows therefore it wont have the adoption that go/rust has (as it's been the case forever now) reply ogogmad 6 hours agorootparentprevThis doesn't count because the idioms are very different from Go and Rust. And I suspect there's a high learning curve for features like Functors. If you check the Wikipedia page for OCaml to find out where it gets used, you'll see why it's ocaML. That is, you'll notice that it's mostly a MetaLanguage, or a language for writing other languages. The same observation applies to other languages in the ML family. reply ForceBru 8 hours agoparentprev> A garbage collected relatively simple language that compiles into a statically linked binary and has a [good] type system Yeah! Pattern matching too. What are currently available languages closest to this? AFAIK, Gleam relies on a virtual machine, but otherwise seems promising. reply blue_pants 8 hours agorootparentStretching 'currently available' a little, there's Roc lang [1]. Though still in development, you can use it for small personal projects. Another caveat is that it's a functional language, which could potentially hinder its wide adoption [1] https://www.roc-lang.org/ reply cmrdporcupine 6 hours agoparentprev> A garbage collected relatively simple language that compiles into a statically linked binary but has a type system similar to rust, rest types etc. So OCaml then (ocamlopt to do native code compilation) reply fire_lake 8 hours agoparentprevTry F# with the new AoT compilation option and publish single file switch. reply MrBuddyCasino 7 hours agoparentprev> people want a middle ground that just doesn't exist currently https://borgo-lang.github.io/ Rust syntax, compiles to Go. reply neonsunset 4 hours agoparentprevC# and F# will be by far the closest. Other options lack sufficiently good type system or tooling to match either of the two. Compile to static native binary with 'dotnet publish -p:PublishAot=true' (or add this property to .csproj to not specify on each publish). In the case of F#, you will need to use Console.* methods over 'print*' because print has unbound reflection inside for structural output on \"%A\" format specifier (it will work most of the time but negatively impacts binary size and causes the compiler to complain). I can especially recommend F# as \"easier more business-focused Rust alternative\" because it is expression-oriented, has discriminated unions, full HM type inference and gradual typing is a joy to work with. Data analysis and domain modeling are very pleasant to do in it too. For systems programming C# is going to be the option to use - it will give you great concurrency primitives, fast (sometimes even zero-cost) native interop, smaller than Go native binaries and a lot of low-level APIs including portable SIMD. Go is often poorly suited for these tasks or can't do them at all (at least without workarounds). There are many new high-performance libraries focused on this domain as .NET gains popularity in non-gaming communities in this area. And of course you benefit from a huge existing ecosystem and won't have to do the all the heavy lifting by yourself unlike in niche languages suggested in sibling comments. reply spirit-sparrow 8 hours agoprevI wonder what makes someone go such a great length to bash a language, any language. I say bashing, because even the few valid points in the post are not written in a constructive style. After all is there a language that can't be criticised? Is the post written to make one feel better having a failed a project the language? (It's not me, it's the language) Or is it the failure to understand that not everyone thinks / works the same and what one finds unacceptably bothersome, others barely notice? Languages that do not cater for a real need would likely vanish on their own, they rarely need help. As for Go, despite the differences compared to \"more sophisticated\" languages it works brilliantly for the projects I've encountered. I hope the author is not forced to work with Go though. For the rest of the community, we keep churning out services, giving our feedback to the Go team and seeing the slow evolution of the language without breaking our stuff in production. reply pas 7 hours agoparent> what makes someone go such a great length to bash ... \"\"\" Inherent complexity does not go away if you close your eyes. When you choose not to care about complexity, you're merely pushing it onto other developers in your org, ops people, your customers, someone. Now they have to work around your assumptions to make sure everything keeps running smoothly. And nowadays, I'm often that someone, and I'm tired of it. \"\"\" > [Go] works brilliantly for the projects I've encountered. Of course, C, C++, PHP and JavaScript works too! Of course many many many things \"work\" in our world. Of course just adding one more lane works too, of course police states work too! Yet something else would work even more brilliantly? reply madeofpalk 7 hours agoparentprevIt's just some person's blog and they're having a rant. It's okay, it doesn't have to be that deep. I would guess the 'why' is because OP feels like they have an opinion that they don't feel is sufficiently represented 'out there'. Indeed, as a not-a-fan-of-go, in 2022 I was confused at go's popularity because it always felt to me to have some pretty glaring shortcomings that were seemingly ignored. Note that people don't really write big blog posts about PHP being a bad language (anymore?) because that's been done to death. reply lexicality 7 hours agoparentprevPerhaps not everyone likes boring clinical reviews and some people like ones that have a bit of passion and humour in them? Just because this blog post isn't written in a way you like doesn't mean it doesn't have value to others. reply LinXitoW 7 hours agoparentprevOne very subjective, very irrational factor for my borderline hate for Go is that for years the Go zealots gaslighted everyone about every single part of Go. Anything that Go did, no matter if it was the most basic implementation or if other languages already did it (better), was essential, the best and only way to solve that issue. Anything Go did not do was superfluous and downright a conspiracy by Big Complexity to keep us unenlightened Non-Goers addicted to the Syntax Sugar: Things like sane null handling, sane error handling, less boilerplate, generics, or not creating special cases for everything (generics and tuples) instead of developing one cohesive solution. Even now, in this thread, the strawmanning continues: Error handling is brought up, and instead of admitting the unmistakable truth that Gos error handling could be much better (s. Rust), people bring up things like JavaScript. As if anyone criticizing Go that JavaScript was the pinnacle of error handling. reply Seb-C 5 hours agorootparentYes, I've also grown tired of this toxic mindset, as well as the whole \"idiomatic Go\" dogma that is very often an excuse for poor engineering practices. reply Groxx 2 hours agorootparentprevYeah, this was/is a part big part of my frustration with the ecosystem too. It set a LOT of very problematic patterns in place in the beginning, and many of them are still not unwound. Stuff like \"Go doesn't need dependency injection because it's simple\". I heard that literally dozens of times. The opposite is true! It's an even bigger pain without DI because the language is so simple! DI everything or make your users suffer! Or a personal favorite: \"Go doesn't need a package manager\". We see where that went, and how much insanely better it is now that we have it. Or errors. Without wrapping. Oh boy we're gonna pay for that for decades. There's stuff to like in the language, but the zealots (especially early ones) really did not know which parts they were. reply KingOfCoders 9 hours agoprevGo: \"I'm a simple language!\" User uses Go for some time. User: \"I hate you, you're a simple language!\" Perhaps it's because I'm 50+, I love a simple language. I feel the \"critique\" is not very balanced, and I view judgements that are not balanced as weak, as everything in technology is about tradeoffs. I of course come to a different conclusion: https://www.inkmi.com/blog/why-we-chose-go-over-rust-for-our... reply serial_dev 8 hours agoparentVery common misrepresentation of any critique of Go. \"You just don't get simplicity, you got them Java brainwormz\"... There are many examples in the article that point out the annoying inconsistencies in the language, those are the opposite of simplicity. I love Rob Pike's presentations on Go, some of them were eye-opening to me. However, I just wish that the Go I see in practice would be much closer to the Go language that Go-fans describe in abstract. reply zozbot234 8 hours agoparentprevScheme is a simple language, Go just hides complexity until it blows up in the worst possible way. (Of course, most reasonable alternatives to Go are even worse from that POV. See Python, Ruby, JS etc.) reply LinXitoW 6 hours agoparentprevGo is not simple, it's easy. The difference, for example is: Go invents an abstraction just for one use case, and just for the standard library/runtime itself, instead of taking the time to create a universal version of that abstraction. Generics for maps and lists, and tuples for error handling. reply heresie-dabord 7 hours agoparentprev> Perhaps it's because I have experience, I love a simple language. (fixed the statement to focus on your value, not age per se) I love language ergonomics above all. Python wins. But for runtime bang-for-the-buck, Go wins. reply Seb-C 4 hours agoparentprevThe problem is that Go is not designed to be a simple language for it's users, it's designed to be a simple language to implement for it's maintainers. In my opinion, a simple language should be highly consistent (have a few rules, but which are universal and consistent everywhere). Instead we have a language with weirdnesses, inconsistencies and workarounds all over the place. A good example is type elision: it's available when declaring arrays, slices and maps, but not for structs. Allowing that would have several benefits in terms of readability, and also allow named parameters via anonymous struct arguments (which would greatly improve the design compared to the ugly workarounds that are currently used). reply richbell 9 hours agoparentprevI wish the discourse that Go is a \"simple\" language would die. Despite its veneer, once you start writing Go it quickly becomes apparent that it isn't simple. Hidden complexity and footguns are abundant (e.g., https://archive.ph/WcyF4). It's nevertheless a useful language, and I use it quite a bit, but it's not \"simple\". reply jonathanstrange 9 hours agorootparentI have no idea why anyone would say it's not simple, it's super-simple. Learning how duck typing works with interfaces and how to use it is perhaps the only hurdle. In my experience, only certain BASIC dialects like VisualBasic are simpler. reply LinXitoW 6 hours agorootparentI think the sticking point is what people mean when they say simple. To me, and likely to many saying Go isn't simple, simple is not a synonym for easy. Go is easy, but it is not simple. For example, solving the problem of generics in a generic way from the start so the same problem can be addressed in the same way everywhere, would be simple, but maybe not (as) easy. Contrast that to giving the runtime/standard library a special exception with maps and lists. That's easy, but not simple. People used to literally use code generators or weird UTF-8 characters to fake generics, that's not remotely simple. reply nulld3v 9 hours agorootparentprevThis 100%, I was just about to type a long rant up about this. There are so many weird parts of the language that took me forever to grasp, and in many cases, I still don't have an intuitive grasp of things. And plenty of other examples that aren't in that article: - You have a struct with an embedded interface. Does the outer struct satisfy the embedded interface? And can I type assert the outer struct into whatever embedded struct is fulfilling the inner interface? - When should I pass by value and when should I pass by reference? Like I generally know when to choose which, but do I really know without performing a benchmark? And what about arrays? Should I store pointers in them? But it also seems that people just don't care and just randomly roll the dice on when to return a pointer or a value? - Shorthand variable declaration. How does it work when you shorthand declare two variables but one of them already exists? Don't both answering the questions, that's not the problem. The problem is that it's just not intuitive enough such that I'm confident I know the correct answer. reply dondraper36 9 hours agorootparentI am not going to answer the questions, but this is a very strange complaint, to be honest. For example, passing by value/passing by reference is something covered immediately in the Go FAQ document once and for all. Everything is passed by value in Go, that is it. There should be no confusion at all. If you spend 15 minutes reading Russ Cox's post on the internals of the most common data types, you will also understand what data structures have implicit pointers under the hood. reply nulld3v 8 hours agorootparentWell yes obviously I know everything is passed by value, just like in literally every other popular language. I'm talking about the difference between pointer parameters/receivers vs value parameters/receivers. reply KingOfCoders 9 hours agorootparentprevYour thinking is too complex for Go. You might be better of with Rust. Same about benchmarking, if you want and need the fastest code, or the best memory management, use Rust. If you need something faster than Python in general but not the fastest, use Go. reply nulld3v 8 hours agorootparentBut that's the thing right? Like I come from Java. In Java, we have objects. They are pointers. That's it. You don't get to decide on whether you want a pointer or a value (I guess primitives are an exception lol). But it was so simple! And same in JavaScript. Everything is a pointer except primitives. That's it. End of story. And I have written Rust too, and while the situation is definitely more complicated there, the guidance is extremely simple and straightforward: If the struct implements Copy, then it is very cheap to copy and you should pass by value. Otherwise, you should pass by pointer/reference. And meanwhile in Go, I just see pointers and values being used seemingly interchangeably, and seemingly at random. reply ralegh 9 hours agorootparentprev> but do I really know without performing a benchmark? Not really. But that’s one of Rob Pikes rules [1], I think the intention is to write whatever is simplest and optimize later. The programmer doesn’t need to remember 100 rules about how memory is allocated in different situations. [1] https://users.ece.utexas.edu/~adnan/pike.html reply nulld3v 8 hours agorootparentI mean it's a great idea, and I fully agree that I do not want to worry about memory allocation. So then why is `make` a thing? And why is `new` a thing? And why can't I take an address to a primitive/literal? And yet I can still take an address to a struct initialization? And why can't I take an address to anything that's returned by a function? reply assbuttbuttass 4 hours agorootparenthttps://go.dev/ref/spec#Address_operators reply LandR 8 hours agorootparentprevIt's easy not simple, but the consequence is that any complexity that other languages handles for you, in go gets forced onto the developer. There's more stuff to think about, because the language is doing less for you. reply KingOfCoders 9 hours agorootparentprevYes, like \"Opening Brace Can't Be Placed on a Separate Line\" (from your link). Everyone can read Go code and understand what happens. There are some minor difficulties like func (*A) vs func (A)., reply eru 9 hours agorootparentRiscV assembly is even easier to read by that metric. reply KingOfCoders 8 hours agorootparentMy assembler days were 4 decades ago, but \"Everyone can read Go code and understand what happens.\" There seems to be a difference between \"easy to read\" and \"understand what happens\" - or what happens on what level. The challenge is that there is a tradeoff between the two. Assembler is too low to understand what \"really\" happens, on the other hand Haskell for example with Monad stacks is again very easy to read + understand what happens \"most of the time\", but hard to understand all the abstracted away side effects. In Haskell with add 3 5 everything can happen beside what you see. In assembler ld a, 3 add a, 5 nothing happens except these two instructions. The tradeoff is how much you want to be explicit, with the downside of creating too much noise, and how much you want to abstract away, with the downside of magic happening somewhere. reply eru 8 hours agorootparentYes. And to be less snarky: Go doesn't really sit on the efficiency frontier here: It requires you to write a lot of stuff by hand and is incredibly verbose, but it also does a lot of magic behind the scenes. reply GaryNumanVevo 8 hours agoparentprevGo is \"simple\" insofar as you're doing simple things. If you've tried writing a KV store or database (as I have) you'll quickly find yourself wanting slightly more modern language features. reply KingOfCoders 8 hours agorootparent\"quickly find yourself wanting slightly more modern language features.\" Use the tool that works for you. reply GaryNumanVevo 8 hours agorootparentFeel free to take a look at any more complicated GoLang code (k8s, gorm, etc) and you'll see that the tool/library you're depending on requires a veritable rats nest of bad practices to work around the Go's inherent limitations. reply matttproud 7 hours agorootparentMy opinion on Kubernetes as a yardstick example of Go is this: This was one of the first large systems developed in Go, not too long after the language hit version 1.0 (ca. 12 years ago). What constituted good Go style and package architecture were not well known at that time even. Given that and hindsight being 20:20, I could easily imagine the internal architecture (not even the public surface) for Kubernetes being a lot different and simpler. The package architecture alone makes me cringe. As a hypothetical counterpoint, I wonder what Kubernetes would have looked like had Dave Cheney (https://dave.cheney.net/) and Rob Pike built it in that era. I think that would have been a better yard stick. I have a unique appreciation for this first large systems perspective as I co-designed an adjacent product in the cloud native ecosystem at that time (Prometheus). There was no good example to follow when it came to large program structure and design in Go at that time. We were all figuring that out for ourselves — organically. I think this point about organic evolution of the architecture is important to call out explicitly, because developers often look at an existing structure and essentially mimic it with their additions, changes, and refactors irrespective of whether the structure was correct for the problem it was trying to solve or even good. Given that reality, is it really any wonder that one of the first major pieces of software ended up being this metaphorical mess? And the truth is that's not a language-specific problem: it could have happened with any new language. And taking a legacy system of this age and refactoring is difficult from a social perspective, ignoring the gradient/cost of refactoring that is language-specific. You'll have a lot of people who will oppose structural change just because, so probably a significant refactoring to achieve these goals is just not in the cards. Had there been so much as something similar to https://google.github.io/styleguide/go at that time (ca. 2012) (it's based on the very spartan https://go.dev/wiki/CodeReviewComments), that would have been tremendously useful and impactful in helping preserve simplicity. reply jchw 10 hours agoprevEvery time I read a critique of Go, I feel the same way: I'm still going to continue to use it anyways. The reason why I am going to continue to use it anyways is because while I understand that it has plenty of easily documented issues in theory (and that people regularly do actually run into in practice), I still find that it is one of the better programming languages in practice anyways. Some of the things that people commonly list as shortcomings I don't agree with (I like the explicit error handling everywhere) and for other things... I often agree, but it doesn't bother me much more than the shortcomings of other programming languages, of which there is certainly no shortage. I guess I feel bad for people who are particularly sensitive to the areas that Go does not succeed in, because they are probably going to be complaining about it for the rest of their lives. The truth is though, I don't use a bunch of rationale about what is the best programming language to choose which one to work on for a project, I choose a language that I feel works well for me, that I feel I can consistently write good software in, that feels, well, nice to work in. Being a person that values correctness, I do sort of wish that language could be something more like Rust, but for now, it's just not, though it's not like I hate Rust, it's just not what I reach for in a pinch. Enough has been written about how terrible Go is by this point. At least now I know what it's like to have been a fan of PHP a few years ago! (That's an exaggeration, but it's not that big of one in my opinion.) reply wokwokwok 9 hours agoparent> I guess I feel bad for people who are particularly sensitive to the areas that Go does not succeed in, because they are probably going to be complaining about it for the rest of their lives. Well, that’s a stellar endorsement of the article, because that’s literally the point they’re making. You’ll use go. …and then regret it. …but by then it’ll be too late, and you’re stuck with it. I think the author makes a compelling argument, which is very difficult to counter, that it is a bad choice and you will regret having it in production in many of the explicitly listed cases, and in many professional situations where companies that are not technically competent use unsuitable tech. Companies should stick to boring tools. …but, for personal projects? Sure, go for it. reply jchw 9 hours agorootparentThere's no tool boring enough to prevent any chance of regret. At the end of the day, it's really, really difficult to anticipate where your pain points will actually wind up in the real world. In practice, I've had lots of really good production success with Go and not too much heartache about the choice. Since adopting it personally (in around 2014) almost every company I've gone to work since has used Go in some capacity and that capacity was usually growing because it was working for them. Will you regret choosing Go? Maybe. Or, maybe not. reply coldtea 8 hours agorootparent>There's no tool boring enough to prevent any chance of regret. I'm not so sure. I know C programmers that swear by it, warts and all, with absolutely zero regrets for using it e.g. in the embedded space. reply jchw 7 hours agorootparentIf anyone tells you C is \"boring\", that's just plain and simple bullshit. C gives you undefined behavior, buggy compilers (yes even for simple C code, MSVC is especially bad at plain C but there are other offenders) and the world's worst standard library for manipulating strings. Using C in embedded development is probably OK, even if you have to suffer with whatever crappy vendor compiler you are stuck with, but that's only considering the pretty severe limitations that very resource-constrained embedded development typically has (e.g. no dynamic allocation.) C is only as boring as you force it to be, and you really have to force it to be. That said... the thing about the embedded space is that most of it is C, always will be, and may continue to be for the foreseeable future. It's really hard to really know what you have to regret if all you've ever known is C. reply coldtea 7 hours agorootparent>If anyone tells you C is \"boring\", that's just plain and simple bullshit. C gives you undefined behavior, buggy compilers (yes even for simple C code, MSVC is especially bad at plain C but there are other offenders) and the world's worst standard library for manipulating strings. Boring doesn't mean \"has no issues\" or \"takes care of everything for you\". It means, stable syntax, lots of mature tooling and compilers, and any problems the language has are well known. Of the various languages around, C is the least likely one to have \"buggy compilers\" (issues in MSVC because MS compilers focus on C++ are a different thing, also falling in the \"known\" category). reply jchw 2 hours agorootparent> It means, stable syntax, lots of mature tooling and compilers, and any problems the language has are well known. In case of C what it really means is that the compiler codebases are extremely old and sometimes in rather bad shape. Speaking of stable, C hasn't just stayed still: The C23 standard adds plenty of fun new things, like an annotation for unreachable code. Modern C has threads and synchronization primitives, #embed, complex numbers, and plenty more. > Of the various languages around, C is the least likely one to have \"buggy compilers\" C compilers are still routinely buggy, because the memory model is surprisingly tricky, there are many edge cases in the specification that are subtle and despite being fairly basic are not stressed often enough to come up, and because optimizing C while adhering to its rules about defined behavior is an unending arms race about what is still \"technically\" compliant with the standard. Again, this is",
    "originSummary": [
      "The article critiques Golang, noting that while it has appealing features like an asynchronous runtime and garbage collector, it also has significant drawbacks, such as the absence of sum types and inadequate error handling.- It argues that Go's perceived simplicity can be deceptive, leading to hidden complexities and challenges in production environments, and stresses the importance of recognizing these issues rather than adopting Go solely for its ease of use.- The article contrasts Go with Rust, suggesting that despite Rust's challenges, it provides more robust solutions for managing complexity and ensuring code correctness."
    ],
    "commentSummary": [
      "The article critiques Go's error handling, contrasting it with Rust's more sophisticated approach, which some find more elegant.",
      "There is a divide in opinion: some appreciate Go's simplicity and explicit error handling, while others find it tedious and error-prone.",
      "Despite criticisms, Go remains popular due to its simplicity and widespread adoption by large companies."
    ],
    "points": 281,
    "commentCount": 366,
    "retryCount": 0,
    "time": 1732608093
  },
  {
    "id": 42242971,
    "title": "Cybertruck's Many Recalls",
    "originLink": "https://www.wired.com/story/cybertrucks-many-recalls-make-it-worse-than-91-percent-of-all-2024-vehicles/",
    "originBody": "CARLTON REID GEAR NOV 25, 2024 7:00 AM Cybertruck’s Many Recalls Make It Worse Than 91 Percent of All 2024 Vehicles Since launch, Tesla’s polarizing electric pickup has been beset by quality issues, and is now heading to be one of the most unreliable EVs made yet. Strangely, Cybertruck owners may not care one bit. PHOTO-ILLUSTRATION: WIRED STAFF/GETTY If you buy something using links in our stories, we may earn a commission. This helps support our journalism. Learn more. Please also consider subscribing to WIRED CYBERTRUCK OWNERShave discovered that a chunk of these stainless steel electric pickups could “stop producing torque” while thrumming along. The fault was noted by Tesla in a November 5 National Highway Traffic Safety Administration recall. Any sudden loss of propulsion might “increase the risk of a collision,” added the filing ominously. The latest recall—the wedge wagon’s sixth this year—requires shop time, not an over-the-air (OTA) update. Reports of instantaneous loss of e-horsepower due to duff drive inverters—devices that convert DC to AC and control an EV’s motor speed and torque—might alarm the average EV motorist, but to extrovert buyers of Elon Musk’s flagship, it’s evidence of the angular pickup’s edginess, Ivan Drury, the director of insights at car shopping guide Edmunds, told WIRED. “The people drawn to [the Cybertruck] don’t have quality of construction or safety at the top of mind,” he says. “That this could be a dangerous vehicle to drive is key to its appeal. Nobody’s buying it to use as an actual truck.” Stomping on the accelerator pedals of the affected Cybertrucks might get their drivers nowhere—a downer, you’d think, for a vehicle that’s faster to 60 mph than a Lamborghini Aventador—but, adds the Hawaii-based Drury, that won’t harm the pickup’s reputation with many wannabe owners. “The more the press and others say it’s bad, the more that [typical Cybertruck customers] see it as good,” says Drury. Mocked online as the Cyberbrick, the car launched in December 2023; Tesla had apparently sold 27,000 of them by October this year, as could be inferred by the number of Cybertrucks listed on the fifth recall. Some 2,431 Cybertrucks were affected by the sixth recall, requiring vehicle drop-off for new drive inverters to be fitted. The recalled drive inverters—equipped with potentially faulty metal-oxide-semiconductor field-effect transistors—were fitted to Cybertrucks manufactured between November of last year and the end of July this year. “It’s common for all-new models to have increased recalls in the first year after launch,” Karl Brauer, executive analyst at car ranking service iSeeCars, tells WIRED. “Where it gets interesting,” Brauer added, “is how quickly this initial spate of recalls falls off after launch versus continuing as the years pass.” MOST POPULAR GEAR The WalkingPad C2 Mini Treadmill Feels Stable Under Your Desk and Feet BY KRISTIN CANNING The Insta360 Link 2 Is a Great-Value Motorized Webcam BY SAM KIELDSEN GEAR Samsung’s Galaxy Book5 Pro 360 Has Too Many Bugs for a $1,700 Laptop BY CHRISTOPHER NULL GEAR The Clip Converts Your Regular Bike Into an Electrified Ride BY STEPHANIE PEARSON And on this metric, the Cybertruck might be wanting. “Cars with ongoing recalls well after launch suggest a much higher lifetime recall total,” says Brauer. He calculates that the Cybertruck’s six recalls to date are “worse than 91 percent” of other 2024 vehicles. All this was possibly foreshadowed in 2023 when a leaked Tesla report showed the Cybertruck had basic design flaws. “We aren’t comfortable making [lifetime recall] predictions on the Cybertruck at this very early stage,” stresses Brauer, “but so far it isn’t doing very well.” Most Cybertruck buyers pay scant attention to longevity estimates, believes Edmunds’ Drury, and plenty probably aren’t exercised by recalls, OTA or otherwise, he said. “Cybertruck customers are in it for the stares and glares—they don’t care about how many times [this vehicle is] going to be recalled over 30 years,” says Drury. “They’re buying this car for now, with zero thought to the future.” “A standard auto customer wants to know if a car will last 10 years or will be ongoing good value for money,” he says. “A Cybertruck customer doesn’t care about any of that. Owning a Cybertruck isn’t practical; it’s a boast. A boast that ‘I have so much discretionary income I can afford to waste it on an impractical car.’” Similar to other critics (earlier this year, a CNN reviewer called the pickup a “disturbing level of individual arrogance in hard, unforgiving steel”), Drury believes Cybertruck buyers are people “who think, ‘I don’t care if I kill people when I drive this thing down the street,’” he says. “There aren’t many of those people out there, so there’s a relatively small market for the Cybertruck.” If Tesla, which was contacted for this piece, is genuinely fishing in smaller pools than originally anticipated by Musk—at a 2023 shareholder meeting he predicted that Cybertruck sales could hit 250,000 by 2025, and reach 500,000 a year once production ramped up—this might help explain the softening of Cybertruck aftersales. “Used values on these things have plummeted dramatically,” said Drury. Price tracking website CarGurus estimates that the average used Cybertruck dropped from $175,000 in April to $110,864 today. The cheapest Cybertruck on Autotrader was $86,000 earlier this week, and many of the other 276 currently listed on the site sport “recent price drop” banners. MOST POPULAR GEAR The WalkingPad C2 Mini Treadmill Feels Stable Under Your Desk and Feet BY KRISTIN CANNING The Insta360 Link 2 Is a Great-Value Motorized Webcam BY SAM KIELDSEN GEAR Samsung’s Galaxy Book5 Pro 360 Has Too Many Bugs for a $1,700 Laptop BY CHRISTOPHER NULL GEAR The Clip Converts Your Regular Bike Into an Electrified Ride BY STEPHANIE PEARSON Aftermarket prices might soften further as Tesla is starting to deliver a new $79,990 model to reservation holders. Not that there’s any longer much need to place reservations—North American Tesla stores now accept walk-in orders for Cybertrucks, with delivery two or three weeks later. “I am calling it: Original reservation list is basically finished,” said reservationist BayouCityBob on a Cybertruck owners forum last month. Tesla had claimed to have banked more than 1 million $100 prelaunch reservations for the Cybertruck. “I was thinking I have to wait a couple of [years] before my time comes,” MC1987 responded to BayouCityBob, citing his invitation to purchase his second Cybertruck. (The poster had returned their first, a top-spec Cyberbeast, because of alleged “build quality issues.”) “This is so wild,” they said. As most other parts of the world have yet to sanction Cybertruck sales, Tesla can’t boost take-up outside of North America. UK automobile listings website Carwow describes the Cybertruck as a “rolling axe head,” a nod to the fact that the sharp-angled pickup is literally too edgy to meet strict European pedestrian-safety regulations. Nor can Tesla rely on the US consumer’s love affair with pickups. “Something like 70 percent of all truck sales involve a truck being traded in,” says Drury. “This isn’t the case with [the Cybertruck],” he revealed, using Edmunds’ trade-in data. “While Cybertruck hasn’t been on the market too long, it’s been long enough for us to capture some of the used ones. Because there’s no sign that Cybertrucks are being traded for trucks—which is what we typically see in America—then this likely isn’t a vehicle being used for trucklike purposes,” says Drury. While the Cybertruck’s six recalls this year might not alarm “edgy” consumers, the bad press that often results won’t impress Tesla shareholders—higher-than-average recalls could tarnish the greater brand. Any spike in general automobile recalls should not necessarily worry consumers since defects range widely in severity, and very few are stop-sale orders or demands to immediately cease driving any particular model. Automakers might hate to file them, but recalls demonstrate that the regulatory system is working as designed. However, with Musk advising the US government—even if it’s at arms-length—some regulators might get their wings clipped, perhaps even reducing the number of product recalls, potentially increasing danger for consumers. Not all Cybertruck owners will be too fussed about that, though.",
    "commentLink": "https://news.ycombinator.com/item?id=42242971",
    "commentBody": "Cybertruck's Many Recalls (wired.com)267 points by marban 12 hours agohidepastfavorite393 comments LeoPanthera 12 hours agoThe article notes that the most recent recall was a physical one, not a software update, and implies that that was unusual. I wonder if you exclude \"recalls\" resolved by software updates, for all cars, where it would rank then? reply stackghost 11 hours agoparent>I wonder if you exclude \"recalls\" resolved by software updates, for all cars, where it would rank then? Why have you put \"recalls\" in quotes? It gives the impression you think this makes it somehow lesser. The cybertruck, for example, was subject to a recall because the rearview camera wouldn't come up, but the mirrors are insufficient to back the vehicle up safely without the camera. That's a safety issue, irrespective of whether or not the fix was in software. reply imiric 11 hours agorootparentThe problem is that those issues shouldn't happen on a public road vehicle to begin with. Tesla's approach is shipping beta software to customers, and using them as testers. This is an insidious practice in modern software development, but is criminal when that software is running a 3-ton vehicle, regardless if it can be fixed with an OTA update or not. There are reasons why strict car safety regulations exist. You can't just sell early access cars and fix issues as customers experience them. reply graemep 10 hours agorootparentI wonder whether OTA updates being possible encourages manufacturers to be sloppy with software quality. They know they can fix problems cheaply. If they had to physically update vehicles they would have a lot more incentive to make updates unnecessary. reply gambiting 7 hours agorootparentAbsolutely. It's easy to see that with Volvo - they had their own operating system called Sensus that could only be upgraded at the dealership and you know, maybe it wasn't the flashiest thing but it worked fine. I've owned a car with it for nearly 5 years and it has never crashed for me once or really did anything weird - it's as stable as any automotive software should be imho. And then they swapped over to AOSS(android based) and it's a complete mess, people have been complaining about bugs and crashes literally constantly, and forums are just full of people going \"are you on 3.452.123? They fixed that bug in 3.465.234, you need that update\" and the updates literally come out every week, but now owners are worried whether an update actually fixes anything or of it makes things worse again. And I 100% blame the OTA updates for this, they just come in fast and thick clearly without the engineering rigor that automotive software should have. reply brookst 5 hours agorootparentIs AOSP running any of the critical systems that would make the car undrivable or unsafe if it crashed? Or just IVI stuff? If the former that is horrifying. reply gambiting 5 hours agorootparentIt's running the infotainment display(the one in the centre of the car) , but not the driver display. Fortunately I've never heard about the driver display crashing while driving, so at least Volvo had the good sense of making sure that is solid - but the infotainment display is known to crash frequently requiring a hard restart(you hold the button under the display for at least 10 seconds then the entire thing reboots), which maybe it's critical but means you can't change any of the climate controls(other than windows demist since it has its own button, thankfully) while the screen reboots. reply sulam 30 minutes agorootparentCorrect, the car can function without the infotainment system but you lose conveniences like climate control, the backup camera, and sometimes even audio, which is weird because you get no feedback from the turn signals. It’s not great but not an immediately pulling over kinda thing. reply myrmidon 9 hours agorootparentprevAs an embedded software developer: 100% yes. Having a cheap OTA option will always lead to more sloppiness in development; the problem is that this makes it possible and easy to delay \"features\" in general when software projects are behind schedule (instead of delaying everything), these delayed features/releaes alone are then additional surface for bugs/regressions to slip through testing. For the manufacturer this is still mainly a win I'd say... possibly even for the customer (because he gets features and fixes faster, at least), even though it sounds bad. reply sulam 27 minutes agorootparentI would argue that often the car or other hardware being delayed is not an option and so you get rushed software anyway, that has to be dealer serviced. We’ve seen that a lot in cars now that they’ve started having non-trivial amounts of software that is expected to multi-task. I don’t even know how many networks your typical EV has, it’s usually at least 4 or 5. reply _puk 9 hours agorootparentprevOf course it does. Think about how, even in software, knowing that a physical shipping of a product (CD / Blu-ray) can be updated from \"day 1\" has led to poor quality releases with last minute patches. The cost of having to physically recall / resend CDs back in the day meant that what went out had to work. The cost of sloppy software has now been externalised. reply imiric 8 hours agorootparentExactly. Back in the day it was a major milestone for video game projects to \"go gold\". It meant that the rigorous QA process was passed, and that retail copies were ready to be manufactured. Since that involved significant costs and physical logistics, companies certainly didn't want major bugs shipped at this stage. There were some exceptions, but ultimately this led to much higher customer satisfaction. Removing this \"inconvenience\" for companies and allowing them to ship updates and fixes at any point is a major reason why most modern game releases are a clusterfuck at day one. They treat it as public betas, and day one customers (or, worse, preorder suckers) are testers and a replacement for a QA process they don't have. It's essentially crowdsourced development. This also allows them to hype the game up to boost initial sales, and then go \"oops, sorry\" while they finish implementing it. This scam has been pulled numerous times over the past decade+, yet these companies keep doing it because it's profitable and there's no legal accountability for it. The fact these practices are now seeping into software on which human lives depend is criminal, and should be prosecuted and strictly regulated. reply liontwist 3 hours agorootparentIt’s also bad for preservation because the disc doesn’t actually have the playable game anymore. reply imiric 15 minutes agorootparentPhysical media is almost entirely dead, and will not exist in a few years anyway. So, yeah, the move to digital releases, pervasive DRM, and live service games make preservation difficult. I hope industry heads realize that they're actively pushing consumers towards piracy. These days you often get a better experience with a pirated game than a legally bought one: it runs better without DRM and doesn't infect your system, you can play it offline, and no license change will take it away from you. At least GOG seems to be the only store giving you DRM-free and offline installers, but not all games are released on it. reply TeMPOraL 2 hours agorootparentprevIndeed; at this point, a disk is just treated as a cache for speeding up the initial installation. reply thrw42A8N 5 hours agorootparentprevI get it's wrong in cars. But who the hell cares about games? So your game updates after a month with a fix. So what? It's not like you pay for the update. reply lancesells 5 hours agorootparentPeople who pay for a working game to play probably care. I'm not a gamer but I occassionally will try to pop something in every six months or so. Two hours later after I have to sign in from a controller, update the OS, then update the game I'm over it. I can't say that's all from shitty software development but some of it probably is. reply brookst 5 hours agorootparentprevIt’s bad business because it gives your most enthusiastic customers the worst experience. It’s good business because it pulls revenue forward and sometimes pulls revenue into higher seasonality. Those two somewhat offset and you end up with the classic business decision of whether quality is important. reply TeMPOraL 2 hours agorootparentprev> So your game updates after a month with a fix. Tell that to your kid this December, when you gift them a new console and a newly released game. Watch the wonderful experience of it taking forever to download a few gigabytes of patches from overloaded servers, and then not starting anyway because auth/billing/DRM/other bullshit server couldn't handle the demand. Game studios love releasing around Christmas. I'm surprised they're not being held accountable for ruining that holiday for kids year over year. reply ssl-3 4 hours agorootparentprevWho cares about a $183bn industry, indeed. reply imtringued 5 hours agorootparentprevThere are lots of studios who just run away with your money and never deliver the fixes. reply r2_pilot 4 hours agorootparentI'm still deeply disappointed about Kerbal Space Program 2,for that matter. reply mrguyorama 1 hour agorootparentprevPeople say this a lot but there were in fact examples of software being released back before ubiquitous internet patching that were broken. For example, no copy of Space Station Silicon Valley for the N64 is completable because of a bug that prevents you from collecting the final plot device. I'd say it's definitely worse now, with nothing meaningful being on the disc you pay $70 being insane. reply imiric 34 minutes agorootparentLike I said, there were some exceptions. But the current state is reversed: most games ship broken/incomplete at launch, and the exceptions are when a game is actually complete and relatively bug-free. It's not a matter of not relying on patches or updates altogether. The issue is when companies use this as a crutch to offset their shoddy development practices. Or worse: scamming consumers into buying their broken/incomplete game, and then taking years to deliver a stable experience or what they advertised (No Man's Sky, Fallout 76, Cyberpunk 2077, etc.), shutting down (Redfall, The Day Before, etc.), or straight up taking the money and running. Consumers are lucky if they even get their money back. Thankfully most storefronts have decent refund policies, but that's not an excuse. The saddest thing is that a lot of people either don't care, or end up forgetting about this if companies appear to work hard to ship updates, which is how they eventually gain their good will back. The reputations of studios like Hello Games, CDPR and even Bethesda are practically untarnished despite of their scams, which gives them the freedom to keep doing this. reply tzs 2 hours agorootparentprevIt sure seems to be that way with pure software products. I'd not be surprised if is the same with embedded software in things like cars if they can be easily updated. Back in the days before home internet was common I worked at a place that developed software for personal computers. The software was sold in retail stores such as Egghead, CompUSA, etc on floppy or CD. Management was very insistent on high software quality. If a bug got through that was bad enough that an update to existing customers was needed the only option, aside from the small number of customers who would be able to use a modem to download an update from a dial up BBS, would be for us to mail them a floppy or CD with the update. Even minor bugs that could be easily worked around by the customer were bad, because we had no means of contacting our customers except by writing or calling those who had filled out and returned in the prepaid registration card that was in the box when they bought the software. Instead what would happen is they would call our toll free support line. Remember, this was not a subscription service. It's a one time purchase. If someone needs a long support call to solve a problem that could easily wipe out all profit made on that customer and more. This wasn't just about bugs. If they had trouble using some feature due to poor UI or inadequate documentation that also meant calls to the toll free support line and so management also was very insistent on clear, well written, comprehensive documentation. Compare to today. Updates are easy, you can have FAQs and searchable support articles on your website, someone will make a Reddit group where your customers can help each other, there will be bloggers and YouTubers posting tricks and solutions that in the old days would have been in the manual, and no one expects everything to work in any given release. reply silon42 8 hours agorootparentprevIMO... every OTA/mandatory update should make warranty be extended, as a minimum reply brookst 5 hours agorootparentGreat way to eliminate fixes for minor issues. reply mywacaday 6 hours agorootparentprevIts not just car manufacturers, ever since SW stopped shipping in CDs the pressure to just ship and fix it with a patch has permeated everywhere. If there was no further opportunity to fix later I wonder would the Boeing issues have been as prevalent. reply avar 4 hours agorootparentprev> There are reasons why strict car safety regulations exist. They exist? Then why isn't Tesla getting fined in amounts that make this practice unprofitable? reply imiric 4 hours agorootparentBecause car industry regulations haven't caught up with cars being software on wheels yet. Most regulations in general haven't caught up with software eating the world and the \"move fast and break things\" mentality. This is not just an arguably bad mentality, but when human lives are at stake, it's a very dangerous one as well. Besides, companies the size and influence of Tesla can lobby their way out of regulations. With Musk now becoming more politically involved, it's doubtful we'll see any of this change in the next 4 years at least. reply Aeolun 11 hours agorootparentprevI completely agree, but saying it's a 'recall' when your car sits in your driveway still feels entirely wrong. reply saghm 10 hours agorootparentI'm not sure I agree. From the perspective of a customer, not being able to drive the car due to it being unsafe is the part that matters, not where and how the manufacturer has to fix it. If you're opposed to it based on the assumption that it wouldn't take as long, I agree that might be true, but by that logic we should be categorizing _all_ recalls based on length (regardless of whether it's a software update or otherwise), since I'm not convinced that the average length of time until a problem is fixed will always be perfectly split with the software ones being super quick and the ones that would need to happen in person being super slow. What if the mechanics are already aware of how to fix the issue and can do it the same day, or if the software issue turns out to take a long time due to the developers needing a lot of time to fix the bug? If you're opposed to it purely from the perspective of linguistics and \"recall\" sounds like \"return to the manufacturer\", I think I'd disagree due to the word \"recall\" not being super commonly used for that in other circumstances. If anything, the other usage of the word that springs to mind most readily to me is recalling someone or something \"from service\", which I think fits perfectly here. reply quonn 7 hours agorootparent> not being able to drive the car I had plenty of recalls and it never happened even once that I could not drive the car before the recall. reply croes 6 hours agorootparentMaybe better, shouldn’t drive the car because of safety issues reply infecto 5 hours agorootparentThis is hyperbole. Most of the Tesla recalls that I can remember did not impact the safety of the vehicle. reply thunky 5 hours agorootparentIs there no difference between a software \"recall\" and a software update? I'm imagining (uneducated guess) that the software is updated more often than it is recalled, and so a \"recall\" is an update that addresses a safety issue. reply infecto 5 hours agorootparentSpecifically speaking on software and tesla most/all recalls are for items that no longer comply with a government rule. No argument, the rules should be followed but I do believe there is a shade of gray as a number of them are imo tail events. They should be fixed but I would not classify them as the car cannot be used because safety is an issue. \"The Boombox function allows sounds to be played through an external speaker while the vehicle is in motion, which may obscure the Pedestrian Warning System (PWS) sounds. As such, these vehicles fail to comply with the requirements of Federal Motor Vehicle Safety Standard number 141, \"Minimum Sound Requirements for Hybrid and Electric Vehicles\" \"A software error may cause a valve in the heat pump to open unintentionally and trap the refrigerant inside the evaporator, resulting in decreased defrosting performance. As such, these vehicles fail to comply with the requirements of Federal Motor Vehicle Safety Standard number 103, \"Windshield Defrosting and Defogging Systems.\" \" A factory reset muted the Pedestrian Warning System (PWS) sounds. As such, these vehicles fail to comply with the requirements of Federal Motor Vehicle Safety Standard number 141, \"Minimum Sound Requirements for Hybrid and Electric Vehicles.\" \"An incorrect font size is displayed on the instrument panel for the Brake, Park, and Antilock Brake System (ABS) warning lights. As such, these vehicles fail to comply with the requirements of Federal Motor Vehicle Safety Standard number 105, \"Hydraulic and Electric Brake Systems\" and 135, \"Light Vehicle Brake Systems.\" Outside of these there are a handful of FSD recalls and a couple that are more critical, like rear-view cameras not working due to software. Stating again for the eventual naysayers, all of these absolutely should be fixed but I believe they are shades of gray in terms of how critical they are to safety. reply aprilthird2021 5 hours agorootparentprevYou are aware that in this very thread are examples of Tesla recalls that impacted the safety of the vehicle? reply infecto 5 hours agorootparentYou are aware I said most right? I am responding to the typical Tesla hyperbole. All issues that warrant a recall classification absolutely should be fixed but I would not go as far to say the vehicle should not be used due to safety. I think I can manage even though the font size in the instrument cluster is incorrect. That's not to say there are not more serious issues like the rear-view camera that did not work for specific models/software version combos. reply mewpmewp2 8 hours agorootparentprevIt still doesn't sound right to me. Sure we can get adapted to the new meaning of the term as time goes on, but to me the term has strong implications to physically bring it back, and it is weird that people are claiming that it doesn't. Surely this causes a lot of confusion at the very least people thinking those vehicles actually have to be brought back. reply DonHopkins 5 hours agorootparentAnd the word \"Email\" has strong implications to actually putting a piece of paper in an envelope with stamp, and physically delivering it to their mailbox. But the world adapted and survived the change in the strong implications of the word \"mail\", somehow, and continues to turn on its axis. reply mewpmewp2 5 hours agorootparentHow does e-mail have that implication when it specifically has e in front of it? reply Topfi 7 hours agorootparentprevBut it isn’t a new meaning. Rather, those arguing that recall should now exclude OTA want to change a long established definition. reply mewpmewp2 5 hours agorootparentGoogle \"definition of product recall\" First 2 answers in asq.org and wikipedia Recall is the act of officially summoning someone or something back to its place of origin. A product recall is defined as a request to return, exchange, or replace a product after a manufacturer or consumer watch group discovers defects that could hinder performance, harm consumers, or produce legal issues for the producers. and A product recall is a request from a manufacturer to return a product after the discovery of safety issues or product defects that might endanger the consumer or put the maker or seller at risk of legal action. Product recalls are one of a number of corrective actions that can be taken for products that are deemed to be unsafe. It specifically says \"request to return.\" It's very important because the word \"recall\" to me is rather about the cost to the business and consumer rather than severity of an issue or it being specifically safety issue. Recall could be something that is done as a response to a safety issue, but recall could be done for some other reasons as well. E.g. the product could not just be performing as well, but not be a safety issue. There's a defect that means product lifetime will be limited, etc. reply Topfi 5 hours agorootparent> Google \"definition of product recall\" I did, just to be safe, once again look up the legal definition of recall (both in regard to cars and other products) and continue to find that it was never written in a way that would make OTA not fit the term. It isn't limited to vehicles, but extends even to perishables for which a physical return directly to the manufacturer is not expected even using a more liberal interpretation of the word. Here, this is what recall, in this context means: > A recall is issued when a manufacturer or NHTSA determines that a vehicle, equipment, car seat, or tire creates an unreasonable safety risk or fails to meet minimum safety standards. Most decisions to conduct a recall and remedy a safety defect are made voluntarily by manufacturers prior to any involvement by NHTSA. > Manufacturers are required to fix the problem by repairing it, replacing it, offering a refund, or in rare cases repurchasing the vehicle. Source: https://www.nhtsa.gov/recalls#recalls-7746 OTA falls under that. Can you perhaps find some source saying the contrary somewhere on the internet (just like one can find numerous sources proclaiming the earth to be flat)? Yes. Does that change the legal definition as it has been in place for decades? No. And if you want to fully ho by whatever some random person may believe recall to mean, then does that mean that product recalls for produce, milk, etc. do not exist according to you, because the item isn’t necessarily interfacing with the manufacturer again, but may just get disposed? That was my point. If people feel this definition should be changed, more power to them, but arguing that OTA have as of yet not been covered by the definition is just dishonest. I'd be partial to calling these recalls something akin to \"Mitigating unreasonable safety issue\", but I feel that for some reason, some people may feel compeled to argue that this wouldn't be fair to Tesla either, no matter how dangerous or forseeable a fault might be. reply mewpmewp2 5 hours agorootparentYeah, the argument is about the legal definition being misleading and confusing and what should be changed, because of the bias and meaning of the word \"recall\". The fact that most google results will imply physical return of the product is evidence of popular definition and legal definition diverging. In addition the word inherently implies bring it back because of the \"re\". It doesn't imply \"there's a safety issue that needs to be addressed\". > some people may feel compeled to argue that this wouldn't be fair to Tesla either, no matter how dangerous or forseeable a fault might be. Also the point isn't about whether it's fair to Tesla or not. I don't care about Tesla here. It could be any manufacturer, point is that it makes it seem like it's a financial and logistical nightmare to come, but clicking on headline, it's just a software update. Of course by now, I've personally seen this in headlines many times and grown indifferent, but it occurs to me still every time. In my native language the term \"recall\" has even stronger implications of physical return, it means \"call to bring it back\". It sounds even more bizarre than in English for a software update. reply Topfi 3 hours agorootparent> [...] point is that it makes it seem like it's a financial and logistical nightmare to come, but clicking on headline, it's just a software update. If we go by what \"it seem[s] like\" (to a layperson), we could reasonably argue that the word (scientific) \"theory\" should be changed because a significant part of the populus confuses that with the definition of hypothesis. This also, again, ignores that, long before OTA was a thing for cars, the logistics behind recalls for items ranging from meat, over mattresses, to medical equipment, etc. differed widely in execution and financial impact, yet again, have all been covered under the same term. This started because you stated that by considering severe safety issues as recalls we'd have to adapt to a \"new meaning\", simply because you felt it is weird that OTA addressable safety issues are covered by that, when this has historically been the accepted, commonly used and well worn definition. If you want to change it, than more power to you, but don't claim that yours is the original definition, when it isn't. Especially since terms like these are regulated to ensure companies cannot weasel around them. > In my native language the term \"recall\" has even stronger implications of physical return, it means \"call to bring it back\". German? If we go purely by the literal definition of words, then I'd question why no one ever saw fit to complain that a recall/Rückruf rarely contains an actual call (or shout in the case of German) to the customers affected. Add to that, is it really reasonable to just point at the most literal definition a, both well established and for good reasons regulated, word has? One that, again, has been in use for decades across many product types where the remedy was not having to bring the item to a garage for a fix to get wrenched on? Idiomatic expressions exist, I hope deadlines aren't taken literally at any modern workplace. Nilpferd also sounds bizarre considering they are closer to whales than horses, evolutinary speaking, yet we somehow manage, so I feel Rückruf isn't even close to the worst offender in that regard. reply mewpmewp2 2 hours agorootparentNot German. Okay, from other side of the view. Why not change it? It's clearly confusing a lot of people, and with such an important topic, where people would have to pay attention. > but don't claim that yours is the original definition, when it isn't. Especially since terms like these are regulated to ensure companies cannot weasel around them. I'm not claiming it's an original definition. It's just a very confusing and misleading term, that gets used in odd ways. Someone in here suggested \"Public Dangerous Defect Notice\". Doesn't have to be this, but what about something similar? reply infecto 5 hours agorootparentprevMy model y has had a number of recalls and I never knew about it. From things to the UI not meeting government requirements to other software issues. It was never unsafe to drive. reply jmb99 11 hours agorootparentprevRecall is the industry-standard (and populace-understood) term for “this vehicle has a safety or reliability issue that must be resolved by the manufacturer.” It is all encompassing; anything from possibly loose lugnuts to faulty airbags to engine failures to yes, the reverse camera failing to appear. It doesn’t matter if the manufacturer ships an OTA update, shows up at your house with a loaner and a flatbed to take your car, or requires you to go into the dealership for service, it’s a recall. reply aikinai 9 hours agorootparentIt might be industry-standard term, but it’s certainly not understood that way by layman, including car customers. To the public, “recall” will always carry the implication that the vehicle needs to go back to the manufacturer for a fix. reply lupusreal 8 hours agorootparentI don't think that's true. Most people are accustomed to there being many \"recalls\" for trifling matters which can be simply ignored until maybe the next time they bring the car into the shop. A typical conversation about recalls: > \"I changed your oil, and I also replaced one of your door seals because there was a recall for the rubber cracking in cold weather\" > \"Oh, okay.\" It doesn't colloquially imply that the manufacturer is going to come take your car due to some major issue. reply Kinrany 7 hours agorootparentThis was the argument that convinced me. reply nobody9999 8 hours agorootparentprev>Recall is the industry-standard (and populace-understood) term for “this vehicle has a safety or reliability issue that must be resolved by the manufacturer.” Exactly. But not just for vehicles or other durable goods. We see recalls of all sorts of food products[0] pretty regularly, and no one has to bring their carrots or ground beef back to the store/manufacturer to \"remediate\" the issue. Rather, you just throw it away. And such activities are unfailingly called recalls. [0] https://www.fda.gov/safety/recalls-market-withdrawals-safety... reply justinclift 9 hours agorootparentprev> and populace-understood Nah, doesn't really seem like it. reply daghamm 10 hours agorootparentprev\"recall\" is a legal term, it means the vehicle is not road-legal and requires immediate action. Doesn't really matter if it hw, sw or maybe even the owners manual that needs to be changed ASAP. reply robertlagrant 9 hours agorootparentIf I have a flat tyre it's not a recall. Not everything about not being road-legal and requiring immediate action is a recall. That's a bad definition. reply fenomas 7 hours agorootparentGP said \"recall → not street legal\"; you've replied to disagree with \"recall ↔ not street legal\". reply robertlagrant 5 hours agorootparentNothing about the definition implies the existence of the former construction. reply macintux 5 hours agorootparentHN discourages this type of bad faith misinterpretation in the name of pedantry. reply robertlagrant 5 hours agorootparentThat is itself a misinterpretation, but I'll assume good faith. I'm not misinterpreting; I'm saying the definition is too broad. A useful reply seems simple enough: - yes it is too broad, and here's the additional criterion to narrow it correctly - no it is not too broad, and here's why a flat tyre is a recall reply fenomas 2 hours agorootparentIt wasn't a definition. The person you replied to used \"means\" in the sense of \"necessarily implies\", not in the sense of \"is precisely equivalent to\". That's what the symbols in my earlier comment meant. E.g. > \"Murder\" is a legal term, it means the act was premeditated. Somebody who says that is not claiming that all premeditated acts are murder. reply atwrk 8 hours agorootparentprevThe proper definition will clearly mean the model, not the model instance. reply Retric 6 hours agorootparentprevThere’s been recalls over paint, watertight seals, etc issues that impact a vehicle’s lifespan. Generally the kind of thing a company fixes not to be sued, or just cheaply maintain a solid reputation. There’s also a “safety recall” system run by NHTSA, which is what most people think of by recall. Which then has a bunch of reporting requirements so people who aren’t taking their cars to the dealership still get notified. reply brookst 5 hours agorootparentprevNote that many recalls have nothing to do with legality to drive or immediate action. Last time I took my car in they handled two recalls: the bracket for the spare tire could crack (mine wasn’t) and a floor drain plug was the wrong size. Very few recalls are urgent or prohibit the car from being g driven. reply Symmetry 5 hours agorootparentprevIt might be the correct technical term, but general interest publications like Wired should avoid jargon that confuses laypeople, especially in headlines. reply karlgkk 10 hours agorootparentprevSure it's wrong, but it's actually to Tesla's benefit IMO Which of these would you, a Tesla stakeholder, prefer the news to report: - Tesla Cybertruck has recall - Tesla Cybertruck receives software update that resolves issue that put public safety at risk reply justinclift 9 hours agorootparentThat's really not clear? And for equivalence, the first would need to be: - Tesla Cybertruck has recall that resolves issue that put public safety at risk reply croes 6 hours agorootparentprevThe recall doesn’t know how the fix is applied it’s just the fix is mandatory for manufacturer and he to inform the customers reply soco 9 hours agorootparentprevThis is a point which comes up regularly when discussing right to privacy laws (hello EU). Startup culture wants to move fast and break things - including my bones in this case - while evil regulations only come in the way of progress. reply robertlagrant 9 hours agorootparent> Startup culture wants to move fast and break things - including my bones in this case - while evil regulations only come in the way of progress. All car companies comply with thousands of regulations, and it's fine. If you have a simple view of the world, I don't think inverting it and claiming the people you think are the baddies hold that inverted view is going to get anyone closer to understanding anything. reply bilekas 10 hours agorootparentprev> You can't just sell early access cars and fix issues as customers experience them. But then how will you know what you HAVE to fix. /s To be frank I find the whole Tesla cult to be mind blowing. Seems to be the realization of believing in something that seems so good to be true alwith a company who promises to do so many great things, delivers non road worthy vehicles and people still dedicate their time and money defending them. Is it sunk cost fallocy? Or just people who just wish Tesla was what they promised they would be? reply rightbyte 8 hours agorootparentIf you remember how some kids in school argued over which gaming consoles were best or whichever Transformer would win over another. I think it is the same concept 'fanboyism' among those Tesla apologists. Of which sunk cost were a huge thing. Your parents bought you only one... reply bastloing 5 hours agorootparentprevAll of my Tesla updates have been production updates. All software has bugs, and Tesla is one of the most innovative and responsible software companies. Compare that to Microsoft, for example. Or Oracle. Lots of their software runs medical systems that are mission critical. How about the latest Palo Alto networks vulnerability? Lots of critical infrastructure behind their firewalls. All in all, I'd say Tesla is among the top on software and hardware engineering, and I'd hire any of their engineers in a heartbeat. reply jart 9 hours agorootparentprevIt's not just Tesla. All the online services you depend on are run this way, and none of it's regulated. Governments don't know how to regulate software companies like Tesla. They know how to regulate people constructing homes, cooking food, and building bridges but most attempts at regulating software development have petered out. This is a good thing. Thanks to Tesla's modern approach to car manufacturing it is now possible for anyone in the middle class to purchase a self-driving bulletproof truck that's faster than a Lamborghini. You can't make this stuff up. I swear. reply zelon88 3 hours agorootparentWhy do Tesla people see servitization and paying for something in perpetuity forever as being such an enabling thing for people? We need to own the things we pay for, not rent them. In a decentralized way. A car is a machine that needs to be capable of functioning completely independent of everything else. There is no reason it should be a PaaS other than to extract as much value out of the market as possible. Companies need to leave some value for regular people to live. reply aiono 9 hours agorootparentprevMost of the online services I use can't kill me if they have a bug. reply Topfi 6 hours agorootparentprevIf Tesla is a software company rather than a car company, then did you also consider Wework one rather than office rentals? Honestly interested why their, as of now, main product shouldn’t define the type of company they are. reply orwin 9 hours agorootparentprevPeople who have 100k to burn on a car are middle-class? What's the median yearly income in the US? 90k? reply atwrk 8 hours agorootparentEven less, the median household income is 80k: https://www.census.gov/library/publications/2024/demo/p60-28... reply dagw 8 hours agorootparentprevYou can apparently lease a Cybertruck starting at $1000/month. That is definitely within range for someone on a 90k income, even if it is isn't the most financially smart thing you can do. reply siva7 2 hours agorootparentWhat? More like 190k income needed to even get a quote reply FireBeyond 3 hours agorootparentprevWas it MKBHD who made some weird claim that he spoke to a bunch of people, \"most of whom thought the cyber truck was closer to $1M and were amazed to hear it was more like $100K\"? I have NEVER heard that. It makes zero sense and sounds more like the fever dream of a Teslarati: \"Wow, I know the median home price in my area is $500K, and the median household income $80K, but all of a sudden there is this explosion of million dollar Teslas on the road!\" reply krige 9 hours agorootparentprevI need to ask - is this a joke? No, Tesla is not particularly bulletproof compared to other cars. Its doors may stop a 9mm sure enough, but the windows won't manage even that, and anything above 9mm will go through the doors as well. And 9mm is not even that common among criminal shooters, they usually go for bigger calibers / higher penetration naturally already. And which Lamborghini specifically? reply fnord123 9 hours agorootparent> And which Lamborghini specifically? It's faster than a Lamboghini Spire. reply TheOtherHobbes 8 hours agorootparentBut not as robust off-road. reply bbarnett 9 hours agorootparentprevIt's a pretty simple fix. In the old days, governments had departments which would inspect cars, verify they complied with legislation, and even examine build quality. Quite easy, when everything was basically physical. Now the ECU and modules are a black box, unknowable to such entities. Things can be caught (see VW scamming fuel/emissions tests), but it's by luck. So solution? We pass laws that all code, every bit of it, all chip schematics too, all firmwares are open source. Note I said open source, which in the old days just meant \"readable\". We're not talking GPL, all copyright would remain. On top of that, all build scripts and methods to flash modules / etc would be provided to governement test environments. Now we can test. Now, we can look for crappy code, hacky junk, fake emission cheats, bugs and more. Don't like it? You don't sell cars. Tough. The entire supply chain would be required to fall inline. It's really not that hard. In terms of security, that's what signing updates is for. And (for example) you can already take hobby tools, such as forscan (for fords) amd flash updates to modules. As long as it is signed. reply jimmydddd 6 hours agorootparentprevI think recall in quotes because the original meaning of recall was that you had to return (recall) the thing to the shop so that it could be fixed. Like when the brakes might fail and I have to schedule an appointment to bring my car into the shop for a week seems different from pushing out a software fix to adjust the AC temperature or whatever. It seems more like an \"update\" like when Apple pushes out a fix to the iPhone. They call that an update, not an iPhone recall. (However, your camera example does sound like a major issue.) reply llamaimperative 5 hours agorootparentThere are tons of non-mission-critical recalls that you can have fixed (or not) at your leisure. reply johnp314 9 hours agorootparentprevWell, the OED definition of 'recall' is \"official order to return to a place\" and a software update or fix requires no return to the place of purchase. I wonder how may legally defined 'recalls' Microsoft Windows has experienced with each new version. reply 0xEF 9 hours agorootparentI think the gist is \"recall\" is the same saying \"safety issue that needs to be fixed in the software or hardware\" since we don't have a word for software issues, which carry a bit more weight on a fly-by-wire vehicle like the Cybertruck. Interesting to note that this thread seems to be full of a lot of people choosing to be pedantic over the word \"recall\" rather than taking a critical look at the Cybertruck and it's issues. While I agree that language is important (and dynamic), I suspect that discussion around this vehicle is also charged by politics and sycophantic thinking. If I remove my personal opinions about Musk, I find that I actually do not hate the Cybertruck. Sure, I think it looks absolutely stupid and it bothers me that the flat paneling does not line up perfectly, but I am also cheering on it's attempts to break some rules, try something different and possibly spark some future innovation. With that in mind, it's easy for me to expect that it will have problems since anything new that breaks the mold tends to, so with that reframing, a lot of recalls make sense and could even be looked at as good since solutions are being developed. reply ryandrake 7 hours agorootparent> Interesting to note that this thread seems to be full of a lot of people choosing to be pedantic over the word \"recall\" rather than taking a critical look at the Cybertruck and it's issues. Being pedantic over a single word out of 100 is HN's bread and butter. How many times have you read a comment here that boiled down to \"The author's entire article is invalid because in paragraph 3, word 65 he used the wrong word!\" Bonus points when the poster is himself also wrong about the meaning of the word, as is the case in this thread where people don't know what a \"recall\" is in the automotive world. reply 0xEF 6 hours agorootparentI suppose that is probably true, but I hate to generalize...then again, I am guilty of generalizing and being pedantic, myself. We do kind of exist in a strange space where the meaning and intention of words matter for the sake of clarity, but to ignore the dynamic nature of language results in less clarity. If that makes any sense? reply mewpmewp2 8 hours agorootparentprevPeople are pedantic over this because they have a strong association from their past with this term and now it is being used unexpectedly. No matter how bad Tesla's issue might be, then maybe a different wording has to be used to highlight the issue. As right now it seems as if when the term is used it is almost intentionally made to seem as if the vehicle has to be physically transported back which would be a logistical and financial disaster. Sure, safety issues can be bad, but it is entirely different level of feasibility of physically recalling back all those vehicles. Why are people pretending it is not? The whole issue is a spectrum of consequences. Right now it seems that the term used confusingly is always justified because the issue is binarily bad in the first place. reply squigz 3 hours agorootparentprevI think the 'pedantry' is actually trying to push back against the politics and whatnot, in a way. As others have noted, 'recall' brings to mind physically returning an item. I have, repeatedly, had to clarify to people that it was actually an OTA software update, which - to most people - is a lot less significant than the idea of having to return your car several times a year. reply s1artibartfast 2 hours agorootparentprev>The cybertruck, for example, was subject to a recall because the rearview camera wouldn't come up, but the mirrors are insufficient to back the vehicle up safely without the camera. Every vehicle since 2018 has been required to have one for safety. No car or truck is sufficiently safe without one. reply justinclift 11 hours agorootparentprev> Why have you put \"recalls\" in quotes? Maybe because an over-the-air update is hard to seriously consider a \"recall\"? reply devjab 11 hours agorootparentThey are called recalls because that’s what the term for legalisation mandates. They are issued when a vehicle is unsafe to drive and they are mandated because a manufacturer is required to take a lot of steps to remedy their failure. If they fail to repair the issue they are forced to issue full refunds as an example. So no, the word recall is used because is the official terminology used for these issues regardless of the solution required to fix them. The word does have its origin in a world where solutions were rarely software updates. That they are software issues make them no less serious though. I suspect that in some cases the software issues might indeed be far more dangerous than errors which mandate physical recalls. reply robertlagrant 9 hours agorootparentIsn't this just restating part of the previous point? \"We use a silly word for OTA software updates: 'recalls'\" \"Recalls is the word we use\" \"Yes...\" reply TheOtherHobbes 8 hours agorootparentThe important part is the legal obligation to produce a safe product. If that doesn't happen there are legal consequences. The fact that software is involved changes exactly nothing. And honestly - the entire industry needs to get over the bizarre belief that software has a magical right to be less performant, less competent, less reliable, less robust, and more consumer hostile than hardware, just because it can be updated. reply robertlagrant 6 hours agorootparentNone of this seems relevant to the single point that \"recall is a misleading term to use for an OTA software update\". reply rickdeckard 10 hours agorootparentprevAt the very least, it puts the quality control of Tesla in question, as those are failures which should have been caught PRIOR to a commercial launch. Tesla is treating its product-launches like it's just some browser-game in the cloud, instead of treating it as what it is: The handover of a 6000 pound bullet into the hand of a customer who will fire it into a crowd in expectation to not hit anyone... reply noapologies 11 hours agorootparentprevWhy? Are safety issues not \"serious\"? reply damon_c 11 hours agorootparentThe word recall sort of implies that the vehicle is recalled to the manufacturer. Calling a software update that happens in your garage at night and takes 20 minutes a “recall” definitely is worthy of quotes. reply mkipper 39 minutes agorootparentFocusing on the dictionary definition of the word \"recall\" seems pedantic. Not all recalls are for big mechanical problems. I've received several recall notices for issues that just required the dealership to flash the ECU or some other controller with new firmware. Whether that update is flashed OTA or with a special dongle at the dealership doesn't really make a difference if you're looking at it from the perspective of vehicle safety/reliability/stability. The point is that a recall is a significant enough issue that the manufacturer _must_ notify their customers about it. I agree that if you count _every_ OTA update as a recall, you can't really do an apples-to-apples comparison. The cost of pushing an OTA update is much lower than the cost of a recall, so Tesla probably pushes a bunch of minor OTA updates that other manufacturers wouldn't bother with. But it's a fair comparison if only some subset of OTA updates are counted as recalls (i.e. updates addressing a safety issue). reply judofyr 10 hours agorootparentprevThe word bug sort of implies that the device was struck by a terrestrial arthropod animal. Calling a software defect that happens due to a programming error a \"bug\" definitely is worthy of quotes. (The etymology of a word can be quite different from its current meaning today.) reply taberiand 11 hours agorootparentprevUnless of course that patch can't be delivered because the truck was totalled in an accident that the patch was too late to prevent reply liontwist 2 hours agorootparentDid this happen? Or? reply alxlaz 9 hours agorootparentprev> The word recall sort of implies that the vehicle is recalled to the manufacturer. It does not, no more than gaslighting implies lighting a gas lamp or the phrase crossing the Rubicon implies actually crossing a river in Italy, in any case. It hasn't meant anything of that sort since the mid-sixties. Recall is what legislation requires you to call it if something is unsafe for public use and it has to be withdrawn for the market until it's remedied. It doesn't matter how that's done. The NHTSA guidelines don't include physically getting the product to a manufacturer or a distributor as a requirement to issue, or as a criteria for fulfillment, of a recall. (I don't think recall guidelines in any industry do, it's just the NHTSA's that apply in this case). Yes, this also applies for firmware upgrades. No, it doesn't matter where they're performed. The FDA has issued firmware-related recalls for devices with programmable logic since programmable logic in medical devices was a thing so like... fourty years. If anyone in some company's safety staff just learned about that's let's all please give them a warm welcome to the 20th century. The main reason why recalls typically involve returning the product to the manufacturer (or, more often, as the vast majority of recalls are for food, medicine or cosmetics, to the distributor) is traceability. Manufacturers need to maintain documentation that shows they took reasonable action to notify all customers, that depending on how they chose to handle it they made repairs for free, replaced them for free, or that the refund they issued made reasonable allowances for depreciation, and so on. Some foodstuffs or medicine also have disposal safety rules that require you to maintain adequate documentation as well. It's just the easiest way to deal with it, both in terms of remedying the actual issue, and in terms of legal risk. But it's got nothing to do with returning something to the factory, it hasn't meant anything of that sort in like half a century. reply GJim 8 hours agorootparentOnce again, an intelligent post such as yours has been lazily downvoted simply because somebody doesn't agree with it, and they can't be arsed composing a coherent rebuttable. This leads to unpopular ideas being buried, groupthink and a lack of intelligent discourse..... ....And yes, this IS leading to this place becoming more and more Reddit like (no, that isn't a tired cliché, no matter what the FAQs claim). Downvoting needs to be reserved for comments that detract from the conversation. At this rate, we will need some form of meta moderation to ensure this happens. reply alxlaz 7 hours agorootparent> Downvoting needs to be reserved for comments that detract from the conversation. I understand where you're coming from but truth is a post like that one does detract from the usual excusemaking conversation that some companies enjoy here, so I'm not surprised it was downvoted :-). It's one of the many ways in which karma points killed online discourse. I just happen to be old enough to have racked up a lot of pre-karma posting (including, of course, flamewars, what would life be without spice?) so downvotes don't really register on my radar. reply wobfan 11 hours agorootparentprevIf you think about the financial implications then probably yes. Software issues, safety related or not, can probably easily be fixed OTA and thus don’t even cause a fraction of the costs a (let’s call it \"real\") recall costs. Also, in the head of most people, I think, a recall is something where the car needs to be returned physically. But still, obviously, the issues can be as serious as physical issues. It’s just that we’re used to physical recalls. reply lmm 11 hours agorootparentprevGetting an OTA software update to your vehicle is far less disruptive to your life than having to take it in to the dealer. Obviously safety issues shouldn't happen, but how easy they are to fix also matters. reply justinclift 9 hours agorootparentprev\"Recall\" is wording that means (to me) the vehicle needs to be physically returned to a dealer workshop for something to be done. That it's used for OTA updates just (to me) means they should use some more suitable wording for it. And yeah, as other people have pointed out it's the word used for the legislation. I still think they should use some kind of different wording though. reply rob74 10 hours agorootparentprevMaybe, to eliminate ambiguity (and make it sound cooler), they should call it a \"roadworthiness directive\", similar to \"airworthiness directive\" for aircraft. Of course airworthiness directives are issued by the authorities (the FAA in the US), while \"recalls\" are from the manufacturers, but still... reply mrguyorama 1 hour agorootparentMaybe Tesla doesn't deserve to have the world redefined around them to make their products look less shit? This conversation happens every single time Tesla produces a product that is legally unsafe. We keep having this conversation because Tesla keeps making an unsafe product. reply matsemann 11 hours agorootparentprevA recall is about the issue, not how it's resolved. A recall means there are some serious security flaws that needs fixing. Even if they can be fixed OTA, they're still a flaw, and Tesla has had many of those. reply iamkoch 11 hours agorootparentCouldn't agree more. Recall means \"sufficiently dangerous to need to recall the vehicle to the manufacturer\" - yes, in the modern world it can be fixed OTA, but it's still dangerous enough to require a mass fix to a fast-moving death machine. reply rsynnott 6 hours agorootparentprev> but the mirrors are insufficient to back the vehicle up safely without the camera. I'm quite surprised that that is allowed at all. Seems like an unnecessary risk. reply chasd00 6 hours agorootparentI suspect that’s a bit of an exaggeration by the author. If the mirrors were not required by law and therefore comply with the law they wouldn’t exist. “The best part is no part” reply Tostino 5 hours agorootparentprevAll new cars are required to have backup cameras, so manufacturers have used that \"freedom\" to change car shapes. Not only Tesla. reply liontwist 2 hours agorootparentI can’t understand why a regulatory group would feel the need to make that a requirement. That vastly increases the cost to develop a vehicle. Why can’t I choose that feature as a consumer? Did their brother in law have the patent for backing cameras? reply Tostino 2 hours agorootparentI get it, but the reasoning behind it was that little kids kept getting run over by their parents / family members because they couldn't see them. The cost at this point is pretty negligible. Mandate went into effect (in the US) in 2018: https://en.wikipedia.org/wiki/Backup_camera#Mandates reply liontwist 1 hour agorootparentThanks for the info. Yeah the cost is negligible if you are already planning to have a screen running chrome on Ubuntu server, but I hope it was still an option not to have that. reply mrguyorama 1 hour agorootparentUh no. Pretty much every single backup camera on every single vehicle is the exact same little module that produces analog video and literally just runs a component cable to the head unit which has a standardized analog video input. Zero software required. reply losvedir 4 hours agorootparentprevWell, \"recall\" implies both a safety issue and a convenience issue. Pre-Tesla OTA updates a \"recall\" was a logistical nightmare, meaning you had to take the day off work, wait at the dealership all day, sometimes deal with a loaner car, etc. To a lot of folks, maybe older than the average HN user at this point, the reason recalls were such a hated thing was not primarily that it implied a safety defect, but because they were super inconvenient. For example, I had a recall on my Tesla having to do with the automatic window closing being dangerous. That is a safety issue, and I do have kids, so I realize that there's a very small chance they could damage or lose a finger with it, so I do want it fixed. But because it was fixed OTA, I don't really mind it or hold it against them. In contrast, if fixing it meant taking it into a dealership and losing a day, it would be a hugely negative thing! I ascribe a negative sentiment to the fact that there was a safety issue in proportion to the chance of damage and how severe it was, so: minor. In other words, I know why it's called a \"recall\" and it makes sense. But on the other hand, you have to realize why people put it in scare quotes, too. The plain denotation of the word hasn't changed, but the connotation is really disproportionate to the experience of someone with a Tesla. reply eigart 10 hours agorootparentprevYeah, software recalls are recalls. The car has steer by wire, so even steering wheel issues could be ”just” a software recall. reply hot_gril 10 hours agorootparentUsually your steer by wire isn't fixable over the air reply gibolt 12 hours agoparentprevVery few others have recalls that can be done OTA. Even if it is just software, expect an hour to days at a service center to do the update, if the techs can figure it out. For Tesla, the software recalls are nearly automatic (one click install, just like other update), such that few owners even know that their car ever had a recall. reply davedx 11 hours agorootparentMy Model 3 has had: - charge port needing replaced shortly after I bought: mobile service - front and rear suspension forks replaced: 2 trips to service center (common according to them) - rear light needed replacing (mobile service) 4 years old. Still not as bad as our last car (diesel VW Sharan), but Teslas have plenty of defects that can’t be fixed OTA. reply darkwater 11 hours agorootparentSure, but \"recall\" in the automotive market has a very precise meaning, and refers to safety issues (at production line level, so impacting many VINs)that needs to be fixed. In Tesla's case, unlike most of other vendors, generally these recalls can be fixed with an OTA update. The CT OTOH had many \"classical\" physical recalls due to hardware issues. reply seanmcdirmid 12 hours agorootparentprevThis is kind of true. Say what you want about Tesla, but they have the best software experience out of the other car manufacturers. I love my BMW i4, but had to take into the shop just to fix a botched softwares update. Well, at least the cabin is silent. reply k4rli 11 hours agorootparentI'd say my 2006MY car has the best software experience. The less, the better. reply caconym_ 11 hours agorootparentprevI've bought several new cars in the past few years and zero of them have had a recall I'm aware of, software or otherwise. My old car went almost 20 years with just two recalls that I can remember. Even ignoring software recalls, then, the Cybertruck has a significantly higher recall rate (per unit time) than anything I've owned, so the fact that it has had even more recalls that could be serviced OTA is really neither here nor there. The user-facing software systems in these modern luxury cars really do seem to cause a lot of issues. reply FireBeyond 3 hours agorootparentprev> Even if it is just software, expect an hour to days at a service center to do the update, if the techs can figure it out. Ahh, yes, the dinosaurs and their bumbling cavemen 'techs'. Meanwhile in the real world, while my Audi gets a lot of OTA updates, updates done at the dealership are done by plugging a USB drive into the car. Hopefully the techs can figure that out, if we give them a few days... reply croes 6 hours agoparentprevIf you’re a victim of one of these errors does it matter it was later fixed by a software update? Recall mean an error the manufacturer to fix, the how doesn’t matter at that point. reply friendzis 6 hours agoparentprev> I wonder if you exclude \"recalls\" resolved by software updates, for all cars, where it would rank then? Why would you? A recall is part of *safety* process, where company internal safety lifecycle failed to a level that adults had to step in and say \"this is actually unsafe, fix your shit or it gets pulled out of market\". The fact that issue can be remedied with software update is irrelevant, because in the end the core issue is that an unsafe product was distributed to the market. The fact that you put recall into quotes is telltale sign how Tesla managed to undermine public safety concerns drawing a divide between software-only and hardware fixes. reply bboygravity 12 hours agoparentprevIf that's indeed it, I would rank the article as worse than 91 percent of ordinary boring Elon and Tesla bashing in 2024. I guess on a positive note they stopped posting that Tesla is almost bankrupt for the 1000th time because Elon. reply mst 4 hours agorootparentIt isn't, this time. I've seen plenty of articles, contemptuous tweets, etc. that conflated OTA updates that were classified as 'recalls' by the safety authorities with physical issue recalls. And I'm annoyed that the article didn't provide the answer itself. But per https://www.cars.com/research/tesla-cybertruck/recalls/ 5/6 of the recalls for the cybertruck to date have been physical, only the immediately previous one was resolveable by an OTA update. (I'm in the \"I think Elon is a dick but Tesla is pretty cool and SpaceX is awesome\" camp personally, so bad criticism of Tesla annoys me not just on a basis of hating bad criticism of things in general but also because I get doubly annoyed when somebody criticises somebody I believe *is* worth of criticism and then manages to do it inaccurately ... you're in a target rich environment! Stop shooting at squirrels instead!) reply clarionbell 10 hours agorootparentprevElon continues to baffle by avoiding bankruptcy. reply throwaway8754aw 11 hours agorootparentprevElon is out for Elon always ..a slimey used car salesman I.e. bring out the AI Optimus robots all controlled by AI. Not! Elon is Trump's next Omarosa... reply szszrk 9 hours agoparentprev> where it would rank then? I would rank it as \"still isn't legal to import or drive in most of the Europe\". reply mst 4 hours agoparentprevThat would make a significant difference for other Teslas but apparently not for the cybertruck: https://www.cars.com/research/tesla-cybertruck/recalls/ Not sure where it would rank at '5 recalls' instead of '6 recalls' but suboptimal nevertheless. (I think the article noted that because the immediately previous one *was* a software update ... but it appears that's the only one that was) reply tromp 10 hours agoparentprevRecalls come in two types, soft recalls that can be fixed by a software update, and hard recalls that cannot. Many people are still not used to regarding the former as recalls, as they associate the word not with the safety issue itself but the way it gets resolved. It would be nice for articles discussing recalls to point out what type it entails, both for being more immediately informative, and to get people used to software updates being able to qualify as recalls. reply miohtama 6 hours agorootparentIf the article were more honest it would be less clickbaity, selling less on Mr Musk hate. As we can see raging HN discussion, being pedantic or not. reply FireBeyond 3 hours agoparentprev> I wonder if you exclude \"recalls\" resolved by software updates, for all cars, where it would rank then? That depends, are you also going to exclude recalls for other cars, like one I got last year, \"Please insert this piece of paper between page 129 and 130 of your owner's manual. It contains supplemental information about the operation of a vehicle system\"? Or will only CyberTruck get this special treatment? reply karlgkk 10 hours agoparentprev> I wonder if you exclude \"recalls\" resolved by software updates, for all cars, where it would rank then? While I agree that the term \"recall\" is overloaded, the Cybertruck has had some pretty spicy safety related \"recalls\". Issues that, frankly, it should not have been allowed onto public roads with. reply jmward01 12 hours agoprevCompletely unrelated to safety, these vehicles don't look like they are aging well. They are all completely new but the ones I am seeing on the road already look a bit beat up. That finish on them and their general styling emphasize every minor blemish. reply lukah 12 hours agoparentCar panels are convex on nearly all other cars for very good reason. Flat panels are structurally susceptible to damages which wouldn’t mark a standard panel. Adding a highly reflective surface was another great move. reply filmor 9 hours agorootparentThere is a whole section in James May's review where he is confirming with a steel ruler that the surfaces are slightly convex :-) https://youtu.be/CQzYhMDNLPA?t=216 reply Reubachi 4 hours agorootparentThis has been normal/understood since pre-demo phases. The \"car youtuber\" culture around shitting on this car is almost as laughable as the car itself. reply anon373839 11 hours agorootparentprevNot only that, but the finish looks like trash, too. I'm seeing 10-20 Cybertrucks a day and almost all of them are weirdly splotchy and dull. reply Tempest1981 9 hours agorootparentCurious what city that's in... I rarely see them. reply nrb 9 hours agorootparentThere are TONS of them in Los Angeles. reply jmward01 12 hours agorootparentprevI wonder what tests car companies generally do to predict how durable a style choice is (how scratches, corrosion, etc will impact the look). Protecting your brand is also about what people will see in the future, not just what is on the showroom now. If 5 years from now all of these vehicles look terrible that won't help sales for any of their models. reply jaggederest 11 hours agorootparentI know in the past they've looked at data from used cars, and they also have HALT/HASS (highly accelerated life/stress) testing which does things like e.g. spray the car with concentrated salt solution in a wind tunnel, things like that. I believe many manufacturers also look at data from people like Munro & Associates who tear down cars, figure out what they're made from, and how they were made. reply btbuildem 4 hours agorootparentprevYou ever notice how all cars look like all the other cars? There's rarely an exception, they mostly just copy each other. They seem to stick to a design, then someone makes an enormously brave move of slightly changing some small thing, and next year they all copy it. reply detritus 2 hours agoparentprevI thought their having a 'patina' from use was part of the original schtick, before they even came out? I know that was one aspect that made me go \"huh\" and think they might be onto something interesting. reply hot_gril 9 hours agoparentprevNot sure if it's age, but whenever I see one now, it looks dirty. Maybe it picks up dirt a lot, or owners are afraid to wash them, I dunno. reply cglace 4 hours agoparentprevAlmost all of the cybertrucks I see are wrapped reply baron816 12 hours agoparentprevEvery one I’ve seen has a wrap for this reason reply bamboozled 7 hours agorootparentSeems weird to buy an $80k car and then spend $6k on a paint job just to make it look decent... reply analog31 6 hours agorootparentThe DeLorean was this way too. People were paying to get it painted because the bare stainless steel finish didn't weather well. reply Cerium 2 hours agorootparentprevSeems even stranger when you see a $25k car with a professional wrap. reply s1artibartfast 2 hours agorootparentprevPeople spend money to modify $200k cars too. reply gibolt 11 hours agoparentprevThe panels will not rust, which is the reason to avoiding scratches on other vehicles. You can beat the hell out of it and not have to care. reply randerson 9 hours agorootparentStainless steel can definitely rust. Leave your favorite knife in the sink for a few weeks and watch what happens, especially if it has any scratches on it. reply JumpCrisscross 7 hours agorootparentAnd then put some salt in that water. reply lttlrck 11 hours agorootparentprevOh did they fix the rusting issue from the beginning of the year? reply qwerpy 11 hours agorootparentI think that story turned out to be media sensationalism. I’ve owned one for over half a year, in the distinctly not dry PNW. No rust at all. reply cjrp 5 hours agorootparentHave they started salting roads there yet though? reply coldpie 4 hours ago [flagged]rootparentprevnext [5 more] > I’ve owned one for over half a year I'm so sorry you're having to go through that. Hope you get through this rough patch soon and things are better on other side. reply qwerpy 4 hours agorootparentIt has been terrible! I’ve been asked to transport large items for friends, and had to deal with excited children wanting to take pictures with it. When there was an extended power outage recently I didn’t get the chance to have the “real” outage experience because my truck powered my house. If only I had listened to HN and Reddit. reply liontwist 2 hours agorootparentprevThis is not contributing the discussion and sounds like sour grapes (never owned a Tesla product btw). reply coldpie 1 hour agorootparentJust poking fun :P You can't drop one hundred thousand dollars on one of the ugliest gimmick vehicles ever made and not expect at least a little bit of stick. reply qwerpy 4 minutes agorootparentCT owners have to have thick skins when interacting online so I chose to take it as good-natured ribbing :) It has been a legitimately awesome purchase for me so it’s pretty easy to be positive. lotsofpulp 6 hours agorootparentprevThe PNW is distinctly dry during the summer, which would compose at least half of the months you have owned it. Also, in my experience, it’s road salt/ice melt chemicals that really exacerbate rust rather than just rain. I assume that is why I see a lot more old cars everyday in PNW compared to the northeast. reply acomjean 5 hours agorootparentSalt was bad for cars. (Having had a 80s car that was rusting badly). There was a hole in one of my rear wheel wells. Salt/ sand got into the hollow steel and rusted from the inside out at the base of the passenger door. It was was bad all over however. Cars are galvanized now so they don’t rust. Is that truck running bare stainless with no finish? Are you going to stick to it during the winter? reply qwerpy 4 hours agorootparentYeah my truck has been running bare this whole time and I don’t plan to change that. I’ve parked it near a house by the ocean for days on end so it probably got some salty condensation on it. reply refulgentis 11 hours agorootparentprevnext [3 more] [flagged] qwerpy 11 hours agorootparentnext [3 more] [flagged] refulgentis 10 hours agorootparentSo is the media sensationalist, or what I need to read to understand the media was sensationalist? :) (sorry, I'm just cranky about Duh Media characterization in general. there's a wide set of experiences in this world, and its not nefarious to describe some of them!) reply BLKNSLVR 7 hours agorootparentMy two cents from a disinterested party: Media sensationalism is a thing, and a thing that's growing (in order to compete for attention) and a thing that doesn't like Elon Musk for various reasons, some worthy, some not. One internet-person's anecdote is not trustworthy in and of itself. \"The media\" as a single mass is of equivalent trustworthiness to one internet-person's anecdote. Some media outlets are more trustworthy than others. Take the media reports and individual anecdotes as data points to form ones own opinion. If one person has a good experience, then they'll generally disregard \"media reports\" as sensationalism that, sensationally or otherwise, announce the opposite. Hmmmm. Reading that back, I'm taking the side of Duh Media characterization. I guess that's where I've landed. But I don't feel like I'm a declarer of \"fake news\" for every headline I disagree with. Hmmmm. This has ended up being a conversation with myself, sorry. reply Reubachi 3 hours agorootparentprevWhat a statement to make. Tons of photo evidence of bare stainless steel panels rusting, cyber truck or not. I live in New England, neighbor has a cybertruck that lives in the (outside) driveway. Every panel has a like....brown tinge to it. 2 years from now it will be manhole covered. reply serf 11 hours agorootparentprevmost people care about scratches because they look like shit. paint is a bit more than just corrosion protection at this point, otherwise mfgs would just slap on the thickest toughest machine-tool grade enamel and call it a day. why don't they? because it looks terrible. reply fastball 11 hours agorootparentAnd most cars end up scratched anyway, and most owners do not take their car in for detailing every time this happens (if at all). reply avgDev 1 hour agorootparentMy car is 5 years old, 0 scratches. Other than a ding and some rock chips. reply IshKebab 11 hours agorootparentprevMost cars don't end up scratched. They definitely don't end up scratched after 1 year. reply Eikon 9 hours agorootparentWe are not seeing the same cars or not have the same definition of what a scratch is, lol. reply pbreit 12 hours agoparentprevCould not disagree more. They are the freshest looking vehicles on the road. reply elmerfud 12 hours agorootparentTo each their own on this matter because styling is a matter of taste. I remember the shift from the '70s style boxy cars to the mid to late '80s rounded style cars. I wasn't a fan of that new style when it came out but it did eventually grow on me. For me the cybertruck looks like something out of a low budget'70s Sci-Fi movie. Who knows in 10 years maybe it will start to appeal to me but for right now it doesn't. reply hot_gril 9 hours agorootparentNah, it's objectively not good-looking, and in a few years it'll be more obvious. There are probably more than 0 people who still genuinely think the Pontiac Aztek looks good, and they're wrong. reply acomjean 5 hours agorootparentI’ll agree that there are cars that just look bad. I love my Honda Element, but even I admit it’s not a great looking vehicle.. it’s old enough that it’s no longer sticks out, though easy to find in a parking lot as few cars are shaped that boxy. https://en.m.wikipedia.org/wiki/Honda_Element 62 page pdf about how the design of vehicle came together which is kind of interesting. (Function driving design) http://www.ramblerdan.com/e/files/schumaker.pdf reply mewpmewp2 4 hours agorootparentThat is not an ugly car, a bit cute looking in my opinion. Truly ugly cars are Aztec and Fiat Multipla. Also Chrysler PT Cruiser. reply alt227 9 hours agorootparentprev'Good looking' is a subjective term by definition. Stop trying to tell people what they should think. reply BLKNSLVR 7 hours agorootparentprevI just appreciate your post because you start objectively incorrectly and just lean further into it (intentionally, I believe). I laughed, it was worth it. reply relwin 12 hours agorootparentprevLooks like a \"Death Race 2000\" El Camino... reply bitwize 11 hours agorootparentLooks like the DeLorean from the latest Back to the Future video game... with a buggy level-of-detail algorithm. reply masklinn 9 hours agorootparentIt looks like 3D on 16 bit / 4th gen consoles. Straight out of StarFox. reply cglace 4 hours agorootparentprevI like that I laugh every time I see someone driving one. No other car makes me do that. It never gets old how ridiculous they look. reply system16 11 hours agorootparentprevI think it looks ok from a distance. But up close it looks like a prop from the set of a Back to the Future remake. reply sitkack 12 hours agoprevThe article has about equal time about what Cybertruck owners dont care about, safety, reliability, etc. > Similar to other critics (earlier this year, a CNN reviewer called the pickup a “disturbing level of individual arrogance in hard, unforgiving steel”)—Drury believes Cybertruck buyers are people “who think ‘I don’t care if I kill people when I drive this thing down the street,’” he says. “There aren’t many of those people out there, so there’s a relatively small market for the Cybertruck.” The Josh Johnson on this one is gonna be good. reply nahwut 12 hours agoparentWhat's the term right wing folks use? Virtue Signaling? The cyber truck is that. People buy them because they are politically charged, and you can be seen as a tribe member. But I kinda think it's sad, because I think most tribes make fun of the cyber truck buyer, so they've bought a very expensive, very shoddy ticket into an in group, only to mark themselves as an outsider who bought their way in. reply flapadoodle 11 hours agorootparentLike nearly all gigantic trucks driven in cities (and almost never off road), though the cybertruck is a new low in likelyhood to kill innocent pedestrians. The most accurate term for this phenomenon is either Emotional Support Vehicle or Gender Affirming Vehicle. reply mst 4 hours agorootparentprevThere *are* a bunch of people who've bought them because they genuinely think they look cool. I ... do not agree with that opinion ... but people are allowed to have different taste than I do, even if I think theirs is terribad. reply qwerpy 3 hours agorootparentprevCT owner here. Maybe I’m just more boring than the usual person who gets this truck, but my reasoning for getting one was a lot more simple than this. “My Teslas have been great, this one looks cool, my family can benefit from having a truck, I’ll put down a deposit!” Not sure which group I was supposed to be buying a ticket into. Around where I live the group seems to be Asian men with families, but I was already a member of that group before the truck :) reply the_clarence 11 hours agorootparentprevI'm a leftist and would definitely buy a cyber truck if I could park it easily reply wtcactus 11 hours agoparentprevThe article is a smear piece. As it was expected coming from Wired. reply mst 4 hours agorootparent5/6 recalls so far have been for physical problems, not software ones. It's not a particularly positive article but (unusually for articles about Tesla, admittedly) calling it a smear piece is counterfactual. reply ehnto 5 hours agorootparentprevThe numbers do somewhat speak for themselves. It is an unpopular car. reply aprilthird2021 5 hours agorootparentprevI hate the idea that just being a hater is a bad thing and horrible and you're smearing the good, honest, hardworking Tesleratti. Sorry, these are expensive, multiple-ton machines being sold at incredible profit, and we all deserve to get to take pot shots at it. We collectively pay for the privilege reply austin-cheney 9 hours agoprevWhy are people spending so much money for something so hideous and so defective? Is this a weird fashion thing? My old 2017 Kia Soul costs about 5x less and has had 0 defects. reply tremarley 8 hours agoparentTo you it may be hideous but to many, it’s one of the most attractive cars on the road reply austin-cheney 8 hours agorootparentEven if it were the most beautiful thing ever built would you still spend 5x more for something that doesn’t work? reply ponector 6 hours agorootparentDoesn't work? You definitely can drive it. Why people buying it? To show off. Why people are buying Mercedes G-wagon? Or several expensive watches? reply piyuv 8 hours agorootparentprevDepends on a definition of “many”. Trump is regarded as a great leader by “many”. I guess the sets have a big intersection? reply s1artibartfast 2 hours agorootparentYou might even say a majority. reply nobody9999 2 hours agorootparent>You might even say a majority. One might, but that would be incorrect. Trump received ~77,000,000 votes[0], which was 49.9% of the votes cast. As such, plurality is the appropriate term in this case. What's more, there were ~240,000,000 eligible voters and ~161,000,000 registered voters at the time of the 2024 election.[1] So the winner received less than a majority of votes cast by a small margin, and less so among registered voters (~48%) and even less so among eligible voters (32%). [0] https://www.reuters.com/graphics/USA-ELECTION/RESULTS/zjpqne... [1] https://www.mirror.co.uk/news/politics/election-explained-ho... reply NotYourLawyer 5 hours agoparentprevNot a fan of the cybertruck’s looks, but I’d take it over a Kia soul. reply shepherdjerred 3 hours agoparentprevI think it looks pretty cool, or at least different. reply MaxHoppersGhost 4 hours agoparentprevIf you drive a Kia soul I’m not sure your opinion on if a car is hideous is valid. reply bamboozled 7 hours agoparentprevTribalism. They are political statements now, before the were saving the environment vehicles. reply MaxHoppersGhost 4 hours agorootparentBeing anti cyber truck is also a political statement. It’s just a truck but for some reason it tends to trigger so many people because it’s “ugly” even though there are plenty of ugly cars (G wagons, Kia Souls, new Broncos, etc) that don’t elicit such a strong response. reply abraxas 5 hours agorootparentprevMaybe Musk saturated the eco crowd with his cars and now going after the alt right to keep expanding into the market. reply timbowhite 3 hours agorootparentprevI'm honestly ignorant - what political statement is it making? It seems that liberals dislike Teslas because they dislike Elon Musk. And conservatives dislike Teslas because they dislike new and different things. reply aprilthird2021 5 hours agoparentprevNot only do they pay, they come here and rabidly defend all its flaws in the face of so much evidence to the contrary! \"Oh the recalls are just software updates, so it's fine there were so many safety issues with my car, they fix them quickly and I don't even buy half the regulations are important anyways\" \"Oh it's fine if the trunk cuts your finger off, who puts their finger in a trunk anyways?\" And on and on reply MaxHoppersGhost 4 hours agorootparentAnd the other side comes in and rabidly tries to bring down the cyber truck. You’re somewhat blind if you can’t see how ironic your statement is. reply austin-cheney 1 hour agorootparentSo, for me its about returned utility per dollar per life of product. Numerically, this seems a complete no brainer by possibly more than an order of magnitude. That is why I asked if this is just about a fashion statement, because my dollars and time perspective absolutely cannot understand why anybody would purchase a vehicle with possibly less utility for so much more money that costs so much more to maintain. I am trying to understand, but just cannot see it. The only thing I can imagine why people would defend the more expensive option is due to vanity. I suspect that is important to some people. As somebody more cash strapped it just seems absurd, at least to me. reply pbreit 12 hours agoprevAre these \"recalls\" just simple over the air software updates? Just because car people are not used to this doesn't make it necessarily bad. Obviously no bugs are better than some bugs. But people here tend to know how software development works. reply Veserv 11 hours agoparentThis always comes up. A “recall” is not a description of the remediation, it is a description of the problem. A recall is a public dangerous defect notice. The dangerous product version can no longer be deployed, existing systems suffering from the dangerous defect are identified, and then the version with those dangerous defects is removed from the market with all due speed by either refunding, replacing, or remediating at the manufacturer’s expense. The defective version is thus no longer present, i.e “recalled”. The term has a precise meaning as I laid out. Unfortunately, it has been so thoroughly intentionally poisoned by bad actors in recent years that the term should be retired. We should use the descriptive term: “Public Dangerous Defect Notice” to avoid such bad faith misrepresentation going forward. reply tareqak 10 hours agorootparentTo your point, both things can be true. The CyberTruck can have recalls worse than 91% of all 2024 cars, but many of its recalls can be cheap to for Tesla to fix. I think that is where the two clusters of people that I see commenting here are converging / possibly arguing past each other. One popular form of headline that comes to my mind from business new channels of which I remember no specific instance basically goes like this: “Car manufacturer recalls X many cars costing over them Y dollars because of some fault”. X is usually in the tens of thousands or more and Y is usually in the millions of dollars (now maybe tens of millions of dollars). reply TeMPOraL 10 hours agorootparentprev> The term has a precise meaning as I laid out. Unfortunately, it has been so thoroughly intentionally poisoned by bad actors in recent years that the term should be retired. Nah, that's a sleigh of hand. Recall literally means recall. Whatever the actual technical definition, the common-man understanding has always been \"manufacturer asking you to give your car back, because they screwed up badly enough to be legally forced to fix it\". The focus is, and always has been, on the physically give your car back to manufacturer part. The precise meaning you laid out? That's arguably a typical case of using ancillary aspects of a thing as a proxy, because they're much easier to precisely pin down than the thing you actually want. Think every other term explicitly defined in any contract - the definition tends to not be what's intended, but something that mostly overlaps with intent and is easier to spell out concisely. The overall point being: regardless of what the technical meaning of \"recall\" is, if you put Tesla's OTA fixes together with everyone's repairs that require shipping the car itself to the manufacturer, and then treat them all as equal, that's just blatant, bald-faced lie, a clear indication of purposeful dishonesty. reply egeozcan 9 hours agorootparentOn https://www.nhtsa.gov/recalls#recalls-7746 it says: > A recall is issued when a manufacturer or NHTSA determines that a vehicle, equipment, car seat, or tire creates an unreasonable safety risk or fails to meet minimum safety standards. Most decisions to conduct a recall and remedy a safety defect are made voluntarily by manufacturers prior to any involvement by NHTSA. > Manufacturers are required to fix the problem by repairing it, replacing it, offering a refund, or in rare cases repurchasing the vehicle. So the person you're replying to seems to be correct, or is there another source that shares your claim that \"the focus is, and always has been, on the physically give your car back to manufacturer part\"? reply schiffern 9 hours agorootparentThere's a lot here about technical and legal definitions, but from the customer's perspective what they mostly care about is the inconvenience associated with a physical recall. Most people are accustomed to our phones and devices (auto-)updating, so software recalls just get mentally bundled in with that. The reporting seems dishonest because (in the mind of most readers) the headline exaggerates the inconvenience of Tesla ownership. Even though logically people shouldn't necessarily conclude that (because of the technical/legal definition of \"recall\" already discussed), that's still going to be the general public perception. reply lapcat 7 hours agorootparent> There's a lot here about technical and legal definitions, but from the customer's perspective what they mostly care about is the inconvenience associated with a physical recall. No, customers care about the safety of their vehicles. If there wasn't a safety issue, then customers could safely ignore the recall notice and experience no inconvenience. reply TeMPOraL 2 hours agorootparent> No, customers care about the safety of their vehicles. Yes. But safety is a spectrum, and is denominated in inconvenience against perceived expected loss - perceived, because people round the loss down to 0 when they feel like it won't happen to them. See: how many people don't fasten their seat belts, or hold a phone in their hand while driving, despite both being highly dangerous to life, limb and wallet, and against huge social and regulatory pressure. See also: how many people ignore or tape over error lights on the dashboard; how many people don't repair stuff until absolutely necessary. See also: how many people try to bribe their way through yearly inspections, and would happily skip them entirely if the law didn't force them. So it's like GP said: people mostly care about how much time and money they're going to waste on the newly discovered problem. A recall involving the car being physically taken to manufacturer is hugely disturbing, possibly very costly (it's not like most people drive for fun). A recall involving an OTA update? That's \"just another auto-update ¯\\_(ツ)_/¯\". Most people won't even notice it happening. Putting the two types of \"recall\" in the same class is, from regular person's POV, deception through equivocation. reply lapcat 1 hour agorootparentThe people who won't wear seatbelts, who tape over dashboard lights, who would skip inspections, will also ignore recall notices, so for them, for those who don't care much about safety, dealership recalls are not an inconvenience. And they probably don't care one way or the other about OTA recalls either. The people who bring their vehicles in for dealership recalls do care about safety, otherwise they would have ignored the recall notice, like the people who don't care. You can compare the vehicle software updates to non-vehicle software updates. Those who don't care about security will happily ignore security updates and continue to use old versions of the software on their computers forever. And those who do care about security will update, even if the updates tend to cause other problems, such as removing features or otherwise making the UI worse. reply egeozcan 48 minutes agorootparentprevSome people do not care about safety so we should change the definition of what a recall is and blame an article for being deceptive? You gave a bunch of examples of some people not caring about safety then generalized to everyone. You may be in a bubble, or you may not be, but you're presenting zero evidence to call the article deceptive. reply alkonaut 9 hours agorootparentprev> A recall is a public dangerous defect notice. Yes that's a much better term. In peoples minds \"recall\" means MY vehicle has to be transported somewhere to be fixed. For the individual customer, a recall can be a massive frustrating hassle, which an OTA isn't. That doesn't change the severity of the issue, but a model that has 9 physical recalls to fix some brake issue, and 1 OTA update is going to be seen as a disaster, while a model that has 0 physical recalls and 10 OTA updates will be seen as a pleasure to own. Recalls in consumers' minds are a frustration measurement more than a safety record. Most recalls are about very small/hypothetical risks, so the risk I want to avoid when I look at manufacturers recall history is the risk of having to fix my vehicle physically. Because that's a real/large risk, while the risk of it catching fire spontaneously could be catastrophic but is usually tiny. reply lapcat 7 hours agorootparent> Recalls in consumers' minds are a frustration measurement more than a safety record. Most recalls are about very small/hypothetical risks, so the risk I want to avoid when I look at manufacturers recall history is the risk of having to fix my vehicle physically. Recalls are frustrating precisely because they're safety issues. Otherwise, customers could safely ignore recall notices and experience no frustration. There's no law that says a customer has to respond to a recall. That's entirely voluntary. reply alkonaut 5 hours agorootparentTrue, but tiny risk of catastrophic failure will be addressed by owners almost no matter how trivial. And it won't see as a relief by me that I managed to fix it before it spontaneously caught fire, I'll still see it as a frustrating failure by the manufacturer. Because I can both see it as a near-zero risk, and still do it with near 100% certainty. Especially if I don't know whether the window for fixing it easily/for free could close and it could affect the second hand value of the car. reply lapcat 5 hours agorootparent> And it won't see as a relief by me that I managed to fix it before it spontaneously caught fire, I'll still see it as a frustrating failure by the manufacturer. In this respect, there's no difference between OTA updates and dealership repairs. reply alkonaut 19 minutes agorootparentNo, the frustration comes from taking time in my schedule. OTA are a stain on the safety record. Physicall recalls are a chore. reply BeefWellington 11 hours agoparentprevIt's not the only one. There's been been a number of physical recalls for the Cybertruck, including: - Accelerator pedal sticking[1] - Trunk bed trim detaching[2] - Front windshield wiper failures[3] - This latest drive problem[4] From what I could find via the NHTSA there's only been six this year for Cybertrucks, so it seems like the majority are physical problems. Edit: Forgot HN's formatting for lists. [1]: https://www.nhtsa.gov/recalls?nhtsaId=24V276000 [2]: https://www.nhtsa.gov/recalls?nhtsaId=24V457000 [3]: https://www.nhtsa.gov/recalls?nhtsaId=24V456000 [4]: https://www.nhtsa.gov/recalls?nhtsaId=24V832000 reply screye 12 hours agoparentprevJust because we tolerate it doesn't mean others should too. Legacy industries view software projects as a 1-and-done deal. The \"we'll fix it live\" approach in tech is a short-coming of our discipline. We can ignore it when failure means a mild inconvenience. But, hard engineering isn't as forgiving. Even if the fix is 'just' a software update, the bug can put lives at risk. [1] Each industry and its regulators come with certain norms. Cars are expected to be delivered as 'complete' products. If Tesla can't abide by that expectation, then that's their problem. Don't drag the entire software industry into this. [1] https://www.cars.com/research/tesla-cybertruck/recalls/ reply stackghost 11 hours agoparentprev>But people here tend to know how software development works. Yes and the way software development tends to work is absolutely unacceptable in safety-critical systems in a 7000 lb vehicle. reply pdpi 11 hours agoparentprev> Just because car people are not used to this doesn't make it necessarily bad. Of course it's bad. If this were a purely software discussion, would anybody be saying \"It's OK they have a bazillion zero-days every year because they're quick to fix them when they learn about them\"? Also, remember that the flipside is also true: with aggressive OTA updates, they have the ability to create new issues that weren't there to begin with. I wouldn't trust somebody with that bad a QA track record to not introduce new issues. reply qwerpy 11 hours agoparentprevMine has two physical recalls active, but they’re not serious so I haven’t scheduled any maintenance yet. Unlike my Honda civic a couple of decades ago which had an airbag that was killing people. That one I got taken care of quickly. reply Arcuru 12 hours agoparentprevFrom the first paragraph of TFA: > The latest recall—the wedge wagon’s sixth this year—requires shop time, not an over-the-air (OTA) update. reply gibolt 12 hours agorootparentThat's the point... It is the only one and affects less than 3000 vehicles. A prior physical recall affected a few hundred. For most automakers, a recall involves hundreds of thousands to millions and is almost never an OTA software update. reply BeefWellington 11 hours agorootparentIt is not the only one. While it's true this affects roughly 1/5 to 1/3 of all Cybertrucks sold so far this year (depending on which random sales numbers online you believe), some of the others have affected every Cybertruck sold, such as the wiper motor burning out or the pieces of bed trim falling off while driving. I cannot find any evidence that previous physical recalls this year only affected a few hundred units. Two are 2000+ units and the others seem to be however many had sold by the date of the recall (10k+ in both instances). reply liontwist 12 hours agorootparentprevThat doesn’t answer which one they are considering to determine this ranking, reply WithinReason 11 hours agorootparentprevThis implies some of the recalls were just OTAs reply pbreit 12 hours agorootparentprevSo...one? This article is a hack job. I didn't see any positive commentary. Wired is cooked. reply forgotoldacc 9 hours agorootparentIs it their job to write positive commentary about Tesla? Because I'm not sure it is. I'd argue the opposite, since that'd just be an advertisement. reply __m 11 hours agoparentprevExcept that this is car development with clear guidelines and if you don't adhere to them you have to live with your bugs being labeled as recalls. People should be made aware of when players don't adhere to industry standards with safety implications and you don't get that by just sweeping them under the carpet as \"bugs\". reply hot_gril 10 hours agoparentprevRecall means safety issue, which is necessarily bad. It's nice that they can fix these things over the air, but there was still some elevated risk before they caught it. Despite this, seems like at least the regular Teslas are among the safest vehicles on the road all things considered. reply sitkack 12 hours agoprevhttps://archive.is/SNGSo reply aucisson_masque 10 hours agoprevI don't know about you but there are some kind of car brand and models that if I see on the road i know i must be very carefull of its driver. For instance, brand new Peugeot you know the driver might mistake at some point the brake for the accelerator. I have yet to see a cybertruck in Europe but god knows i will be careful of them. No sane people could ever buy them. I consider this as a feature for other drivers, it's like a big red sign pointing 'i put crayon up my nose'. reply alt227 9 hours agoparentYou seem to be really derogatory and dismissive of people whos opinion you dont agre with. reply ragazzina 4 hours agorootparent>people whos opinion you dont agre with. \"I don't like broccoli\" and \"I don't care about the life of the pedestrian I might run over\" are different kind of opinions. reply GordonS 10 hours agoparentprevAFAIK these monstrosities are not road-legal in Europe, so you're unlikely to see them. reply aucisson_masque 3 hours agorootparentCurrently it's limited but tesla isnt going away anytime soon and now that they reach the end of their preorder list, it's only a matter of time before they expand in Europe. Could be with an European version of the truck for instance. reply masklinn 9 hours agorootparentprevSadly there are already multiple. I think there’s one in Poland because their standards are low and negotiable, iirc there was one in or near Austria as well? Saying that I’m not up to snuff with type approval is an understatement but I think for imports there‘a probably ways to play silly buggers with temp plates if you have the money. But if you’re the sort of persons who decides to import a cybertruck to the eu I don’t think that’s going to stop you. Hell, I think you can drive for up to a year on non-eu plates before the car has to be locally registered as an import. reply p_l 7 hours agorootparentIn Poland, cybertruck as currently sold is at the very edge of acceptable mass for B-class driver license (aka the \"normal\" car driver license, not truck one). However that comes with addendum that you need to have that license for at least two years before driving one, because its based on the exception that allows maximum mass to go from 3500 kg to 4250 kg if the car has alternative fuel drive system, an exception added among other things for electric cars which can add 500kg in just batteries over internal combustion ones. reply masklinn 7 hours agorootparentThat p",
    "originSummary": [
      "The Tesla Cybertruck has experienced multiple recalls since its launch, with the latest involving faulty drive inverters affecting over 2,400 units, raising concerns about its reliability.",
      "Despite quality issues, the Cybertruck's unique design continues to attract buyers, although its inability to meet European safety standards limits its market reach.",
      "The recalls, while demonstrating regulatory effectiveness, may harm Tesla's brand reputation, especially given Elon Musk's significant influence on US regulations."
    ],
    "commentSummary": [],
    "points": 267,
    "commentCount": 393,
    "retryCount": 0,
    "time": 1732601537
  },
  {
    "id": 42243746,
    "title": "Y Combinator often backs startups that duplicate other YC companies, data shows",
    "originLink": "https://techcrunch.com/2024/11/22/y-combinator-often-backs-startups-that-duplicate-other-yc-companies-data-shows-its-not-just-ai-code-editors/",
    "originBody": "The Silicon Valley dream is to build a tech startup that is such a unique idea it alters the commercial universe and turns its founders into billionaires. Participating in the Valley’s most famed startup factory, Y Combinator, is often part of that dream. Airbnb, Coinbase, and Stripe all got started there. Yet, a deep dive into the data from all of the nearly 5,000 companies YC has backed to date reveals a surprising truth: YC startups don’t have to be unique. Far from it. YC commonly accepts startups that are building similar or nearly identical products to previous YC grads. Some of them are direct competitors; others differ slightly by targeting a new geography (Asia or Latin America), or are a subset of a larger market (point-of-sale software for bars versus coffee shops). Data analysis startup Deckmatch conducted the research, inspired to look at competing YC products after a controversy over a YC-backed startup called PearAI. Critics said that PearAI’s code editor product wasn’t much more than a cloned version of another YC product, called Continue — and PearAI’s founder essentially admitted it. There were more reasons that Pear found itself in hot water (including the bravado of its founders and how it handled the open source licensing). But the uproar concluded with Pear’s founders vowing to start over from scratch. YC CEO Garry Tan defended the company, and the fact that YC accepted this behavior, by posting on X, “More choice is good, people building is good, if you don’t like it don’t use it.” This is clearly more than lip service for Tan, who has himself, for instance, championed two police bodycam startups a few years apart: Flock Safety (Summer 2017 cohort) and Abel Police (Summer 2024). Along the same lines, more than a dozen startups building AI code editors went through the YC program between 2022 and 2024 — some in the same batch with the same YC partner. When asked about its propensity to back competitors, a YC spokesperson said that the organization is more interested in the founders’ backgrounds than their business ideas. “YC invests in founders over ideas, focusing on individuals with the potential to build transformative companies — no matter the space they operate in. Our investment strategy focuses on backing the most promising founders with vision, resilience, and ability to execute, which is clear in our RFS process,” a spokesperson told TechCrunch. Some founders love YC’s approach One of YC’s big benefits is its cozy network, where startups often seek customers, partners, and the like. Consequently, some alums dislike competition if they feel another’s product mimics theirs, rather than differentiates. Around the time of the PearAI controversy, YC alum Bryan Onel, founder of security startup Oneleet, posted on X about his experience with this. A few others chimed in to commiserate. (Onel did not respond to our requests for comment.) Then again, other YC alums think this kind of direct competition is good, especially when the same YC partner advises them. Restaurant PoS systems is one area that has been popular in YC, and YC alum Nick Evans, co-founder CEO of restaurant PoS Avocado, is fine with competitors. He should know. Evans famously founded a device-tracking startup called Tile, which went crazy with crowdfunding, raised money from traditional VCs, took on Apple’s AirTags, then sold to Life360 in 2021 for $205 million. “I think it’s stupid that most investors don’t invest in competing companies,” Evans told TechCrunch about YC competition. “I want investors that deeply understand my business and industry. How the hell would they know anything useful if they aren’t working with similar companies? Startups don’t die by murder; they die by suicide. You are not fighting against other startups. You are fighting against people not giving a s— about your product.” Deep dive inspired by PearAI controversy Before diving into the specifics of the categories YC has particularly favored, it’s worth noting that Deckmatch is not a YC company and has never applied to be one, CEO Leo Gasteen tells TechCrunch. Deckmatch was inspired to analyze YC products by the PearAI situation as a demo test for its new product AlphaLens. Deckmatch sells product analysis data on about 8 million startups to private market participants like investors, and corporate innovation and M&A teams. It wants to do for product data what PitchBook did for company-level data, Gasteen says. Earlier this month, Deckmatch raised a $3.1 million seed round co-led by Alliance VC and Luminar Ventures, with participation from its pre-seed investors First Degree Capital and Skyfall Ventures. It’s raised $4.2 million to date, it says. AlphaLens lets Deckmatch customers comb through its database to find unique and similar products, build scatter charts, cluster maps, and the like. But the results of the YC analysis, shared exclusively with TechCrunch, should be fascinating to any founder wondering what kinds of startups YC tends to accept. Types of products YC loves, according to the data According to this data, the current popular product categories, each with at least a dozen startups, include: AI code editors: Beyond Continue and PearAI, another example is Void (another open source alternative to Cursor, the popular Andreessen Horowitz/OpenAI-backed startup). Then there’s EasyCode, Ellipsis, Cosine, Greptile, and more, each applying AI to various coding tasks. Food/beverage/restaurant point of sale systems: Most of the PoS startups were accepted into the program between 2020 and 2023, including Avocado, Dripos, or Latin American startup Polo. Business finance/payroll: With the success of YC alums Gusto and Rippling came many competitors, some aimed at different international markets. Examples include Warp and Zeal. AI sales and customer relationship management. This is a very hot area of development for the big players (Salesforce, Microsoft) and startups. YC alums include Apten, Persana AI, and Topo. AI meeting assistants: Circleback, Onward, Sonnet, and Spinach AI are but a few examples. AI legal assistants: Dioptra, Leya, and Tower are some examples. Then again, several areas were popular but have been recently less so. These include: Crypto trading platforms: Given the success of YC grad Coinbase, YC was gung ho on this for a while, with about a dozen grads, largely from 2014 to 2022. E-commerce store platforms: In the wake of Shopify (not a YC alum) YC accepted about a dozen such companies since 2018, with the majority in the 2018 to 2022 time frame. Corporate expense cards: After YC alum Brex came many others, mostly from 2018 to 2022. Topics ai coding assistant, Exclusive, point-of-sale, Startups, TC, Y Combinator Most Popular YC-backed Circleback is out to become the best meeting notetaker Ivan Mehta OpenAI’s Sora video generator appears to have leaked Kyle Wiggers Anthropic proposes a new way to connect data to AI chatbots Kyle Wiggers Appcharge raises $26M to help gaming apps cut out Apple and Google from virtual goods revenues Ingrid Lunden Executive assistants, high salaries, and other ways early-stage founders will trigger a seed VC Julie Bort Tesla says it has reached a ‘conditional’ settlement in Rivian trade secrets lawsuit Anthony Ha Y Combinator often backs startups that duplicate other YC companies, data shows — it’s not just AI code editors Julie Bort Julie Bort @julie188 View Bio Newsletters See More Subscribe for the industry’s biggest tech news TechCrunch Daily News Every weekday and Sunday, you can get the best of TechCrunch’s coverage. Add TechCrunch Daily News to your subscription choices TechCrunch AI TechCrunch's AI experts cover the latest news in the fast-moving field. Add TechCrunch AI to your subscription choices TechCrunch Space Every Monday, gets you up to speed on the latest advances in aerospace. Add TechCrunch Space to your subscription choices Startups Weekly Startups are the core of TechCrunch, so get our best coverage delivered weekly. Add Startups Weekly to your subscription choices No newsletters selected. Subscribe By submitting your email, you agree to our Terms and Privacy Notice. Related Apps YC-backed Circleback is out to become the best meeting notetaker Ivan Mehta 33 mins ago Startups Roon raises $15M to replace ‘Dr. Google’ with real doctors sharing videos about illness treatments Marina Temkin 33 mins ago Startups From $19M to $1.5M, here’s how much Anduril pays top execs like Palmer Luckey in cash and stock Margaux MacColl 60 mins ago Latest in Startups See More Startups Roon raises $15M to replace ‘Dr. Google’ with real doctors sharing videos about illness treatments Marina Temkin 33 mins ago Startups From $19M to $1.5M, here’s how much Anduril pays top execs like Palmer Luckey in cash and stock Margaux MacColl 60 mins ago AI Perplexity mulls getting into hardware Kyle Wiggers 3 hours ago",
    "commentLink": "https://news.ycombinator.com/item?id=42243746",
    "commentBody": "Y Combinator often backs startups that duplicate other YC companies, data shows (techcrunch.com)240 points by isaacfrond 10 hours agohidepastfavorite133 comments light_triad 4 hours ago> YC startups don’t have to be unique. Far from it. Slow news day. YC has made it clear over the years it's almost all about the founders not the idea. It's almost impossible to know if an idea is good at the early stages, and ideas change, startups pivot, often the timing is wrong, and the market unproven etc. More often than not most of what startups do is not unique, otherwise there's no market for it. A new take on an old idea has a much higher chance of success than something totally new that ends up being too early. reply ActionHank 3 hours agoparentSteve Jobs or maybe just Apple used to pit teams against each other to develop a solution to a problem and then pick the strongest of the two. It also offers a convenient way to pull investment out early if the doppelganger startup isn't meeting targets. Just get the original startup to buy them out. It's not something that is a negative for YC, but it could be for the startups they back. reply osigurdson 3 hours agorootparentAgree. I have watched several YC videos where they state this again and again. What they are less specific about is how precisely founders are selected. reply fsckboy 1 hour agorootparentprev>Steve Jobs or maybe just Apple used to pit teams against each other to develop a solution to a problem and then pick the strongest of the two. probably that's Apple. Steve Jobs is more famous for jumping from the failing project (Lisa) onto the winning team (Macintosh) reply drewda 3 hours agorootparentprevEven today there are decent odds that within Apple the macOS and iOS groups are locked in a multi-decade long fight in which one may prevail... reply throwaway894345 3 hours agorootparentprevReminds me of how pelicans will raise two chicks and then kick the weakest of the two out of the nest and devote all future resources to foster the stronger of the two. YCombinator as convergent evolution, I guess. reply dxbydt 1 hour agorootparent> how pelicans will raise two chicks and then kick the weakest there is a youtube video ( pls don’t watch ) that gave me nightmares for days on end. i had completely forgotten about those pelicans and now you had to bring it up again in the comments. the little pelican had a broken leg. It was limping, so mom and brother together pushed it out of the nest. Very cruel. reply alganet 37 minutes agorootparentPelicans are brutal mindless beasts. They are known to fly to neighbour nests and eat other birds, sometimes other pelicans. So similar to companies. reply Taylor_OD 1 hour agoparentprevThe CEO of YC gave a talk about how you should first assemble a founding team and THEN come up with an idea. The idea seems way less important than identifying founders they want to back. reply itsoktocry 1 hour agorootparent>The idea seems way less important than identifying founders they want to back. Especially when you can take an idea you like from a team you don't, and give it to a team you do! reply mrcwinn 3 hours agoparentprevThey say that, but what if the opposite was the actual truth? Look every VC is \"founder friendly\" because without founders there's no deal flow. At the end of the day though every VC is optimizing for exit value. So it makes sense to bet on the same idea twice. If Founder A screws the pooch, you've still got Founder B. reply ericjmorey 2 hours agorootparentThis matches typical VC investment patterns. reply JumpCrisscross 4 hours agoparentprev> More often than not most of what startups do is not unique The message is also that if your start-up is based on deep tech moats choose another seed. (Think of YC's home runs. None had an industry secret/IP secret sauce.) reply jedberg 2 hours agorootparentYC has only been investing in deep tech for about a decade, I'm not sure that's long enough for them to have had a home run yet. For example, Boom Supersonic was founded 10 years ago, and just this summer completed their second test flight. They are scheduled to launch commercial operations in 2029. Lucid bots was founded in 2018, and just did their Series A to fund their growth of their autonomous drone fleet. I wouldn't count YC out of the deep tech game just yet. reply light_triad 4 hours agorootparentprevYC's roots are definitely the software hacker/painter/kill FAANG from a coffee shop type founders, although they've shown that deep tech teams can make interesting progress without years of R&D. Finding investors that get your idea and can help with their network is paramount. YC might not be the best fit in all cases. reply j45 2 hours agorootparentprevYC is more focused on B2B now, so this will change. reply itsoktocry 1 hour agoparentprev>It's almost impossible to know if an idea is good at the early stages But identifying the \"right\" people is easy? reply shahzaibmushtaq 0 minutes agorootparentA good idea will not scale if the founder and founding team are bad, a mediocre idea will scale if the founder and founding team are good. reply throwaway314155 1 hour agorootparentprevSpoiler alert: it's \"easy\" when you narrow down to applications who come from wealth and/or went to a prestigious university. reply thatguy0900 1 hour agorootparentprevIt's not, but that's ycs entire business model reply Cthulhu_ 4 hours agoparentprevThe article lists a few, none of which were unique - coinbase? Cryptocurrency exchanges already existed. AirBnB? Booking.com and other hotel or bed-and-breakfast booking websites already existed. Stripe? Yet another payment provider. Reddit? A Digg clone. Dropbox? rsync. etc. reply keiferski 3 hours agorootparentAirbnb was basically a monetized form of Couchsurfing, not a hotel alternative. It only later turned into a direct hotel competitor. To my knowledge there was no similar service at the time. reply light_triad 3 hours agorootparentFolks had been using alternatives to rent space like HomeAway.com, VRBO.com, BedandBreakfast.com and Craigslist for years. Airbnb won because: - Better design, easy to use, nice pictures they took themselves at first (per PG's advice) - Integrated payments and reviews in a seamless experience - Popularized the idea of sharing a space when the host is there - Offered unique accommodations in urban places - Lots of buzz about treehouses, castles, teepees, shipping containers, etc. - Their timing benefited from the Great Recession - Very good at promoting their rag to riches story with YC's help - Also you didn't need to constantly repost like Craigslist, just post once reply dowager_dan99 2 hours agorootparent>> - Popularized the idea of sharing a space when the host is there this was not a popular option, and I don't believe was ever intended to be. It also opened the door for VRBO to gain traction explicitly marketing NOT sharing accommodation. The \"rent out your spare bedroom\" has never been a major component; it was a fake-out to counter the regulatory & licensing complaints they were facing. reply keiferski 3 hours agorootparentprevI agree with the reasons for their success, but I don't believe that any of those alternatives allowed you to book a room/couch in someone's apartment. They were merely ways to either use properties for vacations (VRBO), semi-hotels like beds and breakfasts, or just regular renting space (Craigslist) for short/long term. Not the same thing as AirBnb at all. reply j45 2 hours agorootparentprevThe first resonating problem you solve is rarely the bigger problem you end up completely solving. reply keiferski 2 hours agorootparentBut AirBnb didn't \"completely solve\" the hotel industry, and hotels still exist. If anything, they seem to still basically be operating in the same space as they started: owner-based rentals, for both individual rooms and entire properties. reply thrw42A8N 3 hours agorootparentprevYeah, back then Booking was hotels only, not people's apartments or couches. reply vonneumannstan 1 hour agoparentprevThey have shifting and conflicting and nonsensical reasons for what they do. Just like PG lambasting someone as being too stupid to see when they asked why so many YC companies are things that should just be features on other software. reply soheil 1 hour agoparentprev\"Don't have to be unique\" is very different than \"actively trying to copy\". reply meta_x_ai 40 minutes agoparentprevHN has turned into an anti-billionaire, anti-VC, anti-success cesspool. It's a very dangerous echo chamber that'll destroy within reply onion2k 8 hours agoprevI think there's something missing from this analysis - YC companies might duplicate one another's products but that doesn't say anything at all about their target markets, route to market, product focus, etc. As an example, my first startup was a requirements management app that, on paper at least, was a copy of IBM DOORS. Except DOORS is a massive enterprise app targeted at companies who are building a new airplane that has 250,000 technical requirements that need fully traceability, and my app was aimed at small businesses doing software projects with 1000 requirements. I was not competing with IBM; I was trying to apply the value of that app to much smaller projects (the end goal might have been to become a competitor one day but we failed long before then.) You can't just say \"These two products do the same thing so the companies are dupes.\" It's far more complicated than that. reply hiAndrewQuinn 1 hour agoparentIncidentally, if anyone else wants to reinvent IBM DOORS for ~1000-requirements fields, email me. There's a vast market out there for combined hardware/software shops that are in the no man's land of \"not designing an entire literal train from top to bottom\" and \"not just hiding a Raspberry Pi in a trench coat\". reply mrandish 49 minutes agoparentprevYes, I agree and would add other reasons. It's always seemed obvious to me that YC would invest in multiple startups working on the same problem space for these reasons: First, compared to a traditional VC, YC is micro-investing in far more startups and doing so faster and more frequently. Second, YC highly prioritizes founder/team quality over ideas but teams come to YC by pitching an idea. I just read an article yesterday citing evidence that in the last 10-15 years the diversity of startup ideas has declined significantly. This makes sense. Enormous amounts of information about what other very early stage startups are doing as well as which areas are currently being funded is readily available in near real-time. As a serial entrepreneur who did my first startup in the 80s, second in the 90s and third in the early 00s, I can attest we had nothing like this visibility in earlier eras. Ideas which are getting funded and which sound like good ideas in hot areas are obviously going to influence founder idea selection and cause clustering. Third, YC knows that many startup teams will pivot away from their initial idea once they engage with real customers in real markets. So much so, YC considers it a sign of a smart team working well together. However, like all VCs, at any given time YC has broad sector themes it considers especially ripe for various reasons ranging from new technologies enabling disruption to expected high growth in an emerging sector. Fourth, is the reason you identified. Having a good enough high-concept idea is necessary but far from sufficient for startup success. In addition to the execution details like go to market that you mentioned there's also timing. Even being 12 months earlier or later can make a difference toward making it. Finally, there's the luck factor. While it's true that quality teams are better able to maximize good luck when it happens and also have a tendency to increase their overall odds too, luck still matters. In my successes we had to get a lot of things right but there were also three or four serendipitous things that made a big difference at important moments. The only way for YC to solve for both the micro-timing and luck factors is betting on multiple similar startups around the same time. reply meiraleal 6 hours agoparentprevThat's not the case for most dev tools YC startup, they are targeting the same HN users/ other YC startups (trying to sell shovels) reply fidotron 5 hours agorootparentThe great suspicion with YC is what proportion of YC companies have all their customers being a mix of other YC companies and those with shared investors? There is a real danger with current era Bay Area tech that it is just a game of musical chairs played with money, with remarkably little external value being generated. reply cudgy 4 hours agorootparent> There is a real danger with current era Bay Area tech that it is just a game of musical chairs played with money, with remarkably little external value being generated. So, you don’t think that is already the case? reply fidotron 4 hours agorootparentIt was nothing like as bad as this 10 years ago, no. Just look at the state of successful exits - it is awful. This is not the same as an ecosystem producing Sun, HP, Apple or even Google, which all had enormous positive externalities. reply jjulius 3 hours agorootparentTen years ago was 2014 - Sun, HP, Apple and Google were all very much \"old hat\" and entrenched. Hell, HP was in the process of significantly cutting back it's business and eliminating jobs[1] (34,000 lost jobs in 2013 and ~16,000 in 2014 is an interesting \"positive externality\" lol). There was no ecosystem producing them, they were the people who were either running SV, or realizing that their time in the sun was coming to an end. [1]https://en.wikipedia.org/wiki/Hewlett-Packard reply meiraleal 3 hours agorootparentprevYes it was like this for Apple, Microsoft, Google, Facebook. The difference is that at that time the genius youngster (which is just the son of some already successful people) creating startups were cool. Now nobody wants to make or watch a movie about Sam Altman and OpenAI. reply spacebanana7 2 hours agorootparentprevThis can be a good thing. One of the hardest things about building new software products is getting initial customers to discover your bugs, missed requirements etc and also add credibility. As an enterprises SaaS buyer, I'd much rather use a code documentation tool that'd been circle jerked around a YC batch a few times compared to one with no prior customers. reply josh_carterPDX 24 minutes agoprevWe took our startup through Techstars in 2016. The next year they took a very similar company that ended up getting more funding and attention. We didn't understand it at first, but looking back it was definitely because they were the better team. In the end, both teams failed to scale so it ended up being the wrong platform at the wrong time, but if one of them could have taken off then the bet they made would have paid off. reply mst 5 hours agoprevIf nothing else, having competition helps validate the market. Which is often an advantage to all companies involved - a lot of the time, you're only notionally competing with each other, your main enemy is \"people not using a product in that space at all.\" e.g. for a lot of business SaaS the only enemy worth caring about is Excel. (I've more than once been involved with companies where the \"competitors\" were all on friendly terms because convincing people that using *some* product in that space was a good idea expanded everybody's addressable market) reply snarf21 4 hours agoparentI see it more as more of a hedge. If you believe in the opportunity, placing extra bets makes sense. Uber and Lyft weren't the only ride share companies but sometimes luck wins out and sometimes execution does. Additionally, if one seems to be winning, you just acqui-hire the \"loser\" in the winner, use that problem space expertise to scale faster AND you still get to claim a higher exit rate even if it was just to yourself. reply mst 2 hours agorootparentSure, though I'm not sure that \"I see it more as\" makes sense in context given - You're talking about how YC sees it. I was talking about how it affects things from the companies's point of view when YC does that. As such, so far as I can see our actual statements about the upshot of the situation are probably both correct :) reply zachthewf 4 hours agorootparentprevExactly right. Traditional VCs come up with a thesis and then buy 20% of the company they think will be the winner who fits that thesis when they're at like $1M to 10M revenue (series A) YC can instead get ~10% of every plausible winner they come across when they're at $0 revenue reply mst 2 hours agorootparentAs noted elsethread, I think that almost certainly *is* what's going on; I was explaining why it's probably a good thing from (at least many of) the companies' perspectives too. reply xpe 5 hours agoparentprevYes, this can happen. How often and under what conditions I’m not sure. I’ll give some other lenses: 1. From the point of view of an individual person, growth in the overall market is often an advantage. If one company doesn’t survive, there will be probably be others. Your skills and connections can help in a similar organization with a slightly different angle on the problem. 2. From the point of view of memetics, good business ideas are likely to appear and survive and take many forms in many niches. If you find yourself with competition, this can suggest that the underlying ideas are suited to the current environment. (Warning: this tendency can be distorted by irrational herd behavior and intentional gaming.) reply mst 4 hours agorootparentYes, and as a variant on (1) from the point of view of potential *customers* it makes adoption (at least feel) less risky because if your original chosen supplier goos *poof* there'll be another one to switch to rather than you having to re-adjust your internal processes to go without. (I'm not sure under what conditions either, I mostly know this from having lived it and I wasn't on the business side in any such cases so) reply Ballas 6 hours agoprevIt makes sense. If you are an investor and have a strong belief that a specific product or idea is a good one - you might want to decouple your odds of success from the people/team/company executing that idea. reply authorfly 5 hours agoparentWhat's interesting is that the people who are able to predict and come up with the idea (e.g. a researcher using AI in 2021) are often not the best ones to execute on it (typically, lack of experience or capacity to handle the pain of growth while marketing). What's most interesting is that most people aren't just \"one type\". In your life you go through multiple roles. Just like how most people, regardless of their income at age 20 will be earning top 35% income by age 35, people move up in their roles (and struggle at a young age to understand that this will happen to them). It's all about timing and age. I believe some investors go for multiple teams purely because the ratio of those types is different - like a different risk profile. They invest in one with 80% AI boffins, and one with 80% business folks, and one will likely win in the circumstance of the moment (in the ChatGPT era - mostly the business folks. In the data driven AI era - mostly the AI boffins) reply MrMcCall 5 hours agorootparentHuman beings are the only creatures capable of self-evolution, using our minds to analyse the world around us and the world we have nurtured within us. Over time, we can stagnate into ossified animalistic patterns of selfishness, or we can expand our curiosity, consciousness, and, hopefully, our realization that compassion for others is the source of our own happiness. We can either find ways to better integrate ourselves into a better future for those around us, or wall ourselves off from the world around us. We would do well to remember that carpenters don't make hammers, nor computer scientists their own food, soil, packaging, tractors, or trains. The use of the vast sums available to successful tech investors gives them a greater point of leverage than the rest of us not so endowed. And the more power is given, the responsibility is required, though few acknowledge or heed such wisdom. At the end of the day, no matter how confident many folks are, who really has the humility and intelligence to know if a person is a genius? Very few, I guess. Very, very few, indeed. reply bwestergard 5 hours agorootparentprev\"Just like how most people, regardless of their income at age 20 will be earning top 35% income by age 35\" Could you say more about this, or perhaps provide a link where we can read more? reply hibikir 4 hours agorootparentThe general is that income, like wealth, is correlated with age, not ransom. Most workers under 18 will be making near minimum wage. Workers in their 20s are probably still starting their careers. People fall off again when they are older, as they either retire early or in some professions just become less capable. So when you put it all together, a lot of people with an under average career will have an over average income at 35: Just not an over average income within the cadre of 35 year olds. The lack of correlation with income at 20 comes from how many careers require training that doesn't give good early income. A future doctor, barista or AI programmer are not likely to have. a great income at 20, but their incomes and wealth diverge rapidly as some have longer training with different outcomes. The doctor will hit the 1% after residency. The AI expert might start making money earlier. The barista is probably ahead in his 20s, but it's unlikely their income grew quite as much, although many a barista is working on doing something else. So again, looking just at percentile of income at different ages is going to lead to mistakes as different life curves are being aggregated together. reply codegeek 4 hours agoparentprevExactly. I think this is a brilliant strategy by YC. They know that some ideas have great potential. They just back multiple teams and hope that one of them will win with their execution. reply bingemaker 5 hours agoparentprevYC also says that they don't believe in the idea, but the founders ¯\\_(ツ)_/¯ reply ethagknight 5 hours agorootparentSeems reasonable to pick two horses in a race you believe is worth running reply billy-ilograph 5 hours agorootparentThis creates an extremely obvious conflict of interest. reply xpe 4 hours agorootparentWe need to define conflict of interest. Question: is an investor who gives money to one organization, but is not involved in the decision-making, conflicted if they give money to another organization? Are they self-conflicted (undermining their own likelihood of success)? Are they contractually or legally conflicted? Have they breached the trust of people they invested in? Are they ethically conflicted? Answers to these questions are non-obvious. Attempts to simplify the set of relevant questions means imposing a worldview. On the ethical question, a consequentialist would say it depends on the outcomes. Like many consequentialist analyses, this is complicated. Consider this: Investing in a similar company might validate the market and make it more likely for the company and/or its people to reach viability. reply Suppafly 1 hour agorootparentprev>This creates an extremely obvious conflict of interest. That's not necessarily bad though. People like to throw out these terms that sometimes have negative connotation as if they are inherently negative. If you think the conflict of interest is a bad thing, you need to elaborate on why you feel it's bad, not just pretend it's prima facia bad. reply pavlov 5 hours agorootparentprevThe conflict of interest is simply dead and forgotten in an era where the president-elect has his own cryptocoin, his own social media company, and appoints his billionaire supporters to improve efficiency in the parts of government that directly oversee those billionaires’ own businesses. reply xpe 5 hours agorootparentYes, political corruption is a drag/loss on everyone except the corrupt ones. Worse, it can shift a system out of its viable operating zone. Corrupt individuals in a market can destroy the market. But what is the connection to the parent comment? No matter how corrupt one part of government or a market becomes, it doesn’t excuse further corruption. If anything, more corruption makes additional corruption more likely to break the whole system. reply BeefWellington 4 hours agorootparentI think it's reasonably straightforward they were making the point that conflict of interests are no longer taken seriously, along with many other related things. Politics tends to be a trailer of social views, not a leader. reply svnt 4 hours agorootparentprevThe connection is that the conflict of interest being discussed was only ever a social/ethical contract. That social contract is in the bin right now, so the question is moot. reply xpe 1 hour agorootparentIndividuals who voted for Trump don’t necessarily want to throw away a social contract. Many of them do support societal norms, albeit different ones. And some don’t even think in these terms; they are more motivated by other factors. Along with many, I think their collective actions point in a direction that (a) undermines democratic rule and (b) enables Trump’s corruption, but they seem to be relatively unaware or disagree with such effects. Many of them think Trump will combat one some types of corruption (the “deep state”). Overall, I’m more inclined to think many/most Trump supporters have reasonable core values, especially at the individual and family levels, but due to their information sources and mental processing, their overall choices don’t bode well for us, together. The biggest breakdown I think has to do with epistemic values: how does one find truth. I don’t think most people, of any party, have the individual ability and discipline to make sense of a modern world in a rational, scientific manner. This isn’t something easily achieved, after all. About me: I strive to not “blame” individuals in the traditional sense, because I reject free will as a meaningful concept. (Roll back the clock and a person will the behave the same in a deterministic universe. And if the universe has intrinsic randomness, we can’t ascribe free will to that randomness.) So instead of blaming individuals, I focus on systems and their statistical effects. reply CuriouslyC 4 hours agorootparentprevIf that were true, there'd be no point in ever applying a second time to YC. Anecdotally, people do get accepted after 2-4 failures. Maybe YC was on the fence about those founders and their willingness to slam themselves against a wall repeatedly tipped the scales, or maybe they invest in a certain kind of founder and when they run out of those they invest in \"promising\" ideas. reply robertlagrant 5 hours agorootparentprev> YC also says that they don't believe in the idea, but the founders ¯\\_(ツ)_/¯ That still doesn't preclude them backing two founder groups that have similar ideas. reply pilingual 2 hours agoprev\"Y Combinator seems to be the perfect place for mergers. Every winter for the past three winters, Y Combinator has funded a podcasting company. In winter 2017, Breaker. In winter 2018, The Podcast App. And in winter 2019, Brew.\" https://dan.bulwinkle.net/blog/there-should-be-more-mergers/ reply joshdavham 2 hours agoparentAny idea why mergers aren’t more common? The article doesn’t really seem to have an answer to that. reply debacle 15 minutes agorootparentHow much should one company with a 40mm valuation and no revenue pay for another company with a 25mm valuation and no revenue? reply gwbas1c 5 hours agoprev> YC commonly accepts startups that are building similar or nearly identical products to previous YC grads. Some of them are direct competitors; others differ slightly by targeting a new geography (Asia or Latin America), or are a subset of a larger market (point-of-sale software for bars versus coffee shops). If you're going \"by the book\" with Crossing The Chasm, (https://en.wikipedia.org/wiki/Crossing_the_Chasm), all startups need to conquer a niche before they can own an entire market. It seems like a wise strategy to, if an investment does well (or otherwise proves a market) in one niche, to invest in another niche. It certainly increases the odds that you've invested in whoever will become the dominant player. reply BeefWellington 4 hours agoparentIt seems pretty obvious to me that in modern times successful companies pick a segment of business to build expertise in and then expand outward from there. Your ability to understand a specific client's business is usually gonna be your key differentiator against large incumbents. reply serial_dev 8 hours agoprevIt makes sense as an investor, doesn't it, you identify markets you see an opening, you check which companies are in that domain, pick the ones where they can execute and have good people on their team. It would worry me as a startup company, who knows where the information I share with them will end up... reply soco 6 hours agoparentA more interesting question is, do you as a startup company have any information worth sharing? Ideas are dime a dozen and like you just said, the differentiator is the capabilities of those people. And that is not something that can be shared, and no other startup can benefit of it. reply srameshc 6 hours agorootparentIt's not always about ideas. Growing startups often have proprietary information they can't share with competitors. reply seizethecheese 54 minutes agoprevIt’s clear why this is good for YC, but it’s probably also not terrible for founders. I’ve heard of funds that avoid funding competitors passing in investments because they didn’t want to be locked out of other startups in the space. This removes the perverse incentive that the better your idea, the more scrutinized you might be as a founder.This also allows YC to be fund a lot more companies than otherwise. reply nextworddev 26 minutes agoprevIt’s more the case of YC batch members often copying fellow batch or older batch’s standouts usually reply cadamsau 8 hours agoprevSeems like a great idea! Stirs up competition within a cohort and there’s bound to be idea sharing between teams. YC claims to bet on people not ideas, so as another commenter pointed out, if a team pivots there’s no longer a competition. My guess is YC also believes in simultaneous invention: the same idea coming from multiple places at once implies said idea’s time has come, while any team wanting to work on it must have already done the work to hit that forefront, making them great people to bet on. reply swaptr 3 hours agoprevNot surprising. This reminds me of HBO's Silicon Valley: Ron LaFlamme: So Pied Piper, You're one of Peter's compression plays, huh? Richard: One of? How many does he have? Ron LaFlamme: Not too many, like six or eight. Richard: Okay. Why are there so many? Ron LaFlamme: You know how sea turtles have a s*t ton of babies because most of them die on their way to the water? Peter just wants to make sure that his money makes it to the ocean. reply cynicalpeace 2 hours agoprev\"That is already being done\" is a good sign in the world of startups. It means there is a market. It's hilariously used to shoot down startup ideas. reply frigidnonce 2 hours agoprevDiversification is a hedge against ignorance. Investors are always more ignorant than founders to begin with (breadth vs depth), and at the stage YC backs most companies at, even the founders don’t have much of a clue how successful they will be. This is the “correct” investment behavior. reply oceanplexian 3 hours agoprevThere are a lot of people who seem to think that zero-sum economics is how businesses work when it couldn’t be farther from the truth. For example, if I open a bar downtown it may attract 100 customers. But if two entrepreneurs open two bars next to each other, they might attract 300 customers between them. The same can apply to entire market segments. reply blitzar 6 hours agoprevVC sees buzzword in pitch and throws money at it, data shows. reply lccerina 5 hours agoparentThis! Other comments imply some kind of strategy from YC, but it's likely a VC bias and/or a lack of communication inside YC. reply ChrisMarshallNY 6 hours agoprevThis isn't really a big deal. Usually, obvious market opportunities have multiple organizations, trying to enter, and it makes sense for an investor to diversify. That's different, from Amazon and Microsoft, who used the data they gathered during their work with smaller orgs, to then actively compete with those orgs. A classic Microsoft joke (hope you like Comic Sans): https://www.davar.net/HUMOR/STORIES/MS-CUISN.HTM reply bhouston 4 hours agoprevI think this is actually okay. Execution matters, not the idea. Also if YC was trying to do coordination between its portfolio companies, it would be against the interests of the founders themselves because the founders do not care if another company in the YC batch succeeds or fails - they don't have a stake in that other company. reply evoke4908 4 hours agoparent> Execution matters, not the idea. This is the core ethos driving Chinese manufacturers to rip off anything and everything they can reply bhouston 4 hours agorootparent> This is the core ethos driving Chinese manufacturers to rip off anything and everything they can I think everyone does this. We just like to focus only on Chinese companies when they do it, because many times when Chinese companies do it, they do it so well that it is a threat. reply typeofhuman 4 hours agoparentprev> Execution matters, not the idea. Execution matters _as much as_ the idea. A good idea executed poorly produces bad results. A bad idea executed well produces bad results. This is what YC is hedging (was it the execution or idea) by investing in duplicative startups. reply _fizz_buzz_ 4 hours agorootparentIt's more like 90/10. 90% execution and 10% idea. And actually almost all successful startups didn't have \"original\" ideas: - google wasn't the first search engine - facebook wasn't the first social network - tesla wasn't the first electric car - ChatGPT wasn't the first chat bot reply bhouston 4 hours agorootparentprevI guess I am saying that ideas are a dime a dozen. Every idea will likely get executed by multiple teams - it is incredibly rare for an idea to only get explored by a single team. Those teams that succeed did so because of execution. reply snarf21 4 hours agorootparentprevI disagree. True \"good execution\" will not produce bad results over the medium and long term but that is what good execution is actually solving for. Good execution should recognize when/how pivots should be made to produce value. reply whoknowsidont 4 hours agoprevI think a different, perhaps weird, way of looking at this is that YC looks at the founders first and then market opportunity (\"idea\") secondly as justification. YC doesn't need a single version of an idea to \"win\", and it's often stated plainly it doesn't even necessarily value the \"idea\" behind a lot of these investments but the founders themselves. If you look at it that way, it's no surprise that multiple startups are working at the same market opportunity (\"idea\"), and that YC actually just wanted the founders themselves regardless of whose execution of the \"idea\" ends up winning (if any). reply paulryanrogers 3 hours agoparentThe cynic in me wonders if one of their goals is to keep the most talented people away from the competition, as cheaply as possible. With the goal of preventing independent competition, which could be seen as a greater threat to their golden boys. reply paxys 6 hours agoprev> Yet, a deep dive into the data from all of the nearly 5,000 companies YC has backed to date [...] The answer is in that number. YC is a startup accelerator. These days they back 500-1000 companies every year. There is no intention of having only one horse in every race. At that volume that is impossible. Their funding model allows them to bet on as many horses as possible to increase their own chances of success. reply postit 2 hours agoprevTo be honest, I would do the same. If it’s a good idea, maybe the only difference between It’s succeeding or fail is the team executing it. So in order to maximize the outcome, you double the bet and it’s even better than have someone doing it outside your control reply tptacek 1 hour agoprevAnd? As the article says, they've funded thousands of companies. Of course there's overlap. It would be weirder, much weirder, if they kept a conflict list like a law firm and avoided funding things that intersected their portfolio. It's hard to emphasize enough how little YC knows about the companies they fund, and how little influence they have over the companies once they've funded them. Often, there's barely a company at all, just a small team that YC takes a shine to. Even if YC wanted to keep a conflict list, they couldn't: companies joint the program with one idea and launch with another. To have a problem with that is to have a problem with the entire concept of YC; this is the process they invented, that they're notorious for. reply fsckboy 1 hour agoparent>It would be weirder, much weirder, if they kept a conflict list like a law firm modern portfolio theory (MPT) is the only acceptable measure of investment performance, and calculating co-variance is the salient feature of MPT, so no, it would not be weird to track overlap. to sum it up in a single sentence, is OpenAI a good investment for you at $50 a share? Without even knowing anything else about the company, if the rest of your portfolio is already 100% shares of OpenAI, or even AI companies, no, for you, probably this would not be a good investment: the market does not reward diversifiable risk. reply tptacek 1 hour agorootparentAt 5000 companies this is a little like saying that investors should avoid the S&P 500 because it includes both Coke and Pepsi. reply addcn 5 hours agoprevUseful here-say from some investors at the last few demo days: not all of the companies that are \"copiers\" apply + are accepted with the \"copying\" idea. Many founding teams end up pivoting during the batch and scramble to get proof points on the board before demo day. They're most likely to end up pivoting to well-known problems, therefore the clustering around a few common themes. It doesn't explain all of the data, but it's a big part of it. When you have to come up with a fundable new idea in a week and prove it out in a month this can happen... reply svnt 3 hours agoparentNot to be pedantic but maybe it will help in the future: it is spelled hearsay. You heard someone say it. reply logtrees 1 hour agoprevMakes sense. That way it reinforces those accumulated companies so they remain best in class. reply neallindsay 3 hours agoprevMaking lots of bets is a core part of the venture capital model. reply ninth_ant 5 hours agoprevOf course YC backs multiple companies in the same spaces, they have for quite some time if not from the start. The controversy with pearai was not because it competed the market space of another YC company, but that it had the appearance of being a direct clone of the exact product of another YC company. reply MaxPock 5 hours agoprevReminds me of some German guys who specialise in copying successful companies and have made billions in the process . https://en.m.wikipedia.org/wiki/Rocket_Internet reply phyzome 6 hours agoprevIt's interesting how many of the commenters are starting from the assumption that this is both intentional and desirable (or in some other way smart on the part of YC) and then reasoning only on that basis. (Or maybe it's not interesting, since this is on YC's own site... :-P) reply dahart 3 hours agoparentWhy is that interesting to you? Do you have a contrasting point of view to share? Do you have experience with startups and/or venture capital? I have no involvement with YC, but with a little startup and VC pitching experience, I can tell you that in my experience, lots of VCs like the idea of founders doing something incremental that builds on successful ideas. The article notably did not compare YC to any other VCs, but the truth is likely that all VCs “often” back startups that duplicate others in some way. VCs will tell you not to build something original, because there’s no demonstrated market for it. And statistically they’re right, your chances of success with an unproven idea are lower than with an incremental change to something people already buy. The article also notably did not talk about how often startups duplicate other businesses on their own, before there’s VC involvement. In that sense, despite the claim in the title that there’s data, the article is unscientific. This article struck me as one of those “studies” that shows something everyone already knows, and the title kinda seems designed to sound dramatic and/or accusatory to appeal to readers drawn to controversy even if there isn’t really any controversy there. reply Gormo 6 hours agoparentprevConcluding that it's not intentional depends on the premise that they don't know they're doing it, which seems unlikely. reply smt88 5 hours agorootparentThere's an enormous amount of evidence that almost no VC knows what they're doing (almost none beat index funds in the long run). YC seems to have a spray-and-pray approach, and it used to be run by Sam Altman who has repeatedly failed upward, so I think it's very reasonable to assume this is either not a conscious strategy or it's just a bad strategy. Either way, the VC's value is to be able to predict whether Dropbox or Zumodrive deserves their bet, and they clearly couldn't. reply empath75 4 hours agorootparent> Either way, the VC's value is to be able to predict whether Dropbox or Zumodrive deserves their bet, and they clearly couldn't. This is an extremely wrong-headed view of what VCs do, and one thing investors do _in general_ is to have a strong idea of what they know and don't know, and in particular, what _nobody_ knows is which companies or products in particular will succeed or fail. If they knew that, they'd put all their eggs in one basket. What VCs do is allocate capital in a way that mitigates risk for themselves. There's actually a _really_ interesting way to think about what VCs do, which is that they _offload_ their own risk onto founders and early hires of startups. VCs invest their money across a broad basket of investments, founders invest all their time and money into _one_. VCs and early hires are taking a massive amount of personal risk. Almost all of the profits of VCs come from what is essentially a risk arbitrage -- they get more profits than they should be from the smaller risk they're taking by investing in a startup, and founders and early hires get less profits than they should be from the personal risk they're taking by starting a company. The structure of investment deals is often setup in such a way that even events that \"feel\" like they should be a payday for the founders, such as a funding round or even a sale, could end up with them getting nothing because their shares get diluted, because they have lower priority ownership stakes than the VCs do. reply smt88 3 hours agorootparentYour description of a VC's value is describing them as an index fund of startups, and I suppose you could say YC has moved in that direction. But most VCs are paid to make bets on which companies are most likely to succeed. I never said they're supposed to know with certainty. That's a silly straw man that no one who understands basic finance would suggest. But they spend much of their time supposedly screening deals to find good ones, and there is a preponderance of evidence that they're worse at it than a dumb index fund. There have even been experiments to automate picking deals, and even a fairly naive algorithm is better than most VCs. Only people on the VC side of the table think they have any added value other than lucking into being trusted to invest other people's money. reply svnt 3 hours agorootparentprev> VCs and early hires are taking a massive amount of personal risk. I think you meant “Founders and early hires” here? reply mailarchis 6 hours agoprevthis was true 17 years back too. dropbox and zumodrive were both in same batch and were tackling the same problem with different approaches. P.S. Not saying its a bad thing. Early stage startups evolve and like YC has shared a lot of the early bet is on the founders. reply zitterbewegung 5 hours agoprevJust because you have similar or the same idea doesn’t mean at the end you have a sustainable one. Also, companies get bought out but while ideas are the same execution and scope can change given the level of technology available . reply _heimdall 6 hours agoprevThis isn't anything new. Heck, the first season of Silicon Valley called this out with Peter Gregory backing a number of compression startups. Interestingly for the OP, that same season featured Tech Crunch Disrupt. It's a bit ironic to see TC publishing this concept now as though it is a new revelation. reply MrMcCall 5 hours agoparentIt makes sense to scatter a bunch of seeds on fertile ground, and then pick the strongest plant that shows the most growth. There are many kinds of smart, ambitious people; some will form teams that succeed, others will fall prey to unforeseen impediments, and not. reply mhh__ 4 hours agoprevHedge funds that use a \"pod\" structure (Chinese walled groups of traders) will hire one pod to mimic another pods strategy. Not hard to guess why. reply Jabbs 2 hours agoprevDoes anyone know the best alternatives to YC? reply bwb 8 hours agoprevIn executing on an idea, the end result is often very different from where you started. They might look similar now, but in 5 years one went enterprise on feature set and the other consumer etc etc. reply blitzar 6 hours agoparentSo true, all the Blockchain startups are now LLM startups. reply thih9 6 hours agorootparentYou mean there is a trend in existing blockchain startups pivoting to LLMs? Is there a source / more details? reply Taylor_OD 1 hour agoprevI dont think the headline is all that surprising. YC makes a lot of investments and most start ups fail. They are hedging their bets. The more surprising thing about this article is that one YC company blatantly copied anothers product. To the point where the founder said they would start over and build their own product. reply dangoodmanUT 5 hours agoprevAnd Garry apologized for being wrong shortly after, but that’s not mentioned? reply T4iga 5 hours agoprevThis is reads as if that is negative an unusual. One might think supporting copycats is bad but obviously a company cannot simply be copied and is more than just its product. While a product might be, you can still outclass your competitors with better engineering, or sourcing of materials or marketing. You wonder if this is a telling story about YC or just one about the startup space in general. Possible outcomes include: 1. There is little unique ideas going around. YC is truely and knowingly funding blatant copycats. 2. There is little unique ideas going around. Due to the large amount of duplicates, all accelerators invariably invest in mostly copycats. 3. There is unique ideas going around, they are just not popular with YC. This could have various reasons. I wonder what they might be reply jjulius 3 hours agoprevPardon my ignorance, but is it wrong for my reaction to this piece to essentially be, \"... okay, and?\". If we're being honest with ourselves, platitudes are just platitudes and the ultimate point of YC is to further enrich investors, and to enrich founders. Of course they're going to try everything they can to make that happen. It's not like YC's #1 mission is, \"Do original, world-changing things\", no, it's, \"We want more money\". reply aabhay 6 hours agoprevOne of the reasons that YC is so great is that they back founders, not ideas. YC has never backed the same founder twice in a batch to my knowledge, and doing so is antithetical to its values. reply jakubmazanec 2 hours agoprevIs this surprising? almost 10 years ago even Silicon Valley (the TV show) made fun of the fact that the main guys' company was only one of many compression startups their investor invested in. reply newsclues 6 hours agoprevInvesting in a market opportunity rather than a company or person. reply dowager_dan99 2 hours agoprev>> The Silicon Valley dream is to build a tech startup that is such a unique idea it alters the commercial universe and turns its founders into billionaires. The headline is a false assumption. While there are still people who are trying to build off unique ideas to change the world, it seems very few of them are motivated by becoming billionaires. The latter have no intention or particular desire to create a sea change, and actively look to (at best) have a \"X for Y\" strategy. Even more are \"Yet anotherX\". reply petesergeant 4 hours agoprevI don’t mean this in a bad way, but like, at this point YC’s business model is spray and pray right? They weed out obvious losers, and then use vibes to pick a section of the rest, push them through a standard accelerator program, and demo day at the end, and this works because having been accepted into YC is like being able to say you went to a fancy university: no guarantees on future performance, but you’re probably not an idiot. They should be (and are — YC Fall Batch!) trying to maximise the number of companies they take on while not diminishing returns below their cost of capital and brand dilution. This is not a boutique investment shop. reply 0points 8 hours agoprevreinforcement learning done right, amirite? reply wslh 6 hours agoprevThat approach is perfectly valid and could even be smarter than other VCs that avoid funding competing projects. I also think that, since YC operates at an earlier stage than major funds like Sequoia, different heuristics could apply. Ultimately, it's about balancing risk and reward—duplicating efforts can increase the chances of success while mitigating risks. Of course, it's not easy to establish processes that help competing companies in the same space without inadvertently favoring one over the other. reply RecycledEle 4 hours agoprev [–] YC should back duplicate startups because each startup's chance of success is low. If it makes sense to back a startup with a 10% chance of being a big company later, then a second such startup also makes sense, even if there is a 1% chance that they both make it and one of them is not needed. The problem is when someone says the quiet part out loud, and the duplicate startups start talking to each other. The people at one startup refuse to do useful work because they incorrectly believe the other startup have a better idea. You want to leave those decisions to senior management, who can move technologies to make one company successful. Have you ever suddenly received an influx of employees because someone jast realized that there was a duplicate company doing the same thing (fake gasp) and that it was time to combine the best parts of both the get a product? Somehow, despite nobody knowing there were duplicate startups, a proof of concepts has been working for a few months combining the best bits from each of 3 (fake gasp) startups, and the third one had very little to contribute so you will not see their people around. I'm sorry if my post hurts anyone's plans. Fell free to delete it. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Y Combinator (YC), a prominent startup accelerator, is known for launching successful companies like Airbnb and Stripe, but many of its startups build similar or identical products to previous YC companies.",
      "The PearAI controversy highlighted this trend, where PearAI was accused of cloning another YC product, yet YC CEO Garry Tan defended the practice, citing the importance of choice and innovation.",
      "Deckmatch's analysis of YC trends shows popular startup categories include AI code editors, restaurant PoS systems, and business finance tools, with some areas like crypto trading seeing decreased interest."
    ],
    "commentSummary": [
      "Y Combinator frequently invests in startups that replicate other YC-backed companies, prioritizing the founders' potential over unique ideas.- This strategy is seen as a way to hedge bets on similar concepts, as success often depends on execution and timing rather than the originality of the idea.- While some view this as a potential conflict of interest, it is a common venture capital practice to fund multiple companies in the same sector to validate markets and enhance the likelihood of success."
    ],
    "points": 240,
    "commentCount": 133,
    "retryCount": 0,
    "time": 1732611000
  },
  {
    "id": 42241851,
    "title": "Fly.io outage – resolved",
    "originLink": "https://status.flyio.net",
    "originBody": "All Systems Operational About This Site This page is for updates about global incidents. It does not include updates about routine hardware failures or isolated infrastructure events that have limited impact. For a personalized view of all events that might affect your apps, please check the personalized status page in your Fly Organization's dashboard. For all internal incidents and other activities, please check Infra Log. Customer Applications Operational Dashboard Operational Machines API Operational Regional Availability Operational AMS - Amsterdam, Netherlands Operational ARN - Stockholm, Sweden Operational ATL - Atlanta, Georgia (US) Operational BOG - Bogotá, Colombia Operational BOM - Mumbai, India Operational CDG - Paris, France Operational DEN - Denver, Colorado (US) Operational DFW - Dallas, Texas (US) Operational EWR - Secaucus, NJ (US) Operational EZE - Ezeiza, Argentina Operational FRA - Frankfurt, Germany Operational GDL - Guadalajara, Mexico Operational GIG - Rio de Janeiro, Brazil Operational GRU - Sao Paulo, Brazil Operational HKG - Hong Kong Operational IAD - Ashburn, Virginia (US) Operational JNB - Johannesburg, South Africa Operational LAX - Los Angeles, California (US) Operational LHR - London, United Kingdom Operational MAD - Madrid, Spain Operational MEL - Melbourne, Australia Operational MIA - Miami, Florida (US) Operational NRT - Tokyo, Japan Operational ORD - Chicago, Illinois (US) Operational OTP - Bucharest, Romania Operational PHX - Phoenix, Arizona (US) Operational QRO - Querétaro, Mexico Operational SCL - Santiago, Chile Operational SEA - Seattle, Washington (US) Operational SIN - Singapore Operational SJC - San Jose, California (US) Operational SYD - Sydney, Australia Operational WAW - Warsaw, Poland Operational YUL - Montréal, Canada Operational YYZ - Toronto, Canada Operational Persistent Storage (Volumes) ? Operational Deployments ? Operational Remote Builds Operational Logs Operational Metrics ? Operational SSL/TLS Certificate Provisioning Operational UDP Anycast ? Operational Fly Machine Image Registry 1 Operational Fly Machine Image Registry 2 Operational Extensions Operational Upstash for Redis Operational DNS Operational Fly Machine .internal DNS ? Operational Fly Machine External DNS Operational *.fly.dev Nameservers Operational *.flyio.net Nameservers Operational Billing Operational Usage Metrics API Operational Stripe API Connection Operational Corrosion ? Operational Operational Degraded Performance Partial Outage Major Outage Maintenance Past Incidents Nov 26, 2024 Degraded Connectivity Resolved - We have determined that some customers' machines are being throttled due to our full rollout of CPU quotas, separate from the incident yesterday. This in turn caused apparent networking issues. We have now temporarily rolled back these changes while we work with customers to better adapt to CPU quotas. Nov 26, 16:11 UTC Investigating - We are aware of customer-reported issues with internal networking and are investigating. Nov 26, 14:30 UTC Degraded API Performance Resolved - This incident has been resolved. Nov 26, 08:15 UTC Update - We've scaled up our systems and applied fixes to our API. Everything should be operational now. Nov 26, 07:52 UTC Update - We are scaling up our systems to handle the increased traffic Nov 26, 05:43 UTC Update - All hosts have completed the restoration process and we are seeing our overall Corrosion cluster health and performance return to normal. Machine API and GraphQL API error rates are improving, but some users may still see elevated rates of request timeouts and/or 504 errors when using the Machines API or Flyctl commands. We are continuing to monitor these services as they recover. Nov 26, 03:42 UTC Monitoring - The restore process has completed on the majority of hosts in our fleet and we are seeing overall Corrosion cluster health and performance return to normal. There are a small number of hosts that are still being worked on, we aim to have them restored shortly. Nov 26, 02:31 UTC Update - We are running a restoration and reseed process to bring the Corrosion cluster back to a healthy, current state. During this restoration process, you may see elevated error rates on machines or apps that have been recently updated. Nov 26, 02:06 UTC Update - The updates have been applied, however we are still not seeing recovery on all Corrosion nodes. We are continuing to work on a fix. The machines API and proxy performance remains in a degraded state, especially with newly created and updated machines. Nov 25, 23:58 UTC Update - The Machines API issues stem from a propagation delay in our global state store, Corrosion. We have completed deploying a configuration change to our Corrosion cluster and will be applying these changes to each node shortly. We expect improvement once the changes are applied. In the meantime users may still see degraded machines API and proxy performance, especially with newly created machines Nov 25, 22:15 UTC Identified - The issue has been identified and a fix is being implemented. Nov 25, 20:20 UTC Investigating - We are investigating degraded API performance Nov 25, 20:10 UTC Nov 25, 2024 Nov 24, 2024 No incidents reported. Nov 23, 2024 No incidents reported. Nov 22, 2024 Log Search unavailable Resolved - This incident has been resolved. Nov 22, 04:29 UTC Monitoring - A fix has been implemented and we are monitoring the results. Nov 21, 21:41 UTC Investigating - We are investigating an issue with application log search. This impacts Fly Metrics log search panels, and historical app logs. Streaming logs using `fly logs`, the Live Logs page in the dashboard, and Fly Log Shipper services continue to work as expected. Nov 21, 15:56 UTC Nov 21, 2024 Nov 20, 2024 Emergency network maintenance in ARN Completed - The scheduled maintenance has been completed. Nov 20, 05:00 UTC In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary. Nov 20, 02:00 UTC Scheduled - Our network provider is performing an emergency switch replacement during this window. An up to one hour network outage is expected during this maintenance window. Please verify your fly apps are deployed to more than one region to avoid impact. Nov 20, 00:30 UTC Nov 19, 2024 Log Search unavailable Resolved - This incident has been resolved. Nov 19, 19:48 UTC Monitoring - A fix has been implemented and we are monitoring the results. Nov 19, 19:04 UTC Investigating - We are investigating an issue causing application log search to be unavailable. This is affecting the Fly Metrics log search panels, and historical application logs initially returned from the `fly logs` command. Streaming logs using `fly logs`, the Live Logs page in the dashboard, and Fly Log Shipper services continue to work as expected. Nov 19, 18:46 UTC Nov 18, 2024 No incidents reported. Nov 17, 2024 No incidents reported. Nov 16, 2024 No incidents reported. Nov 15, 2024 No incidents reported. Nov 14, 2024 Degraded IPv6 connectivity in IAD Resolved - This incident has been resolved. Nov 14, 09:45 UTC Investigating - Our upstream provider has degraded IPv6 connectivity in IAD. We are actively working with them to resolve this. Nov 14, 09:16 UTC Nov 13, 2024 No incidents reported. Nov 12, 2024 QRO network issues Resolved - This incident has been resolved. Nov 12, 23:59 UTC Update - We are working with our upstream provider to fix the network issue. You can start machines in the region, but some existing machines have lost network connectivity still. Nov 12, 20:10 UTC Identified - The issue has been identified and a fix is being implemented. Nov 12, 19:09 UTC Investigating - We are currently investigating this issue. Nov 12, 18:54 UTC ← Incident History Powered by Atlassian Statuspage",
    "commentLink": "https://news.ycombinator.com/item?id=42241851",
    "commentBody": "Fly.io outage – resolved (flyio.net)227 points by punkpeye 17 hours agohidepastfavorite217 comments benhoyt 16 hours agoMy fly.io-hosted website went down for 5 minutes (6 hours ago), but then came right back up, and has been up ever since. I use a free monitoring service that checks it every 5 minutes, so it's possible it missed another short bit of downtime. But fly.io has been pretty reliable overall for me! reply nomilk 15 hours agoparentWould be fascinated to see your data over a period of months. Application up time is flakey, but what was worse were fly deploys failing for no clear reason. Sometimes layers would just hang and eventually fail for no particular reason; I'd run the same command an hour or two later without any changes and it would just work as expected. I'd love to make a monitoring service to deploy a basic app (i.e. run the fly deploy command) every 5 minutes and see how often those deploys fail or hang. I'd guess ~5% inexplicably fail, which is frustrating unless you've got a lot of spare time. reply jrockway 6 hours agorootparentI used to run a service that created k8s clusters on GCP for our customers. We did want to check that that functionality kept working and had a prober test it periodically. It was actually broken a lot. Always good to monitor your dependencies if you have the time. Then when someone complains about an issue in your service, you can check your monitoring to see if your upstream services are broken. If they are, at least you know where to start debugging. reply sanswork 13 hours agorootparentprevMy downtimes from fly are pretty rare but generally global when they happen, in this outage we had no downtime but couldn't deploy for a few hours. I have issues with deploying about once per quarter(deploy most days across a few apps) reply nomilk 13 hours agorootparentIf that’s the case I suspect fly is getting a lot more reliable. I stopped using them about a year ago so haven’t kept up on their reliability since. Glad to hear, it’s good for a competitive market to have many providers, and fly might have issues but hopefully has a bright future reply sanswork 12 hours agorootparentThey are definitely getting more reliable. I was an early user and moved off them to self hosted for quite a while because of the frequent downtime in early days. Their support still leaves a lot to be desired even as someone that pays for it but the ease of running and deploying a distributed front end keeps bringing me back. reply rozenmd 11 hours agorootparentprevThis may be of interest to you: https://news.ycombinator.com/item?id=42243282 reply Joel_Mckay 15 hours agorootparentprevnext [5 more] [flagged] LorenzoGood 15 hours agorootparentWhat does rust have to do with fly.io? reply aobdev 14 hours agorootparentSnark aside, Joel is suggesting that because Fly uses rust-based virtualization software they should have a more reliable deployment process. reply LorenzoGood 14 hours agorootparentThanks for clarifying. reply Joel_Mckay 14 hours agorootparentprevnext [2 more] [flagged] nomilk 14 hours agorootparentBy asking directly and someone answering, it solves the problem for the person wondering, but also anyone else wondering (i.e. asking directly scales very nicely). reply rozenmd 11 hours agoparentprevI externally monitor fly.io and it's docs here: https://flyio.onlineornot.com/ Looks like it lasted 16 minutes for them. reply dprotaso 1 hour agoparentprevWhat free monitoring tool do you use? reply davidgl 9 hours agoparentprevSame for us, down for ~5 mins, back up and fine, error was 501 reply TacticalCoder 3 hours agorootparentSomeone said 16 minutes: so it's not even 5 nines service. reply beezlewax 12 hours agoparentprevDo you mind if I ask what monitoring service that is? reply buzzier 5 hours agorootparenthttps://github.com/louislam/uptime-kuma reply benhoyt 12 hours agorootparentprevSure, it's UptimeRobot: https://uptimerobot.com/ reply andrew-jack 7 hours agorootparentprevUse https://pulsetic.com/ reply vextea 4 hours agorootparentIs it your service? reply jart 14 hours agoprevfly.io publishes their post-mortems here: https://fly.io/infra-log/ The last post-mortem they wrote is very interesting and full of details. Basically back in 2016 the heart or keystone component of fly.io production infrastructure was called consul, which is a highly secure TLS server that tracks shared state and it requires that both the server certificate and the client certificate be authenticated. Since it was centralized, it had scaling issues, so fly.io wrote a replacement for it in 2020 called corrosion, and quickly forgot about consul, but didn't have the heart to kill it. Then in October 2024 consul's root key signing key expires, which brought down all connectivity, and since it uses bidirectional authentication, they couldn't bring it back online until they deployed new SSL certificates to every machine in their fleet. Somehow they did this in half an hour, but the chain of dominoes had already been set in motion to reveal other weaknesses in their infrastructure that they could eliminate. There was this other internal service whose own independent set of TLS keys had also expired long ago, but they didn't notice until they tried rebooting it as part of the consul rekey, since doing so severed the TCP connections it had established way back when its certificate was valid. Plus the whole time this is happening, their logging tools are DDOSing their network provider. It took some real heroes to save the company and all their customers too when that many things explode at once. reply ignoramous 13 hours agoparentOn that Consul outage, Fly Infra concludes, \"The moral of the story is, no more half-measures.\" On their careers page [1], the Fly team goes, \"We're not big believers in tech debt.\" As an outsider, reads like a cacophony of contradictions? [1] https://fly.io/docs/hiring/working/#we-re-ruthless-about-doi... reply jart 13 hours agorootparentNo one actually lives up to their principles, but it's still important that we have them. If you actually do live up to yours, then you need to adopt better principles. reply whilenot-dev 11 hours agorootparentAny principle in itself isn't without critique, agree, but it's still the choice being made to pick this specific principle that tells the whole story. There are so many principles to pick from and the tech dept pick follows up with a \"We have a 3-month “no refactoring” rule for new hires. This isn’t everyone’s preferred work style! We try to be up front about stuff.\", which sounds a bit like an additional perform or else... principle that just delays ownership of the stuff you're supposed to work with. In the best case that sounds like naiive optimism and in the worst case that's gross negligence... neither one speaks \"engineering\" to me. reply Aeolun 10 hours agorootparentprevTwo contradictory statements do not read like a 'cacophony' of anything to me xD I think you need a whole lot more than two to do that word justice. reply JimDabell 5 hours agorootparent“No more half-measures” and “We’re not big believers in tech debt” aren’t even contradictory statements, let alone a cacophony of them. reply mattgreenrocks 4 hours agorootparentprevThe comment section doing what it does best! reply ignoramous 3 hours agorootparentFor brevity I chose to put up only the conclusion from a postmortem (of which I've read plenty by now) and another point from their otherwise comparatively shorter careers page, which imo capture the inherent tension between building out fast & building out right. This is not something I've started complaining about today or yesterday. I've used Fly in prod for 4 years and spilled much ink on this topic on their forums already. Even if I critique, I remain optimistic about Fly despite the seemingly endless list of failure modes building such complex systems entail: https://community.fly.io/t/fly-down/10224/15 (personally speaking, I'm humble enough because I can hardly build a toy side-project right!) reply bdcravens 4 hours agorootparentprev\"full measures\" aren't the same thing as tech debt. Complexity isn't even the same thing as tech debt. reply cryptos 10 hours agoprevFly.io seems to be a bit of a mixed bag: https://news.ycombinator.com/item?id=41917436 https://news.ycombinator.com/item?id=35044516 https://news.ycombinator.com/item?id=34742946 https://news.ycombinator.com/item?id=34229751 If a cloud platform doesn't really provide reliability, I'd say it's probably not worth it. You could better just rent a (virtual) server and save the cloud tax. reply ARCarr 12 minutes agoparentI tried out Fly.io and deployed a little test app. I couldn't even access the app, because they put it onto a server that was under \"emergency maintenance\" and had been that way for twelve days. reply huijzer 9 hours agoparentprevFor experiments and hobby projects the value proposition is amazing. Where else can you spin up an independent instance for $1.94 per month?* *Note this is for an instance with only 256MB RAM (https://fly.io/docs/about/pricing/), but it's definitely possible to run non-trivial projects on that. Rust-based web servers like Rocket require only about 10MB RAM. Basic PHP servers should also fit from what I can find. reply oefrha 8 hours agorootparentThere are plenty of better deals as long as you don’t limit yourself to big clouds and clouds with startup-esque landing pages frequently posted to HN. LowEndTalk may be the most well-known place for finding such deals. (Not saying the typical cheap VPS on LowEndTalk has comparable PaaS features. Only responding to parent’s use case of a single cheap instance.) reply pajeetz 44 minutes agorootparentprevi recommend lowendtalk what fly.io doing is running colocated baremetal servers and using firecracker to overcommit (probably via memory ballooning and other disk compression on demand) if you are going to haggle over $2/month then you are better off just connecting your raspberry pi with wireguard/cloudflare tunnel on a residential connection reply throwaway63467 6 hours agorootparentprevBest business model in the world, buy stuff in big bags, put it in smaller ones, sell at a multiple of the original price. Fly is mostly (to my knowledge) reselling Netactuate and OVH servers, their main innovation is the developer experience on top, using Docker on a MicroVM based approach. Of course not only that, but I think it’s their main differentiator. Haven’t used that in a while but Scaleway offered ridiculously cheap dedicated ARM hardware close to these price points, not sure if they still do. reply input_sh 8 hours agorootparentprevNowhere? Because that's a ridiculously low amount of RAM to offer even in your cheapest offerings? You can easily get 4 GB of RAM for $5 from the likes of Hetzner or Hostinger, so that's 16x more RAM for 2.5x the price. One relatively unknown provider I have used in the past offers 2 GB of RAM for €3.6/month (if paid monthly, €3 if anually), so 8x more RAM for 1.5-2x the price. I'm sure I could find something even cheaper, but I'm just looking at providers I have personally used. BTW that dropdown seems to be sorted cheapest > most expensive. If you go to the bottom of the list the price for that same VPS doubles. reply KomoD 7 hours agorootparent> Nowhere? Because that's a ridiculously low amount of RAM to offer even in your cheapest offerings? There's definitely places that offer it... also 512m I know because I've personally bought such plans and that was $5-10/yr because I didn't need dedicated ipv4. reply pc86 4 hours agorootparentprevMaybe if you're limiting yourself to AWS-wrapper cloud companies. What good is a $2/mo cloud instance if it's down multiple times a month? Just get a $5/mo VPS instead if you're really concerned about a few dollars a month. reply hansvm 4 hours agorootparentprevI used to use Racknerd for that sort of thing, and the costs were around there -- maybe $1.90/mo for a 512MB instance. It was easy to squeeze several hobby projects onto the machine. reply hobo_mark 8 hours agorootparentprevOne such microVM per month used to be within the free monthly allowance, is that not the case anymore? reply kelvinjps10 7 hours agorootparentprevI'm getting 1$ for a 2gb ram vps in ovh for the first year reply belter 8 hours agorootparentprevSounds like a Lambda function.... reply TiredOfLife 7 hours agorootparentprevOracle free is one 4 core 24gb ram vps + 2 dualcore amd vps. reply treesknees 4 hours agorootparentAnd actually, it's the resources that are free (CPU, memory, network) and you're allowed to split them up into multiple VMs if you want to. One of my VMs had an uptime of more than 1050 days before the infrastructure rebooted it, so in terms of availability they've certainly surprised me. The only downside I've come across with Oracle Free is that the 'best' regions are typically full. I ended up provisioning my free VMs in another region/country and it works fine. I suppose another downside (if you want to view it this way) is they will delete idle unused free VMs after a certain time period. You have to add a credit card to your account to \"upgrade\" your account and run free resource indefinitely. While you're not charged for anything, it makes me nervous forking over a CC number to Oracle. reply zackify 5 hours agoparentprevThe reliability is very very bad. It was really insane that 2 times in the past few months the main dashboard was down as I’m demoing something. Not to mention the deploy outages and almost daily some random thing was unavailable or delayed. I had to leave a few months ago after the price raises and how many times my boss saw some issue in the project I had with them. They also deprecated and removed their sqlite backup service. Back to GCP and not worrying about so many outages now. reply pajeetz 41 minutes agorootparenttheres just so many anecdotes/nightmare stories from people using fly.io here much more than the ones linked by GP expect to see more of these \"post-mortem apologies\" from fly.io in the future because it won't be the last reply pc86 4 hours agorootparentprevNow just to worry about GCP getting shut down with a few days' notice. /s But in all seriousness the gall to raise prices before actually fixing the reliability problems is pretty shocking. I understand it's a bit of a chicken-and-egg thing where you maybe are tight on resources but there's no scenario where it's acceptable to have a product with these kinds of problems and then raise prices on existing customers who are putting up with it. reply encom 4 hours agorootparentNo /s is needed. Relying on any Google product long term is crazy. reply sofixa 2 hours agorootparentGoogle's b2b products are relatively stable (relative to their b2c free services). You generally get somewhere like a year of notice if they shut it down. reply qeternity 9 hours agoparentprevI don't really understand the value prop of fly.io. They seem to have an impressive engineering team despite the outages, but is edge compute really something that 99.9% of devs need? There are tons of large companies that operate out of a single AWS region and those services are used by millions around the globe. It just strikes me as something that enables premature optimization right out of the box. reply k__ 9 hours agorootparentIt's basically the new Heroku with less lock-in, because it works with Docker. You get edge computing, autoscaling, and load balancing without additional configuration. Not as flexible as AWS, but also much easier to setup and maintain. But the reliability issues suck now and then. reply gurgunday 6 hours agorootparentDigitalOcean has been doing this for years, and their value proposition is unmatched IMO For $5 you get: Latest gen CPUs and RAM HTTPS DDoS protection Cloudflare CDN Autoscale Competent support I'd say the best part is the predictable monthly prices And while most people probably don't care, they are an established public company, so there is more chance they will exist in 10 years reply dijksterhuis 5 hours agorootparentare global r/w token permissions still a thing, or did the token scopes thing finally come out of beta? also, my experience with support was not the same as yours. they were utterly useless for the most part. for a personal web dev (or similar) project, like, i agree, they’ve got good value. but having worked in a small biz where DO was what they built everything on — no. bad idea. spend more. use aws (graviton ec2 instances)/azure. reply fragmede 5 hours agorootparentprevthe $5 droplet is underpowered and can't run anything substantial. it's just the price to get you in the door. reply pajeetz 40 minutes agorootparentyou wouldn't be able to run anything substantial with that kind of budget but GO and pocketbase is on record for supporting 10k concurrent requests per second on low powered VPS reply yabones 5 hours agorootparentprevIt doesn't really need to run anything \"substantial\" though. Running some janky wordpress site with some scabbed-on ecommerce customizations is like 50% of the internet. reply infecto 5 hours agorootparentpreva 1vCPU 512mb instance is plenty for most base cases. Maybe you need one additional machine to act as a background worker. I am sure there are some noisy neighbors but to say its underpowered is silly. reply fragmede 4 hours agorootparentI'm calling it underpowered because the $5 one had trouble running my custom ssh daemon. ssh! the cryptography for that shouldn't chug down the server I'm renting from them. a bigger instance from them isn't having the same problems. reply ignoramous 6 hours agorootparentprev> Not as flexible as AWS Today, Fly.io is more or less in the same market as Lightsail, not AWS. And when you compare it to Lightsail, it blows it away. reply watermelon0 5 hours agorootparentDid you count reliability into your assesment here? I'm reading about Fly.io outages multiple times a year, whereas Lightsail seem to be as stable as AWS EC2. reply mtlynch 6 hours agorootparentprevAnd when you compare it to Lightsail, it blows it away. This is a bit of a confusing sentence because there are so many pronouns. Do all of the \"it\"s refer to Fly.io? reply dijksterhuis 6 hours agorootparent> And when you compare [fly.io] to Lightsail, [fly.io] blows [Lightsail] away. reply nikodotio 7 hours agorootparentprevThis is precisely it. The ease of deploy, https domain configuration, scaling. Additionally, having machines that turn off when not in use is easy to configure, which I never managed on AWS. reply ignoramous 6 hours agorootparent> which I never managed on AWS I haven't looked at it recently, but App Runner could do a few of Fly.io esque things (but slightly more expensive): https://aws.amazon.com/apprunner/ reply infecto 5 hours agorootparentprevI have asked this multiple times but is anyone really using edge compute and getting value out of it? I am certain there are cases but I have not seen any of them written up before. reply sofixa 4 minutes agorootparentDepends on what you mean by edge compute, but you probably are. 5G towers are a ton of compute on the edge to secure and protect the traffic passing through them. Or if by edge you mean having stuff close to your consumers, every non trivial operation does that. reply pier25 4 hours agorootparentprevWe have an embeddable audio player served globally with very low latency. This wouldn't be possible without edge compute/data. reply infecto 4 hours agorootparentprevI am going to go out on a limb and say there is no real value prop to fly.io. I could completely be wrong but it always feels like the modern MongoDB. Everyone wants to use it but I am not sure they are extracting value from it and instead its a shiny toy that is fun to build from. reply austinpena 5 hours agorootparentprevI have an SSR Astro project. Using Fly makes my project fast. For dynamic data I use SWR. I could use Cloudflare workers but it doesn’t play so nice with Astro. I also have a “form submission service” where I receive a Post and send an email. I need maximum uptime to avoid revenue loss. It’s a go service so I deploy ~6 machines across the US to ensure I don’t drop any requests. I haven’t had downtime in years. reply victorbjorklund 8 hours agorootparentprevIf half your customers are in new your and half in sidney it makes you app faster if you run it in both places. There is a lot of things we do for our users that we don't need (no one \"needs\" SPA etc). But if it is easy to make your app faster for your users, why not? reply victorbjorklund 8 hours agorootparentAnd it is easier than AWS to deploy. reply jrockway 6 hours agorootparentprevI would take edge compute if it's free and easy. That's fly.io's value prop. In a world where much web browsing starts with ACK SYN ACK, it is nice if the server is close to you. reply brainzap 6 hours agorootparentprevI typed fly launch, fly deploy and my node.js project was deployed. So I guess hobby projects? reply pajeetz 46 minutes agoparentprevfly.io has a very bad reputation for reliability there doesn't seem to be any damage control beyond hackernews and even here the consensus seems to be \"dont run anything mission critical on fly.io or expect data redundancy\" in fact, you can almost get the same thing fly.io does by running firecracker on your own bare metal servers and cheaper too. I'm afraid the public sentiment towards fly.io has been tainted for good (I can't count how many times they apologized now). reply akoculu 8 hours agoparentprevAlso: https://news.ycombinator.com/item?id=36808296 reply punkpeye 11 hours agoprevContrary to the title of the post, Fly.io API remains inaccessible. Meaning, users still cannot access deploys/databases, etc. For accurate updates, follow https://community.fly.io/t/fly-io-site-is-currently-inaccess... reply neya 12 hours agoprevPersonal experience between Fly.io and Railway.com - Railway wins for me hands down. I have used both and the Railways support is stellar too, in comparison. Fly.io never responded to my query about data deletion till date. Despite emailing on their support email. I have had my Railway app online till date without any major downtimes too. I recommend anyone looking for a decent replacement to try them. reply ignoramous 6 hours agoparentFly builds on their own hardware. Is Railway doing the same? If not, that'd explain some of why Railway has relatively less number of outages (they're engineering fewer things). I understand that end-users want reliability (and Fly gets a bad rep despite pretty significant investment on this front in the past 2 years), but such outages aren't exclusive to one provider & not the other. Building cloud infra is no one's definition of easy. reply andai 11 hours agoparentprevI've used Railway control panel maybe a total of 10 times in my life and half the time it was having weird issues. Control panel UI not loading or not working, actions failing, deploys randomly failing... I love the idea but in practice it's not something I'd want to use for anything serious. reply punkpeye 11 hours agoparentprevHow does it compare in terms of price? reply shubhamjain 16 hours agoprevThis is probably 5th or 6th major outage from Fly.io that I have personally seen. Pretty sure there were many others and some just went unnoticed. I recommended the service to a friend, and within two days he faced two outages. Fly.io seriously needs to get it together. Why it hasn’t happened yet is a mystery to me. They have a good product but stability needs to be an absolute top for a hosting service. Everything else is secondary. reply SOLAR_FIELDS 15 hours agoparentI get this but I think if people can give GitHub a pass for shitting the bed every two weeks maybe Fly should get a bit of goodwill here. I am not affiliated with Fly at all but I do think that people should temper their expectations when even mega corp can’t get it right I guess the secret is to be the incumbent with no suitable replacement. Then you can be complete garbage in terms of reliability and everyone will just hand wave away your poor ops story reply ojame 15 hours agorootparentThe biggest difference is GitHub in your infrastructure is (nearly always) internal. Fly in your infrastructure is external. Users generally don't see when you have issues with GitHub, but they do generally see when you have issues with Fly. That's the core difference. reply fragmede 15 hours agorootparentprevWho's giving GitHub a pass on shitting the bed? They go down often enough that if you don't have an internal git server setup for your CICD to hit, that's on you. reply SOLAR_FIELDS 14 hours agorootparentMy point is made by your very post - getting off GitHub onto alternatives is not seriously discussed as an option - instead it’s “well, why didn’t you prepare better to deal with your vendor’s poor ops story” reply fragmede 14 hours agorootparentI wasn't going to bring up being on an internally hosted gitlab instead of github, but that would be the \"not giving them a pass\" part. reply adityapatadia 16 hours agoparentprevWe left it about a year ago due to reliability issues. We now use digitalocean apps and working like a charm. Zero downtime with DO. reply subarctic 12 hours agorootparentYou mean their App Platform right? How does the pricing compare to fly? reply adityapatadia 12 hours agorootparentYes, App Platform. Pricing is a little higher but way lower than AWS but it is fully justified. Zero downtime in the last 1 year. With Fly, we had 3-4 downtimes in 2023 in a span of 4 months. reply mcqueenjordan 16 hours agoparentprevReliability is hard when your volume is (presumably) scaling geometrically. reply paxys 16 hours agorootparentCan't use the \"reliability is hard\" excuse when you are quite literally in the business of selling reliability. reply mcqueenjordan 15 hours agorootparentIt’s just not that big of a mystery. It’s not an excuse; it’s just true. Also, they’re not especially selling reliability as much as they’re selling small geo-distributed deployments. reply ilrwbwrkhv 16 hours agoparentprevDoes anyone use them beyond the free tier? Same with Vercel for example. reply gk1 16 hours agorootparentVercel has revenue of over $100M. So yes at least a few companies use them beyond the free tier. reply dizhn 13 hours agorootparentprevWhich company? GitHub? As far as I know fly.io does not have a free tier. reply HellsMaddy 16 hours agoprevSuspiciously, Turso started having issues around the same time. Their CEO confirmed on Discord it's due to the Fly outage: > Ok.I caught up with our oncall and This seems related to the Fly.io incident that is reported in our status page. Our login does call things in the Fly.io API > we are already in touch with Fly and will see if we can speed this up reply pier25 14 hours agoparentNot the first time Turso goes down because of Fly issues. It must suck to have built a db service and have this downtime. Apparently Turso are going to offer an AWS tier at some point. reply jonasdoesthings 12 hours agorootparentLast month Turso released AWS-hosted databases to the public (still in Beta): https://turso.tech/blog/turso-aws-beta reply pier25 5 hours agorootparentThanks! reply marvin-hansen 15 hours agoprevNo surprise. About a year ago, I looked at fly.io because of it's low pricing and I was wondering where they were cutting corners to still make some money. Ultimately, I found the answer in their tech docs where it was spelled out clearly that an fly instance is hardwired to one physical server and thus cannot fail over in case that server dies. Not sure if that part still is in the official documentation. In practice, that means if a server goes down, they have to load the last snapshot from that instance from the Backup and push it on a new server, update the network path, and pray to god that not more server fail than spare capacity is available. Otherwise you have to wait for a restore until the datacenter mounted a few more boxes in the rack. That explains quite a bit the randomness of those outage reports i.e. my app is down vs the other is fine and mine came back in 5 minutes vs the other took forever. As a business on a budget, I think anything else i.e. a small civo cluster serves you better. reply ignoramous 15 hours agoparentFly.io can migrate vm+volume now: https://fly.io/docs/reference/machine-migration/ / https://archive.md/rAK0V > a fly instance is hardwired to one physical server and thus cannot fail over I'm having trouble understanding how else this is supposed to be? I understand that live migration is a thing, but even in those cases, a VM is \"hardwired\" to some physical server, no? reply mzi 12 hours agorootparent> I'm having trouble understanding how else this is supposed to be? I understand that live migration is a thing, but even in those cases, a VM is \"hardwired\" to some physical server, no? You can run your workload (in this case a VM) on top of a scheduler, so if one node goes down the workload is just spun up on another available node. You will have downtime, but it will be limited. reply ignoramous 7 hours agorootparent> so if one goes down ... just spun up on another On Fly, one can absolutely set this up. Multiple ways: https://fly.io/docs/apps/app-availability / https://archive.md/SJ32K reply dilyevsky 15 hours agoparentprev> Ultimately, I found the answer in their tech docs where it was spelled out clearly that an fly instance is hardwired to one physical server and thus cannot fail over in case that server dies. Majority of EC2 instance types did not have live migration until very recently. Some probably still don't (they don't really spell out how and when it's supposed to work). It is also not free - there's a noticeable brown-out when your VM gets migrated on GCP for example. reply ixaxaar 14 hours agorootparentCan you shed some more light on this \"browning out\" phenomenon? reply toast0 14 hours agorootparentHere's the GCP doc [1]. Other live migration products are similar. Generally, you have worse performance while in the preparing to move state, an actual pause, then worse performance as the move finishes up. Depending on the networking setup, some inbound packets may be lost or delayed. [1] https://cloud.google.com/compute/docs/instances/live-migrati... reply pier25 14 hours agoparentprevIf you want HA on Fly you need to deploy an app to multiple regions (multiple machines). Fly might still go down completely if their proxy layer fails but it's much less common. reply sb8244 4 hours agorootparentThe proxy layer was the cause of yesterday's outage according to support. reply pier25 4 hours agorootparentYes but the previous comment was about hardware failure. reply fulafel 15 hours agoparentprevThe status tells a story about a high-availability/clustering system failure so I think in this case the problem is rather the complexity of the HA machinery hurting the system's availability vs something like a simple VPS. reply xyst 14 hours agoprevRecurring pattern I notice is outages tend to occur the week of major holidays in US. - MS 365/Teams/Exchange had a blip in the morning - Fly.io with complete outage - then a handful of sites and services impacted due to those outages Usually advocate against “change freezes” but I think a change freeze around major holidays makes sense. Give all teams a recharge/pause/whatever. Don’t put too much pressure on the B-squads that were unfortunate to draw the short stick. reply paxys 13 hours agoparentBad code rarely causes outages at this scale. The culprit is always configuration changes. Sure you can try and reduce those as well during the holiday season, but what if a certificate has to be renewed? What if a critical security patch needs to be applied? What if a set of servers need to be reprovisioned? What if a hard disk is running out of space? You cannot plan your way out of operational challenges, regardless of what time of year it is. reply oarsinsync 12 hours agorootparent> Sure you can try and reduce those as well during the holiday season, but what if a certificate has to be renewed? What if a critical security patch needs to be applied? What if a set of servers need to be reprovisioned? What if a hard disk is running out of space? Reading this, I see two routine operational issues, one security issue and one hardware issue. You can’t plan you way around security issues or hardware failures, but operational issues you both can and should plan around. Holiday schedules like this are fixed points in time, so there’s absolutely no reason why you can’t plan all routine works to be completed either a week in advance, or a week after, the holiday period. Certificates don’t need to be near the point of expiry to be renewed. Capacity doesn’t need to be at critical levels to be expanded. Ultimately, this is a risk management question (as a sibling has also commented). Is the organisation willing to take on increased risk in exchange for deferring operational expenses? If the operational expense is inevitable (the certificate will need renewing), that seems like an easy answer when it comes to risk management over holidays. If the operational expense is not inevitable (will we really need to expand capacity?), it then becomes a game of probabilities and financials - likelihood of expense being incurred, amount of expense incurred if done ahead of time, impact to business if something goes wrong during a holiday. reply jimmyl02 12 hours agorootparentprevI think a good way of looking at it is risk. Is the change (whether it is code or configuration, etc.) worth the risk it brings on. For example if it's a small feature then it probably makes sense to wait and keep things stable. But, if it's something that itself causes larger imminent danger like security patches / hard disk space constraints, then it's worth taking on the risk of change to mitigate the risk of not doing it. At the end of the day no system is perfect and it ends up being judgement calls but I think viewing it as a risk tradeoff is helpful to understand. reply bobsyourbuncle 13 hours agorootparentprevThis is a good observation. Do you have any resources I can read up on to make this safer? reply ploxiln 14 hours agoparentprevI think you can't avoid the fact that these holiday weeks are different from regular weeks. If you \"change freeze\" then you also freeze out the little fixes and perf tuning that usually happens across these systems, because they're not \"critical\". And then inevitably it turns out that there's a special marketing/product push, with special pricing logic that needs new code, and new UI widgets, causing a huge traffic/load surge, and it needs to go out NOW during the freeze, and this is revenue, so it is critical to the business leaders. Most of eng, and all of infra, didn't know about it, because the product team was cramming until the last minute, and it was kinda secret. So it turns out you can freeze the high-quality little fixes, but you can't really freeze the flaky brand-new features ... It's just a struggle, and I still advise to forget the freeze, and try to be reasonable and not rush things (before, during, or after the freeze). reply willsmith72 3 hours agorootparentAny big tech company with large peak periods disagrees with you. It's absolutely worth freezing non-critical changes. Urgent business change needs to go through? Sure, be prepared to defend to a vp/exec why it needs to go in now. Urgent security fix? Yep same vp will approve it. It's a no-brainer to stop your typical changes which aren't needed for a couple of weeks. By the way, it doesn't mean your whole pipeline needs to stop. You can still have stuff ready to go to prod or pre prod after the freeze reply ignoramous 14 hours agorootparentprevSome shops conduct game days as the freeze approaches. https://wa.aws.amazon.com/wellarchitected/2020-07-02T19-33-2... / https://archive.md/uaJlR reply vrosas 14 hours agoparentprevThen you just get devs rushing out changes before the freeze… reply subarctic 13 hours agorootparentAs a developer I don't see why I would rush out a change before the freeze when I could just wait until after. Maybe a stakeholder that really wants it would press for it to get out but personally I'd rather wait until after so I'm not fixing a bug during my holiday. reply vrosas 13 hours agorootparentCongrats on not working for the product team I work for reply fragmede 13 hours agorootparentprevand stampeding changes in after the thaw, also leading to downtime. so it depends on the org, but doing a freeze is still reasonable policy. Downtime on December 15th is less expensive than on black Friday or cyber Monday for most retailers, so it's just a business decision at that point. reply cess11 12 hours agoparentprevBlip? 365 has an ongoing incident since yesterday morning, european timezone. The reason I know is because I use their compliance tools to secure information in a rather large bankruptcy. reply aaomidi 14 hours agoparentprevWhat do \"Freezes\" mean? Like, do you stop renewing your certificates? Do you stop taking in security updates for your software? Sure maybe \"unnecessary\" changes, but the line gets very gray very fast. reply vrosas 14 hours agorootparentNo unnecessary code deployments. reply Spivak 13 hours agorootparentprevIt's not very grey, prod becomes as if you told everyone but your ops team to go home and then sent your ops team on a cruise with pagers. If it's not important enough to merit interrupting their vacation you don't do it. reply fragmede 13 hours agorootparentprevCerts shouldn't still be done by hand that this point; if another heartbleed comes out in the next 7 days then the risk can be examined, escalated, and the CISO can overrule the freeze. If it's a patch for remote root via Bluetooth drivers on a server that has no Bluetooth hardware, it's gonna wait. you're right that there's a grey line, but crossing that line involves waking up several people and the on call person makes a judgement call. if it's not important enough to wake up several people over, then things stay frozen. reply kbolino 4 hours agorootparentThere's still a lot of situations where automatic certificate enrollment and renewal is not possible. TLS is not the only use of X.509 certificates, and even then, public facing HTTPS is not the only use of TLS. It needs to get better but it's not there yet. reply aaomidi 11 hours agorootparentprevRight, that's basically what I mean. There are a lot of automated changes happening in the background for services. I guess the whole thing I'm saying is that not every breakage is happening because of a code change. reply akshayshah 15 hours agoprevThe series of outages early in 2023 also had some Corrosion-related pain: https://community.fly.io/t/reliability-its-not-great/11253 reply __turbobrew__ 14 hours agoparentSeems like rolling their own datastore turned out to be a bad bet. Im not super familiar with their constraints but scylladb can do eventual consistency and is generally quite flexible. CouchDB is also an option for multi-leader replication. reply arusahni 16 hours agoprevOof, hugops to the team. reply stevefan1999 16 hours agoprevYep...can confirm my self hosted Bitwarden there is completely FUBAR connection wise even if it is in EA, so it should be a worldwide outage...lemme guess, some internal tooling error, consensus split brain, or if it looks like someone leaked BGP routes again? reply odo1242 14 hours agoparentIt was a consensus split-brain (“database replication failure”) it seems reply satoru42 15 hours agoparentprevMine is in Asia and it's still accessible. reply jasonjayr 16 hours agoparentprevDNS. It's always DNS. /s reply jart 16 hours agorootparenthttps://github.com/jart/cosmopolitan/blob/master/third_party... reply monkaiju 16 hours agorootparentprevMight be! Shameless plug of a DNS tool i wrote years ago for anyone this pushes to learn more about DNS https://dug.unfrl.com/ reply redslazer 16 hours agoprevfly.io just has the weirdest outages. It has issues so regularly we dont even need to run mock outages to make sure our system fail overs work. reply duxup 16 hours agoparentWhen I worked for a company who worked with big banks / financial institutions we used to run disaster recovery tests. Effectively a simulated outage where the company would try to run off their backup sites. They ran everything from those sites, it was impressive. Once in a while we'd have a real outage that matched the test we ran as recently as the weekend before. I was helping a bank switch over to the DR site(s) one day during such a real outage and I left my mic open when someone asked me what the commotion was on the upper floors of our HQ. I said \"super happy fun surprise disaster recovery test for company X\". VP of BIG bank was on the line monitoring and laughed \"I'm using that one on the executive call in 15, thanks!\" Supposedly it got picked up at the bank internally after the VP made the joke and was an unofficial code for such an outage for a long time. reply latch 14 hours agorootparentIn most BIG banks, \"Vice President\" is almost an entry-level title. Easily have 1000s of them. For example, this article points out that Goldman Sachs had ~12K VPs out of more than 30K employees: https://web.archive.org/web/20150311012855/https://www.wsj.c... reply SteveNuts 4 hours agorootparentJust like all Sales folks have heavily inflated titles, no customer wants to think they're dealing with a junior salesperson/loan officer when you're about to hand over your money. It seems like every vendor sales team I work with is an \"executive\" or \"director of sales\" even though in reality they're just regular old salespeople. reply jart 12 hours agorootparentprevVP at Goldman is equivalent to Senior SWE according to levels.fyi and their entry level is Analyst. I'm surprised by the compensation though. I would have thought people working at a place with gold in the name would be making more. Also apparently Morgan Stanley pays their VPs $67k/year. reply philipwhiuk 7 hours agorootparentTech outstripped big finance corps tech a while ago. Traders make loads, not the SWEs reply bormaj 6 hours agorootparentprevThat VP comp number seems quite low fwiw reply jart 28 minutes agorootparentYes how much longer till we see Morgan Stanley VPs picketing outside demanding a living wage and humming The Internationale. reply NetOpWibby 15 hours agorootparentprevThankfully your comment was positive! reply benreesman 16 hours agoparentprevIn fairness to the fly.io folks (who are extremely serious hackers), they’re standing up a whole cloud provider and they’ve priced it attractively and they’re much customer-friendlier than most alternatives. I don’t envy the difficulty of doing this, but I’m quite confident they’ll iron the bugs out. reply redslazer 15 hours agorootparentThe tech is impressive and the pricing is attractive which is why we use them. I just wish there was less black magic. reply benreesman 14 hours agorootparentI don’t always agree with @tptacek on social/political issues, and I don’t always agree with @xe on the direction of Nix, but these are legends on the technical side of things. And they’re trying to build an equitable relationship between the user of cloud services and the provider, not fund a private space program. If I were in the market for cloud services I’d highly prize a long-term relationship on mutual benefit and fair dealings over a short-term nuisance of being an early adopter. I strongly suspect your investment in fly is going to pay off. reply xena 13 hours agorootparentXe here. As a sibling comment said, I didn't survive layoffs. If you're looking for someone like me, I'm on the market! reply benreesman 13 hours agorootparentHiring people is above my pay grade, but I can vouch to my lords and masters and anyone else who cares what I think that a legend is up for grabs. b7r6@b7r6.net reply xena 13 hours agorootparentI'd email but I'm about to pass out in bed. Please see https://xeiaso.net/contact/ in case I don't get back to you in the morning. reply verelo 13 hours agorootparentprevI want to believe, but in the meantime they’re killing the product I’ve been working hard to build trust with my own customers though. There is a limit to my idealism, and it’s well and truly in the past. reply reissbaker 13 hours agorootparentprevFWIW Xe was let go from Fly earlier this year during a round of layoffs. reply benreesman 13 hours agorootparentUnfortunate. Xe rocks. reply foldr 8 hours agorootparentprevI suspect that making a cloud service provider run reliably requires tons of grunt work more than it requires technical heroism from a small number of highly talented individuals. reply punkpeye 17 hours agoprevIt is not reflected in their status page, but fly.io itself is not even loading. reply nomilk 15 hours agoparenthttps://fly.io/ loading for me reply duxup 16 hours agoparentprevConfirmation ;) reply mattbee 7 hours agoprevIt feels like fly is trying to repeat a growth model that worked 20 years ago: throw interesting toys at engineers, then wait for engineers to recommend their services as they move on in their careers. Part of that playbook is the old Move Fast & Break Things. That can still be the right call for young projects, but it has two big problems: 1) AWS successfully moved themselves into the position of \"safe\" hosting choice, so it's much rarer for engineers to have influence on something that's seen by money men as a humdrum, solved problem; 2) engineers are not the internal influencers they used to be, being laid off left and right the last few years, and without time for hobby projects. (maybe also 3) it's much harder to build a useful free tier on a hosting service, which used to be a necessary marketing expense to reach those engineers). So idk, I feel like the bar is just higher for hosting stability than it used to be, and novelty is a much harder sell, even here. Or rather: if you're going to brag about reinventing so many wheels, they need to not to come off the cart as often. reply Huppie 1 hour agoprevIt's interesting to see this discussion about fly.io's reliability on a day that (after over three days of downtime) Microsoft Azure finally decided the update of Azure Static Web Apps they deployed last Friday is indeed broken for customers using specific authentication settings... ...with not a single status update from Microsoft in sight. reply xyst 14 hours agoprevI can’t even login to my old account. Password reset is timing out yet still receive password reset e-mail. Password reset link broken, with 500 status code. reply teaearlgraycold 15 hours agoprevI'm grateful to HN for keeping me well aware of Fly's issues. I'll never use them. reply kachapopopow 15 hours agoparentIt's still 99.99+% SLA? Would you really pay 100% more forIt's still 99.99+% SLA But this is simply not accurate. 99.99% uptime isto the Fly team as they work to resolve this (and hopefully get some rest). reply fulafel 15 hours agorootparent99.99+% SLA typically means you get some billing credits for the downtime exceeding 99.99+ availability. So technically do get a \"99.99+% SLA\", but you don't get 99.99+% availability. Other circles use \"SLO\" (where the O stands for objective). (Anyone know what the details in fly.io SLA are?) reply runako 14 hours agorootparentYou are correct in the legal/technical sense! Technically, anyone could offer five- or six-nines and just depend on most customers not to claim the credits :-D Actually hitting/exceeding four nines is still tough. reply cj 15 hours agorootparentprevI think what a lot of people fail to understand is that there are certain categories of apps that simply “can never go down” Examples include basically any PaaS, IaaS, or any company that provides a mission-critical service to another company (B2B SaaS). If you run a basic B2C CRUD app, maybe it’s not a big deal if you service goes down for 5 minutes. Unfortunately there are quite a few categories of companies where downtime simply isn’t tolerated by customers. (I operate a company with a “zero downtime” expectation from customers - it’s no joke, and I would never use any infrastructure abstraction layer other than AWS, GCP or Azure - preferably AWS us-east-1 because, well, if you know the joke…) reply toast0 14 hours agorootparent> I think what a lot of people fail to understand is that there are certain categories of apps that simply “can never go down” I refuse to believe that this category still exists, when I need to keep my county's alternate number for 911 in my address book, because CenturyLink had a 6 hour outage in 2014 and a two day outage in 2018. If the phone company can't manage to keep 911 running anymore, I'd be very surprised what does have zero downtime over a ten year period. Personally, nine nines is too hard, so I shoot for eight eights. reply bri3d 13 hours agorootparentprevMy experience with very large scale B2B SaaS and PaaS has been that customers like to get money, if allowed by contract, by complaining about outages, but that overall, B2B SaaS is actually very forgiving. Most B2B SaaS solutions have very long sales cycles and a high total cost to implement, so there is a lot of inertia to switching that “a few annoying hours of downtime a year” isn’t going to cover. Also, the metric that will drive churn isn’t actually zero downtime, it’s “nearest competitor’s downtime,” which is usually a very different number. reply macNchz 15 hours agorootparentprevEvery PaaS and IaaS I’ve ever used has had some amount of downtime, often considerably more than 5 minutes, and I’ve run production services on many of them. Plenty of random issues on major cloud providers as well. Certainly plenty of situations with dozens of Twitter posts happening but never any acknowledgement on the AWS status page. Nothing’s perfect. reply cj 14 hours agorootparentYea, when running services where 5 minutes of downtime results in lots of support tickets, you learn to accept that the incident will happen and learn to manage the incident rather than relying that it will never occur. reply MobiusHorizons 14 hours agorootparentprevyou realize all of those services you mention can't give you zero downtime, they would never even advertise that. They have quite good reliability certainly, but on long enough time horizons absolutely no-one has zero downtime. reply sgrove 11 hours agorootparentprevAll of your examples have had multiple cases of going down, some for multiple days (2011 AWS was the first really long one I think) - or potentially worse, just deleting all customer data permanently and irretrievably. Meaning empirically, downtime seems to be tolerated by their customers up to some point? reply littlestymaar 14 hours agorootparentprevIf your app cannot go down ever, then you cannot use a cloud provider either (because even AWS and Azure do fail sometime, just look up for “Azur down” on HN). But the truth is everybody can afford some level of outage, simply because nobody has the budget to provision an infra that can never fail. reply vrosas 14 hours agorootparentI’ve seen a team try and be truly “multi-cloud” but then ended up with this Frankenstein architecture where instead of being able to weather one cloud going down, their app would die if _any_ cloud had an issue. It was also surprisingly hard to convince people it doesn’t matter how many globally distributed clusters you have if all your data is in us-east. reply mrcwinn 15 hours agorootparentprevThis is not my experience at all, as a former paying customer. reply PUSH_AX 9 hours agorootparentprevYou say that like it's their only issue. Earlier in the year they had a catastrophic outage in LHR, we lost all our data. Yes this is also on me, I'm aware. Still, that's a hard nope from me, we migrated. reply DataOverload 16 hours agoprevWe switched from Fly to CF workers a while ago, and never looked back reply punkpeye 15 hours agoparentThey are fundamentally different. If Cloudflare provided a way to host docker containers with volumes though, that would be game over for so many paas platforms. reply stoicjumbotron 14 hours agorootparentCan't wait: https://blog.cloudflare.com/container-platform-preview/ reply punkpeye 13 hours agorootparentwow, this will be huge reply Aeolun 10 hours agorootparentOnly if they can sort out their atrocity of a documentation website. reply frakkingcylons 15 hours agoparentprevI switched from apples to oranges and never looked back. reply pier25 15 hours agoparentprevOur stuff on CF Workers has been working non stop for years now. About 6 months ago we migrated our most critical stuff from Fly to CF and boy every time Fly has issues I'm so glad we did. reply jpgvm 5 hours agorootparentToo much custom stuff too quickly, there is a lot of efficiency in vertical integration and a fully cohesive stack but it takes a very long time to stabilize if you take that route. We spent months trying to convince them of problems with their H2 implementation in their LB/proxy (they insisted nginx was at fault, spoiler - it wasn't) but had to leave (we also went to CF, which has its own problems). Eventually one of their employees wrong a long blog post about H2 that made it obvious they finally found and fixed those problems but months too late for my employer at the time. It would have been infinitely better for us if they could have just fixed their stability problems, that abstraction suited us as did their LB/proxy impl and SNI pricing. I wish them well, some really smart folk over there but I can imagine these reliability problems are probably really grinding down morale. reply rstupek 15 hours agoparentprevHow are they equivalent? reply eek2121 15 hours agoparentprevcongrats on not developing a playbook for the time you have to 'look back'. Providers will fail. good contingencies won't. ...hears faint sound...I SAID GOOD, QUIET YOU! reply gigapotential 14 hours agoprevHUGOPS Everything is going to be 200 OK! reply mrcwinn 16 hours agoprevI tried Fly early. I was very excited about this service, but I've never had a worse hosting experience. So I left. Coincidentally I tried it again a few days ago. Surely things must be better. Nope. Auth issues in the CLI, frustrations deploying a Docker app to a Fly machine. I wouldn't recommend it to anyone. reply steve_adams_86 15 hours agoparentI find their user experience to be exceptional. The only flake I’ve encountered is in uptime and general reliability of services I don’t interface with directly. They’ve done a stellar job on the stuff you actually deal with, but the glue holding your services together seems pretty wobbly. reply pier25 15 hours agoprevMy apps on Fly have not gone down this time. reply MaxfordAndSons 16 hours agoprevKinda funny that they've named their global state store \"Corrosion\"... not really a word I'd associate with stability and persistence. reply lordofgibbons 16 hours agoparentIt's an internal project based on Rust, not a product. So I don't think it matters too much what they name it. It's opens source which is great, but still not a product that they need to market. reply SOLAR_FIELDS 15 hours agorootparentAnd to be fair, it’s a bit of a cute meme to name rust projects things that relate to it. Oxide, etc reply toast0 14 hours agoparentprevI stored important data in mnesia, so who would I be to talk. :p reply throwawaymaths 13 hours agorootparentamnesia means forget, so mnesia means remember, I would guess? reply kermatt 16 hours agoparentprevhttps://community.fly.io/t/reliability-its-not-great/11253 https://github.com/superfly/corrosion reply dumah 15 hours agoparentprevI take your point but corrosion-resistant metals such as Aluminum, Titanium, Weathering Steel and Stainless Steel don’t avoid corrosion entirely but form a thin and extremely stable corrosion layer (under the right conditions). reply littlestymaar 14 hours agorootparentGold and platinum really are corrosion resistant though (but have questionable mechanical properties…) reply EGreg 16 hours agoprevWhat exactly does flyio.net do? reply HellsMaddy 15 hours agoparentIf you mean specifically flyio.net and not just fly.io the company, I'm guessing they host their status page on a separate domain in case of DNS/registrar issues with their primary domain. reply stackghost 15 hours agoparentprevIIRC their value prop is that they let you rapidly spin up deployments/machines in regions that are closest to your users, the idea being that it will be lower latency and thus better UX. reply vachina 15 hours agoparentprevIt’s basically what Heroku used to be but with CDN-like presence. reply michaelbuckbee 15 hours agoparentprevHosting service that has a lot of interesting distributed features. reply eek2121 15 hours agoparentprevWEB 2.0. SEE. TOLD YA! THEY SHOULDA UPGRADED TO THAT NEWFANGLED 3.0! ;) reply theideaofcoffee 14 hours agoprevColor me not surprised. My few interactions with people there just gave off the impression of them being in a bit over their heads. I don't know how well that translated to their actual ops, but it's difficult to not connect the two when they continue to have major outage after major outage for a product that 'should' be their customer's bedrock upon which they build everything else. reply tamrix 10 hours agoprevnext [2 more] [flagged] viraptor 9 hours agoparentThey actually hired some serious people. If they keep failing, it's not due to lack of total experience. That experience may not be well utilised, but the potential exists and they're far from kids. reply travisgriggs 4 hours agoprevDon’t a bunch of Elixir/Erlang guys work at fly.io? It’s weird to me that that hallmark of reliability is associated with something that the public sees as unreliable. What gives with that association? reply veggieWHITES 16 hours agoprev [–] I was considering these guys the other day until I saw their pricing page: https://fly.io/pricing/ (There's not a single price on there, why even create the page?) reply rascul 16 hours agoparentThere's a link to what appears to be the actual pricing page https://fly.io/docs/about/pricing/ There's also a link to the pricing calculator https://fly.io/calculator reply totetsu 15 hours agorootparentIs that calculator hourly or monthly? reply radicalriddler 15 hours agorootparentLiterally says \"Monthly Costs\" in the green panel on the right that calculates the total. reply eviks 15 hours agorootparentprevIt's right there: \"Monthly Cost\" reply Aeolun 10 hours agoparentprevOMG, that's hilarious. I use them, and I know what my prices are, but I'd never noticed that the page called pricing doesn't actually have any. reply schmichael 16 hours agoparentprev [–] The prices are just one click deeper. Hardly a nefarious dark pattern. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Fly.io experienced a temporary outage, which was resolved, but it raised user concerns about the platform's reliability due to past incidents.- Users compared Fly.io to alternatives like Railway and Cloudflare, highlighting differences in reliability, features, and user experiences.- Despite the challenges, some users value Fly.io for its ease of use and competitive pricing, while discussions emphasized the importance of high availability and monitoring dependencies."
    ],
    "points": 227,
    "commentCount": 217,
    "retryCount": 0,
    "time": 1732585645
  },
  {
    "id": 42239721,
    "title": "A Short Introduction to Automotive Lidar Technology",
    "originLink": "https://www.viksnewsletter.com/p/short-intro-to-automotive-lidar",
    "originBody": "Share this post Vik's Newsletter A Short Introduction to Automotive Lidar Technology Copy link Facebook Email Notes More Discover more from Vik's Newsletter Weekly engineering insights into the magic of communication technology. Over 3,000 subscribers Subscribe Continue reading Sign in A Short Introduction to Automotive Lidar Technology A guide to the operating principles, techniques and technology in lidar systems for self driving cars. Nov 24, 2024 18 Share this post Vik's Newsletter A Short Introduction to Automotive Lidar Technology Copy link Facebook Email Notes More 2 2 Share Ubiquitous adoption of lidar in self driving cars needs one major thing: lower cost. Lidar has proven to be a capable technology for level 4 autonomous driving, and is already used in self driving taxis by Waymo and Cruise. But the spinning lidar domes on top of these cars cost thousands of dollars, and that number needs to drop by at least an order of magnitude. There are over 140 startups in the lidar space looking to make that happen and more. In this post, we will cover the basics of automotive lidar technology: Lidar for autonomous vehicles Wavelength of operation Photodetectors Ranging techniques Mechanical lidar Scanning systems MEMS mirrors Solid-state lidar Flash lidar Optical phased arrays References Read time: 12 mins The post may be too long for email. Please read it online. Lidar for Autonomous Vehicles Lidar stands for Light Detection and Ranging and is a method where infrared laser light is used to measure the distance to a remote object. This technology is not new. For years, it has been used for imaging vegetation, urban terrain, hidden archeological sites, building construction and recently, in augmented reality. Its particular superpower is that it can generate high resolution images of its surroundings much better than radar can. While lidar and radar are fundamentally similar in operation, the use of shorter wavelengths (lasers) compared to radar (microwaves) gives it the ability to generate highly detailed images. In last week’s article, we looked at the camera versus lidar debate for self driving cars. If you missed that, you can read it below. Tesla’s Big Bet: Cameras over LiDAR for Self Driving Cars Vikram Sekar · Nov 17 Read full story Since 2020, lidar has become especially relevant as the “eyes” of autonomous vehicles. Its ability to rapidly generate precise 3D images of the surroundings is critical in making accurate distance estimations for self driving. The downside of lidar is cost. Laser sources, detectors and associated electronics and mechanics are expensive. The rise of solid-state lidar technologies may still offer a competitive price point for widespread adoption of lidar in self driving cars. The next sections will explain the inner workings of lidar technology. Thanks for reading Vik's Newsletter! Subscribe for free to receive new posts and support my work. Subscribe Wavelength of Operation Lidar systems are predominantly designed to operate in one of two wavelengths that are in the infrared region (750 nanometers to 15 micrometers) of the electromagnetic spectrum, but outside visible range (380 to 700 nanometers). 905 nm (near infrared, or NIR) 1550 nm (short wave infrared, or SWIR) The choice of wavelength in a lidar system depends on the output power of laser sources, sensitivity of detectors and the interference from natural and artificial light sources in the same spectrum. Sunlight is one of the dominant sources of interference which has a lot of energy in the infrared region of the spectrum. A measure of sunlight’s impact is called the solar photon flux, which is the amount of sunlight hitting the earth at any given wavelength. Impact of sunlight at ground level versus wavelength. Source: Ouster There are some noticeable dips at 905, 940 and 1550 nm due to absorption by water vapor in the upper atmosphere, which conveniently reduces interference in systems at ground level. Unfortunately, the same effect absorbs radiation in foggy and rainy road conditions. The proximity of the 905 nm wavelength to the visible range causes two other concerns: 905 nm laser wavelengths are easily absorbed by the retina causing damage from prolonged exposure. As a result, there are strict standards for lidar eye safety that must be adhered to. There are plenty of interference sources near visible light, both from the sun and from vehicle headlamps that degrade the system performance. However, at shorter wavelengths, photodetectors are generally more sensitive and laser sources are more powerful and inexpensive. Ouster, for example, has actually adopted 850 nm for its lidar technology despite high solar photon flux due to better visibility in damp conditions, improved source and detector performance, with patented approaches to rejecting environmental interference. 1550 nm wavelength mitigates some problems; lower interference from solar radiation, and lower eye safety concerns because this wavelength only penetrates up to the cornea, thus protecting the retina. Better eye safety implies that more power can be used at 1550 nm for longer periods of time, providing longer detection range. The downside of 1550 nm wavelength is that the high absorption by water vapor makes it difficult to use in wet conditions. The choice of wavelength also depends on the capabilities and economics of photodetectors. Photodetectors Avalanche photodiodes (APDs) are the most commonly used detectors in lidar. They are specially engineered PN semiconductor junctions that utilize the photoelectric effect to generate electron-hole pairs in response to incident photons. They generate a current proportional to the number of photos incident, which depends on the amount of reverse bias on the diode. APDs are most often built with Silicon (Si), Germanium (Ge), and Indium Gallium Arsenide (InGaAs), but each of them respond differently to infrared wavelengths. Silicon APDs respond well to NIR and are inexpensive to manufacture, while InGaAs works well for SWIR wavelengths but are more expensive. Responsivity curves for various infrared sensor APDs. Source: Phlux A popular detector used in lidar systems is the single-photon avalanche diode (SPAD). SPADs are especially interesting as photodetectors. Unlike traditional avalanche photodiodes (APDs) which generate a signal proportional to the amount of light, SPADs generate a near binary response to the arrival of a photon by operating in “Geiger-mode” where the photodiode is heavily reverse-biased. The avalanche breakdown effect in the diode generates massive amounts of current when incident even with a single photon. With this, the timing of photon arrivals can be determined to pico-second (trillionth of a second) accuracies which allows accurate distance measurements using these sensors. An added benefit is that SPADs can be implemented in a CMOS process making them low cost. This also allows massive amounts of signal processing to be integrated right next to the detector array. Especially at 905 nm, silicon photomultipliers (SiPMs) have largely replaced Si APDs. SiPMs are arrays of microcells comprising of a SPAD with a quenching resistor to self-limit the flow of avalanche current. SiPMs provide very high photoelectric gain and are capable of detecting the precise number of incident photons depending on output current levels. Ranging Techniques The detection of object distance using lidar is called ranging, and there are two popular approaches that are often used. 1. Direct Time-of-Flight (dToF) Much like how sonar echo-location or pulsed doppler radar works, ToF sensing using lidar involves emitting laser bursts and measuring the time taken to detect the reflected signal. The total time elapsed from signal emission to reception is called the round-trip delay. Since the actual time to the object is half the round-trip delay, the distance is calculated using the speed of light in the propagating medium. ToF lidar ranging. Credit: Philip Sandborn, Berkeley Technical Report UCB/EECS-2019-148: https://www2.eecs.berkeley.edu/Pubs/TechRpts/2019/EECS-2019-148.pdf The smallest distance that can be measured using ToF depends on the resolution of the timing electronics. A nearby object might have a short round-trip delay that the detector might not resolve. Hence the minimum depth of such radars are usually limited to a few centimeters. The largest distance that can be measured depends on the transmitted power, detector sensitivity and free space path loss. If the reflected signal is indistinguishable from background noise, then the distance to the object cannot be resolved. Commercial dToF systems have a maximum range of 100-200 meters. Most lidar systems today use dToF ranging methods due to simplicity. A slightly different temporal detection approach is to use continuous wave signals, and detect the phase shift of the reflected wave. This method is called indirect ToF (iToF), or more specifically amplitude modulated continuous wave (AMCW). It is less sensitive to timing drift and better suited to short distance measurements. AMCW lidar ranging. Credit: Philip Sandborn, Berkeley Technical Report UCB/EECS-2019-148: https://www2.eecs.berkeley.edu/Pubs/TechRpts/2019/EECS-2019-148.pdf 2. Frequency Modulated Continuous Wave (FMCW) While ToF uses pulsed or continuous wave signals of a fixed wavelength, there are benefits to modulating it. Lidars that use the modulation of the wavelength or frequency of the transmitted pulse are called FMCW lidars. While many sources online claim that FMCW lidar is new technology, it is not. It has been around since the 1960s and the concept is widely used in automotive radar technology. FMCW LiDAR. Credit: Philip Sandborn, Berkeley Technical Report UCB/EECS-2019-148: https://www2.eecs.berkeley.edu/Pubs/TechRpts/2019/EECS-2019-148.pdf Each burst of frequency modulated signal is called a “chirp”, and the reflected signal received after a time delay has an instantaneous frequency difference between transmitted and reflected pulses. This “beat” frequency can be downconverted using a mixer and used to compute both distance and velocity of the object. I have explained before how this works for radar, and the same principles apply to lidar. How Automotive Radar uses Chirp Signals for Sensing Vikram Sekar · Jun 9 Read full story How Automotive Radar Measures the Velocity of Objects Vikram Sekar · Jun 16 Read full story FMCW lidar systems are complex to implement due to the need for a frequency tunable laser source for modulation and additional electronics needed to extract information from the transmitted and received signals. But they do give lower interference from nearby lidar systems because the frequencies are different at any point in time. Also, FMCW lidar requires lower peak power from a laser compared to ToF which has implications in eye safety requirements especially at 905 nm. Mechanical Lidar Systems 1. Scanning Lidar A mechanical lidar has an infrared laser that is mounted on a brushless DC motor that rotates the sensor thus providing it a 360° field of view (FOV) in the horizontal direction and eliminating any blindspots. The FOV in the vertical direction is still limited to about 90-95°. An example of a mechanical scanning lidar sensor is Waymo’s Laser Bear Honeycomb, which is often seen mounted on top of its self driving fleet of cars. The motors and its associated precision moving parts add to the bill of materials, and are subject to wear and tear from repeated use. As a result, scanning lidar systems are bulky and expensive. Waymo’s Jaguar iPace with a scanning Lidar sensor. Source: Waymo 2. MEMS-Mirror Lidar Instead of moving the laser source and sensor around like in mechanical scanning, another approach is to reflect the laser light off a movable micro-electromechanical (MEMS) mirror. By oscillating the MEMS mirror back and forth at a fixed rate, the lidar can be scanned across 3D space. MEMS mirrors can be made to move with electrostatic (only electric field), electromagnetic (electric and magnetic field), or electrothermal (with heat) actuation mechanisms. Below is a nice demonstration of the concept; video credits: TTP. A trade-off in MEMS mirror design is weight versus scanning rate; a heavy mirror will have low scanning rate. While the video above shows 1D scanning, 2D MEMS mirrors have also been implemented where the mirrors have a slow and fast axis. The mirrors move quickly along one direction allowing fast raster scanning, while moving slower in the perpendicular direction to only produce a static positional shift for a new rapid scan. Arguably, the greatest benefit is the fact that MEMS mirrors can be fabricated using back-end-of-line processes in a legacy CMOS foundry and are considered a mature technology. This enables low-cost implementations of scanning lidar technology. Solid-State Lidar Systems 1. Flash Lidar Instead of scanning 3D space, think of flash lidar as a photographic capture that illuminates the space in front of it. Flash lidar consists of a vertical-cavity surface-emitting laser (VCSEL) as laser source that is diffused to illuminate a target. The reflected signals are detected with an SiPM array. These lidar flashes are taken at rates up to 30 frames per second providing a real-time rendering of 3D space. By the nature of how it works, flash lidar has a reduced FOV compared to a rotating mechanical lidar scanner. The resolution of flash lidar is limited by how many pixels fit into a given area, much like a digital camera. Compared to the scanning type, flash lidar has lower signal-to-noise ratio (SNR) because the limited optical laser power needs to be distributed to all pixels in the array. The detection sensitivity is also limited by background noise in the environment at the same wavelength as the laser. SNR is the ultimate limiting factor in the detection range of flash lidar with sensing distances up to 100 meters and centimeter-scale resolutions being reported in literature. Some companies have adopted a multi-beam approach to flash lidar, illuminating only those parts of the environment where the detector is looking for information. This allows greater optical power to be directed at fewer, but more relevant pixels in the array, enhancing SNR. It is a combination of scanning and flash lidar, with the advantages of both. Overall, the lack of moving parts means that the system is much more reliable, immune to vibration effects, and has increased data capture rate. 2. Optical Phased Array (OPA) Lidar The most recent approach still in the research phase is to use silicon photonics to implement scanning lidar on a chip. The idea is borrowed from phased array antennas which allow scanning of the radiated beam by adjusting the phase shift of each signal fed to an antenna array. Phase shifts are implemented either with integrated optical waveguides, or by using integrated heaters to slow light through the mechanism of thermo-optic coupling. Depending on the phase shift, the direction of the radiated wavefront can be scanned in 3D space. I have explained this in a previous article. How does a Phased Array Antenna work? Types of Beamforming Architectures Vikram Sekar · Feb 11 Read full story Now, the same approach is being utilized to steer infrared lasers by implementing phase shifts to integrated optical modulators in a photonics platform. The benefits of OPA are greatly increased scanning speeds due to electronic control and no moving parts. The cost and reliability benefits from a purely integrated approach on 300-mm diameter silicon wafers is also attractive. The use of optical frequencies presents its own challenges in the context of phased arrays: Thermal management: The heat generated from many on chip laser sources must be dissipated effectively. Proximity of elements: Phased arrays require elements spaced half a wavelength apart. At 1550 nm laser wavelength, that means each laser source needs to be spaced under a micron apart. Scanning angle: In phased arrays, the best quality beam is at “boresight” or right in front of the array. As the beam is scanned away from the center, say beyond 60°, grating lobes degrade the beam width. Analog Photonics is a spin off from MIT, founded by Prof. Michael Watts, that is working on commercializing OPA technology and is worth keeping an eye on. References EETimes: What’s the Direction for Automotive LiDAR: 905 nm or 1550 nm? Texas Instruments: An introduction to automotive lidar Onsemi: Introduction to the Silicon Photomultiplier Aeye: Time of Flight vs. FMCW LiDAR: A side-by-side comparison IEEE Spectrum: Lidar on a chip puts self driving cars in the fast lane Phlux: The role of infrared sensors in light detection and ranging - lidar N. Li et al., “A Progress Review on Solid‐State LiDAR and Nanophotonics‐Based LiDAR Sensors,” Laser & Photonics Reviews, vol. 16, no. 11, p. 2100511, Nov. 2022, doi: 10.1002/lpor.202100511. D. Wang, C. Watkins, and H. Xie, “MEMS Mirrors for LiDAR: A Review,” Micromachines, vol. 11, no. 5, p. 456, Apr. 2020, doi: 10.3390/mi11050456. If you like this post, please click ❤ on Substack, subscribe to the publication, and tell someone if you like it. 🙏🏽 Subscribe If you enjoyed this issue, reply to the email and let me know your thoughts, or leave a comment on this post. Leave a comment Join a Discord community of professionals, enthusiasts and students, and get in on the discussion. Join Discord The views, thoughts, and opinions expressed in this newsletter are solely mine; they do not reflect the views or positions of my employer or any entities I am affiliated with. The content provided is for informational purposes only and does not constitute professional or investment advice. 18 Share this post Vik's Newsletter A Short Introduction to Automotive Lidar Technology Copy link Facebook Email Notes More 2 2 Share",
    "commentLink": "https://news.ycombinator.com/item?id=42239721",
    "commentBody": "A Short Introduction to Automotive Lidar Technology (viksnewsletter.com)219 points by kayson 22 hours agohidepastfavorite157 comments Animats 21 hours agoThat's a reasonable basic overview. I'm surprised that rotating scanners are still used. It's been twenty years since Velodyne built their first one. They work OK, but cost too much. I was expecting flash LIDAR or MEMS mirrors to take over. Continental, the auto parts company, bought the leading flash LIDAR company over a decade ago, but the volume market a big parts company needs never appeared. Waymo is still using rotating LIDARs even for the little ones at the vehicle corners. Those need less range. There needs to be a cheap, flush-mounted replacement for those things. The location is too vulnerable. Maybe millimeter phased array radar mounted behind Fiberglas body panels. Waymo needs to solve that problem before they do New York. The LIDAR on top may not be a problem. Insisting that it has to go away to \"look like a car\" is like insisting that cars had to have the form factor of horse-propelled buggies. Early cars looked like buggies, but that didn't last. One big advantage of pulsed LIDAR over continuous is that the interference problem between identical units is much less. The duty cycle is tiny. Data from one pulse round trip is collected in less than a microsecond. Just put some randomization in the pulse timing and getting multiple conflicts in a row goes away. reply 0_____0 19 hours agoparentWaymo have in house radar, I think in the 70GHz gap in the absorption spectrum. They're pretty obvious as sort of paperback book sized planes, mounted near other sensors IIRC. The old Velodyne units were actually susceptible to damage if you left two units running right next to each other. I did hear a proposal at some point for a different but similar unit to use GPS time to sync the rotations of all the units we had live so they wouldn't be pointed at each other, but in practice it seemed to not be a huge issue. BTW I once gave you guff about continuing to bring up Conti's flash LIDAR, and in retrospect I wish I hadn't, I really enjoy your contributions here. reply deepnotderp 17 hours agoparentprevThe SNR for flash Lidar is really low because you spread the beam out over such a large area. Most automotive Lidar already operate in a “photon starved regime”, ~200-300 photons per return[0]. If you spread that over the entire scene, your snr drops quickly. This forces you into 1550nm, and a large detector array and high power laser at 1550nm is extremely expensive. As for MEMS, it’s been a while but I think FOV/steering angle range , steering speed and even maximum beam power were concerns EDIT: my Lidar friend Jake reminded me that the appetizer size is also an issue with MEMS- smaller aperture = less light collected = lower SNR [0] https://www.hamamatsu.com/content/dam/hamamatsu-photonics/si... reply hwillis 4 hours agorootparent> Most automotive Lidar already operate in a “photon starved regime”, ~200-300 photons per return[0]. If you spread that over the entire scene, your snr drops quickly. Translating: Normally you have a large single sensor per laser, which makes measurements at a very high rate. With flash lidar, you split the sensor up like an image sensor. In a normal image sensor, each pixel can collect light for a long time, but if you do that with lidar you have no distance resolution. The sensor is sitting idle 99% of the time, and you pay in sensitivity and accuracy. Array sensors, MEMs, and phased arrays all struggle because they're all really good at small-angle differences, while the reason for scanning lidar is large-angle differences. Maybe one day we'll start making curved dies and it'll be easier to have a really wide FOV without needing multiple sensors. reply deepnotderp 53 minutes agorootparentYou can actually make curved dies already- there’s a company doing that for image sensors, if you thin silicon down it becomes flexible reply michaelt 8 hours agoparentprev> I'm surprised that rotating scanners are still used. It's been twenty years since Velodyne built their first one. They're even older than that. SICK have been pointing laser range finders into spinning mirrors since about 1995 - albeit mostly for industrial safety systems which can be quite price-insensitive. There's a few things to know about LIDAR to understand why spinning lasers make sense. First of all, anything emitting a cone of light encounters \"inverse square dropoff\" - where moving twice as far away means you get a quarter of the light, per unit area. This is most visible with flash photography at night - but it also applies to LIDARs. And in an automotive application, ideally you want to be able to sense things 100m away. Illuminating a laser spot is much more practical than illuminating everything. Secondly, whatever light source you use has to be eye-safe. And sure, IR has safety advantages over visible light here - but a light source bright enough to illuminate things at a 100m distance would be very hard to make safe, even with the advantages of IR. As a scanning laser never lingers in one point for long, it can safely be much more intense. The third thing to know is whatever light source you're using, you're in competition with the sun. Sometimes the sun is low in the sky and directly dazzling your sensors. Other times it's illuminating the same things you want to illuminate. This means you can't make up for a weak light source and inverse-square dropoff with clever signal processing. And finally, the makers of these cars envisage a future where every single vehicle on the road is using this technology. So there's also a risk of the reflected returns of two different vehicles interfering with one another. Even rotating LIDAR can be vulnerable to it, but flash LIDAR is particularly vulnerable. Meanwhile, automotive companies aren't scared of moving parts. A car has loads of spinning parts already; they have mastered the art of making spinning things that can keep spinning for thousands of hours. reply rrr_oh_man 6 hours agorootparent> Meanwhile, automotive companies aren't scared of moving parts. A car has loads of spinning parts already; they have mastered the art of making spinning things that can keep spinning for thousands of hours. Almost an understatement. A typical car wheel hub with a 20-27 inch tire diameter has experienced around 75-100M full rotations by the time it reaches 100K miles. Meanwhile, the engine probably has revolved ~5-10 times more during the same time. reply raisedbyninjas 6 hours agorootparentI may be wrong, but I think the concern was delicate components protruding from the body are susceptible to damage in urban areas. reply HPsquared 3 hours agorootparentCars already have mirrors and lights etc. LIDAR is in the same family as those. reply vel0city 2 hours agorootparentMirrors and lights are generally cheaper than a whole LIDAR unit when I get lightly sideswiped. reply HPsquared 2 hours agorootparentI figure if the LIDAR is mass produced on every car, it won't be much more than a headlight. (Headlight prices notwithstanding) reply JayPalm 15 hours agoparentprevContinental is folding their automotive LiDAR division and is laying off everybody. reply Animats 13 hours agorootparentNot surprised. I don't think they sold many. reply xnx 21 hours agoparentprev> They work OK, but cost too much. Costs have dropped dramatically in the past 20 years and continue to do so. > There needs to be a cheap, flush-mounted replacement for those things. Why? Corners are the optimal mounting position for maximum visibility. It allows the car to -in-effect- see around corners in ways no centrally mounted sensor can. > Waymo needs to solve that problem before they do New York. What? Because of vandalism? reply aftbit 20 hours agorootparentHave you ever seen the corners of a car that has been parked in a big East-coast city? They will sustain damage during the course of normal operation and storage, and many people will not stop and leave their insurance information, especially if the damage is perceived as minor and happens while the car is parked and the owner not present. Currently, the corners of a car are relatively non-critical to its function and usually not too expensive to repair. If both of those change, we'll see more expensive damage that is more challenging to repair as well as less likely to be handled by the responsible party. Also, having the sensors stick out from the corners makes the car's collision box and turning radius bigger. That doesn't help in any tight situation, but I imagine that's not that different between e.g. SF and New York. What is different is the sheer volume of cars and pedestrian activity. reply Animats 13 hours agorootparentRight. It seems to have been Waymo's decision to have zero blind spots around the vehicle perimeter, even if that means having the sensors stick out. Cruise had an accident where another vehicle knocked a pedestrian into a Cruise car, and the pedestrian was dragged. Cruise lost their California DMV autonomous license for that. So there's a good case for full perimeter coverage. Humans don't have that. The same week as the Cruise incident, a NYPD tow truck dragged a pedestrian some distance because they were in a blind spot for the driver. reply ljlolel 8 hours agorootparentThey lost their license for not reporting it properly (as required under the license). Not for the accident. reply Filligree 12 hours agorootparentprevDid the tow truck driver lose their license? reply AlotOfReading 18 hours agorootparentprevThey don't stick out that much. The geely vehicle has front sensors recessed just above the front wheel well, without much additional side clearance. Either way, a collision involves regulatory filings, downtime, and sensor recalibration even if no damage is sustained. reply m0llusk 18 hours agorootparentprevWaymos sometimes stop briefly in parking spots while waiting for assignments, but they don't really park as such except in special lots. The big problem I have seen is they tend not to always pull to the curb when releasing passengers and if a door is left even slightly ajar then they will sit there requesting the door be closed even if they are blocking a lane with many cars behind them beeping. reply UltraSane 15 hours agorootparentNot having a motor and thus having to depend on people to close doors on an autonomous car seems very silly. reply xnx 14 hours agorootparentWaymo's custom designed 6th generation vehicles[1] with self-closing doors were expected to enter service this year, but have [probably] been put on indefinite hold due to tariff issues [1] https://waymo.com/blog/2021/12/expanding-our-waymo-one-fleet... reply UltraSane 14 hours agorootparentcan't they retrofit a door closer to their current cars? reply financetechbro 16 hours agorootparentprevI think it’s due to how often cars bump or scratch against each other in NYC (I.e. the sensors are in a vulnerable spot to be easily damaged). It’s quite funny seeing the number of cars that have bumper skirts in NYC to help minimize damage from inevitable close encounters with other vehicles reply atomic128 21 hours agoprevHere's an interesting \"lidar gem\" from Hacker News a few years ago: https://news.ycombinator.com/item?id=33554679 Lidar obstacle detection algorithm from a Git repo leaked onto Tor This is a drivable region mapping (obstacle detection) algorithm found in what appears to be a git repo leaked from an autonomous vehicle company in 2017. The repo was available through one or more Tor hidden services for several years. The lidar code appears to be written for the Velodyne HDL-32E. It operates in a series of stages, each stage refining the output of the previous stage. This algorithm is in the second stage. It is the primary obstacle detection method, with the other methods making only small improvements. The leaked code uses a column-major matrix of points and it explicitly handles NaNs (the no-return points). We've rewritten it to use a much more cache-efficient row-major matrix layout and a conditional that will ignore the NaN points without explicit testing. This is an amazingly effective method of obstacle detection, considering its simplicity. reply arrakark 16 hours agoparentWhich Tor hidden service was this? Asking for a friend... reply edm0nd 15 hours agorootparentIf it was leaked in 2017, that's when Tor hidden service v2 URLs were still in use. Meaning the site is long gone and inaccessible these days. reply synthos 5 hours agoprevI worked on an automotive FMCW LiDAR that didn't quite make it to market. Cool technology but it was difficult to scale the cost down, which is pretty important for automotive. Margins are very low in that market reply hammock 19 hours agoprevAre LIDAR dangerous to the eyes of other drivers or pedestrians? reply hnisoss 17 hours agoparent1550nm LiDAR Damaged Sony Camera at CES - http://image-sensors-world.blogspot.com/2019/01/1550nm-lidar... reply hammock 5 hours agorootparentThanks for sharing that, clearly there is something at least worth investigating here. The concern about long-term exposure to LiDAR beams is not extensively studied. Current regulations seem to primarily focus on short-term exposure limits, and there is a lack of comprehensive testing under real-world conditions reply deepnotderp 17 hours agoparentprevNo, there’s a class system for laser safety The rating is for you to stick your eyes right up to it for a long period of time and still be fine reply hammock 5 hours agorootparentWhat’s “a Long time”? Does it cover a 2 hour commute in traffic with 20+ cars around you blasting it continuously any direction you look, invisibly? reply Symmetry 4 hours agorootparentLaser safety ratings are based on what would happen if the laser was pointed directly at your eye continuously. In the case of general traffic each lidar is scanning in different direction and while manufacturers try to make the energy produced by their lasers instantaneously brighter than the sun in one specific wavelength but damage to your retina is caused by excessive heating and doesn't care about what wavelength the energy is coming in at in the IR except to the extend that it can get to the retina or not. In your morning commute I'd worry less about the lidars than the much larger amount of invisible IR radiation given off by the sun. And I'd worry much less about the sun's IR radiation than the sun's UV radiation, wearing sunglasses during a 2 hour commute is best for your eyes. reply itishappy 19 hours agoparentprevThey should not be. In theory they can be, but there are strict regulations to prevent that. reply hammock 18 hours agorootparentWhat are the regulations and who are the relevant regulatory bodies? I could not find with a google search reply itishappy 16 hours agorootparentGood question! You're right, this is surprisingly hard to Google. It looks like the FDA is responsible. I would not have guessed that! The National Highway Traffic Safety Administration (NHTSA) would have been my guess, but I'm not finding much there. They have a spec for LIDAR speed measurement devices, and one for the required sensors in vehicles, but nothing on the the output of said sensors. > For manufacturers of laser products, the standard of principal importance is the regulation of the Center for Devices and Radiological Health (CDRH), Food and Drug Administration (FDA) which regulates product performance. All laser products sold in the USA since August 1976 must be certified by the manufacturer as meeting certain product performance (safety) standards, and each laser must bear a label indicating compliance with the standard and denoting the laser hazard classification. https://www.lia.org/resources/laser-safety-information/laser... https://www.fda.gov/radiation-emitting-products/home-busines... https://www.fda.gov/about-fda/fda-organization/center-device... reply hammock 5 hours agorootparentSorry, I don’t believe the FDA is doing anything more than stamping a Class 1 or class 2 sticker on component parts. They are not testing LIDAR arrays in situ under simulated driving conditions I would like to see crash test dummy style research around vehicular LIDAR reply kube-system 58 minutes agorootparentCrash tests in the US are also technically on the honors system too, but NHTSA does test the most common models. But many they don't. For example, the Cybertruck. reply AlotOfReading 18 hours agorootparentprevThey fall under the same regulations as lasers. reply hnisoss 17 hours agorootparentThose can be gamed easily. reply itishappy 16 hours agorootparentPlease explain! reply sroussey 17 hours agoparentprevI know we are talking about car type lidar, but the iPhone Pro has a type of one and gets a depth map of photos. So you’re shooting it everyone you are taking photos of. reply KaiserPro 5 hours agorootparentI don't think the Lidar in apple's stuff is actually a lidar, I think its a structured light sensor.[1] What do I mean by that? lidar sends pulses of light and works out the difference between emission time and arrival time to work out how far the pulse has travelled. The structured light sensor emits a pattern or dots, and any distortion of that can be used to compute the shape of an object. [1] https://image-ppubs.uspto.gov/dirsearch-public/print/downloa... reply flutas 4 hours agorootparentThey aren't talking about FaceID. iPhone 12 introduced a lidar sensor in the back. https://www.nature.com/articles/s41598-021-01763-9 reply bastloing 2 hours agoprevWhy is lidar so expensive? And still needs to be miniaturized. But time should solve those problems, there's enough engineering effort. reply kube-system 46 minutes agoparentWhen you're measuring something as precise as the time it takes light to bounce off something in front of you, you need really precise optics and electronics. Also, automotive lidar is still in the realm of low-volume specialty equipment, so there are little to no economies of scale in manufacturing this stuff. reply rightbyte 21 hours agoprev\"Its particular superpower is that it can generate high resolution images of its surroundings much better than radar can.\" Is this true tough? Car radars are fixed. I guess a comparable lidar would be fixed too and have n points for n lasers. A rovolving radar would have continuous resolution around while a lidar samples? I thought the advantage of lidars were accuracy and being better at measuring heights of objects, where as radars flatten the view. reply ender7 21 hours agoparentThe issue isn't one of fixed vs rotation, it's that radar can't fundamentally achieve the resolution necessary to distinguish important features in the environment. It's easily fooled by oddly-shaped objects, especially concave features like corners, and so while it's great for answer the question of \"am I close to something\" it's not reliable for telling you what that something is, especially at longer ranges. reply xnx 21 hours agoparentprevI believe automotive radar has a cone of sensitivity that is read as a single \"pixel\" worth of data. Even if the radar spun like lidar, the radar cone of sensitivity is thousands of times wider than the lidar beam so you can't make much of a picture with radar. reply 0_____0 19 hours agorootparentIIRC the data coming out of the Conti radars was preprocessed to give bearing, distance, and size of an object in the FOV of the unit. I don't know if I ever saw the true raw data out of one of them, but I'm curious what it looks like. reply rightbyte 11 hours agorootparentYe I have a hard time imaganing how a car radar image looks like. On boat radars it seems like the radar have really high resolution (can see much further than lidars) but have worse accuracy. I.e. things looks like blobs. A lidar image at 50+ meters is very sparse. reply jeffreygoesto 7 hours agorootparentRoughly like in this paper https://www.semanticscholar.org/paper/RadarScenes:-A-Real-Wo... reply rightbyte 4 hours agorootparentThanks. Spot on! reply itishappy 19 hours agorootparentprevI'd be curious if the design of the Cybertruck affects readings at all. It's got angles straight outta an F-117. reply rightbyte 11 hours agorootparentI think \"stealth\" planes assumes the radar is under the plane on the ground? For the geometry. And they have some color or alloy that reflect less. reply 0_____0 2 hours agorootparentIt's cooler than that these days - under the paint are antennas plated? printed? onto the skin panels that are tuned to absorb specific frequencies of interest. reply 0_____0 17 hours agorootparentprevI reckon it's probably not that bad, there are big surfaces that are almost normal to what would be incoming radio energy. Stealth shapes tend to reflect energy in a completely different direction from the source. reply itishappy 16 hours agorootparentHere's the closest thing to data I've been able to find. I have no idea what to do with this info. https://x.com/jwt0625/status/1848218860513628203 reply 0_____0 2 hours agorootparentThe polar plot at the end would be useful if there were plots for other cars and trucks to compare to. I'm assuming that it's a simulation? reply lupusreal 21 hours agoparentprevVery high tech radars can generate amazing imagery, but they'll never top what lidar can do. Conceptually they're both doing the same sort of thing using EM radiation, but lidar uses a much smaller wavelength which gives it an intrinsic resolution advantage. Particularly at distances and with hardware sized relevant to cars. reply MaxPock 22 hours agoprevFantastic tech that Musk hates reply kieranmaine 19 hours agoparentIn a recent No Priors podcast with the Waymo Co-CEO Dmitri Dolgov, he talks about how they evaluated just driving with cameras and how it isn't good enough for full autonomy and doesn't meet their bar for safety [1]. 1: https://www.youtube.com/watch?v=d6RndtrwJKE&t=1119s reply jaimex2 19 hours agorootparentThey went deep down the wrong path and need to justify their mistake. Waymo will be killed off any day now. reply UltraSane 15 hours agorootparentI find opinions like this to be almost as crazy as saying that the earth is flat because Waymo has a working, truly self-driving taxi service RIGHT FREAKING NOW while Musk is still promising to have one some day in the hazy future while NEVER making a single vehicle that can actually drive without someone in the car. Musk rejecting LIDAR means that he fundamentally doesn't understand the technological challenge of self-driving despite have access to the world's experts OR he is cynically using false promises of self-driving to pump up Tesla share price. I know which one I think is true. reply altacc 10 hours agorootparentI think anyone who listens to Musk talking about something they themselves know a lot about quickly realises that Musk's skills are elsewhere. He can motivate and market the hell out of a business whilst snorting more ketamine than a herd of horses but he is not a technical genius by any means. He pays people well to agree with him and fires them when they don't, so I suspect that his companies that produce better and more stable products do so because he micromanages them less. reply mavhc 7 hours agorootparentIt's weird that what he does is so easy yet no one else is making EVs at scale in USA, or landing rockets, 10 years after SpaceX did it reply tordrt 7 hours agorootparentprevKarpathy said in some podcast that Tela uses LIDAR in training, and by doing this they can get a lot of the benefits. Not sure that all off the \"worlds experts\" agree with you that you HAVE to use LIDAR. Rate of progress for FSD has been very impressive lately. I personally think that its very plausible that Tesla might beat Waymo to large scale location independent autonomous driving. reply UltraSane 6 hours agorootparentThe stats on the latest FSD are still terrible. It still needs human intervention far too frequently and is no where near being able to run without a human in the car or Tesla accepting liability for crashes. reply reportingsjr 6 hours agorootparentprevWaymo's recent experiment with multimodal models and a purely camera based system (EMMA) validate some of the claims that using LIDAR data in training does help. Pretty neat! Still not as good as a LIDAR + RADAR based system. reply jaimex2 10 hours agorootparentprevIt doesn't. It has a party trick that works in very specific conditions. reply olabyne 9 hours agorootparentAt least it works. Meanwhile Tesla have nothing to show, even in \"very specific conditions\". reply KeplerBoy 8 hours agorootparentprevDo you expect a car with fewer sensors to fare any better soon? reply UltraSane 8 hours agorootparentprevBut it works vastly better than anything Tesla has made so what does that say about Tesla? reply JaggedJax 15 hours agorootparentprevFrom person experience, the state of the art Tesla vision FSD still can't drive east at sunrise, west at sunset, or in moderate rain. I haven't seen any sign of them solving that fundamental problem with vision, especially given there are existing non-vision solutions. reply bobsomers 19 hours agorootparentprevThat's a bold claim. Care to justify it? reply jaimex2 9 hours agorootparentYeah, it only works in extremely controlled environments driving really slowly. The design is also flawed as it has to work with cameras anyway. The last thing you want is two systems arguing over what they see. reply ra7 4 hours agorootparentExtremely controlled environments like the entire city of San Francisco? Sensor fusion is a thing. There are no two systems that “argue with each other”. I can’t believe the same old ignorant tropes are still making rounds. reply coolspot 39 minutes agorootparentprevWaymos don’t drive slowly, I don’t know where you’re getting this from. If anything, they drive too fast for a thing without a driver. reply Infinitesimus 6 hours agorootparentprevIt doesn't have to be an argument. You know what each system is good at and prioritize inputs accordingly. reply ra7 16 hours agorootparentprevGoogle killing off Waymo by giving them $5.6B just a few weeks ago! reply UltraSane 15 hours agorootparentWhat do they actually use that much money for? reply ra7 4 hours agorootparentNew vehicles and setting up depot operations. reply r17n 21 hours agoparentprevSo there's a video of him addressing this - he doesn't hate the tech. He mentions that it's wildly expensive for cars. But, they use it heavily for SpaceX reply threeseed 20 hours agorootparentThe issue isn't that it's wildly expensive for cars. But rather for Tesla. Because the company has promised that existing Tesla owners would be able to use FSD. Having to retrofit them to add LiDAR sensors would be cost-prohibitive. reply stormfather 20 hours agorootparentAlso he wants to reuse the foundational machine vision tech in Optimus bot, which probably won't have lidar. reply threeseed 20 hours agorootparentBased on presentations we've seen what sets Tesla apart are its datasets not the core technology. And those don't translate across to the Optimus bot. reply stormfather 4 hours agorootparentI think they will though, I think the enormous corpus of video data and the supercluster that powers self driving development are the machine vision analog of internet scale text data that gave rise to LLMs. We'll see the same moment for vision models that text prediction models had once the data is there, where an enormous foundation model becomes much much better, especially at zero-shot tasks. reply sroussey 17 hours agorootparentprevOptimus should probably have LiDAR more than a car… reply stormfather 4 hours agorootparentI would guess the plan is to have the foundational machine vision tech that becomes the core of robotics sensors. Not just Optimus but every robot arm in a factory, robot mule, etc. I don't think everything will have LIDAR if its proven to be unnecessary. reply quonn 22 hours agoparentprevIt‘s not just Musk. Most automobile manufacturers have maintained that they need to find a way to do it with cheap and pretty sensors. reply Klaus23 18 hours agorootparentThis is simply not true. Let's look at the best autonomous driving features available today, i.e. level 3: Mercedes Drive Pilot: Uses a lidar (and a dummy unit) up front. BMW Personal Pilot: Uses a lidar (and a dummy unit) up front Honda SENSING Elite: Uses 5! lidars They all use lidar, and some of the placement locations are downright hideous (Mercedes EQS). I think further development will require even more/better sensors, and manufacturers tend to agree on this point. reply ra7 16 hours agorootparentChinese OEMs (BYD, Xaomi, Nio) use lidar in almost all of their mid to premium segments. Also, Polestar 3. reply UltraSane 15 hours agorootparentHow well do they work? Camera only systems can be easily blinded by sun, fog, dirt, and snow reply asdasdsddd 17 hours agorootparentprevWhat are the benchmarks that say Mercedes, BMW, and Honda have the best level 3 features. reply Klaus23 17 hours agorootparentI ignore the Chinese because it is difficult to get reliable English information. Apart from those, these are the only level 3 systems available, and level 3 is the most advanced system that private individuals can currently get their hands on. Have I missed any? reply luos2 6 hours agorootparentprevIt's not a benchmark, but there is a youtube channel (Out of Spec) which tests these systems, and I think they also say Mercedes are the best in their \"Hogback challenge\". https://www.youtube.com/watch?v=xK3NcHSH49Q&list=PLVa4b_Vn4g... Worth checking out, many cars are very bad. reply GoToRO 1 hour agorootparentSponsored by Magna, probably the contractor selling them the system... reply fragmede 17 hours agorootparentprevDon't forget Blue Cruise from Ford. reply Klaus23 17 hours agorootparentBlue Cruise is level 2+, not 3, and does not rely on lidar. reply tordrt 7 hours agorootparentprevAll of these are far less capable than FSD. They might have more advanced regulatory approval because they have strong limitations of when it can be used, but if you drive the same route and compare, its not even close. reply Klaus23 2 hours agorootparentI doubt it. Yes, FSD is more flexible and can also drive reasonably well on city streets, but there is a reason why it is not certified for level 3 on motorways. It would most likely fail certification. With a level 3 system, I can take my eyes off the road and watch a movie. Doing that with FSD, even in the best conditions, is suicidal. Level 3 vehicles must have an extremely low failure rate. Any crash would quickly be picked up by the media. FSD is a versatile level 2 system, but at best a prototype for level 3. If we are talking about prototypes, it has to be compared to prototypes from other manufacturers like thisfully autonomous system from ... 11 years ago. The reason FSD is available to the average consumer is mostly a matter of philosophy, not technology. reply quonn 7 hours agorootparentprevMaybe they changed their mind on it in the last 10 years. I had as the source a high-ranking BMW manager as well as an Audi one who each gave a public lecture at a university with such a statement. reply Symmetry 50 minutes agorootparentprevIf you want conventional car utilization where the car sits in a parking spot most of the time then the extra cost from the lidars is much more of an issue than if you're operating a fleet that is acting as taxis most of the day. reply juliushuijnk 20 hours agorootparentprev> have maintained that they need to find a way to do it with cheap If the goal is to make roads safer. Aiming for cheap is good, it means aiming for more people who can afford that safer car. If it's not safer than humans, it should not be on the road in the first place. reply tgaj 19 hours agorootparentprevTheoretically if a human can drive a car using a pair of eyes connected to brain, it should be possible to do that using two cameras connected to some kind of image processing unit. reply ProblemFactory 7 hours agorootparent> Theoretically it should be possible to do that using two cameras connected to some kind of image processing unit That \"some kind of image processing unit\" in humans has an awful lot of compute power and software. If you remove $100k of sensors but have to add $200k of compute to run more advanced computer vision software, then it's a bad tradeoff to use only cameras, even if in theory that software is possible. reply itishappy 19 hours agorootparentprevIn theory. In practice neither the cameras nor processors available in cars function anywhere near human level. reply vel0city 16 minutes agorootparentIt's not even entirely true in theory. We use a lot of our senses when driving. Force feedback on the wheel. Sounds from the environment. Inertial senses. And our vision isn't fixed, its constantly moving. And yeah, as you mention, cameras don't really have the same level of range our eyes have and computers don't operate in the same way. reply carbotaniuman 17 hours agorootparentprevTheory isn't really all that applicable to this though - in theory nothing is stopping anyone from writing all code in assembly, but obviously that doesn't happen. I think more practically cars have adding driver assistance feature for a while now - more cameras, blind spot monitoring, ultrasound for parking, lane drift indicators. It is therefore not unreasonable to assume that adding more sensors is helpful (but even the old adage of more data is better than less would probably say that). reply tgaj 11 hours agorootparentTo be honest, it's possible that having too much data can only cause problems in quick decision-making. Any redundant data will only slow down processing pipelines. reply knifie_spoonie 18 hours agorootparentprevIn practice humans aren't particularly safe drivers. reply xdmr 17 hours agorootparentIs that because their vision fails to provide the information necessary to drive safely? Or is it due to distraction and/or poor judgment? I don't actually know the answer to this, but I assume distraction/judgment is a bigger factor. I'm not a fan of the camera-only approach and think Tesla is making a mistake backing it due to path-dependence, but when we're _only_ talking about this is _broadly theoretical_ terms, I don't think they're wrong. The ideal autonomous driving agent is like a perfect monday morning quarterback who gets to look at every failure and say \"see, what you should have done here was...\" and it seems like it might well both have enough information and be able too see enough cases to meet some desirable standard of safety. In theory. In practice, maybe they just can't get enough accuracy or something. reply ra7 16 hours agorootparent> Is that because their vision fails to provide the information necessary to drive safely? In certain conditions, yes. Humans drive terribly in dark and low light, something lidar excels in. reply tgaj 11 hours agorootparentStill, millions of humans drive every night and only a miniscule percentage cause any accidents. So maybe we are not so bad at this. reply ra7 4 hours agorootparentAccording to NHTSA, about half of all fatal crashes occur at night, even though only 25% of driving happens at nighttime. So yes, we are pretty bad at this. reply tgaj 11 hours agorootparentprevI totally agree, I think most accidents are caused by human nature (especially slow reaction time in specific conditions like being tired or drunk) and ignoring laws of physics (driving too fast). And some are just a pure bad luck (something/someone getting on the road right in front of the car). reply KeplerBoy 8 hours agorootparentprevSure, but why strive for that? We can have better than human perception by adding lidar and radar. reply fragmede 19 hours agorootparentprevIf we want the sell driving computer to be only possibly as good as a human. I can't see in the dark, can't see through fog, and have trouble with rain. Why is human visibility the bar to meet here? reply fragmede 15 hours agorootparentOh and the sun. I get blinded when the sun is in my eyes at sunrise and sunset. reply tgaj 11 hours agorootparentAnd how many car accidents did you cause in your life? Probably still no a lot even with your flawed vision. reply jdhwosnhw 18 hours agorootparentprevImagine that same reasoning applied to the car itself. Ugh, wheels?? Humans get around just fine bipedally, so cars should have legs too. reply dawnerd 14 hours agorootparentExplains the Tesla robot actually reply UltraSane 15 hours agoparentprevBecause Musk thinks is much much smarter than he actually is and refuses to listen to anyone. And between how many people he fired at Twitter, Tesla, and soon the US Federal Government I think he gets off on it. reply jaimex2 19 hours agoparentprevMusk has said several times Lidar is great. It's just a stupid idea for automotive use and he's not wrong. There's nothing similar in nature for a reason. reply bobsomers 18 hours agorootparentAirplanes don't flap their wings and boats don't wag their tails. Assuming that all technology should imitate nature is a naive engineering principle. The solution should solve the problem within the given constraints. reply jaimex2 9 hours agorootparentNature came up with something much better in both those cases. Portable, energy efficient, light, doesn't need refined oil, tightly steers... Boats and aeroplanes are terrible in comparison. They only work due to a huge network of global effort. reply vel0city 11 minutes agorootparentI don't see many birds around able to carry an extra 280,000lbs for 2,300 miles without having a meal. reply 542354234235 4 hours agorootparentprev>They only work due to a huge network of global effort. And horses don’t need roads like cars do and cars only work thanks to a huge network of global effort. What point are you trying to make? That we abandon planes until we can develop flight as efficient as nature? Abandoning LIDAR until we can develop visual light perception and processing equal to the human eye and brain? reply itishappy 19 hours agorootparentprevTime of flight ranging is used in nature by bats and whales/dolphins. reply jaimex2 9 hours agorootparentBoth primarily use their eyes. Look it up. reply fragmede 18 hours agorootparentprevand humans! https://www.atlasobscura.com/articles/how-to-echolocate reply maxbond 12 hours agorootparentMy back of the napkin estimate is that a human using time of flight ranging would be unable to distinguish between an object directly in front of their face and 8.6 meters away[1]. I think human echolocation uses a different mechanism (presumably relating to amplitude)? Skimming the Wikipedia article[2], it seems like animals do use time of flight, but also Doppler shifting. (As a side note, some animals have apparently evolved active countermeasures to echolocation!? It seems obvious in retrospect but incredibly cool.) There's interesting research into the mechanisms of human echolocation [3], but it was over my head. My impression was that the jury is out as far as the precise mechanisms involved but that there's a lot of evidence to be considered, I'm sure someone with a better background would get more out of it than I did. (I'm just curious about the mechanism, I agree that LIDAR has natural analogs.) [1] Speed of sound * 25ms, 25ms being the rule of thumb I've memorized for the minimum interval for two sounds to register as distinct from each other. This is just folk wisdom I've picked up hacking on audio, so perhaps I'm mistaken. [2] https://en.wikipedia.org/wiki/Animal_echolocation [3] https://durham-repository.worktribe.com/preview/1375913/1963... reply password4321 17 hours agorootparentprev2024 https://news.ycombinator.com/item?id=42160071 2018 https://news.ycombinator.com/item?id=18208334 reply igorstellar 19 hours agorootparentprevRotorwings are also not found in the nature yet they give us ability to navigate in a short distance 3D space better than fixed wing. reply edm0nd 15 hours agorootparentprevBats kinda have Lidar. reply jaimex2 10 hours agorootparentEcho location but they still mostly use their eyes. Same as dolphins reply bluGill 15 hours agorootparentprevNature makes for bad drivers. for some age groups cars are the largest causeof death. I self driving can do better. reply slt2021 16 hours agoprevLIDARs can be blinded by consumer grade laser pointers, I wonder if there are systems that protect LIDARs against adversarial attacks or DOS attacks reply elictronic 16 hours agoparentDrivers can also be blinded by consumer grade laser pointers. If someone starts attacking safety systems physically I would expect they will get quite a bit of jail time. reply rightbyte 10 hours agorootparentMessing with machines have a way lower ethical threshold for most people. Like fooling vending machines to give you soda. Nothing 'my teenage friend' lost sleep over. reply JumpCrisscross 4 hours agorootparent> Messing with machines have a way lower ethical threshold for most people We’re in the regime of throwing rocks at a freeway from a bridge to cutting brake lines as a prank. The attack surface here isn’t new and isn’t difficult to think through. Someone who can’t is going to be equally dangerous, machine or man. reply rightbyte 2 hours agorootparentI don't think e.g. 'kids' will see it like cutting a brake line. I agree, as a former ECU programmer, though. I am terrified of drive by wire. More like making the Google computer do silly things. More comparable would be the prank of tying some soda tin cans in a string to the exhaust etc, in how I believe my scapegoat kids would see it. reply denysvitali 2 hours agorootparentprevThere are also (stupid) people who like to point laser pointers to helicopter pilots... reply rightbyte 11 hours agoparentprevThey can be blinded by the sun too. I worked as a reaserch engineer at a uni playing with a 16 beam Velodyne when those were fancy. Put it on a car for a demo day, drawing the dots in 3d and marking obstacles red, and during sun set there was artifacts with no obvious way to filter out. Strangely, I was never able to recreate this. I think was some specific athmospheric condition. reply kayson 22 hours agoprevRelated: https://www.viksnewsletter.com/p/teslas-big-bet-cameras-over... reply Animats 21 hours agoparentWaymo tried cameras-only recently as a research project.[1][2] They seem to do about as well as Tesla, which they don't consider good enough. [1] https://www.forbes.com/sites/bradtempleton/2024/10/30/waymo-... [2] https://arxiv.org/pdf/2410.23262 reply xnx 21 hours agorootparentOne of the cool thing about the Waymo Driver is that it can be configured to work with different degrees of quality depending on the sensors available. In a low risk environment (e.g. closed to humans) like operating forklifts in an autonomous warehouse, it would work fine with just cameras. Waymo hasn't been very boastful to date, but some of the capabilities are hinted at in this interview: https://www.youtube.com/watch?v=d6RndtrwJKE reply brcmthrowaway 18 hours agoprev [–] Is there a lidar unit I can take home and scan my house at high resolution (than iphone)? reply coolspot 30 minutes agoparentiPhone pro with LiDAR and Scaniverse or Polycam free apps. reply twelvechairs 17 hours agoparentprevYou can, they are just expensive (other than iphone). Maybe 8k for a handheld basic one (e.g. Trion P1), $15k for a drone attachment (e.g. DJI Zenmuse L1) - more for the ones surveyors use proper including the tripod-mount ones. At the consumer end photogrammetry tends to just be so much cheaper that its preferred unless you really need defined accuracy at a high level of detail. Lidar tends to work currently much better in an industrial/professional context because its more accurate. Whether Lidar will make the jump to lower cost / consumer level is the big open question (and basically the same issue as for cars here) reply brcmthrowaway 17 hours agorootparentiphone has actual lidar not photogrammetry right? reply reportingsjr 5 hours agorootparentYes, the iPhone has a laser array and sensor that it uses for lidar. reply tgot 14 hours agoparentprevLookup the RPLidar family of devices. Cheap 1D, easy to work with. By 1D I mean that it measures ranges in 360degrees around the plane that it is spinning in. reply beeburrt 17 hours agoparentprevI was wondering this too, although for a different use-case. A couple years ago I was walking through a field/vacant lot not far from Centralia, WA and I came across what I think is a grave. The (supposed) \"grave\" was roughly human-sized and human-shaped, the ground was concave, sunken in and deepest at the center, and it was encircled with stones that were slightly larger than grapefruit. The reason I suspect it's a grave is because I stumbled upon a very similar-looking thing at a historical site in Tooele county Utah named Mercur cemetery. With Lidar I could prove/disprove my grave theory, correct? reply edm0nd 15 hours agorootparentI think Ground-penetrating radar (GPR) would be better usage for this grave scenario vs LIDAR. reply qiqitori 16 hours agoparentprev [–] I think this depends on your budget and what exactly you want to do. Do you want to scan your house from outside? Sounds expensive, would probably have to be drone-mounted, and the drone would fly around for a while (depending on the shape of the house.) Inside, and don't mind some minor inaccuracies? Not Lidar, but a Kinect from yesteryear may be enough. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Vik's Newsletter delves into automotive lidar technology, which is vital for self-driving cars due to its ability to produce high-resolution images using infrared lasers.- The newsletter discusses lidar's operating principles, including wavelength choices, photodetectors, and ranging techniques like Time-of-Flight and Frequency Modulated Continuous Wave.- It also examines various lidar systems, such as mechanical and solid-state options, with the aim of reducing costs and enhancing technology for wider adoption in autonomous vehicles."
    ],
    "commentSummary": [
      "Automotive lidar technology is evolving, with rotating scanners like Velodyne's being used despite their high cost, while alternatives like flash lidar and MEMS mirrors face market and technical challenges.",
      "Waymo's continued use of rotating lidars, even in vulnerable vehicle corners, underscores the need for more affordable and integrated solutions, as pulsed lidar offers reduced interference compared to continuous systems.",
      "Lidar provides high-resolution imaging and performs well in low-light conditions, but faces competition from camera-based systems, which some companies, like Tesla, prefer for cost reasons, fueling ongoing debates on the best approach for autonomous driving."
    ],
    "points": 219,
    "commentCount": 157,
    "retryCount": 0,
    "time": 1732565547
  },
  {
    "id": 42239952,
    "title": "Do you need ID to read the REAL-ID rules?",
    "originLink": "https://papersplease.org/wp/2024/11/25/do-you-need-id-to-read-the-real-id-rules/",
    "originBody": "Nov 25 2024 Do you need ID to read the REAL-ID rules? [“The welcoming, friendly and visually pleasing appearance” of the TSA’s headquarters at 6595 Springfield Center Drive, Springfield, VA.] We spent most of a day last week outside the headquarters of the Transportation Security Administration (TSA), trying and failing to find out what the rules are for the TSA’s new digital-ID scheme. What we did learn is that, by TSA policy and practice, you can’t read the REAL-ID rules, get to the TSA’s front door, or talk to any TSA staff unless you already have ID, bring it with you, and show it to the private guards outside the TSA’s gates. The problems we have faced just trying to get access to the text of the TSA’s rules raise issuess about (recursive) incorporation by reference of third-party, nongovernmental text in regulations, secret law, and access to Federal services and rights by those without ID, as well as the underlying issues of REAL-ID, mobile driver’s licenses, and digital IDs. In late October, as we’ve previously reported, the TSA issued a final rule establishing “standards” for smartphone-based digital IDs that would be deemed by the TSA to comply with the REAL-ID Act of 2005. These mobile driver’s licenses (mDLs) will be issued by state driver’s license agencies, but the standards incorporated into the TSA rule require that they be deployed through smartphone platforms (i.e. Google and/or Apple) and operate through government apps that collect photos of users and log usage of these credentials. The standards themselves — the meat of the TSA’s rule — weren’t published in the Federal Register or made public either when the rule was proposed or when it was finalized. Instead, thousands of pages of documents from private third parties were incorporated by reference into the TSA’s rules, giving them the force of law, on the basis of false and fraudulent claims — the falsehood of which was easy for anyone who checked to verify — that they were “reasonably accessible” to affected individuals. Secret laws are per se a violation of due process, and should be per se null and void. How can it be that “ignorance of the law is no excuse” if the government has kept you ignorant of the law, even when you try to find out what the law says? You shouldn’t need ID to read the law, just as you shouldn’t need ID to travel by common carrier. But the TSA doesn’t seem to have read the Constitution. In its final rule, the TSA repeated some of the same easily disproved lies about the availability of the material incorporated by reference in the rule that it had made in its earlier Notice of Proposed Rulemaking (NPRM). But the TSA also made a new claim that the material incorporated by reference in the rule was available for public inspection, by appointment, both at the Office of the Federal Register (a component of the National Archives and Records Administration) and at TSA Headquarters: All approved incorporation by reference (IBR) material is available for inspection at the Transportation Security Administration (TSA) and at the National Archives and Records Administration (NARA). Please contact TSA at Transportation Security Administration, Attn.: OS/ESVP/REAL ID Program, TSA Mail Stop 6051, 6595 Springfield Center Dr., Springfield, VA 20598–6051, (866) 289–9673, or visit www.tsa.gov. You may also contact the REAL ID Program Office at REALID-mDLwaiver@tsa.dhs.gov or visit www.tsa.gov/REAL-ID/mDL. For information on the availability of this material at NARA, visit www.archives.gov/federal-register/cfr/ibr-locations.html or email fr.inspection@nara.gov. We immediately contacted both NARA and the TSA at these email addresses, requesting to inspect the material incorporated by reference in this rule. NARA told us that this material isn’t stored at the office where it will (eventually, if and when it is found) be made available by NARA for public inspection. It might take up to six weeks, possibly longer, to locate and retrieve from offsite storage: I don’t have an estimated date for your appointment. The system we had for public access is not working and we are working to replace it. But how long that will take depends on our parent agency (the National Archives and Records Administration)…. If you wish to inspect all of the material listed in 6 CFR 37.4… it is in storage at a Federal Records Center. We will have to contact the FRC to get the material returned to us. That process generally can take up to 6 weeks; however, given the upcoming change in administration (and increased workloads for all involved), it could take longer. If it might take months for even the Office of the Federal Register to find the text which has been incorporated in Federal regulations and has the force of law, there’s a problem. But NARA says that whether documents incorporated in regulations are reasonably available is determined by whether they are available from the agency which promulgated the regulations — in this case, the TSA — and not by whether they are available from NARA. The TSA’s initial answer seemed more promising: We would be delighted to coordinate a visit to TSA so you may examine the materials requested and will work to provide you with information on how to access our facilities soon. After some back and forth by email, I arranged an appointment at the TSA’s headquarters at 9 a.m. on Monday, November 18th, for an escorted visit to inspect the documents incorporated by reference in the TSA’s “mobile driver’s license” rule. I was told that to inspect the rules I would have to provide my Social Security number and date of birth, “so that you can be cleared through Security.” I did so, reluctantly. Despite my efforts to avoid any surprises and anticipate any problems, my visit didn’t go well. The TSA occupies a purpose-built but privately owned new building in Springfield, VA, an hour outside Washington, DC, and a short-ish walk past the Washington Metro Transit Police Academy from the last stop on one of the Washington Metrorail lines. (There’s a virtual tour here of some parts of the interior that I never got to see.) According to the architects, the fortifications around the building were designed to look “welcoming” and “friendly”, while actually keeping the building secure against public access: LSG’s involvement was focused on the design of the entrance plaza and arrival sequence along the North facing public entrance of the project… LSG assisted with integration of elaborate security requirements in the site design. Security measures included K12 barriers, fences, gates, cameras and a specific vehicular circulation pattern for employees, visitors and services, etc. LSG’s focus was to integrate the security measures necessary without compromising the welcoming, friendly and visually pleasing appearance expected for TSA Headquarters. I arrived at the guardhouse outside the gate in the TSA’s perimeter fence at 8:45 a.m., to allow time for whatever security theater might be required before my 9 a.m. appointment. I described what happened next in an email message I sent shortly afterward: I am outside the entrance to your building, but a guard who refuses to give his name with a badge that says “Golden Services Security” refuses either to admit me without ID, which I don’t have, or attempt to contact anyone with the TSA. I specifically asked you, in my email message of November 1, to advise me of any access procedures for individuals without ID, who are obviously in the class of individuals impacted by the rules I want to access. You did not advise me of any ID requirement. I request that you either admit me to access the rules, as scheduled, or provide formal notice of your decision to deny me access identifying by name and title the decision maker, the basis for the denial including any rules relied on, and any available procedures for administrative review of that decision, so that I can attempt to exercise any available administrative appeals before returning to California empty-handed, having wasted more time and money in a diligent but unsuccessful last-resort attempt to access these rules after all other access methods claimed by you proved to be unavailable. As I noted in my first email message to you, it is impossible to reach you at the phone number specified in the final rule for access requests. You have provided me with no other phone number, and the guard refused my request that they call whatever point of contact or escort is listed on the visitor list. I will wait outside the entrance to your building for one hour, until 10 am, unless I am ordered to leave. I waited on an uncomfortable metal bench, exposed to the elements, just outside the fence. At 9:57 a.m., as I was about to give up and leave, someone called me from inside the building. They told me they had been assigned to escort me and were waiting for me at the “Visitor Center” inside the building. I told them, as I had said in my email message, that the guard wouldn’t let me get to the building without ID, which (as I had told them in advance) I didn’t have. They said they could only escort me once I got to the Visitor Center inside the building, and that they didn’t have “clearance” to come out to the fenceline to meet me. Catch-22. Eventually they conferenced in — at their initiative — Mr. Anurag Maheshwary, an attorney in the Office of the Chief Counsel of the TSA. Mr. Maheshwary was listed in the Federal Register as the point of contact for legal questions about the mobile driver’s license (mDL) rule. Mr. Maheshwary professed surprise and disbelief that I didn’t have ID, despite having been copied on my earlier queries about access procedures and protocols for people without ID. “What ID did you use to fly here from California? You must have ID.” (Wrong, even if I had flown, which I hadn’t said I had done.) When I repeated the request I had made in my email message that, if I was to be denied access to the documents incorporated in the TSA’s rules, I be given formal notice of that denial, Mr. Maheshwary immediately demanded that the person who had called me, and had conferenced him in, disconnect him from the call. “You’ve conferenced in the wrong person!” He repeated that demand and hung up when she called back and tried again to get him to talk to me. A little later, as I was again preparing to give up and leave, the same person called me again to say that, “We’re trying to work out how to get you access to the documents.” As with travel by common carrier, the TSA wants to create the impression that ID is required to read the text of its regulations, and acts in practice as though ID is required, but knows that it would have a hard time defending such a policy in court. So the TSA doesn’t put this policy in writing, and backs down (or at least claims that some alternative, albeit an inconvenient one, is available for people without ID) when confronted with people without ID who it perceives as sufficiently able and willing to assert their rights. Shortly after 11 a.m., by which time I had been sitting on the Group W bench for more than two hours, a TSA staffer came out with two large TSA-logo tote bags of documents. [The face of the TSA] [Some of the documents I was allowed to look at, but not to take with me.] One bag, which I was allowed to take with me, contained about ten pounds of copies of documents that are (purportedly) freely available online. Getting these documents directly from the TSA allowed me to be certain that they matched — or at least were claimed by the TSA to match — the text incorporated by reference into the TSA’s rules. That wouldn’t be possible with copies obtained from a private publisher or other third party, who would be neither able, willing, nor required to warrant that the copies they provided or sold matched what the TSA had depostied with the Office of the Federal Register. So that take-home bag had some official significance, but held no big surprises. The other bag, only slightly smaller, contained copies of documents that the TSA conceded weren’t freely available, but that it wouldn’t allow me to take with me. I was alloted not more than three hours to inspect them, sitting on a bench outside with my TSA minder sitting at the other end of the bench (equally uncomfortably, I presume) to make sure I didn’t try to take away any of the non-public documents included in the TSA’s rules. After another hour, around noon, my minder found another bench at a bus stop nearby that, while equally uncomfortable for us both, was at least partially shielded from the wind. After yet another hour, a little after 1 p.m. — by which time I needed a toilet and was getting increasingly cold, not having dressed to spend five hours outside — my TSA minder (who had been busy texting persons unknown on his phone) told me that arrangements had been made for me to be escorted into the building to continue my reading of the bag of non-public documents. I was given no explanation as to why this hadn’t been done four hours earlier when I arrived for my scheduled appointment. An armed guard came out, and he and the business-suited minder who had been sitting on the bench with me escorted me through the gate in the fence and across no-man’s-land to the entrance to the building. After going theough a metal detector, I was escorted to a restroom (at least they waited outside) and then back out past the metal detector to a conference room just inside the outer entry door that appeared to be there specifically for the purpose of meetings with visitors not cleared to go further into the building. In the limited time that I was allowed to spend with these documents, I was able to confirm that they require that mDL credentials be “provisioned” through a TSA-approved app on a TSA-approved device, with that device biometrically “bound” to an individual. The process of “presenting” a digital ID will be a four-way interaction between the individual, the app (it’s unclear whether or how the user will be able to authenticate the app or know what it is doing), functions on the device to collect biometrics (controlled, in all likelihood, by an operating system with root privileges whose actions can’t be monitored or controlled by the individual), and the credential-issuing driver’s license agency. mDL apps will be required to log each time a digital ID is presented, and to whom. This is described as a measure to protect ID-holders’ privacy, despite the obvious risk posed by police or others being able to know when and to whom you have shown your ID. If you use your digital ID for age verificaiton to show that you are old enough ton be allowed to access adult information about sexual health or abortion, that fact will be logged on your device. Supposely these logs will be available only to the device owner. But in reality, they will also be available to anyone who seizes the device while it is unlocked, cracks the device lock with forensic or criminal tools, or forces an individual — legally or illegally — to unlock it. The TSA is already issuing guidance to other law enforcement agencies on the information about the usage of these digital IDs that may be available to and via Google or Apple. But there’s more. To my dismay, if not surprise, I found that the non-public documents incorporated by reference in the TSA’s rules themselves incorporate by reference other documents, many of them also non-public. How mayy levels of recursion do we have to follow to get to the bottom of the TSA’s rules? It’s turtles all the way down. After leaving the area, I sent the following message to the TSA: Yesterday we finally saw the non-public documents incorporated by reference in the mDL NPRM and Final Rule. It was immediately apparent from the volume and technical nature of this material that understanding, much less assessing or responding to, these documents would require at least several weeks and consultation with technical specialists who are not necessarily available or willing to travel to Springfield. It is clear that neither we nor other members of the public have been provided with “reasonable” access to this material. It also appears that your agency knew when you designated your building in Springfield as the location where these documents were purportedly accessible to the public [that it] was encircled by a guarded fence, and that members of the public without ID were not permitted even to approach the building or speak with any TSA employee. Having fenced yourself off from the public, it is fraudulent[,] and compelling evidence of bad faith[,] to claim that this inaccessible building is accessible to the public. You cannot claim to have been taken by surprise. In our comments on the false and fraudulent claim in the NPRM that these documents were available for inspection at some unspecified DHS facility, we specifically raised the potential problem of access or entry without ID: “Access procedures are especially critical with respect to this proposed rule because ‘the class of persons affected’ – the relevant category pursuant to 1 CFR § 51.7(3), as quoted above – obviously includes individuals who do not have ID deemed compliant with the REAL-ID Act. “It is unclear what, if any, procedures have been established to enable individuals who do not have ID deemed compliant with the REAL-ID Act to obtain access to the relevant premises at ‘DHS Headquarters’, at whichever of the many possible locations that might be, to inspect the material proposed to be incorporated by reference into the proposed rule. “Individuals seeking to review this material can’t simply go the specifiedaddress, since no address is specified, even if they would be allowed in the door, which they probably wouldn’t.” We diligently sought to avoid these problems. We asked explicitly, in writing, in advance of our appointment, whether there was any protocol for access to this building by people without ID. We also asked for a point of contact and phone number we could call in case of access problems. We were provided with no point of contact or phone number, and were told of no ID requirement. Individuals may need a license from a government agency to operate a motor vehicle. They do not need a license to read the law or to travel by common carrier. If the TSA or any other agency proposed a rule to require ID to read the law or travel by common carrier, we and many others would oppose it, for multiple reasons. But no such rules have been promulgated. One of your staff asked me yesterday how I had traveled to the DC area, whether I had traveled by air, and whether I has shown any ID to do so. As a matter of principle and personal security, I do not wish to discuss my travel history, modes, or plans with you, and I am not required to do so. But the consistent position of your agency in litigation has been that no Federal law or regulation requires airline passengers to have, to carry, or to show ID. The responses by your agency to some of our FOIA requests confirm that, as you know, people fly without ID every day. “The public” includes individuals who do not have ID, and to be accessible to the public, documents or services need to be accessible to those without ID. After two hours waiting outside your gatehouse, getting increasingly cold, I was able to leaf through some of the non-public IBRs documents in the brief and inadequate time available. But the material I was allowed to look at was incomplete. Many of the non-public documents incorporated by reference in the Final Rule themselves incorporate by reference additional documents, some of them also non-public, which I was not shown. I have attached to this message, as a non-exhaustive example, the list of documents incorporated by reference in one of the ISO/IEC documents incorporated by reference in the Final Rule. By the explicit terms of the documents IBRd in the final rule, these *additional* documents constitute part of, are essential to understanding, and are included in the requirements of the documents IBRd in the rule. Just as the IBRd documents form part of the rule and have the force of law, all of the these documents IBRd in the documents IBRd in the rule form part of the rule and have the force of law. Compliance with these other recursively IBRd documents is required for compliance with the rule. All such documents, at any level of recursion, are included in our right of access and our request for access to all documents IBRd in the rule…. Presumably you have them in house, since by their own explicit terms they are essential to understanding, and form part of, the documents IBRd in your rules. If you don’t have them available for me to look at today, I request that you make them accessible to the public without my having to return to the DC area. We’ve been promised a response from the TSA by December 2nd. In the meantime, we have reported to the Office of the Federal Register that many of the documents incorporated by reference in the TSA’s mDL rule, including those recursively incorporated by reference, are not reasonably available from the TSA. The approval for incorporation by reference of these documents should be rescinded, and the rule should be withdrawn. Edward Hasbrouck Posted in Biometrics, Papers, Please, REAL ID, Secret Law 5 Comments",
    "commentLink": "https://news.ycombinator.com/item?id=42239952",
    "commentBody": "Do you need ID to read the REAL-ID rules? (papersplease.org)213 points by greyface- 22 hours agohidepastfavorite82 comments munchler 20 hours agoI really admire these folks for standing on a worthy principle. I also dig the performance art vibe of showing up at the TSA headquarters without an ID to read a deeply nested tree of paper documents about IDs. If you're going to joust windmills, these are some good windmills to joust. reply changoplatanero 20 hours agoparentWhy does the TSA building itself need so much security? Are they expecting it to be the target of an attack? reply echoangle 20 hours agorootparentIs that surprising? It’s a federal agency representing the security apparatus of the US. That’s a good target for terrorism. reply michaelt 18 hours agorootparentMost places I've worked, despite similar security needs, have met them in a much less... performative way. After all, the security gate is the first impression visitors get of your industrial facility, or office, or embassy, or whatever. You want it to look welcoming and secure at the same time. Projecting strength but not fearfulness. If you must use the cheap fencing, conceal it with some plants. Where the fence is visible, go for some nice decorative metalwork. Move the turnstiles into a lobby and put a reception desk and some couches next to them. Add some meeting rooms (and toilets and coffee facilities) at the security boundary, so job interviews and meetings with suppliers don't give them unfettered access. reply jfengel 1 hour agorootparentAn influential social media figure started naming federal employees who should be fired because their job is a waste of tax money. Some of those employees are now receiving death threats. The government doesn't generally spend a lot of money on things like \"decorative metalwork\". People get grumpy about it. reply bryant 14 hours agorootparentprevThe performance aspect is necessary when managing optics with a whole nation's population. Plenty of people have no idea what good security looks like, but they expect it looks like a lot of steps and many inconveniences. Corporations generally don't need to worry about this. reply cvadict 15 hours agorootparentprev> That’s a good target for terrorism. Exactly! Nobody would be laughing if Al-Qaeda drove a giant Dasani truck into TSA headquarters, would they? reply deprecative 10 hours agorootparentNot at first. They'd be too happy cheering. reply forgetfreeman 19 hours agorootparentprevRepresenting the security what now? Given the agency's performance over the decades I'd be deeply surprised to find out the TSA could credibly secure a mall parking lot. reply echoangle 19 hours agorootparentThat’s not the point of terrorism. If you blow up a TSA building, citizens get scared because it looks like „if the state can’t even protect itself, how are they going to protect me?“. reply pksebben 19 hours agorootparentthere are semi-competent arms of the US security apparatus that are not principally theatrical, that I would find shocking to be hit. The NSA comes to mind. If anyone ever hit the TSA I'd just laugh at their piss-poor targeting. Thanks for clearing out our dusty community theater, now maybe we can build an ice cream shop there or something else actually useful. reply bobthepanda 17 hours agorootparentMost federal agencies tend to be colocated. Of the 168 people who died in the Oklahoma City bombing, only 8 were from law enforcement. https://en.wikipedia.org/wiki/Oklahoma_City_bombing reply A4ET8a8uTh0 20 hours agorootparentprevTo be honest, I am genuinely surprised an attack never materialized. But then I also remember mentioning my thoughts on the matter to my wife, who was aghast that I would even consider such a scenario. Maybe, on average, people are actually decent and it is people like me, who come up with weird hypotheticals. reply potato3732842 19 hours agorootparent>To be honest, I am genuinely surprised an attack never materialized. I'm not. The TSA is hated by the populace the way the population hates every wasteful boondoggle jobs program. Foreigners hate them for profiling but that's pretty far down the list of grievances. Foreigners looking to strike at America and Americans looking to get off the porch likely have dozens of more preferable targets. I doubt the TSA makes any would be attackers list of top five most deserving agencies. Edit: A fed sponsored false flag attack on the TSA could make a good comedy plot. It might need to be a TV series in order to have time to fit in all the jokes, references, tropes and wise cracks you'd be obligated to make when covering such subject matter. reply wombatpm 18 hours agorootparentprevYou mean I’m not the only one who has wondered if you could etch a knife shape onto the back of an acrylic clipboard? Something that wouldn’t show up on X-ray, but could be punched out with little effort on the plane? My wife says it’s a wonder I’m not on the no fly list. reply harrall 20 hours agorootparentprev~60 airplane hijackings per year in the 70s. We’ve significantly reduced it down toWho's we? Al Qaeda reduced it down to ~4 per year because now passengers no longer assume to survive and act accordingly. You think hijackings decreased because the passengers are more likely to attack the hijackers now? I think it’s more likely that surveillance and intelligence is better now and most hijacking attempts get discovered before they are executed. reply Terr_ 18 hours agorootparentNot parent poster, but absolutely. Look at all those other pre-2001 hijackings, and ask yourself what would have happened if most of the passengers were terrified that the hijackers were preparing to destroy the entire plane and everyone on it, regardless of any demands being made or met. Would-be hostage-taking hijackers know it too: Their business plan, as it were, has been ruined for a generation by their suicidal colleagues. reply potato3732842 19 hours agorootparentprevAirline and security protocols are no longer \"comply with everything so that nobody gets killed\". Doors are locked. Passengers will likely bum rush you. There might be armed security on any given flight. The odds of success are unbelievably long compared to what they were in the 80s. Just not worth it vs a \"boring\" bombing or whatever. reply echoangle 20 hours agorootparentprevHijacking an airplane isn’t an attack on the TSA specifically though. We were talking about the security of the TSA building itself. reply HWR_14 19 hours agorootparentprevYes, but the TSA wasn't created in the 1980s, when most of the decrease happened. The TSA wasn't invented until after 9/11. reply emilamlom 20 hours agorootparentprevWell, they are the best at security theater, so it makes sense their headquarters is too. reply woodruffw 20 hours agorootparentprevDepending on your perspective on security theater, it might be appropriate to observe that a TSA building as exactly as much security as the TSA is capable providing itself. reply jrockway 19 hours agorootparentprevI think it's reasonable. Not the exact details of this installation necessarily. The reality is that there are a bunch of people that want to lash out at the government for whatever reason, and the civil servants that just want to do their job shouldn't be put in a dangerous situation by allowing those people to walk into their offices unimpeded. If you believe that the agency shouldn't exist, lobby Congress. Don't take it out on people that just want to do their assigned administrative work for 8 hours a day. Remember that someone was so mad at the IRS that they filled their airplane with gas cans and flew it into an IRS building, killing an employee: https://en.wikipedia.org/wiki/2010_Austin_suicide_attack Some security precautions are understandable in my opinion. (I don't think the ID requirement is reasonable. Take and store a photograph, deleted after 3 months, and make people go through a metal detector. Also, put the ID requirement documents online. It's free.) reply bobthepanda 19 hours agorootparentThere's also been significantly worse. 168 people died in the Oklahoma City bombing by a domestic terrorist. https://en.wikipedia.org/wiki/Oklahoma_City_bombing reply jrockway 17 hours agorootparentYup, exactly. That's what they're up against. I think it's reasonable to take precautions. I guess I'm getting downvoted for \"checking ID at the gate doesn't prevent you from flying your airplane into the building\" which is true, but we have to realize that most anti-government-inclined folks don't jump right to a terrorist attack as their first intervention. I do think that people hired to do a job deserve some protection from the general public while at their workplace. reply freedomben 20 hours agoprev> These mobile driver’s licenses (mDLs) will be issued by state driver’s license agencies, but the standards incorporated into the TSA rule require that they be deployed through smartphone platforms (i.e. Google and/or Apple) and operate through government apps that collect photos of users and log usage of these credentials. This is really disturbing in a number of different ways. It's bad enough to have the government requiring you to have a government-approved smart phone, but on top of that it's the logging and data analysis wet dream that authoriarian governments the world over could have only dreamed of. reply EvanAnderson 19 hours agoparentAlong with that: > mDL apps will be required to log each time a digital ID is presented, and to whom. This is described as a measure to protect ID-holders’ privacy, despite the obvious risk posed by police or others being able to know when and to whom you have shown your ID. That's down right horrifying. reply Spooky23 20 hours agoparentprevEven worse, the whole thing is a cash cow for Idemia and a couple of other companies, who probably alt wrote the secret rules to benefit their company and prevent competition. reply miohtama 20 hours agoparentprevIt's not a dream. China and India have been doing it for a while. Discussed earlier https://news.ycombinator.com/item?id=41608810 reply nritchie 19 hours agoparentprevNo need to get worked up. You can get a RealID compliant driver's license (plastic credit card sized item) or state ID (ditto) You don't need a smart phone. reply potato3732842 19 hours agorootparent>You can get a RealID compliant driver's license (plastic credit card sized item) or state ID (ditto) You don't need a smart phone. For now. You used to be able to pay for parking without downloading an app. You used to be able to buy dynamite without doxing yourself to the feds (something that you don't really think is all that important until you have to clear a lot of forested land and the reality of your options for dealing with stumps and boulders becomes clear). reply gscott 16 hours agorootparentThe Government kills a lot of people in various non-war wars around the world that create people who would like revenge because they saw their parents get burned to death, etc. by American made munitions. It might be part of becoming the dominant defense supplier. Our weapons are so complicated it takes a lot of American know how to keep them maintained and probably even fire them. This puts Americans everywhere with a conflict. We all get to pay with absolute security. If I killed people all of the time it would be hard to track who I killed, their living relations, even bystanders it makes sense to mistrust everyone. A few months ago going through a TSA line at an airport they were taking everyone's photo for biometrics and made it seem mandatory. I went along with it, not fighting it, maybe you can opt-out for a bit but eventually the Government always makes it mandatory. https://www.kxnet.com/news/top-stories/the-us-is-presently-i... https://www.palestinechronicle.com/let-him-never-be-forgotte... https://www.wsj.com/tech/cybersecurity/u-s-wiretap-systems-t... reply A4ET8a8uTh0 20 hours agoparentprevIt gets worse. In US most of the bigger corps institute some sort of means to authenticate you via cellphone, which means that if you want to be remote, phone is effectively a necessity ( which one usually does ). Only a year ago, it was still possible to avoid having to have a cell ( although that meant you had to be in person -- an interesting trade off in itself ). Anyway, I hate the now. reply TeMPOraL 20 hours agorootparentAt my last workplace, I somehow managed to get away with only Microsoft Authenticator on my phone, with no actual remote management capabilities enabled. That's pretty much exactly where I draw the line: if I have to have a device to perform work functions, the workplace needs to supply it. I'm not going to put work data on my personal machines, and I'm definitely not letting a third party root my phone for me \"for sekhurity\", and apply work policies on my personal device. I'm okay with work 2FA on my phone, but only without MDM, as an exception for where otherwise there's no reason for me to have a work phone. reply rootusrootus 19 hours agorootparentThese days a lot of folks can probably do more than just authenticator on their personal device. Teams and Outlook, for example, are both able to run with the MDM-level controls the company wants but without the device-level MDM. It's part of the app and has no control over anything else. reply HWR_14 19 hours agorootparentAnd, as a plus, your phone can now be subject to a subpoena issued to your employer! I don't want their data on my device for a variety of reasons. Loss of control would be enough on it's own, but there are others. reply henryfjordan 20 hours agorootparentprevWork with your IT dept. A company I previously worked for had a policy that if you had any company data on your phone, they had the right to force you to unlock it and look through it (not sure if they ever actually did but it was in the employee handbook). When IT tried introducing a system that required me to Auth with my phone I refused, citing the policy, and they helped me setup a workaround Yubikey. Might not be possible everywhere but worth a shot. Also always helps to make friends in IT. reply ArchOversight 19 hours agorootparentprevAsk for your corporation to give you a corporate phone. I have one for that reason, and it lives in the office alongside my work issued laptop. reply michaelt 18 hours agorootparentprevMost places can issue you with a physical token instead, like a Yubikey. It's just unusual, so the first line in helpdesk don't always know about it. And people seldom want to start a battle with the bureaucracy on their first day on the job... reply crazygringo 20 hours agoparentprev> It's bad enough to have the government requiring you to have a government-approved smart phone The government isn't requiring that. It's not forcing you to get a mobile ID. Your physical ID continues to be just fine. Mobile device ID's are simply for people who want the convenience of not carrying the physical one. Edit: geez, what's with the downvotes? I wasn't even defending anything. Just trying to keep things factually accurate here. reply philistine 20 hours agorootparentThink long-term. In 40 years, when the last paper IDs are discontinued, we'll all be tracked, but the time to complain was now. reply TeMPOraL 19 hours agorootparentprev> are simply for people who want the convenience That's the technological ratchet at work, as it has been since the dawn of humanity. A solution that's convenient or useful enough to gain wide adoption has a way of becoming a soft necessity, and eventually a hard one. Some examples that meaningfully affected our lives[0], in many ways not for the better: - An accurate clock / watch -- hard necessity. Good luck functioning in society without it. Opening hours, appointments, public transit schedules, are just few among many things synchronized in time, that expect you to have a clock so you can stay in sync too. And no, you can't get away with a rooster or a sundial, like you could 200 years ago - you need precision of at least a minute. - A car -- somewhere between hard and soft necessity, depending on where you live. The society expects you to be able to commute long distances in short time, for things like work, medical services, or government appointments. - Mobile phones, Internet, credit/debit cards -- soft necessities. You can sort of still live without them even in the big cities, but it's going to be a pain, as everything is optimized on the assumptions everyone owns a smartphone, has Internet access, bank account with a card, and increasingly often, means of contactless payments (think e.g. public transit). There's a reason even the poorest people without a roof over their heads still own iPhones, and it's not entertainment. - Government ID app, electronic IDs, other means to do official errands fully on-line - convenience for now. I feel they'll transition into soft necessities within next 10 years, simply because interacting with government is always very annoying, and those tools simplify that process and save you some trips. -- [0] - All in context of the developed/industrialized/western societies; of course this does not apply to societies that did not embrace a particular technology (yet). reply tzs 19 hours agorootparent> An accurate clock / watch -- hard necessity. Good luck functioning in society without it. Opening hours, appointments, public transit schedules, are just few among many things synchronized in time, that expect you to have a clock so you can stay in sync too. And no, you can't get away with a rooster or a sundial, like you could 200 years ago - you need precision of at least a minute. You might enjoy the short story \"Chronopolis\" by J.G. Ballard. It's set in a world where everything had been strictly done according to schedules to maximize efficiency, but it became too much and people rebelled and outlawed clocks. reply SuperNinKenDo 20 hours agorootparentprevUntil they aren't. reply tshaddox 19 hours agorootparentWell sure. Physical IDs are also not tiny government listening devices either. Unit they are. reply ethbr1 20 hours agoprevKudos to Mr. Hasbrouck, who I assume is the narrator, for putting feet to ground to demonstrate the lack of open access to executive branch law. You can't have a law, and also keep it secret. reply CamperBob2 20 hours agoparentIf there's any one lesson to draw from the last several years, it's that the executive branch can do anything they goddamn well please. reply nkrisc 19 hours agorootparentThis is nothing new. reply CamperBob2 2 hours agorootparentActually it is. Nixon, for instance, was confronted by members of his own party. reply JadeNB 19 hours agorootparentprev> If there's any one lesson to draw from the last several years, it's that the executive branch can do anything they goddamn well please. You can agree or disagree with the goals and priorities of the Biden administration, but surely their interactions with the legislative and judicial branches demonstrate that, despite the high concentration of power there, the executive branch definitely still cannot act unchecked unilaterally. reply RunningDroid 19 hours agorootparentIs this satire? You neglected to mention the president that has been convicted of multiple felonies reply beej71 17 hours agorootparentI think the immunity thing is more of an issue than any particular criminal past. reply kortilla 19 hours agorootparentprevHow is that related? The people electing someone despite their background doesn’t have any bearing on the power of the exec branch reply anigbrowl 15 hours agorootparentI'd suggest reading 'Mandate for Leadership' (better known as 'Project 2025') to get an ida of the intended scope of executive branch power under the incoming administration - specifically chapter 2 by Russ Vought, who has just been nominated back to his old job overseeing the Office of Management & Budget. https://static.project2025.org/2025_MandateForLeadership_FUL... Of course these aspirations will be challenged in court, but given recent jurisprudence I wouldn't bet on those challenges succeeding. reply JadeNB 18 hours agorootparentprev> You neglected to mention the president that has been convicted of multiple felonies I took \"the last several years\" to refer to the currently sitting president. Even including the incoming president still illustrates the point, I think; the power he is likely to wield will come not merely because he is in the executive branch, but because the interests of the executive branch that he represents will be, in many respects, aligned with those of the legislative and judicial branches as currently constituted. (The judicial decisions nominally gave enormous immunity to the president, but it is checked by untested language that leaves unspecified, and so presumably in the hands of the judicial branch, decisions about what constitute 'core powers' of the presidency. I don't think it is likely to be successful, but I see it as very much an attempt to render the judicial branch still necessary even while essentially captured.) reply kva-gad-fly 19 hours agoprevThis was intriguing to me: > One of your staff asked me yesterday how I had traveled to the DC area, whether I had traveled by air, and whether I has shown any ID to do so. As a matter of principle and personal security, I do not wish to discuss my travel history, modes, or plans with you, and I am not required to do so. But the consistent position of your agency in litigation has been that no Federal law or regulation requires airline passengers to have, to carry, or to show ID. The responses by your agency to some of our FOIA requests confirm that, as you know, people fly without ID every day. How does one go about this process? reply anticorporate 19 hours agoparentThe TSA is intentionally vague about this. Via https://www.tsa.gov/travel/security-screening/identification > The TSA officer may ask you to complete an identity verification process which includes collecting information such as your name and current address to confirm your identity. If your identity is confirmed, you will be allowed to enter the screening checkpoint, where you may be subject to additional screening. > You will not be allowed to enter the security checkpoint if you choose to not provide acceptable identification, you decline to cooperate with the identity verification process, or your identity cannot be confirmed. reply yibbix 18 hours agorootparentEvery time I’ve flown for the past few years, my ID fails to scan at TSA. Don’t know why but every time it’s happened, the TSA agent will just call over another TSA rep, they will look at it, and then say I’m good to go ahead. Never knew these other methods existed, I’ve never been asked to confirm my identity a different way. reply ehasbrouck 18 hours agoparentprevTSA video walkthrough of flying without ID: https://papersplease.org/wp/2021/04/08/tsa-posts-video-showi... TSA ID verification procedures (redacted) for people without ID: https://papersplease.org/wp/2018/05/08/tsa-releases-redacted... records of how the TSA decides whether to let you fly without ID: https://papersplease.org/wp/2016/06/09/how-does-the-tsa-deci... reply _fat_santa 19 hours agoparentprevMy mother forgot her ID once and she told me there are two ways: - They can verify identify through something like a credit card, bank statements, etc. - Even if you do not have this, they can phone someone that will verify your identity over the phone after getting some info from you. From what I heard you are only allowed to use this method twice per year. reply ADent 15 hours agoparentprevWatch the old folks in wheelchairs. Many of those people have no valid ID. reply ezfe 21 hours agoprevWhile I completely agree that any individual should have access to the laws and texts that govern us, I have a problem with: > “Access procedures are especially critical with respect to this proposed rule because ‘the class of persons affected’ – the relevant category pursuant to 1 CFR § 51.7(3), as quoted above – obviously includes individuals who do not have ID deemed compliant with the REAL-ID Act. These laws \"apply\" to platform makers who are attempting to create Real ID mDLs, not people who want a REAL ID in the abstract. Someone without a REAL ID cannot get an mDL, regardless of the text of these rules. reply ethbr1 20 hours agoparent1 CFR § 51.7(3) [0] is laying out the requirements by which a reference is eligible for being included in rulemaking? The 5 U.S.C. 552(a) [1] it modifies notes that \"Except to the extent that a person has actual and timely notice of the terms thereof, a person may not in any manner be required to resort to, or be adversely affected by, a matter required to be published in the Federal Register and not so published.\" Which seems to be a pretty broad definition of affected person. I'd certainly consider myself to be affected if in order to avail myself of one option of TSA identification for air travel I had to use an app that did... (reference not openly available) [0] https://www.ecfr.gov/current/title-1/part-51/section-51.7#p-... [1] https://www.govinfo.gov/link/uscode/5/552 reply ezfe 18 hours agorootparentBut that’s not what this is. None of this precludes regular REAL IDs so the class of people arguably is only people with REAL IDs who are interested in making them an mDL. (Well, and companies implementing the specification) reply akira2501 20 hours agoparentprevThese laws \"belong\" to the citizens. If the government is not applying it's rules correctly who is there to monitor that? Do I not have that _basic right_? reply Glyptodon 20 hours agoparentprevSomeone (with or without ID) may very much suspect that there are legal issues with gating any federal or governmental behaviors behind real ID, or not allowing open source Real ID mDLs or various similar things. reply dclaw 19 hours agoprevReal-ID is such a farce. I have had an ID since I was 15, and presented my birth certificate and ssn as a minor to do so. There is no instance where I am not \"real\". I have a US Passport that took less effort and paperwork to get than a Real-ID. I will never submit. reply SkyMarshal 19 hours agoparentUS Passports are RealID. reply tedunangst 18 hours agorootparentI have a drivers license and a passport, but the combination is insufficient to get a real id drivers license. reply beej71 17 hours agorootparentTrue. But at least in my state, the remaining documentation is trivial, especially if you have a DMV ID or driver license. reply eqvinox 18 hours agorootparentprevRealID seems to have added some verification requirements on US passports, but it doesn't look like it made US passports a form of \"RealID\". reply indrora 18 hours agorootparentThroughout the RealID farce, a passport has been a perfectly reasonable form of identification. It proves my identity as a US Citizen as much as it has to be. reply bentley 11 hours agorootparentThe passport card is also an option, one that’s small enough to fit in a wallet. It can be used to cross land borders, and to fly domestically (but not internationally). Unlike most driver’s licenses, a passport card doesn’t expose one’s address. This makes it a great form of ID to use in non‐airport situations as well. reply dcminter 21 hours agoprevhttps://archive.is/paqb8 reply amluto 20 hours agoprev [–] > These mobile driver’s licenses (mDLs) will be issued by state driver’s license agencies, but the standards incorporated into the TSA rule require that they be deployed through smartphone platforms (i.e. Google and/or Apple) and operate through government apps that collect photos of users and log usage of these credentials. This is utterly ridiculous, at least for driving. Anyone who needs to validate that someone’s driver’s license is authentic should be well-equipped to query the relevant state’s database and look it up. Just like how they would search for outstanding warrants, Amber Alerts, etc. With that in mind, surely it should be legal to drive with a photo of one’s driver’s license, a copy of one’s license, any app whatsoever that can display a license, etc. There is basically no security added by a fancy add by an approved contractor — at most they can do some “device posture” crap to sort of prove to a reader that the app thinks that the phone it’s on really does belong to the owner of the license, which is a silly form of security by overcomplication. If I want to pretend to be my friend, I can borrow their phone or their actual drivers license just fine. reply sowbug 16 hours agoparent [–] I'm with you in spirit, but I think the threat model they're trying to address is someone passing as you and needing only biometrics or a little personal information to succeed. Maybe your sibling looks enough like you, or someone acquired your ID and then alters their appearance to match. If your one true ID is a single physical token, then this threat is harder to pull off. This is also why IDs are not accepted when they expire -- if they were accepted, then my underage brother might take my old one to buy beer. I've been in the ridiculous situation of saying \"I know my ID has expired, but I assure you I have not. I'm still me.\" reply voxic11 15 hours agorootparent [–] IDs often are accepted after they expire. The TSA for instance will accept expired driver's licenses for up to a year after the expiration date. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "An attempt to access the TSA's new digital-ID rules revealed issues with the REAL-ID Act, as the rules are not publicly accessible and require ID for access, raising concerns about secret laws and due process.",
      "The TSA's digital-ID standards, derived from private documents, involve complex interactions between apps, devices, and government agencies, with privacy concerns over logged ID usage.",
      "The author argues that the TSA's practices violate public access rights and has reported the issue to the Office of the Federal Register, seeking the withdrawal of the rule."
    ],
    "commentSummary": [
      "The discussion revolves around the necessity of having an ID to access REAL-ID rules, with some users questioning the effectiveness of TSA's security measures.- Concerns are raised about the implications of mobile driver's licenses (mDLs) and the privacy issues they may present.- Users express skepticism about the effectiveness of TSA and the concept of security theater, while also questioning the long-term implications of digital IDs like REAL-ID and mDLs."
    ],
    "points": 212,
    "commentCount": 82,
    "retryCount": 0,
    "time": 1732567234
  },
  {
    "id": 42244814,
    "title": "A solution to The Onion problem of J. Kenji Lopez-Alt (2021)",
    "originLink": "https://medium.com/@drspoulsen/a-solution-to-the-onion-problem-of-j-kenji-l%C3%B3pez-alt-c3c4ab22e67c",
    "originBody": "A solution to the Onion Problem of J. Kenji López-Alt Dylan Poulsen · Follow 9 min read · Nov 14, 2021 -- Note: this post has a lot of background information, much like every recipe you find online. To see the solution, jump to the end of the post. I first became interested in the the problem of cutting onions in a way to reduce the variance of the volumes of the slices at a gathering with friends. One of my friends and colleagues, Dr. Gabe Feinberg, also a mathematician, pointed me to the Youtube video below. The origin of the onion problem. In the video, Chef Kenji López-Alt says he has a friend who is a mathematician, who claims that you should cut radially towards a point 60% of the radius below the center of the onion, and claims that this is related to the reciprocal of golden ratio, 0.61803398875… I was intrigued by this, and even began cutting onions at home with this technique, just because it made me happy. A post in the Washington College Mathematics and Computer Science Department Discord where I show the results of cutting onions in this manner. Each time I cut an onion for dinner, my mind would wander. I would think about why this is true, and what techniques I could use to approach the problem. While this was meditative for me, these musings did not lead anywhere substantial over the span of two months. Last weekend, my thoughts actually lead me towards a solution. Within two days I had found the “true onion constant”, which, spoiler alert, is not the reciprocal of the golden ratio. The depth to which you have to aim your knife for radial cuts depends on the number of layers. You can see this by thinking of how to cut an onion with one layer versus an onion with ten layers to keep the pieces as similar as possible. For one layer you would aim towards the center of the onion, but for ten layers you would aim somewhere below the center of the onion. To simplify matters, I therefore thought of an onion with infinitely many layers (or, as Gabe called it, “the great onion in the sky,” which I love). These kind of abstractions are common in mathematics, and make problems tractable. Once there are infinitely many layers, it makes sense to think of infinitely many cuts. This moves the problem into the realm of continuous mathematics, where calculus can be used to great effect. More discussion from the Departmental Discord about the problem. There is also proof that I’m a millennial with the use of the 😂 emoji. Here’s the technical part of the post. You probably need to know multivariable calculus to follow from here. I’m going to switch to using “we” instead of “I” to match mathematical writing conventions, and to indicate that we (you, dear reader, and I) are walking down this mathematical path together. First, we model the onion as half of a disc of radius one, with its center at the origin and existing entirely in the first two quadrants in a rectangular (Cartesian) coordinate system. This ignores a dimension, and perhaps also some geometry of actual onions (are cross sections actually circles?) but makes the problem tractable and is still a good approximation. The insight that leads to a solution comes from the Jacobian. When we change from rectangular coordinates to polar coordinates in integration, small rectangular pieces of area dx dy are transformed into small pieces of area r dr dθ, where x = r cos(θ) and y = r sin(θ). The idea of the Jacobian applies to all changes in coordinate systems. We can calculate the Jacobian as Below is a diagram showing the change of coordinates and the Jacobian in this setting. Notice that the coordinate system cuts the onion, much as usual grid lines cut the plane into rectangles in the Cartesian coordinate system. The radial part of the coordinate system cuts the onion radially (which of course nature does by default, but we need to model this mathematically), while the angular part of the coordinate system cuts the onion as our knife would if we were making straight cuts towards the center of the onion. Even though every piece of the onion is infinitely small (there are infinitely many layers, and infinitely many cuts) The Jacobian r dr dθ gives a measure of how big the infinitely small pieces are relative to each other. Pieces near the center of the onion are smaller than pieces near the edge of the onion, as we can see that since r is smaller towards the center of the onion and larger towards the edge of the onion. We can find the average value of the function f(r,θ) = r over the part of the plane that defines the onion to find the average weight of the infinitesimal area, A. Once we have the average, we can find the variance, v, of the weight of the infinitesimal area by calculating The variance is a good measure of the uniformity of the pieces. If the variance is large, the pieces are not very uniform, and vice-versa. The problem with this analysis, of course, is that we are cutting towards the center of the onion. We want to cut towards a point below the center of the onion. To accomplish this, we need a new coordinate system. We make a coordinate system for cutting towards a point a distance h>0 below the center of the onion. In this coordinate system, we measure the angle theta from the point (0,-h), while we measure the radius from the origin (0,0) (both points in the rectangular coordinate system). The radial part of the coordinate system cuts the onion radially from the origin as before, while the angular part of the coordinate system cuts the onion as our knife would if we were making straight cuts towards the point (0,-h), below the onion. The new coordinate system also cuts the onion, but in exactly the way we want it to in order to model the cutting of an onion radially towards a point beneath the cutting board. This coordinate system only works for the upper half plane, as there are now technically two points in the plane for a given point (r,θ). Luckily, our onion is entirely in the upper-half plane! In this coordinate system, the region of the plane that we model as the onion is defined by -arctan(1/h) ≤ θ ≤ arctan(1/h), and h |tan(θ)| ≤ r ≤1. We can eliminate the absolute value by using symmetry. Since the left side of the onion is a mirror image of the right side of the onion, and therefore both sides would have the same variance in area, we can perform this analysis just in the first quadrant. So, the region is defined by 0≤θ≤arctan(1/h), and h tan(θ) ≤r≤1. The relation between (x,y) and (r,θ) is less clear. Given we know r, h, and θ, we can draw the following triangle, with a new variable c which represents the distance from the point (0,-h) to a given point (x,y) (both in the rectangular coordinate system). First, using the law of cosines, we can calculate Using this, we can find the relationship between (x,y) and (r,θ) as From this, for a given depth h, we can calculate the Jacobian as This is, to put it mildly, complicated. Nevertheless, we have done fairly straightforward calculus computations to get here, which shows the power of making this problem continuous. Mimicking what we did before, given a depth h, we can find the average weight of the infinitesimal area, A(h), by calculating the integral of the Jacobian over the onion region divided by the integral of 1 over the same region And the variance of the weight of the infinitesimal area, v(h), is found by calculating the integral of the square of the Jacobian minus A(h) over the onion region divided by the integral of 1 over the same region Yikes! Integrating this by hand looks really difficult, if not impossible. We should use a computer to help us. Using the power of numerical integration in Mathematica, we can plot the variance versus h, the depth of the point we are cutting towards. We can see the minimum variance is around h=.55. We can use a numerical minimization technique to find the h that minimizes the variance. I am only confident of this number to 7 decimal points, but the “true onion constant” for the “onion in the sky” is given by 0.5573066… To get the most even cuts of an onion by making radial cuts, one should aim towards a point 55.73066% the radius of the onion below the center. This is close, but different from, the 61.803% claimed in the Youtube video at the top. Also, this number will be different for onions for finitely many layers (that is to say, all onions). Nevertheless, I find this answer to be beautiful, and I will forever treasure the true onion constant. I think it would be interesting to consider the effect of the number of layers on this answer. Since with one layer the best strategy is to cut towards the center, I suspect that the best depth h to cut towards increases from zero with one layer, with .5573066… as the upper bound on the depth. So, the best depth for an onion with ten layers would be somewhere between 0 and 0.5573066. I have not investigated this in depth, but this seems like a fun next step. I hope we all now know enough about onions to object. Comic from Li Chen https://www.exocomics.com/685/ Update: I actually was able to evaluate v(h) in a closed form. The techniques used to do it are really fun, and I am hoping to write them up for a recreational mathematics journal. As calculus students know, if you want to minimize a function, you should take the derivative and set it equal to zero. Here, the derivative of v(h) is given by The unique root of the above expression in the interval (0,1) is the onion constant, since it is a critical point for the function v(h) and the sign of v’(h) changes from negative to positive at this point, as seen in the graph of v’(h) below. The derivative of v with respect to h. Notice that the derivative crosses the horizontal axis around 0.55, which is the onion constant. The sign change from negative to positive means this is a minimum of the v function. With this, I can calculate the onion constant to arbitrary precision. Here it is to 1000 decimal places: 0.5573066929856644788510930591459271808320003020727327593398292131946981351272104586975295563488927792384215157297641443660261449855854165046873271472618959107816152780606384065758548635804885244580180000739444280590673621405484408743288174143897178500658897679049099235460450539966379793582365697832234771908624791276216076862484729083731336235000704236891376747519710815301807822317779086701048122723023915093054323298702150340065450327186756623642052156098646912508581593702205375240220768344875026631985363470644632525528856220691258227307037720900190873707797080215945078389222941122441664099620992665469305266348508835318836823451849946341751553954012216070423374355399193069992187951842347509926071534835419058678494025712006870992663407278202945110198402208378584410140122892631419360798953694134222761038423480438048889054739124583187162972867878589998414926409519790844390232917730134252343064728228633559834886507214553757974736357343027167265972675903577598983959532796594227162648681839040… Such a beautiful mathematical constant deserves a name. I choose to use the Hebrew character samekh, because it looks particularly like an onion.",
    "commentLink": "https://news.ycombinator.com/item?id=42244814",
    "commentBody": "A solution to The Onion problem of J. Kenji Lopez-Alt (2021) (medium.com/drspoulsen)177 points by fanf2 5 hours agohidepastfavorite67 comments yunwal 4 hours ago> First, we model the onion as half of a disc of radius one, with its center at the origin and existing entirely in the first two quadrants in a rectangular (Cartesian) coordinate system. Can someone explain to me why a half sphere (the shape of half an onion) can be modeled as a half-disk in this problem? Why would we expect the solutions to be the same? If you think about the outermost cross-sections at the ends of the onion (closest to the heel and tip of the knife), as you get closer and closer to the ends, you approach cutting these cross-sections more vertically. I'd expect that you'd have to make the center cross-section a bit shallower to \"make up\" for the fact that the outsides are being cut vertically. Idk, either way I think declaring this the true \"Onion constant\" is probably wrong. reply gus_massa 2 hours agoparentThe solution is later in the article. > The insight that leads to a solution comes from the Jacobian. It's not a unform half disk. It has more weight away from the Y axis. You can imagine it's painted with watercolors and you want to collect the same ammount of ink. In an uniform disk you have xx xxxx xxxx xxxxxx xxxxxx xxxxxx But in the weighted disk of the article the top and bottom are darker and the center lighter .. x..x x..x x..x Xx..xX Xx..xX Xx..xX but there are no strips like in my ASCII art, the shade changes slowly. reply Maken 2 hours agoparentprevHe's also ignoring that the layers of the onion become significantly thinner the farther away from the center they are. So this analysis is way off even for a perfectly symmetrical onion. reply dole 3 hours agoparentprevEven though onions aren't perfectly symmetrical, they still optimally grow or radiate out from one axis/line through the middle. Stick a toothpick through a sphere as this line, and slice the sphere through perpendicular to the axis, you'll get circles from a sphere, or half-disks from a half onion if you keep slicing perpendicular. I'm lazy and cut my onions perpendicularly through halves, and don't try a radial cut for uniformity. reply yunwal 3 hours agorootparent> Even though onions aren't perfectly symmetrical The question I have is not about modeling an imperfect object as a perfect abstraction, it's about modeling a 3D object as a 2d object, and assuming that the optimization still holds. I think it's pretty plainly clear that it doesn't. Think about some cross-section of the onion that's closer to you and smaller than the center cross-section. Let's say it's of radius 0.25 instead of 1. The slices you take of it will be much more vertical than the center slice. This changes things. My intuition tells me it means the optimal solution is shallower than the solution found here, since you'd want the \"average\" cross-section to follow this constant. reply sgc 1 hour agorootparentThe author dealt with this outside the article, and posted a link to his slides in this HN post. The relevant slides begin at [1]. At the end of the day a straight cut is limiting. The next step would be to design the perfect onion dicing knife. [1] https://drspoulsen.github.io/Onion_Marp/index.html#44 reply stonemetal12 1 hour agorootparentprevI believe you are supposed to calculate R*0.55... once for the max onion radius and use the same cut on the smaller disks. That way the smaller disk is cut identically to the inner part of the larger disk. reply yunwal 1 hour agorootparentThe same cut (in terms of angle) on smaller disks would be impossible with a real knife. You'd have to bend the knife in order to achieve it. reply StrangeDoctor 2 hours agorootparentprevHaven’t had enough coffee to think about this rigorously. My intuition says that as long as you could get to the desired 3D shape from revolving the 2D shape around an axis, essentially integrating the area into a volume, the results will be valid or equivalent. I don’t think that’s the entire story, there are probably other ways to simplify 3D shapes. And yes, onions will have non constant variations (or ones that don’t cancel out to 0) along the sweep which is what actually invalidates the real world application. reply jameshart 1 hour agorootparentIf you model the (half) onion as a stack of these slices, it’s clear that the radius of each slice varies over the height of the onion; so the points below the onion found by this method towards which you need cut will form a curve, not a straight line. That is hard to accomplish with a straight knife that makes planar cuts. reply dylan604 2 hours agorootparentprevIsn't that where calculus and intergrals come into to play? As the radius approaches infinity type of stuff? reply dhosek 3 hours agoparentprevFor a moment, I thought that “the onion problem” related to some challenging issue of topology or group theory, before my brain finally sorted through its connections to identify Kenji Lopez-Alt as a chef and not a mathematician. reply glompers 2 hours agorootparentJ. Kenji Lopez-Alt _was_ actually mentioned (featured?) in alt-weekly The Onion this month. The problem, though, was that it was in an un-funny piece about the beef dimension, and it is not worth footnoting here. I guess they should have researched this 2021 article and spun off of it instead. But maybe a Quanta Magazine and infowars joint venture could enter the beef dimension. An onion with too many alt-layers. reply giraffe_lady 27 minutes agorootparentprevHe's not a chef either he's a food writer and recipe tester. I don't mean this as disrespect at all just they are very different professions, using different skills and producing different outputs. reply Cerium 3 hours agoparentprevI share simular concern, but also think of an onion more as a bulging cylinder due to center weighted thickness variation in layers. Each layer extends from root to stalk. reply drspoulsen 2 hours agoprevHi everyone, the author of the blog here. I'm glad to see the interest here on this piece! I have slides that detail the problem setup and the mathematics, as well as a consideration of three-dimensional onions, here: https://drspoulsen.github.io/Onion_Marp/index.html I have submitted a formal write-up of the details of the problem and the solution to a recreational mathematics journal. I'm also happy to answer any questions about this! reply timClicks 1 hour agoparentI love how deeply nerd-sniped you have been by this topic. It's wonderful to be able to observe your delight in solving this. Thank you for sharing. reply andrewmcwatters 1 hour agoparentprevI feel like mathematics and many other rigorous field-friends have tons of great questions like this that are ripe for fun research. Thanks for publishing this and contributing to that world of curiosity! reply ortusdux 3 hours agoprevOn the other hand, fellow food youtuber Adam Ragusea swears by the importance of heterogeneity. Optimizing for uniformity might not be the best strategy! https://www.youtube.com/watch?v=5cWRCldqrxM reply stickfigure 9 minutes agoparentI literally came in here just to make this comment. Like Ragusea, I prefer every bite to be slightly novel and different. One of my favorite hacks for Ceasar Salad: Take a bag of packaged croutons, put it flat on the table, and crush it with the bottom of a pan. Repeatedly. Until you get a mix of various sized crouton chunks, gravel, and dust. Apply to salad. I ate a Ceasar this way in some fancy restaurant and I've been making it that way ever since. reply jfactorial 2 hours agoparentprevWell the logic presented in that video certainly cannot be argued against. reply andrewmcwatters 1 hour agoparentprevI remember reading about the consistency of cuts from classically trained chefs. I think Adam Ragusea has a lot of niche, quirky practices that don't align with actual profession. He's more of a culinary advocate in the same way that Bill Nye is a science advocate. They're not professional chefs or scientists. reply OJFord 3 hours agoprevIs the problem explained in text anywhere? (TFA delegates to a video and afaict only discusses another video-suggested solution and a novel solution in text, I don't understand what we're solving.) reply ipsento606 2 hours agoparent> Is the problem explained in text anywhere the problem is that you want to cut up an onion in such a way as to minimize variation in the size and shape of the cut-up pieces usually, so that the pieces will cook evenly reply dylan604 2 hours agorootparentmeh, the food processor usually handles that for me pretty damn well reply marssaxman 2 hours agorootparentYou are clearly not the target audience. reply sleepybrett 1 hour agorootparentprevyes, atomization is certainly one strategy, though often people enjoy onions that are not a slurry. reply dylan604 1 hour agorootparenti think you are confusing a food processor and a blender. a food processor has other attachments/blades that do not result in a puree. reply ruds 3 hours agoparentprevYou would like to slice (half) an onion in a way that minimizes the variance in volume of the pieces. The problem is then simplified to slicing half an onion in a way that minimizes the variance in cross-sectional area of the pieces at the widest part of the onion. reply dfxm12 1 hour agoparentprevThe problem is \"I have an onion that is spherical with even layers. How do I cut it into pieces with equal volume?\" It's more of a geometry thought experiment than a practical epicurean \"problem\". reply sampo 3 hours agoparentprev> Is the problem explained in text anywhere? Not very well. There are some snippets: \"to keep the pieces as similar as possible\" \"The Jacobian r dr dθ gives a measure of how big the infinitely small pieces are relative to each other\" \"The variance is a good measure of the uniformity of the pieces.\" reply ska 3 hours agoparentprevThe problem is how to get roughly equal sized pieces from cutting an onion. If you cut towards the center the inner pieces are much smaller than the outer. reply CarVac 3 hours agoprevI'm surprised Kenji still does the horizontal cut at all. With the angled vertical cuts I find the horizontal cut entirely unnecessary. (Also a few years back I gave myself a nice flap avulsion doing the horizontal cut in an onion...) reply ImPostingOnHN 3 hours agoparentInvest in cut-resistant gloves. The few dollars will pay for themselves in non-lost time, plus you can use them on a mandolin. NB: maybe stick a hotdog in one of the fingers to test it first. reply bloopernova 2 hours agorootparentMy standard housewarming gift is cut gloves and a pack of nitrile gloves to put over them. The nitrile gloves are so you don't have to wash the cut gloves so often. reply CarVac 3 hours agorootparentprevI have them now, but's simpler to just avoid that one dangerous and unnecessary cut that proceeds towards my body instead. They taught that in Scouting, never cut towards yourself. reply Kototama 2 hours agorootparentYou need to cut in the direction of your body in some cases (for example when carving wood). Two things to prevent injuries: a) never put any force if the material resists b) do it slowly. reply dylan604 2 hours agorootparent> for example when carving wood I've watched a lot of shows about the tools used for building log cabins in the pioneer days. I don't even know the names of them, but the tool for taking the bark off the tree by pulling the knife to you as you sit on the log is crazy. Also, the one where you straddle the log and swing the blade towards you between your legs is right up there too. Yet, I can't think of any way of making them better without using power tools. reply jcoby 2 hours agorootparentDrawshave or drawknife and adz. The drawknife is the safer of the two by far. It’s fairly hard to cut yourself when your whole body is moving the same direction. Similar to using a paring knife in your palm facing your thumb. The adz however you just have to have good aim or pay the consequences! reply JamesSwift 2 hours agorootparentHaha, jinx : ) reply JamesSwift 2 hours agorootparentprevDraw knife. As long as you are leaning instead of pulling its relatively safe. Same as its safe to pare by contracting your hand muscles instead of pushing a knife toward yourself. reply bloopernova 2 hours agorootparentprevAnd either learn to sharpen your knives yourself, or take them to a sharpening service. Dull knives require more force, and slip/catch more, so are more dangerous. reply resource_waste 3 hours agorootparentprevAnother thing to get out, another thing to clean, another thing to put away. All because we want to chew less. (I suppose nice texture too) reply ImPostingOnHN 2 hours agorootparentJust always keep them on and never wash them. Bonus: immunity to papercuts forever. reply ackbar03 4 hours agoprevYou want even cuts you throw it into a blender reply karaterobot 3 hours agoparentBesides a dice that's as even as possible, the other requirement this solution attempts to satisfy is using the minimum number of cuts. A blender doesn't satisfy that, as it's making hundreds of cuts. Then, when you present your solution to the client, you find out there was a third, unspoken requirement: that it should involve as little cleanup as possible, which the blender also doesn't satisfy. The user researcher was on vacation, and you didn't find out about this before beginning design. Damn! The blender solution turns out to be overoptimized on a single requirement at the expense of the others. reply Hikikomori 1 hour agorootparentThey're optimizing for time as knife cuts = time. A food processor will do it faster if you're more than one onion or so, assuming you can get the size you want. reply karaterobot 1 hour agorootparentAhh, so in addition to having trouble getting consistently-sized pieces the size of a dice or chop, the other reason knives are preferred is that a food processor damages the onion, releasing more water compared to a knife. The result doesn't caramelize as well. This is why higher-end restaurants cut onions by hand, even when operating at scale. reply greenpresident 3 hours agoparentprevThat’s the engineering solution. You could also hire two interns to do it layer by layer, call it the consultant‘s solution. reply octocop 3 hours agorootparentThen it seems you need consultants to get a guide Michelin star reply selimthegrim 3 hours agorootparentThat sounds like a cost plus defense contract if there ever was one reply Maken 3 hours agorootparentprevThe consultant solution would be to buy precut onions, so cutting perfect slices becomes someone else's problem. reply 1propionyl 3 hours agorootparentprevIf you want an extremely fine and even brunoise that's exactly what you do. reply ska 3 hours agoparentprevThat’s really not true, unless you are really mincing it or making a paste . reply ImPostingOnHN 3 hours agoparentprevA blender will make the bottom layer into paste before the top is touched. If you want to toss the paste into a skillet and caramelize it, that'll make a good sauce. Food processor might be better, but still won't be even. Source: I cook onions a lot, and am lazy. This article is great! reply Hikikomori 1 hour agorootparentAlways bust out the food processor when making soffritto or similar very small dice. Can do onions quickly and even with the method but carrots and others take quite some time. reply laweijfmvo 3 hours agoparentprevtwo words: Slap Chop. reply dessimus 2 hours agoprev>To get the most even cuts of an onion by making radial cuts, one should aim towards a point 55.73066% the radius of the onion below the center. This is close, but different from, the 61.803% claimed in the Youtube video at the top. Wife walks into kitchen with 3447 cut onions in piles: \"What are you doing?!\" This guy: \"I just cannot get these onions cut to a point 55.73066% below the origin! The best I have achieved is only 2 significant digits of accuracy.\" Wife, mumbling: \"Maybe that's why Kenji said: 60%...\" reply wcfrobert 1 hour agoparentMathematicians are wired differently than engineers. To us engineers, e is approximately pi is approximately 3. reply momoschili 54 minutes agorootparentthink you got it the wrong way bud reply balls187 1 hour agoprevOff topic, but the incorrect capitalization of the article misleads one to believe this is related to “The Onion” rather than “an onion.” reply mehlmao 1 hour agoparentI saw the headline and assumed it was going to be about the Beef Dimension: https://theonion.com/kenji-lopez-alt-returns-from-beef-dimen... reply jfengel 4 hours agoprevNot \"The Onion\". The original capitalization is \"the Onion Problem\", i.e. the problem of dicing onions into even pieces. reply kunwon1 4 hours agoparentI was confused, especially considering this [1] is still very recent [1] https://theonion.com/kenji-lopez-alt-returns-from-beef-dimen... reply dole 4 hours agorootparentI also thought I had my finger on the pulse of some The Onion/Lopez-Alt beef, like TMZ on the Food Network. ontopic edit: I am interested in an optimal onion cutting technique, while I'm happy with mine, the upside-down banana teaches that there's always a few ways to approach and learn something. reply jghn 3 hours agorootparentprevThis is exactly what I thought of when I saw the headline! reply JimmyWilliams1 4 hours agoprev [2 more] [flagged] perihelions 4 hours agoparent [–] Please don't LLM spam on here. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The post addresses the \"Onion Problem,\" which involves cutting onions to achieve minimal variance in slice volumes for uniformity.- A mathematical exploration using calculus determined the optimal cutting point is 55.73066% below the onion's center, termed the \"true onion constant.\"- This finding refines a previous suggestion of cutting towards a point 60% below the center, providing a more precise method for even onion slices."
    ],
    "commentSummary": [
      "The discussion explores a mathematical model for cutting onions evenly, inspired by J. Kenji Lopez-Alt, using a half-disk representation to simplify the geometry and minimize slice size variance.- Users debate the model's practicality and accuracy, considering the onion's 3D structure and non-uniform layers, and suggest alternative cutting methods or tools.- The conversation also considers the balance between uniformity and culinary preferences, referencing food personalities like Adam Ragusea."
    ],
    "points": 177,
    "commentCount": 67,
    "retryCount": 0,
    "time": 1732621091
  },
  {
    "id": 42239607,
    "title": "Redis is trying to take over the all of the OSS Redis libraries",
    "originLink": "https://twitter.com/TomHacohen/status/1861137484249252093",
    "originBody": "Looks like Redis is trying to take over the all the OSS Redis libraries.Jedis, Lettuce, and redis-py are down, they are now threatening redis-rs (link in reply).— Tom Hacohen (@TomHacohen) November 25, 2024",
    "commentLink": "https://news.ycombinator.com/item?id=42239607",
    "commentBody": "Redis is trying to take over the all of the OSS Redis libraries (twitter.com/tomhacohen)150 points by tasn 23 hours agohidepastfavorite81 comments bradgessler 12 hours agoI worked in a huge docs website a while back and I’ll never forget the day I saw a request from a Redis lawyer that insisted we called it: Redis™* *Redis is a trademark of Redis Labs The dumb thing is they insisted the footnote be on the first webpage Redis™ was displayed, as-if I had control over the order of the pages people viewed on the website. Since then I’ve started removing Redis as a dependency from as many projects as I can. reply Grimblewald 7 hours agoparentYup, technologically illiterate and developer hostile behaviour like that is your canary in the coalmine going belly up. If you stick around after that, you have only yourself to blame for what follows. reply deathanatos 20 hours agoprevTwitter essentially hasn't loaded for me since Musk took over, so: https://archive.is/c5lnW or, > Looks like Redis is trying to take over the all the OSS Redis libraries. > Jedis, Lettuce, and redis-py are down, they are now threatening redis-rs (link in reply). There's no link in the post, and I don't have a link to the referenced reply, but I'm guessing the link is https://github.com/redis-rs/redis-rs/issues/1419 reply tredre3 19 hours agoparentUse xcancel instead of archive.is and you will see the replies. https://xcancel.com/TomHacohen/status/1861137484249252093 reply JeremyNT 5 hours agorootparentOr nitter https://nitter.poast.org/TomHacohen/status/18611374842492520... reply jwilk 4 hours agorootparentIsn't xcancel.com a Nitter instance? reply toxinu 17 hours agorootparentprevDamn it, thank you for the link, sir. Can we please make this SNS disappear? reply tveita 7 hours agoparentprevI didn't realize Jedis was an official Redis client now, looks like it has been since 2020 without any major impact to the development. I guess they are unlikely to add any features to support alternative servers. reply EdwardDiego 18 hours agoparentprevWhat does it mean by \"redis-py\" is down? It's still on Pypi? reply gurchik 14 hours agorootparentI think they meant to say the maintainers for redis-py, lettuce, and jedis have already relinquished control to Redis Inc. The libraries are still online, but their repositories have been moved to the official Redis organization and are \"controlled\" and \"owned by Redis\" according to an email quoted in the linked Github Issue. reply EdwardDiego 9 hours agorootparentThanks, that makes more sense of the \"down\" thing. reply greatgib 18 hours agoprevI'm obviously not a lawyer, but from what I can see, a project like redis_py is way older than any trademark registration from Redis lab. If I'm not wrong looking at the trademark, database, they have registered them recently (2018,2019, and a lot very recently 2023) in a very predatory move. In my opinion, this usage should be considered legitimate based on prior art or something like that. Anyway, in all case I hope that projects will not give up to this asshole move from redis lab. Maybe it might be possible to rename the project, but also forbid redis lab to use the name of the projects it had targeted. Like a poison pill. I think that it is legitimate for them to consider that an usage of name s like redis_py by Redis would be counterfeiting of well known brand of these projects. In the end, it might be assumed that it is the fault of Redis to have not enforced any copyright or branding on these projects for more than 10 years despite the fact that they couldn't have ignored their existence and success. I'm even quite sure that we can find a publication of Redis lab with how-to use any of these open source projects... reply justin66 12 hours agoparent> Anyway, in all case I hope that projects will not give up to this asshole move from redis lab. How much are you willing to pay to finance their attorneys? reply mst 1 hour agorootparentLegal bullying like this does not necessarily indicate a willingness to actually sue. Whether somebody's comfortable finding out is, of course, a different matter. reply greatgib 10 hours agorootparentprevProbably something up to 50$. Redis lab is not that big as a company so they might not be able to hire top notch lawyers. But regarding the history of free software that is made of individuals fighting the overreach of predatory corporations, I will be very saddened if everyone of all these projects would give up... Maybe the fsf or software conservancy can assist some of the projects if they were to ask it? reply justin66 38 minutes agorootparent> Probably something up to 50$. You're really making a difference. > Maybe the fsf or software conservancy can assist some of the projects if they were to ask it? Maybe they'd prefer to change their project name and continue to develop software, rather than change their focus to this stuff. It is almost certainly what they are good at and what they care most about. reply choilive 3 hours agorootparentprev> Probably something up to 50$. $50 might get you 5 minutes of an attorneys time. For someone to fight this in court will in the hundreds of hours of billable time. None of these open source client library maintainers is likely going to be able to fund that. > Redis lab is not that big as a company so they might not be able to hire top notch lawyers. Redis is a company with a billion dollar valuation that has raised hundreds of millions of dollars, they can afford good lawyers. Prior art for redis_py clearly belongs to Redis, regardless of when the trademark is actually filed. This is because Redis had to have existed before redis_py. reply mst 1 hour agorootparentThere are organisations like the Software Freedom Conservancy who, even if they themselves can't help, can probably help you try and find somebody who can. I'm not suggesting any particular maintainer *should* decide to fight, or even consider whether they want to try - that's very much up to them - but assuming that fighting is untenable in advance without exploring options is something I'd consider an unnecessarily unfortunate outcome. reply justin66 24 minutes agorootparentContrariwise, to assume that fighting will cost time, money, and focus is almost certainly correct. I put myself in the shoes of one of these devs and I really think these it's not my money-themed suggestions would just make an annoying situation more annoying. reply 0cf8612b2e1e 20 hours agoprevIs this going to become a trend? Project starts making trademark claims years after a downstream library has been using it without issue. Bit rich, given how accessible libraries can bootstrap an initial product ecosystem. Is it ever safe to use a trademark name? Will all packages now have to be generic enough to imply the product for which they connect? It”s not a Java/AWS/Redis/Wordpress/Shopify extension, but a thingbob, which just so happens to connect to That-Which-Shall-Not-Be-Named. reply SAI_Peregrinus 18 hours agoparentYou can mention a trademark. You can't use the trademark in the name of your own product. Even in cases where you're legally OK (e.g. mentioning your product is better than , or is made for ) they can still sue you, though it'll be much easier to win the case. reply eurleif 18 hours agorootparentThis is called nominative fair use. But Redis's own guidelines for nominative fair use acknowledge that there are cases where you can \"use the trademark in the name of your own product\": https://redis-doc-test.readthedocs.io/en/latest/topics/trade... >you may only name it \"XYZ for Redis™\" However, I'm not sure where courts would land on a name like \"redis-rs\". There are arguments on both sides, and I'm not sure if there's enough case law to make the legal outcome clear. reply bhouston 16 hours agoprevI suspect Redis is going to pull in the various popular Redis libraries under their umbrella and make them incompatible with Valkey. It seems like a defensive move against Valkey. If I was in charge of any of these libraries, I would replace any mention of Redis with Valkey and move on. reply OPoncz 10 hours agoparentSure. You can do that. But the community needs a single standard. Open standard that all in-memory data stores like Redis, ValKey, DragonflyDB, KeyDB, Kvrocks will support. Otherwise you will end up in a vendor lock. reply bhouston 4 hours agorootparent> Otherwise you will end up in a vendor lock. This would be in the interests of the current market leader though - eg. Redis. reply nurettin 13 hours agoparentprevAnd what prevents valkey devs from maintaining their compatibility layer to accommodate those changes? reply exe34 5 hours agorootparentthat's just asking to waste your life forever catching up. reply asciii 1 hour agoprevSo, after reading the Github[1] comments, Redis needs a rust client library. It's hilarious to watch the legal and PM figure this out. No strategy. Just trying to solve \"enterprise\" needs because I'm sure someone asked for a rust client they'd pay over the moon for. [1] https://github.com/redis-rs/redis-rs/issues/1419 reply mst 1 hour agoprevOh dear. I see Redis Inc. have decided to go full Nagios. Never go full Nagios. Though admittedly Nagios' attempt to pull similar assholery wrt CPAN did end up being a source of some amusement to me: http://p3rl.org/Nagios::Plugin I hope the http://crates.io team react similarly. reply ac130kz 14 hours agoprevI switched to Valkey, and not looking back. It's actively maintained and compatible with Redis so far. reply mguijarr 11 hours agoparentKvrocks (under Apache foundation umbrella) is another interesting Redis-compatible alternative, this one is targeting memory and SSD/NVME storage since it is implemented on top of RocksDB. So depending on your needs it may be a more efficient choice compared to Valkey. reply pledess 4 hours agoprevThere is a new comment by antirez in the past few minutes: https://github.com/redis-rs/redis-rs/issues/1419#issuecommen... reply angry_octet 20 hours agoprevThis serves as a lesson when naming projects -- call it something different to the core product. If you must have a package called ${name}-rs make it a wrapper around your repo. reply the_mitsuhiko 10 hours agoparentWhen I started using Redis, Redis was a true open source project. There was no company yet. reply Attummm 18 hours agoparentprevsuffix the language name plus name would give us redis-py. Which in turn would create the filename redis_py.py Which would immediately raise the question why not a dot. redis.py > There are only two hard things in computer science: cache invalidation and naming things. reply wmf 17 hours agorootparentLawyers won't allow any of those. It has to be something like Client for Redis™ in Python™. reply amszmidt 11 hours agorootparentTrademarks don’t (usually) cover file names and function names. See for example the classic GNU Emacs tetris.el debacle… reply mabster 9 hours agorootparentprevAnd 3. Off by one errors. reply Aeolun 19 hours agoparentprevYou can’t do that and expect it to be used. When someone needs a redis client library they search for ‘redis’ not ‘thingamabob’. reply eviks 14 hours agorootparentSince search isn't limited to names, but includes description, they will still find it reply talkin 7 hours agorootparentAll true, but lets be honest: For the technical users searching a library, nothing beats having The Keyword being part of the name. reply ramon156 19 hours agorootparentprevLots of projects do it. I look at starts and update frequencies. My toml parser is called taplo, for example. reply nybcvc 20 hours agoprevTrademark claims are also defended by the otherwise useless PSF. Here GvR objects to someone continuing Python-2 uńder the name \"python-2.8\" as well as \"py28\". Informally, bringing in lawyers was threatened at one point: https://github.com/naftaliharris/tauthon/issues/47 The project was renamed to \"tauthon\". reply echoangle 20 hours agoprevI would love for someone to fight out these bullshit trademark claims. Are you seriously going to win against a crate author for using your business name after letting them do it for 10 years and even featuring and recommending them on your own website? (Referencing https://github.com/redis-rs/redis-rs/issues/1419 ) reply CM30 7 hours agoparentI've sometimes wondered if it would be possible to have a voluntary organisation funded by developers and other tech folks whose sole purpose is to fight legal battles against companies and other organisations looking to bully contributors on dodgy legal grounds. I'm sure a lot of these legal threats would go away very quickly if the possibility of a large scale legal battle was on the horizon every single time. Or at least, they'd be a lot more willing to settle quickly if their opponent was fairly evenly matched resources wise. reply 0cf8612b2e1e 19 hours agoparentprevWho is going to pay to defend? The rust author is making it clear they want no part of this legal mess. Which is going to be true for nearly any open source contributor. The only people with legal representation are going to be the VCs looking for value extraction. reply echoangle 19 hours agorootparentYes, that’s the problem. I understand the perspective of the maintainers but it leads to a situation where the large corporation can do illegal stuff and bully people around because it’s too much effort to fight back. reply solardev 14 hours agorootparentThat's basically our entire society... reply dartos 13 hours agorootparentprevHey welcome to the legal system! reply rty32 6 hours agorootparentprevFSF? reply nrvn 7 hours agoparentprevLet them fail. Does anyone today remember SCO chasing virtually everyone on Earth after unix licensing violation and copyright infringement claims? There is no shortage in KV stores with similar properties so if redisinc. wants to continue their path without delivering actual value to the customers especially to enterprise customers willing to spend money they know exactly where they will land in several years from now. reply ojosilva 11 hours agoparentprevI have to thank Redis Inc. for bringing on this shitshow. I had no idea about Valkey and now I do. Corporate hats should consider the loss of general trust and negative marketing their actions produce. And yeah, I wish FSF or some other, strong and well-funded org would be more \"broken glass\" and take issue with even the tiniest of legal fights over OSS. reply znpy 19 hours agoparentprevThis looks a lot like the wordpress issue… i think the oss wars of the 2020s will be fought around trademarks? One more reason to move to valkey i guess. reply tayo42 12 hours agoparentprevThat issue looks reasonable, there isnt anything about trademarks there, why bring it up? It looks to be about providing support to paying users of Redis in a library that doesn't have much goals. And they offered to pay for it to do it. reply echoangle 11 hours agorootparentDo ctrl+f for „trademark“, there are currently 12 occurrences. Here’s what the Redis representative wrote: > These are not my words nor the message I want to convey. In a private call, I said that it feels strange to use the brand name for a library that keeps supporting all the forks indefinitely and does not grant compatibility for the product itself. I also said that it does not make a lot of sense to pretend to rename a crate and replace it with the same software or an alternative library, thus creating confusion and frustration. However, companies do consider protecting their trademarks where their reputation is challenged. This does not need any explanation. In my private call with Armin, I highlighted that. Of course, there is no plan to take action regarding the trademark, but to find a reasoned and agreed-upon way forward to avoid introducing another Redis Rust client library where changes, improvements, and innovation can be introduced. So basically the point is to make thinly veiled threats of copyright enforcement to „agree to a solution“. The Mafia also agrees with you on a payment plan after highlighting that a fire would be a shame in your establishment. reply tayo42 3 hours agorootparentHe published the emails where it says they want to own the library so they can set the road map for it, they're offering to pay for it.That quote even says they're not looking to take action over the trademark. Your response is alittle over emotional. reply echoangle 3 hours agorootparentI just quoted the part where the Redis representative describes a private call. How does the content of the emails negate that? > That quote even says they're not looking to take action over the trademark. Yeah, he’s basically saying „also, we think you’re infringing but we won’t have to enforce that, right? I’m sure we can come to an agreement, wink wink“. Maybe I’m reading this completely wrong but this reminder is basically a threat to sue you to death if you don’t comply. reply tayo42 2 hours agorootparent> Maybe I’m reading this completely wrong IMO you are because your attempting to \"read between the lines\" and add things that aren't there while ignoring things that contradictory to what this hypothetical lawsuit would include. He says they don't want to cause a mess of renaming, or forking because it would be worse off and confusing for everyone reply RadiozRadioz 20 hours agoprevWhile it's difficult to compare the pettiness, this is definitely sillier than the WordPress thing as far as trademark enforcement goes. Trademark-as-a-Weapon is becoming too common now. Are we going to be forced to use legally distinct (and confusingly different) names for all our libraries now to defend against this nonsense? Pick projects based on the brand protection posture of the licensors? What a waste of time. reply wmf 20 hours agoprevAt least they offered to buy it. reply Aeolun 19 hours agoparentIs that any different from someone offering to buy a browser extension to add malware? reply dartos 13 hours agorootparentYeah, if they don’t add malware. I doubt redis will add malware to specifically their rust client. OSS authors can add malware too. reply whs 4 hours agorootparentThe risk here is they intentionally make it incompatible with Valkey, similar to how Elasticsearch client libraries complain when a newly added slogan header is missing reply wmf 1 hour agorootparentThat's almost guaranteed. Valkey is going to have to fork and maintain their own client libraries. reply tanduv 19 hours agorootparentprevthe library would most likely remain OSS even after the take over reply KomoD 7 hours agorootparentWhat makes you think that? reply foobarian 20 hours agoprevSic transit gloria mundi reply andrewstuart 8 hours agoprevWhat does Redis do that you can’t do trivially with something else? Postgres or SQLite for example. reply aneutron 7 hours agoparentIt's akin to asking the following: What does a car do that you can't trivially do with something else ? Like a boat or an airplane ? You simply cannot compare an in-memory Key-Value store, to a RDBS. And that's on a conceptual level. If you then consider performance, network latency and the guarantees, we're absolutely comparing airplanes to donkeys: You wouldn't use an airplane to climb up a rough mountain, and you certainly wouldn't prefer a donkey to travel to the other side of the world. reply gavinhoward 3 hours agorootparentSQLite has the option to run on a pure in-memory database. reply lofaszvanitt 8 hours agoprevThis open source mess must be changed. Provide a payment infrastructure to OS developers, otherwise you are left without options. Anyone can fork your hard work, anyone with more resources than you can push you out of your own work and expropriate it, noone pays for your hard work. When someone realizes this and changes the license the hardhat OS advocates turn against you and fork your hard work, essentially trying to deprive you of any income. Who on fucking earth thinks this is a good model? And who benefits, anyone dared to ask the question? For a starter: Force github to implement proper monetization options, that is visible and allows the creators to ask a sum for their work. Your work will remain open source, buy you must pay a low sum. If anyone is against this, then go, create your own project and don't use publicly available code. reply tekkk 6 hours agoparentPlenty of OSS developers/companies have survived and make a living out of their work. You just have to provide enough value on top of your project with your closed-source services that people are inclined to pay for them, instead of hacking up their own solutions. Or consultation. No one forces you to make your project OSS. It's a trade-off you should consider carefully before you do. But because people keep doing it, it's clear there are massive benefits to it. And many open source projects are just modules extracted from the company code base to gather developer mindset and to reduce costs by harnessing outside developers. It's, as such, a fine model as it is. Rug-pulling and exertion of power through stupid lawyer BS is what kills those projects if anything. If people are not paying for your OSS addons, perhaps they aren't that good after all. reply concerndc1tizen 8 hours agoparentprevMy theory is that open source was never intended to be based on free work. Rather, it is an open collaboration between companies, which realize that it will lower their cost, while not being a competitive advantage for either party. It improves the profit margin for everyone. What you're proposing is to convert Github into a small-business platform that sells \"open source\" software products, which is entirely different. reply perryizgr8 16 hours agoprevWhat are they hoping to get out of it? Wouldn't this just inconvenience their own users? reply a_vanderbilt 15 hours agoparentConsolidation of control and brand recognition. As soon as Redis revealed their hostile intentions, forks split off. Valkey seems to be the favored one, and in my direct experience is what projects are using moving forward. If we don't intend to use Redis' commercial services we have no reason to use them given the license fuckery. Like IBM with Redhat, they tried to overstep their position and exert undue control, so the community rejected their status as BDFL. Now IBM has Rocky and Liberty to deal with, and Redis has Valkey rapidly gaining credibility as the go-to solution. The social contract of FOSS goes both ways, and greed is righteously punished with the loss of power. reply theideaofcoffee 15 hours agorootparentThis was the obvious play when the original Garantia Data-now-Redis Labs went on their acquisition spree and subsequent buttoning up of the ecosystem via their cloud/hosted enterprise offering even before 2013 and then taking the Redis name for themselves. I believe it was only slowed down because of the community aspect pushing back against the stupid licensing changes of it, otherwise they would have gone full steam into it to get even more control. It's so sad to see a good product like that be hung out on the whims of some scummy leeches. reply stult 14 hours agorootparentprevI’m sure most of it is trademark preservation. Trademarks are enforce it or lose it. reply toast0 12 hours agorootparentYou can enforce by requiring an agreement, and making the terms reasonable enough that an OSS project can agree to them (this isn't entirely easy, but it's not that hard either). Also, there's the thing where these other projects have been using the name since forever, so there's already a history of non-enforcement. reply stult 9 hours agorootparentThe best time to start enforcing their trademark (from a legal perspective) was ten years ago. The second best time is now. Negotiating many agreements with many OSS projects is a lot of legal work which they may not want to pay for. And none of this undermines my original point that their primary motivation is probably just trademark preservation. That they did a poor job previously doesn’t mean they aren’t trying to do better now, and that they could use less aggressive methods doesn’t mean they aren’t primarily focused on trademark preservation. It’s the simple explanation. Occam’s razor. reply Tostino 5 hours agorootparentYou mean the trademark that wasn't filed until 2018[0]? That is part of it....these projects are older than the trademark, by quite a bit. [0]: https://trademarks.justia.com/878/05/redis-87805452.html reply imhoguy 20 hours agoprev [–] Looks like Mysqlization tragedy, the best would be when FOSS maintainers join their corporate agile sprints to meet roadmap demands, with naming rights gun aimed under table lol. First time I hear about Valkey, it acts as MariaDB in this drama. /s reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Redis is reportedly taking control of all open-source software (OSS) Redis libraries, which include popular ones like Jedis, Lettuce, and redis-py.",
      "These libraries are currently down, and another library, redis-rs, is also facing potential control or shutdown.",
      "This situation is significant as it affects developers who rely on these libraries for Redis integration in their applications."
    ],
    "commentSummary": [
      "Redis Inc. is taking control of open-source Redis libraries, sparking controversy and leading some developers to remove Redis dependencies due to trademark enforcement issues.",
      "Libraries such as redis-py, Lettuce, and Jedis have been moved under Redis Inc.'s official organization, seen as a defensive strategy against competitors like Valkey.",
      "The community is worried about trademark claims and potential vendor lock-in, prompting discussions on alternatives like Valkey and Kvrocks, and highlighting the challenges of trademark enforcement in open-source projects."
    ],
    "points": 150,
    "commentCount": 81,
    "retryCount": 0,
    "time": 1732564808
  },
  {
    "id": 42243755,
    "title": "Setelinleikkaus: When Finns snipped their cash in half to curb inflation",
    "originLink": "http://jpkoning.blogspot.com/2024/11/setelinleikkaus-when-finns-snipped.html",
    "originBody": "Friday, November 8, 2024 Setelinleikkaus: When Finns snipped their cash in half to curb inflation On the last day of 1945, with World War II finally behind it, Finland's government announced a new and very strange policy. All Finns were required to take out a pair of scissors and snip their banknotes in half. This was known in Finland as setelinleikkaus, or banknote cutting. Anyone who owned any of the three largest denomination Finnish banknotes — the 5000 markka note, the 1000, or the 500 — was required to perform this operation immediately. The left side of the note could still be used to buy things, but at only half its value. So if a Finn had a 1000 markka note in their wallet, henceforth he or she could now only buy 500 markka worth of items at stores. As for the right side, it could no longer be spent and effectively became a bond (more on this later). Source: Hallitus kansan kukkarolla, by Antti Heinonen Setelinleikkaus was Finland's particular response to the post-War European problem of \"monetary overhang,\" described in a 1990 paper by economists Rudi Dornbusch and Holger Wolf. After many years of war production, price controls, and rationing, European citizens had built-up a substantial chest of forced savings, or involuntary postponed consumption, as Dornbusch & Wolf refer to it. With WWII now over, Europeans would soon want to begin living as they had before, spending the balances they had accumulated on goods and services. Alas, with most factories having been configured to military purposes or having been bombed into dust, there wasn't nearly enough consumption items to make everyone happy. It was plain to governments all across Europe what this sudden making-up of postponed consumption in a war-focused economy would lead to: a big one time jump in prices. This may sound familiar to the modern day reader, since we just went through our own wartime economy of sorts: the 2020-21 battle against COVID and subsequent return to a peacetime economy. The supply chain problem caused by the COVID shutdowns combined with the big jump in spending as lockdowns expired, spurred on by a big overhang of unspent COVID support cheques, led to the steepest inflation in decades. According to Dornbusch and Wolf, European authorities fretted that the post-WWII jump in prices could very well spiral into something worse: all-out hyperinflation, as had happened after the first World War. Currencies were no longer linked to gold, after all, having lost that tether when the war started, or earlier, in response to the Great Depression. To prevent what they saw as imminent hyperinflation, almost all European countries began to enact monetary reforms. Finland's own unique reform — obliging their citizens to cut their stash of banknotes in two — would reduce the economy's stock of banknotes to just \"lefts,\" thereby halving spending power and muting the wave of post-wartime spending. After February 16, 1946 the halves would be demonetized, but until then the Finns could continue to make purchases with them or bring them to the nearest bank to be converted into a new edition of the currency. As for the right halves, they were to be transformed into a long-term investment. Finns were obligated to bring each right half in to be registered, upon which it would be converted into a Finnish government bond that paid 2% interest per year, to be repaid four years later, in 1949. It was illegal to try and spend right halves or transfer their ownership to anyone else (although it's not apparent how this was enforced). In theory, turning right halves into bonds would shift a large part of the Finnish public's post-war consumption intentions forward to 1949, when the bonds could finally be cashed. By then, the economy would have fully transitioned back to a civilian one and would be capable of accepting everyone's desired consumption spending without hyperinflation occurring. To our modern sensibilities, this is a wildly invasive policy. Had setelinleikkaus been proposed in 2022-23 as a way to dampen the inflationary effects of the reopening of COVID-wracked economies, and we all had to cut our dollar bills or yen or euros in half, there probably would have been a revolt. With the benefit of hindsight, we know that setelinleikkaus didn't work very well. Finland continued to suffer from high inflation in the years after the war, much more so than most European countries did. Why the failure? As Finish economist Matti Viren has pointed out, the reform only affected banknotes, not bank deposits. This stock of notes only comprised 8% of the total Finnish money supply, (Finns being uncommonly comfortable with banks) so a major chunk of the monetary overhang was left in place. Another glitch appears to have been the public's anticipation of setelinleikkaus. According to former central banker Antti Heinonen, who wrote an entire book on the subject, banks began to advertise their services as a way to avoid the dangers of the upcoming monetary reform (see images below). So Finns deposited their cash prior to the final date, the monetary overhang to some degree evading the blockade. Finnish bank advertisements warning of the upcoming note cutting Left: \"Bank accounts are fully secured in the banknote exchange.\" Right: \"Depositors are protected.\" Source: Hallitus kansan kukkarolla, by Antti Heinonen (Translations via Google Translate) If the Finnish experiment was a dud, other European responses to the post WWII overhang — either redenominations, temporarily blocking of funds, or all-out write offs of bank accounts — were more successful. Germany's monetary reform of 1948, which introduced the Deutschmark and was later dubbed the \"German economic miracle\", is the one that captures the most attention, but here I want to focus on a lesser known reform. Belgium's Operation Gutt, named after Belgium's Minister of Finance, Camille Gutt, was the earliest and perhaps the most dramatic of the post-war monetary operations. Taking place over four days in October 1944, Belgium contracted its entire money supply, both banknotes and deposits, from 165 billion to 57.5 billion francs. That's a two-thirds decline! You can see it illustrated in the chart below, along with the monetary reform enacted by the Dutch the following year, inspired by the Belgians. A chart showing the incredible contraction of Belgium's money supply in 1944 Source: Federal Reserve Bulletin, October 1946 (red arrow is my emphasis) It's not just the size of Operation Gutt that is striking to the modern eye. It's also the oddity of the tool being used. Today, we control inflation with changes in interest rates, not changes in the quantity of money. To soften the effect of the global COVID monetary overhang, for instance, central banks in the U.S., Canada, and Europe began to raise rates in 2022 from around 0% to 4-5% in 2024. By making it more lucrative for everyone to save and less attractive to borrow, central bankers were trying to reduce our propensity to spend our COVID support payments, and with less spending, prices wouldn't get pushed up as fast. This reliance on interest rates as our main tool of monetary policy is a relatively new phenomenon. In times past, central banks tended to lean heavily on changes in the supply of money, which may explain why in 1945, their main response — in Europe at least — was to obliterate the public's money balances rather than to jack up interest rates to 25% or 50%. It's worth exploring in some more detail how Operation Gutt was designed. On October 9, 1944, Belgian bank depositors had 90% of the money held in their accounts frozen, leaving just 10% in spendable form. As for holders of banknotes, there was no Finnish-style cutting. Rather, Belgians had four days, beginning October 9, to bring all their banknotes to the nearest bank, only the first 2,000 francs qualifying for conversion to newly printed versions. All notes above that ceiling got blocked in a separate account (along with excess deposits), some of which would be released slowly over the next few years—while the rest would remain frozen forever, subject to whether the owner was deemed to have been a collaborator who got rich during the occupation. (Finland's setelinleikkaus also had this same \"cleansing\" motivation.) In 1944, a line forms at the National Bank of Belgium to exchange notes. Source: National Bank of Belgium on Flickr In this sense, the post-WWII European monetary reforms were not only designed to reduce inflation, but also had a moral basis. Think of them as progenitors to India's 2016 demonetization, which was designed to catch so-called \"black money,\" although it failed to do so. Did Operation Gutt work? Incredibly, the decimation of two-thirds of the money supply in just a few days did not cause an immediate fall in Belgian prices. According to Belgian economic historians Monique Verbreyt and Herman Van der Wee, the Belgian retail price index stood at 260 the month of the reform, but had risen to 387 by September 1945. So it would seem that the whole operation failed. This surely draws into question the quantity theory of money, one of the basic tenets of monetary economics. A decline in the money supply, all things staying the same, is supposed to cause a fall in prices. Here is a glaring case in which it didn't. However, the National Bank of Belgium (NBB), the country's central bank, strikes a more constructive tone. In a recent retrospective on Operation Gutt, the NBB describes the reform as a gamble that paid off over time, eventually inspiring the \"Belgian Economic Miracle\", a period of low inflation and fast growth lasting from 1946-1949. By contrast, France did not embark on its own monetary reforms, the NBB takes pains to point out, and it thereby \"paid the consequences of post-World War II inflation well into the 1960s.\" Belgium's inflation rate was also much lower than Finland's in the four or five years after the war. Which gets us back to Finland. Unlike the Belgian central bank, Finland's central bank — Suomen Pankki — notably avoids almost all mention of its post-war reform on its website. According to Matti Viren, setelinleikkaus led to \"distrust towards the authorities and economic policy for decades,\" so there may be some sheepish reticence on the part of the central bank to draw attention to it. But setelinleikkaus and Operation Gutt aren't just archaic monetary policy dead-ends. One day I suspect they'll be back. Not just as a special tool for responding to emergencies, but as a day-to-day policy wrench, albeit in a new and refined form. Cash, which is awkward to immobilize for policy reasons, will be gone in a decade or two, leaving the public entirely dependent on bank deposits and fintech balances which, thanks to digitization and automation, can be easily controlled by the authorities. To rein in a jump in inflation, central bankers will require commercial banks and companies like PayPal to impose temporary quantitative freezing on their clients' accounts, but unlike Finland's 1945 blockade, the authorities will be able to rapidly and precisely define the criteria, say by allowing for spending on necessities — food, electricity, and gas— while embargoing purchases of luxury cars and real estate. The future version of setelinleikkaus won't be clumsy, it'll be a precise and surgical inflation-fighting tool, albeit a controversial one. Posted by JP Koning at 1:42 PM Email ThisBlogThis!Share to XShare to FacebookShare to Pinterest Labels: cash, demonetization, inflation, monetary policy, quantity theory of money 9 comments: OliverNovember 10, 2024 at 10:20 AM How do you reconcile this sentence : \"To our modern sensibilities, this is a wildly invasive policy. Had setelinleikkaus been proposed in 2022-23 as a way to dampen the inflationary effects of the reopening of COVID-wracked economies, and we all had to cut our dollar bills or yen or euros in half, there probably would have been a revolt.\" With this one : \"To rein in a jump in inflation, central bankers will require commercial banks and companies like PayPal to impose temporary quantitative freezing on their clients' accounts, but unlike Finland's 1945 blockade, the authorities will be able to rapidly and precisely define the criteria, say by allowing for spending on necessities — food, electricity, and gas— while embargoing purchases of luxury cars and real estate.\" Without envisioning a revolt against this practice ? ReplyDelete Replies JP KoningNovember 10, 2024 at 3:21 PM That's a good question. I think there's something extra unsettling about having to cut your notes in half and then bring them into the bank to be converted before they expire, braving potentially large lines if the changeover period is a short one. By contrast, a bank account freeze happens smoothly -- the bank does everything for you. I also think that a few decades in the future we'll be much more used to bank account freezes as a tool of law enforcement, so extending this power to monetary policy makers won't be as controversial as it would otherwise be. Delete Replies Reply OliverNovember 11, 2024 at 3:24 AM Do you really think such a power would be a good thing ? Imagine what a dictatorship could do with it. Delete Replies Reply JP KoningNovember 11, 2024 at 6:46 AM \"Do you really think such a power would be a good thing ?\" My post never passed judgement on whether it would be a good thing or a bad thing. It's just an exercise in prognostication. Delete Replies Reply Reply AnonymousNovember 15, 2024 at 8:34 AM > Cash, which is awkward to immobilize for policy reasons, will be gone in a decade or two [citation sorely needed] ReplyDelete Replies JP KoningNovember 16, 2024 at 7:49 AM It's a prediction. I get it from David Birch's parable from William Gibson's Count Zero: https://chyp.com/2016/02/01/cashless-as-count-zero/ Basically, cash is still around in Gibson's imagined future, but it has disappeared from \"polite society\". Delete Replies Reply Reply AnonymousNovember 26, 2024 at 4:43 AM Really interesting post - thank you! ReplyDelete Replies Reply AnonymousNovember 26, 2024 at 5:53 AM \"Cashlessness\" is another type of neoliberal, social engineering fascism, promoted as a virtue-signal \"innovation\" when it really tries to wallpaper over this anti-poor, anti-worker, and anti-privacy authoritarian, corporatist control-freak nonsense. Cash is king, F everyone who would take it away if they wear D or R. ReplyDelete Replies Reply AnonymousNovember 26, 2024 at 1:04 PM Interesting historical analysis, but the predictions around a future setelinleikkaus overlook the elephant in the room: cryptocurrency. If the general public is simply not using Bitcoin (or something similar) extensively enough for it to be a major factor in the outcome of the first future setelinleikkaus, then they certainly will switch to it en masse the day after. Will governments then simply abandon their efforts to surgically control spending? Will they attempt to ban cryptocurrency -- logistically difficult to impossible? ReplyDelete Replies Reply Add comment Load more... Newer Post Older Post Home Subscribe to: Post Comments (Atom) Follow @jp_koning Get new posts by email: Subscribe Powered by Please donate to support my writing! BTC: 1DbXZibSVJrs1MABEZKBXpMTqiApV8GyzL XRP: rMpB2AsrDTdbynCB48hg8MwHLD4wtXJfRJ PayPal: link The Moneyness discussion board is here. I design financial/economics infographics here. Contact me here. I also write for the Sound Money Project, Breaker, Coindesk, and Bullionstar. Popular Posts Setelinleikkaus: When Finns snipped their cash in half to curb inflation On the last day of 1945, with World War II finally behind it, Finland's government announced a new and very strange policy. All Finns we... Memecoins are the point Cypherpunks wanted to change the world. We ended up with memecoins. Our story begins with some very smart and idealistic developers, known a... Pricing the anonymity of banknotes Banknotes are useful. Not only do they provide their owner with a standard set of payments services, they also offer financial anonymity. ... How my views on financial privacy have evolved over a decade I began exploring the topic of financial privacy and payment anonymity in the early days of this blog. Over the past decade, my views have s... Starbucks, monetary superpower I recently spent some time on Twitter discussing the monetary wonders of Starbucks. In this post I'll bring a bunch of tweets togeth... The magnificent Swiss 10-centime coin The Swiss 10-centime coin has a lot to teach us about monetary economics. On its face, the Swiss 10-centime (rappen in German) looks like a ... Subscribe To Posts Comments Labels cash (119) bitcoin (109) Federal Reserve (86) fintech (67) stock market and equities (67) coins (51) stablecoin (51) unit of account (48) inflation (47) liquidity premium (47) Nick Rowe (43) medium of exchange (43) anonymity (42) clearing and settlement (42) gold (42) zero lower bound (42) Bank of Canada (38) ECB (37) moneyness (33) fiat money (31) money laundering (28) Ethereum (27) charts (27) payments (27) nominal interest rates (26) Target2 (25) fungibility (25) David Glasner (24) Gresham's Law (24) demonetization (24) open market operations (24) Miles Kimball (23) Scott Sumner (23) Tether (23) history of thought (23) regulation (23) sanctions (23) central bank digital currency (21) credit cards (19) seigniorage (19) sticky prices (19) Bank of England (18) Paul Krugman (18) medium of account (18) quantitative easing (18) Fedwire (17) Stephen Williamson (17) chartalism (17) dollarization/euroization (17) gold standard (17) George Selgin (16) John Maynard Keynes (16) Mike Sproul (16) central bank independence (16) Fedcoin (15) intra-Eurosystem credit (15) store of value (15) David Andolfatto (14) censorship resistance (14) legal tender (14) lender of last resort (14) personal finance (14) Bill Woolsey (13) DeFi (13) Iran monetary blockade (13) Lars Christensen (13) Ponzi schemes (13) Reserve Bank of India (13) bills of exchange (13) deflation (13) Milton Friedman (12) NCB (12) Ripple (12) SNB (12) cross border payments (12) debit cards (12) free banking (12) market monetarism (12) uncertainty (12) Irving Fisher (11) Larry White (11) Tyler Cowen (11) Zimbabwe (11) collateral (11) convenience yield (11) fundamental value (11) zero-sum games (11) David Beckworth (10) ETF (10) Ken Rogoff (10) Riksbank (10) backing theory of money (10) crude oil (10) liquidity options (10) Bob Murphy (9) Ludwig von Mises (9) MMT (9) bimetallism (9) debt (9) hot potato effect (9) quantity theory of money (9) reflux (9) tax evasion (9) taxation (9) Bank of Japan (8) Great Depression (8) Liquidity insurance (8) Perry Mehrling (8) Robert Shiller (8) TIPS (8) censorship (8) federal funds rate (8) interdistrict settlement (8) natural interest rate (8) overdraft facility (8) short selling (8) Adam Smith (7) Borges problem (7) Carl Menger (7) FinCEN (7) Gavyn Davies (7) John Cochrane (7) SWIFT (7) barter (7) business cycle (7) chain letters (7) counterfeiting (7) money market mutual funds (7) own-rates (7) ACH (6) Billion Prices Project (6) Danmarks Nationalbank (6) RTGS (6) SDR (6) Silvio Gesell (6) card networks (6) currency (6) large value note embargo (6) metallism (6) monetary policy (6) real interest rate (6) Bank of Greece (5) China (5) Dror Goldberg (5) Ireland (5) Larry Summers (5) Narayana Kocherlakota (5) Steve Waldman (5) System of National Accounts (5) US Mint (5) Warren Buffett (5) Willem Buiter (5) William Hutt (5) William Stanley Jevons (5) counting money (5) equity deposits (5) financial inclusion (5) ghost money (5) gift cards (5) gold lease rates (5) money illusion (5) new monetarism (5) ransomware (5) sterilization (5) technical and fundamental analysis (5) Alfred Marshall (4) Antoin Murphy (4) Argentina (4) Austrian economics (4) Barry Eichengreen (4) David Laidler (4) FT Alphaville (4) Friedman rule (4) Hans-Werner Sinn (4) Henry Dunning Macleod (4) IMF (4) L. Randall Wray (4) Michael Woodford (4) Neil Wallace (4) Paul Samuelson (4) Reserve Bank of New Zealand (4) Unidad de Fomento (4) augmentation/diminution (4) chopmarking (4) drachma (4) efficient markets hypothesis (4) forward guidance (4) free coinage (4) futures theory (4) real bills doctrine (4) real estate (4) savings (4) scrip (4) Bangko Sentral ng Pilipinas (3) Fischer Black (3) Friedrich Hayek (3) Greg Mankiw (3) James Hamilton (3) James Tobin (3) Japan (3) Libra (3) London Bullion Market Association (3) Marvin Goodfriend (3) Nobuhiro Kiyotaki (3) People's Bank of China (3) Piero Sraffa (3) Richard Cantillon (3) SOMA (3) Steve Horwitz (3) Van Court's Bank Note Reporter (3) Weimar hyperinflation (3) asset shortage (3) compensated dollar (3) consumption (3) corporate voting (3) crowdfunding (3) electrum (3) endogenous/exogenous money (3) greenbacks (3) hysteresis (3) inflation swaps (3) investment (3) microfoundations (3) narrow banking (3) notgeld (3) required reserves (3) search theory of money (3) secular stagnation (3) stock lending (3) stocks and flows (3) usury (3) ABCT (2) Arnold Kling (2) Brad DeLong (2) CNH (2) CNY (2) Divisia (2) Don Patinkin (2) Eugene Fama (2) Exchange Stabilization Fund (2) FATF (2) Fritz Machlup (2) GOFO (2) George Berkeley (2) Henry Thornton (2) Hong Kong (2) James Steuart (2) John Locke (2) Kaushik Basu (2) LETS (2) NGDP targeting (2) Neo-Fisherism (2) ON RRP facility (2) Peter Stella (2) Randall Wright (2) Reserve Bank of Australia (2) Section 13(3) (2) Thomas Sargent (2) Walter Bagehot (2) William Barnett (2) Yanis Varoufakis (2) autarky (2) bribery (2) capital key (2) cheques (2) cooperative banking (2) corralito (2) coupon money (2) currency vs banking school (2) financial intelligence unit (2) interbank market (2) labour (2) legal restrictions hypothesis (2) monetary optionality (2) postal banking (2) productivity norm (2) shoe leather costs (2) symmetallism (2) term premium (2) the moral economy (2) wash trading (2) ways and means advances (2) yuan (2) Bank of Italy (1) Ben Bernanke (1) Benjamin Anderson (1) Bretton Woods (1) De Nederlandsche Bank (1) FINTRAC (1) Fed Treasury Accord (1) Fred Lavington (1) GDP and net worth (1) ICU (1) ISLM (1) J. Laurence Laughlin (1) John Law (1) Kurt Schuler (1) Ludwig Lachmann (1) New Monetary Economics (1) Norges Bank (1) Robert Clower (1) South Africa Reserve Bank (1) agio (1) coexistence puzzle (1) complete preferences (1) depository receipts (1) fiction (1) helicopter money (1) mercantilism (1) mobile money (1) unions (1) Blog Archive ▼ 2024 (26) ▼ November (3) How my views on financial privacy have evolved ove... Pricing the anonymity of banknotes Setelinleikkaus: When Finns snipped their cash in ... ► October (3) ► September (1) ► August (1) ► July (3) ► June (2) ► May (2) ► April (2) ► March (4) ► February (2) ► January (3) ► 2023 (57) ► December (4) ► November (5) ► October (4) ► September (5) ► August (6) ► July (5) ► June (7) ► May (7) ► April (6) ► March (1) ► February (3) ► January (4) ► 2022 (44) ► December (4) ► November (3) ► October (3) ► September (1) ► August (2) ► July (2) ► June (2) ► May (5) ► April (4) ► March (5) ► February (6) ► January (7) ► 2021 (32) ► December (3) ► November (3) ► October (3) ► September (2) ► August (3) ► July (3) ► June (4) ► May (2) ► April (3) ► March (3) ► February (1) ► January (2) ► 2020 (34) ► December (3) ► November (3) ► October (3) ► September (2) ► August (3) ► July (3) ► June (3) ► May (3) ► April (3) ► March (2) ► February (2) ► January (4) ► 2019 (32) ► December (5) ► November (5) ► October (3) ► September (2) ► August (3) ► July (1) ► June (2) ► May (2) ► April (3) ► March (3) ► February (1) ► January (2) ► 2018 (33) ► December (3) ► November (3) ► October (2) ► September (3) ► August (3) ► July (2) ► June (2) ► May (3) ► April (3) ► March (4) ► February (1) ► January (4) ► 2017 (37) ► December (3) ► November (3) ► October (3) ► September (4) ► August (3) ► July (3) ► June (4) ► May (2) ► April (2) ► March (3) ► February (3) ► January (4) ► 2016 (45) ► December (3) ► November (4) ► October (4) ► September (5) ► August (2) ► July (3) ► June (4) ► May (4) ► April (3) ► March (4) ► February (5) ► January (4) ► 2015 (49) ► December (4) ► November (4) ► October (4) ► September (3) ► August (3) ► July (4) ► June (5) ► May (4) ► April (4) ► March (5) ► February (4) ► January (5) ► 2014 (52) ► December (4) ► November (4) ► October (4) ► September (5) ► August (3) ► July (3) ► June (5) ► May (4) ► April (5) ► March (5) ► February (5) ► January (5) ► 2013 (90) ► December (3) ► November (6) ► October (6) ► September (6) ► August (6) ► July (3) ► June (6) ► May (7) ► April (10) ► March (13) ► February (9) ► January (15) ► 2012 (113) ► December (16) ► November (18) ► October (20) ► September (6) ► August (4) ► July (4) ► June (8) ► May (11) ► April (6) ► March (6) ► February (3) ► January (11) ► 2011 (14) ► December (14) My Blog List Alt-M Bill Mitchell – billy blog Econbrowser MacroMania Cecchetti and Shoenholtz Confessions of a Supply-Side Liberal TheMoneyIllusion Uneasy Money The Grumpy Economist Stumbling and Mumbling Great Wall of Numbers Monetary Freedom Consulting by RPM || Free Advice Blog Stephen Williamson: New Monetarist Economics Worthwhile Canadian Initiative longandvariable Liberty Street Economics Noahpinion Macro and Other Market Musings Economist's View Spontaneous Finance About Me JP Koning Thinking about money is more interesting than making money. View my complete profile Search This Blog Powered by Blogger.",
    "commentLink": "https://news.ycombinator.com/item?id=42243755",
    "commentBody": "Setelinleikkaus: When Finns snipped their cash in half to curb inflation (jpkoning.blogspot.com)145 points by Michelangelo11 10 hours agohidepastfavorite131 comments eru 8 hours ago> It's not just the size of Operation Gutt that is striking to the modern eye. It's also the oddity of the tool being used. Today, we control inflation with changes in interest rates, not changes in the quantity of money. To soften the effect of the global COVID monetary overhang, for instance, central banks in the U.S., Canada, and Europe began to raise rates in 2022 from around 0% to 4-5% in 2024. It's a bit more interesting than 'the central bank sets interest rates'. Simplified: the central bank decide on an interest rate that they want to see. By itself that decision doesn't do anything. What happens next is that they buy and sell government bonds in the open market. The interest rate can be seen as the inverse of the price of bonds. If the central bank wants to see a lower interest rate, they buy bonds with freshly printed money to drive up their prices, ie drive down the interest rate. If the central banks wants to increase the prevailing interest rate, they sell government bonds from their inventory and essentially destroy they money they receive in return. So even when the language of modern central banking talks about interest rates, they still change the quantity of money to implement that. (This is all simplified, especially with the interest on excess reserves that was popular with the Fed for a while. And there's also repos and reverse repos etc.) reply insane_dreamer 4 hours agoparentThat's just one mechanism, but not the primary way in which the Fed controls interest rates. The Fed is a large provider of short-term loans (\"fed funds\") to cover interbank exchanges. It also is the lender of last resort and lends to banks directly (\"discount rate\"). By changing these rates, the FED can influence the rates the banks charge each other for loans, and down the line to consumers. reply xadhominemx 1 hour agorootparentYou are correct. Striking how many HN commentors are so often confident and yet wrong... reply mbar84 56 minutes agorootparentWas he really that wrong? Isn't a bond just another debt instrument? It's not obvious to me, that there is any fundamental difference between the operations that both comments describe. reply rf15 18 minutes agorootparentprevYou are wrong. Striking how many HN commentators are so often confident and yet wrong in their assumption that everything is about the US. reply Polygator 7 hours agoparentprevFor an international perspective: buying and selling government bonds is far from an universal mechanism for interest rate control (AFAIK when discussing central banks the US is almost always a special case) For instance the Canadian, UK and European central banks provide systems for interbank short-term loans. It is almost entirely through these systems that they set their target rate. For Canada the BoC doesn't do any open market operations to reach target interest rate (so almost only repos and reverse repos). Their target rate is in fact called the \"target overnight rate\" and only concerns overnight lending between Canadian financial institutions. reply eru 5 hours agorootparentFor the interested https://en.wikipedia.org/wiki/Interbank_lending_market#Monet... has more on it. As far as I understand, the central banks intervene in this market by offering to loan or borrow above or below otherwise prevailing market rates. This has the effect of adding money to the system (or removing it). So that's pretty much the same mechanism as what I described. They use an intermediate proxy like the 'target overnight rate' to help them decide how much money to add or remove to the system: exactly as much as needed to bring the market interest rate in line with their intermediate proxy. reply neilwilson 7 hours agoparentprevInterest on reserves is very much still in place[0]. Open Market Operations haven't been a thing since shortly after the 2008 Financial Crisis. https://www.federalreserve.gov/monetarypolicy/reserve-balanc... reply wbl 1 hour agorootparentThe OMC desk is still there trading. How is that not \"a thing?\" reply eru 5 hours agorootparentprevAlas, you are right. Compare also https://www.cato.org/working-paper/floored reply IAmGraydon 6 hours agoparentprevAhh…no. The Fed sets the interest rate directly. What you are talking about is yields on treasury bonds, which are manipulated via bond buying to force money into assets by artificially dropping the yield of those bonds, thus creating a more attractive investment in the stocks, assets, etc. reply TeaBrain 1 hour agorootparentThey were entirely wrong about mechanism of setting the interest rate through the discount and fed funds rate, but this description also isn't comprehensive. The feds buying of treasury bonds isn't just to push them down, but is also a mechanism for increasing the monetary supply through the expansion of the fed's balance sheet. This mechanism for increasing the monetary supply is also why the linked article doesn't appear to be accurate either, as they don't seem to understand that the fed does have the ability to manipulate the monetary supply through its balance sheet. reply eru 6 hours agorootparentprevI described Open Market Operations, see https://en.wikipedia.org/wiki/Open_market_operation > The Fed sets the interest rate directly. So which interest rate does the Fed set directly, and how does that setting have any effect on the economy? (I know they have interest on excess reserves. I already accounted for those in my original comment. I know, they are annoying and misguided. I was mostly talking about the system they used before, ie up until about 2008.) reply IAmGraydon 4 hours agorootparentThe Fed directly sets the Overnight Rate, also known as the Federal Funds Rate. It's the rate at which banks can borrow from one another overnight to satisfy reserve requirements. This rate indirectly affects the interest rate of all other lending instruments because the higher cost of overnight bank to bank lending is passed on to the customers in the form of higher loan rates, credit card rates, mortgage rates, etc. The Open Market Operations you described (completely different from the Overnight Rate) are a form of stimulus. Because money always seeks higher risk-adjusted returns, the Fed will buy treasuries, which drives down their yield. This makes them an unattractive investment, so the money goes where it can get a better risk-adjusted return. That's usually in the market. So by adjusting treasury note yields, they can stimulate the economy. Furthermore, those treasury bonds are bought with printed money, so this is effectively a way to inject massive amounts (trillions of dollars) of printed money directly into the economy. It's pretty crazy when you think about the power that these policies have. reply jmyeet 1 hour agoparentprev> Simplified: the central bank decide on an interest rate that they want to see. This is incorrect. Using the US central bank eample, the Fed has a target for inflation. It uses interest rates to try and hit that target. If inflation is higher than 3% the Fed will raise rates to cool the economy. The Fed sets interest rates directly because lending money to the Fed is viewed as \"risk-free\" (as the US government has never defaulted on a debt). So if the Fed offers a risk-free 4%, banks will need to offer more because they are not risk-free. So banks no longer really lend savings out for loans. They borrow money and lend it out at a higher rate (eg mortgage-backed securities). So when you could get a mortgage at 2.5%, it was because the Fed was offering 0%. When the Fed offers 5%, mortgage rates will go up to 7-8%. There is another mechanism that the government could use to control inflation: fiscal policy, specifically taxation. A criticism of monetary policy to control inflation is that it's indiscriminate. People will go out of business and lose their houses. Taxes only target profits. So in 2021-2022, we should've just passed a windfall profits tax of 80%. That would've cooled off inflation real quick and given the governments funds to distribute to those most adversely affected. But that will never happen because the corporations and wealthy who own both parties will never stand for wealth redistribution to the poor. They will however demand wealth be transferred from the government to the rich. reply rpmisms 20 minutes agorootparentThe false equivalence of a profits tax and redistributing wealth to the poor is quite funny. reply hgomersall 7 hours agoparentprevWe don't control inflation with interest rates; we do some economic theatre with interest rates that some people believe controls inflation in a predictable way. reply pjc50 7 hours agorootparentThe evidence is very strong that we do actually control it, because in many countries you can see in the historical data when central bank targeting was introduced that the inflation rate drops fairly rapidly into the target band. It's not a perfect control system because the cost is \"NAIRU\": non accelerating rate of unemployment. That is, economic growth and wage growth are constrained to avoid a wage-price spiral. And sometimes you get a shock from outside the system. reply hgomersall 7 hours agorootparentPlease do show this very strong evidence that the effect is any more than the supply chains sorting themselves out. Even some within the CBs are doubting the causality. Japan had the lowest inflation of any major economy post COVID, and yet persisted with essentially a ZIRP. There's a good argument that in our high reserves world, interest is actually inflationary. reply gruez 5 hours agorootparent>There's a good argument that in our high reserves world, interest is actually inflationary. How's that working out with Turkey? reply nostrebored 2 hours agorootparentIt will probably work out just fine as they become a key player in European LNG. Turkey couldn’t have serviced their debt with rising interest rates and continued government spending on expansion. Being inflation averse makes sense when you can’t reasonably make use of funds. It’s not clear how much of that is actually related to the business cycle and how much is related to MMT, regardless of what adherents would have you believe. reply pjc50 6 hours agorootparentprevUK historical investigation: https://www.elibrary.imf.org/display/book/9781557758897/ch07... - written in 2000, but you can see on this graph https://www.macrotrends.net/global-metrics/countries/gbr/uni... how flat it is from 1992 to 2020. That's a very good record for any piece of policy. Inflation control works for controlling business cycle inflation. However, it's not perfect and the COVID+war shock resulted in unavoidable inflation. It's a \"\"plant\"\" in the https://en.wikipedia.org/wiki/Control_theory sense. (my original comment: \"you can see in the historical data when central bank targeting was introduced that the inflation rate drops fairly rapidly into the target band\". The US graph is similar https://www.macrotrends.net/global-metrics/countries/usa/uni... - although the adoption of inflation targeting wasn't fully formalized, it was definitely used in setting interest rates from the 90s. reply hgomersall 5 hours agorootparentNow show the UK plot for the 2010s. Also, show the one for Japan. It's easy to cherry pick data to show whatever you want. It doesn't constitute strong data for a casual and reliable link between interest rates and inflation. reply fernmyth 4 hours agorootparentPlot the days when your air conditioner is on with the temperature of your room. It will have many concrete examples and a long-term correlation showing that actually, the air conditioner is associated with the temperature going up. All feedback/control systems are like that. reply hgomersall 4 hours agorootparentYour argument would have more weight if the inflation predictions were accurate. Here's the prediction report for the BoE in Aug 2014: https://www.bankofengland.co.uk/-/media/boe/files/inflation-... (It's worth noting that even with the assumption of the models used being useful, the spread on those inflation rates is wild). Here's what actually happened: https://www.ons.gov.uk/economy/inflationandpriceindices/time... It very rapidly hit the bottom end of the prediction range before jumping up again pretty high. All that time interest rates were held constant and low. Given they claim feedback lags of two years or so, one wonders what the point of all this is... (one cannot run a control loop with control lags substantially longer than the time constant of the system; that's basically the recipe for an unstable control system, assuming of course the control system is doing anything). There's an argument that it's inflation expectations that matter, but there are dissenters within the temple that disagree: https://www.federalreserve.gov/econres/feds/files/2021062pap... reply pjc50 3 hours agorootparentprevThe second link https://www.macrotrends.net/global-metrics/countries/gbr/uni... is from 1960 to 2023. reply FuriouslyAdrift 1 hour agorootparentprevThis is the result of demographic crash and stagnation more than anything else... reply hgomersall 1 hour agorootparentYou mean interest rates don't necessarily control inflation? reply wbl 1 hour agorootparentprevHow is that working for Argentina or Turkey? reply dennis_jeeves2 5 hours agorootparentprevCorrect, it's mostly theater , and people nerding out on the numbers. The true measure of inflation is this: For a single day one works (calculated over a lifetime), how many days can one survive without working which will pay off all of one's bills. The lower this figure, higher is the inflation. reply gruez 5 hours agorootparent>The true measure of inflation is this: For a single day one works (calculated over a lifetime), how many days can one survive without working which will pay off all of one's bills. This rapidly falls apart when you try to actually calculate it. Whose income do you use? Is inflation lower for doctors than burger flippers? What do you use as the retirement age? Does inflation go down if the retirement age is raised? What counts as \"survive\"? Does that mean the price of smartphones don't count toward inflation because you can theoretically survive without them? reply eru 5 hours agorootparentprevHuh? What does this have to do with inflation at all? reply dllthomas 1 hour agorootparentUnless I miss something, it's very much not a standard measure of inflation. That said, leaning on that \"at all\": If wages are stickier than expenses, then the gap between wages and expenses will represent recent inflation to some degree. reply Carrentt 8 hours agoprevFascinating piece of financial history I hadn't heard about. Imagine your government telling you to literally take scissors to your money, it's like a weird mix between arts & crafts hour and monetary policy. Though I suppose we're already halfway there with our modern central banks, just without the satisfying snip-snip sounds. The Finnish experiment failing because people just deposited their cash in banks first is a classic example of Goodhart's Law in action. Or as I like to call it, \"If you tell people you're going to cut their money in half, they'll find a way to keep it whole.\" What's really interesting is Belgium's more successful approach, they went full scorched earth on 2/3 of their money supply and somehow managed to pull off an economic miracle. Makes our current inflation-fighting tools look rather tame in comparison. \"Sorry, best we can do is nudge interest rates up a quarter point at a time. reply guenthert 8 hours agoparentWell, as fascinating as your government telling you to surrender all gold you might hold [1]. https://en.wikipedia.org/wiki/Gold_Reserve_Act reply dr_dshiv 7 hours agorootparent“By 1975, Americans could again freely own and trade gold.” Woah. reply morkalork 3 hours agorootparentprevHow about your puppet government colluding with the soviet union to print a new currency in secret and surprising everyone with a currency reform over night: https://english.radio.cz/when-savings-were-lost-and-dreams-s... reply eru 8 hours agoparentprevTurkey dropped six zeroes off their currency in the 2000s. Technically, you could describe that as cutting 999,9999/1,000,000 of their money supply. (Especially if they had done a funny dance like the Finnish, where you would use some scissors to only keep the tiny top left corner of your old notes, and can exchange that for new ones.) In practice, people saw the Turkish currency reform as merely a cosmetic change, not an actually reduction in the money supply. See https://en.wikipedia.org/wiki/Revaluation_of_the_Turkish_lir... reply pjc50 7 hours agorootparentThis is one of those measures that hyperinflation countries have to adopt for sanity, re-numbering the money. The surprising case that worked is the Brazilian \"Real\": by renaming the currency as well as switching it to semi-controlled exchange rates, inflation was drastically reduced. https://en.wikipedia.org/wiki/Plano_Real > Combined with all previous currency changes in the country's history, this reform made the new real equal to 2.75 × 1018 (2.75 quintillion) of Brazil's original réis. (!) reply pydry 3 hours agorootparentRenaming didnt drastically reduce inflation. If it were that easy everybody would do it. It just allowed the government to reduce inflationary expectations while they did the legwork of throttling spending, etc. If they hadnt done the legwork to reduce inflationary pressures this parlor trick would not have worked. And, arguably, if they hadnt done it at all, inflation would still have gone down simply by reducing the inflationary pressures. reply marcosdumay 3 hours agorootparentprevBrazil has cut 3 zeroes from the currency many times. It's actually no big deal if you change the currency name. Cutting the zeroes isn't supposed to have any impact except on making prices easier to write, so you only need to avoid confusion. reply PrismCrystal 4 hours agorootparentprevRomania dropped several zeros in 2005, so 1,000,000 became 100. As someone who was around at that time, I still tend to think of prices in the old system, which makes me look ridiculous to younger people and even most of my same-age peers. Albania dropped a single zero way back in the mid-20th century, and yet even generations born long after that still think of prices in the old system. The first time I went to Albania, I was paranoid and made a scene in shops, thinking every shopkeeper was trying to rip me, a foreigner, off by quoting a price an order of magnitude higher. My face was red when someone finally explained how the country works. reply Workaccount2 4 hours agorootparentprev>In practice, people saw the Turkish currency reform as merely a cosmetic change, not an actually reduction in the money supply. Because it is just cosmetic. Governments chronically think they can directly legislate more value into existence. They fail to understand that currency is just an intermediary for trading labor. reply Sharlin 7 hours agorootparentprevWould've been funny if you had been expected to literally cut out six zeroes from the notes. In 1963 Finland also ended up shifting markka to the right by two, so that the old markka became the new penni (1/100 markka). reply jmyeet 1 hour agoparentprevSo it's not quite the same thing but the US government has historically performed a sovereign devaluation of its currency. I am of course talking about FDR (Executive Order 6102). This made it illegal to own gold. You had to hand it in and get paid at ~$20/oz. After doing this, the US dollar (nominally on the gold standard at the time) was revised to ~$35/oz. reply teractiveodular 8 hours agoprevA recent similar example was the Indian move in 2016 to demonetize the ₹500 and ₹1,000 notes with very little notice, which is in retrospect widely viewed to have been a disaster. https://en.wikipedia.org/wiki/2016_Indian_banknote_demonetis... reply theshrike79 5 hours agoparentIIRC the reasoning was that only criminals have large amounts of valuable notes and by demonetising them they'd hit the criminals where it hurts. But it turns out that tons of people in rural India had their life savings under mattresses in large denomination bills... reply astockwell 2 hours agoparentprevI imagine Finland's would be similarly branded a disaster if the present-day internet/social media megaphones had existed.. History books on 1945 aggressive monetary policy change: \"The public didn't like it\" History books on 2025 aggressive monetary policy change: \"The public went frigging bananas, doxxed their leaders, coordinated widespread disobediance on the scale of GME, etc\" (Granted I live in the US, and that's putting it mildly how the US would react) reply guenthert 8 hours agoparentprevThere were multiple motives for that move, notably however not among those an attempt to curb inflation. reply pjc50 9 hours agoprevMentioning the war on COVID but not the actual war between Russia and Ukraine that caused a huge spike in European energy prices is a big omission, since that caused a lot of global inflation. reply gregman1 8 hours agoparentExtreme hike in energy prices was about four months before the war began. It’s when EU decided to abandon long term gas contracts and turned to spot prices (~11.2021). The war started in 02.2022. reply ArnoVW 8 hours agorootparentFrom your perspective the war started in 2022. In reality that phase of the war started before. Russia did not improvise this war. They started disrupting the supply in 2021 to ensure that Europe would not fill their strategic reserves / winter storage during the summer, thus insuring maximum leverage when they needed it. This is well documented [1] Note that this is just talking about this phase of the war. Hostilities started in 2014 when parts of Ukraine \"suddenly self-liberated\" themselves. Helped by mysterious soldiers in professional but unmarked uniforms. [1] https://www.banque-france.fr/en/publications-and-statistics/... reply nostrebored 2 hours agorootparentNobody has sabotaged European energy security more than European governments. The divestment from nuclear and reliance on LNG was an obvious disaster in the making. Taiwan has made a similar blunder and will pay the price at the next whiff of geopolitical instability. reply poincaredisk 26 minutes agorootparentYou've just read how Russia literally sabotaged European energy security and you compared this to... some European countries taking a suboptimal decision? And Europe is not a single country. My country still happily digs out and burns tons of coal (annoying our neighbors in the process). I hope you're proud of us. reply eru 8 hours agorootparentprevWell, the full scale portion of the war started in February 2022. But you are right otherwise. reply casenmgreen 6 hours agorootparentYes. I was surprised to find out that Russia had been conducted sabotage operations against UA artillery ammunitions dumps for many years prior to full scale invasion, and the Ukrainians had already lost the majority of their artillery ammunition reserves this way by the time invasion began. reply Yawrehto 6 hours agorootparentAnd they conquered Crimea in 2014--partially, if I remember right, to get a port that doesn't freeze over in the winter, something they have wanted for literally hundreds of years. reply ceejayoz 4 hours agorootparentThey already had https://en.wikipedia.org/wiki/Port_of_Novorossiysk for that. reply ttyprintk 7 hours agoparentprevEspecially since the Finnish experience is the only direct comparison to Ukraine today, where both their army and government are fighting Russia. reply skynet97 8 hours agoparentprevnext [12 more] [flagged] jb1991 8 hours agorootparentAs someone who knew a few people under age 35 who died from Covid, I find this comment offensive. reply GeertJohan 8 hours agorootparentA war has casualties on both sides? But really I find this war analogy a bit silly anyway.. reply nextlevelwizard 8 hours agorootparentprevThat's the thing, some people have one experience and other the opposite. I kept hearing \"everyone knows someone who was affected by Covid\", but I for one have none in my family nor have I heard any Covid related losses from any of my colleagues, but obviously there are people who were affected. On other hand I do know a person who died due to depression caused by the isolation of lock down. reply K0balt 7 hours agorootparentCount yourself lucky. I lost 1 parent, 1 good friend and -his entire family- of 3, several close associates that I counted among my friends , and my wife. Random events do not have random distributions, and the early strains of the virus were much, much more aggressive. I still suffer from neurological effects of my COVID infection from the very beginning of the spread of the virus. People with cancer in remission for years of even decades experienced a huge spike in breakthrough tumors. Vascular damage provoked a massive uptick in ischemic events such as stroke or heart attack. The effects of COVID reach far, far beyond “dying of COVID”. reply ajsnigrutin 8 hours agorootparentprevIn my ~2mio pop country, by the end of restrictions, we had (iirc) 5 people in the \"below 35\" age group what died 'with covid' (not necessarily because of it). For comparison, that's way less than traffic deaths. Even way less than suicide rates in that age group (and we all know how bad the lockdowns were for many peoples' mental health). We also had a 20yo girl die because of vaccine-induced clots. She got vaccinated, because she had to (couldn't use public transport without either a vaccine or getting tested every 48 hours, and there was no testing location near her to get tested). Let's not mention all the other promises. We took out a good year of schooling and socializing for the whole generation of kids, with empty promises of \"when we get vaccines, you won't be able to infect grandma\". reply zimpenfish 5 hours agorootparent> In my ~2mio pop country, by the end of restrictions, we had (iirc) 5 people in the \"below 35\" age group what died 'with covid' By way of contrast, as of 2023-04-28[1], England had 157 deaths per 100k in the \"below 30\" age group. Which on a population of 57M[2] gives about 89k deaths. Your ~2mio pop country did exceedingly well if your numbers are correct - scaled to England, you'd have expected about 3000 deaths. [1] https://www.gov.uk/government/publications/covid-19-reported... [2] https://www.ons.gov.uk/peoplepopulationandcommunity/populati... reply K0balt 7 hours agorootparentprevCount yourself fortunate. Your country probably avoided the much more dangerous first wave that I assume was intentionally spread by China as part of their strategy to reduce the economic impact of the epidemic. As an unfortunate anecdote that formed this rather bold and controversial opinion , I had the misfortune of being of a plane literally full of wuhan residents flying to the USA (strangely via the Dominican Republic) in December. I was curious and asked my seat mate what was the big event, and he explained to me that (some organ of government that I didn’t understand) had created an incentive for companies in Wuhan to send people to conferences in the USA that included paid travel and accommodations, and made the companies that participated eligible for some other program that I also did not understand (my new friends English was not great and at the time I was only mildly curious). At any rate, according to my companion there was social media buzz among the travelers complaining about weird routing of their flights through other countries and long layovers, speculation that it was an example of government cost aversion or something. I didn’t think much of it at the time. Within days of landing, I had the worst “flu” of my life with the signature COVID symptoms. A few days later the epidemic started to hit the news in earnest, and people around me started dying. The early versions of COVID were much more aggressive. Should be fun to watch Chinese social media agents vote this down into invisibility lol. But I saw what I saw, I’m not going to pretend I didn’t. reply flanked-evergl 8 hours agorootparentprevThe plural of anecdote is not data. reply 71bw 8 hours agorootparentprevI thought we are in a place where people seem to express some sort of intelligence, and yet, reality always finds a way... reply K0balt 7 hours agorootparentIgnorance. Ignorance always finds a way. Also, HN is rife with political trolls from China, Russia, the USA, and other countries that fund state-sponsored social media programs. reply HPsquared 8 hours agorootparentprevAll wars target the young and healthy. reply Cumpiler69 7 hours agoparentprevFalse. Inflation was mainly caused by the central banks(especially the US FED but also the ECB who followed suit) devalued the currency by over-issuing it during the pandemic, not due to the post pandemic high energy prices which are not that high when you adjust for the crazy Inflation the excessive money printing generated. In short, they barrowed money in your name with you as a guarantor and now you're paying for it, it's that simple. The war caused by Russia is just a convenient scapegoat that's easier to sell to the financially illiterate population to deflect the blame. Keep in mind most voters don't understand basic economics and how currency supply affects inflation, but they do understand \"Russia dropping bombs = bad\". When 80% of the entire world supply of USD (the world reserve currency that the EU also has to use) was printed during the pandemic, how can anyone say that Russia's war caused the inflation? Do people not know arithmetic anymore? https://fred.stlouisfed.org/series/M1SL reply 1986 6 hours agorootparentMoney supply increased significantly in 2020 for sure but that's NOT what you're seeing in the M1 graph. M1 was revised to include savings accounts at the same time and this is the major source of the discontinuity: https://fredblog.stlouisfed.org/2021/05/savings-are-now-more... M2 captures the pandemic influx better and is significantly less dramatic: https://fred.stlouisfed.org/series/WM2NS reply riffraff 7 hours agorootparentprevDo you think that since M1 is down 13% since peak we should be seeing deflation right now, or does M1 growth only impact inflation one way? reply Zanfa 7 hours agorootparentprevIt would be stupid to discount the effect that artificially limiting energy exports and using it for blackmail before and during the full-blown Russian invasion is naive. IIRC my nat gas prices went up like 10x compared to the previous year. reply _heimdall 7 hours agorootparentprevWhile I would also lean towards blaming the US Fed, how can you be so confident in precisely what caused inflation? One of my big issues with economics as a \"science\" is that they try to boil down massively complex systems into a handful of numbers. When it comes to global economics and geopolitics the system is even more complex. How would we ever be able to say any particular time of inflation was causes by exclusively, or primarily, any one factor? At best looking at historic data and seeing graphs that move together show correlation, they will never show causation. reply dennis_jeeves2 5 hours agorootparentprev>Do people not know arithmetic anymore? They never knew it. reply ttyprintk 7 hours agorootparentprevThe European Central Bank said it’s due to energy. reply johngladtj 7 hours agorootparentThe people who caused the problem say they weren't responsible for the problem. Do you see the issue here? reply ceejayoz 4 hours agorootparentThat goes both directions, though. Central banks are prone to understating their responsibility for bad things; the gold bugs and crypto hodl folks are prone to overstating it. The answer likely lies somewhere in the middle. The US's relatively soft landing post-pandemic seems to demonstrate some use of monetary policy in this fashion works. reply Cumpiler69 7 hours agorootparentprevOf course they'd say that. Did you expect them to just say \"yeah, we fucked you over by devaluing the currency causing your wages to be worthless\"?. Don't be naive please and look into how much currency was issued during the pandemic and see for yourself. reply sofixa 7 hours agorootparentprevUtter nonsense, and I say this as nicely as I can. Check the price of Russian gas before and after the invasion, and the resulting price of electricity: https://ec.europa.eu/eurostat/statistics-explained/index.php... Combine with the fact that Russia and Ukraine are some of the biggest exporters of many critical raw materials, like importantly, foodstuffs (wheat, sunflower oil, etc), and steel, aluminium, oil, gas. Hell, there was a crisis in availability of mustard and snails in France due to the invasion of Ukraine (and on the mustard, a series of bad harvests in Canada which further complicated things). We can even clearly see it in the inflation charts, first there was growth in energy inflation towards the end of 2021 (as things were ramping back up from Covid), then a big spike in 2022 due to Russia's invasion, and then over 2022-2023, related spike in other sectors: https://ec.europa.eu/eurostat/statistics-explained/index.php... To pretend none of this had no impact whatsoever is wilful ignorance. reply Cumpiler69 7 hours agorootparentBruh, 80% of all USD in existence was issued during the pandemic alone. How the hell can you tell me with a straight face that that didn't cause the inflation? I feel like I'm taking crazy pills. https://fred.stlouisfed.org/series/M1SL reply _hl_ 7 hours agorootparentRead the notes in the link you posted. I don’t think it says what you think it says. In May 2020, the definition of M1 (monetary supply in “cash”) was changed to include savings deposits. They changed this not due to some conspiracy, but because savings accounts were deregulated to remove withdrawal limits, effectively rendering them cash-equivalent, and thus necessary to include in M1 metrics. I.e. the 80% spike has nothing to do with money being printed. reply sofixa 7 hours agorootparentprevBad faith argument again, or at least terrible tunnel vision. So what? In the EU a lot of that money went into the Recovery fund, which released the funds in multiple steps (only the first one was in 2021), and a lot of it is still remaining in the fund. How do you explain the massive inflation in the EU then? And are you seriously that centred on \"money printing\" that you cannot imagine gas and oil prices raising multiple times, and the disappearance of multiple critical raw material suppliers, had _no impact whatsoever_? reply weberer 6 hours agorootparent>Bad faith argument again Stop saying this whenever somebody disagrees with you. That's not what that term means at all. reply sofixa 2 hours agorootparentI'm not saying this because they're disagreeing with me. Inflation is probably one of the most talked about topics of the past few years, I find it impossible that people haven't heard about the multi-layered complexity of it, and thus anyone blaming it purely on \"money printing\" has to be acting in bad faith. reply theultdev 2 hours agorootparentIt's not really complex, GP is correct. High energy prices cause high transportation and manufacturing costs which causes everything else to rise. But the main cause of inflation is printing of money, especially when you introduce such a large amount in such a short time. People used to know that, they either pretend it's not true now or they're ignorant. And yes the \"bad faith\" parrot line is really annoying and doesn't contribute to the conversation. It's what people say when they don't have a rebuttal. reply sofixa 2 hours agorootparent> It's what people say when they don't have a rebuttal Only I have a rebuttal, and came with receipts from Eurostat. So what exactly are you trying to argue? > But the main cause of inflation is printing of money, especially when you introduce such a large amount in such a short time. The original premise (which is still in bad faith, however much you dislike that part) was that money printing was the main cause. And this is fundamentally and provably wrong (check my comment upthread, Eurostat inflation per sector with the timeline). Inflation was kickstarted by energy inflation which coincides with the Russian invasion. Did money printing contribute? Of course. Did Russia's invasion of Ukraine and all the issues it brought in energy prices and food prices? Of course. Did the Houthis contribute with their attacks disturbing supply chains? Probably. Did Covid contribute with all the supply chain issues it caused? Of course. Trying to pin it solely or mostly on one single reason, especially when it is a global phenomenon that many countries suffered from regardless of their exact specifics (e.g. Sweden's monetary policy was not the same as the EUs nor Canada's, yet they all suffered from serious inflation) is arguing in bad faith. It's so trivially provably wrong, it's not even funny entertaining people who are wrong. reply readyplayernull 6 hours agoprevIt all comes down to math: https://stephaniekelton.substack.com/p/how-to-cut-2-trillion... reply openrisk 6 hours agoparent> To ensure that interest expense falls toward zero over time, Congress could instruct the U.S. Treasury to stop issuing anything with duration beyond a 3-mo T-bill. Voilà! It wouldn’t just save $2 trillion, it would save tens of trillions of dollars over time. umm, not sure if her recipe is meant as a joke (I mean the world is rapidly turning into a bad joke anyway so people getting facetious might be a defense mechanism) but eliminating risk-free money for anything beyond 3 months seems like very... short-termist? No idea what kind of volatility that would do to the broader financial / economic system (including e.g. mortgage finance) but somehow it doesn't sound good. reply nabla9 7 hours agoprev>Today, we control inflation with changes in interest rates, not changes in the quantity of money. That's not full truth. In the last 20 yeas central banks do their big and sudden moves using \"Open Market Operations\". They buy or sell money like assets in market and effectively increase or limit the quantity of money. reply atq2119 7 hours agoparentOpen market operations are the mechanism by which the interest rate is controlled. Basically, the central bank sets an interest rate target and then performs open market operations until the interest rate matches the target. That obviously affects the quantity, but the point is that the target is the interest rate. The quantity just ends up being whatever happens to be necessary to hit the interest rate. reply nabla9 6 hours agorootparentThe target is inflation in both. When you reduce the volume of assets available, or the price renting the asset, you increase it's value. In this case the asset is money. Market interest rate is a signal how effective the action is long before inflation statistics is available. reply neilwilson 7 hours agoprevThis is all very interesting historically. However the rise of repo markets has rendered the money=medium of exchange, bond=store of value belief pretty much redundant. Rehypothecation more so. Banks are liquidity providers. If they think they can make a turn they'll discount any asset into money for you. reply j4nitor 8 hours agoprevTorille! https://www.urbandictionary.com/define.php?term=Torille%21 reply dathinab 8 hours agoprev> To our modern sensibilities, this is a wildly invasive policy. is it? not really cutting the \"value\" of money in half always had been a important emergency tool countries had and sometimes used and \"moving\" half of the value into a found which even pays out some years later is tbh. quite a fair way to do it (instead of just literally halving the money value permanently) reply flanked-evergl 8 hours agoparentFixing inflation is simple, people keep voting for it to not be fixed, so the inflation remains and gets worse. reply BJones12 2 hours agoparentprevYes it is. It's the biggest wage theft in history. reply flanked-evergl 8 hours agoprevI live in Norway, most people I know in private life that are of working age do not actually work, and for the most part they have better lives than I do, and I do work. The amount of people on sick leave have absolutely skyrocketed. 60% of welfare benefits in Norway go to immigrants and the population in cities grows faster than new homes are built. The causes and solutions of inflation are not complex. People just don't want it fixed. reply thaumasiotes 9 hours agoprevSomehow this author has come to the conclusion that price controls are the \"solution\" to inflation. This is a fundamental misunderstanding of why inflation is viewed as bad. reply gostsamo 9 hours agoparentThe article talks about history of alternatives to the interest rates, mainly controlling the currency supply and how it might be implemented in the future. How you discovered price control as the solution in there is still a mystery to me. reply shigawire 8 hours agorootparentI think they are considering the last part of the article to be \"price controls\". If the government prevents people from buying certain goods by selectivity freezing certain purchases I'm not sure I'd call that a price controls -- more like a prohibition. But I could see how this could be done similarly more like a price control. If the control was this granular, then maybe car purchases could be limited to $30,000 instead of blocked fully. This is effectively a price control. Also - the author notes in the comments the post is a prognostication, not endorsement. That said, much like the original Finnish plan, I have no idea how you'd implement this without massive loopholes. I'd imagine even if there were merits to the policy it would fail on account of the difficulty in implementation. I wonder if it is more reasonable if there was equally a carrot to go with the stick - something analogous to the bond portion of the Finish approach. reply pjc50 8 hours agorootparentIn this context, all European countries including Finland were subject to rationing during the war; the question is about how to phase out both rationing and price controls without having a huge discontinuity at that moment. reply eru 8 hours agorootparentOr you can just accept the discontinuity, because sometimes the cures are worse than the disease. reply thaumasiotes 8 hours agorootparentprevYes, the solution he advocates is \"we freeze your assets, allot you a certain basket of consumer goods, and take what we consider the appropriate price out of your frozen assets\". If you'd rather describe that as \"communism\" than \"price controls\", feel free. The theme of the whole piece is that, if you don't allow people to pay more for things, then the price of those things won't rise, and that this is some sort of policy victory. It's a very stupid viewpoint; seeing prices fail to rise because you redenominated the currency means nothing. Seeing prices fail to rise because you prohibit that, on the other hand, doesn't mean nothing - instead, it means your economy is collapsing. reply Lvl999Noob 7 hours agorootparentThe post is not a study or a solution or a suggestion or anything of the sort. The author clarified in the last portion that it is a prediction. Someone predicting the bad consequences of the current direction is not advocating for that direction. reply gostsamo 7 hours agorootparentprevprice control would be if they forbid some goods to be paid beyond certain price where the goal is to regulate the distribution of those goods. Currency supply control is a general policy targeted at the total of goods one can pay for in order to maintain the overall economic activity. This is the difference between heating certain rooms in the house and causing global warming. reply Yawrehto 6 hours agoprevI wonder what the cleansing element of it in Finland was, given that they voted to join the Axis. reply Seasandthequote 3 hours agoparentyou mean WWII? If so, this is just wrong, no-one in Finland \"voted\" to join the Axis, she was not technically a part of Axis and didn't want to side with Germany at all. GB and Sweden were first asked but they weren't able to help in the defence from Soviet Union. AFAIK they weren't fighting anywhere but their own territory reply noinsight 42 minutes agorootparentFinland did cross the old borders in parts, but we specifically didn’t participate in the Siege of Leningrad and refused all German demands for assistance. reply dr_dshiv 7 hours agoprevWithout getting political, please, does anyone have a good argument for the expected inflationary pressures of the next year or two? Tariffs will make prices go up, investment in infrastructure will make prices go up… but on the other hand, AI & robotics seems to be a deflationary pressure… where does one go for scenario analysis of the next year or two? This article scared me a bit with the notion of banks implementing “quantitative freezing.” reply dfxm12 2 hours agoparentIn addition to tariffs, expelling millions of people out of the country will have inflationary effects as well, in many different ways [0]. 0 - https://www.nytimes.com/2024/11/13/business/economy/trump-im... reply pjc50 7 hours agoparentprev> Without getting political > inflationary pressures of the next year You can't really separate these two. If central bank targeting is left alone and the policies implemented aren't too disruptive (i.e. not the wild claims of the campaign), then it won't move much. If some of the wilder claims are implemented, all bets are off. reply marcosdumay 3 hours agoparentprevOk, you mean on the US... Keep in mind that the US government (like almost the entire world) has control of inflation, and through more means than you will think of. Anyway, all of those are real factors, but inflation is a monetary phenomenon. Those two don't need to have any kind of correlation. Personally, I have not follow US news closely enough to understand what Trump wants to do with monetary policy. reply labster 8 hours agoprevI wonder if anyone left his uncut so he could show it off later? It would certainly be more attractive to numismatics later on. reply lutusp 1 hour agoprev> ... snipped their cash in half ... Actually, they snipped their cash in two, not in half. This is one of several afflictions that lurk in modern language, ignored by nearly all. Another is the use of phrases like \"similar effect to ...\" where \"effect similar to ...\" is the correct form. Notwithstanding how it grates on one's ear, this second egregious malaprop seems to be the preferred form. Oh, well. Since print is today either dead or dying, largely replaced by chatbot-generated prose, this kind of complaint might be likened to critical assessment of a cave drawing. reply poincaredisk 18 minutes agoparentThis is a standard English idiom: https://dictionary.cambridge.org/thesaurus/cut-in-half. You may not like it, or think it's illogical, but that doesn't make it incorrect. reply mock-possum 8 hours agoprevThe convenience of cutting paper money in half is a really anachronistic element of this tale - I’ve got a fair amount of money saved up, and approximately none of it exists as paper, so much as it exists ‘on paper’ - that is, as figures marked in a bank’s digital ledger, somewhere in a server farm. How would an effort like this be handled today? … a new crypto currency? reply jsnell 8 hours agoparentThe tricky part isn't money on a digital ledger. That's easy enough to handle with e.g. a one-off deposit tax (IIRC used as recently as the Euro crisis). There's no operational problem here, it just needs to be legislated to happen. Executing the operation properly might take a while (it's not something they'd have a process for), but banks must already have in place systems for e.g. freezing assets which could be used to buy time. Bonds can just have a haircut on their nominal value, which is pretty much standard operation procedure during a financial bailout. The real problem is deposits in foreign banks in foreign currencies. In the modern world by the time a country would be looking into this kind of a measure, a lot of the capital will have already fled. In this case the blocker is jurisdiction / sovereignty, not any kind of technical limitation. reply eru 8 hours agorootparent> The real problem is deposits in foreign banks in foreign currencies. Well, money abroad doesn't contribute to local inflation, does it? reply jsnell 6 hours agorootparentI took the question to be on the logistics of executing this kind of operation with digital ledgers, not on when/whether those operations make sense. Confiscating foreign assets would do little[0] to reduce inflation. But it's the same for local assets. Obviously just chopping off a zero from every note and bank balance doesn't actually reduce inflation, unless accompanied by some other structural changes. [0] I say \"little\" rather than nothing, since it could have the effect of repatriating the money -> increasing the exchange rate -> making imports cheaper. But I can't imagine the effect being strong. reply ben_w 7 hours agorootparentprevIt can do; everyone you export to and import from still has the same money with which to buy and sell, and the same goods have different prices than you'd expect from just exchange rates in different markets. reply nextlevelwizard 8 hours agoparentprevas the article states, it didn't really work back then either as the bank accounts were not touched. > This stock of notes only comprised 8% of the total Finnish money supply reply _heimdall 7 hours agoparentprevThat is basically one of the conspiracy theories I have heard related to central bank digital currencies. As the tale goes, eventually major banks will run into another financial crisis (possibly intentionally if you really like conspiracy theories). The government will say they have no choice but to step in, and their solution will be to open the federal reserve to the public as a government-run bank. All funds lost in the crisis would be covered under an extended FDIC program and the money would be waiting for you in your new Fed bank account, denominated in the newly created CBDC. --- In no way am I vouching for the theory, just sharing it as it is very relates to you question of how it could be handled. reply kmeisthax 1 hour agorootparentThe weirdest problem I see with this conspiracy theory is that I don't see the actual conspiracy? Like, USD is a fiat currency, it's backed by taxes. The only difference between a paper dollar in your hand, a dollar in a traditional bank account, and a dollar on the hypothetical FedCoin blockchain is how easy it is to spend, what gatekeepers are involved, and what anti-fraud/anti-theft tools you can avail yourself of. Moving from a traditional bank account to a CBDC doesn't make it any easier for the government to seize your funds - they already can do that just fine with basically anything. reply _heimdall 18 minutes agorootparentI believe the conspiracy portion of that theory is that the financial crisis would be triggered, or allowed, intentionally to create the excuse for a CBDC and fed bank. I don't see it as likely, just a theory I've seen floated. reply krupan 4 hours agoprev [–] If this second thing last paragraph doesn't make your skin crawl then I don't know what will: \"Cash, which is awkward to immobilize for policy reasons, will be gone in a decade or two, leaving the public entirely dependent on bank deposits and fintech balances which, thanks to digitization and automation, can be easily controlled by the authorities. To rein in a jump in inflation, central bankers will require commercial banks and companies like PayPal to impose temporary quantitative freezing on their clients' accounts, but unlike Finland's 1945 blockade, the authorities will be able to rapidly and precisely define the criteria, say by allowing for spending on necessities — food, electricity, and gas— while embargoing purchases of luxury cars and real estate\" reply mandibles 3 hours agoparentThe quiet part is that you will not be able to spend on those necessities when your politics no longer align with the regime. Total control over money is a guaranteed path to totalitarianism. reply marcosdumay 4 hours agoparentprevI'm quite confident that governments aren't that powerful. They can issue money, and manage their currency. But I've never heard about any government actually controlling what currency people use. Not even extremely authoritarian ones. reply semprity 3 hours agoparentprev [–] Indeed makes my skin crawl. How can one reasonably protect himself from such draconian freezing? Asking for reasonably, not bunker full of gold bars. reply carlosjobim 1 hour agorootparent [–] Bunker full of gold bars. Or foreign currency. Or silver. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "In 1945, Finland implemented \"setelinleikkaus,\" a policy requiring citizens to cut large banknotes in half to combat post-WWII inflation, with one half retaining value and the other becoming a government bond.- The policy was ineffective as it only targeted physical cash, a minor part of the money supply, unlike more successful reforms in other European countries like Belgium.- The article speculates that future inflation control might involve digital account freezes, a more precise but potentially controversial method."
    ],
    "commentSummary": [
      "In 1945, Finland uniquely addressed inflation by physically cutting banknotes in half, contrasting with modern methods like adjusting interest rates.",
      "This historical approach has sparked discussions on current monetary policy, where central banks manage inflation through interest rates and open market operations.",
      "The potential future control of digital money, possibly via central bank digital currencies, raises concerns about increased government oversight of personal finances."
    ],
    "points": 145,
    "commentCount": 131,
    "retryCount": 0,
    "time": 1732611120
  },
  {
    "id": 42239487,
    "title": "Prayer, Placement, and Absolution: Peter Hristoff on Islamic Prayer Rugs (2015)",
    "originLink": "https://www.metmuseum.org/perspectives/peter-hristoff-islamic-prayer-rugs",
    "originBody": "Peter Hristoff. Untitled, 2005. Wool; 159 1/2 x 73 3/4 in. Private collection «At a recent MetFridays event in the Galleries for the Art of the Arab Lands, Turkey, Iran, Central Asia, and Later South Asia, I spoke about prayer rugs (seccades)—not as a scholar of the Islamic arts, but as an artist. In 1997 I started a series of drawings based on my assumptions of what people pray for and why they pray. I eventually turned these drawings into a suite of serigraph prints entitled Ten Prayers that I exhibited, in September 1998, at my first one-man show at the Yapi Kredi Cultural Center's Kazim Taskent Gallery in Istanbul. These works then led to a series of larger \"rug\" pieces done on rice paper, which combined the motifs I was using in my paintings (masks, birds, skulls, stylized flowers, cosmological symbols, and figures) with the formal structure of Anatolian carpets.» My interest in halis (rugs) and kilims (flat weaves) was a natural connection to the journal-like quality of my work. I was fascinated by the diarist elements in traditional Turkish carpet making—the weaver incorporating events, personal beliefs, hopes, and desires with traditional regional symbols into their work—an approach I incorporated (and still incorporate) into my art making. I began to work on rug-inspired prints a few years later, in which I would, on a daily basis, complete a horizontal band of the composition that recorded my interests, personal mythologies, and artistic preoccupations. I always had the intention of eventually creating actual halis and kilims in Turkey, as the notion of the seccade (prayer rugs) particularly fascinated me—an object that creates a sacred space wherever it is placed, and is charged with hope and spiritual connections as well as a physical relationship to the body and geography. Left: Peter Hristoff. Untitled, 2005. Wool; 85 x 51 in. Collection of Ömer Özyürek. Right: A rug maker shows one of her pieces in a fabric shop in Tokat, Turkey, June 2015. Photo by the author The ritual of placement, prayer, and absolution all tap into issues I have been addressing in my work since the early 1980s, which make the seccade—an object of contemplation and decoration—an obvious focus in my work. I am interested in \"bright sadness,\" a term I came upon in the Patriarch Bartholomew's statement in the exhibition catalogue that accompanied Byzantium: Faith and Power (2004), which most accurately describes my interests. In the publication, Bartholomew writes: \"This refers to a mixed emotion of joy, over the anticipated help from God and Salvation, and sorrow, for the suffering of life and sin.\" This juxtaposition of the spiritual and the physical, the happy and the sad—mankind's conflicted nature—continues to be a predominant theme of my work. Peter Hristoff. Ten Seccades, 2011. Wool; variable sizes. Collection of the artist Peter Hristoff. Prayer Niche, 2007 Read all blog posts related to Peter Hristoff's residency at the Met.",
    "commentLink": "https://news.ycombinator.com/item?id=42239487",
    "commentBody": "Prayer, Placement, and Absolution: Peter Hristoff on Islamic Prayer Rugs (2015) (metmuseum.org)123 points by handfuloflight 22 hours agohidepastfavorite61 comments kamikazeturtles 19 hours agoI love this sort of writing. Bringing attention to something you don't pay too much attention to, even as a muslim. My prayer rug has tessellation patterns, probably a testament to God's infiniteness. Where I'm originally from in Diyarbakir, Turkey, the Christians also use prayer rugs when prostrating (to, I think, Jerusalem) so this isn't an exclusively \"Islamic\" thing. reply noufalibrahim 14 hours agoparentIslam generally has a prohibiton on representational art. Most of the artistic skills were pulled into calligraphy and tesselations. It's why these show up in almost all Islamic artifacts. reply karim79 14 hours agorootparentCan you please elaborate on 'representational art' in this context? reply dieppe 8 hours agorootparentThere's a _great_ book about that: https://en.wikipedia.org/wiki/My_Name_Is_Red Helps understand how arts, religion, and power are intermingled. Made me question what we mean by \"tradition\" ; they always seemed fixed to a present observer, but are also the result of constant change. Anyway, there's more to the book and it is really a great read (maybe more so if like me you know close to nothing about the Ottoman empire). (and just to be clear: this book talks about what was admitted as acceptable art at a specific period in a specific region where Islam was the state religion, not commenting on the art in Islam as a whole as I would be incapable of doing so) reply Cyph0n 14 hours agorootparentprevIn the context of Islam: art that depicts living things, especially humans. reply karim79 14 hours agorootparentFrom Wikipedia[0]: \"The Quran, the Islamic holy book, does not prohibit the depiction of human figures; it merely condemns idolatry. Interdictions of figurative representation are present in the hadith, among a dozen of the hadith recorded during the latter part of the period when they were being written down.\" [0] https://en.m.wikipedia.org/wiki/Aniconism_in_Islam#:~:text=T.... reply Cyph0n 14 hours agorootparentIn Sunni Islam, the Hadith is essentially a companion source to the Quran - in particular, Sahih Bukhari and Sahih Muslim. But I should have been more specific. I think Shia art has the same restrictions though - could be wrong. reply noufalibrahim 12 hours agorootparentIslamic canon law (the Shariah) is derived from primary texts (the Quran and Hadith) and uses interpretive methodologies (like deductive analogy - Qiyas) to derive rulings that are not explicitly discussed in the primary source material. This gives some amount of openness and causes some differences of opinion resulting in multiple schools of thought. Practicing Muslims rely on books and texts that systematize this knowledge and provide it in a practical form for daily use rather than go to the source material (which can be overwhelming if one wants to find, for example, the ruling on whether a certain action in a specific context is permissible or not). Most Muslim children, as part of their basic religious education, learn the basics of Islamic law and practice from such a book. That's usually enough to go through ones life. The ruling on representational art is based on this kind of derivation. There are some exceptions (e.g. for educational, security etc. purposes) but they're generally narrowly circumscribed. Sunni tradition relies primarily on the Quran and 6 books of Hadith (of which the two you've mentioned are the main ones). Shia tradition has a smaller Hadith corpus because of theological differences about the reliability of the chains of narration of the Hadith and hence the derived rulings are very different in some areas. I've generally seen Shia works of art where prophetic companions, angels etc. are pictured but non-realistically (unlike Christian iconography). Some of them blank out the faces but it's not a tradition I'm deeply familiar with so I don't know. My theory is that the huge emphasis on calligraphy and tesselations in Islamic art is mainly because of this. We don't have (many) paintings and sculptures of religious figures like in the Christian traditions. reply noduerme 8 hours agorootparent>> That's usually enough to go through ones life. Nit, perhaps, but relying on third parties to interpret the source material, and only learning what is supposedly permissible from secondary texts without learning how to engage with and argue over the subtler aspects of primary texts, is not much of a life. If you want to say it's enough for most people, you must expect most people to accomplish very little. Guess my religious background. reply flir 4 hours agorootparent> Guess my religious background. Protestant? (Based on the emphasis on a personal experience of the text, unfiltered by mediators). reply noufalibrahim 6 hours agorootparentprevIt's a larger discussion but not really HN material so I'll leave it at that reply MrMcCall 10 hours agorootparentprevOnly the Quran is guaranteed to be unchanged (but the interpretations and translations are not so guaranteed), the Hadiths are not. The proper perspective is that if the Hadith and Quran are in conflict, the Quran is the authoratative source. From our Sufi perspective, the Hadiths are a lot of game of telephone. And if the Quran is difficult to apply, remember that compassion is the entire purpose of all God's religions. reply Workaccount2 4 hours agorootparent>And if the Quran is difficult to apply, remember that compassion is the entire purpose of all God's religions. Clearly a few of gods prophets dropped the ball pretty hard on this one. To bad he isn't interested in correcting the record. reply syspec 1 hour agorootparentprev> And if the Quran is difficult to apply, remember that compassion is the entire purpose of all God's religions. Or, in practice, the opposite reply NickC25 4 hours agorootparentprevIt does and it doesn't. AFAIK Shia doesn't specifically ban all images outright, but instead relies on context. For example, the Prophet Muhammad (PBUH)* - Sunni Islam bans all images of him and other prophets, while Shia Islam might allow some images as a reference to him or other prophets as historical figures such as Jesus, Adam, Moses, David, etc... which they undoubtedly are. That's why some images of prophets are basically a white outline surrounded by fire, or a figure with a veil over their face. As long as you aren't defaming or purposely offending the religion, it's frowned upon in most cases, but not explicitly banned due to the historical context - humanity, after all, did and does rely on images instead of words at times and did so when most people were illiterate but could understand images. As long as the prophets aren't made into idols or worshiped or defamed, and are illustrated for historic context in a non defaming way, it's OK, sometimes. *I say this while not Muslim to avoid the notion that I am purposely offending Islam - I'm not. reply cess11 4 hours agorootparentprevShiites often depict their important religious figures and martyrs, but with the face replaced with a bright glow. Like this: https://www.deviantart.com/shia-ali/art/YA-ALI-343625919 It's also not uncommon to depict e.g. Ali with a face. Sunni salafist movements are the ones that are fanatically against such art, and they are quite savvy with propaganda and backed by very powerful friends on the peninsula and elsewhere, such as the backers of the Gulf dictatorships in London and Washington. reply 2-3-7-43-1807 4 hours agorootparentprevlike this: https://en.wikipedia.org/wiki/Buddhas_of_Bamiyan#Destruction... reply renewiltord 14 hours agorootparentprevIdolatry. Graven idols. That sort of thing. Hence geometry instead. reply rayiner 5 hours agoparentprevI remember my grandmother carting her prayer rug all the way from Bangladesh to the U.S. when she came to visit. reply nicbou 7 hours agoparentprevI look for this sort of thing whenever I travel, but it’s really hard to find. reply uptownfunk 17 hours agoprevIn ancient times Hindus had a prayer mat made of kuusha or kaasha grass. Apparently the grass descended from the hairs of the boar incarnation. So this notion of a seat in worship is indeed very ancient. reply mkoubaa 17 hours agoprevIn all the years of using one I never thought it made anything sacred. It keeps my forehead clean and cushions my knees reply PrismCrystal 17 hours agoparentThis reminds me of the practice in Egypt where men would intentionally press their heads into the ground when praying in order to develop a callous, colloquially called a zebiba, showing off their piety. Is this still fashionable? When I traveled rural Egypt in 2008 it was omnipresent; one shopkeeper told me he no longer believed in religion, but he had to develop a zebiba nevertheless, otherwise no one would buy from his shop. reply umeshunni 15 hours agorootparentInteresting. TIL http://news.bbc.co.uk/2/hi/middle_east/7469221.stm reply mkoubaa 8 hours agorootparentprevI knew people who \"groomed\" a zebiba, for lack of a better term. I don't doubt that some people who have one are not doing it for show (otherwise who are they emulating?). Whether or not it's fashionable depends on your social circle, I assumr reply pushupentry1219 14 hours agoparentprev(I'm a Muslim). I mean... Strictly speaking as far as i am aware the mat/rug is not sacred itself. It is just a place to put your head +knees comfortably. In fact the Prophet Muhammad SAW did not use a prayer mat and instead he placed his head on the soil (ground/earth). Also note; Shias place their head on a stone/rock to mimic this practice: https://en.m.wikipedia.org/wiki/Turbah reply MrMcCall 10 hours agorootparentYes, the prayer rug is not sacred, per se, but it can be embued with a measure of sacredness by our practices upon it. Please know that Zikr (remembering/repeating one of the many Names of God) is the highest form of worship. (Zikr is commanded three times in the Quran-i-Kerim.) Al-lah, Yah-weh, Di-os, De-us, Brah-man, ... Many Names in our various languages for the one Creator, one human race, one religion of God: compassionate service to all mankind that comes in many forms across our planet's cultures and epochs. Always love. Teach to always love. Never hate. Teach to never hate. reply mkoubaa 8 hours agorootparentprevYeah I think the author of this article had some subjective understanding and experience. I don't mean to diminish it but it's definitely something I and other (but not all) Muslims find foreign reply smusamashah 7 hours agoprevOnly thing I can add to discussion is that this rug is called \"place of prayer\" in Urdu (which is Jaaey Namaz or جائے نماز). There is often picture of Kaba on it but some people don't agree with that and prefer not using those rugs. reply kasey_junk 19 hours agoprevOne thing I’ve been curious about for a long time is if the use of prayer rugs in the Orthodox Churches predates Islam or was picked up by Christians from their Muslim neighbors. reply pvg 19 hours agoparentThat whole bundle of traditions - prayer times, prayer directions, prayer accessories, etc predates Islam if for no other reason than that Islam is comparatively new. But the cross-pollination could have easily happened more than once and in both directions. reply handfuloflight 19 hours agorootparentIn fact the Muslims initially prayed towards Jerusalem (as did the Jews then and today) until revelation specifically turned them towards Mecca: > \"So turn your face toward al-Masjid al-Haram (the Sacred Mosque in Mecca). And wherever you are, turn your faces toward it...\" Quran 2:144 reply ralmidani 16 hours agorootparentprevEven Muslims do not claim their religion is completely novel. The rituals may differ, but the creed preached by Muhammad (peace and blessings upon him) is the same preached by Adam, Noah, Abraham, Ismail, Isaac, Jacob, Joseph, Moses, Aaron, David, Solomon, John, Jesus, and countless other prophets (peace be upon them all): worship The One True God (Allah, Yahweh) with no partners. As far as prayer rugs and other accessories, those are not actually part of our rituals as Muslims. Some people use them for practical purposes (prayer rugs help you avoid prostrating on dirt, asphalt, a potentially unclean carpet, etc. and prayer beads make it easier to keep count), while some others may have cultural reasons, and some just want to enhance their spiritual experience (e.g. incense and perfume). reply pushupentry1219 14 hours agorootparentMuslims say that Islam _was_ the religion of all the prophets you've mentioned. Because of this they also believe that Islam is the _oldest_ religion, since Adam, the first man, followed Islam. reply ralmidani 4 hours agorootparent“Islam” as a named religion with its own prescribed rituals and laws is specific to the message preached in Arabia in the 7th century. “Muslim” is a more transcendent term that encompasses all the prophets mentioned in the Quran, as well as those not mentioned. Their creed and state of mind (absolute submission to God’s will) is the same, but they did not follow a religion called “Islam”. Earlier prophets and their followers prayed, fasted, and gave charity. Some even made the pilgrimage to Makkah. However, certain details of these rituals may have differed between them and today’s Islam, and between one another. reply mensetmanusman 4 hours agorootparentprevFor completeness to the reader, the creed preached by Jesus is historically very different. Rejection of Jesus’s Divinity: • Islam acknowledges Jesus (Isa) as a prophet but explicitly denies His divinity or status as the Son of God. The Qur’an states: “He [Jesus] was no more than a servant: We granted Our favor to him” (Qur’an 43:59). • The Qur’an emphasizes that Jesus did not die on the cross but was raised to heaven by God (Qur’an 4:157-158). Etc., etc. Islam historically reinterpreted Jesus and rejects the accounts of the first followers of Christ (the Church Fathers circa 100-300 AD). reply ralmidani 57 minutes agorootparentThis is not by any means a “complete” picture. There was no consensus that Jesus is divine, or about the nature of the divinity ascribed to him, even after the declaration in 325 of the Nicene Creed - from which 5 bishops abstained and were at least temporarily exiled. This NPR interview with a former Evangelical who later became a historian and wrote “How Jesus Became God: The Exaltation of a Jewish Preacher from Galilee” is very illuminating: https://www.npr.org/2014/04/07/300246095/if-jesus-never-call... Excerpts: > During his lifetime, Jesus himself didn't call himself God and didn't consider himself God, and ... none of his disciples had any inkling at all that he was God. ... > You do find Jesus calling himself God in the Gospel of John, or the last Gospel. Jesus says things like, \"Before Abraham was, I am.\" And, \"I and the Father are one,\" and, \"If you've seen me, you've seen the Father.\" These are all statements you find only in the Gospel of John, and that's striking because we have earlier gospels and we have the writings of Paul, and in none of them is there any indication that Jesus said such things. > I think it's completely implausible that Matthew, Mark and Luke would not mention that Jesus called himself God if that's what he was declaring about himself. That would be a rather important point to make. This is not an unusual view amongst scholars; it's simply the view that the Gospel of John is providing a theological understanding of Jesus that is not what was historically accurate. > Right at the same time that Christians were calling Jesus \"God\" is exactly when Romans started calling their emperors \"God.\" So these Christians were not doing this in a vacuum; they were actually doing it in a context. I don't think this could be an accident that this is a point at which the emperors are being called \"God.\" So by calling Jesus \"God,\" in fact, it was a competition between your God, the emperor, and our God, Jesus. reply MrMcCall 10 hours agorootparentprevIt's all the Hanif Religion of Abraham, whose texts have been lost to time. Compassion is the only purpose of all God's religions, and is the ultimate arbiter of our life's chosen actions. reply chamanbuga 19 hours agoparentprevI didn't know Orthodox Christians prostrated in their prayer much less use prayer rugs. Curious where this practice remains today. reply raptorraver 12 hours agorootparentEspecially monks and nuns do prostrations as part of their prayer rule. Some do hunderds, some do even thousands prostrations during their prayers. We use prayer rope when recitating Jesus Prayer[1]. Prayer rope helps count the prayers but also it gives your hands something to do while praying, so it's easier to focus. I usually have one in my pocket and I roll it in my hands secretly while in meetings or sometimes even during typing code. I don't really pray then but it reminds me of the spiritual reality and that my boring Teams-meetings and stupid Jira-tickets aren't the purpose of my life ;) 1: https://en.wikipedia.org/wiki/Jesus_Prayer reply giraffe_lady 4 hours agorootparentThe widespread use of the prayer rope with jesus prayer in orthodoxy is very recent, like second half of 20th century. Both things are ancient but the rope was more associated with monastics and some specific balkan regions where they were popular. The jesus prayer has been common but the modern hesychastic application of it was basically practiced only by monks until JD salinger made the way of the pilgrim popular. You hear about this practice a lot on the internet and it is very familiar to english-speaking converts but this practice is not typical among for example greeks christians in greece, or even most russians I don't think but I'm less clear on that. reply handfuloflight 19 hours agorootparentprevThey would as per Matthew 26:39. > \"Going a little farther, he [Jesus] fell with his face to the ground and prayed, ‘My Father, if it is possible, may this cup be taken from me. Yet not as I will, but as you will.’\" reply sramsay 19 hours agorootparentprevProstrations are part of every Orthodox tradition, to my knowledge. You will even see people making prostrations publicly in church especially during Great Lent, but you will generally not see people doing it at a Sunday liturgy since (in most traditions) prostrations are forbidden on Sundays. reply raptorraver 12 hours agorootparentThere are variances in the traditions also here. In Russian tradition all the people in the Church do prostration during the eucharist prayers where wine and old are turned into Communion, and also before they partake the Communion. But in Greek tradition they don't do any prostrations during the liturgy. Prostrations aren't forbidden on Sundays but if you partake Eucharist you are not allowed to do prostration during that day. reply asadalt 16 hours agorootparentprevthere is something about prostrations that’s interesting but i can’t tell what exactly scientifically. But it brings me relief everytime I do it in namaz. I understand that the act of submission is relieving (things will be ok/there is someone looking out for me) but also physically (hard to explain) no wonder it’s the meat of the entire prayer. Taking a prayer break from my messy code problems really resets my brain strain very quickly. reply michaelsbradley 18 hours agorootparentprevThey're part of the Western (Catholic) tradition as well, but less frequently encountered: during the Litany of the Saints that precedes ordination to the priesthood those to be ordained are prostrate; the ministers at the start of the Good Friday liturgy lie prostrate before the altar; and a few other special contexts. reply _DeadFred_ 18 hours agorootparentprevEvery Orthodox service I've been to we stood the entire time, though I never went on special holidays. My inner ex-Catholic wondered is it really church if I don't kneel and stand, kneel and stand, kneel and stand all through the service? My Ukrainian ex had a worship space in a corner. It didn't have any rugs but had hauntingly beautiful hand painted icons. Maybe rugs are more of a thing in the old countries? reply PrismCrystal 17 hours agorootparentPractices in Orthodox services can differ. For example, in some Romanian parishes everyone has knelt while the Gospel was being read, but I have never seen this elsewhere. And as the other poster mentions, full prostrations are done (and widely across the Orthodox world) in certain contexts. Some churches in Greece and Albania have pews -- the concept was brought back from the North American diaspora where Orthodox parishes were set up in former Protestant or Catholic church buildings -- so you can do all the standing, sitting, and kneeling you might be accustomed too. reply jacobolus 17 hours agorootparentprevChurches probably have some kind of theological explanation, but alternately kneeling, sitting, and standing is much better physiologically. Having a large and diverse group of people all stay in the same position for an extended period is very rough on their bodies, and some more than others. Periodically switching prevents injuries caused by maintaining too much static load for too long in any one place. Probably also helps keep everyone awake. For the same reasons it's a good idea to occasionally switch positions while working. reply MrMcCall 10 hours agorootparentLoving God is not for God's benefit, but for ours. Our emanating love towards our Creator helps us emanate compassion for all our fellow human beings. It is the Greatest Command(ment), and the sole purpose of religion. As such -- as you say -- changing positions is good for our body which helps us to be more physically comfortable in this magnificent machine. Happiness is God's desire for us, but It has given us the absolutely free will to choose happiness or its opposites. A prayerful life is for personal and societal growth towards selfless compassion for all others, and away from selfish callous disregard for others. reply ashoeafoot 12 hours agorootparentprevit is all things but diverse https://en.m.wikipedia.org/wiki/Ethnic_groups_in_the_Middle_... reply giraffe_lady 16 hours agorootparentprevThere are theological explanations but it is also explicitly taught that physical movement and awareness of your body is an important part of prayer, similar to how bells and incense ask you to include those senses. Also there isn't a rule against sitting, orthodox churches have seats for people who need or want them and it's absolutely normal to see people sit for some or all of services. It is discouraged to notice who or wonder why. reply br3akaway 2 hours agorootparentprevAs a rule, Orthodox don't kneel on Sundays. Usually, weeknights during Lent you will see kneeling and prostrations. On a weekday liturgy (mass, always done in the morning) people will usually kneel at least for the Lord's Prayer (this is in the US) reply nashashmi 5 hours agoparentprevI believe the prayer rug comes from the Ottomans (uncited) who added prestige to the artifacts. After the conquest of Constantinople, the Ottomons adopted the Roman (Byzantine) Orthodox Church's symbol () to represent Islam (Crescent and Star). They adopted the church dome and bell tower for architecture of the mosque called dome and minaret. And now it seems they also adopted the prayer rug. (Fun anecdote: I traveled to Venice and the tour guide said the architecture of the St. Marks Basilica is different from the Catholic Churches of the rest of Italy as it resembles Islamic Architecture influences. Ha! It resembles Byzantine or Constantinople influences, not Islamic at all but quite similar.) reply mathfailure 1 hour agoprevnext [2 more] [flagged] keybored 1 hour agoparentWhat does your comment accomplish? In my experience people here have eclectict intellecual tastes. reply anArbitraryOne 14 hours agoprevnext [4 more] [flagged] dang 8 hours agoparentPlease don't do this here. reply robobro 14 hours ago [flagged]parentprevnext [2 more] Well, aren't you brave. reply dang 8 hours agorootparentPlease don't respond to a bad comment by breaking the site guidelines yourself. That only makes things worse. https://news.ycombinator.com/newsguidelines.html reply martin1975 7 hours agoprevnext [2 more] [flagged] Philpax 7 hours agoparentWow, thank you for posting this quote showing that Churchill was a right bastard. Really expanded my mind. reply htx80nerd 1 hour agoprev [–] Am I on a tech site? reply syspec 1 hour agoparent [–] Yes, you've been on here for all of 5 days. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Peter Hristoff's artwork delves into the intersection of art and spirituality, particularly through the medium of prayer rugs, starting from 1997.- His creations, including drawings and serigraph prints, evolve into larger \"rug\" pieces on rice paper, inspired by Turkish rugs and kilims, which he views as personal diaries with symbolic meanings.- Hristoff's work embodies \"bright sadness,\" a blend of joy and sorrow, influenced by Patriarch Bartholomew's writings, and continues to explore the spiritual and physical aspects of human nature."
    ],
    "commentSummary": [
      "Peter Hristoff's article explores the significance of Islamic prayer rugs, emphasizing their intricate designs, such as tessellations, which symbolize God's infiniteness.- The article discusses the Islamic prohibition of representational art, leading to the use of calligraphy and geometric patterns in Islamic artifacts.- It highlights the historical and cultural context of prayer rugs, noting their use across various religious traditions, including Christianity, and their practical and spiritual roles."
    ],
    "points": 123,
    "commentCount": 61,
    "retryCount": 0,
    "time": 1732563945
  },
  {
    "id": 42239263,
    "title": "Deno vs. Oracle: Canceling the JavaScript Trademark",
    "originLink": "https://deno.com/blog/deno-v-oracle",
    "originBody": "Deno v. Oracle: Canceling the JavaScript Trademark November 25, 2024 Ryan Dahl #FreeJavaScript On November 22, 2024, Deno formally filed a petition with the USPTO to cancel Oracle’s trademark for “JavaScript.” This marks a pivotal step toward freeing “JavaScript” from legal entanglements and recognizing it as a shared public good. If successful, the petition will eliminate barriers that have stifled community use of the name. Conferences could reclaim titles like “JavaScript Conference” instead of settling for “JSConf.” The language’s specification could finally drop the cumbersome “ECMAScript” moniker and be known simply as the “JavaScript Specification.” Communities like “Rust for JavaScript Developers” would no longer fear legal threats over their use of the term. The full petition is available here and is based on three claims: Claim 1: JavaScript is generic The term “JavaScript” has become the universal name for the programming language defined by the ECMA-262 specification. It is used globally by millions of developers and organizations, entirely independent of Oracle. By law, trademarks that have become generic cannot remain trademarks. Oracle neither controls nor has ever controlled the language’s specification or usage. Millions of developers, companies, and educators rely on JavaScript every day without Oracle’s involvement. As stated in the petition, JavaScript is not a brand; it is a cornerstone of modern programming. Claim 2: Oracle committed fraud When Oracle renewed the JavaScript trademark in 2019, it submitted fraudulent evidence to the USPTO. This included screenshots of Node.js—a project founded by myself and entirely unrelated to Oracle. Presenting Node.js as evidence of Oracle’s “use in commerce” violates the integrity of trademark law. The USPTO likely relied on this false evidence to renew the trademark, which potentially invalidates its renewal. Claim 3: The trademark has been abandoned Oracle has abandoned the trademark through nonuse. The petition demonstrates that Oracle has not offered significant products or services under the name “JavaScript” in years. Obscure offerings like the JavaScript Extension Toolkit or GraalVM, do not constitute genuine use in commerce. U.S. law considers trademarks unused for three consecutive years as abandoned, and Oracle’s inaction clearly meets this threshold. The petition builds on overwhelming community support. Over 14,000 developers, including JavaScript creator Brendan Eich, have signed our open letter urging Oracle to release the trademark. You can also read more about the history of this issue in my 2022 blog post. What Happens Next? Oracle has until January 4, 2025, to respond. If they fail to act, the case will go into default, and the trademark will likely be canceled. We sincerely hope Oracle takes this path, acknowledging that “JavaScript” belongs to its global community—not to a single corporation. However, if Oracle chooses to fight, we are ready. We will present a wealth of evidence proving that Oracle has failed to use the trademark as the law requires. Every step of this proceeding—including all communications with Oracle—will be shared transparently with the community. Help us spread the word by sharing this post. Together, we can ensure that the name “JavaScript” is as open and accessible as the language itself. Discuss on HN.",
    "commentLink": "https://news.ycombinator.com/item?id=42239263",
    "commentBody": "Deno vs. Oracle: Canceling the JavaScript Trademark (deno.com)120 points by frou_dh 23 hours agohidepastfavorite23 comments Brosper 2 hours agoIt soo easy for Oracle to be a hero here. They can win so much by just releasing the name. reply airhangerf15 4 minutes agoparentOracle's only customer for the first three years of its existence was the CIA. There are no heroes there. reply MortyWaves 20 minutes agoparentprevOracle is the kind of company that would understand that and then do the opposite purely for spite. reply gnabgib 23 hours agoprevRelated: Oracle, it's time to free JavaScript (277 points, 70 days ago, 127 comments) https://news.ycombinator.com/item?id=41557383 Deno is filing a USPTO petition to cancel Oracle's JavaScript trademark (7 points, 3 days ago) https://news.ycombinator.com/item?id=42212949 reply paxys 20 hours agoprevClaim #2 is particularly egregious. If it actually has merit, and if a judge is willing to hold a large corporation accountable for breaking the law (lol), someone at Oracle needs to be charged and prosecuted for it. reply xdmr 17 hours agoprevCan anyone speak to whether this is likely to succeed on its merits, or point to analysis of same by competent professionals? Obviously, it looks like a pretty egregious case of Oracle-ness, but it's the job of Deno's lawyers to make it look like that. I'd be interested to see disinterested commentary. reply mbStavola 23 hours agoprevDidn't know about Oracle using Node as an example of them building/selling things utilizing the \"JavaScript\" trademark. I think the post's framing of this being fraudulent is accurate, but even if it didn't legally qualify, it is at the very least extremely dishonest and unethical. reply orliesaurus 21 hours agoprevThis is either for the greater good of the whole JavaScript community or a PR stunt to just get in the news. Or both. reply theadtya 13 hours agoparenteven if it's a PR stunt, it's good thing if JavaScript becomes free reply sgammon 21 hours agoprevGraalJs, it would seem, would count. reply sgammon 21 hours agoparentIt’s not “obscure” or “exotic;” at least, no more than Node was when it was released. Honestly this is such a lame take to those of us who are working in JS outside of Node and Deno. And people do work at that intersection. IMO Deno is the worst option of all new JS runtimes and taking on this fight kind of makes sense that way. If you can’t win mindshare, start a fight. I guess. (In my opinion they should have simply designed Deno to be NPM compatible, but here we are.) reply lucacasonato 21 hours agorootparentDeno is NPM compatible reply sgammon 16 hours agorootparentOnly recently... and yes, to Deno's great credit. Deno has an amazing team and this isn't commentary about their hard work; just disagreement with one of the early decisions. reply ultrakorn2 18 hours agorootparentprev`is:open is:issue label:bug label:\"node compat\"` -> 203 issues currently open. reply harrisi 17 hours agorootparentNeat. But for a relevant query, try searching for \"npm\" instead. 0 issues listed. Further, in the `deno_npm` repo for npm resolving by the deno cli, there are two open issues. One is a feature request, and the other is not clearly an important issue. reply sgammon 16 hours agorootparent> Neat. But for a relevant query, try searching for \"npm\" instead `is:issue is:open npm` 467 open https://github.com/denoland/deno/issues?q=is%3Aissue+is%3Aop... `is:issue is:open label:bug npm` 199 open https://github.com/denoland/deno/issues?q=is%3Aopen+is%3Aiss... For the record, \"NPM compat\" probably does include many feature requests (legitimately), not just bugs. reply harrisi 15 hours agorootparentI did make a mistake. Thanks for pointing that out. However, the point that deno is npm compatible is still true, contrary to what you originally said. reply sgammon 12 hours agorootparentDeno is probably not \"fully\" NPM compatible, arguably, because you still need to prefix imports with \"npm:\" in a lot of cases. I've filed PRs to that effect, is why I know -- a breaking change for many widely-deployed libraries that need to maintain older node compat. That should not be necessary for a runtime that is \"fully\" NPM compliant. Aside from that, Node compliance is a much larger ball of wax than simple NPM compliance, and as far as I know Deno is not there and will not be there for some time. My gripe with Deno is the choice to hard-fork. That is core to the idea; so, we will disagree. No big deal. As a result of Deno's choice, people now have many runtimes to choose from: Bun, LLRT, and the one I have written on top of GraalJs, Elide. Many others, too. Most of these runtimes shoot for as full NPM / Node compat as they can muster without intolerable contortions in the code. Why? Because a ton of software out there runs on NPM, or on Node APIs. Users think of server-side JavaScript and Node as the same thing -- this is not true, Node is both the \"Node APIs\" and the V8-based engine underneath it. But this is exactly what I mean: for Deno to claim that Oracle does not \"use\" the JavaScript trademark requires a completely ignorant stance about technologies like GraalJs. Last we benched, Elide (on top of GraalJs) outperformed Deno by a wide margin. Oracle has invested years of engineering work into it. Enough that it outperforms Node (by a long shot) and Deno, and typically ties with Bun. So why does Deno get to say what happens with the trademark? It makes no sense to me. I might even agree that Oracle should not own it or control it the way they do. I don't know. But I do know that Deno's demands are Node-centric, and the JavaScript ecosystem is simply way bigger than that. reply harrisi 10 hours agorootparentI think the history, causes, and goals here are getting a bit wobbly. Ryan has been outspoken about what he thinks he did wrong with Node and why he decided to make Deno. I'm obviously not him, so I can't speak much about all of that. Node compatibility is interesting because it's only useful until it's not. If all these other runtimes eat away enough at node's share, it won't make sense to be compatible with node anymore. Just to finish the original discussion, as much as I love talking about the current state of js (heh) runtimes: the mention of graaljs supports deno's argument. Oracle, for whatever reason, used \"js\" rather than \"JavaScript\". reply sgammon 8 hours agorootparentGraalJs is just a name. It was created by Oracle so they can use the trademark if they want to. I have no idea what you are talking about. > Ryan has been outspoken about what he thinks he did wrong with Node and why he decided to make Deno Yes, I explained that we disagree. Clearly he also feels he was wrong about Deno’s NPM choices since he reversed course and added NPM support. So Ryan agrees with me, I guess. > If all these runtimes eat away enough at node’s share WinterCG and other efforts are formalizing APIs that can be shared across runtimes. Which is great. Many of these APIs are at least informed by, if not outright standardized versions of, Node APIs. They are here to stay. reply freejs 7 hours agoprev#FreeJavaScript #FreeRust #FreeC #FreeCpp #FreeCSharp #FreeJava #FreePHP #FreeGo #FreeCarbon #FreePython #FreeZig #FreeBF reply BD103 0 minutes agoparent- freejs, created mere minutes before this comment reply orta 22 hours agoprev [–] Good luck reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Deno has filed a petition with the United States Patent and Trademark Office (USPTO) to cancel Oracle's trademark on \"JavaScript,\" aiming to make it a public good and remove legal barriers to its use.",
      "The petition argues that \"JavaScript\" is a generic term, Oracle committed fraud in renewing the trademark, and Oracle has abandoned the trademark through nonuse.",
      "Over 14,000 developers support Deno's move, and Oracle has until January 4, 2025, to respond, with Deno ready to share all proceedings with the community."
    ],
    "commentSummary": [
      "Deno has filed a petition with the United States Patent and Trademark Office (USPTO) to cancel Oracle's trademark on JavaScript, sparking a debate on trademark ethics and community benefits.",
      "The discussion involves Deno's compatibility with Node Package Manager (NPM) and its impact on the broader JavaScript ecosystem, reflecting differing opinions on the future of JavaScript and runtime environments.",
      "Opinions are divided, with some viewing Oracle's trademark use as unethical, while others question Deno's intentions, suggesting it might be a public relations strategy."
    ],
    "points": 120,
    "commentCount": 23,
    "retryCount": 0,
    "time": 1732562433
  },
  {
    "id": 42244987,
    "title": "SQLite Plugin for Jekyll",
    "originLink": "https://github.com/captn3m0/jekyll-sqlite",
    "originBody": "I love Jekyll, especially the Datafiles[0] feature, which lets you use CSV&#x2F;JSON&#x2F;YAML files and iterate through them. Mixed with the Jekyll Data Pages generator[1], which lets you create a page for every row in your dataset, it is a very powerful combination.However, Liquid is a terrible language for data-mangling, and simple filtering&#x2F;sorting&#x2F;merging can become very annoying. So I wrote a Jekyll SQLite plugin that lets you use the same data interface in Jekyll&#x2F;Liquid, but backed by a SQLite file(s).It gives you the simplicity of the Baked Data pattern[2], and the flexibility of using SQL for data-wrangling, within a static site generator.As a demo, I took the northwind dataset, and generated a site[3] with a few sample queries[4]. It demos both site-level, and page-level queries alongside data-pages generator to generate a page for every product&#x2F;category&#x2F;customer.I&#x27;ve been using this across a few sites in production for almost a year, looking for feedback on usage semantics and feature suggestions.[0]: https:&#x2F;&#x2F;jekyllrb.com&#x2F;docs&#x2F;datafiles&#x2F;[1]: https:&#x2F;&#x2F;github.com&#x2F;avillafiorita&#x2F;jekyll-datapage_gen[2]: https:&#x2F;&#x2F;simonwillison.net&#x2F;2021&#x2F;Jul&#x2F;28&#x2F;baked-data&#x2F;[3]: https:&#x2F;&#x2F;northwind.captnemo.in&#x2F;[4]: https:&#x2F;&#x2F;github.com&#x2F;captn3m0&#x2F;northwind",
    "commentLink": "https://news.ycombinator.com/item?id=42244987",
    "commentBody": "SQLite Plugin for Jekyll (github.com/captn3m0)119 points by captn3m0 6 hours agohidepastfavorite35 comments I love Jekyll, especially the Datafiles[0] feature, which lets you use CSV/JSON/YAML files and iterate through them. Mixed with the Jekyll Data Pages generator[1], which lets you create a page for every row in your dataset, it is a very powerful combination. However, Liquid is a terrible language for data-mangling, and simple filtering/sorting/merging can become very annoying. So I wrote a Jekyll SQLite plugin that lets you use the same data interface in Jekyll/Liquid, but backed by a SQLite file(s). It gives you the simplicity of the Baked Data pattern[2], and the flexibility of using SQL for data-wrangling, within a static site generator. As a demo, I took the northwind dataset, and generated a site[3] with a few sample queries[4]. It demos both site-level, and page-level queries alongside data-pages generator to generate a page for every product/category/customer. I've been using this across a few sites in production for almost a year, looking for feedback on usage semantics and feature suggestions. [0]: https://jekyllrb.com/docs/datafiles/ [1]: https://github.com/avillafiorita/jekyll-datapage_gen [2]: https://simonwillison.net/2021/Jul/28/baked-data/ [3]: https://northwind.captnemo.in/ [4]: https://github.com/captn3m0/northwind klandergren 1 hour agoNice! Great to see innovation in the Jekyll space. Quick FYI on Jekyll performance: I noticed slow generation times on one of my sites and traced it to a plugin that was spawning a git process to get the last commit info for every page. I wrote a drop-in replacement called jekyll-last-commit[0] that uses the ruby libgit2 wrapper for improved performance. Details on its origins are in an old HN comment[1] if you are interested! [0]: https://github.com/klandergren/jekyll-last-commit [1]: https://news.ycombinator.com/item?id=34331663 reply captn3m0 42 minutes agoparentThe other last-modified-plugin is now archived, and no longer included in the default Jekyll, but the sitemap and seo plugin still seem to rely on the page.last_modified_at property, albeit undocumented (https://github.com/jekyll/jekyll/issues/9702), so might be worth getting the implementation switched upstream perhaps? We show the last-modified date on endoflife.date products (https://endoflife.date/jekyll), but I'm not sure how we have the property set right now, since I don't remember including either plugin. Thanks for creating this, I have a few other sites where this will be quite helpful. reply sangeeth96 1 hour agoprevAlthough I don’t use Jekyll, loved this idea. I think using SQLite as the main data source would be a nice way to preserve content and play around with different static generators if they all had a plugin like this. Also TIL about the baked data pattern, which I think is exactly what I needed for an upcoming project, so thanks for that. Though, I do align with one of the commenters here—this doesn’t seem like the same thing as the Baked data pattern in that Simon’s approach was about using a server rendered app with read-only data instead of generating a lot of static pages. Nevertheless, this seems like a nice way for static generators to work with bit more complex data sources without dumbing them down to JSON/YAML. reply raminf 2 hours agoprevI'm a little confused. The baked-data model is so you DON'T have to generate a thousand static pages. But this solution does exactly that. Not complaining, mind you. My kid is trying to learn HTML/CSS/JS and wants to put together a read-only website with a database backend. I'll be pointing him this way as an ootion once he's far enough. But it's still puzzling to link it to baked-data. Maybe I'm missing something. reply captn3m0 1 hour agoparent> bundling a read-only copy of your data alongside the code for your application, as part of the same deployment You can see https://github.com/captn3m0/northwind for example, which bundles the entire database alongside the code in the _db/northwind.db file. While Simon considers it primarily for dynamic apps, you have the ability to build PWAs and other interesting apps with the baked data pattern. I'm building blr.today for example using this. reply yboulkaid 3 hours agoprevNice to see this project here! I've been using it with the Steam API to publish a list of the games I've been playing on my personal website: https://yboulkaid.com/games I found that separating data from content on a Jekyll site is a really powerful way to have anything from photo galleries, blog entries, book lists, easily changeable menus etc... reply Alifatisk 4 hours agoprevThis is so cool, bookmarked it. It sort of get the impression that Jekyll is slowing turning away from being a boring static site generator to something way interesting. reply berkes 4 hours agoparentwhat is wrong with \"being a boring static site generator\"? reply jayknight 4 hours agoparentprevNow it's easier to generate static sites that aren't so boring! reply anticorporate 1 hour agoprevThis is awesome, and would solve an actual problem I've been thinking through... if only I had selected Jekyll for my static site generator. Does anyone know of a similar solution for Hugo? Technically I'm sure I could run a script to generate .md (or a format like .csv that Hugo can work with) from my database, but this seems like it might be easier for a database that updates frequently. reply cannibalXxx 4 hours agoprevIn this article I show you a project on how to develop a static blog in jekyll. https://chat-to.dev/post?id=296 reply charles_f 4 hours agoparentIn this one I show how to host it in various stupid ways https://fev.al/posts/blog-infra/ Though the idea of using a database to store the content could make things even better. Maybe sprinkle some redundant postgres? reply raminf 2 hours agorootparentWhen you went from Jekyll to Kubernetes, it was must have been like when Ted Kaczynski first learned about battery-operated timed fuses. Enjoyed the read. Looking forward to when you chuck the whole thing into the bin and run Wordpress on your linux home server, fronted by a free Cloudflare zero trust tunnel. reply zephyreon 4 hours agoprevI’ve always been a big fan of Jekyll because it’s so easy to use & so stable. This adds a lot of value to the ecosystem. I’ve built a bunch of faculty websites over the years where I needed a lot of structured, repetitive data (papers, honors/awards, etc.). It would have been so much easier to manage if I could have stored that data in a database instead of just flat files. reply nikeee 1 hour agoprevIs it possible to generate an SQLite DB from the site data and statically serve it, so sql.js can use it as a DB to provide something like search? reply captn3m0 35 minutes agoparentI'm mostly focusing on sqlite-as-the-data-source, but I can imagine something like this might be in scope for a project like lunr.js[1], which currently uses a large index served as JS/JSON. You can write a Jekyll plugin in a few lines of ruby code (with no gem management needed, just drop it in the _plugins directory), so the \"write-index-to-a-sqlite-file\" part shouldn't be hard to build either. Is FTS5 included in the SQLite browser builds? The SQLite amalgamation includes it by default. [1]: https://lunrjs.com/docs/index.html reply amcaskill 2 hours agoprevWe use a similar “baked data” approach with Duck DB + a static site generator in evidence https://github.com/evidence-dev/evidence reply captn3m0 1 hour agoparentThis is quite interesting. I always wanted to build something similar using SteamPipe (which can pretend to be sqlite/postgres) alongside Querybook. THe craziest production with this approach that I've seen is the crt.sh website, which builds a full dynamic website with postgres and sql: https://github.com/crtsh/certwatch_db/blob/master/fnc/web_ap... reply tajd 2 hours agoprevThis is really cool. Whilst it's been fun mucking around with next.js etc (and arguably that's for a different purpose) for an out the box website Jekyll has proven itself time and again. Looking forwards to trying this out. reply hk1337 3 hours agoprevThis is interesting. I suppose if you wanted to setup your Jekyll data relationally, this would make it simpler to pull a simple list of the combined data. I wonder if DuckDB would make it easier to do this same thing and use the existing Jekyll data files? reply giancarlostoro 3 hours agoprevIn wake of all the WP craziness a few weeks back, I wonder how long before someone builds a \"best of both worlds\" CMS to rival WordPress. \"You get the nice admin UI, but it generates a static site\" type of thing. reply abhiyerra 1 hour agoparentDjango-Distill I found to be pretty good for this. Use Django and the admin interface while being able to generate a static site. reply sofixa 2 hours agoparentprevThere are a few of those, I think the term used to describe them is \"headless CMS\". reply anamexis 2 hours agoparentprevTinaCMS, fka Forestry is trying to do this. It's been a while since I've tried them out, but I love the idea. https://tina.io/ reply tonymet 1 hour agoprevGreat project. I've long thought about creating a static-site transformer for wordpress sites via SQLLite-- in order to reduce costs and improve security. The idea would be to feed the wordpress content into a sqlite DB and re-publish the entire site as a static site. Since wordpress comments have declined in usage, this should work well. Publishing time would be a bit slower, but reads will be 100x faster and 10000x cheaper. reply midzer 4 hours agoprevWow, cool stuff! reply captn3m0 4 hours agoparentThanks! reply jordanmorgan10 4 hours agoprev [–] I have nothing to add other than Jekyll has been rock solid for me for, what, like a decade plus now? I've ran swiftjectivec.com on it, and it's always been the perfect middle ground of taking care of the cruft I don't want to deal with, while allowing me to get my hands dirty and code when I want. Some of my favorite software ever. reply rapnie 1 hour agoparentI have some old jekyll websites that I only very infrequently need to update. Each time something in the ruby / gem / bundler / jekyll chain setup is broken, and with some weird errors it is stackoverflow search time. Very time consuming, highly annoying. I postponed my last typo correction, to first make up my mind on whether to port to astro that I use currently. reply diggan 1 hour agorootparent> , to first make up my mind on whether to port to astro that I use currently. Just sucks that eventually, that will happen with whatever you use after migrating too, the only difference is how long it takes. I'm hoping NixOS or even just Nix for dev envs (or something similar) will help against this, so you end up with environments that just keep working. reply bbkane 1 hour agoparentprevI actually switched my blog to use Zola (similar to Jekyll but packaged as a static binary instead of a Ruby gem) because I couldn't figure out how to build my site with Jekyll after a few years- it kept trying to compile C code? Bear in mind this was 5 years ago and I had never used Ruby before, so probably a user error :) Glad it's been so stable for you! reply Tomte 1 hour agorootparent> it kept trying to compile C code? That‘s something that works incredibly well on Windows, better than I would have expected to. The rubyinstaller.org people are shipping a fantastic installation. Under the hood it‘s msys2, but everything simply works out of the box. reply captn3m0 4 hours agoparentprev [–] I built endoflife.date with it, and it has been great. If I had to do it again, I might pick Mediawiki (or something similar) due to it being a community wiki more than a static site, but Jekyll hasn’t let us down yet. reply freedomben 1 hour agorootparent [–] Really appreciate you building and maintaining it these years! endoflife.date has been an amazing resource ever since you put it up. This has been a great open source success story that I share with people to explain why open source can be great. It's also a great example of the power of crowd-sourcing data. I originally added Ruby[1], Fedora[2], and Alma Linux[3] to endoflife.date and it's rewarded me with years of ability to reference. Prior to this I kept my own notes for the projects I cared most about, but keeping those up to date was a PITA. The best ideas seem obvious in hindsight, and endoflife.date definitely seems obvious :-D [1] https://github.com/endoflife-date/endoflife.date/commit/7dae... [2] https://github.com/endoflife-date/endoflife.date/commit/ab16... [3] https://github.com/endoflife-date/endoflife.date/commit/12a7... reply captn3m0 1 hour agorootparent [–] Thanks a ton! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author has developed a Jekyll SQLite plugin to enhance data manipulation capabilities within Jekyll, a popular static site generator.",
      "This plugin allows users to perform SQL queries for data-wrangling, addressing limitations of Liquid, Jekyll's templating language, in handling complex data operations.",
      "The plugin has been successfully used in production for a year, demonstrated with the Northwind dataset, and the author invites feedback and suggestions for improvement."
    ],
    "commentSummary": [
      "A Jekyll SQLite plugin enables the use of SQLite as a data source in Jekyll, enhancing data manipulation capabilities with SQL.- It integrates Jekyll's Datafiles and Data Pages generator features, allowing for complex data handling beyond traditional CSV/JSON/YAML formats.- The plugin has been in production for a year, receiving positive feedback for making static site generation more dynamic and flexible."
    ],
    "points": 119,
    "commentCount": 35,
    "retryCount": 0,
    "time": 1732623091
  },
  {
    "id": 42241702,
    "title": "Premature Graying of Hair: Review with Updates",
    "originLink": "https://pmc.ncbi.nlm.nih.gov/articles/PMC6290285/",
    "originBody": "Int J Trichology . 2018 Sep-Oct;10(5):198–203. doi: 10.4103/ijt.ijt_47_18 Premature Graying of Hair: Review with Updates Anagha Bangalore Kumar 1,✉, Huma Shamim 1, Umashankar Nagaraju 2 Author information Copyright and License information PMCID: PMC6290285 PMID: 30607038 Abstract Premature graying of hair (PGH) is defined as graying of hair before the age of 20 years in Caucasians and before 30 years in African American population. It can severely affect the self-esteem of an individual. The exact etiopathogenesis remains unknown, although it has been associated with premature aging disorders, atopy, and autoimmune diseases. Patients, who present with PGH, should be assessed for syndromes and metabolism diseases. Hair dyes remain the main modality of the treatment for cosmetic concerns after nutritional supplementation. Key words: Canities, premature gray hair, white hair INTRODUCTION Haalthy hair is a sign of general well-being and youth. Unlike other animals, the function of hair in human beings is being debated. Nevertheless, hair serves as a great esthetic tool and means of nonverbal communication. Hair color and style can significantly alter the physical appearance of a person and thus alter his/her body image. As graying of hair is perceived as a sign of old age, premature graying of hair (PGH) can bear an adverse effect on the self-esteem of the individual. Graying of hair also called canities or achromotrichia occurs with normal aging. However, the age at which it occurs varies in different races. PGH is defined as graying of hair before the age of 20 years in Caucasians and before 30 years in Blacks.[1] Definition of PGH with respect to the Asian population is lacking. The average age of the onset of graying in Caucasians is 34 ± 9.6 years, and in Blacks, it is 43.9 ± 10.3 years.[1] A large population-based study reported that 6%–23% of people have 50% gray hair by 50 years of age.[2] PIGMENTATION OF HAIR Hair pigmentation is one of the most unique features in humans ranging from black, brown, and blonde to red. The color of human hair is due to pigment melanin produced by melanocytes which are neural crest derivatives. Human hair follicles contain two types of melanin as follows: eumelanin and pheomelanin. The diversity of hair color arises mostly from the quantity and ratio of black-brown eumelanin and reddish-brown pheomelanin. It has been hypothesized that the pH and cysteine level of melanosomes influences the phenotype of hair. As pH reduces, there is a progressive reduction in tyrosinase activity leading to increased pheomelanin and reddish or blonde hair.[3,4] A mutation in melanocortin-1 receptor (MC1R) gene causes auburn or red color of hair. This mutation is seen usually in individuals of Northern Europe with less sun exposure.[5,6] A study in 2012 showed a recessive mutation in tyrosinase-related protein 1 (TYRP1) in people with blonde hair[7] There are various differences between pigmentation in the skin and that of hair. Each melanocyte is associated with five keratinocytes in the hair bulb forming a “hair follicle-melanin unit.” In contrast, each melanocyte in the skin is associated with 36 keratinocytes constituting an “epidermal-melanin unit.”[8,9] Unlike in the skin where pigment production is continuous, melanogenesis in the hair is closely associated with stages of the hair cycle. Hair is actively pigmented in the anagen phase and is “turned off” during the catagen phase and absent during telogen.[10] The pigmentary unit is a pear-shaped black structure at the tip of dermal papilla in pigmented hair.[11] In gray hair, the pigmentary unit becomes fuzzy, the melanocytes become few and rounded, and lightly pigmented oligodendritic melanocytes become visible in the proximal hair bulb.[12] During anagen, there is a marked reduction in the number of melanocytes in the hair follicles through autophagolysosomal degeneration leading to pigment loss. This is thought to be central in the pathogenesis of graying.[13] Defective melanosomal transfer to cortical keratinocytes or melanin incontinence due to melanocyte degeneration contributes to graying. Degenerative changes within the hair follicle are associated with an increase in dendritic cells in the hair follicle.[1] Eventually, there are no melanogenic melanocytes in the hair bulb. There is earlier terminal differentiation of pigmented hair than nonpigmented hair. The growth rate, diameter of medulla, and average diameter of nonpigmented hair are higher than its pigmented counterpart.[14,15,16] Genetic and environmental elements influence the hair follicle stem cells and melanocytes. Telomere shortening, decrease in cell numbers, and certain transcription factors have all been implicated in this process of aging. In turn, these molecular alterations lead to structural modifications of the hair fiber, decrease melanin production, and lengthen of the telogen phase of the hair cycle.[17] At the molecular level, various genes and signaling pathways that influence hair pigmentation are being studied. Receptors for bone morphogenic protein and activins that are Bmpr2 and Acvr2a are known to influence hair pigmentation. The reduced activity of Bmpr2 and Acvr2 can cause early graying in experimental mice.[18] The Notch signaling pathway influences various biological processes. Notch 1 and Notch 2 signaling pathways were reported to have a role in maintenance of hair pigmentation.[19] Stem cell factor (SCF) is a cytokine involved in many physiological processes such as hematopoiesis. Recently, SCF and its receptor (kit) are shown to have a role in melanogenesis during anagen phase.[20] POSSIBLE ETIOPATHOGENESIS OF GRAYING Till date, the exact etiopathogenesis of graying remains incompletely understood. PGH can occur as an autosomal dominant primary disease. Graying can also occur with premature aging disorders such as progeria and pangeria. Association with atopic diathesis and autoimmune diseases has also been reported.[21] Perhaps the role of reactive oxygen species (ROS) on graying of hair is most studied. During active growth phase, i.e., anagen phase there is active melanogenesis in the hair follicle. This involves hydroxylation of tyrosine and oxidation of dihydroxyphenylalanine to melanin causing enormous accumulative oxidative stress. The failure of antioxidant effect could damage melanocytes leading to decreased pigmentation.[12,22] Wood et al. demonstrated that the accumulation of hydrogen peroxide in hair follicles and absent expression of antioxidants such as catalase and methionine sulfoxide reductase in gray hair follicles. Their experiment supported the theory of prooxidant role in graying of hair.[23] Oxidative stress can also be a result of ultraviolet (UV) rays, pollution, emotional factors, or inflammatory causes. Experiments have shown melanocyte apoptosis and oxidative damage in graying hair follicles. Furthermore, exogenous oxidative stress showed increased graying in the hair follicles.[12] An experiment on mice demonstrated that UV radiation could cause oxidative damage on hair follicles causing hair graying. They also demonstrated that the protective effect of an antioxidant superoxide dismutase.[24] Numerous studies have demonstrated increased oxidative load due to psychological stress implying that even emotional factors play a role in premature graying.[25,26] A recent study on young adults in Turkey revealed that PGH is closely related to factors causing oxidative stress such as emotional stress, alcohol consumption, and chronic diseases in genetically predisposed men and women.[27] Daulatabad et al. made an attempt to measure oxidative stress load in PGH. They demonstrated an increase in prooxidants such as serum malonaldehyde, whole blood reduced glutathione, and serum ferric reducing antioxidant potential and decrease in antioxidants.[28] Shi et al. demonstrated compromised antioxidant activity in gray hair follicles. Their experiments revealed that catalase protein expression and hydroxyl radical-scavenging activities are strongly repressed in unpigmented hair follicles.[29] Progeroid syndromes are associated with defective repair of DNA. Thus, DNA is more susceptible to oxidative stress.[30] Vitiligo is another condition with early graying of hair. Melanocytes in patients with vitiligo are more sensitive to oxidative stress. ROS damage to melanocytes leads to ectopic differentiation of stem cells and apoptosis of differentiated melanocytes.[31] Vitamin B12 deficiency can cause PGH through unknown mechanism. About 55% of patients with pernicious anemia had graying before 50 years as compared to 30% in the control group.[32] Decreased thyroid hormones cause premature graying, alopecia, and changes in hair morphology. Thyroid hormones T3 and T4 act on hair follicles directly to increase melanogenesis.[33] Certain chemotherapeutic drugs and antimalarials can cause PGH. These drugs are thought to inhibit the receptor tyrosine kinase c-kit found in melanocytes reducing melanogenesis. Chloroquine preferentially reduces pheomelanin production by unknown mechanism.[34,35,36,37] Smoking has been studied as an etiological agent in early-onset achromotrichia. Studies revealed that there was a significant correlation between smoking and premature hair graying. The possible explanation to this is the prooxidant effect of smoking on the body leading to increased ROS damage to hair follicle melanocytes.[38,39,40,41] Reversible hypopigmentation of the hair can be seen in nutritional deficiencies protein-energy malnutrition and diseases of chronic loss of protein. Copper and iron deficiency also can cause graying of hair. A study reported significantly lower levels of copper in patients with PGH when compared to the control group. The study, however, did not report lower levels of zinc or iron in the affected population.[42] A newer study of young Indian population reported lesser serum levels of ferritin, calcium, and Vitamin D3 levels in subjects prone to PGH.[43] Another study highlighted the association of PGH with lower high-density lipoprotein cholesterol (HDL-C) levels in Indian patients aged 2.0.CO;2. [DOI] [PubMed] [Google Scholar] 25.Irie M, Asami S, Nagata S, Miyata M, Kasai H. Relationships between perceived workload, stress and oxidative DNA damage. Int Arch Occup Environ Health. 2001;74:153–7. doi: 10.1007/s004200000209. [DOI] [PubMed] [Google Scholar] 26.Epel ES, Blackburn EH, Lin J, Dhabhar FS, Adler NE, Morrow JD, et al. Accelerated telomere shortening in response to life stress. Proc Natl Acad Sci U S A. 2004;101:17312–5. doi: 10.1073/pnas.0407162101. [DOI] [PMC free article] [PubMed] [Google Scholar] 27.Akin Belli A, Etgu F, Ozbas Gok S, Kara B, Dogan G. Risk factors for premature hair graying in young Turkish adults. Pediatr Dermatol. 2016;33:438–42. doi: 10.1111/pde.12881. [DOI] [PubMed] [Google Scholar] 28.Daulatabad D, Singal A, Grover C, Sharma SB, Chhillar N. Assessment of oxidative stress in patients with premature canities. Int J Trichology. 2015;7:91–4. doi: 10.4103/0974-7753.167469. [DOI] [PMC free article] [PubMed] [Google Scholar] 29.Shi Y, Luo LF, Liu XM, Zhou Q, Xu SZ, Lei TC. Premature graying as a consequence of compromised antioxidant activity in hair bulb melanocytes and their precursors. PLoS One. 2014;9:e93589. doi: 10.1371/journal.pone.0093589. [DOI] [PMC free article] [PubMed] [Google Scholar] 30.Domínguez-Gerpe L, Araújo-Vilar D. Prematurely aged children: Molecular alterations leading to hutchinson-gilford progeria and werner syndromes. Curr Aging Sci. 2008;1:202–12. doi: 10.2174/1874609810801030202. [DOI] [PubMed] [Google Scholar] 31.Jimbow K, Chen H, Park JS, Thomas PD. Increased sensitivity of melanocytes to oxidative stress and abnormal expression of tyrosinase-related protein in vitiligo. Br J Dermatol. 2001;144:55–65. doi: 10.1046/j.1365-2133.2001.03952.x. [DOI] [PubMed] [Google Scholar] 32.Dawber RP. Integumentary associations of pernicious anaemia. Br J Dermatol. 1970;82:221–3. doi: 10.1111/j.1365-2133.1970.tb12428.x. [DOI] [PubMed] [Google Scholar] 33.van Beek N, Bodó E, Kromminga A, Gáspár E, Meyer K, Zmijewski MA, et al. Thyroid hormones directly alter human hair follicle functions: Anagen prolongation and stimulation of both hair matrix keratinocyte proliferation and hair pigmentation. J Clin Endocrinol Metab. 2008;93:4381–8. doi: 10.1210/jc.2008-0283. [DOI] [PubMed] [Google Scholar] 34.Hartmann JT, Kanz L. Sunitinib and periodic hair depigmentation due to temporary c-KIT inhibition. Arch Dermatol. 2008;144:1525–6. doi: 10.1001/archderm.144.11.1525. [DOI] [PubMed] [Google Scholar] 35.Sideras K, Menefee ME, Burton JK, Erlichman C, Bible KC, Ivy SP. Profound hair and skin hypopigmentation in an African American woman treated with the multi-targeted tyrosine kinase inhibitor pazopanib. J Clin Oncol. 2010;28:e312–3. doi: 10.1200/JCO.2009.26.4432. [DOI] [PubMed] [Google Scholar] 36.Etienne G, Cony-Makhoul P, Mahon FX. Imatinib mesylate and gray hair. N Engl J Med. 2002;347:446. doi: 10.1056/NEJM200208083470614. [DOI] [PubMed] [Google Scholar] 37.Di Giacomo TB, Valente NY, Nico MM. Chloroquine -induced hair depigmentation. Lupus. 2009;18:264–6. doi: 10.1177/0961203308097473. [DOI] [PubMed] [Google Scholar] 38.Jo SJ, Paik SH, Choi JW, Lee JH, Cho S, Kim KH, et al. Hair graying pattern depends on gender, onset age and smoking habits. Acta Derm Venereol. 2012;92:160–1. doi: 10.2340/00015555-1181. [DOI] [PubMed] [Google Scholar] 39.Mosley JG, Gibbs AC. Premature grey hair and hair loss among smokers: A new opportunity for health education? BMJ. 1996;313:1616. doi: 10.1136/bmj.313.7072.1616. [DOI] [PMC free article] [PubMed] [Google Scholar] 40.Zayed AA, Shahait AD, Ayoub MN, Yousef AM. Smokers' hair: Does smoking cause premature hair graying? Indian Dermatol Online J. 2013;4:90–2. doi: 10.4103/2229-5178.110586. [DOI] [PMC free article] [PubMed] [Google Scholar] 41.Trüeb RM. Association between smoking and hair loss: Another opportunity for health education against smoking? Dermatology. 2003;206:189–91. doi: 10.1159/000068894. [DOI] [PubMed] [Google Scholar] 42.Zarafonetis CJ. Darkening of gray hair during para-amino-benzoic acid therapy. J Invest Dermatol. 1950;15:399–401. doi: 10.1038/jid.1950.121. [DOI] [PubMed] [Google Scholar] 43.Bhat RM, Sharma R, Pinto AC, Dandekeri S, Martis J. Epidemiological and investigative study of premature graying of hair in higher secondary and pre-university school children. Int J Trichology. 2013;5:17–21. doi: 10.4103/0974-7753.114706. [DOI] [PMC free article] [PubMed] [Google Scholar] 44.Chakrabarty S, Krishnappa PG, Gowda DG, Hiremath J. Factors associated with premature hair graying in a young Indian population. Int J Trichology. 2016;8:11–4. doi: 10.4103/0974-7753.179384. [DOI] [PMC free article] [PubMed] [Google Scholar] 45.Vinay K, Yadav S, Handa S. Zinc deficiency and canities: An unusual manifestation. JAMA Dermatol. 2014;150:1116–7. doi: 10.1001/jamadermatol.2014.368. [DOI] [PubMed] [Google Scholar] 46.Keogh EV, Walsh RJ. Rate of greying of human hair. Nature. 1965;207:877–8. doi: 10.1038/207877a0. [DOI] [PubMed] [Google Scholar] 47.Hollfelder B, Blankenburg G, Wolfram LJ, Höcker H. Chemical and physical properties of pigmented and non-pigmented hair ('grey hair') Int J Cosmet Sci. 1995;17:87–9. doi: 10.1111/j.1467-2494.1995.tb00112.x. [DOI] [PubMed] [Google Scholar] 48.Gao T, Bedell A. Ultraviolet damage on natural gray hair and its photoprotection. J Cosmet Sci. 2001;52:103–18. [PubMed] [Google Scholar] 49.Alijanpoor R, Poorsattar BejehMir A, Mokmeli S. Successful white hair removal with combined coloring and intense pulsed light (IPL): A randomized clinical trial. Photomed Laser Surg. 2011;29:773–9. doi: 10.1089/pho.2010.2940. [DOI] [PubMed] [Google Scholar] 50.Tobin DJ. Aging of the hair follicle pigmentation system. Int J Trichology. 2009;1:83–93. doi: 10.4103/0974-7753.58550. [DOI] [PMC free article] [PubMed] [Google Scholar] 51.Schnohr P, Lange P, Nyboe J, Appleyard M, Jensen G. Gray hair, baldness, and wrinkles in relation to myocardial infarction: The copenhagen city heart study. Am Heart J. 1995;130:1003–10. doi: 10.1016/0002-8703(95)90201-5. [DOI] [PubMed] [Google Scholar] 52.Schnohr P, Nyboe J, Lange P, Jensen G. Longevity and gray hair, baldness, facial wrinkles, and arcus senilis in 13,000 men and women: The Copenhagen city heart study. J Gerontol A Biol Sci Med Sci. 1998;53:M347–50. doi: 10.1093/gerona/53a.5.m347. [DOI] [PubMed] [Google Scholar] 53.Eisenstein I, Edelstein J. Gray hair in black males a possible risk factor in coronary artery disease. Angiology. 1982;33:652–4. doi: 10.1177/000331978203301004. [DOI] [PubMed] [Google Scholar] 54.Gould L, Reddy CV, Oh KC, Kim SG, Becker W. Premature hair graying: A probable coronary risk factor. Angiology. 1978;29:800–3. doi: 10.1177/000331977802901103. [DOI] [PubMed] [Google Scholar] 55.Glasser M. Is early onset of gray hair a risk factor? Med Hypotheses. 1991;36:404–11. doi: 10.1016/0306-9877(91)90020-y. [DOI] [PubMed] [Google Scholar] 56.Aggarwal A, Srivastava S, Agarwal MP, Dwivedi S. Premature graying of hair: An independent risk marker for coronary artery disease in smokers – A retrospective case control study. Ethiop J Health Sci. 2015;25:123–8. doi: 10.4314/ejhs.v25i2.4. [DOI] [PMC free article] [PubMed] [Google Scholar] 57.Morton DJ, Kritz-Silverstein D, Riley DJ, Barrett-Connor EL, Wingard DL. Premature graying, balding, and low bone mineral density in older women and men: The Rancho Bernardo Study. J Aging Health. 2007;19:275–85. doi: 10.1177/0898264307299274. [DOI] [PMC free article] [PubMed] [Google Scholar] 58.Beardsworth SA, Kearney CE, Steel SA, Newman J, Purdie DW. Premature greying of the hair is not associated with low bone mineral density. Osteoporos Int. 1999;10:290–4. doi: 10.1007/s001980050229. [DOI] [PubMed] [Google Scholar] 59.Ozbay I, Kahraman C, Kucur C, Namdar ND, Oghan F. Is there a relationship between premature hair greying and hearing impairment? J Laryngol Otol. 2015;129:1097–100. doi: 10.1017/S0022215115002571. [DOI] [PubMed] [Google Scholar] 60.van Geel N, Speeckaert M, Chevolet I, De Schepper S, Lapeere H, Boone B, et al. Hypomelanoses in children. J Cutan Aesthet Surg. 2013;6:65–72. doi: 10.4103/0974-2077.112665. [DOI] [PMC free article] [PubMed] [Google Scholar] 61.Shah VV, Aldahan AS, Mlacker S, Alsaidan M, Nouri K. Canities subita: Sudden blanching of the hair in history and literature. Int J Dermatol. 2016;55:362–4. doi: 10.1111/ijd.13203. [DOI] [PubMed] [Google Scholar] 62.Tan SP, Weller RB. Sudden whitening of the hair in an 82-year-old woman: The ‘overnight greying’ phenomenon. Clin Exp Dermatol. 2012;37:458–9. doi: 10.1111/j.1365-2230.2011.04211.x. [DOI] [PubMed] [Google Scholar] 63.Helm F, Milgrom H. Can scalp hair suddenly turn white? A case of canities subita. Arch Dermatol. 1970;102:102–3. [PubMed] [Google Scholar] 64.Hoffmann E. Sudden turning gray of the hair caused by fright, canities subita psychogenica. Z Haut Geschlechtskr. 1957;22:74–8. [PubMed] [Google Scholar] 65.Pandhi D, Khanna D. Premature graying of hair. Indian J Dermatol Venereol Leprol. 2013;79:641–53. doi: 10.4103/0378-6323.116733. [DOI] [PubMed] [Google Scholar] 66.Shin H, Ryu HH, Yoon J, Jo S, Jang S, Choi M, et al. Association of premature hair graying with family history, smoking, and obesity: A cross-sectional study. J Am Acad Dermatol. 2015;72:321–7. doi: 10.1016/j.jaad.2014.11.008. [DOI] [PubMed] [Google Scholar] 67.Erdoğan T, Kocaman SA, Çetin M, Durakoğlugil ME, Uğurlu Y, Şahin İ, et al. Premature hair whitening is an independent predictor of carotid intima-media thickness in young and middle-aged men. Intern Med. 2013;52:29–36. doi: 10.2169/internalmedicine.52.7842. [DOI] [PubMed] [Google Scholar] 68.Singal A, Daulatabad D, Grover C. Graying severity score: A useful tool for evaluation of premature canities. Indian Dermatol Online J. 2016;7:164–7. doi: 10.4103/2229-5178.182372. [DOI] [PMC free article] [PubMed] [Google Scholar] 69.McDonough PH, Schwartz RA. Premature hair graying. Cutis. 2012;89:161–5. [PubMed] [Google Scholar] 70.Dweck AC. Natural ingredients for colouring and styling. Int J Cosmet Sci. 2002;24:287–302. doi: 10.1046/j.1467-2494.2002.00148.x. [DOI] [PubMed] [Google Scholar] 71.Morel OJ, Christie RM. Current trends in the chemistry of permanent hair dyeing. Chem Rev. 2011;111:2537–61. doi: 10.1021/cr1000145. [DOI] [PubMed] [Google Scholar] 72.Pande CM, Albrecht L, Yang B. Hair photoprotection by dyes. J Cosmet Sci. 2001;52:377–89. [PubMed] [Google Scholar] 73.Trüeb RM. Pharmacologic interventions in aging hair. Clin Interv Aging. 2006;1:121–9. doi: 10.2147/ciia.2006.1.2.121. [DOI] [PMC free article] [PubMed] [Google Scholar] 74.Pasricha JS. Effect of grey hair evulsion on the response to calcium pantothenate in premature grey hairs. Indian J Dermatol Venereol Leprol. 1986;52:77–80. [PubMed] [Google Scholar] 75.Sieve BF. Clinical achromotrichia. Science. 1941;94:257–8. doi: 10.1126/science.94.2437.257. [DOI] [PubMed] [Google Scholar] 76.Pavithran K. Puvasol therapy in premature greying of hair. Indian J Dermatol Venereol Leprol. 1986;52:74–5. [PubMed] [Google Scholar] 77.Pasricha JS. Can puva darken grey hair. Indian J Dermatol Venereol Leprol. 1987;53:35–6. [PubMed] [Google Scholar] 78.Bellandi S, Amato L, Cipollini EM, Antiga E, Brandini L, Fabbri P. Repigmentation of hair after latanoprost therapy. J Eur Acad Dermatol Venereol. 2011;25:1485–7. doi: 10.1111/j.1468-3083.2010.03949.x. [DOI] [PubMed] [Google Scholar] 79.Skulachev VP, Anisimov VN, Antonenko YN, Bakeeva LE, Chernyak BV, Erichev VP, et al. An attempt to prevent senescence: A mitochondrial approach. Biochim Biophys Acta. 2009;1787:437–61. doi: 10.1016/j.bbabio.2008.12.008. [DOI] [PubMed] [Google Scholar] 80.Hoffman RM. Topical liposome targeting of dyes, melanins, genes, and proteins selectively to hair follicles. J Drug Target. 1998;5:67–74. doi: 10.3109/10611869808995860. [DOI] [PubMed] [Google Scholar] Articles from International Journal of Trichology are provided here courtesy of Wolters Kluwer -- Medknow Publications",
    "commentLink": "https://news.ycombinator.com/item?id=42241702",
    "commentBody": "Premature Graying of Hair: Review with Updates (nih.gov)110 points by luu 17 hours agohidepastfavorite98 comments notamy 14 hours ago> Vitamin B12 deficiency can cause PGH through unknown mechanism. About 55% of patients with pernicious anemia had graying before 50 years as compared to 30% in the control group.[32] /soapbox Get your B12 tested (before taking supplements!). B12 deficiency is known to present in many ways, and also to be often overlooked in clinical settings[1]. It’s known that not everyone presents with the anaemia from it[2], which is often why it’s skipped as a diagnostic option. Additionally, long-term/severe deficiency can present with symptoms almost identical to multiple sclerosis[3]. Deficiency of other B vitamins, such as B2, can cause a functional B12 deficiency as well[4]. It’s also known that supplementation will falsely elevate levels even in the presence of a deficiency. /unsoapbox [1] https://www.mcpiqojournal.org/article/S2542-4548(19)30033-5/... [2] https://www.bmj.com/content/383/bmj-2022-071725 [3] https://researchonline.nd.edu.au/cgi/viewcontent.cgi?article... [4] https://www.iomcworld.org/articles/paradoxical-vitamin-b12-d... reply OutOfHere 14 hours agoparentThe paradoxical B12 deficiency might have a relation with the inactive form being supplemented. If one supplements cyanocobalamin, and one expects the body to convert it to methylcobalamin, and if this conversion doesn't happen for whatever reason, and if the measured form includes the inactive form, then \"paradoxical B12 deficiency\" can be observed. There is a more insidious form of it whereby the active form doesn't enter the brain. Symptoms are: difficulty speaking, tremors and ataxia. This can be tested by CSF (cerebrospinal fluid) testing. This assumes that the active form is supplemented and it is present in blood. It can be remedied by a course of corticosteroid pills, followed by megadosing B12 orally daily. Refer to PMID 38924428. I take a triple active form of it which is methyl+hydroxy+adenosyl, covering all bases, but a total of just 500 mcg per day, above which it harms my sleep. reply jdhendrickson 13 hours agorootparentWould you mind sharing a brand or a link? I have had grey hair since I was in my 20s and struggle with energy. I have been taking nature made multi vitamin for a vitamin b deficiency that was high enough to cause craggy edges on my tongue, caused by the stomach acid suppressant I was prescribed. I would like to compare. reply OutOfHere 5 hours agorootparenthttps://www.amazon.com/gp/aw/d/B09G6ZDCDP Use it with a 2-way or 3-way pill splitter, just once piece per day. For acid, use famotidine to the extent possible, in as low a dose as does the job. For overnight suppression, however, omeprazole is useful. reply notamy 14 hours agorootparentprev> The paradoxical B12 deficiency might have a relation with the inactive form being supplemented. Yup, not everyone can convert cyanocobalamin or hydroxocobalamin to adenosyl-/methylcobalamin. Especially in severe cases, anecdotally I’ve seen people not make progress with the standard cyanocobalamin injections, but then make huge progress with methylcobalamin injections. Unfortunately, methylcobalamin is often not preferred in injectable form due to very quick degradation into hydroxocobalamin upon exposure to light. reply cjsawyer 1 hour agorootparentTaking methylcobalamin B12 supplements essentially fixed my memory problems. Early on I tried out the cyanocobalamin type and got absolutely nothing from it. If I stop taking the supplements for a few weeks, the memory problems come back. If I start again, a week later my memory is excellent. It’s well worth the minor effort. reply FollowingTheDao 6 hours agorootparentprevSubligual methylcobalamin works just as well as the shots and you can buy it anywhere. Also, get a Methylmalonic Acid Test with you B12 levels. An MMA test is more useful because it is an enz=yme which uses B12, so when the active form of B12 is low, so will MMA in most cases. reply notamy 4 hours agorootparent> Subligual methylcobalamin works just as well as the shots It does not. The cobalamin molecules are quite large and don’t penetrate the sublingual mucosa very well. On top of that, the proteins in saliva that bind to B12s will cause any bound B12 to not absorb sublingually, instead having to wait to make it through the digestive system for intestinal uptake. Sublingual is still certainly a better option than oral pills, but the injections are preferable in severe cases for a reason. reply FollowingTheDao 23 minutes agorootparent> The cobalamin molecules are quite large and don’t penetrate the sublingual mucosa very well. Absolute nonsense. Did you even bother to look for one paper on the subject? Here are a few: https://www.scielo.org.mx/scielo.php?pid=S0034-8376202000060... https://pubmed.ncbi.nlm.nih.gov/34871525/ https://www.proquest.com/openview/e9a25963dbc92150e420a3a847... reply voisin 13 hours agorootparentprevOk, so assuming you don’t want to have your CSF tested, is there any risk of harm to supplementing with the active form? reply OutOfHere 13 hours agorootparentThere's no risk in non-smokers if you stick to a sane dose. Megadosing it can harm sleep and severely increase blood pressure and heart rate, although these gradually reverse upon cessation. This can take time to manifest. It is why I limit the dose to 500 mcg per day. Older people, such as those over 70, can need more and tolerate more, even 5 mg per day, due to deteriorated absorption. Also, don't forget the other B vitamins. In truth I take them all. reply notamy 13 hours agorootparent> Megadosing it can harm sleep and severely increase blood pressure and heart rate https://www.researchgate.net/profile/Osama-Arafat/publicatio... > METHYLCOBALAMIN HAS AN EFFECT ON HYPOTHALAMIC–HYPOPHYSEAL– ADRENAL AXIS In rats, but may apply to humans too. reply Terr_ 13 hours agorootparentprev> There's no risk in non-smokers if you stick to a sane dose. Also, B12 is water soluble, which generally means that your body is pretty good at flushing out any excess it can't use, making it difficult to overdose on. (Compared to fat-soluble substances.) reply OutOfHere 6 hours agorootparentA little extra B12 is easily flushed out, but when it's a lot, meaning several milligrams per day for many days, it can quickly and suddenly cause very elevated blood pressure and heart rate, e.g. 160 for both SBP and HR. This effect is easy to undo using a good beta blocker, e.g. atenolol, but if one doesn't, then an emergency hospital visit is required. The point is that despite being water soluble, this adverse effect does happen. Even at just one milligram per day, it can significantly harm sleep. It has a high circulating half life of six days, which implies that it can really build up if repeatedly megadosed in this period. reply vldmrs 13 hours agorootparentprevDoes it help you with grey hair ? reply OutOfHere 6 hours agorootparentNo, it's useless to me in this regard, although it could help someone who is efficient. I think my issue may have more to do with copper. reply Solstinox 3 hours agoparentprevAlso, over half the world's population has H. Pylori, which interferes with B12 absorption. reply pcb-rework 8 hours agoparentprevI take supplemental B12 and a B-complex because I get hair thinning, brittle nails, and peripheral edema and neuropathy without them. (Also taking levothyroxine and slo iron.) I feel way better with it than without it, and it ain't placebo because I have a terrible habit \"memory\", forget it often, and remember forgetting after my feet remind me. There's a noticeable point 30 to 90 minutes later at which they spontaneously feel better, but I completely forget about taking or not taking them. (I really need a pill planner like an old person.) I wasn't aware of the apparent connection until after numerous episodes and connecting the two. reply OutOfHere 6 hours agorootparentUse a pill box. It's not just for old people. It's good for everyone. reply snthpy 14 hours agoparentprevInteresting. I had it 5 years ago and then it went away. It started again a few weeks ago and I halved my vitamin B supplementing a while before that. reply SOLAR_FIELDS 14 hours agoparentprevThe reason I take B12 as a supplement is unrelated to this - I have MTHFR mutation and as such I need to take methyl folate (methylated B9). Apparently this form of B9 absorbs better if it’s combined with the methylated form of B12 (methylcobalamin) I guess if it will keep my hair color longer that’s a nice side effect! reply notamy 14 hours agorootparent> Apparently this form of B9 absorbs better if it’s combined with the methylated form of B12 (methylcobalamin) B9 and B12 are interdependent. When the body uptakes cobalamins, the ligands are pulled off and replaced with ligands sourced from other processes. IIRC in the case of methylcobalamin, the methyl group is pulled off and replaced with a methyl group that’s moved over from methylfolate via a riboflavin-dependent reaction. Been a hot minute since I looked into this so I may have some details incorrect. reply jimbob45 14 hours agoparentprevIt’s also known that supplementation will falsely elevate levels even in the presence of a deficiency. It sounds to me that what you’re saying is that a daily multivitamin would not help in this case. reply notamy 14 hours agorootparentIt may help, but it will also throw off lab tests while you’re taking it and for a time after. Also, B12 absorption depends strongly on good gut health (ex. no SIBO[1], low gastric acid, pernicious anaemia, etc.), as well as on other medications (ex. metformin[2]) not interfering with absorption. [1] https://en.wikipedia.org/wiki/Small_intestinal_bacterial_ove... [2] https://en.wikipedia.org/wiki/Metformin CTRL-F \"B12\" reply FollowingTheDao 6 hours agorootparentGenetics will play a huge role in B12 absorption. Transcobalamin to be specific. https://ashpublications.org/blood/article/114/22/1989/111148... reply OutOfHere 14 hours agorootparentprevMost multivitamins are useless because they generally contain suboptimal forms, doses, and ratios. If you want to see good effects, consider individual vitamins in appropriate forms and doses. reply dotancohen 10 hours agorootparentThe barrier to entry is far too high for most people. A safe, minimal dosage that will help in cases of severe deficiency can be had with an inexpensive pill and negligible time investment. Regular blood tests and dosing is expensive in mental effort, time, and money. reply OutOfHere 6 hours agorootparentA regular blood test is mainly needed just for vitamin A, vitamin D, and sometimes for vitamin B6. The rest won't silently cause critical issues if somewhat exceeded. As for minerals, it is possible to manage the dose without a need for testing. reply dotancohen 4 hours agorootparentThank you. I will go get those tests. reply OutOfHere 3 hours agorootparentFor B6, it's safer to supplement only the P5P form, as it's much less likely to cause any serious or lasting issue. I would limit intake of P5P to a max of 40 mg per day. If you do this, and you don't have any resulting neuropathy problems (you shouldn't), then you don't need to test it. In contrast, the inactive cheaper form of B6 which is pyridoxine is more likely to result in problems if taken at above 30 mg/day in the long term. reply dotancohen 1 hour agorootparentI very much appreciate your advice. How would I recognize the different forms of B6 in the bottle? Would you say that a multivitamin from a well-respected company, like Centrum, would be a good choice? I'm trying to lower barriers here, for myself and my family. Again, thank you. reply OutOfHere 1 hour agorootparentCentrum is a start, but it is not great for dozens of reasons. It would contain B6, the pyridoxine form, not the P5P form, and not in a dose that's sufficient for stress reduction. I take all individual vitamins and individual minerals. reply dotancohen 23 minutes agorootparentTerrific, thank you. Going through your posts I see quite a few terms to google and learn about. I appreciate your advice, and for kickstarting a health improvement in my life that I've been meaning to do for years. reply SOLAR_FIELDS 14 hours agorootparentprevIt’s one of the reasons that regulation would help a lot here. One simple example is chelation. Your body absorbs magnesium when it’s chelated at a rate many multiple times higher than what you might typically get from an OTC multivitamin. So a multivitamin might contain some magnesium, but what it doesn’t tell you on the label is that the form of magnesium they give you is not going to be absorbed at all and just pass right through your body. Most people need some other formulation of magnesium to actually be absorbed. I would go so far as to say that a fair amount of what goes into a lot of multivitamins on the label is borderline fraud because of stuff like this reply A_D_E_P_T 9 hours agoprevI had been looking into this recently. My beard is graying and it's annoying me excessively. 10 years ago, the research consensus behind hair graying was, \"we don't know what causes it, lol.\" Today, it's a little bit better understood -- though far from completely understood. There's a handy review article here: https://pmc.ncbi.nlm.nih.gov/articles/PMC10535703/ To summarize, there's no known agent that can reliably repigment gray hair. Sometimes powerful drugs repigment hair as a side-effect. Hair graying results from the dysfunction or loss of melanogenic melanocytes and the depletion or immobility of McSCs, often due to aging or stress. Lots of cellular signalling pathways are involved. The Wnt/β-Catenin pathway promotes melanocyte stem cell (McSC) proliferation and differentiation, while the MC1R/cAMP pathway, activated by α-MSH, drives melanin production via the MITF transcription factor. The SCF/c-KIT pathway supports melanocyte survival and function, and the Endothelin/EDNRB pathway stimulates both melanocyte proliferation and melanogenesis. In contrast, the PI3K/AKT pathway inhibits melanogenesis by suppressing MITF activity, and the TGF-β pathway maintains McSC quiescence while inhibiting melanogenesis. Stress is actually a factor because activation of the sympathetic nervous system can deplete McSCs, and neuropeptides like CGRP, SP, and VIP, can either enhance or suppress melanogenesis in ways which are, as yet, unclear. Dermal white adipose tissue (dWAT) near hair follicles also plays a role by secreting factors such as adiponectin that affect hair growth and pigmentation. A drug to reverse or prevent hair graying would be very welcome, so I hope that the phenomenon becomes better understood in the near future, and then we get products that work. reply meigwilym 8 hours agoparent> My beard is graying and it's annoying me excessively. So is mine, and now I've lost most of the hair on my head I shave it so completely bald. But I've learnt to accept it. I don't like it, and wish it was different. There's nothing I can do and I have plenty of more pressing problems to worry about. So I keep fit and eat reasonably healthily. So even though I look old, I don't have to act it reply 7bit 7 hours agorootparentI've also learnt to accept my recewding and thinning hairs. Yet, Everytime I look in the mirror I also don't like it and wish it were different. Have I really accepted it? reply whtsthmttrmn 3 hours agorootparentNo but beating yourself up over whether or not you've accepted it won't help. reply aktau 7 hours agoparentprevYou've obviously done a bit of research. Have you ever seen the vitamin B12 link referenced in other studies? Are you taking supplements or did you get it tested? reply abrookewood 7 hours agoprevSlightly off topic, but I have found a Vitamin B supplement all but eliminated my migraines. I was getting them every 1-2 weeks and then, after taking Vitamin B regularly (to counteract alcohol consumption during the festive season), I realised I hadn't had one in ages, despite running on reduced sleep (which seems to be my main trigger). Anyway, I always make this comment anywhere I can because migraines are horrible, vitamins are cheap and maybe it will help someone else (obviously anecdotal; sample size of 1; I can't find any supporting research etc). reply elric 6 hours agoparentVitamin B2 (Riboflavin) in particular can be helpful for those with migraines. But it seems like it only effects those with migraines headaches, and not migraine auras. Did nothing for me :-( Also N=1, but I remember coming across some research that confirmed this when I was taking the supplements. reply casenmgreen 6 hours agoparentprevWhich B vitamin(s)? there are a number of them. reply OutOfHere 15 hours agoprevRegarding PABA, I have been taking 500 mg/day for years, and it hasn't done anything at all for restoring my hair color or even for freezing further change. A much higher dose of it can be risky and it's not worth the gamble. I have also been taking B5 as both calcium pantothenate at 500 mg/day and also as pantethine at 300 mg/day for years, neither of which has done anything either in this context. Similarly, B12 hasn't visibly helped either. I suspect that copper is the issue with me. The lowest dose of MitoQ, a 5 mg capsule per day, had lowered my blood pressure significantly after a month of use, well below normal, approaching an unsafe low. Moreover, it took another month after discontinuation of MitoQ for the blood pressure to normalize. I shudder to think how much more powerful SkQ1 would be if taken orally. My first impression is that SkQ1 seems more relevant for local use than for systemic use. reply notamy 13 hours agoparent> I suspect that copper is the issue with me. Prolonged copper deficiency can cause irreversible neuropathies and CNS damage. You should try to get tested for it if you can. reply keyle 11 hours agoparentprevThanks for your account of these supplements. I thought PABA looked interesting until reading this. Hopefully some day we can have an over the counter drug that restores or slows graying. I don't have issues with mine beyond it being patchy, distracting. reply hackernewds 13 hours agoparentprevseems like unnecessary dangerous experimentation with the hopes of something cosmetic. why not just color hair? reply OutOfHere 13 hours agorootparentJust because you are uninformed about certain things doesn't mean everyone else is too. For example, driving on the highway could look dangerous to a person from the jungle, but it isn't to someone who knows what he's doing. Your comment is the same way. If I try or take something, it's because I have studied it reasonably. The things I named are not untested high-risk substances. In fact, hair colors contain toxic harmful chemicals that even increase the risk of certain cancers. I do color my hair, but I don't like it, and I do it as little as necessary. reply naught0 13 hours agorootparentThe highway is remarkably dangerous for even excellent drivers reply edu 12 hours agorootparentDangerous compared to what? Because in terms of driving highways are safer (have less accidents) than urban and rural roads, and the most dangerous areas are intersections. reply OutOfHere 5 hours agorootparentYou are absolutely correct, of course, but that's clearly not the context I used it in. reply freehorse 10 hours agorootparentprevIf the gray hair indicates an underlying health issue/deficiency, dying them is not going to help with that. reply Cumpiler69 11 hours agorootparentprev>why not just color hair? Or if you're a male, why bother with coloring at all? Just own the young silver fox look and rock with it. Or shave it and grow a huge beard. As a male you have a lot of natural workarounds against imperfect hair color.. reply chownie 2 hours agorootparentSome of us don't grow beards, so you just end up looking like a tall child struggling on hard times. reply james_marks 13 hours agoprev> INTRODUCTION Haalthy hair is a sign of general well-being and youth. Not exactly confidence inspiring when the very first word is a typo. reply onionisafruit 12 hours agoparentI looked at INTRODUCTION letter-by-letter several times before realizing the spelling issue is in “Haalthy”. That’s the second word. reply bowsamic 12 hours agorootparentHeadings don’t count as the body text reply ramraj07 12 hours agoparentprevAuthor list, author affiliation and past history all suggest this might be a green card paper. reply DoingIsLearning 10 hours agorootparentHow does that work in the US they pump papers out as evidence of exceptional ability or something along those lines? I thought that mattered for entry visas not for residency. reply duck 13 hours agoparentprev+1 how is this even possible to do? reply elric 6 hours agoprevAs someone who started balding at ~19, I would have been very happy with grey hair instead. One person's self-esteem damaging condition is another person's hope. reply whtsthmttrmn 3 hours agoparentAs someone who got grey hair in their 20's and doesn't have to deal with it thinning, I am aware of my good fortune. reply gnulinux 13 hours agoprevIn our world today it seems impossible to complain about stress. Everyone around me from boss to family to even therapist keep telling me to remind myself of how great of a life I have and that I should know better than to complain. But it feels to me like I'm drowning in an ocean stress everyday with no way to escape it. I keep being compared to others who can seemingly take the stress well and am questioned why I can't take similar level of high-stakes, indeterministic responsibilities with severe dangers that are beyond my control, and little to no positive pay-off for me. Some people around me are extremely health conscious, they avoid all kinds of chemicals, eat religiously well, work-out regularly, sleep well etc yet when they show obvious signs of stress because they're working 12 hours a day in their torturous FAANG job, I'm treated like an antivaxxer for pointing it out. In this world of hustle culture, it seems like a taboo to talk about how unhealthy stress is. Stress today is like 70s cigarettes, everybody is doing it, so no need to worry about its health effects. And yes I have graying hair at mid-late 20s. Sometimes I'm astonished how people don't complain about their stress levels. I feel weak, child-like, immature, and feeble being unable to tolerate maybe 10% of what my wife can. reply EZ-E 13 hours agoparentUnsolicited advice : have you tried stopping coffee / caffeine? This improved my stress levels immensely although I only drank one cup a day. reply OutOfHere 5 hours agorootparentI drink coffee and tea more for their beneficial phyto chemicals. If I were to stop them, I would lose this benefit. I limit it to a single cup of each. B6 as P5P at 20 mg twice daily has helped my stress. Similarly, magnesium citrate has too. Theanine is also relevant at night. I think the best cure for stress for someone who has saved up money is to quit their day job. reply JanSt 10 hours agorootparentprev+1 reply aeternum 12 hours agoparentprevYou can work hard and not be stressed. Stress often comes from lack of control, what are the severe dangers? The most significant danger seems to be you get a low performance review. As a faang engineer that's a pretty weak a significant danger as there are many other jobs that will simply assume you're good. reply Cumpiler69 11 hours agorootparent>Stress often comes from lack of control Not just lack of control, but lack of a light at the end of the tunnel. A lot of people can tolerate high stress for a while provided they know there's a payoff at the end, an assurance that things are gonna get better. But when that payoff is taken away or non existent (no money, dead end job, shit living conditions, no chance of home ownership, no having a family, friends group, etc) people can start to fall apart even with low amounts of stress. reply FollowingTheDao 6 hours agoparentprevYes, very frustrating. I cannot take stress. It literally makes me psychotic. (I have Schizoaffective Disorder, Bipolar Type) I was making six figures at Cisco in 1999 when I lost it all from the stress. But now that I have seen my actual genetics, I get it. I am different and there is no adapting for me. My environment needs to look much different or I will die early like my brother and mother. I could work if the job was flexible enough but that is impossible. Capitalism needs us to be all the same because that is easier to manage. Now that they weeded people like me out they are ramping up the stress and weeding out the more stable people with this BS gig work and hustle meme. You are being crushed for someone elses profit. P.S. Full head and beard of grey hair at 58 and I love it, would not change it. My grandfather was grey at 50 and lived till he was 98 so who cares. reply CuriouslyC 12 hours agoparentprevBeing able to tolerate stress comes down to reasons. If you're doing stuff for good reasons you focus on those and keep your head down. If you don't have good reasons either find some or stop taking on pointless stress. reply zingababba 3 hours agoprevI started graying when I fucked around with nicotine for a bit. reply ivolimmen 13 hours agoprevWell I have gray hair since I was 16. It never made me feel insecure. reply RajT88 12 hours agoparentI would guess you are a man. It's way less bad for us, visibly aging. One of my oldest friends is a lady who went gray in her 20's. I only found out in her late 30's when she went through some mental health issues and missed aggressively dying to hide the roots. reply Ancalagon 13 hours agoprevMan I sure know a lot of early 30 year olds with graying hair reply shitter 14 hours agoprev> In men, graying first occurs in the temples and sideburns. It spreads to the vertex and rest of the scalp involving the occiput the last. My graying started when I was 15 and first appeared as a single silver strand at the center of my hairline. At 28, I now have a cluster of them in that center spot, as well as diffusely all over my head, including the occiput. I think my temples and sideburns were actually relatively spared. Also of interest to me: once in a blue moon, I shed a hair that appears to be reverting from gray back to pigmented -- it's gray close to the tip and black closer to the root. I wonder what factors might cause this reversal. reply sroussey 14 hours agoparent> I wonder what factors might cause this reversal. Very likely: Stress. Also diet and exercise. They are interrelated. But for people that I’ve known have hair go gray and back again, it was stress. reply treyd 13 hours agorootparentI know this is anecdotal but I know someone that experienced / is experiencing a similar thing. Several silvery hairs reverting back to pigmented after leaving an overly stressful work environment. She only had a few, but of those few perhaps 30%-50% have reverted. reply thaumasiotes 14 hours agoparentprevI have no graying of head hair. But I do have several white hairs in my facial hair. I wouldn't have expected to read that \"graying first occurs in the temples and sideburns\". (I also have one white hair that grows inside my nose...) reply butternet 10 hours agoprevLeaving aside other health concerns grey hair can be attractive on all genders and should be respected and celebrated. reply amarcheschi 9 hours agoprevMy grandfather started having white hairs at 20, I'm a bit older and I have a few strands of white hair. Honestly, I love white hair. It's not very noticeable on me though, I get it can be embarrassing reply n1b0m 7 hours agoprevI was diagnosed with hypothyroidism a decade ago and it seems to coincide with premature graying in my 30s reply FollowingTheDao 6 hours agoparentThis is probably linked to high oxidative stress. H2O2 plays a role both with thyroid production and with greying hair. They talk about it in the paper. reply donatj 7 hours agoprevI started graying in my mid twenties, I am in my late thirties now and my hair is a little over half gray at this point. I would frankly like to do something about it, my wife has been insistent that I don't because she likes it. The possibility of it being some sort of vitamin deficiency that this raises has me feeling like I should at least get my levels tested. I can't imagine that it is a B12 deficiency however due to how much canned fish I consume. It's become my go to easy lunch since the beginning of COVID. reply OutOfHere 5 hours agoparentBe careful with the canned fish, especially canned sardines, as they risk arsenic. Canned tuna and some others can risk mercury. I wouldn't eat canned fish more than once or twice a week. Always make sure to cook them well, otherwise worms are risked. The ocean is used by us as one big garbage dump. reply donatj 1 hour agorootparentYou shouldn't need to cook canned fish, they are cooked after canning at the factory, which is why they are shelf stable for years. reply OutOfHere 1 hour agorootparentYou shouldn't have to, but you would be taking a massive risk, and risk becoming a medical statistic when worms grow and multiply, also entering various organs. In me, canned tuna caused major problems that required antibiotics to resolve after conducting thousands of dollars of tests. The tuna evidently had certain highly inflammatory bacteria. I consider myself lucky that the antibiotic worked at all. reply louthy 6 hours agoparentprev> I started graying in my mid twenties, I am in my late thirties now and my hair is a little over half gray at this point. I would frankly like to do something about it, my wife has been insistent that I don't because she likes it. Full grey here. The amount of comments and compliments I get about my hair is unreal. I never really got any before greying. I say embrace it. reply sterlind 8 hours agoprevI started going gray at 16. My dad would pull my gray hairs out and marvel at them whenever he saw one - without thinking to ask first, of course. they joked it must be from stress - disquieting that it might have been metabolic, and that they never considered taking me to a doctor. reply faraaz98 14 hours agoprevIt can also be caused by autoimmune thyroidism reply FollowingTheDao 6 hours agoparentBut what causes autoimmune hypothyroidism? (Hint, it is probably oxidative stress.) Receipts: https://pmc.ncbi.nlm.nih.gov/articles/PMC5040049/ https://www.sciencedirect.com/science/article/abs/pii/S00088... https://link.springer.com/article/10.1186/s12902-023-01348-9 https://etj.bioscientifica.com/view/journals/etj/12/6/ETJ-23... reply on_the_train 12 hours agoprevThis title had me hoping that this was a gwern article reply dukeofdoom 13 hours agoprevSo what's the best brand for coloring grey hair. Preferably non toxic. reply WrongAssumption 12 hours agoparentI use this. https://www.amazon.com/American-Crew-Precision-Blend-Hair/dp... It’s a temporary hair color, which tends not to damage your hair. I find it lasts about a month. reply dyauspitr 12 hours agoparentprevHenna? Though it will give your hair a shade of red and not black. reply trhway 13 hours agoprevThere seems to be a lot about immune system activation attacking melanocytes like this: https://www.uab.edu/news/research/item/9390-study-explains-o.... Thus not surprisingly “Covid and gray hair” brings some results too, including reversing of Covid associated gray hair. reply slt2021 11 hours agoprevTLDR: root cause is working with dynamic and weakly typed languages and poorly documented and maintained spaghetti code reply theultdev 15 hours agoprevtldr: stress reply mxxc 11 hours agoprevlol at scientific papers using the word \"caucasian\" like it's the 70s reply sandebert 12 hours agoprev [–] OT, but I can't not notice the wild domain. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Premature graying of hair (PGH) is defined as graying before age 20 in Caucasians and before 30 in African Americans, impacting self-esteem.- The causes of PGH are not fully understood but are associated with aging disorders, autoimmune diseases, and oxidative stress, with factors like smoking and nutritional deficiencies contributing.- Common treatments for PGH include hair dyes and nutritional supplements, with studies indicating links between PGH and emotional factors, genetic predispositions, and environmental influences."
    ],
    "commentSummary": [
      "Premature graying of hair is associated with vitamin B12 deficiency, particularly in individuals with pernicious anemia, a condition where the body cannot absorb B12 properly.- Diagnosing B12 deficiency can be challenging as supplementation may artificially increase B12 levels, and some individuals cannot convert certain forms of B12 to active forms, leading to \"paradoxical B12 deficiency.\"- Factors such as stress, genetics, and health conditions like autoimmune thyroidism also contribute to graying, and while some supplements may help, there is no proven method to reverse gray hair."
    ],
    "points": 110,
    "commentCount": 98,
    "retryCount": 0,
    "time": 1732583700
  },
  {
    "id": 42245170,
    "title": "LLVM-powered devirtualization",
    "originLink": "https://blog.thalium.re/posts/llvm-powered-devirtualization/",
    "originBody": "LLVM-powered devirtualization This work was carried out during an internship at Thalium on the subject of deobfuscation of virtualized binaries. Context Obfuscation is the process of deliberately making code difficult to understand in order to hinder its analysis. It is often used in malware to conceal malicious intent and avoid detection. Various binary obfuscation strategies exist today, including: Removing comments / symbols Adding opaque predicates (branches on a constant condition) Control flow flattening Virtualization Virtualization is one of the most popular, and potent1 forms of obfuscation today. Various obfuscators including Tigress, Themida and VMProtect offer virtualization. Due to its potency and the high availability of obfuscators, virtualization has unfortunately been used by threat actors and found in numerous malware (source: MITRE). Here is a rough outline of the architecture of a virtualized obfuscated binary: In a virtualized binary, the original program gets encoded into a sequence of virtual instructions. The obfuscated program is going to contain these virtual instructions as well as an interpreter. The interpreter is responsible for executing the virtual instructions. It often contains distinctive components: a VM entry responsible for context switching and initializing the virtual CPU; a VM exit, also for context switching; a dispatcher which retrieves the virtual instruction at the virtual program counter (VPC), then sends the control flow to the appropriate handler for the given virtual instruction; handlers, which encode the semantics of the different virtual instructions. Virtual instructions can be anything ranging from logical operations (AND, XOR), control flow operations (JUMP, BRANCH) to more complex operations (see Schloegel et. al. Loki: Hardening Code Obfuscation Against Automated Attacks (usenix.org)). Devirtualization strategy Manual analysis If you have ever been led to manually reverse engineering a VM (a program protected with virtualization), you know that it is often a long and complex task. If you haven’t, you can check out our other post kaleidoscope and give it a try yourself! The intuitive approach to devirtualization is usually to identify the VM context (e.g. where are located the virtual program counter and other virtual registers), reverse engineer each handler, and then create a custom disassembler for this VM. We can then disassemble the virtual instructions and reverse engineer the original program. This is extremely tedious, mainly because the architecture used in the VM may change in each obfuscated program. Moreover, virtualized binaries can be further obfuscated, making it hard to identify and reverse engineer the handlers. For example, a common hardening strategy is to encrypt the virtual instructions. Automated analysis A different approach put forward by Yadegari et al. in their paper A Generic Approach to Automatic Deobfuscation of Executable Code (ieee.org) consists in using a dynamic taint analysis to reconstruct the original program’s CFG. Jonathan Salwan et al. expanded this strategy in Symbolic deobfuscation: from virtualized code back to the original (shell-storm.org) by then leveraging symbolic execution and compiler optimizations to recreate a devirtualized program. Our devirtualization approach was heavily inspired by these taint-based methods. Given the limited timeframe of the internship, we slightly simplified this approach. We also wanted to experiment with removing the symbolic execution component which is notoriously slow. For our devirtualization, we perform a dynamic taint analysis on the obfuscated function, tainting its parameters. We then split an execution trace of the obfuscated binary each time we encounter a tainted conditional instruction in the CFG (branches, cmove, etc.). Finally, we combine these blocks to reconstruct a CFG. This approach is not the most robust, but worked well for the Tigress obfuscated binaries we tested against (this will be further detailed in the Results section). Example The best way of explaining our approach is through an example. Let’s consider we have a simple program: int counter(int input) { int i = 0; while (i > ((((unsigned __int8)((input & 0xE4) + 0x19) >> 3) & 7u) - 1) >> 1); if ((v16_0 ^ v16_1) == V16_2 * v16_3) { _mermaid_missing_block(); JUMPOUT(0x2B2LL); } In the C extract, you can see a call to a missing block because we did not visit that execution path. Had we chosen a different input, we could have explored this branch (but not another). To achieve better coverage, we could use symbolic execution to find inputs allowing us to visit various execution paths. Limitations Although we achieved promising results regarding speed and modularity, the scope of this internship was still quite limited: we only considered deobfuscation of pure functions with no calls; we only deobfuscate a single execution path; our deobfuscator struggles with certain loops. Complete report If you found this work interesting, we have decided to make the full internship report publically available here. The report contains additional technical details, a discussion on limitations as well as our future plans. I am continuing with a PhD at Thales, so stay tuned for further developments. If you have any questions feel free to reach out! Potency is the measure of how much obfuscators render code unintelligible. ↩︎ #Deobfuscation #Tooling #LLVM #Reverse Engineering 2024-11-22 by Jack Royer",
    "commentLink": "https://news.ycombinator.com/item?id=42245170",
    "commentBody": "LLVM-powered devirtualization (thalium.re)107 points by dddnzzz334 6 hours agohidepastfavorite1 comment anthk 4 hours ago [–] Also, Bochs can fool most VM detectors as it can emulate a whole CPU in software, but an i7 might be able to run a fully emulated Pentium 4 based computer with ease in almost real time. But Bochs' debugger can do crazy things to most malware and propietary obfuscators. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The internship at Thalium focused on deobfuscating virtualized binaries using LLVM, a compiler framework, to make code more understandable, especially in the context of malware.- Virtualization, a strong obfuscation technique used by tools like Tigress and VMProtect, encodes programs into virtual instructions, making reverse engineering complex.- The project successfully used dynamic taint analysis to reconstruct the control flow graph of Tigress-obfuscated binaries, though it was limited to pure functions and single execution paths."
    ],
    "commentSummary": [
      "LLVM-powered devirtualization improves performance by optimizing virtual function calls, which are typically slower due to their dynamic nature.",
      "Bochs, a CPU emulator, can efficiently emulate a Pentium 4 processor on an i7, even bypassing virtual machine detectors.",
      "The Bochs debugger is particularly effective in analyzing and countering malware and code obfuscators, making it a valuable tool for security research."
    ],
    "points": 107,
    "commentCount": 1,
    "retryCount": 0,
    "time": 1732624714
  }
]
