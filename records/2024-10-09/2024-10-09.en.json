[
  {
    "id": 41786101,
    "title": "Chemistry Nobel: Computational protein design and protein structure prediction",
    "originLink": "https://www.nobelprize.org/prizes/chemistry/2024/press-release/",
    "originBody": "Press release Navigate to: Summary - David Baker - Demis Hassabis - John M. Jumper Prize announcement Press release Popular information Advanced information English English (pdf) Swedish Swedish (pdf) 9 October 2024 The Royal Swedish Academy of Sciences has decided to award the Nobel Prize in Chemistry 2024 with one half to David Baker University of Washington, Seattle, WA, USA Howard Hughes Medical Institute, USA “for computational protein design” and the other half jointly to Demis Hassabis Google DeepMind, London, UK John M. Jumper Google DeepMind, London, UK “for protein structure prediction” They cracked the code for proteins’ amazing structures The Nobel Prize in Chemistry 2024 is about pro­teins, life’s ingenious chemical tools. David Baker has succeeded with the almost impossible feat of building entirely new kinds of proteins. Demis Hassabis and John Jumper have developed an AI model to solve a 50-year-old problem: predicting proteins’ complex structures. These discoveries hold enormous potential. The diversity of life testifies to proteins’ amazing capacity as chemical tools. They control and drive all the chemi­cal reactions that together are the basis of life. Proteins also function as hormones, signal substances, antibodies and the building blocks of different tissues. “One of the discoveries being recognised this year concerns the construction of spectacular proteins. The other is about fulfilling a 50-year-old dream: predicting protein structures from their amino acid sequences. Both of these discoveries open up vast possibilities,” says Heiner Linke, Chair of the Nobel Committee for Chemistry. Proteins generally consist of 20 different amino acids, which can be described as life’s building blocks. In 2003, David Baker succeeded in using these blocks to design a new protein that was unlike any other protein. Since then, his research group has produced one imaginative protein creation after another, including proteins that can be used as pharmaceuticals, vaccines, nanomaterials and tiny sensors. The second discovery concerns the prediction of protein structures. In proteins, amino acids are linked together in long strings that fold up to make a three-dimensional structure, which is decisive for the protein’s function. Since the 1970s, researchers had tried to predict protein structures from amino acid sequences, but this was notoriously difficult. However, four years ago, there was a stunning breakthrough. In 2020, Demis Hassabis and John Jumper presented an AI model called AlphaFold2. With its help, they have been able to predict the structure of virtually all the 200 million proteins that researchers have identified. Since their breakthrough, AlphaFold2 has been used by more than two million people from 190 countries. Among a myriad of scientific applications, researchers can now better understand antibiotic resistance and create images of enzymes that can decompose plastic. Life could not exist without proteins. That we can now predict protein structures and design our own proteins confers the greatest benefit to humankind. Illustrations The illustrations are free to use for non-commercial purposes. Attribute copyright as below: Illustration: A protein can consist of everything from tens of amino acids to several thousand (pdf) ©Johan Jarnestad/The Royal Swedish Academy of Sciences Illustration: How does AlphaFold2 work? (pdf) ©Johan Jarnestad/The Royal Swedish Academy of Sciences Illustration: Top7 – the first protein that was entirely different to all known existing proteins (pdf) ©Terezia Kovalova/The Royal Swedish Academy of Sciences Illustration: Proteins developed using Baker’s program Rosetta (pdf) ©Terezia Kovalova/The Royal Swedish Academy of Sciences Illustration: Protein structures determined using AlphaFold2 (pdf) ©Terezia Kovalova/The Royal Swedish Academy of Sciences Read more about this year’s prize Popular science background: They have revealed proteins’ secrets through computing and artificial intelligence (pdf) Scientific background: Computational protein design and protein structure prediction (pdf) David Baker, born 1962 in Seattle, WA, USA. PhD 1989 from University of California, Berkeley, CA, USA. Professor at University of Washington, Seattle, WA, USA and Investigator, Howard Hughes Medical Institute, USA. Demis Hassabis, born 1976 in London, UK. PhD 2009 from University College London, UK. CEO of Google DeepMind, London, UK. John M. Jumper, born 1985 in Little Rock, AR, USA. PhD 2017 from Uni­versity of Chicago, IL, USA. Senior Research Scientist at Google DeepMind, London, UK. Prize amount: 11 million Swedish kronor, with one half to David Baker and the other half jointly to Demis Hassabis and John Jumper. Further information: www.kva.se and www.nobelprize.org Press contact: Eva Nevelius, Press Secretary, +46 70 878 67 63, eva.nevelius@kva.se Expert: Johan Åqvist, +46 70-425 04 04, johan.aqvist@icm.uu.se, member of the Nobel Committee for Chemistry The Royal Swedish Academy of Sciences, founded in 1739, is an independent organisation whose overall objective is to promote the sciences and strengthen their influence in society. The Academy takes special responsibility for the natural sciences and mathematics, but endeavours to promote the exchange of ideas between various disciplines. Nobel Prize® is a registered trademark of the Nobel Foundation. To cite this section MLA style: Press release. NobelPrize.org. Nobel Prize Outreach AB 2024. Wed. 9 Oct 2024.Back to top",
    "commentLink": "https://news.ycombinator.com/item?id=41786101",
    "commentBody": "Chemistry Nobel: Computational protein design and protein structure prediction (nobelprize.org)424 points by mitchbob 9 hours agohidepastfavorite230 comments eig 6 hours agoI think I disagree with most of the comments here stating it’s premature to give the Nobel to AlphaFold. I’m in biotech academia and it has changed things already. Yes the protein folding problem isn’t “solved” but no problem in biology ever is. Comparing to previous bio/chem Nobel winners like Crispr, touch receptors, quantum dots, click chemistry, I do think AlphaFold already has reached sufficient level of impact. reply roughly 1 hour agoparentIt also proved that deep learning models are a valid approach to bioinformatics - for all its flaws and shortcomings, AlphaFold solves arbitrary protein structure in minutes on commodity hardware, whereas previous approaches were, well, this: https://en.wikipedia.org/wiki/Folding@home A gap between biological research and biological engineering is that, for bioengineering, the size of the potential solution space and the time and resources required to narrow it down are fundamental drivers of the cost of creating products - it turns out that getting a shitty answer quickly and cheaply is worth more than getting the right answer slowly. reply adastra22 12 minutes agorootparentAlphaFold doesn’t work for engineering though. Getting a shitty answer ends up being worse than useless. It seems to really accelerate productivity of researchers investigating bio molecules or molecules very similar to existing bio molecules. But not de novo stuff. reply flobosg 57 minutes agorootparentprevAlphaFold and Folding@home attempt to solve related, but essentially different, problems. As I already mentioned here, protein structure prediction is not fully equivalent to protein folding. reply pama 1 hour agoparentprevAgreed. There are too many different directions of impact to point out explicitly, so I'll give a short vignette on one of the most immediate impacts, which was the use in protein crystallography. Many aspiring crystallographers correctly reorganized their careers following AlphaFold2, and everyone else started using it for molecular replacement as a way to solve the phase problem in crystallography; the models from AF2 allowed people to resolve new crystal structures from data measured years prior to the AF2 release. reply flobosg 1 hour agorootparentSame with Rosetta, and even Foldit[1]! – https://www.nature.com/articles/nsmb.2119 [1]: https://en.wikipedia.org/wiki/Foldit reply JangoSteve 17 minutes agoparentprevInterestingly, the award was specifically for the impact of AlphaFold2 that won CASP 14 in 2020 using their EvoFormer architecture evolved from the Transformer, and not for AlphaFold that won CASP 13 in 2018 with a collection of ML models each separately trained, and which despite winning, performed at a much lower level than AlphaFold2 would perform two years later. reply divbzero 1 hour agoparentprevI agree that it’s not premature, for two reasons: First, it’s been 6 years since AlphaFold first won CASP in 2018. This is not far from the 8 years it took from CRISPR’s first paper in 2012 to its Nobel Prize in 2020. Second, AlphaFold is only half the prize. The other half is awarded for David Baker’s work since the 1990s on Rosetta and RoseTTAFold. reply mhrmsn 3 hours agoparentprevCrispr is widely used and there are even therapies approved based on it, you can actually buy TVs that use quantum dots and click chemistry has lots of applications (bioconjugation etc.), but I don't think we have seen that impact from AlphaFold yet. There's a lot of pharma companies and drug design startups that are actively trying to apply these methods, but I think the jury is still out for the impact it will finally have. reply nextos 2 hours agorootparentAlphaFold is excellent engineering, but I struggle calling this a breakthrough in science. Take T cell receptor (TCR) proteins, which are produced pseudo-randomly by somatic recombination, yielding an enormous diversity. AlphaFold's predictions for those are not useful. A breakthrough in folding would have produced rules that are universal. What was produced instead is a really good regressor in the space of proteins where some known training examples are closeby. If I was the Nobel Committee, I would have waited a bit to see if this issue aged well. Also, in terms of giving credit, I think those who invented pairwise and multiple alignment dynamic programming algorithms deserved some recognition. AlphaFold built on top of those. They are the cornerstone of the entire field of biological sequence analysis. Interestingly, ESM was trained on raw sequences, not on multiple alignments. And while it performed worse, it generalizes better to unseen proteins like TCRs. reply flobosg 2 hours agorootparent> A breakthrough in folding would have produced folding rules that are universal. Protein folding ≠ protein structure prediction > I think those who invented pairwise and multiple alignment dynamic programming algorithms deserved some recognition I would add BLAST as well but that ship has sailed, I’m afraid. reply causal 2 hours agoparentprevI agree. For those not in biotech, protein folding has been the holy grail for a long time, and AlphaFold represents a huge leap forward. Not unlike trying to find a way to reduce NP to P in CS. A leap forward there would be huge, even if it came short of a complete solution. reply flobosg 2 hours agorootparent> Let me get the most important question out of the way: is AlphaFold’s advance really significant, or is it more of the same? I would characterize their advance as roughly two CASPs in one ―https://moalquraishi.wordpress.com/2018/12/09/alphafold-casp... reply singularity2001 6 hours agoparentprev>> I do think AlphaFold already has reached sufficient level of impact. how so? reply eig 5 hours agorootparentWell I'm sure one could look at number of published papers etc, but that metric is a lot to do with hype and I see it as a lagging indicator. A better one is seeing my grad-school friends with zero background in comp-sci or math, presenting their cell-biology results with AlphaFold in conferences and at lab meetings. They are not protein folding people either- just molecular biologists trying to present more evidence of docking partners, functional groups in their pathway of interest. It reminds me of when Crispr came out. There were ways to edit DNA before Crispr, but its was tough to do right and required specialized knowledge. After Crispr came out, even non-specialists like me in tangential fields could get started. reply tananan 4 hours agorootparentIn both academic and industrial settings, I've seen an initial spark of hope about AlphaFold's utility being replaced with a resignation that it's cool, but not really useful. Yet in both settings it continued as a playing card for generating interest. There's an on-point blog-post \"AI and Biology\" (https://www.science.org/content/blog-post/ai-and-biology) which illustrates why AlphaFold's real breakthrough is not super actionable for creating further bio-medicinal applications in a similar vein. reply whimsicalism 3 hours agorootparentThat article explains why AI might not work so well further down the line biology discoveries, but I still think alphafold can really help with the development of small molecule therapies that bind to particular known targets and not to others, etc. reply tananan 3 hours agorootparentThe thing with available ligand + protein recorded structures is that they are much, much more sparse than available protein structures themselves (which are already kinda sparse, but good enough to allow AlphaFold). Some of the commonly-used datasets for benchmarking structure-based affinity models are so biased you can get a decent AUC by only looking at the target or ligand in isolation (lol). Docking ligands doesn't make for particularly great structures, and snapshot structures really miss out on the important dynamics. So it's hard for me to imagine how alphafold can help with small molecule development (alphafold2 doesn't even know what small molecules are). I agree it totally sounds plausible in principle, I've been in a team where such an idea was pushed before it flopped, but in practice I feel there's much less use to extract from there than one might think. EDIT: To not be so purely negative: I'm sure real use can be found in tinkering with AlphaFold. But I really don't think it has or will become a big deal in small drug discovery workflows. My PoV is at least somewhat educated on the matter, but of course it does not reflect the breadth of what people are doing out there. reply adastra22 4 hours agorootparentprevBut Crispr actually edited genes. How much of this theoretical work was real, and how much was slop? Did the grad students actually achieve confirmation of their conformational predictions? reply eig 3 hours agorootparentSurprisingly, yes the predicted structures from AlphaFold had functional groups that fit with experimental data of binding partners and homologues. While I don't know whether it matched with the actual crystallization, it did match with those orthogonal experiments (these were cell biology, genetics, and molecular biology labs, not protein structure labs, so they didn't try to actually crystalize the proteins themselves). reply cmavvv 5 hours agorootparentprevhttps://www.pnas.org/doi/10.1073/pnas.2315002121 reply fedeb95 4 hours agoparentprevwhat has changed? I think people need more from a comment than blind trust. reply dekhn 3 hours agorootparentIt solidly answered the question: \"Is evolutionary sequence relationship and structure data sufficient to predict a large fraction of the structures that proteins adopt\". the answer, surprising few, is that the data we have indeed can be used to make general predictions (even outside of the training classes), and also surprising many, that we can do so with a minimum of evolutionary sequence data. That people are arguing about the finer details of what it gets wrong is support for its value, not a detriment. reply timr 2 hours agorootparentThat's a bit like saying that the invention of the airplane proved that animals can fly, when birds are swooping around your head. I mean, sure, prior to alphafold, the notion that sequence / structure relationship was \"sufficient to predict\" protein structure was merely a very confident theory that was used to regularly make the most reliable kind of structure predictions via homology modeling (it was also core to Rosetta, of course). Now it is a very confident theory that is used to make a slightly larger subset of predictions via a totally different method, but still fails at the ones we don't know about. Vive la change! reply dekhn 1 hour agorootparentI think an important detail here is that Rosetta did something beyond traditional homology models- it basically shrank the size of the alignments to small (n=7 or so?) sequences and used just tiny fragments from the PDB, assembled together with other fragments. That's sort of fundamentally distinct from homology modelling which tends to focus on much larger sequences. reply flobosg 1 hour agorootparent> and used just tiny fragments from the PDB 3-mers and 9-mers, if I recall correctly. The fragment-based approach helped immensely with cutting down the conformational search space. The secondary structure of those fragments was enough to make educated guesses of the protein backbone’s, at a time where ab initio force field predictions struggled with it. reply timr 50 minutes agorootparentYes, Rosetta did monte carlo substitution of 9-mers, followed by a refinement phase with 3-mers. Plus a bunch of other stuff to generate more specific backbone \"moves\" in weird circumstances. In order to create those fragment libraries, there was a step involving generation of multiple-sequence alignments, pruning the alignments, etc. Rosetta used sequence homology to generate structure. This wasn't a wild, untested theory. reply dekhn 18 minutes agorootparentI don't know that I agree that fragment libraries use sequence homology. From my understanding of it, homology implies an actual evolutionary relationship. Wheras fragment libraries instead are agnostic and instead seem to be based on the idea that short fragments of non-related proteins can match up in sequence and structure space. Nobody looks at 3-mers and 9-mers in homology modelling; it's typically well over 25 amino acids long, and there is usually a plausible whole-domain (in the SCOP terminology). But, the protein field has always played loose with the term \"homology\". reply flobosg 35 minutes agorootparentprev> Rosetta used sequence homology Rosetta used remote sequence homology to generate the MSAs and find template fragments, which at the time was innovative. A similar strategy is employed for AlphaFold’s MSAs containing the evolutionary couplings. reply ThePhysicist 7 hours agoprevDemis Hasabis has a really interesting and unusual CV for a nobel laureate [1], he started his career in AI game programming (he worked e.g. on Popoulous II, Syndicate, Theme Park for Bullfrog, and later for Lionhead Studios on Black & White) before doing a PhD in neuroscience, becoming an entrepreneur and starting DeepMind. I would say this is a refreshing and highly uncommon pick for a nobel prize, really cool to see that you don't have to be a university professor anymore to do this kind of impactful research. 1: https://en.wikipedia.org/wiki/Demis_Hassabis reply mk_stjames 2 hours agoparentI'm always interested in hearing about these people who go and get a PhD in an unrelated field to their original studies, often years after leaving university and working in an industry. Here it says Hasabis did an undergraduate degree in a computer science program, and them spent a decade working on computer games at studios, and then somehow just rocked up to a university and asked to do a PhD in neuroscience. I feel if I tried to do that in the US- (where I got a masters degree in engineering, spent a 15 yrs as an aerospace engineer,)- tried to go back and ask to do a PhD in, say, Physics - I'd be promptly told to go fuck myself (or, fuck myself but then enroll in a new undergrad or maaaybe graduate program only after re-taking GRE's. Straight PhD? Never heard it work like that.) reply adastra22 6 minutes agorootparentWhy do you think that? It’s not my experience. At the grad school level they’ll take anyone who can do the work and is interested. Outside experience, even in unrelated fields, is often a plus. Grad students just out of undergrad have no idea how the world works. reply biofox 24 minutes agorootparentprevI have known a several people who made the jump from Computer Science to Biology at graduate school. Usually, it's either via genomics or neuroscience (as in Hassabis' case), where there is a large need for people who can do data crunching or computational modelling. reply triceratops 2 hours agorootparentprevAerospace engineering masters -> Physics PhD doesn't sound like a big leap to me. I don't think that's accurate. reply shnock 1 hour agorootparentprevI think PhD's are generally different enough in Europe vs the US that this might be less surprising upon further research reply lchengify 4 hours agoparentprevDidn't know he worked on Black & White. Black & White was really ahead of it's time for 2001, it did a much better job of having NPC simulations in groups based on how you played as a god. reply nanoxide 1 hour agoparentprevYep. I distinctly remember reading an interview in the German GameStar magazine in '99 or something with him where he talks about his early work with Bullfrog. Over the years I read his name from time to time as he moved towards research. Pretty amazing career. reply seydor 7 hours agoparentprevdoesn't beat that patent clerk guy reply BeetleB 4 hours agorootparentThe patent clerk guy was almost done with his PhD when he became a patent clerk. Not quite comparable. reply huijzer 2 hours agorootparentAre we talking about Einstein? If I remember correctly, according to Walter Isaacson, Einstein managed to get so many good papers out not despite, but because he was not working for an university. It gave him more freedom to reject existing ideas. Also the years I can find on Wikipedia do not seem to support your claim. He started as a clerk in 1903, and had his miracle year and submitted his PhD dissertation in 1905. reply theGnuMe 3 hours agorootparentprevUh. Demis finished his PhD... reply singularity2001 6 hours agorootparentprevwhose father was one of the leading local industrialists (installing the first electric lighting for the 1885 Munich Oktoberfest) reply nextworddev 5 hours agorootparentIt’s fascinating how the affluent backgrounds of many famous scientists and entrepreneurs are downplayed. Eg Warren Buffet, Jeff Bezos, etc reply chongli 3 hours agorootparentBecause having some degree of runway is almost always necessary but never sufficient. Thousands of Americans receive similar amounts of money from their parents in the form of inheritance of the family home and other major assets. Only one took windfall of that size and created Amazon. reply DevX101 4 hours agorootparentprevIf you gave the $300,000 Bezos got from his father to 10,000 random Americans in 1994, none of them would have created a company the equivalent of Amazon's scale. reply stonemetal12 4 hours agorootparentHow many of those 10k would have the same background? We can pretend Bezos' dad raised him in a \"normal\" middle class background then randomly dropped 300K on him, or we can acknowledge he is the business equivalent of an Olympic athlete. reply adastra22 2 minutes agorootparentI think you are agreeing with the poster you are responding to, right? Bezos is the equivalent of an Olympic athlete: a combination of innate talent as well as opportunity. IncreasePosts 17 minutes agorootparentprevMiguel worked at Exxon for 32 years as an engineer and a manager. It's not like he was the CEO or anything close to that. There would literally be hundreds of thousands of people in a similar position to him across the world. Also worth noting that Jeff Bezos was(and I think still is) the youngest person who ever became a senior VP at DE Shaw. That is a position earned by merit alone. reply varelse 3 hours agorootparentprevBezos? Abandoned by his dad who was literally an alcoholic clown and raised by his mom and her 2nd husband Bezos? https://en.wikipedia.org/wiki/Ted_Jorgensen So $300K in 1994 is about $640K. That's nice but about 80th percentile of net worth. It's nice his parents believed in him. How many of your parents would do that for you? I'm sure at 1 in 5 of them have that kind of money because of the distribution here. So the difference here is He was smart, he got lucky, and your parents don't believe in you enough on this front. But compare and contrast Bezos and Musk. Bezos's mid-life crisis is leaving his wife to run around on his yacht banging models. Musk's mid-life crisis is trying to destroy democracy so he and his mom won't have to pay US taxes. Neither one is a role model, but I don't even get the point of the latter. Which brings us back to AlphaFold. The AlphaFold team did something amazing. But also, they had a backer that believed in them. David Baker, for better or worse, didn't achieve what they did and he'd been at it for decades. It's amazing what good backing can achieve. reply itishappy 2 hours agorootparentI assumed the parent post was talking about his adopted father. The man who raised and dropped $250k on Bezos was Miguel. https://en.wikipedia.org/wiki/Miguel_Bezos reply PaulDavisThe1st 4 hours agorootparentprev10k random Americans, sure. 10k random Americans with backgrounds in software and a business idea? Not so clear. You also seem very certain that Amazon's scale is a good thing, overall, which I remain unconvinced of. reply huijzer 2 hours agorootparent> You also seem very certain that Amazon's scale is a good thing, overall, which I remain unconvinced of. What do you find unconvincing about roughly 30 billion in net income and free cashflow in 2023? reply PaulDavisThe1st 1 hour agorootparentThat's one metric, that only reflects Amazon's function as an income generator. I view businesses through other metrics as well, including their impact on society in a variety of different ways. From some of those perspectives, it is not clear to me that Amazon (where I was the 2nd employee) is a net benefit. reply hn_throwaway_99 3 hours agorootparentprevThat may be true, but I don't think that's really the crux of the argument. This article talks about how Amazon was initially funded by Bezos' family members: https://luxurylaunches.com/celebrities/jeff-bezos-parents-in.... The bigger point is that relatively very few parents (like a couple percent maybe?) would be in a position to give their kid $250k to start a new venture, and it's not that surprising that the most financially successful people in the world needed both: intrinsic talent and drive, and a huge amount of support from their birth circumstances. The way I like to put it is that both of the following are true: 1. Bezos is uniquely talented and driven, and his success depended on that 2. Bezos' success also depended on him having an uncommon level of access to capital at a young age. The reason I like to say \"both of these are true\" is that so often today I see \"sides\" that try to argue that only one is true, e.g. libertarian-leaning folks (especially in the US) arguing that everything is a pure meritocracy, and on the other side that these phenomenally successful people just inherited their situation (e.g. \"Elon Musk is only successful because his dad owned an emerald mine\") reply km155 1 hour agorootparentprevhypothesis : it's not per se affluence. it's the culture of the family and social circle. A dollop of $ to have some free time and maybe buy some books would help and might be necessary. imagine a family where youngster is encouraged to work on intellectual problems. where you aren't made fun of for touching nerdy things. or for doing puzzles. where the social circle endorses learning. these things more important than $ in a first world economy. (if third world, yes give me some money please for a book or even just food. and hopefully with time, an internet connected device then the cream will rise they can just watch feynman on YouTube...) that said, it's \"better\" than it used to be. hundreds of years ago most interesting science, etc. was done by the royal class. not because they are smarter (I assume). But they had free time. And, social encouragement perhaps too. bill gates and zuck dropped out of Harvard right? it's not per se Harvard, at least not the graduating bit? being surrounded by other smart people is helpful -- and or people who encourage intellectual endeavors. reply throw310822 5 hours agorootparentprevI would be very happy to be able to multiply my parents'/family's money by whatever factor Bezos, Gates or Musk have multiplied theirs. reply LightBug1 5 hours agorootparentThe multiplier would be significantly different. Think the Wright Brothers first aeroplane vs a rocket ship. reply daedrdev 4 hours agorootparentThere were at least hundreds of thousands if not millions who had easier starts them Jeff Besos. reply timmg 3 hours agorootparentprevThere’s another side to this: if you accept the idea of “nature” — genes capable of carrying “talent” (in some sense) — it should be common for children of talented people to be talented. Of course, talent doesn’t always mean prosperity. But in a society modeled on meritocracy, it often will. reply xdavidliu 5 hours agorootparentprevBill Gates, but (according to the Isaacson book) not Elon Musk reply medo_baayou 4 hours agoparentprevdon't forget that he was IM 2300 rated chess player at 13 yo reply BeetleB 4 hours agoparentprevSyndicate - wow. That brings back memories! I could never get it working on DosBox (some timing issue). Haven't tried in over a decade, though. Should see if I can get it working. reply belter 7 hours agoparentprevThe infinite polygons are Nobel worth.. reply theGnuMe 6 hours agoparentprevYes but he could have been one... He really took to AI and reignited the fire that melted the AI winter. reply dekhn 4 hours agoprevI wasn't expecting to see David Baker in the list (just Demis and John). But I'm really glad to see it... David is a great guy. At CASP (the biannual protein structure prediction competition) around 2000, I sat down with David and told him that eventually machine learning would supplant humans at structure prediction (at the time Rosetta was already the leading structure prediction/design tool, but was filled with a bunch of ad-hoc hand-coded features and optimizers). he chuckled and said he doubted it, every time he updated the Rosetta model with newer PDB structures, the predictions got worse. I will say that the Nobel committee needs to stop saying \"protein folding\" when they mean \"protein structure prediction\". reply throwawaymaths 3 hours agoparentRefreshingly good. Bakers \"early\" (not really early, but earlier than AlphaFold) work (having humans with no background solve folds) really laid the groundwork to proving that heuristic methods were likely to outperform physical forcefield and ab initio/DFT methods for structure prediction. And AI structure prediction if nothing else is heuristic protein folding. reply jboggan 4 hours agoparentprevThe models and tools designed for the CASP competition were an example of running around the solution space at a glacial pace and getting stuck in local minima. I can't speak for Rosetta by my labmates had fairly successful tools that usually ranked right behind Baker's lab, and they were plagued by issues where the most successful models had impossible or idiosyncratic terms in them. For example, a very successful folding model had the signs reversed on hydrophobic and some electrostatic interactions. It made no sense physically but it gave a better prediction than competing models, and it was hard to move away from because it ranked well in CASP. reply dekhn 4 hours agorootparentWhich model had the signs reversed? Yes, CASP was prone to getting stuck in local minima. I think the whole structure prediction field had become moribund. reply siver_john 3 hours agoparentprevThey had to put David Baker on here, his work on protein design if nothing else was ground breaking. I've expected him to win it at some point in a, it's not a matter of if but of when. reply aithrowawaycomm 6 hours agoprevI think putting AlphaFold here was premature; it might not age well. AlphaFold is an impressive achievement but it simply has not \"cracked the code for protein folding\" - about 1/3rd of its predictions are too uncertain to be usable, it says nothing about dynamics, suffers from the same ML problems of failing on uncommon structures, and I was surprised to learn that many of its predictions are incorrect because it ignores topological constraints[1]. To be clear, these are constructive criticisms of AlphaFold in isolation, my grumpiness is directed at the Nobel committee. \"Cracked the code for protein folding\" is simply not true; it is an ML approach with high accuracy that suffers the same ML limitations of failing to generalize or failing to understand deeper principles like R^3 topology that cannot be gleaned stochastically. More significantly: it has yet to be especially impactful in biochemistry research, nor has its results really been carefully audited. Maybe it will turn out to deserve the prize. But the committee needed to wait. I am concerned that they got spun by Google's PR campaign - or, considering yesterday's prize, Big Tech PR in general. [1] https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10672856/ reply InkCanon 6 hours agoparentI think looking back five years from now, this will be viewed as another Kissinger/Obama but wrt STEM. Given far too prematurely under pressure to keep up with the Joneses/chase the hype. reply aithrowawaycomm 5 hours agorootparentI am not so confident or dismissive: the real problem is that testing millions of predictions (or any fairly bold scientific development like AlphaFols) takes time, and that time simply has not elapsed. Some of the criticisms I identified might be low-hanging fruit that in 5 years will be seen as minor corrections - but we're still discovering the things that need to be corrected. It is concerning that the prize announcement itself is either grossly overstated: With its help, they have been able to predict the structure of virtually all the 200 million proteins that researchers have identified [the word 'virtually' is stretched into meaninglessness] or vague, could have been done with other tools, and hardly Nobel-worthy: Among a myriad of scientific applications, researchers can now better understand antibiotic resistance and create images of enzymes that can decompose plastic. I am seriously wondering if they took Google / DeepMind press releases at face value. reply refulgentis 35 minutes agorootparentChew on this a little, I stripped out as much as possible, but I imagine it still will feel reflexively easy to dismiss. Partially because its hard to hear criticism, at least for me. Partially because a lot was stripped out: a lot has gone sideways to get us to this point, so this may sound minor. The fact you have to reach for \"I [wonder if the votes were based on] Google / DeepMind press releases [taken] at face value.\" should be a red blaring alarm. It creates a new premise[1] that enables continued permission to seek confirmation bias. I was once told you should check your premises when facing an unexpected conclusion, and to do that before creating new ones. I strive to. [1] All Nobel Prize voters choose their support based on reading a press release at face value reply InkCanon 3 hours agorootparentprevI have the same views as you (although admittedly the Kissinger comparison didn't convey that, because we all know how that turned out). It's at best quite premature. At worst, should never have been given in hindsight. Will probably land somewhere in between. Second point is spot on. I really, really hope they didn't just fall for what is frankly a bit of SV style press release meant to hype things. Similar work was done on crystal structures with some massive number reported. It's a vastly other thing than the implied meaning that they are now fully understood and able to be used in some way. reply matsemann 4 hours agorootparentprevOriginal intent of the prizes is, however, to reward those that recently contributed good. Not to be a lifetime award after seeing how things pan out. reply InkCanon 3 hours agorootparentYes, but the good has to be extraordinary. If there's logic to it, in these cases they are predicting the good will come much later. Which is an incredibly difficult prediction to make. reply j7ake 5 hours agoparentprevI would say alphafold is to structure prediction as crispr is to gene editing. Crispr did not solve gene editing either, but has been made accessible to the broad biochemistry and biology researchers to use. Both similar impact and changed the field significantly. reply beanjuice 4 hours agorootparentEntire fields are based upon the existence of crispr now, it demonstrated its impact. It has been 2? 3? years, people who were making papers anyway have implemented AlphaFold, it hasn't exactly spawned a new area. reply paulwetzel 8 hours agoprevWhile I am skeptical about yesterdays award in physics, these are totally deserved and spot on. There are few approaches that will accelerate the field of drug development and chemistry as a whole in a way that the works of these three people will. Congratulations! reply trott 7 hours agoparent> There are few approaches that will accelerate the field of drug development and chemistry as a whole in a way that the works of these three people will. As the author of one such approach, I'm skeptical. AlphaFold 2 just predicts protein structures. The thing about proteins is that they are often related to each other. If you are trying to predict the structure of a naturally occurring protein, chances are that there are related ones in the dataset of known 3D structures. This makes it much easier for ML. You are (roughly speaking) training on the test set. However, for drug design, which is what AlphaFold 3 targets, you need to do well on actually novel inputs. It's a completely different use case. More here: https://olegtrott.substack.com/p/are-alphafolds-new-results-... reply jhbadger 5 hours agorootparentProtein structures are similar to each other because of evolution (protein families exist because of shared ancestry of protein coding genes). It's not a weird coincidence that helps ML; it's inherent in the problem. Same with drug design -- very, very, few drugs are \"novel\" as opposed to being analogues of something naturally in the body. reply svara 14 minutes agorootparentThey're referring to the structure of the protein when a drug is bound, that's what's novel. Novel as in, you can't think of it as \"just\" interpolation between known structures of evolutionarily related proteins. That said I'm not sure that's entirely fair, since Alphafold does, as far as I know, work for predicting structures that are far away from structures that have previously been measured. You're quite wrong about small molecule drug structures. Historically that has been the case but these days many lead structures are made by combinatorial chemistry and are not derived from natural products. reply jhbadger 0 minutes agorootparentBut even drugs made by combinatorial chemistry still generally end up being analogues of natural products even if they aren't derived from them. As Leslie Orgel said \"Evolution is cleverer than you are\"; chemists are unlikely to discover a mechanism of action that millions of years of evolution hasn't already found. ackbar03 8 hours agoparentprevI was just wondering when they were going to award the alphafold2 guys the nobel after after seeing Hinton win the physics one. 100% agree, all three of them totally deserve this one. Baker's lab is pretty much keeping Deepmind in check at this point and ensuring open source research is keeping up. Hats off reply theGnuMe 6 hours agorootparentBaker has been in the protein folding game for a long time and was the leader before Alphafold came in... His generative paper came out what last year (2023)? I mean this is a fast award cycle. reply divbzero 1 hour agorootparentDavid Baker’s RoseTTAFold was first released in 2021. [1]: https://www.science.org/doi/10.1126/science.abj8754 [2]: https://cen.acs.org/analytical-chemistry/structural-biology/... reply cowsandmilk 8 hours agoparentprevBoth Rosetta and DeepMind have made contributions outside of protein structure prediction that are far more important for drug discovery. reply mihaaly 7 hours agoparentprevThe physics prize should have went to Elon Musk! Also I really hope the Nobel Prize of Economics goes to Bill Gates! He facilitated sooo much advances by releasing Excel that this must be recognized! And based on this year's announcements so far I am not sure that my sarcastic comments should be taken as a joke! reply theGnuMe 6 hours agorootparentExcept Excel has introduced way to many bugs and how many people has it killed? reply tomp 8 hours agoparentprevAre they? What did Demis do? reply seydor 7 hours agorootparentdidn't he lead early successes in RL which popularized it and culminated in protein prediction? reply onursurme 7 hours agorootparentprevHe writes software in different areas, so he has the potential to get a Nobel prize in any area soon. reply world2vec 8 hours agorootparentprevHe's founder and CEO of the AI lab that build Alphafold? reply devilzhong 8 hours agorootparentThen maybe Sergey and Larry should also get the prize since they founded Google, which owns Deepmind? reply world2vec 8 hours agorootparentThey were not equal contributors to the seminal paper that got the prize. From another post in this thread: \"These authors contributed equally: John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, Alex Bridgland, Clemens Meyer, Simon A. A. Kohl, Andrew J. Ballard, Andrew Cowie, Bernardino Romera-Paredes, Stanislav Nikolov, Rishub Jain, Demis Hassabis\" reply theGnuMe 6 hours agorootparentprevThey bought it and it runs autonomously (or did mostly) reply sega_sai 6 hours agoprevPhysics prize one goes to AI, chemistry too. What next? Nobel in Literature goes to ChatGPT? Jokes aside, I think the chemistry prize seems to make a bit more sense to me than physics one. reply theGnuMe 2 hours agoparentI think it is definitely possible for ChatGPT to win the Nobel Prize in Literature.. maybe not this version or the next but eventually -- especially if it is by proxy aka as is the premise in \"The Wife\" (a good book/movie btw). There's already precedent for anonymous creators, aka Banksy. reply boxed 7 hours agoprevI think it's still too early to know if AlphaFold is a massively overfitted statistical model that will utterly fail on novel structures. reply patall 6 hours agoparentThe prize winners are ultimately selected by a group of mid-age to old professors. And to tell the truth (I work at a research institute in Stockholm), some of the old folks seem to have huge FOMO. They know that they cannot keep up themselves, they have no idea (and no way of finding out) who is actually good and who is just pretending, which leads to recruitment of an 'interesting' bunch of young group leaders. Some of them are surely good, but I know of at least one guy who holds presentations as if he invented AlphaFold himself, while having contributed one single paper of interest to the field. Large turn off for me. reply theGnuMe 3 hours agorootparentI dunno, you should have AI FOMO. Or at least start focusing on computational thinking. reply patall 1 hour agorootparentI do not criticize them for having FOMO. But I have my doubts when it is the 60-year-olds that are the most enthusiastic about something new (as long as it is not a new ABBA album), given the number of grifters out there. And there would have been many others that also deserve a Nobel, those three could easily have waited another 20 years. If it really was those that had the highest impact the last year who won the prize, it (or rather \"Medicine\") should have gone to GLP-1/Semaglutide research. reply flobosg 3 hours agoparentprevAlphaFold2 has correctly predicted novel folds: https://www.nature.com/articles/s42003-022-03357-1 reply YeGoblynQueenne 5 hours agoprevLooks like science is out, black box prediction is in. It's like the era of epicycles all over again. Oh well. Fellow realists, see you all 1500 years from now! reply pelorat 4 hours agoparentThere are things so complex in science that a human mind can never understand them, but a large neural network can. reply hilux 43 minutes agoprevI'm not always a Lex Fridman fan, but the interview with Demis is well worth a listen. reply og_kalu 7 hours agoprevYesterday's physics win was rather odd but this I have no problem with! Lol does this mean there's a chance the Transformer Authors win a Nobel in literature sometime? Certainly seems a lot more plausible than before yesterday. reply onursurme 8 hours agoprevCombine this with the Physics prize, I now have hope to receive a Nobel prize in any area. Seriously, from now on, I won't mention Nobel prizes anywhere anymore. reply uptownfunk 5 hours agoprevTo me this is more controversial than Geoffrey Hinton winning it for physics. reply ocular-rockular 3 hours agoparentThey should've given this prize to Hinton too, for making the machine learning of Alphafold possible. reply gilleain 8 hours agoprevDavid Baker (and colleagues) have always done good work. I guess google have done some things also. (lol - one of the PDF attachments to that page is 'Illustration: A string of amino acids' : actually it's a bit better than the title implies :). Actually, Figure 2 - \"How does AlfaFold2 Work?\" is impressive to fit that on one page. Nice. reply exmadscientist 7 hours agoparentIt is well known around here that Baker does very, very little of the work. He is extremely good at putting his name on his students' work though (this is par for the course in academia)... and removing theirs (this is the bad part). At least he bribes them with lots of happy hours! reply stanford_labrat 3 hours agorootparentusername...checks out reply bawolff 3 hours agoprevAI is cleaning up at the nobel prizes! reply optimalsolver 2 hours agoparentThis is what extreme FOMO looks like. reply joelthelion 8 hours agoprevDid Demis Hasabis actually do any scientific work himself? reply onursurme 7 hours agoparentHe is a chemist now. reply seydor 7 hours agoparentprevYes in neuroscience. But i guess this one is recognition for leading efforts in RL which culminated in this. reply FL33TW00D 8 hours agoparentprevI mean he took the risk and founded DeepMind? reply belter 7 hours agorootparentSo then Mustafa Suleyman and Shane Legg will want a Nobel also? reply muenalan 2 hours agoprevSurely, it is helpful to consider the achievement in terms of the contest setup to detect a Nobel-worthy breakthrough: https://en.m.wikipedia.org/wiki/CASP It moved the needle so much in terms of baseline capability. Let alone Nobel’s original request: positive impact to humanity; well deserved. In biology/medicine it is still awed like coming from a different planet; tech before was obviously that lacking. reply bdjsiqoocwk 6 hours agoprevAnyone else think that giving a nobel prize to the CEO of the company is absurd? reply ilaksh 6 hours agoprevFor some reason this web page doesn't render properly in my browser. The main text overlaps the table of contents. I'm using Chrome on KDE (Ubuntu) on a 1920 wide display (minus the side panel). I checked and I don't have the page zoomed. https://i.imgur.com/fOOQ3Av.png reply sikimiki 8 hours agoprevWell deserved! Especially for Alphafold. It is the most impactful invention in structural biology this century along with Cryo-EM. reply onursurme 7 hours agoparentIt doesn't deserve a Chemistry Nobel prize. It deserves a prize about computers. reply Mklomoto 3 hours agorootparentSo the Nobel commite was wrong to decide this because you think otherwise? Interesting. Any more indepth analys about this? Btw. you don't just build AlphaFold by doing only 'computers'. Take a look at any good docmentary about it and you will see that they do discuss chemistry on a deep level reply seydor 7 hours agorootparentprevphysiology&medicine more likely reply seydor 8 hours agoprev... and the Nobel in Economics goes to OpenAI for innovations in nonprofit business structures. it's the year of AI (ChatGPT preparing its acceptance speech) reply thatsadude 7 hours agoprevWell more deserving than the author of Restricted Boltzmann machine. reply KasianFranks 8 hours agoprevAnd, at the heart of AlphaFold2 is the language model, the tip of the spear in AI today. 'Language' can come in many forms e.g. a protein or amino acid sequence. reply ddalex 8 hours agoparentAlpha* is not LLM-based, it's Q-learning based reply jebarker 7 hours agorootparentAlphaFold 2 wasn't Q-learning based. It was supervised SGD and the \"evoformer\" they introduced is very close to a transformer. So it's not exactly an LLM, but it's a pretty close equivalent for protein data. reply cool-RR 6 hours agoprevI wonder why various outlets, including DeepMind's blog, say that John Jumper is a \"Senior Research Scientist\". That's L5 which sounds like quite a low rank for a Nobel prize winner. I checked his LinkedIn and he's a director, which is around L8. I thought that maybe he was L5 during the publishing of the results, but no, he was either L6 or L7. reply tsimionescu 1 hour agoparentL5 doesn't mean anything to anyone outside of whatever organization you're talking about (Google?). A Senior Research Scientist means \"a person who is a scientist, works in research, and is very experienced in that role\". Even if this is not the title he holds in his organization, it is an objective title that applies to him. reply jltsiren 6 hours agoparentprevMaybe \"Senior Research Scientist\" sounds more respectable among the intended audience. A research scientist is usually an independent researcher rather than someone working in another person's team, while \"senior\" indicates that they have been in an independent role for a while. A director, on the other hand, is someone who failed to avoid administrative duties, and it doesn't imply any degree of seniority. reply hanjeanwat 4 hours agoprevexcellent longread on the development of AlphaFold - with interviews with Baker + Jumper and more: https://www.quantamagazine.org/how-ai-revolutionized-protein... reply lr1970 8 hours agoprevHow come they missed Justin Gilmer? He did most of the original work [0]. [0] https://scholar.google.com/citations?view_op=view_citation&h... EDIT: typos reply jboggan 4 hours agoprevCongrats to David Baker and his lab! reply photochemsyn 4 hours agoprevAlphaFold is a useful tool but it's unsatisfying from a physical chemistry perspective. It doesn't give much if any insight in to the mechanisms of folding, and is of very limited value in designing novel proteins with industrial applications, and in protein prediction for membrane-spanning proteins, extremophilic microbe proteins, etc. Thus things like folding kinetics of transition states and intermediates, remain poorly understood through such statistical models, because they do not explicitly incorporate physical laws governing the protein system, such as electrostatic interactions, solvation effects, or entropy-driven conformational changes. In particular, environmental effects are neglected - there's no modeling of the native solvated environment, where water molecules, ions, and temperature directly affect the protein’s conformational stability. This is critical when it comes to designing a novel protein with catalytic activity that's stable under conditions like high salt, high temperature etc. As far as Nobel Prizes, it was already understood in the field two decades ago that no single person or small group was going to have an Einstein moment and 'solve protein folding', it's just too complicated. This award is questionable and the marketing effort involved by the relevant actors has been rather misleading - for one of the worst examples of this see: https://www.scientificamerican.com/article/one-of-the-bigges... For a more judicious explanation of why the claim that protein folding has been solved isn't really true: \"The power and pitfalls of AlphaFold2 for structure prediction beyond rigid globular proteins\" (June 2024) https://www.nature.com/articles/s41589-024-01638-w reply pelorat 4 hours agoparentThe mechanisms of folding is most likely impossible for a human mind to comprehend. reply LarsDu88 7 hours agoprevNobel committee is all in on the AI hype this year! reply balazstorok 7 hours agoprevNobel peace prize has countless times been awarded to a group of people or institution. It is differently controlled but the idea is not unprecedented. reply ascorbic 6 hours agoparentThe rules are different reply dist-epoch 7 hours agoprevCalling it, to keep with the theme: the Nobel prize in literature will be given to a LLM author. reply rswail 8 hours agoprevThe physics prize was a stretch of the definition of the word \"physics\", but this one is purely about chemistry. reply onursurme 7 hours agoparentNot purely. It includes a software development about Chemistry. reply seydor 7 hours agorootparentevery science includes software development for a few decades now reply proto-n 8 hours agoprevNow this one makes more sense. Chemistry Nobel for advancing chemistry using AI. Contrast that with Phyics Nobel for advancing AI using physics. reply Muller20 8 hours agoparentAlphaFold is also a high impact discovery, while Hopfield networks have very little to do with modern AI and they are only a very interesting toy model right now. reply ConcernedCoder 6 hours agoprevpeople still folding@homereply haunter 8 hours agoprevinb4 Nobel Peace Prize will go to Timnit Gebru reply gizajob 8 hours agoprevWhoa. Happy for Demis today. Amazing achievement. reply lennertjansen 8 hours agoprevwell deserved reply aborsy 8 hours agoprevThe AlphaFold paper has countless authors, many researchers and company resources underlying it. Hassabis’ contribution is management of resources and entrepreneurship, not the actual science. There are hundreds of thousands of scientists out there doing deep technical work, and they aren’t recognized. I think we might be the end of it, as the emphasis shifts to commercialization and product development. These AI demonstrations require so many GPUs, specialized hardware and data that nobody has but the biggest players. Moreover, they are engineering work, not really scientistic (putting together a lot of hacks and tweaks). Meanwhile, the person who led the transformer paper (a key ingredient in LLMs) hasn’t been recognized. This will incentivize scientists to focus on management of other researchers who will manage other researchers who will produce the technical inventions. The same issue arises with citations and indices, and the general reward structure in academia. The signal these AI events convey to me: You better focus on practical stuff, and you better move on in the management ladder. reply mhandley 8 hours agoparentThe Nobel prize cannot go to a team so they have to pick individuals. This is true for many (most?) nobel prize awards. Consider for example the discovery of gravity waves - the team that built and operated LIGO was huge, but they have to pick. This has commonly been the case since the inception of the prizes - the professor gets the prize, the PhD students and postdocs don't usually. Not saying this is right, but it's the way it is. reply elashri 7 hours agorootparentFor gravitational waves discovery, Nobel prize went to the designers of the LIGO which was done long before we actually built it. The example that will fit more your idea would be Carlo Rubbia who got the award in 1984 for leading the CERN team who discovered the W and Z bosons. He did not have any contributions than leading the experiment that did it [1]. It is not like he designed or proposed the way we used to detect them. And the Nobel prize for higgs discovery went to theorists who proposed and predict it not the experimental physicists (thousands) who discovered it in 2012. [1] https://home.cern/science/experiments/ua1 reply erk__ 7 hours agorootparentprevIt is only the Peace Prize that may be given to more than 3 people, which have been done 30 times: https://www.nobelprize.org/prizes/lists/nobel-prize-awarded-... reply nsoonhui 7 hours agoparentprevSo can we expect that Sam Altman will be honored with Nobel prize 2025? After all physics prize went to AI researchers this year, and chemistry prize went to an organizational head. reply patall 7 hours agorootparentHe may still win literature this year. reply pyb 2 hours agorootparentHe's up for the Pulitzer Prize for Fiction. https://x.com/S_OhEigeartaigh/status/1843979139948355893 reply fuzzfactor 6 hours agorootparentprevI can only predict a more prestigious prize on the horizon in the future. Isn't sama on track to end up with more financial resources than Nobel had at his disposal? Plus I think he's got enough of a philanthropic streak which can prove to be not so shabby. There could very well be a foundation someday awarding the Altman Prize well into the 22nd century. Whether or not his most dynamic legacy would be something as simultaneously useful/dangerous as dynamite. reply jsnell 5 hours agorootparentThe Nobel prize's prestige comes from its history, not from the size of the monetary award. For an example, the Millennium Technology Prize is awarded every two years and the prize money is slightly higher than the Nobel prize (1M EUR vs 0.94M EUR). The achievements it's been awarded for tend to be much more practical, immediate and understandable than the Nobel prize achievements. The next one should be awarded in a couple of weeks. And when that happens, it'll get 1/10th the publicity a Nobel prize gets, because the Nobel prize is older than any living human and has been accumulating prestige all that time, while the Millennium prize is only 20 years old. reply patall 6 hours agorootparentprevHassabis should also be a billionaire by now. reply FrustratedMonky 6 hours agorootparentprevYou're really conflating things. Altman is no Hassabis. Just because there is a ton of hype from OpenAI doesn't detract from what DeepMind has done. AlphaGo anybody? Are we really already forgetting what a monumental problem protein folding was, decades of research, and AlphaFold came in and revolutionized it overnight. We are pretty jaded these days when miracles are happening all the time and people are like \"yeah, but he's just a manager 'now', what have they done for me in the last few days\". reply authorfly 6 hours agorootparentI am missing context here and would love to know more. Say I know about ATP Synthase and how the proteins/molecules involved there interact to make a sort of motor. How does AlphaFold help us understand that or more complicated systems? Are proteins quite often dispersed and unique, finding each other to interact with? Or like ATP Synthase are they more of a specific blueprint which tends to arrange in the same way but in different forms? In other words: Situation 1) Are there many ATP synthase type situations we find too complex to understand - regular patterns and regular co-occurences of proteins but we don't understand them? Situation 2) Or is most of the use of Protein situational and one-off? We see proteins only once or twice, very complicated ones, yet they do useful things? I struggle to situate the problem of Unknown proteins without knowing which of the above two is true (and why?) reply pyb 8 hours agoparentprevThe paper has a mention : \"These authors contributed equally: ...\" However, it appears that some the authors were more equal than others. reply falcor84 8 hours agorootparentThe Nobel prize isn't awarded for a paper. Even if (and that's a large if) all of these contributed equally to the results in the paper, some obviously did more than others to prepare the ground for that study. reply abm53 7 hours agoparentprevI did raise an eyebrow at it too, but I doubt his contribution was entirely “management of resources”. I think one must also give him the credit for the vision, risk taking and drive to apply the resources at his disposal, and RL, to these particular problems. Without that push this research would never have been done, but there may have been many fungible people willing to iron out the details (and, to be fair, contribute some important ideas along the way). I’m not a proponent of the “great man” theory of history, but based on the above I can see that this could be fair (although I have no way of telling if internally this is actually how it played out). reply FrustratedMonky 6 hours agorootparentAgree. Hassabis is more than a manager. He did start DeepMind with just a few people and was a big part of the brains behind it. Now that it has grown he might be doing more management. But the groundwork that went into AlphaFold was built on all the earlier Alphaxxx things they have built, and he contributed. It isn't like other big tech managers that just got some new thing dumped in their lap. He did start off building this. reply alkonaut 6 hours agoparentprev> The AlphaFold paper has countless authors, many researchers and company resources underlying it. Hassabis’ contribution is management of resources and entrepreneurship, not the actual science. That's usually how you get a Nobel prize in science. You become an accomplished scientist, and eventually you lead a big lab/department/project and with a massive massiv you work on projects where there are big discoveries. These discoveries aren't possible to attribute to individuals. If you look back through history and try to find how many \"Boss professor leading massive team/project\" vs. how many \"Einstein type making big discovery in their own head\" I think you'll find that the former is a lot more common. > This will incentivize scientists to focus on management of other researchers who will manage other researchers who will produce the technical inventions. I don't think the Nobel prize is a large driver of science. It's a celebration and a way to put a spotlight on something and someone. But I doubt many people choose careers or projects based on \"this might get us the prize...\" reply marcosdumay 6 hours agorootparent> You become an accomplished scientist, and eventually you lead a big lab/department/project and with a massive massiv you work on projects where there are big discoveries. That's a very recent thing. Up to the 90s, the Nobel committee refused to even recognize it. They just started to award those prizes at the 21 century, and on most fields they never became the majority. reply world2vec 8 hours agoparentprevHasn't it always been like that? The lab director gets to receive the prize, not the whole team (which could have hundreds or thousands of people). reply bonoboTP 8 hours agoparentprevThe Nobel prize is aimed at the general public. It has a kind of late 19th century progressive humanistic ethos. It's science outreach. This way, at least once a year, the everyday layperson hears about scientific discoveries. The Nobel isn't a vehicle to recognize hundreds of thousands of deeply technical scientific researchers. How could it be? They have to pick a symbolic figurehead to represent a breakthrough. They could also simply give it to \"DeepMind\" similar to how they give the peace prize to orgs sometimes, or how the Time Person of the Year is sometimes something abstract as well (like the cutesy \"You\" of 2006). But it would be silly. Just deal with it, we can't \"recognize\" hundreds of thousands, and we want to see a personal face, not a logo of a company getting the award. That's how we are, better learn to deal with it. reply dsign 7 hours agorootparent> The Nobel prize is aimed at the general public... Which is okay. The Nobel prize is okay. > This way, at least once a year, the everyday layperson hears about scientific discoveries. Spot on. The problem we have is that the everyday layperson hears very little about scientific discoveries. The scientists themselves, one in a million of them, can get a Nobel prize. The rest, if they are lucky, get a somewhat okay salary. Sometimes better than that of a software engineer. Almost always worse working hours. But I suppose it's all for the best. Imagine a world where a good scientist, one that knows everything about biology and protein folding, gets to avoid cancer and even aging, while the everyday layperson can only go to the doctor... reply adw 4 hours agorootparent> Sometimes better than that of a software engineer There is a reason so many of us work as software engineers now; I earn about 5x more than I would as a university lecturer/assistant professor. reply madmask 7 hours agorootparentprevThat would be a good incentive to become a good scientist reply mrguyorama 5 hours agorootparentprevAt least one American Nobel laureate has had to sell his nobel prize medal to pay for medical costs in their old age. Just insane. reply dist-epoch 7 hours agoparentprev> These AI demonstrations require so many GPUs, specialized hardware and data that nobody has but the biggest players. Moreover, they are engineering work, not really scientistic So is the Large Hadron Collider. reply DevX101 8 hours agoprevHere's a direct quote from the Alphafold paper: \"These authors contributed equally: John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, Alex Bridgland, Clemens Meyer, Simon A. A. Kohl, Andrew J. Ballard, Andrew Cowie, Bernardino Romera-Paredes, Stanislav Nikolov, Rishub Jain, Demis Hassabis\" reply t_mann 8 hours agoparentHere's 8 densely printed pages of contributors to a discovery that lead to a Nobel prize: https://arxiv.org/pdf/1207.7214#page=26 Guess how many of them were included in the prize. It's a shame that the Nobel committee shies away from awarding it to institutions, but the AlphaFold prize doesn't even make the top 10 in a list of most controversial omissions from a Nobel prize. It's a simple case of lab director gets the most credit. reply throwaway48476 7 hours agorootparentPerhaps it's time the prize goes to the discovery itself instead of a person. reply BurningFrog 3 hours agorootparentHow do you give $1M to a discovery? reply ascorbic 6 hours agorootparentprevIt's not a case of shying away from it: the rules of the prize don't allow it. reply myworkinisgood 6 hours agorootparentrules of the prize can be changed. reply afthonos 5 hours agorootparentCan they? I mean, in the sense that you can yolo anything, sure, but the prizes were designed in a time when it was (more) reasonable to award them to individuals, and they are defined in a will. There may not be a mechanism for updating the standards. reply hyperbrainer 3 hours agorootparentYes, they can. In 1901, science was not nearly as collaborative as it is today. Especially considering the need for a Nobel Prize to be experimental and the fact that most major labs today _need_ dozens of people. reply achierius 3 hours agorootparentHe asked _can_ not _should_: what is the legal mechanism for doing so? Personally I don't doubt there is one but I don't think you know it off the top of your head, so I don't see it as fair to disparage OP for not knowing either. reply hyperbrainer 2 hours agorootparentWell, it seems like amendments have been made before. [0] > Before the board ... votes on a proposal to amend the statutes ... with the first paragraph, the prize-awarding bodies shall examine the proposal. https://www.nobelprize.org/organization/special-regulations-... reply afthonos 3 hours agorootparentprevThat…has nothing to do with my question. It was a procedural and legal question, not an abstract moral one. reply hyperbrainer 2 hours agorootparentWell, it seems like amendments have been made before. [0] > Before the board ... votes on a proposal to amend the statutes ... with the first paragraph, the prize-awarding bodies shall examine the proposal. https://www.nobelprize.org/organization/special-regulations-... reply DevX101 7 hours agoparentprevThe nature of scientific work has changed significantly since 1895, when the Nobel Prizes were established. 100 years ago, lots of scientific work really were driven forward largely by a singular person. That's rarely true today for groundbreaking research. I don't know if this means the Nobel needs to change or we need a another prize that reflects the collaborative work of modern science. reply aleph_minus_one 5 hours agorootparent> The nature of scientific work has changed significantly since 1895, when the Nobel Prizes were established. 100 years ago, lots of scientific work really were driven forward largely by a singular person. That's rarely true today for groundbreaking research. The question is: is this a necessity for doing good science today, or rather an artifact of how important research is organized today (i.e. an artifact of the bureacratic and organizational structure that you have to \"accept\"/\"tolerate\" if you want to do want to have a career in science)? reply DevX101 5 hours agorootparentSo I did a mini research project on Claude to answer your question. From 1900-1930, 87% of Nobel Prizes in Physics, Chemistry and Physiology/Medicine were awarded to individual contributions, 13% were awarded to collaborative contributions. This ratio has flipped in the past 30 years, from 1994-2023, where 17% prizes were individual, 83% collaborative. So I'd say yes, collaborative work is increasingly a requirement to do groundbreaking research today. The organizational structures and funding are a part of the reason as you mention. But it's also that modern scientific problems are more complex. I used to have a professor that used to say about biology \"the easy problems have been solved\". While I think that's dismissive to some of the ingenious experiments done in the past, there's some truth to it. reply afthonos 5 hours agorootparentThis begs the question. If all science is now structured as big research teams, we'd expect the breakthroughs to come from such teams. That doesn’t necessarily imply that teams are needed. reply stenl 8 hours agoparentprevAnd further down: ” Contributions J.J. and D.H. led the research. J.J., R.E., A. Pritzel, M.F., O.R., R.B., A. Potapenko, S.A.A.K., B.R.-P., J.A., M.P., T. Berghammer and O.V. developed the neural network architecture and training. T.G., A.Ž., K.T., R.B., A.B., R.E., A.J.B., A.C., S.N., R.J., D.R., M.Z. and S.B. developed the data, analytics and inference systems. D.H., K.K., P.K., C.M. and E.C. managed the research. T.G. led the technical platform. P.K., A.W.S., K.K., O.V., D.S., S.P. and T. Back contributed technical advice and ideas. M.S. created the BFD genomics database and provided technical assistance on HHBlits. D.H., R.E., A.W.S. and K.K. conceived the AlphaFold project. J.J., R.E. and A.W.S. conceived the end-to-end approach. J.J., A. Pritzel, O.R., A. Potapenko, R.E., M.F., T.G., K.T., C.M. and D.H. wrote the paper.” reply stephencanon 7 hours agorootparent> J.J. and D.H. led the research Hey, I wonder who these mysterious \"J.J.\" and \"D.H.\" might be? reply onursurme 7 hours agorootparentprevThese people aren't in the \"These authors contributed equally\" list. reply seydor 8 hours agoparentprevThey didnt give the award to the paper reply onursurme 8 hours agorootparentThe paper represents the work which lead to the prize. if A=B and A=C, then A=C reply SkiFire13 7 hours agorootparent> if A=B and A=C, then A=C Technically true, but you might still want to double check your logic. reply fastball 7 hours agorootparentprevNot all of the work. For example, it doesn't account for the fact that Demis Hassabis, as head of DeepMind, undoubtedly recruited many of the co-authors to participate in this effort, which is worth something when it comes to the final output. reply onursurme 7 hours agorootparentTo recruit isn't scientific work. reply warkdarrior 4 hours agorootparentprevWhat will the paper do with the prize money? reply Rochus 6 hours agorootparentprevAt least they could have read it before awarding the price. reply mhrmsn 7 hours agoprevGreat achievement, although I think it's interesting that this Nobel prize was awarded so early, with \"the greatest benefit on mankind\" still outstanding. Are there already any clinically approved drugs based on AI out there I might have missed? In comparison, the one for lithium batteries was awarded in 2019, over 30 years after the original research, when probably more than half of the world's population already used them on a daily basis. reply pm215 6 hours agoparentArguably awarding early is more in line with the intention expressed in Nobel's will: \"to those who, during the preceding year, have conferred the greatest benefit to humankind\". It seems to have drifted into \"who did something decades ago that we're now confident enough in the global significance of to award a prize\". I suspect that if the work the prize recognized reslly had to have been carried out in the preceding year the recipients would be rather different. reply benrapscallion 3 hours agorootparentShouldn’t this be GLP1RAs (semaglutide etc.) for the last year? reply cmavvv 5 hours agoparentprevGiven that drugs take around 10 years to get to market, and that some time is needed for industrial adoption as well, it's not very reasonable to expect clinically approved drugs before a few years. reply londons_explore 3 hours agorootparent> around 10 years to get to market This is really sad. A new recipe for feeding honeybees to make tastier honey could get to market in perhaps a month or two. All the chemical reactions happening in the bees gut and all the chemicals in the resulting honey are unknown, yet within a matter of weeks its being eaten. Yet if we find a new way of combining chemicals to cure cancer, it takes a decade before most can benefit. I feel like we don't balance our risks vs rewards well. reply wholinator2 56 minutes agorootparentI think the idea is that we're, as a species, much more comfortable with the idea that 15 years down the line that 50% of treated colonies collapse in a way directly attributable to the treatment than we are with the idea that 15 years down the line 50% of treated humans die in a way directly attributable to the treatment. Now if the human alternative to treatment is to die anyway than i think that balance shifts. I do think we should be somewhat liberal with experimental treatments for patients in dire need, but you have to also understand that experimental treatments can just be really expensive which limits either the people who can afford it, or if it's given for free, the amount the researcher can make/perform/provide. 10 years is a very long time. I've had close family members die of cancer and any opportunity for treatment (read: hope) is good in my opinion. But i wouldn't say there's no reason that it takes so long reply nialv7 8 hours agoprevPeople joked maybe the Nobel committee is holding some AI stocks. Now I start to wonder if that is true... reply astrange 8 hours agoparentMaybe the transformers authors will win Literature. reply joelthelion 7 hours agorootparentThey are indeed the largest contributors of new books published on Amazon... reply Ekaros 6 hours agorootparentprevI would actually not even be mad. After all it is fundamental change in production of literature works. reply InkCanon 8 hours agoparentprevNext up: The Nobel peace prize is awarded to ChatGPT, because virtually all communiques and peace treaties will be generated by lazy lawyers. reply ramraj07 8 hours agoparentprevThis is legitimately deserved though. reply nialv7 8 hours agorootparentThese two aren't mutually exclusive. reply mr_mitm 7 hours agorootparentIt does make the first speculation a whole lot less plausible though reply seydor 8 hours agoparentprevThe opposite would be less likely nowadays reply atmosx 6 hours agoprev> Oppenheimer was nominated for the Nobel Prize for Physics three times[...] pondered why he was never bestowed the honor. > “To understand this [...] you have to first examine the man’s academic life before and after the war.” Quote from: https://discover.lanl.gov/news/0609-oppie-nobel-prize/ Not anymore. You're not required to know or have studied Chemistry to get a Nobel in Chemistry. reply og_kalu 6 hours agoparentThe Nobel Prize has always prioritised advances in the field over specific training. Curie was a trained chemist when she won her prize in physics. Michelson was a Naval Officer. Of course naturally, being able to win a Nobel usually means you studied the field your entire life but that has never been a requirement. reply belter 8 hours agoprevFirst Obama got the Peace Prize Nobel, now Demis Hassabis gets the Chemistry Nobel. I expect at a minimum the Nobel Prize in Literature to be Donald E. Knuth. reply mihaaly 7 hours agoparentThe Nobel Prize in Literature should go to Jeff Bezos for the Amazon Kindle, obviously! reply belter 7 hours agorootparentYeah, as the Head of the Engineering Org...Makes sense. reply Rochus 5 hours agoparentprevThe Nobel Committee is not unaffected by the global decline in IQ. reply boxed 7 hours agoparentprevI think a more fitting example would be to give the Literature Prize to my son, aged 8. reply alexmolas 8 hours agoprev [–] And the literature Nobel will go for ChatGPT reply throw310822 8 hours agoparentHaha, wrote the exact same. reply andy_ppp 8 hours agoparentprev [–] Eventually, potentially 10 years away if we continue to see improvements. reply og_kalu 7 hours agorootparent [–] Yes eventually GPT itself may be capable of winning a Nobel off its own writing but before then..the authors of the Transformer might win one ? Certainly seems a lot more plausible now. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The 2024 Nobel Prize in Chemistry was awarded to David Baker for his work in computational protein design and to Demis Hassabis and John M. Jumper for their development of AlphaFold2, an AI model for protein structure prediction.- These advancements have significant scientific implications, such as aiding in the understanding of antibiotic resistance and developing enzymes to break down plastic.- The prize is divided, with Baker receiving half, while Hassabis and Jumper share the remaining half."
    ],
    "commentSummary": [
      "The Chemistry Nobel Prize was awarded for advancements in computational protein design and protein structure prediction, highlighting the impact of AlphaFold.- AlphaFold's rapid protein structure prediction is compared to past breakthroughs like CRISPR, though it has limitations, such as not fully solving protein folding.- The prize also recognizes David Baker's contributions with Rosetta, emphasizing the evolving nature of scientific recognition and AI's role in research."
    ],
    "points": 424,
    "commentCount": 231,
    "retryCount": 0,
    "time": 1728467678
  },
  {
    "id": 41785265,
    "title": "Cognizant found guilty of discriminating against non-Indian employees",
    "originLink": "https://www.siliconvalley.com/2024/10/07/h-1b-visa-company-supplying-thousands-of-tech-workers-to-silicon-valley-discriminated-against-non-indians-jury-finds/",
    "originBody": "BUSINESSTECHNOLOGY SUBSCRIBER ONLY H-1B visa: Company supplying thousands of tech workers to Silicon Valley discriminated against non-Indians, jury finds Non-Indians 'benched' then fired, lawsuit claimed By ETHAN BARONebaron@bayareanewsgroup.comBay Area News Group UPDATED: October 9, 2024 at 10:13 a.m. A company that supplies thousands of workers for Silicon Valley’s technology industry and other Bay Area employers intentionally discriminated against non-Indian workers, a jury has found. The jury verdict against Cognizant, founded in Chennai and now headquartered in New Jersey, came Friday in a class-action lawsuit that revolved around claims the firm abused the H-1B visa process. The visa is intended for workers with specialized skills, and Silicon Valley tech firms rely on it heavily to secure top talent and also to obtain workers for lower-level jobs via Cognizant and other staffing firms. Three U.S.-born workers described in the lawsuit as “Caucasian” — Vartan Piroumian of California, Christy Palmer of Arizona and Edward Cox of Texas — sued Cognizant in U.S. District Court in Los Angeles in 2017. Another plaintiff described as Caucasian, Jean-Claude Franchitti, a green card holder from France, joined as a plaintiff later. The lawsuit claimed Cognizant ousted many non-Indian workers by first taking them off projects and “benching” them without work, then keeping them benched until firing them in accordance with a company policy. Cognizant said Monday it was “disappointed with the verdict” and would appeal. “We provide equal employment opportunities for all employees and have built a diverse and inclusive workplace that promotes a culture of belonging in which all employees feel valued, are engaged and have the opportunity to develop and succeed,” the company said. “Cognizant does not tolerate discrimination and takes such claims seriously.” Federal government data show Cognizant obtains H-1B visas for hundreds of Indian citizens to work in Bay Area jobs per year, said Ron Hira, a Howard University professor who studies the visa and testified for the plaintiffs in the lawsuit. Data from 2023 show Cognizant placed H-1B holders at Bay Area employers ranging from Google, Meta and Apple to PG&E, Kaiser Permanente and Walmart. The H-1B has become a political flashpoint. Critics point to abuses including replacement of U.S. workers by visa holders, while the tech industry lobbies to boost the annual cap on new visas past 85,000. The most recent research, by the Bay Area Council, showed nearly 60,000 foreign citizens on the H-1B were approved to work for Bay Area companies in 2019. The vast majority are from India. Because of Cognizant’s preference for Indian workers, it seeks as many visas as possible, and the company has become a top recipient of H-1B visas by submitting visa applications tied to “jobs that do not exist,” the lawsuit alleged. Rules for the H-1B require companies applying for it to have a job the visa holder will fill, Hira noted. Obtaining visas for non-existent jobs “crowds out companies that are looking for the one real worker that they want,” Hira said. Piroumian, a highly experienced software engineer, started at Cognizant in 2012 and “was repeatedly removed from positions servicing Cognizant clients prematurely and replaced with less qualified, Indian … employees,” the lawsuit alleged. In mid-2017, the company “benched” him, then, following company policy to terminate employees after more than five weeks on the bench, fired him six weeks later, the lawsuit claimed. Cox, also with decades of experience, started at Cognizant in 2014, and was benched in January 2017, the lawsuit said. He interviewed for several open roles in the company, but “less qualified Indian … employees were selected,” and he was fired while benched that April, the lawsuit alleged. Palmer, hired in 2012, had not been benched, but also was “repeatedly removed” from projects and replaced with Indian workers, including after a forced transfer from Tucson, Arizona to California, and after another transfer requested by Cognizant from California to Phoenix, the lawsuit alleged. Palmer also claimed she was deliberately excluded from most team meetings, and the few times she was invited, Indian managers would turn their backs on her when she spoke, the lawsuit claimed. She alleged that discrimination and a hostile work environment forced her to resign in 2016. Franchitti, with a PhD in computer science, was hired in 2007, and in his nine years at Cognizant as a director an executive, witnessed the company’s preference for Indians on visas, the lawsuit claimed. When he secured new business for the company, his manager “would staff the client projects with visa-holding employees from India, rather than non-Indian members of Mr. Franchitti’s group who were already in the U.S. and available for this work,” the lawsuit alleged. Franchitti was fired in 2016 after complaining repeatedly that he was being made to sign hundreds of fraudulent invitation letters supporting H-1B visa applications for jobs that didn’t exist, the lawsuit claimed. The allegedly fraudulent letters are part of Cognizant’s scheme to secure vast numbers of H-1B visas and build a “robust inventory” of Indian nationals to be placed in U.S. companies when opportunities arise, the lawsuit alleged. “These individuals are given first — if not exclusive — preference for new U.S. positions, as they become available,” the lawsuit claimed. “Non-Indians are disproportionately relegated to the bench, as jobs are given to … Indians.” Most of Cognizant’s U.S. workforce are on visas, the majority in jobs for which there is “an abundance of capable and qualified U.S. workers who can do the work and want the jobs,” Hira said. The jury recommended that the court hit Cognizant with punitive damages. Originally Published: October 7, 2024 at 4:26 p.m. Most Popular MOST POPULAR Now that homebuyers are on the hook for paying agent commissions, some ask: Is this worth it? Now that homebuyers are on the hook for paying agent commissions, some ask: Is this worth it? H-1B visa: Company supplying thousands of tech workers to Silicon Valley discriminated against non-Indians, jury finds H-1B visa: Company supplying thousands of tech workers to Silicon Valley discriminated against non-Indians, jury finds Lender seizes big San Jose office building in grim sign of market woes Lender seizes big San Jose office building in grim sign of market woes California tax preparer sentenced to 6 years in prison for $28 million IRS scheme California tax preparer sentenced to 6 years in prison for $28 million IRS scheme Troubled San Jose housing tower switches gears from sales to rentals Troubled San Jose housing tower switches gears from sales to rentals Big tech company extends real estate shopping spree in East Bay Big tech company extends real estate shopping spree in East Bay Office vacancy levels soar to record highs in biggest Bay Area markets Office vacancy levels soar to record highs in biggest Bay Area markets San Jose Fry’s Electronics site is poised to become huge tech hub San Jose Fry’s Electronics site is poised to become huge tech hub Disneyland ticket price increase breaks through $200 barrier Disneyland ticket price increase breaks through $200 barrier A vineyard owner tried to provide free housing for his longtime employee. He says Santa Clara County has fined him $120,000 — and now he’s suing A vineyard owner tried to provide free housing for his longtime employee. He says Santa Clara County has fined him $120,000 — and now he’s suing TRENDING NATIONALLY Category 4 Hurricane Milton spawning tornadoes ahead of Florida landfall WATCH LIVE: Hurricane Milton webcams capture scene across Florida 5 killed in plane crash on California's Catalina Island Sean ‘Diddy’ Combs appeals detention in sex trafficking case Colorado Supreme Court dismisses transgender cake case on technicality",
    "commentLink": "https://news.ycombinator.com/item?id=41785265",
    "commentBody": "Cognizant found guilty of discriminating against non-Indian employees (siliconvalley.com)346 points by Melchizedek 11 hours agohidepastfavorite319 comments supriyo-biswas 8 hours agoMy guess would be that a lot of non-Indians at these companies are rejected on the basis of \"vibes\"; which is slightly different from racism as I explain below, although it probably ends up having the same net effect as overt racism. As an Indian, I've observed that collectivism and subservience towards authority figures are taught as virtues; this obviously makes it quite easy for employers to extract unreasonable demands such as long working hours, transgressions of ethical limits, things that are \"bad\", but that generates benefits for the employer. On the other hand, European and American societies generally focus on individualism and autonomy, which obviously causes a conflict when an Indian hiring manager sees anything other than complete deference to them as a threat, and proceeds to reject such candidates. reply llamaLord 26 minutes agoparentThere's probably a really interesting, bit potentially controversial topic to be discussed here. As a Non-Indian, I'm quite aware/familiar with this kind of culture you speak of within Indian culture in the workplace and, to be blunt, if I were applying for a role as say a high level product manager (my current gig) and there was a native-born/raised Indian person as the hiring manager, I would at the very least be very cautious about this risk. But in this situation, given the power dynamic is in favour of the other party, does this make me \"biased\"?... Or just \"careful\"? reply ErigmolCt 1 hour agoparentprevWhen these hiring decisions are based on gut feelings or \"vibes,\" these subjective judgments often reflect an unconscious bias reply Dalewyn 8 hours agoparentprev>collectivism and subservience towards authority figures are taught as virtues Remind me to never fly on a plane piloted by an Indian flight crew. (Protip: If you can't stand up to your captain, your plane is going down.) reply joncrocks 8 hours agorootparentSee cultural `power-distance`. https://en.wikipedia.org/wiki/Impact_of_culture_on_aviation_... reply fakedang 7 hours agorootparentprev> Remind me to never fly on a plane piloted by an Indian flight crew. (Protip: If you can't stand up to your captain, your plane is going down.) Not sure if you're joking, but this is actually cited as one of the reasons behind one crash. https://en.m.wikipedia.org/wiki/Air_India_Express_Flight_134... reply bjourne 6 hours agorootparentSame with South Korean airline crashes: https://www.salon.com/2013/07/10/cnn_asks_if_koreas_hierarch... But who knows if any of this is true or if \"hierarchical cultures cause airline crashes\" is just pop-sci bs. reply Lammy 1 hour agorootparentAlso KAL 801 https://en.wikipedia.org/wiki/Korean_Air_Flight_801#Investig... reply nasmorn 8 hours agorootparentprevStill very rarely, just significantly more often reply dang 1 hour agoprevWe changed the URL from https://twitter.com/USTechWorkers/status/1843744799607898260 to what looks like the article it points to. Submitters: \"Please submit the original source. If a post reports on something found on another site, submit the latter.\" - https://news.ycombinator.com/newsguidelines.html reply dartharva 6 hours agoprevThe waitlist for Indians to get a green card (i.e. second-class citizenship) is 134 years currently. By the end of this decade, it will touch 200. No regular Indian going on a work visa to the US currently has any significant scope of ever being recognized a citizen. Think of that what you will, but the list of incentives an Indian has to bother integrating into American society is rather short. reply p_l 6 hours agoparentCognizant, Infosys, etc. are also quite well known to avoid putting their H1B (or equivalent) in position to consider staying or have opportunity to stay. reply ErigmolCt 1 hour agoparentprevThe absurdly long waiting times reply wslh 5 hours agoparentprevGenuinely asking as someone not deeply familiar with the complexities of Indian culture: India now has the largest population in the world, is the biggest democracy, and has significant potential for growth (e.g. reducing poverty). At the same time, we see Indian leaders in top positions in major U.S. companies, like the CEOs of Google (1st gen) and Microsoft (2nd gen). How do these factors all connect? What drives this mix of internal challenges and global success? I’m not naive in thinking that a few successful individuals represent an entire population, but I do see a signal there. reply kamaal 5 hours agorootparent1. India has a low gdp per capita. What that means is quality of life for masses is low. That also means slum/ghetto like neighbourhoods. Even if you have the money you mostly stay in a gated community, sandwiched between those areas. So you want to leave ASAP and stay in the west. 2. The Indians in top leadership positions you see in tech firms are rare exceptions. To some extent they also come from fairly well off families and communities in India, who have social capital. Can afford cram school fees for Ivy league exams, money and ability to take loans to study abroad etc. 3. Any body who once makes non-trivial money or sees non-trivial career success instantly realises, given how big India is. Given its politics, and overall spending on Education, R&D, and the rate of industrialisation. The only hope for a good life for their, or atleast at their children is to move to the west. Basically people want to leave India, as fixing India is largely a long term, and also a nearly impossible project. reply Eumenes 5 hours agorootparentThe Indians migrating to the US via h1b visas are not coming from the slums or ghettoes. Higher education to foreigners in the US is essentially an immigration scheme - they pay $60-$100k (+living expenses) for a useless graduate degree which only the rich can afford, then get funneled into consulting firms like Cognizant/Infosys on OPT/EAD visas before picking up their h1b for ~ 6 years. This pretty much gets you in the US for ~ 10 years before getting a green card. > The Indians in top leadership positions you see in tech firms are rare exceptions This is absolutely not the case. Despite being 2% of the population, Indians are way overrepresented in tech leadership positions (not just CEO/CTO). I recall in a meeting at a med size tech firm I worked at, the recruiting/HR department no longer considered Asians of any ethnicity a minority and lumped them in with white males. reply red-iron-pine 2 hours agorootparent> the recruiting/HR department no longer considered Asians of any ethnicity a minority and lumped them in with white males. there was, IIRC, an infamous spat at a FAANG about that reply Rinzler89 3 hours agorootparentprev>recruiting/HR department no longer considered Asians of any ethnicity a minority and lumped them in with white males. Why does HR even need to keep track of ethnicities? Is this a US thing? Here in my EU country you're just employee/applicant #3215, that's it, nobody asks or enters your ethnicity anywhere to even be able to keep tabs on how many are of what ethnicity, since everyone is considered equal by default and judged exclusively on performance (in theory at least, in practice there are still biases, but tracking ethnicities won't fix that, since that's human nature). What you're saying would even be against the law here since then it opens the door to bias and potential discrimination. reply bee_rider 14 minutes agorootparent> (in theory at least, in practice there are still biases, but tracking ethnicities won't fix that, since that's human nature) That’s what we’re trying to fix (or at least mitigate) in the US. You say it is just human nature, but we’re a pretty diverse country, so we have a strong incentive to proactively try and see if we can make it work. reply rsanek 4 hours agorootparentprevnext [3 more] [flagged] thunkshift1 3 hours agorootparentNo more than usa’s reply wslh 3 hours agorootparentYes, I think the democracy issues are more general. reply jl6 5 hours agoprevThe worst HR case I ever got involved in was a semi-onshore (Indian) development team who couldn't deal with having a female (Indian) tech lead. She was eminently qualified for the role, but they would attempt to undermine her at every turn, speak over her, hold meetings without her, always going over her head... Caste did not appear to be a factor; it seemed to be good old fashioned sexism. reply ErigmolCt 1 hour agoparentThat situation is unfortunately all too familiar and it speaks to deeply ingrained gender biases that exist not only in some Indian cultural contexts but also in many other parts of the world reply Rinzler89 3 hours agoparentprevnext [3 more] [flagged] dang 1 hour agorootparentNationalistic/racial/ethnic/religious/whatever-this-is slurs will get you banned here. It's not what this site is for, and destroys what it is for. Please don't post anything like this again. https://news.ycombinator.com/newsguidelines.html reply red-iron-pine 2 hours agorootparentprevsexism and non-egalitarian behavior is pretty common across the globe. I'd argue that the west, esp. N America, is the exception, not the rule. and even there, there are pressures working against it. reply red-iron-pine 2 hours agoprevdefinitely my experience dealing with them at F500s. Indian mafia running the IT org, and a PITA to get things out of them. personal favorite was that someone from network support ran a script that changed ownership of all of the docker containers and associated configs, outputs, and logs to root. we had pretty clear proof in the logs that a Cog support tech did it, and basically had to escalate to the CTO and get him to threaten a lawsuit to get them to fix it. Also watched caste bulling play out in real time in a cramped meeting room in the RDU Triangle, in NC. Wasn't clear what the strain was until later when a full-timer of Indian extraction explained it to me. reply bufferoverflow 2 hours agoparentYup. Currently working at a large multinational. Pretty much 90% of developers and managers are Indian. Very few Americans. Some Chinese and some Slavic. I have never seen such ratios in 20+ years of my career. reply tock 6 hours agoprevIt's weird how racism against Indians online is so normalised. I see the same on reddit too. reply p_l 6 hours agoparentThank WiPro/Infosys/Tata/Cognizant/HCL and everyone who apes their practices. The end result is that you see a mention of \"Bangalore\" and you get immediate shivers because it doesn't matter how nice and good as a person the people you are forced to work with are, if the entire qualification you can discern is that they are statistically expected to speak english, and their work environment and supervisors enforce certain counter-productive behaviours. Which is honestly abusive to the poor person in Bangalore, because it's not like they can learn the skills this way either. To the point that I once had security team in a company where specific area of security might involve \"uncomfortable questions from government\" in cases of failure (fortunately not defense industry) actually sponsor my efforts to bypass supposedly crucial firewall - because connectivity including said firewall were handled by TCS. reply mardifoufs 56 minutes agoparentprevWeird thing to comment on an article about Indian racism, but okay. reply hshshshsvsv 6 hours agoparentprevnext [8 more] [flagged] dang 1 hour agorootparent> It's okay to be racist against Indians according to hacker news guidelines Please let's not respond to false or mean comments by making up more false and mean things. Nothing could be further against the HN guidelines. We (and I personally) have banned many accounts over the years for posting slurs against Indians (same as against any other group). Including in this thread. The only reason posts like that would go unmoderated is that we haven't seen them. We don't come close to seeing everything (or even 10%) of what gets posted to HN. We rely on users to point us to the things that most need attention, so the helpful way to react to such a comment is to flag it and/or email us at hn@ycombinator.com. reply lupusreal 5 hours agorootparentprevI've never seen let alone worked in a team that only had white people. Not once. I've never seen nor heard of a team that only had Japanese or Koreans or Russians or any other singular group except two: Chinese and Indians. These two groups have a unique tendency to form ethnically exclusive teams at companies and get away with it. reply RestlessMind 2 hours agorootparentI have seen exclusively white people teams, especially at smaller startups. They were all Americans though, so not much diversity of nationalities. Given that China and India are countries with 1.4B population, no surprise that one can find enough people to form exclusively Chinese (or Indian) teams. Another factor is that people from other backgrounds do not want to join such teams even if the hiring manager makes them an offer. When I was at a FAANG, my team composition slowly drifted towards only Chinese and Indian, as people from other backgrounds left in 6-12 months after an Indian manager came in. reply lupusreal 2 hours agorootparentI'm sure it happens at small companies, but at large companies I've never seen it. It wouldn't be allowed to stay that way if it did happen. The number of times I've seen it happen with Chinese and Indians teams, relative to how rare it is for white-only teams to exist, seems highly improbable. White people are the plurality if not majority at most big American tech companies, so if it's all just statistical noise it should happen more with white people and that just isn't what happens. reply RestlessMind 2 hours agorootparentI have seen enough white-only teams while at FAANG. Those were not in SV but in smaller US cities. Of course that was rare but then again, chinese-only or indian-only teams were rare as well. reply navigate8310 6 hours agorootparentprevnext [3 more] [flagged] dang 1 hour agorootparentPlease don't perpetuate hellish flamewars on HN. It's not what this site is for, and destroys what it is for. https://news.ycombinator.com/newsguidelines.html reply hshshshsvsv 6 hours agorootparentprevGeneralizing everyone from a region/group with some negative trait is racisam. If your brain try to label me as someone who practice caste system because you think I am from India is lazy/stupid computation and racism. Actually vast majority of the Indians suffer from negative consequences of Castisam since they are supposed to be lower classes and society descrimated against them historically. They want equality more than probably anyone you know of. Indian constitution has done a lot for eliminating this. But there is a lot of cultural baggage that needs cleanup. reply fennecfoxy 6 hours agoparentprevnext [3 more] [flagged] tock 6 hours agorootparentJust my experience. I reckon a lot of recent events like buying oil from Russia has sadly justified it for a lot of people. reply delta_p_delta_x 5 hours agorootparentprevNot the parent commenter, but from what I've seen, they are correct, and as far as Reddit is concerned, especially so. Genuine criticism of India's government and some Indian/Hindu cultural practices can quickly devolve into disgusting racist diatribes and stereotypes that wouldn't fly against people from other countries. Try using the hard-r n-word as a non-black person, or calling someone with East Asian phenotypes a 'Chinaman', or any Native American stereotype today, and see how quickly you bring a firestorm upon yourself. Meanwhile, people on Reddit call Indians street-sh*tters or cow pi*s drinkers, and no one bats an eyelid. I have seen these words verbatim on places like /r/worldnews, without censorship or bans whatsoever. Racism against Indians is normalised, full stop. reply gnabgib 11 hours agoprevAlso (18 points, 1 day ago) https://news.ycombinator.com/item?id=41774300 reply Molitor5901 5 hours agoprevThe only point of contention I have with this whole debate is the general premise that it's wrong to make citizens of a country redundant (lay them off) in order to replace with cheaper labor brought in from other countries. From an accounting standpoint perhaps it makes sense, but from the view of the workers, and the community, it just seems wrong. reply Nefariouspurpus 1 hour agoparentAlthough I'm not deeply familiar with the specifics of this particular case, it raises broader questions about the industry that specializes in outsourcing, particularly to countries like India. If outsourcing is a core part of a company's business model, isn’t it inevitable that more jobs will continue to be funneled to countries like India, and, consequently, to Indian workers? While I understand this particular case involves allegations of discrimination within U.S. offices, the larger trend seems unavoidable. As companies prioritize cost-efficiency, it may no longer make economic sense for citizens of HCOL countries, to seek employment with such firms. Similarly, these companies are likely incentivized to constantly try and minimize the number of high-cost employees—whether they are Indians on H1B visas or American citizens—on their payroll. This raises a concern about whether the government should intervene to address this industry trend of outsourcing. However, if we look at what happened with manufacturing and China, one wonders if such intervention will ever come, or if the shift is already inevitable. reply hooverd 26 minutes agorootparentEh, cheap Indian outsourcing isn't viewed as a near-peer threat. reply Nefariouspurpus 9 minutes agorootparentI understand but that is not what I meant. What I meant was it is an inevitable drift (for a certain type of work to get outsourced) that cannot be stemmed unless the government does something. I still have no clue if doing something about this type of outsourcing is even a good idea. But I was just wondering if this discrimination is inevitable if outsourcing continues... reply lupusreal 5 hours agoparentprevUnionization could, at least in principle, offer a solution. reply dartharva 5 hours agoprevdang? Where are you? This comment section has gone to the gutter. Please flag this thread. This isn't even that significant of a news - we can come back to this when the court actually hands a ruling. reply dang 1 hour agoparentI've done some moderating in here now, but if you want to alert us to something like this, it's necessary to email hn@ycombinator.com. If you see a post that ought to have been moderated but hasn't been, the likeliest explanation is that we didn't see it. You can help by flagging it or emailing us. https://hn.algolia.com/?dateRange=all&page=0&prefix=false&qu... reply bobosha 7 hours agoprevI am a proud Indian-American (naturalized after h1b) and I think this kind of discriminatory mindset has no place anywhere, definitely not in the workplace. I think my community can be secure in its success and learn to do better and be open to critique. Let's work to abolish these attitudes by being more self-aware and raising awareness & use cautionary tales like this as a wake-up call. reply atcognizant 6 hours agoprevSome of favorite coworkers have been from India during my career. This includes both their personalities as I got to know them and their demeanor at work. I have worked at Cognizant for just over a year. Though we don't have this problem on my immediate team, it is something we talk about because it's obvious in other teams and as an overall culture at the company. It doesn't seem like overt discrimination (from me and my coworkers limited perspectives). There's a couple factors 1. People hire who they know. About two thirds of the employees are of Indian descent. So on average they hire their friends and former coworkers who are also Indian. 2. Leadership requires the offshore teams to be on every project. This is to keep costs down and because the other lines of business besides their main and original one (staff augmentation) are relatively new. 3. A minority (but not a small one) of Indians really are just blatantly discriminatory. reply Eumenes 8 hours agoprevThis is obvious at a number of these big box consulting firms for many years (Tata, Cognizant, HCL, Capgemini). Its so easy to verify too - go on linkedin and comb through dozens of search results of exclusively Indians (I'm not talking about American-Indians who go to undergrad in the US like every other person). These firms also take a lions share of the h1b visas which is another conversation - why does India get to have 75% of all h1b visas? Seems not very diverse. reply lmz 6 hours agoparent> why does India get to have 75% of all h1b visas? Seems not very diverse. Not to defend the Indians or their paymasters, but the H1B isn't diversity based, and there are a lot of Indians who also speak English, so... reply dartharva 5 hours agoparentprevBecause there are just that many applicants from India? The result of this is also that Indians have virtually no scope of settling down in the US, since the waitlist for green card for them goes beyond 134 years. Which means none of those H1B holders are going to stay. reply Eumenes 5 hours agorootparentWho processes the applicants? Indian bosses (see the tweet/articles). Many US permeant residents are being passed over at these big consulting firms in favor of Indians. Its nepotism, 100%. Indians in the US are not discriminated against. Despite beingThe fractal nature of it is truly remarkable. When you zoom out, you see what was essentially one people split along religious borders with India and Pakistan. Zoom in, and you see finer and finer levels of cultural discrimination. In-group, Out-group Theory at play https://en.wikipedia.org/wiki/In-group_and_out-group reply lazide 8 hours agorootparentprevYup. One of the most fundamental parts of the human experience is that everything is relative. So, when someone wants to feel better, or even objectively have it better, the easiest way to do it is to make sure there is someone else having it worse nearby. And doing that all the time based off fuzzy, changeable things like merit is a lot of work. Or even worse, something that someone has to do all the time or maintain, like results or wealth. Both for the downtrodden trying to improve their station, and the ones ‘on top’ trying to keep them down. Much more stable to systematize it, and use things like permanent labels on entire family lines, eh? And bonus points if you can do it in a fine grained enough way, that essentially everyone has someone lower than them to shit on. Then all anyone has to do is exist, and push the buttons provided for them. Easy peasy. reply dartharva 6 hours agorootparentprevAll discrimination is contextual and local. The number of groups has nothing to do with it. reply lazide 6 hours agorootparentSo you say, but history has shown that if groups don’t exist to allow easy discrimination, they will be created to allow such discrimination. So it’s a good sign of long, well established, and ingrained discrimination, eh? With associated infrastructure to make it easier to continue, and ‘fairer’ in that it allows almost everyone to have someone under them to take their own angst out on. (Except those ‘outside the system’ anyway, which historically were the Dalits, but now even they are in the system eh?) reply dartharva 6 hours agorootparent> So you say, but history has shown that if groups don’t exist to allow easy discrimination, they will be created to allow such discrimination. Yes, that's what I am saying. In- and out-groups are made contextually in each unique situation. > So it’s a good sign of long, well established, and ingrained discrimination, eh? With associated infrastructure to make it easier to continue, and ‘fairer’ in that it allows almost everyone to have someone under them to take their own angst out on. (Except those ‘outside the system’ anyway, which historically were the Dalits, but now even they are in the system eh?) Can't understand what you're trying to say here. reply lazide 3 hours agorootparentCan’t understand, or refuse to understand? Why create new groups when you have proven and well trod ones handy to choose from? reply TrapLord_Rhodo 2 hours agorootparentprevyour thoughts seem to align closely with René Girard's theories on mimetic desire and scapegoating. reply tho342424334234 9 hours agoparentprevnext [4 more] [flagged] mschuster91 9 hours agorootparent> However, having people of similar cultural/linguistic backgrounds helps a lot with communication, both inter-personal and for business. It helps on communication, definitely - but it is a detriment against \"fresh ideas\" or when testing out new products. reply dudefeliciano 9 hours agorootparentprevso tired of the self-hating/self-loathing label when someone criticises their own nationality/ethnicity reply meiraleal 9 hours agorootparentprevYou can't make yourself look less racist by pointing to other racist cultures (while describing with details your own racist behavior). reply mr90210 6 hours agoprevI am going to disclose my own bias here: I tend to look for multicultural teams and somewhat evenly distributed. As an African so far I have worked and learnt a lot from a Brazilian-Japanese, Bulgarian, Portuguese, Dutch, Belgian, Israeli, Russian, English, Indian, Pakistani, German, Argentinian, and a Polish. Some of which I have the pleasure to call friend. reply miningape 2 hours agoparentAlso an African here, and this is exactly how I feel about it - the world is too interesting to ignore. reply ctxc 6 hours agoparentprevHello, potential friend. reply vitus 7 hours agoprevWhy is the submission a link to a twitter screenshot of a news article? https://www.mercurynews.com/2024/10/07/h-1b-visa-company-sup... is the paywalled source. That said, https://www.siliconvalley.com/2024/10/07/h-1b-visa-company-s... seems to be the exact same article (author, timestamp, and all) published by the same group (Bay Area News Group) but without a paywall. (Both siliconvalley.com and mercurynews.com are registered to MediaNews Group, which is the owner of Bay Area News Group as of 2006.) reply dang 1 hour agoparentSwitched to that latter link now. Thanks! Submitters: \"Please submit the original source. If a post reports on something found on another site, submit the latter.\" - https://news.ycombinator.com/newsguidelines.html reply oefrha 9 hours agoprevSource article is here: https://www.mercurynews.com/2024/10/07/h-1b-visa-company-sup... Paywall workaround: https://archive.is/PiMNU Another non-paywall article on the same topic: https://insider.govtech.com/california/news/jury-finds-discr... It's about the (previously) Indian outsourcing firm and H-1B body shop supplying contractors to SV companies. It's unrelated to the Indian manager only hiring Indians phenomenon which many people here are discussing; FWIW I believe it was quite widespread at least back when I was in the Valley, but this ruling doesn't address that in any way. reply JohnBrookz 9 hours agoprevThis happened at intel and was very noticeable even back in 2010. Indians hires other Indians. At every company I’ve been at if you were not Indian you were not invited to the conversation. Sitting in meetings where you were excluded from 30% of the conversation was wild. I never felt like they were rude or anything though- just that I was an outsider. Best team I worked with was very diverse and they actively worked to help each other get promoted and protect each other. reply quadrifoliate 6 hours agoparent> Sitting in meetings where you were excluded from 30% of the conversation was wild. I am curious, was this due to them speaking in a different language in actual professional meetings at Intel? I have often heard these reports, but in a social context. I have never personally observed this in professional settings; but am curious to hear more so I can watch out for it if/when I do encounter it. It's odd because I would struggle to hold a professional conversation in any of the Indian languages that I speak (I have no idea how to say something like \"thermal characteristics\" or \"power dissipation\" in them); and would likely keep lapsing into English. reply drewbitt 4 hours agorootparent> was this due to them speaking in a different language in actual professional meetings This happened frequently at a WITCH I worked at out of college. The meeting would be in English then have segments change in the middle as certain speakers switched languages. Luckily, I often had a coworker stand up for me to mention to use English although I did miss many conversations. reply fragmede 20 minutes agorootparent> WITCH stands for the Indian tech giants – W- Wipro I- Infosys T- TCS C- Cognizant H- HCL A- Accenture India In case anyone else was wondering about that acronym. reply mytailorisrich 6 hours agorootparentprevI have many Indian colleagues and they do tend not to speak English among them in the office. There needs to be someone else involved in the conversation for them to stick to English. Another aspect is that I have found that they are quite hierarchical, probably a cultural trait. So how much they stick to English also depends on how senior the \"non-Indian(s)\" are compared to them. If you are their senior they are very nice. reply kevin_thibedeau 7 hours agoparentprevI worked at a company with a large multi-ethnic workforce. They had an Indian subsidiary but didn't play the H1B game. The rule was English only in the office to avoid exclusion. reply throwawayha 6 hours agoparentprevThere was an article exposing it wasn't just for / between Indians. Indians were actively discriminating against other Indians if they weren't born into the \"highest\" caste of hinduism (the few percent only allowed to learn to read and write), or avoiding the \"lowest caste\" of hinduism (way over 90 percent). This caste system discrimination in tech, is also used to discriminate against other minorities who are not Indian, originating from the Indian subcontinent. Trapped in Silicon Valley's Hidden Caste System https://www.wired.com/story/trapped-in-silicon-valleys-hidde... Insight: Caste in California: Tech giants confront ancient Indian hierarchy https://www.reuters.com/business/sustainable-business/caste-... https://www.nbcnews.com/news/asian-america/big-techs-big-pro... More: https://www.google.com/search?q=silicon+valley+caste+system reply sandworm101 6 hours agorootparentCanada has been dealing with this more proactively than most counties with Indian diasporas, a process no doubt aided by the by the significant number of indo-canadian politicians familiar with the issue. https://www.cbc.ca/news/canada/toronto/ontario-human-rights-... https://www.cbc.ca/news/canada/british-columbia/caste-discri... https://www.trtworld.com/americas/toronto-school-board-recog... reply dartharva 6 hours agoparentprevIf this has really been going on for more than a decade I wonder what your labor regulators are doing. This is a clear-cut issue of bad labor policy. reply hggigg 6 hours agoparentprevSame experience here. On a positive note I got replaced by a guy in India. Then after a year of them clambering over each other’s corpses for promotion and destroying the org from the inside, I got hired back on contract to unfuck the mess at 4x my previous salaried rate. Nothing against India or Indians but the reason outsourcing fails is no one wants to be an outsourced workforce so they go for promotion first to get a better job. And I don’t blame them. reply navigate8310 6 hours agoparentprevSafe to say that 2010 was the beginning of Intel's downfall reply dopylitty 6 hours agoparentprevnext [10 more] [flagged] AlexandrB 6 hours agorootparentHighly misleading. From 1: > During an exclusive interview with NewsNation, Trump said he planned to strip the legal status of the Haitian immigrants in Springfield, Ohio, who have been granted Temporary Protected Status. Temporary Protected Status[0] is not nearly the same thing as having a green card or being naturalized. [0] https://www.uscis.gov/humanitarian/temporary-protected-statu... reply mc32 6 hours agorootparentprevHe talks of deporting illegal immigtants —something Harris also now claims is a problem and moreover claims they are more efficient at the border than Trump was ;that’s a dubious claim but it shows she sees it as a problem as well. Over 60% of voters also see illegal immigration as a very serious problem, more see it as an issue. In addition India itself is strict and also deports illegals. reply rcbaF 6 hours agorootparentprevNothing? Trump talks a lot. He didn't \"drain the swamp\" while he was president and he did not release the Kennedy CIA archives (he promised both). He'll get a call from Thiel or Musk that they still need H1Bs, and that will be the end of it. (I still think Trump is the lesser evil, since he is far more competent in foreign policy and the economy will improve. And he does exactly nothing w.r.t the scary sounding election talk.) reply bryanlarsen 6 hours agorootparent> since he is far more competent in foreign policy The guy foreign leaders call the \"laughing fool\"? > the economy will improve. Trump's plans would cause the economy to shrink by 10% and cause 28% inflation, according to independent analysis. https://www.nytimes.com/2024/09/26/business/economy/trump-ec... reply lupusreal 2 hours agorootparentI don't need or even want a US president to be respected by other countries leaders. All I need is for the POTUS to not get this country involved in more foreign wars. Trump is a lapdog of Israel so he doesn't fit this bill, but what I'm saying is \"foreign leaders laugh at him\" falls flat. IDGAF. reply rtlasacc 6 hours agorootparentprevThe world was far more stable in 2016-2020. It does not matter what someone is called. Just watch what actually happens and overlook all the talk (we might as well explore what Harris is called; I think the attribute is \"cackling\" and not \"laughing\"). The NYT is not unbiased when it comes to Trump and none of these plans will be implemented. Again, the economy was good in 2016-2020. reply bryanlarsen 5 hours agorootparentJudge not by absolute standings but by relative standings. During the Trump presidency our competitor's economies were growing faster than America's: China, Germany, et cetera. America was losing its status as #1. During the Biden presidency every economy but America's took an absolute beating. Britain & Germany went into a deep recession. China's growth flatlined. America was pretty much the only economy to avoid getting curb-stomped and regained its status as the clear #1 economy. Anybody can do well when there's a rising tide raising all boats. It takes a true master to do OK when everybody else is doing poorly. P.S. The NYT has a significant both-sides-ism bias and is the only place you'll find pro-Trump articles outside of the right wing media. reply akrH 4 hours agorootparent> every economy but America's took an absolute beating. Britain & Germany went into a deep recession. If you blow up Nordstream and lead the Europeans in a war that they only lose from, then their economies will tank. Inflation and job losses for ordinary Americans are high as well. Perhaps on paper the economy is doing relatively well. (It is the first time that I have heard anyone describing Biden as a true master.) reply bryanlarsen 1 hour agorootparentYou have to be consistent. If you say that presidents aren't responsible for external events, then you can't give Trump credit for the smooth sailing lack of external evennts of 2016-2019. If you say that they are, then you have to give Biden credit for weathering the storm during his presidency. > (It is the first time that I have heard anyone describing Biden as a true master.) Just letting the Fed do their job deserves most of the credit. Should be table stakes, but Trump has promised to %^@# that up. reply throwawayian 10 hours agoprevSeeing this happen at an originally Australian unicorn after the technical co-founder stepped down and new CTO swapped out all of their engineering leadership with Indian hires from Microsoft. Previously saw it at a lesser degree to older unicorns and places like Oracle, IBM, etc. Although it’s normalised in the latter ones. reply PedroBatista 8 hours agoparentThis is unfortunately a widespread practice even in less tech heavy companies. It starts with a very aggressive C suit or manager responding to everything 24/7, supported by “cheap” labor who say yes to everything and promise the World, usually extra complicated. Fast forward a couple years and the company/project is a expensive hell-hole of disfunction and suffering. CEOs and the board also are to blame for this, they are aware, they just don’t care as they like people who say yes and don’t give them any bad news. reply Cthulhu_ 7 hours agoparentprevI've been at a number of companies that had a significant Indian workforce, often hired through the likes of Infosys and the like. I don't believe that's down to nepotism or whatever though, but simple market forces: \"you need two dozen SAP experts and can't find them locally? We have them\". That said, there's frequently islands of Indian and \"the other\" developers, in part that'll be cultural, but in part it's down to simply having a different job, expertise, department, etc. reply eastbound 10 hours agoparentprevWhich explains why its market value is hovering at $160 while its former worth, when it was properly managed, was $3-400. Am I spot on? The problem is that every Fortune 500’s internal systems are hooked up to it. reply logicchains 9 hours agorootparentAtlassian? As a user, I haven't noticed any decline in quality; it's just as painful to use now as it was a decade ago. reply wordofx 9 hours agorootparentYou can’t notice decline in quality when the quality is already so bad that any change feels like an improvement while still being bad. reply aleph_minus_one 7 hours agorootparent> You can’t notice decline in quality when the quality is already so bad that any change feels like an improvement while still being bad. In this case we don't have a decline in quality. reply physicsguy 9 hours agorootparentprevA decade ago, you could self-host though. reply throw49sjwo1 8 hours agorootparentNobody (important) wanted that, though. It was done only because the cloud wasn't available, the moment it became available every big company migrated. reply p_l 8 hours agorootparentSelf-hosting JIRA and Confluence was a double edged sword: Done badly it meant you suffocated the application server with too little resources giving dreaded \"super slow JIRA\" effect. Done well it meant you didn't have to deal with Atlassian underprovisioning resources in the cloud or having rigid maintenance times that didn't fit with your company needs. reply jcims 8 hours agorootparentprevI’ve never seen or worked with a company using the cloud version of any of their products. I worked at a place with one of the largest bitbucket installs in the world, so they weren’t little shops. reply physicsguy 8 hours agorootparentprevThe rise of self-hosted GitLab and GHES suggests that's not true. reply throw49sjwo1 6 hours agorootparentRise? I don't see anything other than migration to cloud. reply dicknuckle 8 hours agorootparentprevMigrated from self hosted Atlassian to cloud Atlassian. reply dx034 8 hours agorootparentprevNot true. I‘ve been at several companies that self host Jira. With the amount of critical data and processes in Jira, it can be comforting to have it on-prem with no one else being able to access the server. reply throw49sjwo1 6 hours agorootparentAre you sure they're not just \"preparing for the migration\" and plan to stay on premise? I also worked at many places that had it on premise - all of them planned to migrate ASAP (where asap is sometimes years). reply quadrifoliate 6 hours agorootparentprevThis. Every single Atlassian product I have used has always been painfully slow garbage; and I have been forced to use them at various jobs over a decade. I don't think it was ever a magically good Australian product that Indian hires from Microsoft suddenly ruined. reply eastbound 9 hours agorootparentprevIt’s also probably 5x more expensive than a decade ago. As you said, still painful to use. reply ActionHank 9 hours agorootparentMaybe cheaper labour brings hidden expenses. reply f1shy 9 hours agorootparentYou can ask Boeing ;) Joking, I do not think it is the case, BUT I do know that cheap is often more expensive reply bbor 8 hours agoparentprev…are we complaining about people living in India, or American/Australian citizens that have Indian national heritage? The former seems more like an outsourcing issue than an Indian issue. The latter… didn’t we learn in the 90s not to stereotype people? If I posted a comment complaining about how black Americans are criminals, I think I’d rightfully be called out for oversimplifying in a hurtful way. What am I missing here? reply ewuhic 8 hours agoprevHow to fight back when you see this happening at line management level, but not yet proliferated to upper management? Indian product owner taking mostly to Indian peer developers, bypassing normal communication channels. Indians being friendly with each other and stone cold with the rest. Indians bringing Indian jokes to the table, with no outsider hoping to understand these. Indians bonding to go to Indian restaurants during lunch break, so now most of the colleagues follow suit, how to stop it all? All these culture things (except for the first regarding PO which is spit-in-the-face level of unprofessionalism) add up, and then you find yourself in a corporation described in other messages of this thread. I had to offboard from multiple projects throughout my career because development was hijacked by Indian cronyism. reply quadrifoliate 8 hours agoparent> Indians bringing Indian jokes to the table, with no outsider hoping to understand these. Indians bonding to go to Indian restaurants during lunch break, so now most of the colleagues follow suit, how to stop it all? Are you seriously asking how to \"fight back\" on Indians going to restaurants and joking with each other? As an Indian in a flyover state who has been routinely excluded from golfing events, and had my dietary needs totally ignored while organizing things like steakhouse lunches, this is sort of darkly funny to read. reply loufe 7 hours agorootparentI think that's an unfairly dismissive take and the \"going to restaurants\" bit is a mischaracterization at best. There massive difference is WHERE the discrimination is taking place. Most would not move to India or any other place and impose their culture and exclude locals in a fair and just world. I'm not saying it doesn't take place, and yes colonialism happened and was far worse, but we're talking about what SHOULD be. reply quadrifoliate 7 hours agorootparent> Most would not move to India or any other place and impose their culture and exclude locals in a fair and just world. I'm trying to say that going to restaurants as a group of people and having in-jokes does not qualify as \"imposing your culture\" in any way. These things routinely happen at companies that have few to no Indians, they just take a different form. Also, are you really claiming that if you moved to Bangalore, and had 2-3 coworkers from your hometown that you knew and shared cultural ties with; that you wouldn't tend to hang out together at lunch? reply ethbr1 7 hours agorootparentThe point is not the restaurant or jokes, but whether business decisions are made there (or if relationships built there drive business decisions). reply quadrifoliate 7 hours agorootparentSure, but Americans have been doing this for hundreds of years through exclusionary hobbies like golf, fantasy football, and a hundred other things. That's just how social groups work. They are often cliquey and exclusionary. That's why I think it's weird to only target Indians in this regard. They are building an in-group just like everyone else; the difference is that OP seems to have little experience not being part of the in-group. reply ethbr1 6 hours agorootparentThat's true and they have. But that doesn't make it okay for others to do. We should be working to decrease it in all exclusionary groups by working to make them more inclusionary. That means intentionally rotating comfort zones. And it is a historically seductive siren call that once an immigrant community in any country attains some power, they use it to ramp up exclusion and cronyism. In all fairness, to protect their tenuous grasp on that power from external racism, but it also succumbs to use for less noble, more human ends. E.g. getting ones friend hired. reply TimedToasts 6 hours agorootparentprevI'm glad that you're admitting this is happening. reply gedy 6 hours agorootparentprevThe better question is: if you had a Bangalore company founded by Indians, then a management hire from the US or Britain started hiring immigrants from there too, and they excluded or sidelined Indians, especially ones who didn't speak English - you'd be annoyed too right? reply Cthulhu_ 7 hours agorootparentprevActually you'll find that this happens a lot. Search up expat communities, for example. They don't necessarily \"impose their culture and exclude locals\", but they are for and by the expats from a certain country. Seeking \"your\" people when living/working abroad, or when working in a diverse workspace, is pretty normal and happens everywhere. It's usually harmless though. reply swasheck 6 hours agorootparenti’m not sure that’s the point though. i think the point is lost in op layering restaurants and jokes and such into their narrative. i think the actual point and concern is how being “out-group” affects their employment due to what they perceive as deliberate exclusion. reply n1b0m 7 hours agorootparentprevBrits living in Spain is a classic example reply noisy_boy 6 hours agorootparentBrits living anywhere outside of Britain is a classic example - exemplified by the clubs and societies formed around the world when the sun didn't set in/on their empire. reply 39896880 7 hours agorootparentprevYou seem to be fixating on the term \"fight back,\" when the commenter you're replying to is asking for ways to not feel excluded at his/her job, where s/he spends a majority of his/her waking hours. As someone who has experienced this, I encourage you to draw on that experience and have empathy, even if that experience is expressed in ways that don't immediately resonate with you. You have more in common with the commenter you're replying to than you appreciate. reply zozbot234 7 hours agorootparent> [looking] for ways to not feel excluded at his/her job, where s/he spends a majority of his/her waking hours. And the people GP is talking about are trying to do the exact same thing, though obviously in highly detrimental ways. Part of professionalism entails not making your job into an identity that overwhelms every other aspect of your life. If you just focus on delivering good results to the best of your ability, there's no need to be dependent on constant social approval from 'insider' peers. reply swasheck 6 hours agorootparentthis is fascinating to me because it’s how i have handled my career and it’s had unexpected positive and negative effects. all-in-all i’m more satisfied than my friend group with my day to day work existence but am steps behind them in the traditional career path milestones. though i lack the titles they have, i’m squarely median when it comes to annual base income, which is comfortable and provides very well for my family. however, i’m beginning to age and realize that potential employers are beginning to balk at my title/“informal” leadership (read “experience”) with my age and salary and duration in the workforce. reply mdorazio 8 hours agorootparentprevThe parent post was clearly asking how to fight back against a culture of exclusionism against non-Indians, which leads to racism. reply smcl 6 hours agorootparentI would hope any sensible person would look at two comments by two people saying that they are being excluded by each others groups and think \"the issue is people being excluded because they are not part of the 'in' group in their workplace, and people should work to be more inclusive\" rather than concluding that the problem lies with Indian or non-Indian people specifically reply thih9 7 hours agorootparentprevAnd quadrifoliate (this comment's grandparent) showed that exclusionism against Indians similarly exists and is harmful. Very much on topic in my opinion. reply Hnrobert42 8 hours agorootparentprevI am sorry for your experience. I also feel for GP. Neither of you should experience that. Your experience does not negate his. Nor his yours. reply j45 7 hours agorootparentOne is likely the majority, the other the minority. It doesn’t negate any behaviour based on bias but would require an honest reflection on the unconscious biases existing in the workplace prior to this. reply worthless-trash 7 hours agorootparentAren't they both the minority in their respective situations ? reply j45 2 hours agorootparentOne way to look at it is minority groups who can’t turn off being visible minorities can be different than people who may be minorities in a a few areas of life but still have access to benefit from privilege in others. reply miningape 8 hours agorootparentprevYeah, and I'd expect that treatment as a white person working/living in India, and I would adapt to fit in with local customs / culture or else leave. But they are in the US, working for a US company - why is it somehow acceptable when you are a foreigner to create these isolated professional enclaves that exclude the \"native\" population? (A group of people casually getting together is totally different to someone's work environment where they have to attend to bring a paycheck home - I'm talking exclusively about professional/work environments) reply robofanatic 7 hours agorootparentThats a typical western mentality. If you do it then it is a problem but if I do the same thing it is somehow justifiable! reply miningape 7 hours agorootparent> I would adapt to fit in with local customs / culture or else leave What? It's apparently white to want to fit in with the local culture now. reply quadrifoliate 8 hours agorootparentprev> Yeah, and I'd expect that treatment as a white person working/living in India, and I would adapt to fit in with local customs / culture. But they are in the US, working for a US company - why is it somehow acceptable when you are a foreigner to create these isolated professional enclaves that exclude the \"native\" population? Non-Hispanic white Americans are not native to the US, and I don't see a reasonable basis for concluding that non-Hispanic white culture should be the \"native\" or \"default\" culture in the US. Sure, it might be the dominant culture — but there are other subcultures like Black or Hispanic cultures that are pretty strong here. Would you feel comfortable asking how to stop a group of Black coworkers from going to a restaurant that serves Black cuisine, or Hispanic coworkers from going to the local taqueria? If not, then why are you singling out Indians? reply miningape 8 hours agorootparentHence the quotation marks: \"native\" - you're just derailing a legitimate line of reasoning. > I don't see a reasonable basis for concluding that European-American culture should be the \"native\" or \"default\" culture in the US Like it or not, it is the dominant culture especially in professional environments. If you want to have a conversation about why that is the case, what else it could be, etc. thats fine! But it's not the kind of conversation I'm looking to have here. > Would you feel comfortable asking how to stop a group of Black coworkers from going to a restaurant that serves Black cuisine, or Hispanic coworkers from going to the local taqueria? If they are doing it to the detriment of the overall business yes! - the line of reasoning follows for a predominantly Black business that is having a White enclave forming. Or a Hispanic cultured business with a Slavic enclave forming. Even more importantly a multicultural environment which is having 1 group overtake it. It's fundamentally a job of business leaders to set the tone and direction of company culture - and this is one aspect of it. reply quadrifoliate 8 hours agorootparent> Like it or not, it is the dominant culture especially in professional environments. At least sometimes it's not, which is why the OP feels so excluded and is asking for tips on how to navigate the clearly unfamiliar feeling of not being able to just \"fit in\" as part of the dominant culture. The reality is that the US is a melting pot with a lot of subcultures, and you should learn to navigate those subcultures instead of demanding that they conform to some mythical default. Maybe next time the OP should show some curiosity about what their coworkers are joking about, and shyly ask for a seat at the table. I have done it plenty of times. reply fennecfoxy 6 hours agorootparentShouldn't the members of the enclave proactively reach out themselves, then? As the host country has tried to make things more comfortable for them to start with. That seem to be the major difference between Western and non-Western countries; we're more cognisant of things like racism/being excluded and have taken steps to try to resolve it - you do not get the same in many other countries at all. It seems it's much more acceptable to be exclusionary and racist if you're non-white, sometimes. reply miningape 7 hours agorootparentprev> Maybe next time the OP should show some curiosity about what their coworkers are joking about, and shyly ask for a seat at the table. I have done it plenty of times. Yep 100% this is the only decent solution OP has barring leaving the company. It leads to really interesting conversations and you get to learn a lot about a huge portion of the planet's population. Some people go out of their way for these experiences. But it also shouldn't be forced on someone who just wants to collect a paycheck. reply ethbr1 7 hours agorootparentA lot of arguments here are getting caught on the wrong details. It's good to experience new cultures and stretch out of ones comfort zone! But cultural similarity is also the strongest form of bias in office dynamics. So, it's great if people go to Indian restaurants. It's not great if people only go to Indian restaurants. It's not great if people only go to steakhouses. And it's especially not great if colleagues don't make efforts to include less culturally similar colleagues in events, whatever the cultures in question. reply lupusreal 5 hours agorootparentprevWhen an ethnic group makes a tacit decision to form an enclave and exclude others, there is no \"learning to navigate the subculture\". Either you are pushed out or you find the leverage to make them stop doing that. reply 1over137 7 hours agorootparentprev>Non-Hispanic white Americans are not native to the US... The word Hispanic comes from Hispania meaning \"Iberian Peninsula\", which, I have news for you, is in Europe. They are hardly native to the USA either. reply karpatic 7 hours agorootparentIt was an amazing statement to read. I'm glad someone else caught that reply quadrifoliate 6 hours agorootparentI am aware of Hispania, etc. The thing you're perhaps missing is that 'minignape began their comment with \"as a white person...\". That is usually a characterization used by non-Hispanic white people; hence my reply referenced non-Hispanic whites. I also wanted to highlight that they would probably be tolerant of an unfamiliar Hispanic white in-group at their company, but weren't tolerant of a South Asian one. reply karpatic 5 hours agorootparentText makes communication hard. Have a good day bud. reply quadrifoliate 4 hours agorootparent> And that's just false. And rude. Sorry, I'm just going to have to go with the consensus of the majority of scientific study on this one: https://en.wikipedia.org/wiki/Native_Americans_in_the_United.... I'm sure you'll find some like-minded people that share your worldview and are not \"rude\". reply shrubble 7 hours agorootparentprevActually Whites have as much if not more claims to North America than any other group; there’s multiple times that they came to NA, including the Solutreans of about 12000 years ago: https://insider.si.edu/2012/03/ice-age-mariners-from-europe-... ; plus the Viking settlement of 1021; plus the more recent fact of the USA being settled and turning wildernesses into the agricultural and technological powerhouse of today. reply selimthegrim 6 hours agorootparentThat’s funny I don’t see anything about this in the constitution Also, weren’t Solutreans brown skinned? reply kshGa 7 hours agorootparentprevThe dominant culture of Google and Microsoft when they were founded was [...]. Now they have Indian CEOs and companies like Cognizant bring in H1B visas from India. There is nothing organic about that. Personally I have no experience with Indian co-workers, but I do know that Black and Hispanic people do not exclude whites at all. I have only great experiences with them. reply mc32 7 hours agorootparentprevPeople born in the US are native to the US -or are you arguing for de sanguinis citizenship? Chinese and Indians do follow de sanguinis, so maybe you’re making the case? reply lupusreal 8 hours agorootparentprev> Non-Hispanic white Americans are not native to the US, Weird exclusion; Hispanic white Americans aren't native to the US either; their ancestors came from Spain. reply sandworm101 8 hours agorootparentprev>> non-Hispanic white Americans are not native to the US Then to where is it native? Don't deny a culture its existence just because it isn't the first culture to arise within a particular area. Example: Mormon culture and religion is \"native\" the US despite certainly not being the first culture in the area. reply quadrifoliate 7 hours agorootparentI am not the one throwing around the word \"native\" casually. To cite your example, Salt Lake City was founded less than 200 years ago whereas Puebloans have been living in the area for several thousands of years. You can certainly make a claim that Mormon culture is \"native\" to Utah, but I think at 200 v/s 5000+ you can expect that claim to be contested. reply karpatic 7 hours agorootparentFunny enough, the definition has nothing to do with ancestry. \"Native: a person born in a specified place or associated with a place by birth, whether subsequently resident there or not.\" reply miningape 7 hours agorootparentYeah, this is the exact sense in which I wrote it originally - but I sensed the screams of 1000 idiots and wrote \"native\" to try avoid that entire line of conversation. It seems though that my efforts went unnoticed - oh well! reply everdrive 8 hours agorootparentprevI'm not sure what your point is. \"You can't talk about your problem because I have a problem as well\" seems to be all you're saying. A better question would be how to prevent people from excluding each other based on group membership. reply quadrifoliate 7 hours agorootparent> I'm not sure what your point is. That's a good question. I suppose my point is that this is not something you have \"fight back\" against. Lots of people get excluded from social groups in professional settings due to some silly link that their coworkers have with each other. Learning to overcome your lack of cultural commonality with coworkers and and breaking into social groups is something that all of us need to do at some point. In my case — I sucked it up, refused to learn golf but bonded with coworkers over board games; and ate the appetizers at the steakhouse. A tip for OP would be to try doing the equivalent thing in their context. Go up to your Indian coworkers and ask if you can accompany them to the restaurant. I promise you it will be fine. reply ethbr1 7 hours agorootparent> I promise you it will be fine. Just pass on the gulab jamun if you value your liver. reply quadrifoliate 7 hours agorootparentPass on like 90% of the stuff that has fat and sugar. It is not representative of a household Indian meal, but rather of a rare feast. If you go more regularly, a somewhat healthy meal at an Indian restaurant is: - The tandoor chicken (not the one in gravy) - The veggie salads and/or yogurt raita - Whole wheat rotis if you can find them - Any of the vegetables that don't have a ton of cream (cauliflower is one that's reliably dry) I can't pretend that I don't indulge with anything beyond that; but I tend to not be a regular at the Indian restaurants here. reply xienze 7 hours agorootparentprev> Are you seriously asking how to \"fight back\" on Indians going to restaurants and joking with each other? Yes? Or is it just a problem when white people choose to associate with one another at work? reply quadrifoliate 7 hours agorootparentI am saying that neither is a problem. We all have to break into social groups at work and even before that (ever watch \"Mean Girls\"?). This is not a problem specific to any group of people; and OP's fixation is unnecessary in my opinion. reply swasheck 7 hours agorootparentprevi’m sorry for your experience(s) and how frustrating that must be. what you’ve experienced isn’t “right” and while it’s valid that you bring it up as a general concern and experience, it comes across as though you’re justifying how OP experiences their own work environment. as someone who has essentially experienced the same treatment, but from the opposite side, this is an opportunity you had to validate and confirm the presence of such behavior while expanding the scope of its presence. now we’re hung up on restaurants and golf courses and choosing sides instead of discussing the core problem reply AStonesThrow 6 hours agorootparentprev> this is sort of darkly funny to read. I'm hopeful that some light will be shed on how ridiculously dystopian it is to force D.I.E. mandates with posters like \"United Colours of Benetton\" as if \"All Men/Women/Etc Are Created In Test Tubes Equivalent And Interchangeable\" and we Voters of American Progressive Enlightenment must lobby to crush out every aspect of culture that suggests otherwise. https://youtu.be/vvDYuj1Bs6Y?si=sodV00r3eefBoZ79 Harrison Bergeron for ya So it goes. Namaste. reply gosub100 7 hours agorootparentprevyou chose to move to another country. thats the difference reply the_gorilla 7 hours agorootparentprevnext [12 more] [flagged] quadrifoliate 7 hours agorootparentI am just using a signature American phrase[1]. If anything, this is a mark of how assimilated I am! I mean no contempt; I am happy to live in a flyover state. Also, the state did not \"allow me\" to live here, the Federal Government did; as anyone with an elementary knowledge of American government principles should be able to discern. Your response though, seems to show some anti-immigrant anger. I am sorry you feel that way, and hope you find happiness! ---------------------------------------- [1] https://en.wikipedia.org/wiki/Flyover_country reply the_gorilla 7 hours agorootparentWhat do you think the word \"Flyover\" means? You live in it now, they let you in so you're not flying over it. It's obviously derogatory, and even your link says it's derogatory. That'd be like me going to India, and then complaining \"As an American living next to untouchables...\" reply sourcepluck 7 hours agorootparentHere's the relevant quote from the wikipedia that you're misusing: > The origins of the phrases and the attitudes of their supposed users are a source of debate in American culture; the terms are often regarded as pejoratives, but are sometimes \"reclaimed\" and used defensively.[1] So no, it is not \"obviously derogatory\", and the link does not say that. reply fennecfoxy 6 hours agorootparentI'd identify it as self deprecation as a form of social ingratiation, just have to be careful with that as self deprecation can go beyond the boundary of self. reply sourcepluck 3 hours agorootparentThat's a possibility. Here are other possibilities - the US resident who originally commented may have been: - unaware of the phrase's derogatory meaning - aware, but relishing it, as they resent the state and don't like living there - aware, but they think the word has a useful non-derogatory use - aware, and has no strong opinion either way All things which in reality would be legitimate in various circumstances. Speculating in the first place seems silly to me, and only started because one commenter apparently didn't like the idea of a non-US native having a negative opinion about the US so much that they are (pardon my bluntness) a bit overly sensitive on the issue. reply ttyprintk 6 hours agorootparentprevHis/her comment articulately justifies using it. And the context is relevant because Americans assume and observe differences off the coasts. Flyover practically means “does not have a football team” and is just as derogatory as “college town” or “Bible belt”. reply Cthulhu_ 7 hours agorootparentprevWhat do you mean \"allows them\"? Whatever happened to the ideals of freedom and liberty and whatnot in the US? reply the_gorilla 7 hours agorootparentAmerican States have their own culture and identity. The ideals of America was never being an economic zone to house the entire third world, who think the only utility of a state should be to fly over it. reply sourcepluck 6 hours agorootparentAre you aware that the US ranks below Romania, Belarus, Serbia, Cuba and Thailand on the UN's sustainable development report from 2024? It might be appropriate to update your picture of the place to reflect the reality. Your views here seem to be predicated on some notion of the US as a place the entire \"third world\" wants to move to - perhaps you should consider the fact that it's not really top of everyone's list anymore? With some notable exceptions where it may well be top of the list - the obvious example being some third-level institutions there who have prestige and networking opportunities which are hard to beat, if you can afford it. reply miningape 3 hours agorootparent> Are you aware that the US ranks below Romania, Belarus, Serbia, Cuba and Thailand on the UN's sustainable development report from 2024? Are you aware of how shit of a metric that is? It's literally the %age of GDP spent on sustainable energy, so the US could still be spending more than all those countries combined and still have a lower %age. Let's also not forget who gives out the loans for sustainable development, and who sets up the economic incentives. This is also the equivalent of saying \"You're much less likely to get robbed in Africa, they have a faster declining crime rate than Europe.\" As a baseline Europe is safer and it's therefore harder to decrease the crime rate further[0]. Going from 100 murders a day to 89 is not better than going from 10 to 9. [0] I made up this example - no clue if it's true reply ttyprintk 6 hours agorootparentprevThe founding of the American colonies was very much a gutter for economic classes. Warren Buffet wouldn’t exist today without European trash being indentured. reply nelblu 7 hours agoparentprevAs an Indian immigrant I had noticed this very early on too. It made me very uncomfortable when some of us would crack jokes in non-English language while there were others around who didn't understand the language. After first few years I stopped working for companies that had mostly Indians in IT. I must say I have been lucky since then to always find a good mix of diverse backgrounds in my team that it has not been a problem. reply dlahoda 7 hours agorootparentI am from slavic lands, and found out that I prefer not to work within mostly slavic teams, as any other teams with high genetic and cultural cohesions. reply quadrifoliate 7 hours agorootparentprevAs a fellow Indian immigrant, I actually share your discomfort, and always try to be inclusive and diverse in my social groups. Most of my Indian coworkers and friends behave this way — we came to the US to be a part of a mixed culture, so our social groups should be diverse. What makes me uncomfortable is that this inclusivity is increasingly being taken for granted to the point where not having it starts conversations about how to \"fight back\". Even though it's not my preference I don't think there is anything wrong with Indians cracking jokes in a non-English language, or going to a restaurant by themselves. You will find that Americans will mysteriously be far more tolerant of, say, a group of French people talking among themselves in French; and going to a French restaurant as a group. reply zknow 7 hours agorootparentIn public, yeah people probably wouldn't care about french speakers, but in a shared environment I definitely would prefer they speak English. My brother use to have to tell his Dutch-Canadian in-laws to speak English when we were at his house. I'd feel the same way at work. reply didgeoridoo 6 hours agoparentprevI’ve actually had a very different experience — including in my interactions with Cognizant as a vendor. Indian managers acting utterly cruel and abusive to Indian line workers (especially remote or H1B), while being generally kind and accommodating to non-Indian team members. Maybe there was a class/caste thing at play that was totally opaque to me. It was an extremely uncomfortable environment to work in. reply noisy_boy 6 hours agorootparentTribalism and slave mentality can coexist. reply dotnet00 5 hours agoparentprevEven as an Indian, I don't know how that could be tackled, it seems to be kind of systemic. I'd always been around people from all sorts of places, and many of the schools I went to would intentionally keep kids of similar origins apart to force them to mingle with others, so I grew up to prefer being around people from different places over just sticking with other Indians. I've had several experiences of running into people who seem to take pride in being the way you describe. My interpretation has been that they have a chip on their shoulder about not being \"westerners\" and view anyone who is better integrated as being some sort of traitor. For now, I've only had to experience it in school and university. It's been awful every time. Yours is also a sentiment many people have expressed to me about other Indians once they've opened up to me and realized I won't care if they say something that could be racist. reply ErigmolCt 8 hours agoparentprevCultural bonding is natural, but it's critical that it doesn't undermine team spirit or create divisions. Is there any possibility to escalate the issue with your immediate manager? reply eru 7 hours agoparentprev> How to fight back when you see this happening at line management level, but not yet proliferated to upper management? Vote with your feet, if you don't like it? reply mock-possum 24 minutes agoparentprev> to go to Indian restaurants during lunch break Don’t threaten me with a good time reply amazinteresting 8 hours agoparentprevBest course of action is symmetrical response. reply eru 7 hours agorootparentWhat makes you think so? Usually, the sanest recourse in these kinds of situations is to cut your losses and vote with your feet. reply ErigmolCt 8 hours agorootparentprevIt may feel satisfying, but it risks escalating tensions and fostering more division rather than solving the core issue. reply bbor 8 hours agorootparentprevI’m gonna dip into politics a bit for my assertion that “fight racism with racism” isn’t a great move, either for the societal long term or the personal/legal short term. reply zo1 8 hours agorootparentThat is the entire premise of DEI initiatives. It's yet another \"an eye for an eye\". reply skywhopper 8 hours agorootparentThis is a delusional and hateful lie. reply gruez 7 hours agorootparentAre you claiming positive discrimination has not been advanced under the banner of DEI, or that it has but wasn't true DEI? reply kshGa 7 hours agorootparentprevYou could perhaps assume idealism in 2020. Four years later it is clear that DEI is used as a wedge to displace those you do not like and replace them with unskilled believers. reply ethbr1 7 hours agorootparentFrom your comment, I'm guessing you're talking about the first consequence of DEI policies? - Using candidates' loud belief in DEI as a litmus test, even if the candidate themselves has no diverse characteristics - Hiring diverse candidates reply j45 7 hours agorootparentprevDEI is hardly about unskilled alternatives. It’s only about getting people to the table who are equally qualified and capable and overlooked and under represented. Often to the chagrin of most people who are lamenting on it changing. reply zo1 7 hours agorootparentPlease don't take this personally, or as an attack on you. But having seen DEI first-hand, and the \"equally qualified\" people it has actively displaced and disenfranchised, I'd have to say that this view avoids the ugly reality on the ground wrt DEI. There is an entire country currently affected by it's poor implementation, and it's failure is just seen as more reason for efforts to be doubled and for more-extreme quotas to be put in place. reply j45 7 hours agorootparentThere's nothing personal to take. So you shouldn't take it personally. Perhaps, instead we can think about how could it be taken it professionally.. There's equal or better qualified candidates for every position that don't make it to the table because of existing gatekeeping. That would likely have the effect of helping borderline candidates who can fail upwards, maybe do that a little less... or level up. What's curious is the presumption that one persons experience or interpretation (yours) doesn't mean a better perspective, experience can't exist. Is it possible you might not be the only one experiencing DEI? Gatekeeping has been a thing that's existed for a very, very, very long time. Often to the benefit of many of the people complaining about new kinds of space-making that affect gatekeeping that they didn't realize benefitted them. This current wave of DEI is definitely early. It's not perfect. Neither was the gatekeeping that preceded it. Other things that were early got a lot more leeway and understanding. But it can shows what some folks want to see happen (or not happen) one way or the other. reply zo1 8 hours agorootparentprevHave you been on the receiving end of DEI? The same way a minority has been on the receiving end of racism? We can all wax lyrical and paint pretty pictures about the noble goals of DEI. But till we get an equal and fair world, the ugly picture is that DEI starts by dividing the pie into smaller parts, and taking from one to give to another, instead of making the pie bigger for everyone. And if history is anything to go by, most \"DEI\"-like efforts never ever reach that end-goal. They perpetuate indefinitely until they create yet another oppressed or previously-disadvantaged class, and the cycle will just repeat. reply ethbr1 7 hours agorootparentThe biggest problem with DEI policies in practice was that companies entrusted HR to implement them. HR at most companies doesn't do nuance or complexity. reply j45 7 hours agorootparentprevDEI is more about getting equally qualified candidates to the table that are ignored or missed. Yes that makes it more competitive. Or less-competitive at an advantage to some and both others if it stayed the same way. reply gruez 7 hours agorootparentThat's the motte of the DEI motte-and-bailey. The bailey contains policies like race based quotas or preferential treatment. reply j45 7 hours agorootparentRace based quotas seem like very US things. Theres lots of ways to improve hiring. If holding space for equally qualified candidates is preferential treatment, is it having to exist because of the gatekeeping that existed before it? Still, it remains important for any practice to do a good job of helping everyone understand how it's working better. Too often companies jump to signal trends and keep doing whatever they were all along. Like organizations who's leadership looks nothing like the pool of qualified candidates in the respective country. reply zo1 7 hours agorootparentprevOh that may be the \"on-paper\" goal of DEI, and I'd be all for it if it stayed that way; it sounds very very fair. But it never stays that way, and quotas enter the picture before long. reply j45 7 hours agorootparentTricky and slippery rationalizations are something I try to be mindful of. I put forward a single simple point. Since it's resonating in a response, it might be worth considering why, and see how our viewpoints form and how much of it might be rooted in isolating emotions like fear. In discussing, an open mind to me is one that can openly entertain a viewpoint that isn't their own, and seeing if they're open to growing or changing their viewpoint. Maybe.. the way your country does DEI is trying to do the opposite of the race based separation it did prior and doesn't know if any other levers exist? One nice thing is you're inheriting the world and can help make it the way you think it should be instead of wanting to be a passive beneficiary of past gatekeeping baselines. Hypothetically speaking... could openly entertain a viewpoint that isn't our own.. be similar to believing software could be better, if it was only improved, by trying to improve ourselves and building software better? reply the_gorilla 7 hours agorootparentprev> DEI is more about getting equally qualified candidates to the table that are ignored or missed. This is the first time I've heard that. The normal party line is that, yes, they're worse at the job but it's because they never got the opportunity to learn. You can observe in colleges that DEI-appointed students do massively worse overall despite probably being given even more leeway than normal students. reply j45 7 hours agorootparentWow, ok. So that kind of idea is really completely new, so it can't be valid? I think it's pretty easy to go learn the spectrum of DEI. The world is generally run with gatekeeping, which means withholding access to opportunity to improve one's life to a selected group for a long time. It's possible that your country may codify gatekeeping and privilege, and the only way they may know how is to do the same thing in the opposite way. It might not be a good way of holding space for qualified candidates to get to the same table, or even, not allowing an average person to \"get a chance\" to fail upwards except if you're from one background. The relevance of DEI shouldn't be held exclusively with its implementation at any given time, as long as it's improving. Kind of like software, maybe. I'm not sure where your observation is based on - happy to learn and read from any studies though beyond anecdotal differences. It would be like generalizing that lots of rich kids end up doing nothing as well after their parents pay for their way into and school. Doesn't make it true as a generalization of everyone though. reply the_gorilla 7 hours agorootparent> Wow, ok. So that kind of idea is really completely new, so it can't be valid? Theoretically it could be, but you should be aware if your argument directly contradicts years of other people advocating for DEI. The idea being sold isn't \"we need jobs to be more merit based\" but \"we don't have enough merit and have to discriminate\". > It's possible that your country may codify gatekeeping and privilege, and the only way they may know how is to do the same thing in the opposite way. My country, the US, codifies that you're not allowed to racially discriminate. Somehow this doesn't stop people from declaring that we must explicitly perform racially discrimination in order to offset some perceived discrimination. > The relevance of DEI shouldn't be held exclusively with its implementation at any given time, as long as it's improving. Kind of like software, maybe. The relevance is that I'm a race it explicitly disadvantages, and so it my family. It's illegal, racial discrimination is apparently immoral when it's done to anyone else, and it needs to go. > I'm not sure where your observation is based on - happy to learn and read from any studies though beyond anecdotal differences. Go look at medical schools. High scoring Whites and Asians are about as likely to get in as extremely poorly performing Black students. https://www.aei.org/wp-content/uploads/2017/06/med1.jpg?x850... reply j45 6 hours agorootparentThanks for sharing. Your country, explicitly codifies race based tracking in universities, long before this DEI wave. It's so incredibly wack. Other countries have better language to identify anything unique and under-represented, and maybe are less awkward but still awkward at it. Historically, the creation of places of higher learning in the US were not created for women to be accepted, let alone people of color. It's helpful to take a more historical look at how those pesky college application forms got the checkboxes they did, and how they were added at the moments of change in the particular decade. Imagine all the people who don't get counted. What kind of system names an entire group of people a whole continent like Asia? :) There is historical merit to people not being counted... not counting... or existing.. or qualifying as human enough to vote. When it comes to data.. what gets measured, gets managed.. and maybe some of the wording of what got measured had some unconscious bias. It's also not about whataboutism seeking a perfect solution to undermine change that is trying to be better for more people. I have some international experience in the academic industry and student data collection, management, etc. Race based data in the US always stands out compared to other countries. While it's true that disadvantaged children regardless of background can have similar challenges, its no contest that people of color experience it so very much more. Discrimination starts with the contract that there is a privileged contract place prior to it being adjusted for said offence. There are awful references to suggesting people of color \"work harder\" .. maybe that is advice for everyone? I'm not going to participate in taking shots at any one group of students, especially black students who are way more disadvantaged per capita than any other. About the med-school link - isn't it a little dated (and risking a stereotype) to believe that the best grades are the only thing important about getting into med school? Great doctors are well rounded people who connect with and help all walks of life - understanding people is a key skill beyond maniacal memorization and regurgitation for years of study to only stop and impossibly be behind research after graduation. reply Cthulhu_ 7 hours agorootparentprevIt is if you look at it in a divisive, \"us\" vs \"them\" fashion. However, it's to try and make up for hundreds or more years of active suppression and exclusion from society and opportunities. Ask yourselves, who paid for your education? Was it pulled up by the bootstraps rags-to-riches, or did you get help from e.g. family? Where did you grow up, and how did that contribute to your current career / life? Now imagine you didn't have those opportunities, because your family (going back generations) never was able to build up generational wealth and comfort. DEI is an attempt to make up for that. Is it ideal? No. Does it come across as discrimination to the priviledged people / classes? Sure. Does it personally affect you? Probably not, but I don't know you (generalised you, the reader). That said, if you don't like DEI, vote and act accordingly. Work to make sure everyone earns a liveable wage, owns a house, gets a good education and consequent steady job opportunities regardless of familial wealth. Be and act anti-racist and anti-classist, because it's not enough to simply \"not be racist\". reply bryanrasmussen 7 hours agoparentprev>All these culture things (except for the first regarding PO which is spit-in-the-face level of unprofessionalism) add up, you said it yourself, the first one was unprofessional and should be avoided and may be cause for action. The other things, learn to live as the minority in a group and accept what the group values. I mean maybe your colleagues follow suit about eating Indian food because Indian food is delicious? \"Hey you guys know a good Indian food place around here - that's great!\" would be my response. reply throwaway48476 7 hours agorootparentIt sounds like they do have a group with its own values. reply giantg2 8 hours agoparentprevImagine being autistic and rarely having that bonding you talk about, and certainly not with entire teams. reply trhway 8 hours agoparentprevIt is hard only about the first 10 years. Then you'll learn to stop worrying and love that. And the whole country will get closer to that with the first Indian President. And there will also be the first Chinese, and the first Hispanic Presidents. World is changing. And the \"fighting back\" is like pissing against the wind. reply ThinkBeat 8 hours agoparentprevFor some reason that sounds like how white men are described in general, in many quarters now. reply thinkingtoilet 7 hours agoparentprevNow imagine being a minority. You're white colleagues making white jokes. Hiring only white people. Going to white restaurants. Etc... I like when a privileged class gets a contact high of the real world for so many and can't handle it. Employment discrimination is one thing. Having friends and going to lunch is another. For the record, as a white man I've never met, in my entire life, a single Indian person who refused to be friends with me and was only friends with other Indian people. I'm sure they exist. There are ass holes everywhere. This is not an epidemic. However, your response reeks of privilege. \"How is it possible I'm not on top?!?!?\" reply mellosouls 6 hours agorootparentI think thats an unfair reading of the comment you replied to - racist and exclusionary behaviour is rightly no longer tolerated in policy intent (if not always in actuality) at professional corporations. The issue highlighted here is that it seems at times that there are exceptions that people are afraid to call out and criticise, especially at lower levels. reply ttyprintk 6 hours agorootparentprevI don’t have an opinion if someone is wondering whether one of those cultures is more worthwhile to seek acceptance. But I learned a lot about exclusion from an Indian coworker who was not heterosexual. reply bakugo 7 hours agorootparentprev> You're white colleagues making white jokes. Hiring only white people. Going to white restaurants. Pretty sure white people aren't mass immigrating to India and doing this there. reply throwawayha 6 hours agorootparentEat Pray Love? reply elisharobinson 6 hours agoprevSome of the reasons why this is happening : -> Getting an assignment in the US is viewed as a reward for employees who work in the company for 5+ years. -> H1B visa employess are given a 50% minimum discount to market rate. cause lol what are you gonna do . Quit and go back in the Queue for H1B. -> nepotism / favoritism self explanatory. -> Are more willing to work 40+ hours a week. Less likely to take vacations. These are the observations of myself and members of my family who are from India. reply infecto 6 hours agoparentIs that discount really true? There is a definite discount but I have never seen 50%, all of this data is online, you can see the payroll for H1B employees. https://h1bdata.info/ reply elisharobinson 6 hours agorootparentthanks for the ref , but it kinda proves the point . your source suggests the avg comp for senior software engineer is 100-150 K . on glass door it 178-263 . https://www.glassdoor.com/Salaries/senior-software-engineer-... . reply infecto 6 hours agorootparentYour analysis is flawed. Its much better to run through a FAANG and compare to levels.fyi. I just did for Meta and the base salary numbers are within the ranges I saw on Levels. Actual level is not outlined so its a broad range. These numbers are set using prevailing wage calculations for a geographical area and role. There is an incentive for companies to figure out ways to get it as low as possible but I have never seen 50% myself, that is a hard stretch to pass the DOL. reply elisharobinson 5 hours agorootparentso your telling me my analysis is flawed because i didnt take META as a base. MY guy the article is about CTS. NOT META. H1B is to tech what H2A is for agriculture . Yall want to Drive the combine ( Managment, Software development, VC ) but dont want to do the dirty work ( QA, Devops, IT admin ) reply infecto 5 hours agorootparent>Yall want to Drive the combine ( Managment, Software development, VC ) but dont want to do the dirty work ( QA, Devops, IT admin ) Woah, less caps and racist thoughts and more critical thinking please. I am saying taking a simple average between the two like you did is flawed for a number of reasons. 1) You don't even have an apples to apples comparison. H1B data is based on prevailing wage which is just base pay. So taking your link, 129K - $171K/yr comparing to your other data, 100-150k. Lets take a simple average. and say the glassdoor wage is 150k, that means 50% minimum, remember you said minimum, is 75k. That does not really track with 125k. 2) Prevailing wage is based not only on the role but the geographical location too. An overall average between those two datasets is a very rough guide and I think its better to do direct comparisons at a company level or regional level. Again there is still a lot of normalization and cleanup to do in the datasets, the H1B data is not great for analysis right out of the gate. Your case of a minimum 50% reduction in base pay is hard to defend except in outlier cases. If it was closer to 25% I think its a much easier number to defend. reply elisharobinson 4 hours agorootparentso you would admit that a 25% diff in base pay would be plausible . so would you also admit a 25% diff in benifits would also be plausible . granted the total diff would work out to somewhere around 30~40% range , but i still hold if we are comparing true apple to apple and we exclude FAANG , we would most likely get in the 50% range. Some emphasis should be made on the total percentage of H1B applicants 75% are Indian and at-least ~90% of those are in the TECH working for CTS, TCS both are consulting firms They dont sell any products or services (and dont have offices in SF). They survive by giving the lowest bid for a service . And when the minimum wage is the lower bound why bother paying any higher (60-75% discount), This was the case prior to trump. After Trump H1Bs became scarce and to justify it you needed \"highly skilled\" employees and they made the paygap more justifiable to 45-50%. So i still hold my original statement H1B is for tech what H2A is for agriculture. reply infecto 1 hour agorootparent> so you would admit that a 25% diff in base pay would be plausible . so would you also admit a 25% diff in benifits would also be plausible . granted the total diff would work out to somewhere around 30~40% range , but i still hold if we are comparing true apple to apple and we exclude FAANG , we would most likely get in the 50% range. 25% of the pieces summed would be 25% of the whole. I am not sure how we hand wave to 30-40% and then jump to 50%. Your 50% minimum is still dubious but hey go with it! reply victor106 6 hours agorootparentprevI have personally experienced this in all my previous projects and my current one. H1B does suppress wages as employees don’t care how much they are paid. They only care that they stay on H1 so they get their green cards. However, one they get their green card they feel the same that H1B’s suppress wages…lol… reply infecto 6 hours agorootparentEh, your not wrong but my counter point is when I have looked up people before in the public data, their salary was in line what I and others were making in the same role. The H1B data does not list name but it is detailed enough that you can generally guess who it is in small-medium size companies. reply pk_anon123 8 hours agoprevI just posted this comment with a new account to check how easily hearsay influences people's opinions with no backing, whatsoever. I hope this thread will stay up as a reminder to stay civil and not get carried away by clouded judgement. reply pm90 8 hours agoparentHow is this discrimination manifested? How do these managers know your country of origin? reply mattmaroon 7 hours agorootparentAn Indian person can spot a Pakistani (and vice versa) easily. There’s a good bit of historical enmity between the two nations. reply Cthulhu_ 7 hours agorootparentDoes this apply to castes as well? I mean in western Europe you can often pick out the classism based on how people dress, have their hair and what their accent is (I'm very obvious because of my \"provincial\" accent and terrible sense of style, lol). Classism in the Netherlands isn't too bad though, unless you're from the old aristocracies (they live in certain areas, go to private schools, often have archaic letter combos like Y, AE, CK, or apostrophes in their multi-word surnames, etc [1]), they definitely live on their own island. Often end up in politics, too, because so it goes. [1] https://nl.wikipedia.org/wiki/Lijst_van_Nederlandse_adellijk... reply consp 6 hours agorootparent> Classism in the Netherlands isn't too bad though Wait until they figure out you earn less than them. \"You cannot understand [rich people bs 'struggle']\" I've had the misfortune of hearing over and over again, them showing a complete lack of any form of empathy for anyone except their own class. reply dartharva 6 hours agorootparentprevNo? I am an Indian living in India, and if a Pakistani person were to live next door I wouldn't be able to make it out unless he told me himself. reply mattmaroon 5 hours agorootparentIm neither but I can tell a Pakistani accent from an Indian one. Is that not the case there? And you would certainly know if you were working with someone, at least here. I suppose I have spent a lot of time with Indian Americans but not as much with Indians in India (just zoom calls) so that’s interesting. reply dartharva 9 minutes agorootparentPakistani accents often perfectly coincide with Punjabi and UP (Indian states) accents. reply gruez 7 hours agorootparentprev>>A lot of hatred passes unnoticed beacuse of the lumping together of the South Asian immigrants in all discussions. >How do these managers know your country of origin? The irony. reply lazide 7 hours agorootparentprevPakistani’s are almost always Muslim (due to Partition [https://en.m.wikipedia.org/wiki/Partition_of_India]), and will often act and speak differently in several telltale ways for anyone who knows what to look for. Both given and family/surnames are also often telltales. And if you think Casteism is a bitch, Pakistani/Indian (or Indian Hindu vs Indian Muslim) is a bitch and a half. Just try to apply for a visa for India and see how many times they ask if you or any of your relatives are from Pakistan to get a taste. Edit: ooh, downvotes away. Hah! No better way to tell when you hit a nerve than the torches come out. reply onetimeacc123 7 hours agoparentprevI hear you, brother. I am an Indian Muslim and I have essentially stopped considering any company that has a considerable number of Indians. I actively filter these companies out when searching for jobs. The daily micro-aggressions are detrimetal to my mental health, and I have finally come to terms that this is beyond my power to reform, hence my decision to simply ignore. This is sad, really really sad. reply xeanotods 2 hours agorootparentI can relate. Countless awful experiences but avoidance seems futile. Filtering out these work environments pre-hire is difficult and also, environments change. I've been cut out of information chains by the \"Indian mafia\" and sidelined in financially impacting ways. It's a terrible feeling when you first realize how outgunned you are and how cohesive they are in their tactics. I find it more scary than sad actually because of how unconstrained by empathy and ethics their mob approach is towards anyone they perceive as a challenge to their hegemony. It's actually one of my biggest issues with working in the tech sector and I don't see any way around it. reply eru 7 hours agoprevSince there's a lot of negativity about Indians in the comments here: I just worked for a startup with many Indians (including the founder). It was all fine, and I liked my coworkers. I can understand that other people had different experiences: companies and people differ. For context, everything was remote, and I worked from Singapore. I'm not India, but I do like to prepone my meetings. reply p_l 6 hours agoparentThe big problem are WITCH companies (of which \"C\" is for Cognizant usually) and those that, through osmosis or purposeful actions, copy their practices. The contracting companies probably single-handed build and maintain lots of racist stereotypes against indians, because the level of service those companies offer is always unmitigated failure, even internally. I once accidentally stumbled into working on a contract through WiPro myself, and it was a horrific experience. The way I see it, the line workers that end up the target of ire and stereotypes are just victims of actions taken way above them. The people who can, apparently avoid working for them, or use every opportunity to escape. Those who can't yet but have ambition for more, apparently use every opportunity to escape - whether it is by jumping ship on H1B, or otherwise. Those currently stuck are in no way encouraged to do a good job, and for obvious cost cutting you get people who were in no way prepared to do the job. Occasionally a client will get angry enough and they will pull an actually skilled person to smooth the ruffled feathers, someone they usually trot for dog&pony show when winning the contract as example of who is supposed to work on it. But the \"important\" people, upper management who decide to contract with WITCH, and the upper management of WITCH who run such strategy, they all profit. The managers and/or C-level who outsource sight-unseen with no quality enforcement disappear with their rewards before institutional inertia stops papering over their decisions. The WITCH companies pocket huge amounts of money while paying pittance to line workers and providing worse than zero service. reply eru 4 hours agorootparentYeah, that sounds pretty dreadful. A few jobs ago, when I was working for a third-tier legacy bank, my manager joked that all the outsourcing they were doing was actually really useful and pragmatic, because it means they can fail their IT projects cheaper. (The implication being that their projects were by and large inevitably doomed anyway.) reply seper8 9 hours ago [flagged]prevnext [41 more] What a surprise, racism is imbedded in Indian culture... reply dang 1 hour agoparentPlease don't start hellwars on HN. It's not what this site is for, and destroys what it is for. https://news.ycombinator.com/newsguidelines.html reply bravetraveler 9 hours agoparentprevAs a casual observer, their culture seems to include considerable classism; to the degree that 'where are you from?' appears to be an extremely common and loaded question [saved for the locals]. I only recognize it because it exists in mine, not a judgement. reply DebtDeflation 8 hours agorootparentDon't tiptoe around the topic. By class you mean caste. As a white guy who has worked in tech for 25+ year",
    "originSummary": [
      "A jury determined that Cognizant discriminated against non-Indian employees, favoring Indian workers on H-1B visas, leading to claims of unfair treatment and termination.",
      "Cognizant intends to appeal the verdict, asserting its commitment to diversity and non-discrimination, despite the jury's recommendation for punitive damages.",
      "The case underscores concerns with the H-1B visa process, including allegations of securing visas for non-existent jobs, potentially disadvantaging U.S. workers."
    ],
    "commentSummary": [
      "Cognizant was found guilty of discriminating against non-Indian employees, raising concerns about cultural biases and workplace dynamics.",
      "The discussion explores how cultural differences, such as collectivism versus individualism, may influence managerial decisions and lead to bias.",
      "This situation has sparked a broader conversation about the impact of outsourcing, the need for inclusivity, and the challenges of integrating diverse cultural settings in global workforces."
    ],
    "points": 346,
    "commentCount": 319,
    "retryCount": 0,
    "time": 1728457556
  },
  {
    "id": 41782534,
    "title": "On the Nature of Time",
    "originLink": "https://writings.stephenwolfram.com/2024/10/on-the-nature-of-time/",
    "originBody": "Contents Top The Computational View of Time The Role of the Observer Multiple Threads of Time Time in the Ruliad So What in the End Is Time? On the Nature of Time On the Nature of Time October 8, 2024 The Computational View of Time Time is a central feature of human experience. But what actually is it? In traditional scientific accounts it’s often represented as some kind of coordinate much like space (though a coordinate that for some reason is always systematically increasing for us). But while this may be a useful mathematical description, it’s not telling us anything about what time in a sense “intrinsically is”. We get closer as soon as we start thinking in computational terms. Because then it’s natural for us to think of successive states of the world as being computed one from the last by the progressive application of some computational rule. And this suggests that we can identify the progress of time with the “progressive doing of computation by the universe”. But does this just mean that we are replacing a “time coordinate” with a “computational step count”? No. Because of the phenomenon of computational irreducibility. With the traditional mathematical idea of a time coordinate one typically imagines that this coordinate can be “set to any value”, and that then one can immediately calculate the state of the system at that time. But computational irreducibility implies that it’s not that easy. Because it says that there’s often essentially no better way to find what a system will do than by explicitly tracing through each step in its evolution. In the pictures on the left there’s computational reducibility, and one can readily see what state will be after any number of steps t. But in the pictures on the right there’s (presumably) computational irreducibility, so that the only way to tell what will happen after t steps is effectively to run all those steps: And what this implies is that there’s a certain robustness to time when viewed in these computational terms. There’s no way to “jump ahead” in time; the only way to find out what will happen in the future is to go through the irreducible computational steps to get there. There are simple idealized systems (say with purely periodic behavior) where there’s computational reducibility, and where there isn’t any robust notion of the progress of time. But the point is that—as the Principle of Computational Equivalence implies—our universe is inevitably full of computational irreducibility which in effect defines a robust notion of the progress of time. The Role of the Observer That time is a reflection of the progress of computation in the universe is an important starting point. But it’s not the end of the story. For example, here’s an immediate issue. If we have a computational rule that determines each successive state of a system it’s at least in principle possible to know the whole future of the system. So given this why then do we have the experience of the future only “unfolding as it happens”? It’s fundamentally because of the way we are as observers. If the underlying system is computationally irreducible, then to work out its future behavior requires an irreducible amount of computational work. But it’s a core feature of observers like us that we are computationally bounded. So we can’t do all that irreducible computational work to “know the whole future”—and instead we’re effectively stuck just doing computation alongside the system itself, never able to substantially “jump ahead”, and only able to see the future “progressively unfold”. In essence, therefore, we experience time because of the interplay between our computational boundedness as observers, and the computational irreducibility of underlying processes in the universe. If we were not computationally bounded, we could “perceive the whole of the future in one gulp” and we wouldn’t need a notion of time at all. And if there wasn’t underlying computational irreducibility there wouldn’t be the kind of “progressive revealing of the future” that we associate with our experience of time. A notable feature of our everyday perception of time is that it seems to “flow only in one direction”—so that for example it’s generally much easier to remember the past than to predict the future. And this is closely related to the Second Law of thermodynamics, which (as I’ve argued at length elsewhere) is once again a result of the interplay between underlying computational irreducibility and our computational boundedness. Yes, the microscopic laws of physics may be reversible (and indeed if our system is simple—and computationally reducible—enough of this reversibility may “shine through”). But the point is that computational irreducibility is in a sense a much stronger force. Imagine that we prepare a state to have orderly structure. If its evolution is computationally irreducible then this structure will effectively be “encrypted” to the point where a computationally bounded observer can’t recognize the structure. Given underlying reversibility, the structure is in some sense inevitably “still there”—but it can’t be “accessed” by a computationally bounded observer. And as a result such an observer will perceive a definite flow from orderliness in what is prepared to disorderliness in what is observed. (In principle one might think it should be possible to set up a state that will “behave antithermodynamically”—but the point is that to do so would require predicting a computationally irreducible process, which a computationally bounded observer can’t do.) One of the longstanding confusions about the nature of time has to do with its “mathematical similarity” to space. And indeed ever since the early days of relativity theory it’s seemed convenient to talk about “spacetime” in which notions of space and time are bundled together. But in our Physics Project that’s not at all how things fundamentally work. At the lowest level the state of the universe is represented by a hypergraph which captures what can be thought of as the “spatial relations” between discrete “atoms of space”. Time then corresponds to the progressive rewriting of this hypergraph. And in a sense the “atoms of time” are the elementary “rewriting events” that occur. If the “output” from one event is needed to provide “input” to another, then we can think of the first event as preceding the second event in time—and the events as being “timelike separated”. And in general we can construct a causal graph that shows the dependencies between different events. So how does this relate to time—and spacetime? As we’ll discuss below, our everyday experience of time is that it follows a single thread. And so we tend to want to “parse” the causal graph of elementary events into a series of slices that we can view as corresponding to “successive times”. As in standard relativity theory, there typically isn’t a unique way to assign a sequence of such “simultaneity surfaces”, with the result that there are different “reference frames” in which the identifications of space and time are different. The complete causal graph bundles together what we usually think of as space with what we usually think of as time. But ultimately the progress of time is always associated with some choice of successive events that “computationally build on each other”. And, yes, it’s more complicated because of the possibilities of different choices. But the basic idea of the progress of time as “the doing of computation” is very much the same. (In a sense time represents “computational progress” in the universe, while space represents the “layout of its data structure”.) Very much as in the derivation of the Second Law (or of fluid mechanics from molecular dynamics), the derivation of Einstein’s equations for the large-scale behavior of spacetime from the underlying causal graph of hypergraph rewriting depends on the fact that we are computationally bounded observers. But even though we’re computationally bounded, we still have to “have something going on inside”, or we wouldn’t record—or sense—any “progress in time”. It seems to be the essence of observers like us—as captured in my recent Observer Theory—that we equivalence many different states of the world to derive our internal perception of “what’s going on outside”. And at some rough level we might imagine that we’re sensing time passing by the rate at which we add to those internal perceptions. If we’re not adding to the perceptions, then in effect time will stop for us—as happens if we’re asleep, anesthetized or dead. It’s worth mentioning that in some extreme situations it’s not the internal structure of the observer that makes perceived time stop; instead it’s the underlying structure of the universe itself. As we’ve mentioned, the “progress of the universe” is associated with successive rewriting of the underlying hypergraph. But when there’s been “too much activity in the hypergraph” (which physically corresponds roughly to too much energy-momentum), one can end up with a situation in which “there are no more rewrites that can be done”—so that in effect some part of the universe can no longer progress, and “time stops” there. It’s analogous to what happens at a spacelike singularity (normally associated with a black hole) in traditional general relativity. But now it has a very direct computational interpretation: one’s reached a “fixed point” at which there’s no more computation to do. And so there’s no progress to make in time. Multiple Threads of Time Our strong human experience is that time progresses as a single thread. But now our Physics Project suggests that at an underlying level time is actually in effect multithreaded, or, in other words, that there are many different “paths of history” that the universe follows. And it is only because of the way we as observers sample things that we experience time as a single thread. At the level of a particular underlying hypergraph the point is that there may be many different updating events that can occur, and each sequence of such updating event defines a different “path of history”. We can summarize all these paths of history in a multiway graph in which we merge identical states that arise: But given this underlying structure, why is it that we as observers believe that time progresses as a single thread? It all has to do with the notion of branchial space, and our presence within branchial space. The presence of many paths of history is what leads to quantum mechanics; the fact that we as observers ultimately perceive just one path is associated with the traditionally-quite-mysterious phenomenon of “measurement” in quantum mechanics. When we talked about causal graphs above, we said that we could “parse” them as a series of “spacelike” slices corresponding to instantaneous “states of space”—represented by spatial hypergraphs. And by analogy we can similarly imagine breaking multiway graphs into “instantaneous slices”. But now these slices don’t represent states of ordinary space; instead they represent states of what we call branchial space. Ordinary space is “knitted together” by updating events that have causal effects on other events that can be thought of as “located at different places in space”. (Or, said differently, space is knitted together by the overlaps of the elementary light cones of different events.) Now we can think of branchial space as being “knitted together” by updating events that have effects on events that end up on different branches of history. (In general there is a close analogy between ordinary space and branchial space, and we can define a multiway causal graph that includes both “spacelike” and “branchlike” directions—with the branchlike direction supporting not light cones but what we can call entanglement cones.) So how do we as observers parse what’s going on? A key point is that we are inevitably part of the system we’re observing. So the branching (and merging) that’s going on in the system at large is also going on in us. So that means we have to ask how a “branching mind” will perceive a branching universe. Underneath, there are lots of branches, and lots of “threads of history”. And there’s lots of computational irreducibility (and even what we can call multicomputational irreducibility). But computationally bounded observers like us have to equivalence most of those details to wind up with something that “fits in our finite minds”. We can make an analogy to what happens in a gas. Underneath, there are lots of molecules bouncing around (and behaving in computationally irreducible ways). But observers like us are big compared to molecules, and (being computationally bounded) we don’t get to perceive their individual behavior, but only their aggregate behavior—from which we extract a thin set of computationally reducible “fluid-dynamics-level” features. And it’s basically the same story with the underlying structure of space. Underneath, there’s an elaborately changing network of discrete atoms of space. But as large, computationally bounded observers we can only sample aggregate features in which many details have been equivalenced, and in which space tends to seem continuous and describable in basically computationally reducible ways. So what about branchial space? Well, it’s basically the same story. Our minds are “big”, in the sense that they span many individual branches of history. And they’re computationally bounded so they can’t perceive the details of all those branches, but only certain aggregated features. And in a first approximation what then emerges is in effect a single aggregated thread of history. With sufficiently careful measurements we can sometimes see “quantum effects” in which multiple threads of history are in evidence. But at a direct human level we always seem to aggregate things to the point where what we perceive is just a single thread of history—or in effect a single thread of progression in time. It’s not immediately obvious that any of these “aggregations” will work. It could be that important effects we perceive in gases would depend on phenomena at the level of individual molecules. Or that to understand the large-scale structure of space we’d continually be having to think about detailed features of atoms of space. Or, similarly, that we’d never be able to maintain a “consistent view of history”, and that instead we’d always be having to trace lots of individual threads of history. But the key point is that for us to stay as computationally bounded observers we have to pick out only features that are computationally reducible—or in effect boundedly simple to describe. Closely related to our computational boundedness is the important assumption we make that we as observers have a certain persistence. At every moment in time, we are made from different atoms of space and different branches in the multiway graph. Yet we believe we are still “the same us”. And the crucial physical fact (that has to be derived in our model) is that in ordinary circumstances there’s no inconsistency in doing this. So the result is that even though there are many “threads of time” at the lowest level—representing many different “quantum branches”—observers like us can (usually) successfully still view there as being a single consistent perceived thread of time. But there’s another issue here. It’s one thing to say that a single observer (say a single human mind or a single measuring device) can perceive history to follow a single, consistent thread. But what about different human minds, or different measuring devices? Why should they perceive any kind of consistent “objective reality”? Essentially the answer, I think, is that they’re all sufficiently nearby in branchial space. If we think about physical space, observers in different parts of the universe will clearly “see different things happening”. The “laws of physics” may be the same—but what star (if any) is nearby will be different. Yet (at least for the foreseeable future) for all of us humans it’s always the same star that’s nearby. And so it is, presumably, in branchial space. There’s some small patch in which we humans—with our shared origins—exist. And it’s presumably because that patch is small relative to all of branchial space that all of us perceive a consistent thread of history and a common objective reality. There are many subtleties to this, many of which aren’t yet fully worked out. In physical space, we know that effects can in principle spread at the speed of light. And in branchial space the analog is that effects can spread at the maximum entanglement speed (whose value we don’t know, though it’s related by Planck unit conversions to the elementary length and elementary time). But in maintaining our shared “objective” view of the universe it’s crucial that we’re not all going off in different directions at the speed of light. And of course the reason that doesn’t happen is that we don’t have zero mass. And indeed presumably nonzero mass is a critical part of being observers like us. In our Physics Project it’s roughly the density of events in the hypergraph that determines the density of energy (and mass) in physical space (with their associated gravitational effects). And similarly it’s roughly the density of events in the multiway graph (or in branchial graph slices) that determines the density of action—the relativistically invariant analog of energy—in branchial space (with its associated effects on quantum phase). And though it’s not yet completely clear how this works, it seems likely that once again when there’s mass, effects don’t just “go off at the maximum entanglement speed in all directions”, but instead stay nearby. There are definitely connections between “staying at the same place”, believing one is persistent, and being computationally bounded. But these are what seem necessary for us to have our typical view of time as a single thread. In principle we can imagine observers very different from us—say with minds (like the inside of an idealized quantum computer) capable of experiencing many different threads of history. But the Principle of Computational Equivalence suggests that there’s a high bar for such observers. They need not only to be able to deal with computational irreducibility but also multicomputational irreducibility, in which one includes both the process of computing new states, and the process of equivalencing states. And so for observers that are “anything like us” we can expect that once again time will tend to be as we normally experience it, following a single thread, consistent between observers. (It’s worth mentioning that all of this only works for observers like us “in situations like ours”. For example, at the “entanglement horizon” for a black hole—where branchially-oriented edges in the multiway causal graph get “trapped”—time as we know it in some sense “disintegrates”, because an observer won’t be able to “knit together” the different branches of history to “form a consistent classical thought” about what happens.) Time in the Ruliad In what we’ve discussed so far we can think of the progress of time as being associated with the repeated application of rules that progressively “rewrite the state of the universe”. In the previous section we saw that these rules can be applied in many different ways, leading to many different underlying threads of history. But so far we’ve imagined that the rules that get applied are always the same—leaving us with the mystery of “Why those rules, and not others?” But this is where the ruliad comes in. Because the ruliad involves no such seemingly arbitrary choices: it’s what you get by following all possible computational rules. One can imagine many bases for the ruliad. One can make it from all possible hypergraph rewritings. Or all possible (multiway) Turing machines. But in the end it’s a single, unique thing: the entangled limit of all possible computational processes. There’s a sense in which “everything can happen somewhere” in the ruliad. But what gives the ruliad structure is that there’s a definite (essentially geometrical) way in which all those different things that can happen are arranged and connected. So what is our perception of the ruliad? Inevitably we’re part of the ruliad—so we’re observing it “from the inside”. But the crucial point is that what we perceive about it depends on what we are like as observers. And my big surprise in the past few years has been that assuming even just a little about what we’re like as observers immediately implies that what we perceive of the ruliad follows the core laws of physics we know. In other words, by assuming what we’re like as observers, we can in effect derive our laws of physics. The key to all this is the interplay between the computational irreducibility of underlying behavior in the ruliad, and our computational boundedness as observers (together with our related assumption of our persistence). And it’s this interplay that gives us the Second Law in statistical mechanics, the Einstein equations for the structure of spacetime, and (we think) the path integral in quantum mechanics. In effect what’s happening is that our computational boundedness as observers makes us equivalence things to the point where we are sampling only computationally reducible slices of the ruliad, whose characteristics can be described using recognizable laws of physics. So where does time fit into all of this? A central feature of the ruliad is that it’s unique—and everything about it is “abstractly necessary”. Much as given the definition of numbers, addition and equality it’s inevitable that one gets 1 + 1 = 2, so similarly given the definition of computation it’s inevitable that one gets the ruliad. Or, in other words, there’s no question about whether the ruliad exists; it’s just an abstract construct that inevitably follows from abstract definitions. And so at some level this means that the ruliad inevitably just “exists as a complete thing”. And so if one could “view it from outside” one could think of it as just a single timeless object, with no notion of time. But the crucial point is that we don’t get to “view it from the outside”. We’re embedded within it. And, what’s more, we must view it through the “lens” of our computational boundedness. And this is why we inevitably end up with a notion of time. We observe the ruliad from some point within it. If we were not computationally bounded then we could immediately compute what the whole ruliad is like. But in actuality we can only discover the ruliad “one computationally bounded step at a time”—in effect progressively applying bounded computations to “move through rulial space”. So even though in some abstract sense “the whole ruliad is already there” we only get to explore it step by step. And that’s what gives us our notion of time, through which we “progress”. Inevitably, there are many different paths that we could follow through the ruliad. And indeed every mind (and every observer like us)—with its distinct inner experience—presumably follows a different path. But much as we described for branchial space, the reason we have a shared notion of “objective reality” is presumably that we are all very close together in rulial space; we form in a sense a tight “rulial flock”. It’s worth pointing out that not every sampling of the ruliad that may be accessible to us conveniently corresponds to exploration of progressive slices of time. Yes, that kind of “progression in time” is characteristic of our physical experience, and our typical way of describing it. But what about our experience, say, of mathematics? The first point to make is that just as the ruliad contains all possible physics, it also contains all possible mathematics. If we construct the ruliad, say from hypergraphs, the nodes are now not “atoms of space”, but instead abstract elements (that in general we call emes) that form pieces of mathematical expressions and mathematical theorems. We can think of these abstract elements as being laid out now not in physical space, but in some abstract metamathematical space. In our physical experience, we tend to remain localized in physical space, branchial space, etc. But in “doing mathematics” it’s more as if we’re progressively expanding in metamathematical space, carving out some domain of “theorems we assume are true”. And while we could identify some kind of “path of expansion” to let us define some analog of time, it’s not a necessary feature of the way we explore the ruliad. Different places in the ruliad in a sense correspond to describing things using different rules. And by analogy to the concept of motion in physical space, we can effectively “move” from one place to another in the ruliad by translating the computations done by one set of rules to computations done by another. (And, yes, it’s nontrivial to even have the possibility of “pure motion”.) But if we indeed remain localized in the ruliad (and can maintain what we can think of as our “coherent identity”) then it’s natural to think of there being a “path of motion” along which we progress “with time”. But when we’re just “expanding our horizons” to encompass more paradigms and to bring more of rulial space into what’s covered by our minds (so that in effect we’re “expanding in rulial space”), it’s not really the same story. We’re not thinking of ourselves as “doing computation in order to move”. Instead, we’re just identifying equivalences and using them to expand our definition of ourselves, which is something that we can at least approximate (much like in “quantum measurement” in traditional physics) as happening “outside of time”. Ultimately, though, everything that happens must be the result of computations that occur. It’s just that we don’t usually “package” these into what we can describe as a definite thread of time. So What in the End Is Time? From the paradigm (and Physics Project ideas) that we’ve discussed here, the question “What is time?” is at some level simple: time is what progresses when one applies computational rules. But what’s critical is that time can in effect be defined abstractly, independent of the details of those rules, or the “substrate” to which they’re applied. And what makes this possible is the Principle of Computational Equivalence, and the ubiquitous phenomenon of computational irreducibility that it implies. To begin with, the fact that time can robustly be thought of as “progressing”, in effect in a linear chain, is a consequence of computational irreducibility—because computational irreducibility is what tells us that computationally bounded observers like us can’t in general ever “jump ahead”; we just have to follow a linear chain of steps. But there’s something else as well. The Principle of Computational Equivalence implies that there’s in a sense just one (ubiquitous) kind of computational irreducibility. So when we look at different systems following different irreducible computational rules, there’s inevitably a certain universality to what they do. In effect they’re all “accumulating computational effects” in the same way. Or in essence progressing through time in the same way. There’s a close analogy here with heat. It could be that there’d be detailed molecular motion that even on a large scale worked noticeably differently in different materials. But the fact is that we end up being able to characterize any such motion just by saying that it represents a certain amount of heat, without getting into more details. And that’s very much the same kind of thing as being able to say that such-and-such an amount of time has passed, without having to get into the details of how some clock or other system that reflects the passage of time actually works. And in fact there’s more than a “conceptual analogy” here. Because the phenomenon of heat is again a consequence of computational irreducibility. And the fact that there’s a uniform, “abstract” characterization of it is a consequence of the universality of computational irreducibility. It’s worth emphasizing again, though, that just as with heat, a robust concept of time depends on us being computationally bounded observers. If we were not, then we’d able to break the Second Law by doing detailed computations of molecular processes, and we wouldn’t just describe things in terms of randomness and heat. And similarly, we’d be able to break the linear flow of time, either jumping ahead or following different threads of time. But as computationally bounded observers of computationally irreducible processes, it’s basically inevitable that—at least to a good approximation—we’ll view time as something that forms a single one-dimensional thread. In traditional mathematically based science there’s often a feeling that the goal should be to “predict the future”—or in effect to “outrun time”. But computational irreducibility tells us that in general we can’t do this, and that the only way to find out what will happen is just to run the same computation as the system itself, essentially step by step. But while this might seem like a letdown for the power of science, we can also see it as what gives meaning and significance to time. If we could always jump ahead then at some level nothing would ever fundamentally be achieved by the passage of time (or, say, by the living of our lives); we’d always be able to just say what will happen, without “living through” how we got there. But computational irreducibility gives time and the process of it passing a kind of hard, tangible character. So what does all this imply for the various classic issues (and apparent paradoxes) that arise in the way time is usually discussed? Let’s start with the question of reversibility. The traditional laws of physics basically apply both forwards and backwards in time. And the ruliad is inevitably symmetrical between “forward” and “backward” rules. So why is it then that in our typical experience time always seems to “run in the same direction”? This is closely related to the Second Law, and once again it’s consequence of our computational boundedness interacting with underlying computational irreducibility. In a sense what defines the direction of time for us is that we (typically) find it much easier to remember the past than to predict the future. Of course, we don’t remember every detail of the past. We only remember what amounts to certain “filtered” features that “fit in our finite minds”. And when it comes to predicting the future, we’re limited by our inability to “outrun” computational irreducibility. Let’s recall how the Second Law works. It basically says that if we set up some state that’s “ordered” or “simple” then this will tend to “degrade” to one that’s “disordered” or “random”. (We can think of the evolution of the system as effectively “encrypting” the specification of our starting state to the point where we—as computationally bounded observers—can no longer recognize its ordered origins.) But because our underlying laws are reversible, this degradation (or “encryption”) must happen when we go both forwards and backwards in time: But the point is that our “experiential” definition of the direction of time (in which the “past” is what we remember, and the “future” is what we find hard to predict) is inevitably aligned with the “thermodynamic” direction of time we observe in the world at large. And the reason is that in both cases we’re defining the past to be something that’s computationally bounded (while the future can be computationally irreducible). In the experiential case the past is computationally bounded because that’s what we can remember. In the thermodynamic case it’s computationally bounded because those are the states we can prepare. In other words, the “arrows of time” are aligned because in both cases we are in effect “requiring the past to be simpler”. So what about time travel? It’s a concept that seems natural—and perhaps even inevitable—if one imagines that “time is just like space”. But it becomes a lot less natural when we think of time in the way we’re doing here: as a process of applying computational rules. Indeed, at the lowest level, these rules are by definition just sequentially applied, producing one state after another—and in effect “progressing in one direction through time”. But things get more complicated if we consider not just the raw, lowest-level rules, but what we might actually observe of their effects. For example, what if the rules lead to a state that’s identical to one they’ve produced before (as happens, for example, in a system with periodic behavior)? If we equivalence the state now and the state before (so we represent both as a single state) then we can end up with a loop in our causal graph (a “closed timelike curve”). And, yes, in terms of the raw sequence of applying rules these states can be considered different. But the point is that if they are identical in every feature then any observer will inevitably consider them the same. But will such equivalent states ever actually occur? As soon as there’s computational irreducibility it’s basically inevitable that the states will never perfectly match up. And indeed for the states to contain an observer like us (with “memory”, etc.) it’s basically impossible that they can match up. But can one imagine an observer (or a “timecraft”) that would lead to states that match up? Perhaps somehow it could carefully pick particular sequences of atoms of space (or elementary events) that would lead it to states that have “happened before”. And indeed in a computationally simple system this might be possible. But as soon as there’s computational irreducibility, this simply isn’t something one can expect any computationally bounded observer to be able to do. And, yes, this is directly analogous to why one can’t have a “Maxwell’s demon” observer that “breaks the Second Law”. Or why one can’t have something that carefully navigates the lowest-level structure of space to effectively travel faster than light. But even if there can’t be time travel in which “time for an observer goes backwards”, there can still be changes in “perceived time”, say as a result of relativistic effects associated with motion. For example, one classic relativistic effect is time dilation, in which “time goes slower” when objects go faster. And, yes, given certain assumptions, there’s a straightforward mathematical derivation of this effect. But in our effort to understand the nature of time we’re led to ask what its physical mechanism might be. And it turns out that in our Physics Project it has a surprisingly direct—and almost “mechanical”—explanation. One starts from the fact that in our Physics Project space and everything in it is represented by a hypergraph which is continually getting rewritten. And the evolution of any object through time is then defined by these rewritings. But if the object moves, then in effect it has to be “re-created at a different place in space”—and this process takes up a certain number of rewritings, leaving fewer for the intrinsic evolution of the object itself, and thus causing time to effectively “run slower” for it. (And, yes, while this is a qualitative description, one can make it quite formal and precise, and recover the usual formulas for relativistic time dilation.) Something similar happens with gravitational fields. In our Physics Project, energy-momentum (and thus gravity) is effectively associated with greater activity in the underlying hypergraph. And the presence of this greater activity leads to more rewritings, causing “time to run faster” for any object in that region of space (corresponding to the traditional “gravitational redshift”). More extreme versions of this occur in the context of black holes. (Indeed, one can roughly think of spacelike singularities as places where “time ran so fast that it ended”.) And in general—as we discussed above—there are many “relativistic effects” in which notions of space and time get mixed in various ways. But even at a much more mundane level there’s a certain crucial relationship between space and time for observers like us. The key point is that observers like us tend to “parse” the world into a sequence of “states of space” at successive “moments in time”. But the fact that we do this depends on some quite specific features of us, and in particular our effective physical scale in space as compared to time. In our everyday life we’re typically looking at scenes involving objects that are perhaps tens of meters away from us. And given the speed of light that means photons from these objects get to us in less than a microsecond. But it takes our brains milliseconds to register what we’ve seen. And this disparity of timescales is what leads us to view the world as consisting of a sequence of states of space at successive moments in time. If our brains “ran” a million times faster (i.e. at the speed of digital electronics) we’d perceive photons arriving from different parts of a scene at different times, and we’d presumably no longer view the world in terms of overall states of space existing at successive times. The same kind of thing would happen if we kept the speed of our brains the same, but dealt with scenes of a much larger scale (as we already do in dealing with spacecraft, astronomy, etc.). But while this affects what it is that we think time is “acting on”, it doesn’t ultimately affect the nature of time itself. Time remains that computational process by which successive states of the world are produced. Computational irreducibility gives time a certain rigid character, at least for computationally bounded observers like us. And the Principle of Computational Equivalence allows there to be a robust notion of time independent of the “substrate” that’s involved: whether us as observers, the everyday physical world, or, for that matter, the whole universe. Posted in: Philosophy, Physics Name (required) Email (will not be published; required) Website 6 comments I first encountered the idea of time being a result of human computational boundedness in the series of Star Trek: deep space 9 wherein Captain Sisko meets creatures who could perceive both past and future and hence did not have the concept of ‘time’. Seems pretty close. One more direction in which we can think of is whether the laws of Physics are not the same for all branchial spaces. In that case the approach we follow to analyse and promulgate new laws (experimentation and inference via the scientific method) may be incorrect, because it will only give us a subset of the laws. The other point which is difficult to digest is how does ‘too much activity in the hypergraph’ correspond to ‘no more rewrites’. Shouldn’t ‘too much activity in the hypergraph’ correspond to ‘too many rewrites’ as hypergraphs are instantaneous slices of space? One more point was about ‘how a branching mind will perceive a branching universe’. Again, I encountered it before in the Nolan movie ‘Inception’ where Cobb explains dreams. When we dream, we do experience time but at that time mind is creating and perceiving the dream. At that time the mind is not computationally bounded and so, omniscient. Fits into your theory. Great read Stephen Wolfram! Zafar October 8, 2024 at 7:33 pm Very cool. It’s seems to me you’ve realized a more fundamental understanding of the universe. It seems that everything that can exist, does, right now. Sam October 8, 2024 at 7:44 pm How does the atoms of space idea square with Lorentz invariance? I understand that loop quantum gravity had atoms of space which imply Lorentz invariance violations but these have not been observed. Geordie Rose October 8, 2024 at 9:09 pm Fantastic article. I’m enjoying how, more and more, you manage to explicate features of conventional (QM) physics in terms of Observer Theory and Entanglement Cones in Branchial Space. The discussion of multiple past and future histories coalescing into a single history in ‘observers like us’ is also fascinating. Perhaps the inevitable differences in our worldviews, perspectives, interpetations, and even our (occasionally, slightly) differing notions of obective reality are examples of how we are each, in fact, travesing slightly different histories. Communication then becomes a kind of temporal or historical compression into a shared, more singular, history. I do have a fundamental philosophical question for you, that perhaps you can help me with. A central aspect of time for me is its apparent motion. Something, it seems, is always moving. And I don’t think it fair to say that that is purely perceptual. It really does seem to characteris a central feature of time. My question then is, regarding your computational model as a whole, what exactly is it that either compels (external) or impels (internal) the state of the system to ‘tick’, i.e. progess or move from one state of the system to the next? Surely there is fundamental necessity for this to ‘occur’, either computational or ontologically, if indeed these are different for the Ruliad. If the next state is ‘computed’ from the last, what is the nature or cause or reality of this ‘moment of computing’? Peter Robinson October 8, 2024 at 10:03 pm Computational irreducibility is presented as a fundamental property of the universe. So, no hopes for time travel! Alex October 9, 2024 at 2:24 am It’s fascinating how at the edge of my conscious mind I feel that there is a lot in common betweeh this and almost all esoteric teachings and spiritual experiences. Walter Wartenweiler October 9, 2024 at 3:26 am Related Writings When Exactly Will the Eclipse Happen? A Multimillennium Tale of Computation March 29, 2024 Computing the Eclipse: Astronomy in the Wolfram Language March 29, 2024 Can AI Solve Science? March 5, 2024 Observer Theory December 11, 2023",
    "commentLink": "https://news.ycombinator.com/item?id=41782534",
    "commentBody": "On the Nature of Time (stephenwolfram.com)334 points by iamwil 20 hours agohidepastfavorite232 comments foundry27 18 hours agoI think it’s really interesting to see the similarities between what Wolfram is saying and the work of Julian Barbour on time being an emergent property. Both suggest a similar underlying ontology for the universe: a timeless, all-encompassing realm containing all possible states / configurations of everything. But what’s really fascinating is that they reach this conclusion through different implementations of that same interface. Barbour talks about a static geometric landscape where time emerges objectively from the relational (I won’t say causal) structures between configurations, independent of any observer. On the other hand, Wolfram’s idea of the Ruliad is that there’s a timeless computational structure, but time emerges due to our computational limitations as observers navigating this space. They’ve both converged on a timeless “foundation” for reality, but they’re completely opposite in how they explain the emergence of time: objective geometry, vs. subjective computational experience reply pizza 17 hours agoparentI was literally thinking of the same similarities. Barbour's exposition of the principle of least action as being time is interesting. There's a section in The Janus Point where he goes into detail about the fact that there are parts of the cosmos that (due to cosmic inflation) are farther apart in terms of light-years than the universe is old, and growing in separation faster than c, meaning that they are forever causally separated. There will never be future changes in state from one that result in effects in the other. In a way, this also relates to computation, maybe akin to some kind of undecidability. Another thing that came to mind when reading the part about how \"black holes have too high a density of events inside of them to do any more computation\" is Chaitin's incompleteness theorem: if I understand it correctly, that basically says that for any formal axiomatic system there is a constant c beyond which it's impossible to prove in the formal system that the Kolmogorov complexity of a string is greater than c. I get the same kind of vibe with that and the thought of the ruliad not being able to progressively simulate further states in a black hole. reply psychoslave 11 hours agorootparent>There's a section in The Janus Point where he goes into detail about the fact that there are parts of the cosmos that (due to cosmic inflation) are farther apart in terms of light-years than the universe is old, and growing in separation faster than c, meaning that they are forever causally separated. There will never be future changes in state from one that result in effects in the other. In a way, this also relates to computation, maybe akin to some kind of undecidability. Ho, I love this hint. However even taking for granted that no faster than light travel is indeed an absolute rule of the universe, that doesn't exclude wormhole, or entangled particles. https://scitechdaily.com/faster-than-the-speed-of-light-info... reply nyrikki 4 hours agorootparentIt would be nice if this was a problem with decidablity, but often it is a problem with indeterminacy that is way stronger than classic chaos. The speed of causality or I information is the limit that is the speed of light. Even in the case of entanglement, useful information is not ftl, If I write true on one piece of paper and false on another and randomly seed them to Sue and Bob, Sue instantly knows what Bob has as soon as she opens hers. While we teach QM similar to how it was discovered, there are less mystical interpretations that are still valid. Viewing wave function collapse as updating priors vs observer effects works but is pretty boring. While wormholes are a prediction of the theory, we don't know if the map matches the territory yet. But it is a reason to look for them. But if we do find them it is likely that no useful information will survive the transit through them. Kerr's rebuke of Hawkings assumption that black hole singularities are anything more than a guess from a very narrow interpretation of probably unrealistic, non rotating, non charged black holes is probably a useful read. The map simply isn't the territory, but that doesn't mean we shouldn't see how good that map is or look for a better one. reply nyrikki 4 hours agorootparentKerr's paper that was referenced above. https://arxiv.org/abs/2312.00841 reply csomar 14 hours agorootparentprev> There will never be future changes in state from one that result in effects in the other. You are assuming that the Principle of locality is true and proven. This is far from being the case from my understanding. reply adrianN 8 hours agorootparentYou can’t really prove things in physics, but to my knowledge we don’t have observations that contradict locality. reply ziofill 14 hours agorootparentprevActually, the parts of the universe receding from us faster than the speed of light can still be causally connected to us. It’s a known “paradox” that has the following analogy: an ant walks on an elastic band toward us at speed c, and we stretch the band away from us by pulling on the far end at a speed s > c. Initially the ant despite walking in our direction gets farther, but eventually it does reach us (in exponential time). The same is true for light coming from objects that were receding from us at a speed greater than c when they emitted it. See https://en.m.wikipedia.org/wiki/Ant_on_a_rubber_rope reply adastra22 14 hours agorootparentThey will never reach us because the rate of expansion is accelerating. reply ziofill 14 hours agorootparenthttps://arxiv.org/abs/astro-ph/0310808 reply adastra22 14 hours agorootparentThat article doesn't back up your claim. reply ziofill 4 hours agorootparentYes it does, look at the caption of Fig. 1: \"Photons we receive that were emitted by objects beyond the Hubble sphere were initially receding from us (outward sloping lightcone at tc (gray crosshatching) to the region of subluminal recession (no shading) can the photons approach us\". I can’t reply to your last reply. I agree, in fact I said those regions can be still causally connected to us, not that they are. reply nyrikki 4 hours agorootparentThose photons aren't superluminal, the are in our past light cone, they were headed out way before the emitter was beyond the horizon. It gets complicated because the concept of 'now' is a local property and because those objects aren't moving away ftl, space is expanding. reply adastra22 4 hours agorootparentprevIt shows that SOME “superluminal” photons can reach us, not that ALL can. With accelerating expansion, eventually all galaxies fall out of that interval and become unreachable. reply pyinstallwoes 16 hours agoparentprevWithout time you’d be everything all at once, which isn’t capable of having an experience, that is to also say: a location. To have experience, requires position relative to the all, the traversal of the all is time. More like a play head on a tape, you’re the play head traversing and animating your own projection. reply hackinthebochs 16 hours agorootparentThe universe doesn't need to evolve for us to have experience. We would experience evolution through the state space because its structure is oriented such as to experience evolution through time. Each point in experience-time (the relative time evolution experienced by the structure) is oriented towards the next point in experience-time. Even if all such points happen all at once, the experience of being a point in this structure oriented towards the next point is experienced subjectively as sequential. In other words, a block universe would contain sequences of Boltzman brains who all subjectively experience time as sequential. The real question is why would such a universe appear to evolve from a low entropy past following a small set of laws. reply pyinstallwoes 8 hours agorootparentWell, it doesn’t evolve. You just render it as evolving to perceive yourself / itself. The only way to have the state of being of observation and perception is to not be everything which gives rise to directionality. reply astrostl 40 minutes agorootparentprev> a block universe I first encountered this theory and the related \"eternalism\" philosophy via Alan Moore [1] (Watchmen, V for Vendetta, The Ballad of Halo Jones, Swamp Thing, Batman: The Killing Joke, From Hell, etc.). Watchmen and its non-Moore-affiliated sequel have a lot of riffs on time and determinism. Q: Jerusalem deals with the idea of eternalism: everything that has happened is happening right now and forever. Could you explain your views on this? A: My conception of an eternity that was immediate and present in every instant – a view which I have since learned is known as ‘Eternalism’ – was once more derived from many sources, but a working definition of the idea should most probably begin with Albert Einstein. Einstein stated that we exist in a universe that has at least four spatial dimensions, three of which are the height, depth and breadth of things as we ordinarily perceive them, and the fourth of which, while also a spatial dimension, is perceived by a human observer as the passage of time. The fact that this fourth dimension cannot be meaningfully disentangled from the other three is what leads Einstein to refer to our continuum as ‘spacetime’. This leads logically to the notion of what is called a ‘block universe’, an immense hyper-dimensional solid in which every moment that has ever existed or will ever exist, from the beginning to the end of our universe, is coterminous; a vast snow-globe of being in which nothing moves and nothing changes, forever. Sentient life such as ourselves, embedded in the amber of spacetime, would have to be construed by such a worldview as massively convoluted filaments of perhaps seventy or eighty years in length, winding through this glassy and motionless enormity with a few molecules of slippery and wet genetic material at one end and a handful or so of cremated ashes at the other. It is only the bright bead of our consciousness moving inexorably along the thread of our existence, helplessly from past to future, that provides the mirage of movement and change and transience. A good analogy would be the strip of film comprising an old fashioned movie-reel: the strip of film itself is an unchanging and motionless medium, with its opening scenes and its finale present in the same physical object. Only when the beam of a projector – or in this analogy the light of human consciousness – is passed across the strip of film do we see Charlie Chaplin do his funny walk, and save the girl, and foil the villain. Only then do we perceive events, and continuity, and narrative, and character, and meaning, and morality. And when the film is concluded, of course, it can be watched again. Similarly, I suspect that when our individual four-dimensional threads of existence eventually reach their far end with our physical demise, there is nowhere for our travelling bead of consciousness to go save back to the beginning, with the same thoughts, words and deeds recurring and reiterated endlessly, always seeming like the first time this has happened except, possibly, for those brief, haunting spells of déjà vu. Of course, another good analogy, perhaps more pertinent to Jerusalem itself, would be that of a novel. While it’s being read there is the sense of passing time and characters at many stages of their lives, yet when the book is closed it is a solid block in which events that may be centuries apart in terms of narrative are pressed together with just millimetres separating them, distances no greater than the thickness of a page. As to why I decided to unpack this scientific vision of eternity in a deprived slum neighbourhood, it occurred to me that through this reading of human existence, every place, no matter how mean, is transformed to the eternal, heavenly city. Hence the title. 1: https://alanmooreworld.blogspot.com/2019/11/moore-on-jerusal... reply lukasb 14 hours agorootparentprevThis makes a good argument that the block universe can't exist: https://aeon.co/essays/who-really-won-when-bergson-and-einst... (search \"block\") reply jstanley 13 hours agorootparentThat's not saying it can't exist, it's just saying you can't go outside the universe to look at it. reply raattgift 8 hours agorootparentprevBoltzmann brains are extremely ephemeral. An analogy is that of stirring a vat of alphabet soup and noticing that there is a fair number of single-letter words popping into view (\"A\", \"I\"), a smaller number of two-letter words, an even smaller number of three-letter words ... a very very small chance of a twenty-letter word ... and a vanishingly small chance of the 189819-letter monsterpopping into view. The stirring doesn't stop just because a multiletter word appears, so multiletter words are quickly broken up and even valid single-letter words get hidden behind the \"B\"s and \"Q\"s and other letters in the soup. Boltzmann brains will fluctuate out of existence on the order of a small multiple of the light-crossing time of the brain-matter that fluctuated into existence. As the brains are human, they won't even have a chance to react. Although their false memories are encoded however true memories exist in our own brains, they'll have no time to have a reminiscence or notice their lack of sensory organs. (Which is probably good, since they would quickly suffer and die from lack of pressure and oxygen). A Boltzmann-brain with a full encoding of a life worth of false memories (from never-existing sensory input) is a much larger number of letters. Also, in a cold universe, the stirring is slower, and the letters sparser. Boltzmann brains are tremendously unlikely except in a verrrrrrrrry big volume of spacetime. But with a sufficiently big volume of spacetime, or one with an energetic false vacuum, one should expect a lot of Boltzmann brains. This view puts some limits on our own cosmos's vacuum, since we don't see lots of Boltzmann brains (or even much less complicated but RADAR-detectable and/or eclipsing strucures) fluctuating into brief existence in our solar system. Boltzmann brains are low-entropy. A persisting Boltzmann brain (fluctuating into existence and staying in existence for a long time) is much lower entropy still. This poses problems for hypotheses that the entire early universe fluctuated into existence and then evolved into the structures we see now. Here there are human brains attached to sensory apparatus, whose memories correlate fairly well with their history of input (and recordings by ancestors, and fossil records, and so on): a system with much much lower entropy than Boltzmann brains, so what suppresses relatively high-entropy structures (including Boltzmann brains) from dominating (by count) our neighbourhood? Also, if the universe supports large low-entropy fluctuations, galaxies that briefly (~ hundred thousand years) fluctuated in and out of existence should be much more common than galaxies with a history consistent with billions of years of galactic evolution, and you'd expect random variations in morphology, chemistry, and so forth; that's not what we see. This is a bit annoying, as it would be handy to point to Boltzmannian fluctuation theory as the source of the tremendously low entropy in the very early universe, i.e., it could have arisen spontaneously in a less precisely ordered space. Oh well. > why would ... a universe appear to evolve from a low entropy past following a small set of laws Thermodynamics. The issue is: where did the low entropy past come from? Once you have that, evolving into a higher entropy structure-filled present is not too hard -- that's essentially what we have with the standard cosmology from about the electroweak epoch onwards. So in summary: > sequences of Boltzman brains who all subjectively experience time as sequential whatever these might be, they aren't Boltzmann brains, since the latter don't subjectively experience anything as objectively they fluctuate out of existence in something like a nanosecond. Very briefly, the short existence is driven by interacting fields and the need to keep entropy (relatively) high: if your starting point just before the appearance of the brain is a region that is high quality vacuum, you have to come up with protons, calcium nuclei, ... and all that requires very careful aim to have one split-second \"movie frame\" of brain. You need much better \"aim\" which really drives down the entropy (which corresponds a much larger fluctuation) to go from vacuum to a Boltzmann brain that doesn't disintegrate starting in the very next frame thanks to overshoots of momentum. The higher the entropy of the Boltzmann brain, the clearer the stat mech argument. (If one gets stuck thinking about human brains, C. elegans apparently develop memories and store them in their nerve ring. Why isn't the outer space of our solar system full of those Boltzmann-C.-elegans brains fluctuating in and out of existence with each possessing false memories of sensory stimuli? Smaller fluctuations, so there should be many more of those than human Boltzmann brains). reply hackinthebochs 4 hours agorootparentI agree with all that. Bringing up Boltzman brains was just an alternate way of explaining how inhabitants of a block universe could experience time as sequential without a real sequential ordering of universe states. Presumably if one can conceptualize a Boltzman brain coming into existence to experience one instant of a virtual life with virtual memories, you can imagine a long sequence of them experiencing the entirety of this virtual life. But the order in which this sequences comes into existence doesn't alter the directionality of subjective time evolution for the Boltzman brains. reply JohnMakin 2 hours agorootparentThis is well said - this is exactly how I understood your comment as well and you put it very succinctly and in an understandable way and has been something that I've been pondering for a while now. Thanks. reply CooCooCaCha 5 hours agorootparentprevBut wouldn’t each brain still be frozen in a moment of time? Don’t you still need something that moves the “play head of the universe” from one moment to the next? reply hackinthebochs 4 hours agorootparentIf your experiences were played out of order in some kind of \"God's eye\" time, how could you notice? The experience of each moment seems continuous due to our memory of the recent past. But this memory is just a configuration of our current state. The actual ordering of the evolution of this state doesn't influence the directionality of the subjective experience of evolving through time. reply CooCooCaCha 4 hours agorootparentA god’s eye perspective still requires time. The absence of time implies nothing can change because time is required to differentiate two states. The notion of “observation” implies change because you’re learning something new. You could say we exist in a simulation and the entities outside the simulation can pause the simulation or pre-compute the simulation so that it’s static but then you’re just kicking the can down the road because they would need their own notion of time to observe the simulation they created. reply hackinthebochs 3 hours agorootparentI don't see how this responds to the thrust of the argument. The argument is that if order doesn't matter to the directionality of subjective time then no order doesn't matter either. Time isn't required to differentiate two states just as time isn't required to differentiate two static regions of space. The features of the thing can do the differentiation. Whether you consider all of block spacetime as a single entity or subdivided in various ways is a matter of convention. But regions of this block spacetime can be grouped by way of their apparent dynamical connection. I.e. the appearance of evolution following laws connects some regions with others sequentially. reply CooCooCaCha 1 hour agorootparentAh I think I wasn’t clear. I don’t really care if time moves sequentially or jumps around in random order. My concern is with the existence of time itself. What gives space meaning is coordinates, which allow multiple things to exist separately from each other. Likewise you need another coordinate to differentiate “snapshots” of the universe. So in that sense time is necessary to differentiate two states. But i understand we’re talking about a more fundamental notion of time so i get what you’re saying. Perhaps a better way to put it is time is necessary for events to happen. Let’s say you could view the universe from the outside, ok great but what can you do with that? You still need time to do things even if you’re outside the universe. Otherwise it would literally be frozen and meaningless. That’s my issue with these timeless theories is people imagine viewing the universe as a static 4D object but they still talk about it as if things are happening outside the universe and you need time for events to happen. If time doesn’t exist then a “gods eye view” is meaningless because nothing could happen from that perspective either. It’s also a strong statement about the origins of reality because if time doesn’t exist then reality could not have been created through any process. God or otherwise. reply causal 4 hours agorootparentprevThe universe keeps going even when you're unconscious and having no experience at all. Others experience consciousness without your knowing. So why would you assume your past or future can't exist without your knowing? reply pyinstallwoes 1 hour agorootparentProof? reply CooCooCaCha 3 hours agorootparentprevI didn’t make any such claims regarding consciousness. I’m trying to understand how time as an emergent phenomenon instead of fundamental to the universe could work. reply JumpCrisscross 16 hours agorootparentprev> have experience, requires position relative to the all, the traversal of the all is time You’re describing timelike experience. Photons “experience” events as in they are part of causality. But they do so in a non-timelike manner. reply pyinstallwoes 8 hours agorootparentSaid a human. If it’s not time-like, then it’s everything, thus it can’t have experiences thus god. God splits (monad becomes many) to experience being (shards in multiplicity of the one through division: oooh spooky golden mystery). reply CooCooCaCha 5 hours agorootparentTake your meds reply pyinstallwoes 1 hour agorootparentTake your meds reply marcus_holmes 12 hours agorootparentprevMaybe we do experience everything at once, but then have to process it in a time-like manner to make any sense of it. Like everything else that we \"experience\", maybe the perception that reaches our consciousness has nothing to do with what's actually out there. There are no purple photons. reply pyinstallwoes 8 hours agorootparentYeah, god is everything, which can’t have experience, as it’s experiencing everything at once - thus the monad splits itself, allowing perception as a fraction of the whole which is experienced as time and direction. reply idiotsecant 6 hours agorootparentprevI'm not sure why experience requires the arrow of time or location. Your experience does, and it might seem that is a universal rule, but only because you can't possibly intuit a world in which time doesn't flow. I think Dr. Manhattan is a good fictional reference. He existed in a timeless form. Everything was happening simultaneously for him. For everyone else they experienced him in a time like way, but only as a matter of perspective. reply pyinstallwoes 1 hour agorootparentHow can you imagine any world without experience (observation?) thus any observer is dependent on position thus time simply because it is the partial history that allows the state itself to exist. And your second point is essentially the metaphysical argument for god and early spirituality. Hebrew mystiscm for example describes god pouring itself into lower forms of being to experience itself reply tempaway456456 8 hours agoparentprevI don't think they are saying anything similar at all. Julian Barbour finds a way to get rid of Time completely (by saying every possible state exists and there must be some law that favours states that _seem_ to be related to _apparently_ previous states). Wolfram is more focused on making sense of 'time is change' through the lens of computation. reply yarg 15 hours agoparentprevI think that time isn't what we think it is - but I don't think it's all already set; rather I think that the past can be constrained by the future just as the future is constrained by the past. I don't think that there's spooky action at a distance (it's fundamentally equivalent to retrocausality, and the consequences of the distant foreign event cannot outpace its light cone anyway). I think its a superposition of states of a closed time-like curve thing being fleshed out as its contradictions are resolved and interactions are permitted between its colocated non-contradictory aspects. But I'm not a physicist, so that's probably all just bullshit anyway. reply andoando 13 hours agoparentprevI suspect there are many different mental conceptions that amount to the same facts of nature. reply bmitc 13 hours agoparentprevI generally like the idea of most everything being emergent, but where does it stop? Is it emergence all the way down? reply bbor 13 hours agoparentprevIdk, just looking at it now Barbour seems much, much more rigorous. The linked article is more “using scientific terms to muse about philosophy” than physics, IMHO. For example; In essence, therefore, we experience time because of the interplay between our computational boundedness as observers, and the computational irreducibility of underlying processes in the universe. His big insight is literally the starting point of Hegel’s The Science of Logic, namely that we are finite. That in no ways justifies all the other stuff (especially multiverse theory), and it’s not enough to build a meaningfully useful conception of time, at all. All it gets you is that “if you were infinite you wouldn’t experience time”, which is a blockbuster-sci-fi-movie level insight, IMO. I can’t help but think of Kant as I write this; he wrote convincingly of the difference between mathematical intuition and philosophical conception, a binary Wolfram would presumably—and mistakenly-identify with solid logic vs meaningless buffoonery. But refusing to acknowledge our limits makes you more vulnerable to mistakes stemming from them, not less. …the metaphysic of nature is completely different from mathematics, nor is it so rich in results, although it is of great importance as a critical test of the application of pure understanding—cognition to nature. For want of its guidance, even mathematicians, adopting certain common notions—which are, in fact, metaphysical—have unconsciously crowded their theories of nature with hypotheses, the fallacy of which becomes evident upon the application of the principles of this metaphysic, without detriment, however, to the employment of mathematics in this sphere of cognition. Worth remembering at this point that Aristotle coined “physics” for the mathematical study of physis (nature), which was then followed up by a qualitatively different set of arguments interpreting and building upon that basis in a work simply titled metaphysics (after physics). We’ve learned infinitely more mathematical facts, but IMO “what is time, really?” will forever remain beyond their reach, a fact determined not by the universe but by the question itself. TL;DR: if you’re gonna try to talk cognition you should at least admit that you’re writing philosophy, and ideally cite some philosophers. We’ve been working on this for a hot minute! Barbour seems to be doing something much less ambitious: inventing the most useful/fundamental mathematical framework he can. reply CooCooCaCha 5 hours agorootparentI swear as I get older philosophy feels more and more like religion for intellectuals. If you want to talk about cognition or time you should study science, not philosophy. You’re not going to learn about the universe in any significant way by studying hegel or aristotle or kant harder. reply svieira 3 hours agorootparentFunnily enough, the scholastics thought of philosophy as the handmaid of theology. Ultimately, it's in the name (love-of-wisdom). You can learn wisdom from science, but that body of wisdom eventually becomes a philosophy. And the older philosophers definitely saw something, even if they are not completely correct. reply m3kw9 17 hours agoparentprevSo you are saying there is a version of me that is king of the universe in some timeline? reply grugagag 15 hours agorootparentIn a skin enclosed universe you are already King Meatbag, ruler over your mind and body. reply biofox 11 hours agorootparentMy body disagrees. reply pixl97 17 hours agorootparentprevIf the universe is infinite then there is a possibility that you are a king of an observable universe somewhere. reply xandrius 16 hours agorootparentprevInfinite does not mean that all the permutations are possible. You being you and you becoming a king might simply not be a combination which is compatible. reply kridsdale3 13 hours agorootparentGreat way to let someone down who asks you out. There are no branches in the Ruliad in which you and I end up together. I have foreseen it. reply mensetmanusman 14 hours agorootparentprevYou vastly misunderestimate infinity if you don’t recognize that anything feasible will happen. reply jbotz 10 hours agorootparentDepends on how you define feasible. Take Wolfram's 1-dimensional cellular automata... some of them have infinite complexity, and of course you can \"run\" them for infinite time, and the \"current\" state is constantly expanding (like the Universe). So let's define \"something feasible\" as some specific finite bit pattern on the 1-dimensional line of an arbitrary current state. Is that \"feasible\" bit pattern guaranteed to appear anywhere in the automaton's present or future? I believe, and if I understand correctly, so does Wolfram, that for any reasonably complex \"feasible pattern\" the answer is no; even though the automaton produces infinitely many states, it is not guaranteed to explore all conceivable states. In other words, in a given Universe (which has a specific set of rules that govern its evolution in time) even though there are infinitely many possible states, not all conceivable states are a possible result of that evolution. reply mensetmanusman 6 hours agorootparentIf you exist, you are one of the feasible states. reply pantulis 10 hours agorootparentprevThere are infinite numbers between 3 and 4, yet none of them is number 7. reply mensetmanusman 6 hours agorootparent7 isn’t feasible… reply pishpash 2 hours agoparentprevAs usual with Wolfram, too hand-wavy. It could be true but this is not serious physics. reply adastra22 14 hours agoparentprevIt is simpler than that. Wolfram has a long history of plagiarizing ideas and passing them off as his own. reply mensetmanusman 14 hours agorootparentThat’s the history of 99.9999% of ideas based on the average token generation rate of humanity. reply PaulDavisThe1st 14 hours agorootparentThe mother of someone who was a friend in the 90s used to always pepper her speech with attributions for almost everything she was saying (in any \"serious\" conversation). \"I think it was Popper who said ...\" \"Schenk developed this idea that ...\") It was * so * annoying to listen to. reply adastra22 14 hours agorootparentWe should hold dinner-table conversations and scientific letters to different standards. reply adastra22 14 hours agorootparentprevReal scientists tend to try to be careful about attribution and especially don't just blatantly regurgitate the last thing they read and pass it off as their own. That is highly frowned upon in polite academic society. reply lisper 19 hours agoprevI wrote up more or less the same idea ten years ago, but in what I think is a more accessible presentation: https://blog.rongarret.info/2014/10/parallel-universes-and-a... reply whyenot 18 hours agoparentI have read and appreciated your writings going back to the comp.lang.lisp days, but a blog post that starts with “if you haven’t read the previous post, please do before reading the rest of this one” is not what I would consider accessible. …and that previous post then asks the reader to first read a paper or watch a video before proceeding. While a decade later than what you wrote, Wolfram’s article is much more self contained and complete. reply ttctciyf 2 hours agoparentprevIt's sort of funny that where the title alludes to the arrow of time, opening with a quote asserting \"all measurements are in principle reversible\", it pretty quickly gets to a different arrow of time - that of comprehension: > \"If you haven't read the previous post ... this won't make any sense\" Could you have demonstrated, perhaps accidentally, an alternative organising principle allowing temporal ordering to emerge in a computationally oriented ontology? Can the future only \"make sense\" if it temporally follows the past? Only half kidding! reply lisper 2 hours agorootparentThat's actually a great question, and one I've been wrestling with for years. Why do we perceive time as a sort of continuous monotonic flow? And I think it can be explained in terms of perception and comprehension, which I have a gut feel can be formalized as a kind of preferred basis selection. But rendering that intuition into words (and math) has turned out to be quite challenging, which I why I haven't written about it yet. Maybe in the future :-) reply WhitneyLand 18 hours agoparentprevThank you so much for this. Whenever people criticize Wolfram the comeback is often, he’s just trying to discuss big ideas that mainstream science won’t talk about. Of course that’s not the reason for the criticism at all and I think your work here shows that it’s totally fine to speculate and get a little philosophical. The results can be interesting and thought provoking. There’s a difference between big ideas and grandiosity. It also shows big ideas can stay scientifically grounded and don’t require making up corny terminology (Ruliad? lol). reply Q_is_4_Quantum 15 hours agoparentprevIt is possible to make quantitative statements that I think capture many of the intuitions you assert. Here was one attempt: https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.10... That particular proposal was mathematically wrong for reasons I still find physically perplexing (it turns out that for some events quantum theory allows for stronger memory records - defined via classial mutual information - of entropy decreasing events!). A simple example is in here: https://arxiv.org/abs/0909.1726 (I am second author). reply lisper 13 hours agorootparentVery interesting! Thanks for the pointers! I'll need to take some time to digest these. reply nis0s 17 hours agoprevDo physicists think time actually exists? I wonder if someone has reasoned that time is an accounting method that humans have developed to make sense of their experienced change of systems. Wolfram uses the words progression and computation a lot in his essay, but there’s an implicit bias there of assuming a process is deterministic, or has some state it’s driving towards. But none of these “progressions” mean anything, I think. It seems they are simply reactions subject to thermodynamics. If no one observed these system changes, then the trends, patterns, and periodicity of these systems would just be a consequence of physics. It seems what we call “time” is more the accumulation of an effect rather than a separate aspect of physics. For example, I wonder what happens in physics simulations if time is replaced by a measure of effect amplitude. I don’t know, tbh, I am not a physicist so maybe this is all naïve and nonsense. reply defaultcompany 2 hours agoparentI don't know the answer to your question but tangentially, many human concepts related to time definitely do not exist in a purely physical sense. Like being \"late\" or \"early\", things \"taking too long\" or \"being slow\". Being \"out of time\" or \"just in time\". These are all human concepts. Physically speaking (classically anyway), things all happen right when they are supposed to. reply accrual 1 hour agorootparentI find a lot of interesting links between spirituality and physics like this. One idea or message in spirituality is that everything happens exactly as as \"the universe\" intends it to. It's meant to be a comforting thought as events (good and bad) occur in one's life and to encourage one to detach from outcomes. Yet, it's more or less parallel to classical determinism as you mentioned. > Physically speaking (classically anyway), things all happen right when they are supposed to. reply bubblyworld 14 hours agoparentprevTime \"exists\" in physics in the same way everything else in physics does - namely, the value we measure with clocks in the real world satisfies all of the same properties (at least in certain regimes of the universe) as the thing we call \"time\" in various physics theories like relativity/classical mechanics. And those theories make (reasonably) correct predictions about the values we measure in the real world. Is it possible that these properties are the result of some other interactions that have very different laws at a lower level? Absolutely! But the discovery of particles didn't cause the sun to disappear, if that makes sense. reply goatlover 14 hours agoparentprev> Do physicists think time actually exists? Yes, spacetime is important for General Relativity, cosmology and thermodynamics. Whether it's fundamental or emerges from something more fundamental is an open question though. reply mensetmanusman 14 hours agoparentprevTime is just a measure of change. No change. No time. We are interested in a peculiar rate of time based on the heart beat of our experience. reply deepfriedchokes 13 hours agorootparentIt could be that what changes is our perception of reality, not reality itself. reply worstspotgain 19 hours agoprevThought experiment on the nature of reality: - In a much larger universe, write down in a log book every event to every particle at every instant, from the Big Bang to the restaurant. - Put it on the fireplace mantle and leave it there. This is basically a log of a simulation. It exists in much the same way as an ongoing simulation would, except that its time dimension isn't shared with the simulating universe. But every observer within has had the same observations as if it did. reply skissane 19 hours agoparentThis assumes that a map, if sufficiently detailed, is identical to the territory. Maybe it is, maybe it isn’t - but it is a highly debatable metaphysical assumption. I’m not sure how seriously we should take some people’s claims that they “know” that such an assumption is actually true reply worstspotgain 19 hours agorootparentIt's an argument about simulations, not about reality. If reality is a simulation, then arguments about simulations apply to it, but that's the big if. reply skissane 19 hours agorootparentNot necessarily. Suppose that consciousness/qualia/etc is “something extra” which has to be added to non-mental reality, as some dualists believe. Then, it would be possible that we live in a simulation which contains consciousness because that “something extra” has somehow been added to it. And yet, maybe the “much larger” universe which contains our simulation also contains such a “log book” of a very similar universe to our own, also containing intelligent life - and yet, if the “something extra” has not been added to that “log book”, it would lack consciousness and qualia, unlike our own universe. I’m not arguing that a dualism (of this sort) is actually true, merely that we don’t (and can’t) know for a fact that it is false. But if we can’t know for a fact that it is false, then even if we (somehow) knew our reality was simulated, that wouldn’t give us grounds to make confident inferences about the nature of other simulations, or the nature of simulations in themselves reply worstspotgain 19 hours agorootparentI agree with your post. However, I was using the most mechanical meaning of simulation: \"the production of a computer model of something, especially for the purpose of study,\" which implies determinism and excludes the \"something extra.\" reply skissane 17 hours agorootparentIt doesn’t actually exclude the “something extra”, it is neutral as to whether or not there is any “something extra” Panpsychists claim everything is conscious, even rocks, even atoms. Again, I don’t claim this is true (I’d be rather shocked if I somehow found out it was), but we can’t know for a fact that it is false. Yet if panpsychism (or at least certain versions thereof) is true, every simulation (even a simulation of the weather, or of crop growth) is conscious, simply because absolutely everything is. But I don’t think most standard definitions of “simulation” are excluding that possibility - on the contrary, they are agnostic with respect to it, treating its truth or falsehood as outside of their scope It also doesn’t necessarily imply determinism because some computer simulations use RNGs. Most commonly people use pseudorandom RNGs for this, but there is nothing in principle stopping someone from replacing the pseudorandom RNG with a hardware RNG based on some quantum mechanical process, such that it is indeterministic for all practical purposes, and the question of whether it is ultimately deterministic or indeterministic depends on controversial questions about QM to which nobody knows the answers reply worstspotgain 17 hours agorootparent> It doesn’t actually exclude the “something extra”, it is neutral as to whether or not there is any “something extra” Roger, that's even better. I tried to clarify the log book idea in another reply.[1] The question is whether you can have reality (from the observer's perspective) just based on whether coherent information exists in any setting. Basically the question is whether we can go from \"I think, therefore I am\" to \"something is constructing information.\" The latter is obviously a simpler, lower-level proof than other concepts of existence. That brings us back to the \"something extra.\" Is it required for our observations to be possible, i.e. can we rule out the log book conjecture? I don't think we can, but I might be wrong. [1] https://news.ycombinator.com/item?id=41783599 reply orangecat 16 hours agorootparentprevAnd yet, maybe the “much larger” universe which contains our simulation also contains such a “log book” of a very similar universe to our own, also containing intelligent life - and yet, if the “something extra” has not been added to that “log book”, it would lack consciousness and qualia, unlike our own universe. In that case, the non-conscious people in the log book would spend a lot of time pontificating on their experiences of consciousness and how mysterious it is and whether it's possible for there to be other universes that contain entities like themselves except not conscious. They'd be having these discussions for reasons that have nothing to do with actually being conscious, but coincidentally their statements would perfectly correspond with our actual perceptions of consciousness. Maybe not logically impossible, but it seems extremely improbable. (This is pretty much the argument at https://www.lesswrong.com/posts/fdEWWr8St59bXLbQr/zombies-zo... which I find persuasive). reply mistermann 19 hours agorootparentprevThe word \"simulation\" is it self a simulation. So is the word \"is\". https://en.m.wikipedia.org/wiki/Semiotics Reality is a multi-disciplinary domain, but it gives off the appearance of being physics only, because of its metaphysical nature. reply partomniscient 6 hours agorootparentprevI am King Ozymandias look upon my complete data dump/backup*, ye mighty and despair! *May be subject to entropy over time. reply FrustratedMonky 19 hours agorootparentprevExcept for the randomness introduced in Quantum Mechanics. If they ever solve the randomness, then if the map is down to every particle, then yes, the map and reality could be the same. But think at that point you need a computer the size of reality to keep track of every particle. Or, maybe the entire universe is one giant wave equation. But again, I think you need a computer the size of the universe to solve it. reply skissane 19 hours agorootparentWe don’t know for a fact that QM contains irreducible indeterminism. If many worlds is true, then QM is ultimately deterministic. Same if hidden variables is true. A large class of local hidden variable theories have been ruled out by Bell’s theorem, but non-local hidden variable theories survive it (such as the Bohm interpretation and the transactional interpretation), as do local hidden variable theories which deny the Bell theorem’s assumptions about the nature of measurement, such as superdeterminist local hidden variable theories. reply FrustratedMonky 7 hours agorootparentWasn't the Noble prize last year for eliminating local hidden variables? That spooky action at a distance does occur? And for many worlds. Doesn't it punt randomness into other universes, but doesn't help us solve for results in our own individual universe. Since we can't measure what happens in the other universe we don't really know. If there were two results, and one is in one universe, and one in our universe, sure we determinedly know both results. But we don't know which universe we are in, so instead of a random result, now we have 2 answers and 2 universes, but now randomly don't know where we are? reply kaibee 2 hours agorootparent> now we have 2 answers and 2 universes, but now randomly don't know where we are? We are in both. Both universes are equally real. Each 'copy' of you knows it's in the universe where the result matches the observation. reply FrustratedMonky 1 hour agorootparentI'm pretty sure this is not true. Nobody has proven this. If in my universe I could always predict the correct results, then we would just have determinism, and I could predict exactly when an atom would decay. There would be no need for statistics. Some high level background that might help. https://www.youtube.com/watch?v=z-syaCoqkZA https://www.youtube.com/watch?v=433tAfO4dbA https://www.youtube.com/watch?v=rvG1A795tqE reply goatlover 14 hours agorootparentprevAn MWI universe would be hard to simulate though. There's an unknown vast number of branches. reply zeven7 11 hours agorootparentMaybe with a quantum computer in a larger multi worlds universe? reply worstspotgain 19 hours agorootparentprevAre you saying that some things are just not simulable, given a sufficiently large and powerful computer, or that the universe is or might be infinite? reply FrustratedMonky 19 hours agorootparentIf the universe is real, not simulation. If you know the position and speed, everything, about every particle, then you should be able to extrapolate the future by calculating it. The problem is you need a computer the size of the universe to do that calculation. So even thought the map is the territory, equal scale, and you have the map. It is little worthless because the map ends up being reality. Edit: Little different than the idea that if this is simulation, you can do clipping and only render what we see. I'm saying the entire universe is 'real'. reply worstspotgain 19 hours agorootparentIf the universe is not infinite, and if individual particles and waves are calculable, it follows that one can postulate a larger universe capable of simulating it, or a large enough log book in this example. What I find interesting is looking at whether some observable things look like they might be performance optimizations, or even \"magic seeds\" (as in RNG seeds.) No proof of a simulation obviously, but maybe hints. reply tines 19 hours agorootparentprev> If you know the position and speed, everything, about every particle, then you should be able to extrapolate the future by calculating it. But isn't that the exact thing that quantum mechanics refutes? You cannot know the future just from the past; you can only know the probabilities of different futures. reply worstspotgain 19 hours agorootparentOK, but if you own the machine, you can just pick the outcome you want, or draw it from the distributions at random. We (observers inside the machine) cannot know the future of course. reply FrustratedMonky 19 hours agorootparentprevYes. I referred to the randomness that would prevent this, \"once that is solved\". Guess I'm in the camp that eventually we'll find some model or discover something new, to discover what is behind the randomness, so it is no longer just random. But, yes, that is big IF. Until then, with current theories, we couldn't do these calculations. They'd just be approximations accounting for some randomness. reply Filligree 17 hours agorootparentMany-Worlds doesn’t contain or require any randomness. I guess for whatever reason you don’t consider that to be the correct discovery? reply FrustratedMonky 7 hours agorootparentBecause it doesn't remove the randomness from our universe. It punts it to other universes. That is great, but doesn't allow us to predict things in our individual universe. Or another way of saying it. We have 2 answers, they are determined. That is great, we know the 2 answers, one in each universe. Now the problem is we don't know what universe we are in. Now which universe we are in is random. We didn't move the ball towards doing something useful in our own universe. reply Filligree 6 hours agorootparentThe 'universes' are loose abstractions, not a defined part of the theory; there's no actual hard distinction between timelines, in much the same way as coastlines don't have a defined length. They all blend into each other if you look closely enough. That said, isn't the obvious answer 'all of them'? reply jorvi 18 hours agorootparentprev> The problem is you need a computer the size of the universe to do that calculation. I’m not sure were you get that idea from. The amount of calculations we can do, per say, 1 000 000 molecules dedicated to the calculation has absolutely skyrocketed, and will continue to skyrocket. reply FrustratedMonky 18 hours agorootparent\"The amount of calculations we can do, per say, 1 000 000 molecules dedicated to the calculation \" Lets say it takes 100 molecules in a circuit to calculate 1 particles state. Then you already would need a universe 100X the size to calculate our 1X size universe. I'm assuming all particles, not that this is somehow clipping and only rendering what we see. I'm not talking about the brain in box simulation, I'm talking about idea that entire universe is out there. What would it take to calculate every position of every particle. reply jorvi 7 hours agorootparent> Lets say it takes 100 molecules in a circuit to calculate 1 particles state. Then you already would need a universe 100X the size to calculate our 1X size universe. That’s not how it works though. You’d have a lot of fixed costs to build a computer that simulates exactly the behavior of one particle. But then simulating a secondary particle will have a much, much, much smaller marginal cost. Since you brought up clipping, games are actually a perfect example. You can see games as very crude simulations of our own reality, or slices of it. Take for example Red Dead Redemption 2. Run it on a PS5. Now compare the size of your PS5 to the mass of what was the old Wild West territory :) Plus there’s the whole quantum computing thing, where in a way you’re reaching into “alternate” realities for extra compute. reply FrustratedMonky 6 hours agorootparentYes. Just like a Minecraft World is like the size of 64,000 Earths, but it runs on my laptop. That is what I'm saying is not happening. I'm saying that in a particle collider, we measure particles, and those exist all the time, not just when we are looking at them. Like, I have DNA and bones, they exist all the time, not just a simulation showing a 'skin' so it doesn't have to render everything. Unless you are making a bigger point. That a computer that could be simulating every single particle, must exist outside this universe, and maybe mass and energy in this outside universe is so radically different we can't even grasp the scales of it. Just like someone inside a minecraft world with blocks, couldn't grasp the amount of energy in our world. reply jorvi 2 hours agorootparentWell, I don’t know about outside the universe, but you’re still not understanding how scaling works. And our technological progress. The simplest way I can put it is that at some point of compute, there is a crossover where you need less mass to simulate something than the mass of the actual thing is. This will hold true for particle simulations as well. So no, you would not need more particles than the universe has to simulate the universe perfectly. reply FrustratedMonky 2 hours agorootparentOk. I'll try again. I think the 'scaling' issue here is not understanding the size of the scale if we are talking about if dealing with every particle in the universe. The largest super computers today aren't simulating every particle in even a few molecules. So lets say you have Minecraft running. You can completely build a CPU / Memory, etc... Inside Minecraft with Redstone. Lets say you do this, build a PC inside Minecraft to the point that it is functional enough to run Minecraft. Minecraft running in Minecraft. There is huge overhead. You need an astronomically large real PC that could handle running Minecraft such that the Minecraft version running inside Minecraft is usable. That is the scale problem. I'd have to dig up the citation. But pretty sure this compute power needed to compute the universe has been worked out. reply kouru225 17 hours agoparentprevOk but the act of writing it down would always take longer than the actual unfolding of the universe itself. Just like the halting problem, we can’t skip ahead at any point and we have no idea what will come next. reply worstspotgain 17 hours agorootparentSure, but the timebases are different. Maybe it took the butterflypeople a thousand butterflyweeks to write it out. Let me restate the metaphysics a bit differently. Let's say there's no us, no butterflypeople, nothing at all. Entropy reigns supreme, no information is organized. Now add the butterflypeople. They write the humanpeople's log book. Information exists in organized form. The humanpeople's bits have been divined out of the great entropic nothing. Maybe that's all it takes? reply garaetjjte 17 hours agorootparentReminds me of https://xkcd.com/505/ reply amelius 19 hours agoparentprevIf I took the binary representation of that log and XOR'ed it with a random binary string, then would the result also have observers with the same observations? reply worstspotgain 19 hours agorootparentGood question? :) I'd say no. How about an exact copy of the log book, but with one bit flipped. Voila, mostly universal physics. reply throw310822 12 hours agoparentprevNow shred that log to particles and scatter them everywhere, and you have the \"dust theory\". Neither the time dimension or the log are shared with the simulating universe, and yet they are still valid for the observers within the universe. If the sequence of the log states is entirely deterministic based on the initial state, then you don't even need to actually write down the entire log for it to \"exist\". This is Greg Egan's Permutation City. reply worstspotgain 11 hours agorootparentCan we reduce this to an estimate of survivorship bias? If there is only one universe, then our survival is clearly explained: we're in the only reality there is. If all possible universes exist, then we really lucked out in ending up in this one (well, depending on who wins the election I guess.) In the middle are the permutations selected through the filter of other realities, when they chose which universes to simulate. We lucked out but not as much, because uninteresting universes never made it out of the entropic soup. It would have to be a conditional estimate of course, because our sentience biases our contemplation. reply julianeon 14 hours agoparentprevThere's a hidden condition here. How do you know every event to every particle? The answer to that will literally change what gets written in the log book. reply kridsdale3 13 hours agorootparentThe point is the log is a graph or a tree, not an array. reply nonameiguess 1 hour agoparentprevThis kind of thought experiment seems like it breaks down due to the uncertainty principle. We can't exactly specify the full state of every particle in the universe. The universe might also be infinite and you can't enumerate an infinite set even without uncertainty, though you can write a generating function or recurrence relation for it, which seems to be Wolfram's point. But why bother with this kind of detail? What's the difference between what you're imagining here and a normal reel of film? It can be played back, but even if it isn't, it records the state of events that happened, including observers that once existed and no longer do, experiencing events that once happened but no longer do. It's possible for a record to describe a canonical sequence even if the record itself doesn't change. Somebody outside of the record can view it out of order, speed it up, slow it down, pause it, reverse it. A film reel doesn't share the time dimension of its own universe in that way. I'm struggling to come up with what this implies and why. reply oersted 12 hours agoparentprevQuick appreciation for the Douglas Adams reference :) reply buginprod 12 hours agoparentprevRandall has you covered: https://xkcd.com/505/ reply tunesmith 15 hours agoprevI like thinking about hypergraphs that continually rewrite themselves. I've thought about it in terms of literary critique, or in \"compiling\" a novel. It reminds me of petri nets in a sense, where at any given moment, a character has a static model of the world, which can be diagrammed through a causal graph of conclusions and premises. Then, an event happens, which changes their understanding of the world; the hypergraph gets rewritten in response. I've toyed with this with my own graph software when writing novels. It's of course impossible to fully document every characters' model before and after every event that affects them, but even doing so at key moments can help. I've wished more than once that I could \"compile\" my novel so it could automatically tell me plot holes or a character's faulty leap in logic (at least, one that would be out of character for them). I've also tried the more common advice of using a spreadsheet where you have a column for each character, and rows indicating the passage of time. There you're not drawing hypergraphs but in each cell you're just writing raw text describing the state of the character at that time. It's helpful, but it falls apart when you start dealing with flashbacks and the like. reply drdeca 19 hours agoprevIs any of what he’s saying here, something he hasn’t essentially already said before? The parts of this which were a little surprising to me (e.g. the bit about comparing time to heat, and the bit about running out of steps to do at an event horizon) iirc all linked to a thing posted a while ago? I don’t share his enthusiasm for the phrase “computational irreducibility”. I would prefer to talk about e.g. no-speedup theorems. reply ants_everywhere 19 hours agoparentThere's \"digital physics\" which goes back to the late 60s https://en.wikipedia.org/wiki/Digital_physics. The connection between heat/entropy and time is well explored. E.g. https://en.wikipedia.org/wiki/Arrow_of_time and https://en.wikipedia.org/wiki/Entropy_as_an_arrow_of_time reply whatshisface 19 hours agoparentprevIt has been said before, but by Stephen Wolfram. reply nitwit005 18 hours agoparentprevIt feels like this could be a perfectly decent article if he toned down his ego and referenced existing work (other than his own). But I don't think that's possible for him. reply neom 17 hours agoprevEvery time I read stuff like this I get super drawn to thinking about Sunyata* - In Mahayana buddhism, my understanding is that Sunyata doesn't mean absolute nothingness or no existence, but all things are devoid of intrinsic, independent existence. Everything is empty of inherent nature because everything is interdependent... phenomena exist only in relation to causes and conditions. This relational existence assumes that things do not possess an unchanging essence... the ultimate sense, there is no fixed reality. What might seem like \"everything\" is actually permeated by \"nothingness\" or \"emptiness\" and that phenomena arise dependent on conditions, without intrinsic, permanent nature. https://en.wikipedia.org/wiki/%C5%9A%C5%ABnyat%C4%81 reply kridsdale3 13 hours agoparentMy mind also went here when reading TFA. The all-time-all-space-all-branches brane of the Ruliad we call the Universe is the continuous one-ness and our selves are just the single-perspective projection models of that universe in our neurons that persist across edits to the neurons, until such as point as we update the model to see the larger picture and we can call that Nirvana, if we wish. reply darshanime 17 hours agoparentprevSunyata comes from Sunya, which in Sanskrit means \"zero\", another idea invented by the Indians. reply gibsonf1 3 hours agoprevI'm a big fan of Wolfram's physics project, however, he seems to be confusing thinking about physics (computation) with the continuous and ever-changing substance of the universe itself. Time is a human idea to grapple with the fact that everything is both continuous and constantly changing. Time is simply picking out from that continuous change a sequence of changes or state(s) that occur during a measured standard sequence of change, such as the earth making a single rotation around its axis (day). It helps us manage and refer to and measure both the order of changes and the duration of changes or states using standards. reply inthebin 3 hours agoparentI thought spacetime was a fundamental concept of physics which explains gravity and not merely a human invention for measuring change...? reply gibsonf1 2 hours agorootparentIndeed it is, but that fundamental concept is for human understanding of how physics works based on how we perceive/think about the universe, its not the metaphysics of the universe itself. reply wavewrangler 8 hours agoprevWolfram has always been difficult for me to follow. I think it's because he tends to drone on, I don't know why. I don't think even he knows why. My understanding of what I have managed to listen to or read is that being who we are, we don't process information fast enough in order to see much of what is around us, even while it is happening before us. An example is to take a minute under consideration, you can think about how long a minute is. It's tangible to us. It's not very long. But if we think about how long a femtosecond is, it is not tangible at all. We can't experience a femtosecond. We can experience a whole bunch of femto seconds, but not just one. This is just one example of what I perceived the meaning of his thinking to be. Is that wrong, or so far off? Not only can we not experience a femtosecond, we will never be able to experience a femtosecond because our brains are simply not fast enough and aren't built to exist at such a scale. If that's what it means, then does that mean that he is referring to our ability to exist in certain scales, and our tendency to know the scale in which we exist? And, to exist outside of that scale, requires different computational parameters? Additionally, is this an extension of dimensions, just in time, not space? Does he differentiate between the two? I know that the perception of scale has more to do with, well, perception, whereas computational irreducibility (as I understand it to be, anyway) is more of a function of natural processes....or THE underlying function from which all other functions stemming from that, are built upon. ... Right? Between that and perception of the scale in which we have evolved to exist in, it seems like they are at least closely related... Some of what has been discussed here in the comments has me doubting my understanding, is the reason I ask. To extend my question, could computational irreducibility help to explain why the Universe tends to \"recycle\" so many parts of itself? Is that some sort of telltale sign that when we see these patterns (golden ratio, fractals, recurring structures in naturee), we are looking at a fundamental aspect of the universe in some form, or it's computationally irreducible equivalent, or is this to be determined? reply ziggyzecat 8 hours agoparent> THE underlying function So this is about where it clicked for me: A function, to us normies, is something consisting of at least one part that doesn't do anything and another part that does something but has no tangible form, 'the operation'. So, to me, irreducible can only mean that there is some level where the function is the thing and vice versa, so that this irreducible function, from our (current) space-time-perspective, has no constituents except 'self'. Which is nonsense, because self is worthless without stuff it can react with or to. Except, is it really? A femtosecond can't be experienced because subpixel-sized movements/fractions of reactions happen during this short measurement. But that's irrelevant for the interface between this function and nature and evolution from their current space-time-POV and their, and thus our, space-time-blind-spots. It's like thought and action when there is not enough time to stop a movement or when stopping that exact movement would terminate the intended result. But I actually don't think that irreducibility is the right term. It should be liminality or something, focusing on the fact that nothing temporary is measurable before the emergence of THE underlying function, which is what I used to think The Planck length is for (more or less) constant space. reply Shawnecy 15 hours agoprevIs there anything testable or falsifiable here? Otherwise it's just preaching beliefs. reply kridsdale3 13 hours agoparentThat's the whole point of philosophy. reply thrance 12 hours agorootparentNot really, modern philosophy attempts to present valid arguments based on a few axioms. You can then decide for yourself if you assume these axioms yourself, in which case you also have to accept the conclusion of the argument. reply ndsipa_pomu 9 hours agorootparentSurely that's logic/maths where accepting the axioms means that the conclusion has to be accepted? Philosophy tends to be far less rigorous and can have very dubious steps so that there's often arguments where you don't accept the conclusion despite accepting the axioms. e.g. https://en.wikipedia.org/wiki/G%C3%B6del%27s_ontological_pro... reply thrance 6 hours agorootparentIMHO, the difference between math and (modern) philosophy axioms is that the latter's are way higher level (e.g. \"the world is material\", \"every humans deserves to live\"...) while the former's are very low level and concern themselves with \"simple\" rules (refer to ZFC). Philosophers also make their arguments in natural language, while mathematicians use a formal language (ultimately also described in natural language). Your exemple is interesting, as it makes a bridge between philosophy and mathematics. It's basically Gödel's attempt to prove the existence of God with mathematical rigor. It's basically a form of the original ontological argument[1] with extra flair. You can still translate the axioms into natural language, like: \"P(¬ᵠ)⇔¬P(ᵠ)\" becomes \"a property is bad if and only if the opposite property is good\" or \"P(G)\" becomes \"being God is good\". Finally, mathematicians don't usually concern themselves with \"universal truth seeking\" and are often content to add axioms as it suit them, if it means they can do intersting things (e.g. the Axiom of Choice). [1] https://en.wikipedia.org/wiki/Ontological_argument reply openrisk 19 hours agoprevSeems like an appropriate post on a day when the Nobel of Physics was awarded not for Physics discoveries but for computer science... But from Wheeler's \"it from bit\" to Wolfram's computational universes, the question is: where is the beef. Now, there might be ultimately something worthwhile with the obsession with digi-physics. Mental models that seemed disparate may merge and become fruitful. It doesnt even have to be a fully formed toolkit. Newton's invention of calculus was kinda sketchy. But he was explaining things with it, things that were not undestood before. reply WillyWonkaJr 19 hours agoparentWolfram does offer an interesting alternative to viewing the universe as a manifold with a tensor (the GR view). He believes it's a graph with computational rules. Are they the same? Mathematically, manifolds have a clear notion of dimension. This affects things like the inverse square rule. Wolfram's view of the ruliad, an evolving graph with rules, does bring up the question of dimension. But at the end of the day he needs to make a concrete prediction that differs than the current view in order to have people devote a lot of time studying his world view. He's a brilliant guy and the Wolfram Language is fantastic, but he really needs to humble himself enough to value the work of convincing others. reply bumbledraven 3 hours agorootparent> But at the end of the day he needs to make a concrete prediction that differs than the current view in order to have people devote a lot of time studying his world view Even if it doesn't make any different concrete predictions, a new way of thinking about things can attract scientists' attention. The Many Worlds interpetation of QM is an example. reply kridsdale3 13 hours agorootparentprevI honestly don't think he cares about 'mainstream acceptance'. He is a prolific publisher of his detailed thoughts, which in the pre-academic-gatekeeping-establishment era, was enough for any serious philosopher. He's a hobbyist. That doesn't make him any less prestigious if his ideas are neat. reply openrisk 12 hours agorootparentThe gatekeeping and manipulation going on in formal scientific publishing is notorious, but that is not the issue here. The fundamental algorithm of advancing physical science has always been: a set of \"principles\" or proto-concepts, a set of matching mathematical tools (that dont even need to be very rigorous), using these tools to explain a slice of reality (experimental outcomes) and, finally, predicting unknown behaviors that can be sought, can be confirmed (and celebrated). Sometimes even just a purely equivalent mathematical representation is fine, as it may give handles for calculations and thinking. But whatever the program with digi-physics is, it doesnt follow these age-old patterns that establish validity and usefulness intrinsically and not because some gatekeepers say so. The primary utility seems to be to enhance the prestige and toolkit of computational physics, which is fine, but totally not justifying the universality claims. reply XorNot 18 hours agorootparentprevWorth noting this is ultimately the problem with string theory: String theory does provide a suite of mathematical tools which can solve real physics problems and give valid answers but they're known physics problems that can also be solved with other tools. To be useful as a theoretical framework it always needed to be able to predict something which only string theory could - as a \"more accurate view of reality\". Which is the same problem here: you've got to make a prediction, an accessible prediction, and ideally also not introduce any new incompatibilities. reply visarga 2 hours agoprevSpace is distributed and time is a centralizing force. The serial action bottleneck forces the brain, for example, to unify and send one action at a time. This is also replicated in LLMs that are distributed internally, but generate one token at a time. So time is like the force of centralization while space supports the distributed side. These two tendencies are reflected in the exploration/exploitation tradeoff. The exploitation part is centralized in language and culture, while the exploration part is distributed across the components of a system. They work together to achieve intelligence, both are needed. reply lambda-research 2 hours agoprevThe idea that time is tied to computation makes me wonder if everything we see as 'progress' is just the universe showing us the loading screen percentage of the game of life. reply herodoturtle 4 hours agoprev> At the lowest level the state of the universe is represented by a hypergraph which captures what can be thought of as the “spatial relations” between discrete “atoms of space”. Time then corresponds to the progressive rewriting of this hypergraph. reply GistNoesis 10 hours agoprevI think Stephen at least dares to ask the question. Here is a little thought-experiment on the Nature of Time. You take the three body problem and you pick an initial condition and generate the trajectory of the three body from 0 to T by integrating through time with some numerical scheme like Runge-Kutta. Now you do it again, and again, generating each time a \"universe\" of three-body trajectories. Doing so allows you to build a dataset of physically realist three-body trajectories. And now the kicker : You train a diffusion model on this (potentially infinite synthetic) dataset. Once trained, to build a \"universe\" (aka 3-body trajectories) you only need to sample from this diffusion model. There is no more need to integrate through time. Past, present and future inside the universe just fold themselves into place in order to make sure the universe follows the time-evolution constraint. When working numerically, both these schemes can theoretically be as accurate as desired (error smaller than any chosen epsilon), although the diffusion model seems to potentially necessitate more memory in toy model, it's not evident as the universe is stored in a compressed fashion which necessitate less memory when the universe is no longer a toy model. The underlying question I perceive from Stephen works are is whether it's more efficient computationally to explore all possible universes simultaneously in which case time is a mere constraint you have to solve, or to generate each universe independently stepping through internal time. Although it may seems to be the same (our perception only having access to a slice of the multiverse), as in the end you get in both cases a physically consistent universe, the nature of the sampling process change the distribution of possible states. It also opens the possibility of shifting across various universes, not that we would be physically aware of (the previous universe and future universe), but we would benefit by experiencing a \"better\" universe. It's the same vibe of ideas which states that our universe has been fine-tuned for life to be possible. reply hnax 16 hours agoprevWhere it's nowadays standard practice in science to conceive of time as the dimension along which events are tagged, I would suggest the opposite: process, as a sequence of events, induces time. But also in the modern conception, time is derived from atomic events produced by a nuclear source. So, fundamentally the two conceptions are the same, but the process conception allows for greater freedom in what the underlying process may entail. reply psychoslave 11 hours agoprevOk, so after the article on time as ought to be an emergent property[1], here we go with time from a computational point of view. Can we at least receive a definition of computation that is not somehow depending of time being a given, explicitly or implicitly? Am I alone finding this a bit taking aback? Like this is not physics or even general philosophy but plain old theological focus on the prime mover. [1] https://www.quantamagazine.org/the-unraveling-of-space-time-... reply arkj 13 hours agoprevSW is the Derrida of computation. More words to add more confusion than explain anything. reply FDAiscooked 17 hours agoprevDisregard anything Stephen Wolfram says about anything other than his Mathematica software. He's a pretentious, arrogant twat who thinks he's unlocked the keys to the Universe and is trying to convince the rest of the world of his brilliance. reply fpoling 15 hours agoprevPhysics does not explain flow of time at all. If one films a thrown ball, physics can tell from few frames its speed or where the ball is on the following or previous frames. But it tells nothing about why, when see the film, we perceive the ball moving. Articles like the above misses this. In fact there is no even notion of direction of time in physics. All physical models are time-reversible. And even if we observe violation of, say, CPT, in nature, it still will not explain while we perceive time flowing in a particular direction. This is very well discussed in the book “Time’s Arrow” by Huw Price. reply Kapura 15 hours agoparentThe author discusses some of these points. One excerpt: > But even at a much more mundane level there’s a certain crucial relationship between space and time for observers like us. The key point is that observers like us tend to “parse” the world into a sequence of “states of space” at successive “moments in time”. But the fact that we do this depends on some quite specific features of us, and in particular our effective physical scale in space as compared to time. > In our everyday life we’re typically looking at scenes involving objects that are perhaps tens of meters away from us. And given the speed of light that means photons from these objects get to us in less than a microsecond. But it takes our brains milliseconds to register what we’ve seen. And this disparity of timescales is what leads us to view the world as consisting of a sequence of states of space at successive moments in time. > If our brains “ran” a million times faster (i.e. at the speed of digital electronics) we’d perceive photons arriving from different parts of a scene at different times, and we’d presumably no longer view the world in terms of overall states of space existing at successive times. > The same kind of thing would happen if we kept the speed of our brains the same, but dealt with scenes of a much larger scale (as we already do in dealing with spacecraft, astronomy, etc.). reply fpoling 15 hours agorootparentThis still misses the biggest question about the nature of time. The problem is not that we perceive the world as a set of space-like frames. The problem is why our consciousness perceives the frames moving from one to another at all and in particular direction. reply qaq 14 hours agorootparentIs it a question about nature of time or about our perception of time though? reply goatlover 14 hours agorootparentprevBecause the universe is evolving from a low entropy state to a high one. reply fpoling 12 hours agorootparentThis does not explain the flow of time nor the direction of how consciousness perceives it. A low entropy is just a low probability state. Such state in the past is just as unlikely as in future as physical models are time-reversible. Moreover, there is no evolution in physical models. The universe is just 4-dimensional thing. Surely time in physics is different from space as we can predict across time based on on the condition in 3-d space-like surface, while if one make a slice in the 4-d universe with 2 space dimensions and one time-dimension, predicting across the remaining space dimension is impossible. But that does not explain why our perception flows from one space-like slice to another and in particular direction. Surely some of the slices are less common (low entropy) then others (high entropy), but there is no movement or evolution. A good analogy is a rod with a color gradient from white on one end and black on another with white turning into black quickly so most of the rod is black. We can arbitrary call the white side first and even say that the color evolves from white to black. Then as the white side is a low probability as a randomly selected slice of the rod will be black, we can even say that the color evolves from a low probability to high probability stare. But this is arbitrary as in reality color does not evolve and there is just the single colored rod. reply marcus_holmes 12 hours agoprevIm curious about how this relates to deterministic time and the lack of free will. >Our minds are “big”, in the sense that they span many individual branches of history. And they’re computationally bounded so they can’t perceive the details of all those branches, but only certain aggregated features. And in a first approximation what then emerges is in effect a single aggregated thread of history. Does this allow free will? reply causal 4 hours agoparentI've yet to come across a satisfying definition for free will beyond \"it's not determinism but also not randomness\" reply floobertoober 5 hours agoparentprevI've actually thought about free will in the context of wolfram's ideas before, and I like the idea that our minds are computationally irreducible - I think it is a very close analogue to free will. reply _cs2017_ 14 hours agoprevI don't understand how computational irreducibility matters for the perception of time. Surely, even a computationally reducible universe could be so insanely expensive to predict that it wouldn't matter? I also don't understand why our inability to predict the future is related to our perception of time. Overall, my impression is that this is an essay in philosophy (i.e, devoid of any content) rather than science. reply hyperhello 19 hours agoprevOkay. Time is a computation. Patterned or otherwise predictable computations can be performed instantly and thus are not time. Only results that can’t be precomputed are part of our perceptions. That’s what I got out of it. reply DiscourseFan 19 hours agoprevIts certainly interesting, though the language its couched in wouldn't be found in any philosophical discussion on time. This is all to say that it deals with concepts that have been discussed in philosophy for a long time, and these insights wouldn't be considered \"new\" to someone from say mid-19th century Prussia. Certainly the \"progressive unfolding of the truth,\" in qualitatively different steps which Wolfram adopts here as his concept of time is no different from Hegel's concept of time and the movement of history. I would recommend, for anyone interested in this sort of thing, to just read the \"Preface\" to his Phenomenology of Spirit.[0] [0]https://files.libcom.org/files/Georg%20Wilhelm%20Friedrich%2... reply projectileboy 16 hours agoprevFascinating, but I really wish this work was being published as a series of papers in peer-reviewed journals. Otherwise it’s hard to take the work seriously. reply Q_is_4_Quantum 15 hours agoprevSurely Wofram deserves the Nobel as much as Hopfield and Hinton? Not for this stuff of course (which I doubt many take seriously), but because he also provided us with an amazing computational tool without which physics would be very far behind where it is today? [And at least I knew his name already unlike our current laureates whom I just had to look up!] reply tux3 15 hours agoparentThis year is an exception because of the AI Gen AI Artificial Intelligence AI AI zeitgeist. If we keep giving the physics Nobel to people building computer tools, soon it will have to be renowned physicist Linus Torvalds, whose computational platform underlies every big physics experiment. I'm not sure physicists would be thrilled if we keep going in that direction. reply CSMastermind 15 hours agoparentprevI think this is one of the rare times I feel comfortable speculating that had he not created Mathematica than someone else would have. There was a demand and plenty of people with interest. He was just in the right place with the right set of skills to execute on it before others and won the market in its infancy. Also it's a small enough market that the like of Mircosoft didn't feel the need to come in and crush him like they did Lotus 1-2-3. reply Q_is_4_Quantum 14 hours agorootparentI suspect you are right - but multiple Nobel prizes have gone to people who got there only very slightly ahead of others in the race. Would be tough to argue that there are many prizes which are for work that wouldn't have been done within a decade of when the winner actually did do it. reply DataDive 6 hours agoprevWithout even visiting the page I can predict what this writing will be about with uncanny accuracy. 1. Big words at the start - pretending to hack at a problem so big that just swinging the axe is a major undertaking 2. The prose slowly drifts to make less and less sense; words have no practical meaning anymore. 3. Simplistic images galore. Various plots via cellular automata and \"pretty\" images show things that have nothing to do with the topic and are only distant metaphors at best. Yet these images are the proof that it all \"works.\" 4. A nothingburger by the end. Leaves you wondering, why did I read all this? Every essay by Wolfram is the same. reply moi2388 5 hours agoparentYou forgot ample use of “computational irreducibility”, and “like I showed 30 years ago (proceeds to not have shown this)” but yes. Very much this. reply immmmmm 3 hours agoprevDid he tackle Lorentz invariance? reply alkonaut 9 hours agoprevIs this a guest writer? It doesn’t have the Wolfram tone at all. It describes a universe that isn’t centered on Stephen Wolfram, for example. reply sammycdubs 16 hours agoprevHe literally only cites himself in that article… https://media1.tenor.com/m/v6Awsd0YO7IAAAAd/metal-gear-risin... reply kridsdale3 13 hours agoparentSo did God. reply mensetmanusman 14 hours agoprevHow would a bag shaped universe experience time? https://youtu.be/FYJ1dbyDcrI?si=9Ga7PCeac4EV4Y4_ reply hoseja 11 hours agoprevWolfram article on the nature of reality. Cellular automaton on the first screen. reply fuzzfactor 17 hours agoprevYou have to figure time would carry on even if nothing else was happening . . . . . . at the time ;) reply ndsipa_pomu 9 hours agoparentThat doesn't seem likely. If there was nothing happening, then how could you determine one instant from another - without any change there can be no concept of time. reply fuzzfactor 6 hours agorootparent>That doesn't seem likely. Really I guess I've always felt that way when you think about it conceptually, but maybe all it has to do is be slightly more likely than time standing still while other things do not ;) You might also very well be able to say that without time there would be no concept of change either :) reply twilo 19 hours agoprevI believe it's simply a unit of measurement we use to understand the movement or rhythm on which the universe operates, so it could be termed the \"progress of computation\" if that makes more sense but it's all in the same effort. reply aaroninsf 20 minutes agoprevEverytime this work of Wolfram's comes up, I think the same thing: what this is more than anything else, is a tacit argument that the universe we inhabit and are structures/processes within, is computed in a strong sense. I.e., that we are living in a computational \"simulation,\" the substrate of which is not currently accessible. That he doesn't come out and lead with this, I find quite peculiar. I've asked him about this in person and not gotten a less cagey response. I assume that is because he does not want his theoretic hypotheticals to be binned under \"simulation theory\" and his overall world view so categorized. But I don't see another reason to pursue this line of conjecture the way he does. And as I suspect that that premise is actually true, it's all good IMO. Unrelated directly, but certainly adjacent, is that at the intersection of simulation-theories and AI, is the premise that a computed person (i.e, an AI) is uniquely situated to \"jail break\" our own reality, to exist in the framing one. (And you know, maybe it's turtles all the way down a la Flatland, so...) As Douglas Hofstadter and Daniel Dennett foregrounded, a simulated hurricane doesn't get you wet, but a simulated poem is a poem in every frame. So too travel entities defined well by computation. A good reason, if we needed one, perhaps, to get on with the business of elevating ourselves into a purely computational embodiment, I think. I'd like to pop up a level and take a look. reply curiousgeorgio 4 hours agoprevThe thing that bothers me about the idea of the \"Ruliad\" is that it's completely unfalsifiable. Even if we existed in a reality where true randomness existed, or computational irreducibility wasn't a given, you could always argue that what we observe is just one finite local slice of that Ruliad where things appear to be deterministic (or computationally irreducible) due to our boundedness as observers. It's basically the modern equivalent of \"turtles all the way down\" because it pretends to explain the nature of reality by extending our definition of reality to fit within an all-encompassing mental model that only makes sense on a surface level. Granted, the words \"universe\", \"multiverse\", etc. are insufficient in describing everything in a way that includes everything we currently want to include, but giving a new name to that abstract idea of \"everything\" isn't itself a compelling argument to also say that everything exists as a static construct and that everything is computationally irreducibile and deterministic at a fundamental level. Yes, that makes sense in a physics simulation, but in reality, we don't know what we don't know. Placing the unknown in a conceptual box doesn't imply that it's now known. reply causal 4 hours agoparentRight. It feels like conjecture built upon conjecture, I can't tell where the foundation lies. It at least needs to make some rigorous, real-world predictions we don't already have. I'm also dissatisfied with the notion of time is just \"rewriting\" of the hypergraph - that feels ill-defined. It borrows our intuition for flipping bits in physical memory, but what does \"rewriting\" actually mean in the metaphysical domain of this hypergraph? I have a lot of respect for Wolfram, but much of this feels so hand-wavy. reply inshard 12 hours agoprevComputationally unbounded observers see more of the future but what of free will? reply thrance 12 hours agoprevWolfram's theories are still largely pseudoscientific, in that way they look a lot like string theory, minus the public funding the latter received. Neither theory is really falsifiable : if new experiments are made that contradict the theory, it can just be adjusted to fit the new observations. As a consequence, those theories are unable to make any kind of prediction about our reality, which makes them pretty much useless. No wonder this \"research\" was never published in any physics journal. reply smaddox 7 minutes agoparentThis model of physics does make some falsifiable predictions, and there are discussions about how to test them elsewhere. Unlike string theory, this theory does not have any free variables to adjust. It's either true or it's false. I, for one, find it to be trivially true. It fits every observation and is the only theory ever posed that doesn't have the \"But why those initial conditions?\" problem. reply raldi 14 hours agoprevSo you can't go back in time for the same reason you can't go left in Super Mario Bros. reply lostmsu 19 hours agoprevDiscussed in Permutation City reply A_D_E_P_T 18 hours agoparentYeah. I'm in the middle of writing a book about this, but in a sense it was also discussed by the Pythagoreans. And they (correctly, I think,) went a step further: \"The Pythagoreans too used to say that numerically the same things occur again and again. It is worth setting down a passage from the third book of Eudemus' Physics in which he paraphrases their views: ‘One might wonder-whether or not the same time recurs as some say it does. Now we call things 'the same' in different ways: things the same in kind plainly recur - e.g. summer and winter and the other seasons and periods; again, motions recur the same in kind - for the sun completes the solstices and the equinoxes and the other movements; But if we are to believe the Pythagoreans and hold that things the same in number recur - that you will be sitting here and I shall talk to you, holding this stick, and so on for everything else - then it is plausible that the same time too recurs.’\" - Simplicius, Commentary on the Physics 732.23-33. Branching paths, \"all possible mathematics,\" etc. In a universe which appears to be discrete, which can support finitist arguments, and where the potential number of paths is starkly finite -- this eventually leads to the conclusion that all paths eventually recur. reply Filligree 17 hours agorootparentStrictly speaking, it only leads to the conclusion that eventually the universe will enter a loop passing through a finite number of states. There’s no requirement that the current state is part of the loop. Or indeed that any state containing conscious observers is. reply pizza 15 hours agoparentprevThe bit in Permutation City about siphoning compute by exploiting the magnitudes of vector computations as a kind of scratch space out of algorithms that only needed the resulting angles… wonder if you could modify the DoRA parameter-efficient finetuning algorithm to do something like that lol, since it also splits up the new weights into angular and magnitude components.. reply hiddencost 18 hours agoprevI really think that Wolfram's descent into fringe science has hurt a lot of well meaning people that don't know better and think that because he's developed useful software that he should be listened to in these domains. reply qaq 16 hours agoparentOh maybe because he has a PhD in particle physics from Caltech ? reply xanderlewis 14 hours agorootparentEric Weinstein also has a PhD in physics; it doesn't preclude you being (or becoming) a crank. reply gammarator 14 hours agorootparentIt’s part of their life cycle https://www.smbc-comics.com/comic/2012-03-21 reply qaq 14 hours agorootparentprevWhat is specifically crank about his theory? From outsiders perspective having theories that require a bunch of extra dimensions just to make the math work sound no less cranky. reply xanderlewis 13 hours agorootparentI'm not claiming to be qualified to judge it, but it's quite clear that no one who is takes it seriously. He also seems to spend most of his time pontificating about things he has no expertise in and using his genuine expertise in physics to show off in front of easily-impressed podcast hosts — not a great sign. reply qaq 13 hours agorootparent\" pontificating about things he has no expertise in\" again he has PhD from Caltech in particle physics he had a good number of published works in quantum field theory how are you coming to the conclusion he is pontificating about things he has no expertise in? reply XorNot 18 hours agoparentprevThe crackpot trajectory of otherwise smart people is fairly well trodden with a number of indicators and nobel laureates who have walked it - one of which is when people start stepping well outside their field...and then also tend to start stepping into \"the biggest problems\" of wherever they point themselves. reply Mistletoe 18 hours agorootparentI call it helicoptering, my old boss used to love to do it. Helicopter down onto a problem, act like everyone that already studied it was an idiot and hadn’t spent their life trying to solve X, stir a bunch of dust up, accomplish nothing, and helicopter away again to something else. reply nyc111 12 hours agoprev\"(as I’ve argued at length elsewhere)\" Everything he writes is \"at length\". This looks like an interesting read with good ideas but it is so long and has no structure that I gave up reading. It may help to give an abstract in the beginning of the article. The problem with the treatment of time in physics is that we can only measure time intervals not the philosophical Time (with capital T). But physicists gladly conflate the two. Mach said: Absolute time [the philosophical Time] cannot be measured by comparison with another motion, it has therefore neither a practical nor a scientific value. Which means that all of the \"t\" terms standing for time in astronomical equations are for time intervals and tell us nothing about the philosophical Time. reply akomtu 17 hours agoprevTime and space probably belong to consciousness, rather than the real world. The objective \"true\" reality may be utterly incomprehensible in its complexity, but we can imagine a \"slice\" of that reality that arbitrarily defines space and time so that the interior of that slice follows some reasonable rules. That slice of reality can be thought of as a high-level consciousness that defines rules of our physics. Other slices of the same reality are possible, GR-like or QM-like, including those that are computational and discrete in nature. One universe, but many interpretations. Within each slice of reality, it may be possible to define smaller subsets of reality, corresponding to smaller consciousness, down to the human or even more primitive levels. So what Wolfram is describing may be true, objectively, to the observers of a computational slice of the universe, just like the MWI may be simultaneously true to the observers of the MWI slice of reality. reply vivzkestrel 15 hours agoprevwhen you die, people say that your time has ended. Does anyone know scientifically speaking what happens to time for a dead person reply mistermann 19 hours agoprev> If we were not computationally bounded, we could “perceive the whole of the future in one gulp” and we wouldn’t need a notion of time at all. Maybe, if we assume we aren't axiomatically bound, despite knowing that we are (but that knowledge is rarely in context, so we can only know it sometimes...once again: time...weird). \"Thought is Time.\" - Jiddu Krishnamurti reply downboots 18 hours agoparent> perceive the whole of the future in one gulp \"Therefore, as regards such knowledge, they know all things at once\" Summa reply Vecr 16 hours agoparentprevYou could perceive (maybe? Depends on how it's hooked up) a future (a simulation based the information you have), but there's no reason to think that's what the future is with certainty. Map/territory stuff too. reply mistermann 14 hours agorootparent> but there's no reason What is it that you refer to here? reply Vecr 1 hour agorootparentYou can't exactly predict the future unless you have all the information, even theoretically. reply jpitz 17 hours agoprevAlmost like time is the stack and space is the heap. Meh. Almost. reply zaptheimpaler 19 hours agoprevI think he's a quack trying to torture an explanation of the universe out of his pet theory that uses a lot of words to say simple things but doesn't predict anything. If \"time is what progresses when one applies computational rules\" then how is the order in which the rules are applied defined in the first place? Computational irreducibility is a neat idea but i'm not sure its novel or something that explains the entire universe. My basic intro course on differential equations taught us that the vast majority of them cannot be solved analytically, they have to be approximated. I don't know if the irreducibility idea is anything fundamentally different than saying some problems are hard, whether its non analytical equations or NP hard problems. reply kouru225 17 ho",
    "originSummary": [
      "The post explores the computational view of time, suggesting that time is the universe's ongoing computation, and due to computational irreducibility, we cannot predict the future or \"jump ahead\" in time.",
      "It discusses how our perception of time as linear is due to our computational limits, while fundamentally, time may be multithreaded, and our experience is shaped by our bounded exploration of the ruliad, a concept representing the entangled limit of all computations.",
      "The conclusion challenges traditional concepts like time travel, aligning the computational view of time with the Second Law of thermodynamics, which states that entropy, or disorder, tends to increase over time."
    ],
    "commentSummary": [
      "Stephen Wolfram and Julian Barbour propose that time is an emergent property, suggesting a universe that is fundamentally timeless and contains all possible states.",
      "Barbour's perspective is that time emerges from static geometric relationships, whereas Wolfram attributes it to our computational limitations within a timeless computational framework.",
      "While both theorists agree on a timeless foundation for reality, Wolfram's ideas are often viewed as speculative and philosophical, lacking empirical support, and relate to concepts like eternalism and block universe theories."
    ],
    "points": 334,
    "commentCount": 232,
    "retryCount": 0,
    "time": 1728427321
  },
  {
    "id": 41781457,
    "title": "A modest critique of Htmx",
    "originLink": "https://chrisdone.com/posts/htmx-critique/",
    "originBody": "A modest critique of Htmx At work, we really like the basic simple idea of Htmx. Based on using Htmx in a non-trivial user interface, across a team, we’ve found that the following cases are actually not simple and are quite complicated. Inheritance of Htmx properties is a definite mistake Across pieces of code, it’s very surprising and it’s implicit. Like in CSS, inheritance is a cheap hack but you get what you pay for. It contradicts the author’s reasonable argument of locality of behaviour. It’s not local, it comes from all the way up there or some other module. Pretty much dynamic binding. Default inheritance differs across various properties (e.g. hx-delete is not inherited, but hx-confirm and hx-ext are). So you have to remember these exceptions and you end up just being explicit about everything, which means inheritance is pointless. Most interesting web apps cannot replace wholesale a DOM element Because DOM elements almost always have browser-local state, such as the open/closed state of aelement, the input of anelement, the open/close state of a dropdown element (which, note, is not encoded by an attribute of the element when you click it). All of this state is lost if you replace outerHTML directly with the naive happy path of Htmx. Even morphdom overwrites some things you’d expect it not to, so we had to patch it to avoid messing with input elements, and details elements. Storing state in the DOM element itself is a bad idea Morphdom is intended to correct the pains of the previous heading, but we discovered that the way that Htmx works assumes it’s based on replacing elements wholesale: it stores the request queue for the element on the DOM element itself. When you kick off a request, either from this element or from another that points to it, you have a request queue. Some bad failure modes are avoided by wholesale-replacing the DOM element, as the queue is reset. But with morphdom, the queue is retained because the element is retained. You’re now in a sort of undefined behavior land, where the designs of Htmx are violated. The default queuing mode is bonkers By default, Htmx will cancel requests that are in-flight if you trigger another request on the same queue (element). That’s the default strategy. We discovered this afterwards. It’s highly unintuitive, it meant we were losing work. Event triggers are non-local Event triggers often help to make things happen, but they’re a non-local effect, and suffer from similar issues as property inheritance. A bit of DSL work in the server-side language can help with this, but it feels like old-school JavaScript callback-based programming to some extent; where you “subscribe” to an event happening and do something. Component state cannot be maintained very well A broader problem, similar to the DOM element state issue, is that your own components have their own state. E.g. if you want a page that consists of three sections that have their own state that the server needs (e.g. which page of a set of results) and state that some e.g. React or WebComponents need, then you have a problem of synchronising state between a parent component and the child component. Htmx does not provide a good story for this. We have some ideas, but they all have big caveats: use query parameters, use hidden form inputs, use event triggers. React and Halogen (see also Halogen is better than React at everything) do have an answer to this. In both cases, child components have their own state, and parents can give them “props” which are pretty much “advice”, and they also have their own internal state, and can choose to ignore/take precedence over props. The props are typically sourced from the server or derived from the server, and the state is usually some client-side state. We often do need to use React for off-the-shelf components or components that we have to use that are just provided as React. React and Htmx do not interact nicely. We’ve done some unsatisfying work with WebComponents, but those things have bizarre limitations that are surprising. We’ve also made a bridge directly to React components that we use from our server-side language, but in general Htmx and React fight for control over the flow of state and management of DOM elements. We’ve played with Alpine, which is nice, but it represents yet another client-side-programming library and is therefore redundant if React is already in your codebase. The up sides Our current thinking is that being able to use your server side language is a huge obvious and uncontroversial win, no one on the team would want to go back to writing all this business logic in TypeScript: No serialisation from our DB types to frontend types is needed. No data leaks, and no GraphQL needed. We can use our (in our opinion) more powerful abstraction facilities of the server-side language. We can use the form builder in our server side language; instead of doing one frontend and also one backend implementation of the same validations. But the above downsides are real. Htmx-in-React? An attractive future direction might be to re-implement Htmx in React: The server sends a JSON blob that React converts into virtual DOM components. That would solve the component state problem. It would mean we require no special bridge to use React components. It would let us use our React-connected web fetching library, and carefully avoid the queuing choices made by Htmx. It would solve the morphdom problems and browser DOM input elements problem, too, which is pretty much a solved problem in React. In this way, we could drop the Htmx dependency but retain the benefits of the idea. That is, given a budget to embark on such a big piece of work. © 2024-08-20 Chris Done Read more posts →",
    "commentLink": "https://news.ycombinator.com/item?id=41781457",
    "commentBody": "A modest critique of Htmx (chrisdone.com)287 points by wibwobble12333 22 hours agohidepastfavorite137 comments recursivedoubts 21 hours agoThese all look reasonable to me. I especially go back and forth on attribute inheritance (it can be disabled via the htmx.config.disableInheritance option) Three of the criticisms boil down to the fact that client-side state doesn't always play well w/htmx swaps (especially the simple ones) which is absolutely true. And events can get crazy. They are powerful, but crazy and at times hard to debug. Such is event-driven life. The one thing I don't agree with is the default queuing mode: it is not to cancel an existing request and replace it. Instead it is to keep the current request in flight and queue one and only one additional request. I'd need to sit down w/them to see if they were misinterpreting something, using the hx-sync attribute to implement the behavior they mention, or if there is a bug. I would also like to take this opportunity to market our mug for people who don't like htmx: https://swag.htmx.org/products/htmx-sucks-mug reply ksec 17 hours agoparentI just want to say thank you. Not only because of HTMX, but for being a model and showing what proper \"Engineering\" should be. Knowing trade - offs and accepting the fact no solution is perfect. Although that may be because you have a background of Industrial Engineering. reply Narhem 1 minute agorootparentI know there are other libraries which offer similar features (alpine.js), but none are as simple and focused as HTMX. It seems like such an elegant solution I’m surprised more people haven’t started using it. It just works. reply Nathanael_M 19 hours agoparentprevMan, you’re everywhere. I have a Montana’s restaurant in my city and I’m scared to go in because of you. reply recursivedoubts 19 hours agorootparentyou should be reply dullcrisp 20 hours agoparentprevBy queue only one additional request, do you mean cancel any existing queued request? reply recursivedoubts 20 hours agorootparentthe code is a little gnarly, but if you don't specify anything the default behavior is to keep the \"last\" event that comes in while a request is in flight: https://github.com/bigskysoftware/htmx/blob/1242977d11bebe56... and that dumps the existing request queue and puts request created by the last event in it by itself: https://github.com/bigskysoftware/htmx/blob/1242977d11bebe56... we don't cancel the current request or issue the next request until the current request finishes but there could be a bug in the code, for sure, it's pretty crazy reply jrochkind1 4 hours agorootparentI had to read and think carefully enough about what you just explained (on second try!) to feel like I understood it, so I'm not at all surprised if other people are confused about or misinterpret what they are seeing! (I'm not an htmx user though). Actually, I guess, then, OP just had an off-by-one error? Imagine requests [r0, r1, r2 ... rN], where r0 is still in flight... OP thought r0..r(n-1) would be cancelled, in fact just r1..r(n-1) will be cancelled (I think?). Or maybe OP understood it but just mis-described it! I am curious to hear the reasoning/use cases for this choice being the default strategy. reply dullcrisp 20 hours agorootparentprevI thought they were complaining that any request is being cancelled by a subsequent one, since they wanted all the requests they made to go through (presumably the requests are altering state?) Probably I misunderstood what was meant by “losing work” though. reply recursivedoubts 20 hours agorootparentyeah i don't know if I understand what they were saying either regardless if you have a lot of events flying around then updating the UI via hypermedia exchanges is going to be a bad idea, as I mention here: https://htmx.org/essays/when-to-use-hypermedia/#if-your-ui-s... reply angra_mainyu 19 hours agorootparentprev>https://github.com/bigskysoftware/htmx/blob/1242977d11bebe56... This seems to be a scenario where switch/case blocks could make the elif-trees a bit easier to read. Also, the code could use some care about not going so deep into the Vs: // request headers if (requestAttrValues.noHeaders) { // ignore all headers } else { for (const header in headers) { if (headers.hasOwnProperty(header)) { const headerValue = headers[header] safelySetHeaderValue(xhr, header, headerValue) } } } could just be: // request headers if (!requestAttrValues.noHeaders) { Object.keys(headers) .filter(hdr => headers.hasOwnProperty(hdr)) .forEach(hdr => safelySetHeaderValue(xhr, hdr, headers[hdr])); } Not even sure if that hasOwnProp check is needed, unless header keys are explicitly set to undef. reply recursivedoubts 19 hours agorootparentyeah i prefer just plain ol' if statements, i find them easier to debug reply heavensteeth 12 hours agoparentprevI cannot express the pleasure I felt seeing prices in my local currency automatically, without the Shopify \"Zoinks! Looks like you're on our American store, would you like to change to our Polish store?\" modal I've come to know and hate. Thanks. reply troupo 12 hours agorootparentOften changing to local currency isn't just a straightforward x*(exchange rate). There could be weird taxes applied reply echoangle 4 hours agorootparentAnd regional pricing, although maybe less relevant for hardware reply dzonga 7 hours agoparentprevthe good thing about htmx / javascript or even a framework like Vue - is that the authors know the web browser is not a 'pure' platform as React people try pretend it to be. because of the event system on the web - things yeah are weird. And thanks for bringing intercooler.js / htmx as alternatives to a crazy world. reply nickpeterson 21 hours agoparentprevI’ll definitely pile on with the inheritance causing issues. It made me feel like unsetting them constantly defensively. reply recursivedoubts 21 hours agorootparentYou can disable it globally, see htmx.config.disableInheritance: https://htmx.org/docs/#inheritance reply orangetable99 3 hours agoparentprevyeah, inheritance enabled by default bit me in the ass more than once. With template engines you end up trying to debug some weird behavior and it takes some time for you to realize somewhere up in the tree on a different file there's an hx-* tag being inherited. I should have disabled it early in the project, too late now. I also still haven't figured out how to properly use the \"save history to local storage\" thing. Often there has been a server state change between the user navigating away and clicking the back button and I see no option other than disabling the thing altogether. reply Havoc 19 hours agoprevNot on this in particular but in general: I’ve held off on diving into front end because it’s just such a circus. So many options, so many opinions, so much criticism and then as if that wasn’t enough the whole fkin meta changes monthly. We’re doing react. No actual wasm. Wait no static page. Or maybe htmx. Or vanilla js. Or maybe a mix…well call it remix…it just never ends I do believe everyone involved means well and aims for technically strong outputs but my good the end result is still fuckin chaos. Backend and systems programming has people with strongly held opinions and flame wars too but somehow it feels more like a war between gentlemen and less The Purge chaos. reply shepherdjerred 16 hours agoparentI just don’t understand this opinion. Backend (and frontend!) can be use stable technology choices if you want to. There are also plenty of bleeding edge libraries/frameworks/languages that come out every week. As an example, how do you want to deploy your backend? VPS, severless, Kubernetes? What database? Which language? Which libraries? There are boring stable answers to the above for backend, just like there are boring stable answers to frontend. Really, I think you’ve just found those boring stable options for the backend and you haven’t done that for the frontend yet. React has been out for more than a decade and it’s the dominant way of building UIs. And there are pros and cons to this with 1000 solutions, but, really, plain React will work for most things you want to do. reply Lutger 8 hours agorootparentReact proper may be more or less stable since hooks came out 6 years ago, on its own it isn't sufficient. You still need to think about react-router, mobx, redux, nextjs, remix, vite, tailwind or mantine or material ui or whatever. And some of these have changed radically from year to year. The react ecosystem is not stable. It is unavoidable to have a truly gigantic amount of (transitive) dependencies, and its rare you go a month of updates without it breaking your app. It is true that for backend it is more or less the same, however it is a matter of degree. If frontend is all you do, it may be managable, but for full-stack devs who also need to manage infra it can sometimes be overwhelming. reply alexanderchr 5 hours agorootparent> It is unavoidable to have a truly gigantic amount of (transitive) dependencies, and its rare you go a month of updates without it breaking your app. I've maintained a sizeable react app for 8 years, and this has not been my experience at all. Maybe 10 years ago things would break monthly but nowadays things are quite stable, with very reasonable deprecation schedules and upgrade paths. I have lots of code running that I haven't touched in 5+ years and it is not causing any problems even when I'm keeping the main parts of the appliaction up to date with modern practices. reply shepherdjerred 5 hours agorootparentprev> react-router, mobx, redux, nextjs, remix, vite, tailwind or mantine or material ui or whatever You don't need any of this (aside from a bundler). That's my whole point. You can use these libraries and be on the cutting edge, or go with something simpler. You'll have the exact same problem on the backend if you want to use a library for everything. reply __jonas 5 hours agorootparentprev> You still need to think about react-router, mobx, redux, nextjs, remix, vite, tailwind or mantine or material ui or whatever You really don't need any of this though, you can get super far with React alone, and the new docs on react.dev are great at explaining this and pushing this idea in my opinion. reply mattgreenrocks 6 hours agorootparentprevThe churn in React keeps me out. I also get really suspicious of tech that spawns mini cottage industries, where there has to be a React-ified wrapper for every single thing. Maybe in 2025. I've learned Svelte 5 and think it is pretty good for the tradeoffs it makes though. And I think MPA is the right default for 95% of sites. reply jrochkind1 4 hours agorootparentprevI think part of the issue is that the \"boring stable\" choice for front-end, React, is disliked by many, and often doesn't actually seem very boring (if that means easy to understand how to do it right) or stable (if that means doens't drastically change much) to many. reply zonethundery 4 hours agorootparentprevI think the responses to your comment show the whirlwind of 'wtf?' confronting any programmer contemplating front end development for the first time. What you wrote is probably true (and the one \"see how far you can get in react only\" comment is probably a decent path, but the landscape is overwhelming. reply LegionMammal978 4 hours agorootparentYeah, the frontend scene definitely suffers from hype-driven development: it's not good enough that we have a tool to do the job, we need the tool to be the bestest ever on every possible metric, so we end up with a treadmill of additional frameworks, bundlers, etc. that are hyped up in any given year. E.g., I might think to ask, \"Which tool creates the output with the lesser overhead on the client, Webpack or Vite?\" But I can't find anything solid about that, since everyone's too busy hyping up the dev experience or whatever. It's a shame that you have to swim against the tide so heavily, if you value simplicity over immediate kitchen-sink levels of functionality. Personally, I've landed on pure client-side React (with any bundler, or with Babel alone if you're feeling adventurous) since it doesn't try to have any purpose other than updating components according to state. Many of its competitors have too much poorly-documented magic for my taste. reply WD-42 15 hours agorootparentprevWhat exactly is \"plain React\" though? Not sure I've ever seen a project that used only React and nothing else. It would seem you'd need a build step, at the minimum. reply shepherdjerred 15 hours agorootparentYes, you would need a build step. create-react-app has existed for a very long time and works quite well though advanced applications tend to outgrow it. Alternatives are Webpack (stable, boring), Vite (starting to overtake Webpack since it's significantly simpler), and 1000 other libraries. Again, there is some movement here, but Webpack has been in a dominant position for nearly a decade and there's no reason you can't continue to use it today if you value stability. It's the equivalent of choosing to host your backend on a VPS instead of trying out serverless. Needing a build step is part of the complexity of frontend. I think frontend is getting much simpler with build tools like Vite and the addition of ES modules. But, even with these new things coming out, you can stick to your boring stable choices on the frontend just like you can on the backend. reply WD-42 15 hours agorootparentYea I'm not really buying it. create-react-app is deprecated. https://react.dev/learn/start-a-new-react-project points you directly at Next.js as a *starting point* and the section about using React without a framework does it's best to discourage you from doing it. This is insane for beginners. reply chrisweekly 5 hours agorootparentprevCRA is dead. Vite has emerged as its successor. reply nsonha 12 hours agorootparentprevYou use a lot of words to respond to such a minor point. The main point is that actual \"plain react\" is useless. You need at least a router, some way to manage state, some way to manage style and so on. It's why nextjs is so popular, they are practically \"plain react\" reply __jonas 5 hours agorootparentUseless is a massive exaggeration. > You need at least a router You only need a router if you want to build a SPA with different routes, not all web applications are that! Tons of practical React apps are just a single screen, or even embedded component. > some way to manage state You do not need an external library for complex state management, the React docs have some great guides on how to achieve this with plain React: https://react.dev/learn/scaling-up-with-reducer-and-context > some way to manage style Plain CSS and the JSX style property are entirely enough for most apps. If you are using a build tool like Vite, you can also do CSS modules for scoping styles to your React components for more complex applications. I do agree that styling is not a strong point of React, but I don't think pulling in a dependency is necessary. reply nsonha 3 hours agorootparent> https://react.dev/learn/scaling-up-with-reducer-and-context I just wanna vomit reading that article (which is in official docs!!!). There is a much cleaner way of managing state with just a single `useState` per component instead of bringing over some Redux dogma for some inexplicable reason: ``` const [state, dispatch] = useState() // ... dispatch(doThis) dispatch(doThat) ``` reply __jonas 2 hours agorootparentWell of course component state is what you reach for first, which is why it is the first four sections of those docs about state management I linked: https://react.dev/learn/managing-state I was linking this later section about extracting state logic into a reducer, because you suggested that you would need \"something for state management\", I assumed you were talking about something like redux. But of course you can get pretty far with a single useState and prop drilling as well. Not sure how it supports your point that 'plain react is useless', it sounds like you are arguing against yourself with that comment? reply agos 10 hours agorootparentprevFYI, create-react-app is basically abandoned since two years. Maybe it still works but if there's any bug, good luck reply roywiggins 15 hours agorootparentprevYou only really need a build step to use JSX, which is strictly speaking optional. reply Lutger 8 hours agorootparentTechnically you don't even need react, you can do it all in javascript! Joking aside, you really do need to solve the problems jsx solves, but also webpack, next.js, react-router and depending on your application, something like mantine or tailwind. These are the minimum I think. If you have a smallish project you can get by hand-rolling your own solutions, but it doesn't make the problems themselves go away. In fact, it is exactly the claim in this thread that there are a lot of beaten paths in backend languages with relative stability over time, whereas in frontend it is 1) not clear what the beaten path is (there isn't one) and 2) when there appears to be favored solution, it often changes radically or disappears after just a couple of years. For example, create-react-app used to be the default way to start a react app, it is now deprecated. But for python, Django has been a stable beaten path for many, many years. Same for Rails. C# also tends to change every couple of years, but there is always just one single Microsoft recommended way to do things. Sure there are many experimental libraries and contenders in these languages as well, but there isn't really anything with the same maturity and stability in frontend. reply shepherdjerred 15 hours agorootparentprevThis is true (and part of why I love React/JSX) but in practice nobody writes JSX by hand. reply cowoder 10 hours agorootparentIsn't it the other way around, nobody writes React by hand they all use JSX? reply shepherdjerred 5 hours agorootparentYou're right :) reply WD-42 16 hours agoparentprevMany others including myself have made this observation. I think I've finally formulated why there seems to be such a stark difference between FE and BE/systems cultures. The backend and systems programmers are still writing code in a programming language. So much of FE dev now is stringing together other people's declarative frameworks. When programming is done it's mostly glue. So if you exist in this paradigm you're going to have very strong opinions about whatever framework/tech you are using because it's so hard to operate outside of it. You also gain a vested interest in the survival of whatever you've chosen. This is why I've been slowly trying to make the transition out of web dev, at least FE. I don't want to deal with this crap anymore! reply mattgreenrocks 5 hours agorootparentWebdev is a different culture for sure. I think it brings a lot of newer devs in, which is fine, but then their lack of experience makes them vulnerable to groupthink, choosing easy over simple, demagoguery (\"it's different this time!\" seems true if you don't have prior experience) and needing to jump to the newest flashy thing. I'd also argue that this shift implicitly devalues the actual building of things (which is hard to assess and requires expertise) in favor of social clout (which is easy to assess), but this could just be coincidental with the rise of social media. Regardless, there is brand-building you can do and people routinely conflate social clout with technological prowess. I gave up on webdev when this started to set in. Honestly, it felt like a bunch of kids chasing thoughtful engineering out by simply making more noise. Everyone wanted a magic library that relieved them of thinking, but the only way to make decent software is to think deeply about what data flows where, and how it gets there. I've since rekindled an interest in it after finding some tech that did click with me: Quarkus + Renarde, htmx, Svelte. All of these are off the beaten path and have users that choose to use them, versus those that have to do, and I really think it makes a difference. > This is why I've been slowly trying to make the transition out of web dev, at least FE. Systems/backend dev is a lot more friendly to thoughtful engineering IMO. Check out Java, Golang, and Rust and see what libraries/users you click best with. reply stephenhuey 2 hours agorootparentPretty cool, I hadn't heard of Renarde and it looks promising. Around 8 years ago, I really wanted Aurelia to win the SPA wars but they're still pretty niche even though they've been growing lately. But honestly, after playing with everything over all these years, I still believe a server-side framework with minimal amounts of JS is ideal for most projects, especially all those internal corporate web apps but also for B2B stuff and even the average consumer-facing app. People say consumers are more demanding now, but how many developers are really working on something that caters to the most demanding consumers? Unless you need the most snazzy UX ever, just don't. And honestly, using Rails or other server-side frameworks, you can get very far with way less effort than those expensive front-end teams by simply using Hotwire: https://hotwired.dev Sending down rendered HTML using Hotwire Turbo requires far less time, and HTML over the wire is in reality no heavier than sending down JSON. If you absolutely need a bit more interactivity on the front-end while avoiding a server roundtrip, it's easy to drop in little Stimulus JS controllers as-needed. From my journeys to and fro in the real world, I've seen most projects do not need more than that, and are arguably wasting budget trying to use heavier tools than that! For most sizeable projects, you can do more in Rails & Hotwire with fewer developers than a 6-person team using their favorite server side language and React. I'm not saying you have to use Ruby on Rails. I'm saying I wish the dev world would embrace this paradigm in whatever their favorite language/frameworks are. reply jrochkind1 4 hours agorootparentprevI don't know, I'm a backend programmer myself (although I dabble in front-end, and indeed, don't love it), but \"the frontend programmers aren't really writing code in a real programming language\" claim just sounds like an old greybeard writing in assembley complaining about how programmers that don't know and write in assembley are lazy and don't really know how to program. Iterate and repeat with every new development technology. Don't get me wrong, the front-end certainly seems like a mess to me too. Although I think it's actually not quite as bad as it used to be, maybe over the hump one can hope. reply fnordlord 5 hours agorootparentprevBeware that backend has plenty of those declarative frameworks also... Kubernetes, terraform, most CI environments, CMake. (I'd add SQL but since it's the only one that doesn't drive me crazy, I don't like to think of it as declarative) I don't know FE development enough to speak on it but I agree with your sentiment in general. When I see \"declarative,\" I think \"learn by memorization and trial and error rather than reason and intuition.\" reply mattgreenrocks 5 hours agorootparent> Beware that backend has plenty of those declarative frameworks also Thankfully, they don't take over your application code, though they eat a considerable amount of brainspace, especially in the collective discourse. Discretion and focus remain key skills in tech these days. reply jerf 4 hours agorootparentprevBackend code lives together better too. It's trivial for me to set up a frontend web server that points one URL to a handler in Python, another in Go, another in Java, and whatever else I like. Said frontend can also do a lot of useful abstraction over things like login and authentication, and even to a limited extent authorization. Backend does not compose perfectly but it composes together reasonably well. This architecture may be problematic for other reasons, but it isn't intrinsically a huge problem right out of the gate. Frontend, you largely have to pick a team. Trying to run multiple frontends in the same page has a number of problems, if nothing other than each of them gets so large that even just one can impact performance, but they will also fight over events, and you can't cross the streams, etc. The whole structure is more winner-take-all, by its nature. reply confidantlake 3 hours agorootparentprevBackend, the land of no frameworks. Not a Rails, Django, Spring, or Laravel in sight! reply wpietri 9 hours agorootparentprevAh, this makes a lot of sense to me. Thank you. Interestingly, I think this problem exists in areas of back-end work. I know people who mostly do systems integration, where they're customizing some enterprise behemoth. But since that world is anchored both by the big vendors and by your company's choices of them, it seems saner to me. People may identify as a SAP developer or whatever, but they don't have to advocate for it or have a quasi-religious belief in it. reply papruapap 5 hours agorootparentprevI see BE coding more and more in configuration files nowadays YMMV. reply cageface 16 hours agoparentprevUser interfaces have different, inherent complexity that backend programming doesn't have. If you look at the Android & iOS native toolkits they've churned at least as much in the same time. Also, and it seems like this has to be pointed out in every one of these threads, the complexity of interfaces we're building on the web now is far greater than what we were building 10 years ago. reply gyomu 11 hours agorootparentI have UIKit (iOS) code that’s over a decade old and still compiles and runs perfectly (it might look a bit off on new hardware form factors - eg content bleeds into the notch - but that’s about it). I have Python + Tkinter code that’s close to two decades old and still runs great (doesn’t look great tho but it didn’t look great at the time either). I have some vanilla JS websites that are about as old and still work great. As other commenters have noted, boring stable choices are there. Ignoring them for the fancy novel brittle stuff is entirely on the programmer. reply arvinsim 10 hours agorootparentI bet all of your examples are not something you deploy to a modern application today. All of those are good in theory but don't stand against the demands of modern consumers. reply gyomu 8 hours agorootparentBet lost. If as a technologist you can’t imagine companies making money selling software to happy customers without relying on the brittle trendy framework du jour, my only advice would be to spend less time making silly bets with strangers on internet discussion boards and more time looking at boring profitable companies. reply mattgreenrocks 5 hours agorootparentA lot of devs justify the time they spend keeping up with the technological Joneses as necessary to keep pace with current trends (especially in UI). I think it is a bit of a cope though, because users just want to solve their problems. They don't care about React or htmx. They just want something that fixes the problem. This is actually very freeing: meet a real need and you don't have to believe that software development consists on running on a treadmill of constant changes for their own sake. Just do the job competently. reply seanhunter 4 hours agorootparentprevIt must be very tiring to keep up with the demands of all modern consumers. There are so many of them, and they are so different from each other. Meanwhile when I go out in the world I see all manner of front-ends being actively used. It's almost like different users want different things idk. reply mrbombastic 15 hours agorootparentprevNot sure why this is downvoted without comment, many people underestimate the complexity here. reply skydhash 6 hours agorootparentUi is practically a problem solved (from the technical perspective). The complexity you’re talking about stems from trying to solve the problem with the wrong technology (the DOM). Laying out text for a document is fundamentally different from creating an application interface. An things that are easy to do in any UI toolkit (from GTK to Android) can only be hacked in the DOM. Things like layout constraints (between two components) and lists. reply kitsune_ 12 hours agorootparentprevThere indeed is complexity, but a lot of the projects that are created to deal with that complexity that gain traction are made by people who don't have much experience in general, or lack exposure to non-frontend programming, so you end up having to deal with a lot of half-baked stuff. It's kind of a lethal cocktail. reply stephenhuey 3 hours agoparentprevI entered the working world in the aftermath of the dotcom bust when JS was a nightmare of footguns (much more than now) and Microsoft literally had zero developers working on IE, the most-used browser in the world, for YEARS. Web dev was therefore stuck in an archaic prison and there was much rejoicing the day Microsoft announced they were once again assigning a team of developers to work on the browser. Fast forward a decade and a lot of terrible things in JS were being rectified at the language level. A dozen years ago, I tried a SPA for the first time and it seemed cool to have a framework (Angular, the first version) to provide more abstractions than jQuery and Backbone. However, the groupthink bandwagony insanity that ensued was ridiculous, and I felt like such an old codger trying to tell the kids to just be grateful that JS was finally working pretty well and move on and build stuff instead for a while instead of spending so much energy remaking the tools every 5 weeks. I'm not trying to be dramatic--I really do feel like companies have no idea how much they overspent on web app development in the past decade versus how much more working code they could've gotten for the money they spent. It literally felt like people were creating extra work, and had no idea how much easier it was than a few years before. reply eddd-ddde 16 hours agoparentprevThis is so true. Anyone trying to learn frontend as a newbie today has an unreasonably hard task at hands. So much unwarranted complexity. I feel like the backend is just an inherently simpler problem to solve, request in, response out. Largely stateless. Of course there are harder things to do like stateful 2-way connections, but the solutions are mostly concentrated on a couple of technologies. Like yeah we have stuff like graphql, but at the end of the day the principle is the same. reply shadow28 16 hours agorootparent\"Backend\" is absolutely not a simpler problem to solve, it's an umbrella term encompassing a wide variety of domains. If you're talking specifically about Web backends, then sure, maybe there's a case to be made there. reply arvinsim 10 hours agorootparentprevWhich is why it puzzles me that the \"frontend devs are not real programmers\" meme still exists. Like if it is so easy, then why can't backend devs do it themselves? reply backbeginning 1 hour agorootparentWe can, management just insists on hiring JS kids to do it a thousands times more complicated. \"Just use a sane PHP framework\" will get you fired now. Alas. The problem is thinking that all of these things are different domains when they really aren't. The web browser is just a program; its job is to fetch resources and render HTML. If you know how HTTP and HTML/CSS work, you can absolutely make great web applications. But this requires knowledge and expertise, and also maybe not shoehorning an entire application framework into what was designed to be a collection of hyperlinked documents. I suspect this has more to do with money than anything else... the web is dead, long live the web. reply skydhash 6 hours agorootparentprevComplex tasks are different from complex tools. reply recursivedoubts 19 hours agoparentprevi don't know really what to do about it, but i did write a book on the ideas of hypermedia and how htmx extends them that you can read online for free here: https://hypermedia.systems i think the ideas of hypermedia are fairly stable and relevant, particularly the concept of a uniform interface, even if htmx is just one implementation of them reply Jgrubb 18 hours agoparentprevI considered myself a frontend dev 10 years ago, I loved jQuery and Angular 1.x, Sass. I never really caught the Node fever, and it seems like almost everything connected with the JS scene since then has been constant churn and complexity. I moved into infrastructure and data and never looked back. That said htmx is the only remotely intriguing development I've seen in frontend dev in a decade. reply confidantlake 4 hours agorootparentSaying you love Angular 1.x while complaining about complexity is an interesting take. reply 91bananas 16 hours agorootparentprevI too loved jQuery, we've used Vue which is on 3.x now and doesn't really seem like it will change terribly much, in the future. It solves all of the things that in jQuery became really difficult while still feeling like there is still a DOM somewhere in there and we're not all that far removed from jQuery style js. Will agree though, most of the newer frameworks I just don't get. reply palmfacehn 11 hours agorootparentprevIE is dead, document.querySelector and friends are sufficient where jQuery was needed. Instead of basking in this achievement, frontend tooling has heaped on bloated build steps with thousands of dependencies. According to the current trends this is the standard way to populate a dynamically updated table or serve a static landing page. reply diggan 18 hours agoparentprev> then as if that wasn’t enough the whole fkin meta changes monthly. We’re doing react. No actual wasm. Wait no static page. Or maybe htmx. Or vanilla js. Or maybe a mix…well call it remix…it just never ends Who told you to do all those changes? Maybe change who you're listening to, or even better: don't blindly follow what others say/talk about, but listen and think about it thoughtfully instead. Stay on whatever framework/library/platform you want and feel works best for you and your use cases. No one is forcing you to chase the latest trends. reply jfengel 17 hours agorootparentTrue, but it does suck to have backed the wrong horse. You framework/library/platform can rot if nobody else is using it. You can't get away with just ignoring it. It helps to pick \"boring tech\", and hope that what has worked will continue to work. But it's still unpleasant to hear about the X killer every week, when X is your bread and butter. reply rjh29 15 hours agorootparentHasn't Rails been traditional server with patched with dynamic parts since like 2005? I would pick that reply colordrops 16 hours agorootparentprevI've been ignoring frameworks and focusing on mostly vanilla JS and have worked on some of the most interesting and well paying projects for 15 years now. Haven't been following any of the hype, even Typescript. Was forced to do some React/Typescript at work for the first time last year and it was fine. Was able to get up and running in a few days. reply squidsoup 15 hours agoparentprev> I’ve held off on diving into front end because it’s just such a circus Its only a circus if you're constantly chasing the new shiny thing. Many of us have been productively building apps in react for nearly a decade without significant changes in tooling. reply WD-42 13 hours agorootparentReact how though? Are you raw-doggin React.CreateElement? Doubt it. So qualify \"building apps in react\" with whatever bespoke build system, router, state management, etc combination you are actually using. Just because you found yourself a tent inside the circus doesn't mean it's not a circus. reply arvinsim 10 hours agorootparentHow is that any different from using Django in Python? Laravel in PHP land? reply andyp-kw 15 hours agorootparentprevWhile I don't think react is the best frontend framework, it has become the industry standard and therefore is the most peaceful approach to building apps in 2024. reply klysm 15 hours agorootparentprevThis is the way reply wvbdmp 18 hours agoparentprevYes, this is exactly why you choose Htmx. So you can focus on web fundamentals. Reason about HTTP and HTML. Sometimes throw in some vanilla javascript when absolutely necessary. Pull in one or two hand-picked zero-dependency libs for things like drag and drop or a nicer date-picker or something. reply wg0 11 hours agoparentprevYou should then look at the circus that is CJS and ESM compatibility [0] [0]. https://sokra.github.io/interop-test/by-tool reply sensanaty 9 hours agoparentprevHow people don't get bored of this same exact thread in every single one of these posts is beyond me. But also as someone who does fullstack in a pretty much even 50/50 split, this is such a tired meme by this point seemingly from people who just refuse to actually sit down and think about the FE ecosystem for longer than 3 seconds and end up parroting this same talking point. You could make a similarly dramatic post about the backend world. There you're even fighting which programming language you want to pick. PHP, Ruby, Go, Elixir/Erlang, C#? What about the recent mess with ElasticSearch and Redis? Where and how do you host your backend? Which DB do you use and why? Mongo, anyone? At this point in the FE framework world, you've got 3 very stable players that have been there for a decade. Angular, React and Vue. And sure, they've each gone through some sort of reworks in the past, but nobody's forcing you to pin your dependencies `@latest`, you can pretty much always keep trucking along with whatever version you were on. We've got a huge app still running along on Vue 2 (not even 2.7 which backports the very nice Composition API) and we see no reason to bump versions to 3. I've even had some nightmare scenarios trying to update Rails and Laravel in the past that dwarf any problems I ever encountered in the frontend world. Also, vanilla JS, HTML and CSS still exist and work just fine if you're building some small toy thing, but you'll quickly realize why these frameworks exist in the first place (or you'll do the dumb thing of building your own as some do) if you're doing anything more complex than a brochure website. And the tooling has only gotten simpler if anything with Vite. Want to start a new frontend project? It's literally just `npm create vite@latest`, and you get a nice TUI with which you get to fully customize what you want and don't want. The README even comes with a nice little list of commands for you to run, and hosting can be done literally anywhere where you can deploy the `dist/` folder to, just like the \"good\" old days. For people who always claim superiority for being the genius backenders they are, they sure seem to be having a lot of trouble with something that they will themselves say any coding monkey coming out of a bootcamp can do with relatively few problems. reply teqsun 4 hours agorootparentAs a fellow full-stack I agree. There's a joke I've told many times in interviews if they ask me to define what a full-stack engineer is, to which I joke \"It means I do whatever the senior engineer doesn't want to do, which 99% of the time is front-end\". The people I tell it to laugh, because it's funny but also because it's true. On the front-end you're dealing with more customer/users and it's inherently more of a servile role, while it's much easier to be stubborn, eccentric, 'powerful' in the tech-facing backend. I'd also wager there's a certain \"tech purity\" angle too, given the inherent reverence foisted upon lower level languages and programming fields that is seemingly rife. To play devil's advocate on myself, perhaps it's more that the higher end technical complexity challenges for Front-End aren't commonplace enough. I've had the fortune to work in a field where the level of necessary technical complexity on the front-end part of my job is very high, so ymmv reply confidantlake 4 hours agorootparentprevFor real. React or superObscureFrameworkSixPeopleUseFrom2009? The churn is endless! In the Java world alone: Spring vs EE vs Play vs Quarkas vs Micronaut vs Helidon... Tomcat vs Jetty vs Netty... Jar vs War vs Ear ... Maven vs Gradle vs Ant... reply kccqzy 4 hours agoparentprevMy preferred way of writing frontend code is ClojureScript with Reagent. That hasn't changed in eight years since I discovered Reagent eight years ago. Just don't chase fads. Stick to things that you know are working fine. reply lucis 21 hours agoprevWe have been using HTMX to create performant storefronts and the results are satisfactory. https://farmrio.deco.site/ is one of the largest clothing retailers in Brazil and all the frontend interactions use HTMX, alongside a partial rendering strategy we developed. More info at https://deco.cx/en/blog/htmx-first-class-support reply s6af7ygt 13 hours agoparentThe example on the blog post is one of those that makes me severely question HTMX and the stuff that we're doing. Doing HTTP requests to increase a counter, or affecting any local-only state change at all, seems so wild to me. reply karmarepellent 12 hours agorootparentIt makes me question what we are doing too. I'm using HTMX extensively at work, but I never use it to only update local state. A few lines of Javscript on the client will do that. However I think it's a powerful solution to updating the DOM when state changes on the backend and a roundtrip is required anyway. For that matter the examples on the official project site are better to understand proper use cases for HTMX [0]. Personally I use Active Search and Edit/Delete Row a lot. While Active Search does not change state in the backend, I found it's still a valid use case for HTMX when searching large data sets, since the backend is (in my case) just way faster to search through a large data set. It all comes down to your ability as a developer to find the proper solution to the problem at hand. A counter does not need HTMX. [0] https://htmx.org/examples/ reply halfcat 4 hours agorootparentprev“Counter app” is basically “hello world”, not how best practice is conveyed. If you’re using an HTTP request to update a counter, it would be to update the persistent server-side state of that counter (which you’d also do if you’re using React and a JSON API). No one is advocating for using HTMX for purely client-side state. They’ve been very consistent about this, recommending Alpine, vanilla JS, Stimulus, Vue, and so forth when you need pure client-side state. reply lelandfe 20 hours agoparentprevLove how good the CLS is. I wonder if you can deal with flashes of white during navigation with the new View Transitions API reply lelandfe 4 hours agorootparentOh, it's because the site does `*{visibility:hidden}` during loading. Don't do that, show us the intermediate state >:) You're artificially making your FCP also be your TTI, which means page navigation, when everything should be cached and fast, feels slow. That's not something e.g. Lighthouse tells you. I recommend showing the page right away, even if there's going to be jank. Jank/Cumulative layout shifts can be fixed later. reply boredtofears 15 hours agoparentprevI'm not sure this is htmx's fault but I wouldn't exactly describe that as snappy reply heavensteeth 12 hours agorootparentI thought that too, but whole page loads are slow too; I assume it's just because I live nowhere near Brazil. reply dfabulich 19 hours agoprevThis \"HTMX in React\" idea just reinvented React Server Components. https://react.dev/reference/rsc/server-components An attractive future direction might be to re-implement Htmx in React: • The server sends a JSON blob that React converts into virtual DOM components. • That would solve the component state problem. • It would mean we require no special bridge to use React components. • It would let us use our React-connected web fetching library, and carefully avoid the queuing choices made by Htmx. • It would solve the morphdom problems and browser DOM input elements problem, too, which is pretty much a solved problem in React. In this way, we could drop the Htmx dependency but retain the benefits of the idea. That is, given a budget to embark on such a big piece of work. RSC is still experimental, and the default implementation assumes that you're going to run JS on the server, which undermines some of the point of HTMX. But someday (likely in the next year or so) the RSC wire format will be standardized, and then any language can generate RSC JSON for React to consume. reply sauercrowd 20 hours agoprev> The default queuing mode is bonkers > By default, Htmx will cancel requests that are in-flight if you trigger another request on the same queue (element). This seems like the only default that's reasonable to me don't know if the author has a specific example in mind, but if a user submits an input, changes that input mid-request and submits again the request needs to be cancelled, otherwise the UI will be inconsistent by showing a response to an outdated input Just processing one response after the other won't be possible if a response swaps out content with different IDs, so a second response won't be able to be swapped out in the same way reply naasking 5 hours agoparent> don't know if the author has a specific example in mind, but if a user submits an input, changes that input mid-request and submits again the request needs to be cancelled, otherwise the UI will be inconsistent by showing a response to an outdated input If you queue them and process responses in order, that would also be correct. Probably has fewer failure modes. Some indicator that an element is still being processed would help to prevent user confusion though. reply stavros 20 hours agoprev> An attractive future direction might be to re-implement Htmx in React: The server sends a JSON blob that React converts into virtual DOM components. Isn't this exactly what HTMX was replacing? The client needing to have a ton of logic and state? reply NoGravitas 6 hours agoparentYeah, a lot of the article was reasonable, talking about implementation details they don't like, but once they started talking about problems with local state I feel like they went off the rails. The server needs to be the source of truth for application state, with the application pushing state changes through POST/PUT/DELETE requests. reply stavros 5 hours agorootparentI agree, no matter how much we don't like reloads, the fact that they reset your whole client state makes things much simpler. reply klysm 14 hours agoparentprevTurns out there’s good reasons to do that reply nsonha 12 hours agorootparentOnly veteran web devs are able to come up with such a bonker idea that not managing state explicitly is somehow better. That's just basic programming but I guess they don't need that to write html. reply wordofx 9 hours agoprevReading the complaints about state makes me think the author has never built websites prior to things like react existing. The examples don’t even make sense, it just sounds like someone trying to use htmx like it’s react and then getting upset cos it doesn’t work how he expect it to. Lack of experience. reply jasoncartwright 21 hours agoprevAfter hearing about it for years, and it apparently being popular in the Django community, I tried htmx for the first time just this morning on a very simple task in an admin interface. It's easy to put unobtrusively in place, fun to play with, and it worked. Reminded me of using jQuery for the first time. Would I use it for a complex and involved project? Initial impressions suggest probably not. reply candiddevmike 20 hours agoparentIt's jQuery but for HTML. Same spaghetti result, different language. reply recursivedoubts 20 hours agorootparentmaybe so, but sometimes spaghetti is delicious: https://htmx.org/essays/a-real-world-react-to-htmx-port/ reply hahahacorn 20 hours agoprevIs there an equivalent to Turbo Mount for Htmx? https://evilmartians.com/chronicles/the-art-of-turbo-mount-h... I think it's one of the best ways to use Hotwire/Turbo. Default to Hotwire to whatever degree of understanding you have, and as soon as it feels like HTMX/Hotwire isn't the right tool, you can easily switch to your framework of choice. reply sauercrowd 20 hours agoparentVery much agree with you on this - htmx (turbo, hotwire, alpine,...) are not trying to replace all your client side logic. It's a spectrum of interactivity, and we should try to pick the best (whatever that may mean) tool possible for the job - sometimes that's hx-boosted links, sometimes it's a react app reply pier25 18 hours agoparentprevwhat about using web components? reply ricardobeat 21 hours agoprev> Even morphdom overwrites some things you’d expect it not to, so we had to patch it to avoid messing with input elements, and details elements. Would love to hear more about this issue. Preserving the state of elements like input or detail is the main function of libraries like morphdom, something must have gone wrong there. reply recursivedoubts 21 hours agoparentat the end of the day morphdom (and my own algorithm, idiomorph) can only do so much, because the current DOM APIs nuke much of the state of nodes when they disconnect from the DOM if you look at the algorithms they give up very quickly and simply merge the new nodes in once matching doesn't work at a certain level because of this there is a new API coming down the pipe, moveBefore(), that will allow much better morphing/preservation in general: https://github.com/noamr/dom/blob/spm-explainer/moveBefore-e... htmx supports it today if you enable it in chrome canary: https://htmx.org/examples/move-before/ reply can3p 7 hours agoprevLocal state is indeed a problem that's exacerbated by swapping logic. Simple example: you have a form + a collapsible block inside or maybe a dynamic set of inputs (imagine you're adding items to a catalog and you want to allow to submit more than one). If you save the form and simply swap the html the block state will be reset. Ofc you could set the toggle state somewhere to pass it along to the backend, but that's a pain already compared to spa approach where you don't even bother with that since no state is reset. You could you query string as mentioned in the article, but that's not really convenient when done in a custom way for every single case. Having said that I think that a way to go could be a community effort to settle on the ways to handle different ui patterns and interactions with some bigger project to serve as a testing ground. And that includes backend part too, we can look at what rails does with turbo My opinion is that htmx (and similar) approach is different enough to potentially require a different set of ui interactions and it usually hurts when one tries to apply react friendly interactions to it. reply reidjs 6 hours agoprevI set up HTMX for a toy project recently, seemed fine to me. I think the philosophy of HTMX is the real selling point, sending structured content instead of passing JSON around. There is an opportunity here to either improve HTMX or build a competitor. reply chromanoid 5 hours agoparentFor the record with https://hotwired.dev there is already a rather successful \"competitor\". reply rsyring 5 hours agoparentprevFor a more batteries included option, see https://unpoly.com/ reply Daril 10 hours agoprevBefore HTMX I used Turbo Hotwired and Unpoly. From my very personal and pragmatic point of view (I am a lone developer : I need to create software solutions that are robust and that simply work without the need to spend months to learn a new technology, that probably will be replaced by a new one in two months or so, that no customer is willing to pay), HTMX is the most pragmatic and useful solution. It is complete in features and you can do literally anything with it and is easy to integrate in existing web applications. I used it with CodeIgniter and now with Golang. reply chromanoid 6 hours agoprev\"HTMX in React\" seems to be a very complicated idea. Seems to me like Vaadin territory in terms of complexity. I think you have to embrace the MPA approach otherwise you will not be happy with any MPA technology. reply begoon 10 hours agoprevHas anyone used htmx for a more or less large project? Everything turns into spaghetti code really quickly. Separation of API and frontend isn’t just for fun. It’s for making testing bearable. Finally, mixing data modelling (database) with presentation (html output) in one place (htmx handler) goes out hand really quick too, making testing much harder. But real coding cowboys from the golden age don’t write tests, right? :-) reply hu3 2 hours agoparentI have, using htmx before htmx existed. Similar concept and functionality. jQuery plugin attach events like \"x-post\" and other attributes, sends data to server which always returns HTML. 800+ CRUD pages full of business logic, around 2k routes, single PHP server, single MySQL server, serves up to 3k requests per second seasonally, P95 around 50ms. Team still adding and changing features every 2 weeks, even after years in production. Stack is custom PHP framework, MySQL, custom jQuery plugin that acts similar to htmx. Onboarding is dead easy. It was made with a no-build frontend stack. Meaning there's no build pipeline to understand and fight against. I look at React/SPA misuses and self inflicted pain, and feel sorry for them. reply NoGravitas 5 hours agoparentprevDepending on what you mean by large, I have. Testing is just the same as on any server-side MVC application. And I don't get spaghetti, because the structure is provided by the backend framework. Same with the data modelling - there's a model layer in the backend project that can be tested independently of everything else. I can only guess that you're trying to put all your logic and structure on the front-end, with the backend only serving what the front-end needs ad-hoc? Don't do that. Build a classic MVC app (Django, Rails, ASP.NET MVC), factor your views into small, reusable components, and use HTMX to replace page elements at that level. reply yawaramin 20 hours agoprev> React and Htmx do not interact nicely. I want to dig into this a bit. React of course maintains its own rendered part of the DOM and htmx trying to reach into and change any part of that DOM is not going to go over well. It's just going to be replaced with React's rendering again on the next render cycle. htmx provides two points at which it can interact with other libraries or frameworks: DOM events and CSS classes. I don't see any problem with classes, but React's synthetic events would probably not work well with htmx (or any other non-React library really). Maybe frameworks like Preact which use real DOM events would work better. reply anentropic 5 hours agoprev> An attractive future direction might be to re-implement Htmx in React: > The server sends a JSON blob that React converts into virtual DOM components. I don't understand what is left of HTMX if you do this. Isn't that just React? reply manchmalscott 14 hours agoprevThat last point about dropping HTMX as a dependency but keeping the benefits is why I personally find Phoenix liveview so compelling. reply ipnon 13 hours agoparentYes, I'm all in on Phoenix and LiveView because of this. reply dsego 20 hours agoprev> Htmx-in-React?: The server sends a JSON blob that React converts into virtual DOM components. The author should try inertia.js, it has server-side routing and react templates. reply teqsun 4 hours agoprevI don't have anything personally against htmx, but the general sentiment I've seen tossed around about it has led me to believe that some real hellmesses are going to be calcified in it by \"IDGAF about the FE\" devs which will inevitably be foisted upon some poor intern to \"improve\". reply JodieBenitez 12 hours agoprevComments about HTMX vs. React keep popping up, but I think they miss the point and it's probably a \"marketing\" problem on HTMX side if people feel the need to write these. On the contrary, Unpoly -which shares similarities with HTMX but is probably more high level- makes it very clear on its front page: Progressive enhancement for HTML Get powerful new HTML attributes to build dynamic UI on the server. Works with any language. Gracefully degrades without JavaScript. Granted, HTMX has \"high power tools for HTML\" right on its front page, but it also has \"reduced code base sizes by 67% when compared with react\". Anyway, as always, tools ad trade-offs... reply nprateem 11 hours agoprevFor state, just create a container with x-data and use alpine, then let htmx replace the content. Works pretty well, but you can end up with needing some events which are a mess to keep track of. No use if you want react though... reply andrewmcwatters 19 hours agoprevTotally disagree with state in the DOM. It’s natively where it exists. reply steve_adams_86 16 hours agoparentAlso in the URL. Both are totally okay, and even desirable. We’ve become so accustomed to managing state in abstractions of the DOM that this seems like a crazy idea, but it has led to all kinds of pain points from complexity to, well, the DOM and URL not always reflecting the state of the application accurately. It’s pretty awful. The better we can keep state out of abstractions without losing maintainability or performance, the better. reply imtringued 9 hours agoparentprevnext [–]What an amazing way of handling state! Or the part where you use data tags for something that should be done with JSON... reply naasking 5 hours agorootparentWhy \"should\" it be done with JSON? reply rrgok 45 minutes agorootparentWhat are the other ways? XML inside the attribute value or splitting up in a bunch of data attributes? reply WesolyKubeczek 5 hours agoprevSuperficially, the cure the article is proposing smells way worse than the poison: implement htmx in React. I don’t know, maybe it really solves their problem, but it just feels so… wrong. I don’t know. (Shakes head, gesticulates, exits the scene) reply wetpaws 20 hours agoprev [–] As a former PHP dev it's fun to see people reinventing the same wheels and hitting the same pitfalls industry solved 2 decades ago. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A critique of Htmx identifies several issues, including problematic property inheritance, which is implicit and inconsistent, leading to confusion and necessitating explicit declarations.- Htmx faces challenges with DOM element replacement, state storage, and queuing mode, which can result in loss of browser-local state, flawed state storage, and unintuitive request handling.- Despite integration issues with React, Htmx offers benefits when used with server-side languages, potentially eliminating the need for TypeScript, serialization, and GraphQL, with a suggestion to re-implement Htmx in React to address these concerns."
    ],
    "commentSummary": [
      "The critique of Htmx focuses on challenges such as client-side state conflicts and event complexity, which can be problematic in larger projects.",
      "The discussion includes comparisons to React, highlighting the ongoing debate about frontend complexity and the suitability of different tools.",
      "Despite criticisms, Htmx is valued for its simplicity and effectiveness in specific tasks, underscoring the importance of selecting the appropriate tool for each project."
    ],
    "points": 287,
    "commentCount": 137,
    "retryCount": 0,
    "time": 1728418797
  },
  {
    "id": 41784591,
    "title": "Addition Is All You Need for Energy-Efficient Language Models",
    "originLink": "https://arxiv.org/abs/2410.00907",
    "originBody": "Computer Science > Computation and Language arXiv:2410.00907 (cs) [Submitted on 1 Oct 2024 (v1), last revised 2 Oct 2024 (this version, v2)] Title:Addition is All You Need for Energy-efficient Language Models Authors:Hongyin Luo, Wei Sun View PDF HTML (experimental) Abstract:Large neural networks spend most computation on floating point tensor multiplications. In this work, we find that a floating point multiplier can be approximated by one integer adder with high precision. We propose the linear-complexity multiplication L-Mul algorithm that approximates floating point number multiplication with integer addition operations. The new algorithm costs significantly less computation resource than 8-bit floating point multiplication but achieves higher precision. Compared to 8-bit floating point multiplications, the proposed method achieves higher precision but consumes significantly less bit-level computation. Since multiplying floating point numbers requires substantially higher energy compared to integer addition operations, applying the L-Mul operation in tensor processing hardware can potentially reduce 95% energy cost by element-wise floating point tensor multiplications and 80% energy cost of dot products. We calculated the theoretical error expectation of L-Mul, and evaluated the algorithm on a wide range of textual, visual, and symbolic tasks, including natural language understanding, structural reasoning, mathematics, and commonsense question answering. Our numerical analysis experiments agree with the theoretical error estimation, which indicates that L-Mul with 4-bit mantissa achieves comparable precision as float8_e4m3 multiplications, and L-Mul with 3-bit mantissa outperforms float8_e5m2. Evaluation results on popular benchmarks show that directly applying L-Mul to the attention mechanism is almost lossless. We further show that replacing all floating point multiplications with 3-bit mantissa L-Mul in a transformer model achieves equivalent precision as using float8_e4m3 as accumulation precision in both fine-tuning and inference. Subjects: Computation and Language (cs.CL) Cite as: arXiv:2410.00907 [cs.CL](or arXiv:2410.00907v2 [cs.CL] for this version)https://doi.org/10.48550/arXiv.2410.00907 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Hongyin Luo [view email] [v1] Tue, 1 Oct 2024 17:53:28 UTC (316 KB) [v2] Wed, 2 Oct 2024 15:34:12 UTC (314 KB) Full-text links: Access Paper: View PDF HTML (experimental) TeX Source Other Formats view license Current browse context: cs.CLnewrecent2024-10 Change to browse by: cs References & Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer (What is the Explorer?) Litmaps Toggle Litmaps (What is Litmaps?) scite.ai Toggle scite Smart Citations (What are Smart Citations?) Code, Data, Media Code, Data and Media Associated with this Article Links to Code Toggle CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub Toggle DagsHub (What is DagsHub?) GotitPub Toggle Gotit.pub (What is GotitPub?) Links to Code Toggle Papers with Code (What is Papers with Code?) ScienceCast Toggle ScienceCast (What is ScienceCast?) Demos Demos Replicate Toggle Replicate (What is Replicate?) Spaces Toggle Hugging Face Spaces (What is Spaces?) Spaces Toggle TXYZ.AI (What is TXYZ.AI?) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower (What are Influence Flowers?) Connected Papers Toggle Connected Papers (What is Connected Papers?) Core recommender toggle CORE Recommender (What is CORE?) About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs. Which authors of this paper are endorsers?Disable MathJax (What is MathJax?)",
    "commentLink": "https://news.ycombinator.com/item?id=41784591",
    "commentBody": "Addition Is All You Need for Energy-Efficient Language Models (arxiv.org)256 points by InvisibleUp 14 hours agohidepastfavorite81 comments shrubble 5 hours agoI remember that many years ago, when floating point computation was expensive for Intel CPUs to do, there were multiple ways that programmers used integer trickery to work around this. Chuck Moore of Forth fame demonstrated taking the value, say 1.6 multiplied by 4.1 and doing all the intermediate calculations via integers (16 * 41) and then formatting the output by putting the decimal point back in the \"right place\"; this worked as long as the range of floating point values was within a range that multiplying by 10 didn't exceed 65536 (16 bit integers), for instance. For embedded chips where for instance, you have an analog reading with 10 bits precision to quickly compute multiple times per second, this worked well. I also recall talking many years ago with a Microsoft engineer who had worked with the Microsoft Streets and Trips program (https://archive.org/details/3135521376_qq_CD1 for a screenshot) and that they too had managed to fit what would normally be floating point numbers and the needed calculations into some kind of packed integer format with only the precision that was actually needed, that was faster on the CPUs of the day as well as more easily compressed to fit on the CDROM. reply dajoh 3 hours agoparentWhat you're describing is called fixed point arithmetic, a super cool technique I wish more programmers knew about. Proper finance related code should use it, but in my experience in that industry it doesn't seem very common unless you're running mainframes. Funnily enough, I've seen a lot more fixed point arithmetic in software rasterizers than anywhere else. FreeType, GDI, WPF, WARP (D3D11 reference rasterizer) all use it heavily. reply kccqzy 3 hours agorootparentI have worked on firmware that has plenty of fixed point arithmetic. The firmware usually runs on processors without hardware floating point units. For example certain Tesla ECUs use 32-bit integers where they divide it into four bits of integer part and 28 bits of fractional part. So values are scaled by 2^28. reply phkahler 1 hour agorootparent>> The firmware usually runs on processors without hardware floating point units. I'm working on control code one an ARM cortex-M4f. I wrote it all in fixed point because I don't trust an FPU to be faster, and I also like to have a 32bit accumulator instead of 24bit. I recently converted it all to floating point since we have the M4f part (f indicate FPU), and it's a little slower now. I did get to remove some limit checking since I can rely on the calculations being inside the limits but it's still a little slower than my fixed point implementation. reply sitkack 5 minutes agorootparentThe other great thing about going fixed point is that it doesn't expose you to device specific floating point bugs, making your embedded code way more portable and easier to test. 32b float on your embedded device doesn't necessary match your 32b float running on your dev machine. reply aatd86 3 hours agorootparentprevWhat do they use? Not float I hope. Plus given that some currencies have different precisions... Don't tell me it's rounding errors over trillion monies?! :o) reply Maxatar 2 hours agorootparentAs I indicate in another post, I work in finance and I use binary floats. So do a lot of others who work in the industry. I sympathize with people who think that IEEE floating points are some weird or error prone representation and that fixed point arithmetic solves every problem, but in my professional experience that isn't true and systems that start by using fixed point arithmetic eventually end up making a half-assed error prone and slow version of floating point arithmetic as soon as they need to handle more sophisticated use cases like handling multiple currencies, doing calculations involving percentages such as interest rates, etc etc... The IEEE 754 floating point standard is a very well thought out standard that is suitable for representing money as-is. If you have requirements such as compliance/legal/regulatory needs that mandate a minimum precision, then you can either opt to use decimal floating point or use binary floating point where you adjust the decimal place up to whatever legally required precision you are required to handle. For example the common complaint about binary floating point is that $1.10 can't be represented exactly so you should instead use a fixed integer representation in terms of cents and represent it as 110. But if your requirement is to be able to represent values exactly to the penny, then you can simply do the same thing but using a floating point to represent cents and represent $1.10 as the floating point 110.0. The fixed integer representation conveys almost no benefit over the floating point representation, and once you need to work with and mix currencies that are significantly out of proportion to one another, you begin to really appreciate the nuances and work that went into IEEE 754 for taking into account a great deal of corner cases that a fixed integer representation will absolutely and spectacularly fail to handle. reply vidarh 1 hour agorootparentIt really depends on your need. In some countries e.g. VAT calculations used to specify rounding requirements that were a pain to guarantee with floats. I at one point had our CFO at the time breathing down my neck while I implemented the VAT calculations while clutching a printout of the relevant regulations on rounding because in theory he could end up a defendant in a court case if I got it wrong (in practice not so much, but it spooked him enough that it was the one time he paid attention to what I was up to). Many tax authorities are now more relaxed, as long as your results average out in their favour, but there's a reason for this advice. reply fluoridation 2 hours agorootparentprevThe industry standard in finance is decimal floating point. C# for example has 'decimal', with 128 bits of precision. On occasion I've seen people who didn't know any better use floats. One time I had to fix errors of single satoshis in a customer's database because their developer used 1.0 to represent 1 BTC. reply myst 2 hours agorootparentprevEvery half-competent software engineer knows about fixed point arithmetic, my friend. reply phkahler 1 hour agorootparent>> Every half-competent software engineer... You meant 8192/16384 right? I like q14. reply kragen 3 minutes agoparentprevSure, FRACTINT is called FRACTINT because it uses fixed-point (\"integer\") math. And fixed-point math is still standard in Forth; you can do your example in GForth like this: : organize; gforth Gforth 0.7.3, Copyright (C) 1995-2008 Free Software Foundation, Inc. Gforth comes with ABSOLUTELY NO WARRANTY; for details type `license' Type `bye' to exit : %* d>s 10 m*/ ; : %.type ; ok 1.6 4.1 %* %. 6.5 ok Note that the correct answer is 6.56, so the result 6.5 is incorrectly rounded. Here's how this works. In standard Forth, putting a decimal point in a number makes it a double-precision number, occupying two cells on the stack, and in most Forths the number of digits after the decimal point is stored (until the next number) in the variable dpl, decimal point location. Here I've just decided that all my numbers are going to have one decimal place. This means that after a multiplication I need to divide by 10, so I define a subroutine called %* to do this operation, in terms of the standard subroutine m*/, which multiplies a double-precision number by a single-precision number and divides the result by a divisor, and the standard subroutine d>s, which converts a double-precision number to a single-precision number. (Addition and subtraction can use the standard d+ and d- words.) I also need to define a way to print out such numbers, so I define a subroutine called %. using Forth's so-called \"pictured numeric output\" which prints out an unsigned double-precision number inserting a decimal point in the right place with \"hold\", after printing out the least significant digit. Then I invoked %* on 1.6 and 4.1 and %. on its result, and it printed out 6.5 before giving me the \"ok\" prompt. (There's probably a better way to do %*.) If you want to adapt this to use two decimal places: : %* d>s 100 m*/ ; : %.type ; redefined %* redefined %. ok 1.60 4.10 %* %. 6.56 ok Note, however, that a fixed-point multiplication still involves a multiplication, requiring potentially many additions, not just an addition. The paper, which i haven't read yet, is about how to approximate a floating-point multiplication by using an addition, presumably because in multiplication you add the mantissas. Forth's approach to decimal numbers was a clever hack for the 01970s and 01980s on sub-MIPS machines with 8-bit and 16-bit ALUs, where you didn't want to be invoking 32-bit arithmetic casually, and you didn't have floating-point hardware. Probably on 32-bit machines it was already the wrong approach (a double-precision number on a 32-bit Forth is 64 bits, which is about 19 decimal digits) and clearly it is on 64-bit machines, where you don't even get out of the first 64-bit word until that many digits: 0 1 %. 184467440737095516.16 ok GForth and other modern standard Forths do support floating-point, but for backward compatibility, they treat input with decimal points as double-precision integers. reply andrewla 2 hours agoparentprevI recall playing with FRACTINT, which was a fractal generator that existed before floating point coprocessors were common, that used fixed point math to calculate and display fractals. That was back when fractals were super cool and everyone wanted to be in the business of fractals, and all the Nobel Prizes were given out to fractal researchers. reply candiddevmike 4 hours agoparentprevAFAIK this is still the best way to handle money/financial numbers. reply amanda99 3 hours agorootparentThat's got nothing to do with perf tho. reply Maxatar 3 hours agorootparentNothing to do with perf is a strong claim. If you genuinely don't care about performance you can use an arbitrary-precision rational number representation. But performance often matters, so you trade off precision for performance. I think people are wrong to dismiss floating point numbers in favor of fixed point arithmetic, and I've seen plenty of fixed point arithmetic that has failed spectacularly because people think if you use it, it magically solves all your problems... Whatever approach you take other than going all in with arbitrary precision fractions, you will need to have a good fundamental understanding of your representation and its trade-offs. For me personally I use floating point binary and adjust the decimal point so I can exactly represent any value to 6 decimal places. It's a good trade-off between performance, flexibility, and precision. It's also what the main Bitcoin implementation uses. reply fluoridation 2 hours agorootparentHuh? Bitcoin uses integers. The maximum supply of BTC in satoshis fits in 64 bits. JS implementations that need to handle BTC amounts use doubles, but only by necessity, since JS doesn't have an integer type. They still use the units to represent satoshis, which works because the maximum supply also fits in 53 bits, so effectively they're also using integers. Anyone who uses binary floating point operations on monetary values doesn't know what they're doing and is asking for trouble. reply wbl 1 hour agorootparentSo if I want to price a barrier in Bermudan rainbow via Monte Carlo I should take the speed hit for a few oddball double rounding problems that are pennies? reply fluoridation 1 hour agorootparentI mean, you do you. People generally don't complain if you're a couple hundred nanoseconds (if that) late. They do complain if your accounts don't add up by a single penny. reply wbl 1 hour agorootparentThe quoting of something exotic like this is not well defined to the penny. It's transactions where people really care about pennies. reply dwattttt 5 hours agoparentprevThat particular trick is known as fixed point arithmetic (not to be confused with a fixed point of a function) reply asadalt 3 hours agoparentprevthis is still true for many embedded projects. like pi pico (2040) uses a table. reply visarga 12 hours agoprev> can potentially reduce 95% energy cost by elementwise floating point tensor multiplications and 80% energy cost of dot products It this were about convolutional nets then optimizing compute would be a much bigger deal. Transformers are lightweight on compute and heavy on memory. The weakest link in the chain is fetching the model weights into the cores. The 95% and 80% energy reductions cited are for the multiplication operations in isolation, not for the entire inference process. reply woadwarrior01 7 hours agoparentPre-fill (even in the single batch case) and multi-batch decoding are still compute dominated. The oft repeated trope of \"decoder only transformer inference is bottle-necked on memory bandwidth\" is only strictly true in the single batch decoding case, because you're mostly doing vector matrix mults when the batch size is one. reply SuchAnonMuchWow 11 hours agoparentprevIts worse than that: the energy gains are when comparing computations made with fp32, but for fp8 the multipliers are really tiny and the adder/shifters represent a largest part of the operators (energy-wise and area-wise) and this paper will only have small gains. On fp8, the estimated gate count of fp8 multipliers is 296 vs. 157 with their technique, so the power gain on the multipliers will be much lower (50% would be a more reasonable estimation), but again for fp8 the additions in the dot products are a large part of the operations. Overall, its really disingenuous to claim 80% power gain and small drop in accuracy, when the power gain is only for fp32 operations and the small drop in accuracy is only for fp8 operators. They don't analyze the accuracy drop in fp32, and don't present the power saved for fp8 dot product. reply bobsyourbuncle 6 hours agorootparentI’m new to neural nets, when should one use fp8 vs fp16 vs fp32? reply reissbaker 5 hours agorootparentBasically no one uses FP32 at inference time. BF16/FP16 is typically considered unquantized, whereas FP8 is lightly quantized. That being said there's pretty minimal quality loss at FP8 compared to 16-bit typically; Llama 3.1 405b, for example, only benchmarks around ~1% worse when run at FP8: https://blog.vllm.ai/2024/07/23/llama31.html Every major inference provider other than Hyperbolic Labs runs Llama 3.1 405b at FP8, FWIW (e.g. Together, Fireworks, Lepton), so to compare against FP32 is misleading to say the least. Even Hyperbolic runs it at BF16. Pretraining is typically done in FP32, although some labs (e.g. Character AI, RIP) apparently train in INT8: https://research.character.ai/optimizing-inference/ reply ericlewis 5 hours agorootparentprevHigher the precision the better. Use what works within your memory constraints. reply jasonjmcghee 2 hours agorootparentWith serious diminishing returns. At inference time, no reason to use fp64 and should probably use fp8 or less. The accuracy loss is far less than you'd expect. AFAIK Llama 3.2 3B fp4 will outperform Llama 3.2 1B at fp32 in accuracy and speed, despite 8x precision. reply lifthrasiir 12 hours agoparentprevI'm also sure that fp8 is small enough that multiplication can really be done in a much simpler circuit than larger fp formats. Even smaller formats like fp4 would be able to just use a lookup table, and that makes them more like sort-of-standardized quantization schemes. reply tankenmate 12 hours agorootparenti suspect that you could do fp8 with log tables and interpolation if you really wanted to (compared to the memory required for the model it's peanuts), it just turns into a LUT (log table look up) and bit shift (interpolation). so again, memory bandwidth is the limiting factor for transformers (as far as energy is concerned). reply lifthrasiir 11 hours agorootparentThis time though LUT exists in a circuit, which is much more efficient than typical memory lookup. Such LUT would have to exist per each ALU though, so it can't be too large. reply bee_rider 5 hours agorootparentprevWhat is fp4? 3 bits of exponent and one of mantissa? reply wruza 4 hours agorootparentSEEM (sign, exp, mantissa) reply bee_rider 3 hours agorootparentInteresting… I guess it must be biased, m*2^ee would leave like half of the limited space wasted, so 1.m*2^ee? I always wonder with these tiny formats if 0 should even be represented… reply wruza 3 hours agorootparentI’m not a binary guy that much, but iirc all floats are 1.m*2^e — “1.” is always there except for subnormals. There’s also SEEE FP4 which is basically +-2^([u?]int3). https://medium.com/@harrietfiagbor/floating-points-and-deep-... reply brilee 11 hours agorootparentprevfp4/fp8 for neural networks don't work the way you think they do - they are merely compression formats - a set of, say, 256 fp32 weights from 1 neuron are lossily turned into 1 max value (stored in fp32 precision) and 256 fp4/fp8 numbers. Those compressed numbers are multiplied by the fp32 number at runtime to restore the original weights and full fp32 multiplication + additions are executed. reply lifthrasiir 11 hours agorootparentYou are correct that the accumulation (i.e. additions in dot products) has to be done in a higher precision, however the multiplication can still be done via LUT. (Source: I currently work at a hardware-accelerated ML hardware startup.) reply SuchAnonMuchWow 11 hours agorootparentprevThe goal of this type of quantization is to move the multiplication by the fp32 rescale factor outside of the dot-product accumulation. So the multiplications+additions are done on fp8/int8/int4/whatever (when the hardware support those operators of course) and accumulated in a fp32 or similar, and only the final accumulator is multiplied by the rescale factor in fp32. reply rajnathani 7 hours agorootparentprevThat's how Nvidia's mixed precision training worked with FP32-FP16, but it isn't the case for Bfloat16 on TPUs and maybe (I'm not sure) FP8 training on Nvidia Hopper GPUs. reply imjonse 11 hours agorootparentprevWith w8a8 quantization the hw (>= hopper) can do the heavy math in fp8 twice as fast as fp16. reply imjonse 11 hours agoparentprevThat is true for single user/light inference only. For training and batch inference you can get compute bound fast enough. reply saagarjha 11 hours agorootparentThat really depends on what you're doing. Trying to feed a tensor core is pretty hard–they're really fast. reply api 6 hours agoparentprevSounds like the awesome architecture for transformers would be colocation of memory and compute. reply Joker_vD 5 hours agorootparentYes, that's why we generally run them on GPUs. reply phkahler 1 hour agorootparentThat's why we need a row of ALUs in RAM chips. Read a row of DRAM and use it in a vector operation. With the speed of row reading, the ALU could take many cycles per operation to limit area. reply moffkalast 4 hours agorootparentprevGPUs that pull a kilowatt when running yes. This might actually work on an FPGA if the addition doesn't take too many clock cycles compared to matmuls which were too slow. reply kendalf89 12 hours agoparentprevMaybe this technique can be used for training then since that is a lot more compute intensive? reply mikewarot 6 hours agoparentprevImagine if you had a systolic array large enough that all the weights would only have to be loaded once at startup. Eliminating the memory-compute bottleneck of the von Neumann architecture could make this quite a bit more efficient. reply tantalor 5 hours agoprev[2023] GradIEEEnt half decent: The hidden power of imprecise lines http://tom7.org/grad/murphy2023grad.pdf Also in video form: https://www.youtube.com/watch?v=Ae9EKCyI1xU reply js8 10 hours agoprevHaven't read it, but isn't this just logarithmic tables in some form? I am asking not to dismiss it, I genuinely feel I don't understand logarithms on a fundamental level (of logic gates etc.). If multiplication can be replaced with table lookup and addition, then there has to be a circuit that gives you difficult addition and easy multiplication, or any combination of those tradeoffs. reply pclmulqdq 5 hours agoparentYes, this is logarithmic number systems at work. reply cpldcpu 12 hours agoprevIt puzzles me that there does not seem to be a proper derivation and discussion of the error term in the paper. It's all treated indirectly way inference results. reply Lerc 3 hours agoparentThe paper has an odd feel about it to me too. Doing a gate estimation as a text explanation without a diagram makes it too easy to miss some required part. It wouldn't need to be a full gate level explanation but blocks labeled 'adder'. Seeing the name de Vries in the first paragraph didn't help my sense of confidence either. reply CGamesPlay 12 hours agoprevI believe this reduces the compute required, but still uses 8 bits per value, so it does not reduce the memory requirements required to run inference, so it doesn’t particularly make the models more accessible for inference. Is this storage method suitable for training? That could potentially be an interesting application. reply ein0p 24 minutes agoprevMore than 10x the amount of energy is spent moving bytes around. Compute efficiency is not as big of an issue as people think. It’s just that the compute is in the wrong place now - it needs to be right next to memory cells, bypassing the memory bus, at least in the initial aggregations that go into dot products. reply Buttons840 8 hours agoprevWould using this neural network based on integer addition be faster? The paper does not claim it would be faster, so I'm assuming not? What about over time? If this L-Mul (the matrix operation based on integer addition) operation proved to be much more energy efficient and became popular, would new hardware be created that was faster? reply cpldcpu 8 hours agoprevBill Dally from nvidia introduced a log representation that basically allows to replace a multiplication with an add, without loss of accuracy (in contract to proposal above) https://youtu.be/gofI47kfD28?t=2248 reply concrete_head 9 hours agoprevJust too add an alternative addition based architecture into the mix. https://www.youtube.com/watch?v=VqXwmVpCyL0 reply md_rumpf 13 hours agoprevThe return of the CPU?! reply anticensor 8 hours agoparentThe reign of Threadripper! reply pjc50 8 hours agoprev\"We recommend training and hosting L-Mul-based models on devices integrated with specialized architectural designs. Patent pending\" (from footnote in method section) reply scotty79 10 hours agoprevAll You Need is Considered Harmful. reply TaurenHunter 4 hours agoparentWe will need a paper titled '\"Considered Harmful\" Articles is All You Need' to complete that cycle. reply A4ET8a8uTh0 5 hours agoprevUhh.. I hate to be the one to ask this question, but shouldn't we be focused on making LLMs work well first and then focused on desired optimizations? Using everyone's car analogy, it is like making sure early cars are using lower amount of coal. It is a fool's errand. reply spencerchubb 56 minutes agoparentCheaper compute is basically a prerequisite to making better models. You can get some improvements on the margins by making algorithms better with current hardware, but not an order of magnitude improvement. When there is an order of magnitude improvement in hardware, the AI labs will figure out an algorithm to best take advantage of it. reply itishappy 3 hours agoparentprevCoal (and even wood!) powered cars actually existed long before Ford, but didn't take off because they were too heavy and unwieldly. The Model T was the result of a century of optimization. https://en.wikipedia.org/wiki/Nicolas-Joseph_Cugnot reply lukev 3 hours agoparentprevAlso, making neural networks faster/cheaper is a big part of how they advance. We've known about neural architectures since the 70s, but we couldn't build them big enough to be actually useful until the advent of the GPU. Similarly, the LLM breakthrough was because someone decided it was worth spending millions of dollars to train one. Efficiency improvements lower that barrier for all future development (or alternatively, allow us to build even bigger models for the same cost.) reply Maken 4 hours agoparentprevThe optimizations described could easily work on other models, not just transformers. Following your analogy, this is optimizing plumbing, pistons and valves on steam engines, it could be useful for whatever follows. reply ranguna 9 hours agoprev [–] I've seen this claim a few time across the last couple years and I have a pet theory why this isn't explored a lot: Nvidia funds most research around LLMs, and they also fund other companies that fund other research. If transformers were to use addition and remova all usage of floating point multiplication, there's a good chance the gpu would no longer be needed, or in the least, cheaper ones would be good enough. If that were to happen, no one would need nvidia anymore and their trillion dollar empire would start to crumble. University labs get free gpus from nvidia -> University labs don't want to do research that would make said gpus obsolete because nvidia won't like that. If this were to be true, it would mean that we are stuck on an inificient research path due to corporate greed. Imagine if this really was the next best thing, and we just don't explore it more because the ruling corporation doesn't want to lose their market cap. Hopefully I'm wrong. reply cpldcpu 8 hours agoparentI have to disagree. Nvidia spent a lot of effort on researching improved numerical representations. You can see a summary in this talk: https://www.youtube.com/watch?v=gofI47kfD28 A lot of their work was published but went by unnoticed. But in fact the majority of their performance increase in new architecture is resulting from this work. Reading between the lines, it seems that they came to the conclusion that a 4 bit representation with a group exponent (\"FP4\") is the most efficient representation of weights for inference. Reducing the number of bits in weights has the biggest impact on LLMs inference, since they are mostly memory bound. At these low bit numbers, the impact of using multiplication or other approaches is not really significiant anymore. (multiplying a 4 bit wight with a larger activation is effectively 4 additions, barely more than what the paper proposes) reply nayroclade 8 hours agoparentprev\"Good enough\" for what? We're in the middle of an AI arms race. Why do you believe people would choose to run the same LLMs on cheaper equipment instead of using the greater efficiency to train and run even larger LLMs? Given LLM performance seems to scale with their size, this would result in more powerful models, which would grow the applicability, use and importance of AI, which would in turn grow the use and importance of Nvidia's hardware. So this theory doesn't really stack up for me. reply chpatrick 9 hours agoparentprevIt's still a massively parallel problem suited to GPUs, whether it's float or int, or addition or multiplication doesn't really matter. reply londons_explore 9 hours agoparentprevIf an addition-only LLM performed better, nvidia would probably still be the market leader. Next gen nvidia chips would have more adders and fewer multipliers. reply twoodfin 6 hours agoparentprevI’d estimate that fraction of Nvidia’s dominance that’s dependent on their distinctive advantages in kernel primitives (add vs. multiply) would be a rounding error in FP8. The CUDA tooling and ecosystem, VLSI architecture, organizational prowess… all matter at multiple orders of magnitude more. reply iamgopal 6 hours agoparentprevno matter how fast cpu, network and browser has become, websites are still slow. we will run out of data to train much earlier than people will stop inventing even larger models. reply yunohn 6 hours agoparentprevGoogle & Apple already run custom chips, Meta and MS are deploying their own soon too. Your theory is that none of them have researched non-matrix-multiplication solutions before investing billions? reply miohtama 6 hours agorootparentThere are several patents on this topic so they have reply teaearlgraycold 9 hours agoparentprevNVidia GPUs support integer operations specifically for use with deep learning models. reply raincole 7 hours agoparentprev> I have a pet theory You mean you have a conspiracy theory. Why wouldn't other companies that buy Nvidia GPU fund these researches? It would greatly cut their cost. reply yieldcrv 9 hours agoparentprev [–] Alternatively, other people fund LLM research reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The paper \"Addition is All You Need for Energy-efficient Language Models\" presents the L-Mul algorithm, which uses integer addition to approximate floating point multiplication, reducing computation and energy costs.- L-Mul achieves higher precision than 8-bit floating point multiplication and can reduce energy costs by up to 95% for element-wise tensor multiplications and 80% for dot products.- Testing on various tasks showed that L-Mul maintains precision comparable to traditional methods, making it a viable replacement in transformer models."
    ],
    "commentSummary": [
      "The discussion focuses on enhancing energy efficiency in language models by employing fixed-point arithmetic and integer operations, which are more efficient than floating-point calculations, particularly in systems lacking floating-point units.",
      "There is interest in addition-based architectures for neural networks to further reduce energy costs, though concerns about practicality and accuracy compared to IEEE 754 floating-point standards remain.",
      "The debate includes trade-offs between precision and performance in various computing environments, with speculation on how major companies like Nvidia might influence AI research directions, potentially limiting exploration of efficient computational methods."
    ],
    "points": 256,
    "commentCount": 81,
    "retryCount": 0,
    "time": 1728449260
  },
  {
    "id": 41781777,
    "title": "Practices of Reliable Software Design",
    "originLink": "https://entropicthoughts.com/practices-of-reliable-software-design",
    "originBody": "Practices of Reliable Software Design by kqr , published 2024-12-10 Tags: programming Table of Contents The Practices 1. Use Off-The-Shelf 2. Cost And Reliability Over Features 3. Idea To Production Quickly 4. Simple Data Structures 5. Reserve Resources Early 6. Set Maximums 7. Make Testing Easy 8. Embed Performance Counters Other practices I was nerd-sniped. Out of the blue, a friend asked me, If you would build an in-memory cache, how would you do it? It should have good performance and be able to hold many entries. Reads are more common than writes. I know how I would do it already, but I’m curious about your approach. I couldn’t not take the bait. In the process of answering the question and writing the code, I discovered a lot of things happened in my thought processes that have come with experience. These are things that make software engineering easier, but that I know I wouldn’t have considered when I was less experienced. I started writing a long and sprawling article on it, but it didn’t quite hit the right notes, so this is a much abbreviated version. Time allowing, I might expand on some of these practices in separate articles that can be more focused and concise. The Practices Here are all eight practices I have adopted with experience that I had use for during the exercise of writing a fast, small, in-memory cache. 1. Use Off-The-Shelf Our first response to the question should be something like, can we use Redis? As long as we’re not dealing with a very expensive or complicated component, or a part of the software that generates most of its value, we should default to off-the-shelf solutions. I have written before about why we want to build expensive and complicated stuff. 2. Cost And Reliability Over Features If we find out we can’t use an off-the-shelf solution, we need to build something cheap and reliable. That usually means not having all the bells and whistles, but that’s a trade-off worth making. One of my favourite ways of phrasing this is It is much easier to add features to reliable software, than it is to add reliability to featureful software. Besides, it’s very easy to accidentally think you need features that you don’t actually need. I’m not a big proponent of a design phase. Sometimes you need it, but I think if you need to design your software up front, it will cost you more to develop and it will take longer to finish. On the other hand, sometimes a little up front analysis allow you to eliminate large portions of the design space before even writing a single line of code. In the case of this cache, we may ask question about item durability requirements, request rates, sizes, eviction requirements, and so on. When we do, we would in this case find out that we can get by with a single thread accessing a single data structure, and no active eviction process. That’s a huge win! It simplifies the design a lot. 3. Idea To Production Quickly Part of the reasoning of the previous practice was that it’s easy to think you need features you don’t. How do you, then, know which features you need? For the most part, the cheapest and most reliable way to find this out is in production. If we deploy the bare minimum feature set, we will very quickly find out what additional features are most commonly requested, and it’s not going to be the ones we think. And even if they are, there are usually other requirements on them that we wouldn’t have caught ahead of time. This practice ties in strongly with the previous one: use analysis to pare down the requirements to the bare minimum, and then write the bare minimum code to support them, and get it into production to find out more about the problem we’re trying to solve. 4. Simple Data Structures Complicated data structures are often tempting. Especially when there are libraries that handle them for us. The problem with complicated data structures is that it’s easy to misuse them due to a lack of understanding, which leads to performance problems and bugs. In the case of this specific cache, the number of entries that needed to be stored (we can use queueing theory to calculate this from the ttl and the rate of writes) would comfortably be addressed by 19.9 bits of information – in other words, we can just use a plain array to store each item, and hash the key to address it.11 We found out earlier we didn’t need to actually evict expired items. If we would have needed to, we could keep a separate index for the item nearest to expiry using a min-heap. This is cheaper and simpler than something fully sorted, which it might be tempting to reach for otherwise. Instead of a separate process, we can rely on the high request rate to trigger eviction of items, thus trading a little latency to avoid designing for concurrency up front. Another alternative is to provide a separate API that triggers eviction of the oldest expired entry, letting the caller decide how much time to spend on eviction. This means the caller can vary that time to make dynamic latency–resource requirement tradeoffs. 5. Reserve Resources Early Another potentially controversial decision I made for the cache I designed was that it allocated the full index array up front. This sounds wasteful, but we can tell from back-of-the-napkin analysis that it would need most of this space anyway during continuous operation, so nothing would be saved by postponing the allocation. What early allocation does is it allows the software to crash early if the required resources are not available. This is a desirable trait – it saves the operator the frustration of finding that out only once they’ve gone to bed and receive a call from PagerDuty. It also makes it easier to do capacity planning and reason about performance. New here? Making software that is easy to run in production is a big deal for me, and I write about it as often as I can. You should subscribe to receive weekly summaries of new articles by email. If you don't like it, you can unsubscribe any time. 6. Set Maximums I chose a sloppy open-addressing-with-linear-probing-based method for handling hash collisions in addressing items that is simple and has very good performance in the typical case (again, only very rudimentary arithmetic needed to find this out for any specific use case), but can have very bad performance in the worst case: it can end up iterating through the entire array of all items on every cache miss. Since perfect recall turned out not to be necessary22 This is again why asking questions about requirements is so important!, it was trivial to set a maximum iteration limit of e.g. 500 steps. This makes sure the worst case is not too far from the average case, at the cost of a few more cache misses. Does 500 steps optimise this tradeoff? No. This is one thing I don’t know how to compute ahead of time, so production experience will have to inform future changes to the maximum number of steps. The important thing is usually not what the maximum is, just that there is some maximum, so we’re not waiting surprisingly long times for things to complete. Given the static allocation requirements, we might also want to set a limit on how much data can be stored in the cache at any given time, so we don’t accidentally blow memory limits in the middle of the night. In general, limit all the things! If someone is unhappy with the limit, revise the limit – don’t remove it. 7. Make Testing Easy To avoid regressions and ensure expected behaviour, I made the cache accept commands on standard input. This means we can start the cache and type write hello world in the terminal window to store \"world\" for the key \"hello\". But it also means we can write up a long string of commands in a file, and pipe that into the cache. If we then also give the cache a command that asserts the last value read is something specific33 It also had a few more commands related to testing, such as a sleep command that forces it to believe time has passed (for testing expiration) and a dump command that just prints out everything it is currently storing, and whether it’s deleted, expired, or active., we can write up a whole test protocol in a plain text file, and pipe it to the program to verify its functionality. The input file might look like In[1]: # Cache empty, should not find anything. read abc assert undef # Write and then immediately read the same thing back. write abc 123 read abc assert 123 # Different key should get nothing. read xyz assert undef # Write different value, then read, should match. write abc 567 read abc assert 567 # Write one more key, abc should still be around. write xyz 444 read abc assert 567 In isolation, each of these things is very simple (accept commands at cli, allow asserting previous read) but it’s the combination of them, together with the tools that already exist at hand (shell input redirection) that enables faster cycle times and thus more reliable software. 8. Embed Performance Counters Finally, we will want to know how our program spends its time. We can do that with profiling, or deduce it through logs, but the easiest way to do it is to embed performance counters. These are variables that accumulate how much of something is happening. It can be things like How much time is spent reading keys? How much time is spent writing key–value pairs? How much time is spent on I/O? How many cache misses have there been? How many keys have we had to linearly search over? How many times has the iteration limit of 500 been reached? Note that all of these are accumulative, so we don’t ask “how much storage space is currently allocated?”, but we ask instead the pair of questions How much storage space have we allocated in total from startup until now? How much storage space have we freed in total from startup until now? By breaking the measurement apart into two time-dependent totals we can learn significantly more about how our system behaves. How these numbers evolve over time is also a useful indicator. For example, the value of “how many keys have we had to linearly search over?” might increase steadily (indicating relatively even distribution of hash collisions) or it jumps up a lot sometimes (indicating sudden burst of collisions). These are useful things to be able to tell apart. In combination with “How many times has the iteration limit been reached” it tells us a lot of the pressures put on the system. Other practices There are surely many more of these insights gained from years of software engineering, but these are the ones I thought of during this exercise. I’d be happy to hear from you about others you are thinking about! If nothing else, I hope that could teach me to be a better software engineer. Sidenotes 1 We found out earlier we didn’t need to actually evict expired items. If we would have needed to, we could keep a separate index for the item nearest to expiry using a min-heap. This is cheaper and simpler than something fully sorted, which it might be tempting to reach for otherwise. Instead of a separate process, we can rely on the high request rate to trigger eviction of items, thus trading a little latency to avoid designing for concurrency up front. Another alternative is to provide a separate API that triggers eviction of the oldest expired entry, letting the caller decide how much time to spend on eviction. This means the caller can vary that time to make dynamic latency–resource requirement tradeoffs. 2 This is again why asking questions about requirements is so important! 3 It also had a few more commands related to testing, such as a sleep command that forces it to believe time has passed (for testing expiration) and a dump command that just prints out everything it is currently storing, and whether it’s deleted, expired, or active.",
    "commentLink": "https://news.ycombinator.com/item?id=41781777",
    "commentBody": "Practices of Reliable Software Design (entropicthoughts.com)192 points by fagnerbrack 22 hours agohidepastfavorite42 comments nostrademons 18 hours agoThere is a bunch of good advice here, but it's missed the most useful principal in my experience, probably because the motivating example is too small in scope: The way to build reliable software systems is to have multiple independent paths to success. This is the Erlang \"let it crash\" strategy restated, but I've also found it embodied in things like the architecture of Google Search, Tandem Computer, Ethereum, RAID 5, the Space Shuttle, etc. Basically, you achieve reliability through redundancy. For any given task, compute the answer multiple times in parallel, ideally in multiple independent ways. If the answer agrees, great, you're done. If not, have some consensus mechanism to detect the true answer. If you can't compute the answer in parallel, or you still don't get one back, retry. The reason for this is simply math. If you have n different events that must all go right to achieve success, the chance of this happening is x1 * x2 * ... * xn. This product goes to zero very quickly - if you have 20 components connected in series that are all 98% reliable, the chance of success is only 2/3. If instead you have n different events where any one can go right to achieve success, the chance of success is 1 - (1 - y1) * (1 - y2) * ... * (1 - yn). This inverse actually increases as the number of alternate pathways to success goes up and fast. If you have 3 alternatives each of which has just an 80% chance of success, but any of the 3 will work, then doing them all in parallel has a 97% chance of success. This is why complex software systems that must stay up are built with redundancy, replicas, failover, retries, and other similar mechanisms in place. And the presence of those mechanisms usually trumps anything you can do to increase the reliability of individual components, simply because you get diminishing returns to carefulness. You might spend 100x more resources to go from 90% reliability to 99% reliability, but if you can identify a system boundary and correctness check, you can get that 99% reliability simply by having 2 teams each build a subsystem that is 90% reliable and checking that their answers agree. reply kqr 14 hours agoparentI disagree somewhat, influenced by the teachings of Nancy Leveson. In the 1930's, yes, component redundancy was the way to reliability. This worked at the time because components were flaky and technical systems were simple aggregations of components. Today, components themselves are more reliable, but even when they are not, redundancy adds only a little reliability because there's a new, large, source of failure: interactive complexity. Today's systems are so complicated that many failures stem from insufficient, misunderstood, or ambiguous specifications. These errors happen not because a component failed -- all components work exactly as they were intended to -- it is only that in their intended interactions they produce an unintended result. Failure is an emergent property. The solution is to approach reliability from a system theoretic perspective. This very early draft contains the core of the idea, but not yet fleshed out or edited: https://entropicthoughts.com/root-cause-analysis-youre-doing... reply nostrademons 7 hours agorootparentThis is why Erlang's OTP focuses on supervisor trees. At each level of the component hierarchy, you have redundancy. Subcomponents themselves may have interactive complexity, but a failure or misspecification in any of the interactions making up that subcomponent simply makes that subcomponent fail. This failure is handled at a higher level by doing something simpler. And \"do something simpler\" is actually a core part of this strategy. You're right that \"today's systems are so complicated that many failures stem from insufficient, misunderstood, or ambiguous specifications\". In most cases, yesterday's system worked just fine, you just can't sell it as a competitive advantage. So build simple, well-understood subsystems as fallbacks to the complex bleeding-edge systems, or even just take the software that's been working for a decade. reply detourdog 2 hours agorootparentprevI have the opinion that todays very complicated system is a symptom of over complication for the problem at hand. I’m working on the idea that there is better set of assumptions to use for directing technical development. reply gukov 3 minutes agorootparentSystems are not built in one go. They usually start out simple enough and become complex over time. reply marcosdumay 4 hours agoparentprevAs always, life is not that simple, and redundant components can interact in harmful ways, correctness checks can create incorrectness, process managers or consensus algorithms can amplify small problems... Just like every technique on the article also can turn out to reduce your reliability too. reply yen223 18 hours agoparentprevIn the limit, there is a hard tradeoff between efficiency and reliability. Failovers, redundancies, and backups are all important for building systems that are resilient in the face of problems, for reasons you've pointed out. However, failovers, redundancies and backups are inefficient. Solving a problem with 1 thing is always going to be more efficient that solving the same problem with 10 things. It's interesting to see this tradeoff play out in real-life. We see people coalescing around one or two services because that's the most efficient path, and then we see them diversifying across multiple services once bad things happen to the centralised services. reply nostrademons 7 hours agorootparentThis is a very important point, and often misunderstood on both a business & societal level. Reliability has a cost. If you optimize all redundancy out of a system, you find that the system becomes brittle, unreliable, and prone to failure. Companies like 3M and Boeing have found that in the pursuit of higher profits, they've lost their focus on quality and suffer the resulting loss of trust and brand damage. The developed world discovered that with COVID, our just--in-time efficiency meant that any hiccup anywhere in the supply chain meant mass shortages of goods. reply marcosdumay 4 hours agorootparentprev> In the limit, there is a hard tradeoff between efficiency and reliability. Yes, but notice that most things on the GP's comment have an exponential impact on reliability (well, on 1 - reliability), so they are often non-brainiers as long as they follow that simple model (what they stop doing at some point). reply hippich 17 hours agorootparentprevImho, the problem is that it is hard to estimate trade-offs. Optimizations (not just in computer systems, but in general) often seen as risk-free, when in reality they are not. More often than not one will be celebrated for optimization, and rarely for resilience (dubbed as duplicate, useless work) reply ramchip 5 hours agoparentprev> For any given task, compute the answer multiple times in parallel, ideally in multiple independent ways. Just to be clear, while this particular technique is valid and used in space software, it isn't common at all in Erlang and not part of the \"let it crash\" principle. reply amelius 2 hours agoparentprev> The way to build reliable software systems is to have multiple independent paths to success. That's a heuristic that might work sometimes. If you really want to build reliable software systems, then at least prove them correct. There are some tools and methodologies that can help you with this. And of course even a proof isn't everything since your assumptions can still be wrong (but in more subtle ways). reply alexpetralia 7 hours agoparentprevInterestingly this is exactly how I've come to define truth/correctness: https://alexpetralia.com/2023/01/25/how-do-we-know-if-data-i... reply pistoleer 11 hours agoparentprevWho will replicate the consensus checker? reply the_sleaze_ 3 hours agorootparentBecause he's the failover Gotham deserves, but not the validator it needs right now reply manvillej 16 hours agoparentprevthe simple basic reality of statistics, a binomial distribution. 5 independent systems with 90% chance of success is mathematically as reliable as one that is 99.999%. 100x 90% systems would get you to 100 \"9s\" of reliability aka 99.99999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999% reply kqr 15 hours agorootparentExcept the binomial assumption obviously does not hold because (a) failures are correlated, not independent, and (b) many failures happen not at the component level but at the plane where components interact, and regardless of how much redundancy there is at the component level, there is ultimately just one plane at which they finally interact to produce a result. reply lucianbr 10 hours agorootparentprevActually making 5 completely independent systems would be exceptionally hard. No shared code or team members, no shared hardware... For example, what 5 computing platforms would you use? x86, ARM, RISC-V and...? Math rarely applies so easily to real life. Talking about \"independent\" systems is cheap. If at all possible. How would you transport yourself to work using two independent systems? reply nostrademons 7 hours agorootparentIt's relatively simple at the organizational level, just expensive (but linearly expensive, while often increasing subcomponent reliability is exponentially expensive!). Just give the same problem statement to two independent teams with two different managers, have a clear output format and success criteria, and let them make all their technical decisions independently. Your example of \"how do you transport yourself to work using two independent systems\" is actually very apropos, because I and many other commuters do exactly that. If the highway is backed up, I bypass it with local roads. If everything is gridlock, I take public transportation. If public transportation isn't functioning (and it generally takes a natural disaster to knock out all the roads and public transportation, but natural disasters have happened), I work from home and telecommute. Each of these subsystems is less favored than the alternative, but it'll get me to work. reply lucianbr 5 hours agorootparentWhile these are reasonable approaches, I do not think they live up to the mathematical meaning of \"independent\", and so invalidate the chances calculation. Your two teams might well both use in some place in the system the same hardware or software component. This will make the probability of failure between the systems not be completely independent, for all that you paid two teams and they worked separately. Spent a lot of money, and the results will not be as expected. If they both use x86 Intel, and a Meltdown kind of thing happens, your \"independent\" systems will both fail from the same cause. The transport analogy works great if you somehow imagine the transportation to be instantaneous, and only the decision to matter. But if you are already on a train and the train is delayed, you are not walking back home and taking the car. You have multiple options for transport, but you do not have a system built of independent components. You are not using the train and the car and the highway and the local roads all simultaneously. I don't think you understand the requirements for the formula you wrote to be valid. Your examples do not fit, for all that they are reasonable and useful approaches. Your actual reliability with these approaches falls way below the multiple nines you think of. reply taeric 2 hours agoprevThis misses one of the key things I have seen that really drives reliable software. Actually rely on the software. It sucks, because nobody likes the idea of the \"squeaky wheel getting the grease.\" At the same time, nobody is surprised that the yard equipment that they haven't used in a year or so is going to need effort to get back to working. The longer it has been since it was relied on to work, the more likely that it won't work. To that end, I'm not arguing that all things should be the critical path. But the more code you have that isn't regularly exercised, the more likely it will be broken if anything around it changes. reply bruce511 14 hours agoprevThe first point is one that resonates strongly with me. Counter-intuitivly, the first instinct of a programmer should be \"buy that, don't write it\" Of course, as a programmer, this is by far not my first instinct. I am a programmer, my function is programming, not purchasing. Of course buying something is always cheaper (compared to the cost of my time) and will be orders of magnitude cheaper once the costs to maintain written-by-me code is added in. Things that are bought -tend- to last longer too. If I leave my job I leave behind a bunch of custom code nobody wants to work on. If I leave Redis behind, well, the next guy just carries on running Redis. I know all this. I advocate for all this. But I'm a programmer, send coders gotta code:) do it's not like we buy everything, I'm still there, still writing. Hopefully though my emphasis is on adding value. Build things that others will take over one-day. Keep designs clean, and code cleaner. And if I add one 'practice' to the list; Don't Be Clever. Clever code is hard to read, hard to understand, hard to maintain. Keep all code as simple as it can be. Reliable software is software that mostly isn't trying to be too clever. reply VyseofArcadia 34 minutes agoparentI'm not sure I 100% agree. I've been thinking a lot lately about the cost of off-the-shelf solutions from the perspective of sustainability, and there is a cost beyond money. The performance of software almost always degrades over time. By buying Foo off-the-shelf, you are saying, \"I am ok with getting on the same bloat-dictated hardware upgrade cycle as Foo.\" Of course you have the option of buying Foo and never upgrading, unless Foo has a license that forces you to. But that also walls you off from security bugfixes. But by replicating the essential features of Foo in-house, you can actually set and stick to a complexity and performance budget. Of course if you are a business of any real size, you're already on the hardware upgrade treadmill anyway, and probably all of your customers are too, so what does it matter if your software is a little slower and a little more resource hungry year after year after year? Other than maybe a little twinge of guilt every now and then. reply eikenberry 45 minutes agoparentprev> Counter-intuitivly, the first instinct of a programmer should be \"buy that, don't write it\" I don't think this is counter intuitive at all... this is the whole premise behind free software. Why write it yourself when someone else already has and there is a community around using and updating it. We all buy the vast majority of our software and it is usually our go to move, unless there is an itch. reply aitchnyu 13 hours agoparentprevThis topic deserves an article on its own. I feel my team crossed \"the line\" on a SaaS that hosts docs from our Openapi and page doesnt even refresh safely. But how do we define the line? reply kqr 13 hours agorootparentThe line is where the cost of building is less than that of buying. It sounds like in your case building would have been cheaper, given the simplicity of the problem and the quality issues with the purchased solution. It does get difficult in more complicated cases thanks to a lack of information on what a good solution looks like. This article attempts to straighten it out a little: https://entropicthoughts.com/build-vs-buy reply marcosdumay 4 hours agorootparent> The line is where the cost of building is less than that of buying. Yes, once you factor in transaction costs, integration costs, risks contamination from that 3rd party, risks from lack of value alignment with that 3rd party (remember the Unity game engine?)... Or, in other words, people that say that phrase you said very often don't know the actual cost of buying. But well, nobody knows the actual cost of building before they try either. reply zemvpferreira 4 hours agorootparentprevI would personally replace 'cost of building' with 'cost of maintaining', but otherwise agree with your reasoning. It's worth building in a factor of safety, such that I would formulate this idea as: Only build software if the cost of maintaining it is 1/3 or less than the cost of buying a license. (this has the nice second-order effect of being more robust to errors in the maintenance estimate, hence making it quicker to estimate). reply hamdouni 37 minutes agoprevMy takeaways for a more general pov : 1. Make or buy 2. Release a MVP 3. Keep it simple 4. Prepare for the worst 5. Make it easy to tests 7. Benchmark, monitor, log... reply throwawayha 6 hours agoprevGreat points. But why do we invest so much complexity into outputting html/js/css. reply ketzo 4 hours agoparentBecause html/js/css is the venue for a massive fraction of human-computer interactions, and there a lot of different things we want to accomplish between humans and computers. It’s always funny to me when people act like “websites” are some trivial, silly little area of software, when in fact for a lot of people, it’s their primary use of a computer. reply Traubenfuchs 6 hours agoparentprevhttps://en.wikipedia.org/wiki/Reductio_ad_absurdum reply SomewhatLikely 12 hours agoprevMy first thought upon seeing the prompt: If you would build an in-memory cache, how would you do it? It should have good performance and be able to hold many entries. Reads are more common than writes. I know how I would do it already, but I’m curious about your approach. Was to add this requirement since it comes up so often: Let's assume that keys accessed follow a power law, so some keys get accessed very frequently and we would like them to have the fastest retrieval of all. I'm not sure if there are any efficient tweaks to hash tables or b-trees that might help with this additional requirement. Obviously we could make a hash table take way more space than needed to reduce collisions, but with a decent load factor is the answer to just swap frequently accessed keys to the beginning of their probe chain? How do we know it's frequently accessed? Count-Min sketch? Even with that tweak, the hottest keys will still be scattered around memory. Wouldn't it be best if their entries could fit into fewer pages? So, maybe a much smaller \"hot\" table containing say the 1,000 most accessed keys. We still want a high load factor to maximize the use of cache pages so perhaps perfect hashing? reply mannyv 7 hours agoparentI've been doing this so long that my first thought was \"use redis.\" Why? * it works * it's available now * it scales * it's capable of HA * it has bindings for every language you probably want to use Why bother writing your own cache, unless it's for an exercise? Cache management is complicated and error prone. Unless the roundtrip kills you just use redis (or memcached). reply bespoke_engnr 6 hours agoparentprevI think splay trees would be good for this: https://en.m.wikipedia.org/wiki/Splay_tree reply akoboldfrying 6 hours agoparentprevhttps://en.m.wikipedia.org/wiki/Splay_tree reply l5870uoo9y 11 hours agoprevI would add a ninth practice; throw errors. You find and fix them as opposed to errors that go silently unnoticed in the code base. reply AnimalMuppet 6 hours agoparentFail early, and fail noisily. Don't fail silently. reply BillLucky 17 hours agoprevSimple but elegant design principles, recommended reply u8_friedrich 14 hours agoprev> It is much easier to add features to reliable software, than it is to add reliability to featureful software. Not sure about this tbh. In a lot of cases yeah maybe. But when you are dealing with complicated business logic where there is a lot of bells and whistles required, building a simple reliable version can lead you into a naive implementation that might be reliable but very hard to extend, while making an unstable complicated thing can help you understand the pit falls and you can work back from there into something more reliable. So I think this depends very much on the context. reply jpc0 6 hours agoparentHow are you defining simple here? Simple in my mind has abstractions where they are needed which should naturally lead to easy to extend code. reply ActionHank 9 hours agoprev [–] Quick mental exercise on this. If someone posed this question to you in an interview and you used these principles, would you get the job? Probably not. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "\"Practices of Reliable Software Design\" by kqr presents eight essential practices for developing reliable software, focusing on creating a fast, in-memory cache.- Key practices include using off-the-shelf solutions like Redis, prioritizing cost and reliability over features, and deploying minimal features quickly to learn what is necessary.- Additional practices involve using simple data structures, reserving resources early, setting maximums to prevent performance issues, making testing easy, and embedding performance counters to track system behavior."
    ],
    "commentSummary": [
      "Redundancy, or having multiple independent paths to success, is crucial for building reliable software systems, as demonstrated by systems like Google Search and RAID 5.- While redundancy enhances reliability, it can also introduce complexity and inefficiency, particularly in modern systems where failures often result from interactions between components rather than individual component failures.- Balancing efficiency and reliability is essential, with real-world examples showing that over-optimization can lead to system brittleness; thus, using well-understood, simple subsystems and regular maintenance is key to achieving reliability."
    ],
    "points": 192,
    "commentCount": 42,
    "retryCount": 0,
    "time": 1728421282
  },
  {
    "id": 41781855,
    "title": "Don't let dicts spoil your code",
    "originLink": "https://roman.pt/posts/dont-let-dicts-spoil-your-code/",
    "originBody": "Roman Imankulov Full-stack Python web developer from Porto Home All posts Snippets Links Tags About 22 Nov 2020 Don't let dicts spoil your code updated on 18 Jan 2021 table of contents What’s wrong with dicts? Dicts are opaque Dicts are mutable Treat dicts as the wire format Streamline model creation Create models with dataclasses Alternatively, create models with Pydantic For legacy codebase, annotate dicts as TypeDict For key-value stores, annotate dicts as mappings Take dicts under control Photo by Martin Sanchez How often do your simple prototypes or ad-hoc scripts turn into fully-fledged applications? The simplicity of organic code growth has a flip side: it becomes too hard to maintain. The proliferation of dicts as primary data structures is a clear signal of tech debt in your code. Fortunately, modern Python provides many viable alternatives to plain dicts. What’s wrong with dicts? Dicts are opaque Functions that accept dicts are a nightmare to extend and modify. Usually, to change the function that takes a dictionary, you must manually trace the calls back to the roots, where this dict was created. There is often more than one call path, and if a program grows without a plan, you’ll likely have discrepancies in the dict structures. Dicts are mutable Changing dict values to fit a specific workflow is tempting, and programmers often abuse this functionality. In-place mutations may have different names: pre-processing, populating, enriching, data massage, etc. The result is the same. This manipulation hinders the structure of your data and makes it dependent on the workflow of your application. Not only do dicts allow you to change their data, but they also allow you to change the very structure of objects. You can add or delete fields or change their types at will. Resorting to this is the worst felony you can commit to your data. Treat dicts as the wire format A common source of dicts in the code is deserializing from JSON. For example, from a third-party API response. >>> requests.get(\"https://api.github.com/repos/imankulov/empty\").json() {'id': 297081773, 'node_id': 'MDEwOlJlcG9zaXRvcnkyOTcwODE3NzM=', 'name': 'empty', 'full_name': 'imankulov/empty', 'private': False, ... } A dict, returned from the API. Make a habit of treating dicts as a “wire format” and convert them immediately to data structures providing semantics. Use serializer and deserializer to convert between wire format and internal representation The implementation is straightforward. Define your domain models. A domain model is simply a class in your application. Fetch and deserialize in the same step. In Domain-Driven Design (DDD), this pattern is known as the anti-corruption layer. On top of semantic clarity, domain models provide a natural layer that decouples the exterior architecture from your application’s business logic. Two implementations of a function retrieving repository info from GitHub: Returning a dict import requests def get_repo(repo_name: str): \"\"\"Return repository info by its name.\"\"\" return requests.get(f\"https://api.github.com/repos/{repo_name}\").json() The output of the function is opaque and needlessly verbose. The format is defined outside of your code. >>> get_repo(\"imankulov/empty\") {'id': 297081773, 'node_id': 'MDEwOlJlcG9zaXRvcnkyOTcwODE3NzM=', 'name': 'empty', 'full_name': 'imankulov/empty', 'private': False, # Dozens of lines with unnecessary attributes, URLs, etc. # ... } Returning a domain model class GitHubRepo: \"\"\"GitHub repository.\"\"\" def __init__(self, owner: str, name: str, description: str): self.owner = owner self.name = name self.description = description def full_name(self) -> str: \"\"\"Get the repository full name.\"\"\" return f\"{self.owner}/{self.name}\" def get_repo(repo_name: str) -> GitHubRepo: \"\"\"Return repository info by its name.\"\"\" data = requests.get(f\"https://api.github.com/repos/{repo_name}\").json() return GitHubRepo(data[\"owner\"][\"login\"], data[\"name\"], data[\"description\"]) >>> get_repo(\"imankulov/empty\")While the example below has more code, this solution is better than the previous one if we maintain and extend the codebase. Let’s see what the differences are. The data structure is clearly defined, and we can document it with as many details as necessary. The class also has a method full_name() implementing some class-specific business logic. Unlike dicts, data models allow you to co-locate the code and data. GitHub API’s dependency is isolated in the function get_repo(). The GitHubRepo object doesn’t need to know anything about the external API and how objects are created. This way, you can modify the deserializer independently from the model or add new ways of creating objects: from pytest fixtures, the GraphQL API, the local cache, etc. ☝ Ignore fields coming from the API if you don’t need them. Keep only those that you use. In many cases, you can and should ignore most of the fields coming from the API, adding only the fields that the application uses. Not only duplicating the fields is a waste of time, but it also makes the class structure rigid, making it hard to adopt changes in the business logic or add support to the new version of the API. From the point of view of testing, fewer fields mean fewer headaches in instantiating the objects. Streamline model creation Wrapping dicts require creating a lot of classes. You can simplify your work by employing a library that makes “better classes” for you. Create models with dataclasses Starting from version 3.7, Python provides Data Classes. The dataclasses module of the standard library provides a decorator and functions for automatically adding generated special methods such as __init__() and __repr__() to your classes. Therefore, you write less boilerplate code. I use dataclasses for small projects or scripts where I don’t want to introduce extra dependencies. That’s how the GitHubRepo model looks like with dataclasses. from dataclasses import dataclass @dataclass(frozen=True) class GitHubRepo: \"\"\"GitHub repository.\"\"\" owner: str name: str description: str def full_name(self) -> str: \"\"\"Get the repository full name.\"\"\" return f\"{self.owner}/{self.name}\" When I create Data Classes, my Data Classes are almost always defined as frozen. Instead of modifying an object, I create a new instance with dataclasses.replace(). Read-only attributes bring peace of mind to a developer, reading and maintaining your code. Alternatively, create models with Pydantic Recently Pydantic, a third-party data-validation library became my go-to choice for model definition. Compared with dataclasses, they are much more powerful. I especially like their serializers and deserializers, automatic type conversions, and custom validators. Serializers simplify storing records to external storage, for example, for caching. Type conversions are especially helpful when converting a complex hierarchical JSON to a hierarchy of objects. And validators are helpful for everything else. With Pydantic, the same model can look like this. from pydantic import BaseModel class GitHubRepo(BaseModel): \"\"\"GitHub repository.\"\"\" owner: str name: str description: str class Config: frozen = True def full_name(self) -> str: \"\"\"Get the repository full name.\"\"\" return f\"{self.owner}/{self.name}\" Online service jsontopydantic.com saves my time creating Pydantic models from third-party APIs. I copy the response examples from their documentation to the service, and the service returns Pydantic models. jsontopydantic.com converts Todoist API response to a Pydantic model You can find some examples of me using Pydantic, in the post Time Series Caching with Python and Redis. For legacy codebase, annotate dicts as TypeDict Python 3.8 introduced so-called TypedDicts. In runtime, they behave like regular dicts but provide extra information about their structure for developers, type validators, and IDEs. If you come across the dict-heavy legacy code and can’t refactor everything yet, at least you can annotate your dicts as typed ones. from typing import TypedDict class GitHubRepo(TypedDict): \"\"\"GitHub repository.\"\"\" owner: str name: str description: str repo: GitHubRepo = { \"owner\": \"imankulov\", \"name\": \"empty\", \"description\": \"An empty repository\", } Below, I provide two screenshots from PyCharm to show how adding typing information can streamline your development experience with the IDE and protect you against errors. PyCharm knows about the value type and provides autocomplete PyCharm knows about the missing key and issues a warning For key-value stores, annotate dicts as mappings A legitimate use-case of dict is a key-value store where all the values have the same type, and keys are used to look up the value by key. colors = { \"red\": \"#FF0000\", \"pink\": \"#FFC0CB\", \"purple\": \"#800080\", } A dict, used as a mapping. When instantiating or passing such dict to a function, consider hiding implementation details by annotating the variable type as Mapping or MutableMapping. On the one hand, it may sound like overkill. Dict is default and by far the most common implementation of a MutableMapping. On the other hand, by annotating a variable with mapping, you can specify the types for keys and values. Besides, in the case of a Mapping type, you send a clear message that an object is supposed to be immutable. Example I defined a color mapping and annotated a function. Notice how the function uses the operation allowed for dicts but disallowed for Mapping instances. # file: colors.py from typing import Mapping colors: Mapping[str, str] = { \"red\": \"#FF0000\", \"pink\": \"#FFC0CB\", \"purple\": \"#800080\", } def add_yellow(colors: Mapping[str, str]): colors[\"yellow\"] = \"#FFFF00\" if __name__ == \"__main__\": add_yellow(colors) print(colors) Despite wrong types, no issues in runtime. $ python colors.py {'red': '#FF0000', 'pink': '#FFC0CB', 'purple': '#800080', 'yellow': '#FFFF00'} To check the validity, I can use mypy, which raises an error. $ mypy colors.py colors.py:11: error: Unsupported target for indexed assignment (\"Mapping[str, str]\") Found 1 error in 1 file (checked 1 source file) Take dicts under control Keep an eye on your dicts. Don’t let them take control of your application. As with every piece of technical debt, the further you postpone introducing proper data structures, the more complex the transition becomes. Python Software Development Hey, I am Roman, and you can hire me. I am a full-stack Python web developer who loves helping startups and small teams turn their ideas into products. More about me and my skills 2024",
    "commentLink": "https://news.ycombinator.com/item?id=41781855",
    "commentBody": "Don't let dicts spoil your code (roman.pt)174 points by juniperplant 21 hours agohidepastfavorite105 comments cardanome 16 hours agoThis is absolute key advice. Another way to look at it is the functional core, imperative shell pattern. Wrapping up your dict in a value object (dataclass or whatever that is in you language) early on means you handle the ugly stuff first. Parse don't validate. Resist the temptation of optional fields. Is there really anything you can do if the field is null? No, then don't make it optional. Let it crash early on. Clearly define you data. If you have put your data in a neat value objects you know what is in it. You know the types. You know all required fields are there. You will be so much happier. No checking for null throughout the code, no checking for empty strings. You can just focus on the business logic. Seriously so much suffering can be avoided by just following this pattern. reply sevensor 4 hours agoparentI like to think about this as securing the perimeter. Inside, everything is typed, static analysis constrains what can happen, and I am never surprised as long as the code type checks. Outside, data is probably garbage. All the effort goes into locking down the interface. Pydantic is ok for this, although I find it too intrusive for my taste, and I think mixing arbitrary validity predicates with structural correctness is a mistake. Still, I’d much rather walk into a codebase that uses Pydantic than one that assumes its inputs are valid, because confidently writing business logic that can assume its inputs are correct is incredibly liberating. reply aequitas 6 hours agoparentprev> Another way to look at it is the functional core, imperative shell pattern. A good explanation of this is: https://www.destroyallsoftware.com/talks/boundaries reply cruffle_duffle 4 hours agoparentprevPydantic makes that stuff super simple too. It has all manner of data validation hooks as well as (de)serialization help. reply mcdeltat 10 hours agoparentprevThe \"loosey-goosey\" approach to data in coding is one of my biggest pet peeves. Some people absolutely insist on making everything as dynamic as possible, and then wonder why we end up with a buggy mess. I always found it very natural to move as much as possible into the type system, because why wouldn't I want the machine to find all my inevitable mistakes for me? reply majewsky 6 hours agorootparentMy personal favorite of this has got to be a particular mess of a Python API that led to me implementing \"type veryFlexibleUint64\": https://github.com/sapcc/limes/blob/9ea9d1f86383f8a5fe0fa1d1... reply pbronez 6 hours agorootparentSo brutal reply Kinrany 10 hours agoparentprevThere's little to disagree with here, and yet this comment reads like a slogan soup. reply jimmytucson 17 hours agoprevHere’s an out-there take, but one I’ve held loosely for a long time and haven’t shed yet: dicts are not appropriate for what people mostly use them for, which is named access to member attributes. dict is an implementation of a hash table. Hash table are designed for o(1) lookup of items. As such, they are arrays which are much bigger than the number of items they store, to allow hashing items into integers and sidestep collisions. They’re meant to act like an index that contains many records, not a single record. A single record is more like a tuple, except you want named access instead of, title = movie[0], release_year = movie[1], etc. And Python had that, in NamedTuple, but it was kinda magical and no one used it (shoutout Raymond Hettinger). Granted, this rant is pretty much the meme with the guy explaining something to a brick wall, in that dicts are so firmly entrenched as the \"record\" type of choice in Python (but not so in other languages: struct, case class, etc. and JSON doesn’t just deserialize to a weak type but I digress). reply fallingsquirrel 15 hours agoparentNamedTuples are great, but they let you do too much with the objects. You probably don't want users of your GitHubRepo class to be able to do things like `repo[1]` or `for foo in repo`. Dataclasses have more constrained semantics, so I reach for them by default. In my ideal world they would default to frozen=True, kw_only=True, slots=True, but even without those they're a big improvement. reply aatarax 15 hours agoparentprevDicts in python are for when you have a thing and you aren't sure what the keys are. Dataclasses are for when you have a thing and you're sure what the keys (attributes are). The trouble is when you have a thing and you're sort of sure, but not entirely sure, and some things are definitely there but not everything you might be thinking of. reply jsyang00 16 hours agoparentprevI think most modern Python codebases are using dataclasses/ something like Pydantic. I think dicts are mostly seen, like the author suggests, because something which you hacked up to work quickly ends up turning into actual software and it's too much work refactor the types reply travisjungroth 13 hours agoparentprevI think I once heard a Clojure talk where they were referred to as big and small maps. Small ones are what you’re comparing to arrays. A place where dicts for hard coded keys makes sense is notebooks. The convenience is worth it and it’s unlikely to get out of hand. reply jonathrg 11 hours agoparentprevdicts are used internally in the language to look up class and module attributes. They are optimized for this use case. How can it be wrong to use them that way when the very fabric of the language depends on it? namedtuple is widely used in Python code, especially before the introduction of dataclasses. reply sickblastoise 55 minutes agorootparentIt’s not wrong to use dicts, it’s just bad practice when you could use something like a dataclass or pydantic model instead. Dicts are useful for looking things up, like if you have a list bunch of objects that you need to access and modify, you should use a dict. If you are using the dict as a container like car={“make”:”honda”,”color”:”red”}, you should use a proper object like a class, dataclass, or pydantic model based on whether you need validation, type safety, etc. This drastically reduces bugs and code complexity, helps others reason about your code, gives you access to better tooling etc. reply jpc0 5 hours agorootparentprevA hash function will always be more expensive than a pointer lookup, specially concidering a pointer lookuo is still needed after the hash function. No matter what you do, a lookup into an array will always be quicker than a hash lookup if you don't need to do a linear search, even in a lot of cases the linear search will be quicker. Structs in other languages is a lookup of pointer + and offset. Which to my knowledge is also true in python classes using __slots__. There's no reason to use a dict if you know the contents of the data, use a dataclass with slots=True purely because there's no hash function run on every lookup into the datastructure. reply cruffle_duffle 4 hours agorootparentprevRight? I thought pretty much all the higher level “objecty” stuff in python are dicts under the hood. reply seabrookmx 14 hours agoparentprevSubclassing NamedTuple is very ergonomic, and given they're immutable unlike data classes I often reach for them by default. I still use Pydantic when I want custom validation or when it ties into another lib like FastAPI. reply Jean-Papoulos 12 hours agoprev>\"unstructured data is problematic\" >\"solution : use dataclasses\" Damn, it's almost like using an untyped language for large projects is not a great idea. reply mkesper 9 hours agoparentPython is absolutely typed. By default, it's really dynamic, though. reply ktosobcy 11 hours agoparentprevAnd yet we are overwhelmed by javascript nonsense... I get it - it's so easy to get up to speed with tiny snippets but it quickly becomes hot mess. Yes, decades ago I was also fascinated by python and it's ease of doing stuff (compiler doesn't complain that I missed something) but with time I grew fond of statically typed languages... they simply catch swaths of errors earlier... reply kabes 9 hours agorootparentAre we still overwhelmed by js? I almost only see TS code these days. reply ktosobcy 3 hours agorootparentI had to deal with it ~1-2 years ago so for my faily ancient self it feels \"recent\" though considering that there were JS frameworks popping up every couple of months and were getting dropped a year later then my timeframe may be off ;) reply sosotrue 12 hours agoparentprevnext [5 more] [flagged] jmorenoamor 12 hours agorootparentDynamic languages demand self discipline, they teach you to respect runtime and think ahead of execution time. I've written software with both typed and untyped languages and never had problems (out of the ususal) with them. reply ktosobcy 11 hours agorootparent> Dynamic languages demand self discipline, they teach you to respect runtime and think ahead of execution time. Ah... yes... because static languages doesn't do that by forcing you to properly model everything. And as a bonus you can easily navigate between everything and not fear that you miss something while refactoring... reply jrjrjrjrj 12 hours agorootparentprevI would argue that dynamic languages make a compile time problem a run time problem... So yeah that small not hit portion of code can always be a time bomb if it does not get tested... reply consp 10 hours agorootparent> So yeah that small not hit portion of code can always be a time bomb if it does not get tested... That has nothing to do with the language, and can happen in any language. reply jimberlage 17 minutes agoprevIf you want an example of a language where the exact opposite advice is taken at all times (with all the pitfalls described in this blog post), give Clojure a whirl. reply newaccountman2 1 hour agoprevI work with people who are ambivalent about this and believe using random dicts in a variety of places is a valid way to write Python code. For these kinds of people, no amount of rational evidence or argument is going to convince them this is bad. They practically make an identity out of eschewing anything that seems too orderly or too designed. (Luckily, at work, most of us on our team like `Pydantic` and also (some of us more than others) type-checking, so these people are dragged along) reply ungamedplayer 13 hours agoprevCan someone educate me in why dicts are uncool for explained reasons, but clojure (which seems to be highly recommended on hn) seems to suffer the same issues when dealing with a map as a parameter (ring request etc). I know how to deal with missing values or variability in maps, and so do a lot of people.. what am I missing here? reply bloppe 12 hours agoparentDicts are great when the data is uniform and dynamic, like an address book mapping names to contact info. You never assume that a key must be in there. Lookups can always fail. That's normal for this kind of use-case. When the data is not uniform (different keys point to differently-typed values), and not as dynamic (maybe your data model evolves over time, but certain functions always expect certain keys to be present), a dict is like a cancer. Sure, it's simple at first, but wait until the same dict gets passed around to a hundred different functions instead of properly-typed parameters. I just quit my job tech at a company that shall remain nameless, partially because the gigantic Ruby codebase I was working on had a highly advanced form of this cancer, and at that point it was impossible to remove. You were never sure if the dict you're supplying to some function had all the necessary keys for the function it would eventually invoke 50 layers down the call stack. But, changing every single call-site would involve such a major refactor that everybody just kept defining their functions to accept these opaque mega-dicts. So many bugs resulted because of this. That was far from the only problem with that codebase, but it was a major recurring theme. I learned this lesson the hard way. reply cornholio 11 hours agorootparentThis should be the top answer. It's not about using dicts in their primary use case, it's about abusing them as a catch all variadic parameter for quick prototyping and \"future expansion\" reply scotty79 10 hours agorootparentprevI think the problem is that different data containers have completely different interfaces. If getting a filed of your object had the same syntax as getting a value from a dict you could easily replace dicts with smarter, more rigid types at any point. My dream is a language that has the containers share as much interface as possible so you can easily swap them out according to your needs without changing most of the code that refers to them. Like easily swap dict for BTreeMap or Redis. I think the closest is Scala but it fallen out of favor before I had a chance to know it. reply lispisok 12 hours agoparentprevMaps arent nearly as problematic in clojure because data is immutable by default on top of the functional paradigm where your program is basically a big composition of functions and the language is built around using maps. In Python I largely agree with the author. In clojure I love my maps. Here is Rich Hickey with an extreme counter example although I would argue he's really demonstrating against getters and setters. https://www.youtube.com/watch?v=aSEQfqNYNAc reply orf 11 hours agoparentprevThey also work fine with JavaScript. The issue is that the concrete types are implicit. Depending on the language, runtime or type system expressing the type in a “better” way might be very hard or un-ergonomic. reply nlitened 11 hours agoparentprevIn Clojure, maps don’t have either of the flaws highlighted in the article. They are neither opaque (they are self-describing with namespaces keys) nor mutable. As a result, they are very powerful and simple to use. reply bigstrat2003 17 hours agoprevFor better or for worse, Python doesn't do typing well. I don't disagree that I prefer well defined types, but if that is your desire then I think Python is perhaps not the correct choice of language. reply Ey7NFZ3P0nzAe 15 hours agoparentPersonnaly I became a huge fan of beartype : https://pypi.org/project/beartype/ Leyec, the magic dev behind it managed to make a full python type checker with super advanced features and about 0 overhead. It's crazy reply skeledrew 14 hours agorootparentI tried using it, but beartype quickly became a pain with having to decorate things manually. Then I found typeguard which goes even further and never looked back. Instead of manually decorating each individual function, an import hook can be activated that automatically decorates any function with type annotation. Massive QoL improvement. I have it set to only activate during testing though as I'm unsure of the overhead. reply Mattwmaster58 5 hours agorootparentIt looks like beartype supports the same sort of implicit decoration, because there's mention of an explicit API: >Beartype now implicitly type-checks all annotated classes, callables, and variable assignments across all submodules of all packages. reply nerdponx 17 hours agoparentprevPython does typing pretty darn well now for data like API requests and responses. \"Typed Python\" does poorly (compared to e.g. Typescript) on things like overloading functions, generics, structural subtyping, et al. reply est 16 hours agoparentprev> Python doesn't do typing well Golang does typing, but JSONs are PITA to handle. Try parsing something like `[{\"a': 1, \"b\": \"c\", \"d\": [], \"e\": {}}, null, 1, \"2\"]` in go. Types are a bless as well as a curse. reply emmanueloga_ 1 hour agorootparentgjson [1] and a few other go packages offer a way to parse arbitrary JSON without requiring structs to hold them. re: Python. I like PyRight/PyLance for Python typing, it seems to \"just work\" afaict. I also like msgspec for dataclass like behavior [2]. --- 1: https://github.com/tidwall/gjson 2: https://jcristharif.com/msgspec/ reply jpc0 5 hours agorootparentprev[]inferface But the same issue exists as other dynamic languages, how do you know what the type is of the item you are accessing? If you know the array will be laid out exactly like that before you make the request you can always create a custom parser to return a struct with those fields name what they actually are instead of arbitrary data. The only valid way to parse that dynamically is to try and fail in a loop which is inefficient enough that you should stop using whatever API returns that monstrosity. reply Aditya_Garg 15 hours agorootparentprevThats only because your list has different types. Its a badly formed API and if you really need to support that use case then you can use maps and reflection to handle it. reply est 15 hours agorootparentThe problem is, programmers can't dictate what JSON should look like in the wild. We used to have strict typed XML. Nobody even bothered. reply a57721 13 hours agorootparent> The problem is, programmers can't dictate what JSON should look like in the wild. Not JSONs in general, but a sane API would never return something like that. > We used to have strict typed XML. Nobody even bothered. Nowadays there is OpenAPI, GraphQL, protobuf, etc. and people do bother about such things. reply mook 13 hours agorootparentUnfortunately, a lot of the time you need to deal with other people's APIs. reply shiroiushi 13 hours agorootparentprev>We used to have strict typed XML. Nobody even bothered. Yeah, because it was ugly as hell and not human-readable. reply Turskarama 12 hours agorootparentprevAnd if you got that JSON back in Python, how would you do anything with it? This API is essentially useless. You can deserisalise it, sure, but then what? reply wruza 4 hours agoprevKnew it was python before the first line of code. Python lacks ceremony-free data syntax, that’s why people use dicts. Dataclasses have to be named, initialized and imported, which is tedious. Much easier to just foo({name, age}) and let typings match, but python doesn’t have that. Lack of “POPO” is a design mistake. reply Garlef 11 hours agoprevI don't think dicts themselves are the problem. In typescript using plain JS objects is very straightforward. Of course you have to validate the schema at your system boundaries. But you'll have to do this either way. So: If this works very well in TS it can't be dicts themselves but must be the way they integrate into- and are handled in python. This leads me to the conclusion that arguments presented in the article might be the wrong ones. (But I still think, the conclusion the article arrives at is okay. But I don't think there's a strong case being made in the article about wether to prefer data classes or typed dicts.) reply soulchild77 11 hours agoparentThis. I think types really make the difference here. You can get very far with just plain old JS objects as long as you've got strong types in place. reply hcarvalhoalves 17 hours agoprevDebatable. Here's a counter-point: https://www.youtube.com/watch?v=aSEQfqNYNAc But ok, it's less bad in Python since objects are dicts anyway and you don't need getters. reply Attummm 9 hours agoprevPython has made its rise as an antithesis to Java thinking. Classes used to be seen by some in the community as an anti-pattern. [0] The coding style used to focus on \"Pythonic-ness,\" which meant using Python's expressiveness to write code in such a way that type information could be inferred without explicitly stating the type. Most developers will carry their previous language paradigms into their new ones. But if types, DDD (Domain-Driven Design), and classes are what you're looking for, then Python isn't the best fit. Python doesn't have compiler features that work well with those paradigms, such as dead code removal/tree shaking. However, starting out with dictionaries and then moving over to dataclasses is a great strategy.[1] As a small note, it's kind of ironic that the statically typed language Go took inferred typing with their := operator, while there is now a movement in Python to write foo: str = \"bar\". [0] https://youtu.be/o9pEzgHorH0?si=pv0QQyM-iBrHuXUN [1] https://docs.python.org/3/library/dataclasses.html reply fhdsgbbcaA 17 hours agoprevSeems like the issue is less using dicts than not treating external APIs as input that needs to be sanitized. reply physicsguy 9 hours agoparentThe code in the examples doesn't even check the API response code, let alone the structure of the response. reply pmarreck 17 hours agoparentprevAgreed. If you sanitize/allowlist API data you should not have issues with dicts. reply imron 17 hours agorootparentYou'll have issues if you ever rename things in the dict. Linting tools will pick up on every instance where you forgot to rename the fields of a class, but won't do the same for dicts. reply FreakLegion 17 hours agorootparentTypedDicts solve the linting problem, but refactoring tools haven't caught up (unlike e.g. ForwardRef type annotations, which are strings but can be transformed alongside type literals). reply tomjakubowski 16 hours agorootparentIs there any advantage to using a TypedDict for a record over a dataclass? reply FreakLegion 14 hours agorootparentTypedDicts \"aren't real\" in the sense that they're a compile-time feature, so you're getting typing without any deserialization cost beyond the original JSON. Dataclasses and Pydantic models are slow to construct, so that's not nothing. This of course means TypeDicts don't give you run-time validation. For that, and for full-blown custom types in general, I tend to favor msgspec Structs: https://jcristharif.com/msgspec/benchmarks.html#json-seriali.... reply orf 12 hours agorootparent> Dataclasses and Pydantic models are slow to construct Citation needed? Pydantic is really quite fast, and you can pass raw JSON responses into it. It may be slower (depending on the validators or structure), but I’d expect it to be comparably fast to the stdlib JSON module. reply FreakLegion 11 hours agorootparentPydantic's JSON parsing is faster than the built-in module, on par with orjson, but creating model instances and run-time type checking net out to be much slower. I linked msgspec's benchmarks in the previous post. reply cle 14 hours agoprevDicts can be a problem, but this particular example isn't that great, like in this diagram from the article: External APISer/DeBusiness Logic Life's all great until \"External API\" adds a field that your model doesn't know about, it gets dropped when you deserialize it, and then when you send it back (or around somewhere else) it's missing a field. There's config for this in Pydantic, but it's not the default, and isn't for most ser/de frameworks (TypeScript is a notable exception here). Closed enums have a similar tradeoff. reply mjr00 14 hours agoparentIf external API adds a new field but your software already worked, you didn't need it in the first place, so why should it matter? Dropping unknown/unused fields makes sense in 99% of cases. reply buzer 13 hours agorootparentUnfortunately some APIs assume that they will get all the fields as part of the update. If field doesn't exist in the input it gets it will drop the original value during the update. reply _ZeD_ 12 hours agorootparentyet, again, most of the libraries already deal with extra fields... i.e. for pydantic https://docs.pydantic.dev/latest/concepts/models/#extra-fiel... reply vouwfietsman 11 hours agorootparentI don't deal with external APIs often, but this is a development nightmare. You can't just magically let data flow through your system without knowing about it, because this is not how programming works. Your API has a contract and your code is written to support that contract, if the contract changes it should either be a very consciously decided breaking change that is versioned somehow, or it should be an unversioned non breaking change. Apparently whatever data is added like this is completely meaningless to your program so why do you need to be in charge of passing it back to the API. Changing your API and assuming everything just keeps working is a nonsense cowboy attitude to software compatibility, even if some frameworks bend over backwards to support it through magic that's hidden from the developer. Furthermore, many programming languages are simply incapable of doing this, and this approach to APIs is immediately restricting those languages from use. Finally, transforming objects to an internal domain model is really the cornerstone of a lot of recent well-thought-out programming discipline, and this API design is throwing that in the garbage. It's explicitly asking you to mess up your service architecture, spreading bad architecture like a virus to all systems that interact with the API. reply CraigJPerry 11 hours agoprevThis has merit in some cases but let me try to make a counterpoint. You lose the algebra of dict’s - and it’s a rich algebra to lose since in python it’s not just all the basic obvious stuff but it’s also powerful things like dict comprehensions and ordering guarantees (3.7+ only). You tightly couple to a definition - in the simple GitHubRepository example this is unlikely to be problematic. In the real world, coupling like this[1] to objects trying to capture domain data with dynamic structures is regularly the stuff of nightmares. The over-arching problem with the approach given is that it puts code above data. You take what could be a schema, inert data about inert data, and instead use code. But it might also be an interesting case to consider as a slippery slope - if you can put code concerns above data concerns then maybe soon you will see cases where code concerns rank higher than the users of your software? [1] - by coupling like this I mean the “parse don’t validate” school of thought which says as soon as you get a blob of data from an external source, be it a file, a database or in this case a remote service, you immediately tie yourself to a rocket ship whose journey can see you explosively grow the number of types to accurately capture the information needed for every use case of the data. You could move this parsing operation to be local to the use case of the data (much better) rather than have it here at the entry point of the data to the system but often times (although not always) we can arrive at a simpler solution if we are clever enough to express it in a style that can easily be understood by a newbie to programming. That often means relying on the common algebra of core types rather than introducing your own types. reply zmgsabst 11 hours agoparentYou also make a nightmare of dynamically adding middleware — which can piggyback on a generic dict and have no meaningful way to insert themselves into your type maze. reply cranium 13 hours agoprevPython dataclasses are a good start for internal use. They are just a bit of a pain to serialize/deserialize natively. When it comes to that, I prefer to use Pydantic objects and have all the goodies, at the cost of some complexity. reply cschneid 17 hours agoprevI generally support this. When dealing with API endpoints especially I like to wrap them in a class that ends up being. I also like having nested data structures as their own class sometimes too. Depends on complexity & need of course. class GetThingResult def initialize(json) @json = json end # single thing def thing_id @json.dig('wrapper', 'metadata', 'id') end # multiple things def history @json['history'].map { |h| ThingHistory.new(h) } end ... two dozen more things end reply QuadrupleA 1 hour agoprevHard disagree on most of this. The immutability dogma for one (changing data is \"the worst felony you can commit to your data\"). Computing IS manipulation and transformation of data. The contortions people go through to try and sidestep that seem delusional. Plus all this 1995-era OOP and domain-driven-design crap, \"business logic\" and data layers and all this other architectural rigidity and usually-needless complexity, layers of boilerplate (and then tools to automate the generation of that), etc. If your function takes a dict, and is called from many different places, document the dict format in the function comment. Or yes, create a dataclass if it saves more trouble than its additional boilerplate and code and maintenance causes. But take it case by case and aim for simplicity. Most of the time I call out to an API in python, I process its JSON/dict response right after the call, using maybe 10% of the data returned. That's so much cleaner and simpler than writing a whole Data Object Layer, to be used by my API Interface Layer, to talk to my Business Logic layer, etc. reply xenoxcs 13 hours agoprevI'm a big fan of using Protobuf for the third-party API validation task. After some slightly finniky initial schema definition (helped by things like json-to-proto.github.io), I can be sure the data I'm consuming from an external API is strongly typed, and the functions included in Protobuf which convert JSON to a Proto message instance blows up by default if there's an unexpected field in the API data it's consuming. I use it to parse and validate incoming webhook data in my Python AWS Lambda functions, then re-use the protobuf types when I later ship the webhook data to our Flutter-based frontend. Adding extensions to the protobuf fields gives me a nice, structured way to add flags and metadata to different fields in the webhook message. For example, I can add table & column names to the protobuf message fields, and have them automatically be populated from the DB with some simple helper functions. Avoids me needing to write many lines of code that look like: MyProtoClass.field1 = DB.table.column1.val MyProtoClass.field2 = DB.table.column2.val reply secretsatan 6 hours agoprevI largely moved away from dictionaries when switching to swift. Usually we only use them now when going from legacy code. For the example in the article with JSON, Swift has the Codable protocol, which cleaned all my client code for our back end from the old NSJSONSerialization code. reply karmakurtisaani 15 hours agoprevI've cleaned up code where input parameters came in a dict form. Absolute shit show. - The only way to figure out which parameters are even possible was to search through the code for the uses of the dict. - Default values were decided on the spot all over the place (input.getOrDefault(..)). - Parameter names had to be typed out each time, so better be careful with correct spelling. - Having a concise overview how the input is handled (sanitized) was practically impossible. 0/10 design decision, would not recommend. reply pansa2 12 hours agoprev> convert [dicts] immediately to data structures providing semantics [...] You can simplify your work by employing a library that makes “better classes” for you Python seems to have many different kinds of \"better classes\" - the article mentions `dataclass` and `TypedDict`, and AFAIK there are also two different kinds of named tuple (`collections.namedtuple` and `Typing.NamedTuple`). What are the advantages of these \"better classes\" over traditional classes? How would you choose which of the four (or more?) kinds to use? reply pansa2 12 hours agoparentTo me, the proliferation of \"better classes\" implies there's a problem with Python's built-in classes - but what's wrong? Are they just too flexible and/or too verbose? Or actually deficient in some way? reply zmgsabst 11 hours agorootparentPeople enjoy the flexibility and many Python systems rely on duck-typing via dicts, etc. So people are trying to force Python to be something it isn’t in adherence to their ideology — but it fails to gain consensus because there’s a sizable cohort that use Python because it isnt those things. So we get repeated implementations, from each ideologically motivated group. reply Waterluvian 17 hours agoprevI think one really nice thing about Python is duck typing. Your interfaces are rarely asking for a dict as much as they’re asking for a dict-like. It’s pretty great how often you can worry about this kind of problem at the appropriate time (now, later, never) without much pain. There’s useful ideas in this post but I’d be careful not to throw the baby out with the bath water. Dicts are right there. There’s dict literals and dict comprehensions. Reach for more specific dict-likes when it really matters. reply turnsout 16 hours agoparentDuck typing is so fragile… Once you have implementations that are depending on your naming or property structure, you can’t update the model without breaking them all. If you use a real type, you never have to worry about this. reply pistoleer 13 hours agorootparentYou would still have to update everything if you rename a field in a struct, what do you mean you never have to worry? reply dwattttt 12 hours agorootparentIf you use type checking, the breakage occurs when you introduce the change: the author of the change is the one who can figure out what it means if 'foo' is no longer being passed into this function. If you're duck typing, you find this out in the best case when your unit tests exercise it, and in the worst case by a support call when that 1/1000 error handling path finally gets exercised in production. reply turnsout 5 hours agorootparentExactly… with strong typing, you can do the refactor automatically, because the IDE knows everywhere that symbol is used. (For codebases in your control—for third party users, you can indicate that something has been deprecated or renamed via a warning or other language feature) reply pistoleer 11 hours agorootparentprevI agree with that, in the context of dynamically typed languages. Slowly but surely, new languages are starting to develop with static duck typing. Implicit interfaces if you will. reply zmgsabst 11 hours agorootparentprevAnd now inserting every middleware is an exercise in retyping the system, rather than piggybacking on the parameter dict. reply est 15 hours agoprevdicts are OK, because at least they do have a `key` and it does mean something. un-annotated tuples and too many func params are cancer. reply ramraj07 15 hours agoparentWho does this still?? reply directevolve 11 hours agorootparentIn bioinformatics, one of our main dataflow platforms, Nextflow, is built with unnamed tuples in mind. Implementing the ability to conveniently pass data with HashMaps instead of unnamed tuples was a huge boost to usability for me. reply dijksterhuis 7 hours agorootparenti really want to go on a rant about the general state and historical choices regarding data formats and data structures in bioinformatics, plus all the wheel reinvention. but i’m also trying to move on and do things differently today. let’s just say the situation is displeasing and leave it at that. reply stonethrowaway 15 hours agoparentprevNo no, Un-annotated tuples and too many func params are OK, because at least they are pushed and popped from the stack. Calls and rets without a prologue and epilogue on the other hand… reply est 15 hours agorootparent> from the stack Or many, many stacks you can't comprehend nor amend. I dare to add a new `key` to a dict, can you modify a func call or a tuple with confidence? reply pmarreck 17 hours agoprevLess important in Elixir (where they are \"maps\") due to the immutable nature of them as well as the Struct type which is a structured map. reply nesarkvechnep 13 hours agoparentYes, usually my APIs in Elixir receive their arguments as a well-typed map, not stringly keyed, and transform them to structs which the core business logic expects. reply mikhmha 15 hours agoparentprevYup! I find Elixir makes it really intuitive to know when to represent a collection as a map and when to use a list of tuples. And its easy to transform between the two when needed. reply thebeardisred 17 hours agoprevFYI, posted in 2020, updated in 2021. reply leoh 15 hours agoprevBig structs as params in rust have similar issues reply saintfire 15 hours agoparentIn what way? They're not opaque or mutable (by default). They can be unwieldy but they do define a pretty strongly typed API. reply klyrs 17 hours agoprevLists and sets suffer the same drawbacks. If the advice is to not use any of the batteries included if the language, why are we using Python? If you want an immutable mapping, why not use an enum? reply o11c 17 hours agoparentThis isn't arguing against them in general, but against the unfortunate Javascript-esque abandonment of specified semantics. In particular, whenever anyone thinks that \"deep clone vs shallow clone\" is a meaningful distinction, that means their types are utterly void of meaning. reply scotty79 10 hours agoprev> Ignore fields coming from the API if you don’t need them. Keep only those that you use. This is great if you know what you need from the start. If you only find out what you need after passing your data through multiple layers and modules of your system then you need to backtrack through all your code to the place of creation. If you have immutable data structures then you have to backtrack through multiple places where your data is used from previous structures to create new ones to pass your additional data through all that. So if your data travels through let's say 3 immutable types to reach the place you are working on then even if you know exactly where the new field that you need originates, you need to alter 3 types and 3 places where data is read from one type and crammed into another. If you have a dict that you fill with all you got from the api there's zero work involved with getting the new piece of information that you thought you didn't need but you actually do. It's just there. reply Barrin92 16 hours agoprevIt's a bit of an odd article because the second part kind of shows why dicts aren't a problem. You basically just need to apply the most old school of OO doctrines: \"recipients of messages are responsible for how they interpret them\", and that's exactly what the author advocates when he talks about treating dict data akin to data over the wire, which is correct. If you're programming correctly and take encapsulation seriously, then whatever shape incoming data in a dict has isn't something you should take an issue with, you just need to make sure if what you care about is in it (or not) and handle that within your own context appropriately. Rich Hickey once gave a talk about something like this talking about maps in Clojure and I think he made the analogy of the DHL truck stopping at your door. You don't care what every package in the truck is, you just care if your package is in there. If some other data changes, which data always does, that's not your concern, you should be decoupled from it. It's just equivalent to how we program networked applications. There are no global semantics or guarantees on the state of data, there can't be because the world isn't in sync or static, there is no global state. There's actually another Hickey-ism along the lines of \"program on the inside the same way you program on the outside\". Dicts are cool, just make sure that you're always responsible for what you do with one. reply alfons_foobar 12 hours agoparentI assume you're basically referring to this quote from the article? \"Ignore fields coming from the API if you don’t need them. Keep only those that you use.\" IMO this addresses only one part of the problem, namely \"sanitize your inputs\". But if you follow this, and therefore end up with a dict whose keys are known and always the same, using something \"struct-like\" (dataclasses, attrs, pydantic, ...) is just SO much more ergonomic :) reply gotoeleven 14 hours agoprev [–] Personally I find it is often helpful to keep Dicts in a BigBag ie: BigBag reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Dictionaries (dicts) in Python are mutable and opaque, which can complicate code maintenance and extension.- It is recommended to convert dicts into structured data models using tools like dataclasses or Pydantic for better code management.- For legacy code, TypedDicts can be used to add structure, and Mapping annotations are suggested for key-value stores to prevent technical debt."
    ],
    "commentSummary": [
      "The post emphasizes using value objects, such as dataclasses, for handling data early in the development process to ensure clear data definitions and avoid optional fields.- It suggests that while dictionaries are useful for dynamic data, overusing them can lead to messy code, and structured types should be used for known data to improve code clarity and reduce bugs.- Python provides tools like dataclasses, TypedDict, and Pydantic to facilitate better data handling, promoting clean and efficient code."
    ],
    "points": 174,
    "commentCount": 105,
    "retryCount": 0,
    "time": 1728421814
  },
  {
    "id": 41784287,
    "title": "US weighs Google break-up in landmark antitrust case",
    "originLink": "https://www.ft.com/content/f6e84608-e0e5-48c5-a0eb-dde7675fb608",
    "originBody": "Accessibility helpSkip to navigationSkip to contentSkip to footer Sign In Subscribe Open side navigation menuOpen search bar SubscribeSign In Search the FT SearchClose search bar Home World Sections World Home Middle East war Global Economy UK US China Africa Asia Pacific Emerging Markets Europe War in Ukraine Americas Middle East & North Africa Most Read Starmer refuses to rule out raising employer national insurance rate Jenrick and Badenoch to contest Tory leadership after Cleverly knocked out Argentina’s Milei says his ‘regime of freedom’ not ready to drop currency controls Hurricane Milton could cost $60bn in insurance losses Biden and Netanyahu speak as Israel plans Iran retaliation US Sections US Home US Economy Investing in America US Companies US Politics & Policy US Presidential Election 2024 Most Read Hurricane Milton could cost $60bn in insurance losses Biden and Netanyahu speak as Israel plans Iran retaliation US foreign policy is too volatile to lead the world KPMG US chief calls for urgent reform to halt slide in accounting ranks Treasury market volatility surges as investors rethink interest rate bets Companies Sections Companies Home Energy Financials Health Industrials Media Professional Services Retail & Consumer Tech Sector Telecoms Transport Most Read GSK reaches $2.2bn heartburn medication Zantac settlement in US OpenAI pursues public benefit structure to fend off hostile takeovers Hurricane Milton could cost $60bn in insurance losses British mogul Richard Desmond gets Dubai golden visa for ‘lifestyle’ reasons Google DeepMind duo share Nobel chemistry prize with US biochemist Tech Sections Tech Home Artificial intelligence Semiconductors Cyber Security Social Media Most Read Google DeepMind duo share Nobel chemistry prize with US biochemist Google break-up could turn Big Tech into Medium Tech US weighs Google break-up in landmark antitrust case Google break-up reads like antitrust fan-fiction The new recruitment arms race Markets Sections Markets Home Alphaville Markets Data Crypto Capital Markets Commodities Currencies Equities Wealth Management Moral Money ETF Hub Fund Management Trading Most Read Argentina’s Milei says his ‘regime of freedom’ not ready to drop currency controls Treasury market volatility surges as investors rethink interest rate bets Rio Tinto isn’t bagging a bargain with lithium deal As Chinese stocks slide, should investors bet on a Beijing bazooka? Europastry’s ‘frozen croissant’ IPO delayed a second time Climate Opinion Sections Opinion Home Columnists The FT View The Big Read Lex Obituaries Letters Most Read US foreign policy is too volatile to lead the world Google break-up could turn Big Tech into Medium Tech Lessons in law and economics from the Next pay gap case Rio Tinto isn’t bagging a bargain with lithium deal Italy’s single women fight for the right to IVF Lex Work & Careers Sections Work & Careers Home Business School Rankings Business Education Europe's Start-Up Hubs Entrepreneurship Recruitment Business Books Business Travel Working It Most Read Lessons in law and economics from the Next pay gap case UK companies given greater leeway to award executives big pay rises The new recruitment arms race A chef’s guide to London’s top Sunday lunches Should we let friends share our nanny? Life & Arts Sections Life & Arts Home Arts Books Food & Drink FT Magazine House & Home Style Travel FT Globetrotter Most Read Boris Johnson’s Unleashed — the hero of his own Homeric tale Cate Blanchett enlivens Alfonso Cuarón’s self-indulgent TV psychodrama Disclaimer Designer watch? I’ll keep my £25 Casio, thanks Mark Rylance’s vaudevillian turn unbalances Juno and the Paycock Recipe: the best thing you could do with bread and mushrooms HTSI MenuSearch Home World US Companies Tech Markets Climate Opinion Lex Work & Careers Life & Arts HTSI Financial Times SubscribeSign In Search the FT SearchClose search bar US weighs Google break-up in landmark antitrust case Subscribe to unlock this article Limited time offer Save 40% on Standard Digital was $468 now $279 for your first year, equivalent to $23.25 per month. Make up your own mind. Build robust opinions with the FT’s trusted journalism. Take this offer before 24 October. Take this offer What’s included Global news & analysis Expert opinion FT App on Android & iOS FT Edit app First FT: the day’s biggest stories 20+ curated newsletters Follow topics and set up personalized alerts FT Videos & Podcasts 10 monthly gift articles to share Explore more offers. Trial $1 for 4 weeks Then $75 per month. Complete digital access to quality FT journalism. Cancel anytime during your trial. Select What's included Premium Digital $75 per month Complete digital access to quality FT journalism with expert analysis from industry leaders. Pay a year upfront and save 20%. Select What's included Print Save over 65% $99 for your first year FT newspaper delivered Monday-Saturday, plus FT Digital Edition delivered to your device Monday-Saturday. Select What's included Terms & Conditions apply Explore our full range of subscriptions. For individuals Discover all the plans currently available in your country Digital Print Print + Digital For multiple readers Digital access for organisations. Includes exclusive features and content. FT Professional Check whether you already have access via your university or organisation. Why the FT? See why over a million readers pay to read the Financial Times. Find out why Useful links Support View Site TipsHelp CentreContact UsAbout UsAccessibilitymyFT TourCareers Legal & Privacy Terms & ConditionsPrivacy PolicyCookie PolicyManage CookiesCopyrightSlavery Statement & Policies Services Share News Tips SecurelyIndividual SubscriptionsProfessional SubscriptionsRepublishingExecutive Job SearchAdvertise with the FTFollow the FT on XFT ChannelsFT Schools Tools PortfolioFT AppFT Digital EditionFT EditAlerts HubBusiness School RankingsSubscription ManagerNews feedNewslettersCurrency Converter Community & Events FT Live EventsFT ForumsBoard Director Programme More from the FT Group Markets data delayed by at least 15 minutes. © THE FINANCIAL TIMES LTD 2024. FT and ‘Financial Times’ are trademarks of The Financial Times Ltd. The Financial Times and its journalism are subject to a self-regulation regime under the FT Editorial Code of Practice. Close side navigation menu Edition:International UK Search the FT Search Subscribe for full access Top sections Home WorldShow more World Middle East war Global Economy UK US China Africa Asia Pacific Emerging Markets Europe War in Ukraine Americas Middle East & North Africa USShow more US US Economy Investing in America US Companies US Politics & Policy US Presidential Election 2024 CompaniesShow more Companies Energy Financials Health Industrials Media Professional Services Retail & Consumer Tech Sector Telecoms Transport TechShow more Tech Artificial intelligence Semiconductors Cyber Security Social Media MarketsShow more Markets Alphaville Markets Data Crypto Capital Markets Commodities Currencies Equities Wealth Management Moral Money ETF Hub Fund Management Trading Climate OpinionShow more Opinion Columnists The FT View The Big Read Lex Obituaries Letters Lex Work & CareersShow more Work & Careers Business School Rankings Business Education Europe's Start-Up Hubs Entrepreneurship Recruitment Business Books Business Travel Working It Life & ArtsShow more Life & Arts Arts Books Food & Drink FT Magazine House & Home Style Travel FT Globetrotter Personal FinanceShow more Personal Finance Property & Mortgages Investments Pensions Tax Banking & Savings Advice & Comment Next Act HTSI Special Reports FT recommends Alphaville FT Edit Lunch with the FT FT Globetrotter #techAsia Moral Money Visual and data journalism Newsletters Video Podcasts News feed FT Schools FT Live Events FT Forums Board Director Programme myFT Portfolio FT Digital Edition Crossword Our Apps Help Centre Subscribe Sign In",
    "commentLink": "https://news.ycombinator.com/item?id=41784287",
    "commentBody": "US weighs Google break-up in landmark antitrust case (ft.com)164 points by JumpCrisscross 15 hours agohidepastfavorite219 comments stego-tech 41 minutes agoLoving the discourse in the comments here. Speaking as someone who grew up alongside Google in a sense, I fall on the side of the \"break it up\" camp. It was novel at the time to have a single company so nicely provide us with everything digitally that we could need - G Suite, YouTube, Search, Maps, Advertising, Books, Reader, etc - but in hindsight, we gave too much power to too unaccountable an entity, who in turn used it to choke off any avenues to challenge its dominance. As for the rebuttal of \"bUt ThE gDp\", there's a counter-argument to be made that the GDP would have grown just as much with a higher diversity and stringent M&A regulations by spurring on new businesses and concepts, as opposed to mainstreaming fad after fad that seemingly solely benefits the established players. There's another argument to be made equating GDP values to desirable targets creates a Goodhart's Law problem, thereby making GDP as a measure of growth or success a bad metric; the rise of income and wealth inequality these past few decades, when there was increased focus on GDP as a target of growth or success, could be viewed as evidence supporting said argument. It's worth noting that this case against Google is likely to be the bellwether for future divestments and breakups. Whatever comes next will be an invaluable learning experience for both sides, just like AT&T and Microsoft's own anti-trust woes were learning experiences for the current crop of companies. reply sirspacey 5 minutes agoparentWhat MSFT learned from that was to open offices in Washington DC, lobby and sell to the federal gov. Hard to find the sense in this action. reply hightrix 10 minutes agoparentprev> nicely provide us with everything digitally that we could need... Advertising Sorry for the snark, but no one anywhere needs advertising. Advertising in it's current form is a plague on humanity. reply jajko 16 minutes agoparentprevIf I am worried about anybody holding my private data (apart from US' 3-letter agencies and thats a hill I am not going to die upon since I don't have same basic human rights as US citizens), its Meta. Sure Google is everywhere, but Meta holds much more data on our inner personal sides, which are the easiest things to actually abuse to no end. That and pornhub. reply sitkack 10 minutes agorootparentAre you sure? Google has a search and location data, Google email is on one side if not both of most email chains, receipts, etc. I don't use meta for much. Google on the otherhand takes some pretty dedicated behavior to avoid. reply wonderwonder 2 minutes agorootparentprevGoogle actually tracks associates everywhere you visit in incognito mode in chrome and associates it with your main user id. So whatever pornhub knows, so does google. reply 2OEH8eoCRo0 9 minutes agoparentprevThe GDP argument is weird. If Google being huge is good for GDP why don't we just let them own the whole country? 1 US company for everything, maximum GDP afterall! reply gaiagraphia 54 minutes agoprevThe free market's great IF there's competition. If the market's dead, it's time to reinject some life and keep the cogs turning. Scale's great, but it often comes with a societal cost. For every efficiency made, there's less agency and decent jobs to go round. Seems unfair to make such decisions ad-hoc though, and string it out through years of court cases and m/billions of lawyer fees. Why not establish rules of the market where once a company gets x% market share, the company 'wins', the CEO gets the ability to run for high office, the nation thanks shareholders and gives them a big payoff for supporting innovation, and those 1 run down the food chain get to spin off their own companies and go for gold. Life and death is a part of everything sustainable in life. We should embrace these cycles and utilise them, not let old hat stagnation strangle and squeeze all what's good from life. reply 4star3star 47 minutes agoparentIt would become a game to NOT capture x% market share to the degree that it suited interested parties to avoid it. reply grumpy_coder 0 minutes agorootparentThat game already exists. Intel and Amd had an odd relationship for years when Intel needed Amd so they could claim that competition existed. Cross licensing instruction sets and the like. reply gmueckl 21 minutes agorootparentprevThat would have interesting implications. Companies would have to grow by aggressive diversification into other markets. Instead of a Google or Appple that controls your digital life, you'd have an endgame with a few ACMEs that attempt to lock you in in every aspect of your life: Your pots would work best with a stove from the same company. Your washing machine and detergent only do a good on a specific brand of clothes etc. A completely different kind of market regulation would be required. reply randomdata 17 minutes agoparentprev> The free market's great IF there's competition. If you have a free market finding competition isn't an issue, but maintaining profitability is. If we had a free market when Google came onto the scene, everyone else would have copied PageRank the next day, and then you'd have hundreds of search engines all as good as each other, each sharping their pencils sharper and sharper in an attempt to win customers over on a price basis until there is nothing left. At which point there is nothing left to further innovation. To combat that, we grant short-term monopolies over technology to allow their inventors time to build up a decent business before opening the flood gates, with the intent to balance what makes mixed-market economies great without ending up with no competition. The problem is that those monopoly procedures were established when time moved slowly. Back then, 20 years was barely enough time to get your product to and recognized in the market. These days, you can get there in a few years, or even less, which leaves nearly 20 more years to focus on killing all the competition. Ultimately, we would have been better off if Google was pushed out into a free market after a few years. We benefitted from it having some head start, but it went on much too long. reply s1artibartfast 39 minutes agoparentprevWhat happens to the company when it wins in your scenario. Did you leave that out or did I miss something? reply relistan 1 hour agoprevHilarious to see all the people lamenting the break-up of AT&T. That break-up sparked the long distance phone race, which became the driving force for the massive laying of fiber optics... which enabled the Internet boom of the 1990s. reply bityard 33 minutes agoparentBreaking up AT&T was unquestionably the right call. (So to say.) Long distance was expensive for quite a while even after the break-up. If you called up an out-of-state friend or relative to catch up, you expected the call to cost you at least a few bucks. (And you hoped they would be the ones to call you next time.) Even into the 90's, long distance at $0.10/minute was considered cheap. And in most rural areas, everywhere past a mile or so out of town was long distance. I remember buying long-distance calling cards to bring our phone costs down. For about 5 years, it was cheaper to just get a local-only phone line and then buy your long-distance as phone cards. Each card came with a certain number of minutes pre-loaded. You'd dial the 1-800 number on the back, scratch off your PIN, enter it, and then you'd dial your destination number. Other than the hassle of buying and using the card, the major downside was that your own number didn't (usually) show up on the caller ID. They were also good if you stayed in hotels a lot, since hotels would charge upward of usurious amounts for both local and long-distance calls but they would typically allow toll-free calls to go through without charge. reply nickff 35 minutes agoparentprevAT&T had also been granted a long-term monopoly on long-distance telephony by the federal government; Google has never had such protection. reply CSMastermind 41 minutes agoparentprevMy understanding was that most of the internet infrastructure was laid by Bell before their break-up because they projected video calling being a huge use case in the future. reply floxy 8 minutes agorootparent1982 seem a bit early for the massive fiber rollout. My recollection was that Sprint was the driving factor for laying fiber, since they had the railroad right-of-ways. But that was quite a while back. Seems like someone could have written a good book about internet infrastructure, especially the mid-to-late 90s. Anyone have suggestions? reply geor9e 2 hours agoprevHN is a special place but among the average folk, I get why 90% of people use google. I'm no google fan, but it still baffles me how bad the alternatives still are. Especially with so many companies scraping the whole web for AI data, and so many GPUs chugging on that data. I suppose they are all focused on replacing the concept of search results with LLM prose. Ironically bing is down at the moment https://downdetector.com/status/bing/ reply mm263 1 hour agoparent> it still baffles me how bad the alternatives still are Kagi reply arghwhat 1 hour agorootparentAfter having been a paid subscriber for a number of months, I have to say that it still wasn't cutting it. I still ended up falling back to google all the time. Plus, https://d-shoot.net/kagi.html reply AlexandrB 1 hour agorootparentWhat bugs me most about Kagi at the moment is that their iOS plugin behaves weirdly. Frequently it will completely swallow my search and I'll have to retype it. Eventually I just disabled it :( Arguably this is not their fault - I really wish iOS/MacOS Safari let you set a custom search engine instead of picking from a fixed list. reply kachapopopow 1 hour agorootparentprevInteresting read. reply chiaroscuro1312 36 minutes agorootparentprevthanks for this link reply whiplash451 2 hours agoparentprevMaybe because search is an extremely hard problem and requires hundreds of millions (if not billions) of investment to remain competitive? reply Night_Thastus 2 hours agorootparentAlso: There is a MASSIVE incentive for foul play from everyone being searched. Every single business wants their results at the top. Everyone is grasping for every single advertiser's cents. The monetary encouragement to game the system is so incredibly strong. Google has to fight that uphill battle 24/7. I have a lot of issues with Google as a company, but I do not envy their position when it comes to the web search. It is a cursed problem no matter how you look at it. reply ta_1138 24 minutes agorootparentNo doubt, but there's a whole lot of problems shaped like that in tech today. Look at any two sided market: Amazon/Etsy's third party sellers also want to be at the top, and there are bad actors trying to scam the intermediary and the customer. The same low friction onboarding that allows the company to succeed as an intermediary is also facilitating the fraud. Merchants can be fraudsters, advertisers can be fraudsters, users can be fraudsters producing fake clicks. There's economic incentives everywhere. See the relatively recent news of spotify getting generated songs, which are then played by fake listeners. The companies are entering this situations with their eyes wide open. It's a necessary problem when you want to be an intermediary at a large enough scale. And if you don't have the scale, the per-interaction costs are really high, and your company gets beaten by someone with an easier onboarding funnel. reply mc32 1 hour agorootparentprevIf they only divided it into information search (\"library\" knowledge) and business search (Yelp/YP) where you can tweak your preferences. When you search for \"vacuum cleaner\", you either want online stores or you may want a local store. In either case it's a business query rather than looking for reviews or specs. What's really polluting search, including Google, is the on-demand content generation based on your query. It's a sea of flotsam. reply ryandrake 9 minutes agorootparentUnfortunately, if you did that, the businesses would still do their best to muscle in on the \"information search\" results. Marketers gotta Market, and placement on a SERP is a zero sum game. If one competitor managed to get themselves highly ranked in the \"information search\" then there would be incentive for all competitors to try. reply blackoil 2 hours agoparentprevThat even if True is irrelevant to discussion. Antitrust isn't mostly against monopoly, but its abuse to gain benefit in another area or uncompetitive practices to stifle potential competition. reply voisin 2 hours agoparentprev> I'm no google fan, but it still baffles me how bad the alternatives still are. I have such a hard time understanding your position on this. Google search results are absolute trash now, compared to DuckDuckGo. reply elevatedastalt 1 hour agorootparentDuckDuckGo can have a seat at the table when they implement their own search index. Putting a sticker on Bing and Yandex results doesn't make a Search company. reply Abekkus 1 hour agorootparentIf they are actually nominally offering privacy over bing and yandex, while selling anonymous ads on that sticker, that is a valid USP. reply elevatedastalt 30 minutes agorootparentIt's a valid USP and people are happy to pay them for it, but I don't think they are relevant in a discussion about the difficulty of building a search product. For example, if you are discussing manufacturing issues in China, talking about how good dropshippers on Amazon are is irrelevant reply ergonaught 2 hours agorootparentprevGoogle search results are garbage and are still superior to DDG. I start every search in DDG and have to take it to Google fully 90% of the time. reply voisin 1 hour agorootparentI am just shocked at how different our experiences are. I wonder what leads to the vast gulf? Could your results be better due to what you are searching or is the algorithm producing different results for each of us, based on other Google cues? reply mock-possum 46 minutes agorootparentprevHere are some examples of DDG utterly failing as a search engine compared to google: https://www.tumblr.com/ddgvsggl reply AlbertCory 1 hour agoparentprevI use DDG most of the time. Sometimes I try Google, and then I'm disgusted by all the non-result garbage they put at the top nowadays. Once in a while they do have something valuable that DDG misses. So yeah, I find DDG perfectly adequate. reply enraged_camel 2 hours agoparentprevThe alternatives are bad because the ones that were even remotely good were bought and killed by Google over the years. This includes ones we all have heard of as well as countless others. This is a common tactic of tech giants, and the direct consequence of it is… what we have today. reply aednichols 1 hour agorootparentGoogle has acquired a lot of ad competitors, but I can't remember any general-purpose search engines. It seemed more like they died out naturally over a long period. I welcome new information if that's not right. reply xdennis 2 hours agoparentprev> it still baffles me how bad the alternatives still are Hard disagree. Google almost delights in not showing what you want. For political stuff it's very biased. If you search for something more obscure, Google really likes to \"correct\" your search and eliminate terms. Although Google is better in my maternal language and for images (but AI is ruining image search for everyone). reply tannhaeuser 1 hour agoparentprevHN isn't that special a place considering nerds think about Google as a \"search engine\" lol. Google/Alphabet, first and foremost, is the largest online advertiser via its acquisitions of YouTube, DoubleClick, and others, in addition to selling ad placement on Google Search via AdWords, plus a growing number of consumer portals for price comparisons etc. integrated with Google Search (leaving out tracking your activity on Android devices, Google's cloud business, and Books/scholar). The immediate antitrust perspective starts by looking at Alphabet/Google subsidiaries both providing search results and ads on the pages listed in search results (and to a lesser degree even by pushing Google services via Google Search). This is what had ruined the web in more than one way. reply mvdtnz 1 hour agorootparentgoogle.com is a search engine, regardless of what drives Google's revenue. Weird to say it's not. reply patmorgan23 43 minutes agorootparentIt's an advertising billboard that if you scroll past might have some relevant links at the bottom of the page. reply eagerpace 58 minutes agoprevAnywhere there are eyeballs, there will be ads. And wherever there are ads there are privacy concerns. Netflix waited until they had sufficiently killed off cable TV, then went back to creating the same problem it fixed. No ads in ChatGPT today, but soon as it (or a competitor) gains meaningful marketshare, there will be ads. reply ryandrake 7 minutes agoparent> And wherever there are ads there are privacy concerns. Not necessarily true. Physical billboards are ads but (mostly) without privacy concerns, until they start putting cameras on them watching who walks by and looks. reply tensor 37 minutes agoparentprevWhat I'd love to see is regulation forcing companies to provide reasonably priced ad-free options. There should be a right to opt out of ads. reply yeevs 16 minutes agorootparentI'd argue almost every platform with ads do have this option. You just may disagree about what's reasonably priced. reply strongpigeon 1 hour agoprevSomething I really don't get is the part about Google's monopoly in search text ads. FTA: > Finally, the filing said Google’s dominance over search text ads needed to be addressed by lowering barriers to would-be rivals or licensing its ad feed to others, independently from search results. What Google has is a monopoly on search (which is bad), but I don't think having a monopoly for advertising on your own property is a bad thing. If anything, from a privacy perspective, I'd rather that only one party (the publisher, in this case Google) gets to see my searches, rather than the publisher and an ecosystem of low-scrupules ad platforms. For sure I might be biased as I used to work on Google Ads, but I also know quite a bit about how the sausage is made and how the industry is. That being said, I really don't see how \"licensing the ad feed\" would do any good for end users. reply lcnPylGDnU4H9OF 18 minutes agoparent> What Google has is a monopoly on search (which is bad), but I don't think having a monopoly for advertising on your own property is a bad thing. Where one might run afoul is using your monopoly in one market -- search, in Google's case -- to gain an advantage in another market -- advertising. It's kinda interesting because they clearly don't treat \"search\" as a market given they don't sell it to anyone but it also clearly has value, otherwise people wouldn't use it. Now I wonder if it is without precedent that the supposedly-monopolistic thing they're using as a carrot for their advantage in a different market doesn't actually generate revenue itself; not sure if that context has ever been tried in court. reply soperj 1 hour agoparentprev>low scrupules ad platforms In this case, the search provider is the low scruples ad platform. Bing as well. reply dekhn 1 hour agoprevMy primary interest is how Alphabet will attempt to implement this. In particular, if they break up Search and Ads, or really any major product in google3/borg, Alphabet will have a massive pile of work splitting those up in a way that allows the resulting groups to achieve the same level of horizontal and vertical integration that they currently enjoy. Personally I am skeptical that they have enough technically capable and charismatic leaders to pull this off. reply anon291 24 minutes agoprevI wish that instead of wasting time on antitrust cases, that the government would just directly fund competition by incentivizing it. I.e, the first company to reach x% browser market share will receive $x million (billion?) in prize money or, the first company to build an LLM-based search engine with x% market share will receive $n million Or structure as a direct investment or heavy tax breaks. Always better to incentivize competition versus punish success. Or even better, if government wants to break up \"ACME corp\" then just tell the employees that, if you leave \"ACME corp\" and start your own company competing with ACME corp, we will waive taxes on all income for your first five profitable years. reply mmastrac 43 minutes agoprevFacebook, Google, Apple, Amazon -- the US could easily break all of these up. There's a huge risk to stagnation and over-optimization once these companies get as big as they do. reply mcguire 4 minutes agoprevA fun thing to do with google's search engine: look for \"Scheme tutorial\". For me, the links are: 1. Arun Muthu's Programming in Scheme (https://medium.com/atomic-variables/programming-in-scheme-th...) Part 1 of a 4-year-old series of blog posts (on Medium) that has no other parts. 2. Yet Another Scheme Tutorial (http://www.shido.info/lisp/idx_scm_e.html) A decent looking Scheme tutorial, undated. 3. A Scheme Primer (https://spritely.institute/static/papers/scheme-primer.html) Another decent looking tutorial, 2022. 4. Scheme Tutorial (https://www.cs.rpi.edu/academics/courses/fall00/ai/scheme/re...) A copy of a 1997 tutorial for a 2000 class at RPI; has broken links pointing to http://cs.wwc.edu/~cs_dept/KU/PR/. 5. Kent Dybvig's The Scheme Programming Language 4th ed. (https://www.scheme.com/tspl4/) 6. Reddit \"Best beginner friendly \"write a scheme\" tutorial?\" (https://www.reddit.com/r/scheme/comments/klt0af/best_beginne...) 7. A link to the introduction of a copy of Paul Wilson's un-attributed An Introduction to Scheme and its Implementation (https://www.cs.rpi.edu/academics/courses/fall00/ai/scheme/re...) from RPI in 2000. Google is an advertising company with a big IT department, not a technology company. reply teleforce 14 hours agoprevThere's side effect benefit of big kahuna companies mainly on the significant breakthrough and game changing research output because these excellent researchers are paid handsome money compared to conventional universities or research institutions. We saw this with AT&T Bell research labs with their inventions of transistor and Unix, among others. The same thing happened with Google research with (arguably) deep learning and transformer. Split them up at your own (US) perils, not unlike killing own Golden Goose. reply optymizer 3 hours agoparentDrawing from own experience working with Harvard, MIT and Google researchers, I could not disagree more. When you talk to a researcher, do they strike you as someone who chases handsome amounts of money, or someone who chases ideas? You bring up research labs. I listened to Alan Kay's numerous talks over the years (as an example of a prominent CS researcher), not once does he mention that he joined for the money at Xerox PARC. Yes, he was paid, but the main advantage was being given free reign to conduct research with the best experts in their fields, i.e. to invent and pursue ideas. The important part from a financial perspective, is to be able to have finances to back a research division, where you can spend billions on building a new type of technology, if need be, that may not pan out. You don't need a monopoly to accomplish that. You know who does chase handsome amounts of money? Day traders and everyone gambling on the stock market. reply nickff 2 hours agorootparentBecause you specifically mention Alan Kay, I just finished reading “Dealers of Lightning”, which is about PARC, and says that the researchers there were very handsomely paid. IIRC, they were paid 20% more than their counterparts in ‘regular’ Xerox R&D. Xerox was also a big company, making a lot of money when it started PARC; arguably a monopoly (depending on how you define the term). reply logicchains 2 hours agorootparentprev> is to be able to have finances to back a research division, where you can spend billions on building a new type of technology, if need be, that may not pan out. You don't need a monopoly to accomplish that A company in an industry with very tight margins has much less money to invest in fundamental research. All the recent growth in generative AI has been driven by companies with very high margins; Google, Facebook, Amazon. If all those FANG were in tightly competitive markets and hence had low margins, they wouldn't have had billions of dollars to spend on the GPU compute necessary to develop modern language models. Which is evidenced by the fact that no companies in more competitive sectors have produced any large language models. reply JumpCrisscross 2 hours agorootparent> company in an industry with very tight margins has much less money to invest in fundamental research OpenAI has raised almost $18bn to date [1]. That puts it in the top 10 corporate R&D spenders globally, ahead of Intel and the entirety of big pharma [2]. (And OpenAI's gross margins for its API business are estimated around 40% [3]. Standard fare for tech. If anything, OpenAI subsidising its business with Apple and ChatGPT is behaving more like a tech giant than a start-up.) The top of that list are the big 5 American tech companies, spending about $200bn annually on R&D. By coincidence, that's roughly the pace of U.S. VC spend [4]. The depth of American private capital markets make a solid case against favouring housing these long-shot bets inside tech giants. (Particularly absent non-compete and IP reform.) [1] https://techcrunch.com/2024/10/02/openai-raises-6-6b-and-is-... [2] https://en.wikipedia.org/wiki/List_of_companies_by_research_... [3] https://www.theinformation.com/articles/a-peek-behind-openai... [4] https://www.reuters.com/business/finance/ai-deals-lift-us-ve... reply DinoDad13 3 hours agoparentprevI love it when the top comments on a hacker news thread is justifying monopolies. Fuck economics right? reply cosmic_quanta 3 hours agorootparentDo you think it's impossible to have a nuanced discussion about monopolies? Their net effect may be wholly negative while having some interesting aspects reply brendoelfrendo 2 hours agorootparentNot impossible, but mostly impossible. You can discuss the interesting aspects of large corporations, but you can't really discuss them in a vacuum. The top level post about \"big kahuna\" companies comes across as an unambiguous defense of monopolies, not an attempt at nuanced conversation. reply nickff 2 hours agorootparentNot a participant in your back-and-forth, but the top level comment seems to be much more nuanced than your posts. Perhaps you could: >”Please respond to the strongest plausible interpretation of what someone says, not a weaker one that's easier to criticize. Assume good faith.” reply DinoDad13 23 minutes agorootparent> \"Split them up at your own (US) perils, not unlike killing own Golden Goose.\" reply belter 2 hours agorootparentprevYou should know 80% of Hacker News works for either Google or Facebook. reply benopal64 3 hours agorootparentprevHonestly, it's disgusting. Have these people even read the white papers that Google releases? They are mostly marketing pieces. When systems and technologies are not publicly reproducible, why should scientists and (most) engineers care? I will not take Google at its word and would not recommend it to others. reply JumpCrisscross 3 hours agoparentprev> side effect benefit of big kahuna companies mainly on the significant breakthrough and game changing research output “Given that production could be carried on without any organization, Coase asks, 'Why and under what conditions should we expect firms to emerge?' Since modern firms can only emerge when an entrepreneur of some sort begins to hire people, Coase's analysis proceeds by considering the conditions under which it makes sense for an entrepreneur to seek hired help instead of contracting out for some particular task. The traditional economic theory of the time suggested that, because the market is ‘efficient’ (that is, those who are best at providing each good or service most cheaply are already doing so), it should always be cheaper to contract out than to hire. Coase noted, however, that there are a number of transaction costs to using the market; the cost of obtaining a good or service via the market is actually more than just the price of the good. Other costs, including search and information costs, bargaining costs, keeping trade secrets, and policing and enforcement costs, can all potentially add to the cost of procuring something via the market. This suggests that firms will arise when they can arrange to produce what they need internally, and somehow avoid these costs. There is a natural limit to what can be produced internally, however. Coase notices ‘decreasing returns to the entrepreneur function’, including increasing overhead costs and increasing propensity for an overwhelmed manager to make mistakes in resource allocation. This is a countervailing cost to the use of the firm. Coase argues that the size of a firm (as measured by how many contractual relations are ‘internal’ to the firm and how many ‘external’) is a result of finding an optimal balance between the competing tendencies of the costs outlined above. In general, making the firm larger will initially be advantageous, but the decreasing returns indicated above will eventually kick in, preventing the firm from growing indefinitely.” https://en.m.wikipedia.org/wiki/The_Nature_of_the_Firm reply tivert 2 hours agoparentprevI think you're getting it backwards. The research operations are a desperate attempt stave off regulation to keep the sweet, sweet monopoly profits coming in (and those profits are so big that the bean-counters allow it). I believe that was the explicit strategy at AT&T. We collectively pay way, way more. It'd be way more efficient and cost effective to just set up a well-funded government labs to do that research. reply dehrmann 2 hours agorootparentIt's an interesting argument for monopolies possibly being a net-good, but I don't think regulators really look at it. Companies do R&D because they like their monopoly status and don't want to be caught flat-footed by something new. reply zaphar 1 hour agorootparentprevI am not at all confident that a government lab will be either well-funded or efficient in any sense of the words. reply blackoil 1 hour agoparentprevThat is like wishing a benevolent dictator, who'll be in most cases more efficient then democracy but is a huge risk to take. reply a_wild_dandan 1 hour agoparentprevLina Khan's response to this argument. [1] [1] https://www.youtube.com/live/L_QaZk5iJOA?si=ZkxBe1CHgagmcBcW... reply which 2 hours agoparentprevThis! And where do people think open source funding, hackathons, bug bounties for software that's not even theirs, oss-fuzz, really incredible but not necessarily profitable research like Project Zero comes from? AdWords largesse. reply alexpotato 2 hours agoparentprevRory Sutherland has a great take on this which is (paraphrasing and my own interpretation): Innovation is a lot easier when you have a lot of money to spend on R&D. In order to get that money, you can't compete on price b/c that's a race to the bottom. Instead, you want to focus on quality and/or customer service so that you become a monopoly and then can use monopoly profits to innovate to higher quality products and services. Clip: https://www.tiktok.com/@rorysutherlandclips/video/7314765561... reply mistrial9 2 hours agorootparentclassical MBA says that a firm can compete on price OR branding, unfair advantages (moat) notwithstanding. Competition in commodities is difficult but not impossible given a rational economic environment. Some would say that the modern expectation of returns on investment are irrational, and warp the economics around them too. reply waveBidder 13 hours agoparentprevjust skip the middle man skimming off the top and 10x the national labs funding. reply qnleigh 12 hours agorootparentI wish this were a viable option, but it is not. US national labs are horribly, horribly mismanaged. For some slower-moving fields like particle physics where institutional knowledge is key, they hold up alright, but for fast moving fields like quantum they are very behind. They are stagnant bureaucracies. I could tell stories, but better to just compare the output of national labs in many fields to those of the top universities in the States. reply waveBidder 2 hours agorootparentThen separate out the basic science from the defense work which requires that bureaucratic oversight. Or direct that funding to universities. My main point is allowing monopolies because they direct their excess profits to research to hide their excess profits is just a complicated tax to fund basic research, which we would be better off spending directly on research without bloated executive salaries and distorted markets in e.g. search or browsers. reply onecommentman 10 hours agorootparentprevI think you need to better support the contention that the National Labs are “horribly, horribly mismanaged” [not even just horribly, but horribly, horribly]. I think many of us would like to hear your stories. But note that many in your audience here have decades of experience across both National Labs and leading industrial laboratories. Please, share your stories that span the contributions of tens of thousands of top-level STEM contributors across practically every area of scientific and engineering endeavor over the last, say, 15 years. Remember to stay unclassified… reply busterarm 2 hours agorootparenthttps://www.google.com/search?q=national+laboratory+mismanag.... it has been widely reported on. Search results are eye-opening. reply onecommentman 9 hours agoparentprevTrying to turn a zoo into a farm, as AT&T attempted to do with Bell Labs post-divestiture, had limited success and incurred great emotional and spiritual cost on the institution. Bleah. reply iml7 13 hours agoparentprevThe split of ATT killed Unix2, so we spent 30 years re-implementing Linux+k8s. These things that existed in Unix2 & Plan9 were re-implemented by Plan9 employees in Google Labs. reply lfmunoz4 12 hours agorootparentUNIX exists because ATT was split. They could not profit from software (by law because of an agreement with the government) so early versions of UNIX where made free. This should be well known, simple google search: https://www.linuxquestions.org/questions/linux-general-1/why... reply HillRat 1 hour agorootparentprevThe AT&T split had nothing to do with monopoly regulation (as opposed to the Bell breakup in 1982), other than the fact that Wall Street wasn't rewarding regulated operating companies with dot-com valuations. AT&T wanted to sell hardware to other telcos and dot-coms, so spun off Lucent, which had no idea what it wanted to do with P9/Inferno (which was a fantastic piece of kit!) other than embed it into a couple of network products. Lucent bet heavily on unstable CLECs like Worldcom, generated a couple of headline-creating network crashes, and then failed to capitalize on their pole position in optical long-haul (to be fair, they also bet heavily on a very unstable Global Crossing for that). There's a lot of mismanagement and failures that can be ascribed to Lucent leadership without government or regulatory intervention being involved. reply fsckboy 13 hours agorootparentprevi can't even understand what you are saying? AT&T was good, or bad? AT&T copyrights led to linux, and linux, independent of unix, has been a huge boon for good, and for unixness. the threat to unix now is all the people who by nature prefer Dave Cutlerness, and can't see that their way is the wrong way, now they are using linux (because it won) and trying to ruin it. reply pjmlp 12 hours agorootparentprevUNIX only became a success, because ATT initially wasn't allowed to charge real money for research work. reply flomo 12 hours agorootparentprevSeconding info about \"unix2\". I used to pour over the trade tabloids, and I've never heard of this. Novell bought UNIX and has some grand plans for \"SuperNOS\", which also never shipped. It certainly wasn't anything like K8s. reply Melatonic 13 hours agorootparentprevNever even heard of Unix2 - was it a complete replacement ? reply kibwen 12 hours agoparentprevWhat on Earth... you do realize that antitrust regulation was the only reason we got Unix in the first place, right? reply fsh 12 hours agoparentprevClearly this model no longer works. Bell labs had 11 nobel prize winners. What did Google invent? Slightly better generative neural networks whose offsprings now pollute their search results? reply novia 3 hours agorootparentTwo of Google's researchers are getting the Nobel prize for AlphaFold2 this year. reply fecal_henge 12 hours agorootparentprevGoogle invented the ability to put an animated Gif inside a spreadsheet cell. reply nick__m 2 hours agorootparentwith COM and OLE, this was possible in excel on win3.11 ! reply fragmede 8 hours agorootparentprevSpanner is one thing I'd say they invented but they built a whole bunch of really neat stuff in order to be able to run search, back in 1998. that they're this behemoth conglomerate that it's cool to hate on doesn't erase the fact that they had to build all sorts of new things when they were just starting out. reply GauntletWizard 12 hours agorootparentprevYou could interpret this the other way - Why has Jeff Dean been snubbed by the Nobel committee? Why hasn't Larry Page gotten a Nobel for inventing the search technology that half the planet now depends on? I don't know what category to put that one in, but there's some important results in lightspeed-limited communications in \"The Datacenter as a Computer\" that would be worth extending the Physics category for. reply apercu 2 hours agorootparent\"search technology\" Google is an advertising company, search is a by-product and has been for a long time. reply laborcontract 11 hours agorootparentprevindexing against Nobel prizes is like trying to use patents as a proxy for innovation reply brendoelfrendo 13 hours agoparentprevSo wait, markets don't work, then? A free market, theoretically, promotes innovation by ensuring that businesses must advance their products in order to compete with one another. You're saying that a lack of competition promotes innovation by concentrating all of an industry's capital under one roof. reply Kon5ole 12 hours agorootparentThe thing that makes markets work is the struggle. A Darwinian survival of the fittest in a way. Once the struggle is over and only one contestant remains, the results are generally dystopian. Also I believe that even when working optimally the Darwinian mechanism can't solve certain problems. Some things need to be dealt with by a group of motivated people working for other goals than profit. Markets gave us compuserve and facebook while CERN gave us the open web, for example. reply farts_mckensy 12 hours agorootparentprevBoth can be true in varying degrees at certain points in time. They're not mutually exclusive. There are benefits to centralization and concentration of capital. Competition is the same exact process that leads to monopolistic entities in the first place. reply eastbound 13 hours agorootparentprevRegalian roles are to ensure fair competition by reducing any actor bigger than the state to something smaller, and ensuring the economy works with transparent information (no lying, rule of law, etc.) Companies getting too big are natural; Letting them get too big is what happens when your state borrows a trillion per semester: Your state is obese, intervening in every little sector of the economy (thus the opposite of liberal), and not playing its regalian role. You should indeed reduce the size of both the state and the largest companies, to let the economy self-regulate, but then, how would the US govern the rest of the world? reply LightHugger 14 hours agoparentprevThis is a theoretical benefit which is directly at odds with the benefits of competition in a healthy market. For google, my observation is the \"big kahuna\" benefit of google basically does not exist and competition needs to be restored. Google is famous for not innovating on anything successfully, they produce graveyards of trash. Instead what they do is buy other companies then enshittify them in an anti competitive dance towards causing more damage than productivity. You really have to think about exactly how our modern markets work and why buyouts are such dominant strategy. It's only sometimes about taking what you buy then using it, it's mostly about taking what you buy to stifle competition these days. Look at twitter and Vine, twitter bought then shut down vine as part of a standard operating procedure just to stifle competition, and they had so little interest in capitalizing on what they bought that it left a market gap so wide TikTok filled it instead. But usually these practices do not leave such big market gaps, usually they simply shut down competition successfully and the buyer wins. Then in many cases if the company owners refuse to be bought out, extreme anti-competitive practices begin to destroy their business, which will not be punished until long after the victims get shut down. So owners need to choose between a huge pay out, or their company getting destroyed. Owners tend to choose the former. reply jonas21 28 minutes agorootparent> Google is famous for not innovating on anything successfully, they produce graveyards of trash. - AlphaFold (just won a Nobel prize) - Transformers (the \"T\" in GPT) - Waymo (autonomous vehicles) - Sycamore (quantum computing) These are just a few off the top of my head. If your idea of \"innovation\" is a better RSS reader, then sure, I agree with you. But in terms of things that push the forefront of technology, I have a hard time thinking of another company with greater impact in recent years. reply teleforce 13 hours agorootparentprev> \"big kahuna\" benefit of google basically does not exist I just given you the deep learning and transformer benefits. There's a reason why the darling of AI Renaissance namely transformer was not invented at MIT, Stanford or Berkeley. reply akoboldfrying 13 hours agorootparentprev>Google is famous for not innovating on anything successfully PageRank Gmail Maps MapReduce Chrome Protocol Buffers Go reply treyd 2 hours agorootparentGo you can hardly call an innovation. All of the ideas existed previously, and it's a poor execution on those ideas for reasons that have been discussed on HN at length before. They created it to serve their own needs in conditioning the labor market to make their hiring process easier. reply kajecounterhack 2 minutes agorootparentGosh this is pretty unfair to Rob Pike and Ken Thompson. A lot of infrastructure companies have benefitted from Go the same way Google has, for the same reasons. Typical HN comment writing off significant thoughtfulness as \"not an innovation\" lol kolinko 12 hours agorootparentprevGmail was revolutionary at the start, but stopped innovating 10 years ago - why don’t we still have a good search engine within it? MapReduce would be invented anyway (I implemented it from scratch before learning of it’s existence). Chrome is just a slightly upgraded Firefox (and novadays Safari is just as good if not better with ai) PageRank was what gave Google monopoly, it’s not a result of monopoly. Go - I can give you that. ProtoBuf - not my field, but isn’t it just a format that someone else would develop to fill a niche? (unlike say mp3 that had new compression algorithms baked in) Maps - I can give you that. Some people might argue that it was an acquisition, but without Google’s muscle, Street View would not be feasible. reply nine_k 10 hours agorootparent> Chrome is just a slightly upgraded Firefox Wat. It's like saying that an apple is a slightly upgraded orange. I would understand if you mentioned KHTML and Safari as relatives, but \"slightly upgraded\" does not fit anyway. > PageRank was what gave Google monopoly I don't think so. PageRank has been successfully implemented elsewhere, and outmatched. What helped Google build a monopoly was the first mover advantage, the network effects, and the incessant streams of money from AdWords (invented by Google), DoubleClick (acquired) and a bunch of other advertisement tools. > Maps - I can give you that. Don't :) Google Maps is an acquisition from 20 years ago. (As is Android, AdSense, and many other core flagship products of the Google brand.) If you want a relatively recent, successful Google service for general public, it's Google Photos. reply whitehexagon 58 minutes agorootparent>If you want a relatively recent, successful Google service for general public, it's Google Photos. I seem to recall that followed the acquisition of Picasa. reply nine_k 30 minutes agorootparentPicasa was rather different: it had a desktop client, had tags, did not have a dedicated view mode, etc. It ran as a separate product, and then was shut down, not integrated. https://en.m.wikipedia.org/wiki/Picasa reply teractiveodular 13 hours agorootparentprevMaps was technically an acquisition (Where2). But like YouTube, Doubleclick, Google Docs (Writely), Translate (Word Lens), Google Flights (ITA) and many others, Google successfully grew these products into giants. reply akoboldfrying 9 hours agorootparentI didn't know Maps (or Google Docs, or Translate) were acquisitions, thanks. reply porridgeraisin 11 hours agorootparentprevComparing the innovations of Bell Labs with..... _Protobuf_ of all things makes me gag. reply akoboldfrying 9 hours agorootparentGag if you want to gag, but I'm not comparing anything with Bell Labs. I'm giving evidence that the claim I quoted is false. reply dehrmann 12 hours agorootparentprevDidn't some of the early GPT work come out of Google? reply blackeyeblitzar 12 hours agorootparentThe popular transformer paper, which went on to be used in things like ChatGPT, was authored by Google employees. But “come out of Google” is giving the organization too much credit and the individual too little. Also transformers were themselves a continuation of prior work like multi head attention. And it is possible that transformers were not needed - see this discussion from the other day: https://news.ycombinator.com/item?id=41732853 reply tgma 1 hour agorootparentCome on... that's so unfair. There is a reason such individuals chose to work at Google and not Apple or Amazon for example and were able to \"individually\" come up with such work without being pestered by their management to do other stuff. reply heinrich5991 2 hours agoprevhttps://archive.is/DPtgt reply asdff 1 hour agoprevIf they did this 15 year ago, I bet google reader would still be actively maintained today. More innovation is possible when there are smaller companies working on fewer things. Yes there are economies of scale with large entities, but you then run up against the economy of shareholder attention if your little effort within the larger organization is not the current golden child of the day. When you work at a smaller company, that little effort is the entire company's product. It has to work and get better to succeed. It can't be forgotten about and left to languish like so many potentially great products and technologies that are chained to some large org today. reply aiauthoritydev 1 hour agoparentGoogle engineer's time was not worth Google reader. If you think it was a useful product you could have created it yourself for profit. reply asdff 1 hour agorootparentHighly useful software is not always profitable. Limiting our species to developing technologies that are profitable rather than merely useful is a severe blow to our path of innovation. reply KerryJones 54 minutes agorootparentWhy wouldn't people pay for something highly useful? It seems the opposite, that people would pay for things that are highly useful. People don't pay for things that are marginally useful. reply hightrix 0 minutes agorootparent> Why wouldn't people pay for something highly useful? Typically because they can easily get it for free. Perfect case study is WinRar. How many people actually bought a license for software they used daily? exabrial 28 minutes agoprevGood. Then Apple, Then Amazon, Then Facebook. All of them are squeezing innovation in this country. reply Maledictus 13 hours agoprevhttps://archive.ph/DPtgt reply mensetmanusman 1 hour agoprevDeepMind wouldn’t have happened to the level it did had google been broken up prior. reply giobox 1 hour agoparentThere was no shortage of other deep-pocketed suitors trying to buy DeepMind too during the Google acquisition back in 2014 - I think DeepMind would have likely found a reasonable home regardless. reply asdff 1 hour agoparentprevWhat might deep mind have looked like today if it were given the same resources in an academic or national lab setting and no ultimate profit motive? reply mensetmanusman 1 hour agorootparentSans access to the worlds top supercomputer infra. reply asdff 1 hour agorootparentUsed to be the worlds top supercomputer infra was in national labs or such places and not private companies. Some of the largest are still there. In my hypothetical that would be the case \"assuming same resources.\" reply molticrystal 12 hours agoprevA lot of Google's features when integrated and leading to ads and other Google properties they are justified, but if you were to forbid cooperation between the divisions, they may be shut down or diminished, as on their own they would need to be subsidized considerably and the 3rd party alternatives for ads would take most of the profits probably leading to negative cash flow without decreasing the level of service or charging fees. reply apercu 2 hours agoparentIf they are shut down but valuable, some other company will produce similar products. That's how the market should work. reply ilaksh 10 hours agoprevI think decentralized technologies should be part of the discussion when you it comes to replacing technology monopolies. For example, there might be some protocols for cooperative web indexing or search or to provide a common layer that companies can build on. reply cogman10 3 hours agoparentI'd love to see how something like this could handle the bad actor problem. It is what (IMO) is currently killing the web today. How would you, for example, stop a rouge indexer from spewing an unlimited number of bad indexes to spam their garbage into the distributed protocol? Or how would you address just bad/misleading/faulty indexes? reply idle_zealot 2 hours agorootparentWeb of trust, I guess? Don't accept just anyone doing scraping/indexing. Keep the trust network human-scale. I can imagine a world where the relevant protocols are open and organizations choose their own roots of trust. Common defaults would likely emerge, things like the Wikimedia foundation or Archive would serve as default roots for your average user, but you could add your own or remove those if you knew what you were doing. reply throwaway48476 10 hours agoparentprevCommon crawl? reply EcommerceFlow 3 hours agoprevIf Google isn't allowed to use search data to train LLMs, would the same apply to X and the tweets found there? reply JumpCrisscross 3 hours agoparent> If Google isn't allowed to use search data to train LLMs, would the same apply to X and the tweets found there? If a federal judge finds X/Twitter to have a monopoly on short-form nonsense, yes. reply gerash 1 hour agoprevWhat an incompetent government where the goal is to pad some DoJ lawyer's resume instead of benefiting consumers or the economy reply Clubber 1 hour agoparentThe US government has been working solely for their donors for the last 40+ years. Any benefit the voters get from lawmaking is coincidental. reply esbranson 39 minutes agorootparentThat thought may be comforting, but no. Don't mistake the limited-purpose US Government for your state government. Colorado does quite a bit, from healthcare to environment to policing. And so does the US, for those subjects it has jurisdiction, and has been doing so for hundreds of years. reply IncreasePosts 24 minutes agoprevIs there any chance this could occur in the remaining term of Biden? If not, is Harris going to continue on this path? I assume so, since they are both part of the same machine. reply busterarm 2 hours agoprevThis is all smoke and mirrors. DoJ does not have any intention of breaking up their friendly neighborhood Google. What they're doing is seeing the writing on the wall of the upcoming election and seeing all of their jobs on the line and they're trying to shake down Google for golden parachutes. I guarantee you in the next year, several high-level DoJ officials will secure senior positions at Google in order to defend against upcoming antitrust litigation. Those that don't will try to use their active litigation as an anchor to try and retain their jobs. reply dtquad 12 hours agoprevHilariously shortsighted. Big Tech companies have been a GDP-doubling runaway success for the US economy. It would be like if we here in Denmark started breaking up Novo Nordisk. Our economists would probably do a public lynching of any government official who suggested doing that. However as a European I can't help but welcoming the US shooting themselves in the foot like this. Something tells me we will see more of this as more reddit-brained American millennials get political influence. reply idle_zealot 2 hours agoparentThere is no point in chasing a high GDP when it results in a materially worse world. The point of a society isn't to make the numbers go up. If a monopoly is super efficient at generating some nebulous concept of value by creating and operating the world's largest surveillance system and actively using it to sell influence over people's attention and habits then the only sensible thing to do is to dismantle it. reply richwater 2 hours agorootparent> results in a materially worse world. Please explain how Google has created a materially worse world reply ksenzee 1 hour agorootparentThe choices are not “Google as it currently exists” vs “a world without Google,” but rather “Google as it currently exists” and “Google as subject to stricter regulation.” There’s a fair case to be made that the world is worse than it would have been if the US had kept a tighter rein on Google. reply ssklash 1 hour agorootparentprevWhat part of being a massive ad company whose raison d'etre is to collect as much personal information about you as possible (with limited or no consent) to enable other people to try to convince you to buy stuff even remotely a net positive for the world? reply idle_zealot 1 hour agorootparentprevUh, I did. That's what the whole \"surveillance system\" part of the post was. Also I think advertisements in general are harmful. reply dvngnt_ 1 hour agorootparenthow do people learn of products? reply idle_zealot 1 hour agorootparentAt a time of their choosing they can subject themselves to marketing material (yellow pages) or simple word-of-mouth amplified by the Internet. Treating \"knowledge of your product/service\" as a market commodity is bizarre and has overall negative effects on competition (more money buys more awareness equals more sales). reply ssklash 1 hour agorootparentprevIf ads are the only way people are able to learn about products, then there is clearly a massive failure of imagination, as well as innovation. People know what they need, and have always known. The concept of exploiting human psychology in order to sell more of a product to people who likely don't need it is a relatively recent development in human history. Plus you can just search for stuff you need in a search engine... reply lovethevoid 35 minutes agorootparentprevThe same way you learned about hacker news. reply alexashka 1 hour agorootparentprevIt's a paperclip maximizer. Paperclip maximizers are bad. reply kibwen 12 hours agoparentprevHilariously shortsighted. Breaking up Standard Oil created wildly competitive industries and launched Rockefeller's wealth into the stratosphere. Big Tech is a rent-seeking middleman that chokes the life out of innovation. reply mandibles 1 hour agorootparentStandard Oil had already lost a huge portion of its market share when the forced breakup finally happened. reply openrisk 11 hours agoparentprevHilariously confused. Tech is different from Big Tech, which is yet further different from Big Ad Tech. For the avoidance of doubt, Wintel was Big Tech. The status quo now is Big Ad Tech. The main economically positive thing for the US (and something Europeans absolutely screwed up in relative terms versus the US and increasingly China) is the early investment and adoption of Tech. Digitization as such is a great enabler. But you don't need Big Tech oligopolies for a vibrant digital economy. But even more importantly, you don't need bizarre Big Ad Tech commingled business models that build the economy's entire tech infrastructure - many parts of it having a critical utility like role - on the back of... ads. But there is little scope for European schadenfreude. Arguably the US antitrust gears are moving precisely because people slowly wake up to the limits on economic opportunity placed by the Big Ad Tech status quo. In Europe we are good at words and criticizing mistakes but deeds are scarce. reply bpodgursky 2 hours agorootparent> Hilariously confused. Tech is different from Big Tech, which is yet further different from Big Ad Tech. Big Ad Tech has been a money spigot for R&D in both hard and soft tech. This comes via M&A but also spawning a generation of VCs willing to fritter away adtech money on fun hard tech startups. There is not a big source of VC funding for hardware startups that doesn't come directly or indirectly from Big Tech / Big Ad Tech revenue and valuations. reply CuriouslyC 1 hour agoparentprevWhat you say would have been true 10+ years ago, but not anymore. Big tech is really into rent seeking and competition stifling now, we'd see more innovation by opening up the space so new players can get a foothold. reply MathiasPius 12 hours agoparentprevIt is precisely the large impact on GDP that poses a threat to the host nation. When companies like Novo Nordisk are such a huge part of the economy, they can exert disproportionate influence on society itself. Our economy is absolutely benefiting from Novo Nordisk's size right now, but if/when their demand weakens or they're out-competed, we're going to end up with a lot of unemployed biotechnicians and massive roads to Kalundborg which will need to be maintained. reply exabrial 24 minutes agoparentprevJust curious, who is your employer? reply throwaway48476 10 hours agoparentprevDenmark has separate gdp numbers that don't include novo nordisk because they care about the real economy, not just number go up. reply zer8k 1 hour agoparentprevThe point is to not allow a company to be able to buy out the entire country. Maybe Novo Nordisk is a good steward but historically companies like Google and Amazon act only their own best interests. Breaking up these massive companies ends up doing the better thing anyway - a surge of competition emerges and offers better and often cheaper services. Look at the break up of Bell. The short term was marginally negative (higher long distance costs) but the market was better for it. Google has a substantial amount of control over the flow of information in the United States. To the point it can literally redefine truth. This is a problem - and one that is easily solved by breaking up a de-facto monopoly. Moreover, the acquire-and-kill strategy stifles innovation. Imagine what we would have if Google didnt have the capital to buy and kill so many small companies. reply gnabgib 15 hours agoprevRelated DOJ may want to break up Google (84 points, 2 months ago, 98 comments) https://news.ycombinator.com/item?id=41240716 reply 1penny42cents 1 hour agoprevcrazy double whammy: 1. US gov trying to break up your search monopoly 2. ChatGPT disrupting your search monopoly reply ripped_britches 12 hours agoprevIt will be really ironic if this kills Mozilla / Firefox reply azemetre 2 hours agoparentMozilla killed Firefox itself with its poor leadership. They have had nearly 20 years of Google writing them half a billion checks annually. If they can't come up with a better business plan than literal corporate welfare, maybe they don't deserve to exist? reply asdff 1 hour agoparentprevI don't understand where all the money goes for mozilla. Here is the revenue line from wikipedia for 2022: total revenuepercent from googletotal expenses |software dev expenses $593 million81% ($480 million)$425 million$220 million so basically 168 million in the bank in 2022. the math has been basically this the last 10 years. in 2018 they lost 1 million but in 2019 they gained 330 million. a lot of their software expenses probably comes from the busywork features they saddled upon themselves like pocket or whatever, since it was only $63 million in 2010 and has only gone up by a couple hundred million from there. So just over the last 10 years from my back of the envelope map from that table on wikipedia, they should have a good 1.5 billion in the war chest by today assuming the mozilla foundation did not make investments with it, which they probably have this entire time so probably even more valuable. At a certain point, maybe already, the org should have enough cash socked away in investment to just pay for operating expenses out of dividends alone. reply shiroiushi 12 hours agoparentprevThat's one of several scary scenarios. What if it kills Android, and everyone has to buy an iPhone? (Yeah, I know, Android is OSS and the phone makers could just maintain/improve it as a consortium without Google, but looking at how these companies operate I don't think they're capable of doing this.) (And no, I don't think the USG will break up Apple if this happens. They're already showing highly preferential treatment to Apple compared to Google.) What if it kills YouTube, and the only viable alternative is TikTok? I recommend everyone start downloading all their favorite YouTube videos with yt-dlp right away, just in case. What if it kills Google Maps? Again, there's no real viable alternative here unless you have an iPhone. I can see a lot of ways things could go horribly wrong here if you're someone who doesn't want to be an Apple user. reply busterarm 2 hours agorootparentSamsung could easily maintain Android as they already have their own little Android software ecosystem that differs greatly from Google's. Full of spyware, but yeah. There's tons of video hosting options but what makes YouTube special is access to a large audience and monetization. TikTok's monetization is garbage and not even a contender really. Large content creators are already negotiating their own brand deals to the point where YouTube's ad money is merely the cherry on top. I actually think breaking up YouTube would be good for audiences and in the long run creators themselves. Content creator networks would make a return in a big way. There is already OpenStreetMaps. MapQuest existed before Google Maps and still does. reply labcomputer 1 hour agorootparentYea, but OSM has variable quality by country, and isn’t really a “navigable” map in most places. You can’t get turn-by-turn directions because they don’t (consistently) have things like lane permeability, turn restrictions, directionality, etc. You can’t get accurate ETAs because they don’t have speed limits or free flow speeds. And traffic data of course. Unless things have changed, routing class and surface type are also unreliable, so a shortest-path graph algo will take you down neighborhood streets or unmaintained roads. There is a ton of under-the-hood map data, invisible to the end user, that you need to have to be able to deliver a modern phone navigation experience. reply busterarm 34 minutes agorootparentGarmin (and other) GPS devices still exist. They're quite nice these days even. My phone tends to overheat when I stick it under the window to use for directions, so I tend to prefer the dedicated GPS units anyway. reply labcomputer 1 hour agorootparentprevGoogle Maps isn’t going anywhere. They are a profit center from all the search ads within the map. reply dehrmann 11 hours agorootparentprev> And no, I don't think the USG will break up Apple if this happens. They're already showing highly preferential treatment to Apple compared to Google. The largest business by far is iPhones. It has 16% market share in the PC business, behind Lenovo, HP, and Dell. The only business that makes sense to peel off is the iPhone services (Apple Music, News, etc.) because that's the place it uses its dominant position to help its own products. reply pseufaux 9 hours agoprevUnabashedly one sided, but still a pretty good resource about this case. https://www.usvgoogleads.com reply gavin_gee 33 minutes agoprevclearly, google hasn't paid enough campaign donations vis-a-vis Microsoft. This is just political corruption feigning as doing the right thing for the country. we are having the wrong debate and are being distracted by the sideshow. in many ways this is a test balloon for public support to go after big tech as a narrative around censorship over misinformation that will explode over the next few years reply JumpCrisscross 15 hours agoprev“The DoJ identified four areas that its remedies framework needed to address: search distribution and revenue sharing; generation and display of search results; advertising scale and monetisation; and gathering and use of data. … In addition to potential spin-offs, prosecutors said remedies could include banning the exclusive contracts at the heart of the case — in particular the $20bn that Google pays Apple each year to be its default search engine — as well as imposing ‘non-discrimination’ measures on Google products such as its Android operating system and Play app store. The DoJ is also considering requiring Google to share its vast trove of data gathered to improve search ranking models, indices and advertising algorithms, which prosecutors argue was accumulated unlawfully.” reply ein0p 2 hours agoprevIdk how you could split it up other than right down the middle, creating 2 of each unit. 2 search engines, 2 browsers, 2 ad exchanges, 2 clouds and so on. Anything else is not going to be viable. Though maybe that’s the real goal. reply kibwen 13 hours agoprevGoogle is such a rudderless mess that breaking it up may be the only way to salvage anything of societal value from this company. reply dehrmann 12 hours agoparentOne of the pendulums in business strategy is whether companies should be smaller so they can be more nimble and pursue their own destines or larger so the can be more protected from market demands and can capitalize on \"synergies\" with other business units. In practice, investors usually discount larger companies for efficiency reasons. You can see this with acquisition announcements where the acquirer usually goes down in price. The synergies often fail to pay off because there aren't actually many synergies between making microwaves and running a TV network, and the sprawling empire turns into mostly independent fiefdoms. reply pixxel 12 hours agoparentprev>societal value Genuine question: what societal value would be lost if Google was erased tomorrow (all technical reliance their services was magically replaced overnight with alternatives by pixies)? reply ruthmarx 12 hours agorootparentWhat would happen if you replaced Google with a perfect functional equivalent? Well, nothing. You don't happen to know where one could find these magical perfectly compatible and functional drop in replacements, do you? reply idle_zealot 2 hours agorootparentThe closest thing to a unique offering that they have is YouTube. What other services don't have perfectly reasonable replacements ready and waiting? reply bhelkey 1 hour agorootparentAndroid makes up ~70% of the global phone marketshare [1]. Google maps makes up 70% of the mapping marketshare [2]. Chrome makes up ~65% of the browser marketshare [3]. Those are three of the nine products Google has with over a billion users [4]. [1] https://backlinko.com/iphone-vs-android-statistics [2] https://www.thestreet.com/technology/big-tech-working-to-cha... [3] https://gs.statcounter.com/browser-market-share [4] https://01core.substack.com/p/google-has-9-products-with-ove... reply idle_zealot 1 hour agorootparentExcellent job demonstrating Google's broad market dominance, and that alternatives exist to replace them should antitrust action be taken. reply bhelkey 44 minutes agorootparentWhat drop in equivalent exists for Android? I have no desire to move to iOS. What drop in equivalent exists for Google maps? I have used OpenStreetMap for a personal project and have tried other proprietary options. If Google maps disappeared, life would go on but I would be worse off. What equivalent exists for Chrome? Even on desktop I prefer Chrome over Firefox. On mobile, Firefox falls far behind Chrome. reply idle_zealot 0 minutes agorootparentFortunately for you, Android and Chromium are FOSS and not going anywhere. For maps there is OSM with several available frontends, Bing maps, and Apple Maps has a web version. ruthmarx 1 hour agorootparentprev> What other services don't have perfectly reasonable replacements ready and waiting? How about which single reasonable replacement offers the same services with the same level of integration? Using Yahoo Mail and Amazon Cloud and Office Online and whatever other products isn't quite the same offering as what Google offers. reply dehrmann 12 hours agorootparentprevYou could make that argument for anything, though. reply matthewfelgate 8 hours agoprevI don't understand how Microsoft gets away with being a bigger monopoly for longer. reply busterarm 2 hours agoparentA 40 year relationship with every arm of every Government. reply fyrn_ 2 hours agoparentprevArguably a duopoly with Apple, while ads and web browsing are just google? Not sure just a guess. reply ktosobcy 11 hours agoprevAs I said in the past - Google and the likes (Meta) should have never been allowed to swallow other companies (DoubleClick, youtube and instagram/whatsapp respecively)... reply arthurcolle 15 hours agoprevbreak up google for fumbling transformers alone reply ImHereToVote 14 hours agoparentYeah that's what monopolies do. Make people use inferior products. The US used to break up monopolies all the time. This was followed with a wave of innovation. reply akoboldfrying 13 hours agorootparentI can't tell if you're serious. Not only did Google not \"force\" anyone to use the LLM tech that they largely developed, most people think they're silly for inventing it and then sitting on their hands until another company (OpenAI) ate their lunch. reply DinoDad13 3 hours agorootparentThe anti-trust case isn't about Google's LLM business. reply ClassyJacket 15 hours agoprevThey biggest web advertising company definitely shouldn't control the world's most popular browser. Just like we all knew they would, they're blocking ad blockers, and this problem will only get worse. Tell your friends to use Firefox, people. reply crazygringo 2 hours agoparent> they're blocking ad blockers I've tried uBlock Origin Lite on Chrome and it works... perfectly. I haven't noticed a single ad get through. And isn't it supposed to be a lot more performant? Before, I assumed Chrome really was trying to gradually stop ad-blocking. But now that I see it's had literally zero impact, at least on the sites I visit, I'm starting to wonder what all the fuss was about. Was manifest v3 really about performance and security all along, and not about eliminating ad blockers? Meanwhile, you can't install adblocking on iOS Safari as an extension at all. But I never hear anybody bringing that up. reply wilsonnb3 1 hour agorootparent> Meanwhile, you can't install adblocking on iOS Safari as an extension at all. But I never hear anybody bringing that up. There are a bunch of safari ad blockers in the app store that work the same way manifest v3 blockers work. reply apercu 2 hours agoparentprevIf the ads weren't invasive, covering the content, purposely distracting you and your data wasn't being collected and resold, we wouldn't need ad blockers. reply shiroiushi 13 hours agoparentprevI'm not so sure this is a problem. They're not completely blocking ad-blockers, just neutering them somewhat with MV3. You can still use uBOL (the \"Lite\" version of uBO) and get a lot of ads blocked on Chrome. Remember, Chrome is not installed by default on Windows PCs; Edge is. People are using Chrome because they want to. They could just as easily download Firefox and uBO, like more-savvy users do. Unfortunately, too many can't be bothered. Should they be saved from excessive and intrusive ads? Again, they can easily install uBOL on their Chrome instance, or they can download and install FF+uBO. Or use something else like Brave. >Tell your friends to use Firefox, people. Absolutely, yes. Just don't be too surprised when you visit them later and they're still using Chrome (or Edge) with no ad-blocker at all. You can lead a horse to water, but you can't make it drink. reply akoboldfrying 13 hours agorootparent>Chrome is not installed by default on Windows PCs; Edge is Whenever this is brought up, the silence is deafening. Edge is a good browser, and users are notoriously lazy; most won't read a dialog box before clicking it away. And yet... ~everyone on Windows still downloads Chrome. reply spongebobstoes 2 hours agorootparentdoes google.com still push Chrome? I think it does, and it's most people's first website reply Arainach 13 hours agoparentprevIt's fascinating how \"preventing web extensions from having full access to everything on every site you visit when there is a repeated history of extensions being bought by companies that turn them into spyware data miners\" gets turned into \"blocking ad blockers\". reply cogman10 3 hours agorootparentBecause there were other ways to handle the ad blocker situation. For example, allowing the users to grant access to an extension. The hard protocol ban is heavy handed. reply Arainach 25 minutes agorootparentUsers don't read dialogs. They just click yes so they can get to their shiny talking purple gorilla. This also doesn't address the threat model: a good extensions that users trust and give these rights to which is bought out and changed to do malicious things. reply richwater 2 hours agoparentprev> Tell your friends to use Firefox, people. Mozilla is pretty much entirely funded by Google reply bfrog 2 hours agoprevImagine an ad company locking up talent for decades. Zzzzzzzz reply onecommentman 9 hours agoprevI’ve never understood why Congress hasn’t mandated the Library of Congress to offer an Internet Archive/Google for content created in the US. Expand to all first-world country content if they wish. This is not a technology-limited problem at this point. Be nice to get fresh high-quality scans of analog content online…the early digital scans available commercially or otherwise are often unreadable. reply Madiyan 1 hour agoprev85907 54751 reply blackeyeblitzar 12 hours agoprevIt’s amazing to me that Google is still so rich given how lazy their culture is and how incompetent their product strategy has been. It just goes to show the power of their size and all it brings. Things like capital, monopolies (search), control over platforms (Chrome), network effects (YouTube and ad networks), and just plain old momentum. This break up is long overdue but we also need a drastic rethink of antitrust law and corporate taxes to shift the economy towards innovative smaller instead of concentrating it in a few megacorps that are as powerful as some governments. reply eastbound 13 hours agoprevThere has been many announcements of lawsuits based on antitrust in the last 3 weeks. We can assume the message is “If you reelect the current party, we’ll finish these lawsuits.” There are two perverse effects: - It positions the alternate party as the party that Google should sponsor, - The good choice after reelection will then be to delay the next step of those popular antitrust cases to 3 weeks before the end of the next mandate, to tell the electors that they should reelect. Which ironically puts the current party in the position of the one doing nothing on the popular antitrust case (a corollary to “a party’s platform depends on ensuring the problems it’s supposed to solve keep existing”). reply chipgap98 12 hours agoparentWeren’t some of these cases begun under the previous administration? Neither party is a fan of big tech reply dehrmann 12 hours agorootparentHarris has been thin on policy, so it's hard to say, but seeing that she's from the Bay Area, she might be more careful more careful to not break one of the country's key industries. reply warkdarrior 14 hours agoprev [3 more] [flagged] okdood64 13 hours agoparentWhat's Chat? reply warkdarrior 14 hours agoparentprev [–] Awkward: asking Google.com \"What is Google and why would DoJ want to break it up?\", it does not answer anything about what Google is. Half baked website reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "The US is contemplating a landmark antitrust case to potentially break up Google, targeting its dominance in search and advertising sectors.",
      "This case could establish a precedent for future regulatory actions against major tech companies, reflecting concerns about stifled competition and innovation.",
      "The debate underscores the tension between fostering market competition and preserving the benefits provided by large-scale tech companies like Google, which includes services such as Android and YouTube."
    ],
    "points": 164,
    "commentCount": 220,
    "retryCount": 0,
    "time": 1728445190
  },
  {
    "id": 41785511,
    "title": "I made an SSH tunnel manager to learn Go",
    "originLink": "https://github.com/alebeck/boring",
    "originBody": "The boring tunnel manager A simple & reliable command line SSH tunnel manager. Features Ultra lightweight and fast Local and remote tunnels Compatible with SSH config and ssh-agent Supports Unix sockets Automatic reconnection Human-friendly configuration via TOML Usage Usage: boring list,l List tunnels boring open,o[ ...] Open specified tunnel(s) boring close,c[ ...] Close specified tunnel(s) Configuration By default, boring reads its configuration from ~/.boring.toml. The configuration is a simple TOML file describing your tunnels: # simple tunnel [[tunnels]] name = \"dev\" local = \"9000\" remote = \"localhost:9000\" host = \"dev-server\" # automatically matches host against SSH config # example of an explicit host (no SSH config) [[tunnels]] name = \"prod\" local = \"5001\" remote = \"localhost:5001\" host = \"prod.example.com\" user = \"root\" identity = \"~/.ssh/id_prod\" # will try default ones if not set # ... more tunnels Currently, supported options are: name, local, remote, host, user, identity, port, and mode. host either describes a host which to match SSH configs to, or if no matches found, the actual hostname. mode can be 'local' for local or 'remote' for remote forwarding, default is 'local'. The location of the config file can be changed by setting the BORING_CONFIG environment variable. Installation Get one of the pre-built binaries from the releases page or build it yourself: git clone https://github.com/alebeck/boring && cd boring ./build.sh Currently only supports macOS and Linux.",
    "commentLink": "https://news.ycombinator.com/item?id=41785511",
    "commentBody": "I made an SSH tunnel manager to learn Go (github.com/alebeck)154 points by 0x12A 11 hours agohidepastfavorite54 comments perbu 5 hours agoWell done. The ease of use and high quality of the Go SSH libraries (golang.org/x/crypto/ssh) is a killer feature of Go, imho. Also, there is a high level abstraction, github.com/gliderlabs/ssh, which makes it completely trivial to embed an ssh server into an application, giving you a nice way to inspect counters and flip feature flags and tuneables. reply evanelias 4 hours agoparentThe only major downside to golang.org/x/crypto/ssh is that open issues seem to linger for years lately, even when people try to submit patches. So it's often necessary to look for third-party solutions. The knownhosts handling in particular has a bunch of common land-mines. I'm the maintainer of a wrapper package https://github.com/skeema/knownhosts/ which solves some of them, without having to re-implement the core knownhosts logic from x/crypto/ssh. Just to illustrate how common these land-mines are, my wrapper package is imported by 8000 other repos on GitHub, although most of these are indirect dependencies: https://github.com/skeema/knownhosts/network/dependents reply 0xbadcafebee 3 hours agorootparentI think in an ideal world, this would be the normal case. A hierarchy of packages, maintained by many independent parties, that extend useful base functionality, without too much logic being put in any one package. If one thing doesn't work well you can just create a new package to replace the one part. And building on top of simpler, smaller modules allows you to keep code DRY, reduce maintenance burden (like the 1000 open PRs...), and easily extend functionality by simply making a new package. That was my experience with CPAN, anyway. It's not perfect but it's miles above other language module cultures. reply evanelias 3 hours agorootparentThe base functionality isn't always terribly extensible, though. And Go isn't like Perl or Ruby where you can monkey-patch arbitrary logic in a pinch. I originally created my knownhosts wrapper to solve the problem of populating the list of host key algorithms based on the knownhosts content. Go's x/crypto/ssh provides no straightforward way to do this, as it keeps its host lookup logic largely internal, with no exported host lookup methods or interfaces. I had to find a slightly hacky and very counter-intuitive approach to get x/crypto/ssh to return that information without re-implementing it. And to be clear, re-implementing core logic in x/crypto/ssh is very undesirable because this is security-related code. reply 0xbadcafebee 20 minutes agorootparentSometimes the hierarchy can be used without directly/perfectly extending the code. For example, in the CPAN world, you might publish your own module as \"x/crypto/ssh/knownhosts/client\". You don't even have to use the \"x/crypto/ssh/knownhosts\" code at all, it just looks like a similar namespace. (IIRC, CPAN requires a human in the loop who's moderating what new packages are listed; none of the craziness of PyPI where any insane person can release thousands of typosquatting malware modules) You would hope a new module would reuse as much previous base modules as they can, but sometimes it's enough to just put some new code in that namespace, with the intent then that someone will find it easier, and build off of it. The hierarchy is for organization, discovery and distribution, as much as it is about good software development practice. The goal being to improve the overall software development ecosystem. reply dingnuts 2 hours agorootparentprevI do not mean this as a loaded question, but what happens in this model when maintainers die? Everything you've said sounds great, with the assumption that the maintainers can maintain their pieces indefinitely and independently. But we're mortal. And I know the independent maintainers in places like CPAN are humans, not companies. I guess it's a sign you're getting old when you start worrying about this kind of thing reply 0xbadcafebee 31 minutes agorootparentAssuming people want to keep using/maintaining the code, you just prove the original maintainer has either abandoned it or died, and then you contact the repository admins (i.e. CPAN). Make your case that the original maintainer is gone and they'll probably make you the new maintainer. If nobody wants to maintain the old code, or the design wasn't ideal, often times people will create a \"v2\" or \"-ng\" rewrite of it and try to keep backwards compatibility. Then the people who made sub-modules can simply publish their modules on top of the new base module. Old code continues running with the old dependencies until somebody links the old code to the new base module. reply oefrha 3 hours agorootparentprevAnother thing I want but is completely missing from golang.org/x/crypto/ssh is compression support: https://github.com/golang/go/issues/31369 reply creeble 3 hours agoparentprevHow is performance? We found the native Go SSL libraries (as used in, e.g. the http package natively) to add many ms to web api calls. We eventually substituted OpenSSL (despite not really wanting to). It significantly sped up the app. YMMV, this is for ARM 32-bit targets. reply Thaxll 1 hour agorootparentI highly doubt that claim, maybe it's an ARM thing but there is no way that using the TLS package from Go add ms of processing on requests. Did you tried with GOEXPERIMENT=boringcrypto ? reply tracker1 4 hours agoparentprevDefinitely... first became roughly aware of it with the doorparty connector service[1]. Which is a niche fit, but definitely was cool to see how it worked. 1. https://github.com/echicken/dpc2/ reply LifeOverIP 3 hours agoparentprevI'm curious what are some prototypical use cases for you to embed an ssh sever into an application? reply hiAndrewQuinn 2 hours agorootparent[redacted for accuracy] reply devsda 2 hours agorootparentGoing through the code, I couldn't find a server but only usage of ssh client. May be I missed it. But I think GP was looking for usecases where its helpful to run an embedded ssh server using a go binary. Ansible facts can probably be a cross platform way to collect most of the information you need. For the usecases where scp'ng the binary is needed, I think ansible supports jumphost config too. But I agree that for one off tasks, running a single binary is convenient compared to setting up ansible. reply hiAndrewQuinn 2 hours agorootparentOop - you're right, I missed that they wanted server examples specifically. Thanks for the save. reply ubanholzer 3 hours agoprevWell done! if you want to extend your CLI UI, check out Bubble Tea (https://github.com/charmbracelet/bubbletea) reply madeforhnyo 7 hours agoprevNice project! I would advise to use $XDG_CONFIG_HOME instead of $HOME for storing the configuration file though :) reply xorcist 2 hours agoparentXDG is so bad. There was actually a working best practice before those people came around. Not only did they fragment the ecosystem with their self-defined standards, their standard contains a whole search path with the priority hierarchy baggage, but unspecified enough that all software does it differently. Just ignore it and pretend it doesn't exist. reply porridgeraisin 7 hours agoparentprevI hate XDG stuff so much. I just wish every app had their own folder in which they can put whatever they want. If home directory clutter is the issue, then just ~/crap/.{app1,..n} can be standardised. Basically, I want app/kinds-of-data and not the other way around. reply jasonjayr 6 hours agorootparent$XDG_CONFIG_HOME is usually \"~/.config/{app1,...n}\" so, it's close? Plus it allows a user to redirect it to a path of their choice, if all apps used it to begin with. Don't get me wrong -- some of the choices made by the XDG/FreeDesktop folks rub me the wrong way too ... reply sevg 5 hours agorootparentNo, not quite. XDG-compliant programs end up storing stuff in one or more of the following places: ~/.cache and ~/.config and ~/.local/share and ~/.local/state and ~/.local/bin I used to get annoyed by non-compliance to XDG. Now I wonder if I'd actually prefer apps to reverse the hierarchy (eg, ~/.apps/nvim/{cache,config,state}). reply ants_everywhere 4 hours agorootparentI find it obnoxious when apps make me hunt for all of their cache directories. Just put all the cache data in one place. Make it clear what needs to be backed up, what is ephemeral, and so on. Just put everything in ~/.cache. Chromium in particular is bad at this and has many types of cache. reply tracker1 4 hours agorootparentThat's where I would probably split myself... ~/.cache/appname for cache data, and ~/.???/appname/* for everything else. This is a huge part of why I like docker-compose and docker in general, I can put everything I need to backup in a set of volume maps next to each other. reply nat 4 hours agorootparentprevI would definitely prefer this. I've never wanted to see the \"cache\" stores for all (XDG-compliant) apps, but often want to see everything for a single app. reply jasonjayr 8 minutes agorootparentIt also enables you do things like: a) store caches & libdata on different disk b) consistently 'reset' cached data for kiosk style logins c) make config read-only, or reset to a known good state d) Roaming profiles where the cache is excluded from sync across machines Most computers + home directories are 'personal' where this largly doesn't matter, but there are often sound operational reasons for this seperation in cases where you are responsible for a fleet of computers. I too perfer the 'everything related to this app in one dir' approach. Crazy idea: for apps adhering to XDG, you could point all these vars at a directory under a FUSE-style mount, which then remaps the storage any way you'd like. :) reply eadmund 1 hour agorootparentprevIt’s less about wanting to see all the caches, and more about excluding all the caches, e.g. from backups. Likewise, there is one directory for machine-independent configuration which you might share, and another for machine-specific state (such as window positions). Is the spec perfect? No, of course not. But is it thoughtful, and does it address genuine needs? Yes, certainly. reply PhilipRoman 5 hours agorootparentprevThe reasoning behind historical convention of kinds-of-data/app in Unix is so you can partition the disk easily and apply policies based on type (like backup /etc, tmpfs on /tmp, mount /usr read-only) Although I'll never forgive XDG for renaming etc to config and var to state. Would be so convenient to set PREFIX=~/.local for some things reply q0uaur 6 hours agorootparentprevas someone who works on 3 different machines regularly and likes to have the same environment on all of them... i would LOVE if applications would stop cluttering my .config with cache data and other bullshit i keep having to exclude from sync. reply qwertox 5 hours agorootparent`rsync` should have something like `.nosync` akin to `.nomedia`, and the directory should be added explicitly if one wants it to be synced. Or something like a `--profile` option where `.nosync` then can contain an allow/disallow filter for profiles. I have the same issue with the scripts which trigger `rsync` getting confusingly complex because of all the include/exclude arguments. reply jrms 1 hour agorootparentI've been using a .rsync-filter file for something like what you mean for ages for my homedirs backups. It's a bit tricky probably to make it right the first time but once it's there it just works. https://manpages.debian.org/bookworm/rsync/rsync.1.en.html#f... reply jclulow 2 hours agorootparentprevThat's generally what the Cache Directory Specification attempts to cover: https://bford.info/cachedir/ Lots of things like the Rust tool chain now create the CACHEDIR.TAG files so that backup tools can ignore that part of the hierarchy. Alas, I believe the rsync folks refuse to implement it. reply perbu 5 hours agoparentprevIs XDG_CONFIG_HOME Unix? Isn't it just some Linux convention? reply wrs 3 hours agorootparentXDG = X (pronounced “cross”) Desktop Group, aka freedesktop.org, promulgator of conventions for desktop apps. So, neither one really. reply 0xbadcafebee 3 hours agorootparentYeah, I'm gonna stick with POSIX. All systems I'm aware of (other than Linux Desktop apps) use $HOME. If you want to extend your functionality to use an OS-specific directory, that's fine, but $HOME is the safest default. (Same for things like $TMPDIR) reply collinvandyck76 3 hours agoprevAfter having spent the last year writing rust, it's a breath of fresh air to clone and read through a concise and straightforward repo like this. reply CBarkleyU 3 hours agoparentIs Rust still that hard to grok even after a year to you? This is by no means meant to be disrespectful but I'm itching to start learning Rust but having only worked in Python/C#/Go I'm getting cold feet just looking at a Rust codebase Disclaimer: I'm usually very good at hitting the ground running, but I am just as much bad at \"keeping the pace\", i.e. diving deep into stuff reply collinvandyck76 2 hours agorootparentI wouldn't say that it's hard to grok.. even a year ago I found that rust projects lent themselves well towards understanding the project structure due to rust being fairly explicit about most things, and with an LSP integration I could follow along fairly easily compared to something like a python or a ruby project. Go is just easier to read. You don't have a lot of generics typically to assemble in your mental model, no lifetimes to consider, no explicit interface implementations, and so on. All of those things in Rust are great for what they do, but I think it makes it more difficult to breeze through a codebase compared to Go. reply devsda 2 hours agorootparentprev> I'm usually very good at hitting the ground running, but I am just as much bad at \"keeping the pace\", i.e. diving deep into stuff At a beginner level, rustlings[1] is an excellent resource for following along with any book/tutorial and do relevant exercise to apply the concepts from the learning material. On a more higher level, I guess (re)implementing some tool that you use daily is another way to deep dive into rust. I suspect it's one of the reasons why we see an unusual number of \"rewrite of x in rust\" projects. [1]. https://github.com/rust-lang/rustlings reply KnowtheRopes 5 hours agoprevAh, I just started learning Go, and this project looks awesome! I hope I can write something like this in a couple of months too! Well done! reply 0x12A 38 minutes agoparentThank you. I found that you can get really productive quite fast in Go, so happy learning :) reply miguelfernandez 6 hours agoprevNice work! SSH tunnels can be a pain, so this looks handy. What was the toughest part of building it in Go? Any features you’re thinking of adding? reply 0x12A 6 hours agoparentI agree! Honestly, Go made building this quite pleasant, as it has nice abstractions for networking and a great concurrency model. I'm planning to keep it minimal for now, but I would like to add Windows support, SSH multiplexing and maybe some form of throughput measurement. But I'm open to ideas :) reply jaimehrubiks 4 hours agoprevThis looks so good! I have two questions 1. What happens if the tunnels breaks? Does it retry instantly? Is there any sort of exponential backlog time? Just wondering if the server is down, if it would spike the cpu or would be gentle (while still fast enough) 2. Would you be adding support for Socks Proxy? The ssh command is quite simple, and it is as useful as regular remote and local tunnels. reply 0x12A 4 hours agoparentThank you! Yes, there is an exponential backoff strategy for reconnection attempts. Supporting SOCKS sounds like a nice idea, I'll look into it! reply 0xbadcafebee 3 hours agorootparentI think there are a couple packages out there for using Websockets to proxy a tcp connection, and some of them support SOCKS. I think they all overload that Dialup function as a generic way of opening connections reply richbray 3 hours agoprevI've been meaning to learn Go for a while. This looks like a nice project to go through and pick up a few techniques. reply sirjaz 2 hours agoprevAny plans for windows support? reply 0x12A 33 minutes agoparentYes, it's in my backlog, but I don't have a concrete timeline as of now. reply SG- 4 hours agoprevnice app, i was actually going to make a version of this with a small macos ui myself using a menu item. reply tempfile 5 hours agoprevoh, sweet, I was planning to do something like this, now I don't have to reply leroman 7 hours agoprev [–] The title was so confusing to me, the reason I opened the link was to understand how you made the SSH tunnel manager learn the GO programming language reply kaashif 6 hours agoparentI don't think the title is confusing, if that were the desired meaning then it'd say \"I made an SSH tunnel manager learn Go\" i.e. no \"to\". I don't think \"I made X to do Y\" ever means \"I made X do Y\" does it? reply Veen 5 hours agorootparentNot for native speakers, but I've heard non-native speakers use \"I made X to do Y\" in that way. reply michaelmcdonald 7 hours agoparentprev [–] To be fair: it is a \"Show HN\" title (which I believe is typically used to denote a project being \"shown [off]\" by the op). reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Boring Tunnel Manager is a lightweight command line tool for managing SSH (Secure Shell) tunnels, supporting both local and remote connections.",
      "Users configure tunnels using a TOML (Tom's Obvious, Minimal Language) file, with options for specifying details like host, user, and port.",
      "The tool is compatible with macOS and Linux, and offers features like automatic reconnection and integration with ssh-agent for secure authentication."
    ],
    "commentSummary": [
      "A new SSH tunnel manager has been developed using the Go programming language and is available on GitHub, showcasing the ease of embedding SSH servers into applications with Go's libraries.",
      "Despite the user-friendly nature of Go's SSH libraries, unresolved issues have led to the use of third-party solutions, which this project aims to address.",
      "Future enhancements may include Windows support and SSH multiplexing, with the project already receiving positive feedback and open to suggestions for further improvements."
    ],
    "points": 154,
    "commentCount": 54,
    "retryCount": 0,
    "time": 1728460344
  },
  {
    "id": 41787647,
    "title": "Germans decry influence of English as 'idiot's apostrophe' gets approval",
    "originLink": "https://www.theguardian.com/world/2024/oct/07/germany-influence-of-english-idiots-apostrophe",
    "originBody": "View image in fullscreen Rosi's Bar in the St Pauli district of Hamburg, complete with Deppenapostroph, or idiot’s apostrophe. Photograph: Dirk Renckhoff/Alamy Germany Germans decry influence of English as ‘idiot’s apostrophe’ gets official approval Linguistic body has relaxed rules on use of apostrophe to show possession, not traditionally correct in German Philip Oltermann European culture editor Mon 7 Oct 2024 15.19 EDT Share A relaxation of official rules around the correct use of apostrophes in German has not only irritated grammar sticklers but triggered existential fears around the pervasive influence of English. Establishments that feature their owners’ names, with signs like “Rosi’s Bar” or “Kati’s Kiosk” are a common sight around German towns and cities, but strictly speaking they are wrong: unlike English, German does not traditionally use apostrophes to indicate the genitive case or possession. The correct spelling, therefore, would be “Rosis Bar”, “Katis Kiosk”, or, as in the title of a recent viral hit, Barbaras Rhabarberbar. However, guidelines issued by the body regulating the use of Standard High German orthography have clarified that the use of the punctuation mark colloquially known as the Deppenapostroph (“idiot’s apostrophe”) has become so widespread that it is permissible – as long as it separates the genitive ‘s’ within a proper name. The new edition of the Council for German Orthography’s style guide, which prescribes grammar use at schools and public bodies in Germany, Austria and German-speaking Switzerland, lists “Eva’s Blumenladen” (Eva’s Flower Shop) and “Peter’s Taverne” (Peter’s Tavern) as usable alternatives, though “Eva’s Brille” (“Eva’s glasses”) remains incorrect. The Deppenapostroph is not to be confused with the English greengrocer’s apostrophe, when an apostrophe before an ‘s’ is mistakenly used to form the plural of a noun (“a kilo of potato’s”). The new set of rules came into effect in July, and the council said a loosening of the rules in 1996 meant that “Rosi’s Bar” had strictly speaking not been incorrect for almost three decades. Yet over the past few days, German newspapers and social media networks have seen a pedants’ revolt against the loosening of grammar rules. A commentator in the tabloid Bild said seeing signs like “Harald’s Eck” (“Harald’s Corner”) made his “hair stand on end”, and that the proper use of the genitive form would be bemoaned by lovers of the German language. A columnist in the venerable broadsheet the Frankfurter Allgemeine Zeitung decried the council’s decision as further proof of the English language’s “victory march”, while one newspaper editor on LinkedIn complained that legalising the “idiot’s apostrophe” amounted to “genuflecting to English”. Some linguists question whether the rise of the possessive apostrophe has much to do with the influence of English at all, however. “The familiarity of English names may be a factor, but it could just as well stem from a desire to avoid confusion”, said Anatol Stefanowitsch, a linguist at Berlin’s Freie Universität. “What we tend to see when a language interacts with another prestige language is that it incorporates its vocabulary, and not grammar.” skip past newsletter promotion Sign up to This is Europe Free weekly newsletter The most pressing stories and debates for Europeans – from identity to economics to the environment Enter your email address Sign up Privacy Notice: Newsletters may contain info about charities, online ads, and content funded by outside parties. For more information see our Privacy Policy. We use Google reCaptcha to protect our website and the Google Privacy Policy and Terms of Service apply. after newsletter promotion Even before the rule clarification, the German orthographic council permitted the use of the possessive apostrophe for the sake of clarity, such as “Andrea’s Bar” to make clear that the owner is called Andrea and not Andreas. “There is a long tradition of conservative circles fretting about international influences on the German languages,” said Stefanowitsch. “It used to be French, and now it’s mainly English”. The Dortmund-based association Verein Deutsche Sprache tries to counteract the influence of English with an “anglicism index” that proposes alternative German words, such as Klapprechner instead of “laptop” or Puffmais instead of “popcorn”. Explore more on these topics Germany Europe Language news Share Reuse this content",
    "commentLink": "https://news.ycombinator.com/item?id=41787647",
    "commentBody": "Germans decry influence of English as 'idiot's apostrophe' gets approval (theguardian.com)139 points by pseudolus 5 hours agohidepastfavorite329 comments crazygringo 1 hour agoThe article doesn't mention it, but am I right in assuming this basically comes from McDonald's? There are a lot of places around the world that copy the \"'s\" where it doesn't exist natively, but only for restaurant names or similar -- like \"Bob's\" is the McDonald's clone in Brazil [1]. I'm mostly curious whether \"Rosi's\" and \"Kati's\" in the article are seen by Germans as intentionally trying to look \"foreign\", rather than the apostrophe \"invading\" German. Like, if I go to a Sausage Haus, I'm not exactly worrying about \"Haus\" creeping into English to replace \"House\". Nor would I ever call it the \"idiot's house\" because that would be crazy insulting and perjorative. [1] https://en.wikipedia.org/wiki/Bob%27s reply saghm 53 minutes agoparent> The article doesn't mention it, but am I right in assuming this basically comes from McDonald's? There are a lot of places around the world that copy the \"'s\" where it doesn't exist natively, but only for restaurant names or similar -- like \"Bob's\" is the McDonald's clone in Brazil [1]. For whatever reason, it drives me crazy when I hear people refer to Pizzeria Uno as \"Uno's\". I've had conversations about it multiple times with different people in my family. There's no one named \"Uno\", it's a number! I try not to be a prescriptivist but for whatever reason this bothers me to an irrational degree, and I can't understand why nobody else notices. reply bunderbunder 7 minutes agorootparentThis sounds very Midwestern to me. Where I come from that would happen a lot. It wasn't necessarily that people didn't know the real name of the place. It functioned more like an inflection that helps to distinguish between the company, and a specific storefront operated by that company. Compare it to the distinction between \"Alice\" and \"Alice's\". Alice is the person, and Alice's is her house. For example, you you'd say \"JCPenney stock is up by 32 cents this week,\" but you'd also say, \"I bought this shirt at Penney's.\" reply aiforecastthway 21 minutes agorootparentprevThe establishments are owned by a corporate person named \"Uno Restaurant Holdings Corporation\". So calling it Uno's isn't inconsistent with how we talk about Walmart's stores or Google's website, for example. reply xp84 15 minutes agorootparent> calling it Uno's isn't inconsistent with how we talk about Walmart's stores calling it Uno's isn't inconsistent with how we talk about Walmart's stores or Google's website No, calling it \"Uno Corp's pizzeria\" would be the equivalent. Nobody says they're \"Going down to Walmart's\" or \"doing some research on Google's.\" reply sgerenser 2 minutes agorootparentMy grandmother would indeed refer to shopping at “WalMart’s” or “Penney’s”. Maybe a regional thing (she was from central PA)? bradhanson 8 minutes agorootparentprevTo be honest, even when going to the original Pizzeria Uno (or Due) I’ll probably still call it “Uno’s” ‘cause it’s a weird part of the Chicago dialect. We do the same thing for the grocery store Jewel-Osco, calling it “da Jewels” reply complianceowl 37 minutes agorootparentprevI love Uno's. reply iforgotpassword 3 minutes agoparentprevThis is gonna sound a little pejorative, but I've seen it at barber shops and little stores where my bet would be on ignorance rather than intentionally trying to imply anything. The guy next door did it too so it must be right. reply int_19h 33 minutes agoparentprevIn Russian, the name of the restaurant did include the final \"s\", but without the apostrophe, so I don't think that's consistent. Note though that in German, \"-s\" is also a genitive suffix, it's just the spelling that's different here. reply eleveriven 56 minutes agoparentprevYou're onto something interesting here! The possessive style. reply thangalin 6 minutes agoprevUNICODE and ASCII apostrophes are a bit absurd. For KeenQuotes[1], my library to automatically curl straight quotes, there's an Apostrophe type that defines variations on how to convert a straight apostrophe to a curled one. The main issue is that most suggestions are to use ’, which isn't semantically correct[2], and at one point Michael Everson noted, \"the alphabetic property should be restored to U+02BC\"[3]. I've bucked the x27, U+2019, and rsquo trend with: /** No conversion is performed. */ CONVERT_REGULAR( \"'\", \"regular\" ), /** Apostrophes become MODIFIER LETTER APOSTROPHE ({@code &#x2bc;}). */ CONVERT_MODIFIER( \"&#x2bc;\", \"modifier\" ), /** Apostrophes become APOSTROPHE ({@code &#x27;}). */ CONVERT_APOS_HEX( \"&#x27;\", \"hex\" ), /** Apostrophes become XML APOSTROPHE ({@code '}). */ CONVERT_APOS_ENTITY( \"'\", \"entity\" ); Thoughts? [1]: https://whitemagicsoftware.com/keenquotes/ [2]: https://tedclancy.wordpress.com/2015/06/03/which-unicode-cha... [3]: http://www.unicode.org/L2/L1999/n2043.pdf reply psychoslave 4 hours agoprevHaving French as mother tongue, I always find fascinating when the French and German official bodies go postal about such a topic. It’s like looking some parents complaining of the retro-influence of some common bastard children. :P Now, there are languages for which Globish can be part of an existential threat, but German and French are nowhere close to this. See https://en.wikipedia.org/wiki/Lists_of_endangered_languages There are also a measurable economical issues for non-English-native nations to have to use the de facto lingua-franca of the day that is English. Of course neither German nor French would be a better alternative as a global international neutral language. To my knowledge, the only proposal that gained some modest but significant results on that side over the last century is Esperanto. You know, the language against which France has put its veto has it was proposed as language of communication in League of Nations (1920s) or UNESCO (1954) and still is unhelpful with its adoption in United Nations. Fun fact, Germany has a city where street names and many other things are translated in Esperanto: https://uea.facila.org/artikoloj/movado/la-esperanto-urbo-r3... reply int_19h 4 minutes agoparentEsperanto is still frustratingly complex with regard to phonemes for an international language. I think most speakers of many European languages don't realize just how complex their phonologies are on average. Slavic languages probably take the cake there with stuff like 5-consonant clusters that can even include sequences of plosives and affricates, but then you also have Germanic languages (and French!) with their insanely large vowel inventories. Compared to that, Esperanto is relatively simple, but when you look outside of Europe, having 3-consonant clusters or phonemic contrast between plosives and affricates at the same place of articulation (e.g. \"t\" vs \"t͡s\") is very unhelpful. That said, it's still a massive improvement on English phonologically. Even if you only consider the simpler American varieties, the three-way æ/ɐ/ɑ distinction alone (as in bat vs but vs bar) is a huge WTF for anyone coming from a typical 5-vowel system. And then you have consonants like θ and ð that don't have clear 1:1 counterparts in most other languages, often not even as allophones of something else that you could point at. Still, if you want to see what a more modern take on the concept might look like, I believe Globasa (https://www.globasa.net/eng) is the most active project along those lines. Of course, realistically, the likelihood of it actually being adopted as the universal language is effectively nil, but then that's also the case for Esperanto. reply nicbou 34 minutes agoparentprevComing from Quebec, I understand why people are worried about their language being strangled out and their culture dying with it. For Quebec this has always been a threat. I live in Germany now. There are 10-15 times more German speakers in the DACH area than there are French speakers in Quebec. Even then, it’s weird that companies no longer bother translating their ads and slogans for the German-speaking market. It’s somewhat sad that every culture is slowly becoming a vaguely American, California-based culture. Language and culture are intertwined. I feel that with the globalisation of both, something of value is lost. It’s only right to feel concerned about it. reply jjtheblunt 17 minutes agorootparentI wonder if the spread of English is because it's like a barycenter pulled by multiple languages, so not too far afield if coming from any of those languages. reply JumpCrisscross 3 hours agoparentprevEnglish being an amalgamated language and thus uniquely flexible is part of its power. We have options in style choices many languages formally don’t permit, e.g. when to italicise or, if quoting, whether to “exclude punctuation”, or “include it.” (As well as comma use.) As a fellow French speaker, I think these are strengths other languages could gain from. Couriel or email (or e-mail)? Speaker’s choice. Same for possession. (Particularly for a culture with a tradition of individual liberty like France.) reply wil421 1 hour agorootparentIs English just badly pronounced French?[1] I wish English would’ve adopted conjugation and other patterns the Romance languages use. I doubt it would’ve fit correctly. But it would be better than having 1,000s of badly pronounced French words in the language. [1]https://www.barrons.com/amp/news/english-just-badly-pronounc... reply SJC_Hacker 55 minutes agorootparentEnglish is a Germanic language with a Latin alphabet, as spoken by Celts, after being ruled by people from France who were originally from Norway (or maybe Denmark) reply duped 1 hour agorootparentprev> Is English just badly pronounced French? Oh totally, my American accent sounds just like, \"quand je vais au barbecue le quatre juillet, je vais manger un hot dog avec ketchup.\" > But it would be better than having 1,000s of badly pronounced French words in the language. They're loanwords that changed over time, they're not \"badly\" pronounced at all. French is filled with many loanwords as well that are pronounced nothing like their language of origin reply jcranmer 1 hour agorootparentprev> Is English just badly pronounced French? No, English is a Germanic language whose conjugation rules have severely atrophied, with (mostly specialized!) terminology liberally adopted from Latin, Greek, and other roots. In things like tense and aspect structure, I believe that English hews a lot closer to German than French. reply WalterBright 1 hour agorootparentEnglish is a barbarian language with French nouns, as a result of the Norman conquest of England. Amusingly, using the French words is a signal to being upper class. Such as \"purchase\" (pourchacier) instead of \"buy\" (byan). reply MarkusWandel 58 minutes agorootparentprevBoy have they atrophied. Even as a German speaker in whose first language these words have current equivalents I'm not 100% certain when to use thou, thee, thy, thine etc. that still were part of the language at Shakespeare's time and have since been simplified into you/your/yours etc. But it's true, English takes this stuff in stride, with modernisms e.g. \"sick\" meaning something good gradually being incorporated into the mainstream, rather than fought against by language purists. reply eleveriven 54 minutes agorootparentprevWhile English certainly has thousands of words that came from French, it is far from being a \"badly pronounced\" version of French. reply YawningAngel 1 hour agorootparentprevIt's not a conjugation issue. \"Champagne\" is letter-for-letter identical in both languages, but pronounced differently for phonotactic reasons reply MarkusWandel 55 minutes agorootparentIt's a typical French loanword in German too: \"Champagner\" isn't pronounced with standard German prounciation rules. Even localized ones, e.g. in my childhood a sidewalk was called a \"Trottoir\" in the French pronunciation. For some reason nobody gets exited about French loanwords. reply dqv 2 hours agorootparentprevJust this morning I came across a guy called Mr Techpedia on YouTube and I was really surprised because I heard a lot of English phrases but also phrases that are in a different language or a dialect of English I’m not familiar with. It was actually really cool. It also reminds me of a time when I heard someone codeswitch from US Midwestern English to Malaysian English - there was a clear difference in word choice and pronunciation. Global/Internet English as a concept is really cool as well. I often (accidentally) adopt grammatical constructions from Global English that I believe come from that particular speaker’s native tongue. Anyway, yeah, I love this sort of mixing of languages and I’m glad a lot of cultures are more open about mixing in English. reply catlikesshrimp 1 hour agorootparentI am Spanish native, but the way I structure my sentence seems a google translation from chinese. People around me often don't understand the meaning, so I have to speak slower to structure my sentence in a more proper Spanish way. I suppose languages evolve around the way their corresponding population brains work. People can still learn other languages, or be native to other languages, but there is a language way that is the best fit to some people which is related to biology. reply deng 2 hours agoparentprevBut this is not even about language, it's about spelling. For some reason, people forget that these are entirely different things. We are currently communicating in a language where there's often times no relation between the written and spoken word at all. reply samatman 1 hour agorootparentA more accurate statement is that English is a language where spelling often reflects history and etymology, rather than phonetics. There's always a relation between a spoken word and its written representation, they're the same thing in different mediums. reply int_19h 26 minutes agorootparentThat's not uncommon in general, but English is a particularly bad instance of that, partly because it has so many prominent source languages with widely different spellings, and partly because of the lack of any significant spelling reforms for a very long time. There was an interesting study (https://aclanthology.org/2021.sigtyp-1.1/) where they evaluated phonemicity of various language orthographies by training a neural net and then seeing how accurately it could predict things. Of two dozen languages they had there, the only ones that scored worse for writing are French and Chinese, but most notably, English is the only one that scored below 50% accuracy for reading, and with a significant gap at that. This is very unfortunate for an international language, since reading is kind of the most basic practical thing you can usually do with a second language. reply makeitdouble 1 hour agorootparentprevIf we're going for accuracy, your statement would have to explain how it goes for other situations, for instance: - words spoken by toddlers: what's the spelling of a word that doesn't exist outside of a kid's brain ? In particular parents can accept it as a word without ever setting an associated writing. - written words that don't have a pronounciation: typically Latin is dead and how any of it is pronounced is up to how we feel about it. That's without going into words with phonems unrelated to their written form (XIV as fourteen for instance) and I assume there will be words that exchange spelling and pronounciation with others. Languages are plenty weird, we should embrace their weirdity IMHO. reply WalterBright 1 hour agorootparent> typically Latin is dead and how any of it is pronounced is up to how we feel about it. How words were pronounced can be deduced from poetry. reply ktosobcy 3 hours agoparentprevI wouldn't mind for English to have \"standardisation body\" akin to French or German one (or RAE for Spanish) that could maybe get rid of backward, dumb spelling ;) (see https://en.wikipedia.org/wiki/English-language_spelling_refo...) reply scheme271 2 minutes agorootparentBut those standardisation bodies often get ignored by most of the speakers. Language is a living thing that evolves and changes in spite of the dictates of academies. Also, with global usage, any given body is not going to be able to do much, e.g. a chilean spanish speaker won't care what the RAE says or a Quebecois would probably laugh at what the French language academy dictates. reply BitwiseFool 3 hours agorootparentprevI jokingly suggest that, by definition, the only person who uses English properly and speaks without an accent is the King. reply lenerdenator 1 hour agorootparentIf it's English because the King speaks it, how come the most famous document written in (modern) English tells the King to take a hike? reply anthk 3 hours agorootparentprevHence the \"Royal Academy of English\" -> RAE, as in Real Academia Española. reply onlypassingthru 2 hours agorootparentprevThe beauty of English is that it is controlled by the speakers and not by some pompous authority. It's even flexible enough to allow for regional differences, which allows my fellow Americans and I to spell words correctly like color and theater. reply martijnarts 2 hours agorootparentFor what it's worth, French is also controlled by speakers. The pompous authority is just lagging behind. reply seszett 44 minutes agorootparentThere is not authority for french, it's a myth. The Académie has no authority whatsoever, it's little more than a club for writers. The Education Ministry has authority for school programs and what is accepted in French language classes, but only in France. It only ever allows new uses, never forbids previously allowed things. The OQLF (and French language Ministry) has a broader authority within Québec, but only for Québec. The Ministry of Culture has some authority within the Brussels-Wallonia federation but it's quite limited. No idea what it's like in Switzerland. But there is no global authority for the French language (unlike German or Dutch for example). The language evolves by consensus. reply r00fus 1 hour agorootparentprevYou do realize that what people actually speak (in France) differs quite a bit from the Académie Française. email vs. courriel for example is a good one, but you'll stand out in most places if you don't know l'argot (slang). I don't think an English standardization would change much in how people actually speak. reply graypegg 2 hours agorootparentprevThere are specific subsets of English that are used in certain domains that have standards bodies behind them, like Simplified Technical English for aviation. It even has a working group! [0] VOA also have a Learning English spec for broadcast english [1] but that seems to be a lot looser of a spec. So it's definitely not impossible. The funny thing, is I remember being told in grade school that in English Canada, I was to write numbers with a space as the thousands separator. `$10 000.00`, instead of `$10,000.00`. This is because french Canada uses a comma as a decimal point, `10 000.00 $`, so a space is non ambiguous. I have rarely ever seen the English space format in use here. I don't think English speakers would respect any authority if it wasn't as domain-scoped as Aviation or Learning english. [0] https://www.asd-ste100.org/ [1] https://en.wikipedia.org/wiki/Learning_English_(version_of_E... reply speakeron 2 hours agorootparentThe use of a space as a thousands separator has been around since the 1940s as recommended by the International Bureau of Weights and Measures and it was what we used when I was a kid at school in the UK. They specified it should be a thin (half) space. https://www.bipm.org/documents/20126/28433818/working-docume... reply graypegg 1 hour agorootparentNo I know, it's also part of the French standard. Just more so commenting on how uncommon it is from Canadian English speakers despite it being the Canadian English \"standard\" recommend by a Canadian entity similar to the French or German standards bodies. reply michaelt 1 hour agorootparentprev> So it's definitely not impossible. Well, it's definitely not impossible to publish a document declaring itself the standard form of english. But I'm pretty sure it would be impossible to get english speakers to comply - or even to get any countries to make the standard legally binding. reply catlikesshrimp 1 hour agorootparentImagine either England or the USA accepting to vary their language towards a common standard. I expect strong opposition would stem from pride. reply SllX 3 hours agorootparentprevI’ve seen similar suggestions but one of the best things about English is that we don’t have that nonsense. It would just be a source of annoyance and consternation adding more noise to news and politics in the Anglosphere. reply tannhaeuser 1 hour agorootparentprevI think the article's wording > guidelines issued by the body regulating the use of Standard High German orthography gives a somewhat false impression regarding the influence and standing of this body. Orthography was traditionally what was written in the Duden dictionary/thesaurus. Only in 2004 or so there was a push for a moderate reform for German as taught in schools, and it was deemed necessary to have at least Austria and Switzerland join (hence the council isn't a natioval body), whereas neighbouring countries with German-speaking minorities such as Italy were not sitting at the table it seems. reply meyum33 2 hours agorootparentprevIt won’t work. Just look at the mess of Imperial units in the United States. And this is when the metric system is vastly more straightforward, simply better, and universally adopted. The English language? No way any standardization would work. And that unlike the Imperial system the variations in English is probably a feature, not a bug. reply gtk40 1 hour agorootparentAnd it is not just the US. In the UK from my limited experience visiting they use a mess of different units commonly. Certainly not all in on metric. reply tivert 2 hours agorootparentprev> I wouldn't mind for English to have \"standardisation body\" akin to French or German one (or RAE for Spanish) that could maybe get rid of backward, dumb spelling ;) That's even less likely now than in the past, with the elite cultural trends in English-speaking countries favoring the adoption of foreign spellings and pronunciation. That just piles on the complexity to unmanageable levels. IMHO, for instance, there's no excuse for the requirement that English newspaper readers know Pinyin [1], rather than some more English-friendly romanization system, to be able to read news about China, when Chinese speakers themselves use a completely different, non-roman writing system. What's next, just printing the Chinese characters without romanization? Pinyin has its uses, but writing things out for foreigners is not something it does well. [1] which gives many letters very unexpected values (e.g. c = ts) and many vowels are impossible for an English-speaker to guess correctly. reply makeitdouble 59 minutes agorootparentFor what it's worth CJK countries tend to give less weight to how foreign names are pronounced. For instance the current PRC secretary name is pronounced accordingly to the characters' reading in Taiwan and Japan, and won't have much in common. Same way Chinese people will read Japanese name as the characters sound to them, without referring to the actual Japanese reading, even if in Japan these names have a designated original reading. reply hosh 2 hours agorootparentprevPinyin is pretty good at rendering Manderin in a Latin script. Can you elaborate on what you mean by “English-friendly”? reply tivert 2 hours agorootparent> Pinyin is pretty good at rendering Manderin in a Latin script. It's pretty good for Mandarin speakers. It's terrible for English speakers. > Can you elaborate on what you mean by “English-friendly”? English friendly is something that will produce reasonably-close approximate pronunciations by an English reader without any extra foreign-language training. Basically, something that prioritizes following existing English orthography (e.g. do not use \"c\" for \"ts\", use the closest approximate for sounds that do no exist in English) instead of maximal fidelity to the foreign language. reply hosh 38 minutes agorootparentWades-Giles is closer to English-friendly, but it has a lot of flaws. It has no notion of intonations. I think there is also the issue of cultural dominance. \"English-friendly\" means the foreign language is morphed to better suit English speakers. It could go the other way if Mandarin is the dominant trade language. reply samatman 1 hour agorootparentprevThere are basically political reasons for this. Wade-Giles is associated with Taiwan, and is in fact mostly used when referring to Taiwanese subjects, I've always seen Kaohsiung for the city, never Gāoxióng. The Mainlanders would find it very insulting to not use Pinyin when referring to subjects in the PRC, so understandably, American journalism goes along with that. For what it's worth I think both systems have different disadvantages, in that neither does a good job of reflecting the actual pronunciation of Guoyu. Excuse me, Putonghua. Doing so with the English character set isn't actually possible. reply taylorius 3 hours agorootparentprevNah, such a body would surely be the beginning of the end. Anarchy in the UK, including its language! reply eastbound 2 hours agorootparentFrance has the Académie Française. Well, no-one respects them. The first woman to enter it was Marguerite Yourcenar and she was strongly antifeminist; And they said to not use the Français.e.s spelling and rather keep the usual “Français(es)”, and suddenly all administrations started the dot-based version. reply partiallypro 2 hours agorootparentprevOne reason English is so popular (aside from pure economics) and that other countries quickly adopt English slang words is because we don't have such a thing. reply sjm 55 minutes agoparentprevWhen I lived in Bordeaux I remember seeing billboards basically advising young people to not use \"txt speak\" and instead write \"real French\" to preserve the language. reply Muromec 1 hour agoparentprevWhy invent Esperanto if Dutch already exists and is the most reasonable European language to learn. reply lolc 1 minute agorootparentFinally a modest proposal to unite the European people. reply chimeracoder 3 hours agoparentprev> There are also a measurable economical issues for non-English-native nations to have to use the de facto lingua-franca of the day that is English. Of course neither German nor French would be a better alternative as a global international neutral language. > To my knowledge, the only proposal that gained some modest but significant results on that side over the last century is Esperanto. You know, the language against which France has put its veto has it was proposed as language of communication in League of Nations (1920s) or UNESCO (1954) and still is unhelpful with its adoption in United Nations. Esperanto is not a \"global international neutral language\" either. While artificially constructed, it's functionally a Romance language, deriving over 80% of its vocabulary as well as the majority of its grammatical structure from Latin and/or Romance languages. The majority of the remainder comes from other European languages, primarily Germanic languages. reply int_19h 21 minutes agorootparentEsperanto is indeed not culturally neutral (and was never supposed to be), but it's still vastly better in practice than other European languages precisely because of this overemphasis on Latin (and Greek) roots - because those are exactly the \"fancy\" words that tended to be borrowed most often historically even across language families. Also, interestingly enough, Esperanto attracted more interest in some Asian countries - most notably, Japan - than in much of Europe. I think the bigger problem with Esperanto is phonology. It's too heavy on affricates, including some relatively rare ones (e.g. phonemic \"ts\"), and the consonant clusters get pretty bad. For someone coming from a simple CV language, those are likely to be a bigger challenge than the word list. reply fredgrott 2 hours agoparentprevfun fact its English that is the bastard here...same way Creole was formed as language...i.e. borrowed from elsewhere in this case Danes(Anglo) and Saxon part of Germany... and some minor contribution from the Normans of course... reply makeitdouble 1 hour agoparentprev> lingua-franca of the day that is English. Of course neither German nor French would be a better alternative as a global international neutral language. Being a linga-frinca has nothing to do with merits though. Aside from \"linga franca\" being literally \"French\", it's a matter of which group of nations have a tremendously dominant position on the international scene. If China was to take hold of India and Russia and set the rules for the rest of the world, the defacto linga-frinca won't be English for long, however intricate people might feel about Chinese. reply dbrueck 14 minutes agorootparentMal: \"We got work to do, dong ma?\" reply wolframhempel 1 hour agoprevI feel that most people instinctively assume that some institution, e.g. the government or the dictionary publishers are the authority on what constitutes \"correct language\". It's important to emphasize that language (included spelling) is something that develops organically and that the role of these institutions is just to capture the status quo. At least that should be the case in free societies. Language is power - and controlling it is an important aspect of exercising control. reply Eddy_Viscosity2 1 hour agoparentIndeed, dictionaries and governments are just writing down what's already happening in the language. In a way language is one of the only truly democratic institutions. We all vote for new words and new pronunciations by using them or not using them. The collective action of all these choices is the language. reply duped 1 hour agorootparentThat's not always accurate, many of these government bodies fight the evolution of the vernacular and suppress dialects or loanwords. reply schroeding 1 hour agorootparentprevEh, I wouldn't agree with that. In Germany, we had multiple top-down orthographic reforms in the past (e.g. Rechtschreibreform 1996, had many many significant, brand-new changes to make spelling and speaking more consistent) which were pretty forced, i.e. was not organic at all. I'm not saying it was bad, I like the current state of the German language - but it was not organic or democratic. Germanys equivalent to The Daily Mail even had an entire campaign against it with stickers and all. And there is no true way to \"vote with your feet\" if you get punished for violating the official orthography. Not to mention other sources of non-organic language change, like e.g. suppressing dia- or sociolects, but I also don't want to delve (hehe) too far. :P reply nobody9999 50 minutes agorootparent>And there is no true way to \"vote with your feet\" if you get punished for violating the official orthography. Ich hat das nicht verstanden. Ich kann mit meinem fusse wahlen. Wie konnte ich fur schlecte Deutsch bestraft werden? Ich wohne nicht in Deutschland. Und ja, meinem Deutsch ist sehr schlecht. Das stimmt. Kommen sie damit klar. Edit: Fixed (without really improving) my terrible german. reply sweezyjeezy 1 hour agoparentprevFrance has a mechanism to try to stop this: the Académie Française [1] that publishes the official French dictionary. Rather than simply recording language as its used, they do actively try to steer it. They're most well known for trying to suppress anglicisms, e.g. in the early 2000s they pushed 'courriel' instead of 'email'. That one did not work out - email is much more common, and finally entered the dictionary in 2009 (to this day labelled 'anglicism' and discouraged over 'mél'). FWIW the German equivalent is much less prescriptive, it only weighs in on grammar / punctuation. [1] https://en.wikipedia.org/wiki/Acad%C3%A9mie_Fran%C3%A7aise reply lcouturi 33 minutes agorootparent'Courriel' was coined by French Canadian translator André Clas, not by the Académie Française. The Office québécois de la langue française successfully promoted its usage in Quebec in the 90s and the Académie Française unsuccessfully tried to do the same in France. 'Courriel' is still commonly used by French Canadians, but indeed it was never widely adopted by France. As a French Canadian, I usually use 'courriel\", though the anglicism 'e-mail' is also quite commonly used. Can't say I've ever seen anyone use 'mél', tho. reply BitwiseFool 1 hour agorootparentprevMy absolute favorite example of this is the suppression of \"podcast\" in favor of the far more romantic \"audio à la demande\". reply oersted 1 hour agoparentprevThis has historically been the philosophy of English linguists, but for many languages (Spanish, French, German…) there is a central institution that does indeed decide what is officially correct. Their decisions are taken seriously and intentionally propagated anywhere where language is used in a somewhat official context (not just in public institutions). True they adapt the standard over time following common usage, but the standard is the primary source of truth and many things are decided unilaterally regardless of common usage. reply marcellus23 44 minutes agorootparent> This has historically been the philosophy of English linguists It's not unique to English linguists, it's a tenet of modern linguistics in general. A language is defined by the way people actually speak. If that's influenced by a central organization, fine, but that does not contradict descriptivism at all. Someone studying a language should always study the way the language is spoken by real people, using prescriptivist sources as supplementary sources of information where needed. reply oersted 15 minutes agorootparentI don't disagree, it is certainly not unique to English, and these central institutions do largely have a descriptivist attitude (although the French are known to be rather purist ;) ). But there is a practical difference: textbooks and dictionaries in English have traditionally come from distributed institutions, which are eminent but none of them claims to be official, whereas for example in Spanish they all originate from or closely follow the standards of the Real Academia. Sometimes unified standards have been artificially created, like for Basque or Mandarin, and in those cases prescriptivism is more dominant. reply SSJPython 50 minutes agorootparentprev> This has historically been the philosophy of English linguists, but for many languages (Spanish, French, German…) there is a central institution that does indeed decide what is officially correct. Their decisions are taken seriously and intentionally propagated anywhere where language is used in a somewhat official context (not just in public institutions). This sounds very similar to the common law vs. civil law traditions as well. I wonder if there's a connection between linguistics and legal systems. reply diego_moita 1 hour agoparentprevWell, communication needs standards. As much as we need someone to define what is http, TCP/IP or Posix we also need someone to define what is English, Spanish or any language. If you don't believe it then try to understand whatever language a Venezuelan or Dominican speaks. That blabber is anything but Spanish. reply Wytwwww 34 minutes agorootparent> As much as we need someone to define what is http, TCP/IP or Posix we also need someone Why? Humans are almost infinitely more adaptable and capable of dynamically changing their behaviour compared to computer programs. Learning to understand/speak a new pseudo dialect of your native language isn't particularly hard. Millions of people do that near effortlessly on a daily basis (especially in many German speaking areas). reply sweezyjeezy 57 minutes agorootparentprevBut what are you suggesting? It wouldn't be better if Venezuela had its language dictated by another country (e.g. Spain) - that would just be oppressive. reply diego_moita 42 minutes agorootparentWell, ASALE[1] (Association of Academies of Spanish Language) is already a cross-national body that negotiates these rules among several countries. [1] https://en.wikipedia.org/wiki/Association_of_Academies_of_th... reply moffkalast 1 hour agoparentprevLanguage is power, France is bacon. reply bmulholland 26 minutes agoprevIf the intersection of English and German interests you, this essay beautifully captures how the two intermingle: https://europeanreviewofbooks.com/beamer-dressman-bodybag/ reply nomilk 4 hours agoprevI knew of a building named Water's Edge, but spelled \"Waters Edge\". The absence of a possessive apostrophe was bothersome but I realised there's a case for sacrificing correctness for things like ease of communication and how the words look. An insight from Oscar Wilde: > Mr. Noel, in one of his essays, speaks with much severity of those who prefer sound to sense in poetry. No doubt, this is a very wicked thing to do. But he himself is guilty of a much graver sin against art when, in his desire to emphasise the meaning of Chatterton, he destroys Chatterton's music. In the modernised version he provides of the wonderful Songe to Ælla, he mars the poem's metrical beauty with his corrections, ruins the rhymes, and robs the music of its echo. [1] (^^ that's from a short but wonderful essay, worth reading!) [1] https://ia800203.us.archive.org/23/items/collectedworksau12w... reply mattlondon 3 hours agoparentBut Waters Edge is totally fine if they mean \"by the edge of the waters\" and not \"the edge belongs to the water\". But hey, there are no rules or logic in English so have at it! reply nomilk 2 hours agorootparentI think I understand: I thought about trees: Tree leaves (leaves from a tree) Trees leaves (same but from more than one variety of tree) Same logic for water: Water edge (an edge that happens to be of a body of water) Waters edge (same but of more than one body of water) reply f33d5173 1 hour agorootparentIn tree leaves, it could be leaves from a single tree or multiple trees. Hence, you can't pluralize tree into trees leaves, tree isn't allowed to recieve a plural there. If you write it as tree's leaves, then tree is singular, and the form is possessive (whereas before it served to disambiguate from, say, leaves of a book). Then you can also pluralize tree to trees' leaves, and now it's leaves from multiple trees. reply nomilk 59 minutes agorootparentWould the same logic also invalidate ‘waters edge’? reply dylan604 54 minutes agorootparentprev> Tree leaves (leaves from a tree) or it could be the single tree is vacating the area > Trees leaves (same but from more than one variety of tree) or multiple trees are vacating the area we could equally turn edge into a verb as well. so now we have a whole other meaning outside of an apostrophe reply Andrex 2 hours agorootparentprevHmm, it seems like water can be plural or singular purely depending on the author's preference. I didn't realize that until now. reply underlipton 2 hours agorootparentprevYeah, that could totally be by design. For example,the book \"Rainbows End\" is not \"Rainbow's End\" specifically because the meaning of the first is intended, not the second. reply cdumler 2 hours agoprevBeing an old, grey beard, it's been interesting to see language change in my lifetime. Things I learned: * Third-person singular indefinite (\"he or she\") can be replaced with third-person plural (\"they\"). Of course, a lot of changes around recognizing gender. * Final punctuation within the quote at the end of sentence (Did you just say \"what?\") can be placed after the final quote if the quote is for a literal string (ie, The password is \"123456\".) * Companies switched from being singular plurals (\"Google is deprecating another product.\") to plural singulars (\"Google are deprecating another product.\") * Moving away from verbed nouns (\"Google it\") to multipart verbs (\"search it up\"). * Double infinitives (\"to try to eat\") getting changed to an infinitive and conjunction (\"to try and eat\"). One thing I am very said about is just how lack luster both of my kid's hand writing is. My eldest is in high-school and her hand writing is horrible. Partly because she has little use for long-form writing (forget cursive) and because they rely on the spell checker. reply jasonpeacock 2 hours agoparentSingular \"they\" has been around for a very long time, and used naturally without anyone noticing it as unusual, until recently when there's been more gender discussion and people suddenly realizing they were already recognizing genderless people without knowing it ;) https://en.wikipedia.org/wiki/Singular_they reply nmeofthestate 2 hours agorootparentYou're right - that has been around for a long long time. But I feel like I've seen a general increase in its usage that can make writing more ambiguous to parse. Like we already know the gender of someone being written about in a sentence, but they become referred to as \"they\" at random - it's a subtle effect. I'm talking about examples unrelated to \"gender stuff\" but perhaps that's what's made the usage more popular among younger writers. reply xdennis 1 hour agorootparentprev> people suddenly realizing they were already recognizing genderless people without knowing it ;) Not true. It was used in the past to refer to an unknown person. I.e. \"When a candidate arrives given them the test.\" You don't know what sex the candidate is before he arrives and instead of saying \"he or she\" you say \"they\". But nowadays people use it as a superclass of he and she: \"I asked my boss for a raise but they refused\". It doesn't make any sense. You know very well what sex your boss is, but \"they\" is used for virtue signaling. It's a way of saying \"I know my boss is a man, but I'm going to use they because a woman could do just a good a job and he, sorry, they does.\" reply kayodelycaon 1 hour agorootparent> You know very well what sex your boss is, but \"they\" is used for virtue signaling. I doubt it's virtue signaling. I'll use they to refer to the position not the person. Sometimes it's deliberate obscuration. Other times it's a form of laziness. I don't have to think about which pronoun to use if I just use the generic one. In my case, once I got used to seeing people as people first instead of their gender, it's been easy to slip up on the pronoun. reply jasonpeacock 1 hour agorootparentprevYour sentence is the perfect example for proper use of \"they\", per the wikipedia article \"It typically occurs with an indeterminate antecedent\" - \"boss\" is non-gendered and so \"they\" is grammatically correct. There's no virtue signalling, you're reading too much into it. reply zargon 2 hours agoparentprev> Companies switched from being singular plurals (\"Google is deprecating another product.\") to plural singulars (\"Google are deprecating another product.\") I thought this was just a difference between American and British English. reply emaro 1 hour agorootparentIt's the first time I see a company's name used like that. reply ldoughty 2 hours agoparentprevBeing an old grey beard you probably know these... but for others: > * Final punctuation within the quote at the end of sentence (Did you just say \"what?\") can be placed after the final quote if the quote is for a literal string (ie, The password is \"123456\".) Prior to movable type printing presses, the British \"logical quotation\" system was the norm for English. This changed, and is credited to american newspapers, because of movable type. I've heard different reasoning (from being less likely to break, or to looking cleaner), but both point to printers. Even the alternate name for this quotation style is \"typesetters quotation.\"* Moving away from verbed nouns (\"Google it\") to multipart verbs (\"search it up\"). This is purely branding. In the US, if people say \"Google it\", it creates a synonym between \"Google\" and \"Search\", which hurts cases for Google in defending their brand... If it gets too weak, then you or I could make a \"Google Booster\" company, which focuses on improving search engine rankings in general -- not just Google, and with no direct business relation with Google See: Kleenex, Band-aid, ChapStick, Crock-pot, Jacuzzi reply OJFord 55 minutes agoparentprev> Double infinitives (\"to try to eat\") getting changed to an infinitive and conjunction (\"to try and eat\"). Or worse, 'to try eat', 'to go get', etc. It's very American to my ear, but it's certainly invading. Another corruption triple like that is to do something 'accidentally' / 'by accident' / 'on accident'. reply mostlysimilar 2 hours agoparentprev> * Companies switched from being singular plurals (\"Google is deprecating another product.\") to plural singulars (\"Google are deprecating another product.\") > * Moving away from verbed nouns (\"Google it\") to multipart verbs (\"search it up\"). Resist! Google is trying to get you to stop Googling things, but we don't have to listen to the corporate overlords. reply nmeofthestate 2 hours agorootparentI think organisations (companies, teams) being singular/plural differs depending on what country you're in, so perhaps this is a bleeding across of conventions due to globalisation. reply unsupp0rted 2 hours agoparentprevI bet her calligraphy sucks too. And is she any good at milking a cow? In 2024 there's no need to feel sad about deprecated (or now niche) skills being lackluster. I'd be more concerned if she couldn't find information efficiently when she goes searching for it. That's a skill that mustn't be lackluster. reply ghayes 2 hours agoparentprevIs “search it up” much different from a similar phrase “search for it”? The structure of the original quote is “imperative verb, direct object, adverb” but I wouldn’t call that a change in grammar so much as a change in diction. reply mppm 1 hour agoprevFor those interested, I highly recommend \"The Unfolding of Language\" by Guy Deutscher, in which he describes how languages evolve over time, to the great annoyance of purists, but without either losing or gaining sophistication in the long run. It's a very entertaining read and has considerably reduced my irritation at \"incorrect\" use of language. reply __loam 1 hour agoparentOn top of this I recommend languagejones on YouTube. He's a linguistics PhD that gets into things like how African American Vernacular English is actually more complex than regular English, intentionally obfuscated from white English, and has fast moving slang to stay ahead of adoption of the slang into the wider culture. It's pretty fascinating stuff and he does a lot to show how thinking some of this stuff is incorrect is actually just ignorance. reply cs702 3 hours agoprevReading this, I come away with the impression that European languages today are evolving due to the influence of \"Vulgar English\" (the lowest common denominator of English spoken by the most people worldwide), analogously to how Romance languages like Spanish and French evolved in the past due to the influence of \"Vulgar Latin.\"[a] --- [a] https://www.britannica.com/topic/Romance-languages reply pjmlp 2 hours agoparentYou forgot the part when it was cool to speak French among higher classes, and thus it got spread into many European languages as well. reply adamc 2 hours agoparentprevLanguages change. I'm always amused that we get upset by that, but it's going to keep happening. reply xdennis 2 hours agoparentprev> Romance languages [...] evolved in the past due to the influence of \"Vulgar Latin.\" Minor correction: they are derived, not influenced by Vulgar Latin. That's why so many words are different from Classical Latin, but similar between Romance languages. Like how Latin for house is \"domus\", but Romance languages use casa/casă/chez because common people referred to their house by the word \"casa\". reply wrzuteczka 1 hour agorootparentWeird twist: Slavic languages use words very similar to \"domus\" for \"house\", for example, \"dom\" in Polish or \"дом\" in Ukrainian. reply Wytwwww 25 minutes agorootparentAlso Ancient Greek, Albanian, Sanskrit, Ancient Iranian etc. Supposedly even \"timber\" (in English) is somehow derived from the the same root .. reply Muromec 1 hour agorootparentprevThat's дім, not дом, but of course it's \"вдома\" and \"у домі\" for reasons. reply almostnormal 4 hours agoprevSpeaking of the English language influencing German, I want my Erdbeermarmelade back. I don't care that english marmalde cannot be made of strawberries. reply greenicon 3 hours agoparentIt seems the EU laws for this have changed last year [1], to allow Erdbeermarmelade again. 1: I was only able to find something in German: https://www.wiwo.de/politik/ausland/realsatire-aus-bruessel-... reply zelphirkalt 2 hours agorootparentPhew, that was close. What would we do without our Erdbeermarmelade. reply yamazakiwi 3 hours agoparentprevWhy would you not be able to make it out of Strawberries? You can make Marmalade with any fruit. :) reply xandrius 2 hours agorootparentGenerally marmalade is made out of citrus fruits. reply yamazakiwi 2 hours agorootparentOf course but it doesn't have to be. There are tons of fruits that make great tasting marmalades. After watching the Mexican episode of British Bake Off I don't care about their opinions on food authenticity. reply OptionOfT 3 hours agoprevAustria and German-speaking Switzerland, lists “Eva’s Blumenladen” (Eva’s Flower Shop) and “Peter’s Taverne” (Peter’s Tavern) as usable alternatives, though “Eva’s Brille” (“Eva’s glasses”) remains incorrect. Why is 'Eva's Brille incorrect', but 'Eva's Blumenladen' ok? reply maxnoe 3 hours agoparentBecause, according to the new rule, it's only permitted in proper names \"Eva's Blumenladen\" is the proper name of the shop, what is put on the sign above the door. \"Evas Brille\" is just Eva's glasses. reply OptionOfT 3 hours agorootparentOh shoot, I read it as Eva's Glasses, the name of her eyewear shop. reply qwertox 1 hour agorootparentThat would be \"Eva’s Brillenladen\". reply Semaphor 1 hour agorootparentThat’s not a given, the store could easily be called \"Eva’s Brille\", I’d say that’s even more likely than the archaic sounding Brillenladen. reply abeppu 3 hours agorootparentprev... but even before the rule change, in virtue of being a proper name, if the proprietor calls it \"Eva's Blumenladen\", and it's marked as such, wasn't it proper usage to refer to it that way? If I call my English business, \"Joes Cafe\" (intentionally not using an apostrophe), wouldn't it be incorrect for people to refer to it in writing as \"Joe's Cafe\"? reply jameshart 2 hours agorootparentAbsolutely. You don’t need to come up with fake examples. Take a couple of high end British retail establishments: Harrods and Selfridges, founded by Messers Harrod and Selfridge, and neither styles itself with an apostrophe. reply colanderman 3 hours agoparentprevThe former are names of businesses, the latter is just referring to someone's personal item. (This confused the heck out of me at first too.) reply aeyes 1 hour agorootparentBut I can style the name of my business however I want, \"E'v'a's Blumenladen\" is correct because I say so. I don't need anyones approval. reply carlmr 1 hour agorootparentIn Germany you need approval for business names. reply cardiffspaceman 1 hour agoparentprevCAESARS is correct because it’s a palace of Caesars, not of the Caesar. Something to think about while you’re stuck in Las Vegas traffic. https://www.caesars.com/caesars-palace/things-to-do/nightlif... reply croes 3 hours agoparentprevPeter’s Taverne is the name of the tavern, Eva's Brille is just eva's glasses not a the name of her store. So Evas Blumenladen is called Eva's Blumenladen is correct. reply mlinksva 3 hours agoprevMaybe English/Globish should go in the opposite direction. Apostrophes, at least for the genitive case, are awfully annoying: curly/non-curly, extra character, not pronounced, uncouth... https://en.wikipedia.org/wiki/Apostrophe#Criticism reply ashishb 45 minutes agoprevThere are only two approaches to languages that the world will follow. One is the Chinese approach, where you create a big geographical entity that speaks your language. The other is the European Laissez-faire approach of respecting a plethora of languages with few speakers which is worthless for most foreigners to learn, all of your mini languages die and get replaced by English. https://ashishb.net/short-stories/prague-airport/ reply mattmaroon 5 hours agoprevIn America an idiot’s apostrophe is when someone uses it to pluralize words and it too seems to be growing in use. reply waterproof 5 hours agoparentIn the article they call that the “greengrocer’s apostrophe” as in “twelve potato’s”. reply itronitron 4 hours agorootparentI don't know that I have ever seen that in the wild, but probably only because I refuse to accept that it exists. reply pivo 3 hours agorootparentI saw it in a Home Depot in the US once. It was father's day and there was a sign that read, \"Dad's Love Tools\". Of course they meant to say, \"Dads Love Tools\". I thought it was particularly funny and embarrassing for the store, but I couldn't get the clerk at the store to understand what was wrong. reply kreyenborgi 3 hours agorootparent> but I couldn't get the clerk at the store to understand what was wrong. I would have loved to watch that conversation :-) reply SoftTalker 1 hour agorootparentprevClerk probably had nothing to do with it, could not change it, and didn't care anyway. reply kyleee 3 hours agorootparentprevDad’s Love Tool strikes again reply onlypassingthru 2 hours agorootparentDad's Love Tool is why he's a dad. reply nobody9999 2 hours agorootparentprev>Dad’s Love Tool strikes again That's what she said! reply ToucanLoucan 3 hours agorootparentprev> I couldn't get the clerk at the store to understand what was wrong. Not surprising. Tons of Americans are borderline illiterate. It's one of many things that makes it annoying to live here, especially as the amount of communication done in text increases with more advents in technology. I recall reading somewhere that the standard reading level for the states is about sixth grade, and if anything that comes across to me as slightly generous. Honestly this is one of my few hopes with the proliferation of LLM: that it will make reading communications from other workers less utterly painful. reply Wytwwww 12 minutes agorootparent> proliferation of LLM: that it will make reading communications from other workers less utterly painful. By somehow magically inferring what the person was trying to say and padding it with pointless verbosity? I'm afraid we'll need to wait for Neuralink 20.0 to solve this problem... reply itronitron 1 hour agorootparentprevEven among the highly educated, it's shocking how resistant some of them are to written communication. I used to wonder if there was something wrong with my email, then I considered maybe they were likely busy, indifferent, or lazy, and now I wonder if they are just barely functionally literate so that drafting a response induces a significant mental burden. reply oneeyedpigeon 4 hours agorootparentprevI believe the term originated here in the UK, where it's actually pretty common. Although, ironically, greengrocers aren't so much anymore. reply automatic6131 4 hours agorootparentprevIf you could of seen some of the spelling mistakes I of, you would of run away screaming, and there's nothing else you should of done. reply mckirk 3 hours agorootparentI literally could care less. If people cant handle this alternative way of communicating, thats there problem. reply names_are_hard 3 hours agorootparentSomewhere an LLM is being trained and consuming this thread. Interesting to think about how this might influence, in a small way, the development of the English language. reply Tepix 1 hour agoparentprevIn dutch, it's not even a mistake for certain words ending with vowels. For example \"Photo's\". reply forinti 3 hours agoparentprevI see this a lot in Brazil, and English is not even our language! reply phito 2 hours agorootparentFrench people do make this mistake a lot too. reply Tabular-Iceberg 2 hours agoprevFine, but using a ` or ‘ instead of a ' or ’ should be an arrestable offense. reply Tepix 1 hour agoparentIt's not fine. It's still a disgrace. But i agree, using accents or backticks is so much worse! reply randomtoast 5 hours agoprev‘If a lie is only printed often enough, it becomes a quasi-truth, and if such a truth is repeated often enough, it becomes an article of belief, a dogma, and men will die for it.’ - The Crown of Life (1896) reply gerikson 4 hours agoprevWe see these in Swedish too and they're just as incorrect, grammatically. But the worst thing is usually the acute accent is used instead of a real apostrophe, which just makes it stand out even more. reply aejfghalsgjbae 2 hours agoparentYou get that a lot in Germany and the grave accent too, as with \"Rosi`s\" in the article image. I guess the acute accent is laziness because unlike the apostrophe, it doesn't need the shift key on a standard German keyboard layout. The grave accent is at shift+´ so just weird. reply kemiller 26 minutes agoprevEnglish takes influence from other languages left and right, but there's always someone salty when it goes the other way. reply booleandilemma 16 minutes agoparentThat dynamism English has is something that makes it such a great language, imo. reply qwertox 1 hour agoprev> [...] though “Eva’s Brille” (“Eva’s glasses”) remains incorrect. Where would be the fun if there's no exception to the rule in the German language. reply Etheryte 1 hour agoparentIt's like German humor, it's no laughing matter. reply anigbrowl 1 hour agoprevOne of the annoying things about widespread use of computers has been the butchering of apostrophes by 'smart' quotes because programmers won't take the time to develop understanding and appropriate user interface, eg 1979 -> [open quote]79. Even if the user knows it's wrong, getting the computer to use the closing quote mark instead of 'correcting' it is a trial. reply subpixel 3 hours agoprevI lived in Germany on two occasions and regularly consume German media. English words are all over the German vernacular, to the point that it's really, really annoying. reply guitarbill 2 hours agoparentIt's pretty crazy how quickly it happened/happens. The Deppenapostroph is maybe less problematic; I see it more as a simplification just like the dative replacing the genitive. But Denglish really just makes everything harder to understand; even if you are fluent in both English and German the \"switching\" is tiresome. Still, maybe we should get rid of \"handy\" and \"beamer\" first... Ironically, even British English has the issue of Americanisms sneaking in, see e.g. the IT Crowd episode: \"How hard is it to remember 911?\" \"You mean 999? That's the American one\". reply pjmlp 2 hours agoparentprevAs someone living here for the last 20 years, and also nowadays understands a bit of dialects and related slag, it is kind of curious the amount of Denglish words among the youth. For example, \"Hast Du das geprüft?\" quickly turns into \"Hast du das gecheckt?\". reply zolbrek 2 hours agorootparentI usually find it amusing. „Was meinst, kriegen wir das hin?“ „Safe Digga, das ist so was von easy.“ And they think they're so cool talking like that. The part that irritates me though is when I try to pronounce Denglish stuff with a German accent and the Germans end up not understanding me. I made a joke about strippers once and got only blank looks, then one guy said, \"oh, you mean strippers,\" pronouncing it the way you'd say it in English as best as he could. I had pronounced it schtrippas. reply guitarbill 2 hours agorootparentprevHuh, \"Hast du das gechecked?\" used to mean \"Hast du das kapiert?\", with a quite negative connotation. reply pjmlp 1 hour agorootparentDepends on the context, at least based on my experience from 1live and Cosmo interviews, and some TV series. reply hilux 2 hours agoparentprevSimilar in India. E.g. you cannot find a news report in Hindi that doesn't contain at least a smattering of English words. reply meling 50 minutes agoprev> lists “Eva’s Blumenladen” (Eva’s Flower Shop) and “Peter’s Taverne” (Peter’s Tavern) as usable alternatives, though “Eva’s Brille” (“Eva’s glasses”) remains incorrect. I didn’t understand why Eva’s Brille is incorrect. Anyone understand the difference? Is it only allowed for commercial entities? reply eleveriven 57 minutes agoprevFor many, language is a key part of cultural identity, and changes in grammar can feel like a threat to that identity. reply thrownawaysz 54 minutes agoparentBut we should all bow down to the american cultural imperialism because why not reply renewiltord 52 minutes agorootparentWhether you should or not is irrelevant because you will. No imperialism deigns to ask. reply Tainnor 2 hours agoprevWhile I personally dislike this for \"aesthetic\" reasons, I do recognise that languages change and that's fine. It used to be that very few people would read and write, but with the advent of the internet, text messaging, etc., written language is also evolving more \"democratically\", similar to spoken language. There are also technological forces at work: I've mostly given up on writing compound words the proper way on mobile phones, because it just doesn't work well with autocompletion, for example. That said, I really dislike how \"bureaucratic\" German spelling rules are, including this recent addition. Instead of blanket allowing the use of an apostrophe for the genitive (at least for personal names), the new rule allows it only in very specific circumstances. I'm of the opinion that nobody should have to consult a complicated rulebook in order to write well (in fact, the best way is to just simply read a lot and then mimic what you read). Then again, most people don't need to care about what is or isn't considered proper spelling. In theory it should matter for official documents etc., but that doesn't mean that those never contain errors (quite the contrary, in my experience). reply carlmr 1 hour agoparent>the new rule allows it only in very specific circumstances. I'm of the opinion that nobody should have to consult a complicated rulebook in order to write well Exactly. If you ask me it kind of makes sense to have it for possesive (not plural) use anyway. It clarifies that the s is not part of the name but serves a different function. reply oniony 3 hours agoprevI read somewhere that the apostrophe in English was only used to show elision, but that in Old English the genitive form changed the word ending to 'es', so the apostrophe was just indicating the 'e' had been removed. For example 'hund' (dog) becomes \"hundes\" in the genitive form and was written \"hund's\" when the 'e' was elided. reply asimpletune 2 hours agoprevEnglish had been so heavily influenced by European languages that it’s just a funny coincidence that we’re alive to see the opposite happening. It feels like half (or more) of Italian words have English cognates or they’re so close you could consider them a slant cognate. Another thing is that what happened in the article is something that has occurred a lot in English too. I think a few years back they permitted “myriad of” just because it was so common a mistake. This happened even though myriad is supposed to be used exactly like the words “numerous” or “many” and shouldn’t be followed by an “of”. Still, despite having simple examples of similar words, like numerous, people just couldn’t stop saying “myriad of”. I see it all the time now. I wouldn’t say I love the change, but I don’t get upset about it or correct people, since it’s technically perfectly alright now, even if it’s accepted for sort of a sad reason. reply everforward 2 hours agoparentI believe using \"of\" is correct when using \"myriad\" as a noun, the same as \"many\" or \"number\" (the noun form of numerous). \"He had a great number of seashells\", \"she possessed a myriad of skills\". Apparently it used to/still means 10,000 so it should be usable anywhere 10,000 is. \"There were a myriad of them\"/\"there were 10,000 of them\". reply asimpletune 2 hours agorootparentO that’s funny. Apparently it was originally a noun, hundreds of years ago. It actually changed into the adjectival use I was referring to earlier in the 1800s. If anything it seems that using myriad as an adjective was actually an example of a rule change made to accommodate how people were speaking at the time. reply everforward 1 hour agorootparentLanguage is certainly a fascinating thing. The adjective form of \"legion\" always throws me off, like in the Anonymous slogan (\"we are legion\"). Off topic, but now I do kind of wish the Magic: The Gathering mechanic was named \"Legion\" instead of \"Myriad\". reply p3rls 48 minutes agoprevWhat bothers me is when people confuse ` grave accents with apostrophes ', to the grammar gulags with the lot of 'em. reply mglz 4 hours agoprevPeople get upset about this while you encounter statements like \"life your live\" regularly, sometimes even on TV. reply cdrini 2 hours agoprevI love to see language evolve like this! Dictionaries and grammar rules are not prescriptive, they follow what/how people speak, not the other way around. They do influence in the opposite direction as well, as a bit of a normalizing/consolidating force. I feel like for the last few decades (at least) we've treated dictionaries like gospel, with very strict, almost mathematical definitions of \"correct\". I think giving a bit of freedom to allow new words/etc to develop naturally, like they have since the dawn of human language, is quite nice! I.e. Make fetch happen! reply ghayes 2 hours agoparentJust to clarify, I think you have the terms reversed. Descriptivism is, as you say, describing a language from its everyday usage. Prescriptivism is when you follow a rules body or dictionary to say what is “correct.” reply cdrini 2 hours agorootparentDarn thank you for the correction! I forgot a \"not\" in there. Fixed! reply sigmoid10 2 hours agoparentprevtru dat! Language be evolvin like craZy n we shuld just roll wit it! Dictiunarys try to tell us wut 2 do but we aint gotta lissen! Its lyk, who needz rulez when we can make up wordz as we go, amirite? Letz just keep makin fetch happen!!! reply umanwizard 1 hour agorootparentLanguage is a social and cultural phenomenon. That doesn’t mean there are no rules. It means that the rules are implicitly decided collectively by the community of speakers rather than by a centralized body. reply sigmoid10 1 hour agorootparentIf you spend enough time among the right communities, you'll find tons of people speaking that way or any other way. Especially among the uneducated demographics. The very same ones that created this new rule in the original article (hence the name \"idiot's\" apostrophe). Now should we listen to them or not? I see highly conflicting statements here. reply cdrini 2 hours agorootparentprevHahaha within reason!! I want to see eg authors just introduce new words in books, like Shakespeare. New words shouldn't generally make the language less internally consistent (goodness knows English has enough of a problem with that as it is!). I mean new words like... \"He was a carrapticious old fellow. Always alert, and, despite his old age, had the mischievous sparkling eyes of a boy who has just told a bad joke\". reply sigmoid10 1 hour agorootparentBut who's to say which new words are \"acceptable\" and which aren't? Of course if you ask old people they will give these Shakespearean answers, but they are not the ones defining the future of a language. It's the young generation. And they have a very different approach to creating new words. Why should their new words be worth less? And the example in the original post is actually the worst kind according to your definition, because it makes the language less consistent. reply cdrini 34 minutes agorootparentI think the people decide; if folks like a certain word, they'll start using it, creating traction. A natural selection of words of sorts. Then the dictionaries, being non-prescriptive, will have to add those words since they're needed to understand common parlance. And completely agree about young generations, I've actually been super pleased at how many new words gen z is creating! I feel like the previous few generations created way fewer words. I disagree with things like introducing inconsistent spellings like \"lyk\" in terms of adopting that as a standard, because it just makes the language a headache to learn. But creating words for things that don't have existing words (like carrapticious in my other example), or even creating new sort of word variations which kind of grow/evolve into their own words (like rizz) seem like a nice expressive way of extending language. (I'm a bit more mixed on the value of the latter, though). reply rob74 2 hours agoprev> The new edition of the Council for German Orthography’s style guide [...] lists “Eva’s Blumenladen” (Eva’s Flower Shop) and “Peter’s Taverne” (Peter’s Tavern) as usable alternatives, though “Eva’s Brille” (“Eva’s glasses”) remains incorrect. So they didn't actually simplify it - they made it more complicated? But my single largest pet peeve with the original reform is that they \"outlawed\" the use of the English plural form for loan words like \"Party\". In German, you are now supposed to write \"Partys\", \"Parties\" is incorrect. Bet they didn't change that... or did they? reply ars 47 minutes agoparentCan you explain what's different about Glasses vs Tavern, where it's allowed in one and not the other? reply Wytwwww 1 minute agorootparentSupposedly names of businesses and/or public signs are required to comply with certain rules? Which isn't surprising considering the love Germans have for pointless bureaucracy... So I guess this is some sort of a (certainly not arbitrary) compromise to appease both sides. reply zelphirkalt 3 hours agoprevBackground: In German you would not add an \" ' \" when you want to express something belonging to something. You would simply add an \"s\" in most cases. Example: \"Marias books are at home.\", not \"Maria's books are at home.\" reply pjmlp 2 hours agoparentIt is actually explained in the article, with mention to Barbaras Rhabarberbar. reply glitchc 4 hours agoprevMeanwhile in English we have our own apostrophe catastrophe where it's become commonplace online to add one to a plural. reply FireBeyond 4 hours agoparentOr the Americanism of double quotes for emphasis: > Bob's \"Big\" Bookstore! reply bee_rider 3 hours agorootparentI’ve always read those as scare-quotes, like the store is making fun of itself in some self-aware fashion for not being big. I know I am wrong but I would rather be wrong is a slightly funnier and less stupid world. reply morsch 3 hours agorootparentprevAlso quite common in Germany. I choose to interpret them as scare quotes implying irony and smile to myself at Bob's \"Big\" Bookstore and the grocer selling \"fresh\" fish. reply kristjansson 1 hour agorootparentprevEven worse with food. > Bob's \"Best\" Burgers what's wrong with the burgers?? reply pfdietz 3 hours agorootparentprevIs that for emphasis, or is that to cast doubt on the word? reply g-b-r 3 hours agorootparentprevIs that really common? reply lagniappe 2 hours agoprevAlso, another common one I see more these days is \"it's\" where it should be \"its\". If you can not substitute a usage of \"it's\" for \"it has\", \"it was\", or \"it is\", then you meant to say \"its\". It is hard to even be angry about it, I think the language should be changed so that any instances of \"its\" -> \"it's\" to eliminate the exception. reply cdrini 2 hours agoparentStrong agree. As a kid, I remember I'd have to pause for like 5s every time I wrote \"it's\" or \"its\" to try to remember what was correct. And it doesn't help that what I'd usually remember is \"well apostrophe s denotes possession usually, so surely apostrophe s denotes possession here as well\". But alas not. I think always having the apostrophe makes way more sense. (Nowadays I just don't double check it and it's basically a cointoss if I get it right or not :P) reply Supernaut 2 hours agoparentprevIn certain academic circles, it's been popular for several decades now to exhibit indifference to the general decline in observance of rules such as this. I find that attitude very regrettable. I can't be the only person who has found themselves having to repeatedly re-read a passage of text to discern its meaning, because the author is ignorant of, or indifferent to the use of apostrophes and/or other forms of punctuation. These aren't arbitrary rules, for the most part: they came into existence to assist with reading comprehension. The clarity of expression afforded by modern English is a great gift, and I strongly believe that allowing it to degenerate by abandoning these (very simple!) rules will serve only to make written English less expressive and more opaque. reply bitwize 2 hours agorootparentMy wife still fumes that they don't make kids type two spaces after sentence-terminal punctuation anymore. And she still hasn't processed that periods at the end of sentences are, in certain contexts, considered inappropriately arrogant and to be avoided. reply bitwize 2 hours agoparentprevSay that you have i-t followed by apostrophe s, now what does that mean? You would not use \"it's\" in this case! As a possessive It's a contraction What's a contraction? Well, it's the shortening of a word or group of words by omission of a sound or letter. -- \"Weird Al\" Yankovic, \"Word Crimes\" https://www.youtube.com/watch?v=8Gv0H-vPoDc reply rootusrootus 2 hours agoprevHa, from the picture in the article I thought they were talking specifically about the backtick as an apostrophe. I could totally call that the idiot's apostrophe, but in a completely different context than what the Germans are talking about. reply btbuildem 1 hour agoprevHa. \"greengrocer’s apostrophe\" is a very polite way of referring to those who put an apostrophe before every trailing \"s\". reply NotYourLawyer 17 minutes agoprevIf y’all don’t like the English style, maybe you should have tried harder and won WWII. reply lovecg 3 hours agoprevOh well, at least we don’t need help highlighting every noun in a sentence. reply croes 3 hours agoprevThat's like changing the spelling of Espresso to Expresso reply deng 3 hours agoparentIt's not, unless you are able to hear a difference between \"Tea's Buchladen\" and \"Teas Buchladen\". reply deng 3 hours agoprevHere, too? As the articles notes, this kind of apostrophe has been \"correct\" for many, many years, at least for names, and no, not just for avoiding confusion with names ending in 's'. The \"Duden\" (one of the officially recognized authorities for German spelling) has had the example \"Willi's Würstchenbude\" for many years, despite \"Willis\" not being a common name in Germany. Now that one tries to simplify things, the Cliff Clavins of Germany freak out because they lose one example where they could feel smarter than others. There really is nothing to see here. reply pflenker 2 hours agoprevI approve this as it acknowledges the fact that language is not static. Next up, in my opinion, will be the _Deppenleerzeichen_, the idiot space, between two nouns. This space has rapidly gained in usage thanks to auto correct - it’s easier to use auto correct if you add a space after every noun. Given how the apostrophe thing is received I expect no less than riots and burning tires in the streets once that one is officially allowed. reply allemagne 2 hours agoprevThis makes me really curious how quickly German has actually evolved over time. My assumptions would be entirely informed from extrapolating from historical context and not knowing anything about German. So there was probably a lot of linguistic diversity before unification in 1870, then there would have been a standardization effort started by Bismarck (favoring the dialect predominantly spoken in Prussia) which would carry through WW1, would be relaxed during the Weimar republic, would intensify (or turn into something bizarre and Orwellian) in the Nazi era, and then a slight divergence between East and West Germany in the Cold War. Under this, my rough hypothesis would be that German has actually changed a lot less in the post-WW2 era, especially since the 90s, than it would have in the period before. Is this roughly how things shook out? I'd be really interested where this is completely wrong. reply leipert 2 hours agoparentOh, big official orthographic reforms happened in 1944 and then in 1996. So that happened in the 90s, and a few minor revisions after that. A lot of English vocabulary (technology but also every day life) had an influence on German, especially in Eastern Germany post-reunion. An example: Most people born after 1990 probably invite you to a Geburtstagsparty instead of a Geburtstagsfeier. Compared to the after-war generations, hyper-local dialects probably faded out as bit as well. If I talk to people from my grandparents generation, there were sometimes difference in terms even though people just lived a few villages apart. Biggest development I am happy about, is that the capital ẞ is probably becoming official during my life time. reply RegnisGnaw 4 hours agoprevWhat they need is learn from Canada and have a language police. Go around and fine business that don't follow the rules of German properly. reply umanwizard 1 hour agoparentNothing like what you described exists in Canada. You are presumably referring to the OQLF (a provincial institution, not a “Canadian” one) which enforces French as the dominant public language in Quebec. Given that Quebec, despite being surrounded by the Anglosphere, hasn’t ended up like Louisiana or Ireland where French or Irish as the primary native language is a distant memory, suggests that their efforts are successful. reply Muromec 1 hour agoparentprevIf language police is what I think it is, it doesn't fine for bad command of the language, but rather for using the wrong language after being asked nicely twice. reply forinti 3 hours agoparentprevYou can bet old German ladies already point out people's mistakes. reply RegnisGnaw 3 hours agorootparentDo they go around threatening fines and suspension of their business license? reply pjmlp 2 hours agoprevExtra points for the Barbaras Rhabarberbar mention. reply mattlondon 3 hours agoprev\"idiot's apostrophe\" or to call it another way \"how the English speakers do it\" is quite offensive to a native English speaker! Thanks Germany. reply subpixel 3 hours agoprevJust wait until they get ahold of the quotation marks: https://www.reddit.com/r/suspiciousquotes/ reply surfingdino 1 hour agoprevIt won't make German jokes any funnier. reply alex_john_m 4 hours agoprevI say the EU should adopt the Romanian language. It's spoken the same way as it is written. That should solve all spelling problems forever. :))) reply rafram 3 hours agoparentNo natural language is actually 100% phonetic. Romanian is no exception. Romanian spelling and pronunciation are close to phonetic, but the same is true of German. reply Tainnor 2 hours agorootparentA writing system being phonetic would be impractical, because most languages have tons of little phonetic alterations of individual sounds depending on position in the word/syllable, regional variation, etc. What you usually want is that the writing system be phonemic, i.e. that there is a 1:1 correspondence between phonemes (meaningfully distinctive sound units) and characters. Unfortunately, languages evolve, so even if your writing systems starts out as more or less phonemic, over time the sounds of the language will drift and inertia will usually keep the writing system not fully in sync with these changes. This is particularly bad in the case of English, where there's never been a proper spelling reform accounting for the corresponding sound changes. reply OkayPhysicist 1 hour agorootparentEnglish should just abandon differentiating vowels all together. All dialects of English shwa unemphasized vowels to some extent, and the different dialects largely boil down to how we pronounce various vowels. J`st ch`nge `m t` di`cr`t'c m`rks, `nd `t's st`ll p`rf`ctl` l`g`ble reply Muromec 1 hour agorootparentreject the alphabet and stop writing vowels unless really needed. reply badmintonbaseba 3 hours agorootparentprevHungarian gets pretty close too, but yeah, there are exceptions. reply xdennis 2 hours agoparentprevIt's not though. It's much more regular than english, but there are a lot of issues (which were addressed in the past). Take for example the sentence \"Ea ia ia\". It's pronounced /ja ja ia/. Some examples: * x exists and it's not clear if it's pronounced /ks/ or /gz/. * e is sometimes pronounced /je/ * h is pronounced as /x/ sometimes and Romanians don't realize this. E.g. hrană is [ˈxra.nə] even though people think they say [ˈhra.nə] * i is the worst letter in Romanian. It has three pronunciations: /i/, /j/ and /ʲ/. Take for example \"copiii\". Is it pronounced /kopiji/, /kopiii/, /kopʲji/? Nope, it's /koˈpi.iʲ/ . In the past /j/ and /ʲ/ were written with ĭ making things a bit easier. * Stress is not written which causes confusions between words like \"muie\" /mu'je/ (softened) and \"muie\" /'muje/ (blowjob) * /ɨ/is written as both î and â based on some stupid rule to preserve România being writen as România instead of Romînia. This is to remind foreigners that we were once Romans, but it's pointless because most foreigners think Romania means \"land of Roma (gypsy) people\". I've heard that Serbian in Cyrillic is very phonetic though. reply bagels 5 hours agoprevThe signs are in English, why does it matter how they would be written in German? reply lxgr 5 hours agoparentHow so? \"Harald's Eck\" (one of the examples from the article) doesn't sound very English to me. reply patrickmcnamara 5 hours agoparentprevThe signs are not in English. reply maxnoe 3 hours agoparentprevHow is Blumenladen English? reply thomastjeffery 3 hours agoprevFor an article about linguistics, that title sure is hard to read. reply theandrewbailey 5 hours agoprev> The Deppenapostroph is not to be confused with the English greengrocer’s apostrophe, when an apostrophe before an ‘s’ is mistakenly used to form the plural of a noun (“a kilo of potato’s”). Grocer's apostrophes annoy me, along with words like \"advices\" (advice is an abstract noun and can't be plural, like \"happiness\") and \"learnings\" (use \"lessons\" instead). reply praptak 5 hours agoparent\"Learnings\" is more than annoying corpspeak though. It's a word so old that you can find usage from time it was spelled \"lernynges\" (\"lernynges whiche Cathon gaf to his sone\") https://english.stackexchange.com/questions/118379/first-use... reply vundercind 5 hours agoparentprevCorporate English is a blight. So many ugly substitutions, overloading existing words, for perfectly good and common words we already have. reply mwigdahl 5 hours agorootparentIt is my belief that you may not fully leverage the synergistic potential of the value-added verbosity inherent in corporate linguistics. reply daveslash 5 hours agorootparentAgreed. It's very important to maximize cross-functional coherence to capitalize on strategic imperatives for maximal growth opportunities. reply sokoloff 3 hours agorootparentprevUnless there’s a fulcrum involved, I don’t want to read the word “leverage”. reply wizzwizz4 3 hours agorootparentThe fulcrum is externalities. Good ask, though! reply lxgr 5 hours agorootparentprevLet's double-click on that: There's value in expensive signalling, and sometimes the expense is an intentional (or at least tolerated) lack of aesthetics. reply vundercind 5 hours agorootparentWe should take this offline so we can really zoom in. reply Muromec 1 hour agorootparentprevCorporate life is just really boring and this kind of nonsense is understood as fun by people engaging in it. reply okeuro49 5 hours agoparentprev\"informations\" is a good one for non-native English language speakers to look out for. reply psychoslave 4 hours agorootparentBut did you know that there huge load of \"datas\" out there? reply jjk166 3 hours agorootparentThat one's a bit mean given that data does have a distinct plural, it just happens to be spelled the same because whoever came up with english didn't really grok the phonetic alphabet. reply inkcapmushroom 2 hours agorootparentDatum is the singular, which is one point of data. When you group together a datum with another datum, they become data. reply jamincan 1 hour agorootparentAnd the origin of that plural form comes from Latin. reply nobody9999 3 hours agorootparentprev>That one's a bit mean given that data does have a distinct plural, it just happens to be spelled the same because whoever came up with english didn't really grok the phonetic alphabet. Isn't 'data' already plural, with 'datum' being the singular of the plural 'data'? reply pjot 5 hours agorootparentprevAs is “codes” - I hear this often from non-native colleagues reply ben_w 5 hours agorootparentWith regard to source code(s), I hear it exclusively from scientists, regardless of nationality. None of the coders I've worked with (and I'm in Berlin) have put the s on the end of code. Quite a few will use \"he\" to describe inanimate objects, though: \"I spilled coffee on the table and now he is wet\", that kind of thing. (This is still better than my German, which is embarassing given how long I've been here) reply bee_rider 3 hours agorootparentI think it makes sense, like a scientist might think of their codes as discrete things, because one code was written for each experiment. The work-product is the experiment, the codes are just little things that make it happen. reply marssaxman 2 hours agorootparentprevIn scientific computing, they tend to say \"code\" where the rest of us would say \"program\". reply generic92034 2 hours agorootparentPrograms? I rarely hear it these days. It's all \"apps\". ;) reply samatman 1 hour agorootparentprevHearing \"codes\" generally means you've found yourself around Fortran. Simple as. reply Spivak 5 hours agorootparentprevThis one is honestly confusing. * Error codes — correct. * Their personal codes — correct. * Multiple codes of conduct — correct. And then computer code is used roughly like the noun 'writing' except you can say writings where appropriate. reply pjot 4 hours agorootparentThe difference lies in how we conceptualize the noun: - Computer code is seen as a continuous substance or body of work, like \"writing\" or \"music.\" - Other types of codes are seen as discrete units or systems. It's similar to how we say \"information\" (uncountable) but \"facts\" (countable), even though they're related concepts. reply jameshart 2 hours agorootparentYes, code is a substance, like sand or iron. The system is built out of code (not codes), just like the table is built out of wood (not woods). Hearing someone talk about ‘codes’ has the same weird vibe as when they talk about ‘Legos’. reply Ekaros 2 hours agorootparentprevSource codes of dependencies. Might be acceptable... Also why not pluralise all words? Sources codes. reply nanna 5 hours agorootparentprevNonsense. Informations has long been used in English. I have before me a letter by Albert Einstein to Norbert Wiener regarding a young Kurt Eisemann, in which Einstein writes, \"From his letter enclosed here, you will get informations about his life and studies before he arrived here.\" And in the Princeton translation of Aristotle's Constitution of Athens one finds, \"The Eleven also bring up informations laid against magistrates alleged to be disqualified\". Informations is perhaps a bit obscure but it's perfectly valid. reply psychoslave 4 hours agorootparentWell, for what I red, Einstein primary language stayed German all life through (Information/Informationen). And he learned English rather late in life, starting at 34 apparently.[1] And while not speaking German, he was more likely to practice some Italian as a spontaneous expression desire (informazione/informazioni) and did practice French well enough to give a lecture in this latter language (information/informations). [1] https://www.lingalot.com/what-languages-did-albert-einstein-... reply zztop44 5 hours agoparentprev“Learnings” has a potentially useful nuance, referring specifically to whatever it is one took away from a lesson. I know the word “lesson” itself can also cover that meaning, but “learning” is more specific and given how widely it’s used, that specificity appears to be useful in some circumstances. reply ivan_gammel 5 hours agoparentprevThose plural forms are sometimes referred to as European continental dialect of English and do not raise questions here. If we, Europeans have to use English as lingua franca, we can and we will adapt it to our needs same way as Americans, Afro-Americans or Indians did. So my advice: just get used to it. Edit: cultural possession of language is nonsense, it belongs to all speakers, native and non-native alike. Germans must get used to foreign influence on their language too and Ukrainians should stop fighting Russian language and start writing their own rules for it (what can piss Moscow more?) reply Semaphor 5 hours agorootparentI had never heard of that, but Wikipedia has similar examples in \"Euro English\" [0], though there it is because similar words exist with s in other languages. I wonder if something like \"advices\" exists in another language? [0]: https://en.wikipedia.org/wiki/Euro_English#Inflection reply hydrolox 5 hours agorootparentI think you can say it does? Ie in other languages, the plural of advice (which in English is advice, \"I gave him a lot of advice\") is spelled differently(with the plural ending). From my personal knowledge, in Russian advice is совет and \"advices\" (or advice pl.) is советы. In Spanish, advice is consejo and there is a plural consejos. This can probably be also translated (in both cases) as \"tip\" and\"tips\" or something similar. reply Semaphor 3 hours agorootparentYeah, but at least the examples wikipedia has, are for similar words, not just applying random other grammatical rules: informations (French) -> informations (English) compétences -> competences reply t0mas88 5 hours agorootparentprevThe Dutch may have some \"rights\" to adapt English. They're #1 in non-native English proficiency for 5 years in a row and surpassed Canada (considered native speakers) on overall English proficiency some years ago. One point of debate is that English in the Netherlands has become mostly American English over the last decades due to media influence. While originally \"school English\" in the Netherlands was British English. reply umanwizard 1 hour agorootparent> Canada (considered native speakers) A quarter of the population of Canada is in Quebec where the only official language is French and most people would not be considered native English speakers. reply Spivak 4 hours agorootparentprevI actually go full-descriptivist on this and it erases all the posturing. If you're a speaker of English, native or otherwise, and you say or write something purposefully and don't consider it a mistake then it's correct. Wether other people will join you in your new usage is yet undetermined but also doesn't really matter. AAVE is the perfect example of this happening large scale in the real world. reply vundercind 3 hours agorootparentThere are multiple ways to define “correct”. I tend to favor: having the desired effect. This results in a “correct” that is highly flexible, but doesn’t label anything that one happens to choose as “correct”. reply oneeyedpigeon 4 hours agorootparentprev\"Wether\"—I see what you did there... reply oneeyedpigeon 4 hours agoparentprevSimilar—and even more on-topic—I see a lot of non-first-language writers using \"codes\" in a similar way, to describe source code snippets. reply bee_rider 3 hours agoparentprevI’ve never heard “advices” (in the US). Maybe it is a continental Europe thing? They may have surpassed even us, at the art of inventing new words and spellings to annoy the English. reply cpwright 2 hours agorootparentI've heard my wife say it that way because it is a plural in her native language. reply njtransit 5 hours agoparentprevLearning can be the gerund form of the verb “to learn” and isn’t necessarily a noun referring to the abstract concept. reply causi 5 hours agoparentprevIt's bizarre. I hate seeing it as well but if I don't pay attention I find myself typing them for no discernable reason. reply drewcoo 5 hours agoparentprevI would normally expect \"the grocer's apostrophe\" to refer to a single grocer and \"the grocers' apostrophe\" to refer to a plural group of grocers, which I assume is what you intended. reply oneeyedpigeon 4 hours agorootparentThe term \"(green)grocer's apostrophe\" refers to the misuse of the apostrophe in plurals, which seemingly occurs disproportionately on signs in those shops. It's ironic that it contains a tricky-to-place apostrophe. Should the meaning be \"the apostrophe of the greengrocer\" or should it be \"the apostrophe that greengrocers misuse\"? Either works fine. For the same reason, I always have to check whether it's \"mother's day\" or \"mothers' day\" because... it's both! reply raverbashing 5 hours agoparentprevThere is no more misguided use of an apostrophe than to use it to \"create a pronoun's possessive\" as using \"it's\" in place of \"its\" Do you also write \"he's\" and \"she's\" (as possessive pronouns)? No? Then it's \"its\". reply robertlagrant 5 hours agorootparentEr, yes because they're short for he is and she is. You mean \"his\" and \"hers\". reply raverbashing 5 hours agorootparentBut you don't write them like that, you write \"his/hers\" reply oneeyedpigeon 4 hours agorootparentDifferent words. \"His dog\" means the dog belonging to him. \"He's a dog\" means \"he is a dog\". reply raverbashing 3 hours agorootparent> Different words Obviously I meant people don't write \"he's\" as the possessive form of he. Hence they shouldn't write it's as the possessive form of it. reply ben_w 5 hours agorootparentprev\"He's got a head shaped like a massive orange\" \"Who?\" \"Xtk'act'sbu\" \"Oh no, not the Klingon cosplayer!\" reply itronitron 4 hours agorootparentprevSo is this the proper form? -> it's putting the lotion on its skin reply mouse_ 5 hours agorootparentprevhe's/she's going to have to deal with it reply nozzlegear 5 hours agorootparentOr even \"it's going to have to deal with it\" — though hopefully the pronoun refers to a pet or farm animal of some kind, as referring to a human as \"it\" is dehumanizing. reply jjk166 3 hours agoprevReminds me of the story: > A Pan Am 727 flight, waiting for start clearance in Munich, overheard the following: > Lufthansa (in German): \"Ground, what is our start clearance time?\" > Ground (in English): \"If you want an answer you must speak in English.\" > Lufthansa (in English): \"I am a German, flying a German airplane, in Germany. Why must I speak English?\" > Unknown voice from another plane (in a beautiful British accent): \"Because you lost the bloody war\" reply ktosobcy 3 hours agoparentAs a Pole I'm also fairly annoyed by having to use English daily ;) reply phkahler 3 hours agorootparentIn the air traffic example it's required so everyone on the radio can understand everything going on around them. reply falcor84 2 hours agoparentprevI might regret saying it, but I think we as humanity should go back to wars being definitively won, rather than dragging on indefinitely. It's obviously a poor metaphor, but I'm thinking of something like the \"Fifty-move rule\" in chess - e.g. if no significant area changed sides in e.g. 100 days, then we officially redraw the maps (by treaty if possible), cease hostilities and let people rebuild and get on with their lives. reply mattferderer 5 hours agoprevMade me think of this old joke that's been on HackerNews, Reddit, etc for years: The European Commission has just announced an agreement whereby English will be the official language of the European Union rather than German, which was the other possibility. As part of the negotiations, the British Government conceded that English spelling had some room for improvement and has accepted a 5- year phase-in plan that would become known as \"Euro-English\". In the first year, \"s\" will replace the soft \"c\". Sertainly, this will make the sivil servants jump with joy. The hard \"c\" will be dropped in favour of \"k\". This should klear up konfusion, and keyboards kan have one less letter. There will be growing publik enthusiasm in the sekond year when the troublesome \"ph\" will be replaced with \"f\". This will make words like fotograf 20% shorter. In the 3rd year, publik akseptanse of the new spelling kan be expekted to reach the stage where more komplikated changes are possible. Governments will enkourage the removal of double letters which have always ben a deterent to akurate speling. Also, al wil agre that the horibl mes of the silent \"e\" in the languag is disgrasful and it should go away. By the 4th yer peopl wil be reseptiv to steps such as replasing \"th\" with \"z\" and \"w\" with \"v\". During ze fifz yer, ze unesesary \"o\" kan be dropd from vords kontaining \"ou\" and after ziz fifz yer, ve vil hav a reil sensi bl riten styl. Zer vil be no mor trubl or difikultis and evrivun vil find it ezi TU understand ech oza. Ze drem of a united urop vil finali kum tru. Und efter ze fifz yer, ve vil al be speking German like zey vunted in ze forst plas. --- source: https://www.reddit.com/r/Jokes/comments/leq19j/english_to_be... reply vundercind 5 hours agoparentReads like Mark Twain’s short piece “A Plan for the Improvement of English Spelling” https://faculty.georgetown.edu/jod/texts/twain.html [edit] Maybe Twain, anyway. The attribution is dubious, but common. reply nobody9999 2 hours agorootparent>Reads like Mark Twain’s short piece “A Plan for the Improvement of English Spelling” Which is a gem, regardless of authorship. Another related bit associated with Twain is: “whenever the literary german dives into a sentence, this is the last you are going to see of him till he emerges on the other side of his atlantic with his verb in his mouth.”[0] Which, as a native English speaker who learned German, I find both amusing and (mostly) correct. [0] https://www.goodreads.com/quotes/115614-whenever-the-literar... Edit: Added source reference link. reply Semaphor 5 hours agoparentprevAs a German, I must say this is very well done. It went from clear English, over me having to think about every word, to clear English again (though only if I read it out loud) reply indigoabstract 1 hour agoparentprevNice one. This confirms my long held suspicion that German contains a lot of badly spelled Englisch words. Or maybe, it's the other way around? reply _visgean 5 hours agoparentprevI think the conclusion is wrong; that sounds much more like dutch. reply diggan 4 hours agorootparentI think that says a lot about the origin of the joke, which most likely comes from outside Europe :) reply xxs 5 hours agoparentprevTwo beers fine for an old joke! reply cafard 3 hours agoprevGiven the long and deplorable influence of German on American scholarly prose, this inspires only Schadenfreude. I'm not singling out the Germans, mind you. I smirk also when the French complain of American modes of thought polluting their schools: when municipal bureaucrats let contracts not for demolition but for deconstruction, I say that we have injuries to avenge. reply orwin 2 hours agoparent'deconstruction' in the French theory was only use by Derrida as a mean to critique a literary work. What it means is that you only judge",
    "originSummary": [
      "Germany has relaxed its rules on using apostrophes for possession, allowing forms like \"Rosi's Bar,\" which was not traditionally correct in German.- The Council for German Orthography now permits the use of the apostrophe to separate the genitive 's' in proper names, sparking debate over English's influence on the German language.- This change has led to discussions on international influences on German, with some advocating for German alternatives to English terms."
    ],
    "commentSummary": [
      "Germans are debating the influence of English on their language, focusing on the approval of the \"idiot's apostrophe\" in specific contexts, such as business names but not personal items.",
      "This discussion underscores the tension between language evolution and preservation, with differing opinions on whether language should adapt organically or maintain cultural identity.",
      "The debate also reflects the broader global influence of English and the challenges in upholding linguistic standards."
    ],
    "points": 139,
    "commentCount": 329,
    "retryCount": 0,
    "time": 1728479815
  },
  {
    "id": 41784069,
    "title": "What Is LibreDrive (2019)",
    "originLink": "https://forum.makemkv.com/forum/viewtopic.php?t=18856",
    "originBody": "Skip to content Quick links Unanswered topics Active topics Search FAQ Login Register Board index Optical Disc Drives LibreDrive drives Search www.makemkv.com MakeMKV support forum Search Advanced search What is LibreDrive? Locked Print view Search Advanced search 5 posts • Page 1 of 1 mike admin Posts: 4075 Joined: Wed Nov 26, 2008 2:26 am Contact: Contact mike admin Website What is LibreDrive? Post by mike admin » Sat Feb 02, 2019 1:19 pm A LibreDrive is a mode of operation of an optical disc drive (DVD, Blu-ray or UHD) when the data on the disc are accessed directly, without any restrictions or transformations enforced by drive firmware. A LibreDrive would never refuse to read the data from the disc or declare itself “revoked”. LibreDrive compatible drive is required to read UHD discs. Please read below the overview of LibreDrive architecture. What is an optical disc drive? It is a device that scans the surface of a disc with a laser and passes this information to a controlling software on a PC. Back in time when CD was invented, the optical drive was just that - device that scanned disc surface and passed this data to PC. Many data formats were invented at the time (sector mode 2, p/q channels, sub-channel-data,etc), but in the end it was all about positioning the laser pickup head, scanning the disc, and handling these data over for processing. Things changed for DVD, with the CSS protection scheme. The disc was divided into two areas - one area could be read as usual. However in order to read the data from another area of the disc, the software had to know a certain secret key. This was part of so-called CSS protection and was used both for rights management and to enforce disc region price fixing. A drive locked for Europe would not allow reading the data from the disc that was locked for South East Asia, where exactly same disc could be bought at a cheaper price. This data partitioning was further “enhanced” for blu-ray. In the end, any regular optical disc drive has a complex set of rules that dictate what areas of the disc can be accessed on on what conditions. The drive itself can easily read all the data technically. It is the drive embedded software (firmware) that “plays police” - decides what area of the disc can be accessed and on what conditions. So, how can one read the data from any area on the disc? The data on the optical disc is encoded as a series of small cavities called pits, that are organized in a spiral called a track. Had we had a sharp enough eye, we could've seen them directly. One way is to use a microscope - with a microscope one can clearly see the pits, apply standard (publicly defined) decoding algorithm and get the raw data from the disc. Obviously, this is a rather time consuming and expensive method. There is however a third way - take the drive back to the basic level. Change the optical drive embedded software in a way that the drive becomes a “primitive” device - one that just positions a laser, reads and decodes the data. Make a drive free from “policing” functionality, a drive that just passes all data from the disc to the user. We call this drive a LibreDrive. Top mike admin Posts: 4075 Joined: Wed Nov 26, 2008 2:26 am Contact: Contact mike admin Website What happens in LibreDrive mode Post by mike admin » Sat Feb 02, 2019 1:22 pm When MakeMKV (or any other application that uses LibDriveIo library) talks to the drive, the library checks if drive firmware version is supported. If it is supported, the library uploads a small software (a firmware extension) into drive volatile memory. This extension exposes a new (alternative) interface to read any data from the disc. The firmware extension stays only in drive volatile memory (RAM) and drive firmware is not changed in any way. The moment you turn off your drive, it is gone without a trace. As long as your drive firmware is compatible with LibreDrive, and you do not update your drive firmware, the LibreDrive mode will work forever. Top mike admin Posts: 4075 Joined: Wed Nov 26, 2008 2:26 am Contact: Contact mike admin Website LibreDrive and AACS Post by mike admin » Sat Feb 02, 2019 2:03 pm One famous “feature” of AACS is a so-called host revocation. It was designed to ensure that only “approved” software can use your drive. Every Blu-ray disc contains a file that has a list of host keys known to be used by \"unauthorized\" software. This list has a version number. The moment you insert the disc into your drive, the drive checks if the list is newer than the one it knows about, and if it is, the drive re-flashes itself (updates firmware). With LibreDrive mode your drive just became immune to this nonsense. In LibreDrive mode you always would be able to use this drive with MakeMKV or any other software that uses (open source) LibDriveIo library. No AACS revocation or other update would affect usability of your drive in LibreDrive mode. Please note that LibreDrive mode is not about hacking or meddling with AACS code. In LibreDrive mode the updated software communicates to the drive hardware directly, not touching or hacking AACS DRM code in any way at all. So your drive will continue to self-revoke on MKB updates and require authentication before releasing key material to the \"authorized\" software - it is just with LibreDrive mode all of it is no longer relevant. Top mike admin Posts: 4075 Joined: Wed Nov 26, 2008 2:26 am Contact: Contact mike admin Website LibreDrive library and SDF.bin Post by mike admin » Tue Mar 19, 2019 12:36 pm LibreDrive functionality is implemented as part of open-source LibDriveIO library (at the time of this writing the library source code is not yet released). One of the main design goals of LibreDrive runtime is portability and stability of the code base. For this reason, the LibreDrive logic is data-driven. The library itself is just a simple interpreter, while all low-level logic for a particular drive is defined by a firmware-specific data blob (called SDF). This data blob defines how to perform a certain action (like reading a disc structure) by translating requested action (called Entrypoint) and its parameters into a set of firmware-specific SCSI commands. The master data file that contains firmware-specific blobs for all known firmware versions is called SDF.bin . This file contains SDF blobs for each particular firmware and a master SDF blob that exports an API (yet another Entrypoint) to uniquely fingerprint a drive firmware version. This fingerprint data (called Firmware hash) is then used by the library to select an appropriate SDF blob for particular firmware. This design ensures that the library itself would change very rarely, if at all. Adding support for a new drive model or firmware version doesn't require any changes to the library code – instead only update of SDF.bin file is required. Even adding a new major functionality, like flashing or dumping a firmware, doesn’t require library change – the host application just need to call updated API and SDF blob has to define a new Entrypoint. With this approach it is possible to create a set of universal tools that are not tied to a specific drive platform, model or firmware version. A universal disc access library is already part of MakeMKV, and an universal firmware flashing tool is coming soon and will be released under open-source license as well. Top mike admin Posts: 4075 Joined: Wed Nov 26, 2008 2:26 am Contact: Contact mike admin Website LibreDrive firmware status Post by mike admin » Tue Oct 15, 2019 12:06 pm The list below shows all currently known firmware versions for which LibreDrive support is (or is not) possible. Code: Select all SDF v0061, total 232 firmwares supported - MK firmware: 35 firmware(s): ASUS-BC-12B1ST_b-3.01-WM00900-211711291725 ASUS-BC-12D2HT-3.01-WM00900-211711151926 ASUS-BW-16D1H-U-A204-WM01001-211901041044 ASUS-BW-16D1HT-3.10-WM01601-211901041014 HL-DT-ST-BD-RE_BE16NU50-1.02-NM00500-211711201953 HL-DT-ST-BD-RE_BH14NS50-1.01-NM00000-211711281847 HL-DT-ST-BD-RE_BH14NS58-1.01-NM00500-211711161557 HL-DT-ST-BD-RE_BH16NS40-NS50-1.03-NM00000-211711201943 HL-DT-ST-BD-RE_BH16NS50-1.01-NM00000-211712071200 HL-DT-ST-BD-RE_BH16NS55-1.04-NM01701-211901041342 HL-DT-ST-BD-RE_BH16NS58-1.01-NM00500-211711161545 HL-DT-ST-BD-RE_BH16NS60-1.00-NM00500-211804030945 HL-DT-ST-BD-RE_BP50NB40-NB50-1.02-NM00700-211811011016 HL-DT-ST-BD-RE_BP55EB40-NB50-1.01-NM00200-211711231626 HL-DT-ST-BD-RE_BP60NB10-1.00-NM00200-211711211720 HL-DT-ST-BD-RE_BU40N-BN11-0M00300-211711201351 HL-DT-ST-BD-RE_BU40N-BU12-OM01001-211902230922 HL-DT-ST-BD-RE_BU40N-FR07-JM02202-211801221536 HL-DT-ST-BD-RE_BU40N-1.03-NM00000-211810241934 HL-DT-ST-BD-RE_BU40N-1.01-RM00000-211712221221 HL-DT-ST-BD-RE_BU40N-U101-MM01201-211711301153 HL-DT-ST-BD-RE_BU50N-1.00-NM00100-211711211006 HL-DT-ST-BDDVDRW_CH12NS40-1.01-NM00400-211711211606 HL-DT-ST-BDDVDRW_UH12NS40-1.01-NM00200-211711231550 HL-DT-ST-BD-RE_WH14NS40-NS50-1.03-NM00600-211711211658 HL-DT-ST-BD-RE_WH16NS40-NS50-1.03-NM00600-211711211653 HL-DT-ST-BD-RE_WH16NS58-1.V5-NM00900-211802070957 HL-DT-ST-BD-RE_WH16NS60-1.02-NM00100-211810291936 HL-DT-ST-BD-RE_WP50NB40-NB50-1.01-NM00200-211711231646 ASUS-BW-16D1H-U-A203-WM01501-211801111117 ASUS-BW-16D1HT-3.03-WM00000-211801191558 HL-DT-ST-BD-RE_BH16NS55-1.03-NM00000-211711201704 HL-DT-ST-BD-RE_BP50NB40-NB50-1.01-NM00200-211711231615 HL-DT-ST-BD-RE_BU40N-1.02-NM01201-211711301218 HL-DT-ST-BD-RE_WH16NS60-1.01-NM00100-211711202000 supported - original vendor firmware: 68 firmware(s): ASUS-BW-16D1H-U-A201-W000000-211601141358 ASUS-BW-16D1HT-3.00-W004504-211508101633 ASUS-BW-16D1HT-3.00-W006706-211511031110 ASUS-BW-16D1HT-3.01-W000800-211703311426 ASUS-BW-16D1HT-3.02-W000800-211711241413 HL-DT-ST-BD-RE_BE16NU50-1.00-N000800-211603171336 HL-DT-ST-BD-RE_BE16NU50-1.01-N000900-211606090920 HL-DT-ST-BD-RE_BE16NU50-1.01-N001101-211703130907 HL-DT-ST-BD-RE_BH14NS58-1.00-N004204-211512221504 HL-DT-ST-BD-RE_BH16NS40-NS50-1.00-N002502-211508191146 HL-DT-ST-BD-RE_BH16NS40-NS50-1.01-N000500-211511051001 HL-DT-ST-BD-RE_BH16NS40-NS50-1.02-N000200-211512111436 HL-DT-ST-BD-RE_BH16NS40-NS50-1.02-N001901-211703101637 HL-DT-ST-BD-RE_BH16NS50-1.00-N000500-211512230930 HL-DT-ST-BD-RE_BH16NS55-1.00-N000600-211508191151 HL-DT-ST-BD-RE_BH16NS55-1.01-N000500-211511051004 HL-DT-ST-BD-RE_BH16NS55-1.02-N000200-211512111440 HL-DT-ST-BD-RE_BH16NS55-1.02-N001901-211703101630 HL-DT-ST-BD-RE_BH16NS58-1.00-N002302-211512071657 HL-DT-ST-BD-RE_BH16NS58-1.00-N006206-211512221501 HL-DT-ST-BD-RE_BH40N-A1B0-D014214-211510131446 HL-DT-ST-BD-RE_BH40N-A1B0-D015815-211703101705 HL-DT-ST-BD-RE_BP40N-GP01-V001701-211504221450 HL-DT-ST-BD-RE_BP50NB40-1.00-N002602-211409031732 HL-DT-ST-BD-RE_BP50NB40-1.00-N004304-211412161016 HL-DT-ST-BD-RE_BP50NB40-1.00-N005505-211504221517 HL-DT-ST-BD-RE_BP50NB40-NB50-1.00-N000900-211609091048 HL-DT-ST-BD-RE_BP50NB40-NB50-1.00-N002802-211702151837 HL-DT-ST-BD-RE_BP55EB40-1.00-N001501-211504221534 HL-DT-ST-BD-RE_BP55EB40-NB50-1.00-N000700-211701030941 HL-DT-ST-BD-RE_BP55EB40-NB50-1.00-N002802-211702151828 HL-DT-ST-BD-RE_BU10N-GS01-V001701-211305020921 HL-DT-ST-BD-RE_BU20N-1.00-G002202-211405090917 HL-DT-ST-BD-RE_BU20N-1.00-G004504-211412160950 HL-DT-ST-BD-RE_BU20N-1.00-G005705-211504221429 HL-DT-ST-BD-RE_BU20N-1.00-N005305-211504211939 HL-DT-ST-BD-RE_BU20N-T.02-V001301-211405091428 HL-DT-ST-BD-RE_BU40N-1.00-N003103-211612201528 HL-DT-ST-BD-RE_BU40N-1.00-R002202-211609090954 HL-DT-ST-BD-RE_BU40N-1.00-R004504-211703231903 HL-DT-ST-BD-RE_BU40N-1.01-N000000-211706291017 HL-DT-ST-BD-RE_BU40N-A101-D000700-211703131630 HL-DT-ST-BD-RE_BU40N-BU10-O002602-211701060941 HL-DT-ST-BD-RE_BU40N-BU10-O003703-211705301728 HL-DT-ST-BD-RE_BU40N-FR07-J001101-211612201651 HL-DT-ST-BD-RE_BU40N-T.00-V003403-211606241658 HL-DT-ST-BD-RE_BU50N-0420-N000000-211605241901 HL-DT-ST-BD-RE_BU50N-CCT5-V000700-211705182023 HL-DT-ST-BD-RE_BU50N-GE02-V000700-211705182030 HL-DT-ST-BD-RE_WH14NS40-NS50-1.00-N001401-211508180940 HL-DT-ST-BD-RE_WH14NS40-NS50-1.01-N000500-211511050956 HL-DT-ST-BD-RE_WH14NS40-NS50-1.02-N000100-211512111503 HL-DT-ST-BD-RE_WH14NS40-NS50-1.02-N001401-211703101655 HL-DT-ST-BD-RE_WH16NS40-NS50-1.00-N001401-211508180932 HL-DT-ST-BD-RE_WH16NS40-NS50-1.02-N000100-211512111459 HL-DT-ST-BD-RE_WH16NS40-NS50-1.02-N001401-211703101650 HL-DT-ST-BD-RE_WH16NS60-1.00-N000500-211704251756 HL-DT-ST-BD-RE_WP50NB40-1.00-N000400-211502161437 HL-DT-ST-BD-RE_WP50NB40-1.00-N001501-211504221546 HL-DT-ST-BD-RE_WP50NB40-NB50-1.00-N000800-211701031453 HL-DT-ST-BD-RE_WP50NB40-NB50-1.00-N002802-211702151823 hp-BD-RE_BU20N-BU24-P005005-211504221344 hp-BDRW_BU20N-BW20-P018518-211504221413 hp_HLDS-BDRE_BU40N-BW40-P01C01C-211608101011 hp_HLDS-BDRE_BU40N-BW41-P000200-211608161528 hp_HLDS-BDRE_BU40N-BW41-P001101-211612220920 hp_HLDS-BDRE_BU50N-BW50-P01C01C-211608101014 hp_HLDS-BDRE_BU50N-BW50-P01D11D-211612221052 unsupported - old firmware type 1: 20 firmware(s): TSSTcorp-BDDVDW_SE-506BB-TS00-02.20- TSSTcorp-BDDVDW_SE-506BB-TS00-11.13- TSSTcorp-BDDVDW_SE-506BB-TS01-05.21- TSSTcorp-BDDVDW_SE-506CB-TS00-12.12- TSSTcorp-BDDVDW_SE-506CB-TS01-05.28- TSSTcorp-BDDVDW_SE-506CB-TS02-01.07- TSSTcorp-BDDVDW_SN-506BB-AF04-04.22- TSSTcorp-BDDVDW_SN-506BB-GT01-01.31- TSSTcorp-BDDVDW_SN-506BB-GT06-01.05- TSSTcorp-BDDVDW_SN-506BB-GT08-06.17- TSSTcorp-BDDVDW_SN-506BB-SB00-07.10- TSSTcorp-BDDVDW_SN-506BB-TM01-09.23- TSSTcorp-DVDWBD_SE-406AB-TS00-04.20- TSSTcorp-DVDWBD_SH-B123A-MD03-03.31- TSSTcorp-DVDWBD_SH-B123A-SB03-12.03- TSSTcorp-DVDWBD_SH-B123A-SB04-03.31- TSSTcorp-DVDWBD_SH-B123L-SB00-05.11- TSSTcorp-DVDWBD_SH-B123L-SB02-08.16- TSSTcorp-DVDWBD_SH-B123L-SB03-12.03- TSSTcorp-DVDWBD_SH-B123L-SB04-03.31- unsupported - old firmware type 2: 56 firmware(s): ASUS-BW-12B1ST_a-1.00-E101701-211304231147 ASUS-BW-14D1XT-1.00-M103703-210000000000 HL-DT-ST-BD-RE_BE14NU40-1.00-N0G43G4-211206262029 HL-DT-ST-BD-RE_BE14NU40-1.01-N001601-211406161935 HL-DT-ST-BD-RE_BE14NU40-1.01-N003803-211406251441 HL-DT-ST-BD-RE_BE14NU40-1.01-N005905-211408221631 HL-DT-ST-BD-RE_BH14NS40-1.00-N1A11A1-211201171401 HL-DT-ST-BD-RE_BH14NS40-1.00-N1A37A3-211205021346 HL-DT-ST-BD-RE_BH14NS40-1.00-N1A44A4-211206161823 HL-DT-ST-BD-RE_BH14NS40-1.02-N0A02A0-211312061453 HL-DT-ST-BD-RE_BH14NS40-1.03-N0A09A0-211405071012 HL-DT-ST-BD-RE_BH16NS40-1.00-N1A29A2-211210311917 HL-DT-ST-BD-RE_BH16NS40-1.01-N1A12A1-211304042250 HL-DT-ST-BD-RE_BH16NS40-1.02-N0A02A0-211312061535 HL-DT-ST-BD-RE_BH16NS40-1.03-N0A03A0-211403261115 HL-DT-ST-BD-RE_BH16NS40-1.03-N0C06C0-211505261714 HL-DT-ST-BD-RE_BH16NS48-1.02-N0A02A0-211312061545 HL-DT-ST-BD-RE_WH14NS40-1.00-N1A26A2-211202091126 HL-DT-ST-BD-RE_WH14NS40-1.00-N1A37A3-211204261630 HL-DT-ST-BD-RE_WH14NS40-1.00-N1A44A4-211206161828 HL-DT-ST-BD-RE_WH14NS40-1.01-N1A12A1-211304042325 HL-DT-ST-BD-RE_WH14NS40-1.02-N0A02A0-211312061523 HL-DT-ST-BD-RE_WH14NS40-1.03-N0A09A0-211403261105 HL-DT-ST-BD-RE_WH14NS40-1.03-N0C08C0-211505261653 HL-DT-ST-BD-RE_WH16NS40-1.00-N1A12A1-211304041441 HL-DT-ST-BD-RE_WH16NS40-1.01-N0A02A0-211312061658 HL-DT-ST-BD-RE_WH16NS40-1.02-N0A00A0-211405071026 HL-DT-ST-BD-RE_WH16NS40-1.02-N0C00C0-211505261638 HL-DT-ST-BD-RE_WH16NS48-1.D3-N000700-211602041103 HL-DT-ST-BDDVDRW_CH12NS30-1.00-N0A13A1-211303221728 HL-DT-ST-BDDVDRW_CH12NS30-1.00-N0A20A2-211306131532 HL-DT-ST-BDDVDRW_CH12NS30-1.01-N0A02A0-211312061614 HL-DT-ST-BDDVDRW_CH12NS30-1.02-N0A05A0-211404241908 HL-DT-ST-BDDVDRW_CH30N-AS00-S002102-211306141459 HL-DT-ST-BDDVDRW_CP50NS20-1.00-N004904-211409041512 HL-DT-ST-BDDVDRW_CP50NS20-1.00-N007307-211412161028 HL-DT-ST-BDDVDRW_CU20N-1.00-N003203-211412161022 HL-DT-ST-BDDVDRW_UH12NS30-1.00-N0A13A1-211303221725 HL-DT-ST-BDDVDRW_UH12NS30-1.00-N0A20A2-211306131457 HL-DT-ST-BDDVDRW_UH12NS30-1.01-N0A02A0-211312061625 HL-DT-ST-BDDVDRW_UH12NS30-1.02-N0A04A0-211403261141 HL-DT-ST-BDDVDRW_UH12NS30-1.03-N0A05A0-211404241900 HL-DT-ST-BDDVDRW_UH12NS40-1.00-N000200-211602231042 HL-DT-ST-DVDRWBD_CH30N-A101-D000300-210000000000 hp-BD-RE_BH40N-B7C6-P0A12A1-211303262313 hp-BDDVDRW_CH30L-BC56-P0B00B0-210000000000 hp-BDDVDRW_CU10N-B1C0-P015115-211307031530 hp-DVDWBD_SN-406AB-HH01-0116- TSSTcorp-BDDVDW_SE-506AB-TS00-09.06- TSSTcorp-BDDVDW_SE-506AB-TS01-01.31- TSSTcorp-BDDVDW_SN-506AB-SB00-01.31- TSSTcorp-DVDWBD_SN-406AB-AS02-03.05- TSSTcorp-DVDWBD_SN-406AB-SB00-10.05- TSSTcorp-DVDWBD_SN-406AB-SC00-06.18- TSSTcorp-DVDWBD_SN-406AB-TM00-07.11- TSSTcorp-DVDWBD_SN-406AB-TM01-05.15- upgrade required - MK firmware available: 44 firmware(s): ASUS-BC-12B1ST_b-1.00-_130614-x-211306141158 ASUS-BC-12B1ST_b-3.00-W004704-211512151340 ASUS-BC-12B1ST_b-3.01-W000900-211711291725 ASUS-BC-12D2HT-1.00-_130614-x-211306141200 ASUS-BC-12D2HT-3.00-W004704-211512091147 ASUS-BC-12D2HT-3.01-W000900-211711151926 ASUS-BW-16D1H-U-A101-W000600-211412080959 ASUS-BW-16D1H-U-A203-W001501-211801111117 ASUS-BW-16D1H-U-A204-W001001-211901041044 ASUS-BW-16D1HT-1.01-W100600-211302051930 ASUS-BW-16D1HT-3.03-W000000-211801191558 ASUS-BW-16D1HT-3.10-W001601-211901041014 HL-DT-ST-BD-RE_BE16NU50-1.02-N000500-211711201953 HL-DT-ST-BD-RE_BH14NS50-1.01-N000000-211711281847 HL-DT-ST-BD-RE_BH14NS58-1.01-N000500-211711161557 HL-DT-ST-BD-RE_BH16NS40-NS50-1.03-N000000-211711201943 HL-DT-ST-BD-RE_BH16NS50-1.01-N000000-211712071200 HL-DT-ST-BD-RE_BH16NS55-1.03-N000000-211711201704 HL-DT-ST-BD-RE_BH16NS55-1.04-N001701-211901041342 HL-DT-ST-BD-RE_BH16NS58-1.01-N000500-211711161545 HL-DT-ST-BD-RE_BH16NS60-1.00-N000500-211804030945 HL-DT-ST-BD-RE_BP50NB40-NB50-1.01-N000200-211711231615 HL-DT-ST-BD-RE_BP50NB40-NB50-1.02-N000700-211811011016 HL-DT-ST-BD-RE_BP55EB40-NB50-1.01-N000200-211711231626 HL-DT-ST-BD-RE_BP60NB10-1.00-N000200-211711211720 HL-DT-ST-BD-RE_BU40N-1.01-R000000-211712221221 HL-DT-ST-BD-RE_BU40N-1.02-N000100-211711202017 HL-DT-ST-BD-RE_BU40N-1.02-N001201-211711301218 HL-DT-ST-BD-RE_BU40N-1.03-N000000-211810241934 HL-DT-ST-BD-RE_BU40N-BN11-0000300-211711201351 HL-DT-ST-BD-RE_BU40N-BU12-O001001-211902230922 HL-DT-ST-BD-RE_BU40N-FR07-J002202-211801221536 HL-DT-ST-BD-RE_BU40N-U101-M001201-211711301153 HL-DT-ST-BD-RE_BU50N-1.00-N000100-211711211006 HL-DT-ST-BD-RE_WH14NS40-NS50-1.03-N000600-211711211658 HL-DT-ST-BD-RE_WH16NS40-NS50-1.03-N000600-211711211653 HL-DT-ST-BD-RE_WH16NS58-1.V1-N002102-211711201947 HL-DT-ST-BD-RE_WH16NS58-1.V2-N000100-211711231023 HL-DT-ST-BD-RE_WH16NS58-1.V5-N000900-211802070957 HL-DT-ST-BD-RE_WH16NS60-1.01-N000100-211711202000 HL-DT-ST-BD-RE_WH16NS60-1.02-N000100-211810291936 HL-DT-ST-BD-RE_WP50NB40-NB50-1.01-N000200-211711231646 HL-DT-ST-BDDVDRW_CH12NS40-1.01-N000400-211711211606 HL-DT-ST-BDDVDRW_UH12NS40-1.01-N000200-211711231550 Legend: supported - The firmware is fully supported unsupported - old firmware type 1 - The firmware uses a very old memory layout and codebase. While hardware is supported, it is very unlikely that firmware from this list would be ever supported. unsupported - old firmware type 2 - The firmware uses an old memory layout. Currently these are not supported, but support may be added at any time - all necessary vendor commands are exposed by the firmware. Supporting these is somewhat time-consuming process, and the current plan is to add support on secondary basis, only after other features are done. Note that for many drives the next version of official firmware is supported. This list will be updated periodically. Top Locked Print view Display: All posts 1 day 7 days 2 weeks 1 month 3 months 6 months 1 year Sort by: Author Post time Subject Direction: Ascending Descending 5 posts • Page 1 of 1 Return to “LibreDrive drives” Jump to News and Announcements General MakeMKV discussion Disc formats and error reports ↳ Blu-ray discs ↳ Blu-ray 3D ↳ UHD discs ↳ DVD discs Optical Disc Drives ↳ UHD drives ↳ Drives for sale, Flashing Services, where to buy... ↳ LibreDrive drives OS-specific forums ↳ MakeMKV for Mac OS X ↳ MakeMKV for Linux MKV file processing and playback Advanced MakeMKV usage Archive Forum and Website discussions Board index Contact us All times are UTC Delete cookies All times are UTC Delete cookies Contact us Powered by phpBB® Forum Software © phpBB Limited Style: Green-Style by Joyce&Luna phpBB-Style-Design",
    "commentLink": "https://news.ycombinator.com/item?id=41784069",
    "commentBody": "What Is LibreDrive (2019) (makemkv.com)134 points by Tomte 16 hours agohidepastfavorite27 comments pdpi 6 hours ago> LibreDrive functionality is implemented as part of open-source LibDriveIO library (at the time of this writing the library source code is not yet released) This was posted in 2019. As of today, there is still no published source code. reply rfl890 5 hours agoparentSee this forum post[1] for some insight on the situation [1]: https://forum.makemkv.com/forum/viewtopic.php?t=24312#post_c... reply bombcar 5 hours agorootparentBased on this and my experience with the world, I suspect that he's protecting a revenue stream but intends to open-source the code when he retires or no longer depends on the stream. reply snvzz 9 hours agoprevmakemkv itself is also tremendously useful. Got some inconvenient to work with BD or DVD? Just dump it into a mkv file with makemkv, and now it is convenient to work with. reply pseudosavant 35 minutes agoparentIt is worth pointing out that MakeMKV has a CLI you can use it without the GUI. I have a batch file that rips the main movie from my BD drive and names the MKV based on the BD disc label. My script is old enough I wrote it myself, but ChatGPT/Claude could easily do a better job. When combining MakeMKV's CLI and Handbrake's CLI there is an easy and very repeatable path of going from disc to an optimized MP4. Some might think it is sacrilegious to use MP4 instead of MKV. I've found MP4s with H264 video and AC3 audio can play almost everywhere (for me: Xbox, Roku, iPhone/Safari, Edge, Android, most smart TVs) now, and support surround sound. reply doublepg23 5 minutes agorootparentI don't think anyone is against using different containers for compatibility, you can remux from mkv to mp4 very easily with ffmpeg directly. However it's a little odd to go through the intermediate step of using MakeMKV if you're just compressing the resulting remux using Handbrake. Usually the point of MakeMKV is to get the highest quality copies of retail media. reply opengears 11 hours agoprevUsing libredrive supported devices - would we get some other advantages? Like being able to read from old and broken CDROM and DVD devices more reliably? reply bombcar 8 hours agoparentNo, it actually makes it \"worse\" in that most usual DVD players and drives will do a \"best effort, but keep going\" type of read which may result in a pop or skip or desync for a moment on playback - but these tools are archival and refuse to read if they can't read correctly. It's actually quite annoying at times, for example it's often better to rip audiobooks with iTunes and then grab the files and delete it from iTunes than to use something like XLD directly. reply swijck 10 hours agoprevI love how we have gone full circle on retro technology. reply janandonly 6 hours agoprevVery interesting set of posts. I feel that when this website goes dark and the last grey-beards stops blogging, this kind of arcane information will be lost forever. This makes me sad. reply lloydatkinson 10 hours agoprevI remember several years ago when I wanted to watch a Star Wars DVD on a computer. This was in the UK with the DVDs purchased in the UK and with a DVD reader also purchased in the UK, as that’s where I live. Windows Media Player told me that the region of the Disc was wrong and that if I wanted to watch it, it could instruct the firmware on the DVD player to update its allowed region, but that I could only do this (I think) five times and no more. Rather than dealing with this pompous bullshit I watched it with VLC player which just worked without doing any of that legal nonsense. I’ve remained a big advocate of VLC ever since. reply alias_neo 10 hours agoparentI had a tangentially related situation many (~20?) years ago when I bought a music album released under Sony, I had a sweet PC based Hi-Fi setup, and the DRM ring they'd added to the disc meant it wouldn't play back at anything above MP3 128. I noticed a ~1.5cm ring around the outside of the disc was visibly a different colour/texture to the standard audio part; I tried blanking it out with Sharpie which some people online suggested might work, but eventually gave up and contacted Sony to tell them how pissed off I was that they were preventing PC-based music listeners from listening to what they'd bought. They sent back an apology and a new copy of the disk without the DRM/MP3 crap. I bet LibreDrive might have worked by letting me just read the disc raw and grab the bits I need. reply johannes1234321 8 hours agorootparentSony really did crazy stuff for \"copy protection\" on their CDs ... https://en.m.wikipedia.org/wiki/Sony_BMG_copy_protection_roo... reply n1b0m 7 hours agorootparent“Sony BMG initially denied that the rootkits were harmful. It then released an uninstaller for one of the programs that merely made the program's files invisible while also installing additional software that could not be easily removed, collected an email address from the user and introduced further security vulnerabilities.” That’s diabolical reply l72 4 hours agoparentprevI had a similar issue, of moving from the US to Germany in 2000 and my spouse bringing her favorite DVDs with us. However, once we got there, she was unable to watch any of them, as all the DVD players were for the EU region, while her DVDs were for the US. She is not a technical person, but she is now very acutely aware of B.S. restrictions like this (and later DRM on mp3s) and how to get around them. reply p0w3n3d 8 hours agoparentprevThis is called freedom reply junaru 6 hours agoprevCorrect me if i'm wrong but its not even exploiting some firmware bug or anything. Think there is a 'plugin interface' in those firmware that exposes whatever is needed to read the raw data so all it does is uses that interface to dump data instead of using the official calls. IIRC its why the read speeds are slower. Source: some post on the same forum couple of years ago. reply gjsman-1000 4 hours agoparentDepends on the drive. Early drive firmwares were unencrypted and left debug features available. Modern drives use encrypted firmware and have the debug modes disabled. If you’ve got one of those early firmwares, you’re good to go. If not, you’ll need to patch your drive. However, “encrypted” is fairly weak compared to, say, a game console when the key is the same for all drives and there’s no hardware-level anti-rollback… As a result, it was fairly easily defeated on modern drives. Find key, decrypt firmware, make changes, re-encrypt, update. Thanks MediaTek for keeping the same flawed legally-approved chip architecture for almost a decade. reply therein 2 hours agorootparentI love MediaTek for these things. Same for \"old\" Android phones. reply bitwize 11 hours agoprev [–] Almost certainly a DMCA violation and therefore illegal to distribute. reply jeroenhd 11 hours agoparentI'm not sure why. If the point is to make the firmware read every bit of the drive, that doesn't seem like it would break any copyright law. The encrypted data is rather useless without breaking DRM, but the DRM breaking doesn't happen in hardware. If the firmware is based on existing, proprietary drive firmware, then distributing it may run afoul of copyright law, but if all that's distributed is a patch file then I don't see the problem there either. There are quite a few countries with exceptions in copyright law for compatibility reasons, like modifying programs to make them work on newer hardware without the original authors' consent. The reason VLC (and many other open source projects) can play DVDs is that France, where VLC is based, has laws that make it legal to distribute a DVD playback library. I don't think any Linux distros have faced legal trouble over distributing VLC, even if they are American in nature. I'm sure the copyright lobby will have a different opinion, but I don't think it's quite as black and white without knowing where the author resides and/it what nationality they have. reply tempfile 8 hours agorootparentThe DMCA is American law, so the parent comment can only be referring to America. Obviously doesn't apply outside there. > I don't think any Linux distros have faced legal trouble over distributing VLC, even if they are American in nature. That's because they generally don't distribute libdvdcss, which is the illegal part. > Many Linux distributions do not contain libdvdcss (for example, Debian, Ubuntu, Fedora and openSUSE) due to fears of running afoul of DMCA-style laws, but they often provide the tools to let the user install it themselves. For example, it used to be available in Ubuntu through Medibuntu, which is no longer available. https://en.wikipedia.org/wiki/Libdvdcss#Distribution reply immibis 8 hours agorootparentprevBecause the purpose is to read disks protected by technological protection measures. Circumventing technological protection measures is illegal, period. That's one of the things many people don't like about the DMCA. I'd recommend to start archiving this website now. reply tombert 3 hours agorootparentI mean, the post is from 2019, and MakeMKV has been around much longer than that. I know this isn't always the case, but I feel like if they were going to take down MakeMKV's site, they probably would have done it by now. reply immibis 1 hour agorootparentThey thought that about Yuzu, too. reply tombert 7 hours agoparentprevMaybe to the letter of the law, but I think there’s a reason why no one seems to go after MakeMKV. If you’re using MakeMKV, then almost by definition you have a legitimate copy of the media you’re ripping, and as such not a direct target of any kind of piracy prevention. Obviously you could then post your rip on The Pirate Bay or something, but I don’t think that MakeMKV is generally blamed for that. I have a ton of Blu-Ray movies purchased legitimately, and I use MakeMKV to rip them and play them with Jellyfin. I don’t distribute them, I only play them in my house. Am I technically breaking the law? Probably, DRM law is weird and confusing in the US, but I doubt that the MPAA has a huge problem with what I am doing, since I am paying them for legitimate copies of my movies. reply therein 11 hours agoparentprev [–] Too bad I already flashed it long time ago into my LG. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "LibreDrive is a mode for optical disc drives that allows direct data access without firmware restrictions, enabling the reading of UHD discs and bypassing AACS host revocation.",
      "It uses a firmware extension stored in volatile memory, ensuring no trace is left after power-off, and is supported by the open-source LibDriveIO library.",
      "This approach allows for universal tools that are not dependent on specific drive models or firmware versions, enhancing compatibility with software like MakeMKV."
    ],
    "commentSummary": [
      "LibreDrive, part of the LibDriveIO library, was announced in 2019, but its source code has not been released, leading to speculation about a future release upon the creator's retirement.",
      "MakeMKV is a related tool that converts Blu-ray Discs (BD) and DVDs to MKV files, offering a command-line interface (CLI) for automation, though some users prefer MP4 for broader compatibility.",
      "Discussions around LibreDrive and MakeMKV include challenges with Digital Rights Management (DRM) and region restrictions, with users finding workarounds, while legal concerns about DMCA violations persist."
    ],
    "points": 134,
    "commentCount": 27,
    "retryCount": 0,
    "time": 1728442719
  },
  {
    "id": 41788026,
    "title": "PEP 760: No more bare excepts",
    "originLink": "https://discuss.python.org/t/pep-760-no-more-bare-excepts/67182",
    "originBody": "Pablo Galindo SalgadopablogsalSteering Council Member 1 6h Hi everyone, As the evil twin of PEP 758: Allow `except` and `except*` expressions without parentheses 594, @brettcannon and me present you PEP 760: No more bare excepts. This PEP proposes disallowing bare except: clauses in Python’s exception-handling syntax. Currently, Python allows catching all exceptions with a bare except: clause, which can lead to overly broad exception handling and mask important errors. This PEP suggests requiring explicit exception types in all except clauses, promoting more precise and intentional error handling. Please review the entire document before commenting as it contains several aspects of how to address the deprecation, tooling, rejected ideas and more. Python Enhancement Proposals (PEPs) PEP 760 – No More Bare Exceptspeps.python.org 1.1k This PEP proposes disallowing bare except: clauses in Python’s exception-handling syntax. Currently, Python allows catching all exceptions with a bare except: clause, which can lead to overly broad exception handling and mask important errors. This... Vote in this poll: PEP 760 – No More Bare Excepts PEPs Ok, please vote in this friendly non-binding poll poll 4 PEP 758: Allow `except` and `except*` expressions without parentheses1 11.9k views 207 likes 12 links 31 users 8 7 6 5 4 read 11 min Top replies",
    "commentLink": "https://news.ycombinator.com/item?id=41788026",
    "commentBody": "PEP 760: No more bare excepts (python.org)125 points by ayhanfuat 5 hours agohidepastfavorite245 comments kristjansson 2 hours agoMy concern, and IMO what should be the overwhelming concern of the maintainers, is not the code that is being written, or the code that will be written, but all the code that has been written, and will never be touched again. A break like this will force lots of python users to avoid upgrading to 3.17, jettison packages they may want to keep using, or deal with the hassle of patching unmaintained dependencies on their own. For those Python users for whom writing python is the core of their work that might be fine. For all the other users for whom python is an foreign, incidental, but indispensable part of their work (scientists, analysts, ...) the choice is untenable. While python can and should strive to be a more 'serious', 'professional' language, it _must_ have respect and empathy for the latter camp. Elevating something that should be a linter rule to a language change ain't that. reply isoprophlex 1 hour agoparentExactly this. I totally agree. It's incredible to think that some people still run python 2 scripts; something unpalatable to the point of being nauseating for a day-to-day python programmer, but totally understandable in the context of incidental usage by a scientist dealing with legacy systems. If these things start happening to python 3 on a larger scale, might as well throw in the towel and go for python 4. reply mrbungie 1 hour agorootparentI read \"python 2\" and a part of my soul cried in utf-8. reply bdowling 8 minutes agorootparentAre you sure it didn't cry in `bytes()`? reply rurban 58 minutes agoparentprevmajor changes with breaking backward compatibility would require a major bump. Fine for python 4 I would say. reply kristjansson 21 minutes agorootparentSure I guess? But we've just barely stopped talking about Python3, and that was released 13-16 years ago[1]. Is _this_ change worth another decade of thrashing the ecosystem? Is __any__ change? [1]: depending on if we count 3.0 vs 3.2 when it was actually kinda usable. reply Narhem 1 hour agoparentprevDisagree, I’m so disappointed in companies who do sprint type development refusing to use Python. It works well with the “Silicon Valley startup ecosystem”. That being said, as far as workplace differences I’d say Java shops would be the ideal, slower, less long term problems but so much more initial investment. reply stackskipton 1 hour agorootparentAs SV startup with Python monolith, yes, it's very common for startup but generally gets ejected because lack of strict typing and speed. We are replacing with Go, Node and .Net. reply Narhem 44 minutes agorootparentPython offers typing with static compiling. .Net doesn’t really match with startup culture. I’m on the fence about Go, but maybe that’s my preference to having classes. But yeah I’m the general case if I was an investor I’d be more careful with purely Python based startups. reply bigstrat2003 35 minutes agorootparent> Python offers typing with static compiling. Python doesn't enforce types and as far as I know has no plans to. > .Net doesn’t really match with startup culture. Who the hell cares? If it's the best tool for the job, use it. Anything else is unprofessional as hell. reply Narhem 10 minutes agorootparentTell that to the people who downvote me which seems unprofessional as hell. If I want to learn .Net which is more time consuming and more difficult to find employees why would I use it? Makes sense if you are in an area with a lot of windows people, but that’s not the case anywhere other than Texas. And the compiler enforce typing. Admittedly not as nice as Go since you have to rely on external tools but workable. People like their curly brackets though. Just not as helpful when dealing with system problems. reply cuuupid 4 hours agoprevIf I’m understanding this correctly this proposal would fully break compatibility with many (most?) codebases, actually remove syntactic sugar and force more characters for the same functionality. I fail to see how this is even being considered. I don’t understand the idiomatic viewpoint either here, I understand the author personally finds it confusing when excepts aren’t verbose but I think you would be hard pressed to find many python developers who agree. Even outside the ecosystem, most languages have been adding more support for bare excepts (like js with bare catch) so this feels like a step backwards. But maybe I’m just not understanding this proposal! reply phkahler 3 hours agoparent>> I fail to see how this is even being considered. To me it stinks of an ego-centric person thinking they're a \"language developer\" and knowing better than the actual users of the language what's best for them. Just because something can be misused doesn't mean you have to take it away. I haven't noticed, but since Rust came along is there a trend among languages to enforce \"safer\" programming at the language level? I could see that kind of thinking getting way out of hand. If that's the case, I would see this one as \"I'm going to save the world with this dumb little change that breaks things for a bunch of people!\" I would hope a PEP like this came about from frequent user requests but that doesn't seem to be the case. reply sestep 2 hours agorootparentIt's interesting that you mention Rust, since Rust takes backward compatibility quite seriously; that's why it's still on version 1.x. Granted, sometimes there is a compiler bug that causes some old code not to compile with newer versions of Rust, but that is rare and never intentional like this PEP. reply jbiason 1 hour agorootparent(Just to be sorta pedantic) I don't think it's the version 1.x that promotes backward compatibility, but editions. In 2016, you could call your function `fn async(...) { ... }` without any issues, and you can still compile this code with the modern version of rustc. If you want to use the async features of Rust, you need to change your edition to at least 2018, which brings breaking changes in the language. And you can have a project that mixes those editions, and cargo will correctly use the edition asked for each crate. (So, I guess the ideal world would to first look at the package management in Python, and *then* try to introduce breaking changes. And I'm withholding how much I'm angry at the PSF for postponing having a decent package manager for 20 years and investing in removing the GIL and adding JIT.) reply 0cf8612b2e1e 1 hour agorootparentNot meaning to apologize for Python here, but you have significantly more ability to segregate “editions” when you statically compile code. All the more reason to take a breakage very seriously. This is even worse than the walrus operator. At least I can ignore that. This breaks working code for some notion of purity. reply roblabla 48 minutes agorootparentCode compilation doesn't really have much to do with it. Python already has a somewhat similar ability - opting into certain language features of python on a file-by-file basis - using __future__[0]. It'd be pretty easy to add something like Rust editions by looking for a special statement in the file. And to make it more convenient, put it in the __init__.py and have it be transitive to the module. [0]: https://docs.python.org/3/library/__future__.html reply drdaeman 1 hour agorootparentprev> And you can have a project that mixes those editions, and cargo will correctly use the edition asked for each crate. I'm curious - if you had a `pub fn async(...){...}` in some 2016 crate, can you still call it from a 2024 codebase? reply Lorak_ 44 minutes agorootparentYou can: https://doc.rust-lang.org/rust-by-example/compatibility/raw_... reply phkahler 1 hour agorootparentprev>> It's interesting that you mention Rust, since Rust takes backward compatibility quite seriously; I mentioned Rust because the memory safety guarantees are a significant new thing for a language like that. I forgot about \"managed\" languages like C# because that's quite far from my mind, but that's another significant attempt at safety. This kind of little detail in the PIP is really insignificant by comparison, so I was speculating that it might be driven by some kind of \"save everyone\" mentality. If so, wondering if that's a trend lately and I hadn't noticed. reply umanwizard 2 hours agorootparentprevThey do take it seriously, I agree. However, the commonly repeated meme that Rust only makes backwards-incompatible changes by mistake or to fix soundness issues is wrong. They allow themselves to make changes if they're judged to have a low (but nonzero) risk of causing backwards incompatibility in the wild. For example, adding a new function to a standard trait can be backwards incompatible but they do it all the time. reply kibwen 32 minutes agorootparentIndeed. Although it's worth noting that the same is true of e.g. stable enterprise favorites like Java, which regularly makes minor breaking changes that are judged to have little impact (which is why every Java release is accompanied by a compatibility guide; see the \"Important Changes\", \"Removed Features\", and \"Other Notes\" sections of the most recent release notes: https://www.oracle.com/java/technologies/javase/23-relnote-i...). reply pca006132 2 hours agorootparentprevNo. A serious language designer will try to make things they like at the beginning, not trying to patch it later. Patching via breaking public API (language design) is never a good thing. reply phkahler 1 hour agorootparent>> A serious language designer.... That's why I put \"language developer\" in quotes. reply beeboobaa3 1 hour agorootparentprevLet me just grab my time machine and go back 40 years reply umanwizard 2 hours agorootparentprevThere have been people trying to enforce safer programming at the language level at least since Java positioned itself as the safer alternative to C++ way back in the 90s. reply riffraff 1 hour agorootparentI'm pretty sure Bertrand Meyer's OOSC[0] from 1988 had something like \"if a language has a feature which comes with warnings that you shouldn't use it, it shouldn't have that feature\" (paraphrasing). [0] https://en.wikipedia.org/wiki/Object-Oriented_Software_Const... reply slt2021 2 hours agorootparentprevcan these language improvements be implemented at linter level? so that people can opt-in or opt-out selectively, per their own discretion, for these kinds of rules I dont see the point of this being part of the language since it break compat and brings zero benefits at runtime reply 0cf8612b2e1e 2 hours agorootparentIn the PEP, some of the comments noted this was basically a linting issue. No reason to break the language over it. reply akkad33 1 hour agorootparentprevLinters already flag this as error reply willcipriano 2 hours agorootparentprevFlake8 already provides this for anyone concerned about it. https://www.flake8rules.com/rules/E722.html reply strunz 2 hours agorootparentSo does pylint which has been there forever https://pylint.pycqa.org/en/latest/user_guide/messages/warni... reply gmueckl 2 hours agorootparentprevRust had extremely successful marketing based on its security claims. It's no surprise that other languages jump on that bandwagon to not get left behind, is it? reply chefandy 2 hours agoparentprevPEP contains lots of best practices that aren't enforced by the interpreter though, doesn't it? e.g. PEP 8. It's been a while so maybe PEP 8 is more unique than I realize? It seems like a pretty sensible recommendation that wouldn't necessarily need to change the way exceptions are handled by Python. Right there in PEP 8 it says in big text \"A Foolish Consistency is the Hobgoblin of Little Minds.\" I imagine that enforcing this in the interpreter would fall under that, but it seems like a good piece of advice for folks new to the language, or more likely, new to coding. reply theamk 2 hours agorootparentNo one would anyone would object if this was a \"best practice\" or \"linter rule\". It's the enforcement by compiler, which will break lots of existing code, that makes people unhappy. reply kristjansson 2 hours agorootparentprevThere are PEPs that are not language features, but this PEP is emphatically proposing a modification to the language. reply Larrikin 4 hours agoparentprevPython feels like they are fixing the language the best they can by slowly and properly adding explicit types. Character count is not a valid argument when we have hard drives that are multiple terabytes and IDEs and LLMs that will gladly auto complete a full word with the import. Foo, bar variable names and i, n incrementers should be banished to freshman level undergrad tests meant to intentionally confuse the student. I'm hoping Python 4 will be a big breaking change similar to the previous one and full support for explicit types will be one of the reasons. reply AlexandrB 3 hours agorootparentCharacter count is a valid argument because a human is going to have to read the code at some point. Otherwise, Java style names like \"countriesFromAsiaWhereAreTheMostPlanesAndBoats\" would be just fine. Edit: I don't get the hate for \"i\" or \"j\" as increment variables. When you're working with numerical data they closely match how you would express the same operation with mathematical notation and are idiomatic to the point that everyone with non-trivial experience in programming knows what they represent. There are better options in some cases (e.g. \"for name in names:\"), but there's nothing inherently wrong with i, j, k, etc. reply NAHWheatCracker 2 hours agorootparentAt my last job, there was a frontend developer who added a linter rule that variable names must be at least 2 characters long. The project already had 20,000 lines of code. Every time anyone made a change to a file, they would have to rename all the one letter variables. Usually, this meant all the for loops in the file. I tried to explain how pointless this rule was, but he wasn't having any of it. Most people just renamed variables like i > ii, which was worse. reply morkalork 1 hour agorootparentWhat is even the point of digging in deeper, did they not see everyone around them doing the i>ii workaround? You've lost, call it a failed experiment and move on. reply tomrod 1 hour agorootparentprevEye and Jay are go to as well reply Larrikin 3 hours agorootparentprevBut that's a ridiculous example you're proposing in bad faith. I flag nearly all abbreviations in code reviews because code is meant to be read and the names of things should be clear. reply umanwizard 1 hour agorootparentI work on code that uses `pid` pervasively, and it would drive me crazy if someone insisted I write out `processId` every time. Same with `tx` or `rx` for channel endpoints, `i` for loop variables, `txId` or `tid` for transaction ID in databases, etc. If something is very common in the domain you're working in it's more annoying _not_ to abbreviate it. reply kstrauser 3 minutes agorootparentThat \"in the domain\" bit is an excellent callout, and I'm stealing it. \"Why don't you write out 'process_id'?\" \"Because the people who maintain the sort of code that cares about pids all refer to it as 'pid'.\" Kim_Bruning 19 minutes agorootparentprevIn certain contexts, I'm ok with eg (off the top of my head): * x, y, z (coordinates) * r, g, b, a; c, m, y, k; y, u, v (colors) * i, j, k matrices and such * p, i, d; Kp, Ki, Kd in process control * rv (return value), where it is clear what The Thing To Be returned is. * common variables from eg physics Generally because these are well known and defined in particular contexts. I don't always make it, but sometimes an argument can be made that single letters are better known/recognizable by their single letter name. Compare: * total_energy = mass * light_speed_constant ^ 2 vs * E=m*c^2 Or: * process_control_setting = proportional_component + integral_component + derivative_component vs * q=p+i+d (for those of us who know PID control) reply fwip 4 minutes agorootparentprevRecently, I saw somebody writing that in their projects, they define a Glossary.txt of terms, and then set up their tooling to flag any use of an identifier that isn't a word or defined in the Glossary. (Allowing also for compound words like camelCase and snake_case, of course). So in their project they could define `pid: process identifier [...]`, and then their tooling would be allow a variable named `new_pid`. Upsides: consistent vocabulary in your project, a centralized place to look up jargon, and a subtle friction to avoid adding new terms that people might not immediately grok. I wish I could remember the source; because it sounded like a nice setup to steal. AlexandrB 3 hours agorootparentprevIt's not in bad faith to say that there's a cognitive load to having longer line lengths or, god forbid, wrapped lines when you're trying to figure out what the code is actually doing with that data the variables represent. Obviously the Python \"except\" case being discussed is not a big deal in this regard, but you made a blanket statement about character count being no big deal because of big hard drives and IDEs. reply tsimionescu 1 hour agorootparentprevSo your code is full of HyperTextTransferProtocolSecureClients, using a TransportLayerSecurityCertificate? reply xorcist 1 hour agorootparentprevSo you intentionally make code harder to read for most of the humans on this planet, so satisfy a desire to use longer identifiers? reply lostdog 2 hours agorootparentprevHow about abbreviating the 12 most common things in the codebase, but everything else is long? That's a nice compromise where you need to learn a few core concepts, but the code itself is easier to scan for bugs in a lot of places. reply jerf 2 hours agorootparentprevBy the time you've written a Python that has all explicit types in it, and harvested the speed advantages the interpreter can have when it can count on that, and all the other cascading changes that would result, and the fact that you would discard 100% of all previous modules both Python and C, you might as well just start using one of the existing statically-typed languages that already has a mature ecosystem. Python should be working on being the best Python it can be, not being an adequate Python and an inadequate bodged-on static language on the side. reply akkad33 1 hour agorootparent> harvested the speed advantages the interpreter can have when it can count on that, Does the interpreter actually optimize code based on type information? My knowledge is that it does not reply jerf 14 minutes agorootparentPython 3 does not, but the hypothetical Python 4 would be crazy to put types on everything and then not accelerate its interpreter with the resulting data. The problem Python 3 has is dynamic types, as the dynamically-typed languages implement them, are viral; one little crack lets them in somehow and all the code operating on the data has to assume it's viral. reply gizmo 3 hours agorootparentprevJavascript somehow manages to grow without breaking backwards compatibility. So does C++. Breaking countless packages (and forks of packages) in the pursuit of something as nebulous as language purity is a big mistake. I happen to like explicit typing but it's not the kind of thing you can graft onto a mature language without making awful compromises. Also, it pushes massive externalities onto the millions of people who rely on Python for their work. reply Chris2048 2 hours agorootparentprev> I'm hoping Python 4 will be I'm hoping \"Python 4\" will be another language entirely that displaces it, Fixing some ecosystem problems upfront (packaging, concurrency/GIL). Nim is possible candidate, though Go is pretty popular w/ Pythonistas. My personal unhappiness with how Py3K was handled, plus recent PSF events make me feel new leadership would also be a boon.. reply gjvc 3 hours agorootparentprev>>> I'm hoping Python 4 will be a big breaking change similar to the previous one Python3 did not break enough to justify the jump from 2 to 3, IMHO. >>> and full support for explicit types will be one of the reasons. Fair point! reply klardotsh 48 minutes agorootparentPython 3 broke almost literally every non-trivial Python file on earth, and was saved only by `2to3` and `six` being able to automate or library-ify away 75% or so of the changes. The remaining 25% was make-work for teams needing to avoid the deprecation/EOL/vuln demons (or wanting to take up new language features that became Py3-only), and many teams took the opportunity to instead spend that time rewriting their codebases in Go or TypeScript+Node. I don't know how much more breakage you really wanted out of Python 3 if \"permanently scoured the public image of the language and caused many Python shops to, at least partially, stop being Python shops\" wasn't enough. (I say this as a still-fan of Python who has written quite a lot of it and contributed to MicroPython/CircuitPython's internals - I just also worked at a Python shop during the Py2->3 hell, and frankly, even my current dayjob still talks about that transition as a nightmare to watch out for if any other language starts doing similar talk.) reply hyperbrainer 2 hours agorootparentprevIt did though. Anything worse would be reminiscent of the Perl5->Perl6 disaster. reply bryanlarsen 2 hours agorootparentprev> I'm hoping Python 4 will be a big breaking change similar to the previous one and full support for explicit types will be one of the reasons. History shows that's a good idea if and only if you use a new name for the new language. cf Python 2 -> 3, Perl 5 -> 6 -> Raku, Javascript -> Typescript You can keep Python as part of the name. Call it SuperPython or something. Just don't call it Python 4. reply thiht 2 hours agoparentprevI love using the bare except in small 1-file scripts, it just does the job elegantly reply dataflow 3 hours agoparentprev> force more characters for the same functionality This is actually a good thing in some cases (possibly this one). Risky stuff should inherently be harder to do than safer stuff, otherwise people will reach for the risky alternatives when they don't need to, just to save time - or because they don't realize the risk. Or at least, that's often the case. What's lacking here is evidence that this is actually happening. I can believe it, but evidence is necessary for breaking the language. reply amelius 2 hours agoparentprevI do have to say that this fun little scripting language is starting to look more and more like a serious compiled language. reply williamsmj 3 hours agoprevI have mixed feelings about this. There are two \"problems\" this PEP is trying to solve. One is that bare excepts are permitted. The argument against this is that explicit is better than implicit. A matter of taste, but I don't find this convincing. The other problem is what bare excepts mean. Bare excepts are syntactic sugar for `except BaseException`. This means that an application containing a bare `except` followed by the vast majority of real-world error handling will continue to run even if SystemExit or KeyboardInterrupt is raised. This is almost always a bug. I do find this second argument convincing, and I wish Python did not contain this design wart. If I could go back in time and change Python syntax, it would make it hard for people to silently treat these special interrupts as \"handleable\" like regular errors. The tiny set of applications that really can and should handle them (e.g. TUIs or the mailman example discussed in the final section of the PEP) can explicitly do so with e.g. `except KeyboardInterrurpt` or even `except BaseException`. But I agree with the consensus here that this does not rise to the level of something being worth a backwards-incompatible change. reply theamk 2 hours agoparentDisagree. I do this kind of code all the time: try: something() except: log_tons_of_debug_info() raise and I am very glad that I get my debug info works even if I press Ctrl-C or someone calls sys.exit(). reply 1st1 29 minutes agorootparentJust noting it here: your code is incorrect. In case of a KeyboardInterrupt error and another error raised by `log_tons_of_debug_info()` (there's no error free code, right?), KeyboardInterrupt would end up being masked (it would go into the __context__ attribute of another error). The program won't abort its execution. And it's just one example out of many where it's critical to not mask error types. Correct code would be: try: something() except BaseException as ex: try: log_tons_of_debug_info() finally: raise ex But really, you don't want to mess with BaseExceptions at all, so just do `except Exception` instead of a bare `except:`. reply bee_rider 1 hour agorootparentprevI dunno, this just usually means I’m going to hold control and mash C, hopefully I can get my interrupt to occur inside your except reply rcxdude 1 hour agorootparentThat's unecessary, because of the 'raise' statement. This construct effectively only hooks exceptions, it doesn't swallow them. reply encoderer 49 minutes agorootparentprevSignals are non-reentrant. reply williamsmj 1 hour agorootparentprevAnyone reading your code is going to assume this is a bug. The PEP is right that explicit is better than implicit. You should write `except BaseException` (whether or not this PEP is approved). reply TuxSH 1 hour agorootparent\"except:\" is explicit enough and \"except BaseException\" is redundant. Moreover I think there is a real risk people are going to write \"except Exception:\" instead, which breaks in the fringe case an exception that derives from BaseException (enforced by interpreter) but not from Exception is thrown. Even if catch Exception is what users usually mean, changing code from \"catch (BaseException):\" to \"catch Exception:\" may break some code if improperly reviewed. It's also not worth breaking production code over this. reply rcxdude 1 hour agorootparentprevOnly if they don't understand what 'raise' means. It's obvious this construct is just injecting some additional information in a passing exception, there's no issue if it catches everything. reply rectang 2 hours agoparentprev> Bare excepts are syntactic sugar for `except BaseException`. I'm guessing that a `3to4` script would be provided which replaces bare `except:` with `except BaseException:`. We have the experience of `2to3` to draw on with regards to how that might play out. EDIT: Haha, I now see that this PEP proposes a change without advancing the major version. That surprises me. reply williamsmj 1 hour agorootparent1. Such a script is proposed in the PEP. 2. Python does not use semantic versioning. 3.13 is a different major version to 3.12. reply gmueckl 2 hours agoparentprevJava solved the problem by having Throwable as the root of all exceptions and not advertising that fact loudly. The derived Exception class is the root of all safely catchable exceptions. When someone catches a Throwable, something strange is going on. reply williamsmj 1 hour agorootparentPython does the same thing. It just calls Throwable something different. Java Throwable ~= Python BaseException. Java Exception ~= Python Exception. The problem here is that a bare except catches something similar to Throwable, not something similar to Exception. reply MantisShrimp90 3 hours agoprevI think Rich Hickeys advice of not breaking people applies here. The anger from this potential change is that really all you are doing is taking something away that was working, and now people will need to review their code or keep python on a previous version which sucks. I think that people who propose these kinds of changes don't appreciate the importance of the programming language being at the bottom of the stack so there's really never a good reason to break people even if you think it's nicer as you really can't appreciate how much work you are creating for people. reply dig1 1 hour agoparentIMHO, working on a programming language is only for some, just like working on a database is only for some. The first rule should be: \"you should never break the language, ever\". Just like you should never break the database or kernel behavior. This is why I like to stick to C, Common Lisp, Clojure, and (to some extent) Java/JVM. I don't know about Clojure's future, but C and Common Lisp have been fine for the last 40 years, and I'm not expecting the least in the upcoming years. reply thefaux 5 minutes agorootparentI half agree with this rule. I think that it's fine to break things as long as you make a semantic version change _and_ provide automated tooling for upgrading old code. If you can't build this tool, that is a strong negative signal for both versions of the language. What I don't like about say, c, is that it has various backward compatible additive dialects like c11 vs c99. I personally don't agree that c11 and c99 are the same language in spite of the backwards compatibility and I think it makes the entire ecosystem worse. At some point there needs to be a successor rather than just piling on to old broken designs. I would prefer a better FFI or other tools to interface with legacy code in the new dialect. reply TeddyDD 1 hour agoparentprevPython breaks compatibility across minor versions. I'm not surprised seeing such proposal. reply dimator 1 hour agorootparentdo you have examples? reply sseagull 1 hour agorootparentOne painful one that is still reverberating a bit in some areas is the renaming of \"SafeConfigParser\" to just \"ConfigParser\" in the standard library (in 3.12). This caused a whole lot of breaking in some areas because versioneer (a package for determining a package version from git tags) used it (in code that was placed inside your package, and so couldn't be solved by just upgrading versioneer). Also, I'm starting to get warning about something in tarfile that I will need to track down: https://peps.python.org/pep-0706/ reply joshkel 4 hours agoprevInteresting. The way I was taught Python, you really, really don't want to use bare `except:`, because it catches _everything_: Ctrl-C interruptions, system exit, etc. Instead, you really ought to use `except Exception:` (where `Exception` is the base class for any \"normal\" runtime error). So I definitely understand the rationale, but it's hard to say it's worth the pain of backward incompatibility - we have linters, style guides, etc. that can catch this. reply wild_pointer 4 hours agoparentYes, I was bitten by it in the past. Still, it'd better be a lint, or at least a very very long deprecation period... like, deprecated and removed in Python 4 or something. reply shadowgovt 3 hours agorootparentHaving your linter catch `except:` is both simpler and cleaner than changing the language. reply dtech 2 hours agorootparentDeprecating without removing ever seems reasonable reply dimator 1 hour agoparentprevabsolutely, this should not be done at the language level. the language should not enforce \"best practices\", that's what the ecosystem is for. reply aftbit 4 hours agoprevOne must imagine Sisyphus happy. Python just loves to break working code on a regular basis with its new releases. If your code is protected from untrusted user data and the internet, Python 2 is actually a really nice language that doesn't constantly force rewrites. Oh, you want to know the naive UTC datetime in Python, to interface with something like PostgreSQL that recommends naive times? Back in the old days, a simple datetime.datetime.utcnow(). Now days, you need something like: try: from datetime import UTC as tz_UTC except ImportError: from pytz import UTC as tz_UTC dt = datetime.datetime.now(tz_UTC).replace(tzinfo=None) reply instig007 3 hours agoparent> Oh, you want to know the naive UTC datetime in Python, to interface with something like PostgreSQL that recommends naive times? Postgres never recommended naive datetimes. A TZ-aware datetime is semantiacally the same as a tuple of (, ). Those who recommended dropping the knowledge of the first part from that pair did it because they didn't know better. reply bigstrat2003 3 hours agoparentprev> If your code is protected from untrusted user data and the internet, Python 2 is actually a really nice language that doesn't constantly force rewrites. If Python 2 is acceptable for your use case, then you could stay on an old version of Python 3 just fine as well. reply echoangle 2 hours agorootparentI think the point was making sure that code won’t break in the future. If you tell someone „use python 2 to run my script“, you know it’s going to work basically forever because the latest python 2 won’t be changed. That’s not true for python 3. I still think it’s a bad argument, but that’s what I understood the idea as. reply cdrini 2 hours agorootparentThat argument still seems inconsistent to me, since saying \"use python 2, pin your dependences and never upgrade python so your script runs forever\" is the same as \"use python 3.12, pin your dependences and never upgrade python so your script runs forever\". reply echoangle 0 minutes agorootparentWell you can’t update python 2 so there is nothing to pin kccqzy 2 hours agorootparentprevNo you can't. For example I use a script to compress scanned PDFs by combining individually processed JBIG2 images and that script hasn't been updated for more than a decade: https://github.com/agl/jbig2enc/blob/master/pdf.py It works and generates perfectly good PDFs. No it doesn't work with Python 3 because it mixes bytes and strings copiously. I could spend half an hour upgrading it to work with Python 3 but there's no reason to. Don't forget the whole reason why Python 3 exists is because it broke compatibility with Python 2. Plenty of old unmaintained scripts were forever stuck in Python 2. Not to mention an old version of Python 3 actually performs worse than Python 2. reply aftbit 1 hour agorootparentWell I'd be careful, that's a classic example of untrusted user data. reply kccqzy 34 minutes agorootparentNothing is untrusted. I trust my own scanner. It's just that it produces files that are too large. reply xorcist 1 hour agoprevValid reasons for backwards incompatible changes to language syntax: 1. The language guarantees have become inconsistent, and the syntax breaks security boundaries or realtime guarantees that the language explicitly promises, and it is unfixable. 2. The universe of all written code is small enough that there are guarantees the syntax is unused, or we can change all instances of the syntax in an atomic manner. Invalid reasons for backwards incompatible language changes: 1. Everything else. reply sph 4 hours agoprevThis would be a backward-incompatible change, no? In other languages, such a change would only be possible with a major version bump, though I imagine that because of the Python 3 collective trauma, the language designers now are OK with breaking older code without calling it Python 4. Anything goes, as long as it's called Python 3. (Python lost me in the 2->3 migration and I haven't used it in a decade, so correct me if I'm wrong) reply mananaysiempre 4 hours agoparentI don’t think there have been significant backwards-incompatible changes in Python 3 the language. There have been some, e.g. async and await were first introduced as soft keywords and then switched to being actual ones. But that’s not all that different from the treatment of yield in Python 2. (Recent stdlib changes have been much more destructive, but I’m assuming that, like in the original thread, we’re drawing a distinction between those and changes to the actual language.) Full disclosure, I welcomed Python 3, because for me that was the first time (since 2.4 on Windows XP) that I could count on my programs not randomly shitting their pants upon encountering Cyrillic in files or filenames, which for a native speaker of Russian you can imagine is quite important. (The csv stdlib module in Python 2 did that, IIRC. Perhaps I was holding it wrong, but experience shows that absolutely everybody did.) reply echoangle 2 hours agorootparentI think basically every new python version removes some standard libs and marks new ones as deprecated (at least 3.13 did), that’s potentially breaking. reply neuronexmachina 3 hours agoparentprevYes, from PEP 760's draft: https://peps.python.org/pep-0760/ > This change is not backwards compatible. Existing code that uses bare `except:` clauses will need to be modified. To ease the transition: > * A deprecation warning will be issued for bare `except` clauses in Python 3.14. > * The syntax will be fully disallowed in Python 3.17. > * A `from __future__ import strict_excepts` will be provided to invalidate bare except handlers in earlier versions of Python. > A tool will be provided to automatically update code to replace bare `except:` with except BaseException: reply letmeinhere 4 hours agoparentprevPython doesn't use semantic versioning. The number after the first period is a major (annual) release and can and does contain breaking changes (though so far never on the scale of the 2->3 upgrade). We may never see a 4.0 because of the scar tissue, but the language continues to evolve. reply sph 4 hours agorootparentLinux: let's bump the major version just because Python: for the love of God [1] don't touch the version I wonder if the scar tissue will ever heal and we'll see a Python 4 in two decades. 1: https://www.youtube.com/watch?v=asUyK6JWt9U reply sgarland 2 hours agorootparentK8s: the major version will never change, so instead we’ll introduce breaking changes with minor versions. reply hnlmorg 4 hours agorootparentprevProbably not before a Perl 6 reply pdonis 4 hours agoparentprev> This would be a backward-incompatible change Yes, indeed it would. reply NelsonMinar 5 minutes agoprevPython is also a scripting language, not just a systems engineering language. Sometimes fast-and-loose error handling is appropriate to the task. reply stackskipton 3 hours agoprevThis is what not having sane BDFL does to organization. Only proper response is private email from sane BDFL similar to Linus email of \"WE DON'T BREAK USERSPACE\" reply Chris2048 2 hours agoparentThis is just a proposal though, having a (sane) BDFL needn't change anything. reply stackskipton 1 hour agorootparentSane BDFL would have already stepped in with \"Absolutely not\" and PEP closed with \"Will not implement\" EDIT: Add on, Private Email to Pablo going \"Dude, why are you purposing stuff that will break code spectacularly? I think we need to talk about your approach to language design.\" reply graemep 4 hours agoprevThis reeks of \"our users are idiots and we need to keep them away from sharp edges\". A bare except is something to be flagged up by tools, not disallowed by the language. It is definitely not worth a backward-incompatible change. I am slowly going off Python. reply forgottofloss 4 hours agoparent\"our users are idiots and we need to keep them away from sharp edges\" is exactly what keeps driving me away from Python and pip. It's why I wrote https://pip.wtf -- Python package management would be so simple if they'd just stop adding more and more seatbelts and cushions to Python. reply kstrauser 2 hours agorootparentFor others stumbling across this, the idea was considered in PEP 722 (https://peps.python.org/pep-0722/) and is supported today by uv (https://docs.astral.sh/uv/guides/scripts/#declaring-script-d...). reply rurp 3 hours agorootparentprevNo kidding about pip. The dependency resolver change several years ago was a similar terrible move to the PEP being considered here. It broke so much legitimately working code for no good reason; just paternalism from the core devs. The change pushed my team to stop using pip at all for dependency management. reply kstrauser 3 hours agorootparentHard disagree there. It was way too easy to get yourself into an incompatibility hell with the old resolver, where package A relied on transitive dependency X v1.2 and package B needed X v2.1. Which version of X you got depended on whether you installed A or B first. Yes, the new version did mean I had to straighten out a few projects that were already working before, but they were working by coincidence because my code paths weren’t stumbling across the incompatibilities. The problem already existed. The new resolver just exposed it. reply rurp 2 hours agorootparentMy case was different from yours. Our project wasn't working by coincidence, the dependency resolver was flagging incompatibilities that simply didn't apply to our case, and began refusing to build a stable working project. Yes it's more risky to override that kind of guardrail, that's why I would only do it when I know the risks and tradeoffs and determine it's the best course of action on balance. I strongly believe that tools should ultimately work for the user, over dogmatic principles. I'm fine with the those safety guardrails being the default behavior, but removing any sort of escape hatch because the pip devs think that they know better than the users of the tool 100% of the time is what I object to. In the end we ended up ditching pip entirely for this use case and ended up with a much better system, with absolutely no disasters as a result, but we had a burn a lot of time and angst that could have been spent on actual problems we were trying to solve. reply kstrauser 23 minutes agorootparentAsking out of curiosity, not to insinuate that \"you were holding it wrong\". The docs at https://pip.pypa.io/en/stable/user_guide/#resolver-changes-2... say: > If you don’t want pip to actually resolve dependencies, use the --no-deps option. This is useful when you have a set of package versions that work together in reality, even though their metadata says that they conflict. For guidance on a long-term fix, read Dealing with dependency conflicts. Did that not work? reply ziml77 1 hour agoprevFortunately the votes on the poll for this look to very much be against the PEP's proposal. I don't mind Python being improved, but as we learned from the 2->3 transition it should not be changed in ways that break old code. All that will do is have people forever sitting on an old version of Python. That's a worse situation that having code with bare excepts. reply bee_rider 4 hours agoprevThey explicitly describe the PEP as evil, is there a tradition in the Python community for having obviously terrible PEPs, just to document the reasons for not doing something? Because that would make this a lot more understandable. reply jjice 16 minutes agoparentThis was on April Fool's Day, but the roman numeral constants PEP always gives me a laugh https://peps.python.org/pep-0313/ reply arp242 3 hours agoparentprevIf it's not a serious proposal then that's even worse, because there's a long discussion on that thread. So this non-serious proposal is just wasting people's time. reply veggieroll 2 hours agoparentprevGenius-level risk taking reply bee_rider 1 hour agorootparentKinda yah, I mean, it seems like it is getting a strong negative response based on the poll. reply eesmith 3 hours agoparentprevThere is PEP 666. https://peps.python.org/pep-0666/ > I figure if I make this PEP, we can then ask Guido to quickly reject it, and then when this argument next starts up again, we can say ‘Guido isn’t changing things to suit the tab-haters or the only-tabbers, so this conversation is a waste of time.’ ... > This proposal, if accepted, will probably mean a heck of a lot of work for somebody. But since I don’t want it accepted, I don’t care. I don't know if there are others. reply smbullet 1 hour agoprevThis seems silly. I hate backwards compatibility but this change will just cause people to use `except BaseException` everywhere which seems even less idiomatic than bare excepts. ETA: Nobody is going to dig through a large codebase to find exactly what exceptions can be bubbled up if they didn't design it with explicit exception handling in mind from the beginning. It will also potentially become a pattern people will copy in new code. reply OscarCunningham 4 hours agoprevOne example of a time I used a bare except was when I wanted a program to retry three times if it failed for any reason. I just wrapped everything in a for loop with a catchall except. The problem occurred when our scheduling program (Airflow) noticed the program was taking too long to run and decided to kill it. It sent a kill signal to Python, which dutifully caught the exception, retried and continued to run. I had to add a special case to allow Airflow to kill the program. This PEP just forced me to look up the difference between the classes Exception and BaseException. It turns out that BaseException includes every exception, whereas Exception excludes those that are trying to exit the program (like SystemExit and KeyboardInterrupt). reply williamsmj 4 hours agoparentWith a bare except, your code will continue to retry even if SystemExit or KeyboardInterrupt is raised. This is almost always a bug. In other words, your comment is an argument for the proposal! I don't think it's a good enough argument to make a backwards incompatible change. This is a wart Python has to live with now. But I do think it's a shame that bare excepts behave in a way that is almost always a bug. reply bee_rider 4 hours agorootparentMaybe bare excepts could be modified to just catch Exceptions. It seems like a reasonable expression of the idea: everything that could go with my program but not with the OS. reply kstrauser 3 hours agorootparentThat seems less bad in the sense it would affect fewer people, but the ones it did affect would likely be much more strongly affected. For instance, I could imagine someone with an old daemon that had a too-level loop like: while True: try: serve() except: log(‘oops’) so that it was more or less bulletproof. This might be a highly unpleasant change for those people who counted on it running 24/7 and never dying. In other words, the current behavior is a minor hassle for many people. That change would be a major hassle for a few. I’d be all for a deprecation warning on bare excepts. That might nudge a lot of people to fix their code without actively breaking anything. reply williamsmj 3 hours agorootparentprevPersonally I think that would have been a better choice in Python's original design, but to change it now would be a backwards-incompatible change, i.e. it suffers from the same big problem everyone is highlighting in the PEP. reply cb321 1 hour agoprevPossibly relevant -- Nim has a compile-time warning for bare exceptions, initially enabled by default [1]. I think Nim-core changed their mind about this being a good idea a few months later: https://github.com/nim-lang/Nim/pull/21728 (though the message still says \"The bare except clause is deprecated\" if you do compile with --warning:BareExcept:on - I think the urge to actually deprecate has gone away). I think for Python, rather than breaking bajillions of unpublished lines of code they should start with a more tentative & minimally invasive environment var/CLI switch opt-out warning that says something like \"may be deprecated\" where even the Python ./configure script lets you opt out at Python interpreter-compile-time. Measure the scope of the porting problem for a few years before trying to decide on The Plan for something that might be too disruptive. [1] https://github.com/nim-lang/Nim/commit/91ce8c385d4ccbaab8048... reply wyldfire 1 hour agoprev> Currently, Python allows catching all exceptions with a bare except: clause, which can lead to overly broad exception handling and mask important errors. The fact that this pattern catches NameError and other things which are obviously design errors means that it is a really bad behavior which is unfortunately common. Of course, many folks in this comment section and the PEP discussion thread point out the pitfalls with the suggested remedies. It would be great if some amount of linting/warning/static check could be devised to help people uncover the problem though. reply dcchambers 1 hour agoprevELI5: Why do people love Python so much? From an outsiders perspective: The ecosystem is a disaster, on a level even exceeding that of JavaScript IMO. The 2->3 transition was awful and lead to a rift in the python community for years (still causes issues 15 years later). Maintainers seem happy to introduce breaking changes without major version bumps. It's not that performant of a language (slower than modern Ruby, for example). Best thing it's got going for it is readability. reply linguae 26 minutes agoparentPython is not my favorite programming language, but back in 2006 Python felt like a breath of fresh air for writing short programs. I felt more productive in Python than in C, C++, or Java, which were the other languages I knew at the time (I was an undergraduate CS student), and I still use Python as my first choice for short data processing scripts. In addition, Python has many libraries, built-in and external. When I worked as an AI researcher before returning to academia, Python was our team's main language since there's a rich ecosystem of numerical computing and machine learning libraries. I agree, though, that the ecosystem has many rough edges, especially when it comes to package management, dependency management, and environments/containers. This is especially true in the AI ecosystem, especially as a researcher, where I had to deal with third-party code that is sometimes written by people who are solid scientists but have little software engineering experience. I'm now back in academia as a teaching-oriented professor; it's been a few months now since I've had to use pip and conda, though I have written some Python scripts to aid with grading. I teach C++ and Haskell to undergrads now, and I also have side projects involving Scheme and Common Lisp :). reply gnulinux 48 minutes agoparentprevPython ecosystem is a disaster for certain reasons but there is simply no ecosystem better than Python for other reasons. It's a trade-off. For literally any niche problem you can find out there there will be a Python library somewhere in the annals of the internet. I use many many many programming languages and everything starts out as a Python script in my flow, because by the time you start half the code is already written. reply new_user_final 4 hours agoprevThis person should be removed from the Steering Council Member. What an insane proposal. reply move-on-by 2 hours agoprevI just completed upgrading a monolith from Python 3.8 to 3.11 - no doubt many others in the same position with 3.8 going EOL. It was a monumental effort. I will say the huge majority of work was upgrading libraries that hadn’t been updated in years. I won’t go into the specifics of why we had chosen not to update these libraries earlier (unless there is interest), but I will say Python being as backward compatible as possible has huge real world value. More for the community and ecosystem than the language itself. For the people who care about PEP 760, they have their choice of linting tool to enforce this requirement. reply BurningFrog 2 hours agoprevI've had and seen this philosophical debate a few times. To me, \"if anything goes wrong, do the following\" is perfectly valid semantics that appears a lot, and a bare excepts is a fine way to implement that. I think what confuses these discussions is that a common \"rookie mistake\" is slapping on a bare except when you really should be specific. For some people, this is reason enough to blindly enforce a \"bare excepts\" rule. To me, the costs vastly outweigh the benefits in this case. At it's core, this might be a personality type issue more than anything else. reply _hl_ 2 hours agoparentWhat’s wrong with being explicit about “I really do mean anything that goes wrong” by catching the base class? reply nomel 28 minutes agorootparentI don't understand this. To me, a bare exception is explicitly that. By not providing a specific exception, you're saying \"nothing specific, literally anything\". reply BurningFrog 1 hour agorootparentprevThat's actually the better way for me as well! So my comment is somewhat off topic. I still don't like the proposed change because of how much existing code it would break, but if we're designing a new language I approve. reply _hl_ 26 minutes agorootparentI see - I suppose that’s a fair viewpoint to have! I’m not much of a python programmer but my experience with the language would make me tend to agree actually. There are bigger fish to fry and so the effort to go after this relatively tiny sardine is perhaps not worth it. reply dtech 2 hours agoparentprevThe pep explicitly adresses the use case, and it's still allowed. The point of the pep is that it should be explicit, especially because the short form doesn't show whether catching terminating exceptions is a bug or intentional reply bunderbunder 3 hours agoprevI get that lately Python has decided it wants to be an industrial-grade enterprise programming language. But there's a part of me that misses when the Python community retained a \"we're all adults here\" ethos. reply berdario 2 hours agoprevA bunch of people are mentioning the bugbear of the Python3 migration, but there's an important difference that makes the migration a lot simpler for a backward-incompatible change like this one proposed in PEP760: You can just write code that is compatible with Python runtimes both before and after the change. That means that you can use the same test suite, gradually getting the code more and more compatible with the new version, and you can switch your production runtime, without having to worry about a more complicated and involved rollback process. Notably, in the Python 2->3 migration, it was not really possible (at first[*]) because \"\" (and b\"\") literals became -> b\"\" while u\"\" literals became -> \"\" So, there was no way to write literals that would mean the same thing, and have the same type across the two versions This is also the reason why libraries like six offered a `six.u` function (https://six.readthedocs.io/#binary-and-text-data) but that required banning use of non-ASCII codepoints in your string literals [*] This was eventually addressed with PEP 414 (and of course, even with with PEP 414, the migration was not trivial) https://peps.python.org/pep-0414/ reply move-on-by 52 minutes agoparent> That means that you can use the same test suite, gradually getting the code more and more compatible with the new version, and you can switch your production runtime, without having to worry about a more complicated and involved rollback process. This is all well and good for your own code, but it’s seldom the case the libraries. A new library release that ‘adds support for Python 3.14’ is very likely to include other changes in the same release that may or may not be trivial, even assuming you were already on the latest version of the library prior to needing to update. A change like this to the Python language might be trivial, but it would have a massive impact on the ecosystem. reply tln 3 hours agoprev-1000 Why would you ever consider breaking everyone's throwaway scripts?? For what is already a universal linter rule? reply lpapez 1 hour agoprevIf this was Go, instead of making a breaking change they would opt for a linter rule or a go vet directive. This is the sensible approach IMO to handle a hazardous programming construct. Warn people about it and give them the choice to shoot their foot. But don't break their code. reply thih9 45 minutes agoprevA lot of comments are about this affecting existing unmaintained code and causing problems with upgrades. But couldn’t the old code be automatically updated? E.g. wouldn’t replacing every ‘except:’ with ‘except BaseException:’ make old codebase compatible with the proposed change? Sure, that’s a pain too and I’m not a fan of the change itself either; still, a breaking change that can be addressed automatically sounds relatively easy. reply berkayozturk 2 hours agoprevEven though there are valid use cases for having catch-all clauses, I see people forget about properly handling SystemExit exception. If your service receives SIGTERM from the scheduler, you need to capture it and gracefully handle the shutdown instead of swallowing it. reply saikia81 4 hours agoprevPython errors are a mess. It's no surprise people overuse bare except clauses. Disallowing them is not the solution we need. reply dopylitty 3 hours agoparentThey really are hard to work with compared to languages where you declare which exceptions you'll throw,. I always get angry when pylint raises an error for catching over-broad exceptions[0] Of course I'm catching broad exceptions because I have no idea what kind of exception is going to be thrown 12 dependencies deep and I don't want it to completely crash the program instead of letting me retry or do something else. 0:https://pylint.readthedocs.io/en/stable/user_guide/messages/... reply dmart 1 hour agorootparentYup this drives me crazy. I've been bitten by urllib3 or SSL exceptions being bubbled up by random libraries so many times that now I always include an except Exception: block just in case. reply byb 2 hours agorootparentprev100% agree. Most of my bare except: are followed by import pdb;pdb.set_trace() so I can figure out what went wrong and then fix my code so that it never happens again, but I still leave it there because I I don't have time to consider the millions of ways my hastily thrown-together python script is going to fail nor do I want to game out how many different errors could happen. If Python would have been this hard to use 20 years ago, I wouldn't have been able to learn to program. reply prpl 3 hours agoprevThis is something that can be handle easily with static analysis and should not be a language feature reply veggieroll 2 hours agoprevI switched to Go years ago because I know that code I wrote a decade ago is still going to work. And that guarantee is even more ironclad in recent years with modules and vendoring. With a Go project, I can leave it to sit for 3+ years and then pick it back up and add a feature without any issues. I've never had that with a Python project. (and this isn't even about the 2-to-3 situation, I just mean minor version to minor version and packages) reply joshlk 1 hour agoprevWould it be possible to move all the language developers to work on packaging? IMO the Python language is feature complete but the packaging system needs heart surgery. reply czscout 2 hours agoprevPersonally, I don't agree with this proposal. While yes, I agree, that bare excepts are often a source of bugs, I don't think it should be the language's responsibility to nanny the programmer on such things. To me, this seems to only reduce the functionality of the language. If explicit exception handling is necessary, let the programmer make that decision. reply martinbaun 2 hours agoprevI love Python, but it seems to always get more complicated and more things in the core and now backward incompatible changes. Not so good. I love Python and how much I can get done, but I must admit I also love that I know the Go code I have from 7 years ago can run without problems. It is just more stable reply dsign 1 hour agoprevI know what this is! They want to \"to give it\" to Jeff Bezos, by breaking AWS' boto3, that has a nightmare of a story for handling/catching specific exceptions. It's all politically motivated! reply b5n 2 hours agoprevI've spent a lot of time fixing/explaining python exceptions over the years, and I get pretty annoyed when I encounter bare exceptions. Exceptions themselves are so often misunderstood, it seems most people just take them at face value. However, do we really need to dull all the sharp edges and add guardrails to every fucking thing? In a corporate environment, sure, you can implement all the protections you like _without attempting to force your constraints on all users_. If you care about types, safety, etc. there are plenty of fantastic projects that share your priorities, but they don't need to bleed into everything under the sun. Sharing and adopting new ideas is healthy, but homogenization kills creativity. Maybe I'm just grumpy today. reply zmnayt 1 hour agoprevAs many people have observed here, this is a couple of Steering Council members showing activity. Getting one's PEPs accepted has a totally inflated weight in the Python \"community\". The more, the better (by contrast very few people care about perfect and bug-free code). So, if this thing is accepted, it pads the resume of certain people even more. And many software orgs will have one additional week of job security by rewriting existing code. It's a win-win situation. Ever since the walrus operator coup Python has descended into madness and make-work initiatives. reply pansa2 4 hours agoprevIn terms of “break everyone’s code for no good reason”, this proposal is comparable to the removal of the `print` statement. reply kstrauser 4 hours agoparentAt least that one was beneficial in the long run. It had clear advantages beyond purity. reply Scea91 1 hour agoprevNot all python code is production code. Sometimes I just need to write simple one-off script for one-off task. For that bare except is totally fine. reply kkirsche 1 hour agoprevI wish people would stop holding onto compatibility as if it is some amazing feature. It has benefits, but also comes with many drawbacks to innovation and improvement in established ecosystems reply bjourne 4 hours agoprevAs others have stated this idea is gratuitous breakage. Hope it won't become reality. reply gweinberg 1 hour agoprevThis has to be some kind of joke. If it had gone the other way and python had originally required people to write \"except BaseException\", a PEP to allow a bare except would be an obvious improvement to the language, since it would allow people to do the same thing and save pointless typing. This proposal is suggesting we break virtually all existing Python code to make the language objectively worse. I'm guessing this is a sort of \"modest proposal\" type parody. reply peterhadlaw 4 hours agoprevWhat does Tim Peters think about this change? reply williamsmj 4 hours agoparentWe may never know https://news.ycombinator.com/item?id=41234180. reply drcongo 4 hours agorootparent[ranier-wolfcastle-that's-the-joke.jpg] reply ck45 2 hours agoprevI don't remember there to be that much complaint about https://peps.python.org/pep-0352/ and from having worked on a large Python code base at that time, it was rather easy change. reply jgb1984 1 hour agoprevAs someone who is using python professionally for over 17 years I sincerely hope this PEP gets rejected. reply pjmlp 3 hours agoprevI hope this goes nowhere, it adds very little value, to the expense breaking compatibility. reply braiamp 1 hour agoprevI don't know why this is seen with such aversion. This is the language forcing sane coding practices, since you always include what should be cached. Also, the syntactic sugar for except BaseException is to not catch it in your `try ... except` clause. If you do, for example: >>> try: ... raise TypeError ... except ValueError: ... print(\"keke\") ... raise ... TypeError will bubble up. If you want to catch everything, and handle it, like the example from Mailman[0], you should catch the base exception anyways. This can be solved and detected by static analysis tools anyways. [0]: https://gitlab.com/mailman/mailman/-/blob/master/src/mailman... reply kh_hk 2 hours agoprevSeems like a troll PEP and still would not surprise me. Explicit is better than implicit, except if it makes you look like a fool. reply dataflow 3 hours agoprev> which can lead to overly broad exception handling and mask important errors I'm sure it can, but some evidence of this actually happening feels rather critical for a change that will break the language for everybody. reply cpburns2009 3 hours agoprevThe proposal (not part of PEP 760) to add an implicit `raise` to the end of bare `except:` blocks would be far more damaging than making bare-excepts a syntax error. reply ramses0 3 hours agoparent[Citation Needed]? reply cpburns2009 2 hours agorootparent> Now there’s an interesting idea: don’t make bare except illegal, make it have an implicit raise at the end (and disallow return, break, and continue). https://discuss.python.org/t/pep-760-no-more-bare-excepts/67... reply ramses0 2 hours agorootparent...ahh, a bit of a misreading on my part. However, I was really looking for your explanation of the horrors that could happen rather than what the suggestion was. Thinking through a bit more, yeah, implicit re-raise does have some pretty bad outcomes if you can't change code in a dependency. It still feels like `deno` is somewhat on the right track where permissions are dropped by default, but It'd Be Nice(tm) if programming languages enabled that a bit easier. import sales_tax_calculator as xyz with [ cap.NETWORK, cap.FILESYSTEM, cap.USB, ...etc... ] xyz.calculate( sales_price, state=user.address.state ) We're implicitly allowing imported libraries the full power of the containing programming language where with promiscuous code sharing (trending towards a low-trust environment), it'd be a lot better to _not_ give `cap.FS, NETWORK, USB, etc...` by default. Bringing it back around: `import somelib with [ cap.BARE_EXCEPT, cap.RAISE ]` or something to control their handling of \"unknown\" exceptions is interesting. Let them handle any exceptions or interrupts they've authored, but let me explicitly have control over catching stuff that isn't \"from them\". ...an extended version of dependency injection or inversion of control. reply cpburns2009 1 hour agorootparentAdding an implicit raise to the end of a bare-except would quietly break things, and is non-trivial to detect. Say you have a naive base-except: def loop(): try: check_service() except: logging.exception(\"Error while checking service.\") time.sleep(60) Really you shouldn't be using a base-except here. You should at the bare minimum catch `Exception`. Adding an implicit `raise` at the end will break this function without so much as a warning. Instead of calling the function every minute, the loop is broken with an unexpected exception that was deliberately suppressed (and logged). A more common scenario for myself is write a lot of my scripts in the style: def main(argv: list[str]) -> int: # Parse args, setup logging, etc. try: run_script(args) except: log.exception(\"Script failed.\") return 1 return 0 if __name__ == '__main__': sys.exit(main(sys.argv)) An implicit raise would obnoxiously break them when my bare-except is intentional, and effectively cause the error to be printed twice to the terminal. Now I'm not wholly opposed to forcing `except BaseException:` instead of `except:`, but an implicit raise would cause all sorts of subtle bugs. reply amelius 2 hours agoprevNext step: ... except Exception as err: pass Error: variable err not used! reply __turbobrew__ 2 hours agoprevContinuing Pythons tradition of breaking perfectly functional code. reply wiz21c 4 hours agoprevLet me check, we're not the first of April, are we ? reply torginus 4 hours agoprevwill this be one of those wonderful changes that will make most python programs unrunnable on contemporary versions of python? reply jmyeet 4 hours agoprevI'm not a fan of this for two reasons. First, consider this motivation from the PEP: > Requiring specific exception types makes the programmer’s intentions clear and encourages thinking about what exceptions might occur. This reeks of the same rationale for checked exceptions in Java. That was a failed experiment. You can't force people to deal with exceptions. They end up just swallowing them instead. It's better to propagate an exception than do that almost all the time. Like will we see except: replaced with except object:? I don't even know if that's valid. It's never come up. Second, this would be a breaking change. I really feel like this is where Python 3 went off the rails. In the Python 2 days making breaking changes was essentially verboten. But ever since Python 3 decided breaking changes were OK It is hard for me to articulate how much peps like this reinforce my desire to never start another python project I completely understand this sentiment. Recent python events have made me wonder if there are some people intent on sabotaging the management of the language. I loved the incremental improvements and thoughtful process involved up until a couple of years ago but it feels like python will become brittle and break badly if things continue the way they are. It feels like the adults have been driven out the room when it comes to stewardship. I'm not sure how recoverable the situation is. reply nightpool 3 hours agorootparentAs someone who doesn't follow the language, which recent events are you referring to? reply carapace 3 hours agorootparentIt wasn't recent by Internet time but when the debate on walrus operator drove out the BDFL that was the obvious break. Python has been circling the drain ever since. A lot of motion, yes, but to what end? - - - - Oh! How could I forget!? The creeps actually banned Tim Peters! reply behnamoh 3 hours agorootparentprevUsing \"|\" to merge dictionaries (which was possible in other ways before) instead of offering pipes as in bash and Elixir (a feature that's actually useful). reply eloisius 3 hours agorootparentThe “|” operator was already used for set unions and binary OR, so it’s a little late to reserve it for control flow. Personally I don’t mind having a “dict union” operator at all. reply btown 2 hours agorootparentprevhttps://peps.python.org/pep-0584/ is the PEP for merging dictionaries; sadly, it barely mentions pipes as a consideration. To be fair, the notion that pipes are lower-priority than other syntax needs is not exclusive to Python: in the JS world, discussion in https://github.com/tc39/proposal-pipeline-operator and specifically https://github.com/tc39/proposal-pipeline-operator/wiki/Bike... has been going on since 2018, with things like Tuple Literals taking precedence. On the Python side, though, at least you can build your own pipes! You can define various helper classes that have, say, an `__rrshift__` method, to let you do the following with full type-checking support: load_iterable_from_db() >> to_dict_by(\"id\") >> tee(logger) >> call_(dict.values) >> to_dataframe (With great apologies to FP folks who see a bind operator, and C++ folks who have seen enough operator overloading for a lifetime!) Not necessarily something you want to use unless you want to confuse your team, but quite useful for fluent code in notebooks! reply kristjansson 2 hours agorootparentprevLike all of python, `ab` operator is just `a.__or__(b)`. If you want that operator to do something different in a different context, just override __or__. reply eru 3 hours agorootparentprevElixir's 'pipes' always felt very hacky to me. (But so does most of the language, compared to Erlang.) reply behnamoh 3 hours agorootparentIn what ways does Elixir feel \"hacky\"? I get that it can be inconsistent at times but the whole language is really a Lisp-2 in disguise. reply Narhem 3 hours agorootparentprevI feel like as a scripting language Python excels. Glad to have this PEP, but it would be more pythonic have except be optional. The reason I pick up Python for projects is because it grows with the application; opportunities to add typing etc. Who knows maybe in a few years Python will enforce all the types and it will be as verbose as Java. Personally I’d like to see how they handle declaring a method or function throws exceptions. Pretty narly we have compiled Python apps with poetry, it’s starting to punch out of its weight class. reply diggan 4 hours agoparentprevSo it doesn't matter if it goes through or not, just that someone proposed a change like this is enough to steer you away from Python? If the change goes through, couldn't you just use older Python versions for those specific projects, or has the Python ecosystem still not figured out how to do this without huge hassles? reply hamandcheese 3 hours agorootparentThat version will eventually become EOL, stop getting security patches, eventually stop compiling with the latest OpenSSL, etc. Bitrot. reply diggan 3 hours agorootparentDoes that matter when you just want to run \"old scientific code\"? Old version of libraries like OpenSSL can still be run in that context, granted you don't expose that code to the internet at large. reply lbhdc 3 hours agorootparentOld scientific code broke for many people with the introduction of the mac m1. I would think this would be a continuing trend in the future. Staying on old versions simply isn't possible over a long period without keeping the hardware going with it too. reply diggan 1 hour agorootparent> Old scientific code broke for many people with the introduction of the mac m1. How could the people maintaining Python possibly avoid that? It would be up to Apple to proactively reach out to affected projects, if Apple cares about that. reply dumpsterdiver 3 hours agorootparentprevWhen the qualifier is \"granted you don't expose that code to the internet\" then yes, it matters. reply diggan 1 hour agorootparentWho finds \"old scientific code\" and then exposes a server running that code to the internet without any changes? Sounds like asking for trouble, but I guess we all use computers differently... reply northernman 3 hours agoparentprevPerhaps in a few years we can have another PEP, to require \"except BaseException\" to be replaced with bare \"except:\". Then we can all change our code back again. reply dpwm 4 hours agoparentprevFrom the PEP: > A tool will be provided to automatically update code to replace bare except: with except BaseException:. reply arp242 4 hours agorootparentThat's besides the point. I don't want to muck about with tools on my Python scripts. I have sometimes not run a Python script for a few years, and then when I need it, it stopped working and I need to track down what changed/broke or run some tool or whatnot. I don't keep track of the latest greatest Python changes – like most Python programmers it's not my \"day job\" to write Python code so I now need to track what changed between \"the Python version I used about 3 years ago, whatever that was\" and now. It's pretty annoying. And that's assuming said tool will be fool-proof. Never mind of course that all my dependencies (if any) will need updating too. What will happen in practice is that people will write \"except Exception:\" rather than \"except:\" and do nothing different. Basically nothing will change. Meanwhile, I have C and Go programs that have worked without modification for about 10 years. Not that nothing ever breaks in C or Go, but it's the exception (hah!) rather than the rule. reply pansa2 3 hours agorootparent> like most Python programmers it's not my \"day job\" to write Python code I’d love to know whether that’s true, and to what extreme. I believe you’re right - that people using Python for a few hours a week (or less) greatly outnumber software developers using it as their primary language. I think that’s a real issue for the evolution of Python, because updates to the language design (e.g. the makeup of the Steering Council) come almost entirely from the second group. reply mannykannot 3 hours agorootparentprevWhich just underscores the point that this is mostly software engineering theater. If your goal is a system in which all exceptions are explicitly and appropriately handled, your first mistake was picking Python. I propose a rider to the PEP in which implementation will be deferred until its proponents can correctly affirm that the library reference lists, for each function and method, every exception it might throw. reply hawski 4 hours agorootparentprevThank goodness there was 2to3 tool in the past. It made the migration to Python 3 so smooth and quick. /partial-s I know it is not nearly on the same level, but people seriously overestimate the effort needed between not doing anything at all and even the slightest work, no matter how reliable and easy. The difference between nothing and anything is huge. reply instig007 4 hours agorootparentprevwill there be a tool to upgrade all direct and transitive dependencies of your project to make them work in that new interpreter? reply Joker_vD 4 hours agorootparentprevI propose to call this tool 3to760, in memory of 2to3. reply plesner 3 hours agoparentprevIf someone on my team or in my company proposed to break most of our python code for no substantial reason, unless they were pretty junior I would count that as a real red flag against their judgement. How do people land on the python steering council exactly? reply setopt 3 hours agoparentprevI think emitting a warning every time an unspecific exception is caught might be a better balance. That way, you could still do a quick “try: … except: …” when drafting new code, but the code might warn you if the bare except block is ever used (including what exception was caught, and a suggestion for how to catch only that specific exception). reply linsomniac 3 hours agorootparentWith that PEP you can still do a quick \"try: except:\" it's just spelled \"try: except Exception:\" reply dataflow 3 hours agorootparentprev> warning every time an unspecific exception is caught Caught and not re-raised reply kstrauser 3 hours agoparentprevEh, while I sympathize with what you’re saying, PEPs get written and rejected all the time. I’ve gotten the impression that some were written for the main goal of documenting the reasons why a common request is a bad idea. Like, I don’t know if there’s a PEP to use braces, but it wouldn’t surprise me if someone had made one so that from then on there’d be an official doc you could point people at when they ask about it. Not saying this is one of those, and I see Brett Cannon’s on this one. I am saying not to get too worked up over the existence of a draft PEP. reply cortesoft 3 hours agoparentprevI often find discussions of these sorts, whether for python or other open source projects, get so focused on purity of concept that they totally forget practicality reply yunohn 3 hours agoparentprev> destroying the ability to run old scientific code TBH that kind of code barely survives minor Python version upgrades in my experience. reply shadowgovt 3 hours agoparentprevAnything popular is going to attract an increasingly high-variance group of engineers. With such variance comes such PEPS. reply a-french-anon 4 hours agoparentprevQuickly read the thread, isn't \"hours\" a bit much for what is basically a sed -Ei 's/^([\\t ]*except):/\\1 BaseException:/' **/*.py reply instig007 4 hours agorootparentnow try delivering that change to all of your dependencies before being able to deploy your software with a new interpreter. reply fny 3 hours agorootparentNot that I support the PEP but they could easily add an interpreter flag or environment variable to disable the behavior. reply Joker_vD 4 hours agorootparentprevWill this command be automatically run by venv, or poetry, or whatever, on every package update? reply shadowgovt 3 hours agorootparentprevDefinitely going to want to use Tree-sitter, not regex. That regex just broke my docstrings. reply himinlomax 3 hours agorootparentprevMulti-line strings don't exist. reply sdenton4 4 hours agoparentprevOn the bright side, turning bare exceptions into types exceptions is the kind of thing an llm is great for. It's also basically zero cost for new code. On the other hand, I completely agree that it's not worth a breaking change. reply relaxing 4 hours agorootparentDoes that even require an LLM? It should be possible through traditional static analysis. reply diggan 4 hours agorootparent> It should be possible through traditional static analysis Even better/worse, could do it with regex on text streams. reply sdenton4 4 hours agorootparentprevThere's some space for interpretation in picking exactly which exception type to use depending on context (value error vs runtime error vs not implemented error), and there may be package specific exceptions available. reply pansa2 4 hours agorootparentprevEverything requires an LLM nowadays. reply DrillShopper 4 hours agorootparentEspecially if you want to get funding reply AnonsLadder 3 hours agoprev [–] lol! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "PEP 760, proposed by Pablo Galindo Salgado and Brett Cannon, suggests disallowing bare `except:` clauses in Python to enhance error handling precision.- The proposal aims to prevent broad exception handling that can obscure significant errors by mandating explicit exception types.- The PEP encourages developers to adopt more precise error handling practices, with further details available on deprecation, tooling, and rejected ideas."
    ],
    "commentSummary": [
      "PEP 760 proposes to disallow bare `except:` clauses in Python, which currently catch all exceptions, including critical ones like system exits and keyboard interrupts.",
      "Critics argue that this change could break existing code, requiring users to upgrade or patch dependencies, which may be burdensome for incidental Python users.",
      "The proposal has sparked debate about backward compatibility and its impact on Python's ecosystem, with some suggesting it should be a linter rule rather than a language change."
    ],
    "points": 125,
    "commentCount": 246,
    "retryCount": 0,
    "time": 1728482081
  },
  {
    "id": 41780929,
    "title": "Lunar Lake's iGPU: Debut of Intel's Xe2 Architecture",
    "originLink": "https://chipsandcheese.com/p/lunar-lakes-igpu-debut-of-intels",
    "originBody": "Share this post Lunar Lake’s iGPU: Debut of Intel’s Xe2 Architecture chipsandcheese.com Copy link Facebook Email Note Other Lunar Lake’s iGPU: Debut of Intel’s Xe2 Architecture Chester Lam Oct 08, 2024 12 Share this post Lunar Lake’s iGPU: Debut of Intel’s Xe2 Architecture chipsandcheese.com Copy link Facebook Email Note Other 3 Share Intel has a long history of making integrated GPUs, and they’ve recognized how important iGPUs are to thin and light laptops. Today Intel is locked in fierce competition with AMD, and the company’s fight to hold on to the laptop market is no less intense on the graphics front. Lunar Lake is Intel’s latest mobile offering, and brings with it a new Xe2 graphics architecture. Xe2 is an evolution of the Xe-LPG/HPG architectures used on the outgoing Meteor Lake mobile chips and Intel’s current Arc discrete GPUs. It aims to better fit expected workloads and improve efficiency, and will be used across both Lunar Lake and Intel’s upcoming “Battlemage” discrete GPUs. Here I’ll be looking at the Arc 140V iGPU in the Core Ultra 7 258V. ASUS has kindly provided a Lunar Lake laptop for testing. Without their support, this article would not be possible. For comparisons I’ll be using the iGPU in Intel’s outgoing Meteor Lake, as well as the RDNA 3.5 iGPU in AMD’s competing Strix Point. GPU Overview Modern GPUs are modular, scalable designs and Intel’s Xe2 is no different. An Xe2 iGPU is built from Xe Cores, much like how AMD’s RDNA GPUs are built from WGPs (Workgroup Processors) or how Nvidia’s GPUs are built from SMs (Streaming Multiprocessors). Lunar Lake’s iGPU features eight Xe Cores, further divided into two Render Slices with four Xe Cores each. These Render Slices include fixed function graphics hardware like a rasterizer and pixel backends (ROPs). Lunar Lake’s iGPU is therefore laid out similarly to Meteor Lake’s iGPU, which also has eight Xe Cores split across two Render Slices. Intel’s Render Slice vaguely corresponds to AMD’s Shader Array, which also includes a rasterizer and ROPs. AMD also has a mid-level L1 cache at the Shader Array level, with 256 KB of capacity on RDNA 3.5. Intel also has caches at the Render Slice level, though only for fixed function graphics hardware. The figure below should be taken with a grain of salt because Intel has not published documentation for Xe-LPG, so I’m taking figures from the closely related Xe-HPG. I also find Intel’s documentation confusing. For example, each pixel backend should have its own color cache, but something changed after Tiger Lake and Intel only documents one color cache instance per slice despite drawing it twice in other diagrams. Xe2 aims to improve efficiency for fixed function units, and part of that involves better caching. The hierarchical Z cache helps reject occluded geometry before it gets past the rasterizer by remembering how far triangles are from the camera. Its capacity increases by 50% on Xe2, from 4 to 6 KB. At the other end of the pipeline, color cache capacity has increased by 33%. Xe-HPG had 16 KB of color cache capacity in each slice, so a 33% increase doesn’t make so much sense. Perhaps Xe-LPG had a larger 24 KB color cache. In that case, a 33% increase would bring capacity to 32 KB. Xe2’s new Xe Cores Xe Cores are the basic building block of Intel’s GPUs, and are further divided into Vector Engines that have register files and associated execution units. Xe2 retains the same general Xe Core structure and compute throughput, but reorganizes the Vector Engines to have longer native vector widths. Pairs of 8-wide Vector Engines from Meteor Lake have been merged into 16-wide Vector Engines. Lunar Lake’s Xe Core therefore has half as many Vector Engines, even though per-clock FP32 vector throughput hasn’t changed. Intel here is completing a transition aimed at reducing instruction control overhead that began with prior generations. Longer vector widths improve efficiency because the GPU can feed more math operations for a given amount of instruction control overhead. Meteor Lake’s Xe-LPG already tackled instruction control costs by using one instance of thread/instruction control logic for a pair of adjacent vector engines. But using less control logic makes the GPU more vulnerable to branch divergence penalties. That applied in funny ways to Xe-LPG, because sharing control logic forced pairs of Vector Engines to run in lockstep. A Vector Engine could sit idle if its partner had to go down a different execution path. Because there wasn’t a lot of point in keeping the Vector Engines separate, Intel merged them. The merge makes divergence penalties straightforward too, since each Vector Engine once again has its own thread and instruction control logic. Meteor Lake could do better in corner cases, like if groups of 16 threads take the same path. But that’s an awfully specific pattern to take advantage of, and Xe2’s divergence behavior is more intuitive. Divergence penalties disappear once groups of 32 threads or more take the same path. Even though Intel’s execution unit partitions have gotten bigger, they’re still small compared to AMD’s. RDNA 3.5’s SIMDs are equivalent to Intel’s Vector Engines, as both blocks have a register file feeding a set of execution units. AMD’s SIMDs can do 64 FP32 operations per cycle compared to 16 on Intel’s Vector Engines. However, comparisons are complicated because AMD has to use wave64 mode or compiler magic in wave32 mode to feed all of its FP32 lanes. Compute Throughput Lunar Lake’s iGPU offers very similar throughput compared to the prior generation, which isn’t a surprise because both have eight Xe Cores. The Core Ultra 7 155H (Meteor Lake) has a slightly higher 2.25 GHz maximum GPU clock than the Core Ultra 7 258V at 1.95 GHz. Sometimes Xe2’s architectural improvements make feeding the execution units easier, so some tests show improvement. Meteor Lake’s iGPU didn’t support FP64 through Vulkan Still, AMD’s Strix Point has a huge compute throughput advantage for common FP32 operations. AMD’s WGPs have twice as much per-cycle throughput as Intel’s Xe Cores, and AMD can clock their iGPU up to 2.9 GHz. Even if dual issue limitations prevent Strix Point from feeding all its execution units, AMD has a significant compute throughput advantage. Intel can catch up in some less common integer operations like INT32 multiplies. 64-bit integer performance has dramatically improved on Lunar Lake compared to Meteor Lake, but it’s not enough to catch AMD. Testing with OpenCL because Meteor Lake supports FP64 through that FP64 is another area where Intel catches AMD. Lunar Lake seems to have doubled FP64 throughput compared to its predecessor. None of these GPUs are good at FP64 compute, but Intel is ahead now. Matrix Units Matrix units are back in Lunar Lake after being absent in Meteor Lake’s iGPU. Each Vector Engine gets a 2048-bit XMX unit, offering four times as much throughput as the 512-bit vector execution units for the same data types. In addition, the XMX units support lower precision data types all the way down to INT2. From Intel’s video, they appear to work in SIMD16 mode on a 4×4 matrix with INT8 precision. Per cycle, a lane computes results for a 4-long row of the result matrix. Higher precision data types like FP16 are handled by operating on smaller chunks at a time. AMD RDNA 3.5 has WMMA instructions for matrix multiplication. WMMA operates on larger 16x16x16 matrices, and only goes down to INT4 precision. Unlike Intel, AMD doesn’t have dedicated matrix multiplication units, and executes WMMA instructions on dot product units. I’m glad to see XMX units show up in Lunar Lake. Matrix multiplication units are often used for upscaling, and upscaling is arguably more crucial in laptops than with discrete GPUs. Laptop GPUs often struggle to deliver playable performance in recent games, even with low settings and resolutions, so upscaling can make the difference between a game being playable or not. For discrete GPUs, upscaling simply allows higher quality settings. Cache and Memory Latency Caches remain largely unchanged within a Xe Core. A 192 KB L1 cache serves double duty as local memory, which Intel calls Shared Local Memory (SLM). Higher clock speed means Meteor Lake’s iGPU has slightly better L1 latency, though multiplying latency by clock speed shows cycle count is pretty much the same. Strangely, Lunar Lake seems to be stuck as if local memory were always allocated, so only 64 KB of L1 cache is visible from a latency test. Meteor Lake couldn’t use the whole 192 KB of L1 for caching like the Intel’s discrete A770, but it could still use 160 KB for caching. Meteor Lake would only drop to 64 KB if a kernel needed local memory. AMD uses a separate 128 KB Local Data Share (LDS) for local memory, so local memory allocations don’t affect AMD’s caching capacity. If programs do want local memory, Lunar Lake is more efficient with utilizing L1/SLM capacity. Lunar Lake can have threads allocating up to 1 MB of local memory executing concurrently on the GPU, which would correspond to 128 KB per Xe Core. Meteor Lake got stuck at 768 KB, so each Xe Core was only able to allocate 96 KB of local memory. Xe Cores have a texture cache as well, probably with 32 KB of capacity. Latency is higher because accesses have to go through the texture units, which can do all sorts of texture sampling operations in addition to just grabbing data from memory. Here I’m hitting the texture cache with OpenCL’s image1d_buffer_t data type, and not using any texture sampling options. Again Lunar Lake behaves just like Meteor Lake with a little extra latency, thanks to lower clock speeds. AMD does not have a separate texture cache, and feeds texture units using the vector cache. Vector cache latency on AMD is actually better when the texture units handle address generation. Outside the Xe Core, Lunar Lake doubles L2 cache capacity to 8 MB. That’s the biggest L2 cache I’ve seen in a laptop iGPU so far. However, latency has substantially increased over Meteor Lake’s 4 MB L2. Lunar Lake also sees a weird uptick in latency well before the test approaches L2 cache capacity. It’s not address translation latency because testing with a 4 KB stride didn’t show a latency jump until 64 MB, implying a single thread has enough TLB capacity on hand to cover 64 MB. To investigate further, I split the latency test array into multiple sections. Each thread gets placed into a separate workgroup, and traverses its own portion of the array. I can’t influence which Xe Core handles each thread, but GPU schedulers usually try to evenly balance threads across GPU cores. Eight threads see eight times as much L1D capacity, indicating I have one thread per Xe Core. At L2, using more threads pushes out the latency jump. Perhaps Lunar Lake’s L2 is split into multiple sections, and a Xe Core may have lower L2 access latency if it accesses a closer section. AMD’s Strix Point has a smaller but lower latency L2 cache. Even though the L2 is smaller, Strix Point has a lower latency penalty for accessing DRAM. On Lunar Lake and Meteor Lake, a GPU-side DRAM access has over 400 ns of latency. Strix Point’s iGPU can get data from DRAM in under 250 ns. Lunar Lake has a 8 MB memory side cache placed in front of the memory controller, but I can’t clearly measure its performance characteristics. Atomics Atomic operations can exchange data between threads running on a GPU. And while such operations are comparatively rare from what I can see, it’s still fun to run a core-to-core latency test of sorts on a GPU. Exchanging data through global memory sees Lunar Lake improve slightly over Meteor Lake. Doing the same through local memory means that data exchange can happen within an Xe Core, or the equivalent structure on other GPUs. There, AMD’s very fast Local Data Share continues to take the top spot. However, Lunar Lake has noticeably improved over its predecessor. Cache and Memory Bandwidth Assuming there’s enough work in flight to hide latency, GPUs can demand a lot of bandwidth from their cache hierarchy. Lunar Lake’s first level caches provide similar bandwidth compared to the outgoing Meteor Lake generation, while AMD’s Strix Point has a massive bandwidth lead. That’s because AMD’s WGPs have two 32 KB vector cache instances, each capable of satisfying a wave32 (1024-bit) vector load per cycle. Local memory shows similar behavior. AMD backs local memory with a 128 KB LDS. Since RDNA, that’s been built from two 64 KB arrays each capable of servicing a wave32 load every cycle. Thus AMD’s local memory and vector caches enjoy the same staggeringly high bandwidth. Again, it’s much higher than what Intel’s iGPUs can offer. Testing GPU cache and memory bandwidth from a single test can be tricky, and Intel’s latest drivers seem to defeat Nemes’s Vulkan benchmark. Therefore I’m using my OpenCL bandwidth test, which uses the same general methodology. Lunar Lake and Meteor Lake appear to have similar L2 bandwidth. AMD continues to have a huge L2 bandwidth lead. All three iGPUs tested here have very high DRAM bandwidth compared to those from the DDR4 generation. I’m not sure why Lunar Lake can’t quite match Strix Point or Meteor Lake despite using faster LPDDR5X, but I wouldn’t put too much weight into it because bandwidth testing is hard. I intend to write a different test specifically to target GPU DRAM bandwidth at some point, but time is short. CPU to GPU Copy Bandwidth Integrated GPUs share a memory controller with the CPU. Often this can be a disadvantage because CPUs and GPUs are more sensitive to different memory subsystem characteristics, and iGPUs have to be budget friendly too. But iGPUs do have an advantage in being able to exchange data with the CPU faster. Normally I use OpenCL’s clEnqueueWriteBuffer and clEnqueueReadBuffer functions to test PCIe link bandwidth, but here, the test is just hitting the DMA engines. Lunar Lake seems to have weaker DMA engines than its predecessor and AMD’s Strix Point. They come nowhere near saturating memory bandwidth, and don’t offer comparable performance to say, a PCIe 4.0 x16 link. Improved Raytracing Intel has scaled up the hardware raytracing unit (RTU) in each Xe Core, giving it three traversal pipelines for handling raytracing BVH-es. Each pipeline can do 6 box tests per cycle for a total of 18 across the RTU. For intersection testing at the bottom level of the BVH, Intel can do two triangle tests per cycle to see if the ray hits any geometry. I couldn’t find much on Intel’s prior raytracing implementation, though the Xe-HPG whitepaper does say its RTUs support “a 12-to-1 ratio of ray-box intersection tests per clock to ray-triangle tests per clock”. The prior generation may have had two traversal pipelines per RTU, each capable of 6 box tests per clock. From the Intel® Arc™ Graphics Developer Guide for Real-time Ray Tracing in Games, showing traversal handled in the RTU. For more details see https://old.chipsandcheese.com/2024/04/15/raytracing-on-meteor-lakes-igpu/ Meteor Lake already had an aggressive ray tracing implementation, with RTUs autonomously handling ray traversal until a hit or miss is discovered. It performed well in 3DMark’s Solar Bay raytracing benchmark against AMD’s GPUs. Lunar Lake improves over its predecessor, though it’s hard to compete against AMD scaling up its GPU and showing up at much higher clock speeds. 3DMark’s Solar Bay test also displays average framerates. Laptop iGPUs do quite well because Solar Bay is aimed at tablets and cell phones. Its test scene is very limited, and its raytracing effects are too. Cyberpunk 2077 Unlike Solar Bay, Cyberpunk 2077 has far more complex scenes and long view distances. At 1080P and low settings, mobile GPUs just barely manage playable framerates. Lunar Lake posts a huge performance increase over its predecessor, and manages to slip past AMD’s outgoing Phoenix APU in Cyberpunk 2077’s built in benchmark. But framerates are still low enough that I’d rather not lower it by adding extra eye candy. That includes raytracing. Even though Lunar Lake manages significant improvements, AMD’s current generation Strix Point APU is still ahead. Part of this is because Strix Point was able to pull more power. HWInfo showed the Lunar Lake laptop pulling an average of 43W from the battery, while the Strix Point laptop averaged 50W. Lunar Lake’s huge performance gain over Meteor Lake may be down to power too. Microbenchmarking shows similar compute performance across both Intel generations. But Meteor Lake’s CPU cores pull more power, leaving less power available for the GPU. HWInfo couldn’t read GPU frequency for Lunar Lake, but Meteor Lake’s iGPU often ran below 1.6 GHz. I suspect Lunar Lake’s iGPU ran closer to its maximum 1.95 GHz clock. CPU power counters may not provide the best accuracy, as they only have to be accurate enough to prevent hardware damage. Figures above likely have a generous margin of error. But I think they show how improved architecture and better process nodes combine to let Lunar Lake outperform Meteor Lake in a power constrained laptop. Caching is another power optimization, as data transfer often contributes significantly to platform power consumption. Lunar Lake’s higher caching capacity means it can outperform AMD’s Phoenix APU while using less memory bandwidth on average. Intel has also improved significantly over Meteor Lake, delivering better performance with lower DRAM bandwidth demand. HWInfo used for AMD, as its memory counters appear accurate for Zen 4 and deliver reasonable values for Zen 5. VTune’s UNC_M_CAS_COUNT_RD/WR counters used for Intel AMD’s Strix Point iGPU stands out as being particularly bandwidth hungry. It’s able to take the top spot by a considerable margin, but in the process requires disproportionately more data transfer from DRAM. A deeper look shows highest observed DRAM bandwidth usage over HWInfo’s 2 second sampling interval was 60.6 GB/s, while Meteor Lake only reached a maximum of 52 GB/s over 0.04 seconds. So while none of these platforms are pushing their bandwidth limits, AMD’s higher DRAM bandwidth usage may put Strix Point at a power disadvantage. That’s because data transfer can consume significant power, and DRAM is one of the most power hungry places for a CPU to get data from. The situation may get even more extreme for AMD in other games. Elden Ring for example averaged 63.6 GB/s of DRAM traffic on Meteor Lake. In laptops where every last watt matters, Intel’s caching strategy is good to see. FluidX3D FluidX3D uses GPU compute to carry out fluid simulations. Unlike games, FluidX3D almost exclusively loads the GPU and places little load on the CPU. Lunar Lake also shows improvement over Meteor Lake in this task. However, it lands just short of AMD’s last generation Phoenix part. While FluidX3D tends to be memory bandwidth bound on discrete cards, the picture on integrated GPUs isn’t so clear. The benchmark reported 63 GB/s of bandwidth usage on Lunar Lake, and that’s likely close to DRAM bandwidth usage because FluidX3D tends to have low cache hitrates. Final Words Lunar Lake’s iGPU is a showcase of Intel’s determination to keep pace with AMD in thin and light laptops. Instead of building a bigger GPU as they did with Meteor Lake, Intel is trying to more efficiently operate a GPU of the same size. More cache reduces power hungry DRAM accesses. Reorganized Vector Engines help improve utilization. More efficient CPU cores mean the GPU gets a bigger power and thermal budget to stretch its legs in. VTune collecting metrics from an Elden Ring gaming session, on my Meteor Lake laptop. 55.7 GB/s from DRAM is a lot of data moving around AMD in contrast is going all out for higher performance. Their Strix Point APU has a bigger GPU than the prior generation with minor architectural changes, but continues to use a 2 MB L2 cache and shares the same die with more CPU cores. From brief testing, Strix Point undoubtedly wins from a performance perspective. But AMD also draws more power to get there. I wonder if AMD will have to adopt something closer to Intel’s caching strategy going forward. Even if Strix Point can deliver better performance with smaller caches, it’s a very bandwidth hungry and therefore power hungry way to do so. Lunar Lake’s iGPU is perhaps most interesting as a preview of what Intel’s upcoming Battlemage discrete GPUs can offer. With Xe2, Intel is looking to use the same graphics architecture across their product stack. It’s a strategy AMD has used for a long time, and also shows Intel is serious about hitting higher GPU performance targets. Efficiency improvements from reorganized vector units as well as bigger raytracing accelerators should be very fun to see once they hit the desktop market. Finally, I’ve seen plenty of doom and gloom about Intel’s discrete GPU efforts from various sites. But unlike Intel’s prior dGPU efforts, the company is using their integrated GPUs as a springboard into the discrete GPU market. Integrated GPUs continue to be very important for mobile gaming, so that springboard will continue to exist for the foreseeable future. If you like our articles and journalism, and you want to support us in our endeavors, then consider heading over to our Patreon or our PayPal if you want to toss a few bucks our way. If you would like to talk with the Chips and Cheese staff and the people behind the scenes, then consider joining our Discord. Subscribe to Chips and Cheese Launched a year ago The Devil is in the Details! Deep dives into computer hardware and software and the wider industry... Subscribe Error 12 Share this post Lunar Lake’s iGPU: Debut of Intel’s Xe2 Architecture chipsandcheese.com Copy link Facebook Email Note Other 3 Share Previous",
    "commentLink": "https://news.ycombinator.com/item?id=41780929",
    "commentBody": "Lunar Lake's iGPU: Debut of Intel's Xe2 Architecture (chipsandcheese.com)121 points by mfiguiere 23 hours agohidepastfavorite58 comments transpute 18 hours ago> Xe2, Intel is looking to use the same graphics architecture across their product stack.. integrated GPUs as a springboard into the discrete GPU market. Linux support for Xe2 and power management will take time to mature, https://www.phoronix.com/forums/forum/linux-graphics-x-org-d... Xe SR-IOV improves VM graphics performance. Intel dropped Xe1 SR-IOV graphics virtualization in the upstream i915 driver, but the OSS community has continued improvement in an LTS fork, making steady progress, https://github.com/strongtz/i915-sriov-dkms/commits/master/ & https://github.com/Upinel/PVE-Intel-vGPU?tab=readme-ov-file. reply iforgotpassword 13 hours agoparentAww man this is so disappointing. Intel has a pretty good track record with their Linux drivers. Too bad cost-cutting seems to have reached driver development too. reply transpute 12 hours agorootparentIntel has been developing a new graphics driver stack for Xe, which will officially support SR-IOV from Xe2 onward, https://www.phoronix.com/news/Intel-New-Xe-Linux-Driver. It makes sense to focus Intel resources on the new hardware and driver architecture, but thanks to OSS, the existing stable driver can also be improved by community contributions. reply shadeslayer 11 minutes agorootparentprevThe issue that phoronix is facing might be due to a power management bug that is not related to the driver at all. reply transpute 2 hours agorootparentprevImpact appears to be limited, https://www.phoronix.com/news/Intel-Maintainers-Linux-Depart reply teruakohatu 10 hours agoparentprev> Intel dropped Xe1 SR-IOV graphics virtualization in the upstream i915 driver, I missed this. Wow this is disappointing. reply shadeslayer 10 minutes agorootparentNot sure if we need to support SRIOV on the HW. VirtIO GPU native contexts should be good enough for most consumers. I imagine SRIOV would be useful for more advanced usecases reply reginald78 5 hours agorootparentprevI remember being somewhat excited for Intel dGPUs since I had a real interest in a card that could do GVT-g and also might have super low idle power consumption like their iGPUs that would fit well with my VM server. We ended up with GVT-g canceled and promising of SR-IOV coming eventually and dGPUs with atrocious idle power consumption! reply cassepipe 6 hours agoparentprevSo the state of Xe support on Linux is pretty good ? Is it worth it to run Linux on Alder Lake, can it take advantage of the full power of the iGPU ? reply chmod775 5 hours agoprevThat's a big hit in performance compared to the AMD chip. Just to save $100 on a $1700 notebook? Sadly the article didn't get into power draw too much. That might've been much more interesting. reply phkahler 4 hours agoparent>> Sadly the article didn't get into power draw too much. They covered power quite a bit, but claimed the biggest power draw comes from memory access. I got the impression they were blaming AMDs increased memory bandwidth on their smaller cache size and hence a form of inefficiency. But higher frame rates are going to require more memory accesses. The smaller cache should have less impact on the number of writes needed. IMHO just some top line power consumption numbers are good, but trying to get into why one is higher than the other seems fruitless. reply SG- 20 hours agoprevi wish they covered things like x264/x265/av1/etc encoding/decoding performance and other benefits that aren't just gaming. reply wtallis 17 hours agoparentVideo encode and decode aren't really GPU functions. They're totally separate IP blocks from the 3D graphics/vector compute part of the GPU. On Intel's previous laptop processor generation (Meteor Lake), the video encode and decode blocks were on an entirely different piece of silicon from the GPU. reply jsheard 6 hours agorootparentBenchmarking hardware encode is also a pretty specialized rabbit hole since it's not just the performance that varies, but also the quality of the results. reply dyingkneepad 1 hour agorootparentprev> the video encode and decode blocks were on an entirely different piece of silicon from the GPU. As far as I understand this is not true. It's a different engine within the graphics device, and it shares the execution units. reply shadeslayer 7 minutes agorootparentAFAIK the video encode/decode pipeline is separate from the graphics pipeline. But they do reside on the graphics tile. reply adrian_b 11 hours agorootparentprevTrue. The display controller is also a different block, separated from the GPU and from the video codecs. While on Lunar Lake the GPU and the video codec block are on the same tile, they are still in different locations on the compute tile. In the new Arrow Lake S desktop CPU, to be announced tomorrow, the GPU is extracted on a separate tile, like in Meteor Lake, while the other two blocks related to video output, i.e. the video codec block and the display controller block, are located on a tile that contains the memory controller and a part of the peripheral interfaces and which is made using a lower-resolution TSMC process than the CPU and GPU tiles. reply booi 19 hours agoparentprevIt’s probably just not that interesting. There’s generally a proprietary encode/decode pipeline on chip. It can generally handle most decode operations with CPU help and a very narrow encoding spec mostly built around being able to do it in realtime for broadcast. Most of the video you encode on a computer is actually all in software/CPU because the quality and efficiency is better. reply KronisLV 1 hour agorootparent> Most of the video you encode on a computer is actually all in software/CPU because the quality and efficiency is better. It depends on what you care about more, you don't always need the best possible encoding, even when you're not trying to record/stream something real time. For comparison's sake, I played around with some software/hardware encoding options through Handbrake with a Ryzen 5 4500 and Intel Arc A580. I took a 2 GB MKV file of about 30 minutes of footage I have laying around and re-encoded it with a bunch of different codecs: codec method time speed file size of original H264 GPU 04:47 200 fps 1583 MB 77 % H264 CPU 13:43 80 fps 1237 MB 60 % H265 GPU 05:20 206 fps 1280 MB 62 % H265 CPU ~30:00 ~35 fps would take too long AV1 GPU 05:35 198 fps 1541 MB 75 % AV1 CPU ~45:00 ~24 fps would take too long So for the average person who wants a reasonably fast encode and has an inexpensive build, many codecs will be too slow on the CPU. In some cases, close to an order of magnitude, whereas if you do encode on the GPU, you'll get much better speeds, while the file sizes are still decent and the quality of something like H265 or AV1 will in most cases seem perceivably better than H264 with similar bitrates, regardless of whether the encode is done on the CPU or GPU. So, if I had a few hundred of GB of movies/anime locally that I wanted to re-encode to make it take up less space for long term storage, I'd probably go with hardware H265 or AV1 and that'd be perfectly good for my needs (I actually did, it went well). Of course, that's a dedicated GPU and Intel Arc is pretty niche in of itself, but I have to say that their AV1 encoder for recording/streaming is also really pleasant and therefore I definitely think that benchmarking this stuff is pretty interesting and useful! For professional work, the concerns are probably quite different. reply vbezhenar 13 hours agorootparentprev> Most of the video you encode on a computer is actually all in software/CPU because the quality and efficiency is better. I don't think that's true. I bought a Thinkpad laptop, installed Linux and one of my issues was that watching youtube video put CPU onto 60%+ load. The same with Macbook barely scratched CPU at all. I finally managed to solve this issue by installing Arch. When everything worked as necessary, CPU load was around 10%+ for the same video. I didn't try Windows but I'd expect that things on Windows would work well. So most of the video for average user probably is hardware decoded. reply adrian_b 11 hours agorootparentThe comment to which you replied was about encoding, not decoding. There is no reason to do decoding in software, when hardware decoding is available. On the other hand, choosing between hardware encoding and software encoding, depends on whether quality or speed is more important. For instance for a video conference hardware encoding is fine, but for encoding a movie whose original quality must be preserved as much as possible, software encoding is the right choice. reply foobiekr 2 hours agorootparentprevMost hardware encoders suck. reply ramshanker 18 hours agorootparentprev>>> It can generally handle most decode operations with CPU help and a very narrow encoding spec. This is so much spot on. Video coding specs are like a \"huge bunch of tools\" and encoders get to choose whatever subset-of-tools suits them. And than hardware gets frozen for a generation. reply Dalewyn 17 hours agorootparentprev>Most of the video you encode on a computer is actually all in software/CPU because the quality and efficiency is better. That was the case up to like 5 to 10 years ago. These days it's all hardware encoded and hardware decoded, not the least because Joe Twitchtube Streamer can't and doesn't give a flying fuck about pulling 12 dozen levers to encode a bitstream thrice for the perfect encode that'll get shat on anyway by Joe Twitchtok Viewer who doesn't give a flying fuck about pulling 12 dozen levers and applying a dozen filters to get the perfect decode. reply timc3 13 hours agorootparentIt’s not all hardware encoded - we have huge numbers of transcodes a day and quality matters for our use case. Certainly for some use cases speed and low CPU matter but not all. reply imbnwa 16 hours agorootparentprevNot sure why downvoted, all of serious Plex use runs on hardware decode on Intel iGPUs down to an i3. One only sources compute from the CPU for things like subtitles or audio transcoding reply timc3 13 hours agorootparentBecause Plex and gamers streaming is not the only use case for transcode reply Dalewyn 11 hours agorootparent\"Most of the video you encode ...\" reply Remnant44 12 hours agoparentprevAs mentioned in other responses, that part of the GPU simply isn't interesting from an architectural perspective, which is what Chips and Cheese is all about. GPU compute performance is both technically interesting, and matters to much more than simply gaming! reply wcfields 19 hours agoparentprevI agree, I never really cared about QSV as an Intel feature until I started doing Livestreams, using Plex/Jellyfin/Emby, and virtualizing/homelab work. reply WaxProlix 18 hours agorootparentQuickSync passthrough should get you everything you need on i3+ chips. It's basically intel's only selling point in the homelab/home server space, and it's a big one. [Edit: I think I initially misread you - but I agree, it's a huge differentiator] reply close04 9 hours agorootparent> It's basically intel's only selling point in the homelab/home server space In the homelab/home server space I always thought the OOB management provided by AMT/vPro is probably the biggest selling point. Manageability, especially OOB, is a huge deal for a lab/server. Anyone who used AMD's DASH knows why vPro is so far ahead here. reply BobbyTables2 5 hours agorootparentIntel probably spends more on office supplies than they make from homelab customers… reply close04 3 hours agorootparentMaybe, but I wasn't thinking of Intel's profit. The question was what might be a bigger selling point in a home lab, QuickSync for transcode related tasks (your Plex/Jellyfin machine for example, which would also work with most Nvidia GPUs and some AMD ones), or OOB manageability for your entire home lab especially if it's composed of multiple machines and IP KVMs quickly become cumbersome. reply Wytwwww 1 hour agorootparent> Nvidia GPUs You would need an actual GPU, though. Massively increasing cost, power usage etc. without providing any real value in return for many use cases and AFAIK HW transcoding with Plex doesn't even work properly with with AMDs iGPUs? The N100 can transcode 4k streams at ~20w while costing barely more than a Raspberry Pi. reply wcfields 27 minutes agorootparentYeah, I’d love to use AMD cpus for my Plex/Homelab/VM/unraid system but when you’re building one for home use, every watt matters and an Nvidia GPU, while nice, is hard to justify just for transcodes. I feel like my Dad saying “turn off the damn lights” now that I gotta pay the ‘light bill’ on a machine that runs 24/7 with spinning disks. reply pa7ch 16 hours agoparentprevAgreed, my laptop burns a lot of battery on AV1 video and I'd like information on how chips with AV1 decode perform with chrome. reply hggigg 20 hours agoparentprev100% agree with that. x265 transcoding gets done on my MBP regularly so I’d like to see that as a comparison point. reply TiredOfLife 5 hours agorootparentx265 is a cpu based H.265 encoder and is not accelerated. reply adgjlsfhk1 17 hours agorootparentprevwhat actually uses x265? I thought pretty much everyone used AV1 for their next gen codec. reply throwaway48476 16 hours agorootparentHardware people don't mind paying licenses for x265 because they can just bake in the cost. It just causes problems for software, especially when it's free. reply adgjlsfhk1 15 hours agorootparentright, but if none of the software uses it, the hardware is pretty worthless. reply acdha 5 hours agorootparentThat’s only true if you’re writing the codec. If you’re calling the system APIs, you’re using Microsoft or Apple’s license. The last time I looked it was worth supporting because there was a 20 point gap in hardware support but that’s closed as each generation of hardware adds AV1 support. reply KeplerBoy 10 hours agorootparentprevVideo software doesn't need to license the codec if the GPU driver takes care of it, right? If hardware accelerate decoding works, you just feed the binary video blob to the driver and it returns decoded frames. reply SahAssar 3 hours agorootparentI'm not sure what requires a license as MS sells/sold a package to enable h265 even on devices that have hardware support, so some software fee seems to be required: https://apps.microsoft.com/detail/9nmzlz57r3t7?hl=en-us&gl=U... reply pjmlp 10 hours agorootparentprevProprietary software doesn't have such issues. reply hggigg 13 hours agorootparentprevMe when I want to transcode something to save a bit of disk space. reply nuz 2 hours agoprevNvidias moat is so enormous reply Wytwwww 1 hour agoparentWhat moat? Nvidia is barely even competing in the same Xe2 is in. Their laptop GPUs aren't particularly spectacular and aren't at all suitable for low-power use cases. reply KeplerBoy 10 hours agoprevHere's hoping ARM on the desktop/laptop finally takes off and we see Nvidia returning to these market segments. Their Tegra chips could do a lot in these laptop / handheld gaming devices. reply Sakos 11 hours agoprev [–] Lunar Lake gaming performance is incredible on Windows. It makes me want the Steam Deck 2 to be based on the next Intel platform. That said, the Linux graphics drivers are terrible (https://www.phoronix.com/review/lunar-lake-xe2) and the Phoronix benchmarks for Lunar Lake overall (outside of gaming: https://www.phoronix.com/review/core-ultra-7-lunar-lake-linu...) showed terrible performance in all sorts of aspects, jesus. Xe2 is a huge win, the rest not so much. reply skavi 10 hours agoparentGPU benchmarks in TFA are run on Windows: https://open.substack.com/pub/chipsandcheese/p/lunar-lakes-i... reply Sakos 8 hours agorootparentWeird. I've seen far better results elsewhere. reply automatic6131 10 hours agoparentprevMSI Claw 2 might, given it's original is Meteor Lake based. But it sold like ** so there may not be a successor reply Sakos 8 hours agorootparentDid the first claw even sell well? That said, the Steam Deck competitors aren't interesting to me without the touchpads and four back buttons. reply kaliqt 1 hour agorootparentNo. And I know this by the sheer lack of videos and discussion of any kind on it. reply automatic6131 5 hours agorootparentprev>Did the first claw even sell well? Extremely poorly. The worst of all deck-likes. reply formerly_proven 9 hours agoparentprev [–] With totally new hardware platforms things often take a minute to really work (even on Windows). reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Intel's Lunar Lake introduces the Xe2 graphics architecture, significantly enhancing efficiency and performance for integrated GPUs in thin and light laptops.- The Xe2 architecture features eight Xe Cores divided into two Render Slices, with improvements in caching, Vector Engines, and raytracing capabilities, showing notable performance gains over its predecessor, Meteor Lake.- Intel's strategy emphasizes efficiency and power optimization, using more cache to reduce DRAM access, and previews the upcoming Battlemage discrete GPUs, indicating a strong focus on advancing GPU performance."
    ],
    "commentSummary": [
      "Intel's Xe2 architecture debuts with Lunar Lake's integrated GPU (iGPU), aiming to unify graphics across their product line and enter the discrete GPU market.",
      "Linux support and power management for Xe2 are still in development, while the open-source community continues to enhance Xe1 SR-IOV graphics virtualization.",
      "Discussions emphasize Intel's focus on new hardware and driver architecture, with community contributions, and touch on video encoding/decoding, comparing hardware and software encoding quality and speed."
    ],
    "points": 121,
    "commentCount": 58,
    "retryCount": 0,
    "time": 1728415957
  },
  {
    "id": 41789242,
    "title": "An n-ball Between n-balls",
    "originLink": "https://www.arnaldur.be/writing/about/an-n-ball-between-n-balls",
    "originBody": "An n-ball Between n-balls There is a geometric thought experiment that is often used to demonstrate the counterintuitive shape of high-dimensional phenomena. This article is an interactive visual journey into the construct in the thought experiment, and the mathematics behind it. A Square with Four Circles We start with a 4 × 4 4×4 square. There are four blue circles, with a radius of one, packed into the box. One in each corner. At the center of the box is a red circle. The red circle is as large as it can be, without overlapping the blue circles. Drag the slider to add a third dimension. On this and all other 3D diagrams, you can drag the image to rotate it, and scroll to zoom. Add 3rd dimension Disect construct When extending the construct to 3D, many things happen. All the circles are now spheres, the red sphere is larger while the blue spheres aren’t, and there are eight spheres while there were only four circles. The dimensional extension is decomposed into three phases. First, the circles and square turn into spheres and a cube. Next, the central ball grows, and everything else shifts so that it ends in the center. Finally, four new spheres appear, completing the 3D version of the construct. Defining the Construct There are more than one way to extend the construct into higher dimensions, so to make it more rigorous, we will define it like so: An 𝑛 n-dimensional version of the construct consists of an 𝑛 n-cube with a side length of 4 4. On the midpoint between each vertex and the center of the 𝑛 n-cube, there is an 𝑛 n-ball with a radius of one. In the center of the 𝑛 n-cube there is the largest 𝑛 n-ball that does not intersect any other 𝑛 n-ball. Building Intuitions We want to extend it into any amount of dimensions but before that, we’ll build some intuitions. 2D Intersection of 3D Construct Migrate center ball Rotate intersecting plane Link sliders Isolate plane Initial Final In the default configuration, the intersecting plane is the same as the construct in 2D. We see that when we rotate the plane along the center of the left balls, the balls on the right, vanish and are replaced—within the plane—by the two balls, diagonally across. There are a few important behaviors to note from this interactable. When the red ball migrates from the 2D center to the 3D center, it shrinks and vanishes from the 2D plane. As seen in the isolate plane perspective, the left balls maintain their size and only shift slightly away from the center. The differences between the Initial and Final configuration. The enclosing box’s width grows from 4 4 to 4 2 42. Similarly, the balls’ horizontal positions shift from ± 1 ±1 to ± 2 ±2 The red ball’s radius 𝑟 r grows from 2 − 1 2−1 to 3 − 1 3−1 1D Intersection of 3D Construct Diagonalize intersection Camera angle sidediagonaltopperspective This diagram demonstrates the same phenomena as before, except the intersection is one dimensional and diagonalizes first into two, then three dimensions. Notice the similarities between the previous two diagrams. The left ball shifts to the left, maintaining it’s size, while the right ball vanishes to be replaced by another farther away. Now we perform the same diagonalization operation, starting from three dimensions. As visualizing higher dimensions is not intuitive, we can’t rely on a lower dimensional intersection of a higher dimensional object to guide us. The similarities should be evident however. 3D Intersection of 10D Construct Diagonalize intersection Two of the boxes dimensions maintain their constant height of 4 4 while the third elongates to disect the remaining eight dimension of the 10D construct. It’s length transitions from 4 = 4 1 4=41 up to 4 8 48, the length of the longest diagonal of an 8 8 dimensional box. Viewing the fully diagonalized construct from the side we can see the property it’s known for. The red ball reaches outside of the green box. I repeat: sections of the red ball are outside the green box. This is absurd; considering how enclosed it is in the two and three dimensional versions, and even if it’s mathematically clear that 𝑟 > 2 r>2 when 𝐷 > 9 D>9, the fact is hard to accept. Some people have tried to analogize this property to spikiness [1] [2] of the red ball. I don’t think the analogy is good. A 3D sphere is perfectly circular, no matter how you slice it. Similarly, an n-dimensional hyperball, is perfectly spherical from all perspectives. It’s ballness just reaches into more hard to grasp dimensions than we’re used to. Hamming’s lecture Here is a segment from a lecture that reveals this fact without mincing words. Further Analysis A unit n-cube has a unit volume for any 𝐷 D. A unit n-ball has a volume of 𝜋 𝐷 / 2 Γ ( 𝐷 / 2 + 1 ) Γ(D/2+1)πD/2 which rapidly approaches zero as 𝐷 D grows. Intuitively, a sphere and a cylinder are both 3D extensions of a circle. There is more volume in the cylinder than the sphere. With some imprecision, we can thus consider the sphere loosing some volume when a dimension is added, while a cylinder or a box do not. This volume loss occurs in every additional dimension. So instead of considering n-balls to be spiky, it’s the space around them that outgrows them. Cubing the Ball The volume of the red ball is: 𝜋 𝐷 2 ( 𝐷 − 1 ) 𝐷 Γ ( 𝐷 2 + 1 ) Γ(2D+1)π2D(D−1)D and if we normalize the size of the green box to 1 by dividing it’s sidelength by 4, the volume of the red ball becomes: 𝑉 ( 𝐷 ) = 𝜋 𝐷 2 ( 𝐷 − 1 4 ) 𝐷 Γ ( 𝐷 2 + 1 ) V(D)=Γ(2D+1)π2D(4D−1)D Which has this plot: There are a few notable/interesting values of 𝐷 D. 𝑉 ( 0 ) = 1 V(0)=1 𝑉 ( 1 ) = 0 V(1)=0 𝑉 ( 2 ) ≈ 0.033688... V(2)≈0.033688... 𝑉 ( 264 ) ≈ 0.00001000428... V(264)≈0.00001000428... (a local minimum) 𝑉 ( 1206 ) ≈ 1.01158... V(1206)≈1.01158... (smallest 𝑉 ( 𝐷 ) ≥ 1 ∣ 𝐷 > 0 V(D)≥1∣D>0) 𝑉 ( 1390 ) ≈ 29.741... V(1390)≈29.741... Here the red ball’s volume exceeds the box’s volume by the a larger ratio than the square exceeds the area of the circle, when 𝐷 = 2 D=2 𝑉 ( 1405 ) ≈ 39.499... V(1405)≈39.499... The same as above but 𝐷 = 3 D=3 𝑉 ( 1486 ) ≈ 186.299... V(1486)≈186.299... The same as above but 𝐷 = 10 D=10 Not only does some of the ball reach outside of the box (as happens when 𝐷 = 10 D=10) it goes much further. A more adventurous mathematician than me can try to find the value of 𝐷 D when the volume of all the hyperspherical caps become more than half of the volume of the sphere. At that point, most of the ball’s volume is outside the box. At 𝐷 = 1206 D=1206 the ball becomes larger than the box and as 𝐷 D increases, the relative size of the ball grows without bound. 3D Intersection of 1206D Construct Here we see the relative scale of the red ball in 1206D. Note that as above, this is a perfectly valid 3D slice of the construct; a high dimensional being could, with the right tools, carve this out of the construct, with a single straight cut. There are however, many other ways to slice the construct that look different, similarly to how each slice of a tomato is distinct. Other related stuff Here is a Desmos calculator interactable that visualises an orthogonal 2D slice of the 10D construct. Notice how much empty space there is between the balls. This article was a great inspiration, and the reason I considered the 1206D construct and beyond.",
    "commentLink": "https://news.ycombinator.com/item?id=41789242",
    "commentBody": "An n-ball Between n-balls (arnaldur.be)115 points by Hugsun 3 hours agohidepastfavorite19 comments Sharlin 54 minutes agoA good way to conceptualize what’s going on is not the idea that balls become \"spiky\" in high dimensions – like the article says, balls are always perfectly symmetrical by definition. But it’s the box becoming spiky, \"caltrop-shaped\", its vertices reaching farther and farther out from the origin as the square root of dimension, while the centers of its sides remain at exactly +-1. And the 2^N surrounding balls are also getting farther from the origin, while their radius remains 1/2. Now it should be quite easy to imagine how the center ball gets more and more room until it grows out of the spiky box. reply pfortuny 4 minutes agoparentExactly: a corner of a square covers 1/4 of that part of the plane. A corner of a cube covers 1/8, a corner of a hypercube in dimension n covers just 1/(2^n) of the space. But each side/face/hyperface divides the plane/space/n-dim space just in half. reply drdeca 1 hour agoprevWhy did I imagine that this would be about two shapes that are merely topologically n-balls, each having part of their boundary be incident with one of the two hemi(n-1)-spheres of the boundary of an n-ball (and otherwise not intersecting it)? (So like, in 3D, if you took some ball and two lumps of clay of different colors, and smooshed each piece of clay over half of the surface of the ball, with each of the two lumps of clay remaining topologically a 3-ball.) I don’t know that there would even be anything interesting to say about that. reply Hugsun 1 hour agoparentI can't tell you why you imagined that, but that's pretty funny nevertheless. reply steventhedev 2 hours agoprevThis is a really good demonstration of the curse of dimensionality[0] [0]: https://en.m.wikipedia.org/wiki/Curse_of_dimensionality reply Imustaskforhelp 1 hour agoprevCan I just say how my mind is utterly blown by the animations reply Hugsun 1 hour agoparentThank youThere is a geometric thought experiment that is often used to demonstrate the counterintuitive shape of high-dimensional phenomena. We start with a 4×4 square. There are four blue circles, with a radius of one, packed into the box. One in each corner. At the center of the box is a red circle. The red circle is as large as it can be, without overlapping the blue circles. When extending the construct to 3D, many things happen. All the circles are now spheres, the red sphere is larger while the blue spheres aren’t, and there are eight spheres while there were only four circles. > There are more than one way to extend the construct into higher dimensions, so to make it more rigorous, we will define it like so: An n-dimensional version of the construct consists of an n-cube with a side length of 4. On the midpoint between each vertex and the center of the n-cube, there is an n-ball with a radius of one. In the center of the n-cube there is the largest n-ball that does not intersect any other n-ball. > At what dimension would the red ball extend outside the box? Response: \"[...] Conclusion: The red ball extends outside the cube when n≥10n≥10.\" It calculated it with a step-by-step explanation. This is the first time I'm actually pretty stunned. It analysed the problem, created an outline. Pretty crazy. reply Hugsun 45 minutes agorootparentI'd wager that it's in the training data. reply robwwilliams 2 hours agoprevImpressive, helpful, and now time to rebuild my own embeddings so I can grasp that red n-ball with my new n-D hands. reply joaquincabezas 2 hours agoprevwow discovering Hamming’s lecture was enough for me! so good reply eniwnenahg 1 hour agoprev [–] Matlock, is that you? reply Hugsun 46 minutes agoparent [–] I am not Matlock, who is that? reply lisper 30 minutes agorootparent [–] He probably meant MacGyver. https://en.m.wikipedia.org/wiki/MacGyver reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article presents a geometric thought experiment illustrating the surprising properties of high-dimensional shapes, starting with a 2D square and extending to higher dimensions.- It reveals that in higher dimensions, the central sphere (or n-ball) can extend beyond the surrounding shape, challenging intuitive expectations about space and volume.- The discussion includes mathematical properties of n-balls, showing that as dimensions increase, the space around them grows faster than the n-balls themselves, supported by interactive visualizations and further analysis."
    ],
    "commentSummary": [
      "The post explores high-dimensional geometry, focusing on how n-balls (spheres in n-dimensional space) remain symmetrical, while n-cubes (cubes in n-dimensional space) become \"spiky\" as dimensions increase.",
      "It highlights that in dimensions n≥10, the center n-ball can extend beyond the boundaries of the n-cube, illustrating the complexities of higher dimensions.",
      "The discussion includes user comments and references to the \"curse of dimensionality,\" with users expressing fascination with the geometric animations and thought experiments."
    ],
    "points": 115,
    "commentCount": 19,
    "retryCount": 0,
    "time": 1728489000
  }
]
