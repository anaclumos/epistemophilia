[
  {
    "id": 42809268,
    "title": "A QR code that sends you to a different destination – lenticular and adversarial",
    "originLink": "https://mstdn.social/@isziaui/113874436953157913",
    "originBody": "Create accountLogin Recent searches No recent searches Search options Only available when logged in. mstdn.social is one of the many independent Mastodon servers you can use to participate in the fediverse. Administered by: Server stats: mstdn.social: About · Status · Profiles directory · Privacy policy Mastodon: About · Get the app · Keyboard shortcuts · View source code · v4.3.3 ExploreLive feeds Mastodon is the best way to keep up with what's happening. Follow anyone across the fediverse and see it all in chronological order. No algorithms, ads, or clickbait in sight. Create accountLogin About",
    "commentLink": "https://news.ycombinator.com/item?id=42809268",
    "commentBody": "A QR code that sends you to a different destination – lenticular and adversarial (mstdn.social)499 points by zdw 18 hours agohidepastfavorite69 comments trebligdivad 5 minutes agoSomeone needs to define a human-readable attachment to QR that can be checked by the QR reader; e.g. the root of the URL printed above the QR code at a specific position offset or with a specific mark; so then the QR decoder could OCR it and verify it matched the URL encoded. Only the root of the URL would be included so the QR could include a specific to that location complex path. Now, we just need to backronym SPQR to fit... reply Normal_gaussian 18 hours agoprevI guess an interesting attack would be a screen in a public setting that alters the QR code based on information it has about the current user, without appearing to change significantly. setup: make the QR code as a half/half code have a system to decide preference of target based on external input (e.g. camera based characteristic evaluation) make slight dynamic alterations to the colours of the code to bias the probability of it being picked up as the desired target. Desirable black/white can be made blacker/whiter, less desirable less so. Where to use it maliciously: anywhere where people provide feedback present alternate feedback forms to different demographics to engender the most positive (or negative) results. pretend to offer some form of probabilistic chance to win a prize, but bias winning to some identifiable characteristic. e.g. race, age, \"beauty\" target a specific person have them join a different WiFi network, alter a payment page, etc. In a static setting its less effective. I can't immediately think of a static attack that benefits from siphoning some reduced fraction of users. I'm doubtful most people would notice a QR code dynamically changing, particularly in most public lighting. reply hnlmorg 45 minutes agoparent> alters the QR code based on information it has about the current user, without appearing to change significantly. I doubt many people would notice if your average QR code was to change significantly. Most machine readable formats are just indistinguishable white noise to most people. reply t_mann 17 hours agoparentprevAll of those could be done much more stealthily server-side, though, I don't get what the QR code modification would add here? Also, neither use case makes use of the hack described in the OP. Where I could see an attack based on that hack would be where an attacker plasters their code over a legitimate one. It would be kind of random which code gets read, so they could send some %-age of users to the original destination, hence possibly delaying detection. But it doesn't seem a given that this would compensate for the reduced traffic to their link. reply post-it 17 hours agorootparentSome sort of MITM attack by someone who owns the display but not the server, maybe. Like a malicious ad company. reply t_mann 17 hours agorootparentOk, but then I'd still prefer a method that sends users to a unique URL. OP's method may help with obfuscating the changing of the code, but I'm sure there are ways to better achieve that without having to introduce this quasi-randomness. The simplest would probably be to just to regularly hide/show the code (which would happen anyway on a typical digital ad display that cycles through a number of ads). reply post-it 2 hours agorootparentBut hypothetically, the owner of the ad might pop in and make sure 1. the ad looks correct, and 2. the URL is the one they expect So there may be a use case for a QR code that looks almost identical but goes somewhere else, allowing them to swap it out while someone is looking at it without them realizing. A niche use case, to be sure, but being able to exploit a niche vulnerability is a skill. reply michaelmior 9 hours agorootparentprevBut if you own the display, you can send the user to whatever server you want. reply dspillett 8 hours agorootparentprev> All of those could be done much more stealthily server-side, though, I don't get what the QR code modification would add here? Even if the adversary controls the server side as well, you need to tell the server the information needed to make a decision If you control everything that is easy enough too, but perhaps you want to keep the decision-making process local – for plausible deniability server-side, just to reduce server & bandwidth load, or because you are sending people to completely different destinations not just altering link parameters. Replacing the QR code more statically sends everyone to the new address, not just the target(s), altering the QR code by a bit or two (and the relevant error correction bits too) in response to a pile of information available at the QR reading site (feeds from cameras, and such) would be how the server knows to react differently without having access to that collection of information itself. You might want to use some clever analysis to minimise the visual effect if you are altering the QR code while it is already displayed – the two examples here look very different, but the change could be much more subtle. If sending to very different destinations, so the codes for the URLs will look very different, then adding a link anonymiser between would keep the change minimal. > Also, neither use case makes use of the hack described in the OP. True. reply michaelt 5 hours agoparentprevThere are actually attacks based on changing public QR codes already! They don't need anything as sophisticated as this dual QR code, though the attackers just go to a car park with a \"pay by phone\" sign, slap their own QR code over the \"scan to pay\" code, and wait for the credit card details to start coming in. reply Terr_ 48 minutes agorootparentHmmm, there might be some criminal utility in capturingbanking app displaying the name of the beneficiary and the scammer make an account with a name that look similar at a glance (e.g., swap the l with a 1, or something of the sort). reply fxtentacle 11 hours agorootparentMost likely, they havepretend to offer some form of probabilistic chance to win a prize, but bias winning to some identifiable characteristic. e.g. race, age, \"beauty\" Do a facial recognition lookup against the RealID database (I'm sure someone must be selling a leaked or hacked copy by now) and make the prize depend on the first letter of the person's last name. reply Normal_gaussian 17 hours agoparentprevIdentifying this should be relatively easy in the core libraries; finding alternate valid QR codes using \"less optimal\" grids. Of course the API confusion here becomes non-trivial, which hampers securing against it. And with existing libraries being widespread, its going to linger as an attack for a long time. reply pockmarked19 17 hours agoparentprevYou don’t need any of this if you control the app doing the scanning (or the website/app handling the result). reply Normal_gaussian 17 hours agorootparentYou wouldn't control the app doing the scanning. The attack is that a user looking at a QR code cannot determine that they are being served a different code to another person. In a public setting a user would have no idea they are even capable of being targetted and treated differently. reply t_mann 17 hours agorootparentYou don't even need to change the QR code to treat users differently, that's the point. You just send users to some fixed URL, which is by far the most common use for QR codes (hence completely unsuspicious), and you decide who gets served what there, based on whatever data you gathered about them. And the users that would care are already acutely aware that that's how a majority of the web works nowadays, QR codes or not. reply Normal_gaussian 16 hours agorootparentThe value of an attack vector is not negated by there being other ways to achieve it. This particular vector has the strength of being able to manipulate things other than URLs; QR codes support WiFi credentials, contact details, call, text (with content), email (with content), calendar events. Additionally, many app specific URIs never leave the device. It has the significant downside of being plainly visible as a possibility to those in the know. However, I suspect it may still be desirable to do this on the device rather than the server. The other properties it varies are the obvious lack of reliance on a server. The improved \"transparency\" of showing the target URL for trust. The removal of the need for an internet connection. But for an implementer I'd imagine the most beneficial upside is not needing to manage a timing-attack, and gaining additional targetting accuracy in the common case of a picture. There may also be organisational complexity reduction in pushing all the decisioning to the display/camera system. Its fun to think about, with the significant weakness in both its visibility and its probabilistic nature I wouldn't expect widespread use. reply csomar 16 hours agorootparentprevStatic Qr-Code but serve different content? If you are targeting by the camera, you can try to link the person to the time the QrCode was scanned and have each QrCode print identified. reply jdoe1337halo 10 hours agoprevHey guys I made a website so yall can try this out yourself! I don't have the exact methodology that Christian uses, so here is how I did it: The ambiguous QR code in this application works by combining two different QR codes into a single image using a diagonal split pattern. When two QR codes have different patterns at the same position, the cell is split diagonally one half represents the first QR code and the other half represents the second QR code. When both QR codes have the same pattern at a position (both black or both white), the cell is filled with a solid color. Due to the high error correction capability of QR codes (using error correction level 'H'), QR code scanners can still read either URL depending on the scanning angle, though as noted in the UI, it tends to favor the second URL more frequently. https://dualqrcode.com/ reply HenryBemis 7 hours agoparentIt didn't work for me. I generated two links (BBC, CNN) and tried on my Android, with the app \"QR & Barcode Scanner\" v2.2.47. I also tried with the stock camera of an iPhone 13. In both they couldn't 'read' them. reply Aachen 7 hours agorootparentDoesn't scan from a screenshot in https://f-droid.org/packages/com.atharok.barcodescanner/ either reply andrewla 1 hour agoprevIf you want to try it out, here's some quick and dirty code. Install the qrcode python package and run this code: import qrcode bar = qrcode.QRCode(border=0) bar.add_data('bar') bar.make() bar_mat = bar.get_matrix() foo = qrcode.QRCode(border=0) foo.add_data('foo') foo.make() foo_mat = foo.get_matrix() for l, r in zip(foo_mat, bar_mat): line = '' for lc, rc in zip(l, r): line = line + (lc and '\\u2588' or ' ') line = line + (rc and '\\u2588' or ' ') print(line) print(line) What you get is indeed a dualing qrcode (which I can't quite paste here because no unicode on HN, and using \"8\" or \"0\" isn't enough to get my phone to recognize it). reply danvoell 55 minutes agoprevThis is such a cool concept! If you're focusing on the end goal, another approach could be using a \"switch\" at the URL destination—something that redirects users to a different page based on a randomizer, user data, or other criteria. For anyone exploring this kind of functionality and interested in testing physical stickers for their projects, I work with a lot of SaaS companies on variable labels and would be happy to share insights, print some samples or collaborate. reply nixpulvis 17 hours agoprevThe most interesting thing about this to me is that on iOS a long press on the image claimed it's going to github.com, while the preview itself was for mastadon. This indicates that it's parsing the QR code twice and getting different results? I could see this being used to mislead some people, though I'm not sire how many people look at the long press dropdown URL. reply codetrotter 16 hours agoparentTangential but once in a blue moon I come by some situation where I’m on my phone and I’m looking at something that has a QR code showing on the screen of the phone itself. And so I do something silly like airdropping a screenshot of it to my laptop so I can scan it with my phone camera, or I get someone else (friends, family) to use their phone to scan the code from my phone screen with the camera app on their phone. And all this time I was annoyed why I couldn’t just get the link directly from the image on my phone without involving another device, and without having to install yet another third-party app. And today I learned that all I had to do was long press the QR code in the screenshot in my camera roll and it would actually parse it and make it so I could visit the link! I think I must have tried long pressing QR code in an image in the camera roll years ago because it always seemed like something that would make sense to support via long press. Maybe they introduced this feature after I had tried to long press a QR code in an image in the past. Or maybe it was always possible and I didn’t actually ever try to long press it. Or maybe I long pressed the wrong part of the image that first one or two times I ever tried to do it in the past. Either way, very happy to have learned that this is actually possible. reply kccqzy 16 hours agorootparentLong pressing still doesn't work for me. Perhaps because I turned off some features related to image intelligence. reply poglet 14 hours agorootparentOn iOS I believe the option is in Settings > General > Language & Region > Live Text. This was introduced in iOS 15. reply davchana 16 hours agorootparentprevYes, I usually share it with Google App, and Lens tab. reply layer8 2 hours agoparentprevThis is likely the typical case of related code calling the same function or getter method twice in a context where it is imperative for both calls to return the same result. It is reminds me of code like if someCondition(getFoo()) then doSomethingWith(getFoo()) or even just doSomethingWith(getFoo()) doAnotherThingWith(getFoo()) which is always a code smell, as opposed to foo := getFoo() if someCondition(foo) then doSomethingWith(foo) and foo := getFoo() doSomethingWith(foo) doAnotherThingWith(foo) reply pas 1 hour agorootparentUsually it's called a TOCTOU bug/vuln https://en.wikipedia.org/wiki/Time-of-check_to_time-of-use reply layer8 20 minutes agorootparentI often see it in multiple-time-of-use scenarios as well that need to be consistent, i.e. no check vs. use involved. reply the_arun 14 hours agoparentprevQRCode should also show the target url in text, so the user knows where it is taking something like explicit consent. reply jimjimwii 12 hours agorootparentYep, like how browsers show users urls in their location bars. reply _august 16 hours agoparentprevWhen I long-press on iOS, it shows me the mastadon link as the main \"Open\" link, as well as \"Open in Github\" (app link) in the context menu. reply noitpmeder 17 hours agoparentprevnow THIS sounds like an exploit reply russellbeattie 12 hours agoparentprevI can totally see two parts of the OS both using their own QR parsing code SmartText using one, and the imaging system another. Apparently each one has their own slightly different error correction implementation. I bet it'd be possible to create a standard QR Code with a deliberate error that does the same thing. You'd just have to figure out how they're correcting the error differently. Seems like you discovered a bug-bounty bug just waiting for someone to claim. reply re 18 hours agoprev(Scroll up from the starting position to see the lenticular one) reply dang 13 hours agoparentShould we change the top link to https://mstdn.social/@isziaui/113874436953157913? reply winternewt 7 hours agorootparentYes please reply ShakataGaNai 18 hours agoprevThat is gnarly. My iPhone tended to lock into one or the other, rotating the phone seemed to help it go one way or the other. But a couple times it did flash back and forth between the Mastadon and GitHub links. reply dwheeler 12 hours agoprevShouldn't one of the URLs implement a Rick Roll :-) ? reply mkl 8 hours agoprevMy phone (Samsung Galaxy Note 20) seems to reject almost all of these, and not recognise them as QR codes. I got one to work for one URL by moving way away from the screen. reply 65 18 hours agoprevThis would be cool to use in a scavenger hunt. reply TOMDM 17 hours agoparentMaybe make it so that you need the results of both (all?) QR codes to get the final code/link/key. reply rhet0rica 16 hours agorootparentI summon QR Exodia! reply sschueller 10 hours agoprevI had some trouble with the QR codes readers on my android but I found this demo app[1] (they make a code reader SDK) that work really well (you can also enable continues scanning so you can see the result change as you change the angle). [1] https://play.google.com/store/apps/details?id=com.scandit.de... reply hammock 16 hours agoprevCan someone explain how it works? reply jkingsman 15 hours agoparentA QR code is just a series of square pixels, and whether the pixel is black or white contributes to the data. This has two QR codes, essentially overlayed one on the other. Some pixels are half black/half white, so depending on the angle you hold your phone at, the software that decodes QR codes will detect a different color as being centered in the pixel it's examining. So, based on the angle you hold your phone (some people had better success with rotation), you get one QR code or the other. reply hammock 4 hours agorootparentWhat is half black/white though? I don’t see any gray. Only black and white Did he subdivide the qr code cell into four sub pixels and make the left two one color and the right two another? That’s what I would have guessed for the “lenticular” effect at different angles. But the subpixels I see are more checkerboard ish reply layer8 2 hours agorootparentQR code decoders usually work by sampling the values from a dot grid overlayed on the camera image (or something along those lines). If a QR square isn’t actually uniform, the sampled value for that square will change if you move the camera a bit. When you hold a QR code in front of the camera, the picture is continuously sampled, yielding different bit values all the time because you don’t hold the camera a 100% still (in addition to sensor noise and the like). The QR decoder is however satisfied as soon as the result passes the QR checksum verification. The image recognition that outputs the bit pattern of the QR code is inherently heuristic in nature, and only the checksum verification is what decides if the recognition worked successfully. The trick in TFA is to produce an image where two different results of the heuristics can both pass the checksum verification, so which one you get depends on circumstantial factors. reply hammock 2 hours agorootparentMore non-deterministic than I would have thought. Your explanation makes sense tho thanks reply Joker_vD 6 hours agoprevWait, don't QR codes have checksums in them? Or do they have some sort of error correction built-in? That's the only way I can imagine it can possibly work with e.g. 85% of one QR-code blended with 15% of another one. reply blueflow 6 hours agoparentYes, but the checksum is in the same bitstream, so it is swapped together with the data. reply soheil 3 hours agoprevWith the same link you can serve different content, QR codes typically resolve to links. Looking up user IP (and thus country of origin), user agent, etc is enough to determine what content needs to be served. reply buildbot 18 hours agoprevInterestingly, MacOS only sees the mastodon link when right clicking on the QR code. reply etrautmann 17 hours agoparentThat makes sense, I would imagine it would require some variability via a camera with different angles/lighting conditions in order to get both links at different times. reply tzs 17 hours agoparentprevThat's also what Mathematica's BarcodeRecognize[] sees. reply yoz-y 9 hours agoparentprevFunnily though, iOS recognizes the GitHub part when long pressing on it. reply smashah 16 hours agoparentprevCan MacOS natively scan QR codes? reply Gryadn 18 hours agoprevnext [2 more] [flagged] casey2 17 hours agoparentAttention sells products, if the next apple visions' 3D is good enough it will create a whole new product line, creating thousands of technical problem solving jobs. Pretty impressive for plastic waste. reply daft_pink 17 hours agoprev [–] I don’t really understand the value of this compared to just putting another QR code right over the pre-existing code? Why bother getting a fraction of users, when you can get all of them. reply notRobot 17 hours agoparent [–] The value is that it's cool. reply Consider applying for YC's Spring batch! Applications are open till Feb 11. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "QR codes can be manipulated to direct users to different destinations by creating overlapping patterns that change based on scanning conditions.",
      "This technique can be used for targeted attacks or to serve varied content, but simpler methods like server-side redirection are often more effective.",
      "Despite its intriguing nature, the practical use of this QR code manipulation is limited due to its complexity and visibility."
    ],
    "points": 499,
    "commentCount": 69,
    "retryCount": 0,
    "time": 1737676534
  },
  {
    "id": 42810176,
    "title": "The State of Vim",
    "originLink": "https://lwn.net/Articles/1002342/",
    "originBody": "LWN .net News from the source Content Weekly Edition Archives Search Kernel Security Events calendar Unread comments LWN FAQ Write for us Edition Return to the Front page User: Password:| Subscribe / Log in / New account The state of Vim January 10, 2025 This article was contributed by Murukesh Mohanan The death of Bram Moolenaar, Vim founder and benevolent dictator for life (BDFL), in 2023 sent a shock through the community, and raised concern about the future of the project. At VimConf 2024 in November, current Vim maintainer Christian Brabandt delivered a keynote on \"the new Vim project\" that detailed how the community has reorganized itself to continue maintaining Vim and what the future looks like. Vim after Bram Brabandt began with his history with Vim: he has been involved in Vim since 2006, and said his first commit to the project was made in the 7.0/7.1 days (sometime around 2006). He started by contributing small patches and fixes, and then contributed larger features such as the gn and gN commands, which combine searching and visual-mode selection, improved cryptographic support using libsodium, maintained the Vim AppImage, and more. He said he became less active in the project around 2022 due to personal and work-related reasons. That changed in August 2023, when Moolenaar passed away. Moolenaar had been the maintainer of Vim for more than 30 years; while he had added Brabandt and Ken Takata as co-maintainers of Vim in the years before, most development still flowed through him. With his death, a considerable amount of knowledge was lost—but Brabandt and others stepped up to keep the project alive. $ sudo subscribe today Subscribe today and elevate your LWN privileges. You’ll have access to all of LWN’s high-quality articles as soon as they’re published, and help support LWN in the process. Act now and you can start with a free trial subscription. Moolenaar was the only owner of the Vim GitHub organization at the time, so only his account could change certain settings. Initially, contributors tried to use the GitHub deceased user policy to add owners to the organization. That was quite an involved process, and it soon became apparent that the end result would be the deactivation of Moolenaar's account. Having Moolenaar's account be accessible by his family was important, so they abandoned that approach, and instead the family granted access to it as needed for organizational changes. Charles Campbell (known as \"Dr Chip\"), a Vim contributor for more than 25 years also decided to retire soon after Moolenaar's death. His departure was followed by an expansion of the team of maintainers, as Yegappan Lakshmanan joined it, with Dominique Pellé, Doug Kearns, and GitHub users \"glepnir\", \"mattn\", and \"zeertzjq\" joining soon after. More than just the source code He stressed that maintaining Vim is not just about the source code. There are quite a few other things to be managed, such as the Vim web site, FTP server, security disclosures, Vim communities on other sites such as Reddit and Stack Exchange, and more. Vim's site needed work. The design, and most of the code, had been unchanged for quite a while—until 2023, it was based on PHP 5. In recent times, there had been a few occasions where the web site was unstable, and so he started looking for a new host in 2024. The move involved an upgrade to PHP 8, for which some of the code had to be rewritten. Brabandt thanked Mark Schöchlin, who stepped up to take care of all this. He acknowledged that the design has been pretty much unchanged since 2001, doesn't look modern, and can be scary to new users. There has been some work on redesigning it, but the first attempt hasn't been that successful. He prioritizes consistency and does not wish to scare away longtime users. DNS was also troublesome—the vim.org domain was managed by Stefan Zehl, but Moolenaar also owned a number of other domains such as vim8.org, vim9.org, etc. Thankfully, SSL certificates were already managed using Let's Encrypt, so Brabandt had no problems there. Several email addresses, such as bram@vim.org, bugs@vim.org, etc., were forwarded to Moolenaar's personal email; those have since been updated to point to Brabandt's address instead. The FTP server was hosted by NLUUG, but he decided to retire it and says that he hasn't received any complaints so far. ICCF Holland As readers might know, Vim is charityware, and the charity of choice is ICCF Holland, founded by Moolenaar. Brabandt said that the ICCF is very much alive, and plans to reorganize and restructure itself. Quite a few users started donating after Moolenaar's passing, and in 2023, it raised about €90,000. The project plans to continue to work with ICCF and doesn't want to change ICCF's association with Vim. He noted that there is no sponsorship for the maintainers, all of whom are working for free. Traditionally, all money raised has been given to the ICCF and he has no plans to change that. Brabandt said he earns enough from his job that he doesn't need assistance to work on Vim, so he's happy to let all donations go to ICCF. As an incentive to donate, Moolenaar had allowed people who donated to ICCF to vote on Vim feature requests. Donors to the ICCF could link to their Vim.org account when donating, and then vote on features. This is one aspect that he no longer sees a need for, now that issues and enhancements are discussed on GitHub, and so has decided to shut this down. Linking the accounts and donations was also not easy for Brabandt—he was not sure how Moolenaar did this in the past. Communication channels He also talked about the community centered around the Vim mailing lists, which are hosted on Google Groups. In May 2024, he received an automated message from Google informing him that all content from the vim-dev list had been blocked due to spam or malware. This caused a fair bit of trouble, and while it was restored in around a day or so, he still does not know what the exact problem was. There has been some consideration of self-hosting the list, but one drawback is that everyone would have to sign up again. The mailing list is no longer that active now, with more of the community conversations happening on Reddit or Stack Exchange. Security reporting had to be addressed as well. A couple of years ago, people were reporting issues on the Huntr platform. There were quite a few open issues which have since been taken care of. Huntr was acquired by another company in 2023, which refocused it entirely on AI and shut down general open-source vulnerability reporting. Now, Vim is accepting security reports via email or GitHub, and publishing vulnerabilities via GitHub security advisories. There is a private mailing list for as-yet unpublished security issues, and emails are forwarded to all maintainers. Brabandt has started adding a [security] tag to commit messages for marking security fixes, and such commits are announced on the oss-security list (the most recent being from October) and to maintainers of distribution packages. Maintenance mode Brabandt then showed the contribution graph, to demonstrate that development did not stop after Moolenaar passed away. There was a slowdown as Moolenaar's health deteriorated, and then a spike as he cleaned up the open pull requests (PRs). Version 9.1, dedicated to Moolenaar, was released on January 2, 2024—about four months after his passing. The 9.1 release included improvements to virtual text (which enables completion suggestions and such to appear in the editing area, while not being part of the actual text), smooth scrolling, and OpenVMS support. After 9.1, he started adding more potentially controversial changes, such as support for the XDG base directory specification. Now Vim does not need to litter your top-level home directory: ~/.vimrc or ~/.vim/vimrc still work, but $XDG_CONFIG_HOME/vim/vimrc will now work if neither of the above are present. Another such change is Wayland support. It is not complete yet, and he says he is not sure whether remaining problems with clipboard support are Vim bugs or Wayland ones. As he went through the backlog of PRs, he started developing a policy for merging PRs, prioritizing the need to test things well. Tests are now running with continuous integration (CI). He said that it's also important to have good documentation. Vim has interfaces to quite a few languages, including Python 2 and 3, Ruby, Lua, Tcl, and MzScheme. But Brabandt isn't sure which of these are really needed these days. For example, Python 2, Tcl, and MzScheme (which does not build with the latest version of the language) might need to be retired to reduce the maintenance burden. Other areas to improve include the GUI (GTK 4 has been around for a while, but Vim does not use it yet), support for advanced terminal features, and better spell checking (which has largely remained unchanged since Vim 7). Support for the tree-sitter parser generator is wished-for, but it is controversial, and he does not see it coming to Vim soon. He knows there have been some significant changes in Neovim, but he's not sure how many of those can come to Vim. There have been small changes in Vim, but for major changes, you need community support. He does not want to make backward-incompatible changes and is quite hesitant to merge changes that might break things. He said he has to keep in the mind the whole picture, especially the expectations of users, when dealing with PRs. Currently, he said that Vim is more-or-less in maintenance mode. He said he has created an internal repository to keep track of stakeholders and to ensure that if something were to happen to him, other maintainers could pick up where he left off. Brabandt recommended that those new to the project start by making small contributions and becoming familiar with the codebase. He had some pointers for developers. He said it is important to use a defensive style with C to ensure that new bugs aren't being introduced. One should use Coverity, a static-analysis tool, to scan for defects. Some parts of the Vim codebase are complex, he said, and need to be refactored into more manageable units if possible. Maintaining Vim is a full-time job, he said, and it is not only about maintaining the code, but also the community—managing expectations and listening to users' needs. He has to understand the community: what does it want Vim to be? An IDE? Bug-for-bug compatibility with old Vim? How can we make Vim9 script, the new Vim scripting language, more widely used? How can we ensure that the Vim community remains healthy? He ended his talk by thanking all the Vim contributors and then took a few questions. Questions One audience member asked about the difference between Vim and Neovim's maintenance model. Since most PRs are still merged by Brabandt, would that make him the new BDFL for Vim? Brabandt emphatically denied being a BDFL. Currently, he merges most changes because the version number has to be incremented with each change, so multiple people merging can introduce conflicts. However, when he was on vacation, he handed over the main maintainership to Lakshmanan. He emphasized that it's a community project, and he listens to the community before making decisions. It just happens that at this time the other maintainers don't want to merge changes themselves and instead defer to Brabandt, which is fine with him. Another member of the audience wondered about language barriers, since there are many Japanese members of the Vim community as well as many languages in Europe, etc. Brabandt answered that, as an international project, the primary language for working on Vim is English. He also noted that it is easier these days to collaborate across languages thanks to ChatGPT and translation tools, but it still happens that some users do not communicate in English well, and that makes it harder to understand their needs. The rest of VimConf 2024 VimConf was first held in 2013 by the Japanese Vim user group vim-jp. Since then, the group has organized it every year, until 2020 when VimConf was canceled due to COVID. After a hiatus, it resumed in 2023 with a scaled-down version. The full-fledged edition returned to Akihabara, Tokyo on November 23, 2024. Even though most of the organizers and attendees are Japanese, VimConf strives to be welcoming to all. Presentation materials are expected to be in English, and live translation is provided in both Japanese and English for keynotes and regular presentations, except for lightning talks. PDFs for the talks are available on VimConf's website, and all of the talks are now on YouTube. Index entries for this article GuestArticles Mohanan, Murukesh to post comments Thank you Posted Jan 10, 2025 23:33 UTC (Fri) by decaffeinated (subscriber, #4787) [Link] (2 responses) I wondered about the state of the project following Bram's passing. Thanks for this informative update. The ongoing excellence of LWN reporting is why I am a subscriber. Thank you Posted Jan 14, 2025 5:44 UTC (Tue) by antacon (subscriber, #138885) [Link] Seconded, this is a very great example of a postmortem after a open source project lead passes away and how contributors can reorganize. Thank you Posted Jan 14, 2025 8:18 UTC (Tue) by unsignedint (subscriber, #92715) [Link] Me too, I get a lot of value out of LWN articles. Totally worth the subscription fee Controversial? Posted Jan 11, 2025 4:08 UTC (Sat) by intelfx (subscriber, #130118) [Link] (3 responses) > Support for the tree-sitter parser generator is wished-for, but it is controversial, and he does not see it coming to Vim soon. Is there any reason *why* a better syntax support engine would be \"controversial\"? Controversial? Posted Jan 11, 2025 7:01 UTC (Sat) by kmeyer (subscriber, #50720) [Link] (2 responses) Some discussion here I guess: https://github.com/vim/vim/issues/9087 . Controversial? Posted Jan 11, 2025 10:26 UTC (Sat) by intelfx (subscriber, #130118) [Link] (1 responses) Oh... I see. The amount of disingenuous arguing in that discussion is just painful to read. Controversial? Posted Jan 11, 2025 20:37 UTC (Sat) by Heretic_Blacksheep (subscriber, #169992) [Link] Skimming through that exchange as a casual vim user really just had me shaking my head. A lot of opinion referencing other opinions without much investigation on where problems actually lie. Clason seems to be the only sane one trying to keep the discussion on track to discuss the underlying technology rather than a potentially problematic & experimental implementation. Most of the rest seem to be conflating neovim's experimental implementation with the Tree-sitter specification/technology itself. Copyright © 2025, Eklektix, Inc. Comments and public postings are copyrighted by their creators. Linux is a registered trademark of Linus Torvalds",
    "commentLink": "https://news.ycombinator.com/item?id=42810176",
    "commentBody": "The State of Vim (lwn.net)245 points by signa11 15 hours agohidepastfavorite167 comments omoikane 12 hours agoVIM seemed to have fared well under the new leadership, despite not being able to control the timing of this power transfer. Maybe other BDFL projects would be inspired by VIM's experience and setup successors early. https://en.wikipedia.org/wiki/Benevolent_dictator_for_life reply eviks 11 hours agoparentthat'd be the wrong lesson to extend a period of using a bad governance model for longer reply flohofwoe 11 hours agorootparentYou need someone to say 'no' to all the stupid ideas, and also to the occasional good idea to stay focused. Committees and communities are quite bad at that. IME the BDFL model has mostly worked best for open source software development, unless the BDFL is a complete ass of course. (also software projects don't need to be democracies, e.g. people won't starve or sent to the gulag if things go sideways) reply tomrod 5 hours agorootparentOne of the most powerful ideas I've come across is Arrow's impossibility theorem, which basically states that you don't get a lot of nice group choice outcomes without a dictator in the mix. It's a math formulation, so its not commenting on the realities of a despot, but rather, someone's choices will always dominate if you want a certain set of properties in the decision making. https://en.wikipedia.org/wiki/Arrow%27s_impossibility_theore... reply johnnyanmac 9 hours agorootparentprevIt's the best, when you find one. But the approach is also the riskiest. You may not even pick a BDFL to begin with (I'm sure many of us can name certain repos overly held back by a bad or muddy vision, or being overly conservative with feature/pull requests) . or that B fades away for any number of factors. Committees sacrifice that cohesive vision and agility for being able to have some checks to potential rouge actors. reply vlovich123 8 hours agorootparentThis is what another commenter wrote: > Braam really took Neovim personally and got better at getting stuff into vim that he wouldn't merge before once neovim was arround as a competitor. I really lost track of vim in the last years because neovim is just a solid platform with an active community. I think that's the answer the specific makeup of any given team isn't as relevant as competition which spurs competitors to either keep up or the more successful upstarts to take over. reply giancarlostoro 4 hours agorootparentprev> or being overly conservative with feature/pull requests We saw some complaints about a repo like this I forget for what project some weeks back here on HN and it came down to, people dropping a PR and then the maintainer left holding the bag if something goes wrong, having to maintain someone else's code, which can become a problem if its a completely new feature they didn't implement or want, but users wanted. The other case is, they fix a bug, then disappear, so if the maintainer has feedback, now they have to take time to check out the person's code, update it, out of their current planned work. I wonder if more open source projects would benefit from adding plugin architecture so people can do those one-off features as plugins without \"tainting\" the core project. reply w0m 4 hours agorootparentprev> But the approach is also the riskiest Honestly in open source I'd argue it's not. If an OSS project has significant usage, if BDFL struggles/etc community forks can put the project back on pace. NeoVim is the classic (successful) example that gave us a great alternative while also nudging the BDFL into the modern age. reply normie3000 8 hours agorootparentprev> potential rouge actors Communist assets? reply wakawaka28 6 hours agorootparentNot necessarily. Sometimes people infiltrate projects with the intent of sabotaging them. This may be done by causing a controversy, or else making bad technical decisions on purpose. reply ahartmetz 5 hours agorootparentI think you missed the joke. \"Rouge\" is French for red. Most likely it was meant to say \"rogue\" but it was misspelled. reply pif 6 hours agorootparentprev> software projects don't need to be democracies Especially opens source ones, where anyone can spawn their own kingdom via a simple \"git clone\". reply giancarlostoro 4 hours agorootparentprev> unless the BDFL is a complete ass of course. I think in the case of Linus, you REALLY have to be strict. I mean, its arguably one of if not the most used pieces of software deployed in enterprise and globally for all manner of use cases. reply StefanBatory 9 hours agorootparentprevOr HN too. I think most of us would agree that dang is a very good moderator. reply bee_rider 3 hours agorootparentprevYeah, “dictator” was probably the wrong word. The characteristic of an actual dictator (even a benevolent one) is that they can enforce their will through violence if needed (benevolent ones usually don’t, but they could). As quote goes, “I thought we were an autonomous collective.” Humans have some sort of tribal or pack instinct; if we want to do a project, we’ll often naturally form up around the person who’s doing it, willingly and without compulsion. The leader is the leader because everyone agrees they are doing good enough. Open source project is sort of an ideal form of this. Unlike physical projects, the ability to fork for free means that the leader doesn’t even have the implicit moat that a potential alternative leader needs to take the current work-in-progress away from them. BDFL is just a joke title. reply psychoslave 10 hours agorootparentprevUnless your software deals with tracking people within an organization that will of course comply with the political parties that rule the regions where there software operate. reply fsloth 10 hours agorootparentIf Vim tracks you then that’s a willfull act that breaches trust. IMO no governance model can defend against supply chain attacks. IMO you shouldn’t try to fix things with governance model which it doesn’t fix. Use a governance model that leads to good governance. In technical and creative work single highest point of authority is best. Not because they have perfect knowledge or are the best but because dictator led models lead to best alignment which is something you simply need to execute when performing a complex task with many stakeholders. Dictator led models aren’t best for everything. But for complex tasks with specific fixed scope with multiple concurrent contributors it seems to be the only model that works (if there are case studies as counter example I would love read on them! I.e. complex task, more than 10 contributors, non-dictator governance). reply Propelloni 8 hours agorootparentHmh, I don't have a case study at hand, but there are many reports (and management self-help literature) where teams are shown to be the superior approach to complex situations, especially in software development. Furthermore, considering that there are only about 50 identified open source projects out there with a BDFL [1] and only a few of them are clearly best-of-breed, while there are many best-of-breed projects that do not use a BDFL I'd wager that the perceived superiority of BDFL can at least in part be attributed to the perennial hero worshiping we like to do. [1] https://en.wikipedia.org/wiki/Benevolent_dictator_for_life reply flohofwoe 8 hours agorootparentAny tiny project on Github, Gitlab, ... which is maintained by one person but accepts PRs and feedback from outside contributors is essentially a BDFL project. The single project owner decides what goes and doesn't go into the project by merging or rejecting PRs. People are still free to maintain their own forks with contributions that are not accepted into the main version. In a way, the entire Github workflow is built around the idea of BDFL managed projects. reply fsloth 7 hours agorootparentIMHO No having individual contributor workflows is diffenrent thing than aligning 10 people. The question is not who gets to do the decisions but how the team aligns itself and coordinates. It’s an accurate observation that single contributor projects work well since then there is no need to spend effort in coordination and communication. reply flohofwoe 7 hours agorootparent> The question is not who gets to do the decisions but how the team aligns itself and coordinates. ...IMHO it's the other way around, somebody needs to do the difficult and unpopular decisions exactly for those hopefully rare situations where the self-coordination within a team fails (and doing that in a way that doesn't piss off people in the team). In the end, contributors to an open source project are also just a very loosely coupled team. 'Unpopular' decisions are much easier to do in projects that have a universally accepted and respected BDFL (ideally the project founder) than in most 'commercial' teams led by random 'management-caste' peeps. reply fsloth 7 hours agorootparentprev“many reports (and management self-help literature) where teams are shown to be the superior approach” Isn’t the manager himself the BDFL or a part of a BDFL led chain of command for the team he is empowering in this case? He does not give out the organizational governance-authority just by empowering the team. Having a BDFL does not imply micromanagement or not empowering teams. It’a a different granularity role than implementing day-to-day issues. There are people who are really talented leaders. The problem are bad leaders not the governance model IMO. And this is not a “no true scotsman” argument since effective leadership is a documented and observable phenomenon. Lack of authority is probably better than poor leadership but not better than good leadership (e.g. legendary John Kelly of Skunkworks fame for an fairly well documented example). reply Propelloni 3 hours agorootparentGood point, and I wholly agree that there are really good leaders out there. But I wasn't trying to say that leaders are necessarily bad. I was trying to say that BDFL setups are not necessarily \"the best\" and there are good reasons to think that other forms of governance have just as good or even better track records. We do not talk about them that much because, I think, we just like hero stories and they are easier to tell with BDFL than with groups. reply eviks 10 hours agorootparentprevSaying \"no\" is the easiest job in the world, and committees are pretty good at it. That's why we have decades-old design failures everywhere, because saying \"no\" to an improvement is so much easier that doing the actual governing and resource allocation to see it through, especially with volunteers And given the article describes big fails at basically every aspect of project management (from github account to money and website), not sure where you see the benefits of the supposed \"focus\" reply riffraff 10 hours agorootparent> committees are pretty good at it. I don't know it seems to me that committees are famously bad at it, so that we use \"design by committee\" for something that refuses to take a single direction and either makes bad compromises or says \"yes\" to all options. reply heisenbit 6 hours agorootparentprevI agree, committees are good at saying no. They tend to say no to two not perfect ideas and then force a consensus containing most often the worst aspects of both original approaches. The good news is that the result is so bloated that can‘t be changed and so provides a stable foundation for years of misery. There are counter examples of course but the power dynamic of committees is not conductive to results that have properties desirable in software. reply skriticos2 6 hours agorootparentWhen I think about the two models, I have Linux as the dictator type and XML as committee designed. Both are functional enough, but the while so few data points are hardly conclusive, I think it's generally indicative. I'm not a particular fan of XML, even if it's functional enough to get the job done. Of course you have to find a dictator that is ready to invest all the time and energy to care for a project over a prolonged time and is actually capable of doing so while avoiding to alienate the user base. That's a pretty tall order. reply bawolff 5 hours agorootparent> I'm not a particular fan of XML, even if it's functional enough to get the job done. XML by itself is okay-ish. The true design by comittee disasters are the specs surrounding it. XMLSignature, SOAP, etc reply bawolff 10 hours agorootparentprevThere are two types of comittees: Those that are paralyzed by fear of change and say no to everything, and those that are afraid of offending people and say yes to everything. Both are counterproductive. reply eviks 10 hours agorootparentReality is not binary, so this extreme simplification is counterproductive when describing one reply camgunz 10 hours agorootparentprev> Saying \"no\" is the easiest job in the world It depends on your motivation. Probably a lot (many, most?) committees can just sit pretty and try to do as little as possible. But a lot of software projects exist by getting attention, and they tend to do that by adding features. A lot of FOSS maintainers find it super hard to say no. For some anecdata: I have a small/medium project and I've found that just the energy requirements of really thinking about everything everybody proposes are pretty high. I could just say \"yes\", but then I'm on the hook. I could also just say \"no\", but then I'm discouraging people and not really giving them any information about how to contribute productively--this would be something like, \"sure we could add a flag to turn video upside down, but I'm concerned that putting this kind of functionality in flags means we'll have a UX of 1000 flags that no one can remember or use; should we start considering building another place to put this kind of functionality? reply eviks 9 hours agorootparentYour anecdata supports the no part: > just the energy requirements of really thinking about everything everybody proposes are pretty high Indeed, that's why it most often results in a no (your ending quote is just a polite way of saying no), and you're right about the discouragement part, and that's one of the reasons forks like neovim appear. (and unfortunately often you can't \"motivate\" your way into creating enough time for all that extra work either, so with the best intentions... no it is ) reply cjfd 9 hours agorootparentprevIn your first comment you seem to think that the BFDL for life is not so good. Now you are saying negative things about committees. So what do you think would actually be a good way to run an open source project? reply flohofwoe 8 hours agorootparentprev> Saying \"no\" is the easiest job in the world On the contrary, saying No to an otherwise great contribution that doesn't fit into the longterm vision of a project for one or another reason is the hardest thing in the world. reply ykonstant 4 hours agoprev> How can we make Vim9 script, the new Vim scripting language, more widely used? One way is to inform users and prospective plugin writers that 1) Vim9 script is vastly superior to the old Vimscript, to the point where it is not unpleasant to use, and 2) it is much more conductive to writing text editor code than the general purpose Lua. Of course this still does not mean that people will want to learn yet another scripting language to write Vim plugins in particular when they already know Lua, but it is very important to be adequately informed about the two above points. reply otikik 4 hours agoparent> it is much more conductive to writing text editor code than the general purpose Lua. Lua is very much not a general-purpose Language. It can be used like one, but it's a specialized language thought to live inside a \"host\" application, which it then controls. Which does seem to fit the usecase here. Would you be able to substantiate your claim that it is more conductive to text editor code? > it is not unpleasant to use I'm afraid that is a very low bar. Lua is not unpleasant to use either. reply ykonstant 3 hours agorootparentNo, I don't feel like substantiating my claims to someone who assumes a default hostile response to me and makes nonsensical readings of what I say. For instance, when I compare Lua to vim9 script and say the former is general purpose, I am obviously in the context of comparing the one scripting language to the latter. And you know that. And yet, even though you understand the context perfectly, you still choose to write \"Lua is very much not a general-purpose Language...\" and proceed to patronize me on semantics. So, will I be able to substantiate? Yes. Will I bother to do so to you? No. reply otikik 2 hours agorootparentI don't know how my comment as hostile. But sure, don't substantiate. reply sodapopcan 1 hour agorootparentprevSpeaking purely technically here, vim9/L has an actual standard library tailored specifically to Vim. Lua has no standard library and you just end up delegating to vimscript anyway. But ya, if you want to use Lua for whatever reason then it's a pretty hard sell. But that's sorta what OP is getting at... how to make it more attractive. reply sodapopcan 4 hours agoparentprev> Of course this still does not mean that people will want to learn yet another scripting language I understand this sentiment and that there are certainly psychological blockers in having to learn too many languages, but vim9 is very simple to learn. It is much closer to a \"familiar\" language than Lua. Plus, you are going to have to be familiar with Vim's standard library anyway. I believe they are adding more and more helper functions but Lua plugins are full of `vim.cmd` and `vim.fn`. I don't dislike Lua as a language at all, but I much prefer \"scripting Vim\" in some sort of \"VimScript\" :) But to each their own. reply ykonstant 4 hours agorootparentI agree completely. reply surajrmal 4 hours agoparentprevHonestly supporting a language is a lot of work. Documentation, language servers, ramping time for users to learn it, etc. I find it hard to believe vimscript9 is worth it over lua. I've seen the ecosystem for neovim seem to thrive, and in part because it looks like lua is a lower barrier language, especially if you've used it outside of vim. reply cassepipe 4 hours agoprevI had a easy to maintain, easy to understand vim + ALE + Gutentags + ... setup for C/C++ development and it worked very well but when I got into webdev I just gave up and jump to a neovim distribution as I was not able to catch up. So in the end neovim got me not because it is technically superior but because the community created distributions, which I am very grateful for (R.I.P Lunarvim) EDIT: Ok, maybe the reason distributions were created is because the integration of some lsp/treesitter stuff enabled it/made it easier ? So if not technically superior, at least more capable reply giancarlostoro 4 hours agoparentI assume slowly over time Neovim will just win over vim because of this. I do want to say its much more capable than the original vim, I don't know that vim has a headless mode or that it intends on it, but Neovim has that, plus it can essentially let you write plugins in any language with its plugin RPC protocol. So if you want a plugin that targets your language you can leverage existing libraries that directly support your language instead of writing it all from scratch in Vimscript. reply t_mahmood 2 hours agorootparentVim does have headless mode, iirc. I used it with eclipse or netbeans, can't exactly remember. I have Neovim, it still haven't replaced vim yet. But I see the reasoning, IF I want to use a editor to do heavy development, Neovim seems to have more detailed syntax highlighting, and yes LSP integration good. I use intelliJ with ideaVim for my work, and I don't think these editors can fill the capability that JetBrains offers. Even though vim has a special place in my heart reply giancarlostoro 1 hour agorootparent> I use intelliJ with ideaVim for my work, and I don't think these editors can fill the capability that JetBrains offers. Even though vim has a special place in my heart I keep wanting them to make a Neovim headless plugin for their IDEs. reply pdimitar 2 hours agoparentprevI loved LunarVim as well but after switching to AstroNvim I like it more. And it's easy to customize even for a very burned out dev like myself. reply cassepipe 10 minutes agorootparentI also jumped ship to AstroNvim but I still prefer Lunarvim, I liked having all the config in just one file and some of its defaults. But I agree, Astro is the best distro currently maintained distro in my opinion. reply porphyra 11 hours agoprevMany of the Vim nerds I know, including myself, have switched over to Neovim. Only when using a remote server with a default installation do I use regular old Vim. reply 112233 10 hours agoparentNeovim sounded like a good idea, so I switched, too. Then after an update, it broke mouse selection in terminal, by turning on some crazy option by default. I still have to search how to disable it each time. Ok, things like that happen. Then, after another update, it broke terminal update. Like, your screen scrolls up or down a line and the text does not get redrawn correctly. Is it a wezterm issue? Well, the original vim works flawlessly. So do less, top, and any other terminal programs. Except neovim. No thanks. I dont care how many golden elephants are in the trunk if I cannot drive it. reply linsomniac 5 hours agorootparentFellow wezterm user here, I occasionally have redraw issues with neovim, maybe once a month of heavy use. Whatever happened to ^L to redraw the screen, it seems to no longer work... I've moved away from maintaining my own vimrc, and towards vim distributions, and they all seem to be neovim targeted. First it was LunarVim, and more recently after Lunar stopped being maintained to AstroVim. They have been quite good at batteries included vim, and I'll never go back to vim without LSP. reply wruza 32 minutes agorootparentDoes this imply that Vim has no LSP support? Cause ALE is a Vim-spirited zeroconf-ish LSP bridge for those who don’t know. reply arusahni 4 hours agorootparentprevDo you have your terminfo set up properly? I used to experience bad glitching when working with vertical splits, but it went away once I applied this. https://wezfurlong.org/wezterm/config/lua/config/term.html reply bee_rider 3 hours agorootparentprevNeovim sort of reminds me of zsh, in the sense that I can never really figure out if there’s some underlying technical advantage or if it is just the case that some features people like have been turned on by default. reply lynndotpy 4 hours agorootparentprevThis was my experience. Couldn't get into Vim, and once so got used to Neovim it broke. :( I really love the Helix editor, and haven't had these issues yet, but it's not intended as a 1-1 vim replacement. reply maleldil 5 hours agorootparentprevVim is more stable simply because it changes less often. Neovim is constantly improving its APIs, and that can break stuff. If you don't want to deal with breaking changes, just don't update. You can still use most plugins. reply wruza 9 minutes agorootparentBreaking changes and mistakes are orthogonal to changes to defaults and changes for no clear reason. This separates stable software from perpetual “nightly” one. With Vim you can update through a couple major versions before even realizing it. For me it usually happens after system crashes (classic reinstall windows twice a decade) and the vim config and my habits just work with a new vim download. If neovim thought that something was broken, it was a very valid reason for a quick change. That’s how changes are made and I think most Vimers respect that. It was a proper community fork, almost a textbook example despite the initial buzz. Everyone got what they needed. But if you refuse to update, it’s a clear signal for ideological mismatch. The usual issue with picking pace, as I see it through years, is that there’s often no clear finish line where you switch to walking again. The pace just stays like that forever and people start to grow tired of changes they were happy about. Will that kill Neovim? I don’t think so. People who migrated to it (opposed to newcomers) were built for a change too and will probably “survive” all that. Is your method of dealing with it valid? In principle I agree, nothing wrong with that either. reply kps 4 hours agorootparentprevI've also seen terminal misbehaviour with neovim, not using wezterm, including incredibly slow redraws. reply openmarkand 11 hours agoparentprevI have tried several times and I always switched back to vanilla vim. Neovim has various nice features but it requires a lot of time to migrate correctly IMHO. 20 years of habits are hard to leave, I think. Sure the configuration file is retro compatible, but some of the plugins are better suited for neovim and vice versa. I use a dozen of them and if I switch permanently to neovim I'd like to start fresh using more \"modern\" alternatives that make use of the newer features. reply myaccountonhn 9 hours agorootparentI found the neovim community to operate a bit like the node ecosystem, you pull in a plugin for every problem that solves already solved problems their own way. The plugins are also very flashy with tons of animations, colors and emojis, which to me is just distracting. That said I think people should use what they like, and I am happy that there is a big community developing an alternative to VSCode. I just didn't feel it was for me. I ended up moving to Kakoune. The community is small but the tool is so much better designed and integrates well with unix. That means that i can usually glue together whatever I need myself with 1-3 lines of config and don't need an entire plugin when I want something that isn't built-in. reply openmarkand 5 hours agorootparent> The plugins are also very flashy with tons of animations, colors and emojis, which to me is just distracting I also have seen that the very first time I tried neovim. Some people may like it but I consider a terminal to be as simple as possible. Furthermore, I use often the CLI on non-GUI terminals where such non-ASCII characters can have various side effects (e.g. unicode bars, braille like progress bar and so on, those destroy your serial terminal line). reply sevensor 5 hours agorootparentprevThe Unix integration is so very good. I have keybinds to fmt for line wrapping and date to insert timestamps, and I like being able to pipe a selection to dc to do some quick math. It’s so easy, I do it without even thinking about it. reply sgarland 4 hours agorootparentprev> The plugins are also very flashy with tons of animations, colors and emojis, which to me is just distracting. I don't think I've seen plugins with animations (nor would I want to). Agreed that emoji in the terminal, modulo useful glyphs like language logos next to files in a directory tree [0], are distracting. What I don't understand are people who add a million plugins, and then wonder why the startup performance is terrible. I have a fair amount, including some I honestly rarely or never need, and startup time is still 75 msec, which is fast enough for me not to be bothered. [0]: https://github.com/ms-jpq/chadtree reply emblaegh 10 hours agorootparentprevWhen I migrated years ago (mostly to get access to some plugins), nvim gladly swallowed my old configuration with no changes. Then I could change to lua and other modern features at my own pace. reply Flimm 8 hours agorootparentBy \"swallowed\", I thought you meant that Neovim silently deleted your files. I'm glad the context makes it clear that it didn't. reply fp64 9 hours agorootparentprev…I still haven’t switched to lua, all I need still works fine reply wruza 8 hours agoparentprevNeovim turned itself into a pop-blink IDE which some people never wanted. It's good that it serves the needs of its fans, but it's also good that Vim stayed in its own tracks. Losing Vim as it is would be a great loss for many. reply rand0m4r 7 hours agorootparentI'm glad I'm not the only one who thinks the same thing. I also think that \"stability\" and the \"community\" are two other things that made me switch back to vim. reply jitl 5 hours agorootparentprevWith no config file Neovim is just vim but with mouse mode enabled by default. With Lua and the new APIs it’s much easier to write powerful plugins that do crazy shit and use animation but none of that happens by default, you need to go out of your way to get that kind of action, same as with regular vim. reply sodapopcan 4 hours agorootparentIt's not, though. A bunch has been stripped out (interactive :!, :view, etc) and time only time will tell how much further they will drift apart. reply Anthony-G 3 hours agorootparentThanks for that. I was thinking of trying NeoVim but I regularly use the `:!` feature to filter (selected) text through Unix utilities – and `:view` if I forget to use the `-R` option when opening a file that I don’t want to accidentally edit, e.g., log files. reply ossusermivami 2 hours agorootparent:! itself has not been stripped from neovim, it's the \"interactive\" :! that has been stripped (as the OP said) i.e if you do :!bash in vim you enter bash in neovim you won't reply kps 4 hours agorootparentprevAnd as such neovim has dropped POSIX conformance (including things like `:!` as you mention, that I use regularly) and is apparently not interested in it. I wanted to like neovim for its possibly better lsp integration, but if all I need is an IDE with partial vi emulation, I might as well use VSCode. reply rob137 8 hours agorootparentprevWhat does 'pop-blink' mean in this context? reply wruza 7 hours agorootparentSomething pops out and blinks on itself while you are typing. At least that's what I've seen in neovim \"ads\" videos back in the day. reply melodyogonna 7 hours agorootparentVery annoying. I decided to move from manually creating my own config to using Astronvim and there were just so many distracting things in the default installation. The good thing is that you can disable what you don't want which I did. The bad thing is that finding the right option or the actual plugin that is bringing the functionality you don't want takes some time. reply CGamesPlay 6 hours agorootparentWas the prebuilt configuration worth it in the end then? I recently started a from-scratch config and followed kickstart, which is pretty minimal and also not a plugin/distribution/self-updating thing, it’s just an example config file you can copy. reply melodyogonna 3 hours agorootparentYeah. Once you understand the configuration it's really nice, it is also well documented. reply wyclif 6 hours agorootparentprevA good option is to start with kickstart.nvim. This allows you to build up from a very basic install, or keep the simple default install without a lot of plugins. reply CGamesPlay 6 hours agorootparentprevI just set up a Neovim configuration from scratch and I have no idea what you are talking about. I did install a completion plugin literally called blink, but even that supports keybinding activation. reply wruza 46 minutes agorootparentMy transition was unsuccessful for a differrent reason, although I still think that while being open to change, it’s ideologically correct to stay with ideas you are aligned with. When I tried it, nvim-qt was hard to associate with file extensions on windows (required something like “ “filename”” in different shapes and hours of regedit debugging, can’t remember now), then nvim itself had issues with refreshing on manual window resize and with autoresizing the window on setting ‘lines’. And a few more os integration related issues. It was basically unusable gui-wise so I bailed out due to no good reason to stay. I was probably talking about that astro-thing that works like a christmas tree and was the main selling point at the time. reply lawn 6 hours agorootparentprev> Neovim turned itself into a pop-blink IDE What does that even mean? Just sounds like a lazy and false argument. reply mmooss 11 hours agoparentprevFrom the Stack Overflow survey: https://news.ycombinator.com/item?id=42811182 Vim 21.6% Neovim 12.5% I wonder what it looked like in prior years. reply joeblubaugh 11 hours agorootparentI’m willing to believe that “vim” combines some neovim users, but I’m not surprised that the original is still more popular. Anecdotally vim seems to ship new features faster than it did 3 years ago. reply wirrbel 10 hours agorootparentBraam really took Neovim personally and got better at getting stuff into vim that he wouldn't merge before once neovim was arround as a competitor. I really lost track of vim in the last years because neovim is just a solid platform with an active community. But honestly at work, I think I am the only one using either a vim or emacs (I kind of use neovim and emacs but primarily neovim). In my childhood there was a TV series called \"The last of his class\" and it really showed old people (retirement age) doing jobs that will be gone once their retire. While some jobs truly vanished, others just transformed so drastically that they cannot be likened anymore to the job those folks did. Anyway, I feel we are watching changes in developer tooling that will be seen as the end of an era. * https://de.wikipedia.org/wiki/Der_Letzte_seines_Standes%3F reply mmooss 4 hours agorootparentWhy do Neovim users feel the need to take down Bram and Vim? Now you are taking credit for Bram's work! Why not make an application you love and leave everyone else alone? The community's behavior what I see is a deterrent to using Neovim. reply Anthony-G 3 hours agorootparent> Now you are taking credit for Bram's work! That’s a baseless accusation. I’ve been using Vim for about 25 years, and over the years have contributed small changes to some of the default plugins maintained by third parties (mostly language syntax and `filetype` configuration files). I have yet to even try Neovim but I too noticed that the rate of minor version releases and new features in Vim had increased after the Neovim project got off the ground. reply mmooss 1 hour agorootparentIt's not at all baseless the basis is the parent (now GGP) comment, even if you disagree with the analysis. reply bitbasher 5 hours agoparentprevI would prefer to use vim, but most plugins are limited to neovim these days. I also feel the overall speed of neovim is better and less janky when interacting with buffers and navigating files. The treesitter highlighting in neovim is also better/supports more files out of the box than vim. In vim I was far too used to syntax highlighting being broke halfway through a file for one reason or another (lines too long, syntax new or broken, etc). I dislike setting up neovim, but I also dislike vim's heavy use of language specific plugins. In a perfect world I'd have lsp+treesitter in vim out of the box and no need to install any plugins. reply gtsop 11 hours agoparentprevI went back and forth quite a lot until i decided to stick to vim. The reason being that I want to gain deep knowledge of my editor instead of depending on a gazillion plugins that occasionally break. Now, I understand this is something you can do with both vim and neovim, however the documentation of neovim is littered with both the vim docs and the new lua docs, and there is a vibe of \"you already know all of vim, here is the lua equivalent\", which made it very inaccessible to me. I decided I'll spend the time in the more simple editor of the two and maybe reevaluate this decision again in 3-4 years reply nicoloren 11 hours agorootparentI did the same. I mostly use Vim without plugin and a simple vimrc file. For the heavy stuff and big projects I use VSCode. reply gtsop 1 hour agorootparentI still use a bloated version of vim for my day job, since it involves huge react+ts apps. But i slowly build up a clean vimrc while writting my pet projects reply jackthetab 6 hours agorootparentprev:s/VSCode/Jetbrains/ reply bawolff 10 hours agoparentprevPeople who describe themselves as \"vim nerds\" might not be representitive of the average vim user. reply mmooss 3 hours agorootparentMaybe, but it's hard to be a vim user without being somewhat of a vim nerd. At least you have to understand a new concept of interface, several modes, and lots of keyboard shortcuts that would make you a nerd in any other application. reply bawolff 2 hours agorootparentMaybe, but i use vim everyday. I have no intention of switching to neovim simply because i don't know what the difference even is. There are levels to vim usage, and you can still be very productive while only knowing the basic keyboard shortcuts. reply pjmlp 8 hours agoparentprevThat is the only reason why while being on Emacs side, on the whole Emacs vs vi, I ended up learning enough vi to be productive on customers' UNIX systems. reply yodsanklai 8 hours agoparentprevI switched to VSCode with vim mode. I think it's a good tradeoff to get IDE features with modal editing. reply wyclif 6 hours agorootparentDid you use the Vim or the Neovim plugin? I have had a hard time trying to get either to play nice with VSCode. reply dsign 10 hours agoprevThe aggregate value of each soul that goes away is staggering. Bram is a good example; his work in VIM and his help to children in need will be sorely missed. I wish we were doing more to break that cycle. reply dmortin 12 hours agoprevI wonder how long vim and emacs can stay vibrant. I've used emacs in the last 20 years, so I stick with it, but new generations who are trained on vscode and such are less likely to use such \"old fashioned\" tools. Surely, there will still be emacs and vim users 50 years from now, but the user numbers and the community power will diminish as the graybeards gradually leave this plane. reply Ferret7446 11 hours agoparentYou can't really compare vim and emacs beyond a superficial level. Emacs is fundamentally an interactive shell, like Bash. It has a text editor, also like Bash. It is of course generally more powerful and featureful than Bash. Hence, people sometimes live in Emacs, because it's a shell like Bash or Gnome or KDE. I use Emacs and VSCode. VSCode for some code repos, and Emacs for general computer usage. Meanwhile, Vim is a text editor. It is neither a shell nor an IDE, although it can be adapted somewhat into an IDE. I also use vi (alongside Emacs and VSCode). vi is for editing some text if I am not in Emacs for some reason or if I temporarily borked my Emacs config. (I also use ed, for when I'm in a dumb terminal or I don't want to lose screen context.) Vim or Emacs \"dying\" is not really an issue, although Vim or Emacs losing enough mindshare to keep them up to date as competitive IDE options, maybe that might happen. reply qazxcvbnm 10 hours agorootparentJust for the other side of the picture, I live in vim and use it as my terminal multiplexer. Vim’s my shell. I have thousands of buffers in vim and practically never leave it. I’ve used terminal multiplexers for years before I switched to vim in that capacity and never looked back. The integration it’s allowed between all my buffers and commands and shells is difficult to match in my opinion. reply rand0m4r 4 minutes agorootparentI'm going to try this. I use tmux but the idea of being able to use vim for everything seems nice. If you have tips/suggestions, I'm interested. reply exogenousdata 5 hours agorootparentprevAs a 3rd anecdote, I used vim for 10 years as my primary editor and “shell”. Then 10 years ago I learned tmux and fell in love with its window multiplexing. Now I use vim strictly as an editor/splitter. But I use tmux to split my code from my repl window. And maintain multiple windows where I’m working on different projects. reply abraxas 10 hours agoparentprevLots of editors and IDEs came and went while Emacs/Vim persisted. Through my three decade career I recall the ascents and downfalls of tools like BRIEF, CodeWright, NEdit, JEdit, TextPad, Notepad++, Visual Studio, JBuilder, Eclipse, Sublime and a few others so the cemetary (or hospice in some of those cases) is large. reply the__alchemist 5 hours agorootparentSublime, nor Visual Studio are in the cemetery. reply abraxas 4 hours agorootparentVisual Studio is very much dying. Visual Studio Code is its successor that's very much alive. Those are two different products reply ookdatnog 9 hours agoparentprevI'm sticking with emacs for now because it is the only editor I have encountered that actually works well in conjunction with a tiling window manager; by which I mean: it works well as a single process accessed through multiple windows (here I mean \"windows\" as in OS windows internally Emacs calls this \"frames\") although it has features for managing panes internally, it doesn't insist that you use them and each windows is very lightweight (no thick sidebars, embedded terminal, etc that are hard or impossible to remove). Vim offers the second feature but not the first (each window is a separate process), most other editors I've encountered do not offer the second feature. reply sevensor 5 hours agorootparentKakoune does this too, and it’s amazing with a tiling window manager. On a big monitor, I can get 4-5 terminal emulators across, and in any of them, at any time, I can attach a kakoune client, copy and paste between buffers in different windows, edit the same file in two places at the same time, close all the clients and reattach later, and so on. Emacs is the only other editor that does this, as far as I know. reply ookdatnog 4 hours agorootparentThanks, I might give that a try :) reply rob74 10 hours agoparentprevTo put it into perspective: vi was already 15 years old when Bram decided to write vim for the Amiga, which had a GUI so vim already looked out of place on the Amiga too! but it was still successful, of course (I think) mostly because of being ported to Linux pretty much at the same time as Linux got started. reply anthk 4 hours agorootparentAmiga by default had an Emacs clone on every install. reply mmooss 3 hours agorootparentThat's incredible. Why? And why a clone? ??? reply Narishma 3 hours agorootparentProbably because GNU Emacs was too bloated for the Amiga's 512KB of RAM (or whatever amount the original model had). reply mmooss 1 hour agorootparentThey also might have been conserving storage space. Though Emacs did run on other contemporary systems. reply hnfong 6 hours agoparentprevThis google trends graph is very illustrative: https://trends.google.com/trends/explore?date=all&q=emacs,vi... emacs is surely on a decline, but it’s not obvious that Vim is on the same trend. This matches “theory” and anecdotal evidence: the people who chose emacs probably didn’t like modal editing, and when “better” IDEs came along they just switched. But there’s nothing like Vim (except editors specifically inspired by Vim), and those haven’t gotten any traction if only because every one of us Vim users have hjkl muscle memory burned into our brains. reply jiscariot 5 hours agorootparentGoogle translate says vim = 'I came' in portuguese, so I guess that explains Brazil. reply arccy 3 hours agorootparentgoogle trends can disambiguate the context https://trends.google.com/trends/explore?date=all&q=%2Fm%2F0... both decline, but emacs is steeper reply joelthelion 10 hours agoparentprevTo me, vscode is unbearably slow. I think that alone is enough to keep vim alive. Also, I don't really miss anything from more advanced ides when in vim. There are great packages for almost anything. reply atorodius 11 hours agoparentprevI use VIM bindings in VS Code. Always assumed many do but might be wrong reply xarope 11 hours agorootparentDitto. I went from vim to neovim, but the LSPs for python/go(lang) for large files (not that large, maybe 10k loc) seem to really bog it down (back then, no idea if it's better now), whereas with VS Code it was still performant enough. So I ended up using VS Code with Vim bindings. And yes, with ad hoc work, I still end up using system vim when just doing simple edits (e.g. adding a line to README.md or somesuch) reply dailykoder 10 hours agorootparentAt work I often switched to VSCode because I couldn't get pyright to work with our django project. The errors everywhere were just annoying to look at. So I looked around and found \"ruff\" and \"jedi_language_server\". This combination seems to do the trick. I don't have to configure anything. I source my venv and it \"just works\". I assume our python codebase is something around the 10k LOC, too. I am not mainly responsible for the python part, so I don't spent excessive amount of time in there, but for the time I do, it works nicely https://github.com/pappasam/jedi-language-server https://github.com/astral-sh/ruff reply matwood 11 hours agorootparentprevSimilar. I use IntelliJ, Neovim, and some Zed all with vim keybindings. Modal editing just fits my mind. As an aside it’s also one of the reasons I dislike Notion. I feel like I’m always inadvertently changing the content. reply flohofwoe 11 hours agorootparentprevMe too (there doesn't seem to be plugin that's completely free of issues unfortunately). Vi/Vim is basically the universal text input model which allows me to transcend text editors and platforms. Also I quite often find myself starting a vim instance inside the VSCode terminal for quick text edits. reply orlp 5 hours agorootparentprevI use the \"VSCode Neovim\" extension which lets me use a real Neovim instance inside VS Code, including my personalized vimrc and a lot of plugins. Not all plugins work but if they're just textual good chance they do. reply wyclif 6 hours agorootparentprevDo the Vim bindings work in VSCodium as well? reply Cthulhu_ 9 hours agoparentprevI've never been able to use vim productively, at best I use it to write commits, do interactive rebases and some remote server configuration (cheap VPS for a website); I never really \"grew up\" with it and stuck with Notepad++ and Eclipse when I started out in software development nearly 20 years ago. I will concede that VS Code is the default for many, but I just can't get productive in it anymore. I mainly use intellij, which has its own issues. But I can't say I've ever mastered any editor, the closest was sublime text, and that mastery mainly came from being able to use cmd+p and global search effectively. reply skydhash 5 hours agorootparentYou probably have a good reason for not doing it, but mastering your editor is a great power up. Especially when the task can be ruled based and repetitive. Like a loop of find-select-transform action. reply mordae 5 hours agoparentprevOne of the kids in my computer club found himself to be an avid Helix user. I guess once the dust settles around nvim's approach to LSP and stuff, give or take 5 years, it will build itself quite a following, including young people. I think the hackability is attractive. reply makeitshine 10 hours agoparentprevVim may die, but vim-mode will definitely be around. reply mmooss 12 hours agoparentprevEmacs and Vim have remained popular through several generations. What do developer surveys say? Vim was near the top a few years ago, iirc. reply krykp 11 hours agorootparentWith nvim, there has been quite the resurgence of Vim. Good software tends to be resilient. I believe both emacs and vim will see many, many more years. reply openrisk 11 hours agorootparentNeovim feels indeed the proper future-proof evolution of a standard. Its still a bit cumbersome to setup (fonts, lots of plugins to configure, opinionated and overly decorated UI etc.). The acid test of maturity is the dry functionality you get out of the box in a fresh linux. It should be \"just right\", introducing the new thinking and functionality of neovim without getting in the way. reply aktau 6 hours agorootparentI actually like `nvim clean`, which is just the basics. Early on in the Neovim project, a lot of heirloom defaults were changed to be more modern, resulting in a better (IMHO) out-of-the-box experience. I use `nvim clean` as my man-page viewer: MANPAGER=nvim clean c \"colo sorbet\" +Man! Startup speed is blistering. My current config is pretty stable, and not that large. But if it were causing issues, I'd seriously consider only doing LSP setup, which is not that onerous with the latest APIs (it was already fairly easy with `vim.lsp.start`, but `vim.lsp.config` and `vim.lsp.enable` make it easier still: https://neovim.io/doc/user/lsp.html). reply volemo 11 hours agorootparentprevI wonder, will there be something to emacs, as nvim is to vim? reply kdtsh 6 hours agorootparentThere’s (arguably) an argument to be made that Emacs configuration distributions fit that niche Doom Emacs, Spacemacs, and Prelude provide varying flavours for different kinds of Emacs users. Apart from that, I don’t really know what an application would be to Emacs as nvim is to Vim. It’s more like nvim is to Vim what Emacs is to nano, except Emacs came first. reply ykonstant 4 hours agorootparentprevEmacs seems to be a local maximum that is difficult to overcome. An entire Lisp Machine environment would be better, but it would be a tremendous undertaking and the specialists, i.e. emacs devs, don't seem to be interested in such a thing. A multithreaded version of emacs would also be an interesting addition; I read some arguments against moving emacs to a multithreaded model, but I don't really remember them. reply karthink 1 hour agorootparent> I read some arguments against moving emacs to a multithreaded model, but I don't really remember them. Everyone including the maintainers would like this to happen. The arguments against it are technical hurdles. Emacs is a large ball of global state and the lisp evaluator hooks into everything, including the display engine, so it's not clear to anyone how to disentangle things to the point where the interpreter lock can be released. reply sevensor 5 hours agorootparentprevThis already happened, decades ago. Xemacs was widely deployed for a while. For that matter, vim is the most popular of several editors that did this to vi. (Elvis, stevie, nvi, others probably.) At any rate, I think Xemacs is still more or less maintained, but I haven’t seen anybody use it in decades. reply LeonidasXIV 5 hours agorootparentprevLet's not forget that GNU Emacs also had his competitor, XEmacs which spurred GNU Emacs to improve. Similar with GCC and EGCS where the EGCS later became the new GCC. reply bananapub 6 hours agorootparentprevsacrificing all existing elisp code makes a new editor worthless, maintaining compatibility makes it extraordinarily hard to get anywhere. reply eviks 11 hours agorootparentprevhttps://survey.stackoverflow.co/2024/technology#1-other-tool... emacs is 4% vs VSCode 73%, so it's not popular, though vim still is reply mmooss 11 hours agorootparentThanks. In more detail: Visual Studio Code 73.6% Visual Studio 29.3% IntelliJ IDEA 26.8% Notepad++ 23.9% Vim 21.6% Android Studio 16.1% PyCharm 15.1% Jupyter Notebook/JupyterLab 12.8% Neovim 12.5% ... Emacs 4.2% ... Spacemacs 0.4% So Vim + Neovim = 34.1%, essentially second to Visual Studio, the overwhelming leader. reply The_Colonel 10 hours agorootparentStackOverflow isn't representative. It was always skewed towards .NET world (and thus VS). I assume this is the case because the founders were prominent .NET personalities. reply fomine3 9 hours agorootparentThere's no representative community though reply viraptor 11 hours agorootparentprevWhat this doesn't show is people using vscode/vs with neovim driver or vim key bindings. The vim \"backend\" percentage is going to be significant. reply dalai 8 hours agorootparentprevI don't think you can add them, respondents could select multiple editors. I am guessing 34% is the upper limit if no Neovim users use Vim and vice versa, which is hard to believe. reply oneeyedpigeon 9 hours agorootparentprevThis feels like it's skewed by the question: \"Which development environments did you use\". I know that vim is an IDE, but I still think of it as a text editor. I would probably answer \"Zed\" to that question (not even on the list!) and \"vim\" to a \"Which text editor\" question, although I use a third, graphical editor (CotEditor) for a lot of stuff too. I never use a full-blown 'ide' like visual studio. reply the__alchemist 5 hours agorootparentprevYou could probably lump PyCharm and IntelliJ together; different language modes of effectively the same editor. reply imp0cat 11 hours agorootparentprevI wonder why is Spacemacs listed as a separate entry? reply Dalewyn 11 hours agorootparentprevI'm pleasantly surprised Notepad++ is so high up. reply bandrami 5 hours agorootparentprevIsn't stackoverflow's most-accessed question to date \"how do I exit vim?\" reply eviks 4 hours agorootparentMore like top-200+ by views, though even the wouldn’t contradict much else reply PreHistoricPunk 9 hours agoparentprevSpeaking for myself, I only used Neovim because of its modularity and the keybindings. Imho, everyone should give the basic Vim keybindings a chance at least once and see if they like it. At this point, however, I do not really use Neovim anymore. I switched to Zed, the Vim emulation is pretty good and customizable and most functionality I want is already there along with incoming support for Jupyter Notebooks. VSCode also has these features. It is fun to use Vim/Neovim but unless I need to use it, I doubt I will return to it. reply runevault 11 hours agoparentprevI think Neovim helps with this, though last time I was using it (via... bootstrap? One of the prebuilt addon packs) at some point a Mason update broke my LSPs for multiple languages and i went back to VS Code on my Linux laptop because I didn't want to fight with it. reply mobilemidget 11 hours agorootparentI ran screaming from VS when I noticed how much resources it used and what software it copies on remote servers in case you want to work remote. Did this improve at all in the last year+? reply alp1n3_eth 1 hour agoprevI'm glad I was introduced to NeoVim, because it led me to using Vim bindings in Zed. As a new user to NeoVim, I was okay with investing some time, but man it feels like each update to NeoVim itself, or even the popular plugins, breaks something that I then need to go hunt down and fix. Every answer online isn't any better, pointing to 5 different doc pages. I like my IDEs to \"just work\" and continue to do so after I have them configured. reply xenodium 11 hours agoprevTangentially related and as an Emacs user, I still see the editor as a platform that bends to my needs https://xenodium.com/a-platform-that-moulds-to-your-needs reply Maken 9 hours agoprev>he started adding more potentially controversial changes, such as support for the XDG base directory specification It feels like every single user-facing open source project needs to have its own XDG drama at some point. reply BiteCode_dev 9 hours agoparentI don't understand how it's a drama. It's an old spec, it's easy to follow, it's standard, and you can still keep compact with the old .dir at user root if you want. What's the big deal ? reply eadmund 2 hours agorootparentSome people really don’t like change. I understand that, but I think that the XDG directory spec is really worth having. It’s wonderful to unclutter $HOME. But I do get that for those of us who’ve been using Unix for decades it can be a bit weird when stuff which used to be under $HOME/.$WHATEVER ends up under $HOME/.config/$WHATEVER. reply HellsMaddy 12 hours agoprevPrevious discussion: https://news.ycombinator.com/item?id=42665222 reply tannhaeuser 3 hours agoprevWorth noting there are still elvis and the one true vi available, in case vim gets foobar'd, though I haven't checked their respective states now and last used either over ten years ago. Like with shells, I've always wondered why people are so quick to jump onto particular implementations when the value is in the wide availability of editors implementing vi key bindings, and the comfort of building up muscle memory this brings. Obviously, I couldn't care less about \"plugins\". reply kps 3 hours agoparentAnd nvi, the 4.4BSD unencumbered reimplementation. LSP as a replacement for original vi's cscope integration is the main reason I eventually switched from nvi to vim. reply weinzierl 10 hours agoprev\"DNS was also troublesome—the vim.org domain was managed by Stefan Zehl\" vim.org was created (probably around 1998) and has been owned by Sven Guckes for most of its existence. In the beginning Sven also managed the content but I think at some point Bram took over. Unfortunately Sven passed away not long before Bram. reply skirge 6 hours agoprev [–] vim is everywhere. Vim bindings in window manager. If will last as long as we will be using keyboards. reply Consider applying for YC's Spring batch! Applications are open till Feb 11. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Following the death of Bram Moolenaar, Vim's founder, in 2023, the community has reorganized to ensure the continuation of Vim's development, with Christian Brabandt taking a more active role. The project is addressing challenges such as managing the Vim GitHub organization, updating the website, and considering changes like XDG support, while remaining in maintenance mode. VimConf 2024 in Tokyo highlighted international collaboration, with presentations in English and Japanese, and emphasized community input and careful testing over a single leader approach."
    ],
    "commentSummary": [
      "Vim's success under new leadership has sparked discussions on governance models, such as BDFL (Benevolent Dictator For Life), with differing opinions on single leadership versus committee-based approaches.",
      "Neovim, a modern alternative to Vim, has gained traction due to its features and community support, though some users remain loyal to Vim for its stability.",
      "The conversation highlights the challenges of sustaining open-source projects and the advantages of plugin architectures, with Vim and Emacs continuing to be resilient in the evolving landscape of developer tools."
    ],
    "points": 245,
    "commentCount": 169,
    "retryCount": 0,
    "time": 1737688970
  },
  {
    "id": 42810300,
    "title": "UI is hell: four-function calculators",
    "originLink": "https://lcamtuf.substack.com/p/ui-is-hell-four-function-calculators",
    "originBody": "Just a moment...*{box-sizing:border-box;margin:0;padding:0}html{line-height:1.15;-webkit-text-size-adjust:100%;color:#313131;font-family:system-ui,-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,Noto Sans,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji}body{display:flex;flex-direction:column;height:100vh;min-height:100vh}.main-content{margin:8rem auto;max-width:60rem;padding-left:1.5rem}@media (width Enable JavaScript and cookies to continue(function(){window._cf_chl_opt={cvId: '3',cZone: \"lcamtuf.substack.com\",cType: 'non-interactive',cRay: '907246ff2ff768ea',cH: 'ZWT.SDa4QjYzyFrDES592oFZRHotqvaVAoVAQL.omUk-1737745341-1.2.1.1-cdaL6maKPs3ueaBTX3sFJQiMTaHstRjvo2BSYFgO1YH8sF8WVFXx4gY47b4N5Q1t',cUPMDTk: \"\\/p\\/ui-is-hell-four-function-calculators?__cf_chl_tk=HO1VOGdhqXZXEDfSBxzXBGz6CErqtaI3AsRBwDWNCug-1737745341-1.0.1.1-JcyJRcPtHLp3IgvGFOD.9HKHMIuAl5quezWdVi2IhRM\",cFPWv: 'b',cITimeS: '1737745341',cTTimeMs: '1000',cMTimeMs: '120000',cTplC: 0,cTplV: 5,cTplB: 'cf',cK: \"\",fa: \"\\/p\\/ui-is-hell-four-function-calculators?__cf_chl_f_tk=HO1VOGdhqXZXEDfSBxzXBGz6CErqtaI3AsRBwDWNCug-1737745341-1.0.1.1-JcyJRcPtHLp3IgvGFOD.9HKHMIuAl5quezWdVi2IhRM\",md: \"XnhxAWmChR9Wvkq.A7bBNJCxzVLtTKvvzH6WXTr2ZeM-1737745341-1.2.1.1-OsNdsHJoQBH4zrNEsKq4Q.wIdNWrjhSGjcybpTeHjT3JNSRIWPg.0D46nlvlsfPaHVQ1er._QCfEpCcGee0nDTQvm8T9hzPQQTk5EZxh8s8jYCafqHNu37tt9A0GN_Of92TTeo1b09DkjxNB3ouMGpoKlKKJp1s4AUV.fIb970RAw0XZYn7MyGupfv7U5MyIdcM_ib1gucidhyqKdSfAiNBHwEDPjzbQaempLuef6qWav2WtCPW56qRLopGAgHmIyRGiRFkGCl_L5EH5GLAf664Ac8Sqt9xJkhSR07.NL5uYoyPhDeas23ML9WhXswboB.lQSxyplm47l4uKA_eBXMzDS3pbgWC150HA8UolhsI3XefpBU4_zaV2EiRBeoBc1jfaGCchsdoAuQ.1hKIsb29AMRsnfEUF.V1gXkdSaUaSvJgCPY3xpQVzGoR3huBN96Myt2A5jKQNPo.NEFUw68QrKKEmvCuCSzuNEAOuAW9.1vDXta8REVcu1gCHvn7eL6Qzu2SWZ4UvNTYjWeS5FICyXy0L0IUvHL2.oRZ0u_BFPxfujF6CkFhmGafIBaI6cQeSlMhp0PODCgEgIGaO7GQKSgAXD4YFLcQYFuCzVq.S_CTJX4tBZEQeC.1Whytho8Wr_D6HAW9wIBxCow6qGojGfFpUclzxg.Q0D30wZeJ62bR44eHPM4Sx2esL_U6QJRE9jJDK_Oq_jbug76NDUo1gfsH0MsOY_ThEMtEOC8fC1A4trtgJiW0oMrz_TR4D6aQMW7Wwgz.mZybWwhKQ2rORQSAY4gJ_9CGFU8tqFmrfRL7sVC2fWYPzy.yqmq.HuOd_Ef8YS.2ztCJeXviApAORcfYiIEk851PTdQydn5CXyOpFy_6mWJYQgmELHlxlEw4HU2.7o2HQY.TMyKme8FTeltQ_NxxTXEwaPwXNDZmw..2NAfO3v1Xv6TxMHMXDD84flTiAar4vfKB8TqtWDFkpo3n6kr46mj606VROaiNM0qxu3Uot0Bf6_5YKR94Rhu0Df2gp3ieH.OCY3_QSP4hZ6NVs9MBJT3zYxyRUhItKegfbuWQbBKGWKqdc2fuJq..R8xa1bRWswLgeiyH6S97fxmu3ORaMd5pfhOUWtjPI.8dOY3ckVRO2zI6puxyPqj8oYbnIDtDMLCu_sObAo.scBPoiZx3dGMDK4swQnvC9rtPnx8KyUu.viArCVSTfFLj2hlkJZumx4N0i_YXtzyi2GbXyLMo.rIlkozqP0y.._Zwq_1hFI5_CtxTJ_IwERr63rer2qwAJir_XZbFld.ClxqIuUItSqhmcMWMRvTDCJDOaJxVzACYK5g6q2V8ohpv_.MHDouDRAYiRHla4CoUKse2b9FFuiYJdu5d3KsxCi3IgDY91BTDoNSJjIEX9d0ebamW668BXn9Ig3Hq8iR9nzUTLmxmxGV.o3KD13NkD8Z.JfK7NbSTdql.3fDHppCiYDMlaTh6ynfzul42VOk9BTh8yWg1aP_Iwy2RDhz3FXTqKnGTZ1xSJfe0_x22JJEyM0JTPG6vO_WtyI0q.jiwWQsj8Om_d30HLB5vOtsUIS2Sc09EuJLxUuS_dK0C0KyRmb7jBuGJY43hRaKZpgrmtoWa.wRWTXz88h3xnl9ChsfiiS.fzn8y0Pt4lonQiR69XqqHogdtopxiqHxUQf.0YKZj3TjiPatAKxwfLG1Rj90eSutzKGGPp6EEgPbEtHZV28sdHhqQryryKTD.qX63XRbsgVSvzXiIsB5YbB0ayefis.oeOFmxCeMXCvWLlzrR8M1vS_GpFOS_4T3zUxzIW65Qj0rlFQ3H47YSOFps7GPS.SmrQBJM21CwSgcgj\",mdrd: \"1tDgrZ1G1cwJeCcuQOam3A5_p4XWpDOgoE1Pt5VxJqM-1737745341-1.2.1.1-texaH56qEASBQwtltxcVG0cmKX.aFfeV3hvuqhuB8cyTWRxN87ysAXoaytPo6Hr.JavRXZs.bs269g8aTIj3x7KsN.pHzZ9Y5VxjGEBvu_.is3xIcjQr2SLnADAOxVhWpb75a9uy1aT_PK.Htf8kZ0C4itXUi6cmk5D3JX5L687TaTM.WysFwVovh4xOgtxg3.On6F1It25rOp0nBjRi7Mcs2b1b5NAcwV9CROqLj.FK4FNu7kkEHw5KZOKKkDI3weq3w9xdorO.NTkExc5LAWsz0FfWg315bj3xVd7sbznIfEst9aGmhuXO0jHlKZCy5DaIknT8FZpVQL11gxpxty_Pwh0Hj1owdCaeCJaf5rSbRKiGmOoHWOvQQsHCNAxo6jEO3qqusA1qPzg0Ea2HsZIMVdiRV62oPktRFcmqzNLyu2oQYFVRClk91SmsWeJW8rnZJ_9H6Thu5.62_FmTdGR2Z_a_Zv.cgGtgY4rAPKzjeLnJNVIAaqS5PVC3_eymkTeAMBppWT77Ba.j6Uc53GhNi65sRELo0YgdPWKF3gdn2nfxEZ5BgO1H2jsyuDM5HZHNDzCFBzAt3bmd_G2CrOLW5tsscTsjL9.rn2i4wXVnJRqCZAea5zuvkHnd3v5xHXmlb0uTzhoGnlQzVa08YI85UigwVeJhpJIPotXqvvxlvd1yCH.jDd1AEjntclRWisK254bWh8YCt3Bge9xlf5mMlJGwx_Rf_RYJsh44Xk4GKN.UNs4.z0_J1X4Uq16ZSddTe0PzeNiim84o5esanx7.9Sv07JkDcMu6QXsrp2kS07sPCpnPLPMbD.iPesDr61OZ7ZmeKD7hmCSat6a8w7ROUOS_5sgyaqVLouUZG9l2VWWbWFYEHxGuSBRcmwclsi2UdMsy1b0tRN34GlA5AI8026Q8b85EsCkIoBHXFdYFQPvKWE95DsFpFzKqH4Ftu0v3B6q5zPPEm89Byksq8WBur_z1837xN8ju5KLc7cmlOYtinXA1W2XWLjznC2W0l94fpDEwblmZrnR0oNB1YS4ordp0M6EEr0v20OKTVaxEZa3wcVDlQ0aZm7PqB7bkAsEkt6O4xPIDAzmZ.a4tCCXC5YjIyW1vC3bos287bbARyvCjNcPKEMXlddOu4rMih5.2cEOzXTPWYG14efUhlBGeGQVunBLlbQ5yQ2VIgBeQvVOLli21x8pQeX88QpuqyAPCqEQHrjO30EF.skWNR83yOEbcutWLTT.93F3lcJyU4fPBheJQJViFqvkjK6eSL9CaE11fuIqsTFE9tEtYYtWZwPRkommrpzuxvY7oH7Kg.FJZ9oKKcv62BMAOLkJ2uFGfbnk16IXBcZ.Cvj5aZr7ChfnkihaT4qBkOR3QIuGcPwvQCf97a84Jz4Ltd.CbDH1AEyGp36SsUcxTB8sTXCrqBKVrEKUIyfDcWS0oQXhFjrISYTo54mR8wKS4X7KK7A8VsxXvLzVWcVERd8agHYaped.zrsZhtiUXyEaVHEUIj.FG0koVrYf0FKx1nYjhZM15mCufhltvIUTXP0PFqQRXF12iURflwqGm8rEQxszLNt27Y16vNks_GpnR7UUcc6OdnK23MvOT0RRpElMY3lmPQuJjGu__bj.Bij26tMriVdeJTGzVhiQgUJzz86nV6epsHx5L7GhuDPOPc5rr0DVWUWDUr2bH8DHA3KFEp3Dzu7_dyWGtyd5z0_XRjWW0pH6rPR7aF_cTCXQykivFSUKnoTSoGmbXvxgRMyiCWlkW9GagDjdqgifsuPUhHLV9DmL2r8aU0POkxz6DDYwGT00xAtACMmER17M5AMwSzWBTKilQ2zL6MmwDkommoGEtCD43p6QF.lPcRiKVdzrdTPTsuIsP9LZ.LdaNmt9ixIqnyBZ0oLXRH7LRnW_BGKVYcX74JpV7Y.6sW8PR7x2f0MUxVEUNpLl_sv5aYgxc31JdYgtbzo.V3bR2WfbGtEMFt9yinuVYQb8o46K49DWq0NRbd16EuEDUVYwt5mmvrTfcLyE7txYhPdt1agmbOnp2TmUGjAqoCK9LWe0ZRuNydtS3NUcvyEPXeOJOWba5UkjrmHvT0Im.uhRqskUnt1hi0.S1I6pj3GzoDtW.XjzkJ1wq8vOFJPeH_bYxgnnL4f1nv0bWvTrKTOXJwgNbwMOoJLarFSoRhhHbIz8B0NFUNGqweTHXGthIq.3WZf5RWI3vwozKcowpW0d94.0vURRBuBM5LOQTQvlI4WYl.HqfshSwLWg92x5mJ5oDBrtQHyv_5KwnSEP6TnKszU.u_rZ1UcgaqjDeXYggYi5Igkw966S6CgSqb1NCp7.GGzmQ7vsIQIO1yFAGn9E8YSOAn58FKkj2bK.hwvPIjEAXtShSZ30IPglkwvTdiujMWwdDWXoZJMDXuZj_u14YpUr0IBeH\"};var cpo = document.createElement('script');cpo.src = '/cdn-cgi/challenge-platform/h/b/orchestrate/chl_page/v1?ray=907246ff2ff768ea';window._cf_chl_opt.cOgUHash = location.hash === '' && location.href.indexOf('#') !== 1 ? '#' : location.hash;window._cf_chl_opt.cOgUQuery = location.search === '' && location.href.slice(0, location.href.length window._cf_chl_opt.cOgUHash.length).indexOf('?') !== 1 ? '?' : location.search;if (window.history && window.history.replaceState) {var ogU = location.pathname + window._cf_chl_opt.cOgUQuery + window._cf_chl_opt.cOgUHash;history.replaceState(null, null, \"\\/p\\/ui-is-hell-four-function-calculators?__cf_chl_rt_tk=HO1VOGdhqXZXEDfSBxzXBGz6CErqtaI3AsRBwDWNCug-1737745341-1.0.1.1-JcyJRcPtHLp3IgvGFOD.9HKHMIuAl5quezWdVi2IhRM\" + window._cf_chl_opt.cOgUHash);cpo.onload = function() {history.replaceState(null, null, ogU);}}document.getElementsByTagName('head')[0].appendChild(cpo);}());",
    "commentLink": "https://news.ycombinator.com/item?id=42810300",
    "commentBody": "UI is hell: four-function calculators (lcamtuf.substack.com)192 points by surprisetalk 15 hours agohidepastfavorite112 comments carb 3 hours agoIf anyone else was curious about the final image and the `x÷` button: http://www.vintagecalculators.com/html/sharp_el-8.html > The keyboard on this calculator has the number of keys reduced to the minimum by the use of only three function keys, including a combined \"x÷\" key. > Here, pressing \"x÷\" gives the multiplication function if \"+=\" is subsequently pressed to give the answer, and gives the division function if \"-=\" is subsequently pressed to give the answer, so: > 4 x÷ 2 += gives the answer 8. > 4 x÷ 2 = gives the answer 2. reply thih9 22 minutes agoparentThank you! This looks surprisingly intuitive, perhaps even comfortable, especially when muscle memory kicks in. Then again… I guess 4 += 1 += gives the answer 5. What would be the result of 4 += 1 = ? Edit: the answer is “3”, elaborated on the linked page: http://www.vintagecalculators.com/html/calculator_keyboard_l... reply gf000 7 hours agoprevJust putting it out here before the recentish redesign of ios's default calculator, it had a widely non-intuitive behavior. Due to how it looked I assumed it works as a standard four-function calculator, but it was actually storing the whole expression in memory and executing it as per the standard precedence rules. I do usually prefer the latter behavior, but without it being displayed as in more advanced calculators (it has been changed to show it) it's just not what I would expect at all, resulting in wrong calculations. But maybe it was just a \"me-problem\". reply ATMLOTTOBEER 3 hours agoparentYou’re right it was terrible for a while. I’m glad they fixed it. reply Suppafly 2 hours agoparentprev>Due to how it looked I assumed it works as a standard four-function calculator, but it was actually storing the whole expression in memory and executing it as per the standard precedence rules. Isn't that how even fairly cheap 'real' calculators do it now though or do really cheap ones really just do one operation at a time from left to right? reply SoftTalker 2 hours agorootparentI would have assumed that cheap (i.e. 4-function, LCD) calculators today have the same internals as cheap calculators from the 1980s. Why reinvent what works, when you can just keep selling it. reply Someone 2 hours agorootparentWhy would you assume iOS, not being the cheap option for a smartphone, would choose to emulate a cheap calculator? There were non-cheap calculators in the 1980s, too, that did proper evaluation. The ones I remember had keys for parentheses, though, allowing you to enter, for example (3 + 4) * 7 = to get 49. For an example see the TI-25 at http://www.datamath.org/Sci/Slimline/TI-25.htm reply kevin_thibedeau 1 hour agorootparentThe point is, precedence following expression evaluation calculators starting from the 80s have always shown you the expression. No expression and no parentheses buttons (accumulator stack) implies a simple accumulator based design. reply Retric 22 minutes agorootparentThe app did have parentheses buttons, depending on rotation you’d get simple or scientific UI. Rotation changing how the calendar handled order of operations seems even less intuitive to me. reply pasquinelli 51 minutes agorootparentprev> Why would you assume iOS, not being the cheap option for a smartphone, would choose to emulate a cheap calculator? i'd assume that because the ui for the ios calculator app is the same as the ui of a cheap calculator. not sure i would ever consider the platform that app is running on. reply Retric 25 minutes agorootparent> ui for the ios calculator app is the same as the ui of a cheap calculator. Rotate it and the app was a basic scientific calculator. reply SoftTalker 2 hours agorootparentprevWell I was responding to a post about \"really cheap calculators\" not non-cheap ones or iOS ones. reply the-grump 5 hours agoparentprevOne of first things I do when I set up a new iThing is to delete the calculator app and install a third-party calculator. reply ChrisMarshallNY 1 hour agorootparentI’ve been using PCalc, since last century (it does have an RPN mode). https://pcalc.com/ reply ghaff 5 hours agorootparentprevI've been RPN for so long that I pretty much need an RPN calculator app even for relatively simple stuff. reply kstrauser 2 hours agorootparentI didn’t start using RPN until many years after graduating college, but took to it like a fish to water. I never again have to guess what the calculator might be doing behind the scenes. It’s doing exactly what I told it, no more or less, in precisely the order I told it. reply ghaff 4 minutes agorootparentWhen I started college, HPs were still really expensive so I got a TI which still wasn’t cheap. But calculator prices were plummeting like a stone and got a (relative) deal on a HP-55 a year or two later. reply kqr 11 hours agoprevI realised reading this that, even though I used to be quite adept with these common four-function calculators, I have since forgotten most of their operation. The past ten years or so I have exclusively been using RPN calculators – I find them so much easier to work with, and going by this article, possibly even easier to implement sensibly? (My main drivers are Emacs Calc on the computer and RealCalc on Android devices. For kitchen work I use a slide rule, which is something I think more people should do! There is nothing better for translating proportions.) reply 7thaccount 6 hours agoparentI own several RPN calculators (they are very cool), but still find modern TI or Casio calculators to be superior now when I'm not using a spreadsheet or something like Mathematica. Even 15 years ago, you could type out an entire equation that would have to be used multiple times and just update a single part of it and hit enter and then scroll up, modify, and hit enter again. All of that without having to resort to keystroke programming. RPN is easier to reason about, but in aggregate doesn't save much time when you have a really slick modern interface and can scroll around and see things pretty printed. In engineering it is common to have some crazy looking things that are just easier to reason about when you can see it matches what is on the paper in front of you. reply billmcneale 53 minutes agorootparentI used to be in love with RPN and owned multiple HP calculators (HP-41, HP-15) but RPN is cheating by offloading effort on the user. When you see a complicated, parenthesized expression, it's up to you to figure out the deepest expression, enter it first, and then work your way out. Standard calculators require more keystrokes but they can be entered without much thought, left to right. And these days, you can also edit and modify your input, so it's hard to justify RPN. reply kccqzy 44 minutes agorootparentWhy would you need to enter the deepest expression first? reply kstrauser 2 hours agorootparentprevI have an HP 50g that makes it easy to flip between RPN and textbook-style graphical equations where you can arrow around and tweak any value. You can have both approaches in the same place. reply 7thaccount 2 hours agorootparentAgreed, but the 50g is ancient and is the only calculator to my knowledge that has that capability. For the most part the options are an older style RPN calculator (even the super nice swiss micros don't have equation scroll, although you can at least see the contents of the stack on the screen like the later HP calcs could), or what TI and Casio have. reply bruce343434 2 hours agorootparentprev> things [] are just easier to reason about when you can see it matches what is on the paper in front of you. I have this sentiment every time I dive into some \"other\" programming math notation. Such as (lisp), rpn, infix without precedence, etc. In theory I like it, but then in practice it's just kind of painful. We humans are entrenched in our bad but default notation! reply roelschroeven 2 hours agorootparentIs infix notation even that bad? I guess RPN has advantages for use as an input method for calculators, but as an actual algebraic notation? It seems to me it's very cumbersome to perform standard algebra on equations in RPN notation (or prefix notation, for that matter). I can't be sure but as far as I know nobody has using anything other than infix for algebra. Every time I see a text explaining the advantages of RPN, it's always in the context of calculator input. reply 7thaccount 2 hours agorootparentThere are disadvantages to RPN as well. I read a book on the Forth programming language (also uses RPN) and some calculations that are algebraically simple require pretty bizarre stack shuffling with RPN stacks. I wish I had the source example. It was something pretty standard in a highschool algebra book like the midpoint formula or something like that. The reverse might be true as well in some situations, but I can't recall any examples outside of RPN's historical advantage against old TI calcs in the 80s. Maybe a forth or RPN enthusiast could help. reply kibwen 1 hour agoparentprevAgreed, my spicy take is operator precedence was a mistake that we regrettably inherited from mathematical notation. Parentheses for grouping are fine, at least. reply taeric 1 hour agoparentprevI'm super intrigued on the slide rule idea! Where did you get one and what sort of stuff do you typically do with it? reply alexjm 16 minutes agorootparentI have a kitchen slide that I use for ratios in recipes. It's an old plastic one from Think Geek. For example, I usually put 15 grams of coffee with 8 oz of water (please excuse the mixed units). To make a different amount, I align the 1.5 on the top rule with the 8 on the bottom rule to set the ratio. Then each number on the top rule (coffee in grams) matches the scaled value on the bottom rule (water in oz). The 6 on the bottom rule aligns with ~1.1 on the top, meaning I should brew my little six-ounce cup with 11g of coffee. In practice, I do this a lot with bread, but the \"baker's percent\" convention for writing bread recipes makes it a more complicated example. Another way to use a kitchen slide rule is when scaling a recipe. Say I want to make 2/3 of a batch of cookies. I line up the 3 on top with the 2 on the bottom. Then for each ingredient, I find the recipe's quantity on top, and read off the scaled quantity on the bottom. This works better with recipes that use weights, to avoid awkward fractions or converting between units so you can subdivide. reply layer8 7 hours agoparentprevYes, RPN is easier to implement, and for the same reason also easier to reason about as a user, reply persnickety 11 hours agoparentprevRPN is definitely easier to implement. I helped someone do that as a student project and while it was minimally complex, there were no edge cases with the operators. You pay for that by having a stack rather than a small fixed number of variables. reply enriquto 9 hours agorootparent> You pay for that by having a stack rather than a small fixed number of variables. you can easily add variables to your rpn calculator. For example \">x\" pops the top of the stack into the variable x, and \" I haven't used any 4-function calculator that behaves like this; instead, I get (without clearing): Then you haven't used very many calculators: https://tedmuller.us/Math/Calculator-1'Introduction.htm reply gnatolf 13 hours agoprevThis article makes it sound more complicated than it is, collecting all those edge cases. Many of them can be simply ignored, as in being no-op. Which is also what most cheap calculators do... reply mazambazz 12 hours agoparentI, too, have been caught in the trap of trying to accommodate all possible user flows. Sometimes you just have to put your foot down and just say \"No, that is not how you use this tool\". reply jansan 11 hours agorootparentTraditionally this would be solved by inserting a comment into the code that insults the user (for example the classic \"The user is a wanker\" comment [1]). [1] https://www.theregister.com/2006/10/13/code_outrage/ reply mrbombastic 9 hours agoparentprevI think there is a lesson here. Don’t let edge cases define your complexity whenever possible. Check if it is in fits your valid expressions and if it doesn’t do nothing, otherwise you get a mess of if x then y reply userbinator 4 hours agoparentprevThose \"edge cases\" actually aren't; they're simply \"emergent behaviour\" of how the state machine in the ASIC works. reply the__alchemist 4 hours agorootparentYou don't need an ASIC in the way the article's challenge described the lunch problem. Do it with an MCU. The article goes into how the author did it at the register level, but that is not how a modern audience would do it. If you're doing registers, it's probably for your MCU's CORDIC peripheral etc for trig! You can even go fancy with floating point if you want (Probably elims the $0.30 MCUs). Tiny, cheap, and you can model it using appropriately expressive data structures (like rust enums) that handle the edge cases at the CPU level. I don't mean to dismiss your comment; it's important. I think this article is skirting a gray area of which constraints are applied. Then the conclusion is altered. Is it saying making a 4-function calculator is complicated!, or is it saying making a 4-function calculator is complicated if you add a number of specific restrictions and requirements (No modern CPU code, exact behavior replication to all combinations of user input etc). The latter is less interesting. reply manchmalscott 2 hours agoprevThis is why every calculator ever should have a RPN mode. Postfix notation removes all of that parsing ambiguity. I ended up making my own RPN calculator in C++/Dear ImGui because I wasn’t happy with any of the options for desktop Linux and, implementation wise, RPN is dead simple. Grab values off the stack, apply operator, push back onto stack. reply gushogg-blake 8 hours agoprevI think calculators should have \"into\" and \"from\" so you can do division and subtraction on the previous result without retyping it. I made a demo: https://fluent-calc.vercel.app/ Also supports saving a result into a variable for later reference, and x \"as proportion of\" y, which is just an alias for division. reply beeflet 8 hours agoparentMaybe you could do the same thing by having a negation and reciprocal button? reply layer8 7 hours agorootparentYes, I use those all the time for exactly that purpose. In many cases you don't even need to worry in which order to enter the operands when calculating a difference or ratio, just press negate or reciprocal if the result doesn't look right. reply gushogg-blake 7 hours agorootparentprevYeah, having dedicated operators for going the other way just fit my mental model better for the types of tasks I had in mind for it. reply beeflet 7 hours agorootparentHmm, well then maybe it would be simpler to have a single button that swaps the input and the accumulator? into and from may be easier for the average joe to read though reply layer8 6 hours agorootparentRPN calculators have that (swap button). reply openrisk 11 hours agoprev> Let’s start with the basics: the simplest calculator has ten digit keys, a decimal dot, four arithmetic operators (+, , ×, ÷), a result button (“=”), and a “C” key to reset state. Mathematically there is already quite a lot happening here and in addition (pun) we use base-10 representation of numbers, which is not the simplest. Might be interesting to explore if decomposing the problem into even simpler \"sub-calculators\" (e.g., starting with binary addition) and then putting everything back together as a sort of progressive enhancement would reveal the most logical or economical UI. reply kazinator 3 hours agoprevSimple calculators are actually a bit like functional pipes. There's no precedence. Everything is left or right. Every button is a prefix command: OP ARG or else just OP. For instance 3 + 4 x 3 1 x 9 can be evaluated in a particular Lisp dialect as (lflow 3 (+ 4) (* 3) ( 1) (* 9) ) Where lflow is a left inserting pipe operator. It begins with the value 3, inserts it as the left argument into (+ 4) to actively produce (+ 3 4) and so on. In the calculator, the result of the previous operation is similarly another argument to OP, which is inserted on the left. It can be understood as being held in an accumulator register. If OP requires ARG, the evaluation doesn't occur until another OP is entered, or else the = button, which roughly means end of ARG. But OPs that take no argument other than the accumulator dispatch immediately. When OP ARG is followed by multiple =, it is repeated that many times. When an ARG is entered not preceded by an OP, it replaces the accumulator. In some calculators, when a new ARG value is entered this way, followed by =, the value is somehow substituted to the previous operation and it is reevaluated, instead of just becoming the new accumulator. reply rbanffy 8 hours agoprevI remember fondly having figured out how to implement precedence with two stacks, one for operations and other for operands, the same way my Texas Instruments calculator did. It is possible to implement a desktop accessory calculator in a lunch break, but only because you get a whole lot of abstraction done by the GUI environment. Modelling the UI as a state machine in this case is essential so you don’t get crazy with all the corner cases. reply jamesfisher 10 hours agoprevAs usual, this is completely unreadable on mobile due to Substack's hatred for zoom reply brundolf 54 minutes agoprevThe intrinsic hardness of UI programming is representing state and state transitions. OP's problem would be much easier in a full language with things like structs and dynamic lists, but it still wouldn't be trivial People sometimes miss the point and think of UI dev as painting pictures with code, which is part of it, but the hard part is state reply tmjdev 6 hours agoprevI think this just illustrates why hidden state is so difficult for the user. It's hard to replicate old calculators because it was poor UI that was making do with what was yet available. reply bawolff 14 hours agoprevI always found percent key behaviour to be such a bizarre choice. reply theamk 14 hours agoparentWhen I was a student, I thought the same. Later when I designed some expression evaluators I've started to appreciate the language design (and hacks) required to make \"100 5%\" work \"as expected\" (that is, return 95). It's a pretty unusual grammar, and it is neat that it was implemented on something with only few dozens bytes of RAM. reply contravariant 11 hours agoprevI still wish to find the idiot who thought the reasonable thing to do when someone types 2 followed by SQRT is to return 2sqrt( reply bowsamic 8 hours agoprevI'm not joking when I say I'm 30 and I still don't understand how to use a calculator without a proper display where you see the entire expression to be evaluated. At school in the UK we all had calculators where entire expressions could be typed in including proper fractions and parentheses. I find the more basic kind of digital calculators, including built-in calculator apps, to have totally impenetrable user interfaces reply khaki54 6 hours agoprevNew technical interview question just dropped reply andrewfromx 14 hours agoprev\"those cheap plastic rectangles hide decades of edge cases, quirks, and questionable design choices. Want to change a number's sign? That's a special case. Chain operations? Special case. Divide by pressing equals repeatedly? That's a feature called K-constant, except multiplication plays by different rules because... reasons. By the time you've handled every possible button combination, you'll wish you'd stuck to programming smart fridges\" reply labster 12 hours agoparentBut the calculators last much longer than smart fridges because they don’t rely on central server existing to continue operating. reply bathtub365 12 hours agorootparentThey fixed that by making solar calculators that rely on the continued existence of our temporary sun. reply TeMPOraL 11 hours agorootparentI thought those solar panels are fake decorative elements only; the calculator still needs a battery to work and stops working when it runs out? reply Ekaros 8 hours agorootparentSome didn't. In today's world it is actually amazing how little power those things used. Both for that type of LCD screen and doing some calculations. Just compare it to almost anything else we use daily. reply persnickety 11 hours agorootparentprevThere were calculators in the '90s where the display would go faint when you covered the tiny solar panel. Perhaps the battery was already drained. Quite common I would say. reply saagarjha 10 hours agorootparentI had one in the 2010s when I was going through grade school. Casio fx-260 solar; it would die in seconds if you covered the panel. reply aitchnyu 8 hours agorootparentThe screen would go faint and slow when the power sources werent running at peak. reply 01HNNWZ0MV43FF 11 hours agorootparentprevThere's counterfeit ones yes, but somewhere I have a real solar calculator. I even had a duplicate of it where I unplugged the broken solar panel and put 2x AA batteries on the back to power it. So the solar panel definitely worked reply iamnotsure 4 hours agoprevDoes your calculator use bigints for subtraction? why? why not? reply 2-3-7-43-1807 3 hours agoprev> Yes, there are buttons. No, it doesn’t support double-tap or long press. how do you diff between div/mul if not by double tap? reply billmcneale 47 minutes agoparentThe keyboard on this calculator has the number of keys reduced to the minimum by the use of only three function keys, including a combined \"x÷\" key. Here, pressing \"x÷\" gives the multiplication function if \"+=\" is subsequently pressed to give the answer, and gives the division function if \"-=\" is subsequently pressed to give the answer, so: 4 x÷ 2 += gives the answer 8. 4 x÷ 2 = gives the answer 2. reply troupo 11 hours agoprevOh, it doesn't even get to the juicy bit of percentages. Can't remember how physical calculators deal with them, but software calculators deal with them differently. To the point that iOS calc and MacOS calc are different. And variations of Windows calc (regular/scientific/engineering) are different reply ForHackernews 4 hours agoprevI think there's something to be said for UX consistency even if it's \"wrong\" in some ways. To a first order, every basic calculator works exactly the same, and it's easy to use them all once you've learned how to use one. In contrast, every microwave in the world has reinvented its own new system for entering a time, and added a bunch of extra buttons for useless fake features. reply kazinator 11 hours agoprevGoogle's calculator on Android is an absolute shitshow. Sorry for omitting details, but this issue is easy for anyone to repro without any specific steps. reply kccqzy 37 minutes agoparentDoesn't the Android calculator use constructive reals? https://cacm.acm.org/practice/small-data-computing/ reply quesomaster9000 14 hours agoprev [–] TL;DR PEDMAS is bullshit If you look at the internal state of the typical physical calculate it's a beautifully simple machine that tries to catch the balance between RPN and logic... except it has stupid nonsensical human rules interjected. So, every time this article where points out \"actually it's more complicated than it seems\" is where historically somebody has made deliberate design decisions to do it 'the dumb way' even though it adds complexity, to force something unnaturally. And what you end up with is something that's half intuitive, it's in between the two systems, but it's more complicated than it needs to be. Please, I would much prefer device for RPN calculation, that follows consistent logic, rather than silly imposed rules. 4+7*3 etc. or 4 7 + 3 * = Where the latter is much closer to what we do in our heads and conceptually think about it. reply dlcarrier 12 hours agoparentMy grandma, born in the 30's, isn't any good at computers or technology in general, and has trouble operating even the simplest TV remotes and cordless phones. One thing she can operate is her HP-12C calculator. At the time of it's initial release, someone taught her how to use it, knowing that the RPN interface would be much easier to teach her than infix notation that was equally popular at the time. She never figured out infix notation, at least for more than single operations. Pretty much everything uses infix notation now a days, so that's what everyone learns first, making RPN an additional skill that usually gets passed by. It's interesting to learn that there was a time when a calculators interface was a toss-up, and some people learned RPN because it was the easiest to learn, as opposed to it now being an additional skill that would conflict with earlier experience. reply kazinator 12 hours agorootparentInfix isn't \"nowadays\". It's been with us for hundreds of years. Moreover infix constructs occur in natural languages. You know \"apples and oranges\". Someone born in 1930s would have learned infix arithmetic in school just like somebody born in the 2010s. Cannot work a remote, but can use RPN on an HP calculator? That sounds AI generated. reply SoftTalker 2 hours agorootparentTVs and their remotes mostly have fantastically terrible UX. My MIL can operate her phone and iPad but is constantly confused by the Roku and has to ask for help. reply dlcarrier 10 hours agorootparentprevIt's nowadays that everyone learns infix calculator input methods as children. In the 40's they sure weren't learning any calculator user interfaces. It's stateful and modal interfaces that are an extra step she never figured out, and some TV remotes require different modes for different functions, especially for different equipment. CEC has solved this on modern TVs, but it used to be common to have control the VCR/DVD player by selecting a mode, then control the TV by selecting another mode, and even without the other equipment, she could accidentally get the remote in the wrong mode. She never got the hang of the states that are possible in a calculator with an interface using infix notation, so instead of using parentheses or order of operation, if she didn't have access to an RPN calculator, she'd write down results and enter them back as needed, to only have one operation in progress at a time, in the calculator. reply ziml77 13 hours agoparentprevI don't see how PEMDAS is relevant for a 4 function calculator. The implicit equals when you press an operator means that they always work sequentially rather than jumping around. reply airstrike 13 hours agorootparentI think they're saying it's bullshit because if we used (the superior) RPN across the board, we wouldn't need PEMDAS at all reply ziddoap 13 hours agoparentprev>RPN \"Reverse Polish notation\", I think, for people like me who aren't familiar with the acronym. reply mattkrause 2 hours agorootparentIt really should stand for “Notation: Polish (Reversed).” reply nejsjsjsbsb 11 hours agorootparentprevYou thank! reply layer8 6 hours agorootparent!ęjukęizD reply jwagenet 4 hours agoparentprevAs far as I can tell, PEMDAS has nothing to do with the calculator itself and is just the rules we’ve arrived at for how to read and write equations in a consistent way. reply Jaxan 7 hours agoparentprevYou also don’t need pedmas if you just use parentheses to disambiguate your expressions. That’s what mathematicians would do. reply x3n0ph3n3 12 hours agoparentprev [–] RPN requires implementing a stack on silicon, which adds even more complexity and uncertainty. How deep can the stack go? What happens to your calculation when you exceed its capacity? reply Pinus 12 hours agorootparentClassic HP calculators used a fixed stack of four registers. In practice, you can do quite complicated things before you blow that up. Especially if you understand how the mechanism works, and plan ahead a bit. Besides, infix-style calculators also seem to use a stack to handle operator precedence — if you squint a bit at the manual, you can usually see the shunting-yard algorithm at work behind the curtains. Actually, some really early HP:s (only desktops, though, like the 9100) used a three-level “stack”, which I put in quotes, because it relied on the user to shift the registers up and down as needed. But they cheated a bit and displayed all three registers, so that the user could see what was where. reply roelschroeven 8 hours agorootparent> Besides, infix-style calculators also seem to use a stack to handle operator precedence True, but simple calculators, even ones a bit more advanced than the one in the article (e.g. with a sqrt and/or % key) don't handle operator precedence. I'm not sure it's even correct to call them infix calculators, for that reason. They simply work sequentially; no stack needed. reply kazinator 11 hours agorootparentprev [–] Compared to what? RPN allows for complex, nested expressions for which infix requires nested parentheses. Nesting requires a stack, the same way. reply Consider applying for YC's Spring batch! Applications are open till Feb 11. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "The article explores the complexities in designing user interfaces for basic calculators, focusing on challenges with operations and edge cases. It highlights historical quirks in calculator design, such as the combined \"x÷\" key and non-intuitive behaviors in some apps. The discussion includes the benefits of Reverse Polish Notation (RPN) for simplifying calculations and reducing ambiguity, alongside user experiences and preferences."
    ],
    "points": 192,
    "commentCount": 112,
    "retryCount": 0,
    "time": 1737690379
  },
  {
    "id": 42812641,
    "title": "Build It Yourself",
    "originLink": "https://lucumr.pocoo.org/2025/1/24/build-it-yourself/",
    "originBody": "Armin Ronacher's Thoughts and Writings blog archive tags projects talks about Build It Yourself written on Friday, January 24, 2025 Another day, another rant about dependencies. from me. This time I will ask you that we start and support a vibe shift when it comes to dependencies. You're probably familiar with the concept of “dependency churn.” It's that never-ending treadmill of updates, patches, audits, and transitive dependencies that we as developers love to casually install in the name of productivity. Who doesn't enjoy waiting for yet another cargo upgrade just so you can get that fix for a bug you don't even have? It's a plague in most ecosystems with good packaging solutions. JavaScript and Rust are particularly badly affected by that. A brand new Tokio project drags in 28 crates, a new Rocket project balloons that to 172, and a little template engine like MiniJinja can exist with just a single dependency — while its CLI variant slurps up 142. If that doesn't sound like a big deal, let's consider terminal_size. It is a crate that does exactly what its name suggests: it figures out your terminal dimensions. The underlying APIs it uses have effectively been stable since the earliest days of computing terminals—what, 50 years or so? And yet, for one function, terminal-size manages to introduce three or four additional crates, depending on your operating system. That triggers a whole chain reaction, so you end up compiling thousands of other functions just to figure out if your terminal is 80x25 or 120x40. That crate had 26 releases. My own version of that that I have stuck away in a project from 10 years ago still works without a single update. Because shocker: nothing about figuring out terminal sizes has changed. [1] So why does terminal-size have so many updates if it's so stable? Because it's build on top of platform abstraction libraries that constantly churn, so it needs to update to avoid code duplication and blowing up compile times even more. But “big supply chain” will tell you that you must do it this way. Don't you dare to copy paste that function into your library. Or don't you date to use “unsafe” yourself. You're not qualified enough to write unsafe code, let the platform abstraction architects do that. Otherwise someone will slap you. There are entire companies who are making a living of supplying you with the tools needed to deal with your dependency mess. In the name of security, we're pushed to having dependencies and keeping them up to date, despite most of those dependencies being the primary source of security problems. The goal of code in many ways should be to be written in a way that it does not need updates. It should eventually achieve some level of stability. In the Rust ecosystem stable code is punished. If you have a perfectly working dependency but you have a somewhat inactive bug tracker, RUSTSEC will come by and give you a chunk rating. But there is a simpler path. You write code yourself. Sure, it's more work up front, but once it's written, it's done. No new crates, no waiting for upsteam authors to fix that edge case. If it's broken for you, you fix it yourself. Code that works doesn't necessarily need the maintenance treadmill. Your code has a corner case? Who cares. This is that vibe shift we need in the Rust world: celebrating fewer dependencies rather than more. We're at a point in the most ecosystems where pulling in libraries is not just the default action, it's seen positively: “Look how modular and composable my code is!” Actually, it might just be a symptom of never wanting to type out more than a few lines. Now one will make the argument that it takes so much time to write all of this. It's 2025 and it's faster for me to have ChatGPT or Cursor whip up a dependency free implementation of these common functions, than it is for me to start figuring out a dependency. And it makes sense as for many such small functions the maintenance overhead is tiny and much lower than actually dealing with constant upgrading of dependencies. The code is just a few lines and you also get the benefit of no longer need to compile thousands of lines of other people's code for a single function. But let's face it: corporate code review culture has also has infected Open Source software. Companies are more likely to reward engineers than scold them for pulling in that new “shiny library” that solves the problem they never actually had. That creates problems, so dependabot and friends were born. Today I just dread getting dependabot pull requests but on projects but I have to accept it. I'm part of an ecosystem with my stuff and that ecosystem is all about churn, churn, churn. In companies you can also keep entire internal engineering teams busy with vendoring dependencies, internal audits and upgrading things throughout the company. Fighting this fight is incredibly hard! Every new hire has been trained on the idea that dependencies are great, that code reuse is great. That having old code sitting around is a sign of bad engineering culture. It's also hard to fight this in Open Source. Years ago I wrote sha1-smol which originally was just called sha1. It became the standard crate to calculate SHA1 hashes. Eventually I was pressured to donate that package name to rust-crypto and to depend on the rest of the crypto ecosystem as it was so established. If you want to use the new sha1 crate, you get to enjoy 10 dependencies. But there was just no way around it, because that name in the registry is precious and people also wanted to have trait compatibility. It feels tiring to be the only person in a conversation pushing to keep the churn down and dependencies low. It's time to have a new perspective: we should give kudos to engineers who write a small function themselves instead of hooking in a transitive web of crates. We should be suspicious of big crate graphs. Celebrated are the minimal dependencies, the humble function that just quietly does the job, the code that doesn't need to be touched for years because it was done right once. And sure, it's not black and white. There are the important libraries that solve hard problems. Graphics libraries that abstract over complex drivers, implementations of protocols like HTTP and QUIC. I won't be able to get rid of tokio and I have no desire to. But when you end up using one function, but you compile hundreds, some alarm bell should go off. We need that vibe shift. To celebrate building it yourself when it's appropriate to do so. To give credit to library authors who build low to no-dependency Open Source libraries. For instance minijinja celebrates it in the readme: $ cargo tree minimal v0.1.0 (examples/minimal) └── minijinja v2.6.0 (minijinja) └── serde v1.0.144 And it has a PR to eventually get rid of the last dependency. And sometime this year I will make it my goal to go ahead proudly and trim down all that fat in my projects. [1] Disclaimer: you will need one dependency for UNIX: libc. That's because Rust does not expose the platform's libc constants to you, and they are not standarized. That however is such a common and lightweight dependency that you won't be able to avoid it anyways. This entry was tagged rust and thoughts © Copyright 2025 by Armin Ronacher. Content licensed under the Creative Commons attribution-noncommercial-sharealike License. Contact me via mail, bluesky, x, or github. You can sponsor me on github. More info: imprint. Subscribe to the atom feed. Color scheme: auto, light, dark.",
    "commentLink": "https://news.ycombinator.com/item?id=42812641",
    "commentBody": "Build It Yourself (pocoo.org)190 points by todsacerdoti 6 hours agohidepastfavorite155 comments palata 4 hours agoI like Rust-the-language, but I hate the Rust-dependencies situation. Honestly I like C++ a lot better for that. People will complain that \"it's hard to add a dependency to a C++ project\", but I actually see it as a feature. It forces you to think about whether or not it is worth it. In C++, I control my dependencies. In Rust I get above 100 so quickly I just give up. In terms of security, let's be honest: I have no clue what I am shipping. Also Rust does not have ABI compatibility and no culture of shared libraries (I guess it wouldn't be practical anyway given the number of libraries one needs). But that just destroys the OS package distribution model: when I choose a Linux distribution, I choose to trust those who build it. Say Canonical has a security team that tries to minimize the security issues in the packages that Ubuntu provides. Rust feels a lot more like Python in that sense, where anyone could push anything to PyPi. reply fxtentacle 4 hours agoparentHow is Debian / Ubuntu secure? It's signed by a maintainer. And maintainers are vetted. You trust Debian/Ubuntu to only allow trustworthy people to sign packages. How are Docker / Python / Rust secure? I don't know any of the people who created my docker images, PyPi packages, or Rust crates. Yes. We're basically back to sending around EXE and DLL files in a ZIP. It's just that now we call it a container and proudly start it as root. BTW, I agree with the author of the article: Sometimes you're best off just merging dependency source code. It used to be called \"vendoring\" and was a big thing in Rails / Ruby. The big advantage is that you're not affected by future malicious package takeovers. But you can still merge security patches from upstream, if you choose to do that. reply KronisLV 3 hours agorootparent> How are Docker / Python / Rust secure? I don't know any of the people who created my docker images, PyPi packages, or Rust crates. I know who created the Docker images, because I'm the person who built them! A lot of the time you can build your images either from scratch, or based on the official base images like Alpine or Ubuntu/Debian or some of the RPM base images, not that much different than downloading an ISO for a VM. With a base, you can use apk/apt/dnf to get whatever packages you want if you trust that more, just remember to clean up the package cache so it's not persisted in the layers (arguably wastes space). For most software, it actually isn't as difficult as it might have initially seemed. As an alternative, you can also look for vaguely trustworthy parties that have a variety of prepackaged images available and you can either borrow their Dockerfiles or just trust their images, for example, https://bitnami.com/stacks/containers and https://hub.docker.com/u/bitnami Most likely you have to have trust somewhere in the mix, for example, I'm probably installing the Debian/Ubuntu packaged JDK instead of compiling mine in most cases, just because that's more convenient. Also, rootless containers are pretty cool! People do like Podman and other solutions a lot, you can even do some remapping with Docker if there are issues with how the containers expect to be run https://docs.docker.com/engine/security/userns-remap/ or if you have to use Docker and need something a bit more serious you can try this https://docs.docker.com/engine/security/rootless/ reply woodruffw 4 hours agorootparentprevI don’t follow this reasoning: you might trust this distribution packager to be honest, but this doesn’t stop them from honestly packaging malicious code. It’s unlikely that the typical distribution packager is reviewing more than a minority of the code in the packages they’re accepting, especially between updates. There are significant advantages to the distribution model, including exact provenance at the point of delivery. But I think it’s an error to treat it as uniquely trustworthy: it’s the same code either way. reply WhyNotHugo 4 hours agorootparent\"Trust\" as two meanings in English: You can trust someone as in \"think they're honest and won't betray me\". You can trust someone as in \"think they are competent and won't screw-up\". In this case, we trust distributions packagers in both ways. Not only do we trust that they are non-malicious, we also trust that they won't clumsily package code without even skimming through it. reply woodruffw 3 hours agorootparentThe point was not about competency. I think typical distribution packagers are very competent. \"They won't screw up\" is the wrong way to look at security: humans always screw up. We're essentially fallible. We would all do well to remember that the xz backdoor was not caught by distribution code review: it was caught due to a performance side effect that the next adversary will be more careful to avoid. Software stacks are getting bigger, largely due to engineering pressures that are outside of the control of distributions. It's a basic fact that that means work for the same (or fewer) people, which in turn means less time spent on more complicated packages. Under those conditions, competency is not the deciding factor. This is, separately, why having lots of little packages can be (but isn't necessarily) a good thing: small packages that decompose into well-understood units of work can be reviewed more quickly and comprehensively, and can be assigned capabilities that can be tracked and audited with static analysis. You can't do that as easily (or at all) when every package decides to hand-roll its own subcomponents. reply akerl_ 2 hours agorootparentprevThe average distro package maintainer is not reviewing the underlying code changes in packages to determine if they are malicious, nor is the average disto package maintainer qualified to spot intentionally obfuscated malicious code. The the average distro package maintainers is “person who cares that package repos stay up to date and is willing to commit their time to that work” reply palata 1 hour agorootparentprevIt's not only the packager. Some distros have an actual security team. That does not mean they audit everything, but it's vastly better than getting random code from PyPi. reply tonyhart7 4 hours agorootparentprevYou use in house solution??? any of these guys that want build something minimal dependency free is valid concern, I agree with that but but what if people just don't want that??? it given people jobs and things to do (lol, this is serious) also there are certain things better to use third party library than develop in house like crypto reply liontwist 4 hours agorootparentprev> How is Debian / Ubuntu secure? You’re also forgetting process isolation and roles which provide strong invariants to control what a given package can do. No such guarantees exist for code you load into your process. reply akerl_ 4 hours agorootparentprevThe vetting process for open source maintainers has very little overlap with the vetting process for “is this person trustworthy”. This is true for individual libraries and also for Linux distros. reply jvanderbot 4 hours agoparentprevSo, the feature of \"Here's a set of easily pulled libraries\" is an anti-feature because it makes it easy to pull supporting libraries? I suspect this is actually about developers and not Rust or JS. Developers choose what dependencies to pull. If nobody pulled a dependency it would wither and die. There are a lot of dependencies for most libraries because most developers prefer to use dependencies to build something. But I digress. If we're talking build system, nobody is forcing you to use Crates.io with cargo, they just make it easy to. You can use path-based dependencies just like CMake/VCPkg,Conan, or you can DIY the library. Even with crates.io, nobody is forcing you to not use version pinning if you want to avoid churn, but they just make it easy to get latest. It's easy to build software on existing software in Rust. If you don't like the existing software or the rate it changes don't blame Cargo. Just do it the way you like it. reply a-french-anon 4 hours agorootparent> because it makes it easy to pull supporting libraries? No, because it's used as an excuse for a lack of large(r) standard library. It's the equivalent of \"the bazaar/free market will solve it!\". You basically end up with a R5RS level of fragmentation and cruft outside your pristine small language; something that the Scheme community decided is not fun and prompted the idea of R6RS/R7RS-large. Yeah, it's hard to make a good, large and useful stdlib, but punting it to the outside world isn't a proper long-term solution either. It's really a combination of factors. reply materielle 1 hour agorootparentStandard library omissions aren’t there just because. For almost any functionality missing in the standard library, you could point to 2-3 popular crates that solve the problem in mutually exclusive ways, for different use cases. Higher level languages like Go or Python can create “good enough” standard libraries, that are correct for 99% of users. Rust is really no different than C or C++ in this regard. Sure, C++ has a bigger standard library. But half of it is “oh don’t use that because it has foot guns for this use case, everyone uses this other external library anyways”. The one big exception here is probably async. The language needs a better way for library writers to code against a generic runtime implementation without forcing a specific runtime onto the consumer. reply BoingBoomTschak 1 hour agorootparentYou're just elaborating on my use of the word \"hard\". Yes it is, and C++ is a good example of what not to do. Also, \"good\" is a spectrum, which means something a bit less shiny but standard is worth existing. What R7RS-large is doing by standardizing stuff that was already discussed at length through SRFI seems like a good way. From an external PoV, Clojure seem to be doing okay too. reply WhyNotHugo 4 hours agorootparentprev> If we're talking build system, nobody is forcing you to use Crates.io with cargo, they just make it easy to. Using cargo with distributed dependencies (e.g.: using git repositories) has several missing features, like resolving the latest semver-compatible version, etc. No only is it _easier_ to use cargo with crates.io, it's harder to use with anything else because of missing or incomplete features. > You can use path-based dependencies just like CMake/VCPkg,Conan, or you can DIY the library. Have you tried to do this? Cargo is a \"many things in one\" kind of tool, compiling a Rust library (e.g.: dependency) without it is a pain. If cargo had instead been multiple programs that each on one thing, it might be easier to opt out of it for regular projects. reply jvanderbot 4 hours agorootparentCompared to ... cmake? vckpg? conan? I have never had a good experience with those. However, using mydep.path =in cargo has never been an issue. And I hate to say it, path-based deps are much easier in C++ / C than other \"make the build system do a coherent checkout\" options like those mentioned above. So we're at worst parity for this use case, IMHO and subject to my own subjectivity, of course reply TeMPOraL 46 minutes agorootparentSpeaking of Conan. Something I don't see anyone talking about is, with such systems you have not one but at least two dependencies to audit per any library you want to include. One is the library itself, but the other one is the recipe for the package manager. At least with Conan, the recipes are usually written by third parties that are not affiliated with the dependency they're packaging. Now, Conan recipes are usually full-blown Python scripts that not only decide where to pull the target sources from themselves, they also tend to ship patches, which they apply to the target source code before building. That is, even if package source is specified directly and fixed on specific release, you still can't assume you're actually building the exact source of that release, because the recipe itself modifies the code of the release it pulled. IMHO, trying to do OSS clearance on dependencies pulled via Conan makes very little sense if you don't also treat each Conan recipe as a separate library. But no one does. reply johnnyjeans 4 hours agorootparentprevyes a large component of it is about developers. if developers were perfect beings, we wouldn't need rust in the first place. reply nindalf 3 hours agorootparentRust may not be what you want to write, but it's what you want your coworkers to write. reply chrisco255 4 hours agorootparentprevnext [3 more] [flagged] palata 4 hours agorootparent> They also prefer languages with buffer overflow and use-after-free errors. Bad faith? My first sentence clearly says that I like the language, not the dependency situation. reply johnnyjeans 4 hours agorootparentprevhe literally said he likes rust as a programming language, so no. also it's not \"optional\" when it's the de-facto standard in the language. you lock yourself out of the broader tooling ecosystem. no language server, no libraries (because they all use cargo), etc. oftentimes you run into ergonomic problems with a language's module system because it's been hacked together in service of the enormous combo build/dependency management system rather than vice versa. you're running so far against the grain you might as well not use the language. this kind of passive-aggressive snark whenever someone leverages this very valid criticism is ridiculous reply SkiFire13 3 hours agoparentprev> People will complain that \"it's hard to add a dependency to a C++ project\" The way I see it the issue is that it's hard to add a dependency _in such a way that no people will have issues building your project with it_. This is problematic because even if you manage to make it work on your machine it may not work on some potential user or contributor's. > But that just destroys the OS package distribution model: when I choose a Linux distribution, I choose to trust those who build it. Distros still build Rust packages from sources and vendor crate dependencies in their repos. It's more painful because there are usually more dependencies with more updates, but this has nothing to do with shared libraries. reply palata 1 hour agorootparent> The way I see it the issue is that it's hard to add a dependency _in such a way that no people will have issues building your project with it_. From my point of view, if it's done properly I can just build/install the dependency and use pkgconfig. Whenever I have a problem, it's because it was done wrong. Because many (most?) developers can't be arsed to learn how to do it properly; it's easier to just say that dependency management in C++ sucks. reply a-french-anon 4 hours agoparentprevThat's what I say when I converse about my colleagues: having a package manager from the start that greatly lowers the friction of adding or publishing yet another package deprives a language's ecosystem from something very useful: a good measure of natural selection. Add to that a free-for-all/no curation repository situation like pypi, npm or cargo together with a too small standard library and prepare to suffer. reply XorNot 3 hours agorootparentI wonder how much of this is just the move away from shared libraries. In the .NET space nuget certainly makes it easy to add dependencies, but dependencies do seem to be overall fewer and the primary interesting difference I'd note is that a dependency is in fact it's own DLL file to the extent that it's a feature that you can upgrade and replace them by dropping in a new file or changing configuration. It strikes me that we'd perhaps see far less churn like this if more languages were back to having shared libraries and ABI compatibility as a first class priority. Because then the value of stable ABIs and more limited selections of upgrades would be much higher. reply a-french-anon 3 hours agorootparentThe quest for performance makes macros, monomorphisation/specialization and LTO too attractive for simple dynamic linking to remain the norm, unfortunately. And in a way, I understand, a Stalin/MLton style whole-program optimizing compiler certainly is worth it when you have today's computing power. reply lolinder 3 hours agoparentprevThere's a corollary here to \"build it yourself\", which is \"vet it yourself\". Cargo, npm, and pip all default to a central registry which you're encouraged to trust implicitly, but we've seen time and time again that central registries do not adequately protect against broken or malicious code that causes major security flaws downstream. Each of these ecosystems trains its developers to use hundreds of dependencies—far more than they can personally vet—with the understanding that someone else must surely have done so, even though we've seen over and over again that the staff of these registries can't actually keep up and that even long-running and popular projects can suddenly become insecure. I'd like to see an ecosystem develop that provides a small amount of convenience on top of plain old vendoring. No central repository to create a false sense of security. No overly clever version resolution scheme to hide the consequences of libraries depending on dozens of transitive dependencies. Just a minimal version resolution algorithm and a decentralized registry system—give each developer an index of their own libraries which they maintain and make downstream developers pick and choose which developers they actually trust. Maybe a bit like Maven if Maven Central didn't exist? reply oersted 4 hours agoparentprevThis frankly sounds like a rationalization for an aesthetic preference. It is undeniable that being able to easily build on top of others' hard work is an enormous advantage in any domain. Duplicate work should only happen if you are confident that your requirements are significantly different, and that you can deliver an implementation as good as a team that has likely focused on the problem for much longer and has acquired a lot more know-how. It is true that such an attitude might be justifiable for certain security or performance critical applications, you might need end-to-end control. But even in those cases, the argument for trusting yourself by default over focused and experienced library authors is dubious. Either way, a good dependency manager opens the door for better auditing, analysis and tagging of dependencies, so that such critical requirements can be properly guaranteed, again probably better than you can do yourself. reply Ygg2 3 hours agoparentprev> In C++, I control my dependencies. In Rust I get above 100 so quickly I just Just don't add dependencies, it's that simple. If you have enough time to control C++ dependencies, you can control them in Rust as well. reply lolinder 1 hour agorootparentThat's not an answer when the entire ecosystem is built around the idea of adding lots of dependencies to do stuff. I don't need no dependencies, I'd like to live in a world where I can add two or three. But if the culture is so far gone that those two or three transitively import 20 each, I don't have that as an option—it's all or nothing. reply Ygg2 1 hour agorootparent> That's not an answer when the entire ecosystem is built around the idea of adding lots of dependencies to do stuff. Again. Don't add dependencies. Just don't. Vendor it yourself. Or write it yourself. Absolutely nothing is forcing you to use the dependencies except your own desire to save time. Cargo is giving you the option to A) save your own time B) minimize dependencies. You choose A) and blame Cargo. reply palata 56 minutes agorootparentI don't think it's entirely fair. You're telling me \"don't use any dependency at all\". Vendoring is not a solution, because it does not remove the transitive dependencies. So I have to essentially rewrite it myself. I find many libraries in C++ that don't pull 20 dependencies. It's common for me to have a C++ project that has ~6-8 dependencies. In Rust I have never seen that. Either I have no dependency at all, or I have 100+. I actually had this toy project that I wrote both in C++ and in Rust, for the learning experience. Both did exactly the same thing, with a very similar design. In C++ I had 8 dependencies. In Rust, 186. The thing is, 186 is enough for me to not even read all their names in the lock file. As soon as 25 dependencies magically appear in my lock file, I stop caring and start YOLOing it. reply Joker_vD 5 hours agoprev> it figures out your terminal dimensions. The underlying APIs it uses have effectively been stable since the earliest days of computing terminals—what, 50 years or so? No, they haven't been stable, not really. The TIOCGWINSZ ioctl has never been standardized to my knowledge, and it has many different names on different Unixes and BSDs. The tcgetwinsize() function only got in POSIX in 2024, and this whole thing has really sad history, honestly [0], and that's before we even get to the Windows side of things. [0] https://news.ycombinator.com/item?id=42039401 reply horsawlarway 4 hours agoparentThis was vaguely my take away from the article: It's not that his replacements are simpler because they're better or made by him. They're simpler because they're only handling his use-cases. Sometimes that's fine. Sometimes that's making his software worse for folks who have different use-cases, or are running on systems he doesn't understand or use himself. The real value of a library, even with all those dependencies (and to be clear, I disagree that 3 or 4 dependencies for a library that runs across windows/linux is \"all that many\", esp when his platform specific implementation still uses at least 1), is that it turns out even relatively simple problems have a wealth of complexity to them. The person who's job it is to write that library is going to be more experienced in the subject domain than you (at least in the good cases) and they can deal with it. Most importantly they can deal with your unknown, unknowns. The places you don't even have the experience to know you're missing information. reply liontwist 4 hours agorootparent> They're simpler because they're only handling his use-cases. This is a major part of the thesis of no dependencies. General code is bad code. It’s slow, branchy, complex, filled with mutexes, nan checks, etc. Read “the old new thing”, to see the extreme When you have a concrete goal you can apply assumptions that simplify the problem space. A good example of this was Casey’s work on a fast terminal. At first all the naysaying was “production terminals are really hard because you have to handle fonts and internationalization, accessibility, etc”. Indeed those problems suck, but he used a general windows API to render a concrete representation of the char set on demand, and the the rest was simple. reply horsawlarway 2 hours agorootparent> General code is bad code. For whom? I think most times, as a user of software, I almost always prefer to have something that solves my problem, even if it's got some rough edges or warts. That's what general code is stuff that solves a problem for lots of people. Would I prefer a tool that solves exactly my problem in the best way possible? Yeah, sure. Do I want to pay what that costs in money, time or attention? Usually no. The general purpose tool is plenty good enough to solve the problem now and let me move on. The value I get from solving the problem isn't really tied to how optimally I solve the problem. If I can buy a single hammer that drives all the nails I need today that's a BETTER solution for me than spending 10 hours speccing out the ideal hammer for each nail and each hand that might hold it, much less paying for them all. I'll have already finished if I just pick the general purpose hammer, getting my job done and providing value. So to your terminal example I think you're genuinely arguing for more general code here. There's performance in making a terminal run at 6k fps. It's an art. It's clearly a skill and I can respect it. Sounds like it's an edge case that dude wants, so I'm in favor of trying to make the terminal faster (and more general). But... I also don't give a flying fuck for anything I do. Printing 1gb of text to the terminal is useless to me from a value perspective (it's nearly 1000 full length novels of text, I can't read that much in a year if it was all I did, so going from 5 minutes to 5 seconds is almost meaningless to me). The sum total of the value I see from that change is \"maybe once or twice a year when I cat a long file by mistake, I don't have to hit ctrl-c\". I also genuinely fail to understand how this guy gets meaningful value from printing 1gb of text to a terminal that quickly either... even the fastest of speed readers are still going to be SO MANY orders of magnitude slower to process that, and anything else he might want to do with that text is already plenty fast copying it to a new file? already fast. Searching it? fast. Deleting it? fast. Editing it? fast. So... I won't make any comment on why this case is slow or the discussion around it (I haven't read it, it sounds like it could be faster, and they made a lot of excuses not to solve his specific edge case). All I'll say is your argument sure sounds like adding an edge case that nearly no one has, there-by making the terminal more general. Any terminal I wrote for myself sure as fuck wouldn't be as fast as that because I don't have the rendering experience he has, and my use case doesn't need it at all. reply jvanderbot 4 hours agorootparentprev100%. If OP is willing to maintain a rust crate that takes in no dependencies and can determine terminal size on any platform I choose to build for, then I will gladly use your crate. OTOH, if minimizing dependencies is important for a very specific project, then the extra work of implementing the functionality falls on that project. It will be simpler because it must only support one project. It may not receive critical compatibility updates or security updates also. It does not fall on the community to go and remove dependencies from all their battle tested crates that are in common use. I think anyone and everyone would choose a crate with fewer over more dependencies. So, go make them? reply the_mitsuhiko 4 hours agorootparent> If OP is willing to maintain a rust crate that takes in no dependencies and can determine terminal size on any platform I choose to build for, then I will gladly use your crate. I already mentioned this on twitter but not a lot of people work this way. I don't have to point you farther than my sha1-smol crate. It was originally published under the sha1 name and was the one that the entire ecosystem used. As rust-crypto became more popular there were demands that the name was used for rust-crypto instead. I have given up the name, moved the crate to sha1-smol. It has decent downloads, but it only has 40 dependents vs. >600 for sha1. Data would indicate that people don't really care all that much about it. (Or the sha1 crate is that much better than sha1-smol, but I'm not sure if people actually end up noticing the minor performance improvements in practice) reply andyferris 4 hours agorootparentNot trying to be perverse but why does sha1-smol need any dependencies, let alone 40? Isn’t it a single public pure function from a slice of bytes to a u128? (Implemented with some temporary state and a handful of private functions.) (I am likely being completely naive, but I am well satisfied by Julia’s stdlib function for this, so wondering what makes Rust different since I’ve been doing more and more rust lately and am trying to understand what the difference might be). reply the_mitsuhiko 3 hours agorootparent> Not trying to be perverse but why does sha1-smol need any dependencies, let alone 40? It has zero dependencies, but only 40 crates on the eco system depend on it. Compared to sha1 which has 10 dependencies, but > 660 crates depend on it. reply jvanderbot 4 hours agorootparentprevHa! Hi, I think we're having parallel conversations on there as well. Well, good to make a second loop closure. I won't copy my reply here. reply liontwist 4 hours agorootparentprev> but it only has 40 dependents Isn’t sha1 a several hundred line function? What’s going on? reply the_mitsuhiko 3 hours agorootparentIn case I was unclear it has 0 dependencies, but 40 crates in the ecosystem depend on it. reply liontwist 3 hours agorootparentMy mistake. reply gpm 3 hours agorootparentprevThe rust-crypto sha1 crate has four direct dependencies cfg-if a tiny library making it easier to use different implementations on different platforms. Maintained by the official rust-lang libs team. Also used by the rust std library. No recursive dependencies. cpufeatures a tiny library for detecting CPU features maintained by the rust-crypto people, with a recursive dependency only on libc (which is maintained by rust-lang and used by the standard library). digest a library providing traits abstracting over different hash functions, maintained by the rust-crypto-people. In turn digest depends blockbuffer (some code to minimize boundchecks) and cryptocommon (more traits for abstracting), both maintained by the rust-crypto people. Both in turn depend on typenum, a third party library for compile time math, no dependencies. generic-array, a third library for fixed length arrays with lengths computed by typenum, typenum is its only dependency. version_check, a third party library for checking what features the rustc being used supports (only block-buffer depends on this one). sha1-asm A library with a faster assembly implementation of sha1, maintained by the rust-crypto people. Dependent only on cc (c compiler support), maintained by rust-lang, used in building the rust compiler. cc is itself in turn dependent only on shlex, a third party library for splitting things the same way a shell does. So this tree ends in an actual third party dependency, but one you're already indirectly dependent on by simply using the rust compiler at all. Oh, and in the next release sha1-asm is being removed The crypto common dependencies are being replaced with hybrid-array, which is a replacement for generic-array maintained by the rust-crypto people, and is only dependent on typenum. The version_check dependency is being dropped. So what's going on is 1. This is a faster implementation of sha1, using all the tricks to have different implementations on different platforms. This resulted in 2 third party dependencies, down to 1 in the next release. 2. This is fitting into a trait ecosystem, and is using tricks to abstract over them without degrading performance. This resulted in 2 third party dependencies, down to 1 in the next release. 3. The count of non-third party dependencies is inflated because rust-crypto has split up its code into multiple crates, and the rust-lang team has split its code up into multiple crates. 4. It's not 40, it's 9. Is it worth the extra code? I guess that depends on how many times you are hashing things and how much you care about the performance and environmental impact of extra code. Or if you're doing something where you want to be generic over hash functions. reply Izkata 1 hour agorootparent> 4. It's not 40, it's 9. Just like a bunch of others, you understood their comment backwards. Dependents, not dependencies. 40 other things rely on it. reply gpm 1 hour agorootparentUh, no, I understand the comment I was replying to forwards. I simply decided not to point out the source of their misunderstanding since it had already been pointed out by the time I replied, and this comment is already way too long. reply WhyNotHugo 4 hours agorootparentprevDo you still stand by your decision to give over the `sha1` name? reply the_mitsuhiko 3 hours agorootparentI don't care that much about the name. It was already clear that even ignoring the name, that the rust-crypto ecosystem became the mainstream choice. reply the_mitsuhiko 5 hours agoparentprevIt's not standardized but those calls do not change. The windows calls in particular are guaranteed ABI stable since they are compiled into a lot of binaries. There are definitely issues with ioctl but the changes landing in terminal-size or any of the dependencies that caused all these releases, are entirely unrelated to ioctl/TIOCGWINSZ constants/winsize struct. That code hasn't changed. reply mrweasel 3 hours agoparentprevIn this case the terminal-size crate just calls Rustix tcgetwinsize, which in turn just calls the libc tcgetwinsize. So I suppose you could save yourself a whole bunch of dependencies by just doing the same yourself. The only cost is Windows support. If this particular API has been stable, or at least reasonably defined for 50 or 25 years is a detail, because the dependency doesn't even pretend to deal with that and the function is unlikely to change or be removed in the near future. reply Joker_vD 3 hours agorootparent> If this particular API has been stable Well, it hasn't. The tcgetwinsize() was proposed (under this name) in 2017 and was standardized only in 2024. So it's less than a 10 year old API, which is missing from lots of libc implementations, see e.g. [0]. Before its appearance, you had to mess with doing ioctl's and hoping your libc has exposed the TIOCGWINSZ constant (which glibc by default didn't). [0] https://www.gnu.org/software/gnulib/manual/html_node/tcgetwi... reply mrweasel 3 hours agorootparentI had to check the Rustix implementation again, because that would indicate that that terminal-size wouldn't work on a number of operating systems. However Rustix also uses TIOCGWINSZ in it's tcgetwinsize implementation. reply titzer 4 hours agoparentprevTerminals are a good example of something that seems really simple but is a major PITA because of too many different vendors in the early days, and no industry standard emerged. What is the closest thing? VT100? VT102? I mostly write raw to those, but stuff like terminal size and various other features like raw (non-cooked) mode are crappy and require ioctl's and such. Frankly, it sucks. ...but the libraries suck even more! If you don't want to link against ncurses then may God have mercy on your soul. reply Joker_vD 3 hours agorootparentPrevious summer I've toyed with trying to write an \"async prompt\" a-la Erlang's shell with output scrolling and line-editing (see e.g. [0] for example of what I am talking about), but it is so bloody difficult to do correctly, especially when the input spans several lines and there are some full-width characters on the screen, that I've abandoned it. [0] https://asciinema.org/a/s2vmkOfj6XtJkQDzeM6g2RbPZ reply jumpkick 4 hours agoprevI recently revived a web app I wrote in 2006, my first startup. It was a social media site focused on media sharing. A pretty simple LAMP stack for the time. PHP 5, MySQL 3.2, but it has all of your typical (for the time) social media features. I revived this app because I wanted some hands-on time with new CI/CD tech that I don't get to use at my day job, so I'm working to extremely over-engineer the app's deployment process as a learning project. I could have used Wordpress or some other Hello World app, but this is a lot more fun. I had written nearly all of the PHP from scratch. I wrote libraries for authentication/authorization, templating, form processing etc. I used one PEAR library for sending email. The frontend was vanilla HTML and there was barely any JavaScript to speak of. We used Flash for media playback. In other words, myself and my small team built nearly all of it ourselves. This was just how you did most things in 2006. It only took me about an hour to get the 19-year old app up and running. I had to update the old PHP mysql drivers to mysqli, and update the database schema and some queries to work in MySQL 8 (mostly wrapping now-reserved words with backticks and adjusting column defaults which are now more strict). The only thing that didn't work was the Flash. An hour to revive an app from 2006. Contrast this with my day job, wherein we run scores of Spring Boot apps written in Java 8 that have pages of vulnerabilities from tens of dozens of dependencies, which are not easy to update because updating one library necessitates updating many other libraries, and oh my goodness, the transitive dependencies. It's a nightmare, and because of this we only do the bare minimum of work to update the most critical vulnerabilities. There's no real plan to update everything because it's just too tall of an order. And the funny thing is, if you compare what this PHP app from 2006 did, which had truly, barely any dependencies, to what these Spring Boot apps do, there is not a lot of difference. At the end of the day, it's all CRUD, with a lot more enterprise dressing and tooling around it. reply skydhash 3 hours agoparentGo and the C linux world have sold me on the fat library philosophy. You brought a library to solve a problem in a domain, then add your specific bits. You don't go and bring a dependency for each item in your check list. Yes there may be duplicate effort, but the upgrade path is way easier. reply ok123456 3 hours agoparentprevMost of that new CI/CD tech is standard now precisely because of all the complexity added by maintaining third-party dependencies and constant changes in the runtime environment. This isn't a problem for the most part for an old LAMP application deployed by scp. reply 999900000999 4 hours agoprevI agree 100% . Even though NodeJS is largely responsible for my career, NPM has given me more trauma than my messed up childhood. Imagine you're a new programmer, you're working on a brand new app to show to all your friends. But you want to add a new dependency, it doesn't like all the other dependencies, cool you say I'll just update them. Next thing you know absolutely nothing works, Babel is screaming at you. No worries, you'll figure something out. Next thing you know you're staring at open git issues where basic things literally don't work. Expo for example has an open issue where a default new react native project just won't build for Android . It's like no one cares half the time, and in that case the solution isn't even in the node ecosystem, it's somewhere in the Android ecosystem. It's duct tape all the way down. But this can also inspire confidence if a billion dollar project can ship non-functional templates, then why do I have imposter syndrome when my side projects don't work half the time! reply abound 5 hours agoprevThis was something that surprised me about the Rust ecosystem, coming from Go. Even a mature Go project (e.g. some business' production web backend) may only have 10-20 dependencies including the transitive ones. As noted in this post, even a small Rust project will likely have many more than that, and it's virtually guaranteed if you're doing async stuff. No idea how much of it is cultural versus based on the language features, e.g. in Go interfaces are implicitly satisfied, no need to import anything to say you implement it. reply Cyph0n 5 hours agoparentFor Rust and Go in particular, the difference is in the standard library. The Rust stdlib is (intentionally) small. reply chris_overseas 4 hours agorootparentAgreed, and the small stdlib is one of the main reasons for this problem. I understand the reasoning why it's small, but I wish more people would acknowledge the (IMHO at least) rather large downside this brings, rather than just painting it as a strictly positive thing. The pain of dealing with a huge tree of dependencies, all of different qualities and moving at different trajectories, is very real. I've spent a large part of the last couple of days fighting exactly this in a Rust codebase, which is hugely frustrating. reply palata 4 hours agorootparentWhat is the reason to keep it small? Genuinely interested, I actually don't understand. Embedded systems maybe? reply burntsushi 4 hours agorootparent(I've been on libs-api, and libs before that, for 10 years now.) API Stability. When the standard library APIs were initially designed, \"the Python standard library is where packages go to die\" was very much on our minds. We specifically saw ourselves as enabled to have a small standard library because of tooling like Cargo. There are no plans for Rust 2.0. So any change we merge into std is, effectively, something we have to live with for approximately forever. (With some pedantic exceptions over edition boundaries that don't change my overall point.) Nuance is nearly dead on the Internet, but I'll say that I think designing robust and lasting APIs is a lot harder in Rust than it is in Go. Rust has a lot more expressiveness (which I do not cite as an unmitigated good), and the culture is more heavily focused on zero-overhead abstractions (which I similarly do not cite as an unmitigated good). That means the \"right\" API can be very difficult to find without evolution via breaking changes. But the standard library cannot, generally speaking, make breaking changes. Crates can. I would suggest not reading the OP as a black-and-white position. But rather, a plea to change how we balance the pros and cons of dependencies in the Rust ecosystem. reply WhyNotHugo 3 hours agorootparentI can understand not wanting to add SMTP or CGI to the stdlib. But a lot of common POSIX functionality (which is sometimes a single syscall away) is missing too. reply burntsushi 3 hours agorootparentA lot of common POSIX functionality is not missing though. I was able to write ripgrep, for example, by almost entirely sticking to the standard library. (I think the only place I reach out to `libc` directly is to get the hostname of the current system for rendering hyperlinks in your terminal.) We also came at the standard library with a cross platform mentality that included non-POSIX platforms like Windows. You want to be careful not to design APIs that are too specific to POSIX. So it falls under the same reasoning I explained above: everything that goes into std is treated as if it will be there forever. So when we add things to std, we absolutely consider whether the risk of us getting the API wrong is outweighed by the benefit of the API being in std in the first place. And we absolutely factor \"the ease of using crates via Cargo\" into this calculus. reply WhyNotHugo 3 hours agorootparentI peeked at the code for gethostname in ripgrep, and it's nice and straightforward. Much like op said here; we have a culture of \"don't write unsafe code under any circumstance\", and we then pull in a dependency tree for a single function that's relatively safe to contain. It solves the problem quickly, but at a higher price. BTW, thanks for ripgrep. I don't actually use it, but I've read through different portions of the code over recent months and it's some very clean and easy to understand code. Definitely a good influence. reply ziml77 3 hours agorootparentI don't think you should treat unsafe code as that level of toxic. It's necessary when interfacing with with system APIs. The important part is that you try to have safe wrappers around the unsafe calls and that you document why the way you're using them is safe. reply palata 1 hour agorootparentprevI've been using ripgrep for years! Thanks a lot for that! reply saghm 3 hours agorootparentprevIn addition to the points burntsushi gave in the sibling comment, I'd also add that keeping the standard library small and putting other well-scoped stuff like tegex, rand, etc. in dependencies also can reduce the burden of releases a lot. If some a bug gets found in a library that's not std, a new release can get pushed out pretty quickly. If a bug gets found in std, an entire new toolchain version needs to be published. That's not to say that this wouldn't be done for critical bugs, but when Rust already has releases on a six-week cadence, it's not crazy to try to reduce the need for additional releases on top of that. This probably isn't as important as the stability concerns, but I think it still helps tilt the argument in favor of a small std at least a little. reply afiori 3 hours agorootparentprevThe three main reasons I see being given are: backward compatbility: a big std lib increases the risk of incompatible changes and the cost of long term support pushing developers to be mindful of minimal systems: a sort of unrelated example is how a lot of node library use the 'fs' module just because it is there creating a huge pain point for browser bundling. If the stdlib did not have a fs module this would happen a lot less a desire to let the community work it out and decide the best API/implementations before blessing a specific library as Standard. In my opinion a dynamic set of curated library with significantly shorted backward compatibility guarantees is the best of both worlds. reply SkiFire13 3 hours agorootparentOther reasons also include: less burden on the stdlib maintainers (which are already overworked!) faster iteration on those libraries, since you don't need to wait a new release of the compiler to get updates for those libraries (which would take at least 12-16 weeks depending on when the PR is merged) reply michaelt 3 hours agorootparentprevOne risk with a bigger standard library is that you'll do an imperfect job of it, then you'll be stuck maintaining it forever for compatibility reasons. For example, Java developers can choose to represent time with Unix milliseconds, java.util.Date, java.util.Calendar, Joda-Time or java.time.Instant reply 7bit 4 hours agorootparentprevAFAIK: Rust compiles to machine code. Even if the stdlib would be 600 mb, If you have 3 lines of Code your programm would be microscopically small. reply chikere232 4 hours agorootparentprevIt might be a bad choice on rust's part. IMO they should over time fold whatever ends up being the de-facto choice for things into the standard library. Otherwise this will forever be a barrier to entry, and a constant churn as ever new fashionable libraries to do the same basic thing pops up. You don't need a dozen regex libraries, you just need one that's stable, widely used and likely to remain so. reply burntsushi 4 hours agorootparent> You don't need a dozen regex libraries, you just need one that's stable, widely used and likely to remain so. That is the case today. Virtually everyone uses `regex`. There are others, like `fancy-regex`. But those would still exist even if `regex` was in std. But then actually it would suck, because then `fancy-regex` can't share dependencies with `regex`, which it does today. And because of that, you get a much smoother migration experience where you know that if your regexes are valid with `regex`, they'll work the same way in `fancy-regex`. A better example might be datetime handling, of which there are now 3 general purpose libraries one can reasonably choose. But it would have been an unmitigated disaster if we (I am on libs-api) had just added the first datetime library to std that arose in the ecosystem. reply palata 4 hours agorootparentprevAgreed. > and a constant churn as ever new fashionable libraries Isn't that the situation in Javascript? I don't work in Javascript but to me it feels like people migrate to a new cool framework every 2 months. reply LamaOfRuin 4 hours agorootparentLess so than it once was, but more so than other languages/ecosystems. To be fair though, I'd argue the environment Javascript lives in also changes faster than any other. reply sesm 4 hours agorootparentprevI would expect crates like `stdlib-terminal` and `stdlib-web-api` in that case. Honestly, something feels off with Rust trying to advertise itself for embedded: no stdlib and encourage stack allocation, but then married to Clang (which doesn't have a good embedded target support) and have panic in the language. Building a C++ replacement for a browser engine rewrite and building a C replacement for embedded have different and often conflicting design constraints. It seems like Rust is a C++ replacement with extra unnecessary constraints of a C replacement. reply palata 4 hours agorootparentI often wonder about this: obviously Rust is fashionable, and many people push to use it everywhere. But in a ton of situations, there are modern memory-safe languages (Go, Swift, Kotlin, Scala, Java, ...) that are better suited. To me Rust is good when you need the performance (e.g. computer vision) and when you don't want a garbage collector (e.g. embedded). So really, a replacement for C/C++. Even though it takes time because C/C++ have a ton of libraries that may not have been ported to Rust (yet). Anyway, I guess my point is that Rust should focus on the problem it solves. Sometimes I feel like people try to make it sound like a competitor to those other memory-safe languages and... even though I like Rust as a language, it's much easier to write Go, Swift or Kotlin than Rust (IMHO). reply jitl 5 hours agoparentprevOne big reason is because Go has a very nice complete standard library, and Rust really does not. Things you can find in go’s built in to the language or in standard libraries that need a dependency in Rust: green threads channels regular expressions http client http server time command line flags a logger read and write animated GIFs I don’t love the Go language, but it’s the leader for tooling and standard library, definitely the best I’ve used. reply drrotmos 4 hours agorootparentWhich may or may not be fine in a Go binary that runs on a modern desktop CPU, but what if your code is supposed to run on say an ESP32-C3 with a whopping 160 MHz RISC-V core, 400 KB of RAM and maybe 2 MB of XIP flash storage? You could of course argue that that's why no-std exists in Rust, or that your compiler might optimize out the animated GIF routines, but personally, I'd argue that in this context, it is bloat, that while it could occasionally be useful it could just as easily be a third party library. reply jitl 4 hours agorootparentIt’s the same as in C, Rust, or any other programming language I’ve ever used. If you don’t use a library, it doesn’t end up linked in your executable. Don’t want to animate GIFs on your microcontroller, then you don’t write `import “image/gif”` in your source file. For a microcontroller sized runtime, there’s https://tinygo.org/ I think the lack of strong standard library actually leads to more bloat in your program in the long run. Bloat is needing to deal with an ecosystem that has 4 competing packages for time, ending up with all 4 installed because other libraries you need didn’t agree, and then you need ancillary compatibility packages for converting between the different time packages. reply IshKebab 4 hours agorootparent> It’s the same as in C, Rust, or any other programming language I’ve ever used. If you don’t use a library, it doesn’t end up linked in your executable. I don't think that's true. If the standard library is pre-compiled, and it doesn't use `-ffunction-sections` etc. then I'm pretty sure you'll just get the whole thing. There is experimental support for building Rust's standard library from source, but by default it is pre-compiled. reply XorNot 3 hours agorootparentIt's hardly the case that a good reason to not have a more complete standard library on the basis of having to do a tiny bit more work in a more special case to get binary sizes down. reply IshKebab 2 hours agorootparentNo I totally agree. IMO Rust should have a more complete standard library. reply chikere232 4 hours agorootparentprevDoes anyone use the full standard library for embedded targets? I've not seen it done in C, java has a special embedded edition, python has micro-python, rust seems to usually use no-std, but I might be wrong there. It seems like a bad reason to constrain the regular standard library reply IshKebab 3 hours agorootparentI have in the past, but most people don't. E.g. for esp32 see https://docs.esp-rs.org/book/overview/using-the-standard-lib... reply 7bit 4 hours agorootparentprevI hate that. I don't want dependencies for serialization or logging. But you do and now you have to choose which of the dozen logging crates you need. As a beginner this is horrible, because everybody knows serde, but I have to learn that serde is the defacto, and that is not easy because when coming from other languages, it sounds like the second best choice. And that is with most rust crates. reply burntsushi 3 hours agorootparentThis is why things like https://blessed.rs exist. Although since they're unofficial, their discoverability is also likely a problem. reply petecorreia 5 hours agoparentprevGo's vast standard library helps a lot with keeping dependency numbers down reply lionkor 4 hours agorootparentExactly this. You want logging in Rust? You will need at least `log` and another logger crate, for example `env_logger`, maybe the `dotenvy` crate to read `.env` files automatically, you already have 3 direct dependencies + all the transitive ones. In Go: https://pkg.go.dev/log reply cpursley 5 hours agoprev> It's 2025 and it's faster for me to have ChatGPT or Cursor whip up a dependency free implementation of these common functions I sort of stumbled upon this myself and am coming around to this viewpoint. Especially after dependency hell of a big react app. And there's also the saas/3rd party services dependencies to consider. Many of them are common patterns and already solved problems that LLMs can clone quickly. reply macNchz 4 hours agoparentI’ve definitely become much more likely to start with small internal utility functions implemented by AI before adding a library than I would have been in the past—it’s quite effective for contained problems, I can have the AI write much more complete/robust implementations than I would myself, and if I do eventually decide to add a library I have a natural encapsulation: I can change the implementation of my own functions to use the library without necessarily having to touch everywhere it’s being used. Makes it easy to test a couple of different libraries as well, when the time comes. reply nostradumbasp 5 hours agoprevLove the thesis statement. There is a lot of hidden cost in allowing abstractions from other libraries to be exposed over your own. I'm not here to say that should never be done but the future costs really need to be balanced at the decision point. If the encapsulating package churns over its design, or changes it's goals it creates instability. Functionality is lost or broken apart when previously it was previously the simplest form of guarantee in software engineering in existence. It also deters niche but expert owners who aren't career OSS contributors from taking part in an ecosystem.\"I made a clean way to do X!\" being followed up by weeks of discussion, negotiation, politics so that \"X fits under Y because maybe people like Z\" is inefficient and wasteful of everyones time. If there's one thing I've learned in my life it's that the simplest things survive the longest. Miniliths, and monoliths should be celebrated way more often. Rust isn't alone in this by the way, I've seen this across languages. I've often seen OSS communities, drive hard for atomistic size packages, and I often wonder if it's mostly for flag planting and ownership transfer purposes than it is to benefit the community that actually uses these things. reply bluGill 5 hours agoprevWorse, sometimes the upstream is complex enough that you don't want to do it yourself then the upstream quits maintaining their project. I have in my company some open source projects that we still use that haven't been touched upstream since 2012, but either there is no replacement or the replacement is so different it isn't worth the effort to upgrade. Fortunately none of these are projects where I worry about security issues, I'm just annoyed by the lack of support, but if they faced the network I'd be concerned (and we do have security people who would force a change) reply adrianN 5 hours agoparentSoftware that hasn’t been touched in ten years and still does the job is about as ideal as a dependency can be. reply palata 5 hours agorootparentI tend to agree, but it may have downsides to: it may do the job and have serious security issues. If you don't know what it does, no reason to know about the security issues. reply WhyNotHugo 3 hours agorootparentprevIn theory yes. Although a ten year old dependency is written in Python, it's likely not going to work any more due to changes in the build system, stdlib, etc. reply bluGill 3 hours agorootparentprevBut does it? If the software is a spell checker for a language I don't know I will have no idea if it is any good. reply adrianN 25 minutes agorootparentThe same can be true for the dependency that releases weekly updates. reply bluGill 14 minutes agorootparentBut the dependency will also get better of time. (One hopes) reply qup 4 hours agorootparentprevMaybe it's even actively maintained! reply xnorswap 4 hours agoparentprevOr they bait-and-switch freedom. So they start off with an open source Free solution, and then later switch to a paid for model, and abandon the Free version. This is particularly painful when it's a part of your system that's key to security. You're left between wondering if you should just pay the ransom or switching to a different solution entirely, or gamble and leaving it on an old unpatched version. ( Looking at you, IdentityServer ) Either way you regret ever going with them. reply hitchstory 4 hours agoprevThe other extreme of this is: * Bad abstractions which just stick around forever. There are some examples of this in UNIX which would never be invented in the way they are today but nonetheless aren't going anywhere (e.g. signal handling). This isn't good. * Invent all of your own wheels. This isn't good either. There's a balance that needs to be struck between all of these 3 extremes. reply chikere232 4 hours agoparentI know it's just an example, but if you're on linux there's signalfd() which makes signals into IO so you can handle it in an epoll()-loop or whatever way you like doing IO We can't remove the old way of course, as that would break things, but that doesn't stop improvements reply IshKebab 3 hours agorootparentSometimes removing the old way is the improvement though. E.g. adding an alternative to symlinks doesn't help if symlinks are still allowed. reply zelphirkalt 3 hours agoprevYou need capable engineering for building it yourself. If you only got engineers, who only ever reached for libraries, in ecosystems like NPM or PyPI, you will find them hard-pressed to develop solutions for many things themselves, especially so, if they are supposed to be solutions, that stand the test of time, and have the flexibility they need. It takes a lot of practice to \"avoid programming yourself into a corner\". Another thing I noticed is, that one can often easily do better than existing libraries. In one project I implemented a parser for a markdown variant, that has some metadata at the top of the file. Of course I wrote a little grammar, not even a screen of code, and just like that, I had a parser. But I did not expect the badness of the frontend library parsing the same file. That one broke, when you had hyphens in metadata identifiers. At first I was confused, why it could not deal with that. Then it turned out, that it directly used the metadata identifiers as object member names ... Instead of using a simple JSON object, they had knowingly or unknowingly chosen to artificially limit the choice of names and to break things, when there are hyphens like in \"something-something\". In the end my parser was abandoned, people arguing, that they would have to \"maintain\" it. Well, it just worked and could easily be adapted for grammar changes. There was nothing difficult to understand about it either, if you had just a little knowledge about parsers. Sounds incredible, but apparently no one except me on the team had written a parser by using a parser generator library before. And like that, there are many other examples. reply TZubiri 4 hours agoprevA metric that I would like to focus in is dependency depth. We had this with the OSI model way back, but at this point it seems that anything beyond layer 7 just gets bucketed into 8+. We need to know if a dependency is level 1 or level 2 or level 3 or 45. And we need to know what the deepest dependency on our project is. I might be naive, but I think we should strive to reduce the depth of the dependency graph and maybe aim for like 4 or 5 layers deep for a web app, tops. reply themk 4 hours agoparentI've often thought the same. I would love a depedency manager that not only surfaced this information, but required you to declare upfront what level your library is. I think it would reign in the bloat. reply semanser 4 hours agoprevI'm actually working on a linter for dependencies that checks all your dependencies on 15+ rules. https://github.com/DepshubHQ/depshub It's true that dependency-free software is very rare these days. The most obvious reason is that people don't want to \"reinvent the wheel\" when doing something. While this is a 100% valid reason, sometimes people simply forget what they are building and for whom. Extensive usage of dependencies is just one of the forms of overengineering. Some engineering teams even do their planning and features because of the new shiny thing. The problem of dependencies is massive these days, and most companies are focusing on producing more and more code instead of helping people manage what they already have. reply gwbas1c 4 hours agoprev> But when you end up using one function, but you compile hundreds, some alarm bell should go off. About a year ago I ran a project to update 3rd party dependencies. One of the dependencies was a rich math library, full of all kinds of mathematical functions. I did a little bit of digging, and we were only using one single method, to find the median of a list. I pointed the engineer to the Wikipedia page and told him to eliminate the dependency and write a single method to perform the mathematical operation. But, IMO, the real issue isn't using 3rd party dependencies: It's that we need a concept of pulling in a narrow slice of a library. If I just need a small part of a giant library, why do I have to pull in the whole thing? I think I've heard someone propose \"microframeworks\" as a way to do this. reply cluckindan 4 hours agoparentWow. For those who don’t know, here’s a pseudocode implementation: Median(list) { let len = length(list) if len % 2 == 0 { let x = floor(len/2) return (list[x] + list[x+1]) / 2 } return list[len/2] } Note: assumes the list is already sorted. Managed to resist calling is_odd there! reply gpm 2 hours agorootparentIn most cases I'd probably use nearly this. I note that it contains a bug due to integer overflow if naively translated to most languages. But if I have a big enough list that I care about space usage (I don't want to make a copy that I sort and then throw away), or speed (I care about O(n) vs O(n log(n))) I'd be looking for a library before implementing my own. Here are the relevant algorithms if you really want to implement your own fast median code though: https://cs.stackexchange.com/questions/1914/find-median-of-u... reply cluckindan 2 hours agorootparentI use that math textbook algorithm in production to produce a median from a list which has a bounded size and is already sorted by the db, though that bound could technically grow to INT_MAX if someone managed to make that many requests in five minutes. Not very likely. :-) reply gpm 1 hour agorootparent> and is already sorted by the db, Right, if it's already sorted just taking the midpoint is the obviously correct algorithm (and O(1) time/space). It's only in the unsorted cases where with giant lists you should start thinking about alternatives. If I'm working with gigabytes of photon counts (each element representing the number photons detected in a time interval) I don't want to sort my gigabyte long list before getting the median sorting would destroy the very important structure of the data so I'd just have to throw away the copy afterwards. This is referencing some code I worked on a long time ago. I'm not sure I had to calculate a median specifically, but similar enough statistics. It's a simple function, but not a one size fits all algorithm. reply ebiester 4 hours agoparentprevThat's how we get things like leftpad in the JS ecosystem. On one side, I think if we had a good system of trust, that's not a problem. And part of me likes the idea of something like Shadcn you like a component? Copy it into your library. However, if there ends up being a vulnerability, you have no idea if you are affected. For some code, that's not a problem. For other code, we truly depend on having as many eyes as possible on it. reply saghm 3 hours agoparentprevIn Rust, a library can define \"features\" that can be conditionally enabled or disabled when depending on it, which give a a built-in way to customize how much of the library is actually included. Tokio is a great example of this; people might be surprised to learn that the total number of direct dependencies that are required by tokio is only two[1]; everything else is optional! Unfortunately, it doesn't seem like people are super diligent about looking into the default feature set that's used by their dependencies and proactively trimming that down. It doesn't help that the syntax for pulling in extra features is less verbose than removing optional but default ones (which requires both specifying \"no-default-features\" and then manually adding every one of the default features that you do still want back to the list of ones you pull in), and it _really_ doesn't help that the only way for libraries to expose the ability to prune unneeded features from their own dependencies to the users who inherit them is by manually making their own feature that maps to the features of every single one of their own dependencies. For example, if you're writing a library with five dependencies, and every one of them has one required dependency and four optional ones, giving the users of your library full control over what transitive features they pull in would mean making 20 features in your own library mapping to each of those transitive features, and that's not even counting the ones that you'd want to make for your own code in order to be a good citizen and not force downstream users to include all of your own code. More and more I'm coming to the opinion that the ergonomics around features being so much worse for trying to cut down on the bloat is actually the catalyst for a lot of the issues around compile times in Rust. It doesn't seem to be super widely discussed when things like this come up though, so maybe it's time that I try to write a blog post or something with my strong feelings on this so at least I'll have something to point to assuming the status quo continues indefinitely. [1]: https://github.com/tokio-rs/tokio/blob/ee19b0ed7371b069112b9... reply dartos 4 hours agoprevThe issue is, imo, rust is a big pain to write. It’s really nice when someone else has created all the abstractions for you. It’s fairly easy and fun to discover features of crates thanks to the type system. But to actually build out those low level abstractions yourself is cumbersome and not very fun. It’s easy to type system yourself in a corner. You even lose the whole big selling point of rust when you use `unsafe`. There was an article a while ago comparing zig and rust and it had a good quote that went something like “Rust is for programming in the large, and zig is for programming in the small” And I think what you’re looking for is programming in the small. Small, focused, programs with minimal dependencies. reply palata 4 hours agoparentIMHO, Rust is a replacement for C/C++, which are not so easy to write correctly. Rust should not be a replacement for e.g. Go, Swift or Java, which are a lot easier to write and have memory safety. I wonder who actually needs Rust (as opposed to using e.g. Go) and finds it a lot harder to write than C/C++? reply dartos 4 hours agorootparentI’d say rust is a replacement for C++. Powerful, but complex, language features. C is drop dead simple. reply bmn__ 2 hours agoprev2025: Mitsuhiko discovers ::Tiny modules :) https://metacpan.org/search?size=500&q=%3A%3ATiny The article shows it's a cultural problem, not a technical. The normal case should be that implementations come in all sizes of complexity on the market, and the end user can choose freely; and I consider this a healthy state of affairs. Rust and JS are a monoculture that has fallen prey to some mind virus where only big supply chain is valued, and so the market becomes distorted. Tell me whether I'm wrong in my assessment. reply teddyh 4 hours agoprev“When you’re working on a really, really good team with great programmers, everybody else’s code, frankly, is bug-infested garbage, and nobody else knows how to ship on time.” — Joel Spolsky, In Defense of Not-Invented-Here Syndrome:reply andrewaylett 3 hours agoprevI have a little JS library that has an analogue within cargo which really helps with this: https://www.npmjs.com/package/downgrade-build It means I can keep my dependency spec broad, because I actually test my releases with minimum supported versions. Cargo has (or maybe had I can't find it any more) a similar feature built in. Renovate will even broaden (rather than replace) dependency specifications, if asked, so if my package works with both the old and the new version then I can make a patch release with the wider dependency spec and not break anything. (On which note: I can really highly recommend using Mend Renovate to keep dependencies up to date: https://www.mend.io/renovate/ ) reply Voultapher 5 hours agoprevI sympathize with the author, it's a tricky balance to strike and writing it yourself because how hard could it be can be a trap, but at the same time babelian towers of abstractions can and often do make for a great waste of mental resources. One of the reasons I wrote https://crates.io/crates/self_cell/ was to avoid the dependency trees pulled in by proc macros like ouroboros. reply the_mitsuhiko 4 hours agoparentIt is really hard! And thank you for self_cell. It's one of the crates I'm happily pulling into projects because of having such a great balance between size and utility and also making it a goal to not have dependencies. reply dazzawazza 4 hours agoprevIt's important to remember that the most powerful form of code reuse is copy and paste. reply jrmg 4 hours agoprevCompanies are more likely to reward engineers than scold them for pulling in that new “shiny library” that solves the problem they never actually had. Perhaps I’m sheltered: is this really true in people’s experience? reply swiftcoder 4 hours agoparentDepends on the company. Amazon had a pretty bad case of NIH, Meta tended to embrace open-source (and as often as not, transitioned to embrace-extend-extinguish if they liked it enough) reply montroser 5 hours agoprevAs the author says, it's a balance. There are many, many problems we deal with that are relatively self-contained, whose solution is going to be stable over long swaths of time. For those types, I hereby grant everyone permission to craft/extract/copy the smallest most straightforward implementation directly into your codebase, and enjoy a life free of deprecation warnings and bogus security alerts and endless dependency churn. If you want some guidelines these sorts of cases should be at most one or two hundred lines of code, be free of any dependencies, and have a small api surface area. reply bluedino 4 hours agoprevIt's all about straddling the line of 'do you want to know how it works', and 'do you want to get your task done'. The second developer gets all the glory. reply palata 4 hours agoparenta.k.a. quality vs productivity. And as you said, the second gets all the glory. reply chikere232 4 hours agorootparentReinventing wheels does not always get you better wheels... Often, with the type of developer who insists on reinventing wheels because it's \"easy\", you get a pretty bad wheel, as it's the first such wheel the developer has built and their estimation of \"easy\" was built on a foundation of ignorance. reply krapp 3 hours agorootparentTo be needlessly pedantic, reinventing wheels never gets you better wheels. The entire point of the idiom is that the wheel is already a perfect machine and any effort made trying to improve upon it is wasted. At best, you just expend time and effort to get the wheel again. Which is why it doesn't belong in software there is no software version of the wheel. reply palata 1 hour agorootparentYeah. Many times in software, I feel like I'm told: \"Creating your own skateboard is reinventing the wheel, you don't want to do that. Instead use this train because it was written by Google and you won't do better\". Sure, but I need a skateboard, not a train. reply beej71 4 hours agoprevRemember leftpad! :) \"Every dependency is an asset. Every dependency is a liability.\" It's a balance. reply medhir 3 hours agoprevAgreed with the sentiment of the author, but are teams really arguing for pulling in dependencies in the name of security? I’ve seen the exact opposite — every dependency introduces a new opportunity for vulnerabilities, and to be judicious in pulling in new ones due to the potential security risk. reply otoolep 3 hours agoprevRelated, but not exactly the same: https://blog.gopheracademy.com/advent-2014/case-against-3pl/ reply sz4kerto 3 hours agoprevI'll sound like a grumpy old man, but: that's one of the big advantages of using Java. The ecosystem is _super stable_ if you go with the usual stack (Spring Boot, etc.), you generally get support for all the modern cloud stuff, and it breaks very rarely. reply palata 4 hours agoprevThere was the \"not-invented-here\" syndrome, I think that Rust (cargo), Python (pypi) and Javascript (npm) push to the other extreme. If you ship a library, you are responsible for its security. If you dynamically link to a library you don't ship, then it's not your problem anymore (someone else is shipping it). I think we tend to forget that: shared libraries have advantages in terms of security. And whether you ship the dependency or not, you should be ready to replace it or maintain it yourself. Too many projects depend on a project they don't understand, and get screwed down the line because it becomes unmaintained. Many times it's actually better to write the code you need than to depend on a third-party you don't understand. reply swiftcoder 4 hours agoparent> If you ship a library, you are responsible for its security. If you dynamically link to a library you don't ship, then it's not your problem anymore In what universe is that true? In this universe, you are now on the hook for the security of both libraries, and the extra one you don't actually have any influence over. reply palata 4 hours agorootparentWell to be fair, in practice in this universe, nobody gives a damn about security. I meant it more in a normative way. Nobody cares much about the security of their own code, and even less for the security of their dependencies. At least if you link dynamically, you give an opportunity to your distribution to have someone care about it. reply melodyogonna 2 hours agoprevWorking with Go moved my mindset from use libraries for everything I had with JS/TS to writing things myself helped by stdlib reply jitl 4 hours agoprevAt least in the JavaScript/NPM ecosystem, “if you want it done right, you have to do it yourself”. Often the most popular package for X is actually low quality and/or obviously incorrect, especially for small-to-medium complexity topics. There may be better alternatives but they’re hard to find due to low usage numbers or poor SEO, so I usually need to build stuff myself out of necessity. For example, the most popular sql`…` tagged template literal package available on NPM isn’t type safe, doesn’t understand differences in SQL dialects, can’t escape queries for debugging or manual execution, doesn’t have console.log formatter, lacks utilities like .join or binding helpers, etc. it’s an anemic package yet everyone uses it. Maybe there are better ones that are specific to a database like Postgres, but it’s a simple enough thing to build, plus if you build it yourself in the monorepo it’s super easy to improve things as they come up. So, I stated with a little 300 line one for SQLite that’s grown over the years to have a nice Postgres dialect as well, an ecosystem of helpers and lint rules, and it seems far ahead of whatever in NPM. Another area is ESlint rules. Often there’s an open source rule that does like 80% of the job, but the maintainers seem hostile or dislike typescript or something. So much easier to copy the rule into our repo and improve it there rather than trying to contribute back. Or, it’s the usual quality issue original rule is doing some crazy slow shit and it’s much easier to write from scratch a simpler more correct version. reply infocollector 5 hours agoprevI believe there's a balance to be struck between writing your own code and relying on dependencies. If Armin (the author of this post) is the one writing the code, I would highly recommend using that dependency! reply hyperpape 2 hours agoprevI'm so tired of rants like these. Both the pro and anti-dependency versions. It's just impossible to express an opinion on the issue without looking at specific applications. What I want to see is someone actually break down a real application and its dependencies and argue through what's included and what's omitted. At least there's one example. I just think it's wrong (and I'm not the only one...https://news.ycombinator.com/item?id=42813003). The terminalsize crate has special behavior for illumos. The average developer should never have to think about illumos. (Note in case Bryan Cantrill walks by: no slight towards illumos or you--you seem great, loved the Oracle rant. Illumos seems cool. It's just that most developers should not think about it). reply baggy_trough 4 hours agoprevFunctions that fit on a page should usually be published as sample code rather than gems / crates / whatever. reply wiz21c 4 hours agoprev\"You write code yourself.\" I don't see myself writing wgpu or tokio. And wgpu is a hefty price to pay in dependencies. So yeah, dep's are a plague, but hey, we are writing super complex stuff nowadays. And I don't care if my project needs 464 (yeah, you read it well) dependencies. I do my share by making sure I don't have 2 versions of the same lib and that's about it. reply mberning 5 hours agoprevI agree. Here is a similar post from another amazing developer. https://www.mikeperham.com/2016/02/09/kill-your-dependencies... reply thrance 4 hours agoprevI used to think the same way seeing cargo pull so many LOCs, but let's be real, I am not going to write my HTTP server by hand, nor my postgresql driver. And those are the ones that pull a lot of dependencies. As for smaller QOL dependencies, I might as well use them directly as I need them, instead of spending 3h building something similar but half as good and robust, just to decrement my dependencies counter by 1. I've had very few issues with Rust where an old project can't be built anymore because of rotting dependencies, when it happens almost every time with JS or Python. This is my largest gripe with dependencies. I do still think Rust would have been better off with a larger stdlib, but ultimately it's really not that big of a deal and isn't worth it rewriting code for the sake of reducing a meter. reply pbronez 4 hours agoprev> when you end up using one function, but you compile hundreds, some alarm bell should go off. That seems feasible to test automatically. Shouldn't an intelligent compiler automatically trim these things out? reply ForHackernews 4 hours agoprev> But there is a simpler path. You write code yourself. Sure, it's more work up front, but once it's written, it's done Well, assuming you wrote it perfectly and didn't introduce any security vulnerabilities... that you will never be alerted to, because no one else is reviewing your code. reply liontwist 4 hours agoparentThis line of reasoning can’t get you far. Where do dependencies come from? > security vulnerabilities Does this code parse untrusted data? Does your process have unlimited access to sensitive resources? reply TZubiri 5 hours agoprev [–] Yet another security issue that can be fixed by slapping a nominal cost. https://en.wikipedia.org/wiki/Sybil_attack#Economic_costs It can also allow for maintainers of the RUSTSEC rating agency to perform some kind of basic check to see if there's any abuse. reply Consider applying for YC's Spring batch! Applications are open till Feb 11. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Armin Ronacher addresses the problem of dependency churn in software development, especially in JavaScript and Rust ecosystems, where frequent updates and numerous dependencies can lead to security vulnerabilities and complexity.",
      "He suggests a shift towards writing code with minimal dependencies, emphasizing the advantages of stable, self-written functions over extensive dependency graphs.",
      "Ronacher calls for a cultural change in both corporate and open-source communities to prioritize simplicity and stability in coding practices, which can lead to reduced maintenance and faster implementation."
    ],
    "commentSummary": [
      "The text highlights the challenges of managing dependencies in programming, comparing Rust's tendency to accumulate dependencies with C++'s control over them.",
      "It discusses security concerns related to package management systems, contrasting Debian/Ubuntu's systems with Docker/Python/Rust, and raises trust issues with unknown maintainers.",
      "The conversation emphasizes the importance of balancing the use of existing libraries with writing custom code to minimize complexity and security risks, considering long-term implications."
    ],
    "points": 190,
    "commentCount": 155,
    "retryCount": 0,
    "time": 1737723335
  }
]
