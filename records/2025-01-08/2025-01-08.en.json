[
  {
    "id": 42627227,
    "title": "Mistakes engineers make in large established codebases",
    "originLink": "https://www.seangoedecke.com/large-established-codebases/",
    "originBody": "Mistakes engineers make in large established codebases Working in large established codebases is one of the hardest things to learn as a software engineer. You can’t practice it beforehand (no, open source does not give you the same experience). Personal projects can never teach you how to do it, because they’re necessarily small and from-scratch. For the record, when I say “large established codebases”, I mean: Single-digit million lines of code (~5M, let’s say) Somewhere between 100 and 1000 engineers working on the same codebase The first working version of the codebase is at least ten years old I’ve now spent a decade working in these codebases. Here’s what I wish I’d known at the start. The cardinal mistake is inconsistency There’s one mistake I see more often than anything else, and it’s absolutely deadly: ignoring the rest of the codebase and just implementing your feature in the most sensible way. In other words, limiting your touch points with the existing codebase in order to keep your nice clean code uncontaminated by legacy junk. For engineers that have mainly worked on small codebases, this is very hard to resist. But you must resist it! In fact, you must sink as deeply into the legacy codebase as possible, in order to maintain consistency. Why is consistency so important in large codebases? Because it protects you from nasty surprises, it slows down the codebase’s progression into a mess, and it allows you to take advantage of future improvements. Suppose you’re building an API endpoint for a particular type of user. You could put some “return 403 if current user isn’t of that type” logic in your endpoint. But you should first go and look to see what other API endpoints in the codebase do for auth. If they use some specific set of helpers, you should also use that helper (even if it’s ugly, hard to integrate with, or seems like overkill for your use case). You must resist the urge to make your little corner of the codebase nicer than the rest of it. The main reason to do this is because large codebases have a lot of landmines in them. For instance, you might not know that the codebase has a concept of “bots”, which are like users but not quite, and require special treatment for auth. You might not know that the internal support tooling in the codebase allows an engineer to sometimes authenticate on behalf of a user, which requires special treatment for auth. There’s definitely another hundred things you might not know. Existing functionality represents a safe path through the minefield. If you do your auth like other API endpoints that have stuck around for a long time, you can follow that path without having to know all the surprising things that the codebase does. On top of that, lack of consistency is the primary long-term killer of large codebases, because it makes it impossible to make any general improvements. Sticking with our auth example, if you want to ever introduce a new type of user, a consistent codebase lets you update the existing set of auth helpers to accommodate that. In an inconsistent codebase, where some API endpoints are doing things differently, you’ll have to go and update and test every one of those implementations. In practice, that means that the general change does not happen, or that the hardest 5% of endpoints to update are just left out of scope - which in turn decreases consistency further, because now you have a user type that works for most-but-not-all API endpoints. So when you sit down to implement anything in a large codebase, you should always first go and look around for prior art, and follow that if at all possible. Is anything else important? Consistency is the most important thing. Let me quickly run through some other concerns as well, though: You need to develop a good sense of how the service is used in practice (i.e. by users). Which endpoints are hit the most often? Which endpoints are the most crucial (i.e. are used by paying customers and cannot gracefully degrade)? What latency guarantees must the service obey, and what code gets run in the hot paths? One common large-codebase mistake is to make a “tiny tweak” that is unexpectedly in the hot path for a crucial flow, and thus causes a big problem. You can’t rely on your ability to test the code in development like you can in a small project. Any large project accumulates state over time (for instance, how many kinds of user do you think GMail supports?) At a certain point, you can’t test every combination of states, even with automation. Instead, you have to test the crucial paths, code defensively, and rely on slow rollouts and monitoring to catch problems. Be very, very reluctant to introduce new dependencies. In large codebases, code often lives forever. Dependencies introduce an ongoing cost in security vulnerabilities and package updates that will almost certainly outlive your tenure at the company. If you have to, make sure you pick dependencies that are widely-used and reliable, or that are easy to fork if needed. For related reasons, if you ever get the chance to remove code, take it with both hands. This is some of the riskiest work in large codebases, so don’t half-ass it: first instrument the code to identify callers in production and drive them down to zero, so you can be absolutely certain it’s safe to remove. But it’s still worth doing. There are few things in a large codebase more worthwhile than safely removing code. Work in small PRs and front-load the changes that affect other teams’ code. This one is important in small projects too, but it’s critical in large ones. That’s because you’ll often be relying on the domain experts in other teams to anticipate things you’ve missed (since large projects are just too complex to anticipate it all yourself). If you keep your changes to risky areas small and easy-to-read, those domain experts have a much better chance of noticing problems and saving you from an incident. Why bother? Finally, I want to take a second to defend these codebases in general. One common take I’ve heard goes like this: Why would you ever decide to work in the legacy mess? Spending time knee-deep in spaghetti code might be hard, but it isn’t good engineering. When faced with a large established codebase, our job should be to shrink it by splitting out small elegant services instead of wading in and making the mess larger. I think this is totally wrong-headed. The main reason is that, as a general rule, large established codebases produce 90% of the value. In any big tech company, the majority of the revenue-producing activity (i.e. the work that actually pays your engineering salary) comes from a large established codebase. If you work at a big tech company and don’t think this is true, maybe you’re right, but I’ll only take that opinion seriously if you’re deeply familiar with the large established codebase you think isn’t providing value. I’ve seen multiple cases where a small elegant service powers some core feature of a high-revenue product, but all the actual productizing code (settings, user management, billing, enterprise reporting, etc) still lives in the large established codebase. So you should know how to work in the “legacy mess” because that’s what your company actually does. Good engineering or not, it’s your job. The other reason is that you cannot split up a large established codebase without first understanding it. I have seen large codebases successfully split up, but I have never seen that done by a team that wasn’t already fluent at shipping features inside the large codebase. You simply cannot redesign any non-trivial project (i.e. a project that makes real money) from first-principles. There are too many accidental details that support tens of millions of dollars of revenue. Summary Large codebases are worth working in because they usually pay your salary By far the most important thing is consistency Never start a feature without first researching prior art in the codebase If you don’t follow existing patterns, you better have a very good reason for it Understand the production footprint of the codebase Don’t expect to be able to test every case - instead, rely on monitoring Remove code any chance you get, but be very careful about it Make it as easy as possible for domain experts to catch your mistakes recruiters │ posts │ resume │ github │ linkedin │ rss",
    "commentLink": "https://news.ycombinator.com/item?id=42627227",
    "commentBody": "Mistakes engineers make in large established codebases (seangoedecke.com)683 points by BerislavLopac 22 hours agohidepastfavorite303 comments peterldowns 20 hours agoI agree that consistency is important — but what about when the existing codebase is already inconsistent? Even worse, what if the existing codebase is both inconsistent and the \"right way to do things\" is undocumented? That's much closer to what I've experienced when joining companies with lots of existing code. In this scenario, I've found that the only productive way forward is to do the best job you can, in your own isolated code, and share loudly and frequently why you're doing things your new different way. Write your code to be re-used and shared. Write docs for it. Explain why it's the correct approach. Ask for feedback from the wider engineering org (although don't block on it if they're not directly involved with your work.) You'll quickly find out if other engineers agree that your approach is better. If it's actually better, others will start following your lead. If it's not, you'll be able to adjust. Of course, when working in the existing code, try to be as locally consistent as possible with the surrounding code, even if it's terrible. I like to think of this as \"getting in and out\" as quickly as possible. If you encounter particularly sticky/unhelpful/reticent team members, it can help to remind them that (a) the existing code is worse than what you're writing, (b) there is no documented pattern that you're breaking, (c) your work is an experiment and you will later revise it. Often asking them to simply document the convention that you are supposedly breaking is enough to get them to go away, since they won't bother to spend the effort. reply sfink 16 hours agoparentHopefully, you have a monorepo or something with similar effects, and a lack of fiefdoms. In that case, if the current way is undocumented and/or inconsistent, you make it better before or while adding in your new approach. If there are 4 ways to do the same thing and you really want to do it a different way, then replace one of those ways with your new one in the process of adding it. For extra credit, get to the point where you understand why you can't replace the other 3. (Or if you can, do it! Preferably in followups, to avoid bundling too much risk at once.) A lot of inconsistency is the result of unwillingness to fix other people's stuff. If your way is better, trust people to see it when applied to their own code. They probably have similar grievances, but it has never been a priority to fix. If you're willing to spend the time and energy, there's a good chance they'll be willing to accept the results even if it does cause some churn and require new learning. (Source: I have worked on Firefox for a decade now, which fits the criteria in the article, and sweeping changes that affect the entire codebase are relatively common. People here are more likely to encourage such thinking than to shoot it down because it is new or different than the status quo. You just can't be an ass about it and ignore legitimate objections. It is still a giant legacy codebase with ancient warts, but I mostly see technical or logistical obstacles to cleaning things up, not political ones.) reply zwnow 12 hours agorootparentThats not how software engineering works in a business setting though? Not a single company I have been in has the time to first fix the existing codebase before adding a new feature. The new feature is verbally guaranteed to the customers by project managers and then its on the dev to deliver within the deadline or you'll have much greater issues than a inconsistent codebase. I'd love to work in a fantasy company that allows for fixing legacy code, but that can take months to years with multi million line codebases. reply cwalv 2 hours agorootparent> I'd love to work in a fantasy company that allows for fixing legacy code You're not supposed to ask. It's like a structural engineer asking if it's okay to spend time doing a geological survey; it's not optional. Or a CFO asking if it's okay to pay down high interest debt. If you're the 'engineer', you decide the extent it's necessary reply zwnow 4 minutes agorootparentThats also not applicable in a business setting. If you have multi million line codebase, you simply cant refactor within reasonable time. Also refactoring can cause issues wich then need further fixing and refactoring. If I touch code that I am not supposed to touch or that does not relate to my direct task I will have HR talks. I'd be lucky to not get laid off for working on things that do not relate to my current task. reply tomohawk 1 hour agorootparentprevIt's why Scotty was always giving longer estimates that Kirk wanted, but Kirk was also able to require an emergency fix to save the ship. The estimate was building in the time to get it done without breaking too much other stuff. For emergency things, Scotty would be dealing with that after the emergency. If your captain is always requiring everything be done as an emergency with no recovery time, you've got bigger problems. reply rusk 54 minutes agorootparentScotty manages his tech debt reply huijzer 12 hours agorootparentprevAnd then at some point the codebase becomes so unusable that new features take too long and out of frustration management decides to hire 500 extra programmers to fix the situation, which makes the situation even more slow. As I understand, there is a balance between refactoring and adding new features. It’s up to the engineers to find a way to do both. Isn’t it also fair if engineers push sometimes back on management? Shouldn’t a civil engineer speak up if he/she thinks the bridge is going to collapse with the current design? reply danjac 6 hours agorootparentOften the problem with companies running the Feature Factory production treadmill too long is you have code supporting unused features and business logic, but nobody knows any more which features can be dropped or simplified (particularly after lots of employee churn and lack of documentation). So the problem is not so much technical debt, but product debt. You can refactor, but you're also wasting time optimizing code you don't need. A better approach is to sit down with rest of the company and start cutting away the bloat, and then refactor what's left. reply bluGill 4 hours agorootparentI was involved with a big rewrite. Our manager had on his desk the old system with a sign \"[managers name]'s product owner\". Nearly every time someone wanted to know how to do something the answer was load that old thing up and figure out what it did. Eventually we did retire the old system - while the new code base is much cleaner I'm convinced it would have been cheaper to just clean that code up in place. It still wouldn't be as clean as the current is - but the current as been around long enough to get some cruft of its own. Much of the old cruft was in places nobody really touched anymore anyway so there was no reason to care. reply potatolicious 1 hour agorootparent> \"I'm convinced it would have been cheaper to just clean that code up in place\" Generally agreed. I'm generally very bearish on large-scale rewrites for this reason + political/managerial reasons. The trick with any organization that wants to remain employed is demonstrating progress. \"Go away for 3 years while we completely overhaul this.\" is a recipe for getting shut down halfway through and reassigned... or worse. A rewrite, however necessarily, must always be structured as multiple individual replacements, each one delivering a tangible benefit to the company. The only way to stay alive in a long-term project is to get on a cadence of delivering visible benefit. Importantly doing this also improves your odds of the rewrite going well - forcing yourself to productionize parts of the rewrite at a a time validates that you're on the right track. reply bluGill 40 minutes agorootparentPart of our issue with the rewrite is we went from C++ to C++. For an embedded system in 2010 C++ was probably the right choice (rust didn't exist, though D or Ada would have been options and we can debate better elsewhere). Previous rewrites went from 8 bit assembly to C++, which is the best reason to do a rewrite: you are actually using a different language that isn't compatible for an in place rewrite (D supports importing C++ and so could probably be done in place) reply david-gpu 3 hours agorootparentprev> while the new code base is much cleaner I'm convinced it would have been cheaper to just clean that code up in place I saw one big rewrite from scratch. It was a multi-year disaster, but ended up working. I was also told about an earlier big rewrite of a similar codebase which was a multi-year disaster that was eventually thrown away completely. I did see one big rewrite that was successful, but in this case the new codebase very intentionally only supported a small subset of the original feature set, which wasn't huge to begin with. All of this to say that I agree with you: starting from scratch is often tempting, but rarely smooth. If refactoring in place sounds challenging, you need to internalize that a full rewrite will be a few times harder, even if it doesn't look that way. reply hinkley 1 hour agorootparentI stayed at a place that was decades old, in part to decipher how they’d managed to get away with not only terrible engineering discipline but two rewrites without going out of business. I figured it would be good for me to stick around at a place that was defying my predictions for once instead of fleeing at the first signs of smoke. I’ve hired onto places that failed before my old employer did at least twice and I feel a bit silly about that. I wasted a lot of my time and came away barely the wiser, because the company is spiraling and has been for a while. Near as I can figure, the secret sauce was entirely outside of engineering. If I had to guess, they used to have amazing salespeople and whoever was responsible for that fact eventually left, and their replacement’s replacement couldn’t deliver. Last I heard they got bought by a competitor, and I wonder how much of my code is still serving customers. reply hinkley 2 hours agorootparentprevRewrites are much like any act of self improvement. People think grand gestures and magical dates (like January 1 or hitting rock bottom) are the solution to turn your life around. But it’s little habits compounding that make or break you. And it’s new habits that kill old ones, not abstinence. I worked with another contractor for a batshit team that was waiting for a rewrite. We bonded over how silly they were being. Yeah that’s great that you have a plan but we have to put up with your bullshit now. The one eyed man who was leading them kept pushing back on any attempts to improve the existing code, even widely accepted idioms to replace their jank. At some point I just had to ask him how he expected all of his coworkers to show up one day and start writing good code if he won’t let them do it now? He didn’t have an answer to that, and I’m not even sure the question landed. Pity. The person who promised him the rewrite got promoted shortly before my contract was up. This promotion involved moving to a different office. I would bet good money that his replacement did not give that team their rewrite. They’re probably either still supporting that garbage or the team disappeared and someone else wrote a replacement. That whole experience just reinforced my belief that the Ship of Theseus scenario is the only solution you can count on working. Good code takes discipline, and discipline means cleaning up after yourself. If you won’t do that, then the rewrite will fall apart too. Or flame out. reply hinkley 2 hours agorootparentprevIn the same way people go to their doctor or dentist or mechanic too late and prevention and sometimes even the best treatments are off the table, software developers (particularly in groups vs individually) love to let a problem fester until it’s nearly impossible to fix. I’m constantly working on problems that would have been much easier to address 2 years ago. reply zwnow 12 hours agorootparentprevThe issue is that management usually doesn't care. Personally I usually have about 3-4 days to implement something. If I can't deliver they will just look for more devs, yes. Quantity is what matters for management. New New New is what they want, who cares about the codebase (sarcasm). Management doesn't even know how a good codebase looks. A bridge that is missing support probably wouldn't have been opened to the public in the first place. Thats not correct for codebases. reply hinkley 1 hour agorootparentThat’s why consistent messaging matters. If everyone agrees to make features take as long as they take, then management can’t shop things around. Which they shouldn’t be doing but we all know That Guy. When children do this it’s called Bidding. It’s supposed to be a developmental phase you train them out of. If Mom says no the answer is no. Asking Dad after Mom said no is a good way to get grounded. reply staunton 11 hours agorootparentprev> A bridge that is missing support probably wouldn't have been opened to the public in the first place. That's not always been the case and came to be because people have died... Is anyone going to die if your codebase is an unmaintainable mess? reply hinkley 1 hour agorootparentCompanies die because nobody is willing to work on the code anymore. If VCs ever came to expect less than 90% of their investments to essentially go to zero, maybe that would change. But they make enough money off of dumb luck not leading to fatal irreversible decisions often enough to keep them fat and happy. reply staunton 1 hour agorootparentThat doesn't sound nearly as bad or serious as people dying. reply baq 11 hours agorootparentprev> The issue is that management usually doesn't care. Neither do customers. The product is an asset. Code is a liability. reply zwnow 11 hours agorootparentCan't be held accountable for work conditions engineers dont have power over. If I dont have time to write tests, I cant be blamed for not writing tests. Especially now with hallucinating bs AI there is a whole load of more output expected from devs. reply strogonoff 7 hours agorootparentRecently I got an email that some severe security defects were found in a project, so I felt compelled to check. A bot called “advanced security AI” by Github raised two concerns in total, both indeed marked as “high severity”: — A minimal 30 LoC devserver function would serve a file from outside the current directory on developer’s machine, if said developer entered a crafty path in the browser. It suggested a fix that would almost double the linecount. — A regex does not handle backslashes when parsing window.location.hostname (note: not pathname), in a function used to detect whether a link is internal (for statically generated site client-side routing purposes). The suggested fix added another regular expression in the mix and generally made that line, already suffering from poor legibility due to involving regular expressions in the first place, significantly more obscure to the human eye. Here’s the fun thing: if I were concerned about my career and job security, I know I would implement every damn fix the bot suggested and would rate it as helpful. Even those that I suspect would hurt the project by making it less legible and more difficult to secure (and by developers spending time on things of secondary importance) while not addressing any actual attack vectors or those that are just wrong. Security is no laughing matter, and who would want to risk looking careless about it in this age? Why would my manager believe that I, an ordinary engineer, know (or can learn) more about security than Github’s, Microsoft’s most sophisticated intelligence (for which the company pays, presumably, some good money)? Would I even believe that myself? If all I wanted was to keep my job another year by showing increased output thanks to all the ML products purchased by the company, would I object to free code (especially if it is buggy)? reply baq 10 hours agorootparentprevDon't check in any code, only prompts. The product is reconfabulated on every build. reply xamde 5 hours agorootparentThere will be companies founded on executing this idea. reply watt 8 hours agorootparentprevSounds like a dogma that got us (as industry) into this mess. reply baq 6 hours agorootparentI’d rather say it’s an observation of real behavior. Customers of yours don’t buy code (unless it is your product) - they buy solutions to their problems. Thus, management and sales want to sell solutions, because that gets you paid. Engineering is fulfilling requirements within constraints. Good custom code might fit the bill. Bad might, too - unless it’s a part of requirements that it shouldn’t be bad. It usually isn’t. reply hinkley 2 hours agorootparentprevOr it becomes so unusable that the customers become disenchanted and flee to competitors. If management keeps making up deadlines without engineering input, then they get to apologize to the customer for being wrong. Being an adult means taking responsibility for your own actions. I can’t make a liar look good in perpetuity and it’s better to be a little wrong now than to hit the cliff and go from being on time to six months late practically overnight. reply mrkeen 11 hours agorootparentprev> As I understand, there is a balance between refactoring and adding new features. True, but has drifted from the TFA's assertion about consistency. As the thread has implied, it's already hard enough to find time to make small improvements. But once you do, get ready for them to be rejected in PR for nebulous \"consistency\" reasons. reply hansvm 3 hours agorootparentprevThey don't think they have the time, but that's because they view task completions as purely additive. Imagine you're working on this or that feature, and find a stumbling block in the legacy codebase (e.g., a poorly thought out error handling strategy causing your small feature to have ripple effects you have to handle everywhere). IME, it's literally cheaper to fix the stumbling block and then implement the feature, especially when you factor in debugging down the line once some aspect of the kludgy alternative rears its ugly head. You're touching ten thousand lines of code anyway; you might as well choose do it as a one-off cost instead of every time you have to modify that part of the system. That's triply true if you get to delete a bunch of code in the process. The whole \"problem\" is that there exists code with undesirable properties, and if you can remove that problem then velocity will improve substantially. Just do it Ship of Theseus style, fixing the thing that would make your life easier before you build each feature. Time-accounting-wise, the business will just see you shipping features at the target rate, and your coworkers (and ideally a technical manager) will see the long-term value of your contributions. reply bcrosby95 11 hours agorootparentprevNew features are the right time to refactor. If you can't make the code not complete shit you don't have time to add the feature. Never refactor code to make it prettier or whatever, refactor it when it becomes not-fit-for-purpose for what you need to do. There's obviously exceptions (both ways) but those are exceptions not rules. At least, that's what I teach our devs. reply zwnow 11 hours agorootparentMy company didn't even have time to keep the dependencies up to date so now we are stuck with Laravel 5 and Vue 2. Refactoring/Updating can be an incredible workload. Personally I'd say rewriting the whole thing would be more efficient but that's not my choice to make. If you have plenty of time for a task, I fully agree with you. reply physicsguy 9 hours agorootparentIt's still also often the right business choice, especially for small businesses which aren't making a profit yet. reply cess11 1 hour agorootparentprevI was in an organisation that made decent money on a system built on Laravel 3, I think. The framework was written in an only static classes style, which they over ten years had run with while building the business so everything was static classes. Once you have a couple of million lines of that, rewrite is practically impossible because you need to take the team of two OK devs and a junior off firefighting and feature development for years and that will hurt reputation and cashflow badly. My compromise was to start editing Laravel and implementing optimisations and caching, cutting half a second on every request within a month of starting, and then rewriting crude DIY arithmetic on UNIX epoch into standard library date/time/period functions and similar adjustments. I very openly pushed that we should delete at least two hundred thousand lines over a year which was received pretty poorly by management. When I left in anger due to a googler on the board fucking up the organisation with their annoying vision where this monster was to become \"Cloud Native\" on GCP credits he had, a plan as bad as a full rewrite, it only took a few months until someone finally convinced them to go through with deletions and cut LoC in half in about six months. I don't think they do containers or automatic tests yet, probably never will, but as of yet the business survives. reply peterldowns 15 hours agorootparentprevI don't disagree with what you've written at all, but let me just say: > Hopefully, you have a monorepo or something with similar effects, and a lack of fiefdoms ah to be so lucky... reply sverhagen 14 hours agorootparentIt's a bit of a non-issue in this context. If you don't have a mono-repo, you should maintain reasonable consistency within each repository (and hope they're consistent between each other, but that's probably less important here). reply StellarScience 14 hours agorootparentprevGreat points, I'd just add: > A lot of inconsistency is the result of unwillingness to fix other people's stuff Agree, so we find it best to practice \"no code ownership\" or better yet \"shared code ownership.\" So we try to think of it all as \"our stuff\" rather than \"other people's stuff.\" Maybe you just joined the project, and are working around code that hasn't been touched in 5 years, but we're all responsible for improving the code and making it better as we go. That requires a high trust environment; I don't know if it could work for Firefox where you may have some very part-time contributors. But having documented standards, plus clang-format and clang-tidy to automate some of the simpler things, also goes a long way. reply sfink 48 minutes agorootparent> That requires a high trust environment; I don't know if it could work for Firefox where you may have some very part-time contributors. Ironically, that's why it works for Firefox. Contributors follow a power law. There are a lot of one-shot contributors. They'll be doing mostly spot fixes or improvements, and their code speaks for itself. Very little trust is needed. We aren't going to be accepting binary test blobs from them. There are relatively few external contributors who make frequent contributions, and they've built up trust over time -- not by reporting to the right manager or being a friend of the CTO, but through their contributions and discussions. Code reviews implicitly factor in the level of trust in the contributor. All in all, the open nature of Firefox causes it to be fundamentally built on trust, to a larger extent than seems possible in most proprietary software companies. (There, people are less likely to be malicious, but for large scale refactoring it's about trusting someone's technical direction. Having a culture where trust derives from contribution not position means it's reasonable to assume that trusted people have earned that trust for reasons relevant to the code you're looking at.) There are people who, out of the blue, submit large changes with good code. We usually won't accept them. We [the pool of other contributors, paid or not] aren't someone's personal code maintenance team. Code is a liability. > But having documented standards, plus clang-format and clang-tidy to automate some of the simpler things, also goes a long way. 100% agree. It's totally worth it even if you disagree with the specific formatting decisions made. reply post-it 3 hours agorootparentprev> In that case, if the current way is undocumented and/or inconsistent, you make it better before or while adding in your new approach. Sometimes, but oftentimes that would involve touching code that you don't need to touch in order to get the current ticket done, which in turn involves more QA effort. reply The_Colonel 13 hours agorootparentprevI agree, but this presupposes a large comprehensive test suite giving you enough confidence to do such sweeping changes. I don't doubt Firefox has it, but most (even large, established projects) will not. A common case I've seen is that newer parts are relatively well covered, but older, less often touched parts don't have good coverage, which makes it risky to do such sweeping changes. reply devmor 2 hours agorootparentprev> Hopefully, you have a monorepo or something with similar effects, and a lack of fiefdoms. In that case, if the current way is undocumented and/or inconsistent, you make it better before or while adding in your new approach. Unfortunately, this is how you often get even more inconsistent codebases that include multiple tenures' worth of different developers attempting to make it better and not finishing before they move on from the organization. reply digging 19 hours agoparentprev> If it's actually better, others will start following your lead. Not really my experience in teams that create inconsistent, undocumented codebases... but you might get 1 or 2 converts. reply peterldowns 19 hours agorootparentIt depends on the day but generally I believe that most engineers want to write good code, want to improve their own skills, and like learning and critiquing with other engineers. Sometimes a small catalyst is all it takes to dramatically improve things. Most of the times I've thought that individual contributors were the problem, the real issue was what the company's leaders were punishing/rewarding/demanding. reply Tallain 10 hours agorootparentExactly this. I (relatively recently) joined a team with a handful of developers all sort of doing things their own way. No docs, no shared practices, just individuals doing their own thing. After reviewing the code, submitted PRs with fixes, putting together docs for best practices, the entire team shifted their stance and started working closer together in terms of dev practices, coding styles, etc. Not to say I got everyone to march to my drum -- the \"best practices\" was a shared effort. As you said, sometimes it just takes someone to call things out. We can do things better. Look at how things improve if you approach X problem in Y manner, or share Z code this way. Maybe the team was overwhelmed before and another voice is enough to tip the scales. If you don't try, you'll never know. reply aleph_minus_one 16 hours agorootparentprev> I believe that most engineers want to write good code But the opinion what makes code good differ a lot between software developers. This exactly leads to many of the inconsistencies in the code. reply darepublic 17 hours agorootparentprevdoing some recent contract work I discovered someone putting this into a PR (comments my own) ``` let susJsonString = '...' // we get this parseable json string from somwhere but of course it might not be parseable. so testing seems warranted... try { // lets bust out a while loop! while(typeof susJsonString === 'string') { susJsonString = JSON.parse(susJsonString) } } catch { susJsonString = {} } // also this was a typescript codebase but all the more reason to have a variable switch types! this dev undoubtedly puts typescript at the top of their resume ``` I suppose this works?! I haven't thought it through carefully, it's just deciding to put your shoes on backward, and open doors while standing on your head. But I decided to just keep out of it, not get involved in the politics. I guess this is what getting old is like seriously you just see younger people doing stuff that makes your jaw drop from the stupidity (or maybe its just me) but you can't say anything because reasons. Copilot, ai assisted coding only further muddies the waters imo. reply liontwist 16 hours agorootparentThis is totally fine. If you're given shit data this seems like a reasonable way to try to parse it (I would personally bound the loop). Typescript is not going to make it better. The problem is whoever is producing the data. reply bryanrasmussen 15 hours agorootparentI think the complaint here is they have a string, which even has the word string in the variable name, and they turn it into an object at the end. Hence references to Typescript. I suppose what is wanted is something like let parsedJSON = {} try { parsedJSON = JSON.parse(susJsonString) } catch { //maybe register problem with parsing. } reply accoil 14 hours agorootparentThat's quite different though. It looks to be dealing with the case that a serialised object gets serialiased multiple times before it reaches that point of code, so it needs to keep deserialising until it gets a real object. E.g: JSON.parse(JSON.parse(\"\\\"{foo: 1}\\\"\")) I'd guess the problem is something upstream. reply darepublic 12 hours agorootparentThe code is either going to loop once and exit or loop forever no reply andrewf 11 hours agorootparentPutting this in my web console: let susJsonString=JSON.stringify(JSON.stringify(JSON.stringify({foo:1}))) console.log(\"initial:\", susJsonString); try { while(typeof susJsonString==='string') { susJsonString = JSON.parse(susJsonString); console.log(\"iteration:\", typeof susJsonString, susJsonString); } } catch { susJsonString = {}; } I see: initial: \"\\\"{\\\\\\\"foo\\\\\\\":1}\\\"\" iteration: string \"{\\\"foo\\\":1}\" iteration: string {\"foo\":1} iteration: object {foo: 1} A comment explaining the sort of \"sus\" input it was designed to cope with may have been helpful. reply accoil 7 hours agorootparentprevIt will stop when it gets something that's not a string due to while(typeof susJsonString==='string') { susJsonString = JSON.parse(susJsonString); as it'll keep reassigning and parsing until gets a non string back (or alternatively error out if the string is not valid json) reply bryanrasmussen 11 hours agorootparentprevHmm, yeah ok, didn't pick this out of the let susJsonString = '...' example but evidently it is not just that it is serialized multiple times, otherwise it shouldn't need the try catch (of course one problem with online discussion of code examples is you must always assume, contra obvious errors, that the code actually needs what it has) Something upstream, sure, but often not something \"fixable\" either, given third parties and organizational headaches some places are prone to. reply accoil 6 hours agorootparentYeah. I imagine that's a bandaid around having to consume a dodgy api that they didn't have access/permission to fix. The blanket catch is odd though, as I'd have thought that it would still be outputting valid json (even if it has been serialized multiple times), and if you're getting invalid json you probably want to know about that. reply liontwist 5 hours agorootparentprevNow two of you are misunderstanding. It’s applying the operation recursively. reply darepublic 12 hours agorootparentprevWhy the while loop reply otabdeveloper4 11 hours agorootparentBecause some upstream idiot is calling JSON.stringify several times. reply accoil 6 hours agorootparentI've seen this happen when someone's not familiar with their api framework and instead of returning an object for the framework to serialize, they serialize it on their own and return a string. Which then gets serialized again by the framework. reply liontwist 6 hours agorootparentprevYou came in so confident it was wrong, but it turns out you don’t really know what it does. Please take a lesson from this. Good code is not the one that follows all the rules you read online. Your coworker you dismissed understood the problem. reply digging 3 hours agorootparentDid you reply to the wrong comment? I think asking questions is ideal. Even when I'm 99% sure a line is blatantly wrong, I will ask something like, \"What is this for?\". Maybe I missed something - wouldn't be the first time. reply liontwist 2 hours agorootparentDarepublic originally posted his coworker’s code to make fun of above. reply watwut 8 hours agorootparentprevSure, but that does not imply they will follow whatever you found out to be the best for the piece of code you are working on right now. reply citizenpaul 17 hours agorootparentprev>Not really my experience in teams that create inconsistent, undocumented codebases... but you might get 1 or 2 converts. This has also been my experience. Usually there is a \"Top\" sticky/unhelpful/reticent person. They are not really a director or exec but they often act like it and seem immune from any repercussions from the actual higher ups. This person tends to attract \"followers\" that know they will keep their jobs if they follow the sticky person for job security. There usually are a few up and coming people that want better that will kinda go along with you for their own skill building benefit but its all very shaky and you can't count on them supporting you if resistance happens. I've literally had the \"I was here before you and will be after\" speech from one of the \"sticky's\" before. All these HN how to do better write ups seem to universally ignore the issues of power and politics dynamics and give \"in a vacuum\" advice. Recognizing a rock and a hard place and saving your sanity by not caring is a perfectly rational decision. reply hinkley 1 hour agorootparentThere are however some people who think they are sticky but aren’t really. Some but not all of them use Impostor Syndrome to keep their followers in line. You can recruit most easily from people they’ve left twisting in the wind when their suggestions and ideas turned out to not work, but only if you always deal with the poor consequences of your own decisions. People will follow ideas they don’t quite understand if they know they won’t be working alone at 7 pm on a Thursday fixing it. These sort of people will vote for you publicly. However some lot them will still take the path of least resistance when you aren’t looking. It was sort of a nasty surprise when I figured out one day that there are people in this industry that will agree with high minded sentiments in public but not lift a finger to get there. I ended up in a group that had two or three of them. And one day due to a requirements process fuckup we had a couple weeks with nothing to do. They just did the Hands Are Tied thing I’d been seeing for over a year (yes we should do X but we have to do Y for reasons) and I saw red. Luckily I was on a conference call instead of sitting in front of them at that moment. But I’m sure they heard the anger lines over the phone. If the boss doesn’t give you an assignment, you work on tech debt they haven’t previously insisted that you work on. Simple as that. At most places if my boss disappeared, I could keep busy for at least three months without any direction. And keep several other people busy as well. If you don’t know what to work on then I don’t know what’s wrong with you. reply jimbokun 17 hours agorootparentprevWell HN was created as a forum for discussing start up best practices, which is all about disrupting big companies weighed down by internal politics. reply awesome_dude 14 hours agorootparentThe linked article is about dealing with legacy codebases with millions of lines of code. The response is accurate - anyone that's had to deal with a legacy code base has had to deal with the creators of said birds nest (who proudly strut around as though the trouble it causes to maintainability makes them \"clever\"). reply peterldowns 15 hours agorootparentprevI tried my best to offer a pragmatic recommendation for dealing with those sorts of people. I'd love to know what you would recommend instead? reply awesome_dude 14 hours agorootparentIME it's politics, so you need to find someone that the sticky person fears/respects, and get them onboard. The only other way I have succeeded is to appeal to the sticky person's ego, make them think that it's their idea. Note: I have also had to deal with Sticky person: Do it this way Me: But X Sticky Person: No, do it the way I have decreed [...] Three hours later (literally) Sticky Person: Do it X way reply billy99k 4 hours agorootparentprevThis exactly. I worked at a place one time with a terrible code base. They based it on open source and slapped on additions with no style or documentation. My first day, I couldn't even stand the code base up on my local dev environment, because there were so many hard-coded paths throughout the application, it broke (they were unwilling to fix this or have me fix it). I tried to accept their way of coding and be part of the team, but it got too much for me. They were staunch SVN supporters. This isn't much of a problem, but we had constant branching problems that Git would have resolved. As I got assigned work, I noticed I would have to fix more bugs and bad coding, before I could even start the new addition/feature. It was riddled with completely obvious security vulnerabilities that were never fixed. Keep in mind that this was the new product of the entire company with paying customers and real data. The team lead was also very insecure. I couldn't even nicely mention or suggest fixes in code that he had written. The interesting thing is that he didn't even really have a professional coding background. He went straight from tech support to this job. I lasted about a year. I got let go due to 'money issues'. Shortly before this, they wanted me to merge my code into my branch with the Jr. developer's code right before my vacation (literally the day before). I merged it and pushed it up to the repo (as instructed) and the team lead sent me nasty emails throughout my vacation about how various parts of my code 'didn't work'. Not only were these parts the Jrs code, it wasn't ready for production. The other thing to know about the team lead is that he was extremely passive aggressive and would never give me important project details unless I asked (I'm not talking details, just high-level, what needs to be completed). We had a call where he told me I 'wasn't a senior developer'. I wanted to tell him to fuck off, but I needed the job. The company went out of business 2 months later. I found out their entire business model relied only on Facebook Ads, and they got banned for violating their rules. reply LAC-Tech 18 hours agorootparentprevahh, there's a lot of scenarios here. in my scenario, those people were gone. reply LouisSayers 5 hours agoparentprev> but what about when the existing codebase is already inconsistent? Then you get people together to agree what consistent looks like. I find the easiest way to do this is to borrow someone else's publicly documented coding conventions e.g. Company ABC. Then anyone disagreeing isn't disagreeing with you, they're disagreeing with Company ABC, and they (and you) just have to suck it up. From there on in, you add linting tools, PR checks etc for any new code that comes in. reply cess11 1 hour agorootparentIf there's resistance to picking a style guide, autoformatting might be a viable start and will probably do quite a bit for shallow consistency at the price of large PR:s once per file. Once one has worked with a forced style for a while it starts to feel weird to see breaches of it, and I think that might help softening people to adapting a style guide regarding more subtle things like error handling or attitude to standardised protocols like HTTP. reply agentultra 18 hours agoparentprev> In this scenario, I've found that the only productive way forward is to do the best job you can, in your own isolated code, and share loudly and frequently why you're doing things your new different way. Now you have N+1 ways. It can work if you manage to get a majority of a team to support your efforts, create good interfaces into the legacy code paths, and most importantly: write meaningful and useful integration tests against that interface. Michael Feathers wrote a wonderful book about this called, Working Effectively with Legacy Code. I think what the author is trying to say with consistency is to avoid adding even more paths, layers, and indirection in an already untested and difficult code base. Work strategically, methodically, and communicate well as you say and it can be a real source of progress with an existing system. reply peterldowns 17 hours agorootparentI’ll check out that book, thanks for the reference. reply mgfist 20 hours agoparentprevI rarely see large 10m+ LOC codebases with any sort of strong consistency. There are always flavors of implementations and patterns all over the place. Hell, it's common to see some functionality implemented multiple times in different places reply peterldowns 20 hours agorootparentAnd it's fine, right? Honestly I think people need to realize that part of being a good engineer is being able to deal with inconsistency. Maybe submodule A and submodule B do network requests slightly differently but if both ways are reasonable, working, and making the company money, it's probably not worth delaying product improvements in order to make things \"more consistent.\" On the other hand if no one in your company cares about consistency, at some point everything becomes so awful you basically won't be able to retain engineers or hire new ones, so this is a place where careful judgement is needed. reply citizenpaul 17 hours agorootparent>and it's fine, right? The hard part of being an engineer is realizing that sometimes even when something is horribly wrong people may not actually want it fixed. I've seen systems where actual monetary loss was happening but no one wanted it brought to light because \"who gets blamed\" reply Rastonbury 2 hours agorootparentThat's crazy, is there no opportunity to get credit for preventing monetary loss? reply jimbokun 17 hours agorootparentprevThat’s always a strong signal to start polishing your resume. Layoffs are probably just around the corner. reply dasil003 18 hours agorootparentprevYeah 100%. Honestly style / technique / language consistency are implementation details, it helps with engineer fungibility and ramp up, but it also works against engineers applying local judgement. This is something to briefly consider when starting new services/features, but definitely not something to optimize for in an existing system. On the other hand, data and logic consistency can be really important, but you still have to pick your battles because it's all tradeoffs. I've done a lot of work in pricing over the decades, and it tends to be an area where the logic is complex and you need consistency across surfaces owned by many teams, but at the same time it will interact with local features that you don't want to turn pricing libraries/services into god objects as you start bottlenecking all kinds of tangentially related projects. It's a very tricky balance to get right. My general rule of thumb is to anchor on user impact as the first order consideration, developer experience is important as a second order, but many engineers will over-index on things they are deeply familiar with and not be objective in their evaluation of the impact / cost to other teams who pay an interaction cost but are not experts in the domain. reply Supermancho 17 hours agorootparentA common experience (mostly in the Pacific North West) I have had is to implement a feature in a straightforward manner that works with minimal code, for some backlog issue. Then I'm told the PR will be looked at. A couple days later I am told this is not the way to do X. You must do it Y? Why Y? Because of historical battles won and lost why, not because of a specific characteristic. My PR doesn't work with Y and it would be more complicated...like who knows what multiplier of code to make it work. Well that makes it a harder task than your estimate, which is why nobody ever took it up before and was really excited about your low estimate. How does Y work? Well it works specifically to prevent features like X. How am I supposed to know how to modify Y in a way that satisfies the invisible soft requirements? Someone more senior takes over my ticket, while I'm assigned unit tests. They end up writing a few hundred lines of code for Y2.0 then implement X with a copy paste of a few lines. I must not be \"a good fit\". Welcome to the next 6-12 months of not caring about this job at all, while I find another job without my resume starting to look like patchwork. Challenging people's egos by providing a simpler implementation for something someone says is very hard, has been effective at getting old stagnant issues completed. Unnaturally effective. Of course, those new \"right way\" features are just as ugly as any existing feature, ensuring the perpetuation of the code complexity. Continually writing themselves into corners they don't want to mess with. reply liontwist 16 hours agorootparentThis sounds like you are missing important context. Here is a similar conversation: \"Why do I have to use the system button class. I implemented my own and it works.\" \"Because when the OS updates with new behavior your button may break or not get new styling and functionality\" \"But this works and meets the spec, that's 10x harder\" reply Supermancho 3 hours agorootparentMore like we have to use the god object to make all http calls for consistency in logging, despite this being a gcp pubsub. reply dasil003 17 hours agorootparentprevHard for me to comment definitively here since I don't have the other side of the story, but I will say that I have seen teams operating based on all kinds of assumed constraints where we lose sight of the ultimate objective of building systems that serve human needs. I've definintely seen cases where the true cost of technical debt is over-represented due to a lack of trust between between business stakeholders and engineering, and those kind of scenarios could definintely lead to this kind of rote engineering policy detached from reality. Without knowledge of your specific company and team I can't offer any more specific advice other than to say that I think your viewpoint of the big picture sounds reasonable and would resonate in a healthy software company with competent leadership. Your current company may not be that, but rest assured that such companies do exist! Never lose that common sense grounding, as that way madness lies. Good luck in finding a place where your experience and pragmatism is valued and recognized! reply eru 17 hours agorootparentprevI feel your pain. But I guess you need to work harder on detecting these kinds of work places upfront, instead of joining them one after another? reply Supermancho 3 hours agorootparentGenerally, companies filter out candidates who request to look at any measurable amount of source code as part of the process. Larger companies leveragethe 6-12 mo contractor to hire. You are still stuck there until you are not. These topics are common knowledge, if you have interviewed in the last 5 to 10 years. I have been working for 25, so I find the blame trying to be redirected, by some, misguided. reply wrs 19 hours agorootparentprevAnd to be practical, that's fine. In a big codebase it's more important to encourage consistent, well-defined, small interfaces, and a clean separation of concerns, than to try to get consistency in the lower-level implementation details. Other non-code concerns like coordinating releases and migration of shared services are also way more important than getting everyone to use the same string library. (Of course, if you carry that principle to the extreme you end up with a lot of black-box networked microservices.) reply dotancohen 4 hours agoparentprev> your work is an experiment and you will later revise it I advise against this if you have not been allocated the time or budget to revise the code. For one thing, you're lying. For another thing, were you hired to be a part of the contributing team or hired to be part of a research team doing experiments in the contributing team's codebase and possibly deploying your experiment on their production systems? I would immediately push back on any new guy who says this, no matter how confident he seems that his way is the right way. reply mihaaly 4 hours agorootparentCounter-thought: We are making brand new things here and not being in an assembly line coming up with the very same thing dozens to million times. We are paid to make new products never existed, having novelty elements in it desired to be a bigger extent than not! Those pretending knowing exactly what they are doing are lying! Of course we are speculating here about the size of novelty content to a differing extent, which is never 0% and never 100%, but something inbetween. But those pushing back on those at least trying to revise the work - putting emphasis on it -, deserve no-one coming to them to be pushed back (at least for the inability of allocating resources for this essential activity of development. Development!). (tried to mimic the atmosphere of the message, sorry if failed) reply mihaaly 7 hours agoparentprevConsistency in huge legacy codebase is like virginity in a brothel: desired but non-existent. reply stravant 15 hours agoparentprevSometimes people are too afraid of attempting to make it consistent. I've done several migrations of thing with dozens of unique bespoke usage patterns back to a nice consistent approach. It sometimes takes a couple straight days of just raw focused code munging, and doesn't always end up being viable, but it's worth a shot for how much better a state it can leave things in. reply sfn42 6 hours agorootparentHighly agree. I've done quite a few large refactors of unnecessarily complex systems that resulted in significant improvement - from lots of bugs to nearly no bugs, incomprehensible code to simple straight forward code, no tests to great test coverage. I did have one bad experience where I ended up spending way too much time on a project like that, I think I made some mistakes with that one and got in a bit too deep. Luckily my team was very supportive and I was able to finish it and it's a lot better now than it was. reply whilenot-dev 11 hours agoparentprevConsistency is never the sole reason to change something, there's always consistency and at least one of the following: - coherency - type safety (shout-out to dynamically typed languages that adapted static typing) - concurrency - simplicity - (and more) > If it's actually better, others will start following your lead. A lot of people don't want to improve the quality in their output and for various reasons... some are happy to have something \"to pay the bills\", some don't want to use a programming language to its full extend, some have a deeply rooted paradigm that worked for 10 years already (\"static types won't change that\"), others are scared of concurrency etc. For some people there's nothing to worry about when a server can be blocked by a single request for 60 secs. reply lamontcg 17 hours agoparentprevIt is terrible to just do this on your own, particularly as the n00b. If there are 5 different standards in the codebase, don't just invent your own better way of doing things. That is literally the xkcd/Standards problem. Go find one of the people who have worked there the longest and ask which of the 5 existing standards are most modern and should be copied. And as you get more experience with the codebase you can suggest updates to the best standard and evolve it. The problem is that you then need to own updating that whole standard across the entire codebase. That's the hard part. If you aren't experienced enough with the codebase to be aggressive about standardization, you shouldn't be creating some little playground of your own. reply peterldowns 15 hours agorootparent> If there are 5 different standards in the codebase, don't just invent your own better way of doing things. That is literally the xkcd/Standards problem. Go find one of the people who have worked there the longest and ask which of the 5 existing standards are most modern and should be copied. I strongly disagree with you and believe you've missed the point of my comment. Think about this: why are there 5 different standards in the codebase, none of which meet your needs? Do you think any engineers on the team are aware of this situation? And how might you get more experience with the codebase without writing code that solves your problems? reply lamontcg 41 minutes agorootparent> Do you think any engineers on the team are aware of this situation? Yes, there probably are. If you haven't been working there for long enough to know who they are, then you shouldn't be YOLO'ing it. The fact that it hasn't all been cleaned up yet is due to that being an order of magnitude harder than greenfielding your own standard. That doesn't mean that nobody is aware of it, or working on it. I've absolutely worked for a decade on a codebase which had at least 5 different standards, and I was the one responsible for cleaning it all up, and we were understaffed so I could never finish it, but I could absolutely point you at the standard that I wanted you to follow. It also probably was somewhat deficient, but it was better than the other 4. It evolved over time, but we tried to clean it all up as we went along. Trying to ram another standard into the codebase without talking it over with me, was guaranteed to piss me off. reply baq 11 hours agorootparentprevPeople who created 3 of the 5 no longer work at the company, test coverage is minimal or nonexistent and the system mostly does what it’s supposed to. In this situation, ‘getting more experience in the code base’ is more or less synonymous with ‘getting paged on the weekend’. reply jiggawatts 15 hours agorootparentprev“Time” is the answer almost always. Standards evolve over time, as do the languages and frameworks. Old code is rarely rewritten, so you end up with layers of code like geological strata recording the history of the developer landscape. There’s a more complicated aspect of “Conway’s law, but over time” that’s hard to explain in a comment. And anyway, Casey Muratori did it better: https://youtu.be/5IUj1EZwpJY?si=hnrKXeknMCe0UPv4 reply nuancebydefault 6 hours agoparentprevTo me the (a), (b) and (c) tactics remind me of people who were very hard to work with. I believe a better approach is indeed like you already mentioned, explain and document, but, as an extra: also be open to comments on the docs and implementation. Often there's a reason that your particular approach was not used earlier, like explained in the article. reply DrBazza 7 hours agoparentprev> but what about when the existing codebase is already inconsistent It depends. If it's a self contained code base, automatic refactoring can safely and mechanically update code to be consistent (naming, and in some cases structurally). If it's not self contained and you're shipping libraries, i.e. things depend on your code base, then it's more tricky, but not impossible. \"Lean on tooling\" is the first thing you should do. reply darepublic 17 hours agoparentprevMy last experience with this, in the section of code I had to navigate for my first ticket at startup X we had some code that was querying the same tables multiple times unnecessarily. We were also using a highly bespoke (imo) code library with a relatively small following on github but this library permeated the entire codebase and dictated the way things had to be done. I tried to just make my changes by touching the code as little as possible, but avoiding the most outstanding inefficiencies. I thought of my changes as a small oasis of sanity in a desert of madness. In that first PR the seniors tore me a new one. It turned out there was a v2 of \"the right way of doing things\" that I had to learn about and then write my code to conform to. v2 had it's own issues, though was perhaps not as bad as v1. Later on when I became more influential I was able to successfully advocate to management to change many things to my liking, including axing the beloved exotic library that distinguished our codebase. But the old guard remained highly resistant to changing the way our code was written, and switched their stance from 'this is great' to, 'it sucks but its too much effort not to keep with it'. I am left feeling that it was all not worth it, not just the struggle but whether the product was appreciably effected one way or another. Just another crappy war story of my blighted career. reply dirtbag__dad 17 hours agorootparentThis resonates hard. In particular, managing the old guard’s emotions is a defeating process. It is almost always easier to jump into a project and critique it than it is to start from scratch. New perspectives and better ideas should be welcome, but instead they can be shut down because folks take things personally. My (rather unfortunate) conclusion is that when I encounter this behavior I move to another team to avoid it. If that’s not possible it’s honestly worth looking for another job. reply physicsguy 9 hours agorootparentprevI had this in 2018 with a company that were still using csh scripts in all of the development tooling. reply HideousKojima 19 hours agoparentprevMy approach is what I call defensive programming, with a different meaning than the usual usage of the term. I assume that my coworkers are idiots that aren't going to read my documentation, so I make all public classes and methods etc. as idiot-proof as possible to use. Hasn't saved me from every issue caused by my teammates never reading my docs or asking me questions, but it's definitely prevented several. reply staunton 17 hours agorootparent> assume that my coworkers are idiots I know (most?) people don't mean it literally when writing something like this but I still wonder why such self-evident ideas as \"make things easy to use correctly and hard to use incorrectly\" are framed in terms of \"idiots who don't rtfm\". The best documentation is what wasn't written because it (actually!) wasn't needed. On the other hand, even if people aren't \"idiots\", they still make mistakes and take time to figure out (perhaps by reading tfm) how to do things and complete their tasks, all of which has a cost. Making this easier is a clear benefit. reply s4i 8 hours agorootparentThe bigger the codebase, we all eventually find ourselves in scenarios where we were the idiot. Making the APIs as foolproof as possible, utilizing tools like Semgrep, deprecating stuff in the way your language supports that it shows up in the IDE,… all that stuff should be utilized. reply virgilp 11 hours agoparentprevRelevant xkcd: https://xkcd.com/927/ reply bandrami 13 hours agoparentprev\"Now we have five inconsistent coding conventions...\" reply mjr00 20 hours agoprev> The other reason is that you cannot split up a large established codebase without first understanding it. I have seen large codebases successfully split up, but I have never seen that done by a team that wasn’t already fluent at shipping features inside the large codebase. You simply cannot redesign any non-trivial project (i.e. a project that makes real money) from first-principles. This resonates. At one former company, there was a clear divide between the people working on the \"legacy monolith\" in PHP and the \"scalable microservices\" in Scala/Go. One new Scala team was tasked with extracting permissions management from the monolith into a separate service. Was estimated to take 6-9 months. 18 months later, project was canned without delivering anything. The team was starting from scratch and had no experience working with the current monolith permissions model and could not get it successfully integrated. Every time an integration was attempted they found a new edge case that was totally incompatible with the nice, \"clean\" model they had created with the new service. reply newaccountman2 16 hours agoparentAm I naive for thinking that nothing like that should take as long as 6-9 months in the happy case and that it's absurd for it to not succeed at all? reply 000ooo000 11 hours agorootparentYou know so little about the team, the organisation, the codebase, other concurrent obligations (e.g. prod support), and the way the project is run. The only way I can imagine one having confidence in a statement like \"nothing should take that long\" is naïveté. reply franktankbank 31 minutes agorootparentThen maybe 18 months wasn't too long and they should have been given more time. But seriously? reply mjr00 11 hours agorootparentprevIt really depends. Honestly 6-9 months would have been an optimistic estimate even if it were 2-4 devs intimately familiar with the existing codebase. Permissions is a very cross-cutting concern, as you might imagine, and touched a huge amount of the monolith code. A big problem was that permissions checks weren't done in a consistent layer, instead scattered all over the place, and the team responsible for the rewrite, being new to the code, was starting from scratch and finding these landmines as they went. Scoping was also unclear and kept changing as the project went along, at first to pull in more scope that was peripherally related, then to push stuff out of scope as the project went off track. And during all these changes you have to keep the existing auth system working with zero downtime. The devs were good developers, too! Two people on the team went off to Google after, so it's not like this was due to total incompetence or anything; more just overambition and lack of familiarity with working on legacy code. reply lkjdsklf 3 hours agorootparentprevAt a large enterprise, 6-9 months is blazingly fast. Everything takes longer than you think and this sounds like it involves at least 2 teams (the php team and the scalar team). Every team you include increases time line factorially in the best case. It takes a lot of time to have meetings with a dozen managers to argue over priority and whatever. Especially since their schedules are full of other arguments already reply franktankbank 30 minutes agorootparent> the php team and the scalar(*scala) team LOL why is this two teams!? reply Aeolun 13 hours agorootparentprevMaybe. There's a lot of dragons hidden inside enterprise code. Only if you know all of them can you really succeed the first time around. reply tuyiown 7 hours agorootparentprevauthorization and access control is an awfully difficult problem, as soon as you derive from user defined ACLs on all persisted objects and user/groups based granting data. Each access can have an arbitrary rule that must be evaluated with all dependant data, that will end up being anything in the persisted data. How to you make this long rule list maintainable, without redundancy, ensuring that changing re-used rules won't introduce regressions on all call sites ? reply yid 11 hours agorootparentprev> Am I naive for thinking that nothing like that should take as long as 6-9 months in the happy case and that it's absurd for it to not succeed at all? Bluntly, yes. And so is every other reply to you that says \"no this isn't naive\", or \"there's no reason this project shouldn't have finished\". All that means is that you've not seen a truly \"enterprise\" codebase that may be bringing in tons of business value, but whose internals are a true human centipede of bad practices and organic tendrils of doing things the wrong way. reply arkh 8 hours agorootparent> whose internals are a true human centipede of bad practices and organic tendrils of doing things the wrong way Currently there. On one hand: lot of old code which looks horrible (the \"just put comments there in case we need it later\" pattern is everywhere). Hidden scripts and ETL tasks on forgotten servers, \"API\" (or more often files sent to some FTP) used by one or two clients but it's been working for more than a decade so no changing that. On the other: it feels like doing archeology, learning why things are how they are (politics, priority changes over the years). And when you finally ship something helping the business with an easier to use UI you know the effort was not for nothing. reply tuyiown 7 hours agorootparentprevIf you find me any resources to build access control on arbitrary (I mean it, arbitrary) rules the right way, I would be very very (very) glad. reply baq 11 hours agorootparentprevAuthz can make the most otherwise trivial features into a depressing journey in the valley of edge cases. reply murkt 16 hours agorootparentprevNo, you’re not naive. If it’s done by one-two people that know what they’re doing, it should be done much faster. If it’s a big new team that doesn’t know what they’re doing, working separately from existing codebase, with lots of meetings… I see no reason why it would finish at all. reply cratermoon 20 hours agoparentprevI worked at a company that had a Rails monolith that underwent similar scenario. A new director of engineering brought in a half dozen or of his friends from his previous employer to write Scala. They formed up a clique and decide Things Were Going to Change. Some 18 months and 3 projects later, nothing they worked on was in production. Meanwhile the developer that was quietly doing ongoing maintenance on the monolith had gradually broken out some key performance-critical elements into Scala and migrated away from the Ruby code for those features. Not only had it gone into production, it made maintenance far easier. reply mjr00 19 hours agorootparent> Meanwhile the developer that was quietly doing ongoing maintenance on the monolith had gradually broken out some key performance-critical elements into Scala and migrated away from the Ruby code for those features. Yep and that's what I've seen be successful: someone who really knows the existing code inside and out, warts and all, needs to be a key leader for the part being broken out into a separate system. The hard part isn't building the new system in these projects, it's the integration into the existing system, which always requires a ton of refactoring work. reply cratermoon 15 hours agorootparent> needs to be a key leader for the part being broken out into a separate system Indeed, the developer was one of the best programmers I've known and absolutely the key person on the system. The New Guys Clique were the sort of developers, you might know some, who come in, look at the existing systems, decide it's all wrong and terrible, and set out to Do It Right. reply usefulcat 14 hours agorootparentI've seen almost this exact scenario play out, although in my case it was just one person as opposed to a clique. He had just come from a much larger company in the same business, and almost right away he proposed that we should rearchitect a significant portion of our software to match the way things were done at his previous employer. His proposed architecture wasn't without elegance, but it was also more complex and, more importantly, it didn't solve any problems that we actually had. So in the end it was more of an ideological thing. He seemed to take it personally that we didn't take him up on his proposal, and he left a few months later (and went back to his previous employer, though to a different group). Don't think he was around for even a year. He wasn't young either; he was pretty experienced. reply photonthug 18 hours agorootparentprevEven if you’re generally suspicious of so called best practice/ design patterns / Martin Fowlerisms.. this is a time for the strangler approach. (Parent and siblings are already talking about the same idea without naming it.) Rewrites from scratch never work with sufficiently large systems, and anyone that’s been involved with these things should be savvy enough to recognize this. The only question is around the exact definition of sufficiently large for a given context. https://en.m.wikipedia.org/wiki/Strangler_fig_pattern reply mjr00 17 hours agorootparentA similar, more concrete approach is parallel implementations, as written about by John Carmack[0]. I suppose the main difference is that parallel implementation has you very explicitly and intentionally leave the \"old stuff\" around until you're ready to flip the switch. I've used this approach in large-scale refactorings very successfully. One of the benefits is that you get to compare the new vs old implementation quickly and easily, and it lets you raise questions about the old implementation; every time I've done this I've found real, production-impacting bugs because the old system was exhibiting behaviors that didn't match the new system, and it turned out they weren't intentional! [0] http://sevangelatos.com/john-carmack-on-parallel-implementat... reply Tknl 15 hours agorootparentI second this approach. I've utilized it successfully in an ongoing migration. I also second the need to have engineering knowledge from the previous system available. I was lead for 5 years on a previous system before being tasked as solution architect of new total rewrite to cloud native. The hardest part of such a migration is maintaining sponsor buy-in while you build the parallel run evaluation strangler fig integration with the old system and get some existing flow polished enough to take live. If you happen to have a rule or scripting system in place piggy pack off of it so you can do an incremental migration. reply harrall 19 hours agorootparentprevAlso an issue is that the director attempted a full rewrite as a separate project. You can do successful rewrites but your rewrite has to be usable in production within like a month. If you don’t know how to achieve that, don’t even try. The quiet developer was able to get their own rewrite done because they understood that. Looks like the director of engineering showed some classic inexperience. You can tell when someone has done something before and when it’s their first time. reply simoncion 5 hours agorootparent> You can do successful rewrites but your rewrite has to be usable in production within like a month. I strongly disagree with this, and it reminds me of one of the worse Agile memes: \"With every commit, the product must be production-ready.\". [0] The rewrite has to be generally not behind schedule. Whatever that schedule is is up to the folks doing the work and the managers who approve doing the work. [0] I've worked for an Agile shop for a long time, so please don't tell me that I'm \"Doing Agile Wrong\" or that I misunderstand Agile. \"No True Scotsman\" conversations about Agile are pretty boring and pointless, given Agile's nebulous definition. reply harrall 2 hours agorootparentOK so you're actually right, but the actual criteria of \"whether you can do this\" depends on a lot of factors from the project to the people. But there's no way to really describe it. It's like explaining to somehow how to parallel park or do a kickflip... you can only explain it so much. I like to say \"it should be usable in production soon\" because it's generally a good approximation that takes into account what you might have to work with. It's an upgrade from advice like Joel's who just say \"IT NEVER WORKS\" reply cratermoon 1 hour agorootparentprevIf the schedule is three years, and in the meantime the product being rewritten isn't getting maintenance, the company might as well go ahead and fold and save everyone pain and disappointment. https://www.joelonsoftware.com/2000/04/06/things-you-should-... reply cratermoon 15 hours agorootparentprev> a full rewrite as a separate project. And it was never constrained to rewriting the existing system. The rewrite plan was motivated by the entirely reasonable desire to make further improvements possible, an additional mistake was the attempt to add major improvements as part of the rewrite. The new guys made their disdain for the existing system obvious, to the extent that their intent for the rewrite ballooned into a ground-up rebuild of everything. Things You Should Never Do, Part I: https://www.joelonsoftware.com/2000/04/06/things-you-should-... reply LAC-Tech 18 hours agorootparentprevliterally what I wanted to do as an opinionated junior reply LAC-Tech 18 hours agorootparentprevThis is the way. You absolutely can turn shit codebases into something nicer and faster, and this is best done by people who know the system, and maybe even more important, knows the operational context the system exists in. I once came into an old codebase like this as a junior, thinking I can start again. And I was gently but firmly told by my boss that it wouldn't work, this software is crucial to operations that support all our revenue, and while it can be improved it has to keep working. And we improved the hell out of it. reply zzbzq 1 hour agoprevWrong, wrong. Opposite of everything he said. All his examples are backwards. The article is basically inversing the Single Responsibility Principle. First of all, consistency does not matter at all, ever. THat's his main thesis so it's already wrong. Furthermore, all his examples are backwards. If you didn't know the existence of \"bot\" users, you probably don't want your new auth mechanism to support them. Otherwise, the \"nasty surprise\" is the inverse of what he said: not that you find you don't support bot users, but you find out that you do. Build stuff that does exactly what you want it to do, nothing more. This means doing the opposite of what he said. Do not re-use legacy code with overloaded meanings. reply pablobaz 19 hours agoprevIn my experience with very large codebases, a common problem is devs trying to improve random things. This is well intentioned. But in a large old codebase finding things to improve is trivial - there are thousands of them. Finding and judging which things to improve that will actually have a real positive impact is the real skill. The terminal case of this is developers who in the midst of another task try improve one little bit but pulling on that thread leads to them attempting bigger and bigger fixes that are never completed. Knowing what to fix and when to stop is invaluable. reply ninalanyon 3 hours agoparent> common problem is devs trying to improve random things. Been there, been guilty of that at the tail end of my working life. In my case, looking back, I think it was a sign of burnout and frustration at not being able to persuade people to make the larger changes that I felt were necessary. reply jimbokun 17 hours agoparentprevWhich can lead to trying to rewrite Netscape Navigator from scratch and killing the company: https://www.joelonsoftware.com/2000/04/06/things-you-should-... reply Kinrany 10 hours agoparentprevDo you think boyscouting, \"leave it better than you found it\" is misguided as well? reply bricestacey 2 hours agorootparentI do not believe in \"boyscouting\". I think if you want to leave it better, make a ticket and do it later. Tacking it on to your already planned work is outside the scope of your original intent. This will impact your team's ability to understand and review your changes. Your codebase is unlikely to be optimized for your whimsy. Worse though is when a reviewer suggests boyscouting. I've seen too many needless errors after someone happened to \"fix a tiny little thing\" and then fail to deliver their original task and further distract others trying to resolve the mistake. I believe clear intention and communication are paramount. If I want to make something better, I prefer to file a ticket and do it with intention. reply bogdan 8 hours agorootparentprevI always took it as \"leave it better than you found it\" across the files that I've been working on (with some freedom as long I'm on schedule). My focus is to address the ticket I'm working on. Larger improvements and refactorings get ticketed separately (and yes, we do allocate time for them). In other words, I don't think it's misguided. reply Kon5ole 3 hours agoprevSometimes the right approach is to keep the consistency. Other times, that approach is either impossible or catastrophic. IMO software development is so diverse and complex that universal truths are very very rare. But to us programmers, anything that promises to simplify the neverending complexity is very tempting. We want to believe! So we're often the equivalent of Mike Tyson reading a book by Tiger Woods as we look down a half-pipe for the first time. We've won before and read books by other winners, now we're surely ready for anything! Which leads to relational data stored in couchDB, datalayers reimplemented as microservices, simple static sites hosted in kubernetes clusters, spending more time rewriting tests than new features, and so on. IMO, most advice in software development should be presented as \"here's a thing that might work sometimes\". reply Animats 13 hours agoprev> Single-digit million lines of code (~5M, let’s say) > Somewhere between 100 and 1000 engineers working on the same codebase > The first working version of the codebase is at least ten years old That's 5,000 to 50,000 lines of code per engineer. Not understaffed. A worse problem is when you have that much code, but fewer people. Too few people for there to be someone who understands each part, and the original authors are long gone. Doing anything requires reverse engineering something. Learning the code base is time-consuming. It may be a year before someone new is productive. Such a job may be a bad career move. You can spend a decade learning a one-off system, gaining skills useless in any other environment. Then it's hard to change jobs. Your resume has none of the current buzzwords. This helps the employer to keep salaries down. reply atmavatar 12 hours agoparent> A worse problem is when you have that much code, but fewer people. Too few people for there to be someone who understands each part, and the original authors are long gone. Maybe. I spent most of my career at a small mom and pop shop where we had single-digit MLOC spanning 20-25 years but only 15-20 engineers working at any given time. This wasn't a problem, though, because turn-over was extremely low (the range in engineer count was mostly due to internships), so many of the original code owners were still around, and we spent some effort to spread out code ownership such that virtually all parts were well understood by at least 2 people at any given moment. If anything, I rather shudder at the thought of working somewhere that only has ~5M lines of code split up amongst 100 (and especially 1000) engineers over a span of 10 years. I can't imagine producing only 5-50 KLOC over that time, even despite often engaging in behind-the-scenes competition with colleagues over who could produce pull requests with the least (and especially negative) net LOC. > Your resume has none of the current buzzwords. That's one of my bigger pet peeves about software development, actually. While you probably didn't mean it this way, over the years, I encountered a number of people who'd consistently attempt to insert fad technologies for the sake of doing so, rather than because they actually provided any kind of concrete benefit. Quite the contrary: they often complicated code without any benefit whatsoever. My more experienced colleagues snidely referred to it as resume-driven development. I can't hate people doing this too much, though, because our industry incentivizes job hopping. reply tsarchitect 3 hours agoparentprevLOC is 'not a good' metric to 'you should be able to understand a codebase'. In either scenario, too many people or too few people, or (my favorite) 'not enough' (whatever that means). Mythical Man-Month comes to mind. What I think you're trying to get at is you need skill to reverse engineer software. And even if you have that skill it takes time (how much?). We work in a multifaceted industry and companies need to build today. At any given project, the probabilities are small that there is a dev who has the skill. We all know 'they can do it/they can learn on the job/they'll figure it out'. And then OP's observation comes into fruition. reply codingdave 8 hours agoparentprevBeing able to navigate and reverse engineer undocumented legacy code in a non-modern stack is a skill set in and of itself. Most people don't enjoy it in the slightest, so being one of the few devs who does means that I have been able to take on the gnarly legacy problems nobody else will touch. It might not build buzzwords on my resume, which does limit using this particular aspect of dev work to get an initial call back on jobs. But it absolutely exposes me to a variety of ways of doing things and expands my skills in new directions, and that expanded perspective does help in interviews. You lost me on how this helps employers keep salaries down. My value is greater by being able to do such things, not less. If I can work on modern stacks, legacy stacks, enterprise platforms, and am willing to learn whatever weird old tech you have, that does not decrease my salary. reply lmm 17 hours agoprevI've worked in codebases like this and disagree. Consistency isn't the most important, making your little corner of the codebase nicer than the rest of it is fine, actually, and dependencies are great - especially as they're the easiest way to delete code (the article is right about the importance of that). What's sometimes called the \"lava layer anti-pattern\" is actually a perfectly good way of working, that tends to result in better systems than trying to maintain consistency. As Wall says, the three cardinal virtues of a programmer are laziness, impatience, and hubris; if you don't believe you can make this system better then why would you even be working on it? Also if the system was actually capable of maintaining consistency then it would never have got that large in the first place. No-one's actual business problem takes 5M lines of code to describe, those 5M lines are mostly copy-paste \"patterns\" and repeated attempts to reimplement the same thing. reply edanm 6 hours agoparent> No-one's actual business problem takes 5M lines of code to describe, those 5M lines are mostly copy-paste \"patterns\" and repeated attempts to reimplement the same thing. I'm pretty sure this is trivially untrue. Any OS is probably more than 5M lines (Linux - 27.8 lines according to a random Google Search). Facebook is probably more lines of code. Etc. reply lmm 4 hours agorootparent> Any OS is probably more than 5M lines (Linux - 27.8 lines according to a random Google Search). Linux is notoriously fragmented/duplicative, and an OS isn't the solution to anyone's actual business problem. A well-factored solution to a specific problem would be much smaller, compare e.g. QNX. > Facebook is probably more lines of code. IIRC Facebook is the last non-monorepo holdout among the giants, they genuinely split up their codebase and have parts that operate independently. Does Facebook have more than 5M lines of code now? I'm sure they do. Does that actually result in a better product than when it was less than 5M lines of code? Ehhhh. Certainly if we're talking about where the revenue is being generated, as the article wants to, then I suspect at least 80% of it is generated by theSo I mean yeah, on some level solving the business problem can take as many lines as you want it to, because it's always possible to add some special case enhancement for some edge case that takes more lines. But if you just keep growing the codebase until it's unprofitable then that's not actually particularly valuable code and it's not very nice to work on either. I think this misunderstands how the companies that have stayed in business for so long have done so. Excel is the software we all use every day because it kept adding more and more features, stealing the best ideas from new products that tried to innovate. It's still doing so, though obviously to a lesser extent. reply TheBigSalad 5 hours agoparentprevI work on a 5M+ line code base. It's not copy/paste or the same problems solved in different ways. It's a huge website with over 1K pages that does many, many things our various clients need. reply jimbokun 17 hours agoparentprevPulling in lots of dependencies will eventually grind progress on features to a halt as you spend more and more time patching and deploying vulnerabilities. The probability of seeing new vulnerabilities I believe is pretty much linear in the number of dependencies you have. reply abc-1 16 hours agorootparentAs opposed to your in-house code which is vulnerability free? The issue isn’t vulnerability's, it’s dependency hell where all your packages are constantly fighting each other for specific versions. Although some languages handle this better than others. reply jimbokun 13 hours agorootparentIn house code could very well have many fewer vulnerabilities, as you only write exactly the functionality you need, vs pulling a large dependency and only using a small percentage of the API. reply lmm 9 hours agorootparent> pulling a large dependency and only using a small percentage of the API. This is normally a direct result of trying to limit the number of dependencies. People are much more able to use small, focused dependencies that solve specific problems well if you have a policy that permits large numbers of dependencies. reply devnullbrain 7 hours agoparentprev>making your little corner of the codebase nicer than the rest of it is fine, actually As TFA points out, you might find out that you've made your little corner worse, actually. reply dktoao 2 hours agoprevThis is good advice but only it has been followed from the beginning and consistently throughout the development of the original code. It is applicable to large organizations with lots of resources who hire professional developers and have a lot of people who are familiar with the code that are active in code reviews and have some minimum form of documentation / agreement on what the logic flow in the code should look like (the article does not claim otherwise). But I would implore those who work at the 80% of other companies that this advice is nearly useless and YMMV trying to follow it. The one thing that I think is universally good advice is to try and aggressively remove code whenever possible. reply IvyMike 21 hours agoprevThe \"The cardinal mistake is inconsistency\" is 100% true. We used to call the guiding philosophy of working in these codebases \"When in Rome\". reply pryelluw 21 hours agoparentI have this bad codebase at work. Really bad. One of the things I’ve been working on for the past two years is making it consistent. I’m almost at the point where interfaces can be left alone and internals rewrites in a consistent style. People often ask why I hardly ever have any prod issues (zero so far this year). This is part of the reason. Having consistent codebases that are written in a specific style and implement things in similar manner. Some codebases make me feel like I’m reading a book in multiple languages … reply bizzletk 20 hours agorootparent> People often ask why I hardly ever have any prod issues (zero so far this year). It also helps that we're still only in January! reply bballer 18 hours agorootparentBahh thanks for the chuckle. The man is 7/7 as of today! reply cyco130 20 hours agorootparentprev> zero so far this year I saw what you did there. reply onemoresoop 20 hours agorootparentMaybe that’s not even that bad if number of issues went down from multiple a day to none in a couple of days. reply onemoresoop 20 hours agorootparentprev> Some codebases make me feel like I’m reading a book in multiple languages … In most cases the codebase does consist of muliple languges. reply SoftTalker 21 hours agoparentprevYep when I have to work on old code I find something in the existing code that's close to what I want to do, and copy/paste it. I do not try to abstract it into a common function, unless that's already been done and can be used verbatim. You don't know the 10 years of reasons behind why the code is the way it is, and the safest thing is to stay as close as possible to how the existing code is written, both to avoid landmines, and so that future you (or someone else) has one less peculiar style they have to figure out. All that said, the more usual case is that the code is already a huge mess of different styles, because the 100 different developers who have touched it before you didn't follow this advice. reply criddell 7 hours agorootparentDo you ever find yourself taking an existing function and adding another parameter to it? The benefit is that you don’t break existing code. The problem is that the function is now more complicated and likely now does more than one thing because the extra parameter is effectively a mode switch. reply jimbokun 16 hours agorootparentprevThe only time I abstract something into a function even if it’s only used in one place, is if the generic case is as easier or easier than the code for the specific case. But that’s a judgment call based on experience. reply aranw 20 hours agoparentprevI don't like this philosophy as it often leads to stagnation in patterns and ways of working that seep into newer systems. \"That's not how we do things here\" becomes a common criticism, resulting in systems and services that share the same flaws and trade-offs, making progress difficult. Engineers often adhere too rigidly to these principles rather than taking a pragmatic approach that balances existing practices with future improvements. reply djeastm 20 hours agorootparent>improvements Therein lies the rub. Everyone has a different idea of what is an improvement in a codebase. Unless there's some performance or security concern, I'd much rather work in an \"old\" style codebase that's consistent than a continually partially updated codebase by multiple engineers with different opinions on what an \"improvement\" is. reply mrkeen 10 hours agorootparent> Everyone has a different idea of what is an improvement in a codebase Yes, and consistency is the tie-breaker. So the status quo remains, and improvements aren't made. reply peterldowns 20 hours agorootparentprevI completely agree with this. reply rstuart4133 20 hours agorootparentprev> I don't like this philosophy as it often leads to stagnation in patterns and ways of working that seep into newer systems. The rule isn't \"don't introduce change\", it's \"be consistent\". Using the example from the post, if you want to use a different method of doing auth that simpler the \"be consistent\" rule means you must change the way auth is done everywhere. Interestingly, if you do that the negatives he lists go away. For example, if the global auth mechanism handles bots specially, you will learn that if you are forced to change it everywhere. reply Cthulhu_ 20 hours agorootparentprevAnd that's a fair criticism, however, if you change a pattern without changing it everywhere, you now have two patterns to maintain (the article mentions this). And if multiple people come up with multiple patterns, that maintenance debt multiplies. Progress and improvement is fine, great even, but consistency is more important. If you change a pattern, change it everywhere. reply rudasn 19 hours agorootparentChange it at once everywhere on an existing large codebase? That's going to be one huge PR no one will want to review properly, let alone approve. Document the old pattern, document the new pattern, discuss, and come up with a piece by piece plan that is easy to ship and easy to revert if you do screw things up. Unless the old pattern is insecure or burns your servers, that is. reply dml2135 14 hours agorootparentI don’t think you and the comment you are replying to are in conflict. Documenting and rolling it out piecemeal is the correct way to make a large change. I think the point is, either actually commit to doing that, or don’t introduce the new pattern. reply cowsandmilk 18 hours agorootparentprevIf things are consistent enough, tools like open rewrite can be used. I’ve seen reviews where it is the recipe for generating the code transformation that gets reviewed, not the thousands of spots where the transform was applied. reply not2b 19 hours agorootparentprevYes, it usually can't be done with one massive checkin. First get buy-in that the old pattern will be changed to the new pattern (hopefully you won't have some senior developer who likes it the old way and fights you), then as you say, come up with a plan to get it done. The down side to this that I've experienced more than once, though, is incomplete conversions: we thought we had agreement that the change should be done, it turns out to be more difficult than planned, it gets partially completed and then management has a new fire for us to fight, resources are taken away, so you still have two or more ways of doing things. reply yuliyp 19 hours agorootparentprevNaturally unless it's trivial do it in steps, but committing to doing the whole migration as quickly as prudent or rolling it back completely is key. reply cratermoon 19 hours agorootparentprevWhat's bad is code exhibiting multiple fragmentary inconsistencies, and no plan or effort exists to bring older code up to match the new patterns. An example I was closely involved with: A java programmer who wrote a plethora of new code in a pure functional paradigm, scattering Vavr library uses all over it. The existing code was built on Dropwizard and any experienced Java programmer could rapidly get comfortable with it. The difference between the existing code and the new was jarring (sorry for the pun) to say the least, and I wonder if, later, the company ever managed to recruit anyone who understood both well enough to maintain the system. ETA: upon reflection I'd consider that programmer a canonical example of the kinds of mistakes the author covers in the article. reply gwillz 20 hours agoparentprev\"When is Rome\" is good, might use that. My old boss used to say: \"Be a chameleon. I don't want to know that I didn't write this.\" reply layer8 20 hours agoparentprevHow do you tackle the case where the codebase is consistent in a bad way, like pervasive use of antipatterns that make code difficult to change or to reason about? If you want to improve that, you have to start somewhere. Of course, Chesterton’s Fence applies. reply viraptor 19 hours agorootparentOr when people keep the old pattern because there's a \"higher priority\". I've worked on a project with absolutely terrible duplication of deserialisers of models, each one slightly different even those most properties were named the same and should've been handled the same. But we can't touch anything because important things are happening in business and we can't risk anything. The ignored part was that this led to bugs and extreme confusion from new people. They were even too worried to accept a repo-wide whitespace normalisation. reply jimbokun 16 hours agorootparentIn the past I have performed needed refactorings as part of new feature development, without asking permissions. Even though my boss at the time said “don’t make it pretty, just make it work.” Of course, I knew that writing the code the “pretty”, more maintainable, easier to understand way wouldn’t take any longer to write, and might take less time as the refactoring requires less new code overall. But I didn’t bother explaining all that. Just nodded then went and implemented it the best way as I was the experienced software engineer. reply dav 16 hours agoprevI have three maxims that basically power all my decisions as an engineer: 1. The three C’s: Clarity always, Consistency with determination, Concision when prudent. 2. Keep the pain in the right place. 3. Fight entropy! So in the context of the main example in this article, I would say you can try to improve clarity by e.g. wrapping the existing auth code in something that looks nicer in the context of your new endpoint but try very hard to stay consistent for all the great reasons the article gives. reply ctxc 5 hours agoparentIt's got a nice ring to it :) reply protonbob 21 hours agoprevI don't have a real critique because I don't have that many years in a codebase the size of OP (just 2). But I struggle with the advice to not try and make a clean section of the code base that doesn't depend on the rest of the application. Isn't part of good engineering trying to reduce your dependencies, even on yourself? In a latter part of the post, OP says to be careful tweaking existing code, because it can have unforeseen consequences. Isn't this the problem that having deep vertical slices of functionality tries to solve? High cohesion in that related code is grouped together, and low coupling in that you can add new code to your feature or modify it without worrying about breaking everyone else's code. Does this high cohesion and low coupling just not really work at the scale that OP is talking about? reply gleenn 21 hours agoparentIt's one thing to reduce dependency and another to have reduced consistency. If you have 10 web routes and 1 behaves differently, it doesn't matter if the code is cross coupled or not, it matters if it behaves similarly. Does it return the same status codes on error? Does it always return JSON with error messages inside? Do you auth the same way? The implementation can be wholly separate but end users will notice because logic on their side now has to special-case your 11th endpoint because you returned HTTP 20x instead of 40x on error. Or when you realize that you want to refactor the code to DRY it (Don't Repeat Yourself), now you can't reduce all the duplication because you have bespoke parts. reply mbivert 20 hours agoparentprevI think the gist of it is humility: as a newcomer, you don't really know what's out there and why, and there are often good reasons for things to be how they are. Not always, but often enough for avoiding being too original to be favored. This doesn't imply relinquishing on \"good engineering habits\" either. Now, once you have a deeper understanding of the codebase, you'll know when and why to break away from existing patterns, but in the beginning phase, it's a good habit to start by learning carefully how things are designed and why. reply Salgat 20 hours agoparentprevConsistency makes code predictable and reduces mental overhead. It doesn't mean you have to write it poorly like the rest of the codebase, but it does mean using the same general practices as the rest of the codebase. Think of it like using knockoff legos vs the real thing. They both work interchangeably which makes it easy to use them together, but you'd prefer to use the nicer lego pieces as much as possible in your builds because the material is higher quality, tighter tolerances, just overall works better even if it's the same shape as the knockoff pieces. reply Cthulhu_ 20 hours agoparentprevThese are two different concepts though; reducing dependencies is good, but you can have minimal dependencies AND have the code look / feel / behave like the rest of the codebase. Always remember, it's not your code. Assume someone else will need to read / maintain it. Thousands might. You might have made the perfect section of code, then get an offer you can't refuse or get hit by a bus. reply mrkeen 20 hours agoparentprevNope, you've got it. Code-consistency is a property just like any other property, e.g",
    "originSummary": [
      "Working in large, established codebases, often with millions of lines of code and involving hundreds of engineers, presents significant challenges for software engineers. - A common mistake is inconsistency, where engineers ignore existing code patterns and implement features in isolation, which can lead to maintenance issues and hinder future improvements. - To effectively work within large codebases, engineers should research existing patterns, understand the codebase's production impact, be cautious with dependencies, and carefully remove unnecessary code to maintain quality and value."
    ],
    "commentSummary": [
      "Engineers in large, established codebases often encounter challenges with inconsistency and undocumented practices, necessitating a focus on personal code quality and documentation. - Balancing improvements with maintaining local consistency is key, especially when the existing codebase is inconsistent, while engaging with the wider team can help align practices. - The ultimate goal is to enhance the codebase's functionality and maintainability, requiring pragmatism and adaptation to existing patterns."
    ],
    "points": 683,
    "commentCount": 303,
    "retryCount": 0,
    "time": 1736282675
  },
  {
    "id": 42627453,
    "title": "Magic/tragic email links: don't make them the only option",
    "originLink": "https://recyclebin.zip/posts/annoyinglinks/",
    "originBody": "Magic/Tragic Email Links: Don’t make them the only option 2025-01-07Guillaume Ross The term “Magic Links” once meant a futuristic PDA. Nowdays, companies like Auth0 use it to refer to the slightly-magical feat of including a login link in an email. Last week, the great website you should subscribe to if you haven’t already (it’s great, when you’re not logged out), 404 Media, posted “We Don’t Want Your Password” in defense of so-called magic links. Of course, as stated in the article, such email links are harder to phish than passwords, can’t lead to a breach of passwords, and protect the site itself against users who might reuse passwords previously compromised. The article even covers some of my annoyances with this system, but throws out this sentence: We find this to be a much easier login process and wish it was more common across the web where appropriate. Easier than what? Easier than a long password, without a password manager? Easier than a passkey? Easier than an OTP sent to the same email address? This sentence reads to me as one written by someone mostly working and living from a single laptop and mobile device. The second part of the sentence, calling for more sites to do this is why I am writing this. For any scenario with a minimal amount of complexity, like users with multiple computers, and you’re looking at a scenario where the site’s unwillingness to deal with other login methods shoves friction on the end-user. What makes them tragic:# Multiple devices. Who doesn’t use at least a few computers weekly? I don’t have my email on my gaming PC, nor do I have it on my work laptops. Slower. From 2 seconds slower to minutes slower, depending on SMTP delays as well as how awkward it is to get the link to the right browser. Anti-mobile. As mentioned by 404 in their own article, this breaks the ability to use in-app browsers, which is quite annoying especially for RSS reader type apps. It makes interacting with any local link in the RSS feed extremely annoying. Indirect security downsides. Pushing people to access personal email on work devices (or vice-versa) isn’t exactly a win for security. Another annoying passwordless system is to email or SMS an OTP the end user can type in. While this sucks, it at least allows you to easily log in in situations where you don’t have a clear and easy copy/paste path from the email client to the browser you want to log in to. Stratechery, powered by Passport, uses this type of scheme (click link OR type in OTP), which is still shifting annoyances onto end-users to free developers from implementing passkeys, but at least has a bit more of an appreciation for end-users. If you insist on using magic/tragic links by default, at least consider offering a robust alternative, such as passkeys, especially if your audience is technical and privacy-focused. Update: This great post by Ricky Mondello was pointed out to me and explains how passkeys can make this better. I highly recommend reading it. Magic Links Have Rough Edges, but Passkeys Can Smooth Them Over Read other posts The Verge: Subscription and Trackers",
    "commentLink": "https://news.ycombinator.com/item?id=42627453",
    "commentBody": "Magic/tragic email links: don't make them the only option (recyclebin.zip)653 points by gepeto42 21 hours agohidepastfavorite459 comments WaitWaitWha 5 hours agoJust a pet peeve with passkeys (and other authN) that presses users towards biometrics - In the US, because the Fifth Amendment Self-Incrimination Clause, passwords cannot be demanded. Passwords are testimonial evidence. [United States v. Hubbell (2000); re Grand Jury Subpoena Duces Tecum (11th Cir. 2012)] Biometrics on the other hand are not. The court ruled that a defendant could be compelled to unlock a phone with biometrics because it is not testimonial. [Commonwealth v. Baust (Virginia, 2014); State v. Diamond (Minnesota, 2017)] Basically, passwords cannot be compelled to be disclosed, while biometrics can. There is similar legal stance in Canada, UK, Australia, India, Germany, and Brazil to name a few. Finally, under duress, passwords can be held, while biometrics cannot, without self harm. reply philipwhiuk 4 hours agoparent> There is similar legal stance in Canada, UK, Australia, India, Germany, and Brazil to name a few. There is not a similar stance in the UK. You can be compelled to provide a password. Section 49 of the Regulation of Investigatory Powers Act 200 (RIPA and let that doublespeak sink in a second) allows the police to compel it subject to a warrant from a judge. The sentence (subject to sentencing guidelines) is up to two years in prison or 5 years for national security / child indecency cases. You can claim you don't remember/know it as a defence, but in most cases that's not going to be believed by a jury. In theory once you got out you could be re-served with the notice and face another 2-5 years. Rinse and repeat. reply BoxOfRain 1 hour agorootparentWhat happens in the case of plausibly-deniable keys? Say someone has an encrypted drive with a hidden volume, one key decrypts decoy files and one decrypts the true files. If the person gives up the key to the decoy files, is the onus on the prosecution to prove additional keys exist or on the defence to prove they don't? reply philipwhiuk 26 minutes agorootparentNot a lawyer but I expect it would be on prosecution to convince a jury that they had failed to make \"a disclosure of any key to the protected information that is in his possession\" as per RIPA 2000 Section 50, 2 a) To do this, they'd likely need some evidence to persuade the jury, beyond reasonable doubt, that the encryption system had such a feature. reply hiatus 3 hours agorootparentprev> In theory once you got out you could be re-served with the notice and face another 2-5 years. Rinse and repeat. Is there no concept of double-jeopardy in UK jurisprudence? reply jetpackjoe 3 hours agorootparentNot from the UK and not a lawyer, but if a new warrant was served, then not providing the password would be a new offense and double jeopardy would not apply reply moffkalast 1 hour agorootparentprevThe UK always surprises with how close their reality is to V for Vendetta. reply jesseendahl 1 hour agoparentprevYou can’t unlock your iPhone with biometrics at first boot, and holding down the two side buttons will make it so your phone immediately disables biometric unlock, and instead requires your passcode for the next unlock. But none of this has much to do with the biometric auth you do with passkeys, because passkeys are used in places passwords would be used — logging into apps and websites. Which you see only doing when your device is already unlocked and you are actively using it. reply latexr 8 minutes agorootparent> holding down the two side buttons will make it so your phone immediately disables biometric unlock Also pressing the lock button five times in a row. reply hirvi74 12 minutes agorootparentprevYou can also quickly press the lock button 5 times and then your iPhone won't unlock with Face ID until a passcode is entered. reply injidup 4 hours agoparentprevBiometrics then need a mix of non-testimonal and testimonial input. ie it only unlocks when it sees it is your face and your face blowing a rasberry. Can you be compelled to blow a raspberry? reply HWR_14 3 hours agorootparentIn the US, the answer would be yes you can be compelled to blow a raspberry. reply joncrocks 2 hours agoparentprev> There is similar legal stance in Canada, UK, Australia, India, Germany, and Brazil to name a few. In the UK the Regulation of Investigatory Powers Act (RIPA) makes it a criminal offence to not divulge a password if compelled via a RIPA notice. https://www.legislation.gov.uk/ukpga/2000/23/section/53 reply joering2 1 hour agorootparentI wonder what would happened if you willingly keep providing a wrong password. The possibility of your device malfunctioning IS and always will be > 0. Can the judge really throw you, and re-throw you multiple times to jail because the password you keep providing did not work? reply gepeto42 5 hours agoparentprevWhat I'd recommend is if you're worried about this (or worried about it in certain instances), disable biometrics to unlock the device itself. Then, passkeys on it don't really matter anymore. reply beala 4 hours agorootparentOn iPhone, you can quickly do this by holding down the lock button and either volume button until the shutdown screen appears. Once it appears, your phone is now locked and it will only accept the PIN (you don't need to actually shut down). reply minitoar 3 hours agorootparentAlternatively one can press the lock button 5 times quickly. reply aftbit 2 hours agorootparentOn Android, pressing lock 5 times quickly automatically dials 911. reply TeMPOraL 17 minutes agorootparentThankfully, it doesn't. It asks you to confirm by sliding some on-screen control, and then dials 911 / 112. If it dialed immediately, I'd be in jail already, going by the amount of times I managed to trigger the \"call 911?\" screen by accident in the last year or so. reply WaitWaitWha 4 hours agorootparentprevThis works if the event, which forces unlock, is expected. Often such events are not expected and there are but seconds. reply WaitWaitWha 50 minutes agorootparentI beg to differ to those who write that such events are expected, just press a few buttons, disable, or something similar. Imagine you are not in a a relatively \"democratic\" nation. (0) You are asleep. You phone is on the nightstand. At 4:00 in the morning, you wake up with a rifle stuck in your face. (1) You are walking down the street, middle of the day. Your phone is in you jacket inside pocket. Two burly individuals grab each of your hands, tie them and then toss you into a van that just pulled up. (2) You are walking around, let wind on your face and feel it in your hair. Your cell phone is in your jilbab or burqa, you changed out of. A rock hits your head and you black out. (3) you walk into the public WC/bathroom in the bar, but you do not take your phone in with you because it is just ... ick. You come back out and the phone is in the hands of a local law enforcement agent. Each one of these have happened in real life. There are just a myriad of real scenarios where someone is not in reach of their cell phones. reply kdmtctl 1 hour agorootparentprevThe event itself is often expected. Nothing happens out of the blue. The exact time of the event is unknown. So, extra precautions like disabling biometrics before leaving home is a normal risk mitigation practice. reply OKRainbowKid 4 hours agorootparentprevOn my android phone, if I hold the power button I get the option to \"lockdown\", which immediately locks the phone and disables biometrics for the next unlock, requiring the PIN/password. I assume that would work for the situations you have in mind. reply OKRainbowKid 4 hours agorootparentprevOn my android phone, if I hold the power button I get the option to \"lockdown\", which immediately locks the phone and disables biometrics for the next unlock, requiring the PIN/password. I assume that would work for the situations you have in mind. reply jesseendahl 1 hour agorootparentYup and iPhone has the same feature. Seems like parent may not be aware of this. reply brightball 2 hours agoparentprevI've always thought of passkeys as a good 2nd factor in conjunction with a password. Similar to the way you'd use a Yubikey or anything else with FIDO2/WebAuthn. Seeing passkeys as a dedicated login on their own is...strange. For all of the reasons that you indicate. reply WaitWaitWha 1 hour agoparentprevThank you for the correction on the UK laws. reply hazmazlaz 4 hours agoparentprevI agree, and I wish there was an option to always require both a passcode/password AND biometrics in iOS and MacOS. While it would become a hassle having to do it every time, it would at least guarantee that one could retain their 5th Amendment rights if the device were seized. reply circuit10 14 minutes agorootparentHaving no backup to biometrics could lock you out permanently if it stops recognising you for some reason, so it would need to accept just the password, and at that point you can just turn biometrics off entirely reply frantathefranta 4 hours agoparentprevWould you happen to know what the rule is on Yubikeys and the like? I assume if it's PIN-protected, it counts as a password but what if it's just set up for tap-to-unlock? reply WaitWaitWha 4 hours agorootparentNot a lawyer and do not know your jurisdiction. I extrapolated this as anything that is in the mind (PIN, password, some secret) cannot be demanded, while anything outside of the mind, biometrics, geolocation, physical object (key) can. Again, I am just a hairless monkey smashing rocks together. Consult experts. reply HWR_14 2 hours agorootparentIn the US this is a pretty good nationwide summary. reply mminer237 3 hours agorootparentprevOf course they can use that. The Fifth Amendment protects the right to not testify against yourself. You can keep silent. That's it about self-incrimination. The government can seize any physical object and do essentially anything it wants with it with a warrant. They can physically decap a TPM and read the security key if they really want to. reply rickcarlino 21 hours agoprevIssues I’ve encountered building an app with magic links: 1. Include a fallback sign-in code in your magic link, in case the user needs to log in on a device where accessing their email isn’t practical. 2. Make sure the sign-in link can handle email clients that open links automatically to generate preview screenshots. 3. Ensure the sign-in link works with email clients that use an in-app browser instead of the user’s preferred browser. For example, an iOS user might prefer Firefox mobile, but their email client may force the link to open in an in-app browser based on Safari. reply ctm92 11 hours agoparentThese days there's also to consider that some Mail Threat Protection Tools (at least Microsoft Defender in Exchange Online does this) click links in Mails to check them. Recently ran into this issue as new mail accounts got confirmed automatically and magic links were invalid when the user clicked them, because Microsoft already logged in with it during checking. reply TeMPOraL 5 hours agorootparentCome to think of it, magic links by definition violate the principle that GET requests should not change state. Defender & preview tools are actually following the established norms here - norms that were established decades ago precisely because we hit the more broad problem with C, U & D parts of CRUD, and collectively agreed that doing destructive operations on GET requests is stupid. reply kragen 4 hours agorootparentYou can GET awhich POSTs when you click the \"log in\" button. reply GTP 3 hours agorootparentYes, but the GET itself isn't changing any state. The state changes only after clicking on the button. This is OP's point. reply kragen 3 hours agorootparentTeMPOraL said, \"magic links by definition violate the principle that GET requests should not change state\". That is a reasonable thing to think, but it is not true, because you can GET awhich POSTs when you click the \"log in\" button, unless you think a link to such apage should be excluded from the definition of \"magic link\". reply TeMPOraL 37 minutes agorootparent> unless you think a link to such apage should be excluded from the definition of \"magic link\". Yes. Linking to a form requiring user to press a button to submit an actual POST request is one proper way of doing it, and won't confuse prefetchers, previewers and security scanners - but it lacks the specific \"magic\" in question, which is that clicking on a link alone is enough to log you in. Can't really have both - the \"magic\" is really just violating the \"GET doesn't mutate\" rule, rebranding the mistake we already corrected 20+ years ago. (EDIT: Also the whole framing of \"magic links\" vs. passkeys reads to me like telling people that committing sins is the wrong way of getting to hell, because you can just ask the devil directly instead.) reply kragen 23 minutes agorootparentAha, then we agree on the facts, just disagree about nomenclature. Your theological analogy is hilarious! reply Jolter 46 minutes agorootparentprevIn your example, it seems to me that the POST request is the action that changes the state. reply kragen 23 minutes agorootparentAgreed. reply ku1ik 23 minutes agorootparentprevThis is the way. reply robertlagrant 9 hours agorootparentprevThat seems like a really thoughtless idea. reply aspect0545 10 hours agorootparentprevWhat can you do to prevent automatic confirmation in that case? reply mixedbit 10 hours agorootparentI run an authorization service that allows to log-in using magic links and we managed to solve this. First approach was for the link opening GET request to do not log the user in, but to include an HTML page with JavaScript that issued a POST request with a code from the link to log the user in. This worked well for a long time, because email scanners were fetching links from emails with GET requests but did not execute JavaScript on the fetched pages. Unfortunately, some time ago Microsoft tools indeed started to render the fetched pages and execute JavaScript on them which broke the links. What works now is to check if the link is open in the same browser that requested the link (you can use a cookie to do it) and only automatically login the user in these cases. If a link is open in a different browser, show an additional button ('Login as ') that the user needs to click to finish the login action. MS tools render the login page but do not click buttons on it. The issue that MS tools introduced is broader, because it affects also email confirmation flows during signups. This is less visible, because usually the scanners will confirm emails that the user would like to confirm anyway. But without additional protection steps, the users can be signed up for services that they didn't request and MS tools will automatically confirm such signups. reply szszrk 9 hours agorootparent> check if the link is open in the same browser that requested the link (you can use a cookie to do it) and only automatically login the user in these cases. If a link is open in a different browser, show an additional button ('Login as ') that the user needs to click to finish the login action. Thanks for checking if it's the same browser. Some companies don't care about that (cough booking cough) so harmful actors just spam users with login attempts in hope a user will click by accident. And puff, random guy gets full access to your account. I got those every day, if I ever needed to login this way I would not be able to figure out which request is mine. reply vanviegen 5 hours agorootparentWouldn't that just log you in on the browser doing the clicking, instead of the attackers browser? reply szszrk 48 minutes agorootparentYou mean in the booking example? They logged in the browser that... requested access. So basically anyone that knew your login/email. I think it should check if browser requesting is the same as the one confirming, or just drop that whole dumb mechanism entirely. reply ku1ik 17 minutes agorootparentprevThat check for the same browser is a great idea. Thanks! reply keskival 8 hours agorootparentprevOk, what if an email has \"click this link if it was you who tried to log-in\", or \"if it wasn't you\"? Will Microsoft automatically authenticate malicious actors, or block yourself from services built with assumptions that the email client won't auto-click everything? reply mixedbit 8 hours agorootparentLogin links from my service were automatically clicked and rendered and I know that other services discovered similar problems. Based on this I think that it is very likely the case with all the links in emails, but I don't know if there is any additional heuristic involved that would treat some links differently. See also this issue which suggests that all links are opened: https://techcommunity.microsoft.com/discussions/microsoftdef... Note that this doesn't affect all Outlook users, this Microsoft Defender for Office 365 is a separate product that only some companies use. reply pmontra 5 hours agorootparentprev> But without additional protection steps, the users can be signed up for services that they didn't request and MS tools will automatically confirm such signups. Indeed it's a bad thing but how bad? The admins of some web service get a database of emails, send them those registration links, make their mail software create the accounts and? They end up with a service with accounts that they could create without sending those emails, before they send some emails to solicit users to perform some action on their (long forgotten?) account. There is no additional threat unless I'm missing something. The admins have only an extra thin layer of protection because of the confirmation step but I think that any court can see through it. reply mixedbit 4 hours agorootparentThe exploitation and potential damage would be service specific. Say a Dropbox like service for computer file syncing: An attacker creates an account for 'alice@example.org' and gets the signup email automatically confirmed. The attacker uploads some malware files to the account. After some time Alice attempts to create a valid account and resets password for 'alice@example.com'. Then Alice installs a desktop file syncing client provided by the service and malware files from the attacker get downloaded to her machine. Another example would be if a company hosted a web app for employees that allowed signups only from @company.com addresses. In such case an attacker could be able to signup with such an address. reply aftbit 2 hours agorootparentprevIt defeats the email verification entirely. If that weren't necessary for something, why would the site require it? reply miohtama 10 hours agorootparentprevThe link leads to a page where you need to press a button (HTTP POST). reply sandermvanvliet 10 hours agorootparentprevAs far as I know there’s nothing. The alternative is to send an OTP in the mail and tell the user to enter that. In that way there is no link to auto confirm. However, if you do that ensure that you have a way to jump straight to the page to enter the OTP because (looking at you Samsung) the account registration process can expire or the app is closed (not active long enough) and your user is stuck reply devnullbrain 7 hours agorootparentprevI've had this problem as a user, accidentally previewing a link in iOS by tapping for too long. reply tuetuopay 2 hours agorootparentThis is even worse for copying the link. On iOS the contextual menu comes with the preview, which will destroy the magic link. reply littlestymaar 5 hours agorootparentprev> These days there's also to consider that some Mail Threat Protection Tools (at least Microsoft Defender in Exchange Online does this) click links in Mails to check them. What an insane policy, why am I surprised Microsoft came up with it… reply sbarre 4 hours agorootparentIt's not actually insane if the application hosting the link follows the principle that GET requests should not mutate state. This problem is ~20 years old from when CMS platforms had GET links in the UI to delete records and \"browsing accelerator\" browser extensions came along that pre-fetched links on pages, and therefore deleted resources in the background. At the time the easiest workaround was to use Javascript to handle the link click and dynamically build a form to make a POST request instead (and update your endpoint to only act on POST requests), before the fetch API came along. reply mooreds 20 hours agoparentprevWe went to a lot of trouble to make our magic link implementation work with anti-phishing software, corp link checkers and more. https://github.com/FusionAuth/fusionauth-issues/issues/629 documents some of the struggle. I think that a link to a page where you enter a one time code gets around a lot of these issues. reply andrei_says_ 20 hours agorootparentI arrived at the same conclusion after going through the steps and seeing that some corporate systems mark the login link as malicious, and there’s nothing I can do about it. Sending a code goes around a lot of issues. reply js2 16 hours agorootparentAlso: Safari can autofill codes from both email and text messages on macOS and iOS. It then automatically deletes the message too. https://www.webnots.com/how-to-autofill-verification-codes-i... reply hackernewds 10 hours agorootparentprevone times codes are very vulnerable to phishing. users are prone to entering codes on any resembling website reply adastra22 13 hours agorootparentprevWhy not just support a password? reply mooreds 5 hours agorootparentOh, we support passwords! (And passkeys, and social login, and OIDC, and SAML.) Just want to make sure magic links work as well as they can. Different folks have different requirements, and since we're a devtool, we try to meet folks where they are at. We actually recently added a feature which lets you examine the results of a login, including how the user authenticated, and deny access if they didn't use an approved method. reply matt-p 9 hours agorootparentprevOnly having magic links gets you a load of stuff for free, Higher level of security than just user+pass (w/ forgot password) Email verification Lifecycle management - in a SAAS when a user no longer has a corporate email, they can defacto not log in, wheras with a user+pass you need to remember to remove their account manually on each SAAS or have integration with your AD (for example) reply adastra22 8 hours agorootparentIt’s not a higher level of security than password-based authentication. Why do you state that? One-time email verification is not the same as security model as magic links. Magic links require instant access. Many security sensitive sites require a time delay and secondary notification for password reset links, which you can’t reasonably do for login links. Lifecycle management is an interesting point. There are some underlying assumptions that might not hold though—losing an email doesn’t necessarily mean downstream accounts should be auto disabled too. Think Facebook and college emails, for example. reply mooreds 5 hours agorootparent> It’s not a higher level of security than password-based authentication. Why do you state that? It could be, depending on how the user has secured their email inbox access. I know I pay a lot more attention to my inbox than some random account. I don't have data, but I think this is true of most people. I'm also more likely to enable MFA on my email account than I will on every random account I sign up for. And as far as the account providers, I trust the big email providers to be more secure than some random website with an unknown level of security. You raise some valid points about tying access to a third party and what makes sense. It's not a simple issue. reply michaelt 1 hour agorootparentprev> It’s not a higher level of security than password-based authentication. Why do you state that? Personally I'm no fan of magic links. But the people who do like magic links would say the typical 'forgot password' flow is to send a password reset magic link by e-mail. That means you've got all the security weaknesses of a magic link, and the added weaknesses of password reuse and weak passwords. Of course you can certainly design a system where this isn't the case. Banks that send your password reset code by physical mail. Shopping websites where resetting your password deletes your stored credit card details. Things like that. reply davidmurdoch 6 hours agorootparentprevOne weird reason I've personal run into: when building on the edge, like with cloudflare workers, you can run into timeout issues because of how long password hashing takes. reply kentonv 4 hours agorootparentThat should be only if you're on the free plan that has a 10ms time limit. Paid plan gets 30 whole seconds which is plenty of time to hash lots of passwords. reply ThePowerOfFuet 10 hours agorootparentprevBecause ≈everyone reuses passwords and so accounts get taken over. reply adastra22 8 hours agorootparentA majority of internet users (>60% in 2024 and growing) use password managers and don’t reuse passwords. reply dspillett 3 hours agorootparentIn my experience 60% seems too high even for supposedly technical users (ref: I work in a dev firm), at least away from their jobs. I definitely don't believe it for the wiser population (my gut, again based on people I know, says the number is more like 10%, maybe 15). Even the 36% figure on the report on security.org posted above seems dubious, I suspect they have some bias in their survey. Unless that is some people who use the iCloud password manager for some things and no password manager for everything else, so it isn't claiming 36% routinely use a password manager away from a few key accounts. reply sebastiennight 4 hours agorootparentprevThis is an extraordinary claim on two counts: 1. Sixty percent seems astronomically high, do you have a source? and 2. Most \"normal\" non-tech-savvy people I know who do use a password manager (which I've typically installed for them), are revealed a while later to still use a variation of password reuse : either storing the same password per category of websites, or having a password template they use on all sites, e.g. \"IdenticalSecretWord_SiteName\" reply schnable 37 minutes agorootparentRight, use of a Password Manager does not imply they are using Password generation - it may just mean they click \"Save this password\" after logging in using a re-used password. reply lkbm 2 hours agorootparentprevI don't have the source, but don't think 1Password/LastPass/KeePass. Think the \"would you like to save this login\" built in to Chrome, Firefox, Edge, Windows, and iOS. These days, you have to opt-out of password management. reply JW_00000 6 hours agorootparentprevDo you have a source for that number? 60% seems extremely high based on non-technies I know. reply mooreds 5 hours agorootparentAgreed. I'd be thrilled if it were true, though! Because password reuse (esp without MFA) is a big problem. reply ctxc 5 hours agorootparentprevI'm surprised. >60% seems high even for tech literate software engineers! reply wccrawford 7 hours agorootparentprevhttps://www.security.org/digital-safety/password-manager-ann... 36%. reply sam_lowry_ 10 hours agorootparentprevSo what? reply victorbjorklund 10 hours agorootparent1) It means your users will complain that their account was hacked (even if it was their fault) and might cancel their service 2) hackers can exploit your system which hurts you (you are a VPS provider and someone mines crypto and you have to wave it for PR) or you run an email service and someone uses your app to spam (which hurts your email rep) etc. reply presentation 11 hours agorootparentprevI've had success with sending a code, and the link takes you to a page where the input is pre-filled with the code, and you just have to click \"Login\". reply mooreds 5 hours agorootparentYup, that's a good option. Any kind of user action like a form submission is less likely to run afoul of a link checker. reply lelanthran 10 hours agorootparentprev> I think that a link to a page where you enter a one time code gets around a lot of these issues. I've done both in my SaaS product - link is GET with the OTP in the link, the target page checks if the link is in the URL, and if not, then the user can type it in. Only for signup, though. For sign-in, the default is to always have the user type it in. reply tzs 15 hours agoparentprev4. Make sure the sign-in link on mobile works with your mobile app. When McDonald's switched from email/password to magic links I had a hard time getting the magic link to work with the McD app. It usually would just open in the McD website. Thus was quite annoying because about 98% of the time I eat McD's I would not do so if I could not order via the app [1]. I finally gave up and switched to using \"Sign in With Apple\" (SIWA). There was no way that I could find to add SIWN to an existing McD account, so had to use the SIWA that hides the real email from McD. That created a new McD account so I lost the reward points that were on the old account, but at least I could again use the McD app. [1] They have a weekly \"Free Medium Fries on Friday\" deal in the app available for use on orders of at least $1. Almost every Friday for lunch I make a sandwich at home and then get cookies and the free fries to go with it from McD. reply k4rli 8 hours agorootparentMcD app is the absolute worst. 1) rooted or bootloader-unlocked Android devices are not allowed (granted it's easy enough to get past it for now but the checks are still there). 2) 2FA requirements as if anyone would bother to steal coupons from others It appears that they want ordering burgers to have the same level of enhanced security as banking apps. Not even crypto or trading apps bother to block unlocked devices in such a way. Blocking rooted devices doesn't even make banking apps more secure but for them I can at least understand the reasoning. reply SOLAR_FIELDS 15 hours agorootparentprevI have heard that you are basically paying double what you normally would if you aren’t hunting for deals in mcd’s app these days. How much truth is there to that? reply packtreefly 15 hours agorootparentA lot. MCD corporate seems determined to get on the user data gravy train, and appears to be subsidizing it for the franchisees. Three large fries ordered at the counter costs over ten dollars. reply mediaman 14 hours agorootparentIt’s not about data, it’s customer segmentation. Frequent customers are more price sensitive, and are willing to use the app to get all the discounts, while occasional customers will not, so they can capture both the more price sensitive part of the market while getting higher margins from occasional buyers. reply andy800 10 hours agorootparentAs someone who spent many years segmenting customers and generating personalized marketing offers -- McDonald's is awful at this. I was a 2-3x/monthly customer (USA based) for years (even more frequent a decade ago, but I'm talking about since the app), ordering the exact same core items every time (except during breakfast). When they began \"value meals\" last summer (which don't include their flagship items) they also removed the best deals from the app, the ones that did include Big Mac, QPC, 10-nuggets. I've placed one non-breakfast order in 6-8 months, whenever they started this. I'm just one person, but if a customer declines from an expected 15-20 visits over a half-year period to 1, and you don't adjust your offer algorithm (and you're the biggest restaurant company in the world so no lack of resources), something is seriously wrong. reply r00fus 0 minutes agorootparentWhenever this happens to me I keep wondering how much I am of the A/B data test where I'm in the \"less important group\". Is it possible that their changes engaged (or profited from) the more active (daily/weekly customers) by making your situation worse? MoreMoore 9 hours agorootparentprevThey used to have great deals on the app in Germany. I used to go to McDonald's all the time. The deals suck now, and now I only go if I'm really craving a McMuffin Bacon & Egg. Whatever they're doing also isn't working for me. reply packtreefly 1 hour agorootparentprev> they also removed the best deals from the app They've captured the user base with the money that corporate was pumping into the app deals, and are in the process of enshittifying it by transferring the value to themselves instead of the users. reply crazygringo 14 hours agorootparentprevSure they want user data to observe people's purchasing habits. But they already have that if you always use the same debit or credit cards like most people do. But the more people use the app, the less cashiers they need and the less ordering kiosks they have to install. Plus customer satisfaction goes up because you can order ahead and your food is ready when you arrive. And getting used to the discounts means you probably won't switch to Burger King or Wendy's. I think additional user data is a relatively minor part of it. reply packtreefly 13 hours agorootparent> you can order ahead and your food is ready when you arrive That just sounds like a great way to get cold McDonald's... > I think additional user data is a relatively minor part of it. You're probably right about that, but I've always undervalued user data because I don't think it's ethical to exploit people like that. I'm sure that a well-timed push notification suggesting a personalized meal deal right around hungry-o'clock is the real goal of pushing this stupid app on their customers. reply crazygringo 2 hours agorootparent>> you can order ahead and your food is ready when you arrive > That just sounds like a great way to get cold McDonald's... The idea is to order 3 or 4 minutes in advance, not half an hour before... reply cardiffspaceman 13 hours agorootparentprev> your food is ready when you arrive. The food does NOT start cooking when you order it if you’re picking up at drive thru. It starts cooking when you pull up to drive thru and give the magic code. In fact if the food is not easy to prepare you get put in a special parking space, where you wait for your order to be prepared. If it includes soft drinks they might serve those before they make you go park. reply andy800 9 hours agorootparentprevDisagree on not going to BK/Wendy's. The \"deals\" game becomes a habit, switching costs are basically zero, people start to comparison shop each app for the best deal (like shopping for air travel). It's a bit of work because there is no single consolidator but it only takes a few seconds to scan each apps offers. At this point, being a fast food chain that doesnt have an app with deals is probably not viable - but I am very skeptical it generates any loyalty. reply butterknife 13 hours agorootparentprev> But they already have that if you always use the same debit or credit cards like most people do. Don't they have only the last 4 digits and the issuer of the card? It is likely enough but there will be some noise. Not to mention any potential legal trouble if they used the card details without explicit consent. App contracts will get around that. reply crazygringo 2 hours agorootparentThey have your name too. From what I understand, the tracking is generally done via something like the hash of the card number though. I've never heard of any legal or compliance issues with that, since the card number itself is not stored. reply butterknife 41 minutes agorootparentSubmitted a Subject Access Request to McDonald's here in the UK. I'll update here on progress. reply SOLAR_FIELDS 11 hours agorootparentprev> Three large fries ordered at the counter costs over ten dollars. This is kind of hilarious and depressing but I live in a high enough cost of living city in the states and I order mcd’s rarely enough that I cannot tell contextually whether your statement indicates this is overpriced or underpriced. reply ChristianGeek 11 hours agorootparentIt depends on how recently they came out of the fryer, how fresh the oil is, and the grease-to-salt ratio. reply packtreefly 1 hour agorootparentI will sadly admit that the high price of fries only angers me when they're not fresh. reply koolba 13 hours agorootparentprev> Three large fries ordered at the counter costs over ten dollars. Ask for a “bundle box” next time you’re there. They’re usually named after a local sports team. Two Big Macs, two cheeseburgers, two fries, and a 10-piece nuggets for $12-15 depending on the market. I think retail for just the Big Macs is that much these days. No app required. reply andy800 10 hours agorootparentprevMuch more - most McD's in USA charge over $4 for a large french fries. reply bpeebles 14 hours agorootparentprevIt depends on order size. I think orders for one or two people over time you'd save close to 50% between deals and using points. For larger orders 20% off once a day is about the best you can do. (I'm my area/experience.) reply andy800 10 hours agorootparentprev\"Normally would\" is more likely, prices from the mid-2010's. The order I used to pay about $12 and change for in 2015 (I know this because I ate there at least once a week), is now about $13, by using the app deals. However since the rollout of \"value meals\" last summer, they took away some of the better deals and now McDonald's is simply expensive (for McDonald's) even with the app. reply red_phone 15 hours agorootparentprevThe difference isn’t nearly that dramatic, but there are definitely savings to be had via the app. reply miki123211 10 hours agoparentprevOne of the biggest advantages of magic links is that they're unphishable while still being easy to use (unlike passkeys). Having a code completely negates that advantage, as attackers can just set up a fake website that asks for the code. Magic links should log you in on the device you click them, not on the device that requested the login session. Anything else, while being a little bit less annoying, is a security issue and should be treated as such. reply tarxvf 7 hours agorootparentThat would require I have my email on every device I might want to log in with. I don't like that for a number of reasons. reply hombre_fatal 14 minutes agorootparentThe conventional password system requires you to have a shared password manager on every device or that your reuse or memorize passwords. And that none of the service's users reuse passwords. It's all trade offs, else it would be easy. reply layer8 21 hours agoparentprev1 and 3 are mentioned in the article. It’s still annoying if there is no password option. reply KronisLV 8 hours agoparentprev> For example, an iOS user might prefer Firefox mobile, but their email client may force the link to open in an in-app browser based on Safari. Hey, wasn’t Firefox on iOS based on Safari related tech anyways? https://en.m.wikipedia.org/wiki/Firefox > However, as with all other iOS web browsers, the iOS version uses the WebKit layout engine instead of Gecko due to platform requirements. I do agree with what you’re saying though! Just those two in particular will probably have pretty good compatibility, which I was amused to find out when I looked into it. reply yieldcrv 16 hours agoparentprevOne thing I recently found annoying with Slack was that I wanted the company chat on my phone, but I didnt want the company’s email on my phone given the overly broad control of my phone so I got the magic link on their computer and then I made a qr code but wait, the email quarantine system had altered the whole link so I had to extract that but wait the redirect url back to slack was malformed because of the url encoding and i had to fix that and then make the qr code like wow just give me a qr code or code instead in the original magic link email! reply tzs 15 hours agorootparentWould it have worked to forward the mail from your work email system to your personal email? reply sthatipamala 15 hours agorootparentIf it’s a big corp, they probably have strict data exfiltration policies for corp email. Maybe this one email would have been fine, but if it gets tripped, it’s not worth the headache. reply k3lsi3r 14 hours agorootparentThese same corps have opinions on where users can be logged into Slack as well. And ffiw most enterprises that have this kind of device management don't allow login via magic links via email anyways. reply yieldcrv 13 hours agorootparentYes, accurate, I was able to access some slack workspaces but not the main one without putting the invasive management profile on my phone reply adastra22 13 hours agorootparentprevNow your private email contents are subject to discovery. reply tzs 36 minutes agorootparentAssuming we are talking about discovery in a civil lawsuit involving your employer the party opposing your employer can ask for all documents you have that are relevant to the lawsuit. It is then up to you to turn over those documents. If they specifically ask for documents that are not relevant or if their request is too broad so will produce a lot of irrelevant documents your company's lawyers will tell them no. By the time someone is actually specifically giving you a list of things to turn over that includes your private email it will only be asking for things that are relevant. Most of your personal email will be excluded. reply drsnow 7 hours agorootparentprevGenuinely: why? All that is related to the business email is already accessible, it's just been forwarded elsewhere. The info is already known. What's to discover? reply indeed30 7 hours agorootparentTo discover where else you then subsequently forwarded it. I'm not suggesting this is actually a problem, but that's how an argument could go. reply yieldcrv 15 hours agorootparentprevIt wouldnt have worked any differently from the first qr code with quarantine, and been a flagged violation, eventually reply michaelmior 21 hours agoparentprev> Make sure the sign-in link can handle email clients that open links automatically to generate preview screenshots. Any suggestions on what needs to be handled here? My first thought is UA checking to see if it looks like a real browser. reply snthd 20 hours agorootparentThe link is a \"safe\" GET request. The page loaded via the link should do an \"unsafe\" POST for the login, via javascript with a form button for fallback. https://www.rfc-editor.org/rfc/rfc7231#section-4.2.1 >The purpose of distinguishing between safe and unsafe methods is to allow automated retrieval processes (spiders) and cache performance optimization (pre-fetching) to work without fear of causing harm. In addition, it allows a user agent to apply appropriate constraints on the automated use of unsafe methods when processing potentially untrusted content. Exactly the same for email unsubscribe links, or a one click \"buy now\" link. reply apitman 12 hours agorootparentIf you want to do this without JS just add a page with a \"Click here to complete login\" button that does the POST. reply justinator 19 hours agorootparentprevAutomatic link pre-fetchers know JavaScript too and will trigger your JavaScript to post. I've had to implement a system where if the link was minted x minutes ago, the JavaScript on the landing page is disabled. It's just another arms race. It shouldn't be this hard, but in email it seems everything is additionally harder to do. reply adastra22 13 hours agorootparentWhy not just have a username & password. Why make everything so complicated? We just successfully got password managers deployed to most users, only to drop passwords entirely for a subpar system? reply cuu508 12 hours agorootparent> We just successfully got password managers deployed to most users Source? reply adastra22 9 hours agorootparentEvery desktop and mobile OS has a built-in password manager perfectly adequate for this use case, with encrypted sync and backup capabilities. reply rickcarlino 21 hours agorootparentprevIn my app, I just added an “Almost there!” Page with a button that the user needs to click. I still need to add a fallback option that uses a one time code for the other reasons mentioned above. reply paxys 19 hours agorootparentprevSave a browser cookie when the login is initiated. When the link is clicked check if the same cookie is present. If not, ignore it. Expire the link and the cookie after n minutes. reply Macha 19 hours agorootparentSurely this breaks the \"email is not on same device as login\" use case? At least with normal magic links, they're merely incredibly annoying but doable (via e.g. typing in the URL) reply paxys 18 hours agorootparentThat use case still works. In fact it works better because if you click the link on your phone you don't automatically get logged in on your phone browser (or your email client's in-app browser). You can then copy the same link on your desktop and it will work as expected. reply apitman 12 hours agorootparentI'm confused. How do you get the cookie from the original device to the other device? reply paxys 4 hours agorootparentIt's the other way around. You copy the URL to the device that has the cookie. reply apitman 1 hour agorootparentHow do you copy the link between devices? QR code? reply shakna 17 hours agorootparentprevAs long as you also have a code to enter, then things will feel fine across devices. reply michaelmior 19 hours agorootparentprevAfter reading the other replies, this seems like one of the more effective approaches. Thanks! reply timfsu 21 hours agorootparentprevUsing a time-based expiration rather than a usage-based expiration should help reply nine_k 21 hours agorootparentprevE.g. require to click a button to actually sign in, don't consume the token and establish the session on mere URL access. reply ThrowawayTestr 11 hours agoparentprevIs 3 even possible? reply paxys 20 hours agoparentprev3 isn't really possible, because the redirect needs to take you back to the browser session where you initiated the login from. reply Groxx 19 hours agorootparentIt definitely does not. Have your logging-in session wait for / poll \"has visited magic link\", and authenticate that session when it's done. Tons of systems do this. It works great, and it can quite easily work without any web browser at all on the logging-in side because it just needs to curl something -> poll for completion and save the cookies -> curl against the API. A number of oauth flows for e.g. TVs work this way, for instance, because it's a heck of a lot easier than integrating a full browser in the [embedded thing]. Many app-based 2FA (e.g. Duo) works this way too. reply sunnybeetroot 17 hours agorootparentThis seems dangerous: 1. Attacker starts a log in and triggers a magic link email 2. Email received and my browser client previews the link without my desire 3. Attacker is now logged in reply codetrotter 16 hours agorootparentThat’s why you combine it with a check for source IP and tell your user that they need to approve from a device that has same IP as the one they are logging in on. So if I’m logging in on my laptop, and approving with my phone, it will be rejected if my phone is using mobile data while my laptop is using landline, but will approve if my phone is connected to WiFi of the same network my laptop is connected to, or if my laptop is tethered via my phone, because then I have same external source IP on both devices. reply apitman 12 hours agorootparentThis has security and usability issues. NAT/CGNAT means a potentially large number of people can hijack your login. reply witrak 15 hours agorootparentprevThis scenario is a solution only in simplest cases. It doesn't work when someone routinely uses a VPN on the phone (when often uses free public wi-fi in airports, railway stations, markets etc) because of possible MITM attacks. reply asddubs 13 hours agorootparentalso some ISPs will give you a different IP every request reply johnmaguire 14 hours agorootparentprevThe links are one-time use so you need to take this into account anyway or users simply can't login. It's usually done with a required button click after following the magic link. Or you can try JavaScript techniques to detect a real browser. reply paxys 18 hours agorootparentprevNo \"tons\" of systems do not do this. If you come across one that does it was built by a team that has no idea about security. TVs etc. are special cases because obviously there is no way to redirect to them, and even there developers will always have some kind of secondary checks like having you enter a code displayed on the screen. reply edoceo 17 hours agorootparentWhen I sign on to a bank it sends an SMS. Then my phone prompts me to share that code with the brower, on my desktop. It's a neat QOL \"feature\" - but kinda feels too automated to be secure. reply lupire 17 hours agorootparentThat's because the server recognizes both clients, and you are prompted to approve the other client from the client that has the code. You are giving permission to the remote client, not taking permission from the remote client. reply edoceo 15 hours agorootparentThe server recognized my device from the messenger/SMS app? That seems not correct. But somehow, the desktop browser and my mobile are tied together for this app. But no other sites have this magic. reply happyopossum 10 hours agorootparentIf you have an iPhone and a mac, most sites that use SMS OTPs work this way reply edoceo 4 hours agorootparentLinux desktop, android phone. reply Groxx 15 hours agorootparentprevI've come across dozens. Google does it. Paypal does it. Duo does it. Lots of single-sign-on systems do it. All of those including not-TV scenarios, just normal computer-and-phone stuff, as well as sometimes other weird flows. Many of these are far beyond what most would label as \"security competent\", into \"login security is a large part of their business and they have significant numbers of specialists hired\". (it is probably safe to say none are \"truly secure\" or \"actually security obsessed\", but I doubt that's actually possible in large quantities. the requirements are too steep, for both implementers and users.) It's not the most common, certainly, nor anywhere close. But it's very far from nonexistent. reply apitman 12 hours agorootparentWhere does Google do this? reply Groxx 4 hours agorootparentLog in on browser -> push notification \"is this you?\" on your phone -> browser automatically continues when you say \"yes\". reply apitman 2 hours agorootparentBut that's only as a second factor right? reply renewiltord 18 hours agorootparentprevSo I misclick a link in my email client and the evil guy who requested in is now logged in on his browser god knows where? Surely that can’t be real. It sounds awful. TVs involve copying a code to make sure the right device is being authenticated, or the ones I’ve used have at least. reply Groxx 18 hours agorootparentDuo 2FA works the same way. In principle yes. And it's basically always accompanied by a \"click this link\" -> \"are you trying to log in, and is this you? yes/no\" page to resist that. Small code copying is also a very good answer though, yes. Roughly as easily manipulated, but nothing's perfect, and it's less \"I didn't mean to click that button\"-prone. reply renewiltord 18 hours agorootparentYeah but I routinely click links in emails whereas logging in is the sole purpose of Duo. I could easily just intend to scroll the page and end up tapping the link. reply Groxx 15 hours agorootparentso have that open a site that says \"confirm login? y/n\". I don't mean to imply that just visiting the link should be enough to complete a login. That's a GET and there's a LOT of issues with doing anything important on GET. Just \"do something on a different machine, then automatically complete login on the one logging in\", and magic links to trigger that flow are a rather straightforward option. There's no reason at all that it has to all occur on the same machine, and many reasons why attempting to require that doesn't work out in practice even when it does happen on the same machine. reply renewiltord 13 hours agorootparentAh I see. Yes, that makes sense. reply o999 19 hours agorootparentprevNot necessarily, some services would have the magic link verify the session that triggered it regardless of browser or device. I assume it generates a session on the post-login screen and authorize that session upon accessing link reply ydant 19 hours agorootparentThat has the problem of opening up an attack where the attacker requests the sign-in link, the person receiving the link blindly clicks it, and the attacker now has access. People blindly click links all the time. It would have a low success rate, but would be more than 0%. reply pests 19 hours agorootparentprevI might receive a magic link on my phone but then sometimes I'll copy/paste that over to my desktop or another device. This works on 99% of magic links I've tried except for cases when they are trying to prevent account sharing. I remember the Bird bike app did this, where they required the magic link to be clicked on the same device login was initiated on. I was using my friends account and he would just forward me the link until one day this stopped working. reply duxup 19 hours agorootparentIt seems like “prevent account sharing” and magic links are somewhat at odds by design. reply pests 13 hours agorootparentThe idea being the user(s) would then have to share a email inbox to share logins, not just a password. It might not be the most inconvenient - this is partly how Netflix did their \"Household\" lockdown. You can request a travel code and this gets emailed to the primary email. I feel the way Netflix did this broke the social contract of profile sharing on purpose - before, if you were a good tenant, you could freeload off another paid account without inconveniencing them at all. Memes and jokes formed of still being on an ex-partner's account or how people would rename themselves \"Settings\". Getting an email and being harassed for the code by all those account sharers? Much more open and open for annoyingness. reply johnmaguire 14 hours agorootparentprevMore than a shared password? reply sebastiennight 12 hours agoprevWe've been using Magic Links for a few years (and yes, one reason was to avoid the security issue of storing user passwords when we were just at MVP stage) and found the top problems with it are: 1. Some users (0.1%) just don't ever get the email. We tried sending from our IP, sending from MailGun, sending from PostMark, having a multi-tier retry from different transactional tools. Still, some people just will not ever be able to log in. 2. People click old Magic Links and get frustrated when a 6-month old link \"doesn't work\". We've decided to remedy that by showing them a page that re-sends the link and explains the situation (like Docusign does) instead of an error message. 3. People will routinely mis-spell their email and then blame the system when they don't get the code. All of this still results, I feel, in way fewer support tickets than the email+password paradigm, so I'm still in favor of Magic links. reply 255kb 12 hours agoparentIt's indeed interesting the number of people misspelling their email address, or having an inbox so full that it cannot receive emails anymore. I never tried to add magix links, but I added Google Sign in to my SaaS several month ago, and since then, it accounts for more than 90% of new sign-ups (users are devs, so rather tech savvy and privacy aware). I'm now convinced that no other method is a priority (I still have email/password of course). reply kstrauser 18 minutes agorootparentI have my HN username at a venerable webmail service. I check it about once a year, tops. My name isn't unimaginably rare, but neither is it \"Smith\". I am shocked, shocked, by the number of different K. Strauser people who have typed that email address into some random website or another. I've gotten bank notifications, loan documents, Facebook signup info, meeting minutes from some random volunteer work, and all kinds of other things. When I can figure out from context who the intended recipient is, I try to let them know so they can fix it. On one occasion, the person sent me back a swear-laden diatribe for \"hacking their email\". Sigh. I think this has made me a better engineer, though. When someone says something in a meeting like \"...as long as they type their email correctly\", I can jump in and address that myth head-on. No, people will not type it correctly. If it's a minor pain in the neck for me, with an uncommon name, I can only imagine the traffic that the world's John Smith's get. reply wodenokoto 10 hours agorootparentprev> but I added Google Sign in to my SaaS several month ago, and since then, it accounts for more than 90% of new sign-ups (users are devs, so rather tech savvy I do it for services I don't care about. In my mind it is more privacy for me. Keeps you out of my real inbox and my password out of your system and I believe that I can - to some extend - remove myself without having to go through whatever crap account deletion process that services has tried to cobble together. Worst offenders let me login with google and then immediately asks for name and phone number or email and asks me to verify it. reply lolinder 4 hours agorootparent> my password out of your system This shouldn't be a factor because your password should be a random series of characters that are unique to that site. > I believe that I can - to some extend - remove myself without having to go through whatever crap account deletion process that services has tried to cobble together. To an absolute minimal extent: you can make it so Google won't tell them whatever it was they already told them again. But you can't make them delete the data that they already lifted from your Google account. For keeping surfaces out of your inbox, that's what email aliases are really good for. Register with an alias and then block that alias if they abuse it. reply watermelon0 11 hours agorootparentprevWouldn't privacy aware users prefer passkeys or passwords, instead of any kind of SSO? In general, I do understand that use of SSO is due to convenience. Especially since in many cases websites provide less friction when signing up via SSO instead of using username+password. reply hackernewds 10 hours agorootparentBelieve it or not, most users are not that privacy cautious reply 255kb 9 hours agorootparentprevThat would be my guess too. I think convenience always win. reply yvoschaap 11 hours agorootparentprevThe number of support requests I got last year because of [hotmail|gmail|msn|yahoo].con > 30 reply revicon 11 hours agorootparentI just auto switch any incoming .con to .com on the back end. 100% of the time it is a user typo reply 255kb 9 hours agorootparentNice trick! I check on the frontend for all gmal, gmial, etc variations :) reply bshacklett 6 hours agorootparent> I check on the frontend This is the way. The user can benefit from feedback that they got something wrong, in addition to a helping hand. reply andy800 9 hours agorootparentprevJust a very small detail, but want to point out the distinction between these two comments. \"Revicon\" is demonstrating 10x thinking, it's not about being better at rewriting a linked list algorithm or some leetcode challenge. Player 1 gets the same support request over and over, does nothing about it, (\"hey, that's what the user entered, they should be more careful!\"), complains about it online, and who knows how many hours are wasted in the back and forth with the customers. Player 2 simply makes the necessary change on the backend, the users don't even realize they made a typo, totally seamless flow. Hat tip to you. Hope you screenshot these two comments and bring this up in every interview to exemplify the contrast between \"technically correct\" and high-efficiency problem solving. reply pixelsort 7 hours agorootparentA tasteful post and distinction well highlighted. Humorously, Yvo Schaap is no stranger to 10x thinking. For one thing, Yvo publishes diagrams on SaaS/dev topics that always seem consistently way ahead of their time in terms of their organization and completeness. reply sebastiennight 4 hours agorootparentprevA couple of years ago I think I saw a frontend library that warned the user / auto-fixed those typos, but I can't remember its name, and all I can find now are SaaS offers for that kind of service. Which I'm not entirely enthusiastic about as it leaks all user emails to some random service. reply skerit 9 hours agoparentprevMagic links can be very useful, but for some users the issue is in only supporting magic links. reply flessner 11 hours agoparentprevAlso what's the reasoning behind not wanting to store passwords? It's not like the rest of the customer's data is not valuable? If you don't feel comfortable storing passwords, the amount of data I'd trust you with is strictly zero. reply lolinder 4 hours agorootparentNo, kudos to them for looking at a piece of data and asking themselves if it's worth storing—more companies should do that with more data. It's not that the rest of the data isn't valuable to some extent, it's that every piece you have makes the blast radius of a leak that much bigger, so why hold stuff you don't need? Planning for a breach doesn't make you more likely to have one—if anything it makes you less likely! reply gepeto42 4 hours agorootparentprevTo be fair to 404, they're trying to limit the amount of data they hold which IS good, but in the end they need to have the email address of subscribers. reply Kiro 5 hours agorootparentprevIn my country even the social security number of people is public information. reply danenania 11 hours agorootparentprevThe UX debate is valid, but magic links (and emailed one-time codes) are clearly more secure than password + password reset. Control of the email account gets you in either way. Passwords are an additional attack vector. reply tjoff 12 hours agoparentprev... but the usability is a nightmare. reply graemep 12 hours agorootparentAbsolutely. Users have to wait for the email before signing in, it does not work on devices without email without copying the link, etc. reply watermelon0 11 hours agoparentprevEmail should not be considered a secure channel. Username+password (or passkeys) with a password manager (which ensures that credentials are used on the correct domain) via HTTPS is probably the only end-to-end encrypted way of exchanging credentials with good UX for general public. reply danenania 11 hours agorootparentWith password reset, you are also trusting email. reply gepeto42 4 hours agorootparentEven with passkeys or TOTP 2FA, we've decided email is the root, for better or for worse (for people with gmail, it's likely better than SMS would be on a crappy carrier, but it depends on so many factors, including how many hundred apps have Gmail read access via OAuth...) reply dpifke 19 hours agoprevI've been a loyal Mercury customer for a while now, but their forced use of magic links as a third authentication factor any time my IP address changes (after authenticating with a secure password from my password manager and after a valid TOTP) has me ready to move my company's banking elsewhere. I could understand requiring a third factor to authenticate if signing in from a different location or a different ISP than I've been using for the past 5 years, but it's ridiculous to do so if nothing has changed (except the final octet of my DHCP-assigned address) since I last signed in yesterday. I use a different computer (via SSH) to read my email than I do for web browsing, and cutting-and-pasting a signin link that's hundreds of characters long (spanning multiple lines in Emacs, so I have to manually remove \\ where it crosses line boundaries) is a PITA. Adding friction on every sign-in colors all subsequent interactions I have with an app, and makes me hate using it. reply MaxGabriel 19 hours agoparentI’m the CTO of Mercury You shouldn’t get the device verification requirement if you’ve used the device before (we store a permanent cookie to check this) or for the same IP. Any chance your cookies are being cleared regularly? We added this after attackers created clones of http://mercury.com and took out Google ads for it. When customers entered their password and TOTP on the phishing site, the phisher would use their credentials to login and create virtual cards and buy crypto/gold/etc. The phisher would also redirect the user to the real Mercury and hope they figured it was a blip. This device verification link we send authorizes the IP/device you open it on, which has almost entirely defeated the phishers. Since WebAuthn is immune to this style of phishing attack, we don’t require device verification if you use it. I highly recommend using TouchID/FaceID or your device’s flavor of WebAuthn if you can—it’s more convenient and more secure. You can add it here: https://app.mercury.com/settings/security That said, we are talking internally about your post and we do recognize that as IPv6 gets more traction IPs will rotate much more regularly, so we’ll think if we should loosen restrictions on being a same-IP match. reply dpifke 18 hours agorootparentYes, I clear cookies every time I close my browser, as a layered approach to privacy on top of uBlock Origin and NoScript. There isn't a great way to exclude certain sites from this, other than setting up a dedicated web browser in a container just for Mercury. I wasn't aware that WebAuthn didn't have this requirement. I prefer TOTP because I actually like having a second factor in addition to a credential stored on my computer's hard drive (whether a password or a private key in my password manager), but I might be willing to reduce my security posture to get rid of this annoyance. One suggestion: the link would be half as annoying if it was easily cut-and-pasteable rather than a long email-open-tracking link spanning multiple lines. This is what it looks like when I copy it out of my email: https://email.mg.mercury.com/c/eJxMzs1u4jAUBeCncXZB9vVfvPACZshoWIwYoiasdgkra2KV_JCGqPTpK-imq7xxx40vlO9IKia6ggL6zUlQHObdF6\\ JI0alRHBWQvWKRuD4loLZxsJSRXZAwfNBQeQWozasdgeWsMyFZozE4RKZ4d151NOFtuq9w6IqLb-d5fGdyzaBmUIdx_NkzqBeacrqXkZaMxGSNQyQmf7_9GW7\\ Hf1cJ8zW9TshAwwba3ccLuN3u_r_PR9j_GkxxxmadDu32c59jMfkYFmKKP0baIT0vzP4ynHN_-yyhZOTy9jmPPQn6gL-VLMfvvIA_XxbywRYhUbZUp0RpVCUC\\ qDsbasJHeObFMZ4YrFw1cAAAD__4XPZXw I have to manually remove the backslashes and re-combine the lines before pasting into my web browser. Edit to add: looks like email.mg.mercury.com is hosted by Mailgun. Are you intentionally sharing these authentication tokens with a third party by serving them through this redirect? Do your security auditors know about this? reply incompatible 16 hours agorootparentI set Firefox to delete cookies at shutdown, and also an add-on called Cookie AutoDelete, but they both have an option to whitelist a site. reply packtreefly 14 hours agorootparentprev> I wasn't aware that WebAuthn didn't have this requirement. I prefer TOTP because I actually like having a second factor in addition to a credential stored on my computer's hard drive (whether a password or a private key in my password manager), but I might be willing to reduce my security posture to get rid of this annoyance. I've seen passkeys support something like what you're after. The browser will produce a QR code you scan with your phone, and then you authenticate with the passkey via the phone, which then authorizes the original browser. I'm not absolutely certain that this is part of the spec or how it actually works. I'd like to know. It solves a couple different usability issues. You could always use something like a Yubikey. reply dpifke 14 hours agorootparent> You could always use something like a Yubikey. This is the option I prefer, but only on sites that allow me to enroll more than one device (primary, and backup for if the primary gets lost or damaged). AFAICT, Mercury only allows a single security key. I have an encrypted offline backup of my TOTP codes, so if I drop my phone on the ground, I don't get locked out of all my accounts. I keep this separate from the encrypted offline backup of the password manager on my computer, and as far as I know, neither has ever been uploaded to anyone else's \"cloud.\" Malware would have to compromise two completely separate platforms to get into my accounts, rather than just iCloud or whatever credentials. I understand the desire for phish-proof credentials, but—given that I don't click links in emails—my personal threat model ranks a compromised device (via attack against a cloud service provider, or software supply chain attack against a vendor with permission to \"auto-update,\" or whatever) much higher likelihood than me personally falling victim to phishing. I readily admit that's not true for everyone. reply packtreefly 13 hours agorootparent> my personal threat model ranks a compromised device ... much higher likelihood than me personally falling victim to phishing I completely understand that. I'd actually be interested in reading anything practical you might have on that topic if you don't mind. I asked some experts who gave a talk on supply chain security last year ... they didn't have a lot of positive things to say. Developing software feels like playing with fire. reply dpifke 13 hours agorootparentIt feels unstoppable, which is why the best I can do is try to mitigate its impact. Some mitigations that come to mind: The development environment where I'm downloading random libraries is on a completely separate physical machine than my primary computer. I generally spin up a short-lived container for each new coding project, that gets deleted after the resulting code I produce is uploaded somewhere. This is completely separate from the work-supplied machine where I hack on my employer's code. On my primary computer, my web browser runs in an ephemeral container that resets itself each time I shut it down. My password manager runs in a different, isolated, container. Zoom runs in a different, also isolated, container. And so on. Wherever possible, I avoid letting my computer automatically sync with cloud services or my phone. If one is compromised, this avoids spreading the contagion. It also limits the amount of data that can be exfiltrated from any single device. Almost all of the persistent data I care about is in Git (I use git-annex for file sync), so there's an audit trail of changes. My SSH and GPG keys are stored on a hardware key so they can't be easily copied. I set my Yubikey to require a touch each time I authenticate, so my ssh-agent isn't forwarding authentication without a physical action on my part. I cover my webcam when not in use and use an external microphone that requires turning on a preamp. I try to host my own services using open source tools, rather than trust random SaaS vendors. Each internet-facing service runs in a dedicated container, isolated from the others. IoT devices each get their own VLAN. Most containers and VLANs have firewall rules that only allow outbound connections to whitelisted hosts. Where that's not possible due to the nature of the service (such as with email), I have alerting rules that notify me when they connect somewhere new. That's a \"page\" level notification if the new connection geolocates to China or Russia. I take an old laptop with me when traveling, that gets wiped after the trip if I had to cross a border or leave it in a hotel safe. I have good, frequent backups, on multiple media in multiple offline locations, that are tested regularly, so it's not the end of the world if I have to re-install a compromised device. reply packtreefly 12 hours agorootparent> The development environment where I'm downloading random libraries is on a completely separate physical machine than my primary computer. I generally spin up a short-lived container for each new coding project, that gets deleted after the resulting code I produce is uploaded somewhere. This is completely separate from the work-supplied machine where I hack on my employer's code. Something like VS Code remote dev with a container per project? Just plain docker/podman for containers? > On my primary computer, my web browser runs in an ephemeral container that resets itself each time I shut it down. My password manager runs in a different, isolated, container. Zoom runs in a different, also isolated, container. And so on. Qubes, or something else? I've been looking at switching to Linux for a while, but Apple Silicon being as good as it is has made making that leap extremely difficult. reply dpifke 12 hours agorootparentMostly Linux with systemd-nspawn, also some Kubernetes, plus the occasional full VM. (If I were setting this up from scratch, I'd probably try to figure out how to run my desktop as 100% Kubernetes, using something like k3s, but I don't know how practical things like GPU access or Waypipe forwarding would be via that method.) I live inside Emacs for most things except browsing the web, either separate instances via SSH, or using TRAMP mode. If you switch to Linux, I highly recommend configuring your browser with a fake Windows or MacOS user agent string. Our Cloudflare overlords really, really hate Linux users and it sucks to continually get stuck in endless CAPTCHAs. (And doing so probably doesn't hurt fighting against platform-specific attacks, either.) reply MaxGabriel 13 hours agorootparentprev> AFAICT, Mercury only allows a single security key. We allow multiple security keys. You can add more here: https://app.mercury.com/settings/security reply dpifke 13 hours agorootparentOh, nice! This wasn't obvious from the help text. Maybe add it to the FAQ on the \"Adding security keys\" sidebar? reply watermelon0 11 hours agorootparentprevIs there a reason that TOTP cannot be used as a second factor when using Passkeys? Not sure why we suddenly went from 2 factors (password + TOTP) to 1 factor (passkey), even if passkeys themselves are better. TOTP should at least be an option for the users. reply mhitza 18 hours agorootparentprevAt the very least, you can be creative with workarounds for such issues. A bookmarklet can be convenient. javascript:void(window.location.href = window.prompt().replace(/\\\\\\s*/g, '')); reply jeremyjh 18 hours agorootparentprevYou have to send emails through third parties or people won't get them, because you are also always sending them to third parties who host the recipients email and manage their spam. It might be a good reason not to send magic links but here we are talking about a tertiary confirmation, so its useless on its own right? reply dpifke 18 hours agorootparentThe link in the email could be a direct link to Mercury's website, rather than one that passes through a third-party HTTP redirect service. Authentication tokens (even tertiary ones) usually are supposed to have pretty strong secrecy guarantees. I've done multiple security audits for SOC, PCI, HIPAA, etc., and in every case the auditors would have balked if I told them signin tokens were being unnecessarily logged by a third-party service. (Also: I strongly disagree that the only way to get reliable delivery is via a third-party email service, especially at Mercury's scale, but that's a digression from the topic at hand.) reply MaxGabriel 16 hours agorootparentOh good find, the link going through Mailgun as a redirect is a recent regression. We have a PR to fix that going live soon. That said, our security team and I agree there is no security issue here. Mailgun already can see the text of the emails we send. reply dexterdog 14 hours agorootparentHow is there no security issue here? Email is not secure and it's even less so when you are sending it via a 3rd party. If this were a photo site or something that would not be a big deal but we're talking about a bank. SMS is not much better. Like somebody said elsewhere in the thread, you should allow people to opt out of insecure third-factor verifications since they are just an annoyance and are ultimately security theater. reply apitman 12 hours agorootparentThe emails in question are a third factor, not a magic login link. Even if they were, almost all email goes through third parties which are trusted implicitly. That's not great, but email is the only federated system in existence capable of implementing this type of decentralized login at scale. Maybe someday we'll be able to use something like Matrix, Fediverse OAuth, or ATProto OAuth instead, but those are all a ways off. reply adastra22 13 hours agorootparentprevIt's not security theater. He explained above how this is used to defeat a specific phishing attack that they've actually seen in the wild. There are other, different threat vectors (e.g. compromise of the mail server) that it doesn't prevent. But that doesn't make it theater. as it does provide other value. reply dexterdog 1 hour agorootparentWhat does it stop? You already did a 2FA at this point. If an attacker has my 2FA he most likely already has my email so the 'value' being provided is at the cost of more complexity for the user. If this adds value then why not also do an SMS as well to be really, really sure that the user is legit? That would add even more value. And again, I wasn't saying that you can't do all of this nonsense, but users who see it as nonsense should be able to turn it off. reply mbreese 14 hours agorootparentprev> passes through a third-party HTTP redirect service The vendor might not be the only party to use an HTTP redirect service too! My email goes through a security screen by $EMPLOYER, which also rewrites links to get processed through their redirect service. Sure, it's for company-approved reasons, but it's still another party that has access to the login token. reply thefreeman 6 hours agorootparentprevSo you are intentionally crippling your browser and ability to access email (you need to ssh to another computer and access via terminal). You also aren’t able to handle emacs wrapping of long lines. And you are complaining that the security in place to prevent stolen credentials is “inconveniencing you”. reply adastra22 13 hours agorootparentprevPretty sure that is eMacs formatting, not the email itself? Can you kill-copy the URL? reply lyime 18 hours agorootparentprevWhat would be a more secure (yet reliable) method for email delivery for such emails? reply dpifke 18 hours agorootparentMake the link in the email https://mercury.com/something instead of https://mailgun.com/something (which then redirects to https://mercury.com/something). Or (in addition to, or instead of, a hyperlink) provide a 6-10 digit numeric or alphanumeric code that could be copied out of the email message into a form field on the signin screen. reply sebastiennight 3 hours agorootparentMy understanding (as CEO of a startup using Mailgun for magic links) is that you're seeing mailgun in the URL because they have click tracking enabled — which, to be fair, is not super useful in the case of verification emails. They could use a custom subdomain for this click tracking and \"hide\" the mailgun url from you, but we're finding that for some reason Mailgun doesn't just use a let's encrypt certificate, so some users will complain that the tracking links are \"http\" (and trigger a browser warning when clicked). Anyway, even with click tracking disabled and links going straight to mercury.com, the security issue would remain the exact same (since Mailgun logs all outgoing email anyway). But my understanding is that the contents of that email and its link do not provide \"login\" capability but \"verification\" capability. As such, a Mailgun employee accessing your data, or an attacker accessing your Mailgun logs, would only be able to \"verify\" a login that they had already initiated with your password AND your OTP —which means that's effectively a third hurdle for an attacker to breach, not a one-step jump into your account. reply MaxGabriel 16 hours agorootparentprev> 6-10 digit numeric or alphanumeric code that could be copied out of the email message into a form field on the signin screen. To be clear this is what we're trying to avoid. An easily typeable code like that can be typed into a phisher's website. reply dpifke 15 hours agorootparentHow about giving me a setting to disable device verification: \"I know how to type mercury.com into the URL bar and accept all risk of getting phished.\" I appreciate you guys are trying to protect people, but no other financial institution I deal with requires this level of annoyance, and at some point I'd rather switch to a less \"secure,\" but more usable service. (I put secure in scare quotes, because some suggestions, like trading true 2FA, where I have two separate secrets on two separate devices, for a single WebAuthn factor, are actually accomplishing the opposite, at least for those of us who don't click links in emails and don't use ads on Google for navigation.) Edit to add: or maybe save the third factor for suspicious activity, such as \"new device adding a new payee,\" rather than every signin. It's been months since I onboarded a new vendor, and I'd be OK with only having to do the cut-and-paste-the-link dance a couple of times a year, rather than every single time I want to check my balance. reply miyuru 11 hours agorootparentprev> IPv6 gets more traction IPs will rotate much more regularly unfortunately, only few ISPs do IPv6 correctly by assigning a fixed prefix to customers. most of the ISPs apply the ipv4 logic when adding ipv6 planning hence this situation. hopefully this will improve in the future and more stable prefixes will be given to users. reply m463 13 hours agorootparentprevI like the schemes that send a numeric verification code that you manually type in without an email link. can also use a text message. Maybe allow this to be configured. security = 1/convenience but also vice versa reply filmgirlcw 20 hours agoprevI think this is really great as a response to 404's post last week. I love 404 but I'm as annoyed by Magic Links as OP for the same reasons they mention. Ricky Mondello wrote a really great blog last week[1] about how passkeys, as OP alludes to at the end, can be used alongside Magic Links, that I think is worth a read. [1]: https://rmondello.com/2025/01/02/magic-links-and-passkeys/ reply danudey 20 hours agoparentI haven't had these specific issues with magic links specifically, but I do remember when Epic launched the Epic Games Store and they would e-mail you two-factor codes to log in. I consistently had issues where I wanted to log into their store, got prompted to enter the two-factor code they e-mailed me, got no email for several minutes, requested another code, didn't get that either, gave up and did something else, and then got both codes 30 minutes later. The fact is that even in the best of times, e-mail isn't reliable. Things go to your junk folder. Links get blocked by work spam filters. Mailboxes get full (I assume? it's been a while). Personally, I have my e-mail on my iPhone and anywhere else (work laptop or gaming PC) I have to log into icloud.com to check my e-mail; it's cumbersome. Let me put in a password. Let me scan a QR code like embedded devices do. Give me at least one other option. reply lolinder 20 hours agoparentprevThank you, this is a better piece than TFA! Reading TFA I was rather confused at how passkeys are an alternative to magic links—it makes a lot more sense to view them as a complement. Magic links allow you access to passkeys, which are basically \"Remember this Computer\" on steroids. reply gepeto42 20 hours agoparentprevThanks for that link, I had not seen it and if I had known Ricky Mondello had written that, I probably wouldn't have bothered. I'm still used to Apple people being almost completely invisible publicly. reply cmiller1 20 hours agorootparentI'm confused, what's going on here? Is there a reason you wouldn't have bothered reading a post from that specific person? reply gepeto42 20 hours agorootparentI meant Ricky’s post is great and if I had known about it first I might not have written mine! Added a link to it at the bottom of mine. reply cmiller1 20 hours agorootparentAhh! Thanks for clarifying! reply filmgirlcw 20 hours agorootparentprevthanks for writing what you wrote -- I think it's important that we have this conversation as broadly as we can reply lolinder 20 hours agoprevAm I misunderstanding something, or are passkeys not actually an alternative to magic links? Every implementation of passkeys I've seen has presented me with the option to create a passkey after I've already logged in with some other method. I'll admit that I haven't dug into it deeply, but the UX I've been presented with consistently makes passkeys appear to be an alternative to the \"Remember this computer\" button, not to passwords in general. Somehow the service has to know that this new device is authorized. I know depending on the provider there's such a thing as passkey syncing, but that doesn't solve the problem of getting the initial authentication done. The key insight with magic links is that your security system is no stronger than its recovery mechanism. We are never going to get to a world where passkeys are treated as the only authentication mechanism—there will always be a recovery mechanism, and in most cases an automated one via email. Given that that is the case, magic links simplify things by just not pretending that we have a more secure layer on top. By making the recovery mechanism the primary means by which you interact with the authentication flow you're being more honest about the actual security of your auth system. Edit: filmgirlcw has a link to an article that is much better than this one that explains how the two actually complement each other: https://news.ycombinator.com/item?id=42628226 reply filmgirlcw 20 hours agoparentI think as Ricky wrote last week [1], they should augment Magic Links or other auth methods. There are some positives about Magic Links for sure (though I don't know if making your email an even stronger attack vector is necessarily one of them), but for people who use a password manager, for example, they are a definite friction point that I think passkeys most certainly could alleviate. There are definite UX problems around passkeys that could be improved and I think exporting will make syncing across systems a lot better (one of the reasons I use 1Password as my primary password and passkey system is so I can use my passkeys across devices; of course it helps that my employer uses 1Password as our system so I am logged into my personal and enterprise accounts and can auth then from personal or work devices, provided additional auth or enrollment isn't needed) -- but if the problem as 404 defines it is that they don't want to be responsible or even have to worry about storing your passwords/auth controls, I think passkeys is at least better for a subset of users than Magic Links. But again, like Ricky, I don't think it should be viewed as either or. It should be both. [1]: https://rmondello.com/2025/01/02/magic-links-and-passkeys/ reply lolinder 20 hours agorootparentThank you for the link! I saw your other comment and actually edited mine to point to that, because it's definitely the answer to my question! > though I don't know if making your email an even stronger attack vector is necessarily one of them I'm unconvinced that magic links do make your email an even stronger attack vector. Essentially every service that would be inclined to use magic links would already have a way to reset your password entirely once the email is compromised. All magic links do is make this the primary way to interact with the auth flow. The bad guys already know that your email is the best target. Magic links just make that very explicit. reply filmgirlcw 20 hours agorootparent>The bad guys already know that your email is the best target. Magic links just make that very explicit. That's a good point. I guess my rationale is that it being explicit makes me feel less comfortable for my parents/non tech-savvy friends, who already may not follow best-practices for email hygiene (and may not use email providers that enforce stricter hygiene like 2FA or other methods of protection) and thus, systems like this, make their email even more explicitly the ultimate place to go for access to stuff. reply notatoad 19 hours agorootparent>feel less comfortable for my parents/non tech-savvy friends, who already may not follow best-practices for email hygiene making people feel less comfortable is probably a good thing. i've managed to convince my dad to start taking his email security more seriously by reminding him a few times that if somebody gets access to his email, they can reset his password on every site where he uses that email address. it's good to remind people of why email security matters, and that it's not just about the personal messages from friends. reply adastra22 12 hours agorootparentprev> Essentially every service that would be inclined to use magic links would already have a way to reset your password entirely once the email is compromised Well, don't do that. reply lolinder 12 hours agorootparentDo you have an alternative proposal for letting users back into their accounts when they inevitably lose their passkey? Because if you don't, this isn't a serious answer. reply madeofpalk 6 hours agorootparentHow do you do account recovery when you lose a password or MFA token? Of course, any website's auth system is as weak (or strong) as their recovery process. Different sites will implement this differently. reply lolinder 5 hours agorootparentTypically by email, which OP says \"don't do\". reply adastra22 9 hours agorootparentprevPassword, not passkey. Recovery codes should be setup on account creation, but recovery of the password manager itself is what is required, and that usually has its own recovery mechanism. Social key recovery is an underutilized solution as well. reply adastra22 13 hours agorootparentprev> There are some positives about Magic Links for sure Like what? I'm failing to come up with a single benefit (for the user). reply apitman 11 hours agorootparentNot needing to remember passwords or use a password manager. reply adastra22 9 hours agorootparentPassword managers are now built into every operating system / browser, with trusted encrypted sync capabilities. The UX of using the built-in password manager is better than that of a magic link. reply Gigachad 20 hours agoparentprevPasskeys are in a transition period right now. There is no reason you have to have an alternative login method if you are using Passkeys, but no service has switched over to being Passkey only yet. Some users on older OSs / Linux might not be able to generate and store Passkeys yet, many users are not using a cross platform credential manager so if you've created passkeys with iCloud Passwords, there isn't a way to log in via linux right now. Give it a few more years and I suspect we will start to see services start with creating a passkey and never collecting a password. The passkey portability specs will be implemented, and hopefully Gnome/KDE implement passkey support. reply lolinder 20 hours agorootparentWhat does the final end state of passkeys look like? What happens if I lose the device I created the passkey on, if it gets bricked, or if I get banned by the platform that was supposed to be syncing my passkeys? reply skybrian 16 hours agorootparentPasskeys are essentially an API for logging into websites that requires a password manager to use. The end state is that we become completely dependent on our password managers. To avoid a single point of failure, hopefully you own multiple password managers, and they’re on independent devices, and there is a way to sync them. reply Gigachad 19 hours agorootparentprevI'd think it's exactly the same as using a password manager. Yes in theory you could memorize 500 unique passwords or write them down, but no one is doing that. There are a few things unique to passkeys though. You can register multiple passkeys for the same account so you could in theory have a physical USB key and cloud synced passkeys. Not many people would do this I would think though it would be easier than memorizing every password. There are also data portability specs in progress right now that let you export/import passkeys between services. But at the end of the day I would suggest that it should be straight up illegal for a company to freeze your account without letting you export your data. It probably actually is by the GDPR. This problem also already exists for email too. If Google bans you, you'll find a lot of your accounts become unusable. Anything with email OTPs wont work, and some services like Discord won't allow updating your email without access to the existing one. reply Ajedi32 46 minutes agorootparentIf your passkey manager account gets frozen the clients on all your devices should still have local copies of the passkey database that you could continue to use until you have a chance to export and migrate to another passkey manager. reply lolinder 19 hours agorootparentprevIt can't be the same as my password manager if email password reset flows disappear. If I lose access to my password manager but not my email, then I can go through and systematically reset all of the accounts that I remember exist. What you're describing with passkeys, though, would not allow me to do that. > But at the end of the day I would suggest that it should be straight up illegal for a company to freeze your account without letting you export your data. This would be great but it only addresses the least likely failure mode out of the ones that I brought up. And note that in many cases we're currently better off under the existing system if Gmail does ban you than we would be in your proposed world: only services that send OTPs on every login would be immediately inaccessibile, so you'll have time for most services to log in and switch to a new email address. reply Gigachad 18 hours agorootparentI think for most services you'd still be able to email reset your passkeys unless it's a particularly sensitive service, the kind which don't allow email resets of your 2FA tokens today. reply lolinder 18 hours agorootparentA password/passkey reset flow is semantically equivalent to an alternative login method and if done via email is semantically equivalent to a magic link. Which means that any service that claims to be passkey-only but supports email resets should just acknowledge that they support both magic links and passkeys as options—they're kidding themselves and their users if they pretend otherwise. reply Gigachad 15 hours agorootparentPasskeys are at least more convenient than magic links as they do not require opening an email or pulling your phone out for an SMS code. You're right though that they Passkeys + email reset is no more secure than email magic links, but I'd say email magic links are perfectly secure for most use cases. There really is no reason to continue using passwords these days and every website should switch to either magic links, Email OTP, or passkeys. For more sensitive accounts like bank accounts and government services. You'd probably have to go through some other reset process involving real ID and possibly an in person visit to a support location. reply Glyptodon 20 hours agoparentprevThe first thing I thought when I read this is how can the author make the specific criticisms of links/otp codes and then suggest passkeys, which have pretty much the same issues x10. Like if using a OTP from your phone or copying a link from your phone when using a work PC to visit a website is a pain, how much easier/better/same is it to try and have your work computer work with your personal passkey from a laptop or something? reply avianlyric 18 hours agorootparent> how much easier/better/same is it to try and have your work computer work with your personal passkey from a laptop or something? Passkeys support authentication via a secondary device over Bluetooth (and this is supported in every major browser on every major platform). So you can login to a site on a machine that’s completely disconnected from your personal passkey store by scanning a QR code with your personal phone. The login flow basically goes “request login with passkey” -> “browser recognises it doesn’t have the needed passkey, and offers a QR code to scan” -> “scan QR code with phone” -> “phone and browser handshake via Bluetooth” -> “passkey handshake happens between website and phone” -> “login completes”. I’ve personally used this flow with my work laptop and my personal iPhone many times. iOS has built in support for the Passkey QR codes, so you can scan the code with the standard camera app. Additionally iOS supports allowing 3rd party passwords managers to take over the Passkey flow once you’ve scanned the QR code. So in my case I complete the flow with 1Password. End-to-end the flow is pretty damn seamless, I’ve never personally had it fail, and take 30seconds to complete. The most annoying part is trying to remember where my phone is. reply apitman 11 hours agorootparentWhat if the target device is a workstation or library desktop with no Bluetooth? reply adastra22 13 hours agoprevI refuse to use any service that only supports magic links for auth. It is incredibly user-hostile, and absolutely worse from a security perspective than passwords (with a password manager). Most critically it simply does not work in my personal setup where I do not have access to my email account from the machine I am using to login, precisely for security reasons and the safety of my accounts. Anthropic has been the once exception to this personal policy simply because Claude is the best LLM out there. But it's a mountain of pain every time I have to re-login, and I've complained to them multiple times about this. reply budding2141 8 hours agoparent>absolutely worse from a security perspective than passwords Is it though? Majority (if not all) services I frequently use have email as recovery option for forgotten passwords. reply adastra22 8 hours agorootparentIt is certainly not all, and most security conscious sites offer other recovery options like one time use codes. Many also allow for time delayed account recovery, which aren’t a usable option for magic links. In any case the correct approach here is to fix password reset/account recovery (e.g. with social key recovery) rather than reduce everything to the lowest common denominator. It also can be said to lower security because it instills the behavior of clicking on links in incoming emails as a standard practice. reply apitman 11 hours agoparentprevSadly not enough people use password managers for it to be worth it for companies to target those flows. reply adastra22 9 hours agorootparent60% of Internet users rely on a password manager, and that number is still growing by a significant amount each year. reply apitman 9 hours agorootparentA quick search is indicating more like 30%, which honestly is way higher than I expected. Where are you seeing 60%? reply adastra22 8 hours agorootparentIt is ~30% for personal use, and ~60% when you include personal + work. So 30% use it in both contexts, and an additional 30% just for work. A combined 60% use password managers in some capacity in their work or personal lives. reply jerieljan 16 hours agoprevEvery time I see magic links, I always think: \"I thought we weren't supposed to click links in emails in the first place?\". When links in email come into mind, so does phishing. I hate these magic links a lot. reply crazygringo 14 hours agoparentIs anyone confused by that? Password reset links have always been sent to email. The point is",
    "originSummary": [
      "Magic Links provide a passwordless login method via email links, offering enhanced security but can be inconvenient due to email delays and multi-device use.",
      "They may pose security risks by encouraging users to access personal emails on work devices, highlighting the need for alternatives like One-Time Passwords (OTPs).",
      "For improved user experience, especially for tech-savvy users, passkeys are recommended as a more flexible and secure option."
    ],
    "commentSummary": [
      "Magic links, used for authentication, face reliability issues due to email problems and can push users towards biometrics, which may be legally compelled unlike passwords. - Passkeys are proposed as a more secure and user-friendly alternative to magic links, but their adoption is still ongoing, with many services yet to implement them. - The discussion on balancing security and usability in authentication methods remains active, highlighting the challenges in finding an optimal solution."
    ],
    "points": 653,
    "commentCount": 460,
    "retryCount": 0,
    "time": 1736283984
  },
  {
    "id": 42631873,
    "title": "Operating System in 1,000 Lines – Intro",
    "originLink": "https://operating-system-in-1000-lines.vercel.app/en",
    "originBody": "Hey there! In this book, we're going to build a small operating system from scratch, step by step. You might get intimidated when you hear OS or kernel development, the basic functions of an OS (especially the kernel) are surprisingly simple. Even Linux, which is often cited as a huge open-source software, was only 8,413 lines in version 0.01. Today's Linux kernel is overwhelmingly large, but it started with a tiny codebase, just like your hobby project. We'll implement basic context switching, paging, user mode, a command-line shell, a disk device driver, and file read/write operations in C. Sounds like a lot, however, it's only 1,000 lines of code! One thing you should remember is, it's not as easy as it sounds. The tricky part of creating your own OS is debugging. You can't do printf debugging until you implement it. You'll need to learn different debugging techniques and skills you've never needed in application development. Especially when starting \"from scratch\", you'll encounter challenging parts like boot process and paging. But don't worry! We'll also learn \"how to debug an OS\" too! The tougher the debugging, the more satisfying it is when it works. Let's dive into exciting world of OS development! You can download the implementation examples from GitHub. This book is available under the CC BY 4.0 license. The implementation examples and source code in the text are under the MIT license. The prerequisites are familiarity with C language and with UNIX-like environment. If you've gcc hello.c && ./a.out, you're good to go! This book was originally written as an appendix of my book Design and Implementation of Microkernels (written in Japanese). Happy OS hacking!",
    "commentLink": "https://news.ycombinator.com/item?id=42631873",
    "commentBody": "Operating System in 1,000 Lines – Intro (operating-system-in-1000-lines.vercel.ap...)602 points by ingve 11 hours agohidepastfavorite72 comments nuta 4 hours agoAuthor here. I wrote this book so you can spend a boring weekend writing an operating system from scratch. You don’t have to write it in C - you can use your favorite programming language, like Rust or Zig. I intentionally made it not UNIX-like and kept only the essential parts. Thinking about how the OS differs from Linux or Windows can also be fun. Designing an OS is like creating your own world—you can make it however you like! BTW, you might notice some paragraphs feel machine-translated because, to some extent, they are. If you have some time to spare, please send me a PR. The content is written in plain Markdown [1]. Hope you enjoy :) [1] https://github.com/nuta/operating-system-in-1000-lines/tree/... reply sesm 2 hours agoparent> Designing an OS is like creating your own world Or like building your temple so you can talk to God directly reply sohang 1 hour agorootparentTempleOS: https://en.m.wikipedia.org/wiki/TempleOS reply viraj_shah 2 hours agoparentprevHi OP, this looks super cool. I remember hearing about this (https://www.linuxfromscratch.org/) many years ago but have never done it. Curious, what are the prerequisites for this? Do I have to know about how kernels work? How memory management, protection rings or processes are queued? Some I'd like to definitely learn about. reply johnmaguire 1 hour agorootparentLFS is about building a Linux distribution from scratch (i.e. using the Linux kernel.) The book in question is about how to build your own operating system (i.e. a non-Linux) kernel from scratch. > We'll implement basic context switching, paging, user mode, a command-line shell, a disk device driver, and file read/write operations in C. Sounds like a lot, however, it's only 1,000 lines of code! reply johndoe0815 3 hours agoparentprevAre you planning to also provide an English translation of your microkernel book? That sounds very interesting... reply ilikeorangutans 4 hours agoparentprevThis is fantastic. Thank you so much for writing this. reply cplusplus6382 2 hours agoparentprevExcellent content! reply pjmlp 3 hours agoparentprevKudos for not being yet another UNIX clone tutorial. reply pshirshov 4 hours agoparentprevCould you port this to riscv64 please? reply exDM69 5 hours agoprevVery delightful article. Based on my experience in \"hobby\" OS programming, I would add setting up GDB debugging as early as possible. It was a great help in my projects and an improvement over debugging with the QEMU monitor only. QEMU contains a built-in GDB server, you'll need a GDB client built for the target architecture (riscv in this case) and connecting to the QEMU GDB server over the network. https://qemu-project.gitlab.io/qemu/system/gdb.html reply quruquru 1 hour agoparentAgree, and I'll add 3 other really useful QEMU features for osdev: 1) Record & Replay: Record an execution and replay it back. You can even attach GDB while replaying, and go back in time while debugging with \"reverse-next\" and \"reverse-continue\": https://qemu-project.gitlab.io/qemu/system/replay.html 2) The QEMU monitor, especially the \"gva2gpa\" and \"xp\" commands which are very useful to debug stuff with virtual memory 3) \"-d mmu,cpu_reset,guest_errors,unimp\": Basically causes QEMU to log when your code does something wrong. Also check \"trace:help\", there's a bunch of useful stuff to debug drivers reply jannesan 1 hour agorootparentthanks for sharing! qemu is very powerful, but it’s hard to discocer a lot of these features reply khaledh 4 hours agoprevVery cool to see someone tackling a small OS for RISC-V. Shameless plug: I've written hobby OS (well, a kernel actually) in Nim for x86-64[0] and it's all documented as well. I put its development on hold until I create a JetBrains plugin for Nim (in heavy development right now). [0] https://0xc0ffee.netlify.app/osdev reply ramon156 10 hours agoprevFor any rust enthusiasts, phil-opp's guide is such a fun exercise to try out. It was actually the first thing I tried in Rust (very silly idea) and ended up only understanding ~5% of what I had just typed out. I tried it again 2-3 years later and took the time to go over each subject. I even planned in advance to make sure I was going to finish it. reply quibono 2 hours agoparentHey, any chance you could link it please? reply vrnvu 2 hours agorootparenthttps://os.phil-opp.com reply quibono 1 hour agorootparentThank you! reply pm2222 36 minutes agoprevTwo projects mentioned by this one: https://github.com/nuta/microkernel-book/ https://github.com/mit-pdos/xv6-riscv reply sschmitt 6 hours agoprevCool! Will be interesting to compare to https://github.com/mit-pdos/xv6-riscv! Shameless plug for my html version of the xv6 book: https://xv6-guide.github.io/xv6-riscv-book/ reply pshirshov 6 hours agoparentxv6-riscv is 7000+ LoC. Half of that is userland utilities though. reply agentkilo 8 hours agoprevVery cool! I just started to dvelve into RISC-V, and the book I'm reading (not in English) offers their own emulator[1], which, at a glance, is much simpler than QEMU, and comes with a weird license[2]. I wonder if people actually used it, since it looks like an academic project. Maybe I can also follow this tutorial and test it out. [1] https://github.com/NJU-ProjectN/nemu/tree/master [2] https://github.com/NJU-ProjectN/nemu/blob/master/LICENSE Edit: wrong link reply corank 5 hours agoparentIt likely doesn't have performance that's good enough for production use. Doesn't look like there's JIT so it's all instruction by instruction interpreting. reply markus_zhang 6 hours agoparentprevYou can follow the MIT course and use QEMU if so wish. reply kwakubiney 5 hours agorootparentLink to the MIT course? reply markus_zhang 4 hours agorootparenthttps://pdos.csail.mit.edu/6.1810/2024/ reply davidw 2 hours agoprev> The tricky part of creating your own OS is debugging. The older I get, the more I think I can figure out most problems that don't require some really gnarly domain expertise if I have a good way to iterate on them: code something, try it, see the results, see how they compare with what I wanted. It's when pieces of that are difficult or impossible, or very slow, things get more difficult. reply unwind 9 hours agoprevVery nice, I always enjoy some low-level discussion like this. I found a small typo/editing glitch on the \"RISC-V 101 page\" [1]: - It's a trending CPU (\"Instruction Set Architecture\") recent years. It should probably say \"ISA\" instead of \"CPU\", and the word \"in\" is missing from after the parentheses, right? Edit: Markdown, don't format the quote as code. Oops. 1: https://operating-system-in-1000-lines.vercel.app/en/02-asse... reply nuta 4 hours agoparentSo do I :D I was wondering that too. I'll update it with other examples (x86 and Arm). reply ge96 3 hours agoprevThis is on my to do list of things to learn but I also don't know yet the purpose other than it being your own. Maybe security since most of what I work with is on top of the OS eg. programming languages. Maybe for RTOS applications at any rate this and OS Dev good resources. reply atan2 8 hours agoprevJust a small typo in the RISC-V 101 chapter. It says \"it's a trending CPU (ISA) recent years.\" I believe it should read \"in recent years.\" reply jraph 8 hours agoparentDon't hesitate to send the author an email, open a ticket or a PR to make sure they see this, it seems more appropriate than HN comments for this kind of things :-) https://operating-system-in-1000-lines.vercel.app/en/17-outr... reply wccrawford 7 hours agorootparentI'm sure they know that those things were possible. Perhaps you could have done it instead? They bothered to stop and suggest improvements here. That's enough work for them. They don't need to go elsewhere and do more, any more than you did. reply vanderZwan 7 hours agoprevThis looks nice! I would love to have an ebook version to read on my ereader. I wonder how much effort it would take to use the markdown files in the GH repo and convert those. [0] https://github.com/nuta/operating-system-in-1000-lines reply cheeseface 7 hours agoparentYou can clone the repo and install pandoc. Then run \"pandoc index.md *.md -o operating-system-in-1000-lines.epub\" in \"website/en/\" folder and you will have a fully working ebook. reply crowdhailer 3 hours agoprevI'd love to try implementing this in some other languages. reply dailykoder 8 hours agoprevThis looks nice. Thank you! Been considering something like that for quite a while now (Since my own risc-v CPU, written in VHDL, is working actually). I might get this as inspiration and rewrite it in Rust (tm) - Because I wnat to learn Rust, too reply ChrisMarshallNY 4 hours agoprevI never counted, but my first professional software project was an operating system (firmware) for an RF switching box. It was written in 8085 ASM, and was probably in the neighborhood of 1,000 lines. Apples to oranges, though. It was a specialized firmware system. Probably the biggest part was the IEEE-488 communications handler. reply com 11 hours agoprevI’m interested in the use of things like virtio instead of real hardware. Are there other virtualisation-driven designs for hardware devices out there rather than the qemu stuff? reply johndoe0815 9 hours agoparentVirtio is not qemu only. For example, the macOS virtualization framework on Apple Silicon Macs (and x86 machines IIRC) also provides virtio devices, other hypervisors. An overview of the available devices can be found in this presentation: https://crc.dev/blog/Container%20Plumbing%202023%20-%20vfkit... reply sim7c00 8 hours agoparentprevyou can try plain KVM its a bit more pain to get a system up tho compared to qemu which does a lot for you (--enable-kvm) reply globular-toast 11 hours agoprevI started a toy OS years ago based on the book Operating System Design by Douglas Comer. Personally I just couldn't get excited about anything that didn't run on real hardware, so I made mine for Raspberry Pi. Is there any real hardware that this could run on? Looking through this seems to use a lot of assembly. In the above the amount of assembly is kept to a minimum. Pretty much just bootstrapping and context switching. The rest is done in C. reply johndoe0815 10 hours agoparentComer was also my introduction to OS design and I still like the approach used in his Xinu books. I had a quick glance at the OS in the linked article. This seems to be based on a 32-bit RISC-V with MMU. However, AFAIK, all available RISC-V SoCs with MMU are 64-bit. The 32-bit cores are only used for embedded controllers (unless you want to start designing an FPGA-based system). The 32 and 64 bit versions of RISC-V are _not_ binary compatible, but the differences are rather small. Porting the MMU code from 64 to 32 bit or the other way round is not very complex, see my RV32 port of xv6 at https://github.com/michaelengel/xv6-rv32 (the regular MIT xv6 version only supports RV64). The major difference is that virtual address translation on RV32, sv32, uses a two-level page table (10 bit index for the first level, 10 bit index for the second and 12 bit offset) whereas there are several modes of translation for RV64. The most common one, sv39, uses 39 bits of the virtual address split into three 9-bit indexes (so you need a three-level page table for 4 kB pages) plus 12 bit offset. If you make the modifications, running the OS on real hardware should not be too difficult. The Allwinner D1 is a relatively simply RV64 single code SoC (boards can be found for $20 upwards from aliexpress) and getting the CPU and a UART to work is not that difficult. You can check out my xv6 port to the D1 as a reference: https://github.com/michaelengel/xv6-d1 reply cess11 1 hour agorootparentThat's encouraging, thanks for sharing. reply DrNosferatu 9 hours agoparentprevI guess this could run on the Raspberry Pico - RP2040 / RP2350. reply anonzzzies 8 hours agoprevAh yes, I have a few (too many as always, but I think that's good, especially when getting older; need to not get complacent) resolutions for 2025, one of them is to write a OS/DB with a development environment. Just to see how far I can take it. So these kind of tutorials are great. I was already going to make it RISC-V first because i'm interested. reply roetlich 9 hours agoprevThis looks great, thank you for making this. reply lproven 7 hours agoprevA noble idea, but Github is literally littered with hobbyist home-grown Unix-like kernels in C. As an industry are we not supposed to be trying to move away from hoary old unsafe C? Could we not have a hobbyist educational OSes in more of the C replacements? Drew DeVault wrote Bunnix in Hare, in one month. There's the proof of concept. How about tiny toy Unix-likes in Zig, Nim, Crystal, Odin, D, Rust, Circle, Carbon, Austral? How about ones that aren't ostensibly suitable for such tasks, such as Go or Ada? Yes I know Ada is not a good fit, but there has already been a Unix-like OS entirely implemented in a derivative of Pascal: TUNIS. https://en.wikipedia.org/wiki/TUNIS This might need work from skilled expert practitioners first. That's good. That's what experts are for: teaching, and uplifting newbies. There was a project to do C# on the bare metal. https://migeel.sk/blog/2023/12/08/building-bare-metal-bootab... How about a Unix-like in C#? Get the Unix and .NET folks interested in this stuff. Even if the OS never leads to anything, maybe the tooling might prove useful. I am sure someone somewhere would have uses for bare-metal GoLang. Saying that, I really don't think we need any more Unix-like OSes. There are far far too many of those already. There is a huge problem space to be explored here, and there used to be fascinating OSes that did things no Unix-like ever did. OSes that are by modern standards tiny and simple but explored interesting areas of OS design, and are FOSS, with code out there under permissive licenses: * Plan 9 https://github.com/plan9foundation/plan9 * Inferno https://github.com/inferno-os/inferno-os * Symbian https://github.com/SymbianSource * Parhelion HeliOS https://archive.org/details/Heliosukernel There is already an effort at Plan 9 in Rust: https://github.com/dancrossnyc/r9 Why not Plan 9 in Zig, or Hare, or even D? Plan 9 imposes and enforces considerably more simplicity on C as it is: you can't #include stuff that already has #include statements of its own. The result is a compilation speedup of around 3 orders of magnitude. That would be a benefit to the would-be C replacements too, wouldn't it? reply netbsdusers 4 hours agoparent> Yes I know Ada is not a good fit, but there has already been a Unix-like OS entirely implemented in a derivative of Pascal: TUNIS. Isn't it? There is a very well-developed kernel written in ADA with SPARK and formally verified at that: https://ironclad.nongnu.org And PASCAL-derived languages were very popular for operating systems in the 80s. To name a few: Apple's LISA OS, DEC's VAXELN, and OBERON. There were others as well that didn't quite make it, like DEC's MICA and Acorn's ARX. reply lproven 1 hour agorootparentThanks for this! I did not realise VAXELN was in Pascal. The others I did know of, yes, although Ironclad only from another comment in this thread. reply rep_lodsb 4 hours agoparentprevDijkstra famously said that programmers who started with BASIC are \"mentally mutilated\". But I think this applies a lot more to C and UNIX. Most of them don't seem to understand how anything substantially different could exist in the world of computing - every other language and operating system is seen as either an inferior copy, or as another layer of abstraction building on top of C and UNIX. reply Rochus 0 minutes agorootparent> Dijkstra famously said that programmers who started with BASIC are \"mentally mutilated\" He knew how to make friends. reply lproven 1 hour agorootparentprevExactly my own sentiments! reply Rochus 5 hours agoparentprev> Yes I know Ada is not a good fit Why? > https://en.wikipedia.org/wiki/TUNIS Interesting; do you know whether the source code is available somewhere? reply lproven 1 hour agorootparentNo, sadly, I don't know of any. You might ask co-developer Prof James Cordy: https://en.wikipedia.org/wiki/James_Cordy Or approach the University of Toronto: https://www.utoronto.ca/ reply Rochus 2 minutes agorootparentI had a look at the available literature. Many of the ideas are pretty similar to the work of Per Brinch Hansen a decade earlier. Actually the successor of Concurrent Euclid, called Object-Oriented Turing, is pretty interesting, because it is still a Pascal descendant, but supports modules, declarations in the statement flow, OO and concurrent programming. As it seems it's worth a closer look. reply twic 3 hours agoparentprevSPIN in Modula-2: https://www-spin.cs.washington.edu/ reply nj5rq 6 hours agoparentprevThere are very good kernels written in Ada, like Ironclad[1]. Besides, what's the point of this comment? What if people wanted to write a million more Unix-like kernels in C? Do you think this is bad? Why do you care? If you want, just write your own in whatever language you want, with whatever design you want. > Why not Plan 9 in Zig, or Hare, or even D? Because nobody to this point was interested in doing this. It's really that simple. [1] https://ironclad.nongnu.org/ reply amiga386 6 hours agorootparentI agree. the GP's comment has a flavour of \"people shouldn't like the things I don't like\". \"Make your own kernel\" is a thing-in-itself, and \"runs onhardware/VM\" + \"provides -like API for programs\" are tangible, concrete goals to aim for, even if you personally don't like theAPI or the architectural choices it implies. To give an analogy: https://www.nand2tetris.org/ is an amazing learning experience, even though games other than Tetris should and do exist Personally, I like the AROS project, aiming to provide an operating system that implements the AmigaOS APIs and runs on many architectures, but lots of users are interested in running it on 680x0 Amigas and spiritually-related PowerPC devices: https://en.wikipedia.org/wiki/AROS_Research_Operating_System It's OK for programmers to write a thing just for the learning experience. If it gains adoptees, that's a happy accident. reply lproven 6 hours agorootparentprev> There are very good kernels written in Ada, like Ironclad[1]. Interesting. Thanks. > Besides, what's the point of this comment? What if people wanted to write a million more Unix-like kernels in C? Do you think this is bad? Why do you care? Because it seems to me that modern OS design is caught in a deep deep rut, and the \"OS in 1000 lines\" article that we are discussing is digging that rut even deeper. Don't repeat the mistakes of the past. Make interesting new mistakes. It's more fun. reply pomatic 7 hours agoparentprevSymbian is anything but simple! reply lproven 6 hours agorootparent:-) Fair point. Very very small, compared to Android or iOS or any modern Linux, though. reply davio 3 hours agoprevSomeone probably has a python one-liner for it on leetcode reply 6forward 4 hours agoprevThis article brilliantly demonstrates the elegance of simplicity in systems design, proving that even complex concepts like operating systems can be demystified with clarity and minimalism. It's a reminder that understanding fundamentals often unlocks deeper innovation. How might this inspire rethinking other \"big\" systems? reply mnoronha 4 hours agoparentthis comment reads... a lot like chatgpt reply 6forward 3 hours agorootparentI always use an LLM to recraft my comments to ensure they are clear (English isn't my first language). Is that not allowed here? reply hansvm 3 hours agorootparentHN ostensibly tries to have thoughtful, engaged conversations. LLM grammatical cleanups and translations are absolutely fair game. The comment has certain hallmarks (broad, sweeping summary, certain word choices, ...) suggesting that an LLM had more creative liberty than HN really likes to see -- in sort of an Uncanny Valley situation, it's hard to tell if the LLM produced the post or if a person did (plus, somebody apparently went digging and didn't see a lot of value in your other comments while also finding self-endorsements, which paints this one in a worse light). You might have better luck with prompts that try to adhere better to your intent: > English is not my first language. Please correct the spelling/grammar and perhaps teach me up to one idiom that would fit well in the following comment, but leave the meat of the message largely the same. > Please translate the following [franglish/spanglish/(English mixed with a few words from your native language as appropriate to better convey your point)] to English suitable for a forum post. reply 6forward 3 hours agorootparentI appreciate this response. Thank you reply whatevermom 4 hours agorootparentprevIt’s just an alt used to promote some guy’s startup. You should look up it’s comments… reply unethical_ban 4 hours agorootparentAll the comments from the account are deleted. reply demarq 3 hours agoparentprevdang clean up on isle 10 reply qianli_cs 3 hours agoprev [–] The article nicely explains how to build a minimalist OS — works great as an intro material. I think understanding basic OS concepts is essential for performance tuning and debugging. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The book guides readers through building a small operating system from scratch, covering essential functions like context switching, paging, and file operations in C, all within approximately 1,000 lines of code.",
      "It emphasizes learning new debugging techniques, particularly for challenges like the boot process and paging, and provides downloadable examples on GitHub.",
      "The book is licensed under CC BY 4.0, with code under the MIT license, and requires familiarity with C and a UNIX-like environment as prerequisites."
    ],
    "commentSummary": [
      "Operating System in 1,000 Lines\" is a book that guides readers in creating an operating system from scratch using any programming language, not limited to C.",
      "The book emphasizes essential components of an OS, steering clear of a traditional UNIX-like structure, and aims to be an engaging and creative project.\"",
      "It invites contributions and feedback, offering content in Markdown format, and serves as a valuable resource for learning and experimenting with OS development.\""
    ],
    "points": 602,
    "commentCount": 72,
    "retryCount": 0,
    "time": 1736320730
  },
  {
    "id": 42633501,
    "title": "Cracking a 512-bit DKIM key for less than $8 in the cloud",
    "originLink": "https://dmarcchecker.app/articles/crack-512-bit-dkim-rsa-key",
    "originBody": "How We Cracked a 512-Bit DKIM Key for Less Than $8 in the Cloud In our study on the SPF, DKIM, and DMARC records of the top 1M websites, we were surprised to uncover more than 1,700 public DKIM keys that were shorter than 1,024 bits in length. This finding was unexpected, as RSA keys shorter than 1,024 bits are considered insecure, and their use in DKIM has been deprecated since the introduction of RFC 8301 in 2018. Driven by curiosity, we decided to explore whether we could crack one of these keys. Our goal was to extract the private key from a public RSA key, enabling us to sign emails as if we were the original sender. We were also curious to see whether emails signed with this compromised key would pass the DKIM verification checks of major email providers like Gmail, Outlook.com, and Yahoo Mail, or if they would outright refuse to verify a signature generated with such a short key. For our experiment, we chose redfin.com after discovering a 512-bit RSA public key at key1._domainkey.redfin.com (now no longer available): $ dig +short TXT key1._domainkey.redfin.com \"k=rsa; p=MFwwDQYJKoZIhvcNAQEBBQADSwAwSAJBAMx7VnoRmk/wFPeFWxrVUde6AJQI51/uPFL2CbiHGMnRSnLjPs72AgxAVHIe5QrNQ2riR5+7u47Sgh5R5va/d0cCAwEAAQ==\" Decoding the RSA Public Key The public key, located in the DKIM record’s p tag, is encoded in ASN.1 DER format, and further encoded as Base64. To decode the key to obtain the modulus (n) and the public exponent (e), we used a couple of lines of Python code: $ python3 >>> from Crypto.PublicKey import RSA >>> RSA.import_key('-----BEGIN PUBLIC KEY---- + 'MFwwDQYJKoZIhvcNAQEBBQADSwAwSAJBAMx7VnoRmk/wFPeFWxrVUde6AJQI51/uPFL2CbiHGMnRSnLjPs72AgxAVHIe5QrNQ2riR5+7u47Sgh5R5va/d0cCAwEAAQ==' + '-----END PUBLIC KEY----) RsaKey(n=10709580243955269690347257968368575486652256021267387585731784527165077094358215924099792804326677548390607229176966588251215467367272433485332943072098119, e=65537) Factorizing the RSA Modulus With the modulus n in hand, our next step was to identify the two prime numbers, p and q, whose product equals n. This process, known as factoring, can be quite challenging to perform efficiently. Fortunately, we found a powerful open-source tool called CADO-NFS, which offers an easy-to-use implementation of the Number Field Sieve (NFS) algorithm — the most efficient method available for factoring large integers. Since factoring takes a lot of computing power and we didn’t want to tie up our computers for days, we went with renting a cloud server. We chose a server with 8 dedicated vCPUs (AMD EPYC 7003 series) and 32 GB of RAM from Hetzner, installing Ubuntu as the OS. Setting up CADO-NFS was straightforward: git clone https://gitlab.inria.fr/cado-nfs/cado-nfs.git cd cado-nfs make To ensure the server had enough memory for the task, we added 32 GB of swap space: sudo fallocate -l 32G /swapfile sudo chmod 600 /swapfile sudo mkswap /swapfile sudo swapon /swapfile We started the factorization by calling the cado-nfs.py script with n as the input: ./cado-nfs.py 10709580243955269690347257968368575486652256021267387585731784527165077094358215924099792804326677548390607229176966588251215467367272433485332943072098119 The process took approximately 86 hours on our 8-vCPU server, successfully factorizing n into p and q: Info:Complete Factorization / Discrete logarithm: Total cpu/elapsed time for entire Complete Factorization 2.20529e+06/309865 [3d 14:04:25] 97850895333751392558280999318309697780438485965134147739065017624372104720767 109447953515671602102748820944693252789237215829169932130613751100276125683257 Although opting for a more powerful server or distributing the workload across several systems (a process simplified by CADO-NFS) could have expedited the task, we weren’t pressed for time and didn’t mind the wait. Constructing the RSA Private Key After determining p and q, we had all the necessary components to construct the RSA private key. The process involved using Python and the PyCryptodome library: $ python3 >>> from Crypto.PublicKey import RSA >>> from Crypto.Util.number import inverse >>> p = 97850895333751392558280999318309697780438485965134147739065017624372104720767 >>> q = 109447953515671602102748820944693252789237215829169932130613751100276125683257 >>> e = 65537 >>> n = p * q >>> phi = (p-1) * (q-1) >>> d = inverse(e, phi) >>> key = RSA.construct((n, e, d, p, q)) >>> private_key = key.export_key() >>> print(private_key.decode()) These Python lines output the private key in PEM format, ready for use: -----BEGIN RSA PRIVATE KEY----- MIIBOgIBAAJBAMx7VnoRmk/wFPeFWxrVUde6AJQI51/uPFL2CbiHGMnRSnLjPs72 AgxAVHIe5QrNQ2riR5+7u47Sgh5R5va/d0cCAwEAAQJAPliEv2dKk4DyA54nbwEH mSzfLEOiuD8dKXZW9GpMhou72DYYcc5YD0PeQW0uGGsusnTZXRU3Kd3cmVfeR+np 4QIhANhVpOQ440Gqlda3nqCOAag12jq8ET+qr1G7VL8x9PF/AiEA8flYr5rUO6Io /5HRoHq6p7dA75PRK+7v79o0/ijfTjkCIEdWPpCPfckKomxykllpWnyIfZT+rUVs WHHAL1r480erAiAz3xD87ALtGbESQE8gyM50n5sjAJwJf/odf7h2d4qPOQIhAKwr Nv6s5cQiwbYgm1KND83nrkxe6uFQlu9ilkdwAIY4 -----END RSA PRIVATE KEY----- Sending DKIM-Signed Test Mails From @redfin.com With the RSA private key integrated into our OpenDKIM setup, we proceeded to the testing phase. We crafted a simple email with a FROM address of security@redfin.com, then sent it to a variety of email hosting services. Although most providers correctly identified the 512-bit key as insecure and rejected our DKIM signature, three major providers — Yahoo Mail, Mailfence, and Tuta — reported a dkim=pass result. Here’s how each provider responded: Gmail: FAIL Outlook: FAIL Yahoo Mail: PASS Zoho: FAIL Fastmail: FAIL Proton Mail: FAIL Mailfence: PASS Tuta: PASS GMX: FAIL OnMail: FAIL Given that redfin.com also has a valid DMARC record (v=DMARC1;p=reject;pct=100;rua=mailto:a+99923342@fdmarc.net;ruf=mailto:f+99923342@fdmarc.net;ri=3600;fo=1;), passing the DKIM check for redfin.com also means that our email passed DMARC verification and met the requirements for BIMI. Final Thoughts Three decades ago, breaking a 512-bit RSA public key was a feat achievable only with a supercomputer. Today, it’s possible to do so in just a few hours for less than US$8 on a cloud server. And if you have a powerful computer at home with 16 or more cores, you could accomplish this even more swiftly and cost-effectively. There’s no good reason to use 512 or 768-bit keys these days. Email providers should automatically reject any DKIM signature generated with an RSA key shorter than 1,024 bits. We’ve alerted Yahoo, Mailfence, and Tuta about our findings and shared this advice with them. Domain owners must also take action by reviewing their DNS settings for any outdated DKIM records that don’t comply with the 1,024-bit minimum standard. A simple way to check a DKIM record’s p tag is to count its Base64 characters: a 1,024-bit RSA public key will have at least 216 characters.",
    "commentLink": "https://news.ycombinator.com/item?id=42633501",
    "commentBody": "Cracking a 512-bit DKIM key for less than $8 in the cloud (dmarcchecker.app)421 points by awulf 6 hours agohidepastfavorite202 comments jgrahamc 5 hours agoMe writing over 14 years ago: https://blog.jgc.org/2010/06/facebooks-dkim-rsa-key-should-b... This was doable 14 years ago for 512-bit keys. reply matthewdgreen 5 hours agoparentFor a number of years it was (non-officially) thought to be a feature to use weak DKIM keys. Some folks argued that short keys allowed you to preserve deniability, since DKIM signatures would only be short-lived and nobody would be able to use DKIM signatures to prove that any email was authentic. (I’m not saying that this is why most companies used short keys, just that there was a general view that short keys were not all bad.) DKIM deniability is a particular hobbyhorse of mine [1]. These short keys are obviously not the way to achieve it, but I view this kind of thing as a result of the muddled thinking that’s been associated with DKIM and the community’s lack of desire to commit to the implications of an all-email signing mechanism. [1] https://blog.cryptographyengineering.com/2020/11/16/ok-googl... reply linsomniac 5 hours agorootparentI forget where but someone proposed regularly rotating your DKIM key and publishing old keys for deniability. So you can still use strong keys and provide a level of deniability. reply jabart 2 hours agorootparentDoing this only provides deniability in public. If this was brought to a court there are enough server logs to build out if that DKIM record was valid along with a number of DNS history providers. reply singpolyma3 2 hours agorootparentAs if courts care about any of that. They'll just ask the witness \"did you send this email\" reply irrational 1 hour agorootparenthttps://xkcd.com/538/ reply rsc 4 hours agorootparentprevIn the link in the parent comment. :-) reply Ar-Curunir 2 hours agorootparentprevThat was Matt Green, the person you replied to =) reply GTP 4 hours agorootparentprevIsn't deniability at odds with DKIM's goal? What would be the point of setting DKIM then? Sure, it helps with spam scores. But most companies rely on a major email provider to send emails, so maybe they wouldn't have deliverability issues anyway? reply tptacek 44 minutes agorootparentNo. DKIM is meant to apply to emails in transit; it is part of the transaction of exchanging emails. But DKIM signatures are verifiable long after that transaction has completed. That was not an intended feature of DKIM, and it's a grave privacy violation. To satisfy DKIM's design goal, you only need a \"current\" DKIM key that is secure for a window of time. When that window of time passes, you rotate the secret and publish your own key, repairing (hopefully) much of the privacy injury. reply CodesInChaos 4 hours agorootparentprevWhen using deniable authentication (e.g. Diffie-Hellman plus a MAC), the recipient can verify that the email came from the sender. But they can't prove to a third party that the email came from the sender, and wasn't forged by the recipient. reply jagged-chisel 3 hours agorootparentMallory sends a message, forged as if from Alice, to Bob. How can Bob determine that it came from Alice and wasn’t forged by Mallory? reply tialaramex 2 hours agorootparentNo, no, in these systems Alice and Bob both know a secret. Mallory doesn't know the secret, so, Mallory can't forge such a message. However, Bob can't prove to the world \"Alice sent me this message saying she hates cats!\" because everybody knows Bob knows the same secret as Alice, so, that message could just as easily be made by Bob. Bob knows he didn't make it, and he knows the only other person who could was Alice, so he knows he's right - but Alice's cat hatred cannot be proved to others who don't just believe what Bob tells them about Alice. reply commandersaki 3 hours agorootparentprevThis is always a possibility but I'm guessing the happy path is the more likely believable scenario and could be verified OOB. reply Ar-Curunir 2 hours agorootparentprevThe idea is to use a MAC instead of a signature. As long as Alice isn't compromised and sharing her key with Mallory (which she could do even in the signature case), when Bob receives a message with a valid MAC on it, he knows that Alice authorized the message. reply matthewdgreen 12 minutes agorootparentprevDo take a look at the blog post I cited above, where I go into this in loads of detail. The TL;DR is that DKIM only needs to ensure origin authenticity for the time it takes to deliver an email, which is usually a few hours or a day at most. The unintentional problem DKIM is causing is that it actually provides non-repudiation for many years. Those signed emails can sit in someone's mailbox for years, then get stolen by a hacker. The hacker can then blackmail the owner by threatening to dump the email trove, or for newsworthy targets they can just do it. Reasonable people (e.g., high-integrity newspapers, courts of law) will say \"how can we trust that these stolen emails are authentic given that there's no chain of custody?\" DKIM signatures nearly answer that question, which makes stolen emails much more valuable than they would be otherwise. reply marcosdumay 4 hours agorootparentprevYes. Deniability and perfect forward secrecy are at odds with how people use email anyway. But that doesn't stop people from demanding both very loudly, and some people from promising them. reply matthewdgreen 5 minutes agorootparentI don't know what you're saying. Most email isn't encrypted so PFS wouldn't apply to the content of email messages. When you mention PFS are you talking about email encrypted with something like GPG/PGP? Or are you talking about PFS in the context of the TLS connections used to protect SMTP? I'm not sure what \"deniability\" refers to in your post and how you think people use email in ways that would contraindicate it. Once I understand what you're saying, I guess we could move on to who the demanders and promisers are and what you think they're doing. reply ralferoo 3 hours agorootparentprevThat's not quite what he's saying. DKIM's goal is that the receiving system can trust that the message presented to it has come from a host that knows the key, and so could sign it. At the time that message is received, you should be able to assume that only people authorised to send from that domain are able to do so. By publishing older keys, you gain deniability back, because anybody could have forged the message and signed it. Even if you have timestamps that show when you received the message, the onus is one you to prove that those timestamps are not fabricated. This is a much harder problem because secrets can be revealed later but not forgotten. To be able to prove that you did it fact receive the message on that date, you'd probably have to record e.g. the message ID, signature and timestamp (perhaps all encrypted with a key that you can reveal later) on a blockchain, which would then serve as proof of when the message was received. Even then, you'd still have to prove that the key hadn't been disclosed before that time. reply dfc 4 hours agorootparentprevYou should read the blog post Prof. Green linked to. All of your questions are addressed there. reply GTP 2 hours agorootparentYes, I've now read it and it answered those questions. But it also stumbled upon an easier possible solution without realizing it: if you wish to pretend you didn't send some emails, you can still claim that someone stole your password. Whether this claim will be believed or not, is independent of DKIM. Spoofing is a better excuse than a stolen password only in the case of a single email. If there's a conversation spanning multiple messages, a spoofer wouldn't be able to properly answer the messages of the other party, as he doesn't receive them. reply tptacek 43 minutes agorootparentIf you publish DKIM keys, you don't have to convince anybody that you were targeted by someone who stole your password, because your stolen email spool no longer reveals to attackers the authenticity of your emails, which is what you as a user want. reply IncRnd 36 minutes agorootparentprevIf someone lies that their password was stolen to hide them being the originator of a single email, who are they telling the lie? Is it a court? In many cases the person would be caught through behavioral analysis. They didn't change their password, so it wasn't really stolen. No other emails were improperly authored, so it wasn't really stolen. They never reported it to the email server owner, so it wasn't really stolen. Lying isn't a stronger defense to perform abuse than weak keys are for stopping abuse. reply lupusreal 4 hours agorootparentprevMeanwhile in the real world, screenshots of emails without any cryptographic authentication at all are good enough to send people to prison. reply tptacek 41 minutes agorootparentThe issue here isn't rules of evidence, it's user privacy. In the real world, DKIM has repeatedly been used to violate privacy. More importantly: latent verifiable secure DKIM signatures on archived emails offer no value to users; they literally only have real-world value to attackers. reply anonym29 3 hours agorootparentprevI absolutely believe you - lawyers, judges, and juries alike tend to be technologically illiterate, but do you have any references/links to this happening? reply matthewdgreen 1 minute agorootparentIf you read the blog post, you'll see that newspapers frequently verify DKIM signatures on stolen email corpora before they publish about them. This is one example of \"a screenshot\" not being enough. OJFord 1 hour agorootparentprevIn particular where the defendant denies the veracity of the screenshot. reply ls612 2 hours agorootparentprevnext [2 more] [flagged] throitallaway 2 hours agorootparentIt's tiring that most things have to come back to politics nowadays. reply phkahler 4 hours agorootparentprevWhat's the problem with verifiable email? You can just delete old emails (if gmail or whoever archives them that's another problem). Most people probably operate under the assumption that email is verifiable in some way anyway. If you receive malicious email, isn't it a good thing to be able to verify where it came from? Doesn't that benefit senders and receivers alike? Or is this a case of \"I want to verify you, but I don't want you to verify me\"? reply infogulch 4 hours agorootparentIt's more \"I want you to be able to verify I sent an email to you, but I don't want you to be able to prove to a third party that I sent it.\" The fact that this is possible is some cryptography black magic. reply meindnoch 3 hours agorootparentI don't see how this could be possible. If I have some information which I can use to prove that you were the sender, then I can just share the same information with a third party, and they can verify just the same. reply d1sxeyes 2 hours agorootparentSimple, I tell you that in my message, if it’s genuine, I’ll use the word “apple” somewhere. You tell me that you’ll use the word “banana”. Provided no-one except you knows that my secret word is “apple”, you know the message came from me. But it’s perfectly possible for you to fake a secret message and sign it “Love d1sxeyes, P.S. apples”, and so our approach only works between the two of us. A third party probably can’t even confirm that “apples” was the correct keyword, and even if they could, that can only prove that one of us wrote it, they can’t definitively say which one of us. Now extrapolate this to using some more sensible mechanism than dictionary words. reply kstrauser 3 hours agorootparentprevThe idea is that for spam filtering purposes, you can prove this morning that the email I sent you this morning came from me, because I’m the only person who had the signing key on it. Anyone else could validate that too. But let’s say I publish that signing key tomorrow. Once I do that, you can’t prove I sent today’s mail because anyone could’ve used that published key tomorrow forget the signature. reply meindnoch 2 hours agorootparentOk, so there's a time window where it's possible to prove that you were the sender. And if I use a qualified timestamp service to sign all messages arriving in my inbox, then I can prove that you were the sender indefinitely. reply kstrauser 2 hours agorootparentSomething like that, as long as you can also prove I hadn’t published the key prior to that. If I publish at random times and to random URL, that may be challenging. reply freedomben 2 hours agorootparentYes, but then it also encroaches on ability to verify that you were the sender when receiving the original email. Basically, unless the recipient also checks whether the current DKIM key has been published, then they can't trust it because it may be published. If it's being published at random times and to a random URL, then it's nearly impossible to actually check. So I agree that it brings deniability, but I don't agree that it still meets the original purpose of verifying the sender. reply kstrauser 2 hours agorootparentThat’s all true, but I bet 99.99% of all email is delivered within a minute or so of being send. There are exceptions, of course, but in practice the whole thing is pretty fast. So there’s some threat modeling, too. Are you trying to email someone highly adversarial? Maybe you’re at a law office or such and that’s the case! This wouldn’t help a lot there. Not everyone is immediately forwarding all inbound mails to a timestamping service though. (I don’t have skin in this game, so I’ll probably duck out after this. I don’t have opinions on whether publishing old DKIM keys is good or bad.) reply infogulch 2 hours agorootparentprevYes it seems crazy, but besides the obvious \"leak your own key\" as other comments mentioned, this is actually possible. This is one of the biggest discoveries in cryptography in the last decades and its implications are still being researched. I dug around and found this article which seems to do a pretty good job describing the cryptographic concepts of \"non-transferability\" / \"deniability\" / \"deniable authentication\" for a lay audience: https://dinhtta.github.io/zkp/ Also: https://en.wikipedia.org/wiki/Deniable_authentication reply meindnoch 1 hour agorootparentHmm. So basically any protocol with a shared key? I.e. a symmetric key is shared between you and me. If I receive a message with that key, I know it's from you because the key is only known by you and me, and I know I wasn't the sender, so it must be you. But any third party can only say that the message was by one of us. reply toast0 1 hour agorootparentprevIt's pretty simple with a couple concepts. If Alice and Bob each have a public/private key pair, they can do a diffie-hellman key exchange to form a common secret. If they use that secret to authenticate a message, then it can be shown that only Alice or Bob sent the message. If you're Alice or Bob, this is what you want to know --- either you or your corespondent sent it, and if you didn't send it, your correspondent did. But if Alice or Bob asks Carol to validate it, Carol can only determine that Alice or Bob sent it, and perhaps not even that. Anyone in possession of the secret used to authenticate the message can also make a message that would be deemed authentic. If you have participation of Alice or Bob, you can show that the secret was derived from DHE with Alice and Bob's key pairs, but that's all. This is nifty and useful, but it's more appropriate for end to end communication, which is not the domain of DKIM. Email is a multi point store and forward system, where each point would like to know if a message is authentic; deniability like this would probably mean either a) only the final destination could determine authenticity and therefore intermediates would not be able to use authenticity as a signal to reject mail b) only the first intermediate could determine authenticity; it could be used to reject mail, but the end user would have to trust the intermediate Both of these are workable systems, but DKIM provides that all intermediates and the end user can know the mail was authorized (to some degree) by the origin system. reply erikerikson 1 hour agorootparentprevBoth parties use the same key: https://en.m.wikipedia.org/wiki/Symmetric-key_algorithm reply devmor 2 hours agorootparentprevImagine that yesterday, I had only one house key, and I left a copy of it in your mailbox so you could come into my house to borrow a cup of sugar while I was out. You can be sure I allowed it, because you know I had only one key. Today, I made 3 copies of my housekey and gave them to friends. You still know that I was the one that allowed you entry into my house, but you can not prove to anyone else that I was the one that made the copy, because there are now 3 other people that could do that. (For this example, imagine I made the key copies at home and didn't go to a locksmith who could verify when they were made, since we don't need a locksmith to do software crypto) reply codegladiator 14 minutes agoparentprevYour blog got a mention in the comments of http://www.wired.com/threatlevel/2012/10/dkim-vulnerability-... reply brightball 2 hours agoparentprevReminds me of the story of a guy cracking one thinking it was a Google headhunting challenge. https://www.wired.com/2012/10/dkim-vulnerability-widespread/ reply stingraycharles 1 hour agoparentprevJust as an interesting FYI for those who care: the parent poster is the CTO of CloudFlare. reply raverbashing 5 hours agoparentprev> \"Keys of 512 bits have been shown to be practically breakable in 1999 when RSA-155 was factored by using several hundred computers and are now factored in a few weeks using common hardware.\" So we went to a few weeks to 8h in 14 years give or take reply bee_rider 3 hours agorootparent86 hours. 8 dollars. I wonder how scalable it is. They only used: > We chose a server with 8 dedicated vCPUs (AMD EPYC 7003 series) and 32 GB of RAM from Hetzner Not very beefy really. Beating this time is easily in range of, what, millions of people high end gaming machines? reply andix 2 hours agorootparentI'm wondering if this process can be GPU optimized. It's possible to rent really beefy GPU cloud instances for a few bucks per hour.Probably because RSA 2048 is not yet broken […] 3072 has been recommended by various parties for a few years now: * https://www.keylength.com reply jandrese 2 hours agorootparentIs there a compelling reason to use 3072 instead of 4096? If you're going to kick the can down the road you might as well put some effort into it. The difference in memory use/compute time has to be marginal at this point. It's not like the old days when jumping from 512 to 4096 made the encryption unusably slow. reply daneel_w 1 hour agorootparentThere's no good reason at all, which is why RSA-3072 is the rarely seen \"oddball\". reply throw0101c 27 minutes agorootparent> There's no good reason at all Operations per second? * https://wiki.strongswan.org/projects/strongswan/wiki/PublicK... Running MacPorts-installed `openssl speed rsa` on an Apple M4 (non-Pro): version: 3.4.0 built on: Tue Dec 3 14:33:57 2024 UTC options: bn(64,64) compiler: /usr/bin/clang -fPIC -arch arm64 -pipe -Os -isysroot/Library/Developer/CommandLineTools/SDKs/MacOSX15.sdk -arch arm64 -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX15.sdk -DL_ENDIAN -DOPENSSL_PIC -D_REENTRANT -DOPENSSL_BUILDING_OPENSSL -DZLIB -DNDEBUG -I/opt/local/include -isysroot/Library/Developer/CommandLineTools/SDKs/MacOSX15.sdk CPUINFO: OPENSSL_armcap=0x87d sign verify encrypt decrypt sign/s verify/s encr./s decr./s rsa 512 bits 0.000012s 0.000001s 0.000001s 0.000016s 80317.8 973378.4 842915.2 64470.9 rsa 1024 bits 0.000056s 0.000003s 0.000003s 0.000060s 17752.4 381404.1 352224.8 16594.4 rsa 2048 bits 0.000334s 0.000008s 0.000009s 0.000343s 2994.9 117811.8 113258.1 2915.6 rsa 3072 bits 0.000982s 0.000018s 0.000019s 0.000989s 1018.4 54451.6 53334.8 1011.3 rsa 4096 bits 0.002122s 0.000031s 0.000032s 0.002129s 471.3 31800.6 31598.7 469.8 rsa 7680 bits 0.016932s 0.000104s 0.000107s 0.017048s 59.1 9585.7 9368.4 58.7 rsa 15360 bits 0.089821s 0.000424s 0.000425s 0.090631s 11.1 2357.4 2355.5 11.0 (Assuming you have to stick with RSA and not go over to EC.) reply jandrese 22 minutes agorootparentIt's roughly half as fast as 4096, which sounds bad until you realize that 3072 is already 20% as fast as 2048, 3% as fast as 1024, and 1% as fast as 512. In terms of performance tradeoff it's downright mild compared to the other steps up. reply throw0101c 13 minutes agorootparentIf I could waive a magic wand and get a 40-100% performance boost on a service by changing four characters (s/4096/3072/) why wouldn't I take it? (Assuming I need security go to beyond RSA 2028.) reply bluGill 4 hours agorootparentprevRSA 2048 isn't broken, but experts consider it a matter of time. How long I don't know, but since the attacks are known (prime numbers) someone (read not me) can make an estimate with error bars that are concerning enough to consider it as good as broken. reply tptacek 3 hours agorootparentWhat expert considers it a matter of time before 2048 is broken? 2048 is 112-bit-equivalent security. reply bluGill 15 minutes agorootparenthttps://www.keylength.com/en/4/ NIST says 2048 bit RSA is good until 2030. I'm not sure what that means, perhaps that it will be broken considering advances, perhaps just that someone (read governments) who cares to spend 5 years on the problem will break your key. reply lostmsu 3 hours agorootparentprevAFAIK even RSA 1024 isn't broken yet. reply daneel_w 2 hours agorootparentRSA-1024 is \"only\" 80 symmetric equivalent bits. It's a space requiring a tremendous amount of energy to explore, though I personally consider it very likely that the NSA and/or the MSS et al. have poured immense funds into accelerators specifically targeting RSA, and for them there'd be no obstacles at all to be granted an allocation for such energy consumption. reply upofadown 2 hours agorootparentprevBut perhaps with not a very solid justification to do so: * https://articles.59.ca/doku.php?id=em:20482030 reply woodruffw 5 hours agorootparentprevEd25519 has seen broad adoption in TLS and other stacks that are used pervasively where DKIM is also used. What’s blocking it for DKIM uniquely? (This isn’t intended as a leading question.) reply Avamander 4 hours agorootparentX25519 has seen broad adoption (in the key exchange). Ed25519 has not, you can't actually use an Ed25519 certificate on the web. It's in a deadlock between CAs, browsers and TPM manufacturers (and to some extent NIST, because Ed25519 has not been approved by them). It's not being blocked per se, you can use it mostly (98%) without any issues. Though things like Amazon SES incorrectly reject letters with multiple signatures. Google and Microsoft can't validate them when receiving. It's more that a few common implementations lack the support for them so you can't use _just_ Ed25519. reply daneel_w 5 hours agorootparentprevEveryone has to be onboard before the switch can be made, and not everyone is happy about the somewhat messy solution of running dual-stack RSA+Ed25519 setups in the interim - it's a bit different than e.g. supporting multiple methods for key exchange or multiple ciphers for symmetric crypto. It's just one of those things that take time to happen. reply bluGill 4 hours agorootparentIf the big players (gmail, outlook) jump onto it the rest will be forced to follow. Outlook would probably jump in with a checkbox, and perhaps gmail will make at an option for the paid tier while everyone on the free tier gets no choice - but that is still enough. SmallCompany.com cannot do it alone, probably not even a fortune100.com (if any of them even care - their sales staff probably will overrule the few who do), but there are enough players if they all agree on something. Getting the big players to agree and execute though is a lot like herding cats. I'm sure some in the big players are trying. reply paulnpace 4 hours agorootparentprev> What’s blocking it for DKIM uniquely? Mail server administrators. reply tptacek 3 hours agoparentprevWe are. 1024-bit keys are being retired across cryptosystems everywhere, and have been for over a decade (don't get me started on the one laggard). Nothing threatens 2048 bit keys other than QC, which threatens RSA altogether. Progress isn't linear; it's not like 2048 falls mechanically some time after 1024 (which itself is not practical to attack today). reply 1oooqooq 3 hours agorootparentwe are definitely not. most countries registrar's won't support DNS hacks requied for larger dkim. we still use the minimum key size in most countries. reply Avamander 3 hours agorootparentWhat? Just use normal name servers. The registrar doesn't matter one bit, they delegate the zone to whatever name servers you specify. Those can serve whatever records properly. reply Suzuran 5 hours agoparentprevBecause the only way to force their use is to break things, mostly this means transferring the pain directly to the user instead of the service operators in the hope that they will bitch loudly enough for the service operator to care, and this has a good chance of instead causing the user to move to your competitors instead, who will be more than willing to not let a little thing like security get between them and revenue. reply elif 5 hours agoparentprev512 DKIM was exceedingly rare even 8 years ago when I worked in email. You're essentially asking \"why aren't we doing what we're doing\" reply layer8 39 minutes agoparentprevLinear bit size increases require exponential compute increases to break. RSA with 1024 bits is still beyond any practical capability to break. The current practical limit is considered to be around 800-something bits. Still the recommendation is to use at least 3000 bits nowadays, to defend against possible mathematical advances. reply Retr0id 36 minutes agorootparentThis is incorrect. Factoring algorithms like GNFS are super-polynomial but sub-exponential. RSA-1024 is likely breakable at practical-but-very-expensive costs. reply croes 5 hours agoparentprev> Even simple things like forcing TLS 1.3 instead of 1.2 from client side breaks things...including hn site. That’s the reason, it breaks things, and some of them are important and can’t simply be updated. reply wolrah 4 hours agorootparent> That’s the reason, it breaks things, and some of them are important and can’t simply be updated. IMO this is not a valid excuse. If it's exposed to the internet it needs to be able to be updated with relative ease to respond to a changing threat landscape. Especially if it's \"important\". If it cannot be then it is already broken and needs to be fixed. Whether that fix is doing a hard upgrade to get to the point that future upgrades can be easier, entirely replacing the component, or taking the thing offline to a private non-Internet network depends on the situation, but \"we aren't going to change, the rest of the internet should conform to us\" has never been a reasonable response. This is particularly true in the contexts of public mail servers where DKIM matters and anything involving public use of TLS. The rest of the internet should not care if your company refuses to update their mail servers or replace their garbage TLS interception middleboxes. We should be happy to cause problems for such organizations. reply marcosdumay 4 hours agoparentprevThe key sizes we use today are expected to hold against a Dyson sphere focused on breaking them with the best exploit we know today. What size do you suggest? reply RobotToaster 3 hours agorootparentIt's not quantum-safe though. reply Avamander 3 hours agorootparentIt's not quantum-broken though, it might just make it a bit faster. Just half a Dyson sphere. reply marcosdumay 2 hours agorootparentprevLarger keys won't make the algorithms quantum-safe either. reply Tostino 2 hours agorootparentIf I'm not mistaken, larger keys require more qbits in a machine to all be coherent together to be able to break it. So it would be a slight increase in complexity, but if we are able to build a machine with enough qbits to crack 1024 keys, I don't think the engineering is all that far off from slightly scaling things up 2x-10x. reply dist-epoch 1 hour agorootparentprevWhich is why post-quantum algos were invented. reply TacticalCoder 19 minutes agorootparent> Which is why post-quantum algos were invented. Yup. And I don't even think quantum resistance was the goal of some of the algos that, yet, happen to be believed to be quantum resistant. Take \"Lamport signatures\" for example: that's from the late seventies. Did anyone even talk about quantum computers back then? I just checked and the word \"quantum\" doesn't even appear in Lamport's paper. reply xondono 5 hours agoparentprevWe are doing that, just not everyone is as concerned by safety and make different tradeoffs against things like ease of use or accessibility. Different applications have different tolerances and that’s fine. If and when anything quantum is able to yield results (I wouldn’t worry much about this), increasing key size is pretty much meaningless, you need to move to other encryption schemes (there’s lots of options already). reply daneel_w 5 hours agorootparentIn the case of RSA it's not meaningless to increase key size to fend off quantum computers. Quantum computing vs RSA is a case of being the largest contender, because quantum computing in itself doesn't definitively unravel the integer factorization problem. reply inetknght 2 hours agoparentprev> Could someone help me understand why we're not dramatically ramping up key sizes across the board on all encryption? Not as a solution, but as a buy-some-time measure. I am acutely aware that there are SOME places where software only supports RSA and only supports up to 1024-bit or 2048-bit keys, and that is a legal requirement. Ramping up key sizes would be great but even 2048-bit keys aren't quite secure against certain kinds of actors (even disregarding hammer-to-head style of attacks) > Even simple things like forcing TLS 1.3 instead of 1.2 from client side breaks things ... kind've a case in point about the pace of required improvements. reply paulnpace 4 hours agoparentprevIn the case of DKIM, Ed25519. reply formerly_proven 5 hours agoparentprevCompare the time it takes to generate or decrypt/encrypt 4096 bit RSA versus 16384 bit RSA (it's not four times slower). reply Havoc 5 hours agorootparentIndeed. There has got to be some middle ground there though that is both an incremental improvement and still within reason on cost reply tonymet 2 hours agoparentprevKey rotation tooling has never been given adequate attention since you only do it every few years. Something ends up breaking, even if it’s just the name of the key . keys are stateful content like DB schemas, but they don’t receive daily attention, so the tooling to maintain them is usually ad-hoc scripts and manual steps. reply littlestymaar 5 hours agoparentprev> Could someone help me understand why we're not dramatically ramping up key sizes across the board on all encryption? Not as a solution, but as a buy-some-time measure. We've been doing it for decades now… (DES used 56bits back then, AES started at 128). Also, keep in mind that increasing the key length by 1 means that you need twice as much compute to crack it through brute force (that is, unless cryptanalysis shows an attack that reduces the difficulty of the scheme, like for instance with the number field sieve on RSA) so you don't need to increase key size too often: following Moore's law, you need to increase it by on bit every two years, or 5 bits every decades. Additionally key size generally account for many years of compute progress and theoretical advance, so that you really don't need to worry about that over a short period (for the record higest RSA factorization to date is 829 bits, yet people started recommending migration away from 1024 bit RSA a decade ago or so, and the industry is in the process in deprecating it entirely even though it's probably going to take years before an attack on it becomes realistic. reply bananapub 5 hours agoparentprev> why we're not dramatically ramping up key sizes across the board on all encryption? because no one thinks there is a reason to, no one has any fear that classical computers will catch up with RSA-2048/AES-128 before their grand children are dead. post-quantum crypt stuff is happening and people are planning how to migrate to it. reply EVa5I7bHFq9mnYK 1 hour agorootparentWell, even MD4 hasn't been cracked yet. reply kbolino 8 minutes agorootparentWhat is your definition of cracked? Collisions are easy to produce; there's one right on the Wikipedia page. reply 1oooqooq 3 hours agoparentprevbecause everyone recommending those things work on both sides. they recommend 2048 and use 4096 themselves because if they need to ever break your 2048 it's less bad than if you were recommended to use 4069. wink wink same with everyone recommending ed22519 when ed448 is as good and as fast to encode. but all the arguments point to encode speed from a Bernstein paper which used a pentium iii! https://cr.yp.to/ecdh/curve25519-20060209.pdf reply qwertox 5 hours agoparentprevBecause we need that compute for our React frameworks. reply pjmlp 3 hours agorootparentDon't forget Electron as well. reply undersuit 54 minutes agorootparentMy Chromebook all of the sudden needs compute to serve ads. RIP uBlock. reply halfmatthalfcat 4 hours agorootparentprevFunny how HN is so obsessed with squeezing every bit of performance on the backend but when it comes to optimizing the frontend, many take the most uninformed, lazy takes and then hand wave something about just using HTML and CSS. I guess we should all just write assembly in that case. reply jacob019 4 hours agorootparentWell, backend is our precious server resources and front end resources belong to the lowly user. It's a matter of scale though, frontend is optinized for one concurrent user. Plenty of small websites using slow and bloated frameworks on the backend because it's easy. reply parpfish 4 hours agorootparentMan, users are the worst. Sometimes I think that they only exist to stress out my precious beautiful servers. reply moi2388 2 hours agorootparentI agree. Those stateful bastards. I suggest we wrap them all into Monads. That’ll teach them! reply masom 4 hours agorootparentprevSimple economics; who pays for the servers and who pays for the front-end execution? Front-ends are essentially free distributed computing resources while the backends need to be paid for. reply madeofpalk 3 hours agorootparentprevI don't think that is true of \"HN\". This thread, about how wasteful frontend development is, is a pretty standard trope in the comments here. Loads of developers everywhere, frontend and backend, go to lengths to optimise their programs. Loads of other developers also don't care. reply lelandfe 4 hours agorootparentprevI have stopped mentioning frontend performance being my speciality on interviews as it has elicited tepid responses from all but one. reply myself248 2 hours agorootparentI imagine interviewing at McMaster-Carr to be a very different experience than Home Depot. reply potsandpans 2 hours agorootparentprevIt's funny how some folks with no domain experience hn seem to be obsessed with html. reply croemer 1 hour agoprevSlight change of title. The leading \"How\" was dropped. Original: \"How We Cracked a 512-Bit DKIM Key for Less Than $8 in the Cloud\" reply Uriopass 44 minutes agoparentDropping the How is part of Hackernews guidelines reply layer8 21 minutes agorootparentThere’s no such guideline. It’s just part of the default title normalization. The submitter can adjust it afterwards. reply bmenrigh 3 hours agoprevCADO-NFS makes this surprisingly easy to do. A few weeks back I factored a 512bit RSA DKIM key for work using my desktop computer in only 28 hours. Specifically, an AMD Zen 5 9900X. Unfortunately 1024 bit keys are still out of reach of a hobbyist effort but could be pulled off by academics roughly of the same scale as the 2010 effort to factor a 768 bit key (https://eprint.iacr.org/2010/006.pdf) reply zelphirkalt 5 hours agoprevSome DNS providers suck and only let you set 1024 bit long keys. For example wordpress.com. reply Avamander 5 hours agoparentYikes. NIST wants to forbid even 2048-bit RSA by 2035, because it doesn't offer a good enough security level. reply tptacek 3 hours agorootparent2048 achieves the same security level NIST requires from AEADs, doesn't it? What plausibly attacks it? Pushing people past 2048 seems counterproductive. reply Avamander 3 hours agorootparentIt should achieve the same level, yes. It's not exactly described what could attack it. Right now it seems that 2048-bits would be the last allowed step and they're not going to push people past 2048, they want to phase out RSA in general. reply daneel_w 2 hours agorootparentprevCounterproductive how and to what/whom? For the sake of keeping DNS TXT entries and e-mail headers compact? Would you stand by this statement also in the context of a certificate authority's root signing key, or an RSA host key for an ssh daemon? reply tptacek 2 hours agorootparentThere is no plausible threat to 2048, but you'd still rather people switch off RSA, either to curves (because they're more resilient) or to lattices (for post-quantum). Pushing people to higher RSA key sizes is a waste of effort. reply mjl- 1 hour agoparentprevDKIM records are just DNS TXT records. Do they have a limit on the size of TXT records? Or are they going out of their way to try to parse the TXT records you're adding that look like DKIM records, failing on them, and then refusing to add them? reply nyrikki 40 minutes agorootparentRFC1035 imposes a 255 character limit per string on TXT records . reply GuB-42 4 hours agoparentprevRSA-1024 seems to be about 8 million times better than RSA-512, so cracking that would be $64 million in compute. Not NSA-proof, but should be more than enough to keep spammers out, especially considering that DKIM is just one layer of protection. reply upofadown 1 hour agorootparentThat's just the extra compute required. There is a large increase in required memory that needs to be quickly accessible in a particular way. One estimate I saw claimed that breaking 1024 bit RSA would take more than 2 billion dollars over a period of years. reply zahlman 3 hours agorootparentprev512 extra bits of key only gets you 23 bits of entropy? reply GuB-42 3 hours agorootparentYep, the formula is a bit complicated, it comes from the general number field sieve (GNFS) algorithm, you can find some equivalences online between symmetric key algorithms and RSA and 23 bits seems about right. I have also seen lists where they give RSA-512 64 bits and RSA-1024 80 bits, just a 16 bit difference, but it looks a bit arbitrary to me. I think the NIST doesn't even look at RSA-512 anymore, as it is definitely broken, it only starts at 1024. A RSA key is the product of two primes, not any number, so you need a lot more bits to get equivalent security to, say, AES. That's also a reason for elliptic-curve cryptography, which needs a lot less bits than RSA for the same level of security. reply littlestymaar 5 hours agoparentprev1024 is still many orders of magnitude hard to crack than 512. For the record, the last RSA number having been broken was RSA-250 (829 bits) and it took 2700 core-years to crack back in 2020[1]. In comparison, RSA-155 (512 bits) was factorized as early as 1999! You aren't in danger. [1]: https://sympa.inria.fr/sympa/arc/cado-nfs/2020-02/msg00001.h... reply daneel_w 2 hours agorootparentYou and I aren't the ones in immediate danger. The service providers we rely on are. In discussions like these we have a \"tragicomic\" tendency to forget mankind's unstoppable progress. RSA-1024 offers 80 symmetric equivalent bits of security and we've been heading down this path for decades at an exponentially increasing pace. reply littlestymaar 57 minutes agorootparentThose service providers have had plenty of time to migrate to 2048 and most of them have already. > a \"tragicomic\" tendency to forget mankind's unstoppable progress When it comes to compute, it's no faster than Moore's Law, which means roughly one bit of symmetric encryption every two years. > and we've been heading down this path for decades at an exponentially increasing pace. Given that the encryption security is itself exponential in bit length, we are in fact heading down this path linearly! (A doubling in compute power means the ability to crack twice as hard cryptosystems, which means ones that have 1 bit more of security). Key must be extended over time, and they are, and have been for decades. A PoC of an attack of a security system broken since 1999 should be treated exactly like how we are amazed at little computing power was available to the Apollo program: this is a cool bit of trivia that shows the growth of available computing power, but not a display of any kind of security issue. reply DarkmSparks 3 hours agorootparentprevThere are several open source GNFS tools that can do 1024 very efficiently on GPUs, and even cheap consumer GPUs have 10s of thousands of cores now, even by your measure \"2700 core-years\" is only around a month or so on a single consumer grade GPU. Not \"free\", but any malicious actor has access to a lot more than a single GPU. The UK government also has several huge arm based solutions dedicated to cracking internet encryption, zero chance that isn't breaking mostly everything, for sure the Chinese and Russians have similar. reply upofadown 1 hour agorootparent>There are several open source GNFS tools that can do 1024 very efficiently on GPUs, ... Reference? Why has no one demonstrated this? reply littlestymaar 50 minutes agorootparentprevYet nobody is collecting the RSA-numbers bounties? RSA-270 (much, much easier than 1024 compute-wise) has a bounty of $75k, why would it be unclaimed then when you can spend three years worth of cloud rented H100 (I'm being conservative here and count $3/h which is far from the best deal you can get) and still make a profit? Also a GPU core and CPU cores really aren't comparable individually, so your “consumer graphic card having thousands of core already” is comparing apples to oranges. reply dizhn 3 hours agoprev> Although most providers correctly identified the 512-bit key as insecure and rejected our DKIM signature, three major providers — Yahoo Mail, Mailfence, and Tuta — reported a dkim=pass result. Did google really FAIL because of DKIM signature being insecure or because SPF failed? reply awulf 2 hours agoparentThe DKIM verification failed with the result \"dkim=policy (weak key),\" as it should according to RFC 8301: \"Verifiers MUST NOT consider signatures using RSA keys of less than 1024 bits as valid signatures.\" reply kingforaday 5 hours agoprevLove the practicality demonstrated here. It is unclear how old this article is. Based on the poster's previous submissions, I assume today? reply awulf 4 hours agoparentI published the article today, though it was written a few months ago (when the DKIM record was still online). reply philipwhiuk 4 hours agorootparentWas redfin aware you were trying to break their DKIM record? reply Avamander 3 hours agorootparentIt was already broken. This comment also serves as a public notice that I'm going to factor all the 512-bit DKIM RSA keys out there from now on. Start migrating. reply austin-cheney 3 hours agoprevIn case anybody is wondering about whether the 512bit number is big or small it depends on whether it is symmetric or asymmetric encryption technique. Always presume asymmetric encryption is 8x weaker than symmetric encryption. DKIM is asymmetric. So a 512bit DKIM equivalent symmetric hash would be 64bits, which is long broken. Even 160bit SHA1 is considered broken. A DKIM of roughly equivalent strength to a 512bit SHA3 would be at least 4096bits and still does not include SHA3's techniques for mitigating replay attacks. reply bmenrigh 3 hours agoparentDKIM is not an encryption algorithm. It is a standard for embedding and validating signatures in email headers. Unfortunately DKIM only supports rsa-sha1 and rsa-sha256 signatures (https://datatracker.ietf.org/doc/html/rfc6376/#section-3.3). It'd be nice to see DKIM get revised to allow Ed25519 or similar signatures. reply Avamander 3 hours agorootparentEd25519-SHA256 support has existed for a while now. https://datatracker.ietf.org/doc/html/rfc8463 reply bmenrigh 3 hours agorootparentOh excellent. I didn't realize rfc6376 had been superseded. reply austin-cheney 2 hours agorootparentprevWikipedia says it is a correlation check based upon a public key based signature. How is that not a form of encryption? Google says encryption is a process that scrambles data into a secret code that can only be decoded with a unique digital key, which is exactly what public keys are for. reply dist-epoch 1 hour agoparentprev> Always presume asymmetric encryption is 8x weaker than symmetric encryption. RSA encryption is 10x weaker than Elliptic curve (224 bits ECC ~= 2048 bits RSA). Both are asymmetric. Alternatively, asymmetric Elliptic curve is as strong as AES symmetric encryption. But it's quantum vulnerable, of course. reply asim 5 hours agoprevI think the cynic in me says \"so what\" mostly because dkim as an ancient technology is hardly secure. I don't think we're any less prone to email fakery and spam. I'd be interested to see a possible new solution or a revamping of email as a protocol but that's unlikely. We're more likely to keep it like snail mail as we prioritise different forms of communication. Unfortunately nothing has beat the universal email address on the internet. Here's hoping we come up with a chat standard that sticks and people run a user@server.com chat server where you can communicate from anywhere. Sorry xmpp, you don't count. reply wolrah 35 minutes agoparentEmail-style user@host.domain addressing is also used in SIP. At one point in time you could reach me on email, XMPP, and SIP all using the same identifier. We dropped XMPP about a decade ago when all the major service providers stopped federating with it, but if you know my work email address you can also call my desk phone using the same identifier. reply alt227 4 hours agoparentprevYou hit the nail on the head with the word 'Universal'. Because email addresses have existed since the begining of the web, anyone who has ever been on the internet has one and uses it for identification purposes. This will not change without another universal standard which everybody automatically has. Its like IPv4, we all have the ability to use IPv6 but do we? Hell no we just use NAT as its an easier quick fix. Changing any addressing on the internet is tough because you always have to have backwards compatibilty, which kind of ruins the point of moving forward. reply lazide 4 hours agorootparentHaha, definitely not true outside of the US and Western Europe. Most of Asia (and probably Africa) uses phone #. reply alt227 2 hours agorootparentThats because those are mostly developing countries which were late to the internet party, and are accessed mainly through mobile. Phone # could well be a replacement for email globally in the future, once all the younger generation grow up and rule the world. reply MattJ100 4 hours agoparentprevOf course XMPP doesn't count... even though it's a standard, allows people to run their own server, have an email-like address of user@server, allows you to communicate from anywhere, and is how I, my family, and many many others chat online? :) reply TheChaplain 5 hours agoprevAnyone know or have experience how well supported ECDSA is for DKIM? reply paulnpace 4 hours agoparentDKIM supports Ed25519-SHA256. https://datatracker.ietf.org/doc/html/rfc8463 reply 1oooqooq 2 hours agorootparentspecs supports many things. I believe the question was more about the real world. reply paulnpace 12 minutes agorootparentIn the real world, DKIM specs only support Ed25519 and RSA. reply colmmacc 4 hours agoprevBreaking a 512-bit key for a good demonstration is very valuable security research, even if it's been done before. It's also legit to call out \"Hey, here's a list of folks still using 512 bit, they should move off.\" ... but for me, actually cracking a real-world in-use key crosses an ethical line that makes me uncomfortable. IANAL but it might even be criminal. Just seems a bit unnecessary. reply phoe-krk 4 hours agoparent> for me, actually cracking a real-world in-use key crosses an ethical line that makes me uncomfortable They've contacted the company with the vulnerability and resolved it before publishing the article - search the original article for the substring \"now no longer available\". Usually, you demonstrate that an online system is vulnerable by exploiting that vulnerability in good faith, documenting the research, and submitting it for review. It does not matter if you're cracking an encryption scheme, achieving custom code execution for a locked-down game console, proving that you can tamper with data in a voting machine, or proving that you can edit people's comments on a Google Meet Q&A session - the process is the same. If you say something's vulnerable, people can shrug it off. If you say and prove something's vulnerable, the ability to shrug it off shrinks. If you say and prove something's vulnerable and that you'll publish the vulnerability - again, using the industry standard of disclosure deadlines and making the vulnerability public after 60-or-so days of attempting to contact the author - the ability to shrug it off effectively disappears. reply colmmacc 4 hours agorootparentI read the article, and I don't think it changes it. If you crack someone's key, they might be well within their rights to pursue a criminal prosecution. Of course it would also have a Streisand effect and there's reasons not to, but I personally wouldn't allow or recommend a security researcher to do it. It's needlessly risky. In general, subverting security and privacy controls tends to be illegal in most jurisdictions. Best case is when you have clear permission or consent to do some testing. Absent that there's a general consensus that good faith searching for vulnerabilities is ok, as long as you report findings early and squarely. But if you go on to actually abuse the vulnerability to spy on users, look at data etc ... you've crossed a line. For me, cracking a key is much more like that second case. Now you have a secret that can be used for impersonation and decryption. That's not something I'd want to be in possession of without permission. reply alt227 2 hours agorootparent> If you crack someone's key, they might be well within their rights to pursue a criminal prosecution. If that were true there would be no market for white hat hackers collecting bug bounties. You need to be able to demonstrate cracking the working system for that to be of any use at all. No company will listen to your theoretical bug exploit, but show them that you can actually break their system and they will pay you well for disclosure. reply colmmacc 2 hours agorootparentBug bounties are a form of consent for testing and usually come with prescribed limits. Prescribed or not, actually getting user data tends to be a huge no go. Sometimes it can happen inadvertently, and when that happens it's best to have logs or evidence that can demonstrate you haven't looked at it or copied it beyond the inadvertent disclosure. But to pursue data deliberately crosses a bright line, and is not necessary for security research. Secret keys are data that be used to impersonate or decrypt. I would be very very careful. reply Avamander 3 hours agorootparentprevThey can pursue what they want, it doesn't mean it will go through. Looking at public data, using some other public knowledge to figure out something new does not make it inherently illegal. They didn't crack it on their systems, they didn't subvert it on their systems, they did not use it against their systems. I'd love to see some specific examples under what it could be prosecuted under specifically. Because \"that door doesn't actually have a lock\" or \"the king doesn't actually have clothes\" is not practically prosecutable anywhere normal just like that. Especially in the EU, making such cryptographic blunders might even fall foul of NIS2, should it apply to you. In general this also quickly boils down to the topic of \"illegal numbers\" (https://en.wikipedia.org/wiki/Illegal_number) as well. reply colmmacc 2 hours agorootparentIt's more like the door has a weak lock that can be picked. Just like many real world doors do. Here's how it would go in court: \"Are you aware that this key could be used to decrypt information and impersonate X?\" \"Are you aware that this key is commonly called a Private key?\" \"Are you aware that this key is commonly called a Secret key?\" \"Are you aware that it is common to treat these with high sensitivity? Protecting them from human eyes, using secure key management services and so on?\" \"Was it even necessary to target someone else's secret private key to demonstrate that 512-bit keys can be cracked?\" \"Knowing all of this, did you still willfully and intentionally use cracking to make a copy of this secret private key?\" I wouldn't want to be in the position of trying to explain to a prosecutor, judge, or jury why it's somehow ok and shouldn't count. The reason I'm posting at all here is because I don't think folks are thinking this risk through. reply Avamander 2 hours agorootparentIf you want to continue with the analogies, looking at a lock and figuring out it's fake does not constitute a crime. That key can not be used to decrypt anything. Maybe impersonate, but the researchers haven't done that. It's also difficult to claim something is very sensitive, private or secure if you're publicly broadcasting it, due to the fact that the operation to convert one to an another is so absolutely trivial. And they did not make a copy of their private key, they did not access their system in a forbidden way. They calculated a new one from publicly accessible information, using publicly known math. It's like visually looking at something and then thinking about it hard. I wouldn't want to explain these things either, but such a prosecution would be both bullshit and a landmark one at the same time. reply mmastrac 4 hours agoparentprevBreaking a key isn't criminal. It's just math. Sending emails that would constitute fraud or a violation of another jurisdictional law, relying on that math, is illegal. But again -- it's not the math, it's the action and intent. Pointing out that someone is doing something stupid is also not illegal, though they make try to make your life painful for doing so. reply colmmacc 4 hours agorootparentSecrets themselves are often regarded as sensitive information, and often also categorized in the highest classifications for information protection. This stuff isn't just a game, and in my experience it's not how law enforcement authorities or courts see things. Just as if you made a copy of the physical key for a business's real-world premises, you could well find yourself in trouble. Even if you never \"use\" it. reply yellow_lead 4 hours agoparentprevYou can say this about a lot of security research, i.e \"Showing a bug is exploitable is one thing, but writing a POC is crossing the line!\" The problem is that most people won't listen to your security research unless you demonstrate that something is actually possible. reply croemer 4 hours agoparentprevThey only called out 1 out of the ~1.7k domains they found with keys shorter than 1024bits. They _did_ call out the 3 of 10 tested major email providers that don't follow the DKIM RFC properly: Yahoo, Tuta, Mailfence (after having notified them). reply michaelmior 4 hours agoparentprevI am also not a lawyer, but I would suspect it's not criminal to crack a real-world in-use key if you do so using only publicly available information and you don't actually do anything with the result. reply colmmacc 4 hours agorootparentLet's say you local coffee shop is featured in a local news piece and the blithe owner happened to get photographed holding the store key. That's now easy to clone from public information. Would you be comfortable actually doing it? Reporting the issue is fine - \"Hey you should rekey your locks!\". Actually making a clone of the key, and then showing \"hey it fits\" will get you more traction more quickly ... but there's also plenty of Police Departments who might well arrest you for that. reply cafeinux 4 hours agoparentprevAs mentioned in the article, the key's no longer available and I presume the article has been released only after responsible disclosure was done and with the approval of the key owner (hope so). At this point, it's not more unethical than any other security research performed in the same conditions and with the same outcome. reply andix 2 hours agoprevAny tips on how to easily fetch the key length of DKIM-Keys from DNS? Just by looking at the DNS entries I can't easily determine the key length ;) reply deathanatos 2 hours agoparentFetch the TXT record; you should see something like, k=rsa; … p= The base64 data is an RSA public key. You can print in textual form with something like, your-clipboard-paste-commandbase64 -dopenssl rsa -pubin -noout -inform der -text The first line of output will be something like, Public-Key: (2048 bit) Which is the key length. If you fetch with `dig`, note that sometimes dig will do this: example.com. 1800 IN TXT \"k=rsa; t=s; p=blahblahblahblah\" \"blahblahblah\" I.e., it breaks it up with a `\" \"`; remove those, that's not part of the data. I.e., concat the strings dig returns, then parse out the p=, then base64 decode, then openssl. (You can also do what the article does, but without Python, which is jam that base64 between some PEM header guards like the article does, and then feed it to openssl. Same command, but you don't need the -inform der b/c the \"in[put] form[at]\" is now pem, which is the default.) reply awulf 2 hours agoparentprevAn easy way is to check the length of the p= value in the DKIM record. If it's around 216 characters, it's likely a 1024-bit key. A 2048-bit key usually has about 388 characters. reply zorgmonkey 47 minutes agorootparentIf you're going to do this you should also check for k=rsa since other key types will be different lengths. But I'd really recommend something like in the other comment where you base64 decode and parse it with something like openssl. reply dspillett 5 hours agoprevHow common are such small DKIM keys? I'm pretty sure mine are 2048-bit, though I'd have to check as they were last set a fair while ago. reply sva_ 5 hours agoparentFirst sentence: > In our study on the SPF, DKIM, and DMARC records of the top 1M websites, we were surprised to uncover more than 1,700 public DKIM keys that were shorter than 1,024 bits in length reply dspillett 1 hour agorootparentI must do something about my short-term memory/attention. I must have read that and promptly blanked it during the scanning of the rest of the article. reply mrweasel 4 hours agorootparentprevThe interesting question is: Did these sites forget that they have these records, or have they perhaps forgot how to rotate their keys. reply awulf 2 hours agorootparentI guess most of these keys are decades old and no longer in use. They're likely just sitting in the DNS because someone forgot to delete them. Now, no one's sure if they're still needed and is afraid to remove them in case it breaks something. Or maybe they're still used by a legacy system, and no one realizes the impact an old DKIM record could have. reply 1oooqooq 3 hours agorootparentprevdns server limitations. txt records are 1024bits... add the prefix for the key and you get But as a security generality - email is vastly less secure than human nature wants to assume that it is. I don't think this has to do with \"human nature\" anymore than http did. It's a very important, powerful form of communication without any secure replacement. Just as we switched to https, ideally an \"xmail\" or the like would get created as an open standard with open software that was email with better security by default. Sadly I'm not sure we collectively have the ability to do that kind of thing any longer, powerful entities have realized it's just too attractive to lock it up. But even many open source organizations don't seem to feel like bothering. Plenty of security experts even just prefer new shiny and will spout ridiculous \"move to instant messaging\". So status quo rules for the foreseeable future. reply immibis 1 hour agorootparentEmail is one of a great number of internet communication systems. It happens to be one that was created before security was much of a concern (and before it was a problem we could solve), and also one that is highly asynchronous. Newer systems have their own different sets of tradeoffs. None of them happen to be email plus security. But that makes sense: now that everyone is online most of the time, why would you design a highly asynchronous store-and-forward-ish system? Of course you want a recipient to receive a message as soon as you sent it and the sender to confirm the recipient received the message. Designing a new system any other way would be leaving value* on the table. (*actual value not economics value) We can extend email, though. Why isn't there an SMTP GETKEY command to return a PGP key corresponding to an email address? Sure, the sender might not support the same version of PGP, and sure, the connection might be intercepted (hopefully you'd not trust the output of this command except over TLS), but like most of the email system, it would be a big improvement and good enough most of the time. reply xmodem 5 hours agoparentprev [–] Yeah, so I guess there's no point in picking any of the low hanging fruit to make it more secure. reply sam_lowry_ 5 hours agorootparentEmail is actually quite secure, just in a different way that web. For instance, once you disregard so called transactional mail and spam, real email is almost all encrypted for all practical purposes. DKIM and DMARC also work quite well for spoofing protection, aside from the corner cases like the above. Average Software Engineers have an outdated idea of email, formed by 1990 era Internet. reply bell-cot 5 hours agorootparentprev [–] Depends on context. If you're in IT at a carefully run org: You ditched 512-bit keys years ago. This article is nothing but a 20-second story, to help explain to PHB's and noobs why they got an error message, or what sorta important stuff you're always busy keeping your org safe from. If you're in IT at a scraping-by org: Maybe today's a good day to ditch 512-bit keys. And if you get push-back...gosh, here's a how-to article, showing how a \"forged corporate signature stamp\" can be made for only $8. If you're trying to teach senior citizen how to avoid being scammed on the internet: You've got zero visibility or control, so you're stuck with \"sometimes these can be forged, depending on technical details\" generalities. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A study of the SPF (Sender Policy Framework), DKIM (DomainKeys Identified Mail), and DMARC (Domain-based Message Authentication, Reporting, and Conformance) records of the top 1 million websites revealed over 1,700 public DKIM keys shorter than 1,024 bits, which are deemed insecure. - Researchers successfully cracked a 512-bit DKIM key from redfin.com using Python and CADO-NFS for under $8 on a cloud server, demonstrating the vulnerability of short keys. - The study emphasizes the need for email providers to reject DKIM signatures with keys shorter than 1,024 bits, as some providers like Yahoo Mail, Mailfence, and Tuta accepted the compromised signature."
    ],
    "commentSummary": [
      "Cracking a 512-bit DKIM (DomainKeys Identified Mail) key is now affordable, costing less than $8 using cloud services, highlighting the vulnerability of outdated encryption methods.",
      "The discussion emphasizes the necessity for stronger DKIM keys and the challenges in balancing security with usability, as many systems still rely on outdated keys, posing significant security risks.",
      "The conversation underscores the broader implications for email security and the urgent need for updated standards to protect against evolving threats."
    ],
    "points": 422,
    "commentCount": 203,
    "retryCount": 0,
    "time": 1736339554
  },
  {
    "id": 42626431,
    "title": "Microsoft disguises Bing as Google to fool inattentive searchers",
    "originLink": "https://www.pcworld.com/article/2568916/microsoft-disguises-bing-as-google-to-fool-inattentive-searchers.html",
    "originBody": "News Microsoft disguises Bing as Google to fool inattentive searchers If you can't beat Google, trick your users into thinking that they're using Google. By Michael Crider Staff Writer, PCWorld Jan 6, 2025 7:25 am PST Image: Microsoft Microsoft would really, really, really like you to use Bing, its self-branded search engine that competes against Google’s monopoly. Not only is it hardwired into much of Windows and other Microsoft products like the Edge browser, it also employs a lot of sneaky visual tricks to steer you away from Google itself. But the company’s latest trick is more, well, tricky — in fact, it’s just straight-up Google camouflage. This morning, users are discovering that if they search for “Google” in the primary Bing interface, they’re shown a special Bing search page. Before you scroll down to the actual search results, you’re presented with an all-white page with a centered, unbranded search bar and a multicolored doodle above it that’s heavy on yellow, red, blue, and green. It is, in a very practical and (in this writer’s opinion) deliberate sense, a clone of the familiar Google search page. If you aren’t paying attention, you might put in a search or two in this disguised bar without ever realizing that you haven’t actually left Bing itself. You’ll have to scroll up or down to notice something’s off, if you haven’t immediately seen that it’s basically a knock-off Google doodle. The change was noted by Windows Latest (via 9to5Google), which also points out that this page appears whether you’re going to Bing.com directly or searching from the Edge combined search/URL bar. The only place it doesn’t appear is if you’re searching from an Edge tab in which you’ve already signed into a Microsoft account. A little testing on my part verifies these parameters. (I’m seeing the “I can’t believe it’s not Google” search page in Vivaldi, even though I’m signed into my work Outlook account.) This dedicated “I can’t believe it’s not Google” search page does not seem to appear for any searches less specific than “google,” “google search,” etc. Even something only slightly less immediate, like “google mail,” gives you the regular Bing interface and results. The intent seems pretty obvious here, and it’s a very desperate look on the part of Microsoft. Not that I blame anyone for feeling a little desperate in the face of Google’s search dominance. I didn’t call it the Google monopoly idly, because that’s the official determination of a United States federal court. Google controlled an astonishing 95 percent of the mobile search marketplace as of 2024 according to Statista, a number that has barely moved since 2015, with Bing claiming less than one percent despite 15 years of Microsoft backing. The results are slightly less bleak if you look at the desktop search market, where Google has “only” 82 percent to Bing’s growing 10.51 percent share as of early 2024. So, yeah, I don’t envy the Bing team’s task of slaying giants. But that being said, if a user wants Google and they search Bing to get it, deliberately obfuscating the search seems like a failure of a search engine’s sole task. Microsoft is putting its business goals above its users, a strategy that never endears users to the product. Considering how poorly Microsoft’s push to move people off of Windows 10 is going, I wonder if they might benefit from a bit of serious introspection on this approach. Author: Michael Crider, Staff Writer, PCWorld Michael is a 10-year veteran of technology journalism, covering everything from Apple to ZTE. On PCWorld he's the resident keyboard nut, always using a new one for a review and building a new mechanical board or expanding his desktop \"battlestation\" in his off hours. Michael's previous bylines include Android Police, Digital Trends, Wired, Lifehacker, and How-To Geek, and he's covered events like CES and Mobile World Congress live. Michael lives in Pennsylvania where he's always looking forward to his next kayaking trip. Recent stories by Michael Crider: Watch out for fake online stores as Black Friday approaches FTC charges online shopping tool Sitejabber for fake product reviews I block every ad on YouTube and I’m not ashamed to admit it",
    "commentLink": "https://news.ycombinator.com/item?id=42626431",
    "commentBody": "Microsoft disguises Bing as Google to fool inattentive searchers (pcworld.com)284 points by ungut 23 hours agohidepastfavorite206 comments londons_explore 22 hours agoThis is partly preying on the fact googles 'doodles' weaken their brand/trademark. Back when every google doodle clearly had the word \"Google\" in, that was okay. But often now, the doodles are just some random picture. At that point, there is no brand recognition to their homepage beyond a blank white background and centered search box, which microsoft has copied here because those elements alone are not enough to form a legally protectable brand. reply comex 21 hours agoparentI agree, but for the record, if Google wanted to sue, they wouldn’t be completely out of luck. They could make claims under the Lanham Act §1125(a), state unfair competition laws, or other fraud-adjacent laws. But they would have to prove that Microsoft was deceiving customers, and it would be a lot harder without an actual case of trademark infringement. They could also try to claim trademark infringement based on the fact that Microsoft is hijacking searches for the keyword “google”. Courts have previously rejected trademark claims when a company takes out search ads using its competitor’s name as a keyword, but Google could argue that what Microsoft is doing here is more deceptive than that. (IANAL and have only passing familiarity, but I’m fairly confident in the above.) reply oehpr 3 hours agorootparentIANAL as well. but I have to say, if typing into typing Google into the Bing search and getting a page that looks almost exactly like Google can't be proven as intent to deceive, then the law is broken. I can't imagine anything clearer to prove intent than a user requesting that they want to go to Google to Bing, Bing responds to that request by showing them a page that looks like Google's. That is so clear. Is that really not able to be proven in court? reply mrayycombi 19 hours agorootparentprevMicrosoft has been breaking the law for years and was found guilty of antitrust violations, among others. Bill Gates, the friendly philanthropist, was/is a business criminal. His company hasn't changed. https://www.justice.gov/opa/pr/microsoft-agrees-pay-20-milli... https://en.m.wikipedia.org/wiki/United_States_v._Microsoft_C.... They waged a war of \"FUD\" against open-source software. https://lxer.com/module/newswire/view/57261/index.html And so on. And Google is hardly a saint either. \"Do no evil\" was just marketing from that surveillance advertising firm. reply philistine 17 hours agorootparentAt this point the events you’re describing are decades ago. To me they are as relevant to the Google and Microsoft of today as saying that Apple is not to be trusted because they messed up the Performa line. Not irrelevant, but those company are faceless, far bigger, far more insidious than when the events you describe happened. reply mrayycombi 17 hours agorootparentIt's only past history if they stop doing it. If you try to download and install chrome they do the same visual obfuscation, hide the button, and deceptilink you to use their browser. Nothing has changed bro. Open your eyes. They never repented. reply pjmlp 12 hours agorootparentIndeed starting by FOSS folks talking about the man, while complaining that no one from big tech returns anything, even though they play by the rules of the licensing. reply philistine 16 hours agorootparentprev> faceless, far bigger, far more insidious My point is not that they're better. It's that they're faceless now, they no longer have the personality you ascribe to them. reply tim333 8 hours agorootparentprevTechnically \"Microsoft to pay $20 million in civil penalties\" doesn't actually make the CEO a criminal in the legal sense. Not saying he's a saint who's never done anything wrong but who is? reply Incipient 18 hours agorootparentprevThe majority of big business is built on varying degrees of fraud, bribery, nepotism, anti-competitive behaviour, etc. It's just how business is done. If you don't get a leg up on your competitors, they'll get one up on you. reply mrayycombi 17 hours agorootparentA sad excuse for criminal conduct. Believe it or not there are lots of law abiding businesses, and companies that compete on quality, service, and price. And even if that were an extreme minority (and I don't think it is) we should praise them as models instead of resigning ourselves to mediocre businesses using illegal tactics to control a market for super profits. reply tim333 8 hours agoparentprevIn some ways it weakens their brand in that maybe it's easier to pretend to be them but in others it makes people feel good about them. I like the doodles. reply ballenf 22 hours agoprevI bet this is 100x+ more effective at keeping people on Bing than anything else MS has tried. Same idea as knock-off brands with labels and designs inspired by the name brand. People may eventually realize they're not on Google, but probably only after being not displeased in Bing's results. If they have a bad experience, oh well, they were planning on using google anyway. reply stemlord 19 hours agoparentBing is a lot better than google for adult content. Bing actually has pretty neat image search tools reply kbelder 13 hours agorootparentI consider Bing better in some areas, and equivalent in most others. Their image search is definitely better. Text search is... well, not good, but not any worse than Google. reply spacemanspiff01 22 hours agoparentprevMaybe I am cheap, but I have been using bing because of their rewards points stuff, at least then I get paid for my data. reply IncreasePosts 22 hours agorootparentI will help determine if you're cheap. How much money have you saved/made through the rewards points stuff? reply junar 21 hours agorootparentI perused the Bing rewards site [1]. It seems that you need 1,000 Bing searches to get a $5 Microsoft or Xbox gift card (3rd party gift cards seem more expensive). There are also daily caps on rewards from Bing searches. [1] https://rewards.bing.com/welcome reply garciasn 22 hours agorootparentprevFar less than the time saved had they received Google's results instead of Bing's inferior ones. reply norman784 21 hours agorootparentAre Google results good? I stop using Google years ago because of the garbage results (a lot of spam sites that were just serving data crawled from Github and other sites). reply nine_k 21 hours agorootparentMy primary search engine as of now is DDG, Their results are mostly fine (and powered partly by Bing). When their results are not good enough, I ask Google; often, but not always, it's better. For some other kinds of queries, Google fares notably worse than DDG, likely because SEO tricks are disproportionately directed against Google, and not always work against other search engines as effectively. reply DoctorOW 20 hours agorootparentDDG uses Bing behind the scenes. reply nine_k 13 hours agorootparentYes (as mentioned), but not only: https://duckduckgo.com/duckduckgo-help-pages/results/sources... reply unsignedint 21 hours agorootparentprevI've had my issues with Google ever since they ran a series of ads pretending to be for Blender but actually linking to a scam site. (I’m not sure if it’s gotten better since then.) While I do occasionally come across questionable ads on Bing, they’re definitely much less frequent. For what it’s worth, I’ve switched to using Edge as my browser as well, largely because Google refuses to address one of the most frustrating issues with its external link profile behavior. Specifically, Google forces external links to open in the last-used profile, rather than letting you choose, whereas Edge allows you to customize this behavior. On top of that, Bing’s deep search feature has proven to be genuinely useful reply miki123211 21 hours agorootparentIf you use a search engine, you need an Ad Blocker. Not because \"privacy:\", not because \"tracking\", but because malicious ads exist, and you'll click on one eventually. This is even more true for less technical family members. reply recursive 20 hours agorootparentYou've completely overlooked the possibility of a search engine without ads. That's what I use. reply devsda 4 hours agorootparentIf your search engine doesn't have ads that only means the adblocker has nothing to do except use some cpu cycles. Those cycles are mostly earned back when you visit a link with ads (unless the search results are limited to ad free sites). So, it's still a net positive to have an ad-blocker. reply norman784 11 hours agorootparentprev> Specifically, Google forces external links to open in the last-used profile, rather than letting you choose, whereas Edge allows you to customize this behavior. This is a neat feature, I used browserosaurus for a similar behaviour, but also that means I have multiple browsers open, one basically for each profile. reply Arnavion 21 hours agorootparentprevI wrote https://www.arnavion.dev/blog/2020-12-05-ddg-vs-google/ in 2020. The tl;dr is that Bing's (DuckDuckGo's) results were garbage and Google was giving the correct answer within the first five results. Running those same specific queries now, the Google results are as bad or worse than Bing's results at the time, and Bing now frequently gets the results that Google did at the time. But my everyday experience is still that Google gives generally better results than Bing / DDG. reply thaumasiotes 19 hours agorootparentprevSee Dan Luu, \"How Bad Are Search Results?\" https://danluu.com/seo-spam/ reply KTibow 21 hours agorootparentprevI've went through a few stints of using Bing. Eventually I end up starting all my queries with `@google`. reply philistine 17 hours agorootparentprevI don’t believe for one second that Google’s results are better. Their market share is so big that no one running SEO scams is looking at their Bing results. So that immediately makes the Bing results far more organic and sane. reply Barrin92 17 hours agorootparent>Their market share is so big that no one running SEO scams is looking at their Bing results That's pretty unsound logic for two reasons. One is that it's not very likely that SEO optimization for different mainstream search engines requires any more effort than for one, secondly Bings ~5% market share is what, tens of millions of people still? If nobody games it you're leaving free money on the floor, and internet scammers are hyper competitive. reply alt187 7 hours agorootparentprevBing results are inferior, but Google sure as hell isn't superior. reply Incipient 18 hours agorootparentprevI rarely have to search more than once or twice for a topic, and find the result in the top few links on bing. But I'm mostly searching for tech stuff. Local content, or answering questions, google is better. reply pjmlp 12 hours agorootparentprevGoogle no longer provides anything useful, to the extent they were famous for. reply mlekoszek 21 hours agoparentprevThey're really using every tactic they can -- and for the life of me, I have no idea why. They've pushed so hard, for so long, to make Bing succeed -- even forcing it in the Start menu -- and it's still not owning the search space. reply gjsman-1000 21 hours agorootparentIn my opinion, they'd be much more effective if they just killed the Bing brand, killed Bing rewards, killed the Bing newsfeeds, rebranded it to \"Private Search\" at privatesearch.com, and called it a day. Yes, people have memories shorter than a goldfish. reply autoexecbat 21 hours agorootparentAgreed that the Bing brand has to go, but I think they should use their normal naming schemes, something like \"Microsoft Azure Cloud Search Pro 2025 SP1\" reply foobiekr 22 hours agoparentprevHonestly whatever the hell Google offers at this point has been disguising itself as google search for years. It sure as shit is not what people expect from google. reply thaumasiotes 19 hours agorootparentI had similar thoughts; my gut says that this is bad behavior by Microsoft, but that what Google has done to their own supposed product is bad enough to justify it. reply zb3 22 hours agoparentprevNothing can keep me on Bing unless the results improve. Or am I the only one who regularily gives Bing a try only to find out the results are irrelevant? reply amyames 20 hours agorootparentIt just seems I’m doing the google -> bing -> yandex thing a lot now. And then I don’t bother with many competitors because they’re all bing based anyway. Way down the list sometimes I resort to Brave search. Not because it’s good. But in fact, because it’s so bad, it might be indexing something the others tried getting rid of for a good year or two after everyone else tried to memory hole it. Which has helped me pull cached versions of something interesting “to me” that wasn’t interesting enough for someone else to have gotten with archive.today Think the most recent one I went down the whole rabbit hole on was a tv show called “that’s my bush” from Comedy Central. I was willing to buy them but they were Unobtainium. I did end up finding the episodes on archive.org and on torrents, via yandex. Great example of something harmless and hilarious that Big Social and Big Search just HAS to protect my delicate sensibilities and my fragile mind from. Just to underscore how stupid and petty some of this stuff has gotten. Even if it’s not outright censorship of (at best) tangentially “political” content (they had planned on lampooning whoever won, thinking Al Gore was going to be president, and it’s the same guys who did South Park so it’s culturally and historically interesting to some of us) it proves how increasingly irrelevant Google has become. Google and Bing both hid their availability on archive.org from me and I would not have thought to look there. Meanwhile, first hit on Yandex. reply littlecranky67 22 hours agorootparentprevI use bing chat (ChatGPT something) cos it works without login. I have it on a shortcut search trigger in Firefox with temporary tab containers. Replaced more than 50% of my searches, I use Kagi for the rest. reply notahacker 21 hours agorootparentprevHonestly, I'm under the impression they've converged so much recently I can't be bothered to switch on my work machine, and I don't think this is because Bing is getting better. I think there are a few areas where Google still has an advantage (if I search with a city name, Google will match results to the city my IP address is located on and not a smaller, less significant one in the United States) but I think their self promotion and AI Q&A bullshit in results is actually worse. reply zb3 20 hours agorootparentPerhaps something like LMArena but for search engines could help them understand what underperforms.. is there a tool where i could see results side by side? I never thought about that.. reply HeyLaughingBoy 21 hours agorootparentprevI started a new job and the browser default was set to Edge. I never bothered to change it and defaulted to using Bing for search. TBH, I don't notice a difference in results. reply riiii 20 hours agorootparentThat's not because Bing is good. It's because Google has been enshittified. reply jjcm 22 hours agoprevDisclaimer - I used to work on Bing like... 8 years ago. There's probably some debate around whether this is nefarious or genius, but I'd lead towards the later. \"google\" has always been one of the number one search terms, and the amount of people who would open chrome, search for google in the address bar, then open google in the google search results, then do their search, was wild. There's a very large percentage of less technical people who aren't looking for Google, they're looking for search, and in their mind the two are the same. They likely don't care what search engine they're using, so I suspect this actually captures a very large amount of search volume, while still solving the intent of the user. reply CobrastanJorji 21 hours agoparentDisclamer - I owned a restaurant that gave Pepsi products to customers who explicitly ask for a Coke. There's probably some debate about whether this is nefarious or genius, but I lean towards the later. \"Coke\" has always been the number one request from our patrons, and the amount of people who just wanted any soda but said \"coke\" was wild. there's a very large percentage of poorly palated patrons who aren't looking for a Coca-Cola, they're looking for a soda, and in their mind the two are the same. They likely don't care which soda they're drinking, so I suspect this actually captures a very large amount of soda sales, while still solving the intent of the patron. What's that? There's a process server outside? Whatever for? reply quink 21 hours agorootparentA perfect analogy, if I were to trust the glass with my deepest darkest secrets, had a relationship with it going back decades, expect it to point me to the right direction and keep track of much of my correspondence, and so on and so forth. OK, maybe a glass of soft drink somehow doesn’t do that, but I suppose it’s perfect analogy adjacent. reply tbrownaw 20 hours agorootparentprev> Disclamer - I owned a restaurant that gave Pepsi products to customers who explicitly ask for a Coke. I have in fact heard \"coke\" used as a generic before. Just like google, kleenex, champaign, cheddar, ... reply pests 20 hours agorootparentThis example was doomed from the start because of this fact. A lot of the US south uses the generic \"coke.\"* It is not uncommon for this conversation to play out: \"Can I get a coke?\" \"Sure, which kind?\" \"A Coke\" (or a pepsi, or fanta) In my neck of the woods we call it \"pop\" which always sounded strange to me in isolation. * As famously depicted in the 2003 Harvard Dialect Survey. reply CobrastanJorji 1 hour agorootparentThat's what makes it work as a metaphor, I think. Our former Microsoft friend above says that when people ask for \"google search,\" what they mean is \"any search engine,\" just as people in Georgia say \"coke\" when they mean \"a soda.\" You say the right response is \"sure, what kind,\" but the Microsoft solution is to serve them a Pepsi that they disguised to resemble a Coca-Cola at first glance. reply lesuorac 20 hours agorootparentprevTo avoid the whole question of if they carry pepsi or coke I usually just ask for a pepsi-coke and I've yet to run into any problems. reply ziml77 20 hours agorootparentprevBut at the very least they need to say \"No Coke. Pepsi.\" reply lukevp 21 hours agorootparentprevThis was so offensive to imagine as a Coke fan, great choice of metaphor! reply alt187 7 hours agorootparentprevI don't care either way. Brands don't exist, grow up. reply lobsterthief 5 hours agorootparentSorry to break it to you, but yes they do. reply kalleboo 15 hours agorootparentprevIf you ask someone for a Kleenex, are you going to be angry if they give you some other brand of paper tissue? reply bhelkey 20 hours agorootparentprev> I owned a restaurant that gave Pepsi products to customers who explicitly ask for a Coke. Did you tell them they were drinking Pepsi or ask some variant of \"Is Pepsi okay?\" reply CobrastanJorji 18 hours agorootparentIn response to their request, I said nothing and brought them a red and white paper cup with \"Cola\" written on it in the Coca-Cola font. reply riiii 20 hours agorootparentprevAre you from the PR Disaster Mitigation Department trying to find justification for this? reply tedunangst 19 hours agorootparentprevI have no idea why there's a process server outside, but rest easy, it's nothing to do with serving Pepsi. reply jjcm 20 hours agorootparentprevI definitely get what you're saying - there's an element here of taking what a customer asks for and returning something different, but I think it's an imperfect analogy. It's not bringing them a Coke, it's bringing them a dispenser that says \"Cola\" next to a fridge with options. For people who just want Cola, it's immediately available. For those with a brand choice, there are additional options. The reality I'm trying to portray though is that the demographic of people who search \"Google\" in a search field rarely overlaps with the demographic of people who are opinionated about their search tool, so this ends up serving a segment of the population in the way they expected. reply blibble 18 hours agorootparent> I definitely get what you're saying - there's an element here of taking what a customer asks for and returning something different taking money for this is literally Google's business model search for geico, entire initally visible results page is other insurance companies reply m3kw9 20 hours agorootparentprevIt’s a cheap trick from some 20 year old fresh out of college. It works though but it makes Microsoft look soft and somehow non professional. But still good for them if they get to convert a few users reply scotty79 19 hours agorootparentprevI don't see anything wrong with that. Coke is pretty much generic term. And Pepsi and Coke and other brands of cola flavored sweetened water are all pretty much the same. People shouldn't be drinking this stuff at all anyways. It should be mandatorily white labeled anyways. reply thaumasiotes 16 hours agorootparentprev> there's a very large percentage of poorly palated patrons You should look into writing poetry. ;D reply knowitnone 17 hours agorootparentprevexcept you can kill someone by switching their choice of foods. why would you do that? reply quink 21 hours agoparentprev> They likely don't care what search engine they're using That's nothing, for our next iteration our navigation system will take you to the nearest Woolworths because they've got a commercial partnership with us even though the customer quite clearly said 'Coles'. It's likely they don't care. reply Dylan16807 10 hours agorootparentIf you want to make it more accurate, the car started in the Woolworths parking lot or something. But that doesn't capture many of the other aspects. Hmm. My best attempt at this car analogy is more like... you walk over to some idling Lyft drivers and say you need an Uber to Coles. And then one of them drives you to Coles instead of driving you to the nearest Uber idling spot. reply netsharc 19 hours agorootparentprevHuh, imagine if current operating system trends are applied to car computers. \"To store your seat settings across reboots, get our Comfort subscription. [Subscribe] [Not Now]\". In fact, how shitty have OSes become that they are nagware now? reply shiveenp 21 hours agoparentprevThis comment tells me everything I need to know about the kind of people that work at Microsoft. reply ed_mercer 21 hours agorootparentWhich is... that they're all geniuses? reply not2b 21 hours agorootparentMore like, they think that deceiving people for profit is genius. reply jrochkind1 21 hours agoparentprevIf they didn't care what search engine they were using, would it be necessary to make it look so much like the google homepage? reply tokioyoyo 21 hours agorootparentOlder people don’t understand the idea of “search engine”, they understand “google”. They don’t realize you can “google” through Bing as well. I hate it, but it is what it is. reply jrochkind1 17 hours agorootparentI mean, i would describe what you are describing as them actually caring a lot what search engine they are using, but possibly out of ignorance. reply geodel 21 hours agorootparentprevBecause they think it is genius. reply vasco 21 hours agoparentprevIt's genius to copy your competitor because the user might not notice and you can also solve their problem? I don't think it's genius. reply from-nibly 21 hours agoparentprevMisleading people is always nefarious full stop. It's not your job to decide whether or not someone else cares, it's theirs. reply RajT88 21 hours agoparentprevIt can be both. And it is. Machiavellian, even. https://ianchadwick.com/machiavelli/chapters-15-21/chapter-1... reply suddenexample 21 hours agoparentprevThe ones debating whether this is nefarious or not are the ones ruining the tech industry. This is absolutely nefarious. Whether or not it's a clever path to promotion due to corporate incentives is irrelevant. I'm curious what part of Microsoft's culture enables these satirically slimy product decisions. In theory, other megacorps should be no better, but somehow they seem to maintain a bar that Microsoft always manages to stoop below reply szundi 21 hours agoparentprevWith all due respect, still feels bs to rationalizing the intentional misleding of these poor people. It is not a coincidence that Google and search is the same in their heads. reply ocdtrekkie 21 hours agorootparentIs it bad to mislead these poor people when the outcome is better? Google is not good at returning results and is exceptionally good at directing nontechnical users to malicious ads. Bing is saving people. If a user is not equipped to determine the difference between Google and Bing, you should not redirect them to a website which is 80% ads. reply ClassyJacket 21 hours agoparentprevThat makes no sense. If they don't care what search engine they're using, why do it? reply gazchop 21 hours agoparentprevI haven’t heard anyone utter anything but disgust at accidentally using bing. They know. The fact windows is full of dark patterns to try and get you to use it is pathetic disrespectful hubris not genius. reply nneonneo 19 hours agoprevI used Bing on mobile for a while, and I quickly noticed a horrible dark pattern: the mobile website has a little banner that pops up at the top prompting you to download their app, but this banner only loads in after a short delay (maybe half a second) after the rest of the page. In particular, it shows up right where the search bar was (pushing the bar downwards) - meaning that if I aim for the search bar right when the page loads, I often end up hitting the banner ad right as it loads in. I’ve probably loaded their App Store page a dozen times at this point by accident - it’s that annoying. I swear this is deliberate. There’s not really any good reason for a delay on the “you should get our app” banner that I can see, and even less of a good reason to have it load at the exact position of the search bar. Some engineer in Redmond is probably feeling really good about tricking people this way… reply null0pointer 14 hours agoparentI bet it’s a bug but their metrics suffered when they fixed it so they rolled it back. reply cj 21 hours agoprevFun fact: Microsoft Ads (the place you go to buy ads on Bing) is essentially a carbon copy of Google Ads in every way imaginable. The UI is, quite literally, exactly the same. The names of the features are nearly identical. There is very little differentiation, and it's 100% by design - doing this makes it very easy for marketing people to switch between ad platforms without needing to learn a completely new interface. It's quite entertaining to watch. Google will release a feature, and then a few weeks later Microsoft announces the exact same thing. Microsoft is learning that copying success is often easier than creating it from scratch. Making their products look identical to Google's makes it a lot easier to switch between the 2. reply theonemind 18 hours agoparentThey've always used copying as one of their signature moves, see zune vs ipod, win3/95 vs mac, early Internet explorer based on spyglass/NCSA mosaic, Novell eDirectory vs ActiveDirectory, C# vs Java, F# vs Ocaml, and many more I would have to think hard about and take a long time to remember. They tend to enter late with a me-too product, whether they copy, acquire, or embrace-extend-extinguish, but copying does play as large a role as any of their strategies, none of which generally involve actual innovation and often lean heavily on illegal, underhanded, or unethical business tactics. reply neonsunset 17 hours agorootparentPlease try using F# or C# for once and you'll see how incorrect this statement is. Both had huge amounts of novel work that influenced the whole industry. reply oehpr 3 hours agoparentprevAdversarial compatability is not a reason to mock a competitor to an entrenched monopoly. I have no love for Microsoft, but the idea that a locked in monopoly, responsible for tainting or outright destroying huge swaths of the internet, is a \"success\"... Not gonna lie though. Making a fake page that looks like a competitor to show people after they ask you to give them their competitors site is very mockable. I see the similarities between these situations, but the difference is deception, Not that it's \"copying\". reply solarkraft 20 hours agoparentprevThis is smart and I don’t see anything wrong with it. They are familiar with malicious compatibility, though usually from the other side. Props for one of the rare times they apparently thought a UI through. reply Arnavion 22 hours agoprevYou can also see it for yourself without needing Windows or Edge by opening https://www.bing.com/search?q=google in Linux Chromium for example. reply PessimalDecimal 22 hours agoparentThe only other search query I have found that provides a similar \"spoofed Google\" look is https://www.bing.com/search?q=yandex. reply tim333 21 hours agoparentprevBeing charitable you could see that as a tribute to Google to mirror their doodles. reply bangaladore 21 hours agoparentprevInterestingly that doesn't work on Brave Windows (Chromium) but works on Chrome Windows. I wonder if Brave is specifically deleting this element. reply do_not_redeem 21 hours agorootparentInteresting. uBlock Origin is hiding the element. At first I wasn't able to see the search box, but I can see it if I toggle cosmetic filtering. Looks like it's targeting #b_pole (\"Promoted by Microsoft\") reply bangaladore 3 hours agorootparentGood catch. I use Brave daily so I don't have any extensions on my Chrome install. I am using UBlock with Brave. reply riiii 22 hours agoparentprev\"Fuck Microsoft! Fuck!\" -- Dr. Adrian Mallard reply shepherdjerred 17 hours agorootparenthttps://www.youtube.com/watch?v=2zpCOYkdvTQ reply JohnMakin 22 hours agoprevI prefer bing + copilot as a search engine over google if I must use one. Been using it since the beta, have a corporate/business account now. It (usually) provides a good description of my answer and gives sources I can click on to verify. No other search engine I am aware of is doing this right now, although I know chatGPT has recently introduced or talked about a feature like this (I don't really use chatGPT). This is exactly what I want in a good search tool. However, my frustration with bing arises in that from one day to the next there is absolutely no consistency in how \"good\" the tool feels - almost like there are times they downgraded the underlying model to reduce load/cost without informing the user. They should focus on a better user experience than google, which if I can interject my opinion, is a shockingly low bar these days, and let growth happen by simply being a good tool - all the gimmicks and attempts they've made at mass adoption has seemed very forced. And yes, I'm aware of the natural lock-in advantage google has and how hard that is to surmount, but bing has a large enough percentage of search userbase by now to achieve its own critical mass if it needed to, IMO. Forcing adoption and locking it into microsoft ecosystem will probably eventually be the reason I stop using it. reply granzymes 22 hours agoprevI would’ve asked to be taken off of this project if someone had asked me to build this. How embarrassing to need to stoop to this level. reply grumpykitten 22 hours agoparenttbh, if you're working on bing you probably don't really care about the work reply dec0dedab0de 22 hours agoprevI started a new job where I have to use windows, and more than once I didn’t realize I was using bing until I went to turn on verbatim and it wasn’t an option. Side note, I miss search engines from 20 years ago, I can’t believe it’s gotten this bad. reply recursive 21 hours agoparentKagi is pretty good. reply dlachausse 22 hours agoparentprevDuckDuckGo has served me pretty well for the past couple of years. Also, their AI offering duck.ai is pretty solid as well. reply jokethrowaway 22 hours agorootparentIt's my daily driver too. Quality is ok but not as good as Google from a few years ago. Snippets (especially code) and shortcuts are cool. It's less censored than Google, but then they went on and censored russian propaganda during the war. They completely lost all their credibility. I don't care how bad or good the content it is, I want a service without censorship. For copyrighted content they are a bit better than Google but worse than Yandex - simply because 90% of DMCA strikers agencies bother reporting a google search result, 50% bother with duckduckgo, 10% bother with Yandex. reply userbinator 20 hours agoprevThat's the \"offensively inoffensive\" Corporate Memphis art which Microsoft is pushing aggressively everywhere, so I recognised it at first glance as being from MS and not Google. Google has a slightly different style. reply croisillon 22 hours agoprev\"i'm appalled that i ended up searching google on bing when i honestly believed i was searching google on google\" - no one ever reply fullshark 20 hours agoparent??? They aren't searching google on bing, they are issuing bing searches on a search bar designed to spoof google's and fool them. reply seba_dos1 6 hours agorootparent...which appears after you search for \"google\" on Bing. reply asdasdsddd 22 hours agoprevThis is only funny because no one takes bing seriously. reply TacticalCoder 20 hours agoparent> This is only funny because no one takes bing seriously. But Microsoft is way more dangerous than Google. They've been using all the dirtiest tricks in the books since decades longer than Google. MSFT also has a market cap 30% greater than the one of GOOG. Microsoft is known in the industry, all around the world, for illegal kickbacks (including to officials). Google may be bad but Microsoft is just downright an evil company. In addition to that, as the old saying goes, the day Microsoft produces a product that won't suck, it's going to be a vacuum cleaner. At least Google gave back a lot to open source and contributed a huge lot to Linux and to Linux's success. I'm not saying Google is clean but they're not anywhere near as dirty as Microsoft. The whole agenda / narrative that pushed by Microsoft shills atm is also all too obvious \"You must break Google\". I don't think so. I think it's Microsoft that should be broken up by anti-trust regulations enforcement. Shittiest company on earth. reply Krssst 20 hours agorootparentAlso, not using Android or Chrome is very feasible for almost everyone (thanks to iOS and Firefox/Chromium). Not using Windows is almost impossible for a large array of use cases and professions. reply dzhiurgis 21 hours agoparentprevHuh? Everyone's UX is miles better than Google. Heck even Yahoo search is now better than Google. You need to get your head out of sand. reply cptskippy 22 hours agoparentprevI think it's hilarious because they're doing the same shenanigans that Google does. When you search on Google everything above the fold is not \"a list of search results\". Often it's a definition or conversion calculator or some other custom UI that isn't \"a list of search results\". Microsoft has programmed Bing to do the exact same thing. Everything above the fold is a custom UI that coincidentally looks a lot like the Google Search engine. The Chef's kiss is that it scrolls down just the tiniest bit to put the Bing UI above the fold rather than hide it. This gives them plausible deniability. It's brilliant and hilarious. I love it. I'm still not using Bing (or Google for that matter) but I love it. reply ricoche 22 hours agoprevThis is so desperate I feel bad for them reply ehsankia 22 hours agoparentEverything they've done for the past few years has been desperate. If you try to download Chrome on a new Windows install, at every step of the way, it begs you to reconsider, shit talking Chrome, saying Edge runs on Chromium so it won't make a difference, trying to throw pop ups at you to distract you. At some point, Edge would literally open a tooltip in the top right corner of the page where the download button on chrome.com used to be. And it continues as you try to make Chrome the default browser. After all that, there are still plenty of tasks in Windows that still open Edge... reply ryandrake 21 hours agorootparentIt feels really sad and pathetic when a massive company desperately begs you to do something, not just Microsoft. Please install this! Please don't disable that! Please allow this permission! We really want you to do this! I would say \"have some class\" but class doesn't make stonk price go up. reply olyjohn 22 hours agoparentprevI don't feel bad for them. Fuck them. It's a delicious taste of their own medicine. reply dzhiurgis 21 hours agorootparentIt deeply saddens people still use anything Google. GMail and Youtube are big ones that are difficult to switch. But browsers and search engines are eons better now. reply rlpb 20 hours agoprevGiven the tricks that Google play (or at least played) in hijacking their own search results to scare users into switching to Chrome, I shed no tears here. Google set a new lower standard in deceitful behaviour, and Microsoft are simply following. reply netsharc 18 hours agoparentSheesh, if what you say is true I guess decency is headed towards extinction, and even big companies are acting like the supposed con-men sellers of a Middle Eastern bazaar... reply baxtr 21 hours agoprevI bet this was initially an A/B test idea of a product manager eager for promotion. reply jhanschoo 21 hours agoprevIf MS hasn't changed the result in the meantime, the screenshot in the article is slightly dishonest by omission. The journalist has manipulated the browser window's size and scrolled down a bit so that only the \"promoted result\" is visible and without any indication. The journalist's characterization > Before you scroll down to the actual search results, you’re presented with an all-white page with a centered, unbranded search bar and a multicolored doodle above it that’s heavy on yellow, red, blue, and green. is dishonest. In actuality, Google-like interface appears as a full-width promoted result/ad before the organic results. There is vaguely the words \"Promoted by Microsoft\" by the top-left, and a 'X' by the top-right. For large enough viewports, the 'X' and organic search results are visible. The \"Promoted by Microsoft\" is visible without scrolling at any size. Note nevertheless that the journalist has also failed to point out a particular interaction that would support their thesis. For searches that trigger this \"promotion\", the window immediately scrolls the page so that the promotion is aligned to the top of the viewport, and the search bar in the promotion is focused. (The \"Promoted by Microsoft\" is visible without scrolling at any size.) If one is logged in (and on Edge?), this promotion is still present, but as a tiny search box before the organic results. reply pornel 21 hours agoparentI've tried myself (in Firefox on macOS even), and Bing really scrolled down automatically to hide its logo from the top of the page. reply jhanschoo 20 hours agorootparent> Note nevertheless that the journalist has also failed to point out a particular interaction that would support their thesis. For searches that trigger this \"promotion\", the window immediately scrolls the page so that the promotion is aligned to the top of the viewport, and the search bar in the promotion is focused. (The \"Promoted by Microsoft\" is visible without scrolling at any size.) That's what I said. This is still in contradiction with the screenshot, which I described as: > The journalist has manipulated the browser window's size and scrolled down a bit so that only the \"promoted result\" is visible and without any indication. where the \"Promoted by Microsoft\" is NOT visible. I find that dishonest. reply mrayycombi 19 hours agoprevThey'd have to disguise themselves as duckduckgo to fool me. Maybe throw a weird looking duck on their landing page or something. I've avoided Google for years. Quack. reply KoolKat23 21 hours agoprevIt's so sleazy. The logical next step for Microsoft's Bing, MSN and advertising network is their very own online gambling. reply cynicalpeace 22 hours agoprevI'm surprised more startups don't just copy the most famous landing page of all time. reply meltyness 21 hours agoparentI was wondering why Google hadn't replaced 'I'm Feeling Lucky' with something to do with LLMs, or just added an LLM-generate option. I came to the conclusion that they're in corporate denialism over the whole thing. They'd be happier if their finger slipped and made Anthropic and OpenAI vanish until they could resurface and capture the market. Possibly not a great strategy. It seems all of their years of letting the open web decay and vanish has caught up with the fact that many requests can be serviced with an inverse thesaurus manual snippet soup. reply politelemon 21 hours agoparentprevWell currently, every chatbot has seemingly copied the now famous chatgpt layout. Making it a defacto. reply unethical_ban 22 hours agoprevI'm usually not a fan of user deception. But I can't bring myself to care much that Bing is trying to play off the masses' pavlovian trust of the google interface. reply notfed 22 hours agoprevCan't reproduce. Does it only happen from Edge? reply mrweasel 22 hours agoparentWorks in Firefox on macOS. Interestingly enough, you can already use Bings settings to disable all the cruft on bing.com. If you do that, I think the majority of users would not know the difference between Google and Bing, other than a more pleasant search experience and fewer ads (or no ads, I'm currently see zero ads or trackers on bing.com without any ad blocker). Seems hard to justify staying on Google, when Bing yields the same or better results, and less ads. reply arielcostas 22 hours agorootparentAnd, at least in the EU, we get LLM responses (via MS Copilot) on Bing, but no \"AI Overview\" on Google. Though seeing how poorly Google AI Overview on search works, I'd rather not have that offered to me. reply asdff 22 hours agorootparentYou can just scroll past it you know reply mrweasel 22 hours agorootparentYou can disable it entirely apparently. I just checked the settings on Bing and there is a \"Copilot response on result page\" which can simply be turned off. That one good thing about Microsoft, they aren't afraid of offering the users settings. reply franczesko 22 hours agorootparentprevThe copilot feature is pretty handy. reply criddell 21 hours agoparentprevSearch for google in bing. At least that's how I can see it. https://www.bing.com/search?q=google reply VyseofArcadia 22 hours agoparentprevI reproduced in both Firefox on Windows 11 and Firefox on Debian. reply ziddoap 22 hours agoparentprevWeirdly, it works for me on FireFox (I see the Google-like page) but not on Edge (where I just see a link for Google, no mimicry). I am not signed in on Edge. reply LeoPanthera 22 hours agoparentprevIt doesn't work if you're logged in. reply insane_dreamer 22 hours agoparentprevWorks on Safari / Mac reply VyseofArcadia 22 hours agoprevNever quite understood the hate for Bing. I despise Microsoft, but Bing is fine. It's one of the least shit Microsoft products there is. It definitely wasn't competitive on release, but it's fine now. To be fair I think this is a function of both Bing having gotten better and Google having gotten worse. reply franczesko 22 hours agoparentBing is ok. DDG is pretty much its anonymized version. reply asdff 22 hours agoparentprevInitially it reeked of IE by association. But then internet communities realized it was better than google for porn. So now its OK in some circles at least. reply sumtechguy 22 hours agoparentprevHonestly, this is what I want bing's 'homepage' to look like and I usually configure it to be as close to that as I can. The default was/is a ton of news and other junk. I just want a search bar and maybe an identifier picture. It was one of the things I liked when google first came out. It was 'simple' as many of the other search engines from years ago had tons of 'helpful' things on the front page that I just did not want. reply hnlmorg 22 hours agorootparentI always find it embarrassing when a colleague opens up a new tab while screen sharing and Bing / Edge starts spamming trashy news. Given Microsoft is “soo enterprise” it’s always a source of amazement that Microsoft feel it’s acceptable to default to this kind of spammy behaviour. It just goes to show how running businesses by engagement scores really is just a race to the bottom. reply sumtechguy 5 hours agorootparent> starts spamming trashy news. Think I am on my 4th or 5th time going into the settings to turn it off too. I think google may be starting to do something similar too. I think I got caught in an A/B test a week or so ago. Little bits of news popping on the landing page. Off to the settings to turn that off. Which then somehow messed up the settings on my phone as well. Computers doing 'surprising things' makes users angry and feel like they do not have control. reply neocritter 22 hours agorootparentprevThat was back when everyone was trying to make a \"portal\" to compete with AOL. It seems like browsers are headed down the same path now with replacing the simple search box new tab page with the same stuff the portals had. reply Stagnant 21 hours agoparentprevI've never used bing that much, but after google removed its cache feature last year, I found myself using bing a lot more often. A couple of weeks back I was so disappointed to find out that Microsoft had pulled the plug on bing's cache as well. I just can't grasp why anyone would think that removing the one feature that gave you edge over the competition was a good idea. AFAIK now the only search engine remaining with a cache is yandex. reply UniverseHacker 22 hours agoparentprevI have a longtime dislike for Microsoft but google has become almost unusable recently with fake ai pages filling the results, and Bing seems to work better. reply debugnik 22 hours agoparentprevI stopped using DDG, which pulls results from Bing, because Bing fell very easily for SEO slop; yes, even worse than Google. The most painful for me was a set of wikis filled with AI-generated nonsense about OCaml and some other niche languages, which completely shadowed genuine content on the first page of any search. reply elAhmo 22 hours agoparentprevMoves like the one from the linked article is one contributor to the hate. reply mrweasel 22 hours agoparentprevHonestly I feel like Bing has been the better search engine for 6 - 8 years at this point. reply NotYourLawyer 22 hours agoparentprevGoogle and bing suck about equally at search these days. reply datavirtue 22 hours agoparentprevIf I land on a Google page I search for Bing now. Colipot is included with Office and I'm signed in. Copilot is far more useful than Google search. I would use Bard (Gemini) but I don't have a work login for Google properties. Microsoft wins. reply Neil44 22 hours agoparentprevIn my experience the search results on bing are a spam filled trash fire. reply lupusreal 22 hours agoprevBing earns my use simply by virtue of them not captcha-hell banning me for having privacy features enable and using a VPN. Google can go to hell. reply Sephr 22 hours agoparentBoth companies are known for highly invasive tracking. reply lupusreal 22 hours agorootparentBing lets me search even though I block their cookies, trackers, etc. Google doesn't. If I even wanted to use Google I'd have to go through the hassle of whitelisting their crap, and for what? reply pessimizer 21 hours agorootparentI remember people arguing on HN over a decade ago about how awful it was that on Google News you wouldn't get direct links, but instead links to their tracking system that would forward you to the story. Now, they'll even refuse to forward the links unless you do a captcha, and you can't escape from captcha hell unless you accept cookies and you don't forge (or refuse to send) your referer. We were talking about how most of the internet got locked behind walled gardens, but we didn't notice how much of the \"open\" internet secretly became a walled garden. Starting with that Facebook like button, Google Analytics, and Google ads everywhere, and culminating in Cloudflare MITMing everything. aside: One of my personal conspiracy theories is that when the government wants deep activity on a site to be tracked, they DDOS the site until there's no other option than to add Cloudflare. reply jmclnx 22 hours agoparentprevTrue, that alone keeps me away from google. reply SoftTalker 20 hours agoprevSeems pointless to me, I haven't used google.com or bing.com's main page in years. My browser search bar just searches my preferred search engine if I enter anything that isn't a URL. reply interestica 22 hours agoprevThis only happens if one searches for “google” in the Bing search bar. This is less deception and more a fun dig. Try searching for “askew” in google. reply ralferoo 22 hours agoparentEntirely changing the format of the results page based on a keyword match for a competitor is very much deception. Although they've been doing similar for years when you search for Chrome as the very first thing you do on a new install and there the entire screen is basically full of tricks to try to make you stay on Edge and pushing the actual search results down so far you need to scroll. I guess they've got away with that a long time, they probably don't think anyone will care. It all feels a bit like the \"I'm feeling lucky\" button from years ago on Google when it was kind of the default choice for everyone because back then Google actually cared about putting the most useful page at the very top... Remember when one of the best tricks was searching \"French military victories\" and pressing \"I'm feeling lucky\" took you to a page that looked exactly like the google results but said \"No results found, maybe you meant 'French military defeats'\". Classic stuff! reply JumpCrisscross 22 hours agoparentprevIt also only lands because Google has so thoroughly genericised its brand as to be unrecognisable at a glance. reply flerchin 21 hours agoparentprevIs there something I'm supposed to be noticing with \"askew\"? Seems the same as cattywampus. reply Arnavion 21 hours agorootparentThe page gets a CSS filter applied that tilts it by 1deg.body { transform: rotate(1deg) }reply NotYourLawyer 22 hours agoparentprevNo, this is 1000x more deceptive than the askew thing. reply Tistron 22 hours agoprevLooking at it charitably, it doesn't seem very different than going to some outdoor shop NatureLand(r), asking for a Thermos and them showing you a NatureLand(r) thermo flask. Sure, maybe you really wanted a thermo flask of the Thermos brand, but most people just want an insulated bottle for tea or some such. I mean, I wouldn't react more to somebody saying that they googled something with bing than I react when somebody offers me tea but it's herbal infusion. I'll have some reaction that is wrong, but it's also expected and common. reply gardenhedge 21 hours agoprevIs Bing the worst brand name ever? They tried so hard to make it work but it just doesn't. I feel like any other name would be better. reply qgin 19 hours agoparentBard reply zb3 22 hours agoprevI'll immediately switch to Bing if you allow me to search by regex, or at least \"literally literally\". reply londons_explore 22 hours agoparenttrue regex search of the internet is a \"more compute than on all of earth\" type problem. They could at least get closer tho... reply nine_k 21 hours agoprev— Hey, waiter! That cup that you've brought me, is it tea or coffee? — Sorry, sir, you mean you cannot tell by the taste? — I can't. — Then what difference does it make? reply noman-land 19 hours agoparentIt makes a difference because if I drink coffee I'll die. Since you know nothing about me, how about you let me decide what I want and why. reply Dylan16807 10 hours agorootparentThen it was pretty foolish of you to just say \"I can't.\" with no elaboration when the waiter asked about telling by taste. Well, either foolish or a deliberate bait so you could berate him more easily. Neither option looks good. If you wouldn't have answered like that, then the story isn't about you. reply nine_k 13 hours agorootparentprevIt's a joke, and not a serious dialog, exactly because there may be a difference (not necessarily life and death, but maybe the price), but it fades beside the fact that the drink is so bad that its taste is incomprehensible. reply LightBug1 21 hours agoprevI watch Groundhog Day at least once a year at Christmas. Bing.com will never not be associated with Ned Ryerson ... Doesn't matter how much they disguise it! reply linuxftw 21 hours agoprevI love it. If a user doesn't understand how web browsers work, they're deserving of this behavior by MS and Bing. Genius. reply dsissitka 22 hours agoprevRemoved. I see the scrolling happens for me in Chromium so that's not PCWorld's doing. reply Arnavion 22 hours agoparentYour second screenshot is scrolled up to show the top header bar that mentions Bing. The default page load scrolls down just enough to hide it (intentionally or otherwise). reply dsissitka 22 hours agorootparentIt's not doing it for me in Firefox but it is in Chromium. reply Arnavion 22 hours agorootparentSure. It also does it in Edge which is what all the articles are about, since Windows+Edge is the primary reason people end up using Bing by default. reply pie_flavor 22 hours agoparentprevThe page automatically scrolls, 9to5google's article has a gif of it in action. https://9to5google.com/2025/01/06/bing-trick-users-google/ reply Sephr 22 hours agoparentprev> the scrolling probably isn't intentional What makes you believe that? It's pretty clearly intentional even if it only applies to Chromium browsers. reply dsissitka 22 hours agorootparentI was referring to PCWorld there. I've rephrased it. Hopefully it's a little clearer now. reply Sephr 22 hours agorootparentAh, thanks for the clarification reply funOtter 23 hours agoprevSo google owns the \"image and text box on a web page\" design? reply macspoofing 23 hours agoparentNo - google doesn't own the \"image and text box on a web page design\" ... but it is very odd what Microsoft/Bing is doing when you search 'google' .. they even 'scroll' the page down to hide the primary bing search bar. It's odd. reply fullshark 22 hours agoparentprevThe story isn't \"Bing is copying Google's amazing design.\" The story is bing devised a specialized search result page for the query \"google\" which is intentionally designed to trick its own users. reply rtkwe 22 hours agoparentprev> \" users are discovering that if they search for “Google” in the primary Bing interface, they’re shown a special Bing search page.\" That's a little more than just aping the design of Google. It's a pretty intentional effort to deceive users into remaining on Bing. reply dewey 23 hours agoparentprevIt’s about the doodle, not the image and text box. As the article explains. reply saalweachter 22 hours agorootparentI would say it's more that you get to this page by typing \"Google\" into the URL/search bar: > This morning, users are discovering that if they search for “Google” in the primary Bing interface, they’re shown a special Bing search page. Before you scroll down to the actual search results, you’re presented with an all-white page with a centered, unbranded search bar and a multicolored doodle above it that’s heavy on yellow, red, blue, and green. reply Sephr 22 hours agoparentprevThis only comes up after searching for Google reply killingtime74 23 hours agoparentprevIt looks like they came up with the headline first then worked backwards to present something that fit as a thesis. reply Dylan16807 22 hours agoprev [–] This removes a step for someone that would have searched for google and then clicked on the link and then done their real search, so overall it sounds like a nice improvement in the vast majority of cases. I don't expect many people that care about the difference between search engines to be using this method. reply lcnPylGDnU4H9OF 22 hours agoparent [–] They're not actually making a google search if they use that search field. Microsoft is just presenting another Bing search field when people search \"google\" and showing that at the top of the page instead of the link they are ostensibly looking for. reply Dylan16807 22 hours agorootparent [–] And what fraction of the people this affects do you think care about the difference? What fraction even know the difference? My guesses are not very big. reply ClassyJacket 21 hours agorootparent [–] If they didn't care about the difference they obviously wouldn't have searched for Google in the first place reply recursive 21 hours agorootparent [–] Not obvious to me. People that search for Google probably don't have a solid grasp on what the difference is between Google and a search engine. Perhaps it's muscle memory. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Microsoft is reportedly altering Bing's appearance to resemble Google's interface, potentially misleading users into using Bing unintentionally. - This strategy underscores Microsoft's challenges in competing with Google's substantial market share in the search engine industry. - Critics suggest that this tactic may prioritize Microsoft's business objectives over providing a genuine user experience."
    ],
    "commentSummary": [
      "Microsoft is altering Bing's appearance to resemble Google when users search for \"Google\" on Bing, leveraging Google's brand recognition.",
      "This tactic is controversial, with some viewing it as deceptive and others as a strategic move to retain users who may not notice the difference.",
      "Legal experts indicate that while Google could potentially sue, proving deception without clear trademark infringement would be difficult, highlighting Microsoft's competitive strategy in the search engine market."
    ],
    "points": 284,
    "commentCount": 206,
    "retryCount": 0,
    "time": 1736278653
  },
  {
    "id": 42627567,
    "title": "Laid off for the first time in my career, and twice in one year",
    "originLink": "https://dillonshook.com/laid-off/",
    "originBody": "1902 words ~10min read Laid Off for the First Time In My Career, and Twice In One Year January 5th 2025 So, I got laid off again… This time it was the week before Christmas, last time it was a week into paternity leave. Both great timing, I know. Last time I didn’t feel like writing about it since it was a swirl of emotions with a new baby in the house and the first time I had been laid off in my career, 13 years in at that point. But this time I’m ready to get it out in a post that’s cathartic for me to write and something you might relate to. If you don’t relate now, I hope it stays that way. Hopefully this post will give you a bit of perspective if you ever do unexpectedly lose your job. The Indigo Story# For both my stories you’re not going to find any shit-talking here for a number of reasons. First, that’s not the kind of person I am. Second, I feel fairly treated given both companies’ circumstances. And third, I want to keep good relationships with the people both that were laid off at the same time and the people still at both companies. Keeping good connections with your coworkers and not burning bridges is one of the most important things I think you can do in your career. So with that out of the way, let’s talk about the Indigo story a bit. One week into paternity leave delirious with sleep deprivation and all the emotions of now caring and loving for such a tiny helpless little baby I remember getting a text from a coworker saying I should probably check my email. A bunch of thoughts and feels rushed in seemingly all at once. What’s going on? Am I getting laid off? I knew there was a chance since the company had multiple rounds of layoffs before but it wasn’t really happening to me on leave right? What was going to happen to our health insurance? Did I do something wrong? How many more people were getting laid off? It felt like ages till the HR call was scheduled and I found out some of the details. A wave of relief took over after hearing I would get paid out for my paternity leave with severance on top so I could take a deep breath, relax, and enjoy being with my wife and daughter for her first few weeks on the planet we call Earth. After a week or two when it felt like we were starting to form something resembling a routine it was time to crack back open the resume. This always feels daunting after being at a company for awhile. How do you succinctly summarize and highlight all you’ve done there? Luckily I had some notes from my own self review one year to help but I also made a mental note to myself to start writing down accomplishments as they happen to make this easier next time. Once I was happy with the updates I got right into the most difficult job search process I’d experienced since trying to find my first software engineering job out of college. I was being selective and only applying for places I’d realistically want to work for and also screening out companies that didn’t seem to have a viable business model which narrowed the field by more than half. Ultimately it’s a pipeline numbers game though so I kept applying, kept interviewing, and kept persisting until I found a match. My funnel ended up looking like this It’s easy to get disheartened by the process when you’re in the middle of it and you get through so many interviews with a company you really want to work for only to be told a lame reason why you’re not getting an offer at the end. I really wish companies would start giving honest feedback even if it’s hard for the candidates to hear at first. It would be a much better way for candidates to improve themselves and we’re all adults here and can take the feedback. The Pryon Story# The Pryon story is much shorter just like my employment was. I don’t feel like going into any of the details since they don’t feel that important now. It was an 85% surprise to me that it happened at all, and 100% surprise for how deep they cut without many signs the company was struggling. I can only speculate what the reasons were, but someone screwed up to hire dozens of people just to lay them off 6 months later. The shock and whirlwind of thoughts and feelings came and went much quicker this time around. Going through the process once before builds your resilience and you take it less personally. How to Tell a Layoff is Coming# Based on my N=2 experience and the posts I’ve read here are some warning signs to look out for: The product org is struggling to set a vision Many distractions and lack of focus Low products usage Perks being cut, cancelled onsite meetings, signs of financial troubles And last but certainly most telling: There’s a “company update” meeting unexpectedly scheduled four hours from now What to Do if You’ve Been Laid Off# First, take a deep breath, this is going to be stressful. But it’s also a new opportunity to find an even better job, meet new amazing people, and expand your skill set. Update and Improve your Resume# Depending on how long you’ve been at your last job you might have a lot or a little to add. Regardless though, you should try and improve the design a little bit each time you update it. Your resume is highly personal and should reflect some of your personality while still looking as professional as possible, so I use any templates or resume builders myself. Just 100% artisanal HTML and CSS for me baby. I want it to stand out from the hundreds and hundreds of other resumes out there in the application pool. Try some new font pairings. Add a touch of iconography. Improve the spacing. Refine, refine, refine. File for Unemployement Benefits# Don’t feel any stigma for filing for unemployment after you’ve been laid off. Your pay your taxes into the system and deserve to get money back when you’re unemployed. I didn’t file after the first time since I had a long period of severance and didn’t want to deal with the overhead but I’ve already filed this time. Start Searching for Jobs# The best places I’ve found to look in the past are: Stack Overflow jobs (RIP) Monthly Hacker News Who’s Hiring Thread Networking and local meetups. This is much more hit and miss for finding a job quickly but it’s playing the long game to grow your network in your local community that is well worth the effort. LinkedIn. I don’t apply to jobs directly since they’re flooded with applicants. Instead I find places I want to work at that have jobs posted on their company website then reach out to employees in the department and try to make a connection and get an introduction. Recruiters. Don’t discount or blow them off. They have a vested interest in getting you hired and 3 of my jobs have been found through them. Some other site’s I’ve used with mixed results are Hiring Cafe, Otta Welcome To The Jungle (weird rebrand name), and Unlisted Jobs. As I mentioned before, when you’re looking for jobs and you want to avoid a layoff in the future it pays to understand the business model and how the company you’re applying to makes money. Ask where the company is in its profitability journey. How many customers do they have? Are they B2B (selling to other business) or B2C (selling directly to consumers)? How long is the sales or conversion process for each customer? How much money does each customer spend? How many rounds of funding has the company raised and for how much? Is the company visibly blowing a lot of money? One place I interviewed for flew its employees across the Atlantic for on sites once a quarter. You’re not going to get an exact answer to all these questions but you should be getting a sense in your head of how financially stable a company is while you’re interviewing. Keep Track of the Jobs you Applied For# You’re likely going to have to do this for the unemployment benefits anyways but it also serves the purpose of making sure you don’t apply to a place twice (more likely than you think with the number of applications you’re going to have to send out), and tracking your progress through all the stages. You might be able to find patterns with what worked and what didn’t by tracking as well but for me it’s felt like there’s too many variables at play with each application to find much commonality. Work on a Project# While you’re unemployed I’ve found it very helpful to have a side project to work on at the same time. It helps keep me sane through the process of looking through hundreds of job posts. Doing that for 8 hours a day will just make your eyes bleed and brain rot. It could be an idea you’ve had in the back of your head for awhile and never had the time to work on till now or it could be an open source project you’ve used and want to improve. Working on an actual project will keep your skills fresh, break up the days, and you’ll have something to show and talk about during the interview process. With any luck the project might even lead to finding job opportunities directly. Keep At It# The software engineering job market today is very different than it has been prior to (and during Covid). If you haven’t searched for a job since then it will surprise you how much more of an employers market it is now. Ultimately though, if you’ve got the skills you will find a job if you play the game long enough and stay positive through the process. You got this. The Takeaway# Layoffs are increasingly becoming part of the normal tech industry experience. More than 150,000 layoffs were reported in 2024 and a whopping 264,000 in 2023. And those are just the reported ones. During the process the first time around I distinctly remember feeling some guilt/shame as if I hadn’t done enough to prove my worth and avoid the axe for the fourth (or more depending on how you count) time. But ultimately it’s not my fault or decision it happened, just like it’s very likely it’s not your fault if it happens to you. I’m not the one deciding to spend over 400 million dollars on spaghetti to see what sticks to the wall. Not yet at least :) An important part of the process is also learning to retake your pen. This is a concept from the book Crucial Conversations I read recently that’s all about learning to define your own self worth and your story in your own words. Don’t let other people tell your story for you or define success for you. In our society where we place such emphasis on your profession, a job loss can mean a shaken self identity. It’s up to you to tell your (truthful of course) side of the story and define yourself separately from any single employer. I think it’ll still be a while longer before we can start to say the dust has settled in the tech industry from the end of ZIRP and the disruption AI tools are bringing. So keep your chin up and keep learning! Want the inside scoop? Sign up and be the first to see new posts No spam, just the inside scoop and $10 off any photo print! Subscribe © 2025 Dillon Shook. All Rights Reserved. / RSS / Sitemap",
    "commentLink": "https://news.ycombinator.com/item?id=42627567",
    "commentBody": "Laid off for the first time in my career, and twice in one year (dillonshook.com)260 points by luu 21 hours agohidepastfavorite238 comments kjellsbells 20 hours agoOne little tip I learned the hard way: an applicant tracking system (ATS) can claim to have imported your fine-looking Word or PDF resume, but that does not mean that it has correctly parsed it and populated the key fields (eg skills) that it shows to the hiring manager. The problem seems to be that ATSes struggle with the \"modern\" style of resume, much beloved of Word template authors, where you might have a left column with your contact details, github, and maybe some skills and then a borderless table on the right side with your positioning statement and job history. I went from zero callbacks to 80% after I junked Word and rewrote my resume in a much more old fashioned, linear format. I used Overleaf (LaTeX) like it was 1999 and exported to PDF. reply pridkett 7 hours agoparentI have seen one behavior with ATS that was actually tracked down and might hit fellow HN readers. A recruiter I worked with at a company asked me why I didn’t have any experience with artificial intelligence. I said it was all over my resume - I’ve been in the field for more than a decade. He said that it didn’t show up until that ATS. It turned out the ATS didn’t properly parse ligatures like “ffi”, “fi”, etc. It rendered them as a blank space, so “artificial” became “arti cial”. I turned over ligature rendering in my resume and started to get more callbacks. Upon further inspection, I discovered that a lot of LLMs also have problems with ligatures and just ignore them when fed a pdf. So, maybe those annoying job apps where you upload a resume and still have to fill everything in an HTML form aren’t the worst thing. reply ok_computer 19 hours agoparentprevLatex and git has been advantageous to authoring a resume for me. It separates the layout design ant content. “Oh no it’s 2 pages with only education on p2.” Quickly comment out a bullet and print off a fresh 1 page pdf. I don’t use dumb words or phrases now to fit layout, at least far fewer. And git makes me less worried about deleting when it isn’t working. One pattern i use, that i think makes things simpler, is to have a layout/contact template with definitions, then different context specific main latex files inputting from a sub directory of section blocks: experience, skills, education, etc. I’d thought I was slick with word table layouts for sections, until a counselor told me the table structures persist in the supposedly flat pdf. I wish there were an standard optional JSON attachment with a ridiculous small kb size limit to upload along with a human readable resume to help out the user data AST parser. reply noahjk 7 minutes agorootparentAnother option is Typst, which claims to simplify the workflow for producing similarly structured documents. Here's an example of a simple resumé [0]. You can see in the \"Quick Start\" section how the resumé template is used, which is very similar to how you described yours. [0]: https://typst.app/universe/package/basic-resume reply mdaniel 15 hours agorootparentprev> I wish there were an standard optional JSON attachment with a ridiculous small kb size limit to upload along with a human readable resume to help out the user data AST parser. AFAIK one can embed arbitrary RDF into PDFs, and then one wouldn't need an extra field https://pdfa.org/wp-content/until2016_uploads/2011/08/pdfa_m... reply Aurornis 18 hours agoparentprevBlaming the ATS is a popular myth. There may be edge cases where some obscure software can't parse a weird resume format, but in general if you put anything remotely resembling a common resume format (Company name, dates worked there, optional description and/or bullet points) it will be parsed properly. I'm in a big Slack where people ask and give career help. Several hiring managers have offered to test people's resumes on their company's ATS over the years. Nobody has ever found a combination that actually failed the ATS except when it was an obvious problem (like someone who made their resume in Illustrator) or an obvious user error (exporting a resume as an image). reply jlarocco 18 hours agorootparentI wouldn't say it's a \"myth\". My experience is a lot more like the OP's than what you describe. A few job sites, like WorkDay, have mangled my PDF resume every time I upload it. Like the OP, I've had to massage the layout and formatting to make it more compatible. reply cm2187 9 hours agorootparentSame experience from me. Most website will show you the result of the CV parsing and in the end I find it takes me less time to populate the different parts manually on the recruiter's website than to try to correct the mess those systems made with the risk of errors it introduces. I wish there was some kind of json/xml standard for CVs. reply hackable_sand 17 hours agorootparentprevSame. Instantly got two hits back after reformatting. I also simplified my experience section by reducing technicals and focusing more on role responsibilities. reply itake 13 hours agorootparentprevMy sense is _most_ resumes are actually reviewed by a human. Most job postings aren't getting 1000s of applicants. If they are, 3 people can review 1000 applications in 2 hours (30s look). reply coffeebeqn 11 hours agorootparentMost companies are not Google. I did some hiring (3 or 4 positions) at a 10 year old stagnant growth “startup” and we got at most 10 applications per day to these engineering positions. This was 2022-2024 so not just the good years. reply hobs 16 hours agorootparentprevIt's not a myth (I wrote ATS systems, and you usually just pay some third party to do the core parsing job as a library or a service and take your failures on the nose) but at the same time most recruiters are NOT going into past resumes and filtering through them, and most of the wide shot resume hits you get on linkedin are for the lowest tier of jobs. Almost all recruiters are \"most recent resume in\" type of folks or \"curated long term list\" type of folks - they'll either circle you for years trying to get you hired for something or they'll forget your name in 25 minutes. reply kmmlng 6 hours agorootparentprevI've been applying to jobs for the last three months and many companies use systems that automatically parse your resume and then let you manually correct the parsed data. It's nearly always a mess, and I use a very straightforward linear resume template. It's possible that there is some selection bias here, where mediocre parsing systems give you the option to manually correct everything because they know they are mediocre. I remain skeptical. reply polishdude20 13 hours agorootparentprevI make my resume in Figma, would that break it? reply nlitened 11 hours agorootparentWould you yourself be able to write a Figma-generated PDF parser that reliably extracts necessary semantic content from rasterized text with random layout? If no, likely an average programmer that wrote the application tracking system wouldn't be able to do that either. If yes, then you are very good, and top companies are hunting for you already, you don't need a Figma resume. reply calmdown13 20 hours agoparentprevWhen applying for jobs via LinkedIn it’s very important to use a PDF. A huge number of people submit Word documents, however, LinkedIn doesn’t render them in the browser. Given that most roles get hundreds of applications, unless someone’s previous roles really catch my eye, I am probably not going to download anything. reply scarface_74 18 hours agoparentprevFrom my experience, if you are randomly applying via an ATS without a prior outreach , you’ve already lost. Especially for remote positions, there are hundreds of people applying for every open req and it’s hard to stand out reply bn-l 6 hours agorootparentHow do you do the prior outreach without coming across as spammy though ? reply scarface_74 4 hours agorootparentThat I don’t have a general answer for. Except for the year before last, it’s been responding to recruiters that reached out to me. The year before last when I did do targeted outreach, it was because I both had relevant experience in a niche AWS service and I was a major contributor to an open source official “AWS Solution” while I was at AWS that was popular in that niche. But that trick had a very small window. So I guess the answer is do something that allows you to stand out from the crowd and then you can do prior outreach and tell them why they should hire you over someone else. My biggest piece of advice is don’t be a “ticket taker”. Volunteer to lead larger initiatives. reply thomasfromcdnjs 19 hours agoparentprevI'd recommend before others go too far down the ATS rabbit hole to checkout the wiki from /r/EngineeringResumes https://www.reddit.com/r/EngineeringResumes/wiki/index/#wiki... (In short, they might not be as important as you think they are) reply Aurornis 18 hours agorootparentAnd please do not pay anyone for \"ATS compatible resume templates\" or other ATS related services. I don't think people realize how many of the ATS myths have been promulgated by people trying to sell services to job seekers. The ATS myth resonates with people for some reason, so desperate job seekers will often pay (unnecessarily) for various \"ATS friendly\" templates or ATS reviews. reply mjevans 20 hours agoparentprevCan you elaborate on the exact layout syntax these ML idiot savant agents want to read? Lack of an industry wide standard data interface makes this terrible. reply belinder 19 hours agorootparentTake your resume, select all, copy, paste into notepad. Does it still look how you expect? Then youre ok. Otherwise, fix it until it does. That's it. That's all you need to do. reply jkaplowitz 5 hours agorootparentEven that can fail. I’ve had multiple cases now where my name (!) was auto-parsed wrong in a way human would never do, not even after copy-pasting into Notepad. The reason? My resume lists my name as follows, where I means the initial for my middle name: Firstname I. (Nickname) Lastname And yes, even professionally I do use my nickname and my last name, except for things which must match my government ID, such as offer letters and payroll/tax records, where of course I omit Nickname and use the legal Firstname as well as sometimes the middle initial or full middle name. With this format, how does Personio parse my name? It thinks I’m called Firstname Nickname. No human would make this mistake, nor would a copy-paste into Notepad cause a human to do that. And if it has any LLM intelligence at all, it should know that this is unlikely, because Nickname is actually a very common nickname for Firstname, so it should suspect a disperse and have a human double-check its conclusion. Alas. I’ve also had other issues with these systems misparsing my employment history, since they don’t always properly parse jobs that span corporate acquisitions (changing title and employer at that point but being the same job) and are accurately reflected as such on the resume. reply nuancebydefault 30 minutes agorootparentI think mashing up of names happens a lot, since they are not common words. Personally I don't see what the problem is of your name being mangled in a parsed resume, other than when it would look offensive. The name mangling can get easily cleared up after first contact with a real person. reply gruez 3 hours agorootparentprev>Even that can fail. I’ve had multiple cases now where my name (!) was auto-parsed wrong in a way human would never do, not even after copy-pasting into Notepad. [...] sounds like they did something like: first_name, last_name = name.split(\" \") which is an issue, but unrelated to what everyone else is talking about, which seems to be how text data is being parsed/encoded inside pdfs. Pasting into notepad would check for that issue, but obviously wouldn't do anything for bad first name/last name extraction logic. reply jkaplowitz 2 hours agorootparentI felt that the general topic was \"software failing to do the right thing parsing the text in PDF resumes in ways a human would get right\", which includes all of these types of problems, not specific to encoding issues. But, sure. reply kccqzy 7 hours agorootparentprevThis depends on the PDF viewer. I recommend trying a couple different ones (at least Acrobat and macOS Preview). reply kjellsbells 20 hours agorootparentprevI can only speak for what worked for me. A shorthand heuristic might be that if the resume can be read in linear fashion by a screenreader without any weirdness or non sequiturs, it's probably pretty good (another argument in favor of paying attention to accessibility!) In my case, I had a simple layout with sections clearly delineated and very simple formatting (bulleted lists). Dates were spelled out eg September 2024 rather than 9/24. UTF-8 throughout. No difficult latex packages, just classic ones like enumitem and fancyhdr. reply paxys 20 hours agorootparentprevWord/Google Docs -> PDF conversion is perfectly fine, and every system will understand it. Just don't try to get fancy with layouts and stick to headings, subheadings and bullets. People reading the resume will appreciate this as well. reply jareds 18 hours agorootparentThis is how I formatted my resume using Pandoc to generate a word document from a Markdown file. Having everything in Markdown also made it easy to quickly create customized resumes by reordering specific sections or lists in a section as well as tracking changes in Git. reply chiph 19 hours agorootparentprevThere is an industry standard - HR-XML[0] (disclaimer: I was on the committee for a little while). But I would be surprised if any of the job boards let you import it directly. Frankly, plain text rendered to PDF is probably going to be the most easily parsed by their systems. If they let you add attachments separate from your resume, then stick your \"beautified\" resume there. [0] For the curious: https://www.hropenstandards.org/ reply willquack 20 hours agorootparentprevGet a boring LaTeX template like \"Jake's Resume\" I've spoken with two technical recruiters who say they prefer reading templates instead of hand-crafted Resumes on top of them also parsing better in the ATS system ): reply kleiba 20 hours agorootparentCan confirm a similar attitude from an HR person - they prefer a single-column, easy to scan resume that follows a tried and trusted standard structure and layout. reply duxup 20 hours agoparentprevI wish I could just send JSON… Please see the job history array. reply gruez 2 hours agorootparentWhat happens when companies come up with competing json schemas and you need to massage your data into 15 different schemas? https://xkcd.com/927/ reply nicoburns 19 hours agoparentprevThe other approach is to apply to smaller companies that will actually have human read your CV. Depends on where you want to work of course. reply dymk 19 hours agorootparentThey don’t read your resume either, it goes into Greenhouse or some other ATS. Either gotta work with the parser or have a personal referral. reply b8 15 hours agoparentprevI've used LaTeX, online resume generators and Google Docs templates, but they were unfruitful. I've applied to 350 jobs and only got one screening recruiter call (then was ghosted). Despite having 2+ YoE and published research as well as having worked as a contractor for FAANG (I even made one loads of money in stock after they applied my recommendations)! reply nunobrito 10 hours agorootparentIf you've applied 350 times with that kind of success, then it might time to revise your approach, your CV or both. I've never found good leads with cold approaches. Even when getting contacted and receiving an offer, it was always low salaries on that method. What worked best was going to related events, talking to people hosting the booths, talk to presenters of topics where I'm an expert and this way get warm introductions. reply b8 3 hours agorootparentYeah, it's probably because I took a non-tech job (stock broker) for the health insurance. Also I went back to college after I quit my last FT cybersecurity job. I also used ApplyAll which may of messed up parsing my resume in Workday. reply nunobrito 2 hours agorootparentIf you're into tech development nowadays, it goes a long to list a portfolio of things you've built. That goes a long way to tell an employer if you have the skills to build what they need. reply jrockway 19 hours agoparentprevYeah, I just have a text file and convert it to HTML for people that want to view it on the web. Has never been a problem. I'm not a graphic designer and if that's a skill you want, you got the wrong girl. I have done two different types of resumes; long and sort. When I was learning about resumes in high school and college, they said to just list your jobs and maybe some key skills. I have been told this is useless to recruiters. So I rewrote things to have a couple paragraphs about major projects at this jobs. I have been told this is too long and nobody has time to read it. So now I have both and you can pick the one you want. I got laid off recently so this is fresh in my mind, but I got a job through my network instead which did not involve a resume or interviews. That's really how it should be. (I'm kind of just waiting for the founders from my last startup to start something new... they didn't survive the reorg either, which was a \"sort by salary descending and only keep the last 3\". The joys of having your software startup bought by an indecisive large company that doesn't do software ;) reply guenthert 7 hours agorootparent> I got laid off recently so this is fresh in my mind, but I got a job through my network instead which did not involve a resume or interviews. That's really how it should be. Not sure about the 'should', but I think it is. Over the years I had the pleasure to work with quite a few good people, but there were also some buffoons (not counting those in marketing). I can only imagine, that the resume of the latter will look more appealing. reply Taylor_OD 20 hours agoparentprev+1 here. I have two resumes. One looks nice/modern and its what I send recruiters/managers once I have an interview scheduled. One is an ugly, to me, plain looking word doc that application tracking systems can gronk. reply JumpCrisscross 19 hours agoparentprev> much beloved of Word template authors, where you might have a left column with your contact details, github, and maybe some skills and then a borderless table on the right side with your positioning statement and job history The design always struck me as a clumsy attempt to take up space. reply Etheryte 19 hours agorootparentFor me, a similar approach is actually the exact opposite. It's always a struggle to fit a good summary of your professional life on a single page and being able to put some short stuff in multiple columns can help you save space. For example I usually have certifications, trainings, etc in two columns since they're often fairly short entries. reply pdimitar 20 hours agoparentprevI wonder if there's an open(-source) ATS system out there against which we might test such simpler templates? reply Dansvidania 20 hours agorootparentI would try with https://www.open-resume.com/resume-parser they offer the option to parse your cv and see what \"comes out on the other side\" I am not sure whether the idea of the parser is to be a starting point to then use the editor, or a test. reply __turbobrew__ 19 hours agorootparentOpenresume really butchers mine despite using a pretty standard latex template. I wonder if my resume is really that bad? reply bangaladore 19 hours agorootparentIt parses \"Jake's Resume\" well https://www.overleaf.com/latex/templates/jakes-resume/syzfjb... reply doix 12 hours agorootparentI wonder what the origin of that template is, because it looks almost identical to mine, and I yoinked my template from somewhere back in 2012-2013. It says it's based on sb2nov/resume [0], but that only goes back to 2017. They must've based it on something as well. [0] https://github.com/sb2nov/resume/ reply Aurornis 18 hours agorootparentprevIt would be useless to test against a system that companies weren't using. I wouldn't count on anything open source in the HR space, to be honest. I remember seeing a list of companies you could \"apply\" to with your resume that would then show the ATS-parsed version back to you. Every single person who used it got a reasonable result back, which is usually enough to put an end to all of the ATS myths out there. reply pdimitar 18 hours agorootparentYep, you are quite right. Curious about that list of companies, if you are keeping it? reply zblevins 19 hours agoparentprevThanks for posting this! reply duxup 21 hours agoprevThe biggest pain / fear related to layoffs for me isn't the immediate actual loss of income... It was that I have to go job hunting and how demoralizing and toil heavy that process is. Heck I'd likely go job hunting just out of curiosity, the idea of exploring other options should be interesting at the least, but naw it's too much of a pain. >Recruiters. Don’t discount or blow them off. That's all they do for me ... I suspect there's a subset of people who are very attractive to recruiters and they actually do things for those people and I am not in that group. The advice surrounding recruiters is always so disconnected from my experience that it seems strange. reply throwaway2037 12 hours agoparent> Heck I'd likely go job hunting just out of curiosity, the idea of exploring other options should be interesting at the least When I grew up, my father worked in commercial banking in the 1980s and 1990s. There were so many bankruptcies / mergers / financial crises, that he got (painfully) used to being laid off. Watching him go through this had a large impact on my view of my relationship with most employers (hostile, wary, defensive). He once said to me, \"At my level (middle manager), as soon as I start my new job, I start looking for my next job.\" He was exaggerating, but the point stayed with me. I definitely agree with you: Looking for a job (or \"keeping your doors & windows open to new opportunities\") while having a job is much, much easier -- mentally. In my industry, most connections with head hunters are made through LinkedIn. You can set a special flag in your profile that says \"I'm looking for work\", but this is only visible to professional head hunters (they pay a lot of money for an account with these special privileges). It works very well. Normally, the calls start with: \"Them: Are you looking at the moment? Me: No, but I am open to new and exciting opportunities. Them: Oh, great. I have something for you.\" Do that enough, and eventually something very good lands on your doorstep. reply scarface_74 10 hours agorootparent> At my level (middle manager), as soon as I start my new job, I start looking for my next job.\" He was exaggerating, but the point stayed with me. That’s no exaggeration at all. I’m not always looking for a job. But everything I do I do with one eye toward how will this look when I get ready to interview? Am I working on tech that is demand? Am I working at the correct “scope, impact, and ambiguity” or am I just being a “ticket taker”? reply Salgat 20 hours agoparentprevThe leetcode grind is my biggest dread. I can do it and I do well, but I'll be damned if it isn't a lot of mindless rote memorization that immediately leaves my brain the second I land the job, never to be used again (even in my job) until the next time I apply around. reply duderific 18 hours agorootparentAt least you can do it well, imagine the grind when you can't. reply le-mark 18 hours agorootparentRemember when fizzbuzz was the gold standard in trick interview questions? And Joel was a revolutionary for advocating it? Things have certainly changed. reply throwaway2037 12 hours agorootparentIn some sense, leetcode is fizzbuzz on steriods. As I recall, the \"trick\" to fizzbuzz is knowing about the modulo operator. Most leetcode problems have a similar trick that you need to know/learn to complete them correctly. reply dagw 8 hours agorootparentFizzbuzz wasn't about seeing if a candidate knew the 'tricks', it was to test if you knew how to code at all. If you know about loops and if statements you can do fizzbuzz even you've never heard about modulo operators. Fizzbuzz was designed to catch the complete bullshitters who literally could not code. Apparently that was a real problem people were having at the time. reply pavel_lishin 2 hours agorootparentAt the time? It's still happening today! reply satvikpendem 13 hours agorootparentprevLots of jobs don't require Leetcode. I haven't done it in years yet I still get jobs consistently. reply rirze 3 hours agorootparentToo many folks in this industry think any job other than FAANG/startups is not serious. reply eschneider 18 hours agorootparentprevLook at this way: It's just something you do for money. reply diamondfist25 20 hours agorootparentprevI wonder what’s the value of even doing leetcode when AI can solve it in 10 seconds… reply nickff 20 hours agorootparentIt’s not about testing ability to solve those problems, it’s about testing conformity, determination, and IQ (while being job-specific and thus legal). reply rirze 3 hours agorootparentIt's about conformity. Every place that tested leetcode in my interview (easy/mediums) never impressed me with their work culture. Usually it's a proxy test for obedience and/or accepting bureaucracy. Also forget about career advancement. (Why would we promote you when we can replace you with another code puzzle solver?) reply tptacek 16 hours agorootparentprevIQ tests aren't illegal. Several big companies overtly use them. reply nickff 14 hours agorootparentAFAIK Hasn’t been legal unless the test is closely related to the job since Griggs v. Duke Power Company, a 1971 SCOTUS case. reply tptacek 13 hours agorootparentYou can just go look this up with IQ test vendors; you don't have to derive it axiomatically. They proudly list their (large) clients. reply disgruntledphd2 12 hours agorootparentYou need to ensure that the test doesn't discriminate on the basis of a protected characteristic though. reply tptacek 12 hours agorootparentThese are just completely bog-standard cognitive aptitude tests; in fact, they are if anything less rigorous than a modern IQ test. They're generally not specialized to specific jobs. You can read Reddit threads about people taking (and studying for) them. Don't get me wrong: I think this is an incredibly dumb practice. But there's a mythology that IQ testing is a super-effective tool for recruiting that has been suppressed by anti-discrimination law. That is not the case. Most companies don't use IQ tests, because they're not fit for purpose. reply disgruntledphd2 11 hours agorootparentIt really really depends. If you're gonna train junior people then IQ tests are great, otherwise work sample tests are better. It definitely is true that IQ tests are used less in the US because of prior case law though. reply dboreham 12 hours agorootparentprevYou mean they're selecting for lower IQ? (higher candidates will avoid the leetcode jobs) reply saagarjha 8 hours agorootparentprevWhile the actual implementation isn't particularly good having a decent understanding of algorithms is useful in a lot of contexts. reply zamalek 20 hours agorootparentprevThe highly specific/specialized problems leetcode has presented have never held value, outside of a very small set of companies (i.e. MOFAANG). reply saagarjha 8 hours agorootparentWhat's the O in the acronym? reply zamalek 2 hours agorootparentOracle reply gedy 19 hours agorootparentprevHonestly I just stopped interviewing at leetcode places and those that expect you to \"prepare\" for their interview, especially those that are dumb no-name SaaS companies. It's less about me being stubborn as it is those places are hiring based on the wrong skills, and those end up not being good places to work. reply anarticle 19 hours agorootparentprevSimply don't, lean on your network if you can. It ends up bottlenecking your search anyway. reply scarface_74 14 hours agorootparentExcept at smaller companies, your network just gets you a referral and interview. You still have to go through the same interview process. The two exceptions I have had were when a former coworker who is now the director of an f500 non tech company was going to create a strategic position for me. He needed someone he could trust. The other time was when a CTO and the director of application development were both former coworkers from another company and it was just a matter of me saying yes. The first job would have been more stress than I was willing to deal with and I don’t do large companies (I was suffering from PTSD from my time at AWS). The second didn’t have the budget to meet my compensation target. reply Izkata 18 hours agoparentprevI have 3 rock-bottom criteria for responding to a recruiter: 1) They use the correct email address, the one I used on my resume and to apply for jobs, and didn't dig out my personal one somewhere. 2) They don't say something stupid that reveals they didn't even look at my resume. (\"I see you have C# experience\", uh no I don't) 3) They include anything at all that's supposed to interest me, even if it actually doesn't. The vast majority fail at step 1. I've only ever had one email from a recruiter that passed all these criteria. reply mvdtnz 11 hours agorootparentYou're saying that the \"vast majority\" of recruiters receive your CV (from wherever) and instead of contacting you on the email address on your CV they will Google around for your personal email address? I don't believe you. Why would anyone do that extra step when they already have your email address? reply wil421 4 hours agorootparentI’ve had recruiters find my work email after a phone call. They proceeded to send a bunch of emails while I was screen sharing. I was able to hide it but it was almost a disaster. reply bradlys 4 hours agorootparentprevNext to no recruiters out there are doing things by hand. Everything is automated. They’re not googling for you. reply slothtrop 4 hours agoparentprevMore than half of recruiters will ghost me after I've jumped through the hoops (phone call, CV, filling grid, security clearance forms, etc). reply luckylion 20 hours agoparentprevMight depend on the niche you're in, or your location. My experience with recruiters (on the hiring side) mirrored what I heard from friends who got hired through recruiters: they're basically match-makers. If you're not a well-known name, you can have your job-postings but you won't get any applications. Either you spend time on advertising and try to convince people that you're really real and actually really want to hire, or you just get yourself someone who introduces you to people who might be a good fit. That's a recruiter. reply scarface_74 19 hours agorootparentOut of the 10 jobs I have had since 1996, 6 came from external recruiters. I met five of them in person over lunch or in their office when I was looking for local jobs in Atlanta until 2022. The one I didn’t meet in person was a specialized recruiter for my niche. Two came from me reaching out to them and two were from internal recruiters reply throwway120385 18 hours agorootparentYeah -- the best recruiters have been \"let's meet in person and chat about what you're looking for\" people who would send me jobs when I was looking. The worst ones open the conversation by asking for my social over the phone. reply Aeolun 19 hours agoprev> I really wish companies would start giving honest feedback even if it’s hard for the candidates to heard at first. It would be a much better way for candidates to improve themselves and we’re all adults here and can take the feedback. I’ve said it before, and I say it again. This isn’t true. When companies try to be helpful and give you well meaning feedback, you find out that their reasons for rejecting you are absolutely banal, and you’d have been better off not hearing anything. reply Aurornis 18 hours agoparentWhen I first become a hiring manager I thought I'd be the exception and provide everyone with detailed and honest feedback. I didn't last very long. Candidates would see the feedback as an invitation to prove me wrong or argue with my assessment. I got a few very angry e-mails from people who took their rejection very personally and made it clear that I was their enemy. One person (who was actually very unqualified) even went on a mini rampage across the internet, trying to \"name and shame\" my company and even my personally for the rejection. There were even threats of a discrimination lawsuit. So I stopped. It's back to something like \"We've decided to proceed with other candidates\" reply vrosas 18 hours agorootparentI once interviewed for a job where they admitted I was the only person in the pipeline. I was then rejected with a canned \"This was a highly competitive process and we've decided to proceed with other candidates\" email. Did not feel great. reply throwaway2037 12 hours agorootparentDo you think you would feel even worse if they explained, in detail, why you were rejected? I guess that you would feel even worse. reply guenthert 7 hours agorootparentThat surely depends on things the company knows and sadly also on things the company doesn't know. Are the reasons things that the applicant can change (e.g. lack of relevant experience or poor presentation skill?) or things they can't change (too old, wrong gender), but also how sensitized is the applicant to rejection (due to recent experiences)? reply A1kmm 10 hours agorootparentprevIt is better to ask your internal recruiter / HR department to inform the candidate of your feedback (if you work for a big enough company). It is also good practice to always have a panel, not just the hiring manager, doing interviews. So the candidate gets feedback along the lines of: \"Thank you for participating in our interview process. Unfortunately, our panel decided you weren't the best fit for position X at this time, because ...reasons.... Under company policy, we won't accept further applications from you for one year from today, but we would encourage you to apply for a role with us in the future\". There is a chance they will reply back to HR arguing, but it is their job to be polite but firm that the decision is already made, and that they can apply again in one year (and not pass anything back to the hiring manager). The key is to think long term and about the company as a whole - the candidate who gets helpful feedback and is treated fairly is more likely to apply again in the future (after the mandatory cooling off period), when they might have more skills and experience working somewhere else. There is a finite qualified labour pool no matter where you are based, and having the good will even of rejected candidates is a competitive advantage. The message should be \"not now\", rather than \"not ever\" (although of course, if they do go on some kind of rampage, they could turn the not now into not ever - that's a bridge burning move). If a tiny percentage go on a rampage, but the company protects the individuals from it, and has lots of counteracting positive sentiment from prospective and actual staff, then it's still a net positive. reply throwaway2037 12 hours agorootparentprevMany (larger) companies have a strict (defensive) policy against providing honest feedback for exactly these reasons. reply randycupertino 13 hours agorootparentprevThis happened to me when I had to reject a doctor from my research study due to his site manager being unprofessional and his site being an audit risk. I sent a brief professional two-sentence vague rejection note thanking him for his time and wishing the best in his future research and he responded with an absolutely unhinged 17-paragraph rant, threatening lawsuits, calling my vendor CRO a crazy cat lady, saying she was too old to find happiness in life, ranting about San Francisco liberals (his clinic was in NYC), threatening to sue me, threatening to sue my vendor, threatening to turn us all into the FDA for fraud and wasting his time, and on, and on. It was completely shocking to read and really opened my eyes to how viciously and crazily people can lash out when they feel rejected. I was really glad I kept my rejection brief I can't imagine what he would have latched onto and ranted about if I'd given any specifics about why we didn't select his site. The weirdest part is he came to us very highly recommended from another doc! reply horrible-hilde 1 hour agorootparentIf I gave the last candidate I interviewed candid feedback it would be “You need to bump up your technical skills (obvious during interview) and the photos on your photography site which you added to your resume for me to click on makes me question if you are a serial killer.” Instead I sent something akin to “we decided on another candidate” reply anal_reactor 7 hours agorootparentprev> The weirdest part is he came to us very highly recommended from another doc! Some people are fantastic on the happy path but horrible on the sad path. reply test1235 8 hours agorootparentprev>>>we’re all adults here and can take the feedback yeah - I don't know why OP makes that sorta assumption. I'd expect angry replies with no benefits to the time wasted on my part. reply vismit2000 13 hours agorootparentprevThe online world has indeed become more hostile over time. reply nlitened 11 hours agorootparentIt's not about online world, I think. When I was younger, I used to call each candidate with interview results — I've heard some of the rudest words directed at me ever. Stopped forever after a few rude calls. reply mixmastamyk 16 hours agorootparentprevHere's another issue, the interviewer is sometimes wrong. Or, there was a miscommunication. At least twice in my career I missed an offer because the interviewer didn't think I had experience in something, but turns out I had decades of it. But, I don't brag. Heard thru the grapevine, \"oh they said you didn't have database experience,\" hears me using databases since the late 80s. WTF? So I could imagine debating a point while being in the right. reply tracerbulletx 19 hours agoparentprevGoing the other direction.. most of the time I've seen feedback given to a candidate, they freaked out about it and got defensive. Not worth the risk. reply petesergeant 11 hours agorootparentAnd even in the case where the candidate is right to push back on the feedback, the process has usually moved on with another candidate already, so it's just awkward for everyone. reply dilyevsky 19 hours agoparentprevIt’s banal because nobody is going to share the truth for fear of retaliation unless they’ve never done this before reply cjohnson318 13 hours agoparentprevTrue that. I just got some \"honest feedback\" that I didn't agree with at all. I'm all for criticizing myself, but this feedback totally missed the mark. I didn't have deep enough experience with \"object oriented programming\". What does that even mean? I've been creating and using objects every day since 2007. I literally... don't know what to do with that feedback. It's like the carpenter interview, \"Have you ever built brown houses? Our client is very interested in brown houses.\" reply higeorge13 11 hours agoparentprevBeing from both sides, I would say it depends. But especially after a take home assignment, feedback should be given 100% and face to face. It's almost embarrassing to spend X hours/days of your (spare) time only to be rejected with an email and usually some random reasons, without having the chance to explain why you built this toy project that way, how much time you spent and what would you do with more time or in production or even showcase some live coding on top of what you did. But everything has become really inhumane, no-one cares. That's why AI is dominating and ruining the field. reply el_benhameen 15 hours agoparentprevGot rejected after a take home, and the feedback from the recruiter was 1. a reviewer thought I used a library incorrectly (they actually misunderstood how the library worked) and 2. I didn’t implement something that the instructions had explicitly said not to implement. I didn’t argue with 1 because I figured it was pointless and it sounded like the recruiter was having a rough day. I politely pointed out 2 as a courtesy and he said “huh, the other guy yelled at me about that”. Although honestly, I left the process thinking “lol” instead of “I’m a dumbass” like I might have absent the feedback. reply defrost 15 hours agorootparent> and he said “huh, the other guy yelled at me about that”. If this was SASR recruitment that'd be the pysch portion of the test, reject a candidate for a reason that makes no sense and watch their reaction ... The assessment never stops, everything's a meta test, and they push until you quit the recruitment merry go round. In the relatively normal world of software engineering .. that's a recruiter landed with a poor testing pachage and procedures. reply sfriskzo2005 19 hours agoparentprevAnother issue is that positions which are advertised are often already filled by some acquaintance, but we need to go thru the motions of posting the job, sifting thru resumes, interview questions, leetcodes, etc. and interview theatre before we go and hire the acquaintance. Sometimes, HR doesnt know and the interview panel may not know. reply slothtrop 4 hours agoparentprev> their reasons for rejecting you are absolutely banal, and you’d have been better off not hearing anything. If it's banal, I'd like to know. Otherwise you're still left wondering whether it's anything that hinders you in the longrun. reply potato3732842 17 hours agoparentprevIf it's not BS it's stuff that should have been obvious from the interview. They were looking for a network engineer who could program a little and you're a programmer who knows a little about networks or something like that. reply scarface_74 17 hours agorootparentI have a spreadsheet of my job hunts since 2008 across 8 times I was looking for a job. I was working at my first two jobs between 1996-2008 I’ve been rejected three times once I started the interview process. I have also always gotten interviews from companies where external or internal recruiters reached out to me and I submitted my resume. All three times I’ve gotten rejected was post mid 2023. It was clear from one within 10 minutes that I wasn’t what they were looking for. I’m not sure why I didn’t get hired for the second one after going through the rounds even though I have my suspicions. The third I got ghosted after the HR screen where the representative from the target company’s investor interviewed me. reply mixmastamyk 16 hours agorootparentOnly three interview misses in 15 years? reply scarface_74 15 hours agorootparentA little background for context: https://news.ycombinator.com/item?id=42622893 So by 2008, I was 34, starting over and applying for enterprise CRUD jobs where I was competing against people with a lot less experience and maturity. I was cheap labor. That held true in 2008 and 2012 and I was digging my way out of the “expert beginner” years. It wasn’t until 2014 when I was actually starting to be recruited for strategic positions. In 2014, 2016 and 2018 I was an early strategic hire by a then new director/manager/CTO to lead major initiatives. My interviews were two adults talking about strategy. By 2014, I was 40 years old. 2020, a remote position at AWS Professional Services fell into my lap. I was 46 then. https://news.ycombinator.com/item?id=38474212 Now looking for a job in 2023 and 2024 targeting full time roles at cloud consulting companies, can you imagine how easy it is for a 50 year old who knows how to communicate decently, with 25+ years of development experience, 6 years of AWS experience including 3 at AWS to get a job at those companies? Now when you take AWS away from me, I am just another enterprise Dev with above average communication and project management skills. Two of my three rejections were at product companies even then the interviews were behavioral. It does concern me a little that I got rejected from product companies that were looking for “architects”. My title right now is “staff software architect” at a consulting company. reply Ologn 12 hours agoparentprevWith regards to what they're saying, the first thing to do is reverse the perspective. The applicant is hearing a yes or no, so if it's a no they want to know what they did wrong to improve themselves. From the interviewer's perspective - we get someone who is average, then another person who is average, then someone who has trouble with basic questions, then we get this person who may be as average as the first two, then we get someone who answers every question correctly, and has a deep knowledge of the domain if you drill down, then you get an average person again. There's nothing really wrong with the person, they did as well as four other people. It's just that someone else came in who was a standard deviation above the majority of the people in the bell in the normally distributed Gaussian curve. reply keeptrying 12 hours agoprevIf you code, you should be building a business on the side. Especially if you are early in your career and its becoming harder to land a stable job. you need to learn how to market your skills, get clients and deliver. No more excuses to sit in a job and do nothing. This is going to be critical to survive. Eventually I think companies are going to be much smaller entities than they once were which means your have to really buid your own biz. reply scarface_74 11 hours agoparentYes because it’s harder to land a stable job, it’s much more realistic to start a business that is going to convince enough people to pay you enough to support yourself. Also you have to convince companies to do business with you instead of a well known company. Oh and to be competitive you need to have some type of funding. And you need to make enough to pay for health care. reply coldtea 10 hours agorootparent>Yes because it’s harder to land a stable job, it’s much more realistic to start a business In a slowed down market, both get difficult, the latter many times more than the first. reply actionfromafar 10 hours agorootparentIn general yes, but there are always extra opportunities to start a business when times are down, because some things will be more in demand then. reply scarface_74 10 hours agorootparentAnd you are also competing with everyone else thinking the same thing. reply Mc91 4 hours agorootparentprev> it’s much more realistic to start a business that is going to convince enough people to pay you enough to support yourself. I only have to convince one place - Google Ads. Plus bring in the \"eyeballs\" with my free app, but I have accomplished that more than once. > Also you have to convince companies to do business with you instead of a well known company. Just one company in my case (actually several, but 90+% of the money comes from Google) > Oh and to be competitive you need to have some type of funding. I have to be competitive enough to make a few thousand a month, and with my programming (and database design, and UX, and SRE etc.) skills, I have achieved that. > And you need to make enough to pay for health care. In the US you do. reply scarface_74 3 hours agorootparenthttps://techcrunch.com/2024/03/12/most-subscription-mobile-a... > the top 5% of apps generate 200 times the revenue of the bottom quartile after their first year, while the median monthly revenue an app generates after 12 months is less than $50 USD. https://medium.com/beyond-agile-leadership/the-difference-be.... >Success rate: According to Zippia, only 0.5% of mobile apps are successful, with 9,999 out of 10,000 apps failing. Fyresite estimates that 99.5% of consumer apps and 87% of business apps fail. Now imagine what would happen if more people took that advice? For context, a new grad working in a major city in the US not on the west coast - even an ordinary CRUD enterprise framework developer - can make $70k- $80K a year. What exactly is a “few thousand a month”? That’s a good side hustle. But even that’s not enough to support yourself reply keeptrying 11 hours agoparentprevthe most importat skills to learn: 1. Sales = learn to knock on a 100 doors to get one sale. 2. Marketing = learn to communicate your value prop to specific companeis and hiring managers. 3. Delviery = learn to deliver products/projects end to end with all the management that goes in between. If you know all 3 you'll never be in a position where you don't have income. It takes about 4 years to really get a grasp of all 3 so start now ... don't just do a 8 hour job and go home and watch tv. Keep trying to sell your services to others ... at least 2-3 hours after work to other companies/startups/other industries. Its a big world - smeone needs your services - your survival depends on finding those people nad packaging your skilsl so that they buy. reply rwyinuse 11 hours agorootparentGood advice if you don't have a family. Utterly impossible for someone with small kids and a wife who is also working. reply tonyedgecombe 10 hours agorootparentHaving a wife who is working takes some of the financial pressure off. My children were four and eight when I started my business and it wasn't a problem at all. In fact I would say it made many things easier because I had a more flexible schedule. reply benhurmarcel 9 hours agorootparentThis thread is about doing this on the side, while still keeping a full time job. reply travelthrowaway 11 hours agorootparentprevtime-management and sleep deprivation, should write a book with that title 0600 - 0900 Your project 0900 - 1600 Corporate job 1700 - 2100 Family & free time (you need to learn how to make the most of it so that you can recharge as well) 2100 - 2300 The low-CPU low demanding part of managing a business paired with some interesting background content goes here reply mr_mitm 10 hours agorootparentA certain amount of good quality sleep is not optional and sleep deprivation is not sustainable. reply robertlagrant 9 hours agorootparent7 hours isn't sleep deprivation. Sleep deprivation is..no sleep. Or incredibly little sleep. reply latexr 3 hours agorootparentConsidering the schedule has you stopping one task and one time and starting another exactly seven hours later, that’s not seven hours of sleep. Most people don’t stop a task and immediately fall asleep, or wake up and can start working. That gap means less than seven hours of sleep. And for many people seven hours isn’t enough. Sleep deprivation means not having enough quality sleep, it’s not a single number which can be applied to everyone all the time equally. reply robertlagrant 2 hours agorootparentIt's a rough schedule. It's not work to the minute of 11:00pm then start unwinding. You might be pretty unwound by 10:30 and then answer a couple of emails before making a hot drink at 10:45 and calling it a night. It seems a bit silly to overly argue how that can't possibly describe a full night's sleep, when it clearly can. reply latexr 2 hours agorootparent> It's a rough schedule. I agree, it is pretty rough, I wouldn’t wish it on anyone and don’t think anyone should follow it if they intend to live a full happy life.¹ ¹ I know what you meant, I’m making a joke. reply pavel_lishin 2 hours agorootparentprevThat's like saying a 500 calorie diet isn't starvation, because starvation is literally no food or incredibly little food. reply creativenolo 11 hours agorootparentprev4 hours in the evening, nicely compartmentalised. If only. Where are the wake ups, the pickups & drops, the tidying, the admin, etc. reply jodleif 10 hours agorootparentYeah it’s pretty simple. Just teach your kids to walk and the importance of no sleep interruptions, then they can get themselves to kindergarten and make their own food. /s reply nunobrito 10 hours agorootparentprevSome people will criticize but I follow a similar time management schedule. It was only when I've built my first startup that many of those skills had to be learned and to be quite frank, I only wish to have started earlier rather than being throw into for making enough money. Nowadays that schedule is accurate and I'm a family person with two young kids in the house. I just don't wake so early but that is mostly because I keep working on my personal projects until midnight~1 AM. The good thing is that I'm no longer so attached with my employer company. Mismanagement, demotion or those issues don't affect me so personally because my professional value is no longer just defined by the company where I work. My work outside company hours is valued by many others, albeit not profit-driven it serves as a good backup whenever falling into unemployment situations. reply scarface_74 10 hours agorootparentMy “backup” is not working 40 hours a week, then working on a side project and sacrificing time with my family and friends, not having time to exercise and travel and just relax. My backup is an always up to date resume with an up to date skill set, and a longer career document, a years worth of expenses in a HYSA in addition to retirement savings, low fixed expenses, and a decent network. I figure in a year someone , somewhere will give me a job or contract. While my wife hasn’t had to work since 2020 at 44 when I was 46, she has kept her CDL so if push comes to shove, she can get a job with the school system as a bus driver for the benefits while I build up an independent consulting clientele. I found a job quickly both in 2023 and last year. reply jajko 9 hours agorootparentprevYou probably don't have kids. Or if you do, that's really low amount of time and overall pretty horrible long term life quality setup, I guess then wife picks up most of family chores. Maybe your career is stellar, added value as a parent and (not only) emotional anchor for your kids... not so much. Where is commute? Corporate jobs in IT are not 8 hours sharp and ciao, there is also lunch break to count in. Also 23-6 means 7 hours of sleep, too little for many. And so on. reply InitialLastName 1 hour agorootparent> Corporate jobs in IT are not 8 hours sharp and ciao He doesn't even have 8 hours there for the corporate job. reply coldtea 10 hours agorootparentprev>2100 - 2300 Yeah, your kids are gonna love that quality 21:00-23:00 time... reply ropejumper 10 hours agorootparentThe formatting is off, family time is 17:00-21:00 in GP's comment. reply actionfromafar 10 hours agorootparentprevI think it's meant to be Family 17-21 reply scarface_74 11 hours agorootparentprevYes is just that easy. You don’t actually have to have anything to sell. If that’s all it takes, why do only 1 in 10 startups succeed and that isn’t even counting all of the people who are struggling in obscurity. reply coldtea 10 hours agorootparent...they didn't learn to sell well enough /s reply ffsm8 11 hours agorootparentprevThis will not work if the status quo actually changes to a degree that a significant portion of developers have to do this. It'd very quickly saturate the people willing to give you a chance. This was a good advice 10-2 yrs ago, but going forward? We'll have to see, but my gut says this will become just as likely to succeed as becoming a successful influencer... By which I mean that a few will occasionally make it/succeed, but it'll only be such a low fraction of the people trying for it that it rounds to 0.0%. and the ones succeeding will generally have been able to leverage an opportunity that most trying the same never had. (Not to discourage people from trying - without an attempt you won't even have the chance to grasp such an opportunity. I'm just looking at it from the perspective of an observer) reply latexr 4 hours agorootparentprev> It takes about 4 years to really get a grasp of all 3 so start now That 4 years number sounds like an ass pull. What’s your source? Everyone is different and some of those skills come naturally to some people, so I sincerely doubt that number is even close to universal. Honestly, your whole suggestion seems straight out of one of those generic self-help scams that ignore the realities of life and always blame the user: “You gotta do the thing. If you’re not successful it’s because you didn’t want it enough, not because we’re dispelling the same dated advice to everyone”. Note I don’t think that’s what you’re doing, you’re not selling anything. I’m just saying I question the helpfulness and quality of the advice. reply edanm 5 hours agoparentprev> If you code, you should be building a business on the side. This is incredibly far from being the universal claim you say it is. 99% of software developers work for someone else, not at their own business, and most are perfectly happy. By all means, building a business is great for many reasons, sometimes including financial reasons, and you should do it if you want to. But not wanting to is not an \"excuse\" and you shouldn't feel pressured to do so. reply dzonga 9 hours agoparentprev100%. people wanna blame companies for laying people off, doing leetcode, having ATS systems that don't parse resumes correctly. if people realized - aimed at software engineers - that the same company you're applying for - someone went through the pain of creating the initial product, marketing it, selling it. I'm sure you can do the same - maybe not at the same level but at 80%. unlike in zero-sum games - real life you can have multiple winners - 80% will get you there. reply tonyedgecombe 10 hours agoparentprevI do wonder about that. If you had it in you to start a business then I doubt you would have been sitting around waiting for the next downturn to do it. reply saagarjha 8 hours agoparentprevA lot of companies will frown upon such things. reply game_the0ry 18 hours agoprev> More than 150,000 layoffs were reported in 2024 and a whopping 264,000 in 2023. I switched careers in my 30s to get into tech. It was big, difficult pivot. At the moment, I do not regret it and really like what I do. But the job market is shockingly bad. I do not have an optimistic outlook, so I am looking to pivot again, likely a small business. All the extra cash I have after expenses, I put towards various side hustles. One big upside to being a SWE is that I can make whatever app I want and put it on the internet publicly. reply 0xbadcafebee 15 hours agoparentThe job market is bad because a lot of people joined tech when there was a lot of open roles (and a lot of SV money). The money has returned to its pre-bubble level, so there's fewer jobs. But the same number of people looking for work as during the bubble when they switched careers. Most of those new workers lack deep skills, so the existing jobs are going unfilled. Most of the listings I see now are for senior and above. reply game_the0ry 4 hours agorootparentI agree with you, mostly. Another problem is management attitudes towards hiring. They are switching their capricious attention and investment towards AI [1] and off shore to save costs [2, 3]. The combination of all this is making for an ugly combo of negativity. Tech used to be a lot more fun, even before the SV ZIRP hysteria. [1] https://www.salesforceben.com/salesforce-will-hire-no-more-s... [2] https://www.turing.com/blog/top-us-companies-choosing-offsho... [3] I work at a large bank. Almost all our new roles are exclusively in India and the Philippines. reply saos 11 hours agorootparentprevAhh I wonder how bootcamps are doing these days.. reply reducesuffering 25 minutes agorootparentWell, PG's beloved Austen Allred (of Lambda School, now BloomTech), has started a different company... So, bootcamps are probably doing pretty bad reply myth_drannon 18 hours agoparentprevYou and the other million of devs think the same, that's why the indie dev market is exploding and twitter is full of influencers posting their app revenue.. reply vunderba 58 minutes agorootparentYup, I've been calling this out as the other side of the coin of LLMs for a while now. If dev skill is no longer a barrier to entry and you can spin up an application using chatGPT over the weekend, then so can literally EVERYBODY else on the face of the planet. You think it was hard competing against 10 other similar apps? Try 1000 or 10,000 competitors. reply paxys 18 hours agorootparentprevThis is so true, and I'm in fact going through this right now. One of my semi-technical friends followed an online tutorial and created a stock tracker web app, almost entirely using ChatGPT. It is a pretty good achievement (in the context of someone who isn't a programmer and started from scratch), and I'm encouraging him to keep going down this path and developing his skills. He is however convinced that he can launch this app and make millions, and is even considering quitting his job to do it full time. There is zero chance it is going to get any kind of traction, and I keep telling him that, but he is too enamored by all the \"influencers\" on LinkedIn/X telling him that he is basically a 10x engineer now. reply purple-leafy 17 hours agorootparentman, launching and leaving your job is definitely not the way to sustainably, or successfully work for yourself. I encourage others who want to work for themselves to reduce their core work hours if possible, and spend the new free time working on their projects. Otherwise you will burn out reply game_the0ry 4 hours agorootparentI think we will see a small business \"renaissance\" of sorts: - many local small business owners are aging baby boomers, they will be exiting + retiring; that's opportunity to back-fill - the culture among younger people (millennial and younger) seems to be more focused on employment over entrepreneurship; they seem to prefer the safety of stable employment over taking risk on their own (I have no data backing this, just anecdotal experience...but I think its bc of student loans) - stable employment with a big company is not so stable anymore, most of human history leans entrepreneurial, big corps are recent phenomenon I could keep going but I will stop there. reply jajko 9 hours agorootparentprevAt one point, one has to be either 'life smart' and look around at all the life lessons from other people, or end up repeating many of those. reply matheusmoreira 9 hours agoparentprevBeen programming since I was 14, pursued a career in another field so that I could program for fun. I have considered pivoting my career to software development many times. Seeing people far more qualified than me have nightmarish job hunting experiences always makes me think twice. When people with 10+ years of experience can't find a job, there's no reason to believe self-taught programmers without degrees ever will. Starting a business seems like the only real answer. Struggling only makes sense when it's for your own company. reply junto 3 hours agoprevAfter the dot-com crash, I was laid off when the company I worked for went under. They specialized in creating e-learning courses for large corporations. That experience taught me a valuable lesson: there are some industries you just don’t want to be involved with when the economy takes a downturn. Companies tighten their budgets, and the first cuts often come from areas like training and marketing. On the consumer side, people quickly drop non-essential luxuries like streaming services or food delivery. If you work in industries that provide those kinds of services, they’re essentially “fair weather industries”—great during good times but highly vulnerable during tough ones. Since then, I’ve made a point of only working in what I call “recession-proof” verticals. These include energy (avoiding risky sectors), insurance (because companies rarely skip paying premiums), and certain areas of banking (where money flows abundantly). Another critical strategy is diversifying your skill set and building a strong internal network within your company. The more indispensable you become, the more secure your position. In more technical terms, this is akin to “obligate mutualistic symbiosis”—a relationship where both parties thrive because they rely on each other. reply OptionOfT 19 hours agoprev15 years of experience. Laid off twice last year. 10+ years at 2nd last role, 3 months at last. Like the article mentions, it's an employers market. The thing I struggle is the question to why you want to work at a place. Either I'm short and to the point, or it ends up written like I used to when working for one of the Big Three. And coincidentally, that is exactly the kind of stuff that ChatGPT generates. reply eschneider 18 hours agoparent\"I have a mortgage and a family\" is fine. Those people are highly motivated. reply OptionOfT 17 hours agorootparentI'll try that and I'll let you know! reply satvikpendem 13 hours agorootparentIt will end poorly. Even if it's objectively true, employers will not want to see such a reason, so you'll have to make something up, even if it's not true. It's just the name of the game. reply lnsru 11 hours agorootparentExactly. It’s mating dance. Nobody cares about one’s family. One must talk about good products, nice working culture, positive vibes, professional development on topic X, suitable experience in topic Y, similar finished project Z, interesting personalities of the interviewers. Motherload of well crafted lies. I learned a lot to lie during interviews. As a graduate I was absolute truth teller and every interview changed me a bit. After hundreds of them during a decade I am comfortable with any lie. So sad when I think about it. reply duskwuff 15 hours agorootparentprev\"Money can be exchanged for goods and services.\" reply ChrisMarshallNY 20 hours agoprevI sincerely wish this chap luck. I suspect that he'll be OK. > Keeping good connections with your coworkers and not burning bridges is one of the most important things I think you can do in your career. Words to live by. I wish more folks internalized this phrase. reply bilbo0s 19 hours agoparentRelated. But people should also keep in mind that layoffs throughout the industry are extensive. So don't make the mistake of only keeping good connections with your team and managers. Try, to the extent that it's possible, to keep good connections across whatever enterprise you work in. We could easily enter an era, especially in tech, where all of your team and managers are unemployed at the same time you are. Or they otherwise may not have the ability to help you. You should always have a plan for using connections in the event you find yourself in that situation. A prudent pillar of that contingency would be to expand your network as far as you can. Even the facilities guy or the guy in marketing may prove useful one day. No connection is too low or too high. reply tonyedgecombe 7 hours agorootparent>No connection is too low or too high. This is so true, one of my biggest corporate deals came from a guy I worked with a decade before, he was junior at the time. reply OnionBlender 14 hours agoprevHow are people searching for programming jobs? LinkedIn is garbage because it keeps showing jobs that don't even contain the word I searched for. I go directly to the big company's career pages, but it is hard to discover new jobs or new companies. reply halfmatthalfcat 45 minutes agoparentFind a job you find interesting and have a compelling value add. Find that job’s hiring manager on LinkedIn/Other Social. DM the hiring manager and start a conversation. Has worked for me multiple times. You cut the line (or black box) and start to build rapport instantly. reply adityamwagh 14 hours agoparentprevUsing Google site search is a great way! For example - site:lever.co “Software Engineer, Machine Learning” OR “Machine Learning Engineer”. This is helped me search for a lot of jobs that weren’t advertised. You can do this similarly for greenhouse.io, ashbyhq.com, myworkdayjobs.com, breezy.hr, workable.com, dover.io, ats.rippling.com, icims.com and other popular ATS websites. reply daemin 6 hours agoparentprevIn the last 12 years of employment I've pretty much received all my jobs through the friends, friends of friends, and former colleagues network. There was one job which I actually applied for on their website and had a standard interview process for, but I had former colleagues already working there, so I include that. I am in Game Development so it is a much smaller community than the overall tech industry and you're more likely to know people in different companies as people slowly disperse after completing a project. I think as you get older you kind of have to find jobs in this way, relying on your network and reputation, rather than doing a fresh cold application each time. reply GVRV 12 hours agoparentprevI miss StackOverflow jobs – it showed that jobs were probably published there because someone technical advocated for the platform. reply begueradj 13 hours agoparentprevLinkedIn is a place to show your social status. reply imranq 20 hours agoprevI think its amazing that posts like this exist, and more should definitely be written so that people don't feel powerless after a layoff. Too often we tie our identity to institutions and it isn't doing anyone any good (well maybe it helps the shareholders). reply mooreds 16 hours agoparentThis is the one I wrote about my lessons: https://letterstoanewdeveloper.com/2020/05/04/how-to-go-thro... reply triyambakam 19 hours agoprevI also got laid off during parental leave. I had a strong feeling that it would happen but due to the stress of a medically dangerous pregnancy, I made the excuse that I didn't have the capacity to simultaneously focus on a preemptive job search. That was a bad decision. reply cryptozeus 18 hours agoprev\"How to Tell a Layoff is Coming\" another tip from personal experience, a manager and key people always know. Be in the inner circle. Also I was once laid off on my Hawaii vacation, talk about perfect timing. reply saagarjha 8 hours agoparentNot true. Often managers are caught unawares of layoffs until they are executed. reply PHGamer 17 hours agoparentprevi knew someone who was QA his boss approved his PTO to go to the dentist. however that day turned out to be the layoff day so HR threated him to come to the office. was hilarious. god i hate HR. anyways if you ever go on PTO turn your phone off completely. reply ivraatiems 13 hours agorootparentThreatened him with what? Being laid off? reply vintermann 12 hours agorootparentI assume at the point he didn't know, so yeah, that could have worked (and was probably basically it). reply duderific 18 hours agoprevSimilar to the article's author, I also once got laid off shortly after being hired; three months in my case. I was promised they were on a growth track to nine-figure revenue. One month of down subscription growth, the CEO panicked, and I was gone. I still get mad thinking about it. reply magic_smoke_ee 7 hours agoprevMisalignment of interests is the core error of reasoning the traditional employee makes in offering up unquestioning faith and loyalty to a random corporation run by strangers; that greedy incompetence will somehow give them enduring security. This is decidedly not the case. Instead, more sensible employees should form worker-owned co-ops to both share in the treasure of livable earnings with potential immense profits and structure themselves for the maximum security of those who \"sail on that ship\" together rather than optimizing short-term profits of someone else. reply ge96 20 hours agoprevRandom thought I used to reject 6mo contract offers but after being a laborer the 2.5X pay increase even for 6mo made sense. So I accepted one and now I'm here typing this on a 16\" mac at a new job. I am now thinking about making better choices financially. reply the_real_cher 18 hours agoparenthow do you even get those? reply Tade0 10 hours agorootparentLuck mostly. You have to sift through tons of listings because companies which run such projects just throw it all in with the rest. reply ge96 18 hours agorootparentprevFill out your linked in, turn it on Have a GitHub, resume, luck I wrote extensions to through LinkedIn's jobs and other boards but ultimately luck (some company person finds you). It's even harder for me no degree but I have 5yoe. My job though was niche my robotics/hardware projects stuck out to the interviewer. I worked labor for a year it sucked. But yeah I turned down a few 6mo roles thinking they were not secure but I was like f it I need change and will just pour my higher income into my debts. reply sfriskzo2005 19 hours agoprev>> If you haven’t searched for a job since then it will surprise you how much more of an employers market it is now. Could you clarify further? Front what I hear on the news, there are hundreds of thousands of open engineering/ai positions in the US and we cannot find workers to fill them. You mention ZIRP, so I'm assuming you're probably in the US. Practically every news show in the past two weeks has noted the importance of having a concerted US policy to help fill these open positions. They also mention the existential risk to the US caused by the massive shortage of engineers. How does this square with you saying \" If you haven’t searched for a job since then it will surprise you how much more of an employers market it is now.\" Where is the disconnect? reply octopoc 18 hours agoparentThe disconnect is that the news is completely wrong. I can confirm that it’s an employer’s market from personal experience as well. reply rglover 15 hours agoparentprevMy guesses: Fake job postings to give the appearance of growth for funded companies who are struggling to grow to meet investor expectations. Posturing to distract from the actual practice of hiring less-expensive talent overseas while pretending to be on Team America. Political spin to avoid losing face/clout during an election year. In short: some form of lying (or at best, twisting) to avoid the shame of not being as successful as one might look on theirprofile. reply Ekaros 10 hours agorootparentAnd even more cynical take: They are marketing for AI hype cycle. That is there to drive up the stock prices, not actually hire. Spread out effort to make companies and AI overall look more popular. reply Aurornis 18 hours agoparentprev> Could you clarify further? Front what I hear on the news, there are hundreds of thousands of open engineering/ai positions in the US and we cannot find workers to fill them. The only thing that matters is the ratio of job seekers to open positions. There are always a lot of open positions because there are a lot of companies. Even during recessions most companies will be opening to hiring the right candidates. When there are more candidates than positions combined with a lot of layoffs, companies get more selective. Companies are hiring, but they're being more careful about who gets hired. reply throwway120385 4 hours agoparentprevThe other disconnect is that the employers are trying to fill positions at a certain price point which is usually much lower than most people will accept. Asking why we can't just take those positions when desperate is basically asking why you can't work for significantly less than you have been making for the past 10 years. People have houses and families to support. In some places those jobs pay so little that you can barely make rent with roommates. They exist solely to prove there aren't any qualified US workers for H1B purposes. A lot of really good people can be had in certain US states if you're willing to pay them a lot of money relative to their local market. I've gotten several applications from some pretty overqualified people simply because the pay for the associate position is higher than their local employers are offering for mid-level people. If you then turn that around, it suggests that some of these employers struggle to fill positions because they actually have to compete on the nation-wide market for people. reply nextworddev 19 hours agoparentprevLots of job postings are fake reply odyssey7 8 hours agoparentprev> engineering/ai Most software engineers lack credentials to get hired to build ai, unless you view conventional software engineering as ai. The time to build out ai tracks in universities was back in the Obama administration. It’s ridiculous that all of a sudden the whip is being so harshly cracked for everyone to re-skill. I applied to CS grad programs for two straight cycles, with strong GRE scores and a solid CS undergrad degree, and not a single non-remote university program offered me admission, seemingly due to enrollment caps as bottlenecks. I ended up joining a well-known online MS program, and I feel that it is teaching me so little for the effort I put in that I don’t think my ai skill has improved over what I had from outside of the program, but hey, if the system is broken, what can you do? I can learn nothing, remotely, and use that degree to get an ai job where I do genuinely valuable work, but I’m definitely not allowed to make those genuinely valuable work contributions in a remote role. The opportunities available are extremely limited or they don’t make sense/aren’t very good. Just smile and play the game… or is it the game that’s playing with candidates? reply leptons 19 hours agoparentprevJust because a position is advertised does not mean the position is going to be filled - a lot of companies advertise a job that doesn't really exist. They do this for several reasons - to look like they are more successful than they are (if they are hiring they are expanding), and some just collect resumes. They might even do a few interviews, which helps their team practice doing interviews even if the company isn't going to hire. I'm sure there are other reasons. I've been in the tech job market about 30 years, and this might be the second worst time for people in tech, the dot-com bubble of the early 2000s being the worst. My boss knows it too, he knows I can't leave and find another job that pays as well right now. A couple of years ago I was still getting about 20 recruiter contacts per week, and now I'm lucky if I get 1 per month. If you haven't noticed how bad it is, I have to wonder why? reply scarface_74 17 hours agorootparentI started working in 1996. The dot com bust wasn’t bad if you were in a market with profitable enterprise companies like banks, insurance companies, etc. I could throw my resume up in the air and find plenty of commodity Windows programming jobs. It’s much worse now. I found a job quickly both last year and the year before. But that was only because I have a combination of skills and experience that puts me at the top of the pile of resumes in my niche. reply sfriskzo2005 18 hours agorootparentprevI mostly hire and dont actively code for work, so I never got 20 recruiter contacts per week before or now. Agree on not all postings are real (we do some because we're mandated to do a proper search when filling positions with people we know) reply mixmastamyk 15 hours agoparentprevThese stories are bought and paid for, to pressure lawmakers to make it easer to hire H1Bs and/or offshore to lower wages. Much like recent stories about the shortage of engineers or workers in general. Shortage of suckers willing to work below market, that is. None of these companies will consider training existing engineers for a month either. https://paulgraham.com/submarine.html reply formerlurker 11 hours agorootparentI agree. H1Bs are akin to indentured servants who do not have the ability to vote. Meanwhile, employers make large payments to politicians to keep the status quo. What I dislike most is the cheating from H1Bs though. My friend in an AZ university described how foreigners were known to cheat on their exams based the clique of the country they came from. Similarly, I met an H1Bs who sends his work to Indian cheaper workers even though it is supposed to be confidential. They also collude to get their family and friends into the U.S. Bloomberg news covered it recently, https://www.bloomberg.com/graphics/2024-cognizant-h1b-visas-... reply 0xbadcafebee 15 hours agoprevI really, really wish the industry would give the fuck up on resumes. It's such a stupid way to hire. Huge waste of time for all involved, inaccurate, lengthy, difficult. As a hiring manager, I want: - Your references are already vetted and testimonials left on a public profile, private on invite for those who haven't left their job yet - Actual qualifications, like job training and certifications. Even Scrum Training. I want to know you were actually taught the right way at least once, by a reputable source - A written contract that what you say is your experience is true, and that I can use arbitration to seek damages if you lied - A standard set of tests administered by experts chosen at random. - Your requirements to be hired As a candidate, I want: - The salary range, hire type (contract/full-time), benefits - The location - A description of the project you want worked on and skills required - Your company's pitch deck or equivalent - Glassdoor reviews - My name, age, gender, picture, etc are hidden until the company has clicked a button that certifies they are interested in me based on my qualifications. I get that we can't stop hiring bias, but at least make it more obvious when they pass up why they have. We can't we just agree as an industry that we should all pitch in and make this? There's enough capital here, and it's not like we don't know how to build these things. Certainly there's enough people here motivated to work on it. reply scarface_74 14 hours agoparentAll of these signals are problematic. Your references are already vetted and testimonials left on a public profile, No one is going to give you a negative reference on LinkedIn. That doesn’t provide any meaningful signal. Actual qualifications, like job training and certifications. Even Scrum Training. Certifications don’t prove competence. Anyone can memorize enough to pass a multiple choice test and there have been brain dumps for them for decades. I went through the 6 certification .Net “Architect” path back in 2010 basically as a guided learning path with a goal at the end. I knew even back then they were worthless as far as competence and never put them on my resume. More recently, at one point I had 9 of the then 12 AWS certs as late as 2021. I have 6 of the 9 current ones now and I’m working on the other 3 by the end of the year. I got my first one without ever opening the console in 2018. They also served as a guided learning path so I would know what I didn’t know and I could talk the talk. But I don’t consider myself knowing a service until I’ve used them. They are all very shallow marketing certs except I’ve heard good things about the Kubernetes certifications and that they require hands on problem solving. I want to know you were actually taught the right way at least once, by a reputable source How would you know that? I have a degree in CS. I graduated in 1996. Is the source I was taught COBOL and FORTRAN from “reputable” or “relevant”? A written contract that what you say is your experience is true, and that I can use arbitration to seek damages if you lied Arbitration is biased toward the client and the clients industry. The judges don’t get hired if they consistently make judgments that are against the industry that hires them. A standard set of tests administered by experts chosen at random. What I am looking for when hiring is specific to the company. But they have that already - leetCode style interviews that don’t give a signal to whether they are “smart and gets thing done”. reply isbvhodnvemrwvn 11 hours agorootparentK8s certs are not any better, it's just a slightly more practical test (but it falls well short on how most companies use k8s) reply throwaway2037 11 hours agoparentprevThis whole post feels like a elaborate troll. Still, I cannot resist replying. > testimonials left on a public profile As a future candidate, what will you do if someone leaves a very negative testimonial about you? > Actual qualifications LOL. Then, it is basically impossible to hire most good programmers, because none of them will have \"actual qualifications\". To make a joke: Linus Torvalds applies for your latest CRUD role. Has no certifications. \"Linus Torvalds? Who is this guy thinking he can apply with no certifications? What a silly git. Next!\" > A written contract that what you say is your experience is true, and that I can use arbitration to seek damages if you lied. Likewise: You will be personally liable if the candidate wishes to seek damages via arbitration because you oversold (read: lied about) the role during the interview. Forget about tech for a moment: You can re-write it for hiring people in oil and gas field operations, and it seems just as absurd. Re-write it again for a hospital trying to hire nurses. Still absurd. I picked both of those industries because they are very much learn on the job. Yes, certs exist, but there are many people who work most of their career in those industries with few-to-no certs. reply mixmastamyk 15 hours agoparentprevLinked in and Glassdoor already exist, though I don't trust Microsoft to administer. Needs an open alternative. reply purple-leafy 20 hours agoprevHey I got laid off too. It actually has worked out as a positive over-all. Job searching is a shit process though. I applied for ~40-50 jobs, only got 1 offer. Small country too, so thats basically all the jobs I could find. But landed in a really good company, and have a bit of a break before I start. So I've been using all my spare time to learn Graphics programming, C, and Audio Engineering. I've made a little Wolfenstein3D-type raycasting engine [0] that I'm proud of, as I'm just a frontend coder for work. [0] - https://github.com/con-dog/2.5D-raycasting-engine/blob/maste... reply adamc 20 hours agoprevActual title says \"for the First Time in My Career\". Edited title is kind of weird. reply bagels 20 hours agoparentActual title is weird. How is it both the first and second time? reply hcs 19 hours agorootparentThere's just two songs in me, and I just wrote the third reply nephronaut 3 hours agoprevWait this header doesn’t make sense. Laid off for the first time? Twice? reply shove 18 hours agoprevThis thread has a lot of interesting anecdata so I’ll add mine. Over 25 years experience in frontend web, searching since July, and the water is as ice cold as I’ve ever felt it. Low response rates, low interview rates, zero offers. Wondering if the backend grass is slightly greener at the moment or that’s just my perception from the other side of the fence. reply Tade0 10 hours agoparentAlso doing frontend, but my friends from the other side of the stack fence tell me that employers have been piling up responsibilities, so when you apply for a backend position, you're automatically assumed to fit a DevOps role as well. Anyway, I've found that back in our field it's largely React roles which were affected. Angular jobs are fewer and typically lowballed, but not nearly as much as the rest. reply scarface_74 17 hours agoparentprevIf someone (I don’t know if it is true in your case or not) has a generic set of skills like - “full stack developer” or “front end developer”, there are hundreds of people applying for the same job and it’s hard to stand out. You have to lean on your network. reply maxehmookau 9 hours agoprevFiring someone during their parental leave is a special kind of messed up. The fact that it's not completely illegal in any reasonable country is horrifying. reply throwway120385 4 hours agoparentIt's illegal in my state in the US. Also if you do FMLA in the US it would be illegal if you're the parent who gave birth. Outside of a few states here in the US you're basically expected back at work as soon as you get out of the hospital unless you have a doctor's note. reply scarface_74 19 hours agoprevI have been let go three times in almost 30 years and I’m on my 10th job. The first time was at a struggling startup. We all knew they were struggling and the company was very honest with us and kept us abreast of all the companies that our VC backers were pursuing. I was in some of the interviews with potential acquirers. Our backers promised all of us that “we would get paid for every hour we worked”. Of course they couldn’t promise us we would not get laid off. All of us stayed until the bitter end. That day we all got laid off after being acquired for scraps, we went to lunch together and hung out in the office just joking around until the end of the day. We all had something in our back pocket anyway and from looking at LinkedIn, everyone found a job that was either as good or better within a month. This was 2011. Our largest customer arranged for me to get contract with them to finish out a project, the acquiring company gave the customer access to all of their code and gave me permission to keep my work laptop and waived my non compete. They gave everyone a month severance. I was treated fairly and have no ill will toward anyone there. The second time it was Amazon in 2023: https://news.ycombinator.com/item?id=38474212 The third time it was a shit show of a company last year. But this is where I have problems with the author > How do you succinctly summarize and highlight all you’ve done there? I keep my resume and longer form career document up to date at least once per quarter. I list out the details of major accomplishments in STAR format while it’s still fresh. I have both a technical summary and a business oriented summary for non technical people. It looks like he learned that lesson too. > I was being selective and only applying for places I’d realistically want to work He has a newborn baby. His first priority is to work for any company that will allow him to exchange labor for money to support his family. Even if you do have savings, no need to use it unnecessarily. I got a 3.5 months severance from Amazon the year before last and my stretch goal was to get an offer before my paid out PTO of 9 days was over let alone dip into my severance. Of course I reached out to my network first and targeted outreach to companies specializing in my niche (strategic cloud consulting emphasizing app dev). I also spammed my resume to any CRUD enterprise app dev job as a Plan B. I could always keep interviewing while working. I was working remotely. I did end up getting a Plan A job offer within 9 days of leaving AWS. But I knew three months in that it wasn’t going to be a long term job. When the end did come, I was already in the early stages of interviewing for my current job and had an offer three weeks later. But again, I wasn’t going to let the perfect be the enemy of getting any job and I kept applying for Enterprise CRUD jobs until the offer was finalized. I did work on a side project in 2023. But I got paid for it. A former CTO had some work he needed done. reply pavel_lishin 1 hour agoparent> I keep my resume and longer form career document up to date at least once per quarter. I list out the details of major accomplishments in STAR format while it’s still fresh. I have both a technical summary and a business oriented summary for non technical people. I have a bookmarklet that opens a new gmail compose window, with the recipient being \"me+journal@employer.com\", and the subject being today's date. As I'm working throughout the day, I jot down what I'm working on. I also used to review these every Friday, and type up a summary, and every few months - or as I'd remember to - update a \"hype doc\" that's basically like an internal resume that my manager can use to help argue for a raise for me, or whatever. I also copy this data, without any stuff that might be considered company property, to someplace local I can access if I'm fired. reply throwaway2037 11 hours agoparentprevThese posts are so badass. Thank you to share. Please keep writing. We need more of these. Literally, your handle \"scarface_74\" says it all. When PIP comes through the door, you raise your automatic rifle and shout: \"Say hello to my little friend!\" reply scarface_74 10 hours agorootparentFunny enough, I’ve used this handle since the late 90s on various forums - including Yahoo Games. It’s a reference to my favorite obscure Batman villain. https://batman.fandom.com/wiki/Scarface reply nomagicbullet 2 hours agorootparentThank you for introducing me to the Ventriloquist and Scarface. Absolutely bizarre characters. I love it. reply jongjong 20 hours agoprevTough times. I feel like this is caused by massive capital misallocation over decades. The vast majority of tech companies should not exist, if not for the financial environment propping them up. Now with interest rates going up, there is less cheap money and companies are forced to lay people off and/or shut down themselves. Reality is that there are too many software developers chasing a small number of value-creating opportunities in a sea of useless or highly inefficient tech companies. In the meantime, there aren't enough people to produce food, build houses, collect garbage, etc... So costs of essentials keeps going up. It's hard for software devs to transition to physical jobs so it's going to be a tough one. reply nine_zeros 20 hours agoparent> I feel like this is caused by massive capital misallocation over decades Absolutely a large misallocation. And this is not just about the number of engineers. It is also about number of managers, 2 pizza teams, entire management chains merely doing promotion/PIP management, entire sets of VPs and execs with no market experience or engineering experience. Even a large number of PE investors and angel investors who just landed on money but actually don't have any skills beyond betting far and wide. Highly paid roles have been paying people who are just doing administrative work. This is misallocation. And all of a sudden, this misallocation has come to bare. reply thr0w 20 hours agorootparentI think this gets to the heart of the current situation. The hard truth is that a lot of office workers are actually unskilled labor hidden behind the right kind of social conditioning and \"professionalism\". reply Jerrrry 19 hours agorootparentFake job theory. Essentially broken window theory with humans. reply jongjong 16 hours agorootparentprevYeah, it feels like the system created all these jobs because it could afford to support all that deadweight, but most of those people were not creating value; they were not contributing to the company's success so much as merely mooching off of it. Also, the centralization of media put this effect into hyperdrive. Successful, high exposure companies were drowning in money and so they could just throw 100 engineers at each tiny problem and it wouldn't materially affect their bottom line. They also didn't care much whether an employee was doing their job efficiently, so long as they met basic objectives... Which wasn't hard to do when you have so many people in the team and each person is responsible for a tiny piece. Many people could be counter-productive in the long run, they were productive enough to meet their short-term OKRs and so they were left alone but their rushed work set the project up for long term pain... Often it's impossible to trace back issues to specific individuals... In software development, it's trivial to introduce massive technical debt while meeting or even blowing past short-term objectives. Someone who is literally killing the project might appear to be a top performer... They may be promoted before any problems become apparent... Kind of like a bad civil engineer who builds an amazing looking bridge and is celebrated for years until the bridge suddenly collapses because the foundations turned out to be poorly designed. By that point they've already been promoted several times, maybe already retired and they can claim that the collapse was caused by incorrect construction practices or bad maintenance work performed later. However, in software, it's much worse because you can't just point to a single incorrect formula or calculation. Failure is usually the result of many bad decisions. reply coolThingsFirst 19 hours agoprevu guys are getting interviews? 2 years of experience and it's hard. seems the old markers of dev competence are no longer there, some github projects, a degree were enough nowadays it's a lot harder even here in europe where the salaries are on the low end. reply ypeterholmes 12 hours agoprevTypo: \"Your pay your taxes into the system\" Sorry to be this way. reply skeptrune 18 hours agoprev [–] >Recruiters. Don’t discount or blow them off. They have a vested interest in getting you hired and 3 of my jobs have been found through them. Recruiters are often and perform a valuable service. Agree that it can be invaluable to find one you want to work with. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author shares their experience of being laid off twice in one year, emphasizing the importance of maintaining good relationships with former employers and being prepared for job searches. - Key signs of potential layoffs include a lack of company vision, distractions, low product usage, and sudden company meetings, suggesting employees should stay vigilant. - Advice for those laid off includes updating resumes, filing for unemployment, leveraging LinkedIn and networking, understanding business models, and maintaining self-worth beyond employment."
    ],
    "commentSummary": [
      "The author, having experienced two layoffs in a year, emphasizes the importance of using a simple, linear resume format to improve response rates from Applicant Tracking Systems (ATS). - ATS often misinterpret modern resume formats, which can negatively impact job application outcomes; simplifying resumes and focusing on role responsibilities can mitigate this issue. - Networking, maintaining good relationships, and building a diverse skill set are crucial strategies for career stability and security in a challenging job market."
    ],
    "points": 260,
    "commentCount": 238,
    "retryCount": 0,
    "time": 1736284464
  },
  {
    "id": 42634624,
    "title": "Fidget",
    "originLink": "https://www.mattkeeter.com/projects/fidget/",
    "originBody": "Matt Keeter // Fidget projects research blog about links Introduction Fidget is a library for representing, compiling, and evaluating large-scale math expressions, i.e. hundreds or thousands of arithmetic clauses. It's mainly designed as a backend for implicit surfaces, but the library is flexible enough for many different uses! Try the demo What are implicit surfaces? Implicit surfaces are expressions of the form 𝑓 ( 𝑥 , 𝑦 , 𝑧 ) → 𝑑 f(x,y,z)→d returning a single distance value 𝑑 d. If that value is positive, then the point ( 𝑥 , 𝑦 , 𝑧 ) (x,y,z) is outside the model; if it's negative, then the point is inside the model. For example, a sphere of radius 1 can be expressed as 𝑓 ( 𝑥 , 𝑦 , 𝑧 ) = 𝑥 2 + 𝑦 2 + 𝑧 2 − 1 f(x,y,z)=x2+y2+z2−1 By evaluating the expression at many points in space and checking its sign, we can render this function into an image. In this example, we'll render a heightmap, where filled-in voxels are colored based on their distance to the camera: Fidget focuses on closed-form implicit surfaces, which build their expressions out of basic arithmetic operations. This is in contrast with more flexible representations (e.g. GLSL in a pixel shader), which may run entire Turing-complete programs to compute the distance value. I like to think of these functions as an \"assembly language for shapes\": a low-level representation that you're unlikely to write by hand, but easy to target from higher-level representations. Why implicit surfaces? Implicit surfaces are a particularly pleasant representation: they're compact, philosophically straight-forward, and amenable to massively parallel evaluation (e.g. using SIMD instructions or on the GPU). In addition, constructive solid geometry (CSG) operations like union or intersection – famously difficult for meshes and NURBS – are trivial! Here's the union of two exactly-coincident cylinders, which is just min(a, b): Having a closed-form equation opens the door to interesting optimizations. For example, Fidget can capture a trace showing which branches are taken during evaluation; this trace can be used to simplify the expression, reducing the cost of future evaluation. Finally, for the computer enthusiasts, implicit surfaces offer a deep well of entertaining research topics: everything from compilers to meshing algorithms to GPGPU programming. In other words, I just think they're neat. Origins Over the past ten years, I've spent a lot of time thinking about rendering and evaluation of implicit surfaces: I've written 4-5 kernels in various languages, at least one of which was used in commercial CAD software. Back in 2020, I published a SIGGRAPH paper presenting a new strategy for fast rendering on modern GPUs So, why write something completely new, when the industrial-strength libfive kernel is sitting right there? This is personal-scale, mostly-unpaid research, so I have to optimize for my own motivation and energy. The questions that I'm currently finding interesting would be hard or frustrating to investigate using libfive as a foundation: Finding the \"right\" APIs for implicit kernels, with the possibility of making substantial compatibility breaks Experimenting with native (JIT) compilation, to improve performance without moving to the GPU Cross-compiling to WebAssembly and building easily-accessible web demos libfive is 40K lines of mostly C++, and is extremely challenging to hack on, even as the original author. It's also frustrating to touch: if it's been a few months since I last compiled it, it's inevitable that the build will have broken, and I'll have to mess with CMake for 10 minutes to unbreak it. Fidget is written in Rust, which I'm also using professionally. It compiles with a single cargo build, cross-compiles to WebAssembly seamlessly, and can be refactored with confidence (thanks to Rust's strong type system and memory safety). With all that context, let's get started on our tour of the library! Library structure The library is built as a stack of three mostly-decoupled layers, along with demo applications. Click on one to jump to that section of the writeup, or keep scrolling to read through in order. Frontend Math trees and graphs Backend Fast, flexible evaluation Algorithms Rendering and more Demos GUIs, CLIs, and web apps Frontend: building math expressions The frontend of Fidget goes from input script to bytecode: It's not mandatory to use this specific workflow; users of the library can show up at any point in this diagram. Let's walk through step by step and see how things look at each stage! Scripting Fidget includes bindings for Rhai, an embedded scripting language for Rust. Using operator overloading, it's easy to write scripts to build up math expressions: let scale = 30; let x = x * scale; let y = y * scale; let z = z * scale; let gyroid = sin(x)*cos(y) + sin(y)*cos(z) + sin(z)*cos(x); let fill = abs(gyroid) - 0.2; let sphere = sqrt(square(x) + square(y) + square(z)) - 25; draw(max(sphere, fill)); The value passed to draw is a math tree which represents the expression built up by the script. Tree and graph The math tree is deduplicated to produce a directed acyclic graph: SSA tape Doing a topological sort lets us flatten the graph into straight-line code. This code is in single static assignment form; there are arbitrarily many pseudo-registers (named rX), and each one is only written once. r7 = Input[0] r6 = MulRegImm(r7, 30.0) r26 = SquareReg(r6) r16 = Input[2] r15 = MulRegImm(r16, 30.0) r25 = SquareReg(r15) r24 = AddRegReg(r26, r25) r10 = Input[1] r9 = MulRegImm(r10, 30.0) r23 = SquareReg(r9) r22 = AddRegReg(r24, r23) r21 = SqrtReg(r22) r20 = SubRegImm(r21, 25.0) r19 = SinReg(r6) r18 = CosReg(r15) r17 = MulRegReg(r19, r18) r14 = SinReg(r15) r13 = CosReg(r9) r12 = MulRegReg(r14, r13) r11 = AddRegReg(r17, r12) r8 = SinReg(r9) r5 = CosReg(r6) r4 = MulRegReg(r8, r5) r3 = AddRegReg(r11, r4) r2 = AbsReg(r3) r1 = SubRegImm(r2, 0.2) r0 = MaxRegReg(r20, 1) Output[0] = r0 It's possible to evaluate this tape, but it doesn't scale well: you have to allocate a memory location for every single operation, since pseudo-registers aren't reused. Bytecode To improve evaluation, it's helpful to map from pseduo-registers to physical(ish) registers, which can be reused. For example, the tape above can be packed into 6 reuseable registers: r3 = Input[0] r3 = MulRegImm(r3, 30.0) r0 = SquareReg(r3) r4 = Input[2] r4 = MulRegImm(r4, 30.0) r2 = SquareReg(r4) r0 = AddRegReg(r0, r2) r2 = Input[1] r2 = MulRegImm(r2, 30.0) r1 = SquareReg(r2) r0 = AddRegReg(r0, r1) r0 = SqrtReg(r0) r0 = SubRegImm(r0, 25.0) r1 = SinReg(r3) r5 = CosReg(r4) r1 = MulRegReg(r1, r5) r4 = SinReg(r4) r5 = CosReg(r2) r4 = MulRegReg(r4, r5) r1 = AddRegReg(r1, r4) r2 = SinReg(r2) r3 = CosReg(r3) r2 = MulRegReg(r2, r3) r1 = AddRegReg(r1, r2) r1 = AbsReg(r1) r1 = SubRegImm(r1, 0.2) r0 = MaxRegReg(r0, r1) Output[0] = r0 Register allocation uses the simple algorithm that I blogged about a few years back. It's a single-pass algorithm that trades efficienty for speed and determinism; that blog post has much more detail and interactive demos. The bytecode interpreter uses 256 registers, so the register index is stored in a u8. If we run out of registers, then the allocator inserts LOAD and STORE operations to write to secondary memory (using a u32 index). This bytecode tape is the end of the front-end; we now proceed to the backend for evaluation! Backend: fast, flexible evaluation The Fidget backend is decoupled from the front-end with a set of traits (Function, TracingEvaluator, and BulkEvaluator), which roughly represent \"things that can be evaluated\". This decoupling means that algorithms can target a generic Function, instead of being tightly coupled to our math tree implementation. Right now, there aren't any non-math-tree implementers of the Function trait, but this could change in the future. In the current implementation, we have two different flavors of math tree evaluation: interpreted bytecode, and JIT-compiled functions. Here's a high-level sketch of the evaluation process: For our math expressions, the Tape object is the same bytecode tape that we constructed earlier. The Evaluator is a very simple interpreter, which iterates over the bytecode tape and executes each opcode. There are four different evaluation modes: Single-point and array (SIMD) evaluation Forward-mode automatic differentiation Interval arithmetic Bulk and SIMD evaluation Bulk evaluation is straight-forward: the user provides an array of input values, and receives an array of outputs. When using the JIT backend, we generate SIMD code which processes 4 (AArch64) or 8 (x86-64) element at a time. [ 1 , 2 , 3 , 4 ] + [ 5 , 6 , 7 , 8 ] = [ 6 , 8 , 10 , 12 ] [1,2,3,4]+[5,6,7,8]=[6,8,10,12] Forward-mode automatic differentiation The derivative evaluator calculates both a value and up to three partial derivatives (with respect to some variables). For implicit surfaces, it's typically used to calculate the tuple ( 𝑓 , ∂ 𝑓 ∂ 𝑥 , ∂ 𝑓 ∂ 𝑦 , ∂ 𝑓 ∂ 𝑧 ) ( 𝑥 , 𝑦 , 𝑧 ) (f,∂x∂f,∂y∂f,∂z∂f)(x,y,z) At the surface (where 𝑓 ( 𝑥 , 𝑦 , 𝑧 ) = 0 f(x,y,z)=0), the partial derivatives are a good approximation of the surface normal, which can be used for shading: Evaluation uses forward-mode automatic differentiation; values in registers are augmented with derivatives, and the chain rule is used at each step of evaluation. The JIT evaluator packs the value and three derivatives into a single 4 x f32 register for efficiency. Interval arithmetic Interval arithmetic lets us evaluate expressions over ranges of input values: instead of assigning 𝑥 = 1 x=1, we can say 1 ≤ 𝑥 ≤ 5 1≤x≤5. The output will also be an interval, e.g. 2 ≤ 𝑓 ( 𝑥 , 𝑦 , 𝑧 ) ≤ 20 2≤f(x,y,z)≤20. The results of interval arithemetic are conservative: they may not tightly bound the true range of the function, but they are guaranteed to contain all possible outputs (for input values within the given input intervals). Interval arithmetic is a core building block for evaluation of implicit surfaces. If you evaluate a spatial region (using intervals for 𝑥 x, 𝑦 y, 𝑧 z) and find an interval that's unambiguously greater than 0, that entire region is outside your shape and doesn't need to be considered any further! In the render below, green and red regions were proved inside / outside the shape by interval arithmetic; only the black and white pixels were evaluated by the point-wise evaluation. Skipping regions based on interval arithmetic results is already a significant optimization over bulk evaluation, but wait – there's more! Tracing evaluation and simplification Our interval arithmetic evaluator also captures a trace of its execution. For example, consider the following min, evaluated using interval arithmetic: min ⁡ ( 𝑎 , 𝑏 ) where 0 ≤ 𝑎 ≤ 1 and 4 ≤ 𝑏 ≤ 5 min(a,b) where 0≤a≤1 and 4≤b≤5 Because 𝑎 a is unambiguously less than 𝑏 b (within this interval), the expression can be simplified from min ⁡ ( 𝑎 , 𝑏 ) → 𝑎 min(a,b)→a. Each min and max operation records a single choice, indicating which arguments affect the result (left-hand side, right-hand side, or both). These choices can be used to simplify the original function. In our min ⁡ ( 𝑎 , 𝑏 ) min(a,b) example, we don't need to calculate 𝑏 b or min ⁡ min if we've simplified the expression to just 𝑎 a. The One Weird Trick The combination of interval arithmetic and tape simplification is the One Weird Trick that makes working with large-scale expressions feasible. I'm going to circle around this idea from a few different angles, because it's critically important! Interval arithmetic and tape simplification have a positive synergy: we can both skip inactive (empty / full) regions of space, and evaluating the remaining active regions becomes cheaper. You can also think of tape simplification as analogous to a spatial acceleration data structure: we're computing a simplified expression that's only valid in a particular spatial region. Unlike typical raytracing acceleration data structures, we're building the accelerator dynamically during evaluation. For rasterization, the cost of interval evaluation is amortized across many pixels in the interval. Interval evaluation of an 𝑁 × 𝑁 N×N pixel region with a tape length of 𝑇 T is 𝑂 ( 𝑇 ) O(T), i.e. only proportional to the tape length, and independent of pixel count. By the time we get down to single-pixel evaluation (e.g. for an 𝑀 × 𝑀 M×M region, 𝑀that's shared with the worker thread. All of this was a terrible nightmare! Each individual component deserves praise for including a working example; unfortunately, they all used mutually exclusive bundlers, settings, servers, etc. Combining them all into a working system was quite a challenge! (For example, I had to pin an old version of wasm-bindgen because they recently fixed a bug that was load-bearing for wasm-bindgen-rayon) Still, I'm pleased with how it turned out. It even works on my phone, albeit without camera controls (since I'm using mouse events instead of touch events). Aside: Meditations on demos I always have mixed feelings about making demos for my projects. Fidget is first and foremost a library; people should be embedding it into their own projects as infrastructure, not using my demos as actual CAD tools. However, the population of Weird CAD People (which is small to begin with!) breaks down into roughly the following groups: Many more people will be interacting with the demos – even to the point of using them for design work – than building tools with the libraries. There's a tension between making the demos more useful for this larger audience, versus making the libraries better for a smaller set of users. Historically, I've struggled to maintain a kernel and full-fledged CAD UI simultaneously, and have been reducing the scope of demos accordingly – but as you can tell from the web editor, even a \"minimal\" demo is a significant project. I don't know how this will turn out with Fidget! My plan is roughly the following: Keep following my interests, to preserve my motivation and focus (the most limited resources!) Accept suggestions from people using the tools, while setting expectations at a reasonable level Prioritize feedback from people in the smaller circles of tool-builders, because – in an ideal world – they would take pressure off my demos by building full-fledged tools that others could use Future possibilities I've been quietly working on Fidget for about 2.5 years. In that time, it has evolved significantly – for example, it started as an LLVM-backed evaluator! I expect it to continue evolving, albeit at a slower pace, as I continue to find the \"right\" APIs and abstractions. Let me discuss a few obvious future directions, before wrapping up. Doing stuff on the GPU I wrote a whole paper about this, so adding a GPU backend seems like an obvious extension. In fact, I have already done this, in the wgpu-bytecode branch! However, performance isn't particularly compelling, at least on my Apple M1 Max laptop. In particular, the bytecode interpreter loop appears to be incredibly inefficient. I'm continuing to investigate the root cause, and will likely write up a blog post with more details. (If you're familiar with Metal shader optimization, feel free to reach out, and I will happily brain-dump in your direction) Better meshing For serious users of the library, this is the elephant in the room. Fidget is using the same meshing strategy as libfive, but without a bunch of fine-tuning that makes the latter more robust. I haven't spent much time on this, because I'm fundamentally unsatisfied with the available options. I would rather spend time implementing a bulletproof meshing algorithm, rather than adding epicycles to dual contouring. Unfortunately, I haven't seen anything in the literature that meets my requirements, and haven't yet devised anything better myself. We'll see how things shake out! Depending on user demand, I would probably be willing to do some fine-tuning, but also want to keep an eye out for better options. Standard library of shapes and transforms Over the past few generations of software, I've been porting the Fab Modules's standard library of shapes into each new tool. This is tedious, but gives a somewhat-standard foundation for higher-level modeling. libfive hit a high point in complexity, where I wrote each shape in C++, then automatically generated C, Python, and Scheme bindings from the header file. The explanation in the README has a certain \"statements dreamed up by the utterly deranged\" energy to it: libfive_stdlib.h is both a C header and a structured document parsed by a helper script. We're still brainstorming the right approach for Fidget; see fidget#145 for ongoing discussions. Dragging the fab shapes library into Rust would work, but I'm wondering if we can do better – for example, Inigo Quilez's library of primitives takes advantage of GLSL vectors to produce much more elegant code. Bindings to higher-level languages I'm a big fan of wrapping interactive shells around a compiled core. Right now, Fidget only has bindings to Rhai, which was chosen because it's one of the few mature Rust-first scripting languages. Rhai has been very easy to integrate (and I love that it compiles into WebAssembly), but I suspect that most users would prefer Python or Node bindings. Then, there's the question of how to write those bindings: a C API would let people lean on the FFI libraries of their favorite languages, but feels like a step down from Fidget's Rust-first design. Similarly, once we have a standard library (discussed above), it would be great for it to be automatically exposed in each binding, with proper ergonomics (docstrings, default arguments, etc). As always, feel free to reach out if you have thoughts! Conclusion Ever since initial publication, Fidget's README has described it as \"quietly public\"; 19 versions have been published to crates.io, and a few people have started to build on it, but I haven't been talking about it outside of semi-private spaces. This blog posts marks the end of the quietly public phase: it's now loudly public. Go try it out, build some stuff, tell your friends, and let me know how it goes! The source code is on Github and can be added to a Rust project with cargo add fidget. It is released under a weak copyleft license (MPL 2.0), which is friendly for both OSS and commercial usage. Thanks to everyone who contributed to this post and the project as a whole: Martin Galese, Bruce Mitchener, Avi Bryant, Blake Courter, Johnathon Selstad, Bradley Rothenberg, Moritz Mœller, Luke V, and various others in all the group chats. © 2010-2025 Matthew Keeter",
    "commentLink": "https://news.ycombinator.com/item?id=42634624",
    "commentBody": "Fidget (mattkeeter.com)240 points by todsacerdoti 4 hours agohidepastfavorite23 comments mkeeter 1 hour agoHi, this is my project :) I particularly love this corner of CS because there's something for everyone – data structures and algorithms, low-level performance work, compilers, rendering / computer graphics, UI/UX for design tools, GPGPU programming, and more! I'll be answering questions in the threads as I see them, but feel free to connect on social media (https://mattkeeter.com/links/) or follow my blog's RSS feed for further updates (https://mattkeeter.com/atom.xml) reply miniBill 1 hour agoprevOh wow. This would have been incredibly useful to have found when I was writing my own implicit-surface drawer. My approach is similar in some ways (interval arithmetic), and different in others (not as optimized, I directly produce GLSL for a fragment shader). Honestly I'm tempted to just toss everything away and replace it with (a reimplementation of) this. Dunno if I should be happy or sad about that. reply dleink 1 hour agoparentHappy! You work in a field where you can both make your own tools and use tools others have created. Maybe you can steal some of his ideas and use them, or use his stuff and contribute to the project. either way, rad. reply jacobparker 1 hour agoprevCoincidentally I was just reading this other wonderful post from the author https://www.mattkeeter.com/projects/constraints/ reply mkeeter 1 hour agoparentHah, I actually wrote a similar constrain solver that uses Fidget's evaluation + differentiation! This blog post was getting too long, so I'm going to write it up separately, but in the meantime: demo: https://mattkeeter.com/projects/fidget/constraints source: https://github.com/mkeeter/fidget/blob/main/demos/constraint... docs on the solver: https://docs.rs/fidget/latest/fidget/solver/ reply jy14898 1 hour agoparentprevYou might be interested in http://omrelli.ug/g9/gallery/ too reply retzkek 1 hour agoprevYears ago in college I did a bit of work on a nuclear physics simulator (think: reactor modeling) that based its geometrical model on implicit surfaces, specifically R-functions (of which min(x,y) is an example), which have some neat properties such as being differentiable everywhere. This is a good introduction (and probably the only one in English): https://ecommons.cornell.edu/items/35ae0f68-1af5-4f28-8b8b-7... I've been away from the nuclear field for a while, but I imagine it's still using a lot of legacy Fortran codes to do modeling. Fidget has some interesting possibilities as a kernel for a new simulation package. reply peppertree 1 hour agoprevHere is a thought experiment. What if 3D printers support implicit representation natively. Resin printers are basically physical marching cube machines. FSM would need an algorithm for following contours but should be doable. reply daeken 1 hour agoparentI kinda tried doing this with a custom FDM slicer for SDFs a long while back. I hit some roadblocks, but the concept was pretty simple: slicers by definition need to know what exists in a 2d slice of a 3d object. So why not render an SDF directly as slices and then act on that? You're basically then just trying to turn a raster into a vector (a toolpath). The code is simple and hacky as hell -- very much an experiment -- but I still think that it is a plausible route forward. https://github.com/daeken/AjaPrint reply adius 1 hour agoprevRelated: Fornjot - b-rep CAD kernel in Rust https://github.com/hannobraun/fornjot reply syedrezaali 1 hour agoprevMatt is a constant source of inspiration. Congrats on the new project Matt! reply mkeeter 1 hour agoparentThanks Reza! reply agentultra 3 hours agoprevOh neat. I’ve been meaning to explore SDF/implicit surfaces. Optimizing for enjoyment/time and writing from scratch are useful exercises! reply dleink 1 hour agoprevI've been yearning for updated winamp/eggdrop style music visualizations... this is very far outside my domain knowledge but feels like it could be used somehow.. reply turnsout 4 hours agoprevPhenomenal to have a new open source CAD kernel out there! I couldn’t tell from the article if this supports export to common formats like STEP. But if so (or once that’s possible) it should be a great foundation for some of the open source CAD libraries out there. reply mkeeter 3 hours agoparentIt does not support STEP export, unfortunately, because STEP files use a completely different underlying representation. Most STEP files represent shapes as a set of surfaces (e.g. trimmed NURBS). These surfaces have to form a watertight manifold, which can be treated as a solid volume. To make this actually work, you need a kernel for boundary representations (b-reps), rather than Fidget's functional representations (f-reps). Writing such a kernel is a much harder problem – as one example, the intersection of two NURBS surfaces doesn't always have a closed-form representation! In conversations with someone in the industry, they estimated it would take 6 engineers about a year to write a decent b-rep kernel, if they had done it before. If you'd like to learn more, I've coincidentally also written a STEP file viewer, which includes a far-from-industrial-strength b-rep kernel: https://www.mattkeeter.com/projects/foxtrot/ reply actionfromafar 2 hours agorootparentCould this or the other \"Studio\" be used to avoid using OpenSCAD when creating models for 3D printing? reply bhaney 2 hours agorootparentI've been using Studio as my primary way of generating models for 3D printing for years now, no OpenSCAD involved. Write some scheme, get an STL. I do wish Matt would cool it with his constant brilliant endeavors and go back and give Studio some love though. At this point I can't get it to build and can only launch an old version I compiled years ago that breaks if I'm not in some specific working directory in my Downloads folder. reply actionfromafar 2 hours agorootparentThanks! reply rendaw 1 hour agoprevSorry, tangential but is that rust in the web editor? Do you have a wasm compiler or something? It seems super fast! I was looking for a live-coding editor (or live preview? hot reloading with visualizer?) for doing creative coding with something like Nannou and after a bunch of searching I came up dry. reply bhaney 59 minutes agoparent> is that rust in the web editor? From the post: > Fidget includes bindings for Rhai, an embedded scripting language for Rust https://rhai.rs/ reply red_trumpet 4 hours agoprev [–] > Here's the intersection of two exactly-coincident cylinders The picture looks like the union of two cylinders? reply mkeeter 4 hours agoparent [–] You're correct, fixed! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Fidget is a Rust-based library designed for handling large-scale mathematical expressions, particularly for implicit surfaces, which are compact and support operations like union and intersection. - The library is structured into three layers: frontend (converts scripts to bytecode), backend (efficiently evaluates expressions), and demos, with support for SIMD (Single Instruction, Multiple Data) and automatic differentiation. - Fidget is now publicly available on GitHub under the MPL 2.0 license, with future plans for GPU integration and improved meshing, encouraging exploration and development by users."
    ],
    "commentSummary": [
      "Fidget, a project by Matt Keeter, explores diverse areas of computer science, including data structures, algorithms, compilers, and graphics, generating interest for its potential applications in fields like 3D printing and CAD (Computer-Aided Design).",
      "Although it does not support STEP (Standard for the Exchange of Product model data) export due to differing data representations, Fidget offers other functionalities and includes bindings for Rhai, a scripting language for Rust.",
      "The project has inspired creative coding initiatives and discussions, highlighting its versatility and innovative potential in the tech community."
    ],
    "points": 240,
    "commentCount": 23,
    "retryCount": 0,
    "time": 1736346658
  },
  {
    "id": 42628414,
    "title": "Servo Revival: 2023-2024",
    "originLink": "https://blogs.igalia.com/mrego/servo-revival-2023-2024/",
    "originBody": "© Picture by Linux Foundation Servo Revival: 2023-2024 15 November 2024 ENGLISH PLANET SERVO As many of you already know, Igalia took over the maintenance of the Servo project in January 2023. We’ve been working hard on bringing the project back to life again, and this blog post is a summary of our achievements so far. Some history first # You can skip this this section if you already know about the Servo project. Servo is an experimental browser engine created by Mozilla in 2012. From the very beginning, it was developed alongside the Rust language, like a showcase for the new language that Mozilla was developing. Servo has always aimed to be a performant and secure web rendering engine, as those are also main characteristics of the Rust language. Mozilla was the main force behind Servo’s development for many years, with some other companies like Samsung collaborating too. Some of Servo’s components, like Stylo and WebRender, were adopted and used in Firefox releases by 2017, and continue to be used there today. In 2020, Mozilla laid off the whole Servo team, and the project moved to the Linux Foundation. Despite some initial interest in the project, by the end of 2020 there was barely any active work on the project, and 2021 and 2022 weren’t any better. At that point, many considered the Servo project to be abandoned. 2023: Igalia takes over maintenance of Servo # Things changed in 2023, when Igalia got external funding to work on Servo and get the project moving again. We have previous experience working on Servo during the Mozilla years, and we also have a wide experience working on other web rendering engines. To explore new markets and grow a stable community, the Servo project joined Linux Foundation Europe in September 2023, and is now one of the most active projects there. LF Europe is an umbrella organization that hosts the Servo project, and provides us with opportunities to showcase Servo at several events. Over the last two years, Igalia has had a team of around five engineers working full-time on the Servo project. We’re taking care of the project maintenance, communication, community governance, and a large portion of development. We have also been looking for new partners and organizations that may be interested in the Servo project, and can devote resources or money towards moving it forward. Servo Project Updates talk at Linux Foundation Europe Member Summing 2024 (Picture by Linux Foundation) Achievements # Two years is not a lot of time for a project like Servo, and we’re very proud of what we’ve achieved in this period. Some highlights, including work from Igalia and the wider Servo community: Maintenance: General project maintenance, tooling, documentation, community, and governance work. Lots of work has been put in upgrading Servo dependencies, specially the big ones (SpiderMonkey, Stylo and WebRender). There have been improvements in Servo’s CI and tooling. We now have a Servo book, the Servo community has been growing, and the Technical Steering Committee (TSC) is running periodic meetings every month. Communications: The project is alive again and we want the world to know it, so we’ve been doing monthly blog posts, weekly updates on social media, plus conference talks and booths at several events. The project is more popular than ever, and our GitHub stars haven’t stopped growing. Layout engine: Servo had two layout engines: its original one, and a newer one started around 2020, which follows specs more closely but was in very early stages. After some analysis, we decided to go with the new layout engine and start to implement features in it like floats, tables, flexbox, fonts, right-to-left, etc. We now pass 1.4 million subtests (1,401,889 at the time of publishing this blog post) in the Web Platform Tests. And of the tests we run today, our overall pass rate has gone from 40.8% in April 2023 to 62.0% (+21.2). 📈 New platforms: Servo used to support Linux, macOS, and Windows. That’s still the case, but we’ve also added support for Android and OpenHarmony, helping us reach a wide range of mobile devices. 📱 Embedded experiments: We’ve been working with the developers of Tauri, making Servo easier to embed and creating a first prototype of using Servo as web engine in WRY. Our community has also experimented with integrating Servo in other projects, including Blitz, Qt WebView, and Verso. Outreachy: Outreachy is an internship program for getting people from underrepresented groups into our industry via FOSS, and we’ve rejoined the program in 2024. Our intern in the May cohort, @eerii, brought DevTools support back to Servo. Of course, it’s impossible to summarize all of the work that happened in so much time, with so many people working together to achieve these great results. Big thanks to everyone that has collaborated on the project! 🙏 Screencast of Servo browsing the servo.org website and some Wikipedia articles Some numbers # While it’s hard to list all of our achievements, we can take a look at some stats. Numbers are always tricky to draw conclusions from, but we’ve been taking a look at the number of PRs merged in Servo’s main repository since 2018, to understand how the project is faring.2018 2019 2020 2021 2022 2023 2024 PRs 1,188 986 669 118 65 776 1,771 Contributors 27.33 27.17 14.75 4.92 2.83 11.33 26.33 Contributors ≥ 10 2.58 1.67 1.17 0.08 0.00 1.58 4.67 PRs: total numbers of PRs merged. We’re now doing more than we were doing in 2018-2019. Contributors: average number of contributors per month. We’re getting very close to the numbers of 2018-2019, it looks like a good number. Contributors ≥ 10: average number of contributors that have merged more than 10 PRs per month. In this case we already have bigger numbers than in 2018-2019. As a clarification, these numbers don’t include PRs from bots (dependabot and Servo WPT Sync). Our participation in Outreachy has also made a marked difference. During each month-long contribution period, we get a huge influx of new people contributing to the project. This year, we participated in both the May and December cohorts, and the contribution periods are very visible in our March and October 2024 stats:Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec PRs 103 97 255 111 71 96 103 188 167 249 170 161 Contributors 19 22 32 27 23 22 30 31 24 37 21 28 Contributors ≥ 10 2 3 9 3 2 2 2 6 5 8 7 7 Anyway overall, we think these numbers ratify that the project is back to life, and the community is growing and engaging with the project. We’re very happy about them. Donations # Early this year we set up the Servo Open Collective and GitHub Sponsors, where many people and organizations have since been donating to the project. We decide how to spend this money transparently in the TSC, and so far we’ve used it to cover the project’s infrastructure costs, like self-hosted runners to speed up CI times. We’re very grateful to everyone donating money to the project, so far adding up to over $24,500 from more than 500 donors. Thank you! 🙏 Servo booth at Linux Foundation Europe Member Summing 2024 (Picture by Linux Foundation) About the future # Servo has a lot of potential. Several things make it a very special project: Independent: Servo is managed under Linux Foundation Europe with an open governance model, which we believe is very important for the open web ecosystem as a whole. Performant: Servo is the only web rendering engine that uses parallel layout for web content. Though Servo is not yet faster than other web engines that are already production-ready, we’re exploring new ways to render web content that take advantage of modern CPUs with many cores. Secure: Servo is written in Rust, which prevents many security vulnerabilities by design, such as memory safety bugs. Other kinds of unsoundness, such as concurrency bugs, are also easier to manage in Rust. Of course, the project has some challenges too: Experimental: Servo is still in an experimental phase and is not a production-ready product. We still lack several features that are important for web content. Competition: The dominant web rendering engines have huge teams behind them, working on making them better, faster, and more complete. Catching up with them is a really complex task. Funding: This is a key topic for any open source project, and Servo is no different in this regard. We need organizations from public and private sectors to join efforts with us, and grow a healthy ecosystem around the project. To finish on a positive note, we also have some key opportunities: Single web apps: Servo may excel at rendering specific web apps in embedded scenarios, where the HTML and CSS features are known in advance. UI frameworks: UI frameworks in the Rust ecosystem could start using Servo as web engine to render their contents. Web engine: Servo could become the default web engine for any Rust application. Web browser: In the future, we could grow into being able to develop a full-featured web browser. The future of Servo, as with many other open source projects, is still uncertain. We’ll see how things go in the coming years, but we would love to see that the Servo project can keep growing. Servo is a huge project. To keep it alive and making progress, we need continuous funding on a bigger scale than crowdfunding can generally accomplish. If you’re interested in contributing to the project or sponsoring the development of specific functionality, please contact us at join@servo.org or igalia.com/contact. Let’s hope we can walk this path together and keep working on Servo for many years ahead. 🤞 PS: Special thanks to my colleague Delan Azabani for proofreading and copyediting this blog post. 🙏 Previous: Servo at FOSDEM 2024",
    "commentLink": "https://news.ycombinator.com/item?id=42628414",
    "commentBody": "Servo Revival: 2023-2024 (igalia.com)238 points by panic 20 hours agohidepastfavorite83 comments aorth 9 hours agoOh this is nice to hear. It's always pleasant to read updates about Servo. I didn't know they started accepting donations on Open Collective and GitHub sponsors last year https://servo.org/blog/2024/03/12/sponsoring-servo/. I'm happy to contribute something. reply jszymborski 2 hours agoparentFYI: I think their website indicates they pay the fewest fees on GitHub, so you may want to set up your donations there if anyone is still deciding between the two. Sponsoring them was a no brainer for me :) reply diggan 1 hour agorootparent> I think their website indicates they pay the fewest fees on GitHub The difference (https://servo.org/sponsorship/#donation-fees) between the donation going via GitHub/Microsoft and Open Collective (independent) is so small (96% VS ~91%) that I'd rather not centralize funding FOSS with someone who has kind of a shitty track record with it, like Microsoft. Made more sense in the beginning of GitHub Sponsors when Microsoft was matching donations 2x, or whatever it was. But now? I don't feel like it makes much sense anymore. Open Collective is a fully public organization, who lives and breathes FOSS. reply mdaniel 15 hours agoprevInteresting, they also claim that they are the 2nd biggest contributors to Chromium after Google: https://bsky.app/profile/igalia.com/post/3lasylsguzs2f reply jitl 15 hours agoparentIgalia is the real deal. Many companies that want bugs fixed or features added to web browsers hire Igalia to make those changes. They also maintain WebKit on Linux (gtk and wpe) https://planet.igalia.com/webkit/ reply lawik 12 hours agorootparentYeah, been seeing their site a lot when setting up cog and weston for an embedded kiosk thing. Also met a bunch of them at the OpennSource Summit in Vienna. Every now and then you run into these small-ish expert consultancies that actually are the force behind a lot of open source. They seem awesome. reply LeFantome 4 hours agorootparentprevThey also employ the main dev for Chimera Linux: https://chimera-linux.org/ reply topspin 14 hours agorootparentprevThat's interesting. One wonders what their future looks like after Google divests Chrome. Good to see that the knowledge base isn't entirely confined within Google. reply mmastrac 14 hours agorootparentprevYeah. I got to see Andy Wingo's work on a V8 feature and it was an impressive piece of work. reply int0x29 16 hours agoprevThey haven't abandoned it but that title makes it sound like they have. reply benatkin 15 hours agoparentI think that's intended. This indicates that there's a possibility it's default dead. > Servo is a huge project. To keep it alive and making progress, we need continuous funding on a bigger scale than crowdfunding can generally accomplish. If you’re interested in contributing to the project or sponsoring the development of specific functionality, please contact us at join@servo.org or igalia.com/contact. > Let’s hope we can walk this path together and keep working on Servo for many years ahead. https://paulgraham.com/aord.html reply leoc 12 hours agorootparentI assume that they're hoping that the EU or an EU member-state steps up; or failing that, maybe a (probably-US) nonprofit or billionaire donor, perhaps a Laurene Powell Jobs or MacKenzie Scott type. To be clear, something like this very probably should happen. I'm heading to social media to shout into the void about this: dear reader, you should probably do this too, and use any other means you might have to steer the attention of decision-makers towards this. That said, in the longer term the solution to the WWW\"'s Too Big To Fork problem surely has to involve getting much more of the \"specification\" expressed precisely in declarative specification languages, so as to greatly reduce the handwork involved in generating a half-decent implementation. reply LeFantome 4 hours agorootparentWhat we need are multiple implementations, all giving feedback on the spec: https://github.com/LadybirdBrowser/ladybird reply alex_duf 10 hours agorootparentprevI don't think a single big donation is a good idea. We're so used to seeing extreme wealth we don't event question it. Once a big donation is given, you get to wonder what sort of influence that person (willingly or not) has had on the project. A much better model is a large amount of small donations, the incentive becomes to serve the maximum amount of these people. reply leoc 5 hours agorootparentI too would prefer that the funding come from a relatively hands-off source, like some EU pot, if possible. But I think that nearly any (reasonably likely) funding source would be preferable to letting Servo development fail. reply Brian_K_White 14 hours agorootparentprevs/default/defacto reply mappu 13 hours agorootparentNo, \"default dead\" is VC-speak for a business that relies on runway to operate rather than being profitable. reply Brian_K_White 10 hours agorootparentIs Servo a business? I thought it was an open source software library? reply orf 9 hours agorootparentIt’s obviously not a business. reply benatkin 13 hours agorootparentprevs/$/\\// reply tux3 2 hours agorootparentor, s@$@/@, I don't know whether that's more or less cryptic :) reply Brian_K_White 10 hours agorootparentprevadmitted reply cbarrick 13 hours agorootparentprevs/defacto/de facto/ reply Vinnl 8 hours agoparentprevThe article was first posted in 2024, which made it feel less so at the time :) Should probably have a \"(2024)\" appended to the title. reply nar001 12 hours agoparentprevHopefully they don't! I wanna see where this goes, we need more browser engines reply shmerl 13 hours agoprevAny plans to move WebRender to using Vulkan instead of OpenGL? The latter is really not well suited for proper parallelism. I hope Servo will eventually replace Chromium in QtWebEngine and other similar cases. reply benatkin 13 hours agoparentThere's an active attempt to make a DOM rendering engine in Rust using these APIs. https://github.com/DioxusLabs/blitz reply shmerl 12 hours agorootparentThat looks like its own thing, so not going to benefit Servo and Firefox if I understand correctly? Since they are using WebRender. reply nicoburns 9 hours agorootparentCorrect. There are some shared components (notably Stylo, the style system), but Webrender is not one of them. Webrender is still maintained primarily by Mozilla as part of Firefox. If they have any plans to move to Vulkan/Metal/DX10/wgpu then I haven't heard of them. It might be possible for Servo to go down the same route as Blitz and have pluggable rendering backends. If so then the wgpu-based renderering library we are using (Vello [0] - which is an exciting project in it's own right) could be an option. Servo is actively looking at potentially using this library to implement Canvas2D. [0]: https://github.com/linebender/vello reply norman784 11 hours agoparentprevWouldn't be better for them to use wgpu instead? reply pjmlp 3 hours agorootparentGiven that wgpu is based on WebGPU, it is kind of limited by a graphics API designed for the Web sandbox and managed languages. reply cies 9 hours agoprevAll this discussion (hate) on the effort to \"rewrite in Rust\"... At the same time these projects are soooo promissing (to me -- it may be purely subjective). By these projects I mean: Servo and Verso Redox OS System76's COSMIC Desktop's EPOCH RipGrep Deno Zig tree-sitter And lots of web dev libs and frameworks: Actix, Leptos, Dioxus... Currently a web dev stack can run on Redox OS and use significantly less resources than Alpine! (and this stack has not even had the years of tuning Alpine had) reply kragen 3 hours agoparentPolars, a Rust replacement for Pandas. Rewriting things in Rust is a reasonable thing to do. I think the hate is for people who criticize existing software for being written in C on the grounds that hypothetically someone could rewrite them in Rust. \"I rewrote SQLite in Rust\" would be praiseworthy (assuming it's true). \"Why don't you rewrite SQLite in Rust?\" is trolling. reply geodel 2 hours agorootparentExactly. The problem is You do the rewrite because I like Rust. reply kragen 2 hours agorootparentTo be fair, this is a deal I would totally take, at my normal consulting rates. I'll be slower than Rust superfans, but I'm probably cheaper too. reply ivolimmen 4 hours agoparentprevI took a look at COSMIC and it really looks nice. I am not interest in it because it is written in Rust but it simply looks nice and the window management also looks promising. I hope to run it on my main machine soon. reply LeFantome 4 hours agorootparentI have been running COSMIC on my laptop and really enjoying it. It is only going to get better from Here. reply alraj 7 hours agoparentprevWhy is Zig here? reply cies 5 hours agorootparentShit, should have been Zed. The editor. reply actionfromafar 4 hours agorootparentIt would have been funny to suggest rewriting Zig in Rust. :) reply edoceo 4 hours agorootparentRewrite Rust in Zig. reply nasretdinov 30 minutes agorootparentHonestly either should be more than possible to do, although not sure how beneficial. It would certainly be very funny if Zig compiler would be implemented in Rust and, simultaneously, Rust compiler would be written in Zig reply WD-42 11 hours agoprev [–] Of all projects for Mozilla, the supposed champions of the web, to abandon, it still blows my mind that they chose Servo to be the one to lay off the entire team for. reply dralley 3 hours agoparentHN: \"Mozilla has too many side projects that don't make the browser better\" Also HN: \"Mozilla should spend more than a decade and tens of millions of dollars on a brand new browser engine that has no hope of replacing Gecko before it reaches 100% compatibility with a spec thousands (tens of thousands?) of lines long, not to mention the kind of \"quirks\" you see with websites in the wild, while they already lag behind Google with the browser engine they already have.\" People like cool R&D projects, and that's understandable - I like Servo too. But the fact that it was really cool doesn't compensate for the fact that it was not going to be production-ready any time soon and in that light it's understandable why it was cancelled. While some parts of Servo ended up being so successful that they were merged into Firefox, a lot of what remained only in Servo (and not in Firefox) was nowhere close. The layout component was by far the least mature of any part of Servo at the time (unlike Stylo and WebRender, I mean) and in fact it was going through the early stages of a brand-new rewrite of that component at the time the project was cancelled, partly because the experimental architecture ended up not being very suitable. https://servo.org/blog/2023/04/13/layout-2013-vs-2020/ reply demurgos 3 hours agorootparent> that has no hope of replacing Gecko before it reaches 100% compatibility with a spec thousands (tens of thousands?) of lines long When Servo was still managed by Mozilla, they were able to merge some components incrementally into the Firefox. Most famously, Stylo and WebRender were first developed in Servo. They could have kept Servo for experimentation and merge parts incrementally. It may also have enabled better embedding supporting which is a weak point of Firefox compared to Chrome; which is a long-term solution to remain relevant. reply dralley 3 hours agorootparentI covered that. Sure, Stylo and WebRender were successful enough that they made it into Firefox, but the Layout component was very much not. Servo was in the middle of a clean-slate rewrite of the layout component because the initial architecture chosen in 2013 wasn't very good. The CSS engine and rendering engine are a lot easier to swap out than the remaining parts. Again, I get why people like Servo, but \"in 10 years, maybe we'll be able to take on Electron\" isn't that great of a value proposition for a huge R&D project by a company already struggling to remain relevant with their core projects. reply WD-42 1 hour agorootparentI’m pretty sure if Firefox started beating chrome in speed benchmarks (because of a newer, more modern engine) they would be able to claw back some of their lost market share. Even normal people care about speed. reply timschmidt 2 hours agorootparentprev> \"in 10 years, maybe we'll be able to take on Electron\" isn't that great of a value proposition Perhaps not, but \"in 10 years, we'll have a browser that's significantly faster and safer than the competition\" is how you plan to still be relevant 10 years from now. reply dralley 2 hours agorootparentThe browser engine is not what makes Firefox \"relevant\" or not. Their competitors are Apple, Google and Microsoft. The marketing budget for Chrome is larger than Mozilla's entire budget. \"Google\" is synonymous with the entire internet for a large fraction of the non-technical population. Every device you could buy on the market whether a PC, a tablet or a phone has one of their competitors browsers already pre-installed. Their primary leverage is unique features and functional adblockers, neither of which is impacted by the layout engine. And again, you're taking away resources from something that is already behind right now. The canonical example of massive long-term rewrites being a bad idea for the business is literally the precursor to Firefox. Gecko can be refactored in-place, including into Rust if they decided to do so. reply timschmidt 2 hours agorootparent> Their primary leverage is unique features and functional adblockers, neither of which is impacted by the layout engine. Yes, unique features like being written in a memory safe language and depending on memory safe implementations of image and video decode libraries are exactly what I care about in an all-knowing sandbox which touches network services and runs untrusted code on my computer. > And again, you're taking away resources from something that is already behind right now. Disagree. You're talking about every Mozilla project that's not Servo. Firefox/Servo development is Mozilla's core competency. One which they've abandoned. reply dralley 2 hours agorootparent>depending on memory safe implementations of image and video decode libraries are exactly what I care about in an all-knowing sandbox which touches network services and runs untrusted code on my computer. What does that have to do with Servo? Firefox has already been doing those things and continues to do them [0], they don't need to do them in Servo first. We are specifically talking about the utility of rewriting a layout engine from scratch, rather than putting more resources into evolving Gecko - including rewriting small parts of Gecko in Rust incrementally. >Disagree. You're talking about every Mozilla project that's not Servo. Firefox/Servo development is Mozilla's core competency. One which they've abandoned. They obviously haven't abandoned it. It's not like they cancelled Gecko development too and are rebasing on top of Blink. Again, this is all just a philosophical debate over whether rewrites or refactors are more effective when it comes to the most core component of the browser. [0] https://github.com/mozilla/standards-positions/pull/1064 reply timschmidt 2 hours agorootparenthttps://4e6.github.io/firefox-lang-stats/ Do you see those red and orange and green pie slices? 40% of the code. There, be memory errors. Approximately 70% of all errors in that code will be memory safety related and exploitable. Fixing it looks like developing Servo. Don't want to take my word for it? How about the US Department of Defense: https://media.defense.gov/2022/Nov/10/2003112742/-1/-1/0/CSI... reply dralley 1 hour agorootparentMozilla continues to add new Rust to Firefox, despite discontinuing the Servo project. A big parallel rewrite is not the only possible approach to writing Rust. \"Fixing it\" does not have to look like Servo. In fact doing more incremental rewrites will improve the situation shown in that chart much faster than waiting 10 years for parity before doing the replacement. I'm not responding further until you actually read and understand what I'm saying instead of flailing at a strawman. reply timschmidt 1 hour agorootparent> A big parallel rewrite is not the only possible approach to writing Rust. A rewrite is the only way to convert the codebase to Rust or any other memory safe language. Whether that happens in parallel, piecemeal, or both at the same time is down to how well you use a version control system and structure your code. As has already been shown by sharing Servo code with Firefox. A full rewrite is particularly useful with Rust, as the language wants you to structure your code differently than most C/C++ is structured. Doesn't make sense not to have one going if that's the plan. If you're going to rewrite the whole thing anyway, might as well do it in an idiomatic way. reply bogeholm 52 minutes agorootparent> A full rewrite Things You Should Never Do, Part I [0] [0]: https://www.joelonsoftware.com/2000/04/06/things-you-should-... reply timschmidt 21 minutes agorootparentThings Mozilla have done a few times.[0] All with valid reason. 30 years without a rewrite is pretty rare for any piece of software. 0: https://en.wikipedia.org/wiki/Netscape_Navigator steveklabnik 50 minutes agorootparentprevGoogle has demonstrated that writing new code in a memory safe language still significantly improves the safety story of a codebase, even while keeping around the old code. Full scale rewrites are not the only option. reply timschmidt 12 minutes agorootparentYes, every line of C/C++ you can replace with a memory safe language in a critical codebase like a browser improves it's safety story. Which is exactly the reason replacing all of it is so attractive. But just to offer another point, I also still run into memory leaks and other performance issues in long-lived Firefox processes which, based on my experience with Rust, would be unlikely to be a problem in a functional Servo. It'd be nice to have a browser I don't have to occasionally kill and restart just to keep Youtube working. WD-42 2 hours agorootparentprevTrue, I guess investing in the future viability of your core product doesn’t fit with how modern corporations are run. They should just keep launching bookmarking and vpn services that might make money RIGHT NOW. reply dralley 1 hour agorootparentDoes anybody argue that Google is negligent for not doing a complete rewrite of Blink, rather than doing the same incremental software development as everyone else? Did they suffer from their choice to use WebKit in the very beginning rather than do their own thing? reply WD-42 1 hour agorootparentWhy would they need a rewrite? Blink is the market leader. Meanwhile daily driving gecko becomes a worse experience by the hour. reply IshKebab 10 hours agoparentprevI agree. Pretty much the main distinguishing feature of Firefox is that it doesn't use WebKit/Blink. Crazy of them to discontinue working on their own engine's future, especially when it was already yielding results. I'm trying Firefox on Android at the moment and it's noticeably less snappy than Chrome. I wonder if Servo would have changed that. reply soganess 9 hours agorootparentI am always in bizarro world when I read comments like this. I swear I perceive firefox for android to feel snappier and smoother than chrome. To be clear, I am not trying to claim you are wrong! It is the common wisdom that chrome is faster on android. But I swear, scrolling and page loading just feels faster on firefox. My only guess is the adblocker, but I think firefox is faster than brave, so who knows. I wonder if other have a similar experience and can shed some light on the situation? reply IshKebab 9 hours agorootparentI just double checked by closing all tabs, killing the browser and then loading Hacker News. It's about 0.5s on Chrome and 1s on Firefox (roughly). That's a big difference. Firefox is probably faster for ad-heavy sites, but it definitely isn't for sites without obtrusive ads. reply soganess 2 hours agorootparentThat is the kind of results I would expect! Plus V8 (if that is still the JS engine in chrome) has always been faster. And android is probably the priority target for chrome at this point. But on the occasional times I launch it (chrome), it just feels like it bogus down more often doing basic things. Someone once suggested that, paradoxically, it is slower because I don't use it very often. Something to do with ART and how the AOT compilation work on android reply throwaway48476 7 hours agorootparentprevFirefox on android is a million times faster than chrome because it has an ad blocker. reply IshKebab 8 minutes agorootparentNot all sites have egregious ads. Chrome is faster for those. reply usrnm 10 hours agoparentprevWhy? They already have their own browser engine, what would they gain by creating another one? It's a browser company, not a Rust promotion company, from this point of view the decision was completely logical. reply bpye 10 hours agorootparentMany components from Servo (like WebRender) ended up being useful in Firefox. That alone seems like a pretty reasonable motivation to continue? reply dralley 3 hours agorootparentIt's a lot easier to swap out the renderer or the CSS engine for a new one than it is the whole core of the browser engine. Mozilla decided that replacing Gecko as-is was not reasonably likely to actually happen, and that further efforts towards Servo would be better made by continuing to evolve Gecko. reply usrnm 10 hours agorootparentprevNo, it doesn't? It shows that Gecko can be updated and modernised without the need for a complete rewrite. reply ptman 7 hours agorootparentBut a rewrite may allow for an overall better architecture. reply usrnm 7 hours agorootparentIs there anything wrong with Gecko architecture? So wrong that it's a major obstacle and cannot be changed? I don't know anything about its internals or browser engines in general, so I can't really comment on that, but what I do know from practice is that a complete rewrite is a very expensive and a very risky project, that will fail more often than not. There should be very serious arguments behind it, something a lot more serious than the age of the codebase or a new and shiny programming language. reply cassepipe 4 hours agorootparentIt is a very big project and has a big potential for the classic double-free, use-after-free, null-dereferencing, one-off index errors etc. which they designed Rust to get rid of in the first place. I believe they had such a bug some months ago and that it was quite serious. reply torginus 11 hours agoparentprevThis. Especially since it's pretty much supposed to be Rust's flagship project. reply IshKebab 10 hours agorootparentTo be fair, it wasn't really by the time they abandoned it. reply torginus 7 hours agorootparentStill, I think Rust was designed for the style and scale of application that a Web Browser is. Foundational, but not kernel level, highly complex, with a wide feature set, performance is important (but not the most important) and high reliability/maintainability and quality is expected. Building these kinds of apps was commonplace in the 90s/early 2000s: photo editing apps, word processors, IDEs, 3D modeling software etc. Maybe RDBMS count as well. In practice Rust is mostly used by web people to gain clout - rewriting microservices, which are usuallyIn practice Rust is mostly used by web people to gain clout Complete hogwash. reply Certhas 10 hours agoparentprevHonestly though: Why? Large chunks of the most important servo work is in Firefox now. Nobody else maintains even one web engine. What is the importance for the open web of Moz developing two? reply conradfr 10 hours agorootparentDoesn't that mean that those parts they integrated are now maintained and improved twice in separate wasteful efforts? reply nicoburns 9 hours agorootparentNo. Mozilla maintains those parts as part of Firefox (Gecko). Servo imports and syncs back changes. reply cabirum 7 hours agoparentprev [–] Mozilla could run out of money in the middle of engine rewrite, see also: Netscape[1]. I think they just decided to play safe. [1]: https://www.joelonsoftware.com/2000/04/06/things-you-should-... reply LeFantome 3 hours agorootparent [–] Yes, since Firefox IS Netscape, they have some experience with this. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Igalia revitalized the Servo project, an experimental browser engine initially created by Mozilla, by taking over its maintenance in January 2023. - Significant progress includes upgrading dependencies, enhancing continuous integration (CI) and tooling, expanding community support, and adding support for Android and OpenHarmony. - The project has raised over $24,500 in donations, with future goals of becoming a default web engine for Rust applications and developing a full-featured web browser, despite challenges like competition and funding."
    ],
    "commentSummary": [
      "Servo, a browser engine, is experiencing a revival with updates and is now accepting donations through Open Collective and GitHub sponsors. - Igalia, a significant contributor to Servo, is also involved in Chromium development, emphasizing the importance of diverse browser engines. - Despite previous setbacks, such as Mozilla halting its development, Servo remains active, with discussions on the benefits of rewriting in Rust and hopes for future funding."
    ],
    "points": 238,
    "commentCount": 83,
    "retryCount": 0,
    "time": 1736288712
  },
  {
    "id": 42629163,
    "title": "A day in the life of a prolific voice phishing crew",
    "originLink": "https://krebsonsecurity.com/2025/01/a-day-in-the-life-of-a-prolific-voice-phishing-crew/",
    "originBody": "January 7, 2025 16 Comments Besieged by scammers seeking to phish user accounts over the telephone, Apple and Google frequently caution that they will never reach out unbidden to users this way. However, new details about the internal operations of a prolific voice phishing gang show the group routinely abuses legitimate services at Apple and Google to force a variety of outbound communications to their users, including emails, automated phone calls and system-level messages sent to all signed-in devices. Image: Shutterstock, iHaMoo. KrebsOnSecurity recently told the saga of a cryptocurrency investor named Tony who was robbed of more than $4.7 million in an elaborate voice phishing attack. In Tony’s ordeal, the crooks appear to have initially contacted him via Google Assistant, an AI-based service that can engage in two-way conversations. The phishers also abused legitimate Google services to send Tony an email from google.com, and to send a Google account recovery prompt to all of his signed-in devices. Today’s story pivots off of Tony’s heist and new details shared by a scammer to explain how these voice phishing groups are abusing a legitimate Apple telephone support line to generate “account confirmation” message prompts from Apple to their customers. Before we get to the Apple scam in detail, we need to revisit Tony’s case. The phishing domain used to steal roughly $4.7 million in cryptocurrencies from Tony was verify-trezor[.]io. This domain was featured in a writeup from February 2024 by the security firm Lookout, which found it was one of dozens being used by a prolific and audacious voice phishing group it dubbed “Crypto Chameleon.” Crypto Chameleon was brazenly trying to voice phish employees at the U.S. Federal Communications Commission (FCC), as well as those working at the cryptocurrency exchanges Coinbase and Binance. Lookout researchers discovered multiple voice phishing groups were using a new phishing kit that closely mimicked the single sign-on pages for Okta and other authentication providers. As we’ll see in a moment, that phishing kit is operated and rented out by a cybercriminal known as “Perm” a.k.a. “Annie.” Perm is the current administrator of Star Fraud, one of the more consequential cybercrime communities on Telegram and one that has emerged as a foundry of innovation in voice phishing attacks. A review of the many messages that Perm posted to Star Fraud and other Telegram channels showed they worked closely with another cybercriminal who went by the handles “Aristotle” and just “Stotle.” It is not clear what caused the rift, but at some point last year Stotle decided to turn on his erstwhile business partner Perm, sharing extremely detailed videos, tutorials and secrets that shed new light on how these phishing panels operate. Stotle explained that the division of spoils from each robbery is decided in advance by all participants. Some co-conspirators will be paid a set fee for each call, while others are promised a percentage of any overall amount stolen. The person in charge of managing or renting out the phishing panel to others will generally take a percentage of each theft, which in Perm’s case is 10 percent. When the phishing group settles on a target of interest, the scammers will create and join a new Discord channel. This allows each logged on member to share what is currently on their screen, and these screens are tiled in a series of boxes so that everyone can see all other call participant screens at once. Each participant in the call has a specific role, including: -The Caller: The person speaking and trying to social engineer the target. -The Operator: The individual managing the phishing panel, silently moving the victim from page to page. -The Drainer: The person who logs into compromised accounts to drain the victim’s funds. -The Owner: The phishing panel owner, who will frequently listen in on and participate in scam calls. ‘OKAY, SO THIS REALLY IS APPLE’ In one video of a live voice phishing attack shared by Stotle, scammers using Perm’s panel targeted a musician in California. Throughout the video, we can see Perm monitoring the conversation and operating the phishing panel in the upper right corner of the screen. In the first step of the attack, they peppered the target’s Apple device with notifications from Apple by attempting to reset his password. Then a “Michael Keen” called him, spoofing Apple’s phone number and saying they were with Apple’s account recovery team. The target told Michael that someone was trying to change his password, which Michael calmly explained they would investigate. Michael said he was going to send a prompt to the man’s device, and proceeded to place a call to an automated line that answered as Apple support saying, “I’d like to send a consent notification to your Apple devices. Do I have permission to do that?” In this segment of the video, we can see the operator of the panel is calling the real Apple customer support phone number 800-275-2273, but they are doing so by spoofing the target’s phone number (the victim’s number is redacted in the video above). That’s because calling this support number from a phone number tied to an Apple account and selecting “1” for “yes” will then send an alert from Apple that displays the following message on all associated devices: Calling the Apple support number 800-275-2273 from a phone number tied to an Apple account will cause a prompt similar to this one to appear on all connected Apple devices. KrebsOnSecurity asked two different security firms to test this using the caller ID spoofing service shown in Perm’s video, and sure enough calling that 800 number for Apple by spoofing my phone number as the source caused the Apple Account Confirmation to pop up on all of my signed-in Apple devices. In essence, the voice phishers are using an automated Apple phone support line to send notifications from Apple and to trick people into thinking they’re really talking with Apple. The phishing panel video leaked by Stotle shows this technique fooled the target, who felt completely at ease that he was talking to Apple after receiving the support prompt on his iPhone. “Okay, so this really is Apple,” the man said after receiving the alert from Apple. “Yeah, that’s definitely not me trying to reset my password.” “Not a problem, we can go ahead and take care of this today,” Michael replied. “I’ll go ahead and prompt your device with the steps to close out this ticket. Before I do that, I do highly suggest that you change your password in the settings app of your device.” The target said they weren’t sure exactly how to do that. Michael replied “no problem,” and then described how to change the account password, which the man said he did on his own device. At this point, the musician was still in control of his iCloud account. “Password is changed,” the man said. “I don’t know what that was, but I appreciate the call.” “Yup,” Michael replied, setting up the killer blow. “I’ll go ahead and prompt you with the next step to close out this ticket. Please give me one moment.” The target then received a text message that referenced information about his account, stating that he was in a support call with Michael. Included in the message was a link to a website that mimicked Apple’s iCloud login page — 17505-apple[.]com. Once the target navigated to the phishing page, the video showed Perm’s screen in the upper right corner opening the phishing page from their end. “Oh okay, now I log in with my Apple ID?,” the man asked. “Yup, then just follow the steps it requires, and if you need any help, just let me know,” Michael replied. As the victim typed in their Apple password and one-time passcode at the fake Apple site, Perm’s screen could be seen in the background logging into the victim’s iCloud account. It’s unclear whether the phishers were able to steal any cryptocurrency from the victim in this case, who did not respond to requests for comment. However, shortly after this video was recorded, someone leaked several music recordings stolen from the victim’s iCloud account. At the conclusion of the call, Michael offered to configure the victim’s Apple profile so that any further changes to the account would need to happen in person at a physical Apple store. This appears to be one of several scripted ploys used by these voice phishers to gain and maintain the target’s confidence. A tutorial shared by Stotle titled “Social Engineering Script” includes a number of tips for scam callers that can help establish trust or a rapport with their prey. When the callers are impersonating Coinbase employees, for example, they will offer to sign the user up for the company’s free security email newsletter. “Also, for your security, we are able to subscribe you to Coinbase Bytes, which will basically give you updates to your email about data breaches and updates to your Coinbase account,” the script reads. “So we should have gone ahead and successfully subscribed you, and you should have gotten an email confirmation. Please let me know if that is the case. Alright, perfect.” In reality, all they are doing is entering the target’s email address into Coinbase’s public email newsletter signup page, but it’s a remarkably effective technique because it demonstrates to the would-be victim that the caller has the ability to send emails from Coinbase.com. Asked to comment for this story, Apple said there has been no breach, hack, or technical exploit of iCloud or Apple services, and that the company is continuously adding new protections to address new and emerging threats. For example, it said it has implemented rate limiting for multi-factor authentication requests, which have been abused by voice phishing groups to impersonate Apple. Apple said its representatives will never ask users to provide their password, device passcode, or two-factor authentication code or to enter it into a web page, even if it looks like an official Apple website. If a user receives a message or call that claims to be from Apple, here is what the user should expect. AUTODOXERS According to Stotle, the target lists used by their phishing callers originate mostly from a few crypto-related data breaches, including the 2022 and 2024 breaches involving user account data stolen from cryptocurrency hardware wallet vendor Trezor. Perm’s group and other crypto phishing gangs rely on a mix of homemade code and third-party data broker services to refine their target lists. Known as “autodoxers,” these tools help phishing gangs quickly automate the acquisition and/or verification of personal data on a target prior to each call attempt. One “autodoxer” service advertised on Telegram that promotes a range of voice phishing tools and services. Stotle said their autodoxer used a Telegram bot that leverages hacked accounts at consumer data brokers to gather a wealth of information about their targets, including their full Social Security number, date of birth, current and previous addresses, employer, and the names of family members. The autodoxers are used to verify that each email address on a target list has an active account at Coinbase or another cryptocurrency exchange, ensuring that the attackers don’t waste time calling people who have no cryptocurrency to steal. Some of these autodoxer tools also will check the value of the target’s home address at property search services online, and then sort the target lists so that the wealthiest are at the top. CRYPTO THIEVES IN THE SHARK TANK Stotle’s messages on Discord and Telegram show that a phishing group renting Perm’s panel voice-phished tens of thousands of dollars worth of cryptocurrency from the billionaire Mark Cuban. “I was an idiot,” Cuban told KrebsOnsecurity when asked about the June 2024 attack, which he first disclosed in a short-lived post on Twitter/X. “We were shooting Shark Tank and I was rushing between pitches.” Image: Shutterstock, ssi77. Cuban said he first received a notice from Google that someone had tried to log in to his account. Then he got a call from what appeared to be a Google phone number. Cuban said he ignored several of these emails and calls until he decided they probably wouldn’t stop unless he answered. “So I answered, and wasn’t paying enough attention,” he said. “They asked for the circled number that comes up on the screen. Like a moron, I gave it to them, and they were in.” Unfortunately for Cuban, somewhere in his inbox were the secret “seed phrases” protecting two of his cryptocurrency accounts, and armed with those credentials the crooks were able to drain his funds. All told, the thieves managed to steal roughly $43,000 worth of cryptocurrencies from Cuban’s wallets — a relatively small heist for this crew. “They must have done some keyword searches,” once inside his Gmail account, Cuban said. “I had sent myself an email I had forgotten about that had my seed words for 2 accounts that weren’t very active any longer. I had moved almost everything but some smaller balances to Coinbase.” LIFE IS A GAME: MONEY IS HOW WE KEEP SCORE Cybercriminals involved in voice phishing communities on Telegram are universally obsessed with their crypto holdings, mainly because in this community one’s demonstrable wealth is primarily what confers social status. It is not uncommon to see members sizing one another up using a verbal shorthand of “figs,” as in figures of crypto wealth. For example, a low-level caller with no experience will sometimes be mockingly referred to as a 3fig or 3f, as in a person with less than $1,000 to their name. Salaries for callers are often also referenced this way, e.g. “Weekly salary: 5f.” This meme shared by Stotle uses humor to depict an all-too-common pathway for voice phishing callers, who are often minors recruited from gaming networks like Minecraft and Roblox. The image that Lookout used in its blog post for Crypto Chameleon can be seen in the lower right hooded figure. Voice phishing groups frequently require new members to provide “proof of funds” — screenshots of their crypto holdings, ostensibly to demonstrate they are not penniless — before they’re allowed to join. This proof of funds (POF) demand is typical among thieves selling high-dollar items, because it tends to cut down on the time-wasting inquiries from criminals who can’t afford what’s for sale anyway. But it has become so common in cybercrime communities that there are now several services designed to create fake POF images and videos, allowing customers to brag about large crypto holdings without actually possessing said wealth. Several of the phishing panel videos shared by Stotle feature audio that suggests co-conspirators were practicing responses to certain call scenarios, while other members of the phishing group critiqued them or tried disrupt their social engineering by being verbally abusive. These groups will organize and operate for a few weeks, but tend to disintegrate when one member of the conspiracy decides to steal some or all of the loot, referred to in these communities as “snaking” others out of their agreed-upon sums. Almost invariably, the phishing groups will splinter apart over the drama caused by one of these snaking events, and individual members eventually will then re-form a new phishing group. Allison Nixon is the chief research officer for Unit 221B, a cybersecurity firm in New York that has worked on a number of investigations involving these voice phishing groups. Nixon said the constant snaking within the voice phishing circles points to a psychological self-selection phenomenon that is in desperate need of academic study. “In short, a person whose moral compass lets them rob old people will also be a bad business partner,” Nixon said. “This is another fundamental flaw in this ecosystem and why most groups end in betrayal. This structural problem is great for journalists and the police too. Lots of snitching.” POINTS FOR BRAZENNESS Asked about the size of Perm’s phishing enterprise, Stotle said there were dozens of distinct phishing groups paying to use Perm’s panel. He said each group was assigned their own subdomain on Perm’s main “command and control server,” which naturally uses the domain name commandandcontrolserver[.]com. A review of that domain’s history via DomainTools.com shows there are at least 57 separate subdomains scattered across commandandcontrolserver[.]com and two other related control domains — thebackendserver[.]com and lookoutsucks[.]com. That latter domain was created and deployed shortly after Lookout published its blog post on Crypto Chameleon. The dozens of phishing domains that phone home to these control servers are all kept offline when they are not actively being used in phishing attacks. A social engineering training guide shared by Stotle explains this practice minimizes the chances that a phishing domain will get “redpaged,” a reference to the default red warning pages served by Google Chrome or Firefox whenever someone tries to visit a site that’s been flagged for phishing or distributing malware. What’s more, while the phishing sites are live their operators typically place a CAPTCHA challenge in front of the main page to prevent security services from scanning and flagging the sites as malicious. It may seem odd that so many cybercriminal groups operate so openly on instant collaboration networks like Telegram and Discord. After all, this blog is replete with stories about cybercriminals getting caught thanks to personal details they inadvertently leaked or disclosed themselves. Nixon said the relative openness of these cybercrime communities makes them inherently risky, but it also allows for the rapid formation and recruitment of new potential co-conspirators. Moreover, today’s English-speaking cybercriminals tend to be more afraid of getting home invaded or mugged by fellow cyber thieves than they are of being arrested by authorities. “The biggest structural threat to the online criminal ecosystem is not the police or researchers, it is fellow criminals,” Nixon said. “To protect them from themselves, every criminal forum and marketplace has a reputation system, even though they know it’s a major liability when the police come. That is why I am not worried as we see criminals migrate to various ‘encrypted’ platforms that promise to ignore the police. To protect themselves better against the law, they have to ditch their protections against fellow criminals and that’s not going to happen.” This entry was posted on Tuesday 7th of January 2025 06:41 PM A Little Sunshine Latest Warnings The Coming Storm Web Fraud 2.0 800-275-2273 Allison Nixon Aristotle autodoxers Coinbase Crypto Chameleon Discord domaintools Lookout Mark Cuban Okta Perm Shark Tank Star Fraud Stotle telegram Trezor Unit 221B voice phishing",
    "commentLink": "https://news.ycombinator.com/item?id=42629163",
    "commentBody": "A day in the life of a prolific voice phishing crew (krebsonsecurity.com)237 points by todsacerdoti 19 hours agohidepastfavorite76 comments EvanAnderson 1 hour agoMy parents' independent gas station in rural western Ohio (in a town of sub-1000 population, albeit on a state route that sees significant commuter traffic) was targeted for a voice phishing scam over the last week. A caller left voice messages to multiple recipients (we're not sure how many, but it seems like at least double-digits) purporting to be the gas station and asking to settle-up unpaid bills via credit card over the phone. I didn't get to hear any of the callers, unfortunately. The call-back number they left wasn't the gas station's number, nor was the caller ID the gas station's number. At first I felt like it was probably a small-time local scammer. Then I thought about how close we are to being able to run this entire scam using fully automated means (including voice assistant software and an LLM to talk to the callers, probably with a human in the loop for handing exceptions). I assume we'll see a rash of these kinds of scams targeting local businesses once the tool kits to run them become widely available. The idea of building up the automation to run that scam sounds like fun. I wouldn't actually do it but somebody with fewer moral scruples absolutely will (or, rather, probably already has). reply ge96 34 minutes agoparentIt is neat like with Twilio you can produce that audio file for the voicemail with XML but yeah I have no drive to screw someone over myself reply ttul 15 hours agoprevI run a cybersecurity company and I’ve had drinks with Krebs at various events over the years. He’s the real deal, digging up dirt on the people who ruin everything for everyone and risking his life in the process for a minuscule payoff. I don’t know why he does it; I suppose it’s just the journalist’s passion. A really nice guy in person too. reply bostik 12 hours agoparentAs a testament to his effectiveness at digging out the various online scammers, Akamai \"had to\" boot Krebs off of their service - the criminal gangs wanted him and his website out of the picture, and directed enough DDoS volume to overwhelm Akamai's ability to handle the load. IIRC Google intervened and offered to put him behind their shield system. Which I think tells more about Akamai than anything else. (Krebs's website address resolves to a Google network space.) In a fit of irony, even sometime after that event, Krebs's website still sported Akamai's DDoS protection service ads. reply donavanm 10 hours agorootparentUnless you have direct 1p knowledge Im very skeptical of framing that as a capability or capacity problem (“had to” “overwhelm” etc). Im very confident it was purely an effort vs benefit discussion. Which isnt too hard when the benefit is an intangible good will. Ive worked for a very large CDN, and Ive both unilaterally removed a customers access and involved in even more awkward “inviting them to use another provider more suited to their use case” discussions with account managers, PMs, legal, etc. There are a multitude of unsurprising reasons those things happen, even for credible and legitimate paying customers. It was _never_ because we were “overwhelmed.” However attracting a high operational burden or cost burden would certainly play in to the _business decision_. As a trivial example a transparently online gambling site with nominal jurisdiction somewhere difficult in asia may generate very legitimate traffic and even pay their $20 or $200 bill. But that revenue isnt worth the cost of scaling up our network edge all across the AP for unmetered junk bits directed at their distribution, burning goodwill with peers when _their_ network gets blown up, or driving more operational and security load as their gambling site competitors employ more novel and bigger attacks. Simply put not all business is worth it, and you dont have to accept all customers. Part on reasonable terms when possible and apply by relevant laws. Thats the actual obligation. reply bostik 9 hours agorootparentWhile I don't have immediate first-person knowledge, the event and decisions were widely reported at the time. https://www.zdnet.com/article/krebs-on-security-booted-off-a... -- note the quote, in particular https://www.theregister.com/2016/09/26/google_shields_krebs/ -- \"could no longer shield the site without impacting paying customers\" Krebs's own post from the time does not reference the business decisions, only the technical aspects: https://web.archive.org/web/20160922124922/http://krebsonsec... reply sim7c00 4 hours agorootparent\"without impacting paying customers\" reply dylan604 2 hours agorootparentprevEvery company I've worked for has certain clients/customers that the company would (for various reasons) be better off financially to no longer have those clients/customers. At some point, those internal conversations become much less awkward as every realizes the reality of the situation. Those companies that had to undergo bidding processes usually fixed the glitch at that time by making very noncompetitive bids. reply greentxt 2 hours agorootparentEven worse in health insurance. reply timcobb 3 hours agorootparentprevThis all makes sense. But then since Google is not a benevolent entity either, why did Krebs make sense as a customer for Google and not for Akamai? reply tokioyoyo 3 hours agorootparentVery good PR that will get shared in the groups like this one, where some of us are in decision making tables for purchasing such products? reply bragr 11 hours agorootparentprevTo be fair to Akamai, they were providing their services to Krebs free of charge. reply bostik 10 hours agorootparentSure, as a business decision it must have made perfect sense at the time - Akamai had bigger (paying) customers to protect. But that doesn't make the optics around it any less terrible. The message they were telegraphing with their combined actions was effectively: \"We protect some of the largest corporations on the planet... but do not have the resources to keep an individual journalist and blogger online. Your business could be next.\" Whoever made the decision to pull service to Krebs should have also thrown their weight around to get those ads off of Krebs's website, because the compound outlook must have been hideous. (How do you get your ads off of a website without causing any more animosity? You quickly renegotiate an exclusivity deal and then choose not to run any ads at all on it.) reply bravetraveler 6 hours agorootparentprevHeavy is the head that wears the crown (or offers mitigation services advertised on a cybersecurity website) If Akamai can't (or won't) serve Krebs, I'm not sure I would want my business to pay them. reply anitil 15 hours agoparentprevI didn't realise Krebs was a person, I thought it was a collection of people using a unified moniker. To your point though, we're lucky to have 'unreasonable' people like him, I know I don't have the courage reply SnorkelTan 14 hours agorootparentHis first name is Brian. That’s his picture at the top. I can’t think of any other groups or organizations that have the persona of a single person. Can anyone point to an example? Genuinely curious about this. reply sangnoir 33 minutes agorootparentI'm not parent, but at some point McAffee could refer to either the person or the company in the past. Whenever I read a Wolfram blog post that floats to the HN frontpage, I'm never certain if the post is entirely the effort of just Stephen Wolfram, or is a group effort. reply recursivecaveat 11 hours agorootparentprevI always thought Krebbs was a cybersecurity firm organized like a lawyer's or dentist's office, where there is one senior person on the cover but they are rarely involved with individual pieces of work. Crazy to learn it is just one person actually, they do a lot of good work. reply mikebike 13 hours agorootparentprevDoes \"Nicolas Bourbaki\" count? https://en.wikipedia.org/wiki/Nicolas_Bourbaki reply devin 1 hour agorootparentThis is IMO an excellent example, and the one I came to post. reply philipwhiuk 7 hours agorootparentprevPreviously, McAffee (John McAfee) and Norton (Peter Norton). reply dylan604 2 hours agorootparentWhat about Kaspersky? reply philipwhiuk 25 minutes agorootparentAs it happens, Kaspersky was founded by three people, two of which are Kaperskys. For my mind they also haven't really traded on the 'individual security hero come to save you' person(which Norton definitely did in the early years). reply tough 1 hour agorootparentprevWe don't know but some argue Banksy could be a team effort by now, part of the allure of anon work reply 7speter 30 minutes agorootparentSame with Shakespeare, though that might’ve been disproven reply drfuchs 40 minutes agorootparentprevMavis Beacon. reply Physkal 12 hours agorootparentprevI always felt Banksy was a collective of artists. reply have_faith 8 hours agorootparentBanksy is one person but he does have a team that executes most of his projects for/with him. reply jillyboel 8 hours agorootparentHow do you know? reply nickjantz 1 hour agorootparentThere’s video of the creation of the death of a phone box piece in his documentary and it was indeed a team of a few people. reply have_faith 4 hours agorootparentprevThere's multiple interviews from over the years with people that worked very closely with him, including the person who was his publicist for a long time (or a very similar role, I forget exactly). You can obviously decide that it's all part of some long game of deception but it's just not needed. No one is trying to arrest him here and they haven't seriously wanted to for over a decade, he's become an institution like baked beans. reply MrMcCall 5 hours agorootparentprevWell, for one, he can't film himself installing, e.g., fake artworks in the greatest museums in the world, as he did. And, for those who don't know, he is from Bristol, England, the home of the band Massive Attack. I've been digging their music lately, especially their songs with the late Sinead O'Connor. reply anonym29 3 hours agorootparentprevThe \"Tyler Durden\" author on ZeroHedge. reply saghm 2 hours agorootparentUsing that name specifically has a bit of a different connotation than a generic one with no previous association like \"Brian Krebs\", though. If anything, it would be _more_ surprising to find out that someone going by the name Tyler Durden was just a single, regular person rather than something else going on. reply Scoundreller 13 hours agorootparentprevI think it’s on the down low when that’s done. reply geoduck14 6 hours agorootparentprevAnything Elon Musk owns? reply dbtc 12 hours agorootparentprevtrump reply sollniss 10 hours agoparentprevHe also doxxes random people just because their tool got abused as malware. There's a German community donating thousands to cancer research each year because \"fuck Krebs\" (Krebs means cancer in German). reply nilsherzig 8 hours agorootparentPr0s js crypto miner? reply burningChrome 1 hour agoprev>> In Tony’s ordeal, the crooks appear to have initially contacted him via Google Assistant, an AI-based service that can engage in two-way conversations. This type of scam has been going on since the early 2000's. Back in the day when I was a fresh faced high school kid working for a mom and pop wireless shop, criminals would use the NAD rely system to call dealers like the one I worked for. They'd offer credit card payment for phones without any service on it ask for it to be mailed to a PO Box. Back then, companies like Verizon subsidized their phones so to buy a phone without any service on ran $500+ and we rarely, if ever sold phones without service on it since that's how me made our money. As soon as a new model phone would come out, it was like clockwork. We'd start getting relay calls everyday for about a week. Once they figured out we weren't a mark, they'd stop. Kind of interesting thieves are just utilizing newer technology for the same type of scam. reply somerandomqaguy 1 hour agoparentTrue the underlying scam is the same, but the operating costs have gotten quite a bit cheaper. Before one person could only call one target at a time, today with a good SIP trunk a single person can target thousands of numbers a day and not even have to be present. It can be just a background task running on their desktop while the scammer goes to their normal 9 to 5. reply ChrisMarshallNY 17 hours agoprevI have gotten a few of those \"Apple\" phishing attempts. They really look legit. My Apple ID got compromised, many years ago, and people try to use it, from time to time. However, I am pretty up on the state of my accounts, so I won't follow up on them. The only people who ever call me, from Apple, are the Developer Support folks, and that's usually to castigate me, for stepping on some soft spot, or in response to me reaching out to them. I totally ignore calls from numbers that I don't know; a rare privilege. reply eru 17 hours agoparent> I totally ignore calls from numbers that I don't know; a rare privilege. When I am not totally busy, I usually accept them and put myself on mute and put the phone down. They typically waste a minute saying 'hello, hello?' before hanging up, while I keep working. (Alas, I get a lot of spam calls.) reply IncreasePosts 16 hours agorootparentYour method probably leads to more calls since your number will be marked as active if you pick up reply suprfsat 16 hours agorootparentThey usually spend a minute cursing my mother in a language I don't understand, but they aren't organized enough to note that my number is a huge waste of time. reply eru 15 hours agorootparentprevOccasionally, when I'm bored, I actually tried to engage with them, but they immediately hang up, when they notice I don't speak Mandarin; and my attempts at Nihao haven't convinced anyone so far. For context, I'm in Singapore, and I suspect the vast majority of these spam calls are manned by PRC people. reply alisonatwork 12 hours agorootparentI get tons of these on my Canadian VOIP number, even I don't live in Canada. I can't decide if it's because they know a Mandarin phishing will hit 5% of Canadians so it's worth the effort to spam everyone or if it's because they know who I am and that I can speak passable Mandarin, which is somewhat creepier. reply passwordoops 7 hours agorootparentCanadian here who can barely get past ni hao... My voice mail from SPAM tends to be Mandarin, so I think it's a shotgun blast to the 5% reply pavel_lishin 58 minutes agorootparentAs an American with a Texas area code, I noticed a wave of Chinese-language† spam - I recall reading that it was some sort of scam involving threats of deportation, maybe? But it settled down after a few weeks. Maybe their targeting got better, or maybe enough word got around the Chinese-speaking community to make the scam unprofitable. † - I have on idea which language specifically was being spoken. Probably Mandarin, but how would I know? reply Scoundreller 13 hours agorootparentprev> and my attempts at Nihao haven't convinced anyone so far. Try some Nihao’s and then say you’ll go get grandma or something. You’re just the child answering the phone for your immigrant parents that always forget that the call is on hold. reply eru 11 hours agorootparentThey immediately hang up. And I don't sound like a child on the phone. Funnily enough, I am an immigrant parent myself here. reply Scoundreller 13 hours agorootparentprevI have two numbers in the same area code, one work and one personal. I mess with them on the personal line but never the work. (Ok, that’s slightly different than answering vs not). Informally, I don’t see a difference and this is after years of this hilarious activity. reply hoseja 7 hours agorootparentprevI think if you pick up but are silent it's still (mostly) fine. reply paul7986 16 hours agorootparentprevI would think those who answer the calls are automatically placed on a list as this person answers and your number is sold as such. Personally I have the \"Silence Unknown Numbers,\" feature on my iPhone always toggled on. All unknown ..not in my contacts already..I never hear or see calling.. I might see I missed their call but my mind ignores missed call. Overall if I dont know you well your not in my iPhone contacts ..getting to know new folks they are given my Google voice number which is only for texting. reply pavel_lishin 57 minutes agorootparentMy phone number is already on multiple lists like that; I get a minimum of three spam phone calls a day. I don't think that answering or not-answering is going to make a significant dent. > Personally I have the \"Silence Unknown Numbers,\" feature on my iPhone always toggled on. All unknown ..not in my contacts already..I never hear or see calling.. I might see I missed their call but my mind ignores missed call. I have a young child, in school and after-school activities; I don't want to risk missing a relevant phone call, as well as phone calls from actual doctors & such who need to get in touch with me. (And I can't easily whitelist every phone number some given office/person might end up using to reach me.) reply avh02 11 hours agorootparentprevI'd love to do this but too often a call is made by an unknown number to me in response to an action, e.g i requested a dishwasher repair via email, i was called to schedule it by the contractor it was assigned to by my landlord. If i ignored that call it's likely a game of chasing them back up and potentially navigating PBX systems, etc reply eru 9 hours agorootparentI find that caller-id works pretty well for these kinds of expected unknown calls for me. But that might just be a Singapore thing? (Or perhaps it's an Android thing, and Google looks up the number? Not sure.) reply chatmasta 5 hours agorootparentprevThey can leave a voicemail. reply renewiltord 11 hours agoparentprevI just have a number with a rare area code and then block everything from that code using NumberShield, the iOS app. I usually have a few voicemails to delete but I don’t really notice the calls. I do have to laugh at security, though, since many banks and trading companies just call you direct. I’ve definitely received incoming calls that I hesitate about not continuing. Fortunately, I’m not too confident in my skill to detect a phisher so I always go online to find the official account to call. If they can redirect my call then I’m doomed but often it’s exactly a completely normal call. They were just calling to make sure the wire I set up was intentional. Come on, dude! reply IG_Semmelweiss 14 hours agoprevThere are so many non-techy folks that are getting run over by phishers. If tech workers can also be targeted, the rest really have no hope. I really wish someone would make movies or enticing thriller series out of these post-mortems. There are some good stories to be told, plus it would help the most vulnerable to be better prepared.. reply acomjean 14 hours agoparentWe’ve lost control of the telecom system. The fact you can’t trust caller id and bad actors aren’t banned still astounds me. reply doix 12 hours agorootparentYep, we need the equivalent of DMARC, DKIM, and SPF for the telecom system. We solved it for email, feels like we should be able to solve it for telecom. I really hate any system that relies on the telecom system for any sort of verification. I hate every website/app/whatever that doesn't let you disable SMS verification as a \"backup\". So many places that offer (and even force) 2FA just let you bypass your authenticator with SMS verification. reply ceinewydd 11 hours agorootparentThis exists. https://en.wikipedia.org/wiki/STIR/SHAKEN reply dqv 8 hours agorootparentAnd it does work, despite what people will say. My carrier blocks outbound phone calls from caller IDs of number we don't own. The next step will be to for carriers to start refusing calls that don't pass attestation. reply pavel_lishin 50 minutes agorootparentMy carrier is GoogleFi, and I still get several phone calls a day with my cellphone's area code as the incoming number. (At least, it makes it easier to ignore those calls. I really wish I could program my phone to automatically reject any calls from that area code if it's not in my phone book.) reply dredmorbius 8 hours agorootparentprevIt exists. It's utterly ineffective to the scale of attack. reply dcrazy 2 hours agorootparentIt’s still being deployed. Or more precisely, it’s now mandatory and the service providers which haven’t implemented it are in the process of being forcibly removed from the PSTN: https://natlawreview.com/article/fcc-cracks-down-are-you-rea... reply ForHackernews 2 hours agorootparentprevThe FCC is fixing this: https://www.fcc.gov/call-authentication reply voidpointer 8 hours agoprevThe relative ease with which called-IDs can be spoofed seems to be one of the major \"tools\" with which scammers can gain the trust of their victims (or trick other systems into believing that they are the victim). Most of the non-technical folks I know will also more or less blindly trust a caller-ID. Fortunately, many scammers (at least here in Europe) are still calling you claiming they are interpol following up on your Paypal account being breached whilst a +233... number shows on your phone. reply nottorp 56 minutes agoprev> Included in the message was a link to a website that mimicked Apple’s iCloud login page — 17505-apple[.]com. So... the main culprits are the idiots that hide the page URL in the name of user friendliness? reply jmward01 12 hours agoprevI am glad this kind of reporting happens but I am sad it is needed. This type of crime is violent in nature. I would rather be mugged than have this happen to me. Being mugged just gets you hurt but this can destroy you and your family. reply dcrazy 2 hours agoparentIt’s worth pointing out the incongruity of calling online theft “violent in nature” and then directly comparing it to mugging, which works off the threat of implied violence. You clearly understand the difference between violence and mere deceit. The fact that this isn’t a violent crime is probably relevant to its popularity, since recruiters don’t have to filter for people who are willing to resort to violence in the face of resistance. reply fortran77 1 hour agoprevFor now, much of this can be avoided by always hanging up if you receive a call from google, apple, etc, and then--if you really thing there's something going on--contact them via an official way documented on their website. Of course, they try to catch people off-guard as they did Mark Cuban. When I tell my bank or broker if I should get a call that I'm going to hang up and call back on their main number, they always understand and support it. reply joeyagreco 17 hours agoprevSome of these tactics are really clever. reply ForHackernews 2 hours agoprev> KrebsOnSecurity recently told the saga of a cryptocurrency investor named Tony who was robbed of more than $4.7 million in an elaborate voice phishing attack. > Stotle’s messages on Discord and Telegram show that a phishing group renting Perm’s panel voice-phished tens of thousands of dollars worth of cryptocurrency from the billionaire Mark Cuban. > Cybercriminals involved in voice phishing communities on Telegram are universally obsessed with their crypto holdings, mainly because in this community one’s demonstrable wealth is primarily what confers social status. It is not uncommon to see members sizing one another up using a verbal shorthand of “figs,” as in figures of crypto wealth. Seems like this is all players playing each other. Does this stuff also affect normal people who have real money in the bank and not digital Chuck E Cheese tokens? I don't think my 401k provider has a one-click \"bankrupt yourself\" button. reply flerchin 17 hours agoprev [–] Seems like a lot of work and upfront capital. I suppose the VC ride is truly over. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Apple and Google have issued warnings about phone scams, yet a group called \"Crypto Chameleon\" exploits their services to conduct phishing attacks, resulting in significant financial losses for victims like cryptocurrency investor Tony, who lost $4.7 million.",
      "The phishing gang uses sophisticated methods, including phishing kits that mimic authentication pages and fake Apple support calls, with roles such as Caller, Operator, Drainer, and Owner to execute their scams.",
      "Despite internal betrayals, these cybercriminal groups continue to thrive on platforms like Telegram and Discord, with a focus on targeting wealthy individuals, particularly those with cryptocurrency assets."
    ],
    "commentSummary": [
      "A rural Ohio gas station fell victim to a voice phishing scam, where scammers impersonated the station to solicit credit card payments over the phone using fake caller ID. - The incident underscores the potential rise of such scams with the use of automation tools, emphasizing the need for improved security measures. - Cybersecurity journalist Brian Krebs, known for uncovering online scams, experienced difficulties when Akamai ceased DDoS protection for his site, prompting Google to provide support, highlighting ongoing cybersecurity challenges."
    ],
    "points": 237,
    "commentCount": 76,
    "retryCount": 0,
    "time": 1736293903
  },
  {
    "id": 42626964,
    "title": "Streets GL – 3D OpenStreetMap",
    "originLink": "https://streets.gl/#47.35245,8.50958,21.25,42.00,459.10",
    "originBody": "Streets GL",
    "commentLink": "https://news.ycombinator.com/item?id=42626964",
    "commentBody": "Streets GL – 3D OpenStreetMap (streets.gl)212 points by faebi 22 hours agohidepastfavorite46 comments folli 9 hours agoIn case anyone is interested in 3D Terrain simulation (specifically for GPS tracks, e.g. hiking, skiing): I'm working on https://cubetrek.com Source is here: https://github.com/r-follador/cubetrek reply npteljes 2 hours agoparentFantastic project and front page. I'm sorry I have nothing to add, I'm just amazed be the effort, and how smooth, fast and straightforward it all is presented. reply antman 8 hours agoparentprevVery nice! Is there a way to upload GPC files? reply folli 7 hours agorootparentI've never heard of GPC files, so no ;) What kind of format is it? Edit: did you mean GPX files? Yes! And FIT files. reply huesatbri 7 hours agorootparentI’d assume a misspelling of GPX. reply jmkb 15 hours agoprevThis is a beautiful project but unfortunately the OSM data's out of date -- over a year old at this point. Based on https://github.com/StrandedKitty/streets-gl/issues/182 it seems like this might be abandonware. reply reddalo 11 hours agoparentThat's sad, but maybe somebody will see it here and decide to maintain this very cool project! reply NoboruWataya 17 hours agoprevThis is awesome. I didn't realise OSM even had such detailed data on building shapes. As others have noted, outside (or on the outskirts of) major cities the data is quite sparse but central London, for example, looks great. reply justinclift 11 hours agoparentYeah, the OSM data seems really complete for some cities. Co-incidentally, I've recently been putting together some 3D GIS visualisation stuff using OSM data. Try zooming in to pretty much any capital city, then use the right mouse button to rotate the viewpoint so the 3D shows up properly: https://osm.newdash.io That's just a temporary domain and an in-development project, but (to me) it seems to be working pretty well. Can build applications on top of this. reply butz 3 hours agoprevWith textures this looks nicer than demo.f4map.com , but sadly, map data is way too old. And bridges are still a huge issue. With name like \"Streets\" I was expecting to be able to drive around in some sort of vehicle. reply column 10 hours agoprevPress Tab, use WSAD to move and mouse to aim = Discount Flight Simulator! Love it. reply timmg 15 hours agoprevIn case one of the developers is reading, small-ish bug: I increased the vertical field of view \"all the way\" to 120 degrees. When in \"morning\" mode, there is a rectangular \"shade\" over the right bottom corner of the screen. When in \"evening\" mode it is in the left bottom quarter. (At least on my MBP with Chrome.) reply gorbypark 9 hours agoprevThis is awesome! I'm not a gamer but a huge map nerd, however I've always dreamed of a \"real world\" mmorpg. I've taken a stab at learning Unreal to combine with some map data but never got too far. I think this is a better approach. I assume it's using \"regular\" vector tiles and rending things based on that? Like where a map usually has green fill for trees, it's rendering out a patch of trees? Super cool. reply folli 9 hours agoparentThis might interest you: https://cesium.com/learn/unreal/unreal-photorealistic-3d-til... reply fladd 9 hours agoprevDoesn't seem to work in Firefox. You get just a white view. reply Digit-Al 1 hour agoparentSame for me. I get the following messages in the console: Loading failed for the with source “https://streets.gl/js/index.js”. streets.gl:1:940 A resource is blocked by OpaqueResponseBlocking, please check browser console for details. script.js Loading failed for the with source “https://analytics.streets.gl/js/script.js”. streets.gl:1:815 reply pmontra 1 hour agoparentprevI get a CORS error in the console. And a WebGPU error in Chrome. This is a Linux Debian machine. reply cess11 9 hours agoparentprevWorks in FF for me. It's rather resource hungry, maybe an OOM? Judging from RAM consumption I'm guessing it doesn't utilise GPU, but maybe it does and didn't fit in mine. reply matthewbauer 15 hours agoprevVery cool! It looks like it assumes everything is flat, but I bet you could pull in elevation data from OSM as well. reply ygra 11 hours agoparentIt does use elevation data, but does not exaggerate it, I guess. Back when I was working with 3D maps, we noticed that many people liked exaggerated terrain heights better, especially when the terrain is viewed from above and realistic heights looked “flat”. Near where I live it looks fairly close to what it does in real life: https://streets.gl/#48.50063,8.99766,7.25,312.50,135.56 (granted, having added building and roof colors for almost all buildings also helps). reply fitsumbelay 13 hours agoparentprevthe 4 story building I live in is rendered like a basement only dwelling which is actually growing on me the more I look at it ... reply zaik 12 hours agorootparentYou could add a building:levels value to the object in OpenStreetMap, to record the information of how many stories your building has. https://wiki.openstreetmap.org/wiki/Key:building:levels reply ygra 11 hours agorootparentWhile a good idea in general (the StreetComplete app makes this very easy, by the way), this won't help for this app, as the data is from September 2023. Otherwise I'd love to use it more to validate how renderers handle different buildings. F4Map should show the change fairly quickly, though. reply byhemechi 11 hours agoprevThis is pretty cool, but demonstrates why google/apple maps make heavy use of bespoke models vs just the map data, as is demonstrated pretty well by the sydney opera house and harbour bridge This: https://shottr.cc/s/Qxhj/SCR-20250108-r6p.jpeg Apple maps: https://shottr.cc/s/QWES/SCR-20250108-r69.jpeg reply wmlhwl 5 hours agoparenthttps://demo.f4map.com/#lat=-33.8564200&lon=151.2149210&zoom... f4map does a pretty good job, although I'm not sure if they use some extra model here or just render it based on osm data. Sydney opera house seems pretty well mapped and I think you could get something like that from it. But definitely something similar can be rendered. Streets Gl handles complex shapes quite well: https://streets.gl/#52.23135,21.00506,45.00,0.00,1466.87 reply ygra 4 hours agorootparentF4Map has an option \"F4-specific buildings\" which has premade 3D models for certain landmarks, including the opera house. reply qingcharles 15 hours agoprevI like that 11th Century Dover Castle is now an apartment building :) https://streets.gl/#51.12937,1.32037,45.00,0.00,630.67 reply ygra 11 hours agoparentIt's too keen to add windows. This also is a bit annoying with churches. At least garages and sheds don't get windows by default. reply bgirard 15 hours agoprevAll the assets are loading sequentially. I have 100ms of latency to the server so that means several seconds of load time for me. Should be an easy fix to load them in larger batches. reply pixelesque 14 hours agoparentAll the initial assets yes, although once it started pulling in tiles, those seemed to be parallel from then on. It took over 43 seconds to load for me according to Firefox's dev tools Network tab, each image was effectively > 300 ms, but I'm in New Zealand, and the server seems to be in the Netherlands, so that's almost worst-case... reply guax 9 hours agoprevSuper fun to play with the times of the day and check where shadows/light will hit. reply transformi 19 hours agoprevHow is it generated the 3d objects? using CV on the stat-elite data? reply ygra 11 hours agoparentOpenStreetMap has the building outlines (sometimes even parts of the buildings) as well as tagging that specifies • wall and roof color • wall and roof material • roof shape • height (or levels, which typically is then multiplied with 3 m to get a rough height) • starting height (or level), for building parts above ground, e.g. bridges between buildings. This can then get rendered as upwards extruded polygons with a cap in the shape of the roof. There are a few more complicated ways of specifying complex roofs with ridge lines, etc. but few renderers support them and usage is fairly limited. reply stephen_g 19 hours agoparentprevI think it must be using the building outline data (and metadata like the heights) that people have entered, because in my city the inner-city has very complete data but a bit further out, around where I live there aren't any building outlines on OSM and there's nothing in this 3D map apart from just the streets. The result is very impressive given that seems to be the case! Edit: There are some more details on the github about data sources: https://github.com/StrandedKitty/streets-gl reply bloomingkales 17 hours agoparentprevThey have open street data on building height, and roof shape, which lets them render 3d shapes. reply mey 15 hours agorootparentAnd when there is a typo/mistake in the data, it leads to fun results. Like in MS Flight Simulator, https://www.reddit.com/r/gaming/comments/idfogy/a_mistake_ma... reply breadwinner 16 hours agoprevVery cool. Would like to see ability to zoom in & out, and ability to rotate the map. reply ranger_danger 17 hours agoprevPlease check for WebGL support before loading all the assets reply fitsumbelay 13 hours agoprevThis is really impressive. Rather disappointed to see that my condo building is flat despite being 4 floors and a nearly 20 year old renovation of a 100+ year old building. How does stuff like that happen? In the mean time the repo looks like good readin' reply dingensundso 11 hours agoparent> How does stuff like that happen? Simple: nobody entered the height of your condo building into OSM yet. You can change that. Be the change you want to see in the world! reply guappa 11 hours agorootparentMy home has 2 floor if you enter from one side and 4 from the other side. Due to being on a hill. I wonder how to convince osm of that :D reply reddalo 11 hours agorootparentThere's exactly that example on the official OSM wiki page [1]. It's very useful to read their wiki while adding things to the map! OSM and all related projects thrive on users' contributions -- be the change you want to see in the world and start contributing today :) [1] https://wiki.openstreetmap.org/wiki/Key:building:levels reply 3OCSzk 11 hours agorootparentprevYou can tag it accordingly with this guide https://wiki.openstreetmap.org/wiki/Key:building:levels Happy mapping! reply josefx 10 hours agoparentprevWhat I find funny is that they picked tree models that have branches down to the floor. So any tree next to a road blocks at least one lane when its branches should be several feet in the air. reply anthk 10 hours agoprevIt was about time. reply webprofusion 12 hours agoprev [–] Stop what you're doing and use Cloudflare (free) immediately. This is the slowest loading site I've used in years and it deserves better for all the work you've put in. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Streets GL is a 3D OpenStreetMap project enabling users to explore detailed 3D maps, though some users have noted that the OSM data is outdated and certain features, like bridges, have issues.",
      "Users have suggested improvements, including using Cloudflare for faster loading, updating building data, and integrating elevation data to enhance the visual experience.",
      "The project supports GPX and FIT file uploads for GPS tracks, allowing interaction with the map via keyboard and mouse controls."
    ],
    "points": 212,
    "commentCount": 46,
    "retryCount": 0,
    "time": 1736281201
  },
  {
    "id": 42633269,
    "title": "Bye-bye Windows gaming? SteamOS officially expands past the Steam Deck",
    "originLink": "https://arstechnica.com/gaming/2025/01/bye-bye-windows-gaming-steamos-officially-expands-past-the-steam-deck/",
    "originBody": "A gaming OS for US Bye-bye Windows gaming? SteamOS officially expands past the Steam Deck. Legion Go S is cheaper without Windows; upcoming OS beta will allow for personal installs. Kyle Orland – Jan 7, 2025 8:51 PM230 The Lenovo Legion Go S will be the first non-Valve hardware to be officially \"Powered by SteamOS\" Credit: Lenovo The Lenovo Legion Go S will be the first non-Valve hardware to be officially \"Powered by SteamOS\" Credit: Lenovo Text settings Almost exactly a year ago, we were publicly yearning for the day when more portable gaming PC makers could ditch Windows in favor of SteamOS (without having to resort to touchy unofficial workarounds). Now, that day has finally come, with Lenovo announcing the upcoming Legion Go S as the first non-Valve handheld to come with an officially licensed copy of SteamOS preinstalled. And Valve promises that it will soon ship a beta version of SteamOS for users to \"download and test themselves.\" As Lenovo's slightly downsized followup to 2023's massive Legion Go, the Legion Go S won't feature the detachable controllers of its predecessor. But the new PC gaming handheld will come in two distinct versions, one with the now-standard Windows 11 installation and another edition that's the first to sport the (recently leaked) \"Powered by SteamOS\" branding. The lack of a Windows license seems to contribute to a lower starting cost for the \"Powered by SteamOS\" edition of the Legion Go S, which will start at $500 when it's made available in May. Lenovo says the Windows edition of the device—available starting this month—will start at $730, with \"additional configurations\" available in May starting as low as $600. The Windows version of the Legion Go S will come with a different color and a higher price. Credit: Lenovo Both the Windows and SteamOS versions of the Legion Go S will weigh in at 1.61 lbs with an 8-inch 1200p 120 Hz LCD screen, up to 32GB of RAM, and either AMD's new Ryzen Z2 Go chipset or an older Z1 core. Watch out, Windows? Valve said in a blog post on Tuesday that the Legion Go S will sport the same version of SteamOS currently found on the Steam Deck. The company's work getting SteamOS onto the Legion Go S will also \"improve compatibility with other handhelds,\" Valve said, and the company \"is working on SteamOS support for more devices in the future.\" A promised beta version of SteamOS will be released publicly before May, Valve said, \"which should improve the experience on other devices, and users can download and test this themselves. And of course we'll continue adding support and improving the experience with future releases.\" We found this logo hidden deep in an abandoned steel forge. Credit: Aurich LawsonSteam The official launch of the \"Powered by SteamOS\" program has been a long time coming; Valve's Lawrence Yang said as far back as 2022 that the company is \"excited to see people make their own SteamOS machines.\" More recently, Valve confirmed that it was working on official SteamOS support for the Asus ROG Ally. On the Steam Deck itself, the SteamOS experience has been consistently improving over the years thanks to new features and new updates to the Proton compatibility layer that allows Windows-based games to run on SteamOS' Linux core. But SteamOS as a whole has been held back somewhat by the aging Steam Deck hardware, which is not up to the most graphically demanding modern games. Now that SteamOS will be available more widely, players will be able to enjoy the platform's best-in-class interface and gaming features on a wide variety of hardware form factors and power levels. That has to be at least a little bit worrisome to the people at Microsoft's games division, who have gotten used to Windows being the de facto PC gaming solution for decades now. Who knows, maybe Valve CEO Gabe Newell's 2013 prediction that Linux was the future of gaming was simply a little bit too early. Kyle Orland Senior Gaming Editor Kyle Orland Senior Gaming Editor Kyle Orland has been the Senior Gaming Editor at Ars Technica since 2012, writing primarily about the business, tech, and culture behind video games. He has journalism and computer science degrees from University of Maryland. He once wrote a whole book about Minesweeper. 230 Comments",
    "commentLink": "https://news.ycombinator.com/item?id=42633269",
    "commentBody": "Bye-bye Windows gaming? SteamOS officially expands past the Steam Deck (arstechnica.com)194 points by rbanffy 7 hours agohidepastfavorite256 comments VyseofArcadia 5 hours agoI was a console gamer for a long, long time just because I refuse to use Microsoft products, and Linux gaming was fraught except for Battle for Wesnoth and Nethack. Many hours just messing with Wine vs I turn on the machine and the game just starts, no fuss. I've owned a Steam Deck not quite since day 1, I'm beyond delighted now that I can just buy and play Steam games. I think I've thrown maybe one game at Proton that I just couldn't get working. And things like the Heroic launcher have made GoG games (most of which already ran on Linux since they just run via DOSBox) easier than ever. Anyone on the fence about keeping Windows around just for games: unless you play online multiplayer that uses kernel anti-cheat, just make the jump. I promise you, almost every game in your library will just work, and almost all of the rest will work after you set them to run with a specific Proton version. reply liotier 4 hours agoparent> Anyone on the fence about keeping Windows around just for games: unless you play online multiplayer that uses kernel anti-cheat, just make the jump. I promise you, almost every game in your library will just work, and almost all of the rest will work after you set them to run with a specific Proton version. Same applies to Steam on Linux: Proton has opened to us a gaming library of a size we would not have dreamed of a few years before. reply drclegg 4 hours agoparentprevI 100% agree, I bought one to be able to play my library while my wife watches dramas on the TV, and I've been shocked as to how high the compatibility rate is these days. Also, while the competing handhelds are often more powerful, the Deck's trackpads really are a game changer for some games (like Rimworld) reply mmlkrx 3 hours agoparentprevCan you elaborate more on the benefits of the Heroic launcher? From the website I understand it offers the convenience of bundling multiple launchers into one but having only used Steam, I don't fully understand it's benefits. reply drclegg 1 hour agorootparentBasically it handles downloading & configuration of games from some non-Steam providers (e.g. GOG, Epic), and adds them to Steam, so you can launch them like they are Steam games. It's not quite as polished, but it works. reply diggan 2 hours agorootparentprevIf you don't play games from Epic Games, GOG, Amazon or local installs, then there isn't much point to it. If you only use Steam, there isn't really any benefits. reply yayitswei 4 hours agoparentprevWhat if SteamOS included kernel-level anti-cheat? Seems like an elegant solution compared to the current approach of running invasive third-party anti-cheat software. reply out_of_protocol 3 hours agorootparentSteamOS does provide support for common anti cheats (don't know details though), made in collaboration between anti cheat maker and valve, but many games decide to specifically opt out of this support reply VyseofArcadia 3 hours agorootparentprevHow would that work? Valve implements a bunch of kernel modules for each of the different KAC libraries an arbitrary game might use? Both Valve and MS have been making moves to steer game publishers away from KAC. I think the problem will solve itself when the platforms just say, \"you can't do that anymore\". reply matheusmoreira 57 minutes agorootparent> Both Valve and MS have been making moves to steer game publishers away from KAC. Can you elaborate on this? What sort of moves have they been making? > I think the problem will solve itself when the platforms just say, \"you can't do that anymore\". I hope you're right! reply basfo 5 hours agoprevI bought an Asus Rog Ally X on a recent trip to NYC (i'm not from the US, and the steam deck isn't sold on regular retailers). I can't believe how awful the user interface is, they are bascially installing windows 11 in a computer shaped as a portable console, with really small icons and imprecise touch controls, onedrive and microsoft 365 offers, crazy. If you can install Steam OS on it, all those consolized PCs will end up using native Steam OS, and the next step is the living room (console like desktop pcs). Microsoft must be really thinking about an Xbox OS, or at least a native and usable gaming interface for windows 11. Valve is trying to remove microsoft as a dependency for their bussines. And i think microsoft sees valve as it's bigger competitor on the gaming space, not Playstation as everyone thinks, that's why they are going with the \"everything is an Xbox\" ads and rumors about \"third party xboxes\". reply uptown 5 hours agoparentEven the XBOX UI is problematic. I’m stunned that so many games have an additional layer of user onboarding just to get up and running. So many games require that you create an account “typing” your info in an on screen keyboard with an Xbox controller, and verify with email then return to the Xbox and sign in. What should have been a single-sign-on using your gamer tag profile is a clunky fragmented mess. reply nottorp 5 hours agorootparentThe fundamental problem is that games require an account... be it Microsoft, Sony or a game specific account. reply cholantesh 2 hours agorootparentI'm not even sure it's an 'or' at this point; I've recently gone (multiple times) through the surreal experience of having to install a client and register an account having already bought a game through a storefront for which I had to install a client and register an account and which had already ostensibly installed. In the first case it was for a game I had actually previously played before the publisher decided to slap an account on top of its offering, and I decided I didn't care enough to keep going. reply uptown 4 hours agorootparentprevYes. And games are built as multi platform so they don’t want to tie themselves tightly to the XBOX ecosystem. But the UIs for that enrollment are almost always horrible. You can tell they are designed for PCs with keyboards and mice. The scaling is all wrong for a TV based interface. reply nottorp 4 hours agorootparentYou missed the point. They should not require an account. Or are you talking about multiplayer IAP fests? I'm not sure we should call those \"games\". reply ErneX 5 hours agoparentprevThey are: https://www.theverge.com/2025/1/7/24338778/microsoft-xbox-ha... reply basfo 5 hours agorootparentYep, probably that will be the future of xbox, it will become like steam so you can use and play your xbox library on every device. There may be some Microsoft branded Xboxes, like they have the \"surface\" PCs, but will be just another device that runs the xbox client. reply w0m 3 hours agorootparentThat's kind of what they have now; just with console and (every windows machine). Better dedicated mobile hardware would be appreciated. It is nice that GamePass can be used on my Desktop, XBox, or Laptop from my moms kitchen. reply basfo 1 hour agorootparentWell... they want that, but aren't there from my point of view. From the xbox library (including 3rd party games you got on xbox, old 360 games and so on) only a handful of titles are actually playable on pc, around 10% in my experience (those labeled as \"play anywhere\", mostly \"newish\" first party games... 3rd parties want to sell you another copy to play on pc even using the xbox pc client). And the look and feel of \"the platform\" is quite different on pc than the console, i think they will try to make the whole library playable and have the same experience (booting up directly to it or running it as a client on any supported device, similar to steam big picture mode) reply ErneX 4 hours agorootparentprevThat is my guess too, they will still release both a console and a portable but it will be open. Phil Spencer has also talked about allowing third party stores on these future Xbox consoles. reply eigenspace 3 hours agorootparentprevI have zero faith in Microsoft's ability to pull this off in a way that's even close to cohesive. reply hashishen 5 hours agorootparentprevthey should have done this years ago, it may be too late now reply basfo 5 hours agorootparentYep, users try to consolidate their gaming library in a particular platform. If you have 100 games on steam, why you would get the newest fifa-718 on xbox instead of steam, where you have all your games? They plan game pass to get users to the xbox ecosystem, but i'm not sure if it will be enough. reply ErneX 4 hours agorootparentprevAgree. The Steam Deck user experience it’s clearly superior compared to full blown windows. It’s going to be an interesting “battle”. reply jagrsw 5 hours agoparentprevFor gaming, RetroPie is a cool solution (it doesn't require raspberrypi). I have a setup using HP Pro Mini 400 G9 that boots directly into EmulationStation. It's perfect for playing with my kid, covering everything from NES to more recent consoles, and also Steam and Minecraft (via https://github.com/minecraft-linux/appimage-builder/releases) if needed. The offline aspect of non-steam games is a big plus for easier parental management too. reply Pxtl 4 hours agorootparentI've got to say I prefer the raw Retroarch interface over the EmulationStation launcher. EmulationStation is pretty but the seams between it and Retroarch were just too big for me. reply sureglymop 4 hours agoparentprevI highly recommend SteamFork for now! It's very close to upstream SteamOS unlike other derivatives and is easy to install. The experience is just much much better. reply Vt71fcAqt7 4 hours agoparentprevThe issue is they would need to get rid of xbox live and allow installing apps outside their store. PC gamers will not accept having to pay for online and many play games that are not in the xbox store: all valve games, indie games, mods etc. reply Szpadel 6 hours agoprevI very rarely play games, and for long time I had dedicated windows installation just for steam and friends. This was always annoying experience to reboot, hunt for grub, then half year of windows updates until I was able to enjoy my free time. But thanks to proton, I don't remember anymore when was last time I had to do that, everything just works great on Linux ootb including titles like cyberpunk. For people wanting dedicated (console like) gaming machine steam os looks really promising. reply blueflow 5 hours agoparentBonus points: Have Windows ruin your current shooter session by downloading system updates in the background. reply pmarreck 5 hours agorootparentI think they actually fixed that with Gaming Mode or whatever. Even worse when a modal dialog used to pop up over your game... the way Windows used to constantly interrupt whatever you were trying to focus on drove me nuts (this never happened on macOS or Linux) but I think they also finally fixed that reply high_na_euv 5 hours agorootparentprevI cannot think about single time it happened, like seriously Maybe you are using wifi like majority of gamers and blame the wrong thing? reply blueflow 5 hours agorootparentI'm not accepting your implied premise: That Windows is allowed to use the scarce resources in a way that interferes with the User activity. This is independent of where the resource limit is. reply high_na_euv 3 hours agorootparentNo, I suggest that there was no update at all and the lags were caused by wifi reply blueflow 3 hours agorootparentThe update gives itself away by the \"Update and Reboot\" button appearing half an hour later. reply high_na_euv 3 hours agorootparentIf you are sure that it was Windows, then you can also disable it reply dageshi 5 hours agorootparentprevI Pause updates for 7 days and run the updates on a sunday morning Windows gotta update sometime reply danparsonson 4 hours agorootparentWindows used to let the user choose when they wanted to update - that they force it to happen now is a situation entirely of their own doing. reply dageshi 3 hours agorootparentYeah cause people just skipped installs forever. That's the reason. And as I said, you can run it once a week and control when it happens. reply diggan 4 hours agorootparentprev> Windows gotta update sometime For some people, sure. But to force it upon people without any way of skipping it? Kind of disrespectful to control people's computer usage like that when we're talking about \"personal computers\". reply dageshi 3 hours agorootparentPeople are idiots. They will just continue to skip forever and never update then blame Microsoft when their machine gets pwned because it wasn't updated. You're free of course to install another OS on your \"personal computer\". reply diggan 3 hours agorootparentOr, Microsoft could let people chose when to update, so the ones who have to use Windows (like myself) can continue to do so without having to deal with their continued bullshit. reply blueflow 5 hours agorootparentprevHow can i do this? reply dageshi 4 hours agorootparentIt's an option in windows update (for me at least, Win 10) There's a button called \"Pause updates for 7 days\" I run windows update on a sunday then hit the button again when it's done. Works pretty well. reply sumtechguy 5 hours agoparentprev> then half year of windows updates until I was able to enjoy my free time That sums up my experience with the PlayStation and switch. Don't touch them for awhile and 'we are installing updates for the next 2 hours'. I use my pc often enough that it is not an issue. But I have one computer I do not use very often and then it is update city. reply kibwen 4 hours agorootparentAt the risk of defending Nintendo, the software update times on the Switch are notably zippy. I don't think I've ever had it take more than 10 seconds (including the reboot after). That's laudable, and nowhere in the ballpark of, say, Apple, where I once went to lunch and came back to find my laptop still installing updates. reply weberer 4 hours agorootparentprevSwitch updates take less than a minute for me. And this is even after several months of inactivity. reply hn8726 5 hours agoparentprevMy gripe with proton and linux gaming is GPU drivers — on Windows, I have AMD Adrenaline software where I can undervolt the GPU and adjust performance for every game individually. This drastically cuts power consumption, noise and temperature levels, especially for newer games played in 144hz or 4k. Is there a way to adjust the same things on Linux, preferably without restarting anything and messing about in the terminal? reply nikitoci 5 hours agorootparentI’m using LACT[1] which is essentially Adrenaline software. It allows to undervolt/change clock for core, vram; adjust fan speed and all you need. [1]: https://github.com/ilya-zlobintsev/LACT reply nottorp 5 hours agorootparentprevDon't know about amd or a gui tool, but at least for nvidia cards you can just set a power limit from the command line. reply gpderetta 4 hours agorootparentyou should also be able to use Green With Envy. reply jauntywundrkind 1 hour agorootparentprevI spent a couple dozen hours trying to tweak and tune my desktop this spring, get the watts down & see what Linux could do. CoreCtl was the best utility I found for the gpu, even though it left much to be desired & would sometimes work sometimes just not; my memory notably would get locked at max speed sometimes & not be adjustable, taking 20w right away. But it has the obvious knobs; adjusting clocks and mV per each state on each power profile, adjusting fan curves. (Alas my motherboard appears to lack a ton of controls in Linux. I have been having to go into bios to do cpu undervolting and fan speed controls.) More recently, I turned on the newer power tuning utility 'tuned' and it's been amazing. I'd fought down from 140 to 110w on my desktop but it still felt absurdly higher than it should be. Turned on tuned and now it idles at 85w. I haven't tried to sit and tweak it and see what it can do, but my impression is it's not as good at letting users tweak stuff endlessly. But it does do a ton of tweaking itself, and it's smart about adapting - switching to gaming profiles when games start. I could be wrong but it seems like there aren't standards for motherboard management around platform details like fan speed control. And there's many many ways motherboards do things. Where-as gpu's apparently are just much more normative, are tweaked via pretty standard interfaces. Still, a lot is possible. I definitely recommend tuned, as a very all encompassing system tuner. reply matheusmoreira 5 hours agoparentprevThis is great but we must still be careful. Games have come to Linux but so has a lot of the suckiness associated with them. The video games industry wants to own your computer. They don't want you to copy their games or cheat. Taking over your machine is the only way they can possibly hope to accomplish that. Therefore they think nothing of shipping literal rootkits directly to you. This is software whose only distinction from malware is a terms of service buried somewhere that you probably clicked through without reading and technically accepted. There's also the fact they are proprietary software. There's no telling what they are doing. Sometimes there's literal malware in these things. I'm not kidding about this. https://www.vice.com/en/article/fs-labs-flight-simulator-pas... https://www.theregister.com/2016/09/23/capcom_street_fighter... https://twitter.com/TheWack0lian/status/779397840762245124 https://fuzzysecurity.com/tutorials/28.html https://github.com/FuzzySecurity/Capcom-Rootkit I seriously doubt there's any effective way to sandbox them either, they probably need extensive permissions to even work. I wouldn't want to run these things in my personal computer. For these reasons, you might want to set up a completely separate virtualized Linux system just for this purpose. IOMMU and VFIO technologies allow you to map a discrete graphics card to the virtual machine which enables hardware accelerated graphics at near native performance. You might want to consider a dedicated gaming machine. To me it feels like a waste since the hardware is great and should also be used for other things. reply nottorp 5 hours agorootparent> Taking over your machine is the only way they can possibly hope to accomplish that. It's easy. Don't play competitive multiplayer crap. Especially if it's free to play. You get to not waste your time on predatory grindfests, besides not having to install a rootkit. Also stay away from the companies requiring their own account system, like EA, Ubisoft, Rockstar. reply Pxtl 4 hours agorootparent> Don't play competitive multiplayer crap. Especially if it's free to play. Problem is that most good multiplayer online games have gone F2P. I bought Team Fortress 2. Then it went F2P and got overrun with bots. Overwatch also went F2P after I bought it. I like online team shooters, but like many genres it's dominated by the F2P business-model. reply matheusmoreira 4 hours agorootparentprevCompletely agree with you, this is great advice. There are plenty of \"competitive multiplayer crap\" games which I think actually deserve to be botted to oblivion. If you load a game and you see a timer anywhere, it probably deserves it. The timer exists to delay you rewards and resetting it is probably wired to the credit card button. They do this to create reward schedules in order to get players addicted. They want them logging into the game every single day. Automating those silly \"daily tasks\" is an absolutely moral thing to do and I will never fault anyone who does it. They are cures for video game addiction and should be prescribed by doctors. reply nottorp 4 hours agorootparent> Automating those silly \"daily tasks\" is an absolutely moral thing to do and I will never fault anyone who does it. I would. You should just dump the game then. reply matheusmoreira 4 hours agorootparent\"Just dump the game\" is not an easy thing to do when you're addicted. There are people out there who literally wake up at 3 AM to do daily tasks because that's when some idiotic timer resets. It's not something you just \"dump\". I know because I've been there. reply nottorp 3 hours agorootparentHmm I've done my share of wow dailies but even those were boring and i was skipping them. As for free to play games, they're all too expensive. Since you pay for them with your time. reply wqaatwt 5 hours agorootparentprev> They don't want you to cheat Why do you think that is? I’m sure they wouldn’t bother with it if most gamers were fine playing against cheaters. reply matheusmoreira 4 hours agorootparentIt's my computer. I am the god of this machine. If something happens, it's because I will it. I get to read and write arbitrary memory if I want to. It's offensive to me that they even think I shouldn't have this power. Couldn't care less what their reasons are. Cheating? I don't care enough about most games to even consider it. I absolutely do consider it to be my prerogative, however. If they don't like it, they better run their \"anticheat\" nonsense on their computers, where it belongs. If they try to usurp control of my computer, I will do everything in my power to stop it. This is gonna turn into yet another example of illegitimate customers enjoying a superior product while paying customers get treated like dishonest criminals. Legitimate customers get DRM and anticheat rootkits, pirates and cheaters get none of that. Guess which side I want to be on? reply w0m 3 hours agorootparentThe issue w/ competitive gaming (League of Legends, Deadlock, Overwatch) is if you take advantage of your prerogative and cheat, you make the experience worse for everyone else; and it hurts the game itself. The sad state of existence is that if you want to run a multiplayer game, you need to do shitty/controlling things or the game will turn unplayable as soon as it starts to become popular. Games like BG3 however - Who cares if someone is cheating? Just stick to those (great) games if you want more control reply matheusmoreira 2 hours agorootparentIt's unfortunate but at the end of the day they're just video games. They aren't worth sacrificing computer freedom over. We shouldn't end up becoming serfs in the digital fiefdoms of corporations just because people want to play online video games. These companies need to give up and let us play these things on our terms. I'd sooner see multiplayer games disappear than accept this sad state of affairs where our computers come pwned straight off the factory in order to please the so called stakeholders by denying us the freedom to do things that hurt their interests. That's a far more damaging outcome than silly video games becoming unplayable. Cheaters are a small price to pay for freedom and I pay it gladly. Besides, some games should be hurt. Plenty of \"free\" games out there employing literal gambling and drug dealer tactics to get people addicted to their product. Cheaters are doing us all a huge favor by speeding up their demise. reply wqaatwt 40 minutes agorootparent> play these things on our terms Due to some reason a lot of people want to use public servers and matchmaking. Likely they wouldn’t do that if anticheat root kits didn’t exist. I don’t particularly care about games like this either nor do I play them but that’s entirely besides the point. > for freedom and I pay it gladly. That’s great. Again a lot of people feel otherwise. Why do you feel that you have a moral right to impose your personal preferences on them? reply coldpie 1 hour agorootparentprevSo... don't play them? Your reasons for not accepting kernel anti-cheat are completely valid, but so are other users' desires to not play games with cheaters. reply matheusmoreira 1 hour agorootparentI already avoid multiplayer games. It's very rare for me to make an exception. I came here to warn others about these concerns and to cite virtualization with VFIO as a viable solution. I think people should understand what sort of nonsense they're bringing into their computers when they install these games. That's what my original post was about. I didn't mean to start another one of these discussions. We shouldn't have to justify ourselves. They're the ones barging in and usurping control of our computers because of \"cheating\" or whatever. You install some video game and next thing you know you've got some malware exfiltrating private information, taking screenshots, scanning your RAM and disks, making an issue of your developer tools. That's unacceptable behavior, the reason for it is irrelevant. reply wqaatwt 39 minutes agorootparent> ones barging in Well.. no. They are for better or for worse explicitly invited. reply wqaatwt 44 minutes agorootparentprev> I don't care enough about most games to even consider it Well yes, but millions of people feel otherwise. Why do you think that game developers have some nefarious reasons for this and want to usurp your computer? To what ends? I mean sure the current approach is flawed in many ways however without it many multiplayer games couldn’t really exist in their current form i.e. public servers, matchmaking etc. would become near impossible since the experience for most users would become extremely unenjoyable. > pirates and cheaters get none of that True in regards to single player games but I’m not sure how do you think this would work for multiplayer games? How can cheaters bypass the anticheat root kits and still play onlie? reply matheusmoreira 27 minutes agorootparent> Why do you think that game developers have some nefarious reasons for this and want to usurp your computer? Because that is what they do. They think we are all potential cheaters who need to be preemptively stopped and controlled lest we crack their games open and poke at their guts without their authorization. They feel absolutely justified in doing whatever it takes to \"ensure\" their precious game isn't \"tampered with\". It's just what I've come to expect from the copyright industry as a whole. Our computers already come pwned straight off the factory just to appease \"content creators\". We really don't need game companies making the situation even worse. > To what ends? It doesn't matter. The result is we have to install their rootkit to play games we paid for. There are no excuses for that. > many multiplayer games couldn’t really exist in their current form Not a big deal. Maintaining control over our machines is more important. If that's the price I'll pay it gladly. > How can cheaters bypass the anticheat root kits and still play onlie? You'd have to ask them. I'm far from an expert on the subject. I just know that whatever it is that they do no doubt leads to their customers having to put up with a lot less game company malware than they would have needed to otherwise. reply basfo 5 hours agorootparentprevDoes steam run on GNU/Hurd? reply solardev 5 hours agoprevAs a Mac user, I've only been able to watch the Proton improvements from the sidelines. I'm happy to see them but can't make use of them. On the other hand, GeForce Now is what let me get rid of Windows and my gaming desktop altogether. For the supported games, it's a truly superb experience, launching into max graphics with a single click. I don't have to install or upgrade anything (patches or drivers) or worry about hardware obsolescence. It's insanely powerful (RTX 4080 equivalent), has no local heat or noise, and barely sips battery life (compute is all in the cloud). Completely changed the way I game. And this is as someone who grew up on BBS door games and configuring sound blaster and vmem in config.sys. GFN is so so nice and much better than dealing the nightmare that is modern Windows. And a lot easier than managing Proton and WINE too. Nothing beats it for sheer ease of use when I just want to game for a few hours without headaches. reply coldpie 4 hours agoparent> As a Mac user, I've only been able to watch the Proton improvements from the sidelines. I'm happy to see them but can't make use of them. We experimented with macOS support prior to launching Proton, but Apple had spent the previous decade repeatedly kicking Valve and game developers in the nuts[1], which meant no one on the team had any particular passion for it. Between Apple's hostility & unreliability, and the small market share, we decided it wasn't a good use of time, so dropped the idea before launch. You may know this, but CodeWeavers integrates most improvements to Proton into upstream Wine and also into their CrossOver product. So if you use CrossOver, you actually are getting many of the same improvements that are going into Proton. [1] Garbage OpenGL support; killing 32-bit support, which killed a huge chunk of users' Steam libraries; no Vulkan support. reply solardev 3 hours agorootparent100% this is Apple's fault. They never took gaming seriously. I'd hoped that would change with the M series integrated GPUs, but that hasn't been the case lately, with only a tiny number of titles launching with Apple Silicon support. That's on Apple, not anybody else. For what it's worth, I did buy and occasionally use Crossover (thank you!), but only when the game isn't supported on GeForce Now. My M2 Max, even at its best (as in Baldur's Gate 3 running native Apple Silicon code), is still no match for a 4080... not even close. Crossover works well for simple indie games, but AAA games often have more demanding requirements. The extraordinary performance of DLSS (Nvidia's AI upsampling and frame interpolation algorithm) alone makes non-Nvidia GPUs less viable these days. And that's just the performance side. Then, Crossover's UX is a whole other issue. Crossover's Steam takes forever to launch every time (I'm not really sure why this is; never bothered to really look into it). It's never clear to me which proper combination of D3DMetal/DXVK/Esync/Msync to use (I still don't even know what the \"sync modes\" mean). Some games only work with a certain version of Apple's GPT (for others: Game Porting Toolkit, not AI) manually installed, or requiring the bleeding-edge preview version of Crossover, etc. The idea of user-managed \"bottles\" and disks is a lot more complicated than GFN's model of individually vendor-managed sandboxed/containerized games, preconfigured to work right the first time and every time. That's the kind of arcade/console like experience I want these days since it's zero fuss. (But it often does preclude mod and trainer support, as a tradeoff.) Overall... I'm grateful Crossover exists. It's easier to use than Whisky, and Codeweavers contributes a lot of code back upstream. Again, thank you. There are some games I wouldn't be able to play at all otherwise. But most of the time these days, as a working man and not a teenager anymore, I have limited time to spend on gaming. GFN lets me just click a button and play without fuss, vs all the tinkering required of virtualized & emulated games. It's the difference between \"wait a few seconds and I can play at max graphics\" and \"if I tinker with this for a couple hours, it might eventually launch and maybe I can squeeze 30 fps out of it on low-medium\". reply diggan 4 hours agoparentprevMaybe it's because I'm in a shitty location (Spain), but these gaming streaming services never felt good enough for me. I've tried a bunch of them (including GeForce Now and more recently, the PlayStation streaming thingy) and they all introduce too much latency + now the quality of the screen suffer from the same type of quality degradation as YouTube, which just looks fucking horrible. I'd rather play on lowest quality settings and lowest resolution than having to view that sort of compromise on image quality. > And a lot easier than managing Proton and WINE too I can understand wanting to avoid having to manage Wine, prefixes and all that jazz. But Steam + Proton is literally zero management. You install Steam, start it, install the game and hit \"Play\", the only Proton thing you notice is that it download and installs it before the actual game runs. Otherwise there is nothing you have to do as a user. reply ortusdux 3 hours agorootparentAmazon has started pushing their version, which I think is called Luna. I haven't tried it, but if they run an instance out of EU-South-2 in Spain, you might get better results. reply solardev 3 hours agorootparentprevYeah, it's unfortunately only viable if you're near one of their data centers – and even then, some of their international third-party partners have crappy graphics cards and networks. In the US, though, it is an amazing experience, more like streaming in 4K (which it can do) with minimal lag. Even first-person shooters are perfectly playable, as long as you're not trying to go pro. No compression artifacts or visible degradation at all for me. > But Steam + Proton is literally zero management. Absolutely. I had a Steam Deck for a few weeks* and that experience was amazing. Valve did a really really good job there. Unfortunately that's just not how it is in the Mac world :( We have Whisky and Crossover, which are much much harder to use. (* The Steam Deck is a great piece of kit. I eventually sold it only because I already had a Logitech GCloud, https://www.logitechg.com/en-us/products/cloud-gaming/cloud-..., which does the same thing except streamed from GFN and with a bigger/better display than the Deck.) reply petargyurov 4 hours agorootparentprev> shitty location (Spain) Eh? As in, proximity to the streaming server? Otherwise, fiber is prolific in Spain (in major cities at least, to the best of my knowledge). I get 500Mbps down here in Valencia (for just €15 too!) reply diggan 4 hours agorootparent> Eh? As in, proximity to the streaming server? Exactly! I've had symmetric fiber in countless of apartments in Barcelona, with latency to mainstream sites down to something like 2ms at the lowest. But the game companies who do streaming don't seem to cater to Spain (or Barcelona) at all since the latency always been horrible regardless. Have you tried any of them and seen if the latency is shit or not? reply petargyurov 4 hours agorootparentI haven't tried any of them, sorry. reply argsnd 4 hours agoparentprevObviously it’s not initially remotely a plug and play solution but if you install Steam under Game Porting Toolkit you actually can take advantage of Proton and its improvements much of the time, and after the initial setup there’s very little further tinkering needed. reply solardev 3 hours agorootparentThere is a lot of inter-game variability, unfortunately. (I'm a regular tester and contributor to the Crossover compatibility database: https://www.codeweavers.com/compatibility) Some games, especially simple indie titles, run flawlessly. Others won't launch at all. Most mainstream AAA titles launch but with severe lag and/or graphical anomalies. It's a far cry from native, Proton on Linux, or GeForce Now. I wouldn't recommend it except as a last resort. reply basfo 4 hours agoparentprevThe problem with cloud gaming is that isn't available in all countries and it depends a lot on your ISP, so the experience may vary for different people. reply jbombadil 6 hours agoprevThis is amazing news. I wonder if this means we’re going to see a TV/home console with Steam OS soon. I am currently borrowing a friend’s Steam Deck to try it out. It’s absolutely amazing, particularly around starting and stopping gaming sessions. The only thing holding be back from buying it is that the processing power is a couple of years outdated at this point. It still works fine for older AAA games (or newer lighter games), but it can’t keep up with new AAA. Having the option of newer hardware with the Steam OS experience is amazing! reply lucasoshiro 6 hours agoparent> I wonder if this means we’re going to see a TV/home console with Steam OS soon. They existed, and they didn't really take off: https://en.wikipedia.org/wiki/Steam_Machine_(computer). But it was 10 years ago, things are different today reply jsheard 6 hours agorootparentNotably the Steam Machines didn't have any equivalent to Proton, they could only run native Linux games out of the box. Needless to say that didn't work out. reply paxys 5 hours agorootparentProton is a wrapped/enhanced version of Wine, which has been a thing forever. A large chunk of the Steam library worked perfectly fine on Linux before Proton. reply yourboirusty 5 hours agorootparentIt wasn't nearly as streamlined though. Average person wants to just click play on Steam, which is the biggest thing proton brought to the table. reply vanviegen 5 hours agorootparentprev... for modest values of \"fine\". Compatibility, stability and performance have improved immensely the last couple of years, for a large part thanks to Valve! reply jsheard 5 hours agorootparentYeah, WINE may have been around forever but its development was massively accelerated by Valve dumping truckloads of money on CodeWeavers to have them work on fixing games full-time. Plus neither of the two most popular anti-cheat solutions worked in WINE at all until Valve lobbied them to support it. reply Vt71fcAqt7 4 hours agorootparentprev>A large chunk of the Steam library worked perfectly fine on Linux before Proton. Source? My recollection was that it didn't. Wine had awful direct x translation, I'm not sure if it could do dx11 at all when the steam machine came out. DXVK is a proton project and without it few games could actually run at all. reply coldpie 4 hours agorootparent> Wine had awful direct x translation That's not fair. Vulkan didn't exist when the original Steam Machines launched. Wine's Direct3D implementation also had different goals than the DXVK project, such as supporting macOS, older hardware, and non-gaming DirectX uses. reply AndrewDucker 3 hours agorootparentIt doesn't matter what's fair. It's whether it worked or not that counts. It didn't, now it does. That's an awesome positive step. reply coldpie 3 hours agorootparentI don't think it's good to be rude about other peoples' work without a really good reason :) reply Vt71fcAqt7 4 hours agorootparentprevSure, but GP's claim was that the statement \"Notably the Steam Machines didn't have any equivalent to Proton\" is false. reply saidinesh5 5 hours agoparentprevI think the performance thing isn't that big of a deal with the deck yet.. gaming at 720p 30 or 40 fps still works quite well on the device even for recent AAA titles ... You can always install Steam OS clones on Lenovo Legion Go/Asus rog ally etc.. and get a decent experience. Where it is slightly annoying is the anti cheat/the recent Sony shenanigans about their overlay not running in Linux/such issues. But with such huge PC gaming library...i don't think I'll run out of new games to try on the deck any time soon... reply tecleandor 6 hours agoparentprevSeems like the hardware upgrade for the Steamdeck will be very soon... https://videocardz.com/newz/amd-introduces-ryzen-z2-series-c... Edit: although some people say the slide meant that the processor was coming for \"devices like the SteamDeck\", not literally the SteamDeck https://www.pcgamesn.com/amd/ryzen-z2-extreme-announced reply jsheard 6 hours agorootparent> some people say the slide meant that the processor was coming for \"devices like the SteamDeck\", not literally the SteamDeck \"Some people\" being Pierre-Loup Griffais, who works on the Steam Deck at Valve. https://bsky.app/profile/plagman.bsky.social/post/3lf36y66gg... reply marricks 5 hours agorootparentprev> Seems like the hardware upgrade for the Steamdeck will be very soon I doubt it, the Lenovo Steam-OS option is utilizing Ryzen R2 Go, which has the same RNDA architecture as Valve's handheld and pretty similar (though slightly better) specs. There's newer architectures but they chose the same as OG Steam deck for... compatibility? Ease of OS support? Something else? I doubt Vavle would help Lenovo with support on a device they'd shortly eclipse with better specs. reply mizzack 5 hours agorootparent> I doubt Vavle would help Lenovo with support on a device they'd shortly eclipse with better specs. Why wouldn’t they? It more or less locks the user into the Steam store where Valve gets a cut of every sale. That’s their primary income stream. reply exitb 5 hours agoparentprevThis is very achievable today with off-the-shelf hardware paired with ChimeraOS or Bazzite. reply sureglymop 5 hours agorootparentI recommend SteamFork which is a bit closer to upstream SteamOS. It's an install and forget kind of experience. reply VoxPelli 5 hours agorootparentprevNot really the same plug and play experience as a Steam Deck offers though. I personally wish that Framework works with Valve to release a version of Steam OS for their motherboards, so one can retrofit an old Framework laptop motherboard as a TV console. reply exitb 5 hours agorootparentWell, yes, you need to know how to boot up an installer and go through a typical Linux distribution installation process. After it first boots though, it can be handled solely via the Steam UI. reply pbronez 5 hours agorootparentprevI just discovered Bazzite and am excited to try it. Just built a PC for my kid and Windows hates it. Can’t run for 10 minutes without a blue screen. I’d wanted to try an Atomic Desktop originally but thought the gaming support was too weak. Hoping Bazzite gives me stability and the gaming ecosystem… reply Bluecobra 5 hours agorootparentIf you are running into BSODs right off the bat, that sounds like you could have faulty hardware (bad memory, sketchy PSU, etc). reply sk5t 5 hours agorootparentAgreed, it’s worth cleaning contacts and reseating the memory modules to start with. reply preisschild 5 hours agoparentprev> I wonder if this means we’re going to see a TV/home console with Steam OS soon. You can build one yourself pretty easily and just install Bazzite [1], basically SteamOS for generic PCs on it. You can select \"Do you want Steam Gaming Mode?\" \"Yes\" on the download and it will automatically start into gamescope & Steam Big Picture Mode. 1: https://bazzite.gg/ reply pm90 6 hours agoprevI have a steam deck. Its incredibly easy to switch from game mode to “desktop mode” and get a full blown, portable linux device! Its fucking incredible. Gaming has been mostly good. You can trust the steam verification badge. Some games just won’t work but most do. reply phoronixrly 6 hours agoparentLet me expand on that - plug your deck in a laptop docking station connected to your lan, monitor setup, keyboard and mouse and you have a the smooth desktop experience of a not at all shabby laptop! It's amazing! Lately I've taken to using an Arch btw install on an SD card for testing GPU compute stuff on it. It's so smooth... Not enough vram for blender rendering it seems at least for now... reply diggan 5 hours agorootparent> plug your deck in a laptop docking station connected to your lan, monitor setup, keyboard and mouse and you have a the smooth desktop experience of a not at all shabby laptop! It's amazing Just to be clear, this is not even needed, you can use the desktop mode straight up on the Steam Deck without any added accessories, terminal and everything included. Dope for quick maintenance without having to hook it up to anything. Or, if you happen to have a USB-based WiFi antenna with the right chipset, a portable aircrack-ng device :) > Not enough vram for blender rendering it seems at least for now... Doesn't Steam Deck have unified memory and 16GB available? I think it's more of a implementation issue than the amount of VRAM it could theoretically use. reply rjh29 5 hours agorootparentprevOr just buy a laptop, and then you can just play games on it like normal! Amazing :P reply mhitza 5 hours agorootparentI think it's pretty hard to find a laptop with similar performance characteristics, build quality, repairability and firmware support at the steam deck price point. For one. On the other hand portability is also great, and so is the additional software customization on top. Having an easy slider for custom TDP is a great feature. reply LorenDB 5 hours agorootparentNot to mention that by purchasing a Steam Deck, you are helping Valve financially support many opensource projects. Valve invests in the Linux kernel, Proton (and by extension Wine), various KDE software, the Fex emulator for gaming on ARM systems, and more. reply cesarb 5 hours agorootparentprev> I think it's pretty hard to find a laptop with similar performance characteristics, build quality, repairability and firmware support at the steam deck price point. On the other hand, it's much easier to find any laptop at all than the Steam Deck... because the Steam Deck is not yet available in my country, while there are plenty of laptops from many different brands (Dell, Lenovo, Positivo, even Apple) at all price points. reply phoronixrly 4 hours agorootparentprevTrue, but I prefer to optimize my laptop choice for work, not gaming. My point was that the deck does not limit you by its form-factor in any way should you plug it to a display, mouse and keyboard. reply t_mann 5 hours agoparentprevHave you tried Samsung DeX? I find it pretty amazing, you connect your Android smartphone to a docking station, and you have a usable Desktop OS. The only thing holding it back from becoming a full workhorse for eg document editing imho is that most apps will only give you the - substantially nerfed - mobile version. reply DCKing 5 hours agoprevYou don't need to wait for Valve to get this experience today. The HTPC build of Bazzite [1] brings an experience identical to SteamOS to all computers with an AMD or Intel GPU from the past 8 years or so. It works amazingly well and I can't imagine going back to Windows for a PC that is built only for video games. I use it on my \"Gaming HTPC\" (Ryzen 3600, Radeon RX6600, Fractal Design Node 202) and it brings a great console experience to my TV, with access to my PC game library, without being locked into a console ecoystem, and without the enormous cruft and user hostility that Windows has you manage these days. I'm a pretty casual and patient gamer, and for that use case this Steam machine experience is unmatched - despite being built on desktop Linux, it works out of the box and requires zero manual maintenance. For dedicated gaming boxes this Linux user experience is significantly better and easier to use than Windows - we're truly living in the future. [1]: https://bazzite.gg [2]: It's built on top of Fedora and Universal Blue, so under the hood it's different from SteamOS which is built on a custom immutable version of Arch Linux. However, that implementation detail is actually almost totally irrelevant if you want to play games since all software is managed by Steam and Flatpak on both systems. reply JeremyNT 4 hours agoparentI have both a Steam Deck and a (Windows) gaming PC. While the \"happy path\" in SteamOS is truly amazing, there are dark corners where it falls down. Third party launchers (like EA's garbage) are extremely janky. Hardware support in Linux/SteamOS is questionable for exotic peripherals (I have a TrackIR which never worked right, a MS XBox USB controller dongle that requires third party kernel modules, and a HP Reverb G2 which has only preliminary support through third party software). And some types of multiplayer anti-cheat are completely unavailable. Some of this is solvable, some probably isn't. But there's a reason I still keep Windows on the gaming PC - sadly. reply keyringlight 4 hours agoparentprevI've been wondering what the limiting factors are for migrating gamers and I think the larger software ecosystem and cumulative effect of paper-cut issues will cause people to bounce off. Linux and running games under steam/wine/proton is great in the broad strokes, but users will have built up their own collection of tools or ways of doing things they will seek out equivalents for and judge the linux experience as a whole on whether they can do that. Many of the windows applications are very mature compared to linux because that's the ecosystem and audience its had for decades, there's nothing touching Foobar2000 for example (and the UI glitches in wine). Now add in all the other things gamers regularly expect to do, what's needed to accomplish them and how well they do it, overlays, screen recording, using modding tools, etc. It also strikes me with the win10 end of life there's going to be a huge variety of hardware configurations people want to 'just work', in terms of age and which model someone chose in a particular generation. For example support for fan control on my Z270 board doesn't exist, presumably because of the way ASUS made that model. I can appreciate Valve and their direct partners picking their battles on what to support as it's a huge gauntlet to pick up, but I really doubt the needle is going to move large distances and saying \"bye bye windows gaming\" reply kibwen 3 hours agorootparent> cumulative effect of paper-cut issues will cause people to bounce off I disagree, PC gaming has always been rife with papercuts, especially relative to console gaming. The real moat that Windows has is that anonymous-matchmade competitive multiplayer games are decreasingly going to want to run on hardware that supports user freedom. Which for me personally is fine, because I find anonymous-matchmade competitive multiplayer games to be dogwater that I ain't missing, but for a lot of people that's a non-starter. (Disclaimer: proud owner of a Steam Deck which has also served double duty as my desktop machine while I wait for a replacement power supply for my laptop.) reply DCKing 4 hours agorootparentprevIf you are demanding or particular about your gaming experience, then Linux isn't there yet. Compatibility with the very latest AAA titles can sometimes trail behind Windows, anticheat for competitive multiplayer often blocks out Linux compatibility, and you need to adapt to different tools for customizing and surrounding your gaming experience if you're so inclined. What I'm highlighting if you just want to sit down to play some damn games already in your library, especially on a dedicated \"console\" like a handheld or HTPC, then the Linux experience is superior to Windows. And I expect that there's a sizable audience for that. reply hysan 2 hours agoparentprevIs there a distro that brings a console-like experience to systems with Nvidia cards? I’ve also heard about ChimeraOS. How do all the different gaming focused Linux distro compare? reply DCKing 48 minutes agorootparentThe full SteamOS experience is pretty tied up in Linux' open source graphics stack, moreso than regular Linux desktop environments because Valve built it for high performance on the Steam Deck's AMD GPU. Nvidia's proprietary driver has traditionally done its own thing and has been quite incompatible with things targeting the open source stack. So it's hard to replicate the Steam Deck experience on Nvidia, no matter the distro. That said, over recent years Nvidia has made some efforts to improve compatibility. Just a few days ago Bazzite announced a Steam Deck beta image for Turing and later Nvidia cards [1]. It's too early to run though if you want the seamless experience you get on Intel and AMD, and progress mostly depends on Nvidia and Valve, but I hope they get there. [1]: https://universal-blue.discourse.group/t/new-bazzite-deck-nv... reply diggan 4 hours agoparentprevFrom the Bazzite homepage > Bazzite is a cloud native image built upon Seems to be targeting cloud somehow? Very different from SteamOS where everything works offline, except the game purchases/downloads of course. reply DCKing 4 hours agorootparentThey mean \"cloud native\" in the sense that it adopts atomic system updates and containerized application installs, which has been common in \"the cloud\" for years but is much less commonplace in personal Linux installations. Working in this way is a large part of why Bazzite \"just works\". It is also actually exactly how SteamOS works (with some implementation differences under the hood), so SteamOS is \"cloud native\" in the same sense. I do think this marketing is unnecessarily confusing. The dayjob of the original master mind behind Bazzite and Universal Blue is working with cloud systems IIUC, so they find it an important thing to highlight. reply janice1999 4 hours agorootparentprevI think it's referring to that Bazzite uses container technology to build the OS (it's an OCI image). reply akaike 5 hours agoprevThere is no bye to Windows gaming, because the casual PC gamer will still use Windows and won’t bother with SteamOS, and rightfully so, because why bother? If you can’t handle Windows, then certainly you won’t be able to live with Linux as your main system. Windows just works for gaming and all connected devices. SteamOS maybe works well for dedicated handhelds, but I can’t imagine a casual user bothering with Linux and wondering why the newly bought xyz Bluetooth device doesn’t work on it. reply adamtaylor_13 5 hours agoparentThis whole “Windows just works” mantra gets less and less true with every passing day. The number of times I have to fight tooth and nail to stop updates, uninstall edge, nuke bloatware shit installs from orbit, or hell just get my audio to go out the correct output device is astronomical. Windows is no walk in the park. And Linux is easy to use these days. So… sure maybe this is still sorta true. But we’re long past the days of needing to be a hacker to use a Linux OS and it’s only getting better, while Windows is only getting worse. reply akaike 5 hours agorootparentI agree with you partially, but a casual user just doesn’t care or bother. They don’t uninstall bloatware; in fact, I’m pretty sure most don’t even know what it means. Casual users don’t even bother to switch to dark mode or check if their monitor supports more than 60Hz—things like that. That’s also why Apple doesn’t focus on such details. I’m not against Linux; for developers and servers, it’s awesome. But for casual users, I don’t see the appeal. There’s no reason to bother as long as it works—and it does. Do you genuinely think, that a user who complains that Windows is bad and doesn’t work, will be able to install Linux and be happy? First thing someone like that will do, is probably try to execute an exe file and the complain under some YouTube video about it. reply adamtaylor_13 4 hours agorootparentI think the definition of a “casual user” has changed drastically over the last 10 years. This very much used to be true. But most gamers (i.e. the group we’re talking about here) are more than technically savvy enough to run Linux. Most gamers have seen a terminal once or twice and know how to google the solution to common fixes. And yes, I do believe most motivated users (those unhappy with windows) can install Linux. It’s SO easy these days requiring nothing more than a USB and an hour or two of time. reply akaike 3 hours agorootparentThen you have more faith in humanity, or casuals, than I do. :D I mean, this was three years ago, but I don’t think much has changed. https://www.youtube.com/watch?v=0506yDSgU7M&t= Why would anyone want to bother with a terminal just for gaming, a little bit of browsing, casually installing mods, using Photoshop, etc.? I mean, just watch the video, and this is coming from a guy who at least knows one or two things about computers. Plus, don’t forget, Linux also has its quirks, just like Windows, only in different areas. Like not so awesome Nvidia drivers :P reply thefz 5 hours agorootparentprev... nah. I usually remove all unwanted software once and only after a fresh install, and a Windows installation lasts years without maintenance nowadays. reply adamtaylor_13 4 hours agorootparentMy brother in Christ, we must be using different versions of Windows. I have to uninstall crap all the time. :’) reply ghusto 5 hours agoparentprevFunny sentiment to me, because I switched my parents and in-laws over to Linux exactly because of what a hassle Windows is to support. > and wondering why the newly bought xyz Bluetooth device doesn’t work on it It's not the early 2000s anymore, things just work now. reply akaike 4 hours agorootparentWhat exactly is such a big hassle with Windows? Sure, it’s bloated with trash, but again, it’s good and simple enough for everyone to use. Try explaining to a casual user that they can’t execute an exe file or use Photoshop or whatever on Linux without specific workarounds. reply cholantesh 1 hour agorootparentNeither of those scenarios are typical for 'casual users' of the 2020s: at work, most have Windows laptops issued to them that they can't install things onto anyway, and elsewhere they are liable to do most of their computing on a tablet or a phone. There's almost nothing an exe file could do for a casual user that couldn't be done in a browser or mobile app, and most of them understand what an operating system is and that, on some level, there are cross-compatibility issues between them that they may be able to resolve with a tool. >Photoshop or whatever No one who _needs_ Photoshop is a casual user. reply akaike 1 hour agorootparentThat’s not true—these scenarios absolutely apply to casual users, like students or hobbyists who do things like video editing or photo editing. Students, for example, often need Windows-specific software for schoolwork. Even if we ignore Windows-specific software entirely, there are still other pain points: DRM support, HDR support, certain drivers, and even the variety of package managers and ways to install things. You know what I mean—these things are nothing special for us, but for someone who’s just casually gaming or doing some creative hobbies, being forced to use the terminal to, for example, update Nvidia drivers or find a workaround to get an unsupported game launcher to work, can be a total dealbreaker Even something as basic as swapping out PC hardware as a gamer isn’t as seamless on Linux as it is on Windows. That’s a lot of friction for someone who just wants things to “work”. And you know I’m right because if I would be wrong with all these points, we already would have a year of Linux desktops … as it’s being said every year. You underestimate how most people just value ease of use, familiarity and don’t care about freedom and control over a system. Most don’t want to spend their time tweaking or figuring out why something doesn’t work and that’s totally fine. reply detritus 5 hours agoparentprevI get the impression that a load of people are preparing to jump ship once Win10's sundowned and they're faced with having to get rid of a perfectly usable computer to install Win11. I know I am, loathe as I am to move over to Linux, but Microsoft's making its own bed and continuing on its quest to alienate long-term users with 'peculiar' interface and OS changes. That, Mac and Mobile ownership and I do have to wonder what MS's long-term strategy to avoid pissing away Windows Desktop users is, because I can't see it. reply cesarb 5 hours agorootparent> I get the impression that a load of people are preparing to jump ship once Win10's sundowned and they're faced with having to get rid of a perfectly usable computer to install Win11. In my opinion, what will most likely happen is the same thing that happened when Windows XP was retired: nothing at all, people just kept running the same Windows XP they had already installed. That is, people will just keep Windows 10, not caring that Microsoft does not care about it anymore. And, for them, it will work even better, since without constant updates, Windows 10 will become more stable (as in: not changing all the time, not having random automated reboots due to updates, etc). (We might be concerned that, without software updates, the security bogeyman will catch and eat us, but most normal people don't worry about that.) reply mschuster91 5 hours agorootparentprev> I do have to wonder what MS's long-term strategy to avoid pissing away Windows Desktop users is, because I can't see it. Give private users up because you can't extract money from them, but keep milking Office 365, governments and enterprises that Just Can't move away from Windows due to decades of legacy garbage. reply LorenDB 5 hours agoparentprevCounterpoint: LTT recently tried SteamOS on PC[0] and found that even without an official desktop release from Valve, it just works. [0]: https://www.youtube.com/watch?v=tdR-bxvQKN8 reply nebalee 4 hours agorootparentNeat. The printer bit made me roll my eyes, though. They could not get a printer work with the desktop OS that is Windows, and after also unsuccessfully attempting to get it to work on the OS of a _hand-held_ _gaming_ device, which they just showcased as a PC-based alternative to a living room console, their conclusion is \"needs some work\"? I, mean what? Would anybody expect to be able to print from a PlayStation or a Switch? What would you even want to print? reply nzach 2 hours agorootparent> Would anybody expect to be able to print from a PlayStation or a Switch? Interestingly enough PS2 had printer support[0], and so did PS3[1]. But it seems it didn't work very well. [0] - https://www.youtube.com/watch?v=djHWzLZFm7A [1] - https://www.youtube.com/watch?v=_efGW89MyL4 reply gwbas1c 5 hours agoparentprev> SteamOS maybe works well for dedicated handhelds If someone just wants to play games, why pay extra for Windows? Especially if all they are going to do is play games on a handheld / console. reply akaike 5 hours agorootparentMaybe it’s due to peripheral driver support? Not every device works without issues on Linux. Also maybe because of specific mods or tools which only work on Windows. Besides that, currently all the “awesome anti-cheat root kits”, are not supported by Linux. There are a few reasons, but I agree with you that if you use only a handheld, SteamOS probably will work fine. reply pbalcer 5 hours agoparentprevSteamOS is a much more streamlined console-like experience for gaming. Even things as simple as system updates is far less annoying on Linux/SteamOS than it is on Windows. This is especially important in, for example, a set-top box media PC you might want to have for your TV and you don't run every day. And, over time, as SteamOS in its various forms becomes more and more popular, game publishers will be motivated to support it. Many already are, from Steam Deck alone. And, in a few years, it's possible that a \"casual PC gamer\" will actually prefer the far more plug-and-play SteamOS experience versus the Windows one (which, I, for one, highly dislike, but I understand that's a preferences thing). reply TheAceOfHearts 5 hours agoprevThis makes me hopeful that more online games will be pushed to supporting Linux. Most recently it has been very surprising to find out that Marvel Rivals is making sure they support Linux gamers. Unfortunately there's still a lot of high profile games that won't run on Linux, the most notable examples for me being League of Legends and Fortnite. These games have huge communities, and their games are already available on other platforms, so it's not like they're designed with platform exclusivity in mind. Honestly, Linux gaming has been an outstandingly positive experience for me. reply hypeatei 5 hours agoparentJust for those who don't game on Linux: Proton allows a bunch of games to run perfectly on Linux already. The main blocker for games like League of Legends or competitive shooters is the anti-cheat. Most anti-cheats (like Vanguard) run in the Windows kernel so you can't just plop it onto Linux easily. reply jsheard 6 hours agoprevI wonder if Microsoft will start offering discount Windows licenses for gaming handhelds to push back against this, like how they used to offer dirt cheap licenses for netbooks when they were popular. reply basfo 5 hours agoparentEven if it's free, the experience of using windows 11 as a portable gaming device is awful. Microsoft must create a gaming focused version of windows for this kind of devices. There are rumors of an \"xbox OS\", but who knows. reply pmontra 5 hours agoparentprevI wonder if Microsoft will tell Lenovo that the cost of Windows licenses for Lenovo's laptops and desktops will double or more. reply criddell 4 hours agoparentprevI think OEMs get Windows for free if the screen is smaller than 9\". reply mapcars 6 hours agoprevI remember a number of years back when Steam started Proton thing, it didn't look very promising. Since then it evolved into an actual OS, it would be amazing if Valve could pull this off and expand Linux gaming into mainstream devices and maybe even PCs. reply Gormo 6 hours agoparentSteamOS is a Linux distro that originally came out in 2013. Proton is a custom version of Wine that came out later -- Proton has some added features for integration with Steam, but Wine has been around and has been working well for Windows gaming on Linux for decades. reply jsheard 6 hours agoparentprev> Since then it evolved into an actual OS Funnily enough Valve tried to make Steam OS happen before Proton was a thing, for some reason they just expected game developers to port their games to Linux on their own dime. Thankfully they realized that was never going to happen at scale, so Proton was born instead. reply Gormo 6 hours agorootparent> Funnily enough Valve tried to make Steam OS happen before Proton was a thing, for some reason they just expected games developers to port their games to Linux on their own dime. Proton is just a fork of Wine. Wine had already been around for decades, and there were other commercially-supported versions of Wine, like Cedega, long before Proton was around. On top of that, the increasing dominance of off-the-shelf game engines was already making it trivial to \"port\" games to Linux -- in Unity, for example, it's often just a few extra mouse clicks to produce a Linux build in parallel to your Windows build. So lots of game developers did start releasing native Linux versions, and continue to do so. reply jsheard 6 hours agorootparentYes WINE already existed, but SteamOS v1 didn't have it integrated as a core feature like it does today. It was very much intended to only run native Linux titles. WINE also wasn't nearly as seamless for gaming until Valve threw their weight and money behind polishing it, which came after the original SteamOS flopped. reply mapcars 5 hours agorootparentprevInteresting, I guess I missed that reply cube2222 5 hours agoprevThe important part for me > A promised beta version of SteamOS will be released publicly before May, Valve said, \"which should improve the experience on other devices, and users can download and test this themselves. And of course we'll continue adding support and improving the experience with future releases.\" so if you want this to build a custom SteamOS machine, presumably May it is! reply dagw 5 hours agoparentPeople have successfully installed SteamOS on their own hardware using the Steam deck rescue/reinstall image that you can download from Valve. So if you're up for a bit of hacking you can build it today. reply gpderetta 5 hours agoparentprevApparently the SteamDeck recovery images already work well enough as long as the hardware is somewhat similar (i.e. an AMD video card). reply Gormo 6 hours agoprevWasn't SteamOS around long before the Steam Deck? I remember Valve releasing their custom Linux distro back when they were promoting the \"Steam Machine\" concept. reply puzzlingcaptcha 6 hours agoparentYes, but the original SteamOS was built on Debian 8 and ultimately failed to gain market share. Its website is still up https://store.steampowered.com/steamos/buildyourown reply saidinesh5 6 hours agoparentprevBack then it could only run Linux native games... I'm not sure if their old steam OS version even integrated Proton properly back then. The current version (Steam OS 3) is rebuilt from scratch and there are a lot of under the hood changes... reply diggan 5 hours agorootparent> if their old steam OS version even integrated Proton properly back then. \"Proton\" wasn't even a thing that existed back then :) reply gpderetta 5 hours agorootparentWine was a thing and you could run quite a few games, but it wasn't integrated at all: you had to run a separate win32 Steam instance under wine. reply diggan 5 hours agorootparentIndeed, and as a person who always wanted to game on my Linux boxes, it wasn't (still isn't) nearly as good of an experience as Proton, even though Proton is just Wine+patches+other goodies. > you had to run a separate win32 Steam instance under wine. If I remember my trying days during that period, all the Steam games you ran that way (unless hacked around) also shared the same Wine prefix, with all the fun stuff that comes with... reply gpderetta 5 hours agorootparentIndeed. Having to share wine prefixes (or having multiple Steam installs) was a problem. I don't think I bothered running more than a game or two. Proton also include DXVK and VKD3D which weren't a thing at that time (Wine had ok DX9 support, awful DX10/11 and non-existent DX12). reply humptybumpty 5 hours agoprevHere’s the classic clip from 2014 where Linus Torvalds says Valve will save the Linux desktop: https://youtu.be/Pzl1B7nB9Kc?t=309 reply paxys 5 hours agoparentWhat's funny is that in Linus' mind \"saving Linux desktop\" meant that Valve would release a single statically linked binary of a game for Linux and distro maintainers would have no choice but to support that binary, thus ending fragmentation in the ecosystem. Steam was trying exactly that when Linus made the comment, but failed to get any traction (see the first iteration of SteamOS/Steam Machines). Today gaming on Linux is feasible because of Proton, a compatibility layer for Windows binaries. Meanwhile the app distribution and fragmentation problem on Linux is as bad or worse than it was a decade ago. If you asked Linus whether Linux was \"saved\" by Proton, I bet he would have a very different opinion than everyone here. reply pmarreck 5 hours agoprevI've always loved gaming and one thing that never sat well with me was that gaming's home was unofficially on a proprietary OS. It always made more sense to me (particularly from a software preservation standpoint) that the home of gaming should be on an open-source OS. And truth be told, I have in fact encountered a few games now that will no longer run on Windows but will still work great on Wine/Proton. I'm sure there will be more of those over time. reply AdmiralAsshat 5 hours agoprevMy prediction: If the SteamOS version of the portable gaming handhelds start to outsell the Windows variants because of the lower price, Microsoft will probably offer to subsidize the cost of the Windows 11 license in the handhelds to bring their cost to parity with the SteamOS devices, in exchange for a promise from the manufacturer that the hardware line will not include a SteamOS offering. reply npteljes 3 hours agoprevLinux gaming became pretty good on PC too. Steam and Proton was a significant driver here as well, and we now have completely open-source Steam Proton-like solution too, the Heroic Launcher. Can highly recommend, really easy to set up, and adding a Windows game to it is literally just a few clicks. Same with trying different Proton or Wine packages, you just change the package, and try running it, same as Steam. reply tomjuggler 6 hours agoprevHow is SteamOS install different from just installing Steam on Linux? reply eigenspace 5 hours agoparentSteamOS is an immutable Linux distro that is designed designed to be usable without a keyboard / mouse. By default, it boots you into something that's like an optimized version of Steam's Big Picture mode (but with less compositor / desktop environment overhead). From this interface you can launch games, modify all your settings, pull up performance metrics, change refresh rate, change your CPU / GPU TDP, enable FSR or other scaling methods, etc. all using just a controller with no assumptions about you having a keyboard / mouse handy. You *can* launch into a regular desktop environment optimized for keyboard and mouse whenever you need to do certain things on it, but the main focus and value add of SteamOS is the controller friendly interface, and lightweight, resource optimized environment. reply lkramer 15 minutes agoparentprevMostly there isn't any difference. I run steam just fine in Mint, it works about as well as my Steam deck, which is to say very well. I guess there is some optimisation and default settings more aligned to the purpose on Steam OS. reply gpderetta 5 hours agoparentprevBy default it uses a different UI and dedicated \"window manager\" (gamescope), it is not just Big Picture. But you can always switch to a traditional desktop. By far the biggest difference is that SteamOS is an immutable OS. As far as I understand, any additional package (outside of flatpacks) you install will wiped out on an OS update. So everything is fine if what you need is packaged as a flatpack, otherwise you have to hack around. reply chuckwfinley 6 hours agoparentprevMainly packaging and bundling everything together in one drop-in package for every day users to get access to. I've run Linux for 20+ years now, and gotten steam games working a few times over that period of time. Not hard, not impossible, but is probably more than the average Windows user wants to figure out. SteamOS is really about bringing that final experience to the most number of people, and I am really supportive of their efforts! reply surgical_fire 5 hours agorootparentHmm, At least on Mint there is no trickery to get Windows games running on Steam. Install Steam, install game, play it. That is mostly it. Very few games require some fiddling with Proton versions, but I reckon that the fiddling would be the same on SteamOS. reply reddalo 5 hours agorootparentprevMy desktop has dual boot Windows and Linux, but I've never bothered playing games on Linux. Maybe I'll try to dual boot Linux and SteamOS, just to have a \"safe\" and working Steam environment. reply diggan 5 hours agoparentprevThat's basically it, just easier. SteamOS is like Ubuntu, comes with bunch of stuff integrated and tested together for a easier end-user experience, but you could build your own Ubuntu, starting with a base Debian install. Same with SteamOS with Arch (or Manjaro which I think is what they actually use?) reply ekianjo 5 hours agorootparentThey use Arch as a base. reply tuyiown 6 hours agoparentprevNot much different, but there is steam exclusive mode, essentially straight to big picture and nothing else. reply paxys 4 hours agoparentprevPretty much just the UI. SteamOS is better suited for handhelds and TV screens and wherever you don't want a full desktop experience, just a game launcher. reply baal80spam 6 hours agoprevWindows gaming is way, WAY bigger than Steam! reply eigenspace 5 hours agoparent\"way WAY bigger\" is a major overstatement. But even if that were true, SteamOS can handle lots of other non-steam storefronts. For example, I recently went and played some World of Warcraft for old times sake with some friends on Linux. All I had to do was open Lutris which is available through a Flatpak on the SteamOS discovery tool, and then from Lutris you install the Blizzard Launcher. Opening the Blizzard launcher lets one install and play any of their games. reply varnaud 4 hours agorootparentSteam can install and run the blizzard launcher. Add the blizzard-setup.exe using \"add a non-steam game to my library\". Set the compatibility mode to Proton. On first run, it will launch the blizzard client installation. The next runs will just open the launcher. I was able to play Overwatch on an AMD ubuntu laptop flawlessly. reply eigenspace 3 hours agorootparentAh nice, I hadn't tried this but it makes sense! reply dagw 5 hours agoparentprevIs it? A quick search seems to indicate that Steam has 75-80% of the Windows gaming market share. reply paxys 5 hours agorootparentI doubt that. Looking at a random list of most popular PC games, a lot of the top ones aren't on Steam (Fortnite, Minecraft, Roblox, League of Legends, Valorant, Genshin Impact, Rocket League). reply arcxi 13 minutes agorootparentMinecraft famously runs on anything. Roblox works through Sober. Genshin Impact and Rocket League both run through Proton. The rest have kernel level anticheat, unfortunately. reply lizardking 4 hours agorootparentprevSteam is the only place I've ever played Rocket League. Did it move off the platform? reply paxys 4 hours agorootparentYes, since 2020 when the developer got bought out. reply diggan 5 hours agorootparentprevI'm guessing that's based on official and legal purchases, while I'd wager than \"Windows gaming\" is a whole lot bigger than just the legal stuff. Unclear if parent is referring to \"Gaming\" as a whole or just including boring capitalism numbers like \"marketshare\" etc. reply iib 5 hours agorootparentOlder games are usually easier to run on newer versions of linux than windows, in my anecdotal experience. Not sure about cracked AAA titles, I guess that's a bigger part of the \"illegal\" stuff. reply diggan 5 hours agorootparentYeah, that's true. The other week I tried to get Svea Rike II (1998) to run on my desktop. Tried everything with Windows 11 since I thought it would be easier but nope, nothing worked, couldn't even get past the installation. Switching to trying it on Arch with Wine and boom, five minutes later I was up and running. reply moomin 3 hours agorootparentprevHey Gabe, who’s your biggest competitor? Mark Rein? Nah, Peter Sunde. reply hamilyon2 5 hours agoparentprevProton launches my non-steam games on linux just fine. Flawless experience reply solardev 5 hours agoparentprevCite? reply diggan 5 hours agoparentprev\"What do you mean personal computers will change the world? Time-sharing is way, WAY bigger than personal computers!\" reply qiine 5 hours agoparentprevfor now... reply K0nserv 3 hours agoprevI would love for Linux gaming to take off and displace Windows. The current direction is very promising, but there has been roadblocks. Notably, Respawn decided to forbid Linux users last year[0]. 0: https://www.reddit.com/r/linux/comments/1gh3aik/apex_legends... reply tobyhinloopen 4 hours agoprevI have a living room PC I use for couch gaming. I'm waiting for SteamOS to be compatible with it! (nvidia gpu) I'll 100% install SteamOS on there ASAP. reply gerwim 5 hours agoprevLinux gaming is great. Running Bazzite myself, but for proper support you should run full AMD (CPU and GPU). Nvidia drivers seem to be a pain in the arse. reply rockyj 5 hours agoprevI have been using https://cachyos.org/ with the \"Handheld\" build with great success for the last 3 months. Best of both worlds, Arch Linux for work and Steam for gaming (as long as the game is Steam Deck compatible). reply andrewstuart 6 hours agoprevMicrosoft won’t care. Microsoft lost interest in Windows. It doesn’t care what windows users want, it just wants to wring every dime out of it with crap ware and artificial restrictions on hardware so you can’t run it in your perfectly capable old machines. It’s time for better options on the Intel platform. reply internet_points 5 hours agoprevDoes Adobe Lightroom run in proton these days? Would be amazing if one could use SteamOS for photo editing and such. reply selykg 2 hours agoparentdarktable or Rawtherapee will probably run just fine on the SteamDeck! reply ekianjo 5 hours agoparentprevNope reply Tade0 4 hours agoprevThe other day I flipped a switch somewhere in Steam on Linux and games previously marked as \"Available for Windows\" suddenly started installing and, more importantly, working. I dare say this is a game changer. reply iib 5 hours agoprevDoes anyone know if titles like FIFA/FC25 work seamlessly on linux devices with proton / whatever the best gaming software is? I remember Wine having a lot of trouble with past FIFA titles. reply smcl 5 hours agoparentLooks like the last few FIFA/FC games are no-go on the Steam Deck, so I would presume not: https://store.steampowered.com/app/1811260/EA_SPORTS_FIFA_23... https://store.steampowered.com/app/2195250/EA_SPORTS_FC_24/ https://store.steampowered.com/app/2669320/EA_SPORTS_FC_25/ There's a little \"Steam Deck Compatibility\" section that currently says \"Unsupported\". reply nani8ot 5 hours agoparentprevEA introduced their own kernel-level EA anti-cheat which makes it impossible to run their games through wine/proton. reply pbronez 5 hours agoparentprevProtonDB is the place for that information. It reports FIFA23 is Borked [0] but FIFA22 works pretty well [1]. [0] https://www.protondb.com/app/1811260 [1] https://www.protondb.com/app/1506830 reply gpderetta 5 hours agorootparentFIFA22 works well for some values of well: when the EA launcher is not acting up, which happens often, you might still need to set some flags in the config file, which sometimes get reverted. It is true that if you manage to start it at all, it works completely fine. reply archerx 6 hours agoprevI can’t wait for Windows’ monopoly on PC gaming to end after the atrocious windows 11 and them trying to force recall. If Steam OS can run productivity software like 3ds max, maya, photoshop and etc. I will leave windows and never look back. reply bloomingkales 6 hours agoprevI mean, if they add a code editor, terminal, and browser to steam, that should be it (at least for me). I guess I could just use a cloud IDE. There really is absolutely nothing about Windows I use other than clicking games on Steam. Gamers keep Windows relevant and fresh. Once we’re gone, it really will be a legacy OS that people still use because their printer won’t work with shit else. reply eigenspace 5 hours agoparentIt already has a code editor, terminal and browser. You just boot into the desktop mode and they're all there already (and you can install new ones if you don't like the preinstalled ones). It's a full KDE Plasma desktop environment. Plug in a keyboard, mouse and monitor and stash the SteamDeck under your desk and nobody would even know. You can even register these as 'non-steam-games' to your Steam install and then you can open them from the regular non-desktop interface. I do this with Discord and Firefox. reply chuckwfinley 6 hours agoparentprevAs far as I know, you can drop back to desktop mode on SteamOS and be on your way. reply Maakuth 5 hours agorootparentIt's a real KDE Plasma desktop too, not some joke that is just there to check a box! reply phoronixrly 6 hours agorootparentprevYeap, already a fact with the current deck. Just plug it in your laptop's type-c dock and run Desktop mode. reply jimmydoe 6 hours agoparentprevyou can switch to desktop mode once, install all these from its builtin app store, and add them to Steam launcher, then you are good to go. reply bloomingkales 5 hours agorootparentInteresting. How does it handle window management because that’s pretty much the main mechanical workflow of the developer. We are simply typing and alt-tabbing to see the output. reply LorenDB 5 hours agorootparentIt's KDE, so it's got very fully-fledged window management. reply LorenDB 5 hours agoparentprevHopefully gamers don't adopt the Tim Sweeney \"strategy\" to try to fix Windows: https://x.com/timsweeneyepic/status/964284402741149698 reply surgical_fire 6 hours agoparentprevI always theorized that what keeps Wondows relevant is mainly games and Office. It's what keep people stuck there. With gaming quickly moving to Linux, especially with support of Valve, I wonder if there is an opportunity to disrupt Office. While Libre Office suffice for my needs, I understand that it may lack for more professional usage. reply lisp2240 4 hours agoprevI won’t switch to Linux for gaming until there’s a Linux version of Reshade. And, no, vkBasalt is not the same. reply masfoobar 5 hours agoprevFirst they ignore you .. then they laugh at you .. then they fight you .. then you win. This comes from an old Red Hat Linux advert, likely way back to the late 1990s. At the end of the advert it says \"you are here\" which shows an old-style plane (before commercial) about to take off. Point is its just a matter of time it leaves the ground and \"about to win\" Love or hate GNU/Linux, but it has been extremely successful and while not a winner in the desktop field - it has on servers! Many people would never believe Linux getting the popularity it deserves. Of course things are changing - though slowly. Here we have Linux getting the love it needs as a serious gaming system anf Microsoft making some poor decision in the last couple of years especially with Windows. Still a long way to go, especially breaking into the corporate world. Imagine - we could be seeing business laptops/desktops slowing gaining in Linux rather than Windows. That is not going to be easy. Again, it is not about Linux -- but the SOFTWARE. If the Office-space software and tools get more love in the Linux world, Microsoft start to focus away from their Windows platform and be purely about Software/Azure focus. \"You are here\" -- getting closer off the ground! reply openrisk 5 hours agoparentJust yesterday the news was Nvidia's personal AI supercomputer (on Linux). Today the news is about gaming and SteamOS. The \"cloud\" is already for decades overwhelmingly Linux. There are remarkable devices (pun) [1] based on Linux. Whether for fun or serious work, if we connect the dots the future is Linux, or at least Linux-like. Yes, it is a painfully slow journey. People acting as individuals or in organizations have enormous inertia (which is not totally a bad thing- upgrading a Linux desktop is still a dance at the precipice). But the real cause is mostly the extreme centralization and suppression of competition in the broader tech space. This has not helped the adoption of mass-market oriented devices based on Linux. But slowly the wheels of computing history are turning... [1] - https://github.com/reHackable/awesome-reMarkable reply marcodiego 4 hours agorootparentLinux dominates supercomputers for a long time: https://itsfoss.com/linux-runs-top-supercomputers/ reply openrisk 3 hours agorootparentTo the degree the phrase \"a supercomputer in your pocket\" stops being an empty marketing term and reflects real empowerment of users I would expect Linux to get ever more exposed to mass markets. But this development is predicated on non-technical users somehow becoming aware and valuing the agency and empowerment that a FOSS personal computing environment affords them. reply BlueTemplar 58 minutes agorootparentThese days the smartphone is the (very) Personal Computer for a lot of people. Remember how Android is technically Linux ? The risk here is for SteamOS to go the Android way... but I have more faith in today's Valve than in Google of a ~decade ago. At least for now, while Valve is still a private company and Gabe Newell is still leading it and still seems to be in good health and sound mind. Sadly, this will probably only last for a couple of decades at best, but hopefully this is all the time that Linux (and libre software in general) needs. reply quesera 5 hours agoparentprevFYI that quote originates neither from Red Hat, nor from Gandhi (the usual misattribution). Consensus attribution appears to be: Union leader Nicholas Klein in 1914: > And, my friends, in this story you have a history of this entire movement. First they ignore you. Then they ridicule you. And then they attack you and want to burn you. And then they build monuments to you. And that, is what is going to happen to the Amalgamated Clothing Workers of America. reply marcodiego 4 hours agoparentprevhttps://m.youtube.com/watch?v=5EkkMfjetEY&pp=ygUNVHJ1dGggaGF... reply marcosdumay 5 hours agoparentprev> They laughed at Columbus, they laughed at Fulton, they laughed at the Wright brothers. But they also laughed at Bozo the Clown. Just because you are somewhere on that sequence, it doesn't mean you have any chance of moving forward. Also, most things don't follow those steps at all. But yeah, those are the steps that \"disruptive innovation\" follow. The real kind, that the famous book was written about. Not the bullshit kind that people throw around to confuse others. (Also, apparently, Carl Sagan wasn't much of a history nerd, just a physics one. I expected that phrase coming from somebody else. But well, at the time people knew very little about things that were not their specialty.) reply api 6 hours agoprevWindows is really only around in our household so the kids can occasionally play Minecraft with other kids that use tablets. Without that, Windows is gone, and good riddance. It's always 10X more difficult to deal with than either macOS or Linux. reply AdamN 6 hours agoparentMinecraft on iPad is the way to go, then you can be off the Windows train entirely. reply guappa 6 hours agorootparentI thought a self hosted minetest server with the weirdest mods was the way to go. reply ekianjo 5 hours agorootparentprevMinecraft java version works perfectly on Linux. reply Aerbil313 6 hours agoprevDang, never thought Windows dominance will be upended by gaming. reply wtcactus 6 hours agoprevThe article mentions some prices but never presents an apples to apples comparison between the price for a similarly configured device with windows and steamOS. I’m curious to know how much Microsoft charges for these licenses reply SushiHippie 6 hours agoparentThe Legion Go S with Windows will cost $599 and the Legion Go S with SteamOS will cost $499. I suppose those will be the same hardware wise. So a $100 difference https://www.tomsguide.com/gaming/handheld-gaming/lenovo-legi... reply Andrew6rant 5 hours agorootparentWindows version of the handheld will have 500gb more storage than SteamOS. reply jimmydoe 6 hours agorootparentprevwow, this is a good promotion for SteamOS models, as pre-install windows license should cost Lenovo $5 or less afaik. reply nemomarx 5 hours agorootparentI guess part of it is that users vaguely know how much the windows license costs them and will accept that kinda price increase on top for windows? very comfortable margin for Lenovo there reply lvass 5 hours agorootparentPeople may also be under the impression that Microsoft charges a similar price for Windows as they do to individuals. Which is about $180 for Home edition in my country, Brazil. reply solardev 5 hours agoparentprevIt's not really a cost issue. Windows UX sucks on small handhelds. It's just not made for that form factor. SteamOS was. reply Devasta 6 hours agoprevI have a collection of games on steam going back decades, its the only thing keeping me on windows these days; really excited to see this in action. reply surgical_fire 5 hours agoparentI have good news for you, as someone with a 1000+ gaming library spanning 2 decades. Most older games on steam run seamlessly on Linux these days (I have been gaming on Mint for 3 years at this point). Also, with some minor tinkering, you can get games you acquired on GoG (or even some you may have lying around that you pillaged when sailing the high seas) running smoothly using Lutris. If emulation is your thing, Retroarch is a marvel, better than playing on original hardware in many ways, and treat Linux as first-class citizen. The only thing that should stop you is some modern online competitive games that require nasty anti-cheat software. Those games are cancer anyway, and should be avoided. If gaming is holding you back on Windows, I am mostly confident you can unshackle yourself brother. reply ChrisArchitect 6 hours agoprev[dupe] https://news.ycombinator.com/item?id=42625139 reply josefresco 4 hours agoprev [–] Legion Go owner here. I bought a \"Windows gaming\" device specifically because I have a library of games that spans across multiple vendors, not just Steam. Not only does my Legion Go have a decent \"unified UI\" for launching games, I can easily launch Steam in Big Picture mode. Touch on Windows 11 is surprisingly decent. The only downside to all of these new portable gaming are the docks, and the living room experience. \"HDMI out\" usually works (depending on the game), but it's not as seamless as simply hitting power on your PS5/Xbox. I spend way too much time fiddling with graphics settings, and second/mirror/display issues. reply goosedragons 4 hours agoparent [–] You can use games from other vendors on SteamOS no problem. There's tools like Heroic, Junk Store and Lutris that make it really really easy. The only exception is that there isn't AFAIK a way to use purchases from the Microsoft Store, but Epic, GOG, etc. are all fine assuming the game itself doesn't have some underlying issue. SteamOS docking experience generally works pretty well too IME, although we still need a damn controller with dual trackpads that mimics the Deck's layout. reply josefresco 4 hours agorootparent [–] > There's tools like Heroic, Junk Store and Lutris that make it really really easy. The only exception is that there isn't AFAIK a way to use purchases from the Microsoft Store, but Epic, GOG, etc. are all fine assuming the game itself doesn't have some underlying issue. But with Windows I don't need to do any of that - and that's my point. I don't need to install additional apps, configure workarounds or hacks. I turn on my Legion Go, enter my Windows pin and then launch whatever game I want using the built in UI or Steam or Epic etc. Why would I complicate that? reply goosedragons 2 hours agorootparent [–] Did you install Steam and Epic? It's really stupidly easy to install something like Heroic. Like the same level of difficulty as installing Epic. And it will even add games automatically to Steam for you. reply josefresco 1 hour agorootparent [–] My Legion Go has a unified launcher, at this point I don't even know what we're arguing about. The goal is the same, play all my games regardless of platform with minimal hassle. I don't have any issues doing this on my Legion Go with Windows. How would a Steam Deck be a net benefit? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Lenovo's Legion Go S will be the first non-Valve device officially \"Powered by SteamOS,\" indicating a significant shift from Windows for gaming PCs.",
      "The SteamOS version of the device will be more affordable, starting at $500, compared to the Windows version priced at $730.",
      "Valve's plan to release a beta version of SteamOS for personal installs could challenge Windows' dominance in PC gaming, promoting a broader shift towards Linux-based gaming."
    ],
    "commentSummary": [
      "SteamOS is extending its reach beyond the Steam Deck, presenting itself as a credible alternative to Windows for gaming enthusiasts.",
      "The use of Proton, a compatibility layer, enables most games to run efficiently on Linux, although some multiplayer games with kernel-level anti-cheat systems may face issues.",
      "With tools like Heroic and Lutris simplifying access to games from different platforms, SteamOS offers a streamlined, console-like experience, potentially challenging Windows' dominance in the gaming sector as more devices adopt it."
    ],
    "points": 194,
    "commentCount": 256,
    "retryCount": 0,
    "time": 1736337370
  }
]
