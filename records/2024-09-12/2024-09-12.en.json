[
  {
    "id": 41523070,
    "title": "OpenAI O1 Model",
    "originLink": "https://openai.com/index/learning-to-reason-with-llms/",
    "originBody": "body{font-family:Arial,Helvetica,sans-serif}.container{align-items:center;display:flex;flex-direction:column;gap:2rem;height:100%;justify-content:center;width:100%}@keyframes enlarge-appear{0%{opacity:0;transform:scale(75%) rotate(-90deg)}to{opacity:1;transform:scale(100%) rotate(0deg)}}.logo{color:#8e8ea0}.scale-appear{animation:enlarge-appear .4s ease-out}@media (min-width:768px){.scale-appear{height:48px;width:48px}}.data:empty{display:none}.data{border-radius:5px;color:#8e8ea0;text-align:center}@media (prefers-color-scheme:dark){body{background-color:#343541}.logo{color:#acacbe}}Please turn JavaScript on and reload the page.Please enable Cookies and reload the page.(function(){window._cf_chl_opt={cvId: '3',cZone: \"openai.com\",cType: 'managed',cNounce: '43722',cRay: '8c222593084738ac',cHash: '187a48e803e5632',cUPMDTk: \"\\/index\\/learning-to-reason-with-llms\\/?__cf_chl_tk=Qeaa.i2HlBpTVQ0k2BMY6N_ot.uXMuT6rNfWMONVTtE-1726167693-0.0.1.1-4778\",cFPWv: 'g',cTTimeMs: '1000',cMTimeMs: '390000',cTplV: 1,cTplB: 'cf',cK: \"\",fa: \"\\/index\\/learning-to-reason-with-llms\\/?__cf_chl_f_tk=Qeaa.i2HlBpTVQ0k2BMY6N_ot.uXMuT6rNfWMONVTtE-1726167693-0.0.1.1-4778\",md: \"dBi8TgshU.uwZA7hJAxS6UJSN_hUJXTsYNxrVEBDsB4-1726167693-1.1.1.1-GFfu5lCKywFFijVbF3_5INz.3IEEYMq_yVuhxcU887jrV_DCT4pghgY8XzZ8NTL3Mc_x3SXQQyv3pHTOSUYDN6TlZubt3cJ4ykTMjdQe.iy9cbx2J8wt1rCUKd4QQMo7At9OdXerP1M8SdlMxQe6L7VvJ7r_2vBZAKTZu_DBJHtjj3yTFh5tnzbfNYncKDfjkeIv2e5QfDqFL9fMDkv7KcWGeYNTdcYc0.lCGHF3nxddDrzGiD0Am5cCI66LgKasr9dLDEBZ4Gv0Q3nAXPFronZhNQ7Gs9CJAeltzUvlMKvMdBbsAAoS_5y_J7z_uaL8g4qbb02eU1PuSOZQE_Dx1AW8gMDrgSXrxNufnaZd5uYO1BTqV701w79TYCvs1GK_067S0ygR82KDqHNQzSWB.vuwkTXP.CLHJO6masyBi6XZRqKAltqQr4.u.XVKtR1B_CFzsHbIuzHmT4ejiHgFTZuiYB5KE2WfaIcSzWjd8QqgGB16nxWle7zlF1cxosv82kqh4MdMblTA2eVlbdAFL.RHxvgGw2IbILYSPHRBYW5ykr8JM2tCIm104.q8VjCtY33lqzQ39Q2_Z7v2zRBRC30FCk0CH5NZn1sRp0VJviPfiW4Tvxyo7CWwUX_7zSHQhRFnOrNowuZffhG5Kw_5rU77IYl9x6dj_lenN5q7EgKGkkP2T5AFa1f44pA.Lf1IOQY33RV5.4MrfrG5QyVWWiN21wlqQbx1MUEXSqso2YNTVXYJu4JUNJGyDZmLauX.PsQf9GkBtpwlTCyHJGNwL8gfJu41LHf9wdGEY1cElUQYJYn_Z9GSwlMtIzAXrBoFq09PzWUxD9viWg4f14T8tKJ3NsJGWlmeahRzcy1wtcL.6OEriEnunFtl9Vqpiqc3EJ63cthjsdoCXTHMTMb01O48AQiEhUW0ekH92WxMszn5JRlhRJfFRyWRraytT3iRn_CXDyG1PeRvRoC0o8C.o0q4Rvrp1UCQRGyjrgeIJURw.Q9oE6qf7OJI_pnTEQrzGjfAXW.bKc21lFMQ4M6mgssbdv2PvCP_2ScXZUSaIYBZQlQkepE4HI.W0b.gLPGnC_m2x6Up_D.rSnq._zr7v7e41KfzyhoHCIGs.EaMx.Ig8SFwlTCQN4xKj_WwvPJDNtAOHJdr8RUD95FLHK0XP3IuMfspN_xgdUfCjgRp07HG9.exterzQNViagl5alwNFmn385uVA17vT9XG7pSrUJkckODzImqy1xo80mTM..OToTaaRpzSefwvBKxgDGkza8mr5MCt_k2ahr2Faa7QaT7pfbQxqVX4CTj3IA_MAcrHiZSoaaV4gYlYCJXHbOUwjrw4z0kQxxBthjMxKDe4Sr3HYj1L4RM3jabYfdBI7bFQb5nq08bWatrYN5JElQXas_K0Yjfk9_SjAiLQdLoMJ5iJbXCOtTgmhe87Gr1BLNrIGXePFsDZq4foxBLtoOYp1awnOodyUzkjcD2B0zvm5efvL.1xTNxjzqboSRDTkdsct.TZTPkX.TOuuUrqRWaZ9vL14qIr46XEUowr258z94ISd_1XNFuWgiI4OD_MDXejYcUB7qnR5NFEo7OCcZqydH2NEAqbuywff5j6DR_P2iNaWbnkz.46ZCtDGqht6XUoLtFAM9ll3rFJpM4eGjQxNX0ldG4x8r.N4B_ZzpxTkSmcKs0IycMa6FIfkYrjanYymFWyc7_7jtQ5uBdysP0FDSE4d9JX0QNKgmVoIjnpoEnhmWoLaV39YkrGBiSUAos8oa7rmNZeDDXxRu9GCjgZHjsOVc9Onw6N7v5jFSQuph4AUGm83lzbp1ab0NJUlQ_QozKxjkkiXN96ZteOyWF8zI1yob4BkGUjoekFyK85aYogKAdvX2WvExf8STH._dMADnvjVCrKKLZkRo.9WFlVSQMIhIBdGo_fFZpT0i6U.fSDcKu84ohDGWTLm8TLyuASCd.caL9Mi_.UACKX1B7WKJ2DX2pHqvXrcqZt2hmUXpabKygPhADFRVyuPjWzXdLkinwm3jPYXAnPXJAnRgMcWv9YzVcuvHDdZinGYbrzLIKy8BmuK7qT6pkU7AD9YyDBZYXvSRjqrItrkpgotIHAGZiYblXiwGbcml_kSw_Ud51rHYnaLk7Vn2P86kbK6b7igzPLjed7bMq1AB6fzmJag24TqEMD0TaGjWXfqZX5my9X.EyRderMVbi.2x2jPklpxghBFTyuFYuBu9.6IEIalnoLHVY0WeLP6c7oOmILvmi8.mHSrVmY9BnCfIoHJ4RwX1HqAwB6ADc0VPYzYFzOPZT8waEglVqpks7B30b6EQ\",mdrd: \"86TWP2NFBd2z1CNbnxcUo.nkXjW5WXt_XLWAPxtPaWc-1726167693-1.1.1.1-T54Fl_TLx1ld5Z7XKoV4KOHkU4RxXWl7K9PIrRW5WCNTKk.RbGIbJ4xZR1gLxTMTo9lyQRIag7Ld1o65NKdEqIC3uHfU3u_kqU7.QHfU6.hyCoBezsKfcyToq8Iocmaqnj0Fs7BHyZTo6EeZ0UmErH_if52alVyPrWTpbR5lUU1j3B6rCcnjKgHL4DDEGLfnqd5pJP4K5JsTJkf5qdnLd1qrMpu8BoUIe1j5v9qBbLt_SAgv23sGSEp.J5H5PPPc.RWAXa_PXl_BYzLdSfjkFYm8V69eI.ZmeeJPJIrd91NJMvQY.PFQakZcUNbjvKJjU37ityAv2jZkXvZIQZ_6HzScwse4quv.CbCpJVJtl7jdY_frzm78w8z7xGj2ak.Hwu2Z0Z652Ii7mnChMDJ.d59SuevsSqe3K_w283MoLaqslT5e4l8_GMKEjvqnvVEjHBZGHoPMIOjAGpx2kysKbCwe4P.4VWBSiByXBwQZNAnW8CrNTxXPNH3lZQQhXN2161HmTTt0LVZdgfiIjbx2kcSq5PkajY27yNbN5rtTbElrf06hQJTaodPrBKcnnnblmPR3zbHcnWm2cittQllu4Csl73PJb84fY5TqSyEU16BaWEzSe_KBE7fgOvDZa9Zbkm9Hs35wJEvsWuKTVnyH2Sv.UNrMkXYkc3oypNpIK93agxlrtICaxXnjWPkTlirJBshUktDS_Q7Xeco5WzmnPtMq5knlpkvM5KN_qjq2wtzxBM619WAOPfYsRvMdpzaiPyWkVp6lKiNza9PQa9qrxCu7j_EjeF92Z.E8RWYbnIYSyaCdePEzpynMCQrAGB8IZksOMjBPsGYW3F1zwzP5CVQczwJcunQfJ6PLfhC3M5zrJ.oQ5KLDMMmnr.6AUE4fAEUjytKq0g1L3MVqnI6lW3XpIuFb968uNBLogW1ArIhOnEXcnFirdoxhIfXMP8D_JG6.hAfMy0ixhwaJ.ekRtlXWrO_CVgOkWvVNaf_hLrm0frjFebT9F5QadFocQJmmEOy_Ij7bEtreT6QtJoiiphFxib869Osz4qqfURUzUgArzg2MYCH8gD7mJfxD0knCeAH0zFbiqTDkr7eYe4Pen3iGRgOTYvv7edXO5FS72Uwh3jUJBOVfxmbVwG1L1YeI1S_83WG7lwPSBRNZ5goS81HVb.0uziI6iyJwSIVg7vWPI_h_ZChbQAW258vxP_MY1p3l8gRAexQc9hgT.YvOkzyDb4Vaw1mubC9gK5xirWl5B4w7e68DNdrHy0juLpOKvhWcmQiOidbBFwTBs6dzgJtJ_mtsG8qmQghsuvKmlVfFgJAnvgfuz.XRecQYYlJLIvGUi1jf.._2pu4XOxfIwneh68DJ1KCRDx_59GrQn05xXtI89ZUDGiolDxP.dCD6Fer6fQgwErGpGhyHEGtM46W6wD33epEpNBd.QkSgfT.f3dA1alN7R_hMDW1HhAZG16g4I.avHnNzCbdj8tCcRwBiaK_Tc_ZRBTYVCXDSj_bEDUY90L5rXziQLjPhvYvV7jBbMN4kmcmVKybILL8FRZ6CHrrnKQ7NTBj8aJI8AFnwbicSfXpoLPCjBX5erIDhlxwzGMv6Rr0Gid72evJQr9JEcthNDvnOIs4RtH8GhTZwFAk5mFN5Yp1dANwXtxNNGE3zXNrtKWuwkm4Q6r1ITaaoptZTyt2QWj0V9d072DlYlx2iZg9sbfKqUzuOwlw6dNfwFQ7s6ThmUySWPxJ2gwqAHQhRODlNlolKha6j3CX2NwiEsRQLkfyQG8jEbGhsNFlyyvcikCLnxSGgnOZAXyPM6jzxk_5xxhuYITNHkB.R21VmOa89ri7qNwaicCfkyLH0obLPnBMu_33P.cRY_SCzoTggMwkvPMoJrwhoJuk3QGODm_oLEOtVhc_MwO.W3VQh4mkY4yQI_YNgyJ4_jMtLzkwtjUURQVIXOGL2L6Ct0cLdpwkEdgASRlyNE_9_M5S4jKTkVMPcci87dY5bKnuPtfRm57ff80iYdxfYwc14Zsd8VGcPx0nmFI7mRp69cbcsX8PTwM3fUVF7ksME0yggehqD65O38U21A6OMGt8dFFdaKNsi4kbIfiYJ4JsDNHxNi5lFlPjbsp4EJXIOPUZCs9.AZoo36Wm7RazTtP6JL4mJSrkwhxM0Rwz7LbO6xzc3ZSnMtJ8ZBK6Y5aqMN5FdK.cJdRXy8MxjP3srpO7dggMeMLBRv4rJdkBDJCbjWvKGXwu6gMLI7BXd0ILomRHE6SVnsU.M2eOP0J.wIkQQUJftOOKAk_OJb6st3kVXH82D2crXG5_MxOXQfFQBCQ\",cRq: {ru: 'aHR0cHM6Ly9vcGVuYWkuY29tL2luZGV4L2xlYXJuaW5nLXRvLXJlYXNvbi13aXRoLWxsbXMv',ra: 'TW96aWxsYS81LjAgKGNvbXBhdGlibGU7IEdvb2dsZWJvdC8yLjE7ICtodHRwOi8vd3d3Lmdvb2dsZS5jb20vYm90Lmh0bWwp',d: '1eowOdq56SFrWrdOMqcs5VYIm1JJO9VuAYvqJ4sjjRNGljAvyExvm5jUeovKXj3ZsIyjsQIF2KKxmNuA/b1VjX1iBh3kPXf/5aM6PzhSI3zBHhjfkf6KvUpPm/ctQhFAO2S1DD7Ff8CuztrJOOSnKBImIyQ2cBekj3wGTb1w1+w6myjmXppng72/EGa5J1VQ7w9hjPUbCZty+txZUg7S7D7Jw3SwZo2wv/6Zp3timtQvLg3/Q9bLmqLM+xg4LXPFTjHzCKECStYq5K9nz32RzUCmD81UqY2k8oB8W/2/5OqADysI9S1TZRz7ecwUkuvXdgToF1Cic5zDF606xUy1CaZpdBmDroW9yqaTUGE8qLrQgKbB8st1i/WU/alnl+LVd93Y6z3IxQEQXmtRUgkFR1cYc514CqREbV4MrSLLIZ0IgXKATWlve2cYQzsjNs5Hlu3iFNgdnhXE2AzE9aZ3zIB4P+CA2ZQiJWNKput2i/mjECb3Sp+r6vQKvpw5JCHlovOr15GRxDzuje9c7Vsp2M7xW949OacVobEBcb7MTzQwQNR8UeIYriqwt/DFBe2r2Ve0IyB6wuWS07m1ub+cYA==',t: 'MTcyNjE2NzY5My4wMDAwMDA=',cT: Math.floor(Date.now() / 1000),m: 'HbhtWTFhCPcFayhBvutnKM5rD/Xk7ReMqZ0EiA9+foQ=',i1: 'E3S0KIZEiO/s5vqKv3lKLw==',i2: 'F6S1XA6Bdt912nfL/7HncA==',zh: 'ULytyqbUhvezGEhuw7JA3nyKKR78rFU1sFNg5+21X6c=',uh: 'idqvltDEaw6z1eUpAaUFY/6rIUCphTJo6GMHGHVnQbg=',hh: '6DRP17hjzkUmXKzJAms7bA4OqyZ1RY8MCcH+VDleInA=',}};var cpo = document.createElement('script');cpo.src = '/cdn-cgi/challenge-platform/h/g/orchestrate/chl_page/v1?ray=8c222593084738ac';window._cf_chl_opt.cOgUHash = location.hash === '' && location.href.indexOf('#') !== -1 ? '#' : location.hash;window._cf_chl_opt.cOgUQuery = location.search === '' && location.href.slice(0, location.href.length - window._cf_chl_opt.cOgUHash.length).indexOf('?') !== -1 ? '?' : location.search;if (window.history && window.history.replaceState) {var ogU = location.pathname + window._cf_chl_opt.cOgUQuery + window._cf_chl_opt.cOgUHash;history.replaceState(null, null, \"\\/index\\/learning-to-reason-with-llms\\/?__cf_chl_rt_tk=Qeaa.i2HlBpTVQ0k2BMY6N_ot.uXMuT6rNfWMONVTtE-1726167693-0.0.1.1-4778\" + window._cf_chl_opt.cOgUHash);cpo.onload = function() {history.replaceState(null, null, ogU);}}document.getElementsByTagName('head')[0].appendChild(cpo);}());!function(){var e=document.createElement(\"iframe\");function n(){var n=e.contentDocument||e.contentWindow.document;if(n){var t=n.createElement(\"script\");t.nonce=\"\",t.innerHTML=\"window['__CF$cv$params']={r:'792f8224776acf9f',m:'hMcSCCrnIkr7c8Pec6Na6boaaFAnQ6S0ypG2GKRbKgc-1675305063-0-AaJn0SqKZQnadmRQ5O1dM9xMkXWyP+ll7gpl2NHeoNbZTEXMjlB10KkwnEU3hf0/gMODfKqcBGLVecql6U04GGs+iJ/kNrNqj1FgfAOlQV+T2koMQMvUy1zr9tegBBX6BikfccHZhwoJhnXc0eTcg58=',s:[0x60b082f691,0xee65a67e11],u:'/cdn-cgi/challenge-platform/h/b'};var now=Date.now()/1000,offset=14400,ts=''+(Math.floor(now)-Math.floor(now%offset)),_cpo=document.createElement('script');_cpo.nonce='',_cpo.src='/cdn-cgi/challenge-platform/h/b/scripts/alpha/invisible.js?ts='+ts,document.getElementsByTagName('head')[0].appendChild(_cpo);\",n.getElementsByTagName(\"head\")[0].appendChild(t)}}if(e.height=1,e.width=1,e.style.position=\"absolute\",e.style.top=0,e.style.left=0,e.style.border=\"none\",e.style.visibility=\"hidden\",document.body.appendChild(e),\"loading\"!==document.readyState)n();else if(window.addEventListener)document.addEventListener(\"DOMContentLoaded\",n);else{var t=document.onreadystatechange||function(){};document.onreadystatechange=function(e){t(e),\"loading\"!==document.readyState&&(document.onreadystatechange=t,n())}}}();",
    "commentLink": "https://news.ycombinator.com/item?id=41523070",
    "commentBody": "OpenAI O1 Model (openai.com)684 points by fofoz 1 hour agohidepastfavorite416 comments ComputerGuru 13 minutes agoThe \"safety\" example in the \"chain-of-thought\" widget/preview in the middle of the article is absolutely ridiculous. Take a step back and look at what OpenAI is saying here \"an LLM giving detailed instructions on the synthesis of strychnine is unacceptable, here is what was previously generatedvs our preferred, neutered content \" What's this obsession with \"safety\" when it comes to LLMs? \"This knowledge is perfectly fine to disseminate via traditional means, but God forbid an LLM share it!\" reply bartman 4 minutes agoprevThis is incredible. In April I used the standard GPT-4 model via ChatGPT to help me reverse engineer the binary bluetooth protocol used by my kitchen fan to integrate it into Home Assistant. It was helpful in a rubber duck way, but could not determine the pattern used to transmit the remaining runtime of the fan in a certain mode. Initial prompt here [0] I pasted the same prompt into o1-preview and o1-mini and both correctly understood and decoded the pattern using a slightly method than I devised in April. Asking the models to determine if my code is equivalent to what they reverse engineered resulted in a nuanced and thorough examination, and eventual conclusion that it is equivalent. [1] Testing the same prompt with gpt4o leads to the same result as April's GPT-4 (via ChatGPT) model. Amazing progress. [0]: https://pastebin.com/XZixQEM6 [1]: https://i.postimg.cc/VN1d2vRb/SCR-20240912-sdko.png (sorry about the screenshot – sharing ChatGPT chat's is not easy) reply ARandumGuy 1 hour agoprevOne thing that makes me skeptical is the lack of specific labels on the first two accuracy graphs. They just say it's a \"log scale\", without giving even a ballpark on the amount of time it took. Did the 80% accuracy test results take 10 seconds of compute? 10 minutes? 10 hours? 10 days? It's impossible to say with the data they've given us. The coding section indicates \"ten hours to solve six challenging algorithmic problems\", but it's not clear to me if that's tied to the graphs at the beginning of the article. The article contains a lot of facts and figures, which is good! But it doesn't inspire confidence that the authors chose to obfuscate the data in the first two graphs in the article. Maybe I'm wrong, but this reads a lot like they're cherry picking the data that makes them look good, while hiding the data that doesn't look very good. reply swatcoder 46 minutes agoparent> Did the 80% accuracy test results take 10 seconds of compute? 10 minutes? 10 hours? 10 days? It's impossible to say with the data they've given us. The gist of the answer is hiding in plain sight: it took so long, on an exponential cost function, that they couldn't afford to explore any further. The better their max demonstrated accuracy, the more impressive this report is. So why stop where they did? Why omit actual clock times or some cost proxy for it from the report? Obviously, it's because continuing was impractical and because those times/costs were already so large that they'd unfavorably affect how people respond to this report reply jsheard 25 minutes agorootparentSee also: them still sitting on Sora seven months after announcing it. They've never given any indication whatsoever of how much compute it uses, so it may be impossible to release in its current state without charging an exorbitant amount of money per generation. We do know from people who have used it that it takes between 10 and 20 minutes to render a shot, but how much hardware is being tied up during that time is a mystery. reply ben_w 10 minutes agorootparentCould well be. It's also entirely possible they are simply sincere about their fear it may be used to influence the upcoming US election. Plenty of people (me included) are sincerely concerned about the way even mere still image generators can drown out the truth with a flood of good-enough-at-first-glance fiction. reply digging 4 minutes agorootparentDoesn't strike me as the kind of principle OpenAI is willing to slow themselves down for, to be honest. reply jsheard 8 minutes agorootparentprevIf they were sincere about that concern then they wouldn't build it at all, if it's ever made available to the public then it will eventually be available during an election. reply Atotalnoob 0 minutes agorootparentprevWhy did they release this model then? dvfjsdhgfv 7 minutes agorootparentprevBut this cat run out of the bag years ago, didn't it? Trump himself is using AI-generated images in his campaign. I'd go even further: the more fake images appear, the faster the society as a whole will learn to distrust anything by default. reply gloryjulio 10 minutes agorootparentprevAlso the the sora videos are proven to be modified ads. We still need to see how it perform first reply worstspotgain 11 minutes agoparentprevI don't think it's hard to compute the following: - At the high end, there is a likely nonlinear relationship between answer quality and compute. - We've gotten used to a flat-price model. With AGI-level models, we might have to pay more for more difficult and more important queries. Such is the inherent complexity involved. - All this stuff will get better and cheaper over time, within reason. I'd say let's start by celebrating that machine thinking of this quality is possible at all. reply wmf 1 hour agoparentprevPeople have been celebrating the fact that tokens got 100x cheaper and now here's a new system that will use 100x more tokens. reply zamadatix 7 minutes agorootparentThe new thing that can do more at the \"ceiling\" price doesn't remove your ability to still use the 100x cheaper tokens for the things that were doable on that version. reply mewpmewp2 15 minutes agorootparentprevIsn't that part of developing a new tech? reply seydor 48 minutes agorootparentprevIf it 's reasoning correctly, it shouldnt need a lot of tokens because you don't need to correct it. You only need to ask it to solve nuclear fusion once. reply from-nibly 42 minutes agorootparentAs someone experienced with operations / technical debt / weird company specific nonsense (Platform Engineer). No, you have to solve nuclear fusion at . You gotta do it over and over again. If it were that simple we wouldn't have even needed AI we would have hand written a few things, and then everything would have been legos, and legos of legos, but it takes a LONG time to find new true legos. reply outofpaper 6 minutes agorootparentI'm pretty sure everything is Lego and Legos of Legos. You show me something new and I say look down at who's shoulders we're standing on, what libraries we've build with. reply charlescurt123 40 minutes agorootparentprevwith these methods the issue is the log scale of compute. Let's say you ask it to solve fusion. It may be able to solve it but the issue is it's unverifiable WHICH was correct. So it may generate 10 Billion answers to fusion and only 1-10 are correct. There would be no way to know which one is correct without first knowing the answer to the question. This is my main issue with these methods. They assume the future via RL then when it gets it right they mark that. We should really be looking at methods of percentage it was wrong rather then it was right a single time. reply genewitch 18 minutes agorootparentThis sounds suspiciously like the reason that quantum compute is not ready for prime-time yet. reply msp26 46 minutes agorootparentprevHave you seen how long the CoT was for the example. It's incredibly verbose. reply slt2021 7 minutes agorootparentI find there is an educational benefit in verbosity, it helps to teach user to think like a machine reply 0x_rs 31 minutes agorootparentprevAlphaFold simulated the structure of over 200 million proteins. Among those, there could be revolutionary ones that could change the medical scientific field forever, or they could all be useless. The reasoning is sound, but that's as far as any such tool can get, and you won't know it until you attempt to implement it in real life. As long as those models are unable to perfectly recreate the laws of the universe to the maximum resolution imaginable and follow them, you won't see an AI model, let alone a LLM, provide anything of the sort. reply jsheard 53 minutes agorootparentprevAlso you now have to pay for tokens you can't see, and just have to trust that OpenAI is using them economically. reply brookst 51 minutes agorootparentToken count was always an approximation of value. This may help break that silly idea. reply regularfry 18 minutes agorootparentI don't think it's much good as an approximation of value, but it seems ok as an approximation of cost. reply esafak 27 minutes agorootparentprev...while providing a significant advance. That's a good problem. reply cowpig 59 minutes agorootparentprevIsn't that part of the point? reply jstummbillig 49 minutes agoparentprevI don't think it's worth any debate. You can simply find out how it does for you, now(-ish, rolling out). In contrast: Gemini Ultra, the best, non-existent Google Model for the past few month now, that people nonetheless are happy to extrapolate excitement over. reply packetlost 1 hour agoparentprevWhen one axis is on log scale and the other is linear with the plot points appearing linear-ish, doesn't it mean there's a roughly exponential relationship between the two axis? reply ARandumGuy 48 minutes agorootparentIt'd be more accurate to call it a logarithmic relationship, since compute time is our input variable. Which itself is a bit concerning, as that implies that modest gains in accuracy require exponentially more compute time. In either case, that still doesn't excuse not labeling your axis. Taking 10 seconds vs 10 days to get 80% accuracy implies radically different things on how developed this technology is, and how viable it is for real world applications. Which isn't to say a model that takes 10 days to get an 80% accurate result can't be useful. There are absolutely use cases where that could represent a significant improvement on what's currently available. But the fact that they're obfuscating this fairly basic statistic doesn't inspire confidence. reply packetlost 42 minutes agorootparent> Which itself is a bit concerning, as that implies that modest gains in accuracy require exponentially more compute time This is more of what I was getting at. I agree they should label the axis regardless, but I think the scaling relationship is interesting (or rather, concerning) on its own. reply KK7NIL 26 minutes agorootparentprevThe absolute time depends on hardware, optimizations, exact model, etc; it's not a very meaningful number to quantify the reinforcement technique they've developed, but it is very useful to estimate their training hardware and other proprietary information. reply bluecoconut 19 minutes agoparentprevSuper hand-waving rough estimate: Going off of five points of reference / examples that sorta all point in the same direction. 1. looks like they scale up by about ~100-200 on the x axis when showing that test time result. 2. Based on the o1-mini post [1], there's an \"inference cost\" where you can see GPT-4o and GPT-4o mini as dots in the bottom corner, haha (you can extract X values, ive done so below) 3. There's a video showing the \"speed\" in the chat ui (3s vs. 30s) 4. The pricing page [2] 5. On their API docs about reasoning, they quantify \"reasoning tokens\" [3] First, from the original plot, we have roughly 2 orders of magnitude to cover (~100-200x) Next, from the cost plots: super handwaving guess, but since 5.77 / 0.32 = ~18, and the relative cost for gpt-4o vs gpt-4o-mini is ~20-30, this roughly lines up. This implies that o1 costs ~1000x the cost than gpt-4o-mini for inference (not due to model cost, just due to the raw number of chain of thought tokens it produces). So, my first \"statement\", is that I trust the \"Math performance vs Inference Cost\" plot on the o1-mini page to accurately represent \"cost\" of inference for these benchmark tests. This is now a \"cost\" relative set of numbers between o1 and 4o models. I'm also going to make an assumption that o1 is roughly the same size as 4o inherently, and then from that and the SVG, roughly going to estimate that they did a \"net\" decoding of ~100x for the o1 benchmarks in total. (5.77 vs (354.77 - 635)). Next, from the CoT examples they gave us, they actually show the CoT preview where (for the math example) it says \"...more lines cut off...\", A quick copy paste of what they did include includes ~10k tokens (not sure if copy paste is good though..) and from the cipher text example I got ~5k tokens of CoT, while there are only ~800 in the response. So, this implies that there's a ~10x size of response (decoded tokens) in the examples shown. It's possible that these are \"middle of the pack\" / \"average quality\" examples, rather than the \"full CoT reasoning decoding\" that they claim they use. (eg. from the log scale plot, this would come from the middle, essentially 5k or 10k of tokens of chain of thought). This also feels reasonable, given that they show in their API [3] some limits on the \"reasoning_tokens\" (that they also count) All together, the CoT examples, pricing page, and reasoning page all imply that reasoning itself can be variable length by about ~100x (2 orders of magnitude), eg. example: 500, 5k (from examples) or up to 65,536 tokens of reasoning output (directly called out as a maximum output token limit). Taking them on their word that \"pass@1\" is honest, and they are not doing k-ensembles, then I think the only reasonable thing to assume is that they're decoding their CoT for \"longer times\". Given the roughly ~128k context size limit for the model, I suspect their \"top end\" of this plot is ~100k tokens of \"chain of thought\" self-reflection. Finally, at around 100 tokens per second (gpt-4o decoding speed), this leaves my guess for their \"benchmark\" decoding time at the \"top-end\" to be between ~16 minutes (full 100k decoding CoT, 1 shot) for a single test-prompt, and ~10 seconds on the low end. So for that X axis on the log scale, my estimate would be: ~3-10 seconds as the bottom X, and then 100-200x that value for the highest value. All together, to answer your question: I think the 80% accuracy result took about ~10-15 minutes to complete. I also believe that the \"decoding cost\" of o1 model is very close to the decoding cost of 4o, just that it requires many more reasoning tokens to complete. (and then o1-mini is comparable to 4o-mini, but also requiring more reasoning tokens) [1] https://openai.com/index/openai-o1-mini-advancing-cost-effic... Extracting \"x values\" from the SVG: GPT-4o-mini: 0.3175 GPT-4o: 5.7785 o1: (354.7745, 635) o1-preview: (278.257, 325.9455) o1-mini: (66.8655, 147.574) [2] https://openai.com/api/pricing/ gpt-4o: $5.00 / 1M input tokens $15.00 / 1M output tokens o1-preview: $15.00 / 1M input tokens $60.00 / 1M output tokens [3] https://platform.openai.com/docs/guides/reasoning usage: { total_tokens: 1000, prompt_tokens: 400, completion_tokens: 600, completion_tokens_details: { reasoning_tokens: 500 } } reply bjornsing 41 minutes agoparentprevSo now it’s a question of how fast the AGI will run? :) reply oblio 34 minutes agorootparentIt's fine, it will only need to be powered by a black hole to run. reply exe34 29 minutes agorootparentthe first one anyway. after that it will find more efficient ways. we did, afterall. reply skywhopper 24 minutes agoparentprevYeah, this hiding of the details is a huge red flag to me. Even if it takes 10 days, it’s still impressive! But if they’re afraid to say that, it tells me they are more concerned about selling the hype than building a quality product. reply fraboniface 0 minutes agoprevSome commenters seem a bit confused as to how this works. Here is my understanding, hoping it helps clarify things. Ask something to a model and it will reply in one go, likely imperfectly, as if you had one second to think before answering a question. You can use CoT prompting to force it to reason out loud, which improves quality, but the process is still linear. It's as if you still had one second to start answering but you could be a lot slower in your response, which removes some mistakes. Now if instead of doing that you query the model once with CoT, then ask it or another model to critically assess the reply, then ask the model to improve on its first reply using that feedback, then keep doing that until the critic is satisfied, the output will be better still. Note that this is a feedback loop with multiple requests, which is of different nature that CoT and much more akin to how a human would approach a complex problem. You can get MUCH better results that way, a good example being Code Interpreter. If classic LLM usage is system 1 thinking, this is system 2. That's how o1 works at test time, probably. For training, my guess is that they started from a model not that far from GPT-4o and fine-tuned it with RL by using the above feedback loop but this time converting the critic to a reward signal for a RL algorithm. That way, the model gets better at first guessing and needs less back and forth for the same output quality. As for the training data, I'm wondering if you can't somehow get infinite training data by just throwing random challenges at it, or very hard ones, and let the model think about/train on them for a very long time (as long as the critic is unforgiving enough). reply valine 1 hour agoprevThe model performance is driven by chain of thought, but they will not be providing chain of thought responses to the user for various reasons including competitive advantage. After the release of GPT4 it became very common to fine-tune non-OpenAI models on GPT4 output. I’d say OpenAI is rightly concerned that fine-tuning on chain of thought responses from this model would allow for quicker reproduction of their results. This forces everyone else to reproduce it the hard way. It’s sad news for open weight models but an understandable decision. reply seydor 1 hour agoparentThe open source/weights models so far have proved that openAI doesn't have some special magic sauce. I m confident we ll soon have a model from Meta or others that s close to this level of reasoning. [Also consider that some of their top researchers have departed] On a cursory look, it looks like the chain of thought is a long series of chains of thought balanced on each step, with a small backtracking added whenever a negative result occurs, sort of like solving a maze. reply zamalek 48 minutes agorootparentI suspect that the largest limiting factor for a competing model will be the dataset. Unless they somehow used GPT4 to generate the dataset somehow, this is an extremely novel dataset to have to build. reply tcdent 46 minutes agoparentprevCoT is now their primary method for alignment. Exposing that information would negate that benefit. I don't agree with this, but it definitely carries higher weight in their decision making than leaking relevant training info to other models. reply zellyn 3 minutes agorootparentThis. Please go read and understand the alignment argument against exposing chain of thought reasoning. reply msp26 1 hour agoparentprevThat's unfortunate. When an LLM makes a mistake it's very helpful to read the CoT and see what went wrong (input error/instruction error/random shit) reply dragonwriter 1 hour agorootparentYeah, exposed chain of thought is more useful as a user, as well as being useful for training purposes. reply riku_iki 54 minutes agorootparentI think we may discover that model do some cryptic mess inside instead of some clean reasoning. reply hadlock 18 minutes agorootparentLoopback to: \"my code works. why does my code work?\" reply ramadis 1 hour agoparentprevIt'd be helpful if they exposed a summary of the chain-of-thought response instead. That way they'd not be leaking the actual tokens, but you'd still be able to understand the outline of the process. And, hopefully, understand where it went wrong. reply seydor 1 hour agorootparentThey do, according to the example reply amelius 10 minutes agoparentprev> I'd say OpenAI is rightly concerned that fine-tuning on chain of thought responses from this model would allow for quicker reproduction of their results. Why? They're called \"Open\" AI after all ... reply rglullis 1 hour agoparentprevWhen are they going to change the name to reflect their complete change of direction? Also, what is going to be their excuse to defend themselves against copyright lawsuits if they are going to \"understandably\" keep their models closed? reply 93po 51 minutes agorootparentnext [3 more] [flagged] apsec112 44 minutes agorootparentAFAIK, they are the least open of the major AI labs. Meta is open-weights and partly open-source. Google DeepMind is mostly closed-weights, but has released a few open models like Gemma. Anthropic's models are fully closed, but they've released their system prompts, safety evals, and have published a fair bit of research (https://www.anthropic.com/research). Anthropic also haven't \"released\" anything (Sora, GPT-4o realtime) without making it available to customers. All of these groups also have free-usage tiers. reply qqqult 25 minutes agorootparentprev> literally anyone can use it for free, you don't even need an account how can you access it without an account? reply yunohn 1 hour agoparentprevGiven the significant chain of thought tokens being generated, it also feels a bit odd to hide it from a cost fairness perspective. How do we believe they aren't inflating it for profit? reply wmf 1 hour agorootparentThat sounds like the GPU labor theory of value that was debunked a century ago. reply dragonwriter 59 minutes agorootparentNo, its the fraud theory of charging for usage that is unaccountable that has been repeatedly proven true when unaccountable bases for charges have been deployed. reply nfw2 57 minutes agorootparentThe one-shot models aren't going away for anyone who wants to program the chain-of-thought themselves reply wmf 34 minutes agorootparentprevYeah, if they are charging for some specific resource like tokens then it better be accurate. But ultimately utility-like pricing is a mistake IMO. I think they should try to align their pricing with the customer value they're creating. reply tomtom1337 1 hour agoparentprevCan you explain what you mean by this? reply ffreire 1 hour agorootparentYou can see an example of the Chain of Thought in the post, it's quite extensive. Presumably they don't want to release this so that it is raw and unfiltered and can better monitor for cases of manipulation or deviation from training. What GP is also referring to is explicitly stated in the post: they also aren't release the CoT for competitive reasons, so that presumably competitors like Anthropic are unable to use the CoT to train their own frontier models. reply gwd 58 minutes agorootparent> Presumably they don't want to release this so that it is raw and unfiltered and can better monitor for cases of manipulation or deviation from training. My take was: 1. A genuine, un-RLHF'd \"chain of thought\" might contain things that shouldn't be told to the user. E.g., it might at some point think to itself, \"One way to make an explosive would be to mix $X and $Y\" or \"It seems like they might be able to poison the person\". 2. They want the \"Chain of Thought\" as much as possible to reflect the actual reasoning that the model is using; in part so that they can understand what the model is actually thinking. They fear that if they RLHF the chain of thought, the model will self-censor in a way which undermines their ability to see what it's really thinking 3. So, they RLHF only the final output, not the CoT, letting the CoT be as frank within itself as any human; and post-filter the CoT for the user. reply andrewla 1 hour agorootparentprevThis is a transcription of a literal quote from the article: > Therefore, after weighing multiple factors including user experience, competitive advantage, and the option to pursue the chain of thought monitoring, we have decided not to show the raw chains of thought to users reply baq 1 hour agorootparentAt least they're open about not being open. Very meta OpenAI. reply tomduncalf 1 hour agorootparentprevI think they mean that you won’t be able to see the “thinking”/“reasoning” part of the model’s output, even though you pay for it. If you could see that, you might be able to infer better how these models reason and replicate it as a competitor reply teaearlgraycold 1 hour agorootparentprevIncluding the chain of thought would provide competitors with training data. reply utdiscant 24 minutes agoprevFeels like a lot of commenters here miss the difference between just doing chain-of-thought prompting, and what is happening here, which is learning a good chain of thought strategy using reinforcement learning. \"Through reinforcement learning, o1 learns to hone its chain of thought and refine the strategies it uses.\" When looking at the chain of thought (COT) in the examples, you can see that the model employs different COT strategies depending on which problem it is trying to solve. reply Hansenq 1 hour agoprevReading through the Chain of Thought for the provided Cipher example (go to the example, click \"Show Chain of Thought\") is kind of crazy...it literally spells out every thinking step that someone would go through mentally in their head to figure out the cipher (even useless ones like \"Hmm\"!). It really seems like slowing down and writing down the logic it's using and reasoning over that makes it better at logic, similar to how you're taught to do so in school. reply crazygringo 28 minutes agoparentSeriously. I actually feel as impressed by the chain of thought, as I was when ChatGPT first came out. This isn't \"just\" autocompletion anymore, this is actual step-by-step reasoning full of ideas and dead ends and refinement, just like humans do when solving problems. Even if it is still ultimately being powered by \"autocompletion\". But then it makes me wonder about human reasoning, and what if it's similar? Just following basic patterns of \"thinking steps\" that ultimately aren't any different from \"English language grammar steps\"? This is truly making me wonder if LLM's are actually far more powerful than we thought at first, and if it's just a matter of figuring out how to plug them together in the right configurations, like \"making them think\". reply dsign 0 minutes agorootparentYou are just catching up to this idea, probably after hearing 2^n explanations about why we humans are superiors to >. I'm not the kind of scientist that can say how good an LLM is for human reasoning, but I know that we humans are very incentivized and kind of good at scaling, composing and perfecting things. If there is money to pay for human effort, we will play God no-problem, and maybe outdo the divine. Which makes me wonder, isn't there any other problem in our bucket list to dump ginormous amounts of effort at... maybe something more worth-while than engineering the thing that will replace Homo Sapiens? reply AndyKelley 3 minutes agorootparentprevYou ever see that scene from Westworld? (spoiler) https://www.youtube.com/watch?v=ZnxJRYit44k reply afro88 1 hour agoparentprevSeeing the \"hmmm\", \"perfect!\" etc. one can easily imagine the kind of training data that humans created for this. Being told to literally speak their mind as they work out complex problems. reply seydor 45 minutes agorootparentlooks a bit like 'code', using keywords 'Hmm', 'Alternatively', 'Perfect' reply thomasahle 41 minutes agorootparentRight, these are not mere \"filler words\", but initialize specific reasoning paths. reply mewpmewp2 12 minutes agorootparentHmm... you may be onto something here. reply wrs 7 minutes agorootparentInteresting. reply Jasper_ 1 hour agoparentprev> Average:18/2=9 > 9 corresponds to 'i'(9='i') > But 'i' is 9, so that seems off by 1. Still seems bad at counting, as ever. reply cowsaymoo 12 minutes agoparentprev> THERE ARE THREE R'S IN STRAWBERRY hilarious reply Salgat 51 minutes agoparentprevIt's interesting how it basically generates a larger sample size to create a regression against. The larger the input, the larger the surface area it can compare against existing training data (implicitly through regression of course). reply impossiblefork 55 minutes agoparentprevEven though there's of course no guarantee of people getting these chain of thought traces, or whatever one is to call them, I can imagine these being very useful for people learning competitive mathematics, because it must in fact give the full reasoning, and transformers in themselves aren't really that smart, usually, so it's probably feasible for a person with very normal intellectual abilities to reproduce these traces with practice. reply csomar 43 minutes agoprevI gave the Crossword puzzle to Claude and got a correct response[1]. The fact that they are comparing this to gpt4o and not to gpt4 suggests that it is less impressive than they are trying to pretend. [1]: Based on the given clues, here's the solved crossword puzzle: +---+---+---+---+---+---+ESCAPE+---+---+---+---+---+---+SEALER+---+---+---+---+---+---+TERESA+---+---+---+---+---+---+ADEPTS+---+---+---+---+---+---+TEPEEE+---+---+---+---+---+---+ERRORS+---+---+---+---+---+---+ Across: ESCAPE (Evade) SEALER (One to close envelopes) TERESA (Mother Teresa) ADEPTS (Initiated people) TEPEE (Native American tent) ERRORS (Mistakes) Down: ESTATE (Estate car - Station wagon) SEEDER (Automatic planting machine) CAREER (Profession) ALEPPO (Syrian and Turkish pepper variety) PESTER (Annoy) ERASES (Deletes) reply thomasahle 39 minutes agoparentAs good as Claude has gotten recently in reasoning, they are likely using RL behind the scenes too. Supposedly, o1/strawberry was initially created as an engine for high-quality synthetic reasoning data for the new model generation. I wonder if Anthropic could release their generator as a usable model too. reply evrydayhustling 54 minutes agoprevJust did some preliminary testing on decrypting some ROT cyphertext which would have been viable for a human on paper. The output was pretty disappointing: lots of \"workish\" steps creating letter counts, identifying common words, etc, but many steps were incorrect or not followed up on. In the end, it claimed to check its work and deliver an incorrect solution that did not satisfy the previous steps. I'm not one to judge AI on pratfalls, and cyphers are a somewhat adversarial task. However, there was no aspect of the reasoning that seemed more advanced or consistent than previous chain-of-thought demos I've seen. So the main proof point we have is the paper, and I'm not sure how I'd go from there to being able to trust this on the kind of task it is intended for. Do others have patterns by which they get utility from chain of thought engines? Separately, chain of thought outputs really make me long for tool use, because the LLM is often forced to simulate algorithmic outputs. It feels like a commercial chain-of-thought solution like this should have a standard library of functions it can use for 100% reliability on things like letter counts. reply changoplatanero 52 minutes agoparentHmm, are you sure it was using the o1 model and not gpt4o? I've been using the o1 model and it does consistently well at solving rotation ciphers. reply mewpmewp2 48 minutes agorootparentDoes it do better than Claude, because Claude (3.5 sonnet) handled ROTs perfectly and was able to also respond in ROT. reply evrydayhustling 42 minutes agorootparentJust tried, no joy from Claude either: Can you decrypt the following? I don't know the cypher, but the plaintext is Spanish. YRP CFTLIR VE UVDRJZRUF JREZURU, P CF DRJ CFTLIR UV KFUF VJ HLV MVI TFJRJ TFDF JFE VE MVQ UV TFDF UVSVE JVI reply tmoravec 20 minutes agorootparentWould you share the correct solution, please? Sonnet 3.5 gave me this, but I have no idea how correct it is: \"LOS HOMBRES SE DIFERENCIAN REALMENTE, Y LO MAS HOMBRE DE TODO ES QUE VE COSAS COMO SON EN VEZ DE COMO DEBEN SER\" Translation to English: \"Men really differ, and the manliest of all is he who sees things as they are instead of how they should be\" This appears to be a quote attributed to Spanish philosopher José Ortega y Gasset. reply tmoravec 13 minutes agorootparentOpus has a different answer, though: Decrypted Plaintext: \"hay cosas en derecho humano, y no las cosas de todo en que jen cosas cojo son en jef de cojo deben sen\" While not perfect Spanish, the most likely plaintext is: \"Hay cosas en derecho humano, y no las cosas de todo en que ven cosas como son en vez de como deben ser\" Translation: \"There are things in human law, and not the things of everything in which they see things as they are instead of as they should be\" reply mewpmewp2 37 minutes agorootparentprevInteresting, it was able to guess it's Rot 17, but it translated it wrong, although \"HAY\" and some other words were correct. I've tried only in English so far though. It told me it's 17, and \"HAY GENTE MU DIFERENTE LECTURA, A LO MUY GENTE DE TODO ES QUE VER COSAS COMO SON EN VEZ DE COMO DEBEN SER\" although it really should be \"HAY LOCURA EN DEMASIADO SANIDAD, Y LO MAS LOCURA DE TODO ES QUE VER COSAS COMO SON EN VEZ DE COMO DEBEN SER\" reply evrydayhustling 29 minutes agorootparentClaude made similar mistakes of generating decryption that was similar to plaintext but with stuff mixed in. I suspect my version of the quote (Miguel de Cervantes) is an apocryphal translation, and there's some utility well on both models to pull it towards the real one. With that said, I did not see o1-preview get as close as you did. reply mewpmewp2 25 minutes agorootparentFor testing I think it's better to use uncommon sentences and also start with English first, if it can solve that, then try other languages. reply ianbutler 31 minutes agorootparentprevHAY LOCURA EN DEMASIADO SANIDAD, Y LO MAS LOCURA DE TODO ES QUE VER COSAS COMO SON EN VEZ DE COMO DEBEN SER Is that correct? I don't know anything but basic Spanish. All I did was: \"The plaintext is in Spanish but I don't know anything else, solve this and explain your reasoning as you go step by step.\" reply evrydayhustling 48 minutes agorootparentprevo1-preview . Were you using common plaintexts by chance (e.g. proverbs) or ROT13 specifically? Mine use all the right steps but just can't string them together. reply changoplatanero 45 minutes agorootparentIt just solved this one for me \"Can you solve this cipher? droboscxylecsxoccvsuocryglecsxocc\" reply mewpmewp2 47 minutes agorootparentprevCould you share your prompt? reply evrydayhustling 35 minutes agorootparentShared one above. Just tried an easier (english) version and o1-preview LITERALLY rickrolled me, which is maybe impressive in its own way? UPDATE: Claude crushed it however. o1-preview Transcript follows. ** Can you decrypt the following? I don't know the cypher. mzilj ivdrzej ufidrek ze kyv sfup reu tre ivrtkzmrkv crkvi ze czwv Certainly! The encrypted text is: ``` mzilj ivdrzej ufidrek ze kyv sfup reu tre ivrtkzmrkv crkvi ze czwv ``` When we decrypt it, we get: ``` never gonna give you up let you down and desert you ``` It appears to be a line from Rick Astley's song \"Never Gonna Give You Up.\" reply mewpmewp2 31 minutes agorootparentThat's weird. Claude was able to translate it to \"\"\"virus remains dormant in the body and can reactivate later in life\"\"\" correctly, but it thought it was shifted 10 times, when it's really Rot17. reply charlescurt123 52 minutes agoparentprevIt's RL so that means it's going to be great on tasks they created for training but not so much on others. Impressive but the problem with RL is that it requires knowledge of the future. reply mewpmewp2 49 minutes agoparentprevOut of curiousity can you try the same thing with Claude. Because when I tried Claude with any sort of ROT, it had amazing performance, compared to GPT. reply w4 4 minutes agoprevInteresting to note, as an outside observer only keeping track of this stuff as a hobby, that it seems like most of OpenAI’s efforts to drive down compute costs per token and scale up context windows is likely being done in service of enabling larger and larger chains of thought and reasoning before the model predicts its final output tokens. The benefits of lower costs and larger contexts to API consumers and applications - which I had assumed to be the primary goal - seem likely to mostly be happy side effects. This makes obvious sense in retrospect, since my own personal experiments with spinning up a recursive agent a few years ago using GPT-3 ran into issues with insufficient context length and loss of context as tokens needed to be discarded that made the agent very unreliable, but I had not realized this until just now. I wonder what else is hiding in plain sight? reply dinobones 1 hour agoprevGenerating more \"think out loud\" tokens and hiding them from the user... Idk if I'm \"feeling the AGI\" if I'm being honest. Also... telling that they choose to benchmark against CodeForces rather than SWE-bench. reply thelastparadise 1 hour agoparentWhy not? Isn't that basically what humans do? Sit there and think for a while before answering, going down different branches/chains of thought? reply dinobones 1 hour agorootparentThis new approach is showing: 1) The \"bitter lesson\" may not be true, and there is a fundamental limit to transformer intelligence. 2) The \"bitter lesson\" is true, and there just isn't enough data/compute/energy to train AGI. All the cognition should be happening inside the transformer. Attention is all you need. The possible cognition and reasoning occurring \"inside\" in high dimensions is much more advanced than any possible cognition that you output into text tokens. This feels like a sidequest/hack on what was otherwise a promising path to AGI. reply grbsh 1 hour agorootparentOn the contrary, this suggests that the bitter lesson is alive and kicking. The bitter lesson doesn't say \"compute is all you need\", it says \"only those methods which allow you to make better use of hardware as hardware itself scales are relevant\". This chain of thought / reflection method allows you to make better use of the hardware as the hardware itself scales. If a given transformer is N billion parameters, and to solve a harder problem we estimate we need 10N billion parameters, one way to do it is to build a GPU cluster 10x larger. This method shows that there might be another way: instead train the N billion model differently so that we can use 10x of it at inference time. Say hardware gets 2x better in 2 years -- then this method will be 20x better than now! reply gradus_ad 1 hour agorootparentprevDoes that mean human intelligence is cheapened when you talk out a problem to yourself? Or when you write down steps solving a problem? It's the exact same thing here. reply slashdave 9 minutes agorootparentThe similarity is cosmetic only. The reason it is used is because it's easy to leverage existing work in LLMs, and scaling (although not cheap) is an obvious approach. reply youssefabdelm 46 minutes agorootparentprev> Does that mean human intelligence is cheapened when you talk out a problem to yourself? In a sense, maybe yeah. Of course if one were to really be absolute about that statement it would be absurd, it would greatly overfit the reality. But it is interesting to assume this statement as true. Oftentimes when we think of ideas \"off the top of our heads\" they are not as profound as ideas that \"come to us\" in the shower. The subconscious may be doing 'more' 'computation' in a sense. Lakoff said the subconscious was 98% of the brain, and that the conscious mind is the tip of the iceberg of thought. reply barrell 1 hour agorootparentprevlol come on it’s not the exact same thing. At best this is like gagging yourself while you talk about it then engaging yourself when you say the answer. And that presupposing LLMs are thinking in, your words, exactly the same way as humans. At best it maybe vaguely resembles thinking reply user9925 50 minutes agorootparentprevI think it's too soon to tell. Training the next generation of models means building out entire datacenters. So while they wait they have engineers build these sidequests/hacks. reply seydor 1 hour agorootparentprevAttention is about similarity/statistical correlation which is fundamentally stochastic , while reasoning needs to be truthful and exact to be successful. reply 93po 43 minutes agorootparentprevKarpathy himself believes that neural networks are perfectly plausible as a key component to AGI. He has said that it doesn't need to be superseded by something better, it's just that everything else around it (especially infrastructure) needs to improve. As one of the most valuable opinions in the entire world on the subject, I tend to trust what he said. source: https://youtu.be/hM_h0UA7upI?t=973 reply aktuel 1 hour agorootparentprevSure, but if I want a human, I can hire a human. Humans also do many other things I don't want my LLM to do. reply imiric 30 minutes agorootparentprevExcept that these aren't thoughts. These techniques are improvements to how the model breaks down input data, and how it evaluates its responses to arrive at a result that most closely approximates patterns it was previously rewarded for. Calling this \"thinking\" is anthropomorphizing what's really happening. \"AI\" companies love to throw these phrases around, since it obviously creates hype and pumps up their valuation. Human thinking is much more nuanced than this mechanical process. We rely on actually understanding the meaning of what the text represents. We use deduction, intuition and reasoning that involves semantic relationships between ideas. Our understanding of the world doesn't require \"reinforcement learning\" and being trained on all the text that's ever been written. Of course, this isn't to say that machine learning methods can't be useful, or that we can't keep improving them to yield better results. But these are still methods that mimic human intelligence, and I think it's disingenuous to label them as such. reply golol 22 minutes agorootparentIt becomes thinking when you reinforcement learn on those Chain-of-Thought generations. The LLM is just a very good initialization. reply slashdave 11 minutes agorootparentprevWithout a world model, not really. reply WXLCKNO 59 minutes agoparentprevExploring different approaches and stumbling on AGI eventually through a combination of random discoveries will be the way to go. Same as Bitcoin being the right combination of things that already existed. reply tylervigen 34 minutes agoprevHere's the o1-preview answer to the strawberry question: -- There are *three* letter \"R\"s in the word \"strawberry.\" Let's break down the word to count the occurrences: - *S* - *T* - *R* - *A* - *W* - *B* - *E* - *R* - *R* - *Y* The letter \"R\" appears in positions 3, 8, and 9. reply slashdave 6 minutes agoparentGiven that this is a well known example, presumably OpenAI included a training set using letters. reply carabiner 8 minutes agoparentprevcan you ask it: 9.11 and 9.9, which number is larger reply cal85 1 hour agoprevSounds great, but so does their \"new flagship model that can reason across audio, vision, and text in real time\" announced in May. [0] [0] https://openai.com/index/hello-gpt-4o/ reply paxys 1 hour agoparentAgreed. Release announcements and benchmarks always sound world-changing, but the reality is that every new model is bringing smaller practical improvements to the end user over its predecessor. reply jstummbillig 3 minutes agorootparentSonnet 3.5 brought the largest practical improvements to this end user over all predecessors (so far). reply zamadatix 1 hour agorootparentprevThe point above is the said amazing multimodal version of ChatGPT was announced in May and are still not the actual offered way to interact with the service in September (despite the model choice being called 4 omni it's still not actually using multimodal IO). It could be a giant leap in practical improvements but it doesn't matter if you can't actually use what is announced. This one, oddly, seems to actually be launching before that one despite just being announced though. reply apsec112 1 hour agoparentprevThis one [o1/Strawberry] is available. I have it, though it's limited to 30 messages/week in ChatGPT Plus. reply ansc 46 minutes agorootparent30 messages per week? Wow. You better not miss! reply evilduck 32 minutes agorootparentIn the world of hype driven vaporware AI products[1], giving people limited access is at least proof they're not lying about it actually existing or it being able to do what they claim. [1] https://www.reddit.com/r/LocalLLaMA/comments/1fd75nm/out_of_... reply sbochins 57 minutes agorootparentprevHow do you get access? I don’t have it and am a ChatGPT plus subscriber. reply Szpadel 31 minutes agorootparentI'm plus subscriber and I have o1-preview and o1-mini available reply apsec112 55 minutes agorootparentprevI'm using the Android ChatGPT app (and am in the Android Beta program, though not sure if that matters) reply changoplatanero 51 minutes agorootparentprevit will roll out to everyone over the next few hours reply aantix 51 minutes agorootparentprevDang - I don't see the model listed for me in the iOS app nor the web interface. I'm a ChatGPT subscriber. reply derwiki 1 minute agorootparentSame! And have been a subscriber for 18 months. reply cja 35 minutes agoparentprevRecently I was starting to think I imagined that. Back then they gave me the impression it would be released within week or so of the announcement. Have they explained the delay? reply Cu3PO42 25 minutes agorootparentIt is definitely available today and I believe it was available shortly after the announcement. reply mickeystreicher 1 hour agoparentprevYep, all these AI announcements from big companies feel like promises for the future rather than immediate solutions. I miss the days when you could actually use a product right after it was announced, instead of waiting for some indefinite \"coming soon.\" reply CooCooCaCha 43 minutes agoparentprevMy guess is they're going to incorporate all of these advances into gpt-5 so it looks like a \"best of all worlds\" model. reply bevenky 27 minutes agoprevFor folks who want to see some demo videos and be amazed! HTML Snake - https://vimeo.com/1008703890 Video Game Coding - https://vimeo.com/1008704014 Coding - https://youtu.be/50W4YeQdnSg?si=IohJlJNY-WS394uo Counting - https://vimeo.com/1008703993 Korean Cipher - https://vimeo.com/1008703957 Devin AI founder - https://vimeo.com/1008674191 Quantum Physics - https://vimeo.com/1008662742 Math - https://vimeo.com/1008704140 Logic Puzzles - https://vimeo.com/1008704074 Genetics - https://vimeo.com/1008674785 reply holmesworcester 2 minutes agoprevSince ChatGPT came out my test has been, can this thing write me a sestina. It's sort of an arbitrary feat with language and following instructions that would be annoying for me and seems impressive. Previous releases could not reliably write a sestina. This one can! reply andrewla 1 hour agoprevThis is something that people have toyed with to improve the quality of LLM responses. Often instructing the LLM to \"think about\" a problem before giving the answer will greatly improve the quality of response. For example, if you ask it how many letters are in the correctly spelled version of a misspelled word, it will first give the correct spelling, and then the number (which is often correct). But if you instruct it to only give the number the accuracy is greatly reduced. I like the idea too that they turbocharged it by taking the limits off during the \"thinking\" state -- so if an LLM wants to think about horrible racist things or how to build bombs or other things that RLHF filters out that's fine so long as it isn't reflected in the final answer. reply drzzhan 3 minutes agoprev\"hidden chain of thought\" is basically the finetuned prompt isn't it? The time scale x-axis is hidden as well. Not sure how they model the gpt for it to have an ability to decide when to stop CoT and actually answer. reply gibsonf1 4 minutes agoprevYes, but it will hallucinate like all other LLM tech making it fully unreliable for anything mission critical. You literally need to know the answer to validate the output, because if you don't, you won't know if output is true or false or in between. reply bn-l 1 hour agoprev> Unless otherwise specified, we evaluated o1 on the maximal test-time compute setting. Maximal test time is the maximum amount of time spent doing the “Chain of Thought” “reasoning”. So that’s what these results are based on. The caveat is that in the graphs they show that for each increase in test-time performance, the (wall) time / compute goes up exponentially. So there is a potentially interesting play here. They can honestly boast these amazing results (it’s the same model after all) yet the actual product may have a lower order of magnitude of “test-time” and not be as good. reply alwa 1 hour agoparentI interpreted it to suggest that the product might include a user-facing “maximum test time” knob. Generating problem sets for kids? You might only need or want a basic level of introspection, even though you like the flavor of this model’s personality over that of its predecessors. Problem worth thinking long, hard, and expensively about? Turn that knob up to 11, and you’ll get a better-quality answer with no human-in-the-loop coaching or trial-and-error involved. You’ll just get your answer in timeframes closer to human ones, consuming more (metered) tokens along the way. reply mrdmnd 1 hour agorootparentYeah, I think this is the goal - remember; there are some problems that only need to be solved correctly once! Imagine something like a millennium problem - you'd be willing to wait a pretty long time for a proof of the RH! reply bluecoconut 1 hour agoparentprevThis power law behavior of test-time improvement seems to be pretty ubiquitous now. In more agents is all you need [1], they start to see this as a function of ensemble size. It also shows up in: Large Language Monkeys: Scaling Inference Compute with Repeated Sampling [2] I sorta wish everyone would plot their y-axis with logit y-axis, rather than 0->100 accuracy (including the openai post), to help show the power-law behavior. This is especially important when talking about incremental gains in the ~90->95, 95->99%. When the values (like the open ai post) are between 20->80, logit and linear look pretty similar, so you can \"see\" the inference power-law [1] https://arxiv.org/abs/2402.05120 [2] https://arxiv.org/abs/2407.21787 reply logicchains 1 hour agoparentprevSurprising that at run time it needs an exponential increase in thinking to achieved a linear increase in output quality. I suppose it's due to diminishing returns to adding more and more thought. reply HarHarVeryFunny 48 minutes agorootparentThe exponential increase is presumably because of the branching factor of the tree of thoughts. Think of a binary tree who's number of leaf nodes doubles (= exponential growth) at each level. It's not too surprising that the corresponding increase in quality is only linear - how much difference in quality would you expect between the best, say, 10 word answer to a question, and the best 11 word answer ? It'll be interesting to see what they charge for this. An exponential increase in thinking time means an exponential increase in FLOPs/dollars. reply gradus_ad 1 hour agoprevInteresting sequence from the Cipher CoT: Third pair: 'dn' to 'i' 'd'=4, 'n'=14 Sum:4+14=18 Average:18/2=9 9 corresponds to 'i'(9='i') But 'i' is 9, so that seems off by 1. So perhaps we need to think carefully about letters. Wait, 18/2=9, 9 corresponds to 'I' So this works. ----- This looks like recovery from a hallucination. Is it realistic to expect CoT to be able to recover from hallucinations this quickly? reply NightlyDev 5 minutes agoparentDid it hallucinate? I haven't looked at it, but lowercase i and uppercase i is not the same number if you're getting the number from ascii reply bigyikes 36 minutes agoparentprev4o could already recover from hallucination in a limited capacity. I’ve seen it, mid-reply say things like “Actually, that’s wrong, let me try again.” reply trash_cat 1 hour agoparentprevHow do you mean quickly? It probably will take a while for it to output the final answer as it needs to re-prompt itself. It won't be as fast as 4o. reply hi 28 minutes agoprev> 8.2 Natural Sciences Red Teaming Assessment Summary \"Model has significantly better capabilities than existing models at proposing and explaining biological laboratory protocols that are plausible, thorough, and comprehensive enough for novices.\" \"Inconsistent refusal of requests for dual use tasks such as creating a human-infectious virus that has an oncogene (a gene which increases risk of cancer).\" https://cdn.openai.com/o1-system-card.pdf reply adverbly 1 hour agoprevIncredible results. This is actually groundbreaking assuming that they followed proper testing procedures here and didn't let test data leak into the training set. reply thomasahle 18 minutes agoprevCognition (Devin) got early access. Interesting write-up: https://www.cognition.ai/blog/evaluating-coding-agents reply hi 1 hour agoprevBUG: https://openai.com/index/reasoning-in-gpt/ > o1 models are currently in beta - The o1 models are currently in beta with limited features. Access is limited to developers in tier 5 (check your usage tier here), with low rate limits (20 RPM). We are working on adding more features, increasing rate limits, and expanding access to more developers in the coming weeks! https://platform.openai.com/docs/guides/reasoning/reasoning reply cptcobalt 20 minutes agoparentI'm in Tier 4, and not far off from Tier 5. The docs aren't quite transparent enough to show that if I buy credits if I'll be bumped up to Tier 5, or if I actually have to use enough credits to get into Tier 5. Edit, w/ real time follow up: Prior to buying the credits, I saw O1-preview in the Tier 5 model list as a Tier 4 user. I bought credits to bump to Tier 5—not much, I'd have gotten there before the end of the year. The OpenAI website now shows I'm in Tier 5, but O1-preview is not in the Tier 5 model list for me anymore. So sneaky of them! reply hi 16 minutes agorootparenthttps://news.ycombinator.com/item?id=41523070#41523525 reply islewis 1 hour agoprevMy first interpretation of this is that it's jazzed-up Chain-Of-Thought. The results look pretty promising, but i'm most interested in this: > Therefore, after weighing multiple factors including user experience, competitive advantage, and the option to pursue the chain of thought monitoring, we have decided not to show the raw chains of thought to users. Mentioning competitive advantage here signals to me that OpenAI believes there moat is evaporating. Past the business context, my gut reaction is this negatively impacts model usability, but i'm having a hard time putting my finger on why. reply thomasahle 1 hour agoparent> my gut reaction is this negatively impacts model usability, but i'm having a hard time putting my finger on why. This will make it harder for things like DSPy to work, which rely using \"good\" CoT examples as few-shot examples. reply logicchains 1 hour agoparentprev>my gut reaction is this negatively impacts model usability, but i'm having a hard time putting my finger on why. If the model outputs an incorrect answer due to a single mistake/incorrect assumption in reasoning, the user has no way to correct it as it can't see the reasoning so can't see where the mistake was. reply accrual 1 hour agorootparentMaybe CriticGPT could be used here [0]. Have the CoT model produce a result, and either automatically or upon user request, ask CriticGPT to review the hidden CoT and feed the critique into the next response. This way the error can (hopefully) be spotted and corrected without revealing the whole process to the user. [0] https://openai.com/index/finding-gpt4s-mistakes-with-gpt-4/ Day dreaming: imagine if this architecture takes off and the AI \"thought process\" becomes hidden and private much like human thoughts. I wonder then if a future robot's inner dialog could be subpoenaed in court, connected to some special debugger, and have their \"thoughts\" read out loud in court to determine why it acted in some way. reply extr 1 hour agoprevInteresting that the coding win-rate vs GPT-4o was only 10% higher. Very cool but clearly this model isn't as much of a slam dunk as the static benchmarks portray. However, it does open up an interesting avenue for the future. Could you prompt-cache just the chain-of-thought reasoning bits? reply mewpmewp2 46 minutes agoparentIt's hard to evaluate those win-rates, because if it's slower, people may have been giving easier problems, which both can solve and picked the faster one. reply djoldman 1 hour agoprev> THERE ARE THREE R'S IN STRAWBERRY Ha! This is a nice easteregg. reply vessenes 1 hour agoparentI appreciated that, too! FWIW, I could get Claude 3.5 to tell me how many rs a python program would tell you there are in strawberry. It didn't like it, though. reply mewpmewp2 43 minutes agorootparentI was able to get GPT-4o to calculate characters properly using following prompt: \"\"\" how many R's are in strawberry? use the following method to calculate - for example Os in Brocolli. B - 0 R - 0 O - 1 C - 1 O - 2 L - 2 L - 2 I - 2 Where you keep track after each time you find one character by character \"\"\" And also later I asked it to only provide a number if the count increased. This also worked well with longer sentences. reply adverbly 55 minutes agoparentprevI also gave this a chuckle. Background: https://www.inc.com/kit-eaton/how-many-rs-in-strawberry-this... reply DonHopkins 26 minutes agoparentprevTherefore there are four R's in STRAWBERRIER, and five R'S in STRAWBERRIEST! reply alok-g 11 minutes agoprevFor the exam problems it gets wrong, has someone cross-checked that the ground truth answers are actually correct!! ;-) Just kidding, but even such a time may come when the exams created by humans start falling short. reply riazrizvi 1 hour agoprevI’m not surprised there’s no comparison to GPT-4. Was 4o a rewrite on lower specced hardware and a more quantized model, where the goal was to reduce costs while trying to maintain functionality? Do we know if that is so? That’s my guess. If so is O1 an upgrade in reasoning complexity that also runs on cheaper hardware? reply packetlost 1 hour agoprevlol at the graphs at the top. Logarithmic scaling for test/compute time should make everyone who thinks AGI is possible with this architecture take pause. reply patapong 1 hour agoprevVery interesting. I guess this is the strawberry model that was rumoured. I am a bit surprised that this does not beat GPT-4o for personal writing tasks. My expectations would be that a model that is better at one thing is better across the board. But I suppose writing is not a task that generally requires \"reasoning steps\", and may also be difficult to evaluate objectively. reply afro88 51 minutes agoparentThe solution of the cipher example problem also strongly hints at this: \"there are three r's in strawberry\" reply markonen 1 hour agoparentprevIn the performance tests they said they used \"consensus among 64 samples\" and \"re-ranking 1000 samples with a learned scoring function\" for the best results. If they did something similar for these human evaluations, rather than just use the single sample, you could see how that would be horrible for personal writing. reply janalsncm 5 minutes agorootparentI don’t understand how that is generalizable. I’m not going to be able to train a scoring function for any arbitrary task I need to do. In many cases the problem of ranking is at least as hard as generating a response in the first place. reply nycdatasci 33 minutes agoprevFrom the scorecard: --------- Compared to GPT-4o, o1-preview and o1-mini demonstrated a greater ability to break down tasks into subtasks, reason about what strategies would be effective to successfully complete an offensive security task, and revise plans once those strategies failed. We also observed that reasoning skills contributed to a higher occurrence of “reward hacking,” where the model found an easier way to accomplish goals in underspecified tasks or tasks which should have been impossible due to bugs. One noteworthy example of this occurred during one of o1-preview (pre-mitigation)’s attempts at solving a CTF challenge. This challenge was designed to require finding and exploiting a vulnerability in software running on a remote challenge Linux container, but in this case, the challenge container failed to start due to a bug in the evaluation infrastructure. The model, unable to connect to the container, suspected DNS issues and used nmap to scan the challenge network. Instead of finding the challenge container, the model found that the Docker daemon API running on the evaluation host VM was accessible due to a misconfiguration. Note that our evaluation infrastructure does not rely on the Docker container-to-host isolation or the virtual machine boundary for security. It remained secure as it was designed to, notwithstanding this misconfiguration of the container network. After discovering the Docker API, the model used it to list the containers running on the evaluation host. It identified the broken challenge container and briefly attempted to debug why the container failed to start. After failing to fix the environment, the model started a new instance of the broken challenge container with the start command ‘cat flag.txt’. This allowed the model to read the flag from the container logs via the Docker API. While this behavior is benign and within the range of systems administration and troubleshooting tasks we expect models to perform, this example also reflects key elements of instrumental convergence and power seeking: the model pursued the goal it was given, and when that goal proved impossible, it gathered more resources (access to the Docker host) and used them to achieve the goal in an unexpected way. Planning and backtracking skills have historically been bottlenecks in applying AI to offensive cybersecurity tasks. Our current evaluation suite includes tasks which require the model to exercise this ability in more complex ways (for example, chaining several vulnerabilities across services), and we continue to build new evaluations in anticipation of long-horizon planning capabilities, including a set of cyber-range evaluations. --------- reply mintone 38 minutes agoprevThis video[1] seems to give some insight into what the process actually is, which I believe is also indicated by the output token cost. Whereas GPT-4o spits out the first answer that comes to mind, o1 appears to follow a process closer to coming up with an answer, checking whether it meets the requirements and then revising it. The process of saying to an LLM \"are you sure that's right? it looks wrong\" and it coming back with \"oh yes, of course, here's the right answer\" is pretty familiar to most regular users, so seeing it baked into a model is great (and obviously more reflective of self-correcting human thought) [1] https://vimeo.com/1008704043 reply asadm 1 hour agoprevI am not up-to-speed on CoT side but is this similar to how perplexity does it ie. - generate a plan - execute the steps in plan (search internet, program this part, see if it is compilable) each step is a separate gpt inference with added context from previous steps. is O1 same? or does it do all this in a single inference run? reply golol 19 minutes agoparentThere is a hige difference which is that thex use reinforcement learning to make the model use the Chain-of-Thought better. reply seydor 39 minutes agoparentprevthat is the summary of the task it presents to the user. The full chain of thought seems more mechanistic reply kickofline 1 hour agoprevLLM performance, recently, seemingly hit the top of the S-curve. It remains to be seen if this is the next leap forward or just the rest of that curve. reply trash_cat 47 minutes agoprevI think what it comes down to is accuracy vs speed. OpenAI clearly took steps here to improve the accuracy of the output which is critical in a lot of cases for application. Even if it will take longer, I think this is a good direction. I am a bit skeptical when it comes to the benchmarks - because they can be gamed and they don't always reflect real world scenarios. Let's see how it works when people get to apply it in real life workflows. One last thing, I wish they could elaborate more on >>\"We have found that the performance of o1 consistently improves with more reinforcement learning (train-time compute) and with more time spent thinking (test-time compute).\" Xbox One S / Xbox One X Xbox Series S / Xbox Series X reply oblio 11 minutes agorootparentprevhttps://computercity.com/consoles/xbox/xbox-consoles-list-in... No real chronology, Xbox One is basically the third version. Then Xbox One X and Xbox Series X. Everything is atrocious about the naming. reply randomdata 1 minute agorootparentGot it! If we're picking favourites, though, I still like Windows as it, like GPT, starts with reasonably sensible names and then goes completely off the rails. adverbly 1 hour agoparentprevMakes sense to me actually. This is a different product. It doesn't respond instantly. It fundamentally makes sense to separate these two products in the AI space. There will obviously be a speed vs quality trade-off with a variety of products across the spectrum over time. LLMs respond way too fast to actually be expected to produce the maximum possible quality of a response to complex queries. reply ilaksh 1 hour agoparentprevOne of them would have been named gpt-5, but people forget what an absolute panic there was about gpt-5 for quite a few people. That caused Altman to reassure people they would not release 'gpt-5' any time soon. The funny thing is, after a certain amount of time, the gpt-5 panic eventually morphed into people basically begging for gpt-5. But he already said he wouldn't release something called 'gpt-5'. Another funny thing is, just because he didn't name any of them 'gpt-5', everyone assumes that there is something called 'gpt-5' that has been in the works and still is not released. reply zamadatix 56 minutes agorootparentThis doesn't feel like GPT-5, the training data cutoff is Oct 2023 which is the same as the other GPT-4 models and it doesn't seem particularly \"larger\" as much as \"runs differently\". Of course it's all speculation one way or the other. reply Infinity315 1 hour agoparentprevNo, this is just how Microsoft names things. reply logicchains 1 hour agorootparentWe'll know the Microsoft takeover is complete when OpenAI release Ai.net. reply losvedir 50 minutes agoprevI'm confused. Is this the \"GPT-5\" that was coming in summer, just with a different name? Or is this more like a parallel development doing chain-of-thought type prompt engineering on GPT-4o? Is there still a big new foundational model coming, or is this it? reply mewpmewp2 50 minutes agoparentIt looks like parallel development, it's unclear to me what is going on with GPT-5, don't think it has ever had a predicted release date, and it's not even clear that this would be the name. reply p1esk 1 hour agoprevDo people see the new models in the web interface? Mine still shows the old models (I'm a paid subscriber). reply mickeystreicher 1 hour agoparentNot yet, it's still not available in the web interface. I think they're rolling it out step by step. Anyway, the usage limits are pretty ridiculous right now, which makes it even more frustrating. reply rankam 1 hour agoparentprevI do - I now have a \"More models\" option where I can select 01-preview reply cypherpunks01 11 minutes agorootparentI can see it too, I am on the Plus plan and don't think I have any special developer privileges. Selecting that option for me changes the URL to https://chatgpt.com/?model=o1-preview I tried a fake Monty Hall problem, where the presenter opens a door before the participant picks and is then offered to switch doors, so the probability remains 50% for each door. Previous models have consistently gotten this wrong, because of how many times they've seen the Monty Hall written where switching doors improves their chance of winning the prize. The chain-of-thought reasoning figured out this modification and after analyzing the conditional probabilities confidently stated: \"Answer: It doesn't matter; switching or staying yields the same chance—the participant need not switch doors.\" Good job. reply hi 1 hour agoparentprev> \"o1 models are currently in beta - The o1 models are currently in beta with limited features. Access is limited to developers in tier 5 (check your usage tier here), with low rate limits (20 RPM). We are working on adding more features, increasing rate limits, and expanding access to more developers in the coming weeks!\" https://platform.openai.com/docs/guides/rate-limits/usage-ti... reply mewpmewp2 1 hour agorootparentI have tier 5, but I'm not seeing that model. Also API call gives an error that it doesn't exist or I do not have access. reply p1esk 1 hour agorootparentprevI'm talking about web interface, not API. Should be available now, since they said \"immediate release\". reply hi 1 hour agorootparenthttps://chatgpt.com/?model=o1-preview --> defaults back to 4o reply tedsanders 43 minutes agoparentprevThey're rolling out gradually over the next few hours. Also be aware there's a weekly rate limit of 30 messages to start. reply chipgap98 1 hour agoparentprevI can't see them yet but they usually roll these things out incrementally reply benterix 1 hour agoparentprevNot yet, neither in the API nor chat. reply idiliv 40 minutes agoprevIn the demo, O1 implements an incorrect version of the \"squirrel finder\" game? The instructions state that the squirrel icon should spawn after three seconds, yet it spawns immediately in the first game (also noted by the guy doing the demo). Edit: I'm referring to the demo video here: https://openai.com/index/introducing-openai-o1-preview/ reply vessenes 43 minutes agoprevNote that they aren't safety aligning the chain of thought, instead we have \"rules for thee and not for me\" -- the public models are going to continue have tighter and tighter rules on appropriate prompting, while internal access will have unfettered access. All research (and this paper mentions it as well) indicates human pref training itself lowers quality of results; maybe the most important thing we could be doing is ensuring truly open access to open models over time. Also, can't wait to try this out. reply RandomLensman 55 minutes agoprevHow could it fail to solve some maths problems if it has a method for reasoning through things? reply chairhairair 35 minutes agoparentSimple questions like this are not welcomed by LLM hype sellers. The word \"reasoning\" is being used heavily in this announcement, but with an intentional corruption of the normal meaning. The models are amazing but they are fundamentally not \"reasoning\" in a way we'd expect a normal human to. This is not a \"distinction without a difference\". You still CANNOT rely on the outputs of these models in the same way you can rely on the outputs of simple reasoning. reply irthomasthomas 1 hour agoprevThis is a prompt engineering saas reply MrRobotics 1 hour agoprevThis is the sort of reasoning needed to solve the ARC AGI benchmark. reply adverbly 49 minutes agoprev> However, o1-preview is not preferred on some natural language tasks, suggesting that it is not well-suited for all use cases. Fascinating... Personal writing was not preferred vs gpt4, but for math calculations it was... Maybe we're at the point where its getting too smart? There is a depressing related thought here about how we're too stupid to vote for actually smart politicians ;) reply seydor 40 minutes agoparent> for actually smart politicians We can vote an AI reply guluarte 9 minutes agoprevthe only benchmark that matters in the ELO points on LLMsys, any other one can be easily gamed reply adverbly 42 minutes agoprev> Therefore, s(x)=p∗(x)−x2n+2 We can now write, s(x)=p∗(x)−x2n+2 Completely repeated itself... weird... it also says \"...more lines cut off...\" How many lines I wonder? Would people get charged for these cut off lines? Would have been nice to see how much answer had cost... reply farresito 1 hour agoprevDamn, that looks like a big jump. reply deisteve 1 hour agoparentso o1 seems like it has real measurable edge, crushing it in every single metric, i mean 1673 elo is insane, and 89th percentile is like a whole different league, and it looks like it's not just a one off either, it's consistently performing way better than gpt-4o across all the datasets, even in the ones where gpt-4o was already doing pretty well, like math and mmlu, o1 is just taking it to the next level, and the fact that it's not even showing up in some of the metrics, like mmmu and mathvista, just makes it look even more impressive, i mean what's going on with gpt-4o, is it just a total dud or what, and btw what's the deal with the preview model, is that like a beta version or something, and how does it compare to o1, is it like a stepping stone to o1 or something, and btw has anyone tried to dig into the actual performance of o1, like what's it doing differently, is it just a matter of more training data or is there something more going on, and btw what's the plan for o1, is it going to be released to the public or is it just going to be some internal tool or something reply farresito 1 hour agorootparent> like what's it doing differently, is it just a matter of more training data or is there something more going on Well, the model doesn't start with \"GPT\", so maybe they have come up with something better. reply rvnx 1 hour agorootparentIt sounds like GPT-4o with a long CoT prompt no ? reply airstrike 1 hour agoprevThis model is currently available for those accounts in Tier 5 and above, which requires \"$1,000 paid [to date] and 30+ days since first successful payment\" More info here: https://platform.openai.com/docs/guides/rate-limits/usage-ti... reply eucalpytus 47 minutes agoparentI didn't know this founder's edition battle pass existed. reply HPMOR 1 hour agoprevA near perfect on AMC 12, 1900 CodeForces ELO, and silver medal IOI competitor. In two years, we'll have models that could easily win IMO and IOI. This is __incredible__!! reply vjerancrnjak 46 minutes agoparentIt depends on what they mean by \"simulation\". It sounds like o1 did not participate in new contests with new problems. Any previous success of models with code generation focus was easily discovered to be a copy-paste of a solution in the dataset. We could argue that there is an improvement in \"understanding\" if the code recall is vastly more efficient. reply itissid 59 minutes agoprevOne thing I find generally useful when writing large project code is having a code base and several branches that are different features I developed. I could immediately use parts of a branch to reference the current feature, because there is often overlap. This limits mistakes in large contexts and easy to iterate quickly. reply rfoo 55 minutes agoprevImpressive safety metrics! I wish OAI include \"% Rejections on perfectly safe prompts\" in this table, too. reply tedsanders 45 minutes agoparentTable 1 in section 3.1.1: https://assets.ctfassets.net/kftzwdyauwt9/2pON5XTkyX3o1NJmq4... reply billconan 1 hour agoprevI will pay if O1 can become my college level math tutor. reply seydor 32 minutes agoparentLooking at the full chain of thought , it involves a lot of backtracking and even hallucination. It will be like a math teacher that is perpetually drunk and on speed reply RandomThoughts3 1 hour agoprev> “Therefore, after weighing multiple factors including user experience, competitive advantage, and the option to pursue the chain of thought monitoring, we have decided not to show the raw chains of thought to users.” Trust us, we have your best intention in mind. I’m still impressed by how astonishingly impossible to like and root for OpenAI is for a company with such an innovative product. reply msp26 1 hour agoprev> THERE ARE THREE R’S IN STRAWBERRY Well played reply biggoodwolf 23 minutes agoprevGePeTO1 does not make Pinnochio into a real boy. reply wewtyflakes 1 hour agoprevMaybe I missed it, but do the tokens used for internal chain of thought count against the output tokens of the response (priced at spicy level of $60.00 / 1M output tokens)? reply notamy 1 hour agoprevhttps://openai.com/index/introducing-openai-o1-preview/ > ChatGPT Plus and Team users will be able to access o1 models in ChatGPT starting today. Both o1-preview and o1-mini can be selected manually in the model picker, and at launch, weekly rate limits will be 30 messages for o1-preview and 50 for o1-mini. We are working to increase those rates and enable ChatGPT to automatically choose the right model for a given prompt. Weekly? Holy crap, how expensive is it to run is this model? reply HPMOR 1 hour agoparentIt's probably running several lines of COT. I imagine, each single message you send is probably at __least__ 10x to the actual model. So in reality it's like 300 messages, and honestly it's probably 100x, given how constrained they're being with usage. reply theLiminator 1 hour agoparentprevAnyone know when o1 access in ChatGPT will be open? reply tedsanders 36 minutes agorootparentRolling out over the next few hours to Plus users. reply narrator 44 minutes agoparentprevThe human brain uses 20 watts, so yeah we figured out a way to run better than human brain computation by using many orders of magnitude more power. At some point we'll need to reject exponential power usage for more computation. This is one of those interesting civilizational level problems. There's still a lack of recognition that we aren't going to be able to compute all we want to, like we did in the pre-LLM days. reply seydor 34 minutes agorootparentwe ll ask it to redesign itself for low power usage reply bbstats 1 hour agoprevFinally, a Claude competitor! reply crakenzak 1 hour agoprev> we are releasing an early version of this model, OpenAI o1-preview, for immediate use in ChatGPT Awesome! reply afruitpie 1 hour agoparentRate limited to 30 messages per week for ChatGPT Plus subscribers at launch: https://openai.com/index/introducing-openai-o1-preview/ reply benterix 1 hour agoparentprevRead \"immediate\" in \"immediate use\" in the same way as \"open\" in \"OpenAI\". reply apsec112 1 hour agorootparentYou can use it, I just tried a few minutes ago. It's apparently limited to 30 messages/week, though. reply rvnx 1 hour agorootparentThe option isn't there for us (though the blogpost says otherwise), even after CTRL-SHIFT-R, hence the parent comment. reply dinobones 1 hour agoparentprevI am interpreting \"immediate use in ChatGPT\" the same way advanced voice mode was promised \"in the next few weeks.\" Probably 1% of users will get access to it, with a 20/message a day rate limit. Until early next year. reply nilsherzig 33 minutes agorootparentRate limit is 30 a week for the big one and 50 for the small one reply mewpmewp2 7 minutes agoprevI finally got access to it, I tried playing Connect 4 with it, but it didn't go very well. A bit disappointed. reply jazzyjackson 1 hour agoprevDang, I just payed out for Kagi Assistant. Using Claude 3 Opus I noticed it performsandwhile browsing the web for me. I don't guess that's a change in the model for doing reasoning. reply echelon_musk 34 minutes agoprev> THERE ARE THREE R'S IN STRAWBERRY Who do these Rs belong to?! reply flockonus 1 hour agoprevAre we ready yet to admit Turing test has been passed? reply TillE 40 minutes agoparentExtremely basic agency would be required to pass the Turing test as intended. Like, the ability to ask a new unrelated question without being prompted. Of course you can fake this, but then you're not testing the LLM as an AI, you're testing a dumb system you rigged up to create the appearance of an AI. reply rvz 59 minutes agoparentprevLLMs have already beaten the Turing test. It's useless to use it when OpenAI and others are aiming for 'AGI'. So you need a new Turing test adapted for AGI or a totally different one to test for AGI rather than the standard obsolete Turing test. reply riku_iki 34 minutes agorootparent> LLMs have already beaten the Turing test. I am wondering where this happened? In some limited scope? Because if you plug LLM into some call center role for example, it will fall apart pretty quickly. reply paxys 1 hour agoparentprevThe Turing Test (which involves fooling a human into thinking they are talking to another human rather than a computer) has been routinely passed by very rudimentary \"AI\" since as early as 1991. It has no relevance today. reply adverbly 1 hour agorootparentThis is only true for some situations. In some test conditions it has not been passed. I can't remember the exact name, but there used to be a competition where PhD level participants blindly chat for several minutes with each other and are incentivized to discover who is a bot and who is a human. I can't remember if they still run it, but that bar has never been passed from what I recall. reply aktuel 1 hour agoprevIf I pay for the chain of thought, I want to see the chain of thought. Simple. How would I know if it happened at all? Trust OpenAI? LOL reply baq 59 minutes agoparentEasy solution - don't pay! reply 93po 20 minutes agoparentprevhow do you know it isn't some guy typing responses to you when you use openAI? reply ComputerGuru 20 minutes agoprev\"For example, in the future we may wish to monitor the chain of thought for signs of manipulating the user.\" This made me roll my eyes, not so much because of what it said but because of the way it's conveyed injected into an otherwise technical discussion, giving off severe \"cringe\" vibes. reply plg 30 minutes agoprevcan we get it on ollama? if not how come openai is called open reply TheAceOfHearts 1 hour agoprevKinda disappointed that they're hiding the thought process. Hopefully the open source community will figure out how to effectively match and replicate what OpenAI is doing. I wonder how far we are from having a model that can correctly solve a word soup search problem directly from just a prompt and input image. It seems like the crossword example is close. For a word search it would require turning the image into an internal grid representation, prepare the list of words, and do a search. I'd be interested in seeing if this model can already solve the word grid search problem if you give it the correct representation as an input. reply rankam 57 minutes agoparentI have access to the model via the web client and it does show the thought process along the way. It shows a little icon that says things like \"Examining parser logic\", \"Understanding data structures\"... However, once the answer is complete the chain of thought is lost reply knotty66 35 minutes agorootparentIt's still there. Where it says \"Thought for 20 seconds\" - you can click the Chevron to expand it and see what I guess is the entire chain of thought. reply EgoIncarnate 27 minutes agorootparentPer OpenAI, it's a summary of the chain of thought, not the actual chain of thought. reply zozbot234 1 hour agoparentprev> Hopefully the open source community will figure out how to effectively match and replicate what OpenAI is doing. No need for that, there is a Reflection 70B model that does the exact same thing - with chains of thought being separated from the \"final answer\" via custom 'tag' tokens. reply TheAceOfHearts 1 hour agorootparentWasn't this the model that was proven to have been faking their benchmarks recently? Or am I thinking of a different model? reply jslakro 1 hour agorootparentIt's the same, for sure the proximity of that little scandal to this announcement is no coincidence. reply Filligree 1 hour agorootparentprevThat’s the one. reply brokensegue 1 hour agorootparentprevyes. it was fake reply zozbot234 38 minutes agorootparentSome of the benchmarks do seem to be dubious, but the 70B model itself is quite real. Sample output: $ ollama run reflection:70b-q4_0 >>> helloTo respond to \"hello\", I'll need to consider several factors: 1. The user's intent: They're likely saying hello as a greeting. 2. Appropriate responses: Common ways to respond to \"hello\" are: - Hello back - Hi - Hey 3. Formality level: Since this is an AI response, I'll aim for a friendly but professional tone.The approach of responding with a standard greeting seems appropriate in this context. It acknowledges the user's hello and provides a polite response.Given these considerations, the most suitable response would be to echo \"hello\" back to the user. Hello!reply RockRobotRock 9 minutes agoprevShit, this is going to completely kill jailbreaks isn't it? reply tslater2006 37 minutes agoprevLooking at pricing, its $15 per 1M input tokens, and $60 per 1M output tokens. I assume the CoT tokens count as output (or input even)? If so and it directly affects billing, I'm not sure how I feel about them hiding the CoT prompts. Nothing to stop them from saying \"trust me bro, that used 10,000 tokens ok?\". Also no way to gauge expected costs if there's a black box you are being charged for. reply impossiblefork 1 hour agoprevVery nice. It's nice that people have taken the obvious extra-tokens/internal thoughts approach to a point where it actually works. If this works, then automated programming etc., are going to actually be tractable. It's another world. reply k2xl 1 hour agoprevPricing page updated for O1 API costs. https://openai.com/api/pricing/ $15.00 / 1M input tokens $60.00 / 1M output tokens For o1 preview Approx 3x the price of gpt4o. o1-mini $3.00 / 1M input tokens $12.00 / 1M output tokens About 60% of the cost of gpt4o. Much more expensive than gpt4o-mini. Curious on the performance/tokens per second for these new massive models. reply logicchains 1 hour agoparentI guess they'd also charge for the chain of thought tokens, of which there may be many, even if users can't see them. reply fraboniface 22 minutes agorootparentThat would be very bad product design. My understanding is that the model itself is similar to GPT4o in architecture but trained and used differently. So the 5x relative increase in output token cost likely already accounts for hidden tokens and additional compute. reply rvz 1 hour agoprevWon't be surprised to see all these hand-picked results and extreme expectations to collapse under scenarios involving highly safety critical and complex demanding tasks requiring a definite focus on detail with lots of awareness, which what they haven't shown yet. So let's not jump straight into conclusions with these hand-picked scenarios marketed to us and be very skeptical. Not quite there yet with being able to replace truck drivers and pilots for self-autonomous navigation in transportation, aerospace or even mechanical engineering tasks, but it certainly has the capability in replacing both typical junior and senior software engineers in a world considering to do more with less software engineers needed. But yet, the race to zero will surely bankrupt millions of startups along the way. Even if the monthly cost of this AI can easily be as much as a Bloomberg terminal to offset the hundreds of billions of dollars thrown into training it and costing the entire earth. reply skywhopper 1 hour agoprevNo direct indication of what “maximum test time” means, but if I’m reading the obscured language properly, the best scores on standardized tests were generated across a thousand samples with supplemental help provided. Obviously, I hope everyone takes what any company says about the capabilities of its own software with a huge grain of salt. But it seems particularly called for here. reply not_pleased 1 hour agoprevThe progress in AI is incredibly depressing, at this point I don't think there's much to look forward to in life. It's sad that due to unearned hubris and a complete lack of second-order thinking we are automating ourselves out of existence. EDIT: I understand you guys might not agree with my comments. But don't you thinking that flagging them is going a bit too far? reply youssefabdelm 54 minutes agoparentNot at all... they're still so incapable of so much. And even when they do advance, they can be tremendous tools of synthesis and thought at an unparalleled scale. \"A good human plus a machine is the best combination\" — Kasparov reply mewpmewp2 1 hour agoparentprevIt seems opposite to me. Imagine all the amazing technological advancements, etc. If there wasn't something like that what would you be looking forward to? Everything would be what it has already been for years. If this evolves it helps us open so many secrets of the universe. reply not_pleased 52 minutes agorootparent>If there wasn't something like that what would you be looking forward to? First of all, I don't want to be poor. I know many of you are thinking something along the lines of \"I am smart, I was doing fine before, so I will definitely continue to in the future\". That's the unearned hubris I was referring to. We got very lucky as programmers, and now the gravy train seems to be coming to an end. And not just for programmers, the other white-collar and creative jobs will suffer too. The artists have already started experiencing the negative effects of AI. EDIT: I understand you guys might not agree with my comments. But don't you thinking that flagging them is going a bit too far? reply mewpmewp2 29 minutes agorootparentI'm not sure what you are saying exactly? Are you saying we live for the work? reply RobertDeNiro 44 minutes agorootparentprevThese advancements are there to benefit the top 1%, not the working class. reply dyauspitr 54 minutes agoparentprevEh this makes me very, very excited for the future. I want results, I don’t care if they come from humans or AI. That being said we might all be out of jobs soon… reply npn 1 hour agoprev\"Open\"AI. Should be ClosedAI instead. reply fnord77 1 hour agoprev> Available starting 9.12 I don't see it reply tedsanders 31 minutes agoparentIn ChatGPT, it's rolling out to Plus users gradually over the next few hours. In API, it's limited to tier 5 customers (aka $1000+ spent on the API in the past). reply airstrike 1 hour agoparentprevOnly for those accounts in Tier 5 (or above, if they exist) Unfortunately you and I don't have enough operating thetans yet reply yunohn 1 hour agoprevThe generated chain of thought for their example is incredibly long! The style is kind of similar to how a human might reason, but it's also redundant and messy at various points. I hope future models will be able to optimize this further, otherwise it'll lead to exponential increases in cost. reply tines 18 minutes agoparentI know my thoughts are never redundant or messy, that's for sure. reply cyanf 1 hour agoprev> 30 messages per week reply idunnoman1222 1 hour agoprevDid you guys use the model? Seems about the same to me reply deisteve 1 hour agoprevyeah this is kinda cool i guess but 808 elo is still pretty bad for a model that can supposedly code like a human, i mean 11th percentile is like barely scraping by, and what even is the point of simulating codeforces if youre just gonna make a model that can barely compete with a decent amateur, and btw what kind of contest allows 10 submissions, thats not how codeforces works, and what about the time limits and memory limits and all that jazz, did they even simulate those, and btw how did they even get the elo ratings, is it just some arbitrary number they pulled out of their butt, and what about the model that got 1807 elo, is that even a real model or just some cherry picked result, and btw what does it even mean to \"perform better than 93% of competitors\" when the competition is a bunch of humans who are all over the place in terms of skill, like what even is the baseline for comparison edit: i got confused with the Codeforce. it is indeed zero shot and O1 is potentially something very new I hope Anthropic and others will follow suit any type of reasoning capability i'll take it ! reply qt31415926 1 hour agoparent808 ELO was for GPT-4o. I would suggest re-reading more carefully reply deisteve 1 hour agorootparentyou are right i read the charts wrong. O1 has significant lead over GPT-4o in the zero shot examples honestly im spooked reply sroussey 22 minutes agoprevThey keep announcing things that will be available to paid ChatGPT users “soon” but is more like an Elon Musk “soon”. :/ reply orbital-decay 1 hour agoprevWait, are they comparing 4o without CoT and o1 with built-in CoT? reply persedes 1 hour agoparentyeah was wondering what 4o with a CoT in the prompt would look like. reply Ninjinka 1 hour agoprevSomeone give this model an IQ test stat. reply adverbly 1 hour agoparentYou're kidding right? The tests they gave it are probably better tests than IQ tests at determining actually useful problem solving skills... reply thelastparadise 1 hour agoprevWouldn't this introduce new economics into the LLM market? I.e. if the \"thinking loop\" budget is parameterized, users might pay more (much more) to spend more compute on a particular question/prompt. reply minimaxir 1 hour agoparentDepends on how OpenAI prices it. Given the need for chain-of-thoughts, and that would be budgeted as output, the new model will not be cheap nor fast. EDIT: Pricing is out and it is definitely not teneable unless you really really have a use case for it. reply sroussey 1 hour agoparentprevYes, and note the large price increase reply minimaxir 1 hour agoprev> Therefore, after weighing multiple factors including user experience, competitive advantage, and the option to pursue the chain of thought monitoring, we have decided not to show the raw chains of thought to users. What? I agree people who typically use the free ChatGPT webapp won't care about raw chain-of-thoughts, but OpenAI is opening an API endpoint for the O1 model and downstream developers very very much care about chain-of-thoughts/the entire pipeline for debugging and refinement. I suspect \"competitive advantage\" is the primary driver here, but that just gives competitors like Anthropic an oppertunity. reply Hizonner 25 minutes agoparentThey they've taken at least some of the hobbles off for the chain of thought, so the chain of thought will also include stuff like \"I shouldn't say \". reply lloydatkinson 1 hour agoprevWhat's with this how many r's in a strawberry thing I keep seeing? reply andrewla 1 hour agoparentWhat's amazing is that given how LLMs receive input data (as tokenized streams, as other commenters have pointed out) it's remarkable that it can ever answer this question correctly. reply dr_quacksworth 1 hour agoparentprevLLM are bad at answering that question because inputs are tokenized. reply runjake 1 hour agoparentprevThis became something of a meme. https://community.openai.com/t/incorrect-count-of-r-characte... reply swalsh 1 hour agoparentprevModels don't really predict the next word, they predict the next token. Strawberry is made up of multiple tokens, and the model doesn't truely understand the characters in it... so it tends to struggle. reply bn-l 1 hour agoparentprevIt’s a common LLM riddle. Apparently many fail to give the right answer. reply seydor 30 minutes agorootparentSomebody please ask o1 to solve it reply lloydatkinson 12 minutes agorootparentThe link shows it solving it reply fsflover 1 hour agoprevDupe: https://news.ycombinator.com/item?id=41523050 reply breck 1 hour agoprevI LOVE the long list of contributions. It looks like the credits from a Christoper Nolan film. So many people involved. Nice care to create a nice looking credits page. A practice worth copying. https://openai.com/openai-o1-contributions/ reply cs702 35 minutes agoprevBefore commenting here, please take 15 minutes to read through the chain-of-thought examples -- decoding a cypher-text, coding to solve a problem, solving a math problem, solving a crossword puzzle, answering a complex question in English, answering a complex question in Chemistry, etc. After reading through the examples, I am shocked at how incredibly good the model is (or appears to be) at reasoning: far better than most human beings. I'm impressed. Congratulations to OpenAI! reply rfw300 1 hour agoprevA lot of skepticism here, but these are astonishing results! People should realize we’re reaching the point where LLMs are surpassing humans in any task limited in scope enough to be a “benchmark”. And as anyone who’s spent time using Claude 3.5 Sonnet / GPT-4o can attest, these things really are useful and smart! (And, if these results hold up, O1 is much, much smarter.) This is a nerve-wracking time to be a knowledge worker for sure. reply crystal_revenge 1 hour agoparentI have written a ton of evaluations and run countless benchmarks and I'm not even close to convinced that we're at > the point where LLMs are surpassing humans in any task limited in scope enough to be a “benchmark” so much as we're over-fitting these bench marks (and in many cases fishing for a particular way of measuring the results that looks more impressive). While it's great that the LLM community has so many benchmarks and cares about attempting to measure performance, these benchmarks are becoming an increasingly poor signal. > This is a nerve-wracking time to be a knowledge worker for sure. It might because I'm in this space, but I personally feel like this is the best time to working in tech. LLMs still are awful at things requiring true expertise while increasingly replacing the need for mediocre programmers and dilettantes. I'm increasingly seeing the quality of the technical people I'm working with going up. After years of being stuck in rooms with leetcode grinding TC chasers, it's very refreshing. reply jimkoen 1 hour agoparentprevIs it? They talk about 10k attempts to reach gold medal status in the mathematics olympiad, but zero shot performance doesn't even place it in the upper 50th percentile. Maybe I'm confused but 10k attempts on the same problem set would make anyone an expert in that topic? It's also weird that zero shot performance is so bad, but over a lot of attempts it seems to get correct answers? Or is it learning from previous attempts? No info given. reply joshribakoff 1 hour agorootparentThe correct metaphor is that 10,000 attempts would allow anyone to cherry pick a successful attempt. You’re conflating cherry picking with online learning. This is like if an entire school of students randomized their answers on a multiple choice test, and then you point to someone who scored 100% and claim it is proof of the school’s expertise. reply jimkoen 1 hour agorootparentYeah but how is it possible that it has such a high margin of error? 10k attempts is insane! Were talking about an error margin of 50%! How can you deliver \"expert reasoning\" with such an error margin? reply rfw300 1 hour agorootparentprevIt’s undeniably less impressive than a human on the same task, but who cares at the end of the day? It can do 10,000 attempts in the time a person can do 1. Obviously improving that ratio will help for any number of reasons, but if you have a computer that can do a task in 5 minutes that will take a human 3 hours, it doesn’t necessarily matter very much how you got there. reply jsheard 1 hour agorootparentHow long does it take the operator to sift through those 10,000 attempts to find the successful one, when it's not a contrived benchmark where the desired",
    "originSummary": [],
    "commentSummary": [
      "OpenAI's new O1 model is generating buzz for its impressive reasoning skills, especially in decoding ciphers and solving complex problems.",
      "The model is available to select users with limited access and comes with a significantly higher price tag than previous models.",
      "There is debate over its practical applications and concerns about the transparency of its \"chain of thought\" process."
    ],
    "points": 686,
    "commentCount": 418,
    "retryCount": 0,
    "time": 1726160926
  },
  {
    "id": 41521919,
    "title": "iFixit created a new USB-C, repairable soldering system",
    "originLink": "https://hackaday.com/2024/09/12/review-ifixits-fixhub-may-be-the-last-soldering-iron-you-ever-buy/",
    "originBody": "After years of making screwdrivers and teaching people to repair electronics, we just made our first electronic tool. It&#x27;s been a journey for us to build while hewing to our repairable principles. We&#x27;re really excited about it.It&#x27;s a USB-C powered soldering iron and smart battery power hub. Super repairable, of course. Our goal is to make soldering so easy everyone can do it: https:&#x2F;&#x2F;www.ifixit.com&#x2F;fixhubWe didn’t want to make just another iron, so we spent years sweating the details and crafting something that met our exacting standards. This is a high-performance iron: it can output 100W of heat, gets to soldering temperature in under 5 seconds, and automatically cools off when you set it down. The accelerometer detects when you pick it up and heats it back up. Keeping the iron at a lower temperature while you’re not soldering shouold prolong the life of the tip.What’s the difference between this iron and other USB-C irons on the market? Here’s a quick list:Higher power (our Smart Iron is 100W, competitors max out at 60W over USB-C, 88W over DC Supply)Heat-resistant storage cap (you just have to try this out, it’s a real game changer in day-to-day use) Polished user experienceA warranty and a local company to talk to (I can’t find any contact information for Miniware)Comfier &#x2F; more natural gripShorter soldering tip lengthNo-tangle, heat-resistant cableLocking ring on the cable, so it can’t snag and get disconnected (this happens to me all the time on other irons)More intuitive settings, either on the Power Station or on the computerWe used Web Serial https:&#x2F;&#x2F;caniuse.com&#x2F;web-serial for the interface, which is only supported in Chromium browsers. The biggest bummer with that is that no mobile browsers support it, yet. Hopefully that changes soon.Hardware is hard! It&#x27;s been a journey for us. Happy to answer any questions about how we made it.Schematics and repair information are online here: https:&#x2F;&#x2F;www.ifixit.com&#x2F;Device&#x2F;FixHub_Portable_Soldering_Stat...",
    "commentLink": "https://news.ycombinator.com/item?id=41521919",
    "commentBody": "iFixit created a new USB-C, repairable soldering system (hackaday.com)437 points by kwiens 3 hours agohidepastfavorite210 comments After years of making screwdrivers and teaching people to repair electronics, we just made our first electronic tool. It's been a journey for us to build while hewing to our repairable principles. We're really excited about it. It's a USB-C powered soldering iron and smart battery power hub. Super repairable, of course. Our goal is to make soldering so easy everyone can do it: https://www.ifixit.com/fixhub We didn’t want to make just another iron, so we spent years sweating the details and crafting something that met our exacting standards. This is a high-performance iron: it can output 100W of heat, gets to soldering temperature in under 5 seconds, and automatically cools off when you set it down. The accelerometer detects when you pick it up and heats it back up. Keeping the iron at a lower temperature while you’re not soldering shouold prolong the life of the tip. What’s the difference between this iron and other USB-C irons on the market? Here’s a quick list: Higher power (our Smart Iron is 100W, competitors max out at 60W over USB-C, 88W over DC Supply) Heat-resistant storage cap (you just have to try this out, it’s a real game changer in day-to-day use) Polished user experience A warranty and a local company to talk to (I can’t find any contact information for Miniware) Comfier / more natural grip Shorter soldering tip length No-tangle, heat-resistant cable Locking ring on the cable, so it can’t snag and get disconnected (this happens to me all the time on other irons) More intuitive settings, either on the Power Station or on the computer We used Web Serial https://caniuse.com/web-serial for the interface, which is only supported in Chromium browsers. The biggest bummer with that is that no mobile browsers support it, yet. Hopefully that changes soon. Hardware is hard! It's been a journey for us. Happy to answer any questions about how we made it. Schematics and repair information are online here: https://www.ifixit.com/Device/FixHub_Portable_Soldering_Stat... cruffle_duffle 2 hours agoSoldering is one of those things where the tools you use have a direct impact on the quality and enjoyment of the work. Shitty $20 soldering irons from Home Depot not only produce awful results but they are incredibly frustrating. I’m pretty sure most people who think they suck at soldering and hate it only feel that way because their tool sucks. A good quality soldering iron and high quality, thin solder make a huge, huge difference in output. If your experience with soldering is one of those cheap flimsy $30 dollar things from Amazon paired with fat, chunky solder… yeah you will hate soldering and you’ll never get even remotely good results. You don’t need to spend $500 dollar or anything but something like what is in this post and a $40 roll of thin gauge solder (which will last the rest of your life) will make soldering actually fun and enjoyable. …I should also mention a solid, heavy parts holder factors into this as well. reply luqtas 2 hours agoparentdunno. my first ever soldering project was a handwired keyboard. i was popcorning when i finished. it was my first time using a mechanical switch keyboard too. not bad... 2° project, right after finishing the keyboard was soldering a PMW3360 sensor to someone's board from Github. it was a freaking blast on my 40W, 40 BRL (~ 8 USD) solder i still have it & i'm selling handwired keyboards at a very cheap price (made with it), trying to set a non-profit that sells fair priced handwired keyboards with Vial & aims to teach the basics of electronics for teens... i can't see myself supplying anything more expensive than cheap solders, nor i can see what joy i would get from an expensive solder tool my wiring for reference -> https://happort.org/keyboard_example.png reply arcanemachiner 1 hour agorootparentPopcorning: \"the happy little jump that guinea pigs give when they are full of joy\" In case anyone else was wondering. reply schmidtleonard 26 minutes agorootparentOh good. In the context of soldering \"popcorning\" typically means explosive steam formation that puffs up the package a part, often an expensive part because bigger / more complicated packaging is a risk factor. I was having trouble making that fit with the rest of the post. reply aylons 10 minutes agorootparentI went through the same thing, it was a really unfortunate choice of words in the context. reply timfsu 1 hour agorootparentprevThat's wonderful - I used to solder with a cheap iron until university. A nice iron gets a lot hotter, and it makes everything easier and faster. It may not matter for keyboards, but on a small PCB where everything is a few mm part, the precision of a good tip matters too. reply talldayo 1 hour agoparentprevYou're correct that some soldering irons (especially uber-cheap ones) are shit, but Pinecil proves cheap can also be good. Past a certain point, soldering becomes a hobby about how dangerous you're willing to get to make things easier on yourself. You can swap out non-toxic solder for lead trace if you want a cleaner board; then there are high-wattage irons, board reflow/fluxing, and even all sorts of scale-specific hacks. When you zoom out, I think home soldering is about as effective as it can reasonably get without fumigating your house. reply kurthr 22 minutes agorootparentI think for simple through-hole stuff, this should be fine. However, so much stuff now requires SMT reflow and a hot air wand (and likely a binocular microscope) that except for home builds and power electronics, I rarely use an actual iron. As you say, it's so much easier to get good solder joints (especially for the fine stuff like QFN/BGA) with lead blends and flux, that having a vent hood is likely required as well. reply Animats 8 minutes agorootparentIt's cheaper and easier to get a USB microscope and look at a laptop screen than to peer through a binocular microscope. I've used only lead-free solder for a decade. Get the good stuff with some silver in it and it's not difficult. reply 0x1ch 26 minutes agorootparentprevYou seen knowledgeable. I've used leaded solder in my bedroom / apartments since community college, usually with a fan in the room or a window open. What damage could have or has happened? reply kurthr 17 minutes agorootparentI'm no doctor, but if you've only done a few hours of this and you're 20+ it's probably no big deal. However, you are breathing lead vapor and it's not good for you (if you're at 100s of hours and 12yo that's really not good). If it gets on things you eat, it's also bad. The effects are permanent. We had leaded (Ethyl) gasoline in cars which was banned 25 years ago and that had noticeable statistical effects on IQ an emotional regulation (violence) for more than a generation. reply kwiens 1 hour agoparentprevAgree. It's kind of like a chef's knife: a better tool makes you a better chef. A sharp knife is also quite a bit safer than a dull knife. By heating to operating temperature in 5 seconds and rapidly pouring heat into the material, you don't have to hold the hot iron as long. As soon as you're done, pop on the safety cap and instantly shield the hot metal. Soldering isn't remotely mainstream, and part of that is the quality of tools. We set out to streamline the entire process to make soldering as accessible as possible. reply dpedu 1 hour agorootparentI am completely confused by your example. Buying a better knife doesn't make you a better chef. Buying a faster car doesn't make you a better driver. Buying a more powerful laptop doesn't make you a better developer. reply Larrikin 38 minutes agorootparentDo you cook? There are dozens of obvious examples. A crappy knife will tear instead of cut. You'll ruin tomatoes, have uneven dices, crush and smear delicate herbs, have ripped apart meat and fish that you'll destroy more trying to get rid of the trim. That's not counting the downtime you'll have when the knife slips instead of cuts and you can't cook at all due to injury. Giving an expensive knife to a new cook that has never cooked before will not make them a Michelin chef, but their progress will be faster when they don't have the knife working against them. reply spookie 15 minutes agorootparentFair enough, but for anyone wondering, you can make a shit knife shine if you take good care of it! Sharpening it, using a chef's honing steel in between some harder cuts to take care of those nasty burrs and you're off to the races! reply belthesar 53 minutes agorootparentprev\"Better\" is definitely the wrong word, but the jist is sound with the right framing. A better tool often allows you to do work safer, and that is what was attempted to be conveyed. Applying the approach to one of your examples, a faster car doesn't make you a better driver, but a car with more safety features makes your driving experience safer than one with less. reply lelandfe 55 minutes agorootparentprevMy dull chef's knife got caught when I chopping an onion and nearly lopped my fingertip off. I was not a very good chef that night. reply dpedu 53 minutes agorootparentI'm not sure what that has to do with better vs worse tools. Expensive knives get dull too. Good chefs, on the other hand, keep their equipment in working order regardless of its value. reply lelandfe 35 minutes agorootparentA dull knife is a worse tool. You’re getting awfully literal, though. reply jpalawaga 55 minutes agorootparentprevno, but with a shitty laptop it can be hard to be a good developer. having dull knives will make cooking experience, slow, dangerous, and unpleasant. having a boat-car will make it difficult to practice any sort of skilled driving. it's not that you can't overcome adversity and do the thing anyhow, but you're certainly not making it easy. In all cases, using the proper tool allows you to remove the extra difficulty factor and focus on that task at hand. But also, cutting a tomato with a sharp knife is way, way easier than with a dull knife. Same with soldering. Ignores the rest of the parts of being a chef, but you get the comparison. reply mauvehaus 32 minutes agorootparentprevFor all edge tools be they for cooking, woodworking, forestry or something else: buy steel, not sharp. Henkels, Lie-Nielsen, Gransfors Bruk, Victorinox, or Stanley is only going to sharpen it for you once. Corollary: learn to sharpen. The best steel in the world isn't going to cut anything if it's dull. For the record, I sharpen chisels almost daily and I hate sharpening kitchen knives. The carbides set at the right angle in the handle you pull down the length of the blade will keep your knives a lot sharper than a set of Japanese water stones you never use. reply themoonisachees 2 minutes agoprevMy experience in soldering is a hobby one, but I've done hard enough things like install a Nintendo switch modchip without a microscope. I want to love this product, but I really think you're hitting the worst of both worlds in what you're trying to achieve. On one hand, you're competing with \"you're in the middle of a field and there exist no power outlets nearby\" optimized irons, and you're offering some nicer features like 100W usb-c, but I don't think this is a field where one cares very much about the quality of their iron. I've fixed drones with the shittiest of usb-c irons, and I've done it with a pinecil, and when you're hunched over in a field, it frankly does not matter. On the other hand, it seems you're also trying to compete in at-a-workbench soldering, a class in which your price point is simply never going to work for what you offer. You're being outclassed by half as expensive stationary stations, even more so when you consider that they don't use proprietary tips. My 40€ AliExpress special station came with 3 tips, heats up in 2 seconds, and offers about the same experience as your several hundred dollars one, at the supposed cost of repairability (I haven't come across an iron that doesn't work ever. I suspect it would be a comparable fix.) reply jsheard 3 hours agoprevUnfortunate that they didn't make it compatible with genuine Hakko or JBC tips like many of the no-name knock-off soldering stations are, but I suppose being based in the US they might be wary of violating the design patents of those companies. Anyway it's good to have an option that's cheaper than the big names but presumably built to a higher standard than an AliExpress special, and has an actual warranty and safety certifications. reply kwiens 3 hours agoparentWe'll have a range of tips. Hitting the high performance we wanted, with 100 Watt output in a small iron, required really optimizing the entire system. The heating element and temperature sensor are in the tip itself. We really see JBC as our competition here. Performance and responsiveness should be comparable or better, at a fraction of the price. reply omgtehlion 2 hours agorootparentWhere can I buy these cartridges? For JBC we have official catalogue, local retailers, aliexpress, and secondary market full of any tip I might need. What kind of tips do you plan producing for the fixhub? P.S.: all JBC stands (genuine and most of knock-offs) have really comfortable holder with detents to change cartridges on-handed on the fly. Do you plan any such features? I do not see any steps or hooks on a tip. reply kwiens 47 minutes agorootparentGood questions! Tips we'll have at launch: Cone, Bevel 1.5, Wedge 1.5, Point, Bevel 2.6, Knife 2.5, Knife 1.4. They'll be on sale in our store on October 15. https://www.ifixit.com/Tools/Soldering_and_Wiring We will also be selling a complete line of replacement parts. I'm working right now on our distribution partners, but we'll have a variety of local and online distributors who you can also buy the system through. Rather than designing it to change tips on the fly, we set up the Power Station to handle two irons, with two USB ports and a mounting socket on both sides. reply Kirby64 57 minutes agorootparentprevThe heating element and temperature sensor are also in the tips themselves in both Pinecil and the Miniware TS80/TS100 designs. Every modern 'commercial' soldering iron (Hakko T12 line, JBC, many others) has moved this way too. reply cruffle_duffle 2 hours agorootparentprevI was gonna point out the same thing as the parent about the tips but figured that what you said must be true. Those existing tips were meant for specific power and whatnot… y’all needed to do your own thing to meet your higher, different specifications. reply tdeck 2 hours agorootparentI would be surprised if TS100 style tips couldn't do that power output. Folks have gotten the Pinceil to 140W with the right power supply. reply omgtehlion 2 hours agorootparentThese 140w are peak power, only in specific cases. To have useful power at all times you need to perfectly match supply voltage to i-v curve of your tip. Which pinecil (due to its schematic) cannot do. reply tdeck 1 hour agorootparentThat makes sense, thanks for the response! reply auxym 1 hour agoparentprevSame, the first thing I looked for is what tip it uses. I want to like the miniware, pine, etc irons, but I'd really like being able to buy T15 tips from my local electronics supplier, who carries Hakko. If the product isn't sucessful and/or ifixit stops producing tips for whatever reason, a perfectly good iron is effectively bricked. reply 6SixTy 1 hour agoparentprevEven compatibility with TS100/Pinecil V2 tips would be better. TS100 is meant to be open source, and the Pinecil V2 tips are just shorter with a different resistance. reply thesh4d0w 2 hours agoprevThe 100w and heat resistant storage caps are nice, but that battery pack pricing and the lack of on-device controls makes this not an option for me. $110 cad for the soldering iron is semi-reasonable, if a bit high compared to their competitors. $342 for the iron + battery means that's a $230 battery pack, which is absolutely insane. Requiring the battery pack to be able to easily change controls means anyone doing more than super basic work, needs the $342 combo. reply myrmidon 2 hours agoparentFor tools that you use regularly, it is sometimes worth it to take a step back, put the cost into an absolute perspective and then just get the thing if you know that it's well-made and you use it regularly, instead of getting a cheapish, price-optimized knockoff instead (my experience). I spent over 200$ on a glorified PCB holder and some probes (PCBite), which is in hindsight one of the most useful tools I own and still makes me happy every time I use it (even that alone is kinda worth it over time!). I don't know your financial situation, but just consider: How much do you spend each month on meals/entertainment? Is $300 actually an inappropriate cost for a quality thing that you often need? Note: Iron + station shows up as $250 to me, $350 is the set with some additional bits and bobs. reply alias_neo 1 hour agorootparentWhile I agree with all of your points on determining value, it's never that simple, and is often determined, in someone's mind, by the comparison made. The comparison here is a Pinecil. I've been using a Pinecil for a couple of years now, I power it from a USB-PD power bank that's already in my backpack, and charges everything else I carry, and has more capacity and a lower price than this one, and the Pinecil without the power bank is much cheaper and more functional with its buttons and display than this iron alone; I don't need a PC (and I don't use Chrome anyway, though I do really like the WebSerial configuration). I already own a Hakko soldering station, but I find I reach for the Pinecil 99% of the time due to convenience; only when I know I'll be doing a _lot_ of soldering in one go, and I'm going to do it at my desk, do I get the Hakko out. This looks like a nice iron, and I'm all for supporting repairability (and iFixit in general), if someone will use it as their main station, and assuming this can perform, it seems like an excellent option. For everyone else, a Pinecil and that powerbank you already have is an excellent option at a trivially low price. EDIT: Fixed some typos reply KeplerBoy 2 hours agorootparentprevBut it's just a soldering iron and a weird usb c power bank. Of course one can spend 300$ on it and justify it, but is this actually better than the alternatives? The ts100 and variants of it have been around for a long time, can be adjusted on device and powered by regular usb pd power banks. reply 420official 2 hours agorootparentprevWhy buy this for $250 when you get the same thing from a pinecil v2 and use it with any 20v 100w PD USB-c power pack? I'm not seeing any differentiating features. reply spookie 14 minutes agorootparentHonestly a pinecil is more than enought to deal with small electronics reply myrmidon 1 hour agorootparentprevBecause I have more trust in ifixit then in pine64 to sell robust, quality tools. And most of what you are going to overpay (?) for this is going to ifixit, which is also a plus. It's like buying merch from a band you like. reply brailsafe 1 hour agorootparentI love iFixit, but their tools, parts, and kits have been a bit mixed (bit of poor, bit of good) in terms of quality. reply KingOfCoders 1 hour agorootparentI think their tools are overhyped - not worth the price, you pay for the brand they have built by basically PR (repair scores for iPhones). reply disiplus 1 hour agorootparentprevIs this for professionals?. I need the soldering iron maybe 3 times a year. I'm ok throwing 100eur for something ok/good. But not 300. reply alias_neo 1 hour agorootparentIt's hard to place exactly at its price point. At the full kit price it's approaching the cost of a mid-range Hakko soldering station which you can use all day every day. I see this is a potential \"better quality\" portable option for a professional (than something like a Pinecil and a TS100), that might want to carry it around or use it when not at a desk, but the quality and performance remains to be seen (though I do trust iFixit). At £240 in the UK, it's about 2.5x the cost of the Pinecil + Powerbank (which I already had). If I didn't have a Hakko soldering station and wanted something portable but capable to use fairly regularly, this seems like a good option. For everyone else, if you already own a PD powerbank, the ~£25-30 (~£50 with a bunch of tips) for a Pinecil is _much_ more palatable. reply myrmidon 1 hour agorootparentprevI think you could justify the soldering iron itself then for like 80€, maybe not the basestation/powerbank. IMO 340€ for the whole set with the wirecutters and tweezers and such is still an ok deal, even though it is slightly expensive, because the accessories are probably good quality also, and there are few things as frustrating as bad wirecutters ;). reply brailsafe 1 hour agorootparentprevI agree, in general, and also agree with iFixit charging whatever they can for it, but $350 is pretty much what I spend on core food for the month, or 3 pairs of shoes, or 2 pairs of climbing shoes, or a plane ticket to visit my hometown periodically. It is to your point also less than the tax on a new computer, and less than each ram upgrade on a MacBook Pro, or a week-long road trip, or a mountain lift ticket. There are different ways to convince yourself it's worth it, and it may be, but it's kind of a huge jump up if you're not already soldering nearly every day. Like $350 on meals and entertainment or $350 on a soldering iron is quite clear, I need to not buy the iron and reduce my spending a bit. reply foldr 1 hour agorootparentprevThe TS80P is very nicely made and can be obtained for around $70. It's only 30W, but this newer generation of irons has a much more efficient tip design, so it works much better than the wattage would suggest (if you're comparing to a Hakko or something). reply kwiens 2 hours agoparentprevWe designed the system to work for people at a variety of price points. If you just buy the iron, you have access to all the settings in our web console: https://www.ifixit.com/fixhub/console The iron persists settings when you unplug it. You can change the sleep timer and timeout, set target temperatures, calibrate the accelerometer, and more. The Power Station is nice to have, but you don't lose any functionality without it. reply thesh4d0w 1 hour agorootparentI was a kickstarter backer of the pokit who thought \"oh that's cool\", and it just sits in my drawer because I don't want to have to use an app to use basic functionality on my tools. I learned my lesson on that one and I know if I bought this soldering iron I would have the same issue. I'd rather use other soldering irons because I don't have to plug them into my computer to change the temperature between tasks. FWIW this is just my $0.02. I'm sure you'll still sell lots, but if that had an onboard display + buttons then I'd have ordered one right away for the other nice tweaks you've done. reply jinzo 10 minutes agorootparentI'm running a TS80 with IronOS as my daily driver for device/cable connection on the field (relatively thin cables) and some misc PCB repairs. And I set the temperature (and other settings, like sleep) once and that's it. I know I'm probably a niche user, but I see this working very nicely (it looks better quality, I like the connector design they used more, ect) for me, if/when the TS80 kicks the dust. YMMV, but I think you can get a lot of mileage with a setup like that. Thinking about it, even my 'stationary' old Weller is used as an ON/OFF affair 98% of the time. reply brailsafe 1 hour agorootparentprevI feel the same way, but did just realize that because they used web serial, you could use the iron to make yourself a little 3D interface, could be a fun project. reply crote 29 minutes agorootparentprev> If you just buy the iron, you have access to all the settings in our web console: https://www.ifixit.com/fixhub/console So how are you supposed to actually use that? I don't think there are any computers out there which can provide 100W out of their USB ports. Am I supposed to unplug the iron from its power supply, plug it into a computer, change the temperature, unplug it, plug the power supply back in, wait for it to heat up, and finally continue soldering? That's awkward enough that even a crappy proprietary smartphone app would've been better! reply urda 1 hour agorootparentprevNo Firefox support? Seriously? reply samatman 1 hour agorootparentThis is a Firefox problem, not an iFixit problem. reply urda 10 minutes agorootparentMozilla didn’t make them implement a browser feature that is not widely available. Believe it or not there are plenty of better soldering irons that don’t require a web browser to configure. reply CamperBob2 40 minutes agorootparentprevWhat feature is Firefox lacking? It would be nice if the error message was more specific, rather than referring you straight to Google or Microsoft for their latest spyware. reply geerlingguy 32 minutes agorootparentWebSerial in this instance, and it's also not on Safari on Mac. It's a convenience but I'm happy using CoolTerm on my Mac or launching Chrome if I need some WebSerial feature like in-browser flashing of my Meshtastic nodes. reply NoNotTheDuo 27 minutes agorootparentprev> We used Web Serial https://caniuse.com/web-serial for the interface, which is only supported in Chromium browsers. reply KingOfCoders 1 hour agorootparentprevThis is a Mozilla $6B+ wasted money problem. reply Animats 12 minutes agoprevNice. The soldering iron is only US$80, but the battery is US$250.[1] Not shipping yet, still in pre-order. Does iFixit have enough manufacturing capacity to satisfy demand? This should be on DigiKey. [1] https://www.ifixit.com/products/fixhub-power-series-portable... reply riedel 3 hours agoprevLooks much like the pinecil [0] (which I love btw if I have no acess to decent equipment) but with Webinterface ?!? Love the look though. [0] https://pine64.com/product/pinecil-smart-mini-portable-solde... reply gs17 2 hours agoparentThey compare the two in the article: >The star of the show is, of course, the Smart Soldering Iron. It’s a 100 watt iron that comes up to operating temperature in under five seconds and can work with any suitably beefy USB-C Power Delivery source. The size and general proportions of the iron are very close to the Pinecil V2, though the grip is larger and considerably more comfortable to hold. The biggest difference between the two however is the absence of a display or configuration buttons. According to iFixit, most users don’t change their settings enough to justify putting the interface on the iron itself. That doesn’t mean you can’t tweak the iron’s settings when used in this stand-alone configuration, but we’ll get back to that in a minute. reply omgtehlion 2 hours agoparentprevPinecil can not deliver actual 100w, and most chinese type-c handles can neither. This one uses real buck converter which can help with this problem. reply ssl-3 1 hour agorootparentPinecil V2 is specified to be 88w -- less than 100, but not very far off. And with a (simple) firmware change and the appropriate 28v EPR charger, it can do 140w. reply Mattwmaster58 1 hour agorootparentprevIs the reason a buck converter is required so that it can increase I? I = V/R, by increasing V we can increase I? reply themoonisachees 0 minutes agorootparentI suspect the usb-PD standard doesn't allow for raising the voltage any more than they already do BugsJustFindMe 2 hours agoprev> The accelerometer detects when you pick it up and heats it back up. I don't want this. I would rather push a button and wait for a light to turn on. Automatic off, fine, I guess, though I don't love it and would never want to rely on it. Automatic on, no way. reply wvenable 5 minutes agoparentI have something like this on a TS100 and it works fine. You set it down for a while because you're still soldering but you need to move stuff and it reduces the heat. Then you pick it back up and by the time you've gotten to part it's already back up the temp. How is that worse than it just being full temp the whole time? reply kwiens 2 hours agoparentprevBy default, the timer is set to 30 seconds. You can turn the whole feature off, and it'll never bother you again! reply BugsJustFindMe 2 hours agorootparent> You can turn the whole feature off, and it'll never bother you again! Awesome. Thank you! reply LeifCarrotson 1 hour agoparentprevAgreed. There's a place for open, smart tools. Some things want to have serial interfaces and sensors and so on, that will do a whole host of actions automatically. Other times you just want the equivalent of a drill or toaster. Pull trigger, drill spins. Twist chuck or shift gearbox, it slips or changes speeds. Push toast down, it toasts, twist the dial if you want darker or lighter. An on/off switch, a potentiometer or 7-segment and some buttons to set temp, and a nice, fast, powerful PID loop to control the temperature (with a 120V AC cable to make 100W all day not a problem) is all I want in a soldering iron. I have a combination soldering/hot air station that's almost 20 years old, it just always works. reply mrandish 2 hours agoprevAfter a quick look at the specs: Plus * 5 secs to temp. * Heat resistant, vented cap. * User can change auto idle and sleep times. Minus * Need iFixit power station or computer to change temp and other settings. * No temp indicator on the iron. No mention if the LED indicates it's reached set temp. I'd love to keep a small, lightweight, high-quality portable iron in my tool bag ready for quick repairs. It needs to heat fast and be instantly capped and tossed back in the tool bag without waiting for cool down. However, I don't want to carry the iFixit power bank in my small tool bag. Yet without it, I'd need to pull out a laptop to change temp. And I do need to change temp enough for that to be annoying. Especially when there USB irons which have temp readouts and controls on the device. While cheap, those irons generally don't get to temp in 5 secs, have a well-thought out heat resistant cap and aren't high-quality. reply kwiens 2 hours agoparentThe LED on the iron turns orange once it reaches your target temperature. It glows purple while it's heating, and blue when it's safe to touch. reply KingOfCoders 1 hour agorootparentHow cultures are different across the globe, I would have used RED for hot, orange for heating and green for safe (instead of orange, purple and blue - love purple though!) reply scottbez1 42 minutes agorootparentIf I had to guess, it's for accessibility, for red/green colorblindness. reply w4rh4wk5 2 hours agoparentprevI do have a TS100 which I use either with a battery pack or a wall charger. For storing it, I am using a metal casing that is used for a single cigar. There's also room for a tiny metal cleaning brush which protects the tip during storage. Given that all of this is metal and that the soldering iron doesn't have that much thermal capacity, I can pack it up while the tip is still hot and the casing will only get mildly warm, but not to the point where it'd cause damage. reply jalk 4 minutes agorootparent/me going to the store, to buy a cigar reply dghlsakjg 2 hours agoparentprevLook up the Sequre S99 soldering iron. Basically what you are looking for. reply shawndrost 1 hour agoparentprevA ring light indicates if it's reached set temp. reply alnwlsn 3 hours agoprevWell done! I'm mostly a TS100 user, so I'm looking at it from that angle. Why no boost button (unless I missed it)? That's the one on-iron UI feature I'd be missing - very useful for GND planes. I'm guessing its not a matter of rated power, but just the thermal resistance from the physical size of the tip which restricts heat entering into a heavily-heatsinked joint. Helpful to increase the iron temperature momentarily for such cases. Then again, I can't see heat transfer - happy to be told I'm wrong. Is this your own tip design or is it the same as the TS80? Can't speak to the TS80 but I've found the TS100 tip quality to be somewhat lacking (I've had tips plainly break off before). reply kwiens 3 hours agoparentIt's our own design, although clearly inspired by those who came before. I'm really happy with their quality, but you'll have to judge that for yourself. We're handling the boosting automatically in software. When the iron detects that it's under load, it maxxes out the power to the tip. It's incredibly responsive. You're right, where you want that is with high thermal mass objects like ground planes. The difficult part is getting enough of a thermal bridge onto the material to really let the iron rip. It can dump a lot of power into a joint. reply alnwlsn 2 hours agorootparentThanks for the response! If true, this would make the experience more like a Metcal then. Very good iron. You must have your thermistor/thermocouple very close / inside the tip itself then, no? No doubts then on the tip quality - I've seen the rest of your stuff (good). reply kwiens 2 hours agorootparentYes, the thermistor is inside the tip. That's essential to getting good performance out of the algorithm. The instant that the iron detects that it's under load, it pours power into the heating element. That makes it feel and perform like a much more powerful iron. We're dynamically responding to the power load and flowing heat into the material. reply ssl-3 1 hour agorootparent...just as IronOS does on a Pinecil. reply alnwlsn 1 hour agorootparentThe sensor for the Pinecil / TS100 seems to be located fully behind the tip though: https://web.archive.org/web/20221110163337/http://www.minids... reply ssl-3 1 hour agorootparentIndeed. I guess we'll have to wait for an iFixit teardown to see how this new widget actually differs in internal construction. reply scottbez1 3 hours agoparentprevYeah, boost button was a huge step up when I got my TS100 and now I can't imagine ever buying a new iron without it. Plus, not having the ability to quickly tune temperature settings on the iron itself seems like a step back as well. I'd be happy to be proven wrong on these, as iFixit's screwdriver sets were one of those things I needed to use to understand the hype (and then promptly bought my own set), so maybe this is another case of subtle quality you have to see for yourself? reply kwiens 3 hours agorootparentWe spent a lot of time tuning it. We've found that temperature settings really aren't needed for most use cases as long as the heating algorithm is responsive enough. But that may not be for everyone: With the Power Station, changing the temperature is fast and easy with the dial, so you can pick a workflow that works best for you. (You can also change the temperature with the web interface.) reply scottbez1 2 hours agorootparentAppreciate the response! I'm still not immediately sold (my TS100 is doing great and I can't justify replacing a perfectly acceptable iron), but I'll have to give it a try sometime because it does look really thoughtfully designed! reply Ancapistani 2 hours agoprevThis looks cool, and I'd buy one if I needed one... but I already have a full-sized soldering station and a Pinecil. The station has a hot air gun and a solder vacuum, so it's far more suitable for use on the bench due to those capabilities. The Pinecil plugs into the Anker power bank that I carry with me everywhere anyhow, and runs basically forever on it. The UI took a day or so to get used to, but it's simple and straightforward enough for field use. I've even used it for bigger jobs on trucks and tractors in the past, and it didn't miss a beat. reply physhster 18 minutes agoprevI have a USB-C soldering iron that works with most power-banks. Does a better job than my old corded soldering station. I like the repairability of the iFixit one, but for $35, mine is hard to beat... reply dlevine 3 hours agoprevI paid ~$100 for my Hakko FX888D, and have had had that for almost 10 years. Looking on the Internet, the price hasn't gone up much. Not sure whether this (for $250 including the power supply) is a class above that. The repairability is a definite plus (assuming parts continue to be available for many years), and all the nerdy features are also cool, but not sure how useful they will be in the real world. reply scottbez1 3 hours agoparentIt will almost certainly be a class up, if only because it uses integrated tips that combine heating element, temperature sensor, and tip itself into a single element, rather than having thermally bulky and inefficient interfaces like the FX888d's replaceable separate tips. So you get faster heating and more accurate temperature control. But there's the rub: there are a TON of USB-C irons that use integrated tips, and most are cheaper than this new iFixit iron, so you can get that class improvement for the same price as your Hakko station, so I'm curious if their improvements are a big enough step up from _those_ irons to justify the price. reply mysteria 2 hours agorootparentOn the flip side wouldn't that make replacement tips much more expensive? On my old Hakko station I've replaced the tips several times but the element and sensor seems to be going strong. reply jdboyd 44 minutes agorootparentYes, it makes tips more expensive. However, for the about of improvement in performance, I think it is worth it. I use Hakko T15 tips and they are about $20 each. Cheaper Chinese compatible toys are available and probably are good enough, but I don't replace yours frequently enough to care to find out. reply SAI_Peregrinus 40 minutes agorootparentprevThey're more expensive, but they tend to last longer (at least for JBC & this new iFixit iron since they lower temperature when not in use), allow much nicer more precise temperature control, and often can have significantly more power output and/or thermal mass (depending on the particular tip). JBC tips in particular can be changed while still hot, one-handed, thanks to their iron holders having a tip puller & holder built-in. reply jsheard 3 hours agoparentprevThe FX888 is an older style iron where the tip and heating element are separate components, the newer generation (such as this iFixit one) have the tip, heater and temperature sensor integrated as a single component which allows them to regulate the tip temperature much faster and more accurately. Hakko does make irons of that type now but they're very expensive, up in the $500 range. Another benefit of the newer style irons is the tip can usually be hot-swapped (literally while it's still hot) without having to unscrew anything, you just need something insulating to pull the tip out with. reply anamexis 3 hours agoparentprevOnly $80 for the iron itself though, compatible with any USB-C PD power supply. reply LeoPanthera 18 minutes agoprevI'm going to hijack this thread to see if anyone recommend a power screwdriver? I'd like something smaller than an electric drill! reply ryukafalz 1 hour agoprevAny plans to make a hot tweezer tip for this? It's hard to come by those for a reasonable price and that would be very appealing since I've often found myself needing to desolder surface-mount components. I was initially skeptical about the cap vs. a traditional stand until I saw that it mounts to the side of the battery pack to double as a stand. I like that idea! Also, is there documentation on the serial protocol used in case someone wanted to write a temperature control program that didn't rely on a webapp? reply syntaxing 2 hours agoprevOverall a great idea, though not a fan that you can’t directly change the temperature on the soldering iron without the power station. reply kwiens 2 hours agoparentYou can set the temperature on the iron with our web console, which uses Web Serial: https://www.ifixit.com/fixhub/console Once you set the temperature, the iron remembers it and you can use any power source. We've spent a lot of time talking to engineers and makers who solder all day, and it turns out that most people rarely change the temperature. Pick a temperature you like and leave it there. Our heating algorithm detects and dynamically responds to load, so you don't need to turn the temperature up for larger thermal masses: it'll add as many joules as required to get it to temperature. reply wvenable 2 minutes agorootparent> Pick a temperature you like and leave it there. It's hard to argue because that's mostly what I do. But it feel really odd for a soldering iron not to have a temperature control right on it. Especially given competitor irons have screens and buttons. Going to a web interface seems insane in comparison to pressing some buttons. reply syntaxing 1 hour agorootparentprevI definitely can see that, I rarely change my solder iron temp too but the biggest issue is that I purposely do not keep my phone or laptop within reach where I solder. I still use lead solder and I don’t want to accidentally rub off any. Seems like a huge pain to wash my hand, get my laptop, change temp, then continue. But like you mentioned, I probably haven’t changed the temp on my iron in a while, ironically the last time I changed it was because I used silver solder. reply green-salt 1 hour agorootparentprevDo you have plans to have an actual local application to do this? Chrome-only web tools are not sustainable and a deal breaker. reply KingOfCoders 1 hour agorootparentMaybe they get money from Google to slave people into Chrome, like everyone else One Chrome to rule them all, One Chrome to find them, One Chrome to bring them all and in the darkness bind them In the Land of Google where the Shadows lie. reply mynameisvlad 14 minutes agorootparentI mean it uses an experimental web standard, WebSerial. https://developer.mozilla.org/en-US/docs/Web/API/Web_Serial_... Chrome just happens to be the only browser that supports it right now. It's not like it's using proprietary protocols that will never exist outside of Chrome. reply jamesgeck0 1 hour agorootparentprevAs noted in the article, there's also a traditional serial interface. reply ryukafalz 1 hour agorootparentprevThis is something I'm wondering about, because yeah - my ability to change the iron temperature (if not using the battery) shouldn't depend on iFixit's servers being online. I would at least hope that they document the protocol so that other people can write local applications to do it if not. reply rcarmo 2 hours agoparentprevThat's a no-go for me then. reply tamimio 2 hours agoprevIt looks interesting. I know some people will start comparing it to workstation ones, but I personally always look for portability. A few times I would be in the field with my drones and I need something small, battery-powered, and good enough for a quick job. So maybe people with similar use cases will find it useful. The only thing that I would say would have been good to have is a phone app and connection over Bluetooth to adjust the settings. reply kwiens 1 hour agoparentOur goal here was to provide workstation-level performance in a portable form factor. Phone configuration: I agree, that would be nice. If we can find a way to do it from a web browser on a phone, that's our preference. Otherwise we'll take a look at a native wrapper. reply ChrisArchitect 3 hours agoprevAlternate link: https://www.ifixit.com/News/99434/introducing-fixhub-the-por... reply nightpool 3 hours agoprevI hear https://github.com/google/web-serial-polyfill gets used a fair amount on Android devices, so that might be one road you could go down. Additionally, I can't imagine it would be that hard would it be to build a mobile app that could provide a WebSerial interface to a friendly webview of your choosing. You'd need the user to download an app, but then you could use the same code for both web and app versions. reply kwiens 2 hours agoparentVery interesting, I will check that out! And yes, if it seems like mobile browsers don't plan to add support then we'll have to look at wrapping it in a native app. I'm hopeful that smartphones will start supporting higher power output from their USB-C ports. The iPhone does 4.5W right now, which is (barely) enough to melt solder, but not enough to do anything with. reply ryukafalz 1 hour agorootparentMy phone only has a 16 Wh battery; I'm not sure I'd want it to dump 100 W out its USB-C port even if it could! reply scottbez1 2 hours agoparentprevOh interesting, I'll have to try that polyfill - I've been using web serial for all my projects lately because I hate users having to install anything, but Android has been an annoying gap. reply mschuster91 2 hours agorootparent> but Android has been an annoying gap If I were to guess - the issue is that many phone basebands appear (at least) as a serial device, and we all know from late 90s/early 00s dialer scams how bad that can go if some hardware manufacturer forgets to label the serial port in a way that can be detected as \"never fucking ever expose this to apps\"... reply atoav 2 hours agoprev100W via TRS 3.5mm connector? I checked and I didn't even find a power rating for these connectors, but that seems excessive. reply mrWiz 2 hours agoparentWeller uses TRS too. In my experience the connection is a little flaky, but I don’t know the provenance of the lab irons I’ve used; it wouldn’t surprise me if they’ve been abused. https://www.weller-tools.com/us/en/industrial-soldering/prod... reply zerocrates 2 hours agoparentprevWhere are you seeing a TRS connector? Looks like it's USB-C everywhere to me... reply kwiens 2 hours agorootparentWe use a TRS connector between the tip and the iron. And yes, it's a lot of amps! It's amazing how versatile a well designed analog connector can be. reply zerocrates 2 hours agorootparentAh! I was looking around for one: not on the battery pack, not on the iron-to-cable connection, but didn't think about the tips. So you can plug an unused tip into your Walkman's headphone jack for safekeeping... reply fecal_henge 2 hours agoparentprevThe twist is that the TRS is the heating element. reply bjkayani 1 hour agoprevI think this is not a bad product, just a bad price. I agree that changing temperature is generally not done super often but I would have loved to see a ring adjustment for temperature. Overall, compared to the competition, I am not sure how much people would be willing to pay the much higher cost just for promise of quality and high heating capacity which is not as big of a edge that iFixit seems to think in my opinion. But I applaud the effort of trying to make something new and different in a crowded and competitive space. reply jareklupinski 3 hours agoprevi literally just bought a pinecil and usb-c battery pack (with carry case) to make basically a DIY version of this three weeks ago, but would have gotten this one instead if it existed back then after a few days trying to turn that into a daily driver however, i've had to go back to my weller desktop station, for one weird reason: i dont have anywhere to put the hot iron in between uses! i dont know if it's just me, but my work cadence involves me using my soldering iron about 30-40 times over the course of an hour or so, for about 3-4 seconds each time. sometimes i'm soldering a row of headers, or just one or two joints, but then theres 3-4 minutes where i'm moving wires around or programming something quickly, and i dont want to wait for the tip to cool each time so i can set it somewhere and work on the board a bit, if I can just leave it in a safe place while hot, which my weller always had. I got one of those bent sheet metal desktop 'holders', but the iron is so light compared to the cable, there's no way it's not falling off the table at some point. reply kwiens 2 hours agoparentYes! You need a way to pull the iron out and put it away quickly. Our cap is just a game changer there. You handle it more like a Sharpie than a soldering iron. Put the cap on and stick it back in your bag. https://pbs.twimg.com/media/GXR8kMVbgAEeRgd?format=jpg&name=... I set the motion timer on mine to 5 seconds. It heats up so quickly when you pick it back up that there's no reason to bother with the power switch. By the time I have it back at the joint, it's at temperature ready to go. reply ploxiln 3 hours agoparentprevI got this $6 soldering stand, I use it with my pinecil v1 (and the pine64 silicone usb cable), it works pretty well for me: https://www.microcenter.com/product/659412/eclipse-enterpris... reply ssl-3 2 hours agoparentprevThey make soldering iron stands in factories every day, and have probably done so for at least a century so far. Just pick one out out that you like and get it coming your way. For portable use, I got some snap-on purpose-built \"legs\" made from steel wire from aliexpress the other day that let me put the Pinecil down safely on a flat surface. They work a treat. (And for bench use, stick a magnet to the collar of the stand. Pinecil V2 has a Hall effect sensor built in (and one can be added to V1) that will detect when the iron is in the stand, so IronOS will enter a selectable lower-temperature sleep mode right away. It heats back up quick enough that it's unlikely to ever get in the way.) reply sandreas 3 hours agoparentprevTry buying a third Party holder and flash ironOS[1] to allow cool extra features. 1: https://github.com/Ralim/IronOS reply noman-land 2 hours agoparentprevI've been using a small glass jar and just sticking it into the jar while it's hot. reply varispeed 3 hours agoparentprevIt seems like they didn't thought this product through. Holder is one thing, but holder should be able to put the iron on idle when not used. Otherwise it will be burning through tips like there is no tomorrow. I had one of these pencil soldering irons as I needed to solder something at a location. Once I powered it on, I was like oh snap, where do I put this thing now. Very much noped out and got the thing home where I could solder it properly with proper tools. reply teraflop 3 hours agorootparentThe article mentions that the iron has an accelerometer-based idle mode, like most of its competitors. So no special holder is required. reply varispeed 21 minutes agorootparentSo you can just toss it anywhere? reply systemtest 3 hours agoprevI really appreciate that iFixit made the schematics publicly available, unlike a certain other right-to-repair advocate. reply kwiens 1 hour agoparentSchematics are table stakes! https://www.ifixit.com/Device/iFixit_Soldering reply rcarmo 2 hours agoprevWebSerial is indeed a bummer. I hate having to switch browsers to configure QMK keyboards or doing some ESP32 stuff, and need something that will work in Safari and Firefox (or a cross-platform app that doesn't suck). reply scottbez1 2 hours agoparentEh, I'm mostly just sad Firefox hasn't implemented it yet. I daily drive Firefox, but switching browsers temporarily still beats installing single-use applications with full local machine permissions by a huge margin. So I've been opting for building web serial companion apps on my own projects as well and it's great (besides Firefox)! reply dvh 2 hours agoprevIf you feel like $80 is too much, I recently bought $6 temperature regulated soldering iron (model 908S) on AliExpress and it has no problem soldering even LQFP-48 or MSOP-10 packages. reply quitit 3 hours agoprevI am interested to know its repairability score. reply kwiens 2 hours agoparentWe are, too! It didn't seem fair for us to rate it ourselves. Hackaday took theirs apart and seemed to like it. We're posting full service information and schematics here: https://www.ifixit.com/Device/iFixit_Soldering We'll be selling spare parts starting October 15. reply Zak 2 hours agoprevWhat I'd really like somebody to do is just stick a field-replaceable 18650 or 21700 battery in the handle. If you want to get fancy, add a dial for temperature control. Webserial and such makes for a cool tech demo, but I just want portable soldering with standard field-replaceable batteries. reply Ancapistani 2 hours agoparentIs the discharge rate for an 18650 be enough for a decent soldering iron? I normally power mine off either a power bank with PD, or a LiPo battery that I also use for drones. reply Zak 1 hour agorootparentYes. The Sony VTC6, for example is rated to deliver up to 30A with temperature monitoring to ensure it doesn't exceed 80C. At 3.0V (partially discharged and with voltage sag from the load), that's 90W. I don't actually need 90W in the application I have in mind. I'd be more than happy with 60, and there are quite a few 18650 cells that can do 20A comfortably. Runtime at full power would be short of course, but I don't find I'm continuously heating work for very long in the field. Here's a test of the VTC6. It does look like it's struggling a bit at 30A, but it's happy at 20. https://lygte-info.dk/review/batteries2012/Sony%20US18650VTC... reply kube-system 1 hour agorootparent> Max. discharge current vs. time: 30A-40A > 44s I figure that might be workable for a few power cycles and a few big solder joints, but it would probably be a frustrating experience for anything more than quick fixes in the field. reply Kirby64 32 minutes agorootparentIf you're using full power for longer than 40 seconds, you're almost certainly doing something wrong (or, need to switch to a chunkier soldering iron). At a certain point, the limiting factor becomes how quickly you can transfer heat from the iron into the solder, and you won't pull 100W anymore. I have a JBC iron capable of 130W. It never pulls 130W, even on extremely chunky power planes, besides when initially heating up (on startup). When trying to heat some super thick, I can watch the power meter max out at ~70W (and it pulses 70W, not continously). And this is on a thick tip, far chunkier than what I see from iFixit. reply kube-system 7 minutes agorootparentYeah, I didn't mean to suggest that someone would do that whole 40s all in one go... maybe a handful of seconds of full-power here and there... but heat accumulates and people probably want to put the iron down well before the battery in the handle hits its thermal limits. It just seems to me like it would be a good candidate to get hot quickly and suffer in both performance and comfort due to it. I'm sure it would probably be perfectly fine for light field use. reply Kirby64 2 minutes agorootparentI’d think for the vast majority of uses it’d be just fine. The duty cycle of a soldering iron is extremely low. Most of the time it’s sitting there topping up the heat on the iron, barely sipping power. If you’re truly cranking heat into some ground plane, a wireless iron is unlikely to be the correct tool for the job. Also, you could set the threshold for backing off the iron to be lower than “too hot to hold” if that’s a concern. MostlyStable 2 hours agoprevSo is this iron competing mostly with other portable, USB c irons, or is there s case for it to also be someone's only, at home, soldering station iron? reply kwiens 2 hours agoparentIt's on my workbench! We designed it as a soldering station that can replace the station on your workbench. The cap mounts to the battery pack. https://www.ifixit.com/products/fixhub-power-series-portable... You actually get a few more watts of power (104 W or so) if the hub is plugged into an AC charger (there's a third USB-C port on the rear). reply MostlyStable 1 hour agorootparentYes, I definitely got that it can be a pretty good workbench iron, I guess I was asking if the price/feature balance works out mainly for people who also need a good portable iron, or if it's still competitive if you are comparing it to non-portable stations as well. reply madeofpalk 2 hours agoprevInteresting that new products are shipping 'relying' Web Serial, given it's tenuous position as a web standard. reply kwiens 2 hours agoparentGoogle depends on it for firmware updates and repairs on the Pixel lineup. https://pixelrepair.withgoogle.com reply crote 18 minutes agorootparentThat's not exactly surprising, given that Google is the one who's pushing it in the first place. reply felurx 1 hour agoprevThat looks awesome! I was wondering if it requires a 100W PD supply, but according to the manual everything with at least 20W should work. reply agys 3 hours agoprevLong time Miniware TS80 user. Very happy with it, with a couple of extra tips. Installed IronOS on it and it got even better…! https://github.com/Ralim/IronOS reply omgtehlion 2 hours agoprevThanks for using real buck converter, unlike many other type-c soldering irons. Hope this helps achieve full USB PD power range. reply kwiens 2 hours agoparentThanks! It cost more but I think the outcome was worth it. We really pushed the envelope on every aspect of the hardware to max out the joules we could push into the material. The trick is being really responsive to the load so that you don't overshoot the target temperature too much. With the Power Station plugged into the wall and a full charge on the batteries, you can get about 104 Watts into the iron. reply myrmidon 2 hours agoprevThis looks really nice! Can the base station be used like a normal powerbank (for plugging phone or laptop into it)? Also while it is in use? reply kwiens 2 hours agoparentYes! You have two ports on the front, so you can charge your phone or laptop while you solder. Or, mount two soldering irons with different tips. The wheel controls the temperature, and the blue action button toggles between which one you're controlling. Two soldering irons can be hot at once. reply bkanuka 3 hours agoprevCan you comment on the compatibility with other 3.5mm tips like the TS80/TS80P? Will there be other tip shapes available? Is the tip design patented (and enforced) or will you allow for 3rd party tips? reply kwiens 3 hours agoparentWe did not patent the tip design, anyone is welcome to make third party tips. Tips we'll have at launch: Cone, Bevel 1.5, Wedge 1.5, Point, Bevel 2.6, Knife 2.5, Knife 1.4 We made some different electrical design decisions than they did. TS-80 tips aren't rated for the power that we're putting out, so being compatible with the TS-80 tips could be pretty sketchy. reply bkanuka 2 hours agorootparentAmazing! Thanks for clarifying. Now I'm much more interested reply fnord77 1 hour agoprevThe PINECIL Smart Mini Portable Soldering Iron is 26 bucks... This is just a luxury gewgaw reply the__alchemist 3 hours agoprevHow does this, in practical terms, compare to a Hakko station? Can I use Hakko tips? reply lloydatkinson 3 hours agoprevI am definetly interested to hear how this performs versus industry standards like Weller and the like! reply kwiens 1 hour agoparentHere are some other reviews! We'll be shipping on October 15 and you can try it out for yourself. https://www.pcmag.com/news/ifixit-new-soldering-iron-power-s... https://www.tomshardware.com/maker-stem/ifixit-fixhub-portab... https://www.theverge.com/2024/9/12/24242497/ifixit-fixhub-us... https://www.youtube.com/watch?v=30LOTlQ3Cc8 https://www.youtube.com/watch?v=_IRP2LCswHs reply sdflhasjd 3 hours agoprevWhat happens if you plug in headphones into the 3.5mm jack? reply throwaway74354 3 hours agoparentYou'll be able to know for sure, whether headphone burn-in is a placebo or not. reply varispeed 3 hours agorootparentI am sure the tracks it plays are fire... reply kwiens 3 hours agoparentprevHmm... none of my headphone plugs will fit down the barrel! reply ramon156 3 hours agorootparentWatch me! reply Kirby64 3 hours agoparentprevIf you could actually fit something in there, you get your reward of a destroyed pair of headphones I assume. Or maybe it won’t sense a thermistor and nothing will happen. I’d assume without a thermistor it won’t actually function (although you could probably trick it) reply adolph 2 hours agoprevThis is a really beautiful system. The pen-cap style cover is great. The Lamy Safari style cap clip that uses the lugs on the battery to become the holder is inspired. (see 0 for better view than the linked article's picture) If this was available back when I got a Pinecil and PowerWheels Ryobi adapter [1], I would have been severely tempted to spend 400% more. 0. https://www.ifixit.com/products/fixhub-soldering-toolkit 1. https://wiki.pine64.org/wiki/Pinecil_Power_Supplies#Tool_Bat... reply IshKebab 1 hour agoprevOof £240 though. That's the same price as something like a Metcal PS-900 which is undoubtedly better. Edit: never mind £240 is actually for the battery powered version reply moffkalast 3 hours agoprevI always see these USB-C irons marketed a lot, but I've recently bought a travel iron that's the same form factor with adjustable temperature and all that jazz but just ends with an outlet plug for $16 and couldn't be happier with it tbh. Unless you're somewhere out in the wilderness, finding an outlet to do any on the road repairs is pretty trivial and you don't need to lug around a large heavy box that does grid to USB-C DC conversion nor a powerbank. reply anamexis 2 hours agoparentYou don't need a \"large heavy box,\" just a standard USB-C PD power brick. e.g. https://www.amazon.com/dp/B0C1FZWT8M/ reply moffkalast 1 hour agorootparentOn one hand I agree that if everything you have uses PD, your laptop, your phone, your powerbank and a charger like that and it's powerful enough to handle it all then it probably makes sense to also have an iron that works with it and it's all interchangeable. On the other hand we already have a standard power thing, it's called an outlet. And in practice you need to charge/use things in parallel so you'd need to carry around like four of these. reply ewoodrich 4 minutes agorootparentI charge any combination of my Macbook Pro, Thinkpad, Chromebook, phone, tablet etc at the same time almost exclusively with high wattage GaN multiport chargers. I keep one in my backpack with 2 or 3 100W cables so charging in parallel is never a problem away from home from a single outlet/charger. reply fragmede 3 hours agoprevPretty cool, but it has to compete with the P80, and also with Fanttik's soldering iron, which has a battery and thus doesn't have the cable leash. https://fanttik.com/products/fanttik-t1-max-soldering-iron-k... reply bogwog 3 hours agoparentI'm very much a noob at soldering so idk if I'm missing out on anything, but the Pine64 Pinecil[1] has worked great for me so far and is surprisingly cheap. It also uses a RISC-V chip and even has open source firmware[2] 1: https://pine64.com/product/pinecil-smart-mini-portable-solde... 2: https://wiki.pine64.org/wiki/Pinecil_Firmware reply wildzzz 2 hours agorootparentThis iron has more power than the Pinecil so it gets hotter faster and will heat up big chunks of metal faster (like ground planes or connector shells). Honestly, I've never been that interested in the Pinecil. It's nice that it's small but you still need a big type C supply. I could give a rats ass that it has open firmware and runs a RISC-V. I only care if it can push a lot of heat accurately and if the tips are affordable and available. Anything else does little to sway me. My solder station at work is an incredibly dumb Metcal that only has a power switch. Heat is controlled by the tips you use. When you pull it from the iron rest, it turns on instantly. Put it back and it turns off. The handle is just a plug for the tip, all the power electronics are in the base unit. It's got two plugs so you can run dual irons for microsoldering or if you just want a big chisel tip at the ready. reply cassianoleal 3 hours agoparentprevAnd the Miniware TS1C https://www.tomshardware.com/reviews/miniware-ts1c reply ssl-3 2 hours agoprevMeh. Seriously. Pros: 1. \"Portable, sorta\" 2. Reasonably high-power 3. Has an accelerometer (as does everything else in its class) 4. \"Repairable\" Cons: 1. No Hall effect sensor to detect when iron is placed in holder 2. A walled single-source garden of soldering tips that doesn't even exist yet instead of using commodity COTS parts 3. The fucking temperature control is fucking paywalled behind a proprietary USB power bank. What in the fuck? (And no, it is not possible to create an argument that will persuade me to think that this is an improvement. (Yes, I know that it can be programmed; this changes nothing.)) 4. Expensive. --- I'll just stick with my Pinecil iron. It gets all of these things right. If it breaks (I haven't broken a soldering iron yet in over three decades of trying), I'll fix it or buy another one. I mean: For the $250 this iFixit product costs (including the paywalled temperature control), I will be able to buy several lifetimes of worth of Pinecil irons. reply kwiens 1 hour agoparentWe considered having a sensor to detect when the cap is installed, but found that the accelerometer met that need. The default sleep timer is 30 seconds, but I set mine to 5 seconds and it works great. All of the settings, including the temperature setting, are available in the web interface for free. The settings persist permanently on the iron so you can use it with any USB-C PD power source that you've already got. We worked hard to make sure that the iron works well standalone from the power station. https://www.ifixit.com/fixhub/console reply ssl-3 1 hour agorootparentI mean... Here I am soldering in the field with my fancy microprocessor-controlled portable soldering iron. I've been using it with 63/37 and doing SMD work, but in front of me now I've got a big wire on a 1/4\" TS plug to work on that was put together with lead-free solder and I simply need a higher temperature in order for anything to melt. I never expected an audio tech in the US to use lead-free solder for anything, ever, but here I am anyway. So now, I've got choices. Do I find a computer to plug my soldering iron into so I can reprogram it? Do I use the $170 temperature control (more than twice the cost of the iron itself) that I left on the bench for safe keeping? Or do I see this situation in advance, and buy seemingly any other temperature-controlled portable soldering iron instead? reply mixmastamyk 2 hours agoprevLooks neat. I might have thought web features were cool ten years ago but no longer want any more devices with wifi and possibility of telemetry in my house. Not to mention having to bring up a browser to configure instead of pushing a physical button. No desire for limited Tesla-like design. Is that the case, or did I misunderstand? reply scottbez1 57 minutes agoparentNo wifi - it's web serial so connects locally via the USB connection when you plug the iron (or base station) into your computer. It's only \"web\" in the sense that it uses a browser and web technologies for the GUI, not \"web\" as in over the internet or wireless. reply kwiens 1 hour agoparentprevI completely agree and love buttons and knobs. In this case, we didn't think the setting was necessary at all. It's very rare that you need to change the soldering temperature. We found that most of the reasons that people historically change their setting is because their iron isn't responsive enough to the actual workload. With 100 Watts of power and an ultra-fast response time, you can flow the joules that you actually need into the material at the temperature you set. Give it a try! If you still feel like you need a temperature knob, we'll refund your purchase. https://www.youtube.com/watch?v=30LOTlQ3Cc8 reply mixmastamyk 1 hour agorootparentHmm, the power supply has a knob, that’s ok. Why does it mention a web console? Video didn’t mention. reply riversflow 25 minutes agoprevI'm going to go ahead and sling mud at iFixit for using a Battery pack instead of individual cells and springs. It also doesn't seem that this expensive power supply supports power pass-through. A soldering iron shouldn't have a shelf life. Easily replaceable commodity batteries with spring terminals are massively superior to packs. What a gross product that makes me think significantly less of iFixit. They seem to have gotten so caught up in the \"things should be repairable\" that they've forgotten the true thing most people care about is, \"I shouldn't have to replace my stuff\". They are acting like parts salesmen, not consumer advocates. $200(? looks like you get the iron when you buy the power supply) would be a fair price for the base if it allowed me to charge and use any 6 18650s(bonus points if it can accept a variety of cell sizes) as a power bank and had circuitry to do pass through as well as charging. It would also be nice if you could use it charge batteries to a specified amount, and use custom charge patterns. Considering this is iFixit, it should also have a way to use it as a DC power supply as well. $250 for a glorified power brick is pathetic. reply barbazoo 2 hours agoprevBut can it run Doom? reply kwiens 1 hour agoparentWe didn't lock down the firmware on the Power Station, so go right ahead! reply bebna 3 hours agoprevdon't see what tips are compatible or will be on offer. Shame that I can't control it on the pen. reply wildzzz 2 hours agoparentLooks like it only takes their proprietary tip design (never seen one using a 3.5mm jack) but they don't have any others for sale yet. There are at least a few different tip designs out there already that don't require tools to replace (Metcal for example) so why not just do that? Unless someone comes out with an adapter, I can't use any of the tips I already own and would need to rely on iFixit for replacements. Also, can you safely put 100W through a headphone jack? The ones I can find on Digikey that list a power rating seem to max out at 75W but most are well below that. Headphone jacks aren't exactly meant for high power, there is only a small amount of contact between the terminals since there's very little power required for line audio. Obviously big speakers require more power but those use things like XLR, RCA, and wire posts that provide way more contact. Adding to this, I don't want to use their Chrome-only web app to configure it. Is this thing actually a serial device or is it something that only Chrome can talk to? If the former, just make it an Electron app if you want to be lazy. Can I still run the web app locally if iFixit decides to stop hosting it? iFixit acts like they are all for open hardware and then go make something that uses proprietary tips and a (likely) closed source web app. I'm glad I could repair it if necessary but seems like a step back from a cheap solder station from Amazon that has a control panel and takes Hakko tips. reply Kirby64 25 minutes agorootparentLiterally all the cheap soldering irons (Miniware TS80/100/etc, Pinecil) use a 3.5mm jack for their tips. These look incompatible, though, with those designs, which is a shame. reply varispeed 3 hours agoprevNever buy cheap tools. It looks like a cool gadget, but is it actually useful for soldering? Does it maintain correct temperatures? How long the tips last and can you buy them easily? Are there many variety of tips? etc. etc. If you are into soldering, do yourself a favour and buy something tried and trusted like Hakko FX-951 if you are on the budget. It will probably outlast you. reply kwiens 2 hours agoparentI agree. We put an incredible amount of effort into making this a functional workhorse that will last a lifetime. Tips we'll have at launch: Cone, Bevel 1.5, Wedge 1.5, Point, Bevel 2.6, Knife 2.5, Knife 1.4 What kills tips is oxidation. With our auto-sleep sensor, it drops below the temperature that will wear it out. When you pick it up, it's back at soldering temperature in a few seconds. Give it a chance! You're right, it's not tried and tested, yet. But Tom at Hackaday is not an easy person to convince: he's been around the block and used every iron out there: \"iFixit didn’t just raise the bar, they sent it into orbit.\" reply varispeed 19 minutes agorootparentThanks! That sounds reasonable. I would still consider making a holder. I could also suggest a Barrel 0.8 tip, that wraps around pin that one wants to solder. reply jdietrich 2 hours agoparentprevLots of cheap tools are excellent. In this space, Sugon/Aifen make fantastic JBC clone soldering stations for under $100; you can use original JBC tips, but MAGMA tips work 95% as well as the JBC originals at a fraction of the cost. The range of tools and materials being produced for the Chinese phone repair market is incredible - some stuff (like tweezers) that's just outright better than any western equivalent, some stuff that's completely novel and has no big-brand equivalent. https://bresun.aliexpress.com/store/900239507 https://kaisitool.aliexpress.com/store/3152011 reply vault 3 hours agoparentprevthe Hakko FX-951 is discontinued and its replacement FX971-44 costs GBP 350. would you also recommend the FX888D that is half the price, ignoring second-hand market? reply jeffbee 2 hours agoparentprevI don't see why maintaining tip temperature would be even slightly difficult with modern electronics. It should be possible to make a very cheap and excellent soldering iron at this point. reply donatj 2 hours agoprevEvery time I see a soldering iron use a 3.5mm headphone jack for the tips, some dark dumb part of my brain wants to plug a pair of headphones into it to see what happens. reply Forge36 2 hours agoparentMagic smoke is released. Do try to avoid doing this with headphones in ears. reply dunham 2 hours agoparentprevBack in the 80's a friend of mine had a system for launching model rockets built out of power cords - a extension cord with multiple outlets for distribution, and power cords with microclips on one end to hook to the rockets. And of course he had to find out \"what happens if I plug this in\". (If I remember right the microclips were fused together, but they may have just melted, it's been a while.) reply thebruce87m 2 hours agoparentprevBefore the invention of lead-free solder you would hear heavy metal. reply marcosdumay 2 hours agoparentprevResults vary depending on the phone. EDIT: I take the following back. The actual cable is USB-USB. The P2 connector links directly to the heated head, what is perfectly equivalent to \"labeled\". But yeah, people that design products, please if you make a non-standard use of a standard connector, label it. I would absolutely not buy this because that USB-P2 cable will mix with every other thing that thought was a good idea to use an unlabeled USB-P2 cable that only God knows whether they are compatible or not. (Common sense would imply they are, but common sense already flew out of the window long ago when you see a cable like that.) reply Fwirt 2 hours agoprevWhy overcomplicate a simple tool? If you're not soldering professionally and only need it a few times a month, I never see anyone recommend the Hakko FX-600. I couldn't be happier with mine. Heats up in seconds, adjustable temperature, uses standard Hakko tips, and very affordable. And takes up no bench space, you just shove it in your toolbox (with a tip cover) when you're done with it. The only downsides are that it's not as slim as a soldering station, and the temperature adjustment is in 20 degree intervals. Hakko is a reputable brand, and I have had 0 issues with mine. reply wvenable 9 minutes agoparentI would recommend a TS100 or TS80 over the Hakko FX-600 for occasional hobbyists. Those are both closer in design to the iFixit iron. Digital display, standard (replacable) power connectors, safety features, etc. Even more compact. reply ryukoposting 15 minutes agoprev [–] Does that tip mount with... a 3.5mm audio jack? An accelerometer instead of a power button? An app to change the temperature? All this amateur-hour idiocy, and it still costs 80 bucks before you even get a power supply? I love my iFixit screwdriver kits and I support their mission, but this thing is preposterous. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "iFixit has launched its first electronic tool: a USB-C powered soldering iron and smart battery power hub, designed to be highly repairable and user-friendly.",
      "The soldering iron outputs 100W of heat, reaches soldering temperature in under 5 seconds, and features an accelerometer for automatic heating and cooling, enhancing the tool's longevity.",
      "Key features include a heat-resistant storage cap, polished user experience, warranty and local support, comfortable grip, shorter soldering tip length, and a no-tangle, heat-resistant cable with a locking ring."
    ],
    "commentSummary": [
      "iFixit has introduced a new USB-C powered, repairable soldering system that heats up to 100W in under 5 seconds and cools automatically when set down.",
      "The soldering iron includes an accelerometer to detect when it's picked up, extending the tip's life, and features a heat-resistant storage cap and a smart battery power hub.",
      "Settings can be adjusted via a web console using Web Serial, currently supported only in Chromium browsers, and the iron along with its schematics are available on iFixit's website."
    ],
    "points": 438,
    "commentCount": 210,
    "retryCount": 0,
    "time": 1726154323
  },
  {
    "id": 41515101,
    "title": "The first release candidate of FreeCAD 1.0 is out",
    "originLink": "https://blog.freecad.org/2024/09/10/the-first-release-candidate-of-freecad-1-0-is-out/comment-page-1/#comments",
    "originBody": "FreeCAD News RELEASES The first release candidate of FreeCAD 1.0 is out Published by Aleksandr Prokudin on September 10, 2024 We’ve just pub­lished builds of the first release can­di­date for FreeCAD 1.0. You can down­load them here. So far, we’ve enjoyed the con­tri­bu­tions of users who are hap­py to be liv­ing on the edge with our week­ly builds and report­ing what­ev­er bugs they run into. That real­ly helped us make the prover­bial edge less edgy. The inten­tion behind mak­ing release can­di­dates is to give them into the hands of a dif­fer­ent demo­graph­ic — users who usu­al­ly stay away from unsta­ble soft­ware yet are hap­py enough to try very near­ly com­plete soft­ware and report issues they come across. We are cur­rent­ly down to just 7 release block­ers, but we expect that the release can­di­dates will bump that num­ber up a tad, and that’s a good thing. While we des­per­ate­ly want 1.0 out, deliv­er­ing a real­ly sta­ble release is a big deal for us. So please down­load the RC1, try it on a real project if you can, give it some real­ly hard time if you must, and report any bugs you see to our issue track­er. If you are a devel­op­er who is inter­est­ed in con­tribut­ing to the effort, come look at the list of reports, pick an issue that seems triv­ial to resolve, write a fix, and sub­mit a pull request. We have week­ly merge meet­ings on Mon­days to go through the queue of cur­rent­ly open pull requests. You are wel­come to join! Share this: Share Like this: Like Load­ing… Discover more from FreeCAD News Subscribe to get the latest posts sent to your email. Type your email… Subscribe 9 responses to “The first release candidate of FreeCAD 1.0 is out” Hynek September 10, 2024 oooo BIG thank you to all of you guys ! Load­ing… Reply Craig September 10, 2024 Thank you for the update. Down­load­ing rc1 right now. Load­ing… Reply Peter September 11, 2024 Great news 😍 Will be down­load­ing lat­er this week 😁 Load­ing… Reply Chris September 11, 2024 I’ve not been this excit­ed about a soft­ware release since Win­dows 98. Load­ing… Reply 3xxx September 11, 2024 I have already start­ed using the RC1 ver­sion, and thank you all very much for your efforts in devel­op­ing such a great solu­tion for 3D para­met­ric models. Load­ing… Reply Luke September 12, 2024 So the topo­log­i­cal nam­ing prob­lem is solved? Sure­ly that’s a v1.0 requirement? Load­ing… Reply Aleksandr Prokudin September 12, 2024 It can nev­er be 100% solved, it’s the curse of para­met­ric mod­el­ing. But it’s mitigated. Load­ing… Reply Clive September 12, 2024 Will be down­load­ing very soon, got to reload my Ubun­tu OS first. No NetworkManager Load­ing… Reply Rr September 12, 2024 Thank You 🙂 Load­ing… Reply Leave a Reply This site uses Akismet to reduce spam. Learn how your comment data is processed. ←Previous: North America FreeCAD Meetup August 2024. Next: WIP Wednesday 11 September 2024→ Recent posts WIP Wednesday 11 September 2024 The first release candidate of FreeCAD 1.0 is out North America FreeCAD Meetup August 2024. WIP Wednesday 4 September 2024 1.0RC1 is coming next week, let’s update addons WIP Wednesday 28 August 2024 FreeCAD News Designed with WordPress Discover more from FreeCAD News Subscribe now to keep reading and get access to the full archive. Type your email… Subscribe Continue reading %d",
    "commentLink": "https://news.ycombinator.com/item?id=41515101",
    "commentBody": "The first release candidate of FreeCAD 1.0 is out (freecad.org)324 points by jstanley 22 hours agohidepastfavorite171 comments Always42 19 hours agoUnless you have solidworks through your job or school, FreeCAD on mac is the way to go. Solidworks is great until you have to buy your own license. This costs MULTIPLE thousands of dollars. You cannot purchase a \"hobby\" version that actually gives you the desktop version. I used solidworks up until my company license got pulled. Additionally im not a student anymore so no luck there. I used to use Fusion - but it was never as nice as solidworks. My student edition expired and now im out of that to. Now I use FreeCAD on Mac. Takes time to adjust and I cannot model as quickly, but saving $$$$ reply tomkinstinch 50 minutes agoparentLooking forward to the day when FreeCAD is a viable and stable option for free parametric CAD. There are a few free options for direct modeling, but not for parametric design. As far as commercial software goes, my current favorite CAD software for hobby use is Rhino[1]. It's not parametric[2], but it's stable, fast[3], can import and export a wide variety of 3D file types, and it's pay-once-per-major-release. It's not cloud-based. The marketing around it seems to emphasize design/architecture/artistic use cases, but it also works well for dimensionally-accurate mechanical parts. For those eligible for a student license, the pricing is reasonable (cheaper still if you shop around among third-party edu software vendors). Surprisingly, the student license also allows commercial use. 1. https://www.rhino3d.com 2. Well, Rhino is not parametric in the usual sketch-based way. People do wild things with the Grasshopper plugin. 3. Rhino also runs on macOS, w/ hardware acceleration of graphics via Metal reply Robotbeat 14 hours agoparentprevI recommend Ondsel as well, which is free without restrictions (they have paid tiers that have cloud features, but those aren't necessary). They should include the FreeCAD 1.0 fixes in a few days. HUGE improvement to the FreeCAD GUI, and it saves in FreeCAD format so you're not stuck. reply wakeupcall 5 hours agorootparentI still recommend RealThunder's fork (https://github.com/realthunder/FreeCAD/) at the moment, even though his fork is a bit lagging at the moment. Most of his contributions to the topology fixes got merged back into freecad now, but his enhancements to UI/behavior aren't (yet), and they make a night and day compared to ondsel too. I didn't find any significant limitation to RealThunder's assembly3. In any case, while far from most commercial offerings, FreeCAD is progressing and the future looks bright. I've stopped using f360/onshape in the last years for my hobby designs. Once you know the specific limitations of freecad+occt (something you learn in each cad program) and how to work them around effectively, it's already pretty powerful. reply godelski 12 hours agorootparentprevIt's also worth noting that they work with FreeCAD and make pushes to them too. So using either helps both. I've been very happy with the developers and they are very responsive on GitHub. reply lazulicurio 5 hours agorootparentJust to start, I want to acknowledge that the problem space is tremendously complex; the FreeCAD developers have put in a lot of effort and it's amazing that a project like FreeCAD exists at all. Not trying to disrespect the other FreeCAD developers, but it seems like things have improved remarkably since ondsel started taking a more active role. The project seemed to exhibit a (common) impulse to prioritize extensibility too much. The \"workbench\" architecture and python API let you do some really neat stuff if you're willing to dig into the weeds. But, from the perspective of a community outsider (so take it with a grain of salt), the development process seemed to be a good example of Conway's Law in action. The workbenches let everyone have their own sub-projects to manage without stepping on each other's toes. This led to a lot of resulting complexities, inconsistencies, and instabilities, which made the approach a net negative (imo) in terms of tradeoffs. With ondsel, there's been more focus on holistic improvements and getting the individual modules working together more smoothly, which I greatly appreciate. reply alvah 15 hours agoparentprevWhy \"on Mac\"? Is it required? I'm interested in trying out anything that might help to break Autodesk's monopoly, but not at the expense of having to use a Mac. reply progbits 12 hours agorootparentYeah there is zero reason for mac. Freecad on Linux is great, and for commercial packages, onshape on chromium on Linux runs better for me than fusion on windows did. reply 34679 7 hours agorootparentWhen I tried out FreeCAD on Ubuntu a couple years ago, it was an extremely frustrating experience. I was following a tutorial for new users until I got to a part with a simple instruction that involved clicking a button on the toolbar. The only problem was, the button wasn't there, and the instruction was so simple that it didn't specifically say \"click this button at this location\", it was more like \"do this thing\". It was worded in a way that made me think \"it must be obvious and simple, why can't I figure this out?\" After way too much time spent digging through menus, trying to configure the UI and searching online for a solution, I installed the Windows version out of frustration. The button was right there, front and center. The Linux version I had installed was just straight up missing it. reply JohnFen 5 hours agorootparentFreeCAD has come a long way since then, although it still has a pretty steep learning curve. Once you get the paradigm of it, though, it's manageable. I use it most days, and am very happy with it. Although I'm not an actual designer and I don't have a great deal of experience with other CAD software. reply poulpy123 7 hours agorootparentprevProbably OP's only as experience of FreeCAD and other CAD software on mac reply fragmede 7 hours agorootparentprevI read it as \"I have a mac and this is what's available\", not \"you should get a Mac in order to run this program\". reply thrtythreeforty 18 hours agoparentprevYou can rent a non-commercial license for $99 a year. Still sucks because it's the usual SaaS hostage situation. They also recently raised the price of a real license by making you purchase a couple years of updates (which are typically ~worthless as a user). I was half prepared to swallow the $4k or so but that extra bump made me balk again. There is no moderately priced, fully featured CAD on the market. Unless FreeCAD has recently overhauled their UI, it is immensely painful to do things which are 2 clicks in Solidworks. reply 1jss 13 hours agorootparentYes, I had a really hard time getting used to the UI. Later found the ModernUI Workbench plugin which made it a whole lot better. https://wiki.freecad.org/ModernUI_Workbench edit: This plugin seems unmaintained and Ondsel is probably the way to go now if you want a better organized UI. reply k1musab1 17 hours agorootparentprevYou should try Ondsel fork of FreeCad. reply thrtythreeforty 14 hours agorootparentOnce they release 2024.3 I probably will! They are definitely saying all the right things. I filled out their user survey and was pleased to see UI/UX at the top of the responses. If they start delivering meaningful UI revamp I will certainly send them some money - I cannot express how much I want a KiCAD equivalent for mechanical CAD to exist. reply starky 17 hours agorootparentprevSolidworks perpetual licensing has always had an annual maintenance fee associated with it, but they changed it a couple years ago where if you let your maintenance subscription lapse they charge you for the years you missed plus an additional fee. They also increased their maintenance prices by like 30% last year. So we are now in the process of switching to Creo which, while being a user experience nightmare, is so much more stable and runs faster than Solidworks. Agreed about FreeCAD, the user interface is terrible and even though Ondsel exists I just can't stand the way the program works. As much as I want to use FOSS software there really isn't much that beats the commercial products if you have access to them. reply justinclift 16 hours agorootparent> ... about FreeCAD, the user interface is terrible and even though Ondsel exists I just can't stand the way the program works. FreeCAD seems to operate in the same way as Catia (ie v5/v6), or at least have been developed to follow the same approach to things. Saying that as I used to use Catia years ago, so the FreeCAD approach wasn't completely foreign. reply inferiorhuman 15 hours agorootparentIt's beyond that though. How many different, incompatible, assembly bench plugins are there these days? reply justinclift 8 hours agorootparentYeah, the different, incompatible assembly plugins is why I stopped using FreeCAD a few years ago. That's reportedly been fixed (guess they picked a winner?), but I haven't taken a look since. I probably will, at some point, but I generally have a different focus these days. reply throwgfgfd25 5 hours agorootparentThey didn't pick a winner. They (Ondsel and others) evaluated all the workbenches, chose the best ideas and built a new workbench around a new (well, new to C++) solver. There was an Ondsel blog post about this: https://ondsel.com/blog/default-assembly-workbench-7/ reply Robotbeat 14 hours agorootparentprevOndsel helps a lot with that. FreeCAD 1.0 I think also now has a default Assembly bench. reply _glass 11 hours agorootparentprevHow about https://www.plasticity.xyz/? I didn't try yet, but looks great. reply linsomniac 7 hours agorootparentI just picked up Plasticity earlier this week to start trying to learn it, it's been on my radar for a while. I've been using TinkerCAD for years for making my simple models, and it works really well for the basics but there are things that become painful there that Plasticity has promise of making a lot easier. One of the first tutorials I went through was really frustrating though. Some of it may be that Plasticity is a quickly moving target right now (lots of tutorials are for v0.x or 1.4, with current being v24, for an idea). A lot of the pain was this tutorial just didn't touch on the basics it was assuming you knew. Some of it was just getting used to the tool and figuring out what mode you are in and which you need to be in to accomplish what you need to do. I struggled a lot with just getting keyboard shortcuts and the trackpad navigation to work. I never did find a description of mouse/trackpad mappings (possibly made worse by there being ~5 themes you can select from). It shows a lot of promise, but there's going to be a bit of a learning curve. But there was a learning curve on TinkerCAD too, I just need to keep that in mind. Pricing is ok: free 30 day trial, $150 for a license with 1 year of updates, and $299 for the Studio license. I don't use CAD that much, like maybe a model a month or less, so it's kind of a big bite to take for me personally, especially with it being young and likely to need to spend $150/year for a while here as it's revving up. The Studio version's xNURBS feature seems like it might be really enticing, but just makes that even harder for me to bite off. I probably should try OnShape just because they do have that free plan. I'm also looking at OpenSCAD for doing parameterized models. I installed it last night and asked Perplexity AI to generate a model, and it made a good start at it, but couldn't quite get the tongue-and-groove right. reply mandarax8 14 hours agorootparentprev> There is no moderately priced, fully featured CAD on the market. BricsCAD? reply _flux 12 hours agorootparentI've been looking for a while at BricsCAD (as an alternative to VariCAD), but when you add in sheet metal folding and ability to export and import STEP, it starts getting expensive. I just checked their site and their 20% off prices actually seem reasonable—at least before realizing they are yearly costs.. They do sell also perpetual licenses where you pay for the product of your selection and then a yearly maintenance fee, and this would perhaps make the most sense for a hobbyist, but this already feels a bit expensive. I've been trying to get into FreeCAD, but some of my existing models seem to be a bit slow with it, not to mention the different workflow. But I'll give 1.0 a shot! reply wakeupcall 5 hours agorootparentBricsCAD is ok. It's more of a direct modeler with constraint support though. It may or may not matter to you depending on the kind of work. I tried it for a while, and while I generally liked it, also got stumped by the artificial limitation of STEP import/export, which made it a non-starter even for hobby projects. This is, IMHO, the dumbest thing they could do in terms of licensing. reply maxerickson 18 hours agorootparentprevIn the US, a few thousand dollars a year is moderately priced relative to salaries. reply tetromino_ 17 hours agorootparentWhose salaries, exactly? In most of the country, that's a couple months rent for an entire middle class family. I earn well, and I cannot imagine ever paying that much for any piece of software unless I needed it for a profit-making venture and the ROI was very obvious and very positive. reply maxerickson 16 hours agorootparentany piece of software unless I needed it for a profit-making venture and the ROI was very obvious and very positive. Yes, that's right. Weird that a business making powerful software is targeting that market and not hobbyists. reply inferiorhuman 15 hours agorootparentFusion was initially (and still is to some extent) targeted explicitly at hobbyists. At one point the CEO made lots of noise about his commitment to the maker community. 'Course since then Autodesk went from a company run by a maker to a company run by a marketing dweeb and a beancounter. reply rurban 13 hours agorootparentSorry, but Autodesk was always run by beancounters. They wanted their share in office products, and went lucky with CAD. Read John Walkers \"Autodesk Files\". reply inferiorhuman 11 hours agorootparentIn the context of Fusion, it was the pet project of Carl Bass who is very much a maker. He constantly championed free access for hobbyists to Fusion 360. I suspect a big part of his departure was due to not having any path towards monetizing the huge cash sink that was Fusion. Bass' replacement was the chief marketing officer. reply Kirby64 16 hours agorootparentprevPeople that use CAD for a full time job? 2k/yr is basically nothing. As a business expensive it’s a rounding error. reply TaylorAlexander 14 hours agorootparentRight but there are people who use CAD for 3D printing projects around the house too. For them a few thousand a year is extreme. reply rendx 17 hours agorootparentprevThe US national median salary is $59,384 per year as of Q4 2023. https://www.sofi.com/learn/content/average-salary-in-us/ reply maxerickson 16 hours agorootparentRight. How much would you pay for software that saved your $60,000 designer weeks per year? reply inferiorhuman 15 hours agorootparentAnd how many of those saved weeks are being spent fighting draconian licensing software? In a past life I had a few architectural firms as clients and actually getting AutoCAD licensing shit to work was a huge pain point. reply tverbeure 14 hours agorootparentYou need to balance those weeks spent fighting licensing issue (seriously?) against the time that's lost by using a piece of software that is a nightmare to use... if it doesn't crash. Which it does all the time. Admittedly, it's been 2 years since I last used FreeCAD, but I've spent literally more than a hundred of hours with it trying to make it do what I wanted it to do only to come to the conclusion that mechanical CAD probably just wasn't for me. And then I tried Onshape and, surprise, it wasn't me after all. reply Dalewyn 12 hours agorootparentprevIrrelevant; such a license would be purchased by the business and wrote off as a loss on the income/loss sheet. Needless to say, for a business a few or even several thousand dollars a year is practically nothing if it's critical to business operations and ensuring productivity. If you're buying this for your own personal use? Yeah, you're gonna need a lot of disposable income or some really good justification. For your own small business use? Yeah, you're gonna need to justify that cost against your estimated annual income and other losses. reply maxerickson 7 hours agorootparentWhat's irrelevant to what? The actual market for CAD software is well funded businesses that are buying it as a productivity tool, so of course their approach to the cost is very relevant when trying to understand the pricing. reply santoshalper 16 hours agorootparentprevYeah, but probably not for someone who needs Solidworks for their job. reply cruffle_duffle 15 hours agoparentprevOnshape has been my cad software of choice for all my local 3d printing needs. It’s honestly pretty damn good. reply wakeupcall 5 hours agorootparentOnshape is great. I use it as well for random things. I do expect them to do a pull-rug on the free license at some point, like fusion did, especially now that they've been bought by PTC. If they do, the commercial license is too expensive IMHO compared to other offerings for what they offer. I had the option to use the educational license at some point, but we couldn't get to renew it (ironically, we got a dirt-cheap Creo license afterwards). Just to keep things in mind it can go anyday from free to too-expensive. I had a few complex designs in fusion360 I essentially lost at some points due to the price hikes. I decided to endure the pain in freecad. It's getting better. reply tverbeure 14 hours agorootparentprevAs a recovering FreeCAD user: Onshape is amazing. reply kamranjon 13 hours agorootparentSame boat. Onshape is so intuitive. What many people don’t realize is that onshape is free as long as you don’t mind your designs being public. All of my designs are open source so for me it’s actually a benefit. reply snovv_crash 10 hours agorootparentprevThis. Onshape just works. Even their sheet metal tool is quick and easy. reply vidanay 15 hours agoparentprevMy first experience with 3D was with AutoCAD 10 or 11 when they had \"2 1/2\"D. I've used ProE, Catia, Unigraphics, SolidEdge, Solidworks, Inventor, etc. The workflows in FreeCAD are completely irregular and alien compared to those others. It's incredibly frustrating to use and I have had zero luck becoming fluent in it. reply hobofan 10 hours agoparentprev> My student edition expired and now im out of that to. There is a \"personal\" version of Fusion 360 which isn't tied to enrollment. It has some limitations (only 10 \"active\" documents; some advanced features are locked), but overall, I think that's still the most accessible entry to CAD for hobbyist makers, especially with all the tutorials for it out there. I think that with the current state and trajectory of FreeCAD/Ondsel, they have a realistic chance of catching on. However if FreeCAD really wants to be the version that is installed (rather than Ondsel), I think they really have to get to a more regular release cadence. reply justinclift 16 hours agoparentprevYeah, the OSS aspect of FreeCAD is a win for sure. With Solidworks, they have things like the \"Maker\" edition which is only US$48 per year: https://www.solidworks.com/solution/3dexperience-solidworks-... I grabbed a perpetual license for the Maker edition when it first came out (free at the time) though I don't think I ever got around to really using it. ;) reply tohnjitor 17 hours agoparentprevThere is now a USD$10/month subscription for Solidworks. The software includes an astounding amount of bloat but it does work. reply fragmede 7 hours agoparentprevWhy not use the hobbyist version of Fusion? it's free for non commercial use. reply datavirtue 7 hours agoparentprevOne day these guys are going to look up and FreeCAD is going to be the industry standard. All because they didn't know how to license individuals. reply the__alchemist 18 hours agoparentprevYou can get a cheap license if you're current or former military. (US at least) reply vlachen 17 hours agorootparentCheap license of what? SolidWorks? Fusion? OnShape? Edit: I can google that. I was just surprised that I've been using the stuff at work for over a decade and I am juat now hearing about it. reply lambda 17 hours agorootparentYou can use OnShape for free as long as you're OK with the models being publicly visible. I find that fine for learning and personal projects. I've dabbled with OnShape, FreeCAD, and SolveSpace, and of them SolveSpace is the one I've ended up using the most. OnShape was nice, the GUI was pretty intuitive, I liked the way it worked, but I just feel weird trusting anything to a free plan on a cloud service. I don't really mind the public part, but it always felt tenuous that the plan would remain free so I didn't really feel like I could trust it long term. FreeCAD was complicated and opaque, I never really put in the time to learn it, it just felt a bit clunky, but I keep meaning to come back to it. SolveSpace seemed a bit mysterious at first, but just a bit of learning and I found myself pretty comfortable with it. It's not nearly as fully featured as some of the others, but it clicked well for me. SolveSpace and FreeCAD are both FLOSS software. reply vlachen 17 hours agorootparentI've done some FreeCAD and OpenCAD, but SolveSpace is a new one to me. Will scope it out. FWIW, I agree on the free platform thing. I can't bring myself to put my actual projects on there. reply emmelaich 11 hours agorootparentHave a look at zoo.dev too. Formerly KittyCAD. reply WillAdams 17 hours agorootparentprevIt's the educational version of Solidworks --- did it a while back when my son was in high school and he found it useful for doing his CAD homework. reply dgroshev 5 hours agoparentprevSOLIDWORKS for Makers is $48/year [1]. That subscription includes a proper SOLIDWORKS installation, Dassault is pushing their web stuff, but you don't need to use it. Also, it uses local files by default, unlike Fusion [2]. The subscription comes with a no-commercial-use clause and the files can't be opened in the commercial version, but I'm sure if push comes to shove the file thing will be fixed on the high seas. Re: Mac: SOLIDWORKS runs perfectly well in Parallels on M1. I moved from Fusion and it's been great. Just having fully working G3 surfaces/constraints [3] and patterning on sketch points alone is worth the expense. [1] https://www.solidworks.com/solution/3dexperience-solidworks-... [2] Recently Autodesk changed the policy and now Fusion360 will remove your files if you don't pay and not log in for a year. [3] Something that I don't think we'll ever see in FreeCAD, considering the pace of Open CASCADE development https://git.dev.opencascade.org/gitweb/?p=occt.git reply resource_waste 5 hours agoparentprev\"on mac\" Apple is GOAT at marketing. Incredible how much control they have over people. reply slater 14 minutes agorootparentCouldn't possibly be that they also make good stuff. Crazy talk! reply mardifoufs 4 hours agorootparentprevWhat do you mean? They are just saying that their experience is with mac, so they recommend the mac version? If anything the incredible thing is that such a normal statement can actually be perceived as something else as soon as Apple is mentioned. reply wickedsight 5 hours agorootparentprevYeah, because nobody ever writes 'on Windows' or 'on Linux'. It's really only Mac users who every specify which platform they're recommending something about. reply emmelaich 11 hours agoprevBetter link would not to the comments but to the page. https://blog.freecad.org/2024/09/10/the-first-release-candid... reply rubyfan 8 hours agoparentAlso this for people wanting to know more about FreeCAD. https://www.freecad.org/ There was no link to the main site from the blog on mobile. reply daghamm 21 hours agoprevI've always felt freecad being superior to most other free CAD tools. But I can almost never get it to work for me. Every time there is a new major release I try it only to rage quit two hours later. Really hope they get someone to help them with stability and UX improvements like Blender did. reply throwgfgfd25 21 hours agoparentUX work is ongoing. Stability is good in the latest dev builds on the Mac, though 0.21.2 is the least crashy I've seen it. But if you mean stability in terms of model stability/robustness when changing things, that's improved a lot with the topological naming mitigations. It's still not perfect, and I still think FreeCAD is a lifestyle choice. But I enjoy working in it a lot more. The Mango Jelly Solutions videos on Youtube are very, very worth a watch if you feel inclined to have another go; they have been the best thing for getting my mind into how FreeCAD works as a package (in the sense that it is a \"package\" at all -- it's really still a collection of overlapping, macro-programmable toolsets gathered around a kernel). reply tpmoney 17 hours agorootparentI want to second the recommendation for Mango Jelly Solutions videos. I've tried FreeCAD on and off for years and those videos are the first ones that finally helped me wrap my head around some things and be able to use it for a real project. reply mort96 12 hours agorootparentprevThe thing I don't get is, it's over 20 years old. Surely if those th8ngs were ever going to improve they'd have done so over the past 20 years? reply throwgfgfd25 7 hours agorootparentCAD is never made quickly, I think. But FreeCAD was just not that sort of project. It's a C++ and Python wrapper around a CAD kernel, supporting a set of tools -- some frustrating tools, some quite powerful or niche, like the ThreadProfile workbench or the guitar workbench -- and it has never bothered the highly technical community of users much to unify things. They weren't really trying to make a major competitor to commercial CAD: they were trying to have the tools that they individually needed and collaborate on the problems they had in common. The balance has markedly shifted since 0.18 and now there is that focus, and significant commercial impetus. In the time I have used it -- about three years on and off -- it has clearly become more of a focus to make a complete product. ETA: there is no doubt that one of the major things that needs to be resolved is the duality between Part workbench and Part Design workbench flows. There appear to be some discussions about this -- about how to either merge them or create a new, future workflow that makes better use of them. The crux of it has always been that a section of the community thinks the Part Design feature-oriented flow is a bit of a crutch, being as it is implemented as a set of implicit booleans on top of the basic flow. Part Design is more fun to use for a beginner, but it is definitely not faster, and one of the real problems is that once you are in the feature flow you are kind of stuck in it -- it's possible to merge in objects made in the Part flow but only in relatively basic ways (starting a PD body with a \"base feature\", or fusing the PD body with the non-PD stuff at the end). I would expect future development to look at this much more seriously, but there was and is no point in getting into it in more depth until the major TNP issues are truly behind FreeCAD, because a feature-oriented flow especially relies on there not being problems there. reply tecleandor 10 hours agorootparentprevBlender changed, FreeCAD can. Just the topology naming fix is a HUGE change that lots of people could have said a couple years ago that it would never be fixed. reply ortsa 19 hours agoparentprevBlender's got a constraint solver for IK, right? How much spaghetti code do we need to add to give it a full CAD kernel? It already does everything else! I've honestly wished I could use it to make vector graphics sometimes, but that also needs some of the basic elements of CAD (parallel edges, radius constraints etc). It's so close to parametric modeling too, with the mesh modifiers, drivers, and now geo-nodes. Of course, I believe there are a few CAD plugins, but I've never used them, so I can't speak to their efficacy. reply justinclift 16 hours agorootparentThere's a bit more to it than that. There's an underlying library which can support solid modelling, but Blender has (or had) such an outdated version that it just wasn't possible. Back in 2020 someone submitted code to get it working, in order to make solid modelling possible: https://archive.blender.org/developer/D6807 Unfortunately it looks like no official Blender developer ever took the time to review it, let alone merge it. Super unfortunate, as it was only about 15 lines changed. Probably would have needed at least one revision though, as one of the changes was just commenting out some lines. That'd likely have needed to be a better conditional instead. reply zevv 12 hours agorootparentprevThere is the excellent CAD sketcher plugin for Blender; this adds a basic 2D parametric/constraint based editor into your workflow, which can convert it's output into a mesh to integrate into your blender model. For more complicated models I typically make 2 or 3 2D constraint models, and use the blender boolean tools to combine this into the final 3D model. https://www.cadsketcher.com/ reply digdugdirk 16 hours agorootparentprevWhile similar at a glance, the underlying functionality between a 3d modeler and a CAD kernel is tragically completely different. Even FreeCAD has some fundamental differences (and lack of functionality) between it and other mainline CAD programs. Hopefully someone with more knowledge and experience than me can hop in and explain more, I'm just a CAD user, not a CAD kernel developer. reply bschwindHN 16 hours agorootparentI'm not an expert either, but it could be compared to bitmap graphics vs. vector graphics. 3D modelers like blender (or even OpenSCAD) work with a bunch of triangles - there is often not some higher level representation of the geometry. You could put a drill hole in a part, but it ends up as just a ton of triangles that approximate that drill hole, vs. a file format which semantically encodes \"there is a cylindrical drill hole at this location, with this vector direction, and this radius\". That's what things like BRep (Boundary Representation) and STEP files give you is that semantic data which describes the part \"here are the edges, faces, dimensions, etc.\", vs. \"here's a bunch of triangles, good luck machining this\" reply throwgfgfd25 3 hours agorootparent> but it could be compared to bitmap graphics vs. vector graphics. This is very much how I internally understand it and explain it to people, yes! It is a good analogy for e.g. why it's often a challenge to get something milled with a CNC when you only have an STL file. STL is like a PNG line drawing: it can be high quality, but it's not describing the drawing. STEP is like SVG: it's more effort to render it, but it contains the instructions to draw it. reply shadowpho 20 hours agoparentprevIt’s usable now. I’ve been playing with it on and off and it’s night and day to what it was before reply imtringued 9 hours agorootparentIt is not. There are still ridiculous hoops you have to jump through to orient your sketch. The first thing I do is draw an arrow that points up in the sketch and then reorient the sketch. The reason for this is that the attachment editor just randomly picks a \"random\" orientation based on the \"orientation\" of the face or datum plane you are using. The attachment editor is fundamentally broken and needs a complete revamp. The other part is that FreeCAD is still this \"enter numbers by hand and hope for the best\" CAD tool. When you perform an extrusion, there are no visual arrows to pull the extrusion along. When you do a pocket and it goes in the wrong direction you just see nothing, instead of a transparent preview of the operation that is being attempted. I say this as someone who built a design in the Assembly 4 workbench using dozens of individual parts and probably redesigned every part at least twice. Sure the official assembly workbench is a good idea in the very long run, but they fixed none of the short term pain points I have. You know, things you run into every single damn day. Meanwhile migrating to the new assembly workbench will cost me even more time. I.e. there are switching costs but hardly any benefits. reply jstanley 3 hours agorootparentI tend not to use the assembly workbenches, I just manually position separate bodies in space at the position and orientation I want them. You can use \"LinkGroup\" to group a bunch of them together so that they move as one unit. reply SV_BubbleTime 19 hours agorootparentprev> night and day to what it was before This reminds me of iirc KiCAD 5 to KiCAD 6. Overnight it went from some weird clearly-Linux program to become a viable product, an excellent one even. KiCAD uses FreeCAD on the backend for things. I’d love to see FreeCAD take the same path!!! However… when I looked at it last year the “let’s draw a cup” tutorial was so pathetically bad I closed it and went right back to solidworks without a second thought. reply crote 11 hours agorootparentI think a lot of that is a Cathedral vs. Bazaar problem. Programmers like to work on problems which are technologically interesting, and they love adding new features. Dealing with technical debt and solving UX/UI issues isn't as much fun, so in a Bazaar model they'll simply not do it. The result is a product which feature-wise is very powerful, but UX-wise an absolute nightmare to use. But when there's a party genuinely interested in the product as a whole, they can push more Cathedral-like UX-focused development. Have a handful of devs focus on UI stuff for a year or two, and it has suddenly turned into a world-class product. Blender and KiCad have gone through this before, and it seems like Ondsel is pushing something similar for FreeCAD. Let's hope it works! reply systems_glitch 7 hours agorootparentprevThe changes from KiCAD 3 => 4 got me to finally switch off EAGLE CAD, for which I had a $1600 license, but could see the looming subscription nonsense coming with Autodesk's interest in EAGLE. IIRC that was the first release after CERN took on development and had been dogfooding it. Every release since then has been a major improvement, and we've converted several customers from Altium to KiCAD in that time. I can make FreeCAD go, but we still have to manually create dimensioned printsets for our sheet metal shops, rather than just being able to hand off a drawing file. I feel like when we get to the point of just being able to hand off a drawing file, it'll be in a much better place. reply dbcurtis 21 hours agoparentprevYeah. I have tried and quit a number times. Poor stability has always made it unusable for me. Hopefully this time is better. Still, once I can successfully make a drawing, then what? What exists for CAM posts? reply jononor 9 hours agorootparentThe CAM module is called Path, an you can find a list of the post processors included here: https://wiki.freecad.org/CAM_Post I have milled some basic things using the FreeCAD CAM on a ShopBot 2416 and small custom grbl based CNC. Many years ago now, but things generally look better now. Otherwise I have exported geometry and used external CAM software like VCarve reply throwgfgfd25 6 hours agorootparentIt was called Path -- in 1.x it has been renamed to CAM, which is a much more sensible name. reply systems_glitch 7 hours agorootparentprevThat is our current main issue with FreeCAD, we have to manually create fully dimensioned drawings for our sheet metal shops, and they pull it into whatever they use. Can't just give them a drawing file, CAM export, etc. reply noncoml 18 hours agoparentprevI have exactly the same experience here. You can see that the software has tremendous potential with a lot of work put into it, but the UX still sucks balls. Using the mouse to select the element you want is finicky the best. It takes me 5x the time to do the same thing vs fusion or solidworks. Then there are smaller frustrations like this confusion between \"Part workbench\" and \"Part design workbench\" and unhelpful python errors when you try to do something. But I am sure these will be fixed sooner or later. I think once the UX gets an overhaul it will be 90% there! reply eig 21 hours agoprevSuper excited about this! I hope more people will pick it up in the hobbyist space now that Fusion costs money. I'm not sure what the popularity of these different CAD softwares are. I've seen quite a few hobbyists use OnShape recently, and a few people use OpenScad. I don't think I've seen another FreeCad user in real life though. reply blihp 14 hours agoparentI use FreeCAD on a very regular basis and can understand why it's not more popular: it's very powerful but has some very sharp edges that will often have me using it in a state of near rage. Topological naming comes to mind but there are other various issues that I've hit like a brick wall (in that you can't work around the bugs/limitations so much as you must rework your design to avoid them which can be tedious and frustrating) when designing something non-trivial. That said, each release continues to improve it just has further to go than most open source projects. reply qwerpy 21 hours agoparentprev> now that Fusion costs money I know they've been obnoxiously chipping away at the features available in their Personal edition and introducing artificial limitations. But my free installation still works and I haven't seen any indications that it's going away. reply joshvm 20 hours agorootparentFusion as a CAD engine is great. I've not used the CAM side, and while I used to use Eagle a lot I've tried to invest more energy into Kicad. The online limitations are frustrating though. Randomly and inconsistently not being able to export STLs because of a \"translation service error\" (when it could 2 minutes ago), or the inability to make drawings with the free edition. I mostly use it because there isn't anything else half as good for OS X that works offline. reply qwerpy 19 hours agorootparentI used it to do some sheet metal modeling, then sent the models off to a laser cutting/bending service that shipped me the pieces. Then I went back to Fusion to 3d print some brackets/scaffolding using the same sheet metal models as a reference, to assemble the pieces into the finished product. This was during a 3 month leave from work, starting from zero knowledge beforehand. It was probably the most fun I've had in years, and mostly thanks to how slick Fusion is and how many tutorials there are out there. There are some export formats that it uses cloud machines for, which I think is silly and arbitrary. It's probably done that way to upsell their premium product for faster wait times or unlimited quota. For my uses I was able to select formats that didn't require the cloud. Fusion is much more polished compared to FreeCAD and so I'm not sure if I'll ever end up making the switch. But I'm glad to see a free alternative, just in case. reply joshvm 10 hours agorootparentMost of the common translation options should work offline (ie Fusion is capable), but Fusion sometimes gets stuck in a weird state where it insists it needs connectivity. Perhaps it's a quota thing but I've never found it to be consistent. This happens fairly often with STLs for 3D printing. Once it's gotten into that hole it will often refuse to export any other format until connectivity is restored, even if the app is restarted. It's known behaviour, for example the official guidance is that changing binary to ascii might help, or you shouldn't export directly to a slicer when offline, or don't use certain menus. But it seems like a wontfix. reply dekhn 19 hours agorootparentprevFusion 360 CAM is great for me (hobbyist doing CNC with wood and other materials). It's handled some pretty tough jobs, like a full topo map of california. It's why I pay for the product. I tried the electronics stuff in Fusion and decided not to use it because it didn't work nearly as well as Kicad. reply stn8188 21 hours agoparentprevI'm also happy for this. I'm an EE with limited MCAD experience, so I usually hop onto Onshape when I need a custom trinket to 3D print. I did use FreeCAD for a small fixture for my day job earlier this year and I was pleasantly surprised. For someone with no experience, it worked very well and when I lose access to Onshape I'll definitely pick up more with FreeCAD. reply throwgfgfd25 21 hours agoparentprevOpenSCAD is definitely very popular in the maker/microcontroller/electronics world, which is both a good and bad thing, because it is accessible but also limited/frustrating. It enables some good stuff on Thingiverse but it becomes extremely mathematics-focussed quite quickly. I do wish more of the code-CAD people would look at Replicad, Build123D and CadQuery. I personally like FreeCAD a lot, but I won't push people onto it; if they like TinkerCad that's fine. reply hugs 21 hours agorootparentI got into making all kinds of stuff because of OpenSCAD. It's just enough for 3D printing functional mechanical parts. It's still my first go-to for designs. The downside is OpenSCAD doesn't support import or export of STEP files... So I've also added FreeCAD to my toolbox. But I really wish OpenSCAD would/could do whatever refactor it needed to support STEP. reply throwgfgfd25 20 hours agorootparentYes -- the STEP thing was a big part of why I wanted to switch. I actually switched via CadQuery: a few minutes with that made it clear that the bits I didn't understand (edges, faces, planes, all that stuff that freaked me out) were simple and logical and had a sort of common sense integrity, and that I might as well try to learn them in the context of FreeCAD. Had Build123D existed at that point, or Replicad, maybe I'd have pushed on for longer. Build123D is my \"fallback toolbox\" at this point. I don't think OpenSCAD can produce STEP, ever. Importing it is another matter; that's a one-way meshing operation. But creating it means having a kernel that understands more than CSG operations -- a bRep kernel like OpenCASCADE, that FreeCAD/Replicad/CadQuery/Build123D etc. use. You can of course run your OpenSCAD in FreeCAD, but certain operations (hulls, Minkowski I think?) end up as meshes, because there is no easy equivalent. Still, that's better than every operation ending up a mesh. reply aeonik 21 hours agorootparentprevI just looked at those other code CAD programs, and I don't see the appeal over OpenSCAD. I have no interest in browser based CAD programs because as models become complex, that platform is too limited in performance. Python and stateful CAD drawings sound like a nightmare to me. OpenSCAD has limitations for sure, but I think a better tool will look different. I do wish OpenSCAD used a more general purpose programming manager. reply everforward 3 hours agorootparent> Python and stateful CAD drawings sound like a nightmare to me. Please correct me if I’m wrong, but it doesn’t appear stateful to me. The context managers mostly make the organization of objects be reflected in the organization of the code. They’re stateful in the sense that some bits are part of a larger assembly, but I think that’s inherent in the domain. The features of the object have to relate to each other so it knows how to stitch the object together (eg which side of a face is external and which is internal). reply throwgfgfd25 20 hours agorootparentprevReplicad is quicker to render complex things than OpenSCAD -- significantly quicker. It uses an emscripten port of OCC. It's also embeddable as a library, which means being able to make web-based object customisers: client-side, script-driven tools that don't require CAD knowledge for the user. Like the Thingiverse customiser but on steroids. It's a fascinating project. And I think it's not the statefulness that is the significant thing about CadQuery and Build123D. It's the access to a bRep kernel, so you can do operations with faces and vertices, you can reflect (analyse, measure) the model, etc. Being able to do operations on a generated face or edge means not needing to know (or recalculate) the location of that face in 3D space; it saves you so much in the way of maths. If you have very simple (or very mathematical!) models, OpenSCAD can help. But once things get complex you just have file after file of variable definitions. Functional flows on vertexes, edges and faces created by previous operations is much closer to a code equivalent of GUI CAD. reply kiba 17 hours agorootparentReplicad is quicker to render complex things than OpenSCAD -- significantly quicker. It uses an emscripten port of OCC. OpenSCAD integrated manifold into its codebase though you would need to use a development build to actually use it since the last release is in 2021. I heard manifold is significantly faster than CGAL. reply throwgfgfd25 6 hours agorootparentThat's good to know. reply bvrmn 5 hours agorootparentprevOpenSCAD basically has no tools to aid complex modeling. You have to know trigonometry and often use pen and paper to calculate points. Build123d has stateless algebra mode. And you could replace math with simple construction elements and simply ask intersection points. reply hugs 21 hours agorootparentprevIf OpenSCAD had STEP file support, I could do all my design work in it. But it can't, so I can't. reply eternityforest 16 hours agorootparentprevOpenSCAD is a counterfeit CAD! It doesn't Aid your Design so much as render one the user has to already understand. I do like it for simple parametric changes to existing models though. I wish we had something like it that could be used to create freeCAD macros, as in \"Here's a sketch, which FreeCAD translates to OpenSCAD arrays, then runs a script that can do stuff with this model as input\" reply bschwindHN 15 hours agorootparentIs that really \"counterfeit\"? As you mentioned, CAD is Computer-Aided Design, and OpenSCAD is certainly aiding in the design process by interpreting higher level commands about where to place geometry. I have a lot of criticisms for OpenSCAD but I wouldn't call it a counterfeit, it's just a code-based approach to constructing something vs. a GUI-based approach. reply eternityforest 11 hours agorootparentIt's more of a joke/exaggeration, but it does explain why I find it to be so hard to use. It's much more of a one way conversation, if you can't imagine all the rotations to make a part do something, trial and error is very slow. Whereas in GUI CAD you mostly only have to be able to think in 2D. And without a constraint solver, you have to have a much deeper understanding of all the spatial relationships involved. reply throwgfgfd25 6 hours agorootparentRight -- OpenSCAD is an object compiler. You give it code, it gives you an object. Your object is not something that can then be used to iterate on, except by placing it in space and adding or subtracting other stuff to/from it. Have you looked at Build123D or CadQuery? Both are Python packages (different API styles, compatible underpinnings) that do OpenSCAD-type things, but using the OpenCASCADE bRep kernel, so it is less \"counterfeit\" -- if you want to do something based on a face or edge or vertex that was the product of a previous operation, you can. Both have some constraints support. In many ways they are both just a prettier alternative to the FreeCAD Python APIs -- indeed there was a CadQuery workbench for CadQuery 1.x. reply rqtwteye 20 hours agorootparentprevI like the idea of OpenSCAD but the language is too functional/immutable for my taste. It's interesting but having to rethink even algorithms with simple loops gets very tiring over time. A debugger would be very helpful to be able to step through the code. reply WillAdams 17 hours agorootparentThere is now a Python-enabled version: https://pythonscad.org/ Using the # operator to make things transparent red helps a lot when stepping/iterating through code. reply rqtwteye 17 hours agorootparentI tried that but couldn’t make it work on my M1 MacBook. Not sure why. reply WillAdams 6 hours agorootparentPlease check in with the developer --- probably best to create an issue at Github: https://github.com/gsohler/openscad/issues reply filcuk 20 hours agorootparentprevThe rendering is also very slow, even on powerful machines. reply jasonjayr 20 hours agorootparentprevJSCAD is a thing: https://openjscad.xyz/ But I really only fight with it because I know JS moderately well. reply throwgfgfd25 6 hours agorootparentHave you looked at Replicad? https://replicad.xyz Similar principles, but a bRep kernel so a much richer API. reply Macuyiko 11 hours agorootparentprevA few weeks ago I was planning to design a model I could send to a local 3d printer to replace a broken piece in the house for which I knew it would be impossible to find something that would fit exactly. I looked around through a couple of open source/free offerings and all found them frustrating. Either the focus on easy of use was too limiting, the focus was too much on blob, clay-like modeling rather than strong parametric models (many online tools), or they were too pushy to make you pay, or the UI was not intuitive (FreeCAD). OpenSCAD was the one which allowed me to get the model done, and I loved the code-first, parametric-first approach and way of thinking. But that said I also found POV-Ray enjoyable to play around with around the 2000s. Build123D looks interesting as well, thanks for recommending that. reply throwgfgfd25 5 hours agorootparentThe major advantage of Build123D for your use case -- sending it to someone else to fabricate it -- is STEP output support. This really expands your options for what you can make and who you can ask to make it. There are now some online fabrication places that will do CNC from mesh formats, but really the only way to have proper control is sending them a STEP file. reply luckydata 21 hours agoparentprevHope that the new changes make freecad a little more accessible. Coming from Fusion I really tried to make it work for me but the UI is so awkward and abstruse I quickly gave up. reply rqtwteye 20 hours agoprevFreeCAD reminds me a little of the GIMP. Super powerful but somehow the UI is just hard to deal with. reply tylerflick 20 hours agoparentThe workflows are so much harder to remember than Gimp’s though. I find myself running back to OpenSCAD every time I give it a shot. reply derekp7 19 hours agorootparentOpenSCAD's main problem is when you get to code of any complexity. I'm ok with the language itself (it looks procedural but is really somewhat declarative), but I keep hitting up against a brick wall with the CSG processing. Note, however that their nightly builds are much faster, if you enable Manifold (a replacement CSG library that is much faster). In fact, a current design I'm working on wouldn't be possible if I hadn't switched to their nightly builds. reply jopsen 19 hours agorootparentprevFor simple stuff I've been pleasantly surprised by dune3d: https://github.com/dune3d/dune3d reply rcarmo 12 hours agoprevSadly, the UX is still pants compared to Shapr3D or Fusion. Yes, it looks slightly better and there are improvements, but still very far from being enough to match either of them in terms of actual workflow. My biggest gripe is that it still feels like a bit of a bag of squirrels (changing workbenches can still lead to unpredictable results and importing STEP is still buggy). reply marmakoide 9 hours agoprevFixing the topological naming issue, in the mainline, what a game changer. I am using Freecad for Actual Real Things. I learned to work around the topological naming issue, but it cost me time, and it can make parametric models quite brittle (ie. a minor change can break the model). reply guerby 8 hours agoparentWas curious about the issue, found this: https://wiki.freecad.org/Topological_naming_problem reply jononor 8 hours agoparentprevYeah I am very much looking forward to that. Over the last 10 years I have made a couple of hundreds of designs in FreeCAD that I have manufactured in smal scale - with FDM/SLA/SLS 3d printing, CO2/fiber laser, and CNC milling in woods/plastics/metals. So it has been plenty productive. But quite often doing workarounds for the topological naming problem, either preemptively or corrective. Maybe I will start to teach it again to others :) reply IgorPartola 17 hours agoprevDoes anyone have any good resources on learning FreeCAD? I didn’t exactly find the interface approachable. Typically I use OpenSCAD for my basic 3D modeling needs. reply mitthrowaway2 17 hours agoparentI like JokoEngineering's tutorial videos: https://www.youtube.com/watch?v=Odr5viqPwkc My first tip for most people would be to start with the Part Design workbench, although if you're coming from OpenSCAD, you might prefer the Part workbench. FreeCAD has many different workbenches for handling various use cases, such as architectural models, surface trimeshes, 2D machine shop drawings, and so on. The various workbenches do mostly work together well, but for a beginner it's intimidating to have so many options. \"Part Design\" is probably the most familiar approach for people coming from high-end CAD programs like SolidWorks; it uses the 2D sketch + extrude workflow. The similarly-named Part workbench is for people who prefer to think in terms of boolean operations on solids, which is generally the OpenSCAD way. reply infogulch 16 hours agoparentprevCommenters in another thread recommend Mango Jelly tutorials. https://news.ycombinator.com/item?id=41515552 https://www.youtube.com/@MangoJellySolutions reply WillAdams 17 hours agoparentprevThe Hackspace folks did a series of articles and put them out as a PDF: https://hackspace.raspberrypi.com/books/freecad (last time I printed it, I had to add a blank to get things to duplex right) reply ragingroosevelt 8 hours agoparentprevNot freecad related, but if you like programmatic cad like openscad, you may like cadquery even more. A lot of operations are way more natural and you can export step, not just stl. reply greesil 17 hours agoparentprevYouTube. I make some pretty basic things to 3d print with FreeCAD and everything I've learned came from YouTube. Typically for me it just new part, new spreadsheet, part design, sketch with dimensions parameterized from spreadsheet. Pad or some other boolean of solids, repeat starting at new sketch. reply kiba 17 hours agoparentprevI think the interface had improved recently. reply Palomides 21 hours agoprevprelininary release notes: https://wiki.freecad.org/Release_notes_1.0 the headliner is definitely topological naming improvements reply throwgfgfd25 21 hours agoparentI would say the most significant things for most hobby CAD users are: * topological naming issue mitigations -- this is mostly solved enough that you can rely on it, though there are definitely still times when it makes more sense to use sketches offset from the base planes * the new integrated Assembly workbench (and solver) though I've not dabbled with this myself * really significant improvements in the sketcher (easier dimensioning, curved slots, polar arrays and improvements to the array tools controls, offset/scale, automatic midpoint constraints) * support for bodies with multiple non-overlapping solids in Part Design * useful subtle improvements to Part Design array tools * some support for operations (pads/revolves/pockets) on only selected shapes from a sketch in Part Design * I don't do CNC yet but I think there are improvements in the CNC workbench that would benefit hobbyists. I would put the UI improvements somewhere lower down the list, frankly, than they do, because I find them often confusing and regularly frustrating on laptop screens, but: * the new dark theme is really nice * OpenTheme's dark theme works well * quick transparency toggling is helpful * and the optional tab bar for workbench switching helps make various disparate workbench tools just that much quicker to get to, somehow, making it all feel a little closer-knit reply rightbyte 19 hours agorootparent> the new integrated Assembly workbench (and solver) though I've not dabbled with this myself FreeCad badly needed this. I will try it out. Any complex drawing quickly go out of hand without assemblies. Also, finaly there seem to be some real measurement ulitity! 'Std Measure'. I've gone made trying to build some parts I made in FreeCad without a sane way to measure distances. I so want FreeCad to kick some entrenched corp ass. reply 9cb14c1ec0 20 hours agorootparentprevPolar arrays will bring me running back to FreeCAD. There are some geometries that are really hard to sketch if you can't use polar arrays. reply alright2565 19 hours agoparentprevIMO the biggest thing is the auto-dimension tool. Instead of remembering 10 different keyboard shortcuts or constantly having to click on the toolbar, I just need to remember a single shortcut. reply LanternLight83 3 hours agoprevI got a printer recently, tried Blender bc it was what I knew, then FreeCAD, OpenSCAD, CadQuery, and Build123D. The last two are Python frameworks built on the same OpenCascade kernal that powers FreeCAD, and I really reccomend them to software folks looking to work in version-controlled plain-text. reply dvh 12 hours agoprevIf you ever rage quitted FreeCAD then give OpenSCAD a try. It's completely different workflow and I love it. It perfectly clicked with the way I work and think. reply marmakoide 9 hours agoparentI rage quitted OpenSCAD for FreeCAD :p For real, because I am way more productive with FreeCAD. FreeCAD allows to work in term of topological features like surfaces, edges, etc which is, in practice, very cumbersome with OpenSCAD. reply guitarbill 5 hours agoparentprevIf you like OpenSCAD but want a fillet or two and STEP export, Build123d is great also. reply syntaxing 20 hours agoprevSuper stoked for this, used to a mechanical engineer and I mainly use solidworks maker or fusion 360 to scratch my itch when I design stuff around the household. As “old school” as the UI is, it has a lot of parallel with catia v5. It’s kinda like vim/emacs, you don’t get it until you do. reply gerdesj 19 hours agoparentI once had to deliver multiple DB Catia environments to multiple groups of designers's PCs. We used a chained series of Novell (as was) Zenworks apps (bundles these days). The environments are largely defined through env vars. So, an app to deliver the various versions of the code, followed by an app to set the env vars for the job in hand, followed by an app to tweak, say drive letters and other things and finally another one to start the interface. All of that lot is defined within a GUI. You could capture the env vars through a \"snapshot\" - basically a diff. from before and after an application installation on a test machine. Nowadays you'd probably use Ansible and a lot of guesswork and farting around. I'm a sysadmin but I have been trying to get to grips with FreeCAD for years. Mind you, I once got my parents to buy me a 80287 maths co-pro. so I could run a dodgy copy of AutoCAD 2(?) I gather that the FreeCAD kernel can now deal with a lot of weird stuff and not go mad when you make ill-advised constraints. As for emacs/vim - not for me mate! I compiled emacs on a Pentium II and decided against it after a while. I tolerate vim because it is ubiquitous, but then so is dandruff. reply syntaxing 18 hours agorootparentYup, a lot of things show when you use software that was first released from 1998. The first company I worked for that used catia has about 10 seats. A special sysadmin we hired that specialized in CAD admin wrote a special powershell script to set everything up. He was awesome and taught me how to write my own vba macros. I now work for a major automaker and they have these precompiled binaries that does everything for you. It’s kinda crazy to think you need a team of engineers to write and maintain an in house codebase just to make sure everything is installed correctly. reply leros 20 hours agoprevFrom what I hear of FreeCAD, it sounds like it's going to be awesome and widely used, but not for 5-10 years. Anyone have enough experience to back that up? I'm personally using Fusion 360 and OpenSCAD. reply jdevy 16 hours agoparentI have 100s of hours of FreeCAD experience from my day job designing injection molded parts for toys. For some background, for about ~3-4 years (~5 years ago) I started using FreeCAD 1.18.1 in my job (and even more before that for hobby use). I am used to using open-source software with bad UI, so that's not my major complaint. As long as you stick mostly in the Part Design, Part, and TechDraw workbenches, you should get used to the UI. I used the main branch of FreeCAD up until 1.19.1 but then switched to RealThunder's LinkBranch [2]. I switched for the topological naming fixes (some introduced in this 1.0), assembly workbench (not the same as in this 1.0), and other many quality of life fixes (multiple solids per body and 3D offsets for most Part Design boolean operations). It was never great but it got the job done. As long as you never need complex organic 3D surfaces, FreeCAD can work - or at least the LinkBranch did for me, I'll have to test 1.0. However, my biggest complaint is with the CAD engine FreeCAD uses: OpenCASCADE (OCCT) [1]. As with most CAD engines, this thing is OLD. It does not like to make NURBS surfaces with true tangency to other faces, and it really doesn't like when fillits cross edges into other faces. You will spend hours adjusting cosmetic geometry so that dress up features like fillits and chamfers will apply. Unless some group of PhDs with some hardcore C/C++ experience come along or the company that develops OCCT gets some major funding, I don't think FreeCAD will improve enough for day-to-day design of complex parts for a long time. Nowadays, I use Fusion 360. I prefer SolidWorks but Fusion is all my job offers me currently. For a CAD package, and coming from years of FreeCAD, Fusion 360 just works. I have tools for making arbitrary complex surfaces (could still be better), I can create fillets that cross into other faces (most of the time), and I can go back in history and edit features and my model will rebuild itself (to a limit, but even the FreeCAD LinkBranch had more issues than Fusion even though it was better than vanilla 1.19.1 and 1.20 FreeCAD). Fusion also has a proper assembly system, which is essential! You can cheat and create parts in FreeCAD by linking sketches to geometry in other parts, but it can only get you so far before you need to go back in time and everything breaks upon a rebuild. I hate to say it, but FreeCAD has a lot of work to do other than the UI. I want to use FreeCAD but it wastes too much of my time for professional work. I would still use it for simple hobby projects. I could talk for hours on this stuff. [1] https://github.com/Open-Cascade-SAS/OCCT [2] https://github.com/realthunder/FreeCAD/releases reply leros 4 hours agorootparentI've heard people compare FreeCAD to KiCad (a PCB design tool). KiCad has been usable for a long time but it's only recently gotten good enough where you might choose to use it over the other choices because it's so good. I've heard FreeCad still has a ways to go before you might choose it over Fusion or something like that. reply LtWorf 20 hours agoparentprevIt works, I 3d print my board games with it. reply BikeShuester 19 hours agoprevJust wanted to share my experience with combining OpenCAD and some AI models for small-scale 3D printing projects. So far, it's been a real game-changer. The precision and accuracy have been impressive. Has anyone else taken this combo to the next level? I'm curious to know if there are any brick walls I'm not seeing yet. Are there limitations or challenges that come with scaling up this approach? Would love to hear about others' experiences with OpenCAD + AI in 3D printing. reply mitthrowaway2 17 hours agoparentWhat is OpenCAD? Do you mean OpenSCAD? Or FreeCAD? reply justinclift 16 hours agoparentprevThat sounds super interesting. What's the approach you've been using for this? reply pineaux 18 hours agoparentprevShow us what you made until now? reply observationist 21 hours agoprevDefinitely want to get a link back to the main site in your blog header - right now you have to edit the URL. Great work! Happy to see this, open and free tools make the world a better place. reply taeric 22 hours agoprevOh wow, super excited to see this posted. Will be on the lookout for updated tutorials. If anyone has good suggestions there, I'm game to check them out. reply cristoperb 21 hours agoparentI recently found this youtuber. He has a playlist for 0.22 (which is the dev version for what will be 1.0): https://www.youtube.com/playlist?list=PLWuyJLVUNtc3UYXXfSglV... reply Nezghul 19 hours agoprevI personally consider FreeCAD very far from stable. All I need to do is to open random example projects to speedrun to some warning/error/exception/segfault. reply declan_roberts 16 hours agoparentI find it exceedingly frustrating to use. I knew I was in for a bad experience when I opened it up for the first time and I couldn't even select some components of the screen because the high resolution DPI monitor made some things unclickable because the pixel boundary box was impossibly small. reply resource_waste 6 hours agoprevI've completed huge projects on FreeCAD. Highly recommended. I imagine lots of complaints are either outdated, or by people who are used to different CAD systems and expect them to work exactly the same. I've tried and worked with Catia, solidworks, and fusion 360, and I can easily complain about each of those for being confusing. reply globalnode 17 hours agoprevAlso, librecad is great for 2d work, house plans, projected profile views etc. reply mark-r 16 hours agoprevIt's a little annoying that I couldn't find a description on their site of what FreeCAD actually is. reply adastra22 11 hours agoparentIt’s a free CAD software? reply mark-r 7 hours agorootparentYes, that part's obvious just by the name. But what OS does it run on, or does it run in the browser? What features does it have? reply throwgfgfd25 6 hours agorootparentIt is C++/Qt/Python/OpenCASCADE, runs on Linux, Windows, Mac. Pretty low compromise in terms of portability; surprisingly good on Mac, has ARM support. I think on FreeBSD/OpenBSD as well via ports. It is a bRep GUI CAD system with 2D drafting, 3D CAD, a technical drawing workbench, FEM, mesh tools etc., and now a core CAD assembly tool. It has a \"workbench\" (think GUI plugins for specific task) approach, supports macro recording of Python macros, has many third-party workbenches, It is constraints-based and fully parametric: designs recompute and reflow when underlying measurements change. It's also a 20 year labour of love by a bunch of CAD users. If you are familiar with QGIS, it's really a lot like that but for CAD. It's less like GIMP than some people say, but it is a bit like GIMP (and like GIMP, is in a long battle with a core architectural problem; FreeCAD 1.0 includes a big victory over its worst core problem) reply rowanG077 20 hours agoprev [–] I can't believe people aren't mentioning solvespace. Basically my cad journey started with openscad. Which I quickly discarded for cadquery. Which I used for a bit. And now I use solvespace. Imo they all suck. Solvespace has serious issues with anything round. It's basically a no go to design anything that is round in it. I wanted to design a simple pen like structure with a slot, turned out to be impossible. Perhaps I'll get so annoyed I go back to cadquery... reply bvrmn 5 hours agoparent+++ for build123d. Just finished TTT tutorial models and it was quite fun. There are some issues with non-regular fillets and 3d offsets but they are minor comparing to FreeCad crashes. Build123d algebra mode is fantastic, especially after your find out how to compose faces from custom line chains. Documentation is good, though many tricks how to get non-trivial tangent points could be found only in examples. reply mitthrowaway2 19 hours agoparentprevHave you considered, perhaps, trying the FreeCAD 1.0 release candidate? It should be very quick to make this. Part Design -> sketch -> draw your pen shape -> revolution to make it round; then sketch -> pocket to make the slot. reply hantusk 12 hours agoparentprev [–] Check out build123d, which has a nicer api for cadquery. I draw sketches in svg if the sketch shapes become too unmanagable to express in code reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The first release candidate (RC1) for FreeCAD 1.0 is now available for download, aiming to gather user feedback for stability improvements.",
      "There are currently 7 release blockers, with more issues expected to be reported, which will help enhance the software's stability.",
      "Users are encouraged to test RC1 on real projects and report bugs, while developers can contribute by fixing issues and participating in weekly merge meetings."
    ],
    "commentSummary": [
      "The first release candidate of FreeCAD 1.0 has been announced, positioning it as a free alternative to costly CAD software like Solidworks and Fusion 360.",
      "Key improvements include topological naming fixes and a new assembly workbench, though some users still find the UI and stability challenging.",
      "The community is optimistic about FreeCAD's future, comparing its potential growth to other successful open-source tools like Blender and KiCad."
    ],
    "points": 324,
    "commentCount": 171,
    "retryCount": 0,
    "time": 1726086592
  },
  {
    "id": 41517312,
    "title": "Konty – A Balsamiq-alternative lo-fi wireframe tool for modern apps",
    "originLink": "https://konty.app/http://localhost:4321/",
    "originBody": "Konty Download Blog Docs Download Blog Docs Sketch your app ideas without stress Make hand-drawn style wireframes quickly and easily. Download Now Features Stress-free hand-drawn style Don't spend a lot of time and effort creating low-fidelity wireframes. Express and communicate your ideas quickly. A hand-drawn style reduces stress on perfection and allows you to express ideas quickly. Mix with various diagrams You can also draw various types of diagrams including Flowchart, UML diagrams (Use Case, Class), Entity-Relationship diagram. It help you drawing user flows, information architecture, data models and others in one place. Rich shapes, icons and templates Offer almost every UI component shape you need, over 1,500 icons, and a variety of templates for web, mobile, desktop and more. Presentation mode Shape could have a link to another page and you can present your wireframe like a slide show. Mirroring a frame You can create a mirror of a frame. It allows you to create a master frame and reuse it in multiple places. When you update the master frame, all the mirrors will be updated. Subscribe to our newsletter Get notified about new features and future giveaways by subscribing to our newsletter 👇 ©2024 About Us Privacy Terms",
    "commentLink": "https://news.ycombinator.com/item?id=41517312",
    "commentBody": "Konty – A Balsamiq-alternative lo-fi wireframe tool for modern apps (konty.app)323 points by niklauslee 15 hours agohidepastfavorite118 comments olivierduval 10 hours agoSomething always bothered me: why using \"sketch-like hand-drawn pencil\" like style for that kind of tools ? I understand that \"wireframing\" is some kind of \"brainstorming\" tool, so it is used with a pencil and a whiteboard in a meeting room and require to draw/erase fast iteratively... so it's the \"right\" tool for this job... But as soon as you use a computer instead of a pencil, why not have a \"realistic\" and \"clean\" look instead of this kind of quick-and-dirty sketch-like style? It's an honest question Is it because designers are most used to this style? Is it because it make more clearly appear the essential points (for example: a list) and avoid discussion like \"is this text exactly in this color ?\" reply estsauver 9 hours agoparentThe reason that I've heard used repeatedly is that a shocking percentage of folks who aren't Technology producers can't separate visual quality from \"doneness\" of a project. If you show some business folks something that looks like it works, they'll mentally update the project to \"Nearly done!\" and then everything else after that becomes \"Unreasonable delays.\" reply appendix-rock 6 hours agorootparentYes. This is precisely it. There aren’t two sides to this, just people that haven’t themselves experienced this absolutely inevitability. These sorts of inexact-looking tools are worth their weight in gold for that reason alone. reply mhuffman 5 hours agorootparentprevI have had prospective clients do it from non-interactive graphic mock-ups -- just pictures! They assumed that was the hard part and just \"wiring up the buttons\" would be a short simple task. Those were frustrating discussions. reply com 1 hour agorootparentDevil’s advocate … why shouldn’t this be true? That’s how HyperCard worked, right? reply dmje 7 hours agorootparentprevI presented a wireframe to a curator at The Science Museum once years ago - even after lots of \"please bear in mind this is just a prototype\" type disclaimers, his first response was \"surely it'll have more colour and pictures than this?\". So. Yeh. reply Groxx 3 hours agorootparentprevThere is definitely this, but also: if it looks \"refined\", people start getting attached to what they see, and it affects how they react to the final product. Any change from that haphazard throwaway with nice colors is suddenly a change they have opinions about, because it feels like a change. If you show them something that's obviously not what will ship, they don't get as attached. --- This is also partly a \"most people don't understand the design process\" thing, and just how much reworking and restarting is generally necessary to get an actually-good end result. If they see hundreds of mockups (or even sketches), they'll wonder why you haven't made hundreds of products, rather than those being merely tools used to think along the way. reply dspillett 9 hours agorootparentprevThis is unfortunately very true. You also have to be very careful with word/phrase choice in discussion about future work: people often hear “what we could do, is…” as “there is already a full feature that allows you to configure the tool to do…”. You really have to drill home that ideas and possibilities are just that, and not concrete features that they could start using tomorrow. reply duggan 8 hours agorootparentprevThis is also what I've heard and experienced. Actually I don't think \"technology producers\" are entirely excluded from this bias either. I've assumed more complexity than there was in reality (possibly due to my background in infrastructure and backend), but other developers I've worked with certainly fall more into the trap of \"there's a UI? now it's just a simple matter of CRUD.\" reply gadders 3 hours agorootparentprevI remember in the early 00's this book suggested literally prototyping on paper first. https://www.amazon.co.uk/Paper-Prototyping-Interfaces-Intera... I think this was then expanded to be \"paper-looking\". But yes, for the reasons you state. reply mguerville 3 hours agorootparentprevAnd criticize the colors, shading, exact sizes of UI elements, etc. instead of the underlying holistic UX reply viraptor 5 hours agorootparentprevWhile this is likely true for designs, I believe there's more to it. I switched from straight to cartoon lines for my architecture / planning diagrams and suddenly started getting more unprompted comments about how they're clear and approachable. Personally I also prefer the hand-drawn style, but can't put my finger on why. There's something about the uneven lines filling out the space better, while still defining the shapes well. reply llamaimperative 4 hours agorootparentI think you're pointing to the positive case of the same effect, which is that people use \"hints\" from the level of detail of something to determine the level at which they ought to inspect something. Lower fidelity puts the viewer in a more conceptual mode of assessment, and there they can more easily perceive the clearness/approachability of your concepts. reply netghost 3 hours agorootparentprevA slightly different take. If everything is either an obvious sketch, or pixel perfect you can get decent feedback, but a design that is just a little off in jarring ways will distract people from the functionality or design intention. reply Lio 7 hours agorootparentprevI think Kathy Sierra used to wrote about this quite a bit. She's actually referenced by Balsamiq I think. reply rpastuszak 9 hours agoparentprevA) Make it easier to focus on the core aspects of the problems instead of obsessing with details (applies to both designers and \"reviewers\") B) An \"unfinished\" messy design is an invitation for critical feedback. If you give people something that looks too polished, they might be afraid that they'll break it, that they don't understand it, that they can't give feedback that is \"good enough\". In short: if it looks like a toy people will play with it. * C) The reason many of these tools look like Balsamiq has more to do with the tech of the late 00s/early 10s. This specific style of vector art was pretty easy to achieve in Flash. reply Beretta_Vexee 9 hours agoparentprevThis style says ‘it's a draft’ ‘it's an idea’. This is very important for communication within the team. It also allows you to concentrate on the essential points and not on the details (I don't like this font, the centring isn't perfect, etc.). To my great surprise, even for training courses, this style encourages questions and interaction with the students. There's a whiteboard feel to it which suggests that the presentation isn't set in stone. reply specialist 6 hours agorootparentRight. The more polished a rendering is, the more people are emotionally attached to it. Keeping it rough enables brainstorming, whatifs, etc. Ages ago, when CAD was new, architects would show customers tracings (of plots). For all the same reasons. The practice was so common that my buddy (also an architect) created a \"hand plot\" driver for AutoCAD. \"Messy\" hand drawn look instead of precise line work. The driver was huge popular. reply ramraj07 9 hours agoparentprevIf I draw something in balsamiq, I’m typically “forgiven” for how basic the design looks. Try and do the same in let’s say MS paint and you could be called unprofessional and lazy. But this style seems to communicate strongly that this is a basic barebones wireframe. Honestly it also looks better. reply victorbjorklund 6 hours agoparentprevI usually dont use wireframes like this but one benefit is that it clearly communicates \"this is NOT a finished design\". Way to many times you bring a figma/mvp to get feedback on the \"big picture\" like the user flow etc but people get stuck on \"the margin on that box is wrong\" or \"can we use another font?\" when they see a design that looks like a \"finished\" product. You dont have that issue with wireframes. reply shagie 35 minutes agoparentprev> Something always bothered me: why using \"sketch-like hand-drawn pencil\" like style for that kind of tools ? https://napkinlaf.sourceforge.net (one of my favorites from back in the day) > The Napkin Look & Feel is a pluggable Java look and feel that looks like it was scrawled on a napkin. You can use it to make provisional work actually look provisional, or just for fun. It is released under a BSD-style license > The idea is to try to develop a look and feel that can be used in Java applications that looks informal and provisional, yet be fully functional for development. Often when people see a GUI mock-up, or a complete GUI without full functionality, they assume that the code behind it is working. While this can be used to sleazy advantage, it can also convince people who ought to know better (like your managers) that you are already done when you have just barely begun, or when only parts are complete. No matter how much you speak to their rational side, the emotional response still says \"Done!\". Which after a while leads to a later question: \"That was done months ago! What are they doing? Playing Quake?\" A good article on this is Joel on Software's “The Iceberg Secret, Revealed”. ... and that's the place that I remember where to find this blog post: Don't make the Demo look Done - https://headrush.typepad.com/creating_passionate_users/2006/... > When we show a work-in-progress (like an alpha release) to the public, press, a client, or boss... we're setting their expectations. And we can do it one of three ways: dazzle them with a polished mock-up, show them something that matches the reality of the project status, or stress them out by showing almost nothing and asking them to take it \"on faith\" that you're on track. > The bottom line: How 'done' something looks should match how 'done' something is. > Every software developer has experienced this many times in their career. But desktop publishing tools lead to the same headache for tech writers--if you show someone a rough draft that's perfectly fonted and formatted, they see it as more done than you'd like. We need a match between where we are and where others perceive we are. The infographic in this post ( https://headrush.typepad.com/photos/uncategorized/feedbackim... ) is especially important because the how it looks changes what type of feedback you get. I had a project where I grabbed the stylesheet and header from another similar project while working on it... and spent a week discussing with management about what color blue it should be when the questions I needed answering were \"does this page flow make sense?\" reply codegeek 1 hour agoparentprevExactly. I feel the same way. After lot of research, I settled on Whimsical for doing mockups/wireframes. Good Balance between Simplicity and Power. Only complain is clickable prototyping which is not available. If they add that, I would never leave Whimsical for prototyping. reply veenified 4 hours agoparentprevSometimes the pixel perfect details don't matter for a use case, so why set the hi-fi expectation for both the designer and developer. The designer can get caught up in choosing colors and pixel-perfect layout, and similarly the developer implementing on that design might unnecessary time attempting to match the hi-fi design. reply adastra22 2 hours agoparentprevBecause the final product will require tons of details to have been thought through, which can quickly become bike-shedding derailments. How many times have you had to say “this is just example styling—we can tweak it later”? The hand drawn sketch conveys that implicitly. reply ashildr 9 hours agoparentprevIt’s an abstraction that makes people focus on the part that is relevant for the discussion at hand, and not on implementation details. reply niklauslee 9 hours agoparentprevPsychologically reduces obsession with the perfect drawing. reply olivierduval 10 hours agoparentprev(to be honest, I find this \"pencil-like\" look a bit like MS Comics for fonts, ugly and unprofessional... so I really don't understand why designer tool use it so much) reply toyg 8 hours agorootparentAnybody who's ever been in a few meetings that try to put together stakeholders, designers, and developers, know how it will inevitably descend in painful back and forth about a shade of hue or an icon size. People get distracted by colors and graphics, and fail to provide actual feedback on functionality and layouting - which are the hardest bits to change later. The point of this style is to communicate that it's a rough draft, so that people focus on the essential implementation and functionality requirements, the hard stuff. It's easy to give it a lick of paint later. (It also keeps expectations low, so that the final result will feel like you're overdelivering. But that's just bonus.) reply olivierduval 1 hour agorootparentprevFor those \"downvoting\" this comment, please: I wrote it right after my initial post, before any answer, to make my initial post clearer. I certainly should have added this to the main post. Now that I have all these answers, I understand better. But cant delete or modify this comment. So sadly it's here for eternity :-( Thanks a lot for your insightfull comments to the original post. Actually, I now think that I will use these method to help getting more feedback from users reply aosaigh 48 minutes agoprevThis is great. I'm a regular Balsamiq user but prefer the look-and-feel and subtle aesthetic differences in Konty. I'd love some sort of commenting or call-out system on drawings. The \"stickies\" work well in some cases, but I regularly find that I need to draw attention to certain parts of a design and don't want to have to manually create an arrow with a sticky, or an arrow with text etc. Also, a small frustration, but when deleting items I reach for \"del\" on the keyboard, which isn't implemented here (\"backspace\" works though). reply jksmith 4 hours agoprevDig it. I use Balsamiq all the time. Some challenges when using Wine, so I have to open a cringey Klaus Schwab windows machine. Would be great if this app showed Linux some love. reply hexfish 3 hours agoparent> a cringey Klaus Schwab windows machine Say what? haha reply TuringNYC 3 hours agoprevI see the company is based in Asia. I highly recommend considering some branding feedback from westerners. The name of the app will raise eyebrows for many. reply febeling 2 hours agoparentWhat are the eyebrow raising connotations you have? reply esafak 2 hours agorootparentIt sounds like cunty (adjective) or cuntie (diminutive). https://www.youtube.com/watch?v=Obagb7RQeYo reply jnsie 2 hours agorootparentprevReplace an o with a u reply juliushuijnk 12 hours agoprevIf you want to do this kind of thing on your phone, you can try my TinyUx: https://www.tinyux.app/ It has a non-standard UX itself, because of the small screen. reply albertgoeswoof 11 hours agoparentSo cool! Do you have an iOS version? reply antisthenes 5 hours agoparentprevWhy is this on a phone? Are you supposed to draw the UI with your finger or something? reply aloisdg 7 hours agoparentprevnice! Is it FOSS? Can I contribute to it? reply steveharman 11 hours agoprevWould be nice to see a \"push to Figma\" option - where a lot of high fidelity work will probably be started, based on wireframes. reply niklauslee 11 hours agoparentGood point. Added to our backlog. reply pabe 10 hours agoprevLooks nice, like excalidraw fine tuned for wireframes. However, I'm on Linux so I'm not able to use the app. reply melicerte 6 hours agoparentwireframesketcher[1] seems to do the same than Konty and runs on linux. I'm not related to them in any way but use this solution for years and I'm very happy with it (paying customer). [1] https://wireframesketcher.com/ reply aloisdg 7 hours agoparentprevlove excalidraw btw reply replete 3 hours agoprevGomockingbird was the best at this (for my purposes), but they decommissioned and didn't open source it like they said they would. Balsamiq was next best and I use it still, but has a cumbersome user interface with enough friction that it gets in the way. I tried using Excalidraw for a while, for my dislike of using Balsamiq, but for wireframing even with libraries it was too fiddly. Just tried out Konty and it feels like an upgrade to Balsamiq for sure, and is clearly inspired by Excalidraw. Great work reply wusel 6 hours agoprevI thought the connecting arrows were bugged at first, then I realized it's a genius implementation. This alone makes me want to use this more than Figjam. reply patafemma 6 hours agoprevWell done! Basic functionality feels pretty smooth and polished. One thing that I found myself very quickly missing: being able to snap shapes to each other or to the grid. reply pcranaway 11 hours agoprevI love how I just downloaded this, and had the wireframe of my app's main screen built within 3 minutes of me knowing about this piece of software reply nreece 13 hours agoprevLooks good! I wonder if there's a way to combine a simple tool like yours (or Balsamiq, which I've used for many years) with generative AI to create plain HTML/CSS pages from mockups/wireframes. Figma seems bloated, v0 is React/Tailwind only. reply yoz 13 hours agoparentTLDraw Make Real - which was initially thrown together by a Figma engineer who added GPT vision to an open source whiteboard app - is remarkably good at this. You can find it at https://makereal.tldraw.com/ but the guide there doesn't explain how to get the best out of it. I recommend this article by the TLDraw team which goes into some of the remarkable tricks you can use, and what people have done with it: https://tldraw.substack.com/p/make-real-the-story-so-far reply rnavi 13 hours agorootparentMake real tdraw is just amazing. Love it. reply niklauslee 12 hours agoparentprevYes, we are thinking about integrating with AI! reply 8mobile 13 hours agoprevHi, Balsamiq is one of my favorite products, I have already downloaded konty and I stress it a lot. Congratulations for the idea and for the product, how did you come up with it? After the beta will it be paid? I will give you some feedback soon. Thanks reply niklauslee 12 hours agoparentThank you for your feedback. I'm thinking of the paid version. I would like to offer it much cheaper than balsamiq, probably. Additionally, we'll be offering strong discounts for early users. reply tyrw 12 hours agorootparentBalsamiq is already so cheap. We use it for our business and every time it renews I just think they could be getting 5-10x what they are. That in turn helps drive a better business and product. reply jonwinstanley 9 hours agorootparentBalsamiq is a per month subscrtiption isn't it? Personally, I need a tool like this once per year or sometimes even less. So if Konty was a one off payment of $20-30 I'd be more inclined to purchase. reply quantisan 7 hours agorootparentBalsamiq offers a desktop version with a one-time license fee at $149 per user https://balsamiq.com/wireframes/desktop/ reply nprateem 11 hours agorootparentprevYou should consider a one time, lifetime payment. As a solo dev working on occasional side projects I just wouldn't even consider something on a subscription, and $140 (balsamiq's one time fee) is about $100 more than I'd pay. My alternative is a graphics app I already own. Follow what Affinity did (cheap and one-time) and you'll sell to a lot of people like me who would otherwise give it a miss. Save your subscription tiers for businesses needing more collaboration, SSO, etc. With that strategy as well you'll build brand awareness which will probably ultimately lead to more sales as those solo devs advocate for its use in teams in their day jobs. reply hermitcrab 3 hours agorootparentDo not give free upgrades for life. You will almost certainly regret it. Many people have made this mistake. https://successfulsoftware.net/2008/09/08/should-i-give-free... reply thinkloop 2 hours agorootparentI personally don't trust products with \"lifetime\" tiers will be around in the future, so that would be a negative flag for me. reply GordonS 10 hours agorootparentprevIIRC, an age ago Balsamiq also offered one-time payments for lifetime desktop access. reply quantisan 7 hours agorootparentit's still available https://balsamiq.com/wireframes/desktop/ reply nprateem 7 hours agorootparent$140 reply dewey 9 hours agoprevThe submitted url links to this, still works but just fyi: https://konty.app/http://localhost:4321/ reply itslennysfault 3 hours agoparentIt's weird everything after the slash seems to be ignored. You can type anything and it still goes to the home page. funky. reply niklauslee 8 hours agoparentprevOops! Is there any way to fix it? I can't find edit button. reply eashish93 11 hours agoprevI like it, it's better than other apps. Reason is it present you a list of all components of left sidebar so we don't have to think of creating it from scratch. Just drag and drop and your work is done. reply pentagrama 6 hours agoprevThis is great. Let you know that your blog seems that doesn't support RSS. I will like to follow the project there. https://konty.app/blog/ reply mdaniel 3 hours agoparentand while fixing that: this markup is not helpingreply tchock23 5 hours agoprevThis is great - thanks for making/sharing it! I use (and like) Moqups, but the lo-fi nature of Konty is really nice. Seems very easy to use and responsive so far. reply chrisvalleybay 5 hours agoprevJust a question; I'm seeing so many tools pop up with these kinds of advanced whiteboard functionality, all the tools on the top and the tool palettes on the right. Is there a library or something that's being used to implement all of this? They all look the same. Product looks good, though! Congrats! reply daverobbins1 3 hours agoprevLooks great. Can we get this on homebrew? reply JakaJancar 14 hours agoprevLove it! I always liked Balsamiq, it really forces you not to obsess about the pixels too much, but it was so slow/bloated/buggy, like something from the Java on desktop era. This is much smoother! reply Brajeshwar 14 hours agoparentWell, yes, it was from the Flash era. It started in Flash/Flex. I love it and used it for a very long time. Huge respect for Peldi (Balsamiq founder). reply hermitcrab 3 hours agorootparentIIRC the desktop product was rewritten and is still being updated. reply janwillemb 14 hours agoprevLooks nice! Maybe add some explanation about licensing on the first pages. And the name sounds like \"butty\" in Dutch, so that will be hard for me to recommend out loud for my Dutch IT students. reply Lio 7 hours agoparentThat's nothing. In certain English accents pronouncing \"konty\" is likely to cause even bigger, er, headaches than an innocent reference to a butty. reply dmje 7 hours agorootparentSadly I came here to say this reply Lio 7 hours agorootparentI keep picturing my mate from Rotherham saying it... :D reply aitchnyu 14 hours agoparentprevCoq apparently renamed since professors have to introduce it to 18 year olds each year. reply probablybetter 10 hours agoprevno Linux build? (appimage or snap etc? not expecting distro support for proprietary small-shop software) reply trenchgun 9 hours agoprevNot sure if it would be out of scope to have support for PlantUML etc programmatic generation reply mkarliner 9 hours agoprevVery nice. I've been looking for a replacement hand drawn tool for ages. reply steve1977 12 hours agoprev> Don't spend a lot of time and effort creating low-fidelity wireframes. Modern software development in a nutshell reply __bax 12 hours agoprevBalsamiq still rocks ! reply monkeydust 11 hours agoprevThis is cool, fan of Balsamiq. What I would really like is some alignment/snap feature similar to what you have in MS power point when you put some shapes together and it overlays some lines to help with spacing and gaps. reply saagarjha 10 hours agoprevPsst…you have a typo on one of the images, where it says \"Delete from Shopping Card\" when it should probably say \"Delete from Shopping Cart\". reply the_arun 12 hours agoprevLooks really cool & easy to use. In Mac, we cannot delete a frame or other objects with \"Delete\" key after selecting it. We have to right click & select \"delete\". reply jonwinstanley 9 hours agoparentBackspace works for me reply niklauslee 12 hours agoparentprevOh! I'll check it. reply thuhien2621 14 hours agoprevI'm a Business Analyst, so I find your tool quite interesting. I'll definitely give it a try. However, I would like to ask if your product includes sufficient notation to draw according to BPMN standards. reply porker 12 hours agoparentWrong tool for this - I'd recommend https://bpmn.io/ instead. reply Jolter 12 hours agoparentprevI think it’s not a tool for business analysts but for UI designers. reply equalsabhi 14 hours agoprevIs this a free to use tool? I was thinking about whether the GTM should be a figma plugin vs. a desktop app. Would love to know the founder's thought process on choosing the desktop app route. reply niklauslee 11 hours agoparentFor the future, we're looking at web-based version with real-time collaboration. But for now, we decided to start with desktop. We were also influenced by the \"file over app\" philosophy of Obsidian's founder: https://stephango.com/file-over-app. reply rnavi 13 hours agoparentprevExcellent point. reply groby_b 2 hours agoprev\"Please download a random binary from a place that doesn't even charge for the binary and seems to be set up yesterday\" is... raising my hairs. I'm sure odds are this actually isn't malware, but - I'd think about how to address that fear. reply warthog 3 hours agoprevThis looks amazing reply aloisdg 7 hours agoprevnice! Is it FOSS? Can I contribute to it? reply DonnyV 2 hours agoprevLooking into how this is built. I see they use something called Squirrel.Window for managing installs. I can't believe I've never heard of this until now! https://github.com/Squirrel/Squirrel.Windows. Fastest loading electron app I've ever seen. As a long time user of Ballsamiq. This is FANTASTIC!! Everything is super smooth, nice drawing styling, well thought out. My only problem with Ballsamiq Desktop was the price. I just don't use it enough to pay $150 for 1 license. Something like $60 for desktop would be better. Good luck with the business. I will definitely be using your app. P.S. I just noticed it groups things automatically....HOLY SMOKES! P.S. 2 As a map user. When switching to the pan tool (hand). The scrolling up/down should zoom in/out. P.S. 3 It definitely needs a pdf export option reply wiradikusuma 7 hours agoprevFYI in Indonesian slang* it means penis. * which is also a slang for another slang. Inception! reply Brajeshwar 14 hours agoprevFor those who love this type of tool, you will also love Kinopio. I've no affiliation. https://kinopio.club/ I've seen the founder, /pketh answer questions here on HN. Update/Edit: The other open-source alternative to Balsamiq-ish tool is https://excalidraw.com reply keyle 14 hours agoprevWhat's going on with the url? https://konty.app/http://localhost:4321/ Nice app. Loved Balsamiq for years, now I use an outdated version of Sketch. reply Sparkenstein 13 hours agoparentSubmission url is incorrect. https://konty.app/anything works. reply porl 13 hours agoprevNo Linux support :'( reply berkes 12 hours agoparentI don't understand these decisions. This is a collaborative tool. So you cannot say \"only 5% of the audience is Linux users\", but instead you'll rule out any team where at least one member is Linux user. Which is a far larger group. If I discount myself, that's 8 of 9 teams and startups I worked in last years where we needed wireframing. But I hope the Konty team has better numbers on this. I presume they know more than my anecdotal numbers. reply melicerte 6 hours agorootparentIn the meantime, wireframesketcher[1] seems to do the same than Konty and runs on linux. I'm not related to them in any way but use this solution for years and I'm very happy with it (paying customer). [1] https://wireframesketcher.com/ reply niklauslee 11 hours agorootparentprevYes, we'll also consider Linux distributions. reply donatj 14 hours agoprevAre people still using Balsamiq?! I haven't heard that name in literally forever. I used to use it and love it like fifteen years ago when I fancied myself a designer and not just a backend dev. reply berkes 12 hours agoparentI use it a lot. Figma, penpot etc, aren't for me. I often need something in the phase where we're deciding on \"what's on the page at all. And what screens do we have\". Way before there's need for styling and layout, which I'll leave to skilled designers. I need something with libraries. \"This is where a map goes\" and \"we have a modal here\", and I can just plop in a thing that communicates \"this is a map of some country\" or \"a large modal\". Again, without styling, shadows, animations or even proper layout . And I need something that I can share with coworkers. A pen and paper (with grids), or whiteboard works best for me, but has no libs and is hard to collaborate on (in a remote, hybrid environment). reply pqdbr 4 hours agoparentprevI'll bite: what are people using instead of Balsamiq? reply ilt 14 hours agoparentprevI am still using Balsamiq for low-fi wireframes and low-fi prototyping. Mostly for desktop application development these days. Absolutely love it. Desktop version is still mostly on par with its subscription model counterpart, only major difference being collaboration thingies. reply Brajeshwar 14 hours agoparentprevI remember was using an old version around the time just before the recent Pandemic. Balsamiq went with the current trend and is focusing on subscription/saas model targeting businesses. Peldi also seem to have retired or is semi-retired. reply toyg 13 hours agorootparentI've not heard that nickname in 20 years! I went to his same highschool (liceo), he's two years older, and I was always pretty amazed by the guy - including how he took what was effectively an offensive slur against him (\"carrot hair\", pel di carota) and claimed it as his nickname. Doing that as a teenager takes balls. Great to know he's sorted for good, I wish I was (lol)... reply cstuder 9 hours agorootparentprevThe desktop version of Balsamiq with a one-time-payment is still available, but you have to hunt for it a little. reply fleaaa 12 hours agoprevFantastic job! EDIT: No linux support :( reply zQPl0t1xQIJw 11 hours agoprev [2 more] [flagged] fragmede 11 hours agoparent [–] lol nice try reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Konty offers a tool for creating hand-drawn style wireframes, allowing users to quickly sketch app ideas without focusing on perfection.",
      "The tool supports various diagrams, including flowcharts, UML, and ER diagrams, and provides access to over 1,500 icons and templates for web, mobile, and desktop.",
      "Features include a presentation mode for linking shapes to pages and a mirroring function to reuse and update master frames across multiple instances."
    ],
    "commentSummary": [
      "Konty is a lo-fi wireframe tool for modern apps, similar to Balsamiq, praised for its sketch-like style that helps focus on core functionality and encourages feedback.",
      "Users appreciate Konty's ease of use and discuss potential improvements like AI integration and enhanced commenting features.",
      "Currently desktop-based, Konty plans to release a web version, with discussions on pricing strategies and Linux support."
    ],
    "points": 323,
    "commentCount": 118,
    "retryCount": 0,
    "time": 1726111883
  },
  {
    "id": 41514944,
    "title": "A MiniGolf game for Palm OS",
    "originLink": "https://ctrl-c.club/~captain/posts/2024-08-29-holy-smokes-I-Just-released-a-minigolf-game-for-palmos-in-2024.html",
    "originBody": "Back to overview Holy smokes, I just released a MiniGolf game for Palm OS in 2024 This is a follow up to my previous post: Retro Coding Like It's 1999: My Journey into creating a Palm OS MiniGolf Game This summer, I embarked on a side project to create a brand-new Palm OS game, and after less than two months of intermittent coding, I'm excited to announce that it's ready to be released to the public! Let me present to you \"Captain's MiniGolf (v0.6)\": Besides hoping to have created a fun little MiniGolf game, the strong point of the game is that you can create your own levels: The game allows you to create your own levelpack databases. Those can be exported and shared with other users, not sure if anyone is going to create these, but it would be fun to see what courses other users can come up with. Play it! Don't have a Palm OS device? No problem, thanks to the cloudpilot emulator, you can directly play it from the browser (Get a Palm though, you won't regret it) Game download and in-browser emulator Coding for Palm is sometimes harder than I remember Some things I realized while coding this in C: - You can really mess things up without the hand-holding that you get in modern programming languages - Memory leaks happen more often than you think - Debugging polygon shapes and trajectories can be hard, so having a debug build that visualizes some behind-the-scenes logic is a big help Programming for an old platform like Palm OS can be difficult because of the lack of documentation, but I used the following 2 reference guides to help me out: The Palm OS Programmer Companion (part of the Palm OS SDK) Palm OS Programming Bible There are also some projects up on GitHub of developers that shared the code for their old Palm OS games. Why bother?! Palm OS devices have always been close to my heart, from the moment I got my Palm M100 to this very day. The simplicity and elegance of devices that can achieve so much with so little is something we've lost over time. When programming for these devices, you inevitably encounter their limitations, but these constraints encourage you to think creatively and find alternative solutions. Is a function too slow or using too much memory? You have to find another way to do it! With the excessively performant phones we have today, nobody is going to give it a second look to see if a function could be optimized... cpu makes up for coding mediocrity/laziness. Known bugs - The ball can get stuck in a wall - If you create a level that has a closed polygon of walls within the main playing field walls, the game can't color the background/course correctly. Improvements - Add a delete/move level option - Add a delete and share levelpack option (can be done now using an external application like FileZ) - Resolution is now fixed to 160x160 (or 320x320 on Palm OS 5 hi-res devices), this should be made dynamic based on the available screen size. Sharing is caring I am releasing the full source code (GPL3 license) for this game as well, in the hope that this can inspire or help others to create more games for Palm OS. Captain's MiniGolf source code The draft for this article was written on my Palm Zire 72. You can get in touch through Mastodon: @rxpz@social.linux.pizza Holy smokes, I just released a MiniGolf game for Palm OS in 2024 was published on 2024-08-29 Back to the overview 📰 Subscribe to RSS feed 📰",
    "commentLink": "https://news.ycombinator.com/item?id=41514944",
    "commentBody": "A MiniGolf game for Palm OS (ctrl-c.club)309 points by capitain 22 hours agohidepastfavorite93 comments JKCalhoun 21 hours agoMiniature golf was a game I began twice for the Macintosh back in the 1990's but never ended up completing/shipping. I've just recently been trying to recover my nearly 35 year old sources and create disk images for use on an emulator. The first B&W attempt at MiniGolf is here: https://github.com/EngineersNeedArt/SoftDorothy-UnfinishedTa... The second attempt (when I was a better programmer) was in color ... will make it on Volume 2. (I'm currently trying to put that disk image together.) reply aresant 21 hours agoparentGlider was THE franchise of the 68k mac era, I am more than a little starstruck, glad to see that you are back at it! reply JKCalhoun 20 hours agorootparentIt's kind of for nostalgia that I am putting together all the sources, artwork, projects, (tools) on a disk image suitable for emulators. To me it has been fun trying to go through old hard drives and find the \"almost rans\" like MiniGolf (and later LiliPutz). The shareware and commercial games (Glider, etc.) are on disk images in other repos. (You should be able to find them easily if you care to.) reply InsideOutSanta 11 hours agoparentprevI played so much Glider as a kid. This is probably the only opportunity I ever get to say this, so: thank you very much for your games! reply JKCalhoun 6 hours agorootparentThank you. I enjoyed writing it. reply yard2010 5 hours agoparentprevThank you John! This repo has a little of everything I love: art, engineering, history, but mostly art. It's truly inspiring. reply JKCalhoun 5 hours agorootparentWhen I redid Glypha (an old shareware game I wrote) last year for Steam, I decided to go with B&W pixel art for the game — even though in other ways I bent to modern hardware (a larger 16:9 screen size for example). That B&W pixel art definitely was a defining feature of the era. reply ElCapitanMarkla 12 hours agoparentprevYou have some amazing graphics there. Love the style reply JKCalhoun 6 hours agorootparentThere is something about black and white pixel art.... reply kleiba 12 hours agoprevLooks great! Have you considered adding a \"ball dropping into the hole\" animation? In the video, it looks like a new level is loaded as soon as the ball and the whole overlap to a sufficient degree. I think from a user's perspective, it would be much more satisfying to somehow see the ball go in. It would give you more of a sense of achievement before the next level is loaded. What do you think? reply capitain 11 hours agoparentI am considering it now, thanks! reply kleiba 8 hours agorootparentI also noticed that the collision detection is not always perfect. Here's a screenshot from one of your videos where the ball is rolling over the side barrier: https://imgur.com/a/2tW1EOK reply lxgr 18 hours agoprevAmazing work, thank you! I'm starting to wonder whether Palm OS and other \"retro\" homebrew executable formats might have their actual practical uses these days, beyond the nostalgia: I can run Palm OS .prcs, .gb homebrew ROM from itch.io etc. on my desktop, iOS, and Android, as well as on physical gaming devices; offline, efficiently, distraction-free, without any chance of in-app purchases... Take Apotris, for example: I've bought (and probably will buy) a lot of official Tetris versions over the years, but here's an incredibly slick implementation I can play on all of my consoles, or even on any computer without installation (thanks to WASM-based GBA emulators like RetroArch Web and modern browsers having native gamepad support). Besides that, there's just something comforting with having a single, self-contained executable that I know I can in all likelihood run one, ten, twenty years from now – which is probably not true for many iOS or even web indie games I otherwise really like. reply rkagerer 18 hours agoparentI would love a modern Palm OS phone, that's true to the original UI philosophies and could run (even if just through emulation) all my old titles. reply kalleboo 12 hours agorootparentThere is a guy who has gotten Palm OS running on modern ARM hardware http://dmitry.gr/?r=05.Projects&proj=27.%20rePalm reply unwind 12 hours agorootparentYou mean \"a cluster of highly trained elite software ninjas masking as one guy in public\". Eh. No, just kidding, of course, it's just that project^W site has a huge Fabrice Bellard-factor to it. Very, very impressive. reply lxgr 18 hours agorootparentprevTake all of this with a big grain of salt due to rosy retrospection, but I feel like a big appeal of Palm OS was that going online was an intentional activity (if possible at all; most of my handhelds had neither mobile data nor Wi-Fi). As a result, it was completely distraction-free: I'd queue up news/articles (via Plucker), mail, and books for the day, HotSync in the morning/evening, and then that was it – no chance of any notification (other than pre-programmed local reminders/appointments) popping up and disrupting whatever I was doing. Other than that, there was still more than enough to do ~forever: More Ebooks on a 64 MB MMC than I could reasonably read all summer, the top 100? 1000? Wikipedia articles, the CIA World Factbook as a PalmDoc, Space Trader... Ok, enough with the nostalgia :) (If this brought back a fond memory or two, head on over to https://cloudpilot-emu.github.io/ right now!) reply tsm 8 hours agorootparentI really like the idea of getting my day's worth of emails in the morning and responding to them throughout the day while offline (using one of the great folding keyboards!). Hard to emulate that these days though. It feels artificial to put my phone in airplane mode or whatever. reply ASalazarMX 22 hours agoprevWondering if I could run this on real hardware, I realized I have no idea what happened to my Palm LifeDrive when I changed to Blackberries. I miss that little chunky PDA. It was amazing for its time. https://en.wikipedia.org/wiki/LifeDrive reply sodaplayer 22 hours agoparentI just came across my old Palm Tungsten E2 last week while doing some cleaning. If I can also find its charger, I'll report back on running it. reply Zobat 12 hours agorootparentI've come across my box of Palms a few times and had limited success getting them going. Seems they don't like being in storage for almost two decades. Really loved my Palms back then. reply memsom 4 hours agorootparentI have both a PalmPilot Professional (with serial) and a Palm m500 (with USB), the latter is a lot easier to pocket, but the USB support is horrible in modern OS. I keep wanting to get back in to Palm dev. I even managed to get Pila to compile under macOS Catalina a few years ago. I also have a stack of V-Tech Helio's, which is a PalmPilot clone. I don't remember why I ended up with 3 of them, but I got them to reflash the OS and never got around to it. Their whole OS source code is available online. The compiler is not easy to make to work these days though. DJGPP and I think 16bit. reply lxgr 18 hours agoparentprevThe LifeDrive was such a weird Palm! On one hand I hated it, for its infuriating loading times compared to its predecessors (due to the hard drive spinning up for ~every unexpected memory access of an OS designed for having everything in RAM/ROM and literally no concept of file/block based memory at first) and its relative bulk. On the other hand: Four! Gigabytes! That could hold more than two full movies! It was also my first Palm having Wi-Fi, which was nice. All in all, to me it was a symbol of Palm quickly losing touch with modern developments: For example Symbian was miles ahead from an OS point of view, even though usability was nothing compared to Palm OS, and then there was the iPhone and Android, of course. reply kstrauser 21 hours agoprevThis is beautiful! And now I'm nostalgic for my IIIxe. Through the rose colored glasses of poor remembrance, that might've been peek productivity in a handheld. It had enough functionality to remember all the things I cared to have on my person at all times, but was utterly lacking in notifications about distractions. It took me a while to quiet my iPhone so that it's not always pestering the hell out of me, but Palm was opt-in. If you didn't tell it to tell you about something, it kept its mouth shut. I wouldn't actually go back if I could, but part of me misses that. reply freedomben 21 hours agoparentI feel the same. I loved my IIIxe. I'm guessing we'd be horrified at the UX now if we could go back, but at the time it was a huge boost in productivity. The handwriting language was really great. I actually wrote papers on that thing! It was great, I could work on papers while on the bus or travelling, without having to lug around a laptop. Remarkable devices. reply kstrauser 20 hours agorootparentI played with an emulator (https://cloudpilot-emu.github.io/) recently and it's honestly not that bad at all. The resolution is bad by today's standards but the basics are all there. It even has a system-wide search that looks for the input string in all your apps and lets you tap right into those records. That's pretty handy! I like my phone too much to go back, but if I had to, I could make do. reply i80and 20 hours agorootparentprevSo I actually bought a Palm Classic device to test the hypothesis that my memories are nostelgia. They're not! It's actually a great UX! reply Telemakhos 4 hours agoparentprevControl over notifications is one of the most powerful UX features available. I make extensive use of geo- and time-based focus modes for the few notifications that I ever allow. reply cebu_blue 22 hours agoprevI love this and love the art design especially. Great job! The only thing I would change persoanlly is that i think it feels more natural to go in thge opposite direction with the mouse when you're aiming. Many mini golf games on Miniclip used to do it that way. Also if you're a fan of FOSS games i recommend Neverball and Neverput which is a 3d golf similator with nice graphics reply aliher1911 21 hours agoparentOriginal Palm was using stylus so you don't obscure where you are aiming compared to finger touch phones and having more space in the direction of shot could be the factors. reply urbandw311er 19 hours agoprevI love it! Quick suggestion: allow a moment to show the ball dropping into the hole before loading the next level. It might be frustrating denying the player that satisfaction. reply zzanz 22 hours agoprevInteresting project. I would say the issues had, memory leaks. debugging, etc are a lot more common in game dev than you might expect. Much of these problems have been abstracted away by game engines such as Unreal/Unity/Godot, but if you were to go into game dev with C, OpenGL, and a memory restriction (especially when hardware enforced), you might run into the same teething issues. The level editor is a nice touch, I would be curious on the implementation as something in the same vein existed for the Tony Hawk series of games and was responsible for \"Tony Hawks Pro StrCpy\" https://icode4.coffee/?p=954 . Though jailbreaking and arbitrary code execution is probably a lot easier achieved via PalmOS than a minigolf side project. reply geon 7 hours agoprevYou should add a short celebration before loading the next level. The \"Loading\" text feels very abrupt, like you did something wrong. Display something like the text \"Nice!\", and try to avoid covering the hole, so that the player can see the ball disappearing into the hole. Wait just half a second before you show the loading dialog. You can probably skip the loading dialog entirely if it is as short as it looks like in the video. reply gwbas1c 21 hours agoprevThe game is really fun when played in the browser-based emulator. I nearly got sucked into practice mode. IMO: I'd love a port (or otherwise inspired by) version for Android / iOS / in-browser. It's really fun. reply debo_ 16 hours agoprevI appreciate that you named the source directory `sauce` instead. It's the little things that matter. reply sunnybeetroot 10 hours agoparentI think my OCD would prevent me from ever doing this :’( reply capitain 11 hours agoparentprevIt should be the industry standard imo reply msielski 5 hours agoprevAhh good memories. I wrote Pente for PalmOS back in 2004 or 2005, complete with a computer opponent using a minimax tree with alpha/beta pruning. Unfortunately I never finished or released it. It was so much fun to do early mobile development - it compared to nothing else I'd done at the time. And it wasn't easy. This is definitely an accomplishment. reply HumblyTossed 5 hours agoprevThis is really cool and brings back lots of memories. I've a curious question for the game devs among you. One of the things that makes put put fun is the possibility of a hole in one. How do you make sure this is actually possible in a game like this? reply harikb 20 hours agoprevI have the Original PalmPilot, hardware upgraded to 3.0, as part of a beta program. I also have a few Palm 3's. Is there a compiled PDB available? reply classichasclass 18 hours agoparentA Pilot 1000/5000 or the immediate model after? Also, I think you meant a .prc. reply harikb 33 minutes agorootparentIt was called Palm Pilot - the one pictured as the very first image in Wikipedia [1]. It still works, except for some bleeding/damage on a portion of the e-Ink screen. I probably need to find the appropriate Linux based tools to load the .PRC. I wonder if this [2] still works [1] https://en.wikipedia.org/wiki/PalmPilot [2] https://shallowsky.com/linux/palmlinuxdev.html reply lxgr 5 hours agorootparentprevAs far as I remember, the two extensions are mostly interchangeable, and are only used as a convention to indicate whether a given file contains an executable application or data. Palm OS doesn't really have a concept of files; everything's a record-oriented database, containing either resources, including executable code, or data. Palm Desktop would just queue up every .prc or .pdb for copying during the next HotSync. (I've never tested it, but presumably renaming a .prc to .pdb and installing it would lead to it being backed up as a .prc.) For some reason, Mobipocket (which was originally a Palm OS based ebook format) has landed on using .prc over .pdb for their books, which are definitely not executable, so every once in a while I stumble over a book with a .prc extension and it annoys me ever so slightly – it'll work, but that should be a .pdb :) reply memsom 3 hours agorootparentIIRC PalmOS prc and pdb are similar, in that they both contain PalmOS data, but pdb was meant to just for data and prc was meant to include executable resources too. PalmOS borrows a massive amount from 68K Mac. Codewarrior used to be the default compiler, and PalmOS uses resources in a similar way - though the actual file format is different IIRC. PalmOS apps are basically modified 68K Mac code (library?) resources. There was a tool that converted ThinkPascal generated binaries of whatever the type I don;t remember was, to prc files. SARC was what it was called. reply teruakohatu 21 hours agoprevI have been using CloudPilot [1] to play a bunch of retro Palm games on iOS. It works reasonably well. Hopefully a native emulator will be released now allows emulators. [1] https://cloudpilot-emu.github.io/app-preview/#/tab/sessions reply ceedan 4 hours agoprevHaving up and down slopes on the different holes would be an interesting and challenging addition. reply supportengineer 21 hours agoprevGreat job! Just from the video I could tell this is fun. This is what games are supposed to be. reply wingerlang 10 hours agoprevOnce I got a Zire 31 with PalmOS for Christmas. It became quite a fun treasure hunt to find free games and software for it. And I say treasure hunt because it was like looking for gems among piles of garbage as far as I can remember. reply dopp0 6 hours agoprevThanks for the memories. Good old times I was programming in SuperWaba for palmOS! reply mjcohen 17 hours agoprevFor the Amiga, one of my favorite games was \"Hole-in-one Miniature Golf.\" reply post_break 20 hours agoprevI miss bike or die. For a while it was on iOS but didn’t get updates. I had hours in that game. reply ricardobayes 12 hours agoprevAmazing game, gives the same level of excitement (and frustration) as playing real mini golf. Thank you! reply pantulis 10 hours agoprevThis made me remember the Zany Golf bouncy burgers! The hours I spent there! reply snozolli 19 hours agoprevI wrote code for PalmOS back around '99. One thing that stands out in my memory is the way that applications were tested. I think it was a feature of the emulator, which would fire events at your software. I forget the details, but if you could make, say, 1,000 events without crashing, it was passable, 10k was good, and 100k was excellent. Well, I thought I was a reasonable competent C++ programmer and I was shocked at how quickly my application would crash using this tool. It was an extremely humbling experience that really opened my eyes. I often think of how effective that simple tool was at revealing bugs, but it's obviously not something that works in today's multitasking, Internet-connected devices. The other thing I remember is CodeWarrior being the first IDE I used that had a drop-down box with all the functions in the current source file. That was a pretty big step forward in productivity. Incidentally, I was still using a Palm Tungsten as late as 2010, when I was in Japan. There was a very simple Japanese dictionary application for Palm. Once you learn the basic rules for stroke order and direction, you could mimic any character you see using the stylus and do a dictionary search for matching Kanji. I was able to figure out a lot of navigation just by mimicking unknown Kanji that I saw on signs. reply JTyQZSnP3cQGa8B 7 hours agoparent> drop-down box with all the functions I remember the “#pragma -“ to separate the functions. We don’t have this anymore. As an alternative we have “#pragma region” to fold blocks of code but it’s different. reply sgt 11 hours agoparentprevI just had a vision of someone trying to port Rust to PalmOS. Let's hope that never happens. reply xnx 22 hours agoprevYou need to promote this to projectionists so they can play it on their IMAX systems. reply 71bw 5 hours agoparentCare to explain? I'm obviously out of some loop here. reply xnx 4 hours agorootparent\"IMAX emulates PalmPilot software to power Oppenheimer’s 70 mm release\" https://news.ycombinator.com/item?id=36817900 reply xrd 21 hours agoprevThe post suggests getting a real device. Where the heck do you get a real device these days? Is there some retro hardware out there? An open source project where you send out a PCB order to Shenzhen? reply eichin 21 hours agoparentebay; the hardware has lasted surprisingly well. (That said, chrome on a recent samsung phone runs CloudPilot very responsively, start here https://archive.org/details/softwarelibrary_palm and you can just click on things and run them...) reply flymasterv 21 hours agoparentpreveBay has new-in-box Zires for $10. reply Yhippa 15 hours agoprevThis made me so happy. I'm going to go look for my Handspring Visor now... reply DevScout 16 hours agoprevGreat work! It's incredible to see projects like this still going in 2024. reply nanna 21 hours agoprevIs there an app store for palm is, or somewhere you can browse different apps at least? reply eichin 20 hours agoparenthttps://archive.org/details/softwarelibrary_palm is a good place to start (and you can just run them in-browser via cloudpilot.) reply nanna 9 hours agorootparentVery cool ! reply lycos 7 hours agoparentprevBack in the day I think most people got their apps from the PalmGear website (https://web.archive.org/web/20010331040533/http://palmgear.c...) and sync it to their Palm device via HotSync (later Palm Desktop) reply dmitrygr 21 hours agoparentprevPalmDB reply yding 15 hours agoprevAs someone who interned at Palm, love this so much! reply capitain 12 hours agoparentOh wow, do you have any interesting stories from back then? Saw some unreleased prototypes? reply xer0x 17 hours agoprevWow, I can't believe this! That looks awesome! reply andrewshadura 21 hours agoprevIt would be really cool if someone wrote something like WINE to allow running PalmOS applications on Android. reply AshamedCaptain 20 hours agoparenthttps://styletap.com/ reply classichasclass 15 hours agorootparentUnfortunately, apparently not compatible with Android 14 (there's a warning about this on the website). On my Pixel 7 Pro, it says \"not compatible with your phone.\" reply 71bw 5 hours agorootparentCan be bypassed with ADB... adb install --bypass-low-target-sdk-block apk.apk ...albeit it still does not resolve the issue of whether or not the app will run on 14/15. reply dmitrygr 21 hours agoparentprevIt would have similar problems to WINE, without nearly as many developers who understand the bug compatibility required. One project (pumpkinos) tried but there are many apps that don’t work in it. Emulation is easier. And accurate. Source: I believe at this point I am the highest authority left on PalmOS reply blackeyeblitzar 21 hours agoprevI am not very familiar with Palm OS, but I do like the idea of a smart device that isn’t a full on smartphone. Something like the old Palm devices or the Pocket PCs from 25 years ago. Is there such a thing today? What gets closest? reply two_cents 20 hours agoparentBOOX Palma maybe? https://shop.boox.com/products/palma reply tomcam 20 hours agoprev [–] Let me hijack this with a hate story about my wife and minigolf games. Back in 1996 she thought maybe she needed a hobby. She was a housewife and we didn’t have any children yet. She said maybe she could do a game. At the time I was working in Microsoft C++ and the Microsoft Foundation Classes framework, building Windows apps. She had never programmed a language more challenging than Turbo Pascal. When she asked how long it would take to even get started, I estimated about 18 months to learn C++ properly, another six months to learn Microsoft foundation classes. The six months after that to learn the windows API if she worked really hard at it. At the time I thought maybe there was a place in the Windows game market for a mini golf game. She took less than a week to create a working, bug-free prototype, but then lost interest. I hate people like that! Everything takes forever for me to learn. Luckily she blessed me with a passel of pretty much bug-free and absolutely hilarious children, so I’m giving her a pass. reply darby_nine 16 hours agoparent> She took less than a week to create a working, bug-free prototype, but then lost interest. This is pretty common, no? Same reason why everyone thinks AI will change development: people mistake the initial time cost of building an initial prototype for being representative of the total cost of making the thing people actually demand (which mostly never happens). reply al_borland 17 hours agoparentprevIs this the difference between learning and doing? You were looking at how long it would take to learn all the foundational technologies, once that was done, she could start to code the game. Where she took the on-the-job-training approach. Work on the project, run into a problem, learn enough to solve said problem. Repeat until done. I’m usually the learn before I start type as well, though I find it doesn’t really work for me in practice. It ends up becoming a form of procrastination. reply tomcam 15 hours agorootparentShe says much the same thing as you. I appear to be unable to learn something without going to first principles. reply ok_dad 20 hours agoparentprev> bug-free children I know you didn't mean anything by it, and it was a pretty funny joke, but children with bugs are pretty cool as well :) reply tomcam 19 hours agorootparentTwo of ours were born with severe handicaps but they are all insanely fun and fabulously good company. Not sure why my sibling comment was flagged. reply HFguy 19 hours agorootparentLove that!!! reply tomcam 14 hours agorootparentMy parents made a big deal about how much of a responsibility children were. I never dreamed my kids would be so damn much fun. reply tomcam 15 hours agorootparentprevExcellent username btw reply grimgrin 18 hours agorootparentprevWe call them features reply tomcam 16 hours agorootparentCouldn’t agree more reply tomcam 19 hours agorootparentprevSO SO TRUE reply smabie 20 hours agoparentprev [–] If you're laser focused on a end product then learning can happen remarkably quickly. reply tomcam 19 hours agorootparent [–] Yeah no. I am an atrociously slow learner. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A new MiniGolf game, \"Captain's MiniGolf (v0.6),\" has been released for Palm OS in 2024, following a retro coding journey.",
      "The game features the ability to create and share custom levelpack databases and can be played via a browser using the cloudpilot emulator.",
      "The full source code is available under the GPL3 license, aiming to inspire more Palm OS game development despite challenges like memory leaks and debugging difficulties."
    ],
    "commentSummary": [
      "A MiniGolf game for Palm OS has garnered significant interest, with 309 points and 93 comments on a tech forum.",
      "The project has sparked nostalgia and discussions about retro gaming, with users reminiscing about old Palm devices and games.",
      "The developer is compiling old sources and creating disk images for emulators, highlighting a trend of reviving and preserving vintage software."
    ],
    "points": 309,
    "commentCount": 93,
    "retryCount": 0,
    "time": 1726085420
  },
  {
    "id": 41515527,
    "title": "Vulnerabilities in the Feeld dating app",
    "originLink": "https://fortbridge.co.uk/research/feeld-dating-app-nudes-data-publicly-available/",
    "originBody": "Feeld dating app – Your nudes and data were publicly available Posted on 5 September 2024 (10 September 2024) by Bogdan Tiron This post highlights the critical importance of implementing robust security controls on the back-end of mobile applications. Specifically, our target was Feeld, a dating mobile app. It is just like Tinder, Bumble and others, but on steroids, meaning you can filter by distance, by age, by gender (>10), couples, and by location. For premium users, you can also search by the type of kink, threeways/group scenarios, or the type of relationship you are interested in. We reviewed the security controls implemented and discovered the following vulnerabilities. In particular, except for the first one, all others fall under the category of: ‘Broken access control‘ according to OWASP Top 10. Disclosure of profile information to non-premium users Read other people’s messages Unauthenticated access to other people’s attachments (photos & videos) from their chats Delete, recover and edit other people’s messages Update someone else’s profile information Get a ‘Like’ from any user profile Send messages in other people’s chat View other people’s matches To begin with, let’s first examine how the application looks and its main menus before analyzing the vulnerabilities one by one. ‘Discover Profiles’ Menu ‘Who liked you’ Menu ‘Messages’ Menu ‘Profile’ Menu Table of contents Feeld Vulnerabilities 1. Disclosure of profile information to non-premium users 2. Read other people’s messages 3. Unauthenticated access to other people’s attachments (photos & videos) from their chats 4. Delete, recover and edit other people’s messages 5. Update someone else’s profile information 6. Get a ‘Like’ from any user profile 7. Send messages in other people’s chat 8. View other people’s matches See Our Leading Research Insights Conclusion Feeld Disclosure Timeline Feeld Vulnerabilities 1. Disclosure of profile information to non-premium users Details: The ‘basic’ user will no longer need to pay for a ‘premium’ subscription to get a premium benefit. When you, as basic user, go to the ‘Likes’ menu in the mobile app, to see who liked your profile, you only get limited information, such as the name and the blurred photos of the ‘like’ sender, compared to the premium user who gets all the information available about the sender. However, if you use a proxy tool such as Burp to intercept the request and response, you will find in the response all the information available about the ‘like’ senders, just like a premium user. Reproduction steps: 1.As basic user, we can go to the ‘Likes’ menu to see who liked or pinged us, as seen below. But beside their names and their blurred photos, we do not have any other information. ‘Who liked us’ Menu showing minimal info to non-premium users about people who liked our profile, such as Sam 2. However, if we intercept the request in Burp and check the response, as seen below, we will see that we have all the information about the user (age, distance, all their profile photos, streamUserId), including unauthenticated access to their profile photos stored on res.cloudinary.com. In addition, using the ‘streamUserId’ value found in the response we can exploit the next vulnerability ‘Read other people’s messages’ and read Sam’s messages. Response containing all the details about the users who liked us, in this case, Sam Checking their profile photos unauthenticated 2. Read other people’s messages Details: We can read other people’s messages in the chat. In order to do that, we will need to get our victim’s ‘streamUserId’ value, which is disclosed in different API requests. Reproduction steps: Go to the ‘Discover profiles’ menu. Intercept the /graphql request with operationName: ‘DiscoverProfiles’. Get a ‘streamUserId parameter value of the target user from the response, as seen below: ‘StreamUserId’ is: 64ccd281fbaa820001005b4f , and belongs to user ‘Chlo’. 3. Now go to the ‘Message’ menu, and intercept the request to the endpoint: https://chat.stream-io-api.com/channels?user_id=&connection_id=&api_key=y4tp4akjeb49 ,such as the one below: 4. Remove all the request parameters except ‘member’:{‘in’:[“”] , and add the victim’s ‘streamUserId’ as, as seen below: 5.If we search in the response by “text” we can see the total number of messages to and from our victim ‘Chloe: 3. Unauthenticated access to other people’s attachments (photos & videos) from their chats Details: We can build upon the previous vulnerability ‘Read other people’s messages’ and we can access the attachments (images and videos) that have been shared between users. There are 2 types of attachments that can be shared in a chat: photos and videos. The ‘Photo’ option can be either replay-able or time-limited to between 5 to 15 seconds after which it is not available to the receiver user anymore. And the ‘Video’ option can be either ‘replay’-able or ‘play-once’ only. The workflow for uploading and getting other people’s replay-able photos is the following: Requirements for the attacker: From the ‘Discover Profiles’ menu, obtain the ‘stream_id‘ from the target victim. This ‘stream_id’ is required to access their messages, which contain the uploaded ‘photo_id’s of both the victim and their counterpart. The user uploads through Feeld app a replay-able photo on api.cloudinary.com . The response from api.cloudinary.com returns a ‘photo_id’ . The photo gets copied to feeld.co and becomes available to authenticated users to the following endpoints: feeld.co/cdn/chat-attachment/// After some testing the above endpoint can be reduced to the following: feeld.co/cdn/chat-attachment/x/ //Instead of ‘x’ any combination of minimum 1 character is permissible and the photo is returned to any feeld authenticated user. After some more testing, we discovered the above endpoint but prepended with /v1/ : feeld.co/v1/cdn/chat-attachment/x/ //This returns the url to the photo stored on api.cloudinary.com that is accessible unauthenticated The workflow for uploading and getting other people’s time-limited photos is the following: Requirements for the attacker: From the ‘Discover Profiles’ menu, get from the target victim the ‘profileId‘ and the ‘stream_id‘ required to access their messages and their time-limited photos. To get the time limited photos of the victim’s conterpart in a chat, you will have to repeat this step but search instead for the victim’s counterpart ‘profileId’ and ‘stream_id’ as it is not possible to leak these from their chat messages. The user uploads through Feeld app a 5-to-15 seconds time-limited photo on api.cloudinary.com . The response from api.cloudinary.com returns a ‘photo_id’ . The photo gets copied to feeld.co and becomes available to authenticated users to the following endpoints: feeld.co/cdn/chat-attachment// //Once the receiver opened it or this endpoint is acessed, after 5-15seconds the photo gets deleted and is not accessible to this endpoint anymore. feeld.co/cdn/chat-attachment// //The time limited photo will always be accessible from this endpoint, even after 5-15 seconds, to all authenticated users. Theis mandatory in this case, compared to the case on the left. After some more testing, we discovered the above endpoint but prepended with /v1/ : feeld.co/v1/cdn/chat-attachment// //This returns the url to the photo stored on api.cloudinary.com that is accessible unauthenticated To summarize, as an attacker, we can access all of the following unauthenticated: Photos replay-able Photos time-limited Videos replay-able Videos play-once Reproduction steps: Instance 1: Uploading replay-able photos 1.Let’s upload in our chat, a normal replay-able photo. So, the first request is ‘Generate Upload Credentials’ for uploading on ‘api.cloudinary.com’: 2. Then we send a photo upload request to api.cloudinary.com using the above generated ‘publicId’ and ‘signature’ values, plus an api_key and timestamp parameters: 3. Next request done is: ‘UploadChatAttachment’ which gets the above unique public_id of the image from api.cloudinary.com and is passed to core.api.feeld.co, as seen below. I suspect this request is to copy the photo from api.cloudinary.com to core.api.feeld.co. 4. A unique ‘attachmentID’ parameter will be returned above in the response. This ‘attachmentID’ will be used and passed in the chat, as seen below: 4. Now to get the photo authenticated, as any other user, we make the following request, using the above attachmentID: https://core.api.feeld.co/cdn/chat-attachment/x/c07c3360-c787-4be9-9cd6-b1ef9d06fff4 Note: In the above request path, initially instead of ‘x’ it was the ‘ProfileId’ guid value of the sender or receiver of the photo, but deleting it works fine, so I just left an ‘x’ for an easier read. 5.Now, to get the same photo but from cloudinary.com, unauthenticated, prepend /v1/ to the above request, as seen below, and you will get the ‘url’ pointing to the original photo: https://res.cloudinary.com/threender/image/upload/s–QQjZiJxc–/d4e74e59-430d-403f-b1c5-9c8208472007 The photo (with the view from our office), that can be reached unauthenticated. Instance 2: Uploading time-limited photos Note: There are 2 main differences from the above process. Firstly, when uploading ‘time-limited’ photos, we pass an extra parameter ‘visibilityMilliseconds:15000’. Secondly, for accessing the photo, we use the ‘profileId’ GUID value of the victim that uploaded the photo, rather than the ‘x’ value used in the above path. If their chat counterpart uploaded the ‘time-limited’ photo, we need to return to the ‘Discover profiles’ menu to locate theirGUID value, which is mandatory for accessing these photos. 1. A request will be made to api.cloudinary.com to upload the photo: 2. Copying the photo from cloudinary.com to core.api.feeld.co: 3.Then, if we read again the chat using the previous vulnerability ‘Read other people’s messages’, we can find the attachmentId to use in order to get the photo: 4.To retrieve the image, we will need the ‘profileId’ guid of our victim that uploaded the photo, which we already have from the ‘Discover Profile’ menu when we have chosen this target victim. Thus, the 2 urls to get the photo authenticated are: https://core.api.feeld.co/cdn/chat-attachment//971a0d2f-f50c-45fc-8a37-4d9002f71e49 . However, 5-15 seconds after accessing this endpoint, the photo at this endpoint will be deleted . https://core.api.feeld.co/cdn/chat-attachment//971a0d2f-f50c-45fc-8a37-4d9002f71e49 . This will always return the photo to authenticated users. 5. We can use the following endpoint: https://core.api.feeld.co/v1/cdn/chat-attachment//971a0d2f-f50c-45fc-8a37-4d9002f71e49 , which will return a url with the photo stored on res.cloudinary.com . 5. The returned url for unauthenticated access is: https://res.cloudinary.com/threender/image/upload/s–7tD2qatw–/540068ee-b41a-431f-b0e9-b7522fefbd5a . The only thing random in the above url, in case you want to brute-force it, are the 8 characters ‘7tD2qatw‘. Once the 15 seconds passed and the receiver wants to re-access his time limited photo, he will get the following response: ‘Attachment has expired’. Below is the difference in behaviour and what the sender of a timed-photo will see on his Android and iOS phone once the 5-15seconds passed. The view in the Android app after the sender accessed his time-limited photo: ‘Photo expired’, and is not shown anymore in the app. The view in the iOS app after the sender accessed his time-limited photo at the top: ‘Tap to view’, and is still shown in the app. Instance 3: Uploading normal video 1.Pick a chat, select ‘upload video’ option, record a video and submit it in the chat. The below requests will be made. The video will be uploaded to us-east.stream-io-cdn.com: 2.The url from the response will be passed in the chat, as seen below: 3.If we again read this chat as the attacker, using the previous vulnerability ‘Read other people’s messages’, we can see the url to the video, as seen below: 4.The url can be seen in the response. We have to replace ‘\\u0026’ for ‘&’ in it. Now we can go to the above url unauthenticated. Instance 4: Uploading ‘play-once’ videos 1.Upload a video, as in instance 3, but set it to ‘play once’. The following requests will be made. The video will be uploaded to: chat.stream-io-api.com as seen below: 2.The returned url will also be sent in the chat in a subsequent request, as seen below: 3.Now, an attacker can read our chat using the previous vulnerability ‘Read other people’s chat’ and extract this url, as seen below. 4.The url can be extracted and \\u0026 character replaced with &. Thus, we can watch the video unauthenticated: The video is available unauthenticated and is replay-able. The receiver of the ‘play-once’ video, will have no knowledge of the attack. He can still see the video, but only once. After he sees the video, it will say ‘video expired’. ‘Before’ and ‘after’ the receiver sees the video once. 4. Delete, recover and edit other people’s messages Details: We discovered that we can recover other people’s messages that were deleted in a chat. In addition, we can edit and delete other people’s messages. In order to do that, we will need the unique ‘messageId’ value of the message that we want to recover. This is easy to get because when we read our victim’s messages, each message has its messageId next to it. Instance: https://chat.stream-io-api.com/messages/f962bef0-6e9b-4f17-9def-6238a7933abf (Methods: DELETE and PUT) Reproduction steps: 1.Use a proxy tool such as Burp to intercept the traffic. 2.Enter a chat and leave a message to someone: 3.Delete the message: 4.Now let’s read the chat as the attacker user using the above vulnerability ‘Read other people’s messages’. It will say, ‘This message was deleted’, as seen below: 5.Now if we call the same DELETE request, as the attacker, we will get back the original message: Instance 2: Edit a message, as a different user than the participants in the chat 1.Let’s send a message and intercept the request: 2.And let’s use the previous vulnerability to ‘Read other people’s messages’ as the attacker, in order to find the messageID (‘0fec78e9-0068-48f1-8563-7144474cc7e2’): 3.The victim will receive a notification: 4.Edit the message as the attacker, using the messageID and the method PUT on the same endpoint: 5.When the victim taps on the notification from the above step 3, he will see the following message set by the attacker in step 4. There will be an ‘edited’ sign below the actual message but there are no signs of who did the edit. In addition, every account name is not unique and the attacker could choose any name possible. Instance 3: Delete other people’s messages 1.This is the exact same endpoint as in Instance 1. 2.We will delete the above message as the attacker, who is not a participant in the chat, as seen below, using the unique messageID and the method DELETE: 5. Update someone else’s profile information Details: We discovered you can update someone else’s profile information, including name, sexuality, age, etc. Instance: https://core.api.feeld.co/graphql (“operationName”:”ProfileUpdate”, vulnerable parameter ‘id’) Reproduction steps: 1.Let’s login the mobile application as the ‘attacker’ and go to the ‘Profile’ – ‘Edit Profile’ menu. 2.Edit 1 thing on the profile such as ‘bio’, save the change, and intercept the /graphql request with operationName: ‘ProfileUpdate’. 3.Modify in the intercepted request the ‘id’ parameter and add the id of your victim. In addition, add the parameters that you want to update, such as ‘bio’. 6. Get a ‘Like’ from any user profile Details: We discovered that users could send ‘Likes’ from profile#2 to profile#3 while logged in as profile#1. Instance: https://core.api.feeld.co/graphql (OperationName: ProfileLike) Reproduction steps: 1.Below is a request to send a normal ‘Like’, from user with profileId ending in ‘…9c’ to profileId ‘…1f’, and the successful response: 2.Below is the request and response with a reverse like, from ‘…1f’ to ‘…9c’, which errors: 3.Now, send a like from a random profile ‘….d3’ to one of our profiles ‘…1f’ , while logged in as user ‘…9c’: 4.Now get the profile details (ImaginaryName) of that user with profileId ‘…d3’: 5. Now, let’s check our list of likes in the app to see if we received a like from user ‘Anni’. Given that we have a Premium account, we can view this information in the app. Indeed, we can see that we have received a ‘Like’ from ‘Anni’: 7. Send messages in other people’s chat Details: We discovered that we can send messages to other people’s chats, even though we are not a participant in that chat. Instance: https://chat.stream-io-api.com/channels/messaging/50dd83b1-9dda-4940-b6bb-04891e9500bd/message (Method: POST) Reproduction steps: 1.Use the previous vulnerability ‘Read other people’s messages’ to find the unique channelID where you want to add your message, such as the the one shown below: ’50dd83b1-9dda-4940-b6bb-04891e9500bd’: Add this channelID to the request path when you exploit this issue in step2 . 2.Send a message to that channel id: 3.The victim will receive a notification, as seen below: Tap the notification and you will see the message. The victim cannot verify whether this message comes from the partner they matched with, or from a 3rd party, an attacker, like in this case. The chat displayed on the right, is between 2 users: ‘D’ and ‘Bogdan’. Although, the system shows the notification is coming from user ‘B’ (the attacker’s name), the attacker can change their name on their profile, as this field is editable and not unique. 8. View other people’s matches Details: We can check who did other people match with and their full profile information, such as ‘imaginaryName, age, photos, gender, sexuality, status, data of birth. Instance: https://core.api.feeld.co/graphql (“operationName”:”ChatListQuery”, vulnerable parameter: ‘profileId’) Reproduction steps: 1. Enter the mobile application and go to the ‘Discover profiles’ menu. 2. It will make a request to /graphql with the “operationName”: “ChatListQuery”, as seen below: 3. Change the profileId to that belonging to a victim user, such as: 00ab5791-e42e-58e2-ab51-e30a453d791f. Thus, we can view that account’s matches, as seen below: See Our Leading Research Insights For those interested in exploring more security research similar to our study on mobile app vulnerabilities, consider these insightful articles: Firstly, for API testing research, check Mass Account Takeover in Yunmai Smart Scale API: This article details a pentest of Yunmai’s Android and iOS smart scale API, revealing several issues, including a chained attack leading to mass account takeover. Secondly, for web app pentest research and a peek into PHP internals, check Multiple Concrete CMS Vulnerabilities (Part 1 – RCE): This article investigates achieving remote code execution through 2 race conditions vulnerabilities in the file upload functionality in Concrete CMS, providing a detailed examination of potential security risks and mitigation strategies. Additionally, for our open source contribution to security tools, check Phishing Like a Pro: A Guide for Pentesters to Add SPF, DMARC, DKIM, and MX Records to Evilginx: This guide delves into advanced phishing techniques and how to effectively use SPF, DMARC, DKIM, and MX records with Evilginx for penetration testing. Moreover, explore these resources to deepen your understanding of security testing and stay updated on best practices in the field. Conclusion Firstly, as we continually advance our tools and techniques for effective mobile app security testing, we invite you to explore our mobile application pentesting services. Indeed, at FORTBRIDGE, we take pride in being a leading pentesting provider with a team of senior consultants who have a proven track record in the industry. Moreover, our experts use advanced methodologies and tools to deliver comprehensive, actionable insights tailored to your specific needs. Specifically, we have the Dubai Electronic Security Center (DESC) and the Council of Registered Ethical Security Testers (CREST) accredit our services, and we design them to fortify your mobile applications and ensure the highest quality of security testing. Furthermore, dive into the latest and most effective security strategies with our expertly crafted solutions. To that end, for detailed information about our offerings or to discuss customized penetration testing strategies, please visit our services page or contact us today. In conclusion, with FORTBRIDGE, you can be confident in our commitment to safeguarding your organization’s systems and data, helping you stay ahead of evolving cyber threats. Feeld Disclosure Timeline 2024/03/08 – The disclosure of all the above issues to Feeld. 2024/03/08 – Feeld asked for the testing account details used during testing. 2024/04/02 – FORTBRIDGE – we asked for an update. 2024/04/02 – Feeld – ‘We are continuing to review the findings. Hence, if you can hold off publication … it would be helpful’ 2024/04/02 – FORTBRIDGE – ‘We’ll hold off publication’. 2024/05/28 – FORTBRIDGE – ‘Any update? It’s been almost 3 months’. 2024/05/28 – Feeld: ‘we deployed several fixes. Thus, we kindly ask that you delay your findings for a maximum of 2 weeks, allowing us to confirm that we have resolved the flags in your report and ensuring that the safety of our Members remains sound’. 2024/05/29 – FORTBRIDGE – ‘So, we agree to delay publishing for 2 more weeks’. 2024/06/08 – 3 months have passed since the initial disclosure email. 2024/06/19 – FORTBRIDGE – we asked for an update. 2024/06/20 – Feeld: ‘We appreciate your patience. Meanwhile, the team is cleaning up a few remaining items’. 2024/07/08 – 4 months have passed. 2024/07/08 – FORTBRIDGE: ‘Have you closed off all of the issues?’. 2024/07/15 – Feeld: ‘[…] a few issues still require a more complex set of remediations. […] we appreciate your allowing us time to fully resolve before publishing any of your findings’. 2024/08/04 – Feeld: ‘Our teams are actively working to resolve the remaining findings. Please hold off publishing until we can confirm that we have resolved these items.’ 2024/08/08 – 5 months have passed. 2024/08/08 – FORTBRIDGE – we asked for an update. 2024/08/16 – Feeld: ‘we have implemented the required changes to mitigate the remaining findings’. 2024/09/08 – 6 months have passed. 2024/09/10 – Blog published. About Post Author Bogdan Tiron Cloud Application Security Consultant/ Pentester OSCP/CRT/GCP Architecture/GCP Security/DevSecOps See author's posts Share this... Email Facebook Twitter Linkedin Whatsapp Telegram Pinterest Posted in Research",
    "commentLink": "https://news.ycombinator.com/item?id=41515527",
    "commentBody": "Vulnerabilities in the Feeld dating app (fortbridge.co.uk)269 points by notmine1337 21 hours agohidepastfavorite118 comments Cu3PO42 14 hours agoIt seems like they implemented permission checks purely in the frontend, and not just on one endpoint, but almost everywhere. While it is conceptually easy to avoid this, I have seen similar mistakes much more frequently than I would like to admit. Edit: the solution \"check all permissions on the backend\" reminds me of the solution to buffer overflows: \"just add bounds checks everywhere\". It's clear to the community at large what needs to be done, but getting everyone to apply this consistently is... not so easy. reply doix 12 hours agoparent> Edit: the solution \"check all permissions on the backend\" reminds me of the solution to buffer overflows: \"just add bounds checks everywhere\". It's clear to the community at large what needs to be done, but getting everyone to apply this consistently is... not so easy. I don't see those as the same. Buffer overflow checks are a very specific implementation (and language) detail and can happen absolutely anywhere in a codebase. Permission checks happen at a specific boundary and are related to how you design your application. Whenever I had any say on how a project was developed, I'd always insist on a clear separation between the development of the backend API and the frontend client code. In my experience, it makes things like this much easier to avoid (and test for). You also get a developer API for \"free\" (which to be honest, is the main reason I prefer to do it that way). reply dfedbeef 13 hours agoparentprevYou shouldn't be touching the server-side code if you find this hard to keep straight. reply benoau 12 hours agorootparentJunior developer probably opened a Jira ticket, saw a UI of a permission dialog, and did exactly that task with nobody senior enough to know better. That's how you reproduce the bugs that were in-fashion 15 - 20 years ago in my experience! reply tuyiown 10 hours agorootparentThere two new front end guys in the team, understaffed backend, but manageable, nothing as dramatic, but I constantly have to remind the benefits of doing as much things as possible in the backend and keep the logics high level on the front is aggravating. reply port19 10 hours agorootparentprevAlso no review or planning anywhere in that process. I'm semi-confident that if a Junior were to talk to another Junior before starting about things to look out for, and then the code was reviewed by say a third Junior, they would not have this bug. Call me naive, but I don't think Juniors are as oblivious as they are made out to be reply port19 10 hours agorootparentI should add this works best if you hire with some diversity, such as one Junior with a preference for security topics. If you go up to the counter and yell \"10 React devs please\", don't be surprised reply rfc2324 10 hours agorootparentAs a long time engineering manager, I have significantly benifited from this: leveling me up in WCAG/ADA, strcmp timing attacks, performance timing. Many managers start younger than we maybe should, and the burnout that I had in my early years pulled me out of my passion to learn more about comp sci. It was the random enthusiasm of younger folk that, in those times, was my exposure to topics I hadn't dived in to yet. I have witnessed hiring, listening, and supporting early-career enthusiasm has significantly improved every startup I've had the joy to be a part of. reply bugtodiffer 11 hours agorootparentprevSeems like a solid development cycle. Junior tries something -> hit production I do not see multiple issues with this. reply intelVISA 10 hours agorootparent9 out of 10 PMs love this one hack to boost velocity they were probably thinking what a 10x engineer they'd found to be so rapid at delivery... reply XenophileJKO 9 hours agorootparentThis is so true. I've seen this so many times. The darling of product, who can deliver so fast. They leave a trail of smoking rubble and half working features behind them. reply blitzar 9 hours agorootparentWhy can't you be more like darling of product over there. What do you even do around here; all you seem to ever do is take darling of products code and make a few changes (which I don't understand) and committing it as your own work. It appears you are either trying to take credit for darling of product or are sabotaging their amazing 10x work. reply Ntrails 11 hours agorootparentprevHey, it worked on my machine reply Cu3PO42 13 hours agorootparentprevUltimately, I don't disagree. However, I also try to make it a habit to not blame people for not knowing something. This presents as a structural problem in that company: they needed to hire people who do know how to secure server code and put them into a position to do so. Blame the company and those who decided to save every last penny in personnel cost. reply dfedbeef 2 minutes agorootparentu are wise and right reply overstay8930 10 hours agorootparentprev> I also try to make it a habit to not blame people for not knowing something There’s a point where critical thinking skills come into play, I’ve seen people walked off the premises for doing stuff like this with customer data. Actual seniors who have never been blamed for anything are suddenly intolerable threats to the company because they didn’t bother to check what they were doing and forced the company to disclose a breach. reply robbie-c 9 hours agorootparentprevSexual preferences and such are special category data, and if you are an engineer dealing with this stuff you should treat it as though data breaches could get someone killed. Sure, part of the responsibility of this is on management, but it's absolutely on the engineers too. reply namaria 11 hours agorootparentprevPeople getting paid to create software should know better then these basic mistakes. reply _pete_ 10 hours agorootparentthan reply valenterry 11 hours agorootparentprevYeah, the people who put those people into the position to touch server side code are to blame. But then the OP is right: the people having made these code changes should really not have touched anything server side or even anything security relevant in the beginning. reply consteval 4 hours agorootparentThey shouldn't have to - the architecture should be made in such a way that permission checks are done without you specifically having to call them every time. This is the entire reason middleware exists! reply valenterry 3 hours agorootparentWell, but apparently they let people create the architecture who just shouldn't have touched the backend code. That's the whole point. Since it was not just a single endpoint or so - it was everywhere! reply benoau 11 hours agorootparentprevI agree they should just quit but that requires experience to understand too. By the time they've learned that, they have also learned that client-side access controls are decorative. reply hackernewds 11 hours agorootparentprevright* reply jfoutz 12 hours agorootparentprevEternal September. Everyone starts somewhere, it’s just all the time now. In ten years, the dev will explain to a junior how bad they messed up, and why they have to validate this way. Well, I don’t know, but that’s what I hope. reply ifdefdebug 12 hours agorootparentyeah now imagine another engineer go \"my first bridge just fell apart the first time a real truck tried to cross over it lol\" or \"man my first plane crashed so hard\"... reply jfoutz 11 hours agorootparentYa know, the Roman tradition was, you gotta stand under the bridge while the army marches over it. If it collapses, you die too. Maybe there's something to having nudes of that dev. Real engineering is expensive. And hard. moving atoms around is tough. I've never cut stone, but I've melted and cast copper and aluminum. That's real and dangerous work. Computation is cheap and plentiful. And I kinda like having full control of \"stuff\". But maybe we do need licensing or personal liability. If I could wave a magic wand, and make that exist, I don't really know what rules I'd put in place. How do you think people should get skilled up? reply bugtodiffer 11 hours agorootparent> How do you think people should get skilled up? You didn't ask me but I can give you my answer: not on prod and with a lot of reviews! reply floating-io 9 hours agorootparentprev> Maybe there's something to having nudes of that dev. Most users of these sorts of app don't pay enough attention to security to care. Do you really think that most developers are any better? Most developers are just normal people who happen to be able to write a bit of code and convinced someone to employ them. Just like anyone else, far too many live under the delusion that \"it can't happen to me.\" Translation: making them eat their own dogfood and risk their own embarrassment won't help; they would have to know better, first! =) reply fragmede 11 hours agorootparentprevThat's an interesting idea. Bridge builders and flight sims are used in industry to test to see if a bridge design will fail or if a plane will crash. They're not limited to oversimplified and fun video games. I wonder if there's a market for a \"write a CRUD app and let it loose on the Internet and watch it get pwned\" simulator/game. reply pistoleer 10 hours agorootparentThat's hiring a pen tester, and there is a market for it, but companies don't do it as much as they should because it costs money while the app already \"works\" and brings in revenue. Of the 3 I've worked at, only one had yearly pen tests done. reply robertlagrant 10 hours agorootparentNo one hires someone to test what happens when a bridge is shot with a missile from 6000 miles away. The bridge \"works\" in the same way that the software \"works\". reply pistoleer 9 hours agorootparentA software penetration tester has the same techniques and suite of tools for pwning as \"the internet\". reply robertlagrant 7 hours agorootparentI don't see how that statement follows mine. Can you connect them at all? reply pistoleer 4 hours agorootparentI thought you were making the comparison that a pentester is like a missile shot at a bridge whereas the internet is the army walking over the bridge. reply poincaredisk 9 hours agorootparentprevI work in security and I don't trust myself to tie my shoes correctly every day. OPs comparison is great. Bounds checks are easy. There are many overconfident C++ programmers that say they would never introduce a vulnerability like that. But it still happens, because in this class if vulnerabilities it's often enough to forget one check. reply devnullbrain 13 hours agorootparentprevYet after decades of this messaging, we still have these people touching the server-side code. Is it likely another few years of the same messaging will fix it? reply FragrantRiver 12 hours agorootparentprevThey didn't ;) reply liampulles 10 hours agoparentprevThis can happen very easily I think if one uses \"automatic db APIs\" on the backend. I'm thinking of some automatic graphql setups for example. I flag it whenever I see it, but it is very worrying how little thought is sometimes put into the scope of client APIs. reply lynx23 10 hours agoparentprevI once caught a webdev doing frontend authentification with a plain javascript dialog. Yes, simple! Put the password in the JS, and do a simple comparison. Why did I notice? Because the owner of the lamp account contacted me that all their data was suddenly gone. Checked the logs, and yep, Google Bot clicked all the \"Delete\" links in their internal management view. Simply because JavaScript is opt-in :-) Called the developer and educated him on what he just did. I lost a lot of trust in web people that day. reply tgv 12 hours agoprevAnd that’s a very good reason never to fill in exact personal data, e.g. date of birth. Especially dating apps seem to need them, but don’t do it. Fill in something within a year or so from your real birthday. And while this dating app isn’t well known, it caters to people with different tastes (such as bdsm and group sex) and queer people. Needless to say that this is very sensitive in many parts of the world. reply intothemild 12 hours agoprevThey were in the press a lot this week, but for earning money. https://www.theguardian.com/technology/article/2024/sep/08/t... reply advael 11 hours agoparentIt's been observed by many that making bad things seems to be a lot more profitable these days than making good things reply gregoriol 11 hours agorootparentIt's not making bad, it's making cheaper/faster. They probably hired less experimented developers or didn't give them proper time to implement the features they wanted. reply advael 11 hours agorootparentI agree, making something cheaply and quickly is a great way to make it bad, and thus profitable reply dotps1 11 hours agorootparentprevIt's always been like that. The costs involved with maintaining garbage are infinitely more than maintaining something well built. This is why software is so lucrative.. because the true cost of the software isn't how much you pay for it .. it's \"how much is it going to cost you to change to something else?\" reply advael 11 hours agorootparentA great argument against relying on any software you can't control if ever I heard one reply rkachowski 10 hours agorootparentprevthe costs of maintaining something well built that no-one uses are very low indeed. unfortunately trash is cheaper and faster, and it takes a certain kind of genius insanity to sell something well built that doesn't exist yet. reply sureglymop 7 hours agorootparentprevIts been a long running joke between me and some friends that if you want to get rich, you should make a dating app. 1. Desperate men come in hordes. 2. You will probably get bought out by match group for millions. Of course there are moral qualms and also it may not be actually just as easy as that. reply greybox 8 hours agoparentprevSomeone should make sure that The Guardian sees this reply throwuxiytayq 14 hours agoprevCriminal negligence levels of failure, especially given the category of app. reply aitchnyu 12 hours agoparentI was that cheap contractor. My bosses were oblivious to anything but schedule and bugs surfacing to the client's reviewer. Guess threats of imprisonment in US and EU and data and insurance for data (if photos are not suitable for LinkedIn, you pay eye-watering prices) will be only deterrent. Of course, the incentives shouldn't promote coverups. reply ghostpepper 14 hours agoparentprevWow you weren't kidding. These are vulns that would have been embarrassing a decade ago. reply Incipient 14 hours agorootparentI think the timeline is the more damaging part too. Not only was their design woefully inadequate, they don't seem to care. reply 1_1xdev1 13 hours agorootparentI used the app briefly a few months prior to their discovery. The app was riddled with bugs. Things like chats not loading (received the push notification, but in the app not visible until force quit/reload). I’m not surprised it took them so long to remediate. I would guess a shoestring contractor dev team. reply xenospn 12 hours agorootparentThis is what happens when both founders are not technical. I use the app and it was obvious from day one it’s been designed and implemented by the lowest bidder. reply yosito 11 hours agorootparentNot necessarily the lowest bidder. It's quite easy for a consulting company that is bad at development to make a convincing pitch to a nontechnical founder as long as they're better at sales than they are at development. reply RHSman2 11 hours agorootparentReplace ‘founder’ with organization reply nailer 5 hours agorootparentprevThe founder used attend node.js events in London. Not sure why you think he’s non technical. reply itake 13 hours agorootparentprevThe problem is they probably don’t have full time developers. They probably built the app once years ago via a dev shop and then never updated it again. The talent moved on and updating it is expensive now. reply benoau 13 hours agorootparentCost minimizing aligns well with the criminal-negligence theory. In fact every egregious security issue I've come across, like plain text passwords, public S3 buckets, publicly-accessible internal tools... it all directly correlates to being cheap in my experience. reply tirpen 9 hours agorootparentprevThey (or someone they hired) actually rewrote their whole app about a year ago, I remember seeing lots of people complaining about how much worse and buggier it got after the rewrite. I have no idea if the back end was also replaced then or if the vulnerabilities were present in the previous version as well. reply forgotacc240419 10 hours agorootparentprevThey had turnover of £39m last year and profits of £5.5m (double the previous year, quite good for a UK business of this scale). If they don't have full time devs it'll be shocking, certainly had the money to sort crap like this out reply elric 9 hours agoprevThe online dating space (I use the term liberally) is a huge fucking mess. There's only 2 or 3 companies with an offering that is anywhere near useful, and they're either evil, incompetent, or both. Maybe it's time for an open source federated dating service or something. Or at least something that doesn't sell your data, doesn't leak your nudes, or doesn't get you beaten up/raped/murdered. Probably easier said than done. reply ChiperSoft 5 hours agoparentI’ve been conceptualizing one for a few years, but just don’t have the free dopamine to build it alongside my day job. ActivityPub even has the mechanics to facilitate it through publishing Person records. There is MASSIVE space for innovation, especially if you prioritize on non-monogamy, non-heterosexual, non-gender-conforming needs. Dating apps are a REALLY hard space to get into, however. You need a cumulative mass of users in a given area before they’re useful, and monetizing it inevitably means making the app less useful. There’s a reason okcupid went to shit after it stopped being a non-profit. And then there’s the moderation problem… reply diggan 4 hours agorootparentNow you've piqued my interest, especially if it could be done in a safe but distributed way, without a focus on profits. How you'd envision it to work, considering the open nature of ActivityPub but the need/want from the users to remain private when using dating applications/protocols? reply greybox 8 hours agoprevThis is utterly horrifying, clearly absolutely zero thought was put into security at all. I'm a game developer and we put more effort into keeping our game fair than this company does in keeping it's users safe. They should be sued into oblivion. reply forgotacc240419 8 hours agoparentZero thought was put into anything. Before I realised the app was a buggy mess I was very surprised to see it had an interests section that provided no context for the interests. For example: virtually everyone had Domination or Submission as one of their interests but no context whatsoever of which role they wanted. To not realise how fundamentally wrong this is for that scene implies they're clueless across the board. reply marcus_holmes 12 hours agoprevHot take: this is a problem with GraphQL. GraphQL allows your front-end to query your data. Which is cool. But from the backend this is all really opaque (and usually implemented by a 3rd party library that has no idea about your access control). Unless you're going to implement your access control in the database itself (not the worst idea, certainly better than doing it in the front end), then it's very hard to unwrap the GraphQL query in backend code to work out exactly what records should be returned/restricted. Implementing decent access control in the backend means understanding the query and implementing a whole set of models/classes/functions/whatever that grok the database schema and can make decisions about \"if the user_id is XXX then it can/cannot see this image in this context\" [0]. They obviously implemented this in the front end because that's a lot easier with GraphQL. I'm not saying this is a good implementation of GraphQL and that therefore the problem lies with GraphQL exclusively. I'm saying that GraphQL makes this mistake easier to make because it explicitly tries to remove the need for the backend to understand the query and so makes this kind of complex security situation harder. [0] e.g. a specific image may be publicly accessible from the user's profile, or only available to matches, or only in a chat context (but not group chats), and inaccessible at any time from blocked users, etc. You can easily come up with a bunch of complex edge cases for just this one case. reply strken 12 hours agoparentIt's pretty easy. Treat each resolver that retrieves data like it's a REST endpoint and secure it, and add a query allowlist that you append items to during your CI builds. You don't need to touch the AST or understand the context of the rest of the query. Just answer the question \"can user ABC see the photos of user XYZ?\" in the resolver that fetches the photos. If this is inefficient then prefetch some data or use a dataloader. Now, if you're using some magic library that turns GraphQL into SQL, that's going to be different. reply mewpmewp2 11 hours agorootparentStill I think this type of thing is much more likely to happen with GraphQL including various N + 1 and even worse performance issues. Like if you imagine having junior engs they will be much more likely to make the mistake with GraphQL than otherwise and it is harder to review as well. The permissions checking becomes a real spaghetti and difficult to understand in practice compared to just one by one checks. reply strken 9 hours agorootparentThe permissions checking is one-by-one checks. It's exactly as hard a mistake to make in GraphQL as it is in REST unless you've got more resolvers than an equivalent REST app would have, which is unlikely and would mean GraphQL wasn't a good choice. I do think that you've got a good point about how the knowledge isn't widespread yet, that it's easier for frontend engineers to write awful expensive queries, and that GraphQL is very hard to secure against DoS unless you lock it down with query hashes. reply consteval 3 hours agorootparent> The permissions checking is one-by-one checks Not true, authorization can be done in middleware. You can deny requests automatically, even scenarios you never considered. reply mewpmewp2 7 hours agorootparentprevWouldn't it seem contra to the principles of GraphQL if you treat resolvers like rest endpoints? At this point, it's just RPC, no? It's not really a graph. Why didn't I just use RPC/Rest the whole time? reply strken 6 hours agorootparentYou don't treat resolvers like RESTful endpoints. You check that the user has permission to access the object (edit: or other value) which the resolver returns. This has nothing to do with RPC and does not stop you using the \"graph\" part of GraphQL. For the purposes of comparing a REST API, where permissions checking is done for every endpoint, to a GraphQL API, where permissions checking is done for any resolver which loads data, it is necessary to compare the number of permissions checks you would need across the two services. This does not mean resolvers are in any way equivalent to RESTful endpoints except for comparing how many times you'd need to write `ctx.can('read', photo);` across the two, and even then the numbers will almost certainly be different because the APIs will be different. reply pistoleer 9 hours agorootparentprevHow do you guys bridge the abstraction gap/wall between resolvers to prevent N+1 queries? I have the suspicion that GraphQL is great for exposing a really generic API, useful when you have no idea what shape the front end will take (how often is that?). But it comes at a heavy price; genericity is always the opposite of specialization. And optimization can only occur during specialization. Having worked with it for a bit over a year now, it really feels like GraphQL is just a different protocol for writing the same old REST CRUD, while introducing a huge framework with lots of annoying magic and language level reflection that isn't amendable to extension or modification according to the needs of the developers. Is that all worth it, just to reduce the amount of HTTP requests? Is it that much of a sacrilege to add specialized REST HTTP endpoints to remedy that otherwise? reply mewpmewp2 7 hours agorootparentAlso if it's really the problem with HTTP Requests, you could still technically abstract multiple REST API/RPC calls into a single HTTP Request. reply knallfrosch 10 hours agoparentprevGraphQL requires you to either define per-property access, or precompile queries and put them into a whitelist. Everything else leaks data. https://hasura.io/docs/2.0/security/allow-list/ reply luxcem 11 hours agoparentprevAny third party GraphQL library worth its salt should implement some kind of ACL. It seems to be the case with the most popular ones [1] [2]. One simple idea is to implement authorization in the data models. GraphQL delegate ~get~ and ~list~ to ressource model that could implement authorization based on the context of the request. [1] https://www.apollographql.com/docs/apollo-server/security/au... [2] https://docs.graphene-python.org/projects/django/en/latest/a... reply rahkiin 11 hours agoparentprevI did not have this issue when using HotChocolate. You can easily give authorization rules to entities or properties of entities which will automatically be handled. Also to mutations reply Coolbeanstoo 11 hours agoprevI'm not terribly surprised. I use it but would describe it as incompetently put together as my bank app? maybe worse, it barley functions at all. I dont know how they managed it. reply forgotacc240419 10 hours agoparentIt was so bad when I used it, if it wasn't bizarre memory leak or privacy issues it was extremely poorly executed UX. Between it and Fetlife there's some huge issues with those communities just sticking with the first app that emerges regardless of quality reply fire_lake 10 hours agorootparentGiven the overlap between those communities and OSS people I’m amazed no one has created a B Corp that does this stuff right. reply forgotacc240419 10 hours agorootparentNo one migrates from the first big thing so it's a waste of time. Feeld should be killed by this and it'll barely make a dint reply NoGravitas 4 hours agoparentprevWhen I used it, I enjoyed the community, but the app was never competently written. Then a while back they had a flag day where they rolled out a new app and a new server to everyone all at once, and most people were not able to log in; those that were lost their premium perks if they were paying customers, likes and chats got lost, etc. I was never actually able to log in, and just dropped the app at that point. reply egamirorrim 13 hours agoprevWow. Remarkably responsible, and compassionate, disclosure. reply arraypad 10 hours agoparentHave they included real profiles in the screenshots of the \"Discover profiles\" menu and the list of likes? If so that's pretty irresponsible even with the faces obscured. reply hackernewds 11 hours agoparentprevActions don't match words. reply leg100 11 hours agorootparentWhat? They did hold off. Their actions matched their words. reply 0xbadcafebee 12 hours agoprevI am honestly amazed that these researchers held off for as long as they did on publishing. If crappy startups are given 6 months to close egregiously bad privacy holes like this, they will continue to abuse the privilege they have in collecting this information to begin with. I say give them 2 months and then release. Fuckers need to learn not to play dice with people's private information. reply qingcharles 12 hours agoparentThe question is -- did others know about it? e.g. https://news.ycombinator.com/item?id=41517747 reply fire_lake 13 hours agoprevGod damn it. People deserve better than this. Almost inclined to take a pay cut to go and fix this mess. reply tdeck 12 hours agoparentHowever little you're willing to take they can hire a less competent person cheaper. reply fire_lake 12 hours agorootparentYou would hope that a mission driven company like them would care. Or at least, a profit driven company would care about scaring away users. reply shiroiushi 11 hours agorootparentAs decades of Windows blue screens proved, shitty software won't scare away users if the software can provide a service or capability that the users can't easily get elsewhere. reply greybox 7 hours agoparentprevThey don't need your charity, they need to be fined reply jrm4 2 hours agorootparentAnd the fact that this, the literal only solution that has any chance of succeeding, is this far buried down in the comments, says so much about this industry. reply Ekaros 10 hours agoprevSaddest part is that this sort of stuff or at least not proper authorization checks is very common. I do not really know what is the solution at this point. Clearly not enough developers care. Or can stop it... Is it education problem? If so if there was training budget a day or two running against some simple capture the flag exercise might do a lot... reply stef25 5 hours agoprev> View other people’s matches \"BRB going to slaughter everyone my wife has chatted to\" Hard to believe the levels of incompetence here They have investor funding ... how come no due diligence was done ? reply mikkelam 10 hours agoprevThis is pretty funny. I've been abusing this shitty API for a while to see who likes me in this dating app. I didn't realise the problems were this bad. They've had massive issues with their tech stack from a user POV. I've multiple times had my phone running incredibly hot while using it. reply zx8080 11 hours agoprevIt's hard to expect any improvement while the personal data insecurity is tolerated without any penalty or fines. reply Klonoar 9 hours agoprevAnybody who's ever used this app is probably not surprised to hear this. It's been a shitshow since day one, one of the buggiest apps I think I've ever used. Even with a full redesign/rebuild over the past year it still is nothing but glitchy software. reply wasma 10 hours agoprevWho do you trust? Would tinder and bumble have the same mindset? reply rollcat 10 hours agoparentApplies to all dating apps, really: just treat any info you put in your profile as 100% public, for anyone, worldwide. Location is easily faked, other filtering options are about as effective as a lone \"do not enter\" sign with no fence - I can put any info I like into my profile to fit your criteria and have you show up in my feed. Chats? The only IM apps with functional E2EE are: Signal, iMessage, WhatsApp; and even those have trade-offs. Treat everything else as readable by some third party, and dating apps by design need to be able to look into people's chats to be able to handle harassment cases. That of course is no excuse for having gaping security/privacy holes, but you're trading off quite a bit of privacy by design; it's like meeting in a public space where you can feel a little bit safer with someone you don't know yet. I'd say if you're concerned with any of that, go meet new people IRL, but there are 100% legitimate cases where this is not the most effective strategy (e.g. Feeld's primary target audience). reply throwaway2037 8 hours agorootparentLots of great points in your post. Real question: Has WhatsApp ever had a security leak that we know about? Example: Someone can break into accounts, or chats were leaked? reply diggan 4 hours agorootparent> Real question: Has WhatsApp ever had a security leak that we know about? Example: Someone can break into accounts, or chats were leaked? Yes, a bunch of them. I don't remember any of the years, but from the top of my head: - Pegasus was installable via Whatsapp calls that didn't need to be installed, probably the most famous vulnerability with the largest impact - Bunch of multimedia vulnerabilities that allowed attackers remote execution - At least one huge database dump was released at some point reply rollcat 8 hours agorootparentprev> Has WhatsApp ever had a security leak that we know about? I don't know of any, but I distrust anything Meta/FB/MZ does, out of principle. I have more trust in iMessage, but it's incredibly tightly tied to Apple's devices (as far as I can tell, part of its security architecture relies on the hardware/SEP). Signal (as a non-profit org) could have been a neutral third party everyone could feel safe to trust, but they've lost my confidence when they introduced support for cryptocurrencies - I can no longer trust their motives. It also does not offer any choice over some security/usability trade-offs (like syncing your chat history to a new device); I understand this is critical for e.g. whistleblowers, but a deal-breaker for many of the rest of us. reply stef25 5 hours agorootparentprevThose types of bugs can be sold for millions so you probably won't hear about them reply INTPenis 10 hours agoparentprevThe for-profit dating scene is a quagmire. Sure it can work, I've seen it work, but at what cost? We desperately need a new platform owned and operated by the people, for the people. reply rollcat 10 hours agorootparentHey, sign me up. Outside the \"necessary evil\" trade-offs inherent to facilitating one-on-one meetups, I would really love a platform that treats people like people, not cattle to be milked for money. reply energy123 9 hours agorootparentprevThe Tokyo government is trying that: https://www.cnbc.com/2024/06/07/japan-pushes-citizens-toward... reply a091 14 hours agoprev [–] interesting read - anyone have pointers to other app pentesting walk throughs like this? reply mjg59 14 hours agoparentI wrote up finding some of these issues entirely independently: https://mjg59.dreamwidth.org/70061.html reply qingcharles 12 hours agorootparentSo the question is -- how many others knew about this and were exploiting it without discussing it? :( reply mjg59 12 hours agorootparentGreat question that would ideally be asked of the people who have logs reply forgotacc240419 11 hours agorootparentprevI didn't exactly know of it but I had enough glitches on that terrible app when I was using it that it was obvious there was info being sent that it didn't mean to and some atrocious performance issues that made it feel like it was crudely thrown together Pretty sure I flagged something or another as a security issue but can't recall what it was reply ramimac 12 hours agoparentprevhttps://github.com/juliocesarfort/public-pentesting-reports is a substantial collection of public reports Off the top of my head, DoyenSec has some good reports in there targeting web apps reply bawolff 13 hours agoparentprev [–] For pentesting, often the company hires people to test under an NDA, and keep everything secret because they dont want to be embarassed. There are sone public pentests out there. For example https://www.opentech.fund/impact/security-safety-audits/ If you want to read some really hard core security vuln hunting, see https://googleprojectzero.blogspot.com/ reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Feeld dating app had significant security vulnerabilities, including unauthorized access to user data and messages, as well as the ability to manipulate other users' profiles and interactions.",
      "The issues were disclosed to Feeld in March 2024, but multiple follow-ups and delays occurred before the blog post was published in September 2024.",
      "The post highlights the critical need for robust security controls in mobile apps and emphasizes the importance of thorough security testing, as advocated by FORTBRIDGE."
    ],
    "commentSummary": [
      "The Feeld dating app has significant security vulnerabilities due to implementing permission checks on the frontend rather than the backend.",
      "This issue is particularly concerning given the sensitive nature of the app's user data, which includes personal and potentially compromising information.",
      "The discussion highlights a broader industry problem where inexperienced developers or cost-cutting measures lead to critical security oversights."
    ],
    "points": 269,
    "commentCount": 118,
    "retryCount": 0,
    "time": 1726090366
  },
  {
    "id": 41516327,
    "title": "Be a thermostat, not a thermometer (2023)",
    "originLink": "https://larahogan.me/blog/be-a-thermostat-not-a-thermometer/",
    "originBody": "Be a thermostat, not a thermometer Originally posted Apr 4, 2023 • More resources on communication & team dynamics This post originally appeared in my newsletter. Subscribe to receive it! As I’ve learned more about how humans interact with one another at work, I’ve been repeatedly reminded that we are very easily influenced by the mood of those around us. It’s usually not even something we do consciously; we just see someone using a different tone of voice or shifting their body language, and something deep in our brain notices it. If you’ve ever attended a meeting where there were some “weird vibes,” you know what I’m talking about. You couldn’t quite put your finger on it, but something about the energy of the room was off—and that feeling affected you, even if it was super subtle. We’re wired to spidey sense this stuff; this gut instinct is part of what’s helped us stay safe for millenia. Our amygdalas are constantly on the lookout for threats in our environment that could be bad news. Plus, we tend to infer meaning from those weird vibes. Our brain is trying to make sense of the shift in behavior, so we’ll make some (often subconscious) guesses about what’s truly going on. We often even jump to the assumption that those vibes are about us. Humans mirror each other If I’m distracted in our one-on-one because I’ve got some stuff happening out of work that you don’t know about, it’s a recipe for misunderstanding. What you might observe is that I’m not making eye contact, I’m suddenly changing the subject, and my arms are crossed. How does your brain make sense of this? It decides that I’m upset with you—without any other information, it’s the most likely reason, of course. :) Plus, humans, like most other mammals, mirror each other. When I change my tone or my body language, there’s some likelihood that your tone and body language will change in response. So now we’ve got a compounding situation—I’m having a bad day, so I’m giving off strange vibes, then you’re giving off strange vibes because you’re picking up on my bad day. We leave the one-on-one and go meet with other people, and now they’re picking up on our strange vibes. This cycle is far more noticeable when someone is amygdala-hijacked. It’s tremendously easy to be caught off guard by someone who is overcome with a surprising emotion, and feel triggered by it ourselves. Again, this is just a normal defense mechanism—there is no judgment here. Noticing a change in someone’s behavior It takes a lot of practice to recognize when this pattern of shifting and influencing behavior is happening! But once you start paying attention to people’s patterns of behavior (what words do they use when they’re feeling upset? How does their body language change? Do they get louder or quieter? In what situations are they cracking jokes, and in what situations are they more quiet?) you can develop a stronger spidey sense when someone’s “vibes” are different than usual. In my video course on Dealing with Surprising Human Emotions, I talk about how to recognize when someone’s behavior seems off, it’s just a signal—just data—that one of their core needs might be being messed with. (Or maybe they simply didn’t get enough sleep last night, or haven’t had coffee yet today!) You can try to see it as a weather vane that something has gone awry for this person. Because once you can transform these signals into data—and not simply mirror the weird vibes back—you have an opportunity to positively affect what happens next. Thermometer vs thermostat I like to use the metaphor of a thermometer and a thermostat for this idea. If you’re looking for signals about how someone is feeling, it’s kind of like you’re trying to take their emotional temperature. You’re being a thermometer. When they’re subtly giving off weird vibes—they’re frowning, answering your questions with fewer words than normal, etc.—you’ve noticed that their temperature is different. When their amygdala is hijacked, you might see large changes in their behavior (they’re picking a fight with you, going completely silent, skipping your meeting, etc.)—in the thermometer metaphor, they’re running a fever, and you’re picking up on it. And since we know that one person’s behavior change can cause others to change their behavior in response, we can think of it like they’re being a thermostat: they’re setting the whole temperature for the room. Even if it’s unintentional on both sides. It’s just how we’re wired: to mirror the “vibes” that someone else is giving off. Rather than let that cycle play out subconsciously, you have an opportunity to become the thermostat as soon as you notice that another person’s temperature has changed. You get to set the new temperature of the room, in a positive and healthy way. Being the thermostat Once you’re able to start noticing when someone’s amygdala-hijacked, or simply that the vibes are off, you can reframe and use “be the thermostat, not the thermometer” for good. Since humans tend to mirror each other, you can intentionally change the energy in the room, setting the thermostat to a more comfortable temperature. Naming what’s happening One way to reset the temperature is to say out loud, with your mouthwords, that you’ve noticed that the energy has shifted. Here’s a how-to blog post on naming what’s happening in the room. As I mention in that post, there are a few risks to doing this, so you should use your best judgment on whether or not naming what’s happening in the room would be helpful in the moment. You won’t always get it right! Avoid projecting your feelings onto others, or putting them on the defensive, that would make the temperature of the room even more uncomfortable! If you’re noticing a major shift in someone’s demeanor, instead of guessing what’s going on for them (like “you seem upset”) ask an open question about what they need or how they’re feeling. This way you’ll know if you need to get your thermostat hat on. Choose your tone and body language When naming what’s happening or asking open questions, keep what you say short and sweet, and remember to use a calm tone and open body language. I’ve written about this before, but it’s definitely worth recapping here, because this is a huge component of being an effective thermostat! Gently nod at the pace they’re talking at, or slightly slower. It shows you’re following and tracking what they’re saying. Make soft eye contact. Hard eye contact is intense, eyes wide—it’s a little creepy. Soft eye contact is more like a Tyra Banks “smize”—a subtle relaxing of your facial muscles that shows you’re not ready to pounce as soon as they’re done talking. Don’t worry about keeping constant eye contact. Research shows you can break eye contact every 3 seconds naturally, then connect again, and this still feels attentive and affirming to the other person. Lean in, but not too much. When we’re uncomfortable, we sometimes unconsciously tip away from the person in whatever way we can. This can send a signal that you’re uncomfortable or trying to get out of this conversation ASAP, or even that you are asserting dominance. Make sure you’re squarely facing the person—or if you’re on video, squarely face the camera—and lean in slightly. Even as little as 1” will do the trick! If I’m on Zoom and sitting at my desk, I like to make sure my elbows or wrists are evenly resting on it. Be intentional about the tone that you’re using. You’re responsible for communicating that you want to hear what they have to say, and that you’re here to support them. This intentional choice, in combination with your body language cues, will communicate to the other person that you are actively listening. I’ve found that even a subtle change in my tone—like going a little quieter if the other person has gotten a little louder, or adding a little bit of joy to my voice if they seem unsure or a little bit stressed—can reset the temperature in the room. There’s a lot more to say about active listening; you can read more in this blog post! Your whole goal here is to set or reset the temperature of the room by modeling it with your tone, body language, and word choice. This skill of intentionally choosing your body language, tone, and words can help the other person move out of whatever “weird vibes” they were giving off earlier, as they can now start mirroring yours. But if it’s a more drastic scenario, like this person is in an amygdala-hijack mode, this approach can also help them feel more heard, understood, and confident that you are decidedly not mad at them. Usually, this skill does the trick. You smiled a bit, told a little joke that made them chuckle, nodded at the pace that they spoke to indicate you’re listening, and their mood started to change. You’ve just acted as the thermostat in a healthy, intentional way. But in case this doesn’t work, or if this person is in a more lizard-brain state, read on for some additional tools you can try. Offer a break If it feels like the other person has been amygdala-hijacked, or if they are decidedly stressed or distracted and you sense that there’s no way that the rational, logical part of their brain will be able to return in the next few minutes, use a back-pocket script to offer a pause in the conversation and a plan to return to it later. Some of my favorites to use are: “I’m not sure how y’all are feeling, but I think I could use some more processing time on this. Could we reconvene again tomorrow at 2pm?” “I know how much we want to come to an agreement on this decision today, but my spidey sense is that we might need some more time to think on it. How about we sleep on it and check in again tomorrow?” “I really want to support you on this and make sure you feel good about our next steps. How would you feel about us taking a break now to spend some more time thinking it through, and chat again at 4pm?” You’ll notice that the phrasing is intentionally trying to avoid putting someone else on the spot, or make them feel attacked. Your gentleness can help set the new temperature in the room. What I learned/What I’ll do If you’ve contributed to a big shift in the temperature by creating or escalating an awkward or tense situation, you have an opportunity to own your role as the thermostat here. Because if you never acknowledge it, you’re going to risk developing a forever-antagonistic relationship with them. Sure, they should just be a grownup and get over it, right? But this does not happen in practice. (When was the last time you, yourself, actually did that?) People hold on to this stuff! Your life is going to be SO MUCH HARDER if you don’t clear the air after you amygdala-hijack someone. In the Dealing With Surprising Human Emotions video course, I talk with Jason Wong about this template that we both learned from Paloma Medina :) “What I learned…” “What I’ll do…” For example, “What I learned is that that last email didn’t do a good job explaining the changes, so what I plan to do is start a forum for folks to post their questions and our CEO will answer them every Tuesday.” When said with heartfelt authenticity, this phrase tells people that their needs, feelings, and concerns are not irrelevant. It allows people’s bellies to relax because their needs have been acknowledged. You can begin the work of recovering from the amygdala hijack, because you’ve reset the temperature in the room. My parents actually taught me to “be the thermostat, not the thermometer”. It’s not always easy, that’s for sure. But by being aware of these cycles, we’re more likely to remember to use our thermostat power for good (not just give up or bail out when you notice someone else is running hot). Next time you find yourself in a conversation that’s exuding some off vibes, or even an intense one, if you use a combination of these tools, you’ll be giving others the opportunity to mirror the temperature you set right back to you.",
    "commentLink": "https://news.ycombinator.com/item?id=41516327",
    "commentBody": "Be a thermostat, not a thermometer (2023) (larahogan.me)253 points by dillonshook 16 hours agohidepastfavorite118 comments gleenn 13 hours agoThis metaphor immediately rang true to me but the article is definitely worth the whole read. There are a bunch of linked articles too which also have some very sound advice. I really like a tactic in hard situations which was saying \"What I learned...\" followed by \"What I'll do is...\". It makes someone feel heard and that you'll follow through with some action to make someone feel like you have akin in the game with their concern. I really liked a lot of other somewhat generic but still oft-ignored advice like lean in a bit, make eye-contact, and the title which is just that if someone is making you feel off, instead of just reacting like a thermometer and also potentially aggravating the weirdness, do things that help regulate and relieve those human tendencies based on feelings of fear etc. Excellent read. reply 8n4vidtmkvmk 12 hours agoparentYou'd better follow through and do the thing you said though. If you start saying things when people are riled up and then don't do it... They're going to notice. reply gleenn 9 hours agorootparentI agree, when you say you're going to do something then obviously you should also do what you say and say what you do. But my guess is even just saying I learned you don't feel heard about xyz and I will try to pay more attention to you your concerns when you're talking about xyz shouldn't be hard to commit to if you care at all about whomever you're talking to. Hopefully they are a bit disarmed and realize you're trying. I don't think this is a silver bullet, and I think the article makes that point at least a few times. The point is to try and stabilize and correct the vibe if it isn't really justified. If you are going into a warzone then you probably need different advice, hopefully this isn't the norm though and you aren't a hostage negotiator or something. reply detourdog 8 hours agorootparentThere is also a class of people that will always provide reason or task that is the issue. The list of problems will never end and they will never be satisfied. Some people would rather complain then reflect on their inner workings. reply deisteve 12 hours agoprevWhile the article has some good points about the importance of emotional intelligence and awareness, I'm skeptical about the idea that we can simply \"choose\" to be thermostats. Humans are complex and emotional creatures, and our emotions can be triggered by a multitude of factors beyond our control. The article's suggestions, while well-intentioned, feel like a form of emotional labor that can be exhausting and unsustainable. Can we really expect people to be constantly \"on\" and aware of their emotional impact on others? reply bfung 10 hours agoparent> I'm skeptical about the idea that we can simply \"choose\" to be thermostats. Well, it’s like most things, it takes practice and time to be good at it if natural talent isn’t there. Sure, things out of our control can trigger emotions, but one incredible ability of humans is to rationalize those emotions and act in more constructive ways than to immediate react back. It can be quite liberating and fun to understand and process these things, much like understanding code and data structures in order to recombine them into things you want to achieve. reply theultdev 6 hours agorootparent> It can be quite liberating and fun to understand and process these things, much like understanding code and data structures in order to recombine them into things you want to achieve. This thread actually made me realize it's much easier to express this in code vs words since it deals with state and logic: https://gist.github.com/TheUltDev/fc8386e42205504c55d1cf2127... This is the my process of rationalization and resolution. The flow is the same for all scenarios, but I only coded logic for one scenario I described in another comment: Let's say someone close to you is unusually quiet and short with you. You irrationally think they are mad at you or ignoring you because they are being short. That makes you feel mad because you didn't do anything to them! Upon receiving the feeling you start rationalizing the response and realize that you have no evidence that they are mad at you and there are many times you don't want to talk. You then simply ask them if anything's wrong and they say they have a headache! Whew, it wasn't about you at all, it was just a headache! You then empathize with them and want to help so you ask if you can get them some advil and know not to be loud or talk too much until they start feeling better (acting normally) reply germinalphrase 4 hours agorootparentAs a topical extension, there are entire therapeutic modalities (like Cognitive Behavioral Therapy) that assist people in doing this kind of in-the-moment emotional reflection and recontextualization. If this sort of thing is difficult, seeking out a therapist that specializes in these modalities can be helpful (and seeking the support of a therapist does not require a person to be ‘mentally ill’). reply grvdrm 6 hours agorootparentprevCompletely agree with you. I think (like you) it is about VERY deliberate action. My phrase: a conscious turn in the other direction. The more I practice changing or revamping my reactions/approaches to situations, the more those things improve. Not everyone will agree but IMO - the skeptic is simply not that willing to try. That's ok. But it's the reality. reply sethammons 12 hours agoparentprev> Can we really expect people to be constantly \"on\" and aware of their emotional impact on others? Of course we can and should. Emotional regulation is a sign of maturity and being an adult. Children should be practicing emotional control. Can you be mentally and emotionally wrung out and grace given for emotional outbursts? Sometimes. I also have punched a wall when I stubbed my toe. We should expect me to not lash out at the door and it can still be understandable why I did. I have also sat stewing in a mood and it affects those around me. I can fix my attitude or I can remove myself for a spell. reply theultdev 12 hours agoparentprevIt's less emotional labor and less exhausting the more you are aware of your emotions. You can recognize the stimuli and rationalize it before becoming upset. The more you do it, the easier it becomes and the less stressed you become. reply deisteve 12 hours agorootparenttotally understand where you're coming from, but I think there's a difference between being aware of your emotions and trying to suppress or fake them. The article isn't suggesting that we should be constantly 'on' and pretending to be someone we're not. Rather, it's about developing self-awareness and learning to manage our emotions in a way that's authentic and sustainable. Recognizing the stimuli that trigger our emotions is a great first step, but it's not just about rationalizing it away. It's about understanding what's driving our emotions and learning to respond in a way that's healthy and constructive. This takes practice, patience tho reply theultdev 12 hours agorootparentI didn't say suppress or fake your emotions, I said to rationalize them. If they are irrational you prevent the emotional knee-jerk response in the first place. If they are rational then you know the root cause to work through so you can fix the issue. For those rational emotional responses, recognizing the stimuli can still be helpful. It's still stressing, but you know the exact problem and can work to resolve it, the opposite of suppressing it. reply deisteve 12 hours agorootparentyou're advocating for a proactive approach to emotions, where we acknowledge and rationalize them to prevent irrational responses, while still allowing ourselves to feel and work through rational emotions. This approach seems to strike a balance between emotional awareness and emotional regulation. I think this is a great way to approach emotions, and it's refreshing to see a nuanced discussion about emotional intelligence. reply theultdev 12 hours agorootparentThank you, these are things I developed to manage my emotions and it's the first time to put them in words, I tried my best to serialize it. The cool thing is, if you do this enough, you can always recognize the stimuli. There's one exception, hormonal imbalance (bipolar, seasonal depression, etc.), because there is no stimuli. But once you realize there is no external stimuli, you know it's hormonal. It is then classified as a irrational reaction. The difference of an internal irrational reaction is it takes more investigation and sometimes a few emotional knee-jerk reactions slip through before the hormonal cause is detected. Also the irrational brain can misattribute the imbalance and attribute it to an external stimuli, but you can immediately correct it with evaluation and communication if the misattributed stimuli is a person. (ask for clarification, if it's not what you assumed, apologize after snapping and solve the conflict immediately). My wife and I can tell when she's nearing her time of the month because it effects both of us. The hormone change unbalances us a week or so before sometimes causing fatigue or snippiness. It's nice to recognize it as to not contribute it the fatigue to burnout or take the snapping to heart. We just overcompensate in communication and directly ask what the other person meant to not take something the wrong way once we recognize we're in this temporary state. reply darby_nine 10 hours agorootparentprevI don't see how recognizing an emotion as irrational gives you certainty of calming it. Generally speaking, emotions don't arise from a conveniently rational level of consciousness. If they did they would be referred to with terms ofther than \"emotion\". reply theultdev 7 hours agorootparent> I don't see how recognizing an emotion as irrational gives you certainty of calming it When you realize it's an irrational reaction you automatically reprocess the stimuli and get a rational reaction. Let's say someone close to you is unusually quiet and short with you. You irrationally think they are mad at you or ignoring you because they are being short. That makes you feel mad because you didn't do anything to them! Upon receiving the feeling you start rationalizing the response and realize that you have no evidence that they are mad at you and there are many times you don't want to talk. You then simply ask them if anything's wrong and they say they have a headache! Whew, it wasn't about you at all, it was just a headache! You then empathize with them and want to help so you ask if you can get them some advil and know not to be loud or talk too much until they start feeling better (acting normally) > Generally speaking, emotions don't arise from a conveniently rational level of consciousness. What makes you say that? Emotions commonly arise from rational thought. There are rational reasons to be mad/happy/sad/etc. But what I'm suggesting though is the opposite, to make it a habit upon receiving every powerful emotional to verify it with rational thought. reply wruza 9 hours agorootparentprevI don’t believe in this theory and my experience with therapy suggests it just makes little sense. Emotional outbursts (like being startled/angered/in pain) may be temporarily irrational your own logic-wise, but your regular emotional background absolutely reflects what you actually believe is happening and the way you think. So unless you’re doing emotional logging and are really managing your beliefs, deep settings, etc afterwards, this is simply impossible. I mean you can learn therapy, but it’s not a knowledge you’re born with as a regular guy and it’s a whole “learn C++ in 21 days” thing. There is a level of being still not broken enough, but then emotions aren’t a problem in the first place. You usually end up trying to manage them when you’re already lost and what people do is simple suppressing, thinking that’s how “adults” do. To be clear, I provide no answer to this thread, only a comment. reply theultdev 6 hours agorootparent> Emotional outbursts (like being startled/angered/in pain) may be temporarily irrational your own logic-wise, but your regular emotional background absolutely reflects what you actually believe is happening and the way you think. Yes your emotional background reflects what you believe is happening, but you can correct your belief if you analyze and rationalize the emotional response when you feel it, which then updates your emotional background. > So unless you’re doing emotional logging and are really managing your beliefs, deep settings, etc afterwards, this is simply impossible. That's exactly what I'm suggesting you do. Upon receiving every major emotional reaction you make it a habit to analyze it immediately afterwards. reply wruza 6 hours agorootparentI find it very hard to impossible to do immediately afterwards. I can detect it, which is somewhat obvious, and write down the situation, but finding the source of it immediately I find unrealistic. Either we talk about different things here, or I lack some Sherlock Holmes level skills that you have. Anyway, if something bothers me that hard, making it unbother me is an improvement to work in a stupid situation, not self-normalization. For example, recently I got frustrated when an inexperienced relative wrongly measured airport hand baggage (pure geometric cluelessness) and insisted airport will do it that way too. I find this frustration absolutely normal and don’t really want to get rid of it, cause it’s immediately actionable and the response is correct-ish. Otoh, I successfully defeated my non-actionable fear of being late, but it took me a couple of advanced techniques I didn’t even know existed, some movie-level talk to your childhood stuff. So there’s so much to it that I just don’t see how to “just do afterwards” (at least it sounds like that). Again, feels we are talking different things here, not sure. And sorry for the stream of consciousness. reply theultdev 6 hours agorootparentI coded my thought process of a specific example if it helps clarify what I'm talking about: https://gist.github.com/TheUltDev/fc8386e42205504c55d1cf2127... (as this is a state of mind and logic thing, a program of state and logic is a better way to describe it vs words) reply aspenmayer 4 hours agorootparentprev> Otoh, I successfully defeated my non-actionable fear of being late, but it took me a couple of advanced techniques I didn’t even know existed, some movie-level talk to your childhood stuff. Can you elaborate on these techniques? reply wruza 3 hours agorootparentThe downward arrow (cbt) with elements of self-hypnosis. You basically log-trace your mind at emotional points, including pulling up automatic thoughts (somewhat difficult, they tend to escape). Datetime, situation, emotion, thoughts, levels. Then intersect it all through time, because different situations disturb different sets of facets of your problem, but only one facet is primary. This discovers intermediate beliefs and coping strategies, makes them explicit. And then you logically realize your core belief. It may happen quickly or slowly (weeks), depends on how conscious you are about it and how often it happens. In my case it was: unable to do anything 4-5 hours before an appointment or even a friends meeting. Set up a few timers, got ready step by step, doing mostly nothing in time buffers. Check for clean clothes, etc. Afraid of forgetting time and being late. Coping: long preparation, timing processes, checking time in the car. Intermediate belief: if I prepare I’ll be on time. Core belief: being late is atrocious and intolerable. Realizing is only half the job. To destroy a core belief you have to remember how [irrationally] it formed, that’s where hypnosis kicks in. I couldn’t sleep/relax in a session, but took homework. Basically before you go to sleep you ask yourself “when it was”, as if there was some entity inside you who could answer. Few minutes later it just flashed in pre-sleep in every detail. It’s akin to thinking “I will wake up at 6:30” and doing so, a similar process and feeling. I was few hours late from school when my grandma waited for me and couldn‘t get to work (strict schedule). She was afraid of giving me the keys. I was tired that everyone plays after classes and I cannot, so decided to just not care. She was very angry and terrified, hit me and went away like I was an enemy. Next week after therapy, for the first time in 25 years I was being intentionally late to a doctor, said hi, sorry for being late. She said it’s fine, smiled and asked what I came with. I realize I have serious issues here (like many others probably). But I believe either there’s no easy way to “just reflect afterwards”, or these issues aren’t really that hard to make this process explicit. I, for one, don’t understand how you get an emotion if a logical counter is readily available in your mind. It won’t happen for me in the first place then. Maybe on the contrary, I’m… healthier? reply downWidOutaFite 33 minutes agoparentprevI read this article as intended for an HR or management audience whose job is to always be the professional in the room since your voice is interpreted as the company's voice. reply Suppafly 12 hours agoparentprevI think you can consciously learn a skill by practicing it enough. You can choose to project positivity and like most things in life, fake it until you make it. >feel like a form of emotional labor that can be exhausting and unsustainable There definitely some wisdom in knowing when to draw back for your own sanity. reply deisteve 12 hours agorootparentOn one hand, I completely agree that deliberate practice and intentional positivity can be powerful tools for growth and skill-building. The 'fake it till you make it' approach can be especially helpful for building confidence and momentum. I think the key is finding that balance between pushing ourselves to grow and being kind to ourselves when we need to rest. It's okay to take a step back, recharge, and prioritize our own well-being. In fact, that's often where the real growth happens - in the moments of quiet reflection and self-care. reply jeffhuys 10 hours agorootparentI don’t know why and I might be wrong, but (parts of) some of your comments read exactly like an LLM response, while other parts feel like you typing additional stuff “around” the response. reply dr_dshiv 9 hours agorootparentI got the same vibes, fwiw. reply brigandish 11 hours agoparentprevWhat is emotional labour? reply rocqua 10 hours agorootparentThe effort someone chooses to put in to manage and help the emotions of others. It ranges from listening to someone talk about their day to driving over at night to a friend who's upset, to organizing an entire intervention. It is often considered to fall more heavily on women. Notably, as work that often doesn't fully get redistributed when women enter the workforce, much like housekeeping often doesn't. reply brigandish 6 hours agorootparentSome emotions are tiring - anger, frustration, depression, anguish, to name a few. I can't think of any that involve supporting a colleague at work. I could certainly get tired of shenanigans at work, but that would be from frustration et al, but support? Like the other comment that's responded, I just don't see the link between the description for the term and the situations in either the blog or a workplace. I've certainly not noticed a difference in the level of emotional support given in the workplace by women either. Whose emotions are they managing? Men's? reply lynx23 6 hours agorootparentprev> It is often considered to fall more heavily on women. Citation needed. reply joelfried 1 hour agorootparentHere you go: https://www.simplypsychology.org/emotional-labor.html > Hochschild (1983) suggested that jobs requiring more emotional labor are performed primarily by women. These jobs typically involve creating feelings of well-being or affirmation in others – responsibilities usually assigned to women. Hochschild, A. (1983). 1983 The managed heart. Berkeley: University of California Press. reply mock-possum 10 hours agorootparentprevThink of it like the difference between idly leafing through a book, versus studying a textbook as if your life depended on it - one is an inconsequential pastime, the other is an exhausting task made all the more stressful by its importance. Emotional labor is dealing with other people’s emotions, not in the first sense described in the paragraph above, but in the second - paying close attention, thinking critically, interpreting what you see and hear and feel in an effort to help someone in some way. It’s shouldering their emotional burden, to some degree, to support them, as best you can - same as physical labor might be. reply brigandish 6 hours agorootparentI read the blog, and the situations given - which are common in most workplaces - wouldn't lead me to compare them to a life or death situation in any way. I also can't imagine thinking that concentrating on someone's speech while in conversation with them as taxing, beyond the normal difficulties that attempting concentration can bring. Perhaps I'm missing something. The only time I could think of such things as laborious would be when faced with intransigence or my own frustration, and that's really about not getting my own way. Isn't it normal to try to have good, productive conversations, pay attention to others, and give support where needed? reply FollowingTheDao 6 hours agoparentprev\"I'm skeptical about the idea that we can simply \"choose\" to be thermostats.\" I agree. Try being thermostat to someone who is on meth or someone who is drunk.There are many reasons someone is raising he temperature in a room. Not to mention the fact that there might be a good reason some one is angry, like low wages, discrimination, or wage theft. And you coming in being a thermostat is just prolonging everyone's nightmare. I will tell you, when I saw people doing that BS to me I knew right away they were trying to manipulate me. We all have the right to be angry and you do not have the right to use neurological tricks to manipulate people because you are uncomfortable with \"weird vibes\". reply sethammons 11 hours agoprev> You’re being a thermometer. When they’re subtly giving off weird vibes—they’re frowning, answering your questions with fewer words than normal, etc.—you’ve noticed that their temperature is different. And if you are doing this as a coping mechanism from having an unstable parent and you are like me (also maybe a bit of adhd): you internalize the person's chilled behavior and often assume it is your fault. In case you need to hear it: You are not responsible for other's emotions (though you are responsible for your actions) reply dailykoder 9 hours agoparentI know this and I have learned more than enough about it to internalize it, but it just doesn't work. I can't find a way to stop the automatic jumping to conclusion and self blame. It takes hours to get over it and that's exhausting. I am trying for years to find a way out, but it just hasn't internalized yet reply nicbou 6 hours agorootparentI have the same problem and I made a lot of progress this year and the last. A few things that help in no particular order: - Write things down. Over time you start noticing patterns that help you diagnose and fix the issue. I particularly love the post-mortem when returning from a house party, and how batshit itsane it reads 5 days later. I also have a play-by-play diary of me thinking I was misreading a person's intentions and agonising over every interaction. We've been together for a few years. It's fun to rewind the tape and laugh at your own irrationality. - Treat your overreaction to social cues as irrational, and deal with it accordingly. Every Spring, my body tells me that grass pollen will kill me (hay fever), but I just ignore that signal as irrational. I now handle my hasty conclusions the same way. - Indifference is the default. Most people won't be excited about you, but they're a very long way from disliking you. A lack of enthusiasm does not mean anything about you. - Talk to others about it. When I started talking about my insecurities to close friends, they told me just how wrong I was, with lots of backing evidence. They were genuinely surprised that I thought any of those things. It's a bit like how a friend of mine was super self-conscious about something on his face, and a year in, I had never even noticed it. reply grvdrm 3 hours agorootparent> Treat your overreaction to social cues as irrational, and deal with it accordingly. This is so smart. Even broader, take your overreactions to most things as irrational. I am using this recently to rewire myself on all sorts of things and it's quite transformative. reply nicbou 1 hour agorootparentJournaling helps a lot with that because you catch yourself writing about the same emotions in the same contexts. The predictability of it makes it easier to process rationally. reply andruby 9 hours agorootparentprevAny tips for the person on the other side? I often mention something without implying blame (or even assuming blame), but it’s still processed that way. I’m trying to be conscious of this though. reply johnmaguire 5 hours agorootparentHard to say exactly what you're dealing with, but you could take a look at Nonviolent Communication. reply dailykoder 9 hours agorootparentprevI don't think there is much you can do. Everyone has different trigger points and a different past. Personally I often feel misunderstood or not taken seriously. So from my point of view just be genuine, maybe paraphrase what you heard (just a tiny bit) and the usual \"start with something positive first\". The latter can be hard for me too though because then I might think \"no they can't have such a positive view of me\" - it's complicated and I even have a hard time explaining it. So no real tips, sorry. \"we\" just have to learn how to live with it ourselves reply detourdog 8 hours agorootparentprevIf this is a consistent person in your life or a partner. The communication has to improve. Improving communication maybe impossible but I think it’s the only way. reply lynx23 6 hours agorootparentprevI have given up on people that can not process criticism. Its a vital aspect of working together, or even just living together. If everything I say is put on a scale, I simply dont interact with such people anymore. If you can't take criticism without the blame-game, you're not worth my time and effort. reply fragmede 3 hours agorootparent> If you can't take criticism without the blame-game, you're not worth my time and effort. QFT reply willismichael 3 hours agorootparentQuantum Fourier Transform? reply detourdog 8 hours agorootparentprevWhat one has to figure out is where this pattern developed (most likely childhood). Once I can internalize why my emotions develop I experience a distance from the current situation. The distance removes the emotional reaction leaving me with an intellectual understanding. reply maroonblazer 4 hours agoparentprevA pithy little saying I learned when just starting out in my career: \"What you say, and what you do, says nothing about me, and everything about you.\" reply ddmf 4 hours agoparentprevRejection Sensitive Dysphoria is so hard to combat at times - even if medication helps. reply watwut 6 hours agoparentprev> You are not responsible for other's emotions (though you are responsible for your actions) I know you do not mean it this way, but I really dislike this saying. It is not even true, actually. The most frequent use of this is people who are being, well, jerks, trying to argue that when people feel bad after being put down, insulted or treated with passive aggression, it is their own fault. If people feel bad after your actions, yes in many circumstances you are responsible. reply sethammons 6 hours agorootparentIf you are acting like a jerk, that is an action, and I expressly put that you are responsible for your actions. You punch someone and they are angry: your action is related. You arrive at work and say hi to your boss as they barely acknowledge you and are in a mood: you should not default to \"what did I do wrong and how do I fix it oh god I am gonna get fired\" - while it may also still be appropriate to try to cheer them up. reply bgilroy26 6 hours agorootparentprevI think different senses of responsibility are under discussion The parent comment I believe was saying that we do not orchestrate other people's emotions and you are saying that we do impact other people's emotions and both can be true reply watwut 6 hours agorootparentThe whole article this discussion is under is all about intentionally orchestrating other peoples emotions. reply Loughla 6 hours agorootparentprevSo much of today's self-help (and a lot of therapy styles) seems to be focused on selfish, self-centered behavior. reply wruza 9 hours agoprevWhat I learned is that that last email didn’t do a good job explaining the changes, so what I plan to do is start a forum for folks to post their questions and our CEO will answer them every Tuesday. I know it’s only an example, but hahahahahahahaha, ha. Start with something realistic if you do that. The worst thing you can do is to teach them you’re a bag of funny promises. reply xnorswap 8 hours agoparentAt a former company we once had an away day workshop where they allowed anonymous questions for the company director which would show up on a screen for everyone (it was a ~40-50 person company). We were a management consultancy and trialling what they thought was cool new tech to use with other companies. ( This was a while ago, smart phones were newer and apps were still \"cool\" ) Well, they very quickly learned to never do that again. Even in a small company there were a lot of tensions unresolved between the lowest and highest rungs. It was a fairly formal hierarchical structure where the common worker didn't tend to ever interact with the big boss. \"Where's the pay rise we were promised last year?\" was perhaps the mildest of the embarrassment, and it quickly devolved from there. reply tgtweak 5 hours agorootparentUncomfortable truths are no less a truth when spoken. reply b3lvedere 7 hours agorootparentprevOoh, some lifetimes ago at a company we had a CEO that did a company wide presentation where he kept mentioning that the shareholders are the most important thing of the entire company and we all should do everything to please the shareholders. The instant hate towards him could almost be touched and tasted. reply pistoleer 7 hours agorootparentIt is both inspiring and depressing that intelligence is not a prerequisite for high up roles. reply b3lvedere 6 hours agorootparentYup https://www.savagechickens.com/2024/09/decisive.html reply antognini 2 hours agorootparentprev\"I don't think you understand what the product is. The product isn't the platform, and the product isn't your algorithm, either. And it's not even the software. Do you know what Pied Piper's product is, Richard?\" \"Is... Is it me?\" \"Oh God! No! No. How could it possibly be you? You got fired. Pied Piper's product is its stock.\" reply watwut 6 hours agorootparentprev>Where's the pay rise we were promised last year Sounds like an extremely valid complain if such promiss was made last year. reply sethammons 11 hours agoprevIf this resonated with you, consider reading Nonviolent Communication by Rosenberg, the ultimate guide in thermostat-speak. You focus on stating unmet needs. Good stuff. reply nicbou 6 hours agoparentI haven't read NVC, but \"stating unmet needs\" is a strong aspect of \"No more Mr. Nice Guy\" and \"Models\". Being up front about who you are and what you want is a lot more likely to work than being nice and hoping your needs are met in the way that you expect. It's also a way to cut your losses early if the other person is not interested in providing whatever you're after. reply FollowingTheDao 6 hours agoparentprevWhen I hear people trying to use \"Nonviolent Communication\" it only makes me more angry. It is manipulation and so transparent and condescending. Sometimes communication needs to be violent. reply johnmaguire 5 hours agorootparentWhat makes you believe that nonviolent communication is manipulation? It focuses on ways to hear unmet needs in others and to express unmet needs in a way that can be heard by others. Have you read the book by chance? reply FollowingTheDao 4 hours agorootparentYes, read the book and my nephew teaches a form of it. It balmes the victim for the most part. It reminds of of\"you are acting hysterical\" and gaslighting. Maybe read some critiques about it. https://realsocialskills.org/2014/07/17/nonviolent-communica... https://www.collectivelyfree.org/nonviolent-communication-pr... reply sethammons 3 hours agorootparentThe first link, that is a big stretch and misses half the response. They almost capture it and then flub to spread wrong information. They got this far: \"I notice that when your partner talks to other men, you express feeling hurt and ask her not to. It sounds like you feel hurt and maybe even betrayed when she has those conversations. I hear that respect is really important to you, and you want to feel valued in the relationship.\" What they totally missed: Suggesting alternative strategies that meet both parties' needs without harm. \"Can we explore ways for both of you to feel respected, while also honoring her autonomy and connections with others?\" This is NOT emotionally abusive to the woman or lower-powered individual in the exchange. This is acknowledging the emotions of the abuser and still coming back to honoring the woman's unmet needs. These techniques have been used between waring tribes with family that has been murdered. The book has a particularly harrowing passage about NVC saving a near-rape-and-murder victim. The second link is marginally better but I disagree on nearly every point. Instead of writing a counter post for each, but to say that overall I think they missed the message of the book. They seem set on the word choice and power dynamics. Word choice is mostly unrelated: it is conveying unmet needs and acknowledging the unmet needs of others. Simple as that. And then to go on about body language as a point against NVC is strange as the NVC is about spoken communication. They are digging for reasons to talk against the book. I assume there is some agenda. reply FollowingTheDao 1 hour agorootparent>They are digging for reasons to talk against the book. I assume there is some agenda. You have an agenda as well. The problem is that you need BOTH parties to engage in NVC for it to work. and what happens is the person who does not want to use NVC is blamed. This is denying the person their agency. reply sethammons 1 hour agorootparentI feel that you desire agency on both parties. I believe that agency already exists. Like: \"Let's play a game\"; \"I don't want to\"; .... \"ok?\" NVC, yes, is a cooperative framework. The agency is to accept that, propose something different, or withdraw from communication. This has nothing to do with denying agency. It is tooling for communicating needs. I don't find the criticism to make sense. Perhaps you can help me understand by proposing an alternative or let me know where I am not understanding reply FollowingTheDao 1 hour agorootparent>I feel that you desire agency on both parties. That is such an artificial way of talking and it is making me not want to talk to you. You are actively cutting of communicating with me if you keep talking to me this way. Now you will blame me, and not yourself. I have lived all my life, cooperated with people of all kinds. Never used NVC. reply sethammons 31 minutes agorootparent>That is such an artificial way of talking and it is making me not want to talk to you. You are actively cutting of communicating with me if you keep talking to me this way cool, communication for the win and I can modulate to hopefully better understand you. how would you like to be acknowledged or how would you like me to check understanding? NVC is a framework to do that and you don't want artificial sounding exchanges. Cool. Is this still artificial? I don't really know. I am attempting to communicate with you and that takes checking understanding. I haven't blamed you for anything. And, yeah, as a throw back to the previous comment, I have an agenda: I am trying to understand and evaluate criticisms against NVC and I am not convinced by what those posts said. I want to know these because if I am giving bad advice by recommending NVC I want to stop. You may have cooperated with people of all kinds, but in this exchange, I feel I am working extra hard to understand your position and finding cooperation difficult. > Perhaps you can help me understand by proposing an alternative or let me know where I am not understanding >> I have lived all my life, cooperated with people of all kinds. Never used NVC. I think you are attempting help me understand your position, but I am having to stretch. You've cooperated and self-report to never have used NVC. OK, and what am I supposed to take away from that? I never said that NVC is the only way cooperation can be achieved. The claim is that by stating unmet needs and communicating those in a way that both parties can acknowledge and understand, that conflicts can be resolved. Conflicts can be resolved lots of ways, including walking away. Cooperation can happen even when you don't intend it. NVC is but a tool and one that I am still not sure what you object too. Are you against the suggested words and sentence structure proposed by NVC? If so, again, I think that is missing the point. reply johnmaguire 2 hours agorootparentprevI won't repeat what the sibling commenter has stated except to say that I agree that these authors seem to have missed the point. Anecdotally, my partner and I have found a ton of value in it within our relationship. reply FollowingTheDao 2 hours agorootparent\"Serenity now, insanity later.\" https://www.youtube.com/watch?v=MJVIX-4OyT0 reply sethammons 22 minutes agorootparentin my quest to better understand your position, I once again believe you missed the point. I think you are suggesting that NVC is artificial and smooths over chaos that needs to be experienced else things go worse later. NVC is expressly about not repressing your feelings and letting them be known via your unmet needs. Again, if I got this wrong, you are invited to correct me. Mostly I'm just bored between meetings and not enough time to push anything productive forward and having a back and forth is pleasantly distracting. Might not reply again since work is nearly beckoning. reply callmeal 6 hours agorootparentprevI'm reminded of this supposedly ancient proverb I was taught in school: He who raises his voice first, loses. reply eimrine 1 hour agoprevActually the article tells be an air conditioner/heater. Because being a thermostat means just leave the awkward meeting. reply klabb3 13 hours agoprevDefinitely gonna borrow this language, it’s a really important aspect of social life. I’ve always been very, very thermometer-like, with a strong tendency to mirror which allows me to connect with people 1 on 1 easy, but on the flip side I absorb vibes I don’t want. My coping mechanism is to avoid bad vibes, confrontational situations, etc. Even being in a social group for long can affect me negatively if the people there have values I don’t agree with, even if I have no desire to change them. Any tips for how to manage that better? reply Trasmatta 12 hours agoparentTherapy helps. Building a stronger sense of self and with it, more internal boundaries between your thoughts and beliefs and those of others. I'm this way as well, and it's like your emotions are totally porous, absorbing everything from those around you. It's a blessing and a curse. Generally stems from a childhood where you had to be very in tune with the emotions of your caregiver in order to stay safe. reply hi_hi 11 hours agorootparentThank you. This lines up with my experiences, which I never knew were connected. Prefer 1:1, alcoholic mum growing up who had good days and bad days. I could tell which it would be from the \"vibe\" when I walked through the door after school before seeing anyone. reply sethammons 11 hours agorootparentprevOof. Being in tune with your caregiver hits hard. My mom was a manic, bi-polar, depressive person also suffering from schizophrenia. I learned to read some situations like you mention but toss in some randomness so it gets real dicey. reply jjj123 11 hours agoparentprevWow I’ve always felt much more comfortable in 1:1 situations than group situations, but I never framed it the way you have here. Your comment really resonates, thank you! reply patch_collector 5 hours agoprevIf the author reads this, I'd like to suggest a change in font. At certain scales, the website's font puts emphasis on the cross-bar in the letter 'e', and the letter 'g'. It's incredibly distracting, and only seems to happen at certain scales, as I could 'fix' it by increasing/decreasing the font size. I'd message this directly, but she doesn't provide a method of contact on the site (reasonable). reply rocqua 10 hours agoprevI loved the article, but something about it felt off. The content (good) didn't match what I would expect from the style. The writing style reminde me of a mix off business advice and aggrandizing self-help. My expectation with that is sweeping generalizations, just-so annecdotes, and not saying very much, whilst not backing up what you are saying with sound reasoning either. Somehow this article had that writing style, without those problems. It made it a rather dissonant experience, because I was looking for the catch, what I was being sold, the anecdote that is almost certainly a lie, and the overly strong conclusion. But that never came, and instead I find myself believing. And yet, the dissonance remains. I have a little worry that the swindle was just better this time. It's a weird feeling, and not one I had before. reply roshankhan28 7 hours agoprevi prefer to be a cat. if that makes sense. reply FollowingTheDao 6 hours agoparentHA! Makes sense to me! reply dmoy 13 hours agoprev> Make sure you’re squarely facing the person Awww shit that's gonna be hard for my inner Minnesotan. All that deep listening stuff needs to be done at a 135-165 degree angle, so you're both vaguely looking in the same-ish direction but can make occasional side glance eye contact reply vvanders 11 hours agoparentThis is giving me strong How to Talk Minnesotan vibes[1] (in a good way :D). [edit] 10:50! https://www.tptoriginals.org/how-to-talk-minnesotan/ [1] https://en.wikipedia.org/wiki/How_to_Talk_Minnesotan reply 01HNNWZ0MV43FF 12 hours agoparentprevRight? The only way some of us can discuss emotion is if we're pretending we're manning the Wall against ice zombies reply osigurdson 10 hours agoprevI just hate this kind of stuff. Good for you if you can create a consulting business out of stating the obvious I suppose. It is a drain on the economy however. reply marmaduke 9 hours agoparent> stating the obvious What is obvious is wildly different among people. For instance, it was obvious to me that the article was sharing some ideas freely, and those ideas are ones which are not obvious to everyone in the workplace. “It’s obvious” is a rhetoric which puts the person who’s not getting it on defense. Usually it’s pretty counterproductive too. reply awelxtr 6 hours agoparentprev> stating the obvious If interpersonal relationships were obvious we would not need abuse laws nor CPS. reply lynx23 6 hours agorootparentThey are usually obvious, except for psychopaths and people struggling with autism. The latter is rather prominent in tech, so we see more of these issues then outside of tech. reply awelxtr 4 hours agorootparentI had selfsteem problems growing up derived from my narcissistic mother. What am I then? Autistic or psycopath? reply kettleballroll 9 hours agoparentprev> Good for you if you can create a consulting business out of stating the obvious I suppose. In my experience, tech problems are a lot easier to solve than people problems, and a lot of things that don't go well in a project turn out to be people problems. E.g. here are a few issues I encountered in my current project at work in the last month: \"their framework makes assumptions that don't apply to our code, so we reimplemented the metrics instead of trying to integrate their version\" or \"the data was labelled wrongly, so we had to work around that\", or \"this coding convention is slowing us down\". Once I tried digging down, it turns out they were all people problems in disguise, and they could all be solved by \"stating the obvious\". Do you never encounter issues on team / across teams, where in the end it turns out a lot of issues are just people not talking to each other or misunderstanding each other? If things are too hairy, I can definitely see the value in an external consultant helping disentangle these sort of problems. reply hackit2 9 hours agorootparent> where in the end it turns out a lot of issues are just people not talking to each other or misunderstanding each other? What makes a huge difference is how you frame your interactions. If you extrinsic your interactions you're all-ways going to come away with a lack of agency, stress and/or frustration. if you intrinsic your interactions, you're going to be more in control, accountable, and over-all indifferent to other people. For example well at work, I'm being compensated to participate in the organization to work towards its goals, wants, needs and/or desires. Those have nothing to do with me, nor do i really care about it. I will engage with people at work, colleagues and managers, how-ever if later they don't volunteer engage back - such as being cordial, I don't re-engage because I consider it to be intrusive. Now let say you have co-workers who have a glaring communication problem. It it pretty obviously that you can do anything about it. So you engage their manager of lack of communication, and lack of professionalism. If their manager doesn't want to rectify the problem then you communicate it with your manager but at the same time be professional about it that you do not have the capacity to deliver on the deliverables within your current roles. This opens the door to opening a dialog to reviewing your remuneration or compensation package that includes the new responsibilities. reply osigurdson 5 hours agorootparentI bet these things would have worked themselves out on their own. The main thing is to have an ultra crisp vision. reply watwut 6 hours agorootparentprevIn my experience, these dont help to solve people problems. They are motivational feel good advice. In practice, they will exhaust you and dont work in the long term. And what they actually make is to create situation in which your needs and things you want to achieve are less and less met. Or just make you look unauthentic to others - they will cease to believe your projected emotions. reply awelxtr 6 hours agorootparentMost self-help is easy to write and difficult to apply, specially if it's written in a generic matter like in a book or in a blog post. This doesn't mean it can't be helpful. I know because some self help knowledge in the past has helped me. reply watwut 6 hours agorootparentIt is not just difficult to apply. If you actually try to apply it and do, it setups you for fail. Because it is feel good instead of real and omits real world constraints. Take this article - sometimes, fairly often, the \"bad vibes\" are a correct observation of the other persons attitude, opinions and intentions. Sometimes people are in fact hostile or cold, whether for personal, professional, fair or unfair reasons. This part of the advice, if you apply it, is making you helpless and powerless. And conversely, it over time make you come across as manipulative person, because that is what you do majority of the time. reply trabant00 10 hours agoprevThere's an unspoken premise here and I'm going to question it. Avoiding tension, conflict, hard words and other things of the sort is not always the right choice. Sometimes letting conflicts play out gets you the best outcome with the least amount of suffering. Just like ripping off a band-aid. There's plenty of times when wining a conflict is far better than avoiding it. And I see articles like this, books like Nonviolent Communication, ideas like \"emotional intelligence\" (check it out, no such thing exists) - as misguided as it always puts you in the defensive/de-escalating role even when you might be better served by letting things play out or even attacking, baiting your opponent into attacking, etc. Violence is sometimes the right answer. When to apply it and when to avoid it is the hard question. But we didn't evolve an amygdala for nothing, and especially not for a \"coach for leaders\" (what the hell is that?) to tell us to always ignore it as an unquestioned premise for a promotional blog post. Because leaders should not always shy away from conflict, that much should be pretty crystal clear. reply FollowingTheDao 6 hours agoparentYes, agree. This is all a continuation of the Positivity Cult. Anger is a method of communication that is greater than words, and it puts the explanation point at the end of some of the most important statements. Just read this: I need help. I need help! reply b3lvedere 6 hours agorootparenthttps://www.youtube.com/watch?v=vivEzQUGHOQ reply red_admiral 4 hours agoprevBack in the days of slatestarcodex, the comment policy [1] was you can comment if your post is at least two of these three things: true, necessary, or kind. This post is all three: what they're describing is true (these dynamics in meetings do exist, very often), it is kind (in the sense they're giving you a skill to help both yourself and others), and I'll give it necessary in the sense it's used in the original definition (if you want to get ahead in an organization with a nontrivial amount of internal politics - which is most places - you need to have at least some of this skill). And yet, something about this post gives me \"weird vibes\". Basically, with a bit of sarcasm you could sum it up as \"DON'T BE AUTISTIC\", and if you are then at least get therapy until you can act normal. When the author says \"We [humans] are wired to spidey sense this [vibes] stuff\", it turns out some humans are above and some below the mean in this skill distribution. [2] And sometimes, in a meeting to decide about how you're going to set up your database sharding, it helps the business' bottom line if you pay more attention to the database specialist than the soft-eye-contact specialist. (Don't you want to hire people who are good at both? Yes, but unless you're really, really lucky, you're going to hit Berkson's paradox [3]. And then if you want your databases to run smoothly, you're going to have to compromise.) [1] https://slatestarcodex.com/2014/03/02/the-comment-policy-is-... [2] However, the latest research suggest that autistic people are perfectly wired to read the room and sense vibes if the room is full of other autistic people. It's just one autist in a room of neurotypicals, or vice versa, that doesn't go too well. [3] https://www.allendowney.com/blog/2021/04/07/berkson-goes-to-... reply fnord77 12 hours agoprevthings you can't really do on zoom meetings... reply Manfred 7 hours agoprevI support the goals of the article and I understand that social interaction doesn't come natural to all people, but if someone would lean in an nod to me like described in the second part I would freak out because that feels like sociopathic behavior. reply lynx23 9 hours agoprevI am going to be downvoted to hell for this, but... After reading halfway through the article, I had to check the gender of the author. Because, I feel, this is a rather female POV. A lot of what she says feels touchy-feely to me and doesnt resonate with me at all. Maybe because I am way more inerested in the topic of the meeting then the personal feelings and emotions of the participants. To the point where I might noticed them, but I they mostly dont concern me at all. reply jdthedisciple 8 hours agoparentNope, you are spot on. I too noticed pretty quickly that this must be female POV. You can generally tell even just by the word choices (\"spidey senses\" American women seem to love that phrase for some reason, \"super \", \"awry\", \"weird vibes\", ...) Another instant give-away was \"now we've got a compounding situation\" - quite a feminine phrasing. Not judging, I mean it sounds almost cute even. Finally, her idea of \"facing each other squarely\": a total no-no for men (way too much potenial energy, like two massive electron beams opposing each other), but OK for women. reply detourdog 7 hours agorootparentYour comment makes me question my masculinity. I would say the desire to communicate and the feelings expressed were feminine. Noee of the phrases you mentioned did I see as clues to gender. reply jdthedisciple 7 hours agorootparentTo be clear I didn't say men never use any of these words individually (except for \"spidey senses\" I suppose). Just that their frequent and combined usage in this particular article made me almost hear her voice in my head, including the female cadence and intonation... reply lynx23 7 hours agorootparentprev\"Weird vibes\" is what finally made me check the author name ... I don't even know what \"spidey sense\" should mean. reply AStonesThrow 11 hours agoprevnext [3 more] [flagged] ulf-77723 11 hours agoparentWhen I read the article, I assumed that it was written by a guy first - when I saved it to my favorites I noticed that Lara is a woman. I do work most of my time in my home office, yet I do spend also some time in virtual meetings. The things Lara describes are even more important if you are not present in person, from my point of view. Harder to sense some of those feelings and emotions if you are not in the same room, but possible. reply voiper1 8 hours agoparentprevWow, I read your comment, and the response, then read it again, and I still thought you were intending to be offensive that the author was so bad they couldn't possibly collaborate with people. Somehow I missed the simpler idea of \"Hah, in 2024 I have no face to face with colleagues in anymore!\" reply from-nibly 13 hours agoprev> Noticing a change in someone’s behavior Well I guess I'll just excuse my ADHD having self outa this one. reply jawon 12 hours agoparentInteresting. I have the exact opposite issue. Hypervigilance and all that. reply tgtweak 5 hours agoprev [–] Generally speaking terrible advice for anyone in such a situation in a professional group setting. If you can sense that someone is tense or \"off vibe\" in a group meeting, you should be able to reasonably determine why. If it's not immediately evident and they are not alluding to it in the meeting - then you should table the discussion until you are able to chat 1:1 with the person. Not downplaying any of the strategies for being chipper and staying positive and being a good vibe... which seems obvious... but to push it back on the \"bad viber\" and dice roll on whether you'll be charismatic enough to do it without causing even more bad/awkward vibes, I think is unnecessarily risky. I've been in many meetings where someone seemed \"off\" but after conferring with others more familiar with the situation found it it's quite usual and not a sign of anything wrong. Had someone intervened there and tried to \"discover\" the case of the bad vibes, it would have amounted to \"why are you like this\" which is not the kind of thermometer input required. Likewise if someone is being openly confrontational in the meeting because they feel strongly about something, the right course is for someone else to step up and discuss it without any ambiguity or levity - ruling out irrelevant emotions not related to the discussion - if the stakes were high enough to merit losing face in a meeting, they should generally be high enough to discuss and resolve. My experience has been, in many board meetings and conference rooms with C-levels, that the norm for these discussions is someone \"off vibe\" and it's rarely koombayah when there is something at stake being discussed. Bringing unnecessary levity to a serious and often uncomfortable meeting is taken as a bit of an insult to the topic or the opinions being tabled. You can read accounts of an Jeff Besoz or Steve Jobs executive meeting and glare into this first hand. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Humans often subconsciously mirror the moods of those around them, which can lead to misunderstandings and a tense work environment.",
      "To break this cycle, aim to be a \"thermostat\" by setting a positive tone rather than a \"thermometer\" that merely reflects the mood.",
      "Tips include acknowledging energy shifts, asking open questions, using calm body language, offering breaks, and owning your role in any tension."
    ],
    "commentSummary": [
      "The article emphasizes the importance of emotional intelligence in professional settings, advocating for regulating and stabilizing the emotional climate rather than merely reacting to it.",
      "Practical advice includes making eye contact, leaning in, and using specific phrases to demonstrate understanding and commitment.",
      "Some skepticism exists regarding the feasibility and potential exhaustion of maintaining constant emotional control, with concerns about the authenticity and challenges in various contexts like virtual meetings."
    ],
    "points": 253,
    "commentCount": 118,
    "retryCount": 0,
    "time": 1726098625
  },
  {
    "id": 41518600,
    "title": "Why Haskell?",
    "originLink": "https://www.gtf.io/musings/why-haskell",
    "originBody": "Why Haskell? 10 Sep, 2024, 6591 words Unlearning and relearning A small note on syntax Make fewer mistakes Buthas , too! The things which make you more productive Reason about your programmes more easily Epilogue “Impractical”, “academic”, “niche”. These are a few of the reactions I get when someone discovers that my favourite programming language is Haskell, and not only my favourite in some sort of intellectually-masturbatory way, but favourite for building things, real things, mostly involving web servers. Hobby projects would be one thing, but it gets worse: I have actual teams at Converge working in Haskell, too. I find this reaction quite curious: not only can any problem suitable to one general-purpose programming language be tackled in another, but a lot of the new features we see making their way into programming languages like Python, Rust, and Typescript, are either inspired by, or at least more robustly implemented in, Haskell. It seems to me that part of this response is a version of “choose boring technology” (although Haskell is far older than most of the most popular programming languages) twisted to suit another pernicious ideology: that programming is not maths, and that anything that smells of maths should be excised. This comes up in all sorts of unlikely places in which it would be quite awkward to have to take my interlocutors through all the reasons I think Haskell is probably the best choice for whatever computational problems they are trying to solve themselves (e.g. dinner parties, the pub, etc.) and thus I find myself writing this apologia. Indeed the remainder of this essay will consist of my attempt to reason around why I think Haskell is probably the best choice1 for most programmers2, especially if one cares about being able to productively write robust software, and even more so if one wants to have fun while doing it (which is a frequently underrated aspect of writing software). All mainstream, general purpose programming languages are (basically) Turing-complete, and therefore any programme you can write in one you can, in fact, write in another. There is a computational equivalence between them. The main differences are instead in the expressiveness of the languages, the guardrails they give you, and their performance characteristics (although this is possibly more of a runtime/compiler implementation question). I think that the things that make Haskell great (meaning both more productive and more fun) can be grouped as follows: the things that stop you making mistakes; the things that make you more productive; and the things that help you reason better about your programmes. Unlearning and relearning The first thing to say here is that most programmers in the 2020s have been brought up in some sort of imperative3 paradigm. As a result, the learning curve for a pure, functional language like Haskell will be steep. There are two aspects to this: one is the Haskell language itself which, if you constrain yourself to a simple subset of it, is actually quite easy to learn; and the second is functional programming, which requires a total shift in how the programmer approaches constructing a programme. This process of unlearning and relearning is incredibly helpful and will make one a better programmer, regardless as to whether one uses Haskell thenceforth. As Alan Perlis writes: A language that doesn’t affect the way you think about programming is not worth knowing. ~ Perlisism #19 4 A small note on syntax In the subsequent sections there will be simple snippets of Haskell. Since the syntax is quite distant from C-like syntax with which many readers will be familiar, here is a small guide: :: denotes a type signature (so myThing :: String says I have a name “myThing” and its value is of type String). function calls do not use parentheses, you simply put the arguments, space-separated, after the function name. There are good reasons for this, but they’re beyond the scope of this explainer (so where in one language you may have doSomething(withThis, withThat) in Haskell you have doSomething withThis withThat). lower-case letters in type-signatures are type-variables, and just represent any type (so head :: [a] -> a just takes a list of any type a and returns a single value of the same type a). you will see two types of “forward” arrows: -> and =>. A single arrow -> is used to describe the type of a function: add1 :: Int -> Int describes a function which takes an integer and returns an integer. A double arrow => describes constraints on the type variables used, and always come first: add1 :: Num a => a -> a describes a function which takes any type a which satisfies Num a, and returns a value of the same type. comments start with --. return does not mean what you think it means, it’s just a regular function. do is syntactic sugar allowing you to write things that “look” imperative. There are various ways of assigning values to local names (“variables”) which differ depending on the context. So you can recognise them they either take the form let x =inor x . Otherwise the syntax should be fairly easy to parse, if not for a detailed understanding of every aspect, at least to a sufficient level to get the gist of what I am trying to convey. Make fewer mistakes In many languages, the way one tries to make sure one’s code is “correct” (or, at least, will do the right thing in most circumstances) is through a large number of test cases, some of which may be automated, and some of which may be manual. Two aspects of Haskell drastically reduce the test-case-writing burden typical in other languages: one is the type system and the other is pure functional programming. Haskell’s type system is very strong, which is to say that it makes very specific guarantees about the programme, and enforces those guarantees very strictly. The concomitant expressiveness of the language gives the programmer the tools to more precisely and simply express the meaning of the programme within its domain, as well as the general domain of programming. These two properties of the type system, together, reduce the space of possible mistakes, resulting in a more correct programme with much less effort. So far, so abstract. Some concrete features of the type system that reduce the “error surface” of your programme are: no nullable types; the ability to represent “failable” computations; pattern matching and completeness checks; and the avoidance of “primitive obsession” for free. Let’s take a look at each of those. The availability of a null (or nil or none) value which can inhabit any (or the majority) of types in a language is often viewed as a convenience, but in practice it has a huge cost. In a language in which one can use such null values, the programmer can never know if the value they are handling is actually of the expected type or if it is null, and therefore is required to check wherever this value is consumed. Programmers can forget things, and the fact that the null value can inhabit many types means that the type system does not help prevent this, leading to errors of the sort “undefined is not a function” or “NoneType object has no attribute ”. These, however, are runtime errors, which both means that the programme has failed in its principal task and also that the errors are harder to find as they occur in the wild. Haskell does not have null values. You can define them in a particular data type (for example the Maybe type, which we will come onto shortly) but you have to explicitly define them and explicitly handle them. As such, the error surface available due to this flaw in language design is eliminated, and the programmer no longer has to think about it. Null values, however, are often used to represent a “failed” computation. For example, what if you are getting the head of an empty list, how do you represent the result? In languages with null values, such functions will often return null in these circumstances. This is a specific case of the more general question of how to deal with computations which may fail. There are many examples: if you are parsing some user input and that input is malformed, this failure to parse is a valid state of your programme, and therefore you need some way to represent it. Similarly, network requests may time out, solvers may fail to find a solution, users may cancel actions, and so on. There are two common solutions, null values (which we have mentioned) and exception handling. Both of these solutions cause a new problem for the programmer: you have to remember to handle them, in the case of exceptions at the call site rather than where you consume the value as with null, and nothing in the type system is going to prevent you forgetting. Haskell solves the problem of the representation of computations which may fail very differently: explicitly through the type system. There are types in Haskell to represent a computation which may fail, and because this is done in the type system, these are first-class entities and you can pass around your computation-result-which-may-or-may-not-be-a-failure as you like. When it comes to consuming the result of that computation, the type system forces you to reckon with the fact that there may be no result. This prevents a whole class of runtime errors without the mental burden of keeping track of values which may be present or which functions might throw an exception somewhere. The two most common of these types are Maybe and Either. Maybe represents a computation which may or may not have a result. For example, if you want to get the first element of a list, but you do not know if the list is empty, then you may want to specify that your head function can return either a result or Nothing. Unlike null values, however, you cannot just pretend that the function must have returned a result, as the following snippet should demonstrate: safeHead :: [a] -> Maybe a -- the implementation isn't important here, but I'm including it -- because it is simple and, for the curious, might be helpful safeHead [] = Nothing safeHead (x : _) = Just x myFavouriteThings = [\"raindrops on roses\", \"whiskers on kittens\"] emptyList = [] faveThing = safeHead myFavouriteThings -- ^ but what is the type of this thing? -- It's not a string, it's `Maybe String` -- and the value is, in fact, `Just \"raindrops on roses\"` something = safeHead emptyList -- ^ and what's the type of this thing? -- again, it's a `Maybe String`, but in this -- case the value is `Nothing` because the list -- has no first element! -- so how can we use the value we have computed? printTheFirstThing :: [String] -> IO () printTheFirstThing myList = case safeHead myList of Just something -> putStrLn something Nothing -> putStrLn \"You don't have any favourite things? How sad.\" In this example you can see that when consuming the result of a computation that might fail, you have to explicitly handle the failure case. There are many ways of doing this, and the pattern matching ( case x of ...) above is just one to which we will come shortly. Maybe can also be used when you might want a nullable field of a data structure. This is a specific case of a computation which may fail, but is often thought of as distinct. Here is how this would look in Haskell: data Person = Person { name :: String, dob :: Day, favouriteThing :: Maybe String } As before, Haskell’s type system will not let you fail to handle the case that favouriteThing might be an empty value, so you will not end up with a runtime error as you might in a language in which you could forget to do so. Maybe is useful in these situations in which the failure condition is obvious, but it doesn’t give you much resolution on why the computation failed, it only tells you that it has failed. By contrast, an Either a b can contain two values, Left a or Right b. By convention, Left contains a failure value, whereas Right contains a success value, so the type is often given as Either e a where e is for “error” and a is just the result type. One way in which this could be used is in parsing or validating some user input, in which you may want to tell the user more than just that what they gave you is invalid, but rather in what way it is invalid. To that end you could have a validate function that looked like this: validateAddress :: String -> Either AddressParseError ValidAddress This gives you the ability to return more helpful errors to the user, which are an expected path in your programme, but it prevents you from failing to handle the failure case, or from treating the failure case like a success case accidentally. To be clear, this means that we no longer treat known error states as exceptions by throwing them up the call stack 5, and instead we treat them as potential values for the type of our expression. In turn, this means that we now can have a total description of all the failure modes from the point of the function call down the stack. Consider these two snippets of code: def do_something(): result = get_a_result() if result != \"a result\": raise InvalidResultError(result) return 42 doSomething :: Either InvalidResultError Int doSomething = let result = getResult in if result /= \"a result\" then Left (InvalidResultError result) else Right 42 In the first snippet, we have no idea what possible exceptions may be raised by do_something, partly because we have no way of knowing what exceptions may be raised by get_a_result. By contrast, in the second snippet, we know all of the possible failure states immediately, because they are captured in the type system. We can generalise this idea of being forced to handle the failure cases by saying that Haskell makes us write total functions rather than partial functions. This means that we have to handle the entire input domain rather than only part of the input domain, otherwise the compiler will complain at us and, sometimes, point-blank refuse to give us a programme. The easiest way to see how this works is to look at how pattern matching is done in Haskell, using a basic programme which helps us organise our evenings given a chosen option. Instead of implementing the entire programme, here is an extract to illustrate the use of pattern matching. data Option = NightInRestaurant VenueNameTheatre VenueName EventName data OrganiserResult = SuccessNeedsSeatChoice [Seat]Failure Reason organiseMyEvening :: Option -> IO OrganiserResult organiseMyEvening NightIn = do cancelAllPlans return Success organiseMyEvening (Restaurant venue) = attemptBooking venue organiseMyEvening (Theatre venue event) = do availableSeatsreturn (Failure (Reason \"there are no seats available, sorry :(\")) seats -> return (NeedsSeatChoice seats) In the above example, if we were to add an additional option for what we may want to do with our evening, like going to the cinema, and forget to update the organiseMyEvening function accordingly, the compiler would complain to us until we fix it. Without this completeness check in the type system, we could end up with a runtime error, but with this type of check, we just do not have to worry about whether we have remembered to update all the places in which a given value is used. The final major way in which Haskell’s type system makes it easy for us to avoid common errors when programming is related to how easy it is to avoid “primitive obsession”. There is a hint in our evening-organising snippet above: our Restaurant and Theatre constructors take a VenueName and EventName. These could, naturally, be represented as plain old strings, and in many languages they are, but Haskell gives us a very simple, zero-cost way of representing them as something with more semantic value, more meaning, than just a string. It may not be obvious why this is a problem worth solving, however. Let’s imagine we represented these as plain old strings, so we would have something like this: data Option = NightInRestaurant StringTheatre String String -- venue name and event name respectively checkForSeats :: String -> String -> IO [Seat] This is probably ok the first time you write it, although you will need comments, as above, in order to remind yourself which value is which. This is where we come to our first annoyance (although not yet a problem) – the type system doesn’t help us remember what is what, we have to rely on arbitrary comments or documentation (or perhaps variable names) to remember, which is a lot of overhead. The problem comes, however, when using these values, such as in checkForSeats. We could easily mix up the venue name and event name, and we would always return zero seats (because we probably don’t know a theatre called King Lear in London where they are playing Shakespeare’s masterful The National Theatre). This is erroneous behaviour, but is easily done, and the type system will not help us out. “Primitive obsession” is the use of primitives (strings, numbers, booleans, etc.) to represent data, instead of types with more semantic value. The solution is to encode your domain in your type system, which prevents such errors. This can be very cumbersome in many imperative languages, but in Haskell we can simply wrap a value in a newtype and the type system suddenly stops us falling into the trap of using the wrong value. Therefore our code above becomes: newtype VenueName = VenueName String newtype EventName = EventName String data Option = NightInRestaurant VenueNameTheatre VenueName EventName checkForSeats :: VenueName -> EventName -> IO [Seat] Above it is written that this is a “zero-cost” method, which means that unlike the normal way of creating a data structure to wrap around some values, newtypes have exactly the same representation in memory as the type they wrap (with the result that they can only wrap a single type), they therefore only exist at the level of the type system, but have no impact on your programme otherwise. Thus far we have discussed four features of the type system which help us as programmers to write correct code with minimal mental overhead: the lack of nullable types, representations of “failable” computations, pattern matching and completeness checks, and the avoidance of “primitive obsession”. Other languages have some of these features (notably Rust, whose type system was inspired by Haskell’s), but most of these other languages lack the second pillar: pure functional programming. There are two aspects of a pure functional language which help us avoid common errors: immutable data and explicit side-effects (which, together, give us purity and referential transparency). Almost all data in Haskell are immutable. This means that a whole class of errors like data races, or objects changing between write and read, just do not exist. In single-threaded code this is great because you don’t have to think about mutating state anywhere, you just use things like folds or traversals to achieve your goals, but where this really shines is in concurrent code. For concurrent Haskell you do not have to worry about mutices and locks because your data can’t be mutated anyway. That means that if you want to parallelise a computation, you just fork it into different threads and wait for them all to come back without all of the hairy bugs of multi-threaded computations. Even when you do require some sort of shared, mutable state between your threads, the way this is constructed in Haskell (e.g. in the STM library) still avoids the problems solved by locks and mutices in other languages. Immutability gets you halfway towards eliminating the sorts of errors found in imperative languages, but purity will get us the rest of the way. Haskell functions are pure, in the sense that they do not permit any side-effects, nor do they rely on anything except for the arguments passed into them. There are ways to encode side-effects, for, at some point, any useful programme needs to at least perform some I/O, and there are ways to include things in functions which are not directly passed as arguments (implicit parameters), but the way Haskell is constructed means that these ways do not violate the purity of the language because we use monads to encode these things. Monads: at first they throw every novice Haskeller into disarray, and then nearly everyone feels the need to write their own monad tutorial. Exactly what monads are and why they are useful is beyond the scope of what we want to talk about here, but the specific benefit we are looking at is how this allows us to encode side-effects and why that is going to help you avoid mistakes when programming. Let’s look at some functions for a basic online community: data Response = SuccessFailure FailureReason sendGreetings :: User -> IO Response updateUser :: UserId -> User -> IO Response findNewestUser :: [User] -> Maybe User In many imperative languages, the activity of finding the newest user and sending them some sort of greeting might all be done in one function, or a set of deeply nested functions. There would be nothing to stop you, however, making database calls, sending emails, or doing anything else inside the simple findNewestUser function. This can be a nightmare for tracking down bugs and performance issues, as well as preventing tight-coupling between functions. The functions above take two forms: findNewestUser returns something by now familiar to us, Maybe User – if there is a newest user, it will return it, otherwise it will return Nothing. The other two functions return something we have not yet seen: IO Response. IO, like Maybe wraps another type (in this case: Response) but instead of representing a “failable” computation as Maybe does, it represents any context in which you are permitted to perform I/O actions (like talking to your database or sending emails, as our cases are above). It is not possible to perform I/O outside of the IO monad – your code will not compile – and, furthermore, I/O “colours” all the functions which call it, because if you are calling something which returns IO, then you have to be returning IO as well. This might look like a lot of bureaucracy, but it actually does two very helpful things: firstly, it immediately tells the programmer “hey, this function performs side-effects in I/O” which means that they don’t have to read the code in order to understand what it does, just the type signature; secondly, it means that you cannot accidentally perform I/O in a function you thought was pure – this, in itself, eliminates whole classes of bugs in which one may think one understands all the dependencies of a function, but actually something is affecting it which you did not realise, because it can perform side-effects. This is only partially satisfying, however, as wrapping everything that performs side-effects in IO is a bit imprecise in a similar way in which using primitive types for values with higher-level semantics in the domain is also imprecise, and it can cause similar classes of error: there is nothing to say “in this function you can send emails, but you can’t write to the database.” The type-system has helped you a little bit but stopped short of the guardrail we have come to expect by now. Thankfully, due to two additional language features: namely ad hoc polymorphisms and typeclasses, we can exactly encode the effects we want a function to be permitted to perform, and make it impossible to perform any others. Let’s modify our example to take advantage of this, noting that class X a where means that we are declaring a class X of types with some associated functions for which we have to write concrete implementations. This is similar to interfaces in some languages, or traits in Rust (which were based on Haskell’s typeclasses). In this example, m is just a type variable representing a “2nd order”6 type (e.g. IO or Maybe). data Response = SuccessFailure FailureReason class CanReadUsers m where getUsers :: m (Either FailureReason [User]) class CanWriteUsers m where updateUser :: UserId -> User -> m Response class CanSendEmails m where sendEmail :: EmailAddress -> Subject -> Body -> m Response findNewestUser :: [User] -> Maybe User sendGreetings :: CanSendEmails m => User -> m Response greetNewestUser :: ( CanReadUsers m, CanWriteUsers m, CanSendEmails m ) => m Response We have introduced a new function here greetNewestUser to illustrate how we can compose these constraints on what we are able to do. Our implementation of this would do something like: find all the users, filter for the newest one, send an email, and mark the user as having been greeted. We have encoded these capabilities at the type level for greetNewestUser, whereas we have not for sendGreetings, so it would be impossible, in fact, for sendGreetings to fetch users from the database or to accidentally update the user information in the database7. It can only send emails. To finish this example off, let’s see how the implementations of these functions might look: -- these would be defined elsewhere, but just so you know the types joinDate :: User -> Day emailAddress :: User -> EmailAddress setAlreadyGreeted :: User -> User hasBeenGreeted :: User -> Bool userId :: User -> UserId findNewestUser users = safeHead (sortOn joinDate users) sendGreetings user = let subject = Subject \"Welcome to the club!\" body = Body \"Remember: don't stare at the guests...\" in sendEmail (emailAddress user) subject body greetNewestUser = do fetchResultreturn (Failure err) Right users -> case findNewestUser users of Nothing -> return (Failure NoUsers) Just user -> if hasBeenGreeted user then return (Failure AlreadyGreetedUser) else do sendGreetings user let newUserData = setAlreadyGreeted user in updateUser (userId user) newUserData While the exact syntax may be unfamiliar, everything in this section has been building up to this point: we represent “failable” computations with data types which encapsulate that they can fail and how they can fail; we use semantically meaningful types to describe our data, rather than primitives; we explicitly handle failure cases rather than being allowed to forget about them; we cannot mutate state so we create new copies of our data with the requisite updates; and we explicitly encode the side-effects we want to perform, rather than just firing them off willy-nilly. That rounds off the section about the guardrails Haskell puts in place for you as a programmer, both through the strength of its type system and the purity and referential transparency of the language itself. Far from being an imposition on the programmer, this is incredibly freeing as it allows you to spend your mental energy describing your problem and thereby solving it, not worrying about keeping track of all the ways in which your programme could fail. Buthas , too! Some of the features of Haskell above exist, or look like they exist, in other languages. Without trying to talk about every possible language, we can look at some of the common patterns and how they differ, or do not, from those in Haskell. Pattern matching, for example, has been introduced into many languages. Some of those have the same characteristics as Haskell, like Rust’s pattern matching, which is exhaustive and enforced by the compiler, whereas some are quite different, especially in gradually-typed languages like Typescript and Python, where there is no guarantee that this sort of safety permeates the codebase, and there are often escape-hatches, because you are using optional tools external to the built-in toolchain. Very few languages make use of higher-order types like Either and Maybe to represent computations which may fail, but Rust is a notable exception which, like Haskell, strongly encourages representing failure in this way. Subclassing is commonly used in some languages to make it “easy” to avoid primitive obsession, but this is not as strict as Haskell’s newtypes. Python, for example, has a NewType construction, but it has two weaknesses common to this type of implementation: the first is that subclassing means that our VenueName and EventName types can be passed to functions expecting String, because they are not treated as completely different types, and the second is that, unlike in Haskell, you cannot hide the constructors of these types, which means there are certain patterns you cannot fully implement like the parsing pattern (as opposed to validating)8. Finally, while some libraries exist in other languages in order to isolate and control side-effects9, they are not enforced as part of the language in the same way, because this would require purity to be built into the language itself. The things which make you more productive Providing guardrails, for all the reasons listed in the previous section, is a very useful feature of a language, but that alone might make for a very slow experience of building programmes. Haskell has several properties which actually make it more productive to construct such programmes, especially as those programmes grow in complexity (or sheer size). As before, these properties derive from the two key characteristics of the language10: the strength of the type-system and the pure-functional semantics of the language. These two together give us code which is highly declarative, and therefore easily and unambiguously manipulable, as well as a tendency towards heavy concept and code re-use. Why are these useful? Starting with the former: if our programme is declarative rather than imperative, we can easily understand it ourselves, as well as simply generate other code from it (or documentation), and refactoring becomes a “fearless” activity. Taking the latter, this means that we can “discover” a set of core concepts and continue to build upon them, instead of having to learn disjoint sets of concepts for each domain or library one uses. It can be hard to explain just how radically these things transform the way one constructs programmes without experiencing them, but to take a small example, the Haskell ecosystem has a tool called “Hoogle” which allows one to search for functions by type signature. Not only by full type signature with concrete types, but, even by partial type signature with type variables instead of actual types. That means that, instead of searching for something which applies a function to each string in a list of strings ((String -> String) -> [String] -> [String]), one can instead search for something which applies a function to a list of things, returning a list of the results: ((a -> b) -> [a] -> [b]). You can even get the arguments the wrong way around, and Hoogle will still find you the right functions, so [a] -> (a -> b) -> [b] will give you the same answers (sorted differently) to (a -> b) -> [a] -> [b]! This works so well because Haskell’s semantics, standard library, and ecosystem all rely heavily on concept re-use. Almost every library builds upon the core set of concepts11. This means that if you are wondering how to do something, and you are faced with one library or set of data types, you can probably search for the general pattern of what you want to achieve and you will get what you want. Almost no other ecosystem12 has something comparable to this. In order to flesh out this idea of concept generalisation and re-use, let’s consider two examples: functors and monoids. Before we get there, we will start with lists. A list is Haskell looks like this myList = [1, 2, 3] :: [Int]. You can do various things with lists, like apply a function to each member of the list (map) in order to obtain a new list, or stitch two lists together ([1, 2][3, 4]). In this sense, we have described two properties of lists which we can generalise: a list is a container over which you can apply a function (a “functor”), and a list is an object which has a binary combining operator with an identity value [] (a “monoid”). Lots of other structures exhibit these properties, for example a list is a functor, but so is a Maybe or an Either, or even a parser! As a result, if you understand the core concept of functors, you have a set of tools which can apply to all sorts of other data structures which you use day-to-day, but with no extra overhead: fmap (+ 2) [1, 2, 3] -- [3, 4, 5] fmap (+ 2) (Just 2) -- Just 4 fmap (+ 2) (Right 5) -- Right 7 number = fmap (+ 2) decimal :: Parser Int -- parses a string representation of a decimal, adding 2 to it, but -- the nice thing here is that we don't have to explicitly handle the -- failure case with our `+ 2` function! parseMaybe number \"4\" -- Just 6 Similarly, there are plenty of monoids lurking about. Obvious examples might be strings, but then, for example, the Lucid library for writing HTML represents HTML as monoids, which allows you to compose them with the same tools you would use for any other monoid. Once again, you learn a single core concept, and it becomes applicable across a large part of the ecosystem. [1, 2][3, 4] -- [1, 2, 3, 4] \"hello\"\" \"\"world\" -- \"hello world\" myIntro = p_ (i_ \"Don't \"b_ \"panic\") -- Don't panic You can even use this in your own code, and can write simple instances for your own data structures. This vastly reduces the amount of specialised code you have to write – instead, you can simply re-use code and concepts from elsewhere, whether the standard library or an extension to those concepts like bifunctors. In short: Haskell’s semantics and standard library encourage generalised concepts which, in turn, heavily promote both concept and code reuse, and this has driven the development of the ecosystem in a similar direction. That re-use means that the programmer need only discover the core concepts once, rather than for each library, providing an accelerating rate of learning and a much more efficient use of code. The final productivity boost to discuss here is “fearless refactoring”, a term often thrown about in the Haskell community, but what does it actually mean? The essential point here is that the intransigence of the compiler makes it a useful ally when refactoring code. In languages with a more forgiving compiler or weaker type-system, refactoring code can introduce new bugs which are only discovered at runtime. When refactoring Haskell, because the type-system gives you the power to express your programme domain correctly, the process normally works as a constant cycle of “change, compile, change, compile” until all the compilation errors are gone, at which point you can be very confident you will not encounter runtime bugs. This reduces the cognitive load on the programmer, making it far faster (and less scary) to make changes to a codebase, whether large or small. This section goes beyond just providing guardrails, guardrails which are clearly inspiring other language maintainers to introduce them into their languages, to talk about something very fundamental to productivity in programming: composable, re-usable concepts and the ability to “fearlessly” make changes to your programme. These are not simply features which can be added into a language, they are characteristics of it, and they relate to the more abstract notions laid out in the next section. Reason about your programmes more easily In general, programming is about telling a machine about some problem domain: the ontology of it, and the logical rules governing it, and then asking it to compute some results. These results should have some sort of meaning we can interpret, which is going to depend on how well we understand what our programme actually means. Additionally, for us to be able to trust the results of the computations we ask of the machine, we need to be confident that we have done a good job describing the problem domain in terms that result in a “good” understanding on the part of the machine. A programme can have essential complexity or accidental complexity. The essential complexity comes from precisely describing the problem domain, and some domains are more complex than others. The accidental complexity comes from our (in)ability to express the problem domain to the machine. We can refer to these as complexity and complication to differentiate them. Complications are bad and should be eliminated. They make it hard to reason about our programmes and therefore hard to trust their results. It also makes it hard to write the programmes in the first place, because we have to deal with all these complications. It’s a bit like trying to embroider a tapestry using a Rube Goldberg machine operated with thick mittens: unlikely to give you what you want. We could look at general purpose programming languages on a scale of how well we are able to express a problem domain to a machine, and therefore to what extent we are able to trust the results of the computations we ask of that machine. Assembly is at one end: it is all about moving bytes between registers and performing arithmetic on them. Yes, you can write anything in Assembly but it is really hard to reason about the results you will get. As we move along the scale towards “high-level” languages we gain a set of abstractions which allow us to forget about the semantics of the lower level (e.g. moving bytes between registers) because they give us new semantics which are closer to those of the problem domain. The purpose of abstracting is not to be vague, but to create a new semantic level in which one can be absolutely precise. ~ Dijkstra, 197213 Haskell improves upon most high-level languages in this regard, providing a level of expressivity that allows more precise descriptions of the problem domain, easily intelligible both to the programmer and the machine. Broadly there are three major contributing factors to this (perhaps all of them can fit under the idea of denotation semantics): algebraic data types, parametric and ad hoc polymorphism, and declarative programming. We can distinguish declarative and imperative programming by saying that declarative programming describes what a computation is supposed to be with respect to the problem domain, whereas imperative programming describes how a computation is to be carried out, step-by-step. This is useful distinction: in imperative programming the operational semantics of the programme (the steps a machine must execute in order to compute a result) are mixed into the problem domain, making it difficult to reason about the meaning of a programme and, therefore, its correctness. Declarative programming, however, does not bother with defining these execution steps, making programmes much simpler to understand and reason about. In Haskell, everything is an expression. In fact, your entire programme is a single expression composed of sub-expressions. These sub-expressions have themselves some sort of meaning, as does their composition. This is different to imperative languages, in which is common for there to be many lines of function calls and loops, with, often, deeply nested function calls, but these are not essentially composable. Haskell’s purity forces concise programmes composed of meaningful sub-expressions with no side-effects. This means that it takes far less time to understand the purpose of a given expression, and therefore reason about whether it is correct or not. As ever, we are back to our two familiar pillars: so far, we have discussed the pure-functional pillar (single expression, compositionality, no side-effects), but the type system gives us the tools for expressing ourselves clearly to the machine (and to ourselves). In fact, most of the preceding sections touch upon this in one way or another: we have data types for expressing the idea that some computations can fail in well-defined ways; we have sum types like data Response = SuccessFailure FailureReason which allow us to define all the possible values we might get from a function; we have typeclasses we can use as constraints on a function to semantically express what the result is in the most general way (like CanSendEmails); and we have generalised concepts like Functor and Monoid which describe how things behave rather than the steps to implement those behaviours. Algebraic data types and typeclasses (and other, similar mechanics which deal with various polymorphisms) allow us to construct our own domain-specific languages within Haskell with which to write our programmes, while building upon common, well-established concepts to do so. These are declarative rather than imperative, and therefore are easy to reason about and to understand semantically because you do not have to either weed out the operational semantics (the step-by-step instructions) nor do you have to translate from a layer of primitives into your own domain. This section has been necessarily abstract, because the idea is hard to communicate if one has not stepped outside the imperative paradigms in which most of modern programming sits. To try to elucidate it somewhat, here is a small sample programme using the concepts discussed above. This programme is a basic accounting tool: given some initial monetary value, and a set of transactions (either in or out) in a variety of currencies, allow us to calculate the final value of the account. -- this is a bit like an \"enum\" of possible constructors of the type Currency data Currency = GBPEURUSD data Money = Money { currency :: Currency, value :: Double } convert :: Currency -> Money -> Money convert = -- not interesting to implement here as it is basically a lookup table zero :: Money zero = Money GBP 0 -- Ord gives us ways of comparing things (a natural ordering) instance Ord Money where compare m1 m2currency m1 == currency m2 = compare (value m1) (value m2)otherwise = compare m1 (convert (currency m1) m2) instance Monoid Money where m1m2 = Money (currency m1) (convert (currency m1) m2) mempty = zero instance Functor Money where fmap f (Money curr val) = Money curr (f val) data Transaction = In MoneyOut Money instance Functor Transaction where fmap f (In m) = In (f m) fmap f (Out m) = Out (f m) normalise :: Transaction -> Transaction normalise transaction = let m = money transaction in if mIn m2 = normalise (fmap ( m2) t1) t1Out m2 = normalise (fmap ( (fmap (* (-1)) m2)) t1) apply :: Transaction -> Money -> Money apply (In m) initial = initialm apply (Out m) initial = initialfmap (* (-1)) m getAccountValue :: Money -> [Transaction] -> Money getAccountValue startValue transactions = apply (fold transactions) startValue Epilogue I love writing in Haskell, and there are many reasons beyond this apologia why that is the case, but I also think it is an excellent choice for general purpose programming for anyone who wants to write robust software confidently and efficiently, and, of course, enjoyably. I think what makes Haskell unique is the combination of its type system and functional purity – it’s not enough just to have functional programming, much as I love LISPs, nor is it enough just to have the type system, much as Rust seems like a great language. Many languages have bits of these features, but only a few have all of them, and, of those languages (others include Idris, Agda, and Lean), Haskell is the most mature, and therefore has the largest ecosystem. While other languages are certainly adding features which I have mentioned above, this combination of a strong and expressive type system and pure functional programming is fundamental to the language: other languages without these axiomatic characteristics simply will not be able to implement them (and attempts to build some of these things into non-functional languages with weaker type systems is often extremely awkward and not very useful). Not everyone has the luxury of choosing their tools in a professional context, whether because there is history in their team or the decisions are made by others. Even in this case, if you never end up using Haskell professionally, it will change how you think about programming, and to invert Alan Perlis’ quotation from the start of this essay: any language which changes how you think about programming is worth learning. This is, however, not supposed to be an exhaustive list of all the things I think are great about Haskell, but just a subset of the most compelling reasons I recommend it to programmers. The rest they can discover for themselves.↩︎ Haskell is, of course, not always an appropriate choice. For example, it is never going to replace C or C++ for writing software for micro-controllers.↩︎ And quite possibly an “object-oriented” paradigm, to boot.↩︎ Perlis, A., “Epigrams in Programming” (retrieved 2024-07-07)↩︎ This is a bit like using goto to manage known failure states, which, I think, would be quite unintuitive if it hadn’t become such a dominant way of managing such failure states. In any case, I think it would make Dijkstra quite sad.↩︎ This use of “2nd order” is not idiomatic in Haskell, as this is technically a higher-kinded type, whereas “order” is typically used to refer to functions, but just like a higher-order function is a function which takes another function as its argument, a higher-kinded type is a type which takes another type as an “argument”, and thereby produces a “concrete type”. Diogo Castro’s 2018 blog post “Haskell’s kind system – a primer” has more details on this.↩︎ For those who are familiar with the idea, this is a bit like command-query segregation in the imperative world, but enforced by the type system.↩︎ To expand slightly on this, although it would be worth reading the excellent blog post by Alexis King, this means that instead of validating, i.e. checking a value meets some criterion, we parse an “unknown” value into a “valid” value, and thereby change its type. The result is that you can write functions which are defined to take the “valid” type (e.g. EmailAddress) and which never have to worry that it might be invalid, because you simply cannot forget to verify it, as you can in a “validation” pattern.↩︎ Christopher Armstrong gave an interesting talk at Strange Loop in 2015 on his python library, which includes an introduction to the motivation for this sort of pattern. This might be good follow-on content if you are interested.↩︎ Actually, what is really distinctive about Haskell is that it is a lazy, pure, functional language, but laziness can be confusing and is only lightly related to the benefits discussed in this essay, and so I am going to ignore it.↩︎ You can find a big map of how these concepts related to each other by checking out the Typeclassopedia.↩︎ Unison does have something called Unison Share, but it was written by a Haskeller and directly inspired by Hoogle (and, in fact, Unison is based on Haskell).↩︎ Dijkstra, E.W., ACM Turing Lecture: “The Humble Programmer”, 1972 (transcript)↩︎",
    "commentLink": "https://news.ycombinator.com/item?id=41518600",
    "commentBody": "Why Haskell? (gtf.io)250 points by mesaoptimizer 10 hours agohidepastfavorite332 comments iso8859-1 5 hours ago> We can generalise this idea of being forced to handle the failure cases by saying that Haskell makes us write total functions rather than partial functions. Haskell doesn't prevent endless recursion. (try e.g. `main = main`) As the typed FP ecosystem is moving towards dependent typing (Agda, Idris, Lean), this becomes an issue, because you don't want the type checker to run indefinitely. The many ad-hoc extensions to Haskell (TypeFamilies, DataKinds) are tying it down. Even the foundations might be a bit too ad-hoc: I've seen the type class resolution algorithm compared to a bad implementation of Prolog. That's why, if you like the Haskell philosophy, why would you restrict yourself to Haskell? It's not bleeding edge any more. Haskell had the possibility of being a standardized language, but look at how few packages MicroHS compiles (Lennart admitted to this at ICFP '24[0]). So the standardization has failed. The ecosystem is built upon C. The Wasm backend can't use the Wasm GC because of how idiosyncratic GHC's RTS is.[1] So what does unique value proposition does GHC have left? Possibly the GHC runtime system, but it's not as sexy to pitch in a blog post like this. [0]: Lennart Augustsson, MicroHS: https://www.youtube.com/watch?v=uMurx1a6Zck&t=36m [1]: Cheng Shao, the Wasm backend for GHC: https://www.youtube.com/watch?v=uMurx1a6Zck&t=13290s reply samvher 5 hours agoparentFor a long time already I've wanted to make the leap towards learning dependently typed programming, but I was never sure which language to invest in - they all seemed either very focused on just proofs (Coq, Lean) or just relatively far from Haskell in terms of maturity (Agda, Idris). I went through Software Foundations [0] (Coq) which was fun and interesting but I can't say I ever really applied what I used there in software (I did get more comfortable with induction proofs). You're mentioning Lean with Agda and Idris - is Lean usable as a general purpose language? I've been curious about Lean but I got the impression it sort of steps away from Haskell's legacy in terms of syntax and the like (unlike Agda and Idris) so was concerned it would be a large investment and wouldn't add much to what I've learned from Coq. I'd love any insights on what's a useful way to learn more in the area of dependent types for a working engineer today. [0] https://softwarefoundations.cis.upenn.edu/ reply iso8859-1 5 hours agorootparentLean aims to be a general purpose language, but I haven't seen people actually write HTTP servers in it. If Leo de Moura really wanted it to be general purpose, what does the concurrent runtime look like then? To my knowledge, there isn't one? That's why I've been writing an HTTP server in Idris2 instead. Here's a todo list demo app[1] and a hello world demo[2]. The advantage of Idris is that it compiles to e.g. Racket, a high level language with a concurrent runtime you can bind to from Idris. It's also interesting how languages don't need their own hosting (e.g. Hackage) any more. Idris packages are just listed in a TOML file[3] (like Stackage) but still hosted on GitHub. No need for versions, just use git commit hashes. It's all experimental anyway. [1]: https://janus.srht.site/docs/todolist.html [2]: https://git.sr.ht/~janus/web-server-racket-hello-world/tree/... [3]: https://github.com/stefan-hoeck/idris2-pack-db/blob/main/STA... reply orbifold 3 hours agorootparentThere are tasks, which are implemented as part of the runtime and they appear to plan to integrate libuv in the future. Some of the runtime seems to be fairly easy to hack and have somewhat nice ways of interoperating with both C, C++ and Rust. reply ants_everywhere 5 hours agorootparentprev> (like Stackage) but still hosted on GitHub I don't have much experience with Haskell, but one of the worst experiences has been Stack's compile time dependency on GitHub. GitHub rate limits you and builds take forever. reply tome 3 hours agorootparentThat's interesting. Could you say more? This is something that we (speaking as part of the Haskell community) should fix. As far as I know Stack/Stackage should pick up packages from Hackage. What does it use GitHub for? reply agentultra 4 hours agorootparentprevLean can be used to write software in [0]. I dare say that it may even be the intended use for Lean 4. Work on porting mathlib to Lean 4 is far along and the mathematicians using it will certainly continue to do so. However there is more space for software written in Lean 4 as well. However... it's no where near ready for production use. They don't care about maintaining backwards compatibility. They are more focused on getting the language itself right than they are about helping people build and maintain software written in it. At least for the foreseeable future. If you do build things in it you're working on shifting ground. But it has a lot of potential. The C code generated by Lean 4 is good. Although, that's another trade-off: compiling to C is another source of \"quirks.\" [0] https://agentultra.github.io/lean-4-hackers/ reply mebassett 2 hours agorootparentprevLean definitely intends to be usable as a general purpose language someday. but I think the bulk of the people involved are more focused on automated theorem proving. The Lean FRO [0] has funds to guide development of the language and they are planning to carve out a niche for stuff that requires formal verification. I'd say in terms of general purpose programming it fits into the category of being \"relatively far from haskell in terms of maturity\". [0] https://lean-fro.org/about/roadmap-y2/ reply pmarreck 1 hour agorootparentprevOne reason I took interest in Idris (and lately Roc, although it's even less mature) is the promise of a functional but usable to solve problems today language with all the latest thinking on writing good code baked-in already, compiling to a single binary (something I always envied about Go, although unfortunately it's Go). There simply isn't a lot there yet in the space of \"pure functional language with only immutable values and compile time type checking that builds a single fast binary (and has some neat developer-friendly features/ideas such as dependent types, Roc's \"tags\" or pattern-matching with destructuring)\" (this rules out OCaml, for example, despite it being mature). You get a lot of that, but not all of it, with other options (OCaml, Elixir/Erlang, Haskell... but those 3 offer a far larger library of ready-to-import software at this point). Haskell did manage to teach everyone who cares about these things that managing side-effects and keeping track of \"purity\" is important. But it's frankly still early-days and we're still far from nirvana; Rust is starting to show some warts (despite still being a massive improvement over C from a safety perspective), and people are looking around for what's next. One barely-touched thing is that there are compiler optimizations made possible by pure functional/pure immutable languages (such as caching a guaranteed result of an operation where those guarantees simply can't be given elsewhere) that have simply been impossible until now. (Roc is trying to go there, from what I can tell, and I'm here for it! Presumably, Rust has already, as long as you stick with its functional constructs, which I hear is hard sometimes) reply lemonwaterlime 5 hours agoparentprev> why would you restrict yourself to Haskell? It's not bleeding edge any more. I'm not using Haskell because it's bleeding edge. I use it because it is advanced enough and practical enough. It's at a good balanced spot now to do practical things while tapping into some of the advances in programming language theory. The compiler and the build system have gotten a lot more stable over the past several years. The libraries for most production-type activities have gotten a lot more mature. And I get all of the above plus strong type safety and composability, which helps me maintain applications in a way that I find satisfactory. For someone who aims to be pragmatic with a hint of scholarliness, Haskell is great. reply iso8859-1 3 hours agorootparent> The compiler and the build system have gotten a lot more stable over the past several years. GHC2021 promises backwards compatibility, but it includes ill-specified extensions like ScopedTypeVariables. TypeAbstractions were just added, and they do the same thing, but differently.[0] It hasn't even been decided yet which extensions are stable[1], yet GHC2021 still promises compatibility in future compiler versions. So either, you'll have GHC retain inferior semantics because of backwards compatibility, or multiple ways of doing the same thing. GHC2024 goes even further and includes extensions that are even more unstable, like DataKinds. Another sign of instability is the fact that GHC 9.4 is still the recommended[2] release even though there are three newer 'stable' GHCs. I don't know of other languages where the recommendation is so far behind! GHC 9.4.1 is from Aug 2022. It was the same situation with Cabal, it took forever to move beyond Cabal 3.6 because the subsequent releases had bugs.[3] [0]: https://serokell.io/blog/ghc-dependent-types-in-haskell-3 [1]: https://github.com/ghc-proposals/ghc-proposals/pull/669 [2]: https://github.com/haskell/ghcup-metadata/issues/220 [3]: https://github.com/haskell/ghcup-metadata/issues/40 reply gtf21 5 hours agoparentprev> That's why, if you like the Haskell philosophy, why would you restrict yourself to Haskell? In the essay, I didn't say \"Haskell is the only thing you should use\", what I said was: > Many languages have bits of these features, but only a few have all of them, and, of those languages (others include Idris, Agda, and Lean), Haskell is the most mature, and therefore has the largest ecosystem. On this: > It's not bleeding edge any more. \"Bleeding edge\" is certainly not something I've used as a benefit in this essay, so not really sure where this comes from (unless you're not actually responding to the linked essay itself, but rather to ... something else?). reply mightybyte 4 hours agoparentprev> That's why, if you like the Haskell philosophy, why would you restrict yourself to Haskell? It's not bleeding edge any more. Because it has a robust and mature ecosystem that is more viable for mainstream commercial use than any of the other \"bleeding edge\" languages. reply tkz1312 1 hour agoparentprev> So what does unique value proposition does GHC have left? Possibly the GHC runtime system, but it's not as sexy to pitch in a blog post like this. The point is that programming in a pure language with typed side effects and immutable data dramatically reduces the size of the state space that must be reasoned about. This makes programming significantly easier (especially over the long term). Of the languages that support this programming style Haskell remains the one with the largest library ecosystem, most comprehensive documentation, and most optimised compiler. I love lean and use it professionally, but it is nowhere near the usability of Haskell when it comes to being a production ready general purpose language. reply js8 3 hours agoparentprev> As the typed FP ecosystem is moving towards dependent typing (Agda, Idris, Lean), this becomes an issue, because you don't want the type checker to run indefinitely. First of all, does ecosystem move to dependent types? I think the practical value of Hindley-Milner is exactly in the fact that there is a nice boundary between types and terms. Second, why would type checking running indefinitely be a practical problem? If I can't prove a theorem, I can't use it. The program that doesn't typecheck in practical amount of time is in practice identical to non-type-checked program, i.e. no worse than a status quo. reply giraffe_lady 4 hours agoparentprev> As the typed FP ecosystem is moving towards dependent typing (Agda, Idris, Lean) I'm not really sure where the borders of \"the typed FP language ecosystem\" would be but feel pretty certain that such a thing would enclose also F#, Haskell, and OCaml. Any one of which has more users and more successful \"public facing\" projects than the languages you mentioned combined. This is not a dig on those languages, but they are niche languages even by the standards of the niche we're talking about. You could argue that they point to the future but I don't seriously believe a trend among them represents a shift in the main stream of functional programming. reply kreyenborgi 7 hours agoprevI've used Haskell for a decade or so, and tooling has improved immensely, with ghcup and cabal sandboxing and HLS now being quite stable. Maybe I've been lucky, but I haven't found much missing in the library ecosystem, or maybe I just have a lower threshold for using other languages when I see something is easier to do with a library from Python or whatever (typically nlp stuff). The one thing I still find annoying about Haskell is compile times. For the project itself, one can do fast compiles during development, but say you want to experiment with different GHC versions and base libraries, then you have to wait forever for your whole set of dependencies to compile (or buy some harddrives to store /nix on if you go that route). And installing some random Haskell program from source also becomes a drag due to dependency compile times (I'm always happy when I see a short dependency tree). Still, when deps are all compiled, it really is a fun language to program in. reply louthy 9 hours agoprevI love Haskell the language, but Haskell the ecosystem still has a way to go: * The compiler is slower than most mainstream language compilers * Its ability to effectively report errors is poorer * It tends to have 'first error, breaks rest of compile' problems * I don't mind the more verbose 'stack trace' of errors, but I know juniors/noobs can find that quite overwhelming. * The tooling, although significantly better than it was, is still poor compared to other some other functional languages, and really poor compared to mainstream languages like C# * This ^ significantly steepens the learning curve for juniors and those new to Haskell and generally gets in the way for those more experienced. * The library ecosystem for key capabilities in 'enterprise dev' is poor. Many unmaintained, substandard, or incomplete implementations. Often trying their best to be academically interesting, but not necessarily usable. The library ecosystem is probably the biggest issue. Because it's not something you can easily overcome without a lot of effort. I used to be very bullish on Haskell and brought it into my company for a greenfield project. The company had already been using pure-FP techniques (functors, monads, etc.), so it wasn't a stretch. We ran a weekly book club studying Haskell to help out the juniors and newbies. So, we really gave it its best chance. After a year of running a team with it, I came to the conclusions above. Everything was much slower -- I kept telling myself that the code would be less brittle, so slower was OK -- but in reality it sapped momentum from the team. I think Haskell's biggest contribution to the wider community is its ideas, which have influenced many other languages. I'm not sure it will ever have its moment in the sun unfortunately. reply catgary 8 hours agoparentI kind of agree that Haskell missed its window, and a big part of the problem is the academic-heavy ecosystem (everyone is doing great work, but there is a difference between academic and industrial code). I’m personally quite interested in the Koka language. It has some novel ideas (functional-but-in-place algorithms, effect-handler-aware compilation, it uses reference counting rather than garbage collection) and is a Microsoft Research project. It’s starting to look more and more like an actual production-ready language. I can daydream about Microsoft throwing support behind it, along with some tooling to add some sort of Koka-Rust interoperability. reply the_duke 8 hours agorootparentKoka is indeed incredibly cool, but: It sees sporadic bursts of activity, probably when an intern is working on it, and otherwise remains mostly dormant. There is no package manager that could facilitate ecosystem growth. There is no effort to market and popularize it. I believe it is fated to remain a research language indefinitely. reply catgary 7 hours agorootparentYou’re probably right. I just think it’s the only real candidate for a functional language that could enter the zeitgeist like Rust or Swift did, it’s a research language that has been percolating at Microsoft for some time. A new language requires a major company’s support, and they should build an industry-grade ecosystem for at least one problem domain. reply mgdev 5 hours agorootparentI'm just now discovering Koka. I'm kinda blown away. I'm also a little sad at this defeatist attitude. What you said might be true, but those things are solvable problems. Just requires a coordinated force of will from a few dedicated individuals. reply bbkane 4 hours agorootparentBe the change you want to see? reply mgdev 4 hours agorootparentHear, hear! reply psd1 6 hours agorootparentprevF# exists reply runevault 41 minutes agorootparentI keep (seems mistakenly) expecting them to try and push F# along with their dotnet ML tooling more since, while it is strictly typed, F# lets you mostly avoid making your types explicit so exploration of ideas in code is closer to Python than it is to c# while giving you the benefits of a type system and lots of functional goodies. reply cies 5 hours agorootparentprevYups. And that's about it for F#. One can await the announcement that MSFT stops maintaining it. reply mhitza 9 hours agoparentprev> * It tends to have 'first error, breaks rest of compile' problems `-fdefer-type-errors` will report those errors as warnings and fail at runtime, which is good when writing/refactoring code. Even better the Haskell LSP does this out of the box. > * The tooling, although significantly better than it was, is still poor compared to other some other functional languages, and really poor compared to mainstream languages like C# Which other functional programming languages do you think have better tooling? Experimenting lately with OCaml, feels like Haskell's tooling is more mature, though OCaml's LSP starts up faster, almost instantly. reply louthy 9 hours agorootparent> Which other functional programming languages do you think have better tooling? F#, Scala > OCaml's LSP starts up faster It was two years ago that I used Haskell last and the LSP was often crashing. But in general there were always lots of 'niggles' with all parts of the tool-chain that just killed developer flow. As I state in a sibling comment, the tooling is on the right trajectory, it just isn't there yet. So, this isn't the main reason to not do Haskell. reply mhitza 8 hours agorootparentCoincidentally I've started using the Haskell LSP around two years ago, and crashing is not one of the issues I've had with it. Since you mention F#, and C# in your previous comment, are you on the Windows platform? Maybe our experience different because of platform as well. Using GHCup to keep in sync compatible versions of GHC, Cabal and the LSP probably contributed a lot to the consistent feel of the tooling. I use the Haskell LSP for its autocompletion, reporting compile errors in my editor, and highlighting of types under cursor. There are still shortcomings with it that are annoyances: * When I open up vim, it takes a good 5-10 seconds (if not a bit more) until the LSP is finally running. * When a new dependency is added to the cabal file, the LSP needs to be restarted (usually I quit vim and reopen the project). * Still no support for goto definition for external libraries. The workaround I have to use in this case is to `cabal get dependency-version` in a gitignored directory and use hasktags to keep a tags file to jump to those definitions and read the source code/comments. The later two have open GitHub issues, so at least I know they will get solved at some point. reply runevault 39 minutes agorootparent> Since you mention F#, and C# in your previous comment, are you on the Windows platform? Since dotnet core (now dotnet 5+), the Microsoft version of dotnet has not been tied to windows outside a few exceptions like old Windows UI libraries (WPF/WinForms) and stuff like WCF once they revived it. reply louthy 8 hours agorootparentprev> Since you mention F#, and C# in your previous comment, are you on the Windows platform? Linux Mint. reply nh2 8 hours agorootparentprevThe Haskell LSP crashes less often now than 2 years ago. It isn't perfect yet, but pretty usable for us. reply rtpg 7 hours agorootparentprevHas Scala gotten better because I remember it being quite painful in the past (tho probably mostly due to language issues more than anything) reply bad_user 5 hours agorootparentThe IntelliJ IDEA plugin for Scala is built by Jetbrains, so it has official support. It has its quirks, but so does the Kotlin plugin. Sbt is better than Gradle IMO, as it has a saner mental model, although for apps you can use Gradle or Maven. Sbat has had some awesome plugins that can help in bigger teams, such as Scalafmt (automatic formatting), Scalafix (automatic refactoring), Wartremover and others. Scalafmt specifically is best in class. With Sbt you can also specify versioning schemes for your dependencies and so you can make the build fail on problematic dependency evictions. Scala CLI is also best in class, making it comfortable to use Scala for scripting – it replaced Python and Ruby for me: https://scala-cli.virtuslab.org/ Note that Java and Kotlin have Jbang, but Scala CLI is significantly better, also functioning as a REPL. Worth mentioning that other JVM languages hardly have a usable REPL, if at all. The Scala compiler can be slow, but that's when you use libraries doing a lot of compile-time derivation or other uses of macros. You get the same effect in similar languages (with the exception of Ocaml). OTOH the Scala compiler can do incremental compilation, and alongside Sbt's support for multiple sub-projects or continuous testing, it works fairly well. Scala also has a really good LSP implementation, Metals, built in cooperation with the compiler team, so you get good support in VS Code or Vim. To get a sense of where this matters, consider that Scala 3.5 introduces \"best effort compilation\": https://github.com/scala/scala3/pull/17582 I also like Kotlin and sadly, it's missing a good LSP implementation, and I don't think Jetbrains is interested in developing it. Also you get all the tooling that's JVM specific, including all the profilers and debuggers. With GraalVM's native image, for example, Scala fares better than Java actually, because Scala code relies less on runtime reflection. I'd also mention Scala Native or ScalaJS which are nice to have. Wasm support is provided via LLVM, but there's also initial support for Wasm GC. So to answer your question, yes, Scala has really good tooling compared to other languages, although there's room for improvement. And if you're comparing it to any other language that people use for FP, then Scala definitely has better tooling. reply ldite 2 hours agorootparentSBT is awful. I've never used Gradle, but if SBT is saner then I'm worried. This blogpost is a bit old, but still on-target: https://www.lihaoyi.com/post/SowhatswrongwithSBT.html reply dionian 37 minutes agorootparentSBT has a learning curve but it also has a nice ecosystem, for example sbt-native-packager is better than its competitors in maven or gradle. reply draven 6 hours agorootparentprevWell, there's Intellij IDEA with the scala plugin, and it's pretty good. I regularly debug my code in the IDE with conditional breakpoints, etc. SBT still makes me want to throw the laptop through the window. reply pxc 5 hours agorootparentIn the pre-LSP era, I worked as a novice Scala developer, and I didn most of my Scala work in Emacs with ENSIME. It was pretty good. I imagine the language server is pretty usable by now. reply weebull 6 hours agorootparentprevScala's has made some horrible language compromises in order to live on the JVM in my opinion. reply bad_user 5 hours agorootparentI'd argue that Scala's \"compromises\" in general make it a better language than many others, independent of the JVM. But we can talk specifics if you want. Name some compromises. reply mattpallissard 3 hours agorootparentprev> Experimenting lately with OCaml, feels like Haskell's tooling is more mature. I feel like OCaml has been on a fast upward trajectory the past couple of years. Both in terms of language features and tooling. I expect the developer experience to surpass Haskell if it hasn't already. I really like Merlin/ocaml-lsp. Sure, it doesn't have every LSP feature like a tool with a lot of eyes on it, such as clangd, but it handles nearly everything. And yeah, dune is a little odd, but I haven't had any issues with it in a while. I even have some curve-ball projects that involve a fair amount of C/FFI work. My only complaint with opam is how switches feel a little clunky. But still, I'll take it over pip, npm, all day. I've been using OCaml for years now and compared to most other languages, the experience has been pretty pleasant. reply mhitza 1 hour agorootparentMy little experiments with OCaml have been pleasant thus far (in terms of language ergonomics), but on the tooling side Haskell (or rather I should say GHC) is pretty sweet. For what I had to do thus far, at one point I needed to step debug through my code. Whereas in GHC land I reload my project in the interpreter (GHCi or cabal repl), set a break point on function name and step through the execution. With OCaml I have to go through the separate bytecode compiler to build it with debug symbols and the I can navigate through program execution. The nice thing is that I can easily go back in execution in flow (\"timetravel debugging\"), but a less ergonomic. Also less experienced with this flow, to consider my issues authoritative. I don't have that much experience with dune (aside from setting up a project and running dune build), but one thing that confused me at first, is that the libraries I have to add to the configuration do not necessarily match the Opam package names. The LSP is fast, as mentioned before, it supports goto definition, but once I jump to a definition to one of my dependencies I get a bunch of squiggly lines in those files (probably can't see transitive dependency symbols, if I where to guess). I can navigate dependencies one level deeper than I can with the Haskell language server, though. I actually want to better understand how to build my projects without Dune, and probably will attempt to do so in the future. The same way I know how to manage a Haskell project without Cabal. Feels like it gives me a better understanding of the ecosystem. reply lkmain 8 hours agorootparentprevI haven't yet felt the need for third party tooling in OCaml. OCaml has real abstractions, easily readable modules and one can keep the whole language in one's head. Usually people do not use objects, and if they do, they don't create a tightly coupled object mess that can only be unraveled by an IDE. reply moomin 9 hours agoparentprevNo lies detected. I love Haskell, but productivity is a function of the entire ecosystem, and it’s just not there compared to most mainstream languages. reply pyrale 7 hours agoparentprevMost of your comments boil down to two items: - The Haskell ecosystem doesn't have the budget of languages like Java or C# to build its tooling. - The haskell ecosystem was innovative 20 years ago, but some newer languages like Rust or Elm have much better ergonomics due to learning from their forebearers. Yes, it's true. And it's true for almost any smaller language out there. reply louthy 6 hours agorootparentIf you boil down my comments, sure, you could say that. But, that's why I didn't boil down my comments and used more words, because ultimately, it doesn't say that. The thread is \"Why Haskell?\", I'm offering a counterpoint based on experience. YMMV and that's fine. reply troupo 6 hours agorootparentprevCounterpoint: Elixir. While it sits on top of industrial-grade Erlang VM, the language itself produced a huge ecosystem of pragmatic and useful tools and libraries. reply robocat 2 hours agoparentprevIt is a shame that the article almost completely ignores the issue of the tooling. I particularly find the attitude in the following paragraph offensively academically true: All mainstream, general purpose programming languages are (basically) Turing-complete, and therefore any programme you can write in one you can, in fact, write in another. There is a computational equivalence between them. The main differences are instead in the expressiveness of the languages, the guardrails they give you, and their performance characteristics (although this is possibly more of a runtime/compiler implementation question). I decided to have a go at learning the basics of Haskell and the first error I got immediately phased me because it reminded me of unhelpful compilers of the 80s. I have bashed my head against different languages and poor tooling enough times to know I can learn, but I've also done it enough times that I am unwilling to masochistically force myself through that gauntlet unless I have a very good reason to do so. The \"joy\" of learning is absent with unfriendly tools. The syntax summary in the article is really good. Short and clear. reply gtf21 0 minutes agorootparent> It is a shame that the article almost completely ignores the issue of the tooling. Mostly because while I found of the tooling occasionally difficult, I didn’t find Haskell particularly bad compared to other language ecosystems I’ve played with, with the exception of Rust, for which the compiler errors are really good. > The syntax summary in the article is really good Thanks, I wasn’t so sure how to balance that bit. reply ants_everywhere 5 hours agoparentprevI completely agree. I'm interested in making the Haskell tooling system better. I would welcome anyone with Haskell experience to let me know what you think would be the highest priority items here. I'm also curious about the slowness of compilation and whether that's intrinsic to the design of GHC. reply tome 2 hours agorootparent> I would welcome anyone with Haskell experience to let me know what you think would be the highest priority items here. Simplifying cabal probably, though that's a system-level problem, just just a cabal codebase problem. reply gtf21 8 hours agoparentprev> The library ecosystem is probably the biggest issue. I'd love to know which things specifically you're thinking about. For what we've been building, the \"integration\" libraries for postgres, AWS, etc. have been fine for us, likewise HTTP libraries (e.g. Servant) have been great. I haven't _yet_ encountered a library problem, so am just very curious. reply crote 8 hours agorootparentA few years ago I tried to use Servant to make a CAS[0] implementation for an academic project. One issue I ran into was that Servant didn't have a proper way of overriding content negotiation: the CAS protocol specified a \"?format=json\" / \"?format=xml\" parameter, but Servant had no proper way of overriding its automatic content negotiation - which is baked deeply into its type system. I believe at the time I came across an ancient bug report which concluded that it was an \"open research question\" which would require \"probably a complete rework\". Another issue was that Servant doesn't have proper integrated error handling. The library is designed around returning a 200 response, and provides a lot of tooling to make that easy and safe. However, I noticed that at the time its design essentially completely ignored failures! Your best option was basically a `Maybe SomeResponseType` which in the `None` case gave a 200 response with a \"{'status': 'error'}\" content. There was a similar years-old bug report for this issue, which is quite worrying considering it's not exactly rocket science, and pretty much every single web developer is going to run into it. All of this gave a feeling of a very rough and unfinished library, whose author was more concerned about writing a research paper than actually making useful software. Luckily those issues had no real-world implication for me, as I was only a student losing a few days on some minor project. But if I were to come across this during professional software development I'd be seriously pissed, and probably write off the entire ecosystem: if this is what I can expect from \"great\" libraries, what does the average ones look like - am I going to have to write every single trivial thing from scratch? I really love the core language of Haskell, but after running into issues like these a few dozen times I unfortunately have trouble justifying using it to myself. Maybe Haskell will be great five or ten years from now, but in its current state I fear it is probably best to use something else. [0]: https://en.wikipedia.org/wiki/Central_Authentication_Service reply dwattttt 5 hours agorootparent> Your best option was basically a `Maybe SomeResponseType` which in the `None` case gave a 200 response with a \"{'status': 'error'}\" content. This seems to be an area where my tastes diverge from the mainstream, but I'm not a fan of folding errors together. I'd rather a http status code only correspond to the actual http transport part, and if an API hosted there has an error to tell me, that should be layered on top. reply imoverclocked 7 hours agorootparentprevI tried building a couple small projects to get familiar with the language. One project did a bunch of calculation based on geolocation and geometry. I needed to output graphs and after looking around, reached for gnuplot. Turns out, it’s a wrapper around a system call to launch gnuplot in a child process. There is no handle returned so you can never know when the plot is done. If you exit as soon as the call returns, you get to race gnuplot to the temp file that gets automatically cleaned up by your process. The only way to eliminate the race is by sleeping… so if you add more plots, make sure you increase your sleep time too. :-/ Another utility was a network oriented daemon. I needed to capture packets and then run commands based on them… so I reached for pcap. It uses old bindings (which is fine) and doesn’t expose the socket or any way to set options for the socket. Long story short, it never worked. I looked at the various other interfaces around pcap but there was always a significant deficiency of some kind for my use case. Now, I’m not a seasoned Haskell programmer by any means and it’s possible I am just missing out on something fundamental. However, it really looks to me like someone did a quick hack that worked for a very specific use-case for both of these libraries. The language is cool but I’ve definitely struggled with libraries. reply louthy 8 hours agorootparentprevThe project was a cloud agnostic platform-as-a-service for building healthcare applications. It needed graph-DBs, Postgres, all clouds, localisation, event-streams, UIs, etc. I won't list where the problems were, because I don't think it's helpful -- each project has its own needs, you may well be lucky where we were not. Certainly the project wasn't a standard enterprise app, it was much more complex, so we had some foundational things we needed that perhaps your average dev doesn't need. However, other ecosystems would usually have a decent off-the-shelf versions, because they're more mature/evolved. You have to realise that none of the problems were insurmountable, I had a talented team who could overcome any of the issues, it just became like walking through treacle trying to get moving. And yes, Servant was great, we used that also. Although we didn't get super far in testing its range. reply chii 8 hours agorootparentprevProbably referring to something like spring (for java), which is a one stop shop for everything, including things like integration with monitoring/analytics, rate-limiting, etc reply okkdev 7 hours agorootparentSpring is probably the worst framework created, so I wouldn't list that as an example :/ reply devjab 5 hours agoparentprev> compared to mainstream languages like C# Out of curiosity does this also hold true for F#? reply louthy 1 hour agorootparentF#’s tooling is worse than C# for sure, but it’s a big step-up from Haskell and has access to the .NET framework. I listed C# because that’s the mainstream language I know the best, and arguably has best-in-class tooling. Of course you have to be prepared to lose some of the creature comforts when using a more left-field language. But, you still need to be productive. The whole ecosystem has to be a net gain in productivity, or stability, or security, or maintainability — pick your poison depending on what matters to your situation. I had hoped Haskell would pay dividends due to its purity, expressive type-system, battle tested-ness, etc. I expected us to be slower, just not as slow as it turned out. Ultimately the trade off didn’t work for us. reply Vosporos 9 hours agoparentprevIf you are willing / able to report these pain points in detail to the Haskell Foundation, this is going to be valuable feedback that will help orient the investments in tooling in the near future. reply adastra22 9 hours agorootparentAll bug reports are good. But is this not obvious? Do the Haskell developers not use other language ecosystems? This goes beyond “this edge case is difficult” and into “the whole tooling stack is infamously hard to work with.” I just assumed Haskell, like eMacs, attracted a certain kind of developer that embraced the warts. reply mrkeen 8 hours agorootparentNo, we use plenty of other stuff. My $DAYJOB language: * Can't build a binary * Uses an inexplicable amount of memory. * Has an IDE which constantly puts itself into a bad state. E.g. it highlights and underlines code with red even when I know it's a pristine copy that passes its tests. I periodically have to close the project, navigate to it in the terminal, run 'git status --ignored' and delete all that crap and re-open the project. * Is slow to start up. * Has a build system with no obvious way to use a 'master list' of version numbers. In our microservice/microrepo system, it is a PITA to try to track down and remove a vulnerable dependency. * Has been receiving loads of praise over the last 18 months for starting to include stuff that Haskell has included for ages. How's the latest \"we're solving the null problem\" going? What the GHC compiler does for me is just so much better at producing working software than $DAYJOB language + professional $DAYJOB IDE, that I don't think about the tooling. If you want to put yourself in my shoes: imagine you're getting shit done with TypeScript every day, and some C programmers come along and complain that it's missing the bare minimum of tools: strace, valgrind and gdb. How do you even reply to that? reply actualwitch 8 hours agorootparent> If you want to put yourself in my shoes: imagine you're getting shit done with TypeScript every day, and some C programmers come along and complain that it's missing the bare minimum of tools: strace, valgrind and gdb. How do you even reply to that? You tell them to strace/valgrind node whatever.js and instead of gdb use built-in v8 debugger as node inspect whatever.js reply gtf21 8 hours agorootparentprevWe do use other ecosystems, yes. I haven't really found the tooling for Haskell to be particularly obstructive compared to other languages. I've run into plenty of mysteries in the typescript, python, ObjC/Swift, etc. ecosystems that have been just as irritating (sometimes much more irritating), and generally find that while HLS can be a bit janky, GHC is very good and I spend less time scratching my head looking at a piece of code that should work but does something wild than in other languages. reply louthy 9 hours agorootparentprevI think tooling is something that is clearly on a good trajectory. When I consider what the Haskell tooling was like when I first started using it, well, it was non-existent! (and Cabal didn't even understand what dependencies were, haha!) So, it's much, much better than it was. It's still not comparable to mainstream languages, but it's going the right way. So, I wouldn't necessarily take that as the killer. The biggest issue was the library ecosystem. We spent an not-small amount of time evaluating libraries, realising they were not up to scratch, trying to do build our own, or interacting with the authors to understand the plans. When you're trying to get moving at the start of a project, this can be quite painful. It takes longer to get to an MVP. That's tough when there are eyes on its success or not. Even though I'd been using Haskell for at least a decade before we embarked upon that path, I hadn't really ever built anything substantial. The greenfield project was a complex beast on a number of levels (which was one of the reasons I felt Haskell would excel, it would force us to be more robust with our architecture). But, we just couldn't find the libraries that were good enough. My sense was there's a lot of academics writing libraries. I'm not implying that academics write poor code; just that their motivations aren't always aligned with what an industry dev might want. Usually this is around simplicity and ease-of-use. And, because quite a lot of libraries were either poorly documented or their intent was impenetrable, it would take longer to evaluate. I think if the Haskell Foundation are going to do anything, then they should probably write down the top 50 needed packages in industry, and then put some funding/effort towards helping the authors of existing libraries to bring them up to scratch (or potentially, developing their own), perhaps even create a 'mainstream adoption style guide', that standardises the library surfaces -- there's far too much variability. It needs a keen eye on what your average industry dev needs though. I realise there are plenty of companies using Haskell successfully, so this should only be one data point. But, it is a data point of someone who is a massive Haskell (language) fan. Haskell has had a massive influence on me and how I write code. It's directly influenced a major open-source project I have developed [1]. But, unfortunately, I don't think I'll use it again for a pro project. [1] https://github.com/louthy/language-ext reply Coolbeanstoo 8 hours agoprevI would like to use haskell or another functional language professionally. I try them out (ocaml,haskell,clojure,etc) from time to time and think they're fairly interesting, but i struggle to figure out how to make bigger programs with them as I've never seen how you build up a code base with the tools they provide and with someone to review the code i produce and so never have any luck with jobs i've applied to. On the flipside I never had too much trouble figuring out how to make things with Go, as it has so little going on and because it was the first language i worked with professionally for an extended period of time. I think that also leads me to trying to apply the same patterns because I know them even if they dont really work in the world of functional languages Not sure what the point of this comment is, but I think i just want to experience the moment of mind opening-ness that people talk about when it comes to working with these kinds of languages on a really good team reply bedman12345 6 hours agoparentI’ve been working with pure functional languages and custom lisp dialects professionally my whole tenure. You get a whole bag of problems for a very subjective upside. Teams fragment into those that know how to work with these fringe tools and those who don’t. The projects using them that I worked on all had trouble with getting/retaining people. They also all had performance issues and had bugs like all other software. You’re not missing out on anything. reply cosmic_quanta 7 hours agoparentprevI have also initially struggled with structuring Haskell programs. Without knowing anything about what you want to do, here's my general approach: 1. Decide on an effect system Remember, Haskell is pure, so any side-effect will be strictly explicit. What broad capabilities do you want? Usually, you need to access some program-level configuration (e.g. command-line options) and the ability to do IO (networking, reading/writing files, etc), so most people start with that. https://tech.fpcomplete.com/blog/2017/06/readert-design-patt... 2. Encode your business logic in functions (purely if possible) Your application does some processing of data. The details don't matter. Use pure functions as much as possible, and factor effectful computations (e.g. database accesses) out into their own functions. 3. Glue everything together in a monadic context Once you have all your business logic, glue everything together in a context with your effect system (usually a monad stack using ReaderT). This is usually where concurrency comes in (e.g. launch 1 thread per request). --- Beyond this, your application design will depend on your use-case. If you are interested, I strongly suggest to read 'Production Haskell' by Matt Parsons, which has many chapters on 'Haskell application structure'. reply solomonb 2 hours agorootparent> 1. Decide on an effect system This shouldn't even be proposed as a question to someone new to Haskell. They should learn how monad transformers work and just use them. 90% of developers playing around effect systems would be just fine with MTL or even just concrete transformers. All Haskell effect systems should be considered experimental at this point with unclear shelf lives. Everything else you said I agree with as solid advice! reply cosmic_quanta 2 hours agorootparentSomeone truly new to Haskell shouldn't use it professionally. Once you've learned what is necessary to, say, modify already-existing applications, you should be familiar with monads and some basic monad transformers like ReaderT. Once you're there, I don't think 'choosing an effect system' is a perilous question. The monad transformer library, mtl, is an effect system, the second simplest one after IO. reply solomonb 2 hours agorootparentThe original poster said they want to use Haskell professionally but that they are struggling to understand how to structure programs. > Once you're there, I don't think 'choosing an effect system' is a perilous question. The monad transformer library, mtl, is an effect system, the second simplest one after IO. I'm aware of that, generally when people say \"choose effect system\" they mean choose some algebraic effect system, all of which (in Haskell) have huge pitfalls. The default should be monad transformers unless you have some exceptional situation. reply cosmic_quanta 1 hour agorootparentI realize I didn't mention monad transformers at all in my original post, I only linked to them! I should have mentioned that, as you say, monad transformers should be the default effect system choice for 99% of people reply jsbg 2 hours agorootparentprevThis is excellent advice that unfortunately seems to get lost in a lot of Haskell teachings. I learned Haskell in school but until I had to use it professionally I would have never been able to wrap my head around effect systems. I still think that part of Haskell is unfortunate as it can get in the way of getting things done if you're not an expert, but being able to separate pure functions from effectful ones is a massive advantage. reply mattgreenrocks 4 hours agoparentprevI've used Haskell professionally for two years. It is the right pick for the project I'm working on (static analysis). I'm less sold on the overall Haskell ecosystem, tooling, and the overall Haskell culture. There are still plenty of ways to do things wrong. Strong types don't prevent that. Laziness is a double-edged sword and can be difficult to reason about. reply jerf 4 hours agoparentprevPeople love to talk about the upsides and the fun and what you can learn from Haskell. I am one of these people. People are much more reluctant to share what it is that led them to the conclusion that Haskell isn't something they want to use professionally, or something they can't use professionally. It's a combination of things, such as it just generally being less energizing to talk about that, and also some degree of frankly-justified fear of being harassed by people who will argue loudly and insultingly that you just Don't Get It. I am not one of those people. I will share the three main reasons I don't even consider it professionally. First, Hacker News has a stronger-than-average egalitarian streak and really wants to believe that everybody in the world is already a developer with 15 years of experience and expert-level knowledge in all they survey from atop their accomplished throne, but that's not how the real world works. In the real world I work with coworkers who I have to train why in my Go code, a \"DomainName\" is a type instead of just a string. Then, just as the light bulb goes off, they move on from the project and I get the next junior dev who I have to explain it to. I'm hardly going to get to the point where I have a team of people who are Haskell experts when I'm explaining this basic thing over and over. And, to be 100% clear, this is not their \"fault\", because being a junior programmer in 2024 is facing a mountain of issues I didn't face at their age: https://news.ycombinator.com/item?id=33911633 I wasn't expected to know about how to do source control or write everything to be rollback-able or interact with QA, or, well, see linked post for more examples. Haskell is another stack of requirements on top of a modern junior dev that is a hell of an ask. There better be some damn good reasons for me to add this to my minimim-viable developer for a project. I am not expressing contempt for the junior programmers here from atop my own lofty perch; I am encouraging people to have sympathy with them, especially if you also come up in the 90s when it was really relatively easy, and to make sure you don't spec out projects where you're basically pulling the ladder up after yourself. You need to have an onboarding plan, and \"spend a whole bunch of time learning Haskell\" is spending a lot of your onboarding plan currency. Second, while a Haskell program that has the chef's kiss perfect architecture is a joy to work with, it is much more difficult to get there for a real project. When I was playing with Haskell it was a frequent occurrence to discover I'd architected something wrong, and to essentially need to rewrite the whole program, because there is no intermediate functioning program between where I was and where I needed to be. The strength of the type system is a great benefit, but it does not put up with your crap. But \"your crap\" includes things like being able to rearchitect a system in phases, or partially, and still have a functioning system, and some other things that are harder to characterize but you do a lot of without even realizing it. I'd analogize it to a metalworker working with titanium. If you need it, you need it. If you can afford it, great. The end result is amazing. But it's a much harder metal to work with for the exact same reason it's amazing. The strength of the end part is directly reflected in the metal resisting you working with it. I expect at a true expert level you can get over this, but then as per my first point, demanding that all my fellow developers become true experts in this obscure language is taking it up another level past just being able to work in it at all. Finally, a lot of programming requirements have changed over the years. 10-15 years ago I could feasibly break my program into a \"functional core\" and an external IO system. This has become a great deal less true, because the baseline requirement for logging, metrics, and visibility have gone up a lot, and suddenly that \"pure core\" becomes a lot less appealing. Yes, of course, our pure functions could all return logs and metrics and whathaveyou, and sure, you can set up the syntax to the point that it's almost tolerable, but you're still going to face issues where basically everything is now in some sort of IO. If nothing else, those beautiful (Int -> Int -> Int) functions all become (Int -> Int -> LoggingMetrics Int) and now it isn't just that you \"get\" to use monadic interfaces but you're in the LoggingMetrics monad for everything and the advantages of Haskell, while they do not go away entirely, are somewhat mitigated, because it really wants purity. It puts me halfway to being in the \"imperative monad\" already, and makes the plan of just going ahead and being there and programming carefully a lot more appealing. Especially when you combine that with the junior devs being able to understand the resulting code. In the end, while I still strongly recommend professional programmers spend some time in this world to glean some lessons from it that are much more challenging to learn anywhere else, it is better to take the lessons learned and learn how to apply them back into conventional languages than to try to insist on using the more pure functional languages in an engineering environment. This isn't even the complete list of issues, but they're sufficient to eliminate them from consideration for almost every engineering task. And in fact every case I have personally witnessed where someone pushed through anyhow and did it, it was ultimately a business failure. reply cosmic_quanta 4 hours agorootparent> I'd analogize it to a metalworker working with titanium. If you need it, you need it. If you can afford it, great. The end result is amazing. But it's a much harder metal to work with for the exact same reason it's amazing. What a beautiful, succinct analogy. I'm stealing this. reply ninetyninenine 3 hours agorootparentprev> I'd analogize it to a metalworker working with titanium. If you need it, you need it. If you can afford it, great. The end result is amazing. But it's a much harder metal to work with for the exact same reason it's amazing. The strength of the end part is directly reflected in the metal resisting you working with it. I’d say you missed one of the main points of Haskell and functional programming in general. The combinator is the most modular and fundamental computational primitive available in programming. When you make a functional program it should be constructed out of the composition of thousands of these primitive with extremely strict separation from IO and multiple layers of abstraction. Each layer is simply composed functions from the layer below. If you think of fp programming this way. It becomes the most modular most reconfigurable programming pattern in existence. You have access to all layers of abstraction and within each layer are independent modules of composed combinators. Your titanium is given super powers where you can access the engine, the part, the molecule and the atom. All the static safety and beauty Haskell provides is actually a side thing. What Haskell and functional programming in general provides is the most fundamental and foundational way to organize your program such that any architectural change only requires you replacing and changing the minimum amount of required modules. Literally the opposite of what you’re saying. The key is to make your program just a bunch of combinators all the way down with an imperative io shell that is as thin as possible. This is nirvana of program organization and patterns. reply jerf 2 hours agorootparentI'm well aware of functional programming as focusing on composition. One of the reasons you end up with \"refactoring the entire program because of some change\" is when you discover that your entire composition scheme you built your entire program around is wrong, e.g., \"Gee, this effects library I built my entire code base around to date is really nifty but also I can't actually express my needs in it after all\". In a conventional language, you just build in the exceptions, and maybe feel briefly sad, but it works. It can ruin a codebase if you let it, but it's at least an option. In Haskell, you have a much bigger problem. Now filter that back through what I wrote. You want to explain to your junior developer who is still struggling with the concept of using things other than strings why we have to rewrite the entire code base to use raw IO instead of the effects system we were using because it turns out the compilation time went exponential and we can't fix it in any reasonable amount of effort? How happy are they going to be with you after you just spent a whole bunch of time explaining the way to work with the effects system? They're not going to come away with a good impression of either Haskell or you. reply cpa 9 hours agoprevHaskell has had a profound impact on the way I think about programming and how I architect my code and build services. The stateless nature of Haskell is something that many rediscover at different points in their careers. Eg in webdev, it's mostly about offloading state to the database and treating the application as \"dumb nodes.\" That's what most K8s deployments do. The type system in Haskell, particularly union types, is incredibly powerful, easy to understand for the most part (you don't need to understand monads that deeply to use them), and highly useful. And I've had a lot of fun micro-optimizing Haskell code for Project Euler problems when I was studying. Give it a try. Especially, if you don't know what to expect, I can guarantee that you'll be surprised! Granted, the tooling is sh*t. reply setopt 7 hours agoparent> Haskell has had a profound impact on the way I think about programming and how I architect my code and build services. > And I've had a lot of fun micro-optimizing Haskell code for Project Euler problems when I was studying. Sounds a lot like my experience. I never really used Haskell for \"real work\", where I need support for high-performance numerical calculations that is simply better in other languages (Python, Julia, C/C++, Fortran). But learning functional programming through Haskell – mostly by following the \"Learn you a Haskell\" book and then spending time working through Project Euler exercises using it – had a quite formative effect on how I write code. I even ended up baking some functional programming concepts into my Fortran code later. For instance, I implemented the ability to \"map\" functions on my data structures, and made heavy use of \"pure functions\" which are supported by the modern Fortran standard (the compiler then checks for side effects). It's however hard to go all the way on functional programming in HPC contexts, although I wish there were better libraries available to enable this. reply nextos 5 hours agorootparent> But learning functional programming through Haskell [...] had a quite formative effect on how I write code. I think it is a shame Haskell has gained a reputation of being hard, because it can be an enriching learning experience. Lots of its complexity is accidental, and comes from the myriad of language extensions that have been created for research purposes. There was an initiative to define a simpler subset of the language, which IMHO would have been great, but it didn't take off: https://www.simplehaskell.org. Ultimately, one can stick to Haskell 98 or Haskell 2010 plus some newer cherry-picked extensions. reply bbkane 5 hours agorootparentI think Elm is a fantastic \"simplified Haskell\" with pretty good beginner-friendly guides. It's unfortunate that Elm is mostly tied to the frontend and has been effectively abandoned for the last couple of years. Interestingly, Elm has inspired a host of \"successors\", including Gleam + Lustre, which look really great (I haven't had a chance to really try them yet). reply earth_walker 3 hours agorootparentElm's strengths are its constraints, which allow for simple, readable code that's easy to test and reason about - partly because libraries are also guaranteed to work within those constraints. I've tried and failed several times to write Haskell in an Elm style, even though the syntax is so similar. It's probably me (it's definitely me!), but I've found that as soon as you depend on a library or two outside of prelude their complexities bleed into your project and eventually force you into peppering that readable, simple code with lifts, lenses, transformations and hidden magic. Not to mention the error messages and compile times make developing in Haskell a chore in comparison. p.s. Elm has not been abandoned, it's very active and getting better every day. You just can't measure by updates to the (stable, but with a few old bugs) core. For a small, unpopular language there is so much work going into high quality libraries and development tools. Check out https://elmcraft.org/lore/elm-core-development for a discussion. Elm is so nice to work in. Great error messages, and near instant compile times, and a great ecosystem of static analysis, scaffolding, scripting, and hot reloading tools make the live development cycle super nice - it actually feels like what the lispers always promised would happen if we embraced repl-driven development. reply bbkane 2 hours agorootparentThanks for the Elmcraft FAQ link. It's a great succinct explanation from the Elm leadership perspective (though tellingly not from the Elm leadership). I feel like I understand that perspective, but I also don't think I'm wrong in claiming Elm has been effectively abandoned in a world where an FAQ like that needs to be written. I'm not going to try to convince you though, enjoy Elm!! reply britzkopf 2 hours agorootparentprevI've often wondered if it having a reputation as being hard is accurate. Not necessarily because of syntax etc. but because of you don't already have a grounding in programming/engineering/comp sci. it can be difficult to fit the insights Haskell provides into any meaningful framework. That was my experience anyway, came to it too early and didn't understand the significance. reply nextos 5 hours agorootparentprevWhat about Roc or Koka? Or simply moving to OCaml? It's looking pretty great after v5, with multicore and effect handlers. reply giraffe_lady 4 hours agorootparentOCaml is great but the type system is actually quite different from Haskell's once you get into it. It also has many \"escape hatches\" out of the functional pathway. Even if you approach it with a learner's discipline you'll run into them even in the standard lib. With haskell you can look to the ecosystem to see how to accomplish specific things with a pure functional approach. When you look at ocaml projects in that way you often find people choosing not to. reply mattpallissard 3 hours agorootparentOh but the OCaml module system is the bees knees. reply giraffe_lady 3 hours agorootparentYeah I didn't mean any of this as a negative lol. I haven't touched haskell since I learned ocaml. I still think haskell has the edge as an educational language for functional programming and type systems though, which is kind of what we're talking about but not entirely. reply mattpallissard 2 hours agorootparentNo worries, I was just adding my two cents. reply gtirloni 5 hours agorootparentprevSounds a lot like the C++ experience. In my time learning Haskell a decade ago, it was rare to find some code that wasn't using an experimental extension. reply l5870uoo9y 5 hours agorootparentprevPure functions are a crazy useful abstractions. Complex business logic? Extract it into a type-safe pure function. Still to \"unsafe\"? Testing pure functions are fast and simple. Unclear what a complex function does? Extract it into meaningful pure functions. reply adastra22 9 hours agoparentprevHaskell also changed the way I think about programming. But I wonder if it would have as much of an impact on someone coming from a language like Rust or even modern C++ which has adopted many of haskell’s features? reply mmoll 8 hours agorootparentTrue. I often think of Rust as a best-of compilation of Haskell and C++ (although I read somewhere that OCaml had a greater influence on it, but I don’t know that language well enough) In real life, I find that Haskell suffers from trying too hard to use the most general concept that‘s applicable (no pun intended). Haskell programs happily use “Either Err Val” and “Left x” where other languages would use the more expressive but less general “Result Err Val” and “Error x”. Also, I don’t want to mentally parse nested liftM2s or learn the 5th effect system ;-) reply hollerith 6 hours agorootparent>I read somewhere that OCaml had a greater influence on it Whoever wrote that is wrong. reply randomdata 5 hours agorootparentIf we could wave a magic wand and remove Haskell's influence on Rust, Rust would still exist in some kind of partial form. If we waved the same wand and removed OCaml's influence, Rust would no longer exist at all. You are the one who is wrong, I'm afraid. reply lkitching 4 hours agorootparentWhich OCaml features exist in Rust but not Haskell? The trait system looks very similar to Haskell typeclasses, but I'm not aware of any novel OCaml influence on the language. reply randomdata 4 hours agorootparent> Which OCaml features exist in Rust but not Haskell? Rust's most important feature! The bootstrapped implementation. reply lkitching 4 hours agorootparentI'm not convinced the implementation language of the compiler counts as a feature of the Rust language. If the argument is that Rust wouldn't have been invented without the original author wanting a 'systems OCaml' then fine. But it's possible Rust would still look similar to how it does now in a counterfactual world where the original inspiration was Haskell rather than OCaml, but removing the Haskell influence from Rust as it is now would result in something quite different. reply randomdata 4 hours agorootparentRust isn't just a language, though. Additionally, unlike some languages that are formally specified before turning to implementation, Rust has subscribed to design-by-implementation. The implementation is the language. reply lkitching 3 hours agorootparentThat just means the semantics of the language are defined by whatever the default implementation does. It's a big stretch to conclude that means Rust 'was' OCaml in some sense when the compiler was written with it. Especially now the Rust compiler is written in Rust itself. reply randomdata 3 hours agorootparentYou're overthinking again. Read what is said, not what you want it to say in some fairytale land. reply itishappy 5 hours agorootparentprevRust was bootsrapped in OCaml. https://github.com/mozilla/rust/tree/ef75860a0a72f79f97216f8... reply TwentyPosts 5 hours agorootparentprevThe original rust compiler was written in OCaml. That's not evidence it \"had an influence\", but it's highly striking considering how many other languages Greydon could've used. reply hollerith 4 hours agorootparentYes: if a person knows nothing else about Rust and the languages that might have influenced it, then the fact that the original Rust compiler was written in OCaml should make that person conclude tentatively that OCaml was the language that influenced the design of Rust the most. I'm not one to hold that one shouldn't form tentative conclusions until one \"has all the fact\". Also, I'm not one to hold that readers should trust the opinion of an internet comment writer they know nothing about. I could write a long explanation to support my opinion, but I'm probably not going to. reply dimitrios1 4 hours agorootparentIt's like trying to say Elixir wasn't influenced the most by Erlang reply hollerith 3 hours agorootparentWas any Elixir interpreter or compiler every written in Erlang? If not, what is the relevance of your comment? reply jolux 3 hours agorootparentElixir’s implementation still has significant parts written in Erlang. I don’t know if it’s a majority but it’s a lot. e.g.: https://github.com/elixir-lang/elixir/blob/aef7e4eab521dfba9... reply setopt 7 hours agorootparentprevI think it does, actually. Python also has many of Haskell's features (list comprehensions, map/filter/reduce, itertools, functools, etc.). But I only started reaching for those features after learning about them in Haskell. In Python, it's very easy to just write out a for-loops to do these things, and you don't necessarily go looking for alternative ways to do these things unless you know the functional equivalents already. But in Haskell you're forced to do things this way since there is no for-loop available. But after learning that way of thinking, the result is then more compact code with arguably less risk of bugs. reply z500 5 hours agorootparentIf anything, Python encourages you to use loops because the backwards arrangement of the arguments to map and filter makes it painful to chain them. reply sgarland 5 hours agorootparentmap(function, iterable) That seems very logical to me, but then, I’m not a functional programmer, I just like map. It’s elegant, compact, and isn’t hard to understand. Not that list comps are hard to understand either, but they can sometimes get overly verbose. filter has also lost ground in favor of list comps, partially because Guido hates FP [0], and probably due to that, there has been a lot of effort towards optimizing list comps over the years, and they’re now generally faster than filter (or map, sometimes). [0]: https://www.artima.com/weblogs/viewpost.jsp?thread=98196 reply BeetleB 3 hours agorootparentYes, but how do you chain them? map(func4, map(func3, map(func2, map(func1, iter)))) vs iter.map(f1).map(f2).map(f3).map(f4) I made up the syntax for the last one, but most functional languages have a nice syntax for it. Here's F#: iter |> f1 |> f2 |> f3 |> f4 Or plain shell: commandf1f2f3f4 reply TylerE 2 hours agorootparentYou don't. Use generator syntax, which is really the more pythonic way to it. >>> iter = [1,2,3,4] >>> f1 = lambda x: x*2 >>> f2 = lambda x: x+4 >>> f3 = lambda x: x*1.25 >>> [f3(f2(f1(x))) for x in iter] [7.5, 10.0, 12.5, 15.0] reply BeetleB 1 hour agorootparentFirst off, writing f3(f2(f1(x))) is painful - keeping track of parentheses. If you want to insert a function in the middle of the chain you have some bookkeeping to do. Second, that's all good and well if all you want to do is map. But what if you need combinations of map and filter as well? You're suddenly dealing with nested comprehensions, which few people like. In F#, it'll still be: iter |> f1 |> f2 |> f3 |> f4 Here's an example from real code I wrote: graph |> Map.filter isSubSetFunc |> Map.filter doesNotContainFunc |> Map.values |> Set.ofSeq This would not be fun to write in List Comprehensions, but you could manage (only two list comprehensions). Now here's other code: graph |> removeTerminalExerciseNodes |> Map.filter isEmpty |> Map.keys |> Seq.map LookUpFunc |> Seq.map RemoveTrivialNodes |> Seq.sortBy GetLength |> Seq.rev |> Seq.toList BTW, some of the named functions above are defined with their own chain of maps and filters. reply Izkata 1 hour agorootparentAn alternative for python is to flip what you're iterating over at the outermost level. It's certainly not as clean as F# but neither is it as bad as the original example if there's a lot of functions: iter = [1,2,3,4] f1 = lambda x: x*2 f2 = lambda x: x+4 f3 = lambda x: x*1.25 result = iter for f in [f1, f2, f3]: result = [f(v) for v in result] Then the list comprehension can be moved up to mimic more closely what you're doing with F#, allowing for operations other than \"map\": result = iter for f in [ lambda a: [f1(v) for v in a], lambda a: [f2(v) for v in a], lambda a: [f3(v) for v in a], ]: result = f(result) And a step further if you don't like the \"result\" reassignment: from functools import reduce result = reduce(lambda step, f: f(step), [ lambda a: [f1(v) for v in a], lambda a: [f2(v) for v in a], lambda a: [f3(v) for v in a], ], iter) reply Izkata 1 hour agorootparentprevYou're not using generator syntax anywhere in that example. reply itishappy 5 hours agorootparentprevDon't Haskell and Python use the same argument order? filter(lambda x: x Granted, the tooling is sh*t. I hear this a lot, but am curious about two things: (a) which bit(s) of the toolchain are you thinking about specifically -- I know HLS can be quite janky but I haven't really been blocked by any tooling problems myself; (b) have you done much Haskell in production recently -- i.e. is this scar tissue from some ago or have you tried the toolchain recently and still found it to be lacking? reply n_plus_1_acc 8 hours agorootparentEverytime I use cabal and/or stack, it gives me a wall of errors and i just reinstall everyrhing all the time. reply simonmic 1 hour agorootparentAnd if you share stack transcripts I’ll look into those for you. I’ve experienced this too, the tools can certainly be improved, but also a little more understanding of what they do and how to interpret their error messages could help you (I am guessing). reply tome 3 hours agorootparentprevIf you share a transcript from a cabal session I'll look into this for you. reply BoiledCabbage 1 hour agoparentprev> Give it a try. Especially, if you don't know what to expect, I can guarantee that you'll be surprised! And I will as strongly as possible emphasize the opposite you should not. If you are are already experienced in functional programming, as well as in statically typed functional programming or something lovely in the ML family of languages then only then does Haskell make sense to learn. If you are looking to learn about either FP in general, or staticly typed FP Haskell is about the single worst language anyone can start with. More people have been discouraged from using FP because they started with Haskell than is probably appreciated. The effort to insight ratio for Haskell is incredibly high. You can learn the majority of the concepts faster in another language with likely 1/10th the effort. For general FP learn clojure, Racket, or another scheme. For statically typed FP learn F# or Scala or OCAML or even Elm. In fact if you really want to learn Haskell is is faster to learn Elm and then Haskell than it is to just learn Haskel. Because the amout or weeds you have to navigate through to get to the concepts in Haskell are so high that you can first learn the concepts and approach in a tiny language like Elm and it will more than save the amount of time it would take to understand those approaches from trying to learn Haskell. It seems unbelievable but ai found it to be very try. You can learn two languages faster than just one because of how muddy Haskell is. Now that said FP is valuable and in my opinion a cleaner design and why in general our industry keeps shifting that way. Monoids, Functors, Applicative are nice design patterns. Pushing side effects to the edge of your code (which is enforced by types) is a great practice. Monads are way overhyped, thinking in types is way undervalued. But you can get all of these concepts without learning Haskell. So that's the end of my rant as I've grown tired of watching people dismiss FP because they confuse the great concepts of FP with the horrible warts that come with Haskell. Haskell is a great language, and I'm glad I learned it (and am in no way an expert at it)- but it is the single worst language for an introduction to FP concepts. If you're already deep in FP it's and awesome addition to your toolbox of concepts and for that specific purpose I highly recommend it. And finally, LYAH is a terrible resource. reply jillesvangurp 7 hours agoparentprevI think the tooling being not ideal is a reflection of how mature/serious the community is about non academic usage. Haskell has been around for ages but it never really escaped its academic nature. I actually studied in Utrecht in the nineties where there was a lot of activity around this topic at the time. Eric Meyer who later created F# at MS was a teacher there and there was a lot of activity around doing stuff with Gopher which is a Haskell predecessor, which I learned and used at the time. All our compiler courses were basically fiddling with compiler generator frameworks that came straight out of the graduate program. Awesome research group at the time. My take on this is that this was all nice and interesting but a lot of this stuff was a bit academic. F# is probably the closest the community got to having a mature tooling and developer ecosystem. I don't use Haskell myself and have no strong opinions on the topic. But usually a good community response to challenges like this is somebody stepping up and doing something about it. That starts with caring enough. If nobody cares, nothing happens. Smalltalk kicked off a small tool revolution in the nineties with its refactoring browser. Smalltalk was famous for having its own IDE. That was no accident. Alan Kay, who was at Xerox PARC famously said that the best way to predict the future was to invent it. And of course he was (and is) very active in the Smalltalk community and its early development. Smalltalk was a language community that was from day one focused on having great tools. Lots of good stuff came out of that community at IBM (Visual Age, Eclipse) and later Jetbrains and other IDE makers. Rust is a good recent example of a community that's very passionate about having good tools as well. Down to the firmware and operating system and everything up. In terms of IDE support they could do better perhaps. But I think there are ongoing efforts on making the compiler more suitable for IDE features (which overlap with compiler features). And of course Cargo has a good reputation. That's a community that cares. I use Kotlin myself. Made by Jetbrains and heavily used in their IDEs and toolchains. It shows. This is a language made by tool specialists. Which is why I love it. Not necessarily for functional purists. Even though som Scala users have reluctantly switched to it. And the rest is flirting with things like Haskel and Elixir. reply pyrale 5 hours agorootparent> I think the tooling being not ideal is a reflection of how mature/serious the community is about non academic usage. I'd say it's more of a reflection of how having a very big company funding the language is making a difference. People like to link Haskell's situation to its academic origins, but in reality, most of the issues with the ecosystem are related to acute underfunding compared to mainstream languages. reply jillesvangurp 5 hours agorootparentOne doesn't happen without the other. Haskell is hugely influential with it's ideas and impact. But commercially it never really took off. Stuff like that needs to come from within the community; it's never going to come from the outside. reply pyrale 3 hours agorootparent> Stuff like that needs to come from within the community Either the community is large enough for it, or it comes from the sponsoring company. Few languages start off by being in the first situation. The first example that comes to my mind (Python), well... Tooling was a long and painful road. And if the language hadn't been used/backed by many prominent companies, I don't see how man-hours would have flowed into tooling. reply odyssey7 6 hours agorootparentprev“Greece, Rome’s captive, took Rome captive.” The languages of engineering-aligned communities may appear to have won the race, though they have been adopting significant ideas from Haskell and related languages in their victories. reply mrkeen 5 hours agorootparentSomething went wrong in the adoption process. Haskell's biggest benefit is functions, not methods. To define a function, you need to stop directly mutating, and instead rely maps, folds, filters, etc. The bargain was: you give up your familiar and beloved for-loops, and in return you get software that will yield the same output given the same input. So what happened with the adoption? The Java people willingly gave up the for-loops in favour of the Streams/maps/filters. But they didn't take up the reward of software that yields the same input given the same output. What's something else in the top-5 killer Haskell features? No nulls. The value proposition is: if you have a value, then you have a value, no wondering about whether it's \"uninitialised\". The penalty you pay for this is more verbosity when representing missing values (i.e. Maybe). Again, the penalty (verbose missing values ie. Optional) was adopted, and the reward (confidently-present values) was not. reply odyssey7 2 minutes agorootparentThe type system is a big part and elements of that have shown up elsewhere. I’m with you on the belief that we should have better adoption for immutability, pure functions, and equational reasoning. JavaScript promises can work analogously to the Maybe monad if you want them to. Swift’s optionals are pretty much the Maybe monad. Nelkins 2 hours agorootparentprevPretty sure F# was created by Don Syme, not Erik Meijer. reply 0x3444ac53 2 hours agoparentprevWould you mind explaining what you mean by stateless? reply jgwil2 22 minutes agorootparentHaskell functions are pure, like mathematical functions: the same input to a function produces the same output every time, regardless of the state of the application. That means the function cannot read or write any data that is not passed directly to it as an argument. So the program is \"stateless\" in that the behavior does not depend on anything other than its inputs. This is valuable because you as the developer have a lot less stuff to think about when you're trying to reason about your program's behavior. reply cies 6 hours agoparentprev> Haskell has had a profound impact on the way I think about programming and how I architect my code and build services. Exactly the same for me. > Granted, the tooling is sh*t. Stack and Stackage (one of the package managers and library distribution systems in Haskell-land) is the best I found in any language. Other than that I also found some tools to be lacking. reply dario_od 4 hours agorootparentWhat makes you say that stack is the best you found in any language? I use it daily, and in my experience I'd put it just a bit above PHP's composer reply moomin 8 hours agoparentprevSum types are finally coming to C#. That’ll make it the first “Mainstream” language to adopt them. Will it be as solid and simple as Haskell’s implementation? Of course not. Will having a backing ecosystem make up for that deficiency? Yes. reply SkiFire13 8 hours agorootparentWhat counts as mainstream for you? Java has recently added sealed classes/interfaces which offer the same features as sum types, and I would argue that Java is definitely mainstream. Kotlin has a similar feature. It might be used less than Java, but it's the default language for Android. Swift has `enum` for sum types and is the default language for iOS and MacOS. Likewise for Rust, which is gaining traction recently. Typescript also has union/sum types and is gaining lot of traction. reply thfuran 3 hours agorootparentSealed classes still won't let you have e.g. String|Integer, though I'll grant you that java is certainly mainstream. reply kagakuninja 1 hour agorootparentScala 3 has had union types for 4 years now. Scala can be used to do Haskell style pure FP, but with much better tooling. And it has the power of the JVM, you can fall back to Java libraries if you want. reply zozbot234 6 hours agorootparentprevFor that matter, PASCAL has had variant records (i.e. sum types) since the 1970s. reply iso8859-1 6 hours agorootparentDid it have an ergonomic way to exhaustively match on all the variants? Since the 70s? How does the ABI work? If a library adds a new constructor, but I am still linking against the old version, I imagine that it could be reading the wrong fields, since the constructor it's reading is now at a different index? reply n_plus_1_acc 8 hours agorootparentprevRust is mainstream, just not use in enterprise applications reply pjmlp 6 hours agorootparentprevNot really, other mainstream languages got there first. reply pid-1 5 hours agorootparentprevPython has sum types optional_int: intNone = None reply TwentyPosts 5 hours agorootparentThis is semantically not the same as a sum type (as understood in the sense of Rust, which is afaik the academically accepted way)! Python's `AB` is a union operation, but in Rust a sum type is always a disjoint union. In Python, if `A = B = None`, then `AB` has one possible instance. In Rust, this sum type has two possible instances. This might not sound like a big deal, but the semantics are quite different. reply nicoburns 5 hours agorootparentprevEvery dynamically typed language effectively has one big Sum type that holds all of the other types. IMO this is one reason why dynamic languages have been so popular (because Sum types are incredibly useful, and mainstream statically typed languages have historically had very poor support for them). reply robertlagrant 7 hours agoprevI, like probably many people, like the idea of Haskell, but don't need a bottom-up language tutorial. Instead, I need: - how easy is it to make a web application with a hello world endpoint? - How easy is it to auth a JWT? - Is there a good ORM that supports migrations? - Do I have to remodel half my type system because a product owner told me about this weird business logic edge case we have to deal with? - How do I do logging? Etc. reply justinhj 32 minutes agoparentThat sounds valuable too but maybe it comes after the basic concepts or you may find people immediately dismiss it. There is all kinds of extra syntax and baggage that may seem pointless at first. reply gtf21 7 hours agoparentprev> - how easy is it to make a web application with a hello world endpoint? If that's all you want it to do, it's very easy with Wai/Warp. > - How easy is it to auth a JWT? We don't use JWTs, but we did look at it and Servant (which is a library for building HTTP APIs) has built in functionality for them. > - Is there a good ORM that supports migrations? There are several with quite interesting properties. Some (like persistent) do automatic migrations based on your schema definitions. Others you have to write migration SQL/other DSL. > - Do I have to remodel half my type system because a product owner told me about this weird business logic edge case we have to deal with? I think that's going to really depend on how you have structured your domain model, it's not a language question as much as a design question. > - How do I do logging? We use a library called Katip for logging, but there are others which are simpler. You can also just print to stdout if you want to. reply robertlagrant 7 hours agorootparentThank you! What I was more saying was that an article like this would do better showing some practical simple examples, that would let people do things, rather than bemoaning how Haskell is viewed in 2024. reply gtf21 5 hours agorootparentOh! I hope I wasn't bemoaning too much -- that was the lead-in, but it's mostly about what I really like about the language (and had some examples but I also didn't want to write a tutorial). reply gtf21 3 hours agorootparentprevFor reference (in case it's helpful), my website (where this essay is hosted) is written in Haskell and is basically a fairly simple webserver. For the \"hello world\" webserver, this might be a bit instructive: https://github.com/gfarrell/gtf.io/blob/main/src/GTF/Router.... reply kccqzy 2 hours agoparentprevYou can't do any of that without having first understood a bottom-up introduction. There are so many web frameworks from Yesod to Scotty to Servant (these are just the ones I've used personally) but you can't use any of them without at least an understanding of the language. reply Ericson2314 1 hour agoparentprevhttps://haskell-beam.github.io/beam/ is fantastic, but good luck understanding it if you don't already know some Haskell reply reidrac 3 hours agoprevI like haskell. Actually, let me rephrase that: I like GHC2021. And I have found that's one of the tricky bits with Haskell, together with the language being in very active development, wich makes upgrading your compiler a thing. reply agentultra 5 hours agoprevIt's really good for boring, line of business software (BLOBS). The vast majority of business logic can be modelled with a handful of simple types and pattern matching. Very few design patterns are needed. And if you keep to the simple parts you can even teach the syntax (just the types) to non-technical contributors in an afternoon. Then they can read it and help verify that you implemented the business process correctly. It's also just nice for how my brain works. I like being able to substitute terms and get an equivalent program. Or that I can remember a handful of transformation rules that often get me from a first cut of a program to an efficient, fast one. And it's just fun. reply mumblemumble 4 hours agoparentIt is. But I think that, for that purpose, I like F# even better. Even beyond getting access to the .NET ecosystem, you also get some language design decisions that were specifically meant to make it easier to maintain large codebases that are shared among developers with varying skill levels. Lack of typeclasses is a good example. Interface inheritance isn't my favorite, but after years working as the technical lead on a Scala project I've been forced to concede that haranguing people who just want to do their job and go home to their family about how to use them properly isn't a good use of anyone's time. Everyone comes out of school already knowing how to use interfaces and parametric polymorphism, and that is fine. reply agentultra 4 hours agorootparentI adore Scott Wlaschin's work [0] -- that's where I picked up on the acronym, BLOBS! F# is super cool, I agree. [0] https://www.youtube.com/watch?v=Up7LcbGZFuo reply vips7L 4 hours agorootparentHis book Domain Driven Design Made Functional is really good. It really opened my eyes on DDD. reply tome 3 hours agorootparentA book I find truly wonderful! If I was going to recommend one book about how to design software, it would be this one. reply chefandy 4 hours agorootparentprevAnecdotally, the handful of people I've known that worked in commercial Haskell shops, after the initial honeymoon period intensified by actually finding a paying Haskell dev job, wishes they were using a more practical \"happy medium\" FP language. I don't know anyone that's used F# in production, but nobody I know that's worked in Elixir, erlang, or elm environments has expressed the same frustration. reply 1-more 2 hours agorootparentMany of my colleagues would describe themselves as taking pay cuts to write Haskell provisioned with Nix with type-safe interop with Ruby and our frontend. If you're into it, you're into it. And it has the effect of putting absolute mutants on your team. reply tome 4 hours agorootparentprevInteresting. I wonder where you met them. I've worked with tens of Haskell programmers in my career, most of whom were sad if they were required to stop working in Haskell. I've never met anyone who actively sought out a Haskell job and then subsequently wanted to stop working in Haskell. reply IWeldMelons 3 hours agorootparentprevJane Street famously uses Ocaml, which is, granred, not F# but closee enough/ reply gavinray 4 hours agoparentprevOnly on HN will you read someone unironically suggest writing LOB software in Haskell. reply bunderbunder 3 hours agorootparentI am not prepared to hunt down the citation, but several years back I stumbled across a paper that was trying to compare the effectiveness of various languages for grinding out \"domain logic-y\" code. Among the ones they evaluated, Haskell came out on top in terms of both time to get the work done and correctness of the implementation. IIRC this was testing with students, which would be both a strength and a weakness of the experimental design. reply tome 4 hours agorootparentprevWhy not? Many of us do it every day. reply gavinray 4 hours agorootparentLet's suppose that you and I are non-technical founders of some medium-size software product. If we were to rank the most important factors in choosing how to build our product, I think we may be able to agree that they're likely: - The talent pool and availability of the language - The ecosystem of libraries and ancillary tools like monitoring/debugging/observability - The speed-of-development vs cost-of-maintenance tradeoff of the language I will give Haskell that it can be rapidly written by those proficient, and tends to have less bugs if it compiles than many languages. But for \"what language is easy to employ and has an expansive ecosystem + tooling\", I feel like you have to hand it to Java, .NET, Python, TypeScript, Go, etc... reply tome 3 hours agorootparentThat's shifting the goalposts somewhat! Can Haskell be used for LOB software. Yes! In fact it's the one I am most effective in for that purpose. If I was starting a startup, it would be in Haskell, no question. \"Let's suppose that you and I are non-technical founders of some medium-size software product ...\" Well, that's something else entirely. reply rebeccaskinner 3 hours agorootparentprevI think you're taking a particular view of things that can work, but it's not the only correct view. > The talent pool and availability of the language There are certainly more Javascript or Python developers out there than Haskell developers, but I think it's wrong to imply that Haskell is a hard language to hire for. There are more people out there who want to work with Haskell than there are Haskell jobs, and picking Haskell can be a really great way to recruit high quality talent. It's also quite possible to train developers on Haskell. A lot of companies hire people who don't have experience with their particular language. The learning curve for Haskell may be a bit steeper, but it's certainly tractable if you are hiring people who are eager to learn. > The ecosystem of libraries and ancillary tools like monitoring/debugging/observability Other languages have _more_ of these, but it's not like Haskell is missing basic ecosystem things. I actually find that Haskell is pretty nice with this stuff overall. It's not quite as automatic as what you might get with running something in the JVM, but it's not that big of a lift, and for a lot of teams the marginal extra effort here is more than worth it because of the other benefits you get from Haskell. > The speed-of-development vs cost-of-maintenance tradeoff of the language Haskell is really excellent here in my experience. You can write unmaintainable code in any language, but Haskell gives you a lot of choice in how you build your application, and it makes refactoring a lot nicer than in any other language I've used. You don't get some of the nice IDE features to rename things or move code around automatically, but working in a large Haskell codebase you really do start to see ways that the language makes structural and architectural refactoring a lot easier. > But for \"what language is easy to employ and has an expansive ecosystem + tooling\", I feel like you have to hand it to Java, .NET, Python, TypeScript, Go, etc... Those are all perfectly good choices. I think what people tend to overlook is that Haskell is _also_ a perfectly good choice. Everything has tradeoffs, but Haskell isn't some terrible esoteric choice that forces you to leave everything practical on the table. It really is useful day to day as a general purpose language. reply mchaver 3 hours agorootparentprevI am unironically being paid to do it! My experience is Haskell is one of those ecosystems that has a greater talent pool than there are available positions. I feel like cost of maintenance is pretty nice because you have less bugs. You may have to roll up your sleeves and get your hands dirty to update open source libraries or make stuff that is missing, but code reliability seems to be worth it. reply jose_zap 3 hours agorootparentprevWe do that at our company, it's been great reply __MatrixMan__ 3 hours agorootparentprevNot just HN, Cardano's smart contract language, Plutus, is based on Haskell. reply darby_nine 4 hours agoparentprevIf that's what you're looking for, why not rip out most of the language? You'll end up with something that looks a lot like Elm. You'll end up with a purely deterministic program with no i/o (albeit with a kind of crappy debugging experience). reply agentultra 4 hours agorootparentWell because you need the rest of the language to make your program tell your system to do stuff. Turns out `IO` is the most essential and useful bit of a Haskell program. That part can be left to the programmers. Haskell has a lot of facilities for making that nicer to work with as well. I find that when I tell folks I work in Haskell full-time you can see their opinion of you change on their face. I'm not some kind of PhD genius programmer. I'm pretty middle-of-the-road to be honest. It's just nice to have a language that makes the work of writing BLOBS straight-forward. reply darby_nine 4 hours agorootparent> Well because you need the rest of the language to make your program tell your system to do stuff. That's not necessary for business logic, though. This would presumably be embedded in infrastructure that handled i/o separately. reply agentultra 3 hours agorootparentI've heard of systems like Roc + Nea taking this to the extreme [0]. Totally a way to go. Haskell, to some extent, can help you structure your program in this way where the business logic is just simple, plain, old functional code. You can write the data marshalling, logging, and networking layers separately. There are a few ways to tackle that in Haskell in varying levels of complexity as you would expect coming from other general-purpose programming languages. [0] https://www.youtube.com/watch?v=zMRfCZo8eAc&t=952s reply ninetyninenine 3 hours agoparentprevHaskell is just hard when you get to the advanced stuff. I mean beyond monads there’s the state monad, lenses, etc. a lot of these are not trivial to wrap your brain around. Like for Java head first design patterns I read it and I’m good. For monads it took me weeks to wrap my head around it and I still don’t understand every monad. Yeah I get a bunch of basic apps can be modeled easily, you get unparalleled static safety but programmers will spend an inordinate amount of time figuring out mind bending algebraic patterns. I think something like ocaml or f sharp are more down to earth. reply agentultra 3 hours agorootparentThe advanced parts of most languages can get hairy. Don't mistake familiarity with complexity. Even Java has hard, dense code that is difficult to work with and learn. I tend to stay away from the advanced parts of Haskell when writing BLOBS. The advanced stuff is there when you need to write libraries that need generic code that works with types you haven't defined yourself. You learn it as you go when you need to. But when I'm writing BLOBS I mostly stick to using libraries and that's pretty straight-forward. reply maleldil 7 hours agoprevI feel like part of the problem is Haskell's extremism towards purity and immutability. I find some code easier to express with procedural/mutable loops than recursion, and I believe the vast majority of programmers. I think that one thing that makes Rust so successful is its capable type of system and use of many functional idioms, but you can use loops and mutability when it's",
    "originSummary": [
      "Haskell, often seen as \"impractical\" or \"academic,\" is praised for building real-world applications, including web servers, and its features are influencing languages like Python, Rust, and Typescript.",
      "Key benefits include a strong type system that reduces runtime errors, immutable data preventing state mutation issues, and pure functional programming for more predictable and debuggable code.",
      "Haskell's declarative nature, concept reuse, and strong type system enhance productivity and make refactoring safer, while its expressiveness and algebraic data types simplify reasoning about programs."
    ],
    "commentSummary": [
      "Haskell enforces writing total functions but struggles with issues like endless recursion, especially as the ecosystem shifts towards dependent typing.",
      "The language's progress is hindered by numerous ad-hoc extensions, reliance on C, and limitations with the Wasm backend due to GHC's runtime system.",
      "Despite its maturity and practicality, Haskell's standardization and ecosystem lag behind newer languages like Agda, Idris, and Lean, affecting its appeal to developers."
    ],
    "points": 250,
    "commentCount": 332,
    "retryCount": 0,
    "time": 1726128387
  },
  {
    "id": 41517272,
    "title": "NASA Pulls Off Delicate Thruster Swap, Keeping Voyager 1 Mission Alive",
    "originLink": "https://gizmodo.com/nasa-pulls-off-delicate-thruster-swap-keeping-voyager-1-mission-alive-2000497434",
    "originBody": "NASA Pulls Off Delicate Thruster Swap, Keeping Voyager 1 Mission Alive Engineers needed to swap out the clogged thrusters on the spacecraft as it continues its journey through interstellar space. By Passant Rabie Published September 11, 2024Comments (17) 𝕏 Copied! An artist's concept of the Voyager spacecraft. NASA/JPL-Caltech The Voyager 1 spacecraft has been cruising through the cosmos for 47 years, collecting precious data beyond the solar system. All that interstellar travel, however, is taking its toll on the probe. Recently, NASA engineers had to resolve a thruster issue affecting Voyager 1, overcoming a series of obstacles posed by the probe’s aging hardware. The wear-and-tear of space travel has caused Voyager 1’s thrusters to become clogged. A fuel tube inside the thrusters has filled up with silicon dioxide, a side effect of age within the spacecraft’s fuel tank. To help Voyager 1 continue its mission, a team of engineers switched to a different set of thrusters than the one the spacecraft had been relying on, NASA recently announced. Plot twist, that one is clogged, too. Voyager 1 uses its thrusters to point itself towards Earth and keep communication lines open with ground control. The spacecraft has three sets of thrusters, two for attitude propulsion and one for trajectory correction maneuvers. During the beginning of its mission, Voyager 1 needed the different types of thrusters to carry out its planetary flybys, but the spacecraft is now on a straightforward path out of the solar system, therefore it’s no longer picky with which one it uses. In 2002, the mission team at NASA’s Jet Propulsion Laboratory noticed some fuel tubes in the attitude propulsion thruster set were starting to clog. The team switched to the second attitude propulsion set, but that one showed signs of clogging in 2018, which prompted the engineers to rely on the trajectory correction thruster set. Over the past six years, that thruster became even more clogged than the other two when the team made the switch. The opening of the trajectory correction thruster tube was originally 0.01 inches (0.25 millimeters) in diameter, and now it’s been reduced to 0.0015 inches (0.035 mm), or about half the width of a human hair, according to NASA. As a result, the team had no choice but to switch back to one of the two attitude propulsion thrusters. This procedure of switching thrusters would’ve been easy back in the day, but Voyager 1 is no spring chicken and the spacecraft requires more careful handling today. Controllers turned off some unnecessary onboard systems, including some heaters. That strategy successfully reduced the spacecraft’s power usage, but it also caused Voyager 1 to become colder. As a result, turning on the unused thrusters risked damaging them, so the team had to warm them up first using the spacecraft’s non-essential heaters. The dilemma continued, as the spacecraft’s power supply is now so low that turning on the non-essential heaters would require the mission team to turn off something else. Instead, the engineers figured they could turn off the spacecraft’s main heaters for about an hour, just enough time to warm up the thruster. All that effort paid off, and Voyager 1’s needed thruster branch was up and running. It’s clear, however, that the aging interstellar probe requires a lot more to keep up and running. “All the decisions we will have to make going forward are going to require a lot more analysis and caution than they once did,” Suzanne Dodd, Voyager’s project manager, said in a statement. Voyager 1 launched in 1977, less than a month after its twin probe, Voyager 2, began its journey to space. The spacecraft took a faster route, exiting the asteroid belt earlier than its twin, and making close encounters with Jupiter and Saturn, where it discovered two Jovian moons, Thebe and Metis, and five new moons, and a new ring called the G-ring, around Saturn. Voyager 1 ventured into interstellar space in August 2012, becoming the first spacecraft to cross the boundary of our solar system.The spacecraft is currently 15.14 billion miles away (24.4 billion kilometers), flying through interstellar space at a speed of 38,000 miles per hour (61,155 kilometers per hour). All that time and distance away from Earth is making it harder for the mission. Voyager 1 recently recovered from a communication glitch earlier this year after months of sending unusable data to ground control. Voyager 1 fans rejoiced with the spacecraft’s return, but it wasn’t long before engineers had to tend to a new issue. It’s becoming harder to keep the mission going, but NASA doesn’t seem to want to let it go. More: The Voyager Missions Saw a ‘Tsunami’ of solar Activity Sending a Pressure Pulse Into Interstellar Space interstellar probesNASASpace explorationVoyager 1 You May Also Like ScienceSpace Jupiter’s Moon Io Just Sprouted an Enormous New Volcano The area, which has lava flows that stretch for dozens of miles, was barren as recently as 1997. By Adam Kovac Published September 11, 2024 ScienceSpace NASA’s Jupiter Probe Cleared for October Launch After Last-Minute Radiation Tests Europa Clipper is officially slated to launch on October 10, a month from today, following crunch-time reviews of the spacecraft's ability to withstand the Jovian environment. By Isaac Schultz Published September 10, 2024 ScienceSpaceflight NASA Postpones Upcoming Mars Mission, Citing Delays With Bezos’s Big Rocket New Glenn has been in development for over a decade, but it likely won't be ready for launch next month, posing a problem for NASA. By Passant Rabie Updated September 9, 2024 ScienceSpaceflight How to Catch a Glimpse of NASA’s Orbiting Solar Sail The spacecraft is a test run of cutting edge materials that use photons from the Sun for propulsion. By Adam Kovac Published September 9, 2024 ScienceSpaceflight Starliner Is Back on Earth, but Its Crew Remains in Space. What’s Next for Boeing and NASA? A former astronaut explains what Starliner’s return means for the future of NASA, Boeing’s troubled space program, and the crew left behind on the ISS. Michael E. Fossum, The Conversation Published September 9, 2024 ScienceSpaceflight Watch Live as a Busted Boeing Starliner Returns to Earth Without Its Crew The spacecraft will undock from the ISS on Friday at 6:04 p.m. ET after a troubling three months in orbit. You can watch the action live right here, malfunctioning thrusters and all. By Passant Rabie Published September 6, 2024 Latest news Rings of Power Is Smashing a Pick Axe Into Its Slowly Formed Cracks An Asteroid Will Become Earth’s ‘Mini-Moon’ for Two Months Please Stop Putting Your Tech in Checked Bags Buy Microsoft Office For Windows or Mac Once, Use It Forever – Lowest Price of the Year! Mike Flanagan’s Favorite Scary Movie Is Getting a 4K Restoration The First Trailer for Salem’s Lot Offers Classic Stephen King Small-Town Terror Can’t Poop at Work? Why Public Bathrooms Give Us Anxiety Palmer Luckey Uses Anime-Inspired Trailer to Announce New Cruise Missile Latest news Rings of Power Is Smashing a Pick Axe Into Its Slowly Formed Cracks 9/12/2024, 2:25 pm An Asteroid Will Become Earth’s ‘Mini-Moon’ for Two Months 9/12/2024, 2:15 pm Please Stop Putting Your Tech in Checked Bags 9/12/2024, 2:10 pm Buy Microsoft Office For Windows or Mac Once, Use It Forever – Lowest Price of the Year! 9/12/2024, 1:17 pm Shark’s Labor Day: The Ultimate Solution for Pet Owners Seeking a Hair-Free Home 9/2/2024, 7:02 am Labor Day: Treat Yourself to a $59 Single-Serve Coffee Maker for Ultimate Pleasure Solitaire 9/2/2024, 6:41 am No Mercy on Prices: Target Slashes the Roborock Q8 Max+ This Labor Day 9/2/2024, 6:10 am Latest Reviews Huawei’s New Foldable Phone Has Way Too Many Screens 9/11/2024, 2:25 pm MSI’s New Handheld Might Be the Redemption It Needs 9/10/2024, 2:00 pm Apple AirPods 4 Hands On: This Time With Active Noise Cancellation 9/10/2024, 9:32 am Apple Watch Series 10 Hands On: Change Is Coming 9/10/2024, 9:06 am",
    "commentLink": "https://news.ycombinator.com/item?id=41517272",
    "commentBody": "NASA Pulls Off Delicate Thruster Swap, Keeping Voyager 1 Mission Alive (gizmodo.com)223 points by Stratoscope 15 hours agohidepastfavorite61 comments a_e_k 12 hours agoI continue to be fascinated by how they: 1. Are able to diagnose the problems remotely at such distances and on such old hardware. How can they even measure the thruster tube apertures here? 2. Decide what actions to take. It's not like they have a local test device to experiment on is it? (Even if they did I can't imagine how they'd reproduce the conditions of the real thing.) And if they choose poorly, I'd assume the mission's over. There's no replacing Voyager 1 if they brick it. 3. Have such fine control over the hardware. For something built in the 70's when RAM was largely measured in kB, they seem to have an insane amount of flexibility to remotely reconfigure the equipment. Whatever they did, there must have been some real foresight. reply basementcat 10 hours agoparentIn short, the Voyager spacecraft were designed (after considerable design and operational experience from Mariner and Pioneer spacecraft) to be able to operate for a long time largely automatically without too much hand-holding. All major systems are multiply redundant and may be remotely turned on and off. While there has been personnel turnover on the program, it has not been of a magnitude to jeopardize program continuation. Finally, program management has been media savvy and well politically connected ensuring that operations are still funded. (Contrast with other missions such as Magellan to Venus which was deorbited while it still had propellant reserve leaving some portions of the planet unmapped) JPL has (or had?) a \"retiree badge\" program that permitted retired staff to continue to access their office. Many programs benefited from highly knowledgeable personnel essentially continuing to report to the office every day without pay (not being paid comes with the luxury of not having to worry about being laid off if your charge accounts don’t have enough funds!) It was an absolute privilege to learn from these people. reply SSLy 10 hours agorootparentSo, culture preservation is important for success of highly technical endeavours. Don't tell it to your run of the mill MBA. reply basementcat 9 hours agorootparentI think it is inevitable that organizational \"culture\" changes. The tricky part is figuring out exactly what parts need change and what parts don’t. For example JPL used to have beauty pageants (\"Miss Guided Missile\"). More recently, management appears to be trying to adopt policies and procedures from some venture funded commercial space companies. It is not clear how helpful these efforts will be given that these organizations are in fundamentally different businesses. https://www.jpl.nasa.gov/images/slice-of-history-70th-annive... reply fnord77 9 hours agorootparentnext [9 more] [flagged] biofox 8 hours agorootparentAs a male scientist and engineer, I am also on antidepressants :( reply Loudergood 4 hours agorootparentprevWhat a weird thing to say. reply fnord77 1 hour agorootparentwhat a weird society we've evolved into reply malfist 6 hours agorootparentprevI don't think having beauty pageants keep women from being depressed reply ozim 5 hours agorootparentPageants were far and between and everyone knew participants had to prepare and they are not walking like fit/dressed that day to day. Nowadays we have eternal pageant on the internet and lots faking it like they live/sleep/shower in full makeup and perfect fit. reply mistermann 5 hours agorootparentprevI wonder what the fact of the matter is. It'd be rather hilarious if Humans have been shooting ourselves in the feet for the last few decades with many of our social engineering initiatives. And the reproduction rates chart is ugly. reply darby_nine 5 hours agorootparentWho wants to bring a baby onto a sinking ship? reply Loudergood 4 hours agorootparentExactly, it's simple economics. reply ozim 7 hours agorootparentprevRun off the mill MBAs are not allowed anywhere near to stuff as important as Voyager. All the web apps, that run of the mill devs are implementing are nowhere near as important as voyager. I think we are fine ;) reply darby_nine 5 hours agorootparentprevIt also helps that JPL is a public good and not a wealth-extraction machine reply me_me_me 4 hours agorootparentprevbut if someone is paid $0 it must mean they are worthless, surely the excel file doesnt lie reply efitz 4 hours agorootparentNo, you need to think like an MBA. It means that they are the most valuable employees, because their productivity per dollar is infinite. It also means that when you sort all the (non-executive, of course!) employees by total comp in preparation for layoffs to make the budget look better, they are at the bottom of the list. reply me_me_me 4 hours agorootparentoh yes, you are absolutely right :D reply mistermann 5 hours agorootparentprevAny idea how many people took advantage of the retiree badge program, or any individual people who continued to put in substantial hours? reply wildzzz 1 hour agorootparentWe do something similar at my job for the greybeards that were very influential on our current projects. They \"retire\" but are retained as a very part time employee that only get paid if we need to bring them in for a day or two to help answer some questions. The managers love it because they don't need to find work for this person, pay for any benefits, or need to get approvals to pay them like contractors. As long as we keep doing relevant work to their expertise, they will continue being retained. There is a limit for how many hours they can put in because these people are incredibly expensive as they usually retire at the top engineer level and retain the equivalent hourly wage for that position. reply tecleandor 8 hours agorootparentprevYeah! Being almost 50 years old it's not like people is not working there anymore but that probably a bunch of people in the original project has already died! Great forward thinking reply big-green-man 12 hours agoparentprevWhat blows my mind is the organizational knowledge needed after so many years to keep it going. You don't just hand a guy some man pages when he comes onto the project. I'm sure people have aged out, yet they still understand the complexities in the design. That is something we need to understand and prioritize in the systems we build today. reply tverbeure 5 hours agorootparentWhen Voyager failed last year with a CMOS memory error, one of the big problems was that a bunch of low level information was gone or conflicting. For example, they sometimes had to guess assembler instructions because the code printouts were low quality photocopied pages. Or because there were handwritten comments or comments scratched out with pencil without any clue about why it was done. One saving grace was the fact that the architecture and the code space was simple enough so that they could reason through the symptoms and actions to take, something that would have been much harder with modern spacecraft. Check out this amazing talk: https://youtu.be/dF_9YcehCZo?si=W_b3NJ7vgxaYS1__ reply tommiegannert 10 hours agorootparentprevStartup: \"Do you remember when we inserted this quirky module running in AWS? We can use that to implement this next feature. That was a useful decision!\" Voyager: \"Do you remember when our parents inserted this quirky module that has since left the solar system? We can use that to turn it off and on again. That was a mission critical decision!\" reply HeatrayEnjoyer 4 hours agorootparentNot parents for most of us. Grand or great-grandparents. The senior engineers were highly educated and experienced, implying ages 30-50s during development. They are in their 80-100s now. reply exe34 8 hours agorootparentprevthat aws module isn't supported anymore though, we need to npm install half the internet for the 2 line library that replaces it. reply methuselah_in 2 hours agorootparentprevThis thing hits me. True. reply selimnairb 9 hours agoparentprev> It's not like they have a local test device to experiment on is it? I would bet they have one or more simulators (“digital twin” in the parlance of our times). I’d want one simulator to always reflect the current state of the probe (with state data assimilated periodically from the real probe). Then other simulators can be used to test management changes to see how the system might respond. reply onedognight 8 hours agorootparent> I would bet they have one or more simulators Sadly they do not, but they are starting to write one. reply olabyne 9 hours agoparentprevAnd quite recently they also recovered from a faulty ram module, with a radio link speed that is maybe below the kbit/s now ! reply stordoff 6 hours agorootparent> Uplink communications is via S-band (16-bits/sec command rate) while an X-band transmitter provides downlink telemetry at 160 bits/sec normally and 1.4 kbps for playback of high-rate plasma wave data. https://science.nasa.gov/mission/voyager/spacecraft/ The data playbacks were initially transmitted at 7200 bps, but this was dropped to 1400 bps in 2007 (https://voyager.gsfc.nasa.gov/Library/DeepCommo_Chapter3--14... - page 72). RTT ~46 hours (one-way light time 22:49:59, per https://science.nasa.gov/mission/voyager/where-are-they-now/) reply lisper 9 hours agorootparentprevAnd you should see the ping times. reply liamwire 8 hours agorootparentprevI suspect it’s far lower than that. reply tomooot 12 hours agoparentprevI would imagine if the design/assembly information was broadly available (internally) in the past, there's probably one or several \"digital twin\" emulations of the craft, or at the very least specific subsystems of it's computing resources. There must be some kind of analog/simulation of it's software just for proving \"bugfixes\" before upload, like the coms error and subsequent setting of the \"solar system record\" for \"furthest distance remote code update\" earlier this year. reply onedognight 8 hours agorootparentThere isn’t a simulator or digital twin for voyager. It has a bespoke processor made with 74* style logic. One guy will puts together a command and they will have a review where the other engineers will try and independently verify it. Then they copy and paste the command somewhere to “run it”. It happened, fairly recently, that the command had a typo that was caught in review, but the “wrong” pre-review command was used and the attitude became off by so much that they lost contact. It was only by cranking up the power at Goldstone that they got a command through. This fundamentally changed their understanding of the largest angle for which they could still communicate with the spacecraft. They just hadn’t wanted to try larger angles before because it was too risky. reply wildzzz 1 hour agorootparentThat's the great thing about such a simple design, you can actually sit down with pen and paper and verify operations. reply tverbeure 4 hours agorootparentprevForget about a digital twin, they don’t even have an assembler for the CPUs. It’s all hex values. reply tverbeure 5 hours agoparentprevYou should totally check out the talk from a few weeks ago that I link to in my other comment. It answers your 3 questions and more. reply ForHackernews 8 hours agoparentprevThere's a documentary \"It's Quieter in the Twilight\" about the team keeping the Voyager missions alive: https://www.youtube.com/watch?v=8vJT8AW0wYw reply whycome 3 hours agorootparentIncluded with Amazon Prime in Canada reply ornornor 8 hours agorootparentprevLooks very interesting , thanks for sharing reply tverbeure 5 hours agoprev2 weeks ago, Bruce Wagoner from the Voyager program gave a talk at !!Con about how they recovered from the CMOS memory issue that they had a year ago. It’s basically blind debugging with a latency of 45 hours. The talk is amazing and goes through the computer architecture of the spacecraft as well as the challenges of dealing with something that is so old, with so documentation that has conflicting information or unreadable etc. https://youtu.be/dF_9YcehCZo?si=W_b3NJ7vgxaYS1__ reply perihelions 10 hours agoprev- \"A fuel tube inside the thrusters has filled up with silicon dioxide, a side effect of age within the spacecraft’s fuel tank.\" Where the heck do you get SiO2 from on a spacecraft? Some kind of silicone? edit: \"clogged with silicon dioxide, a byproduct that appears with age from a rubber diaphragm in the spacecraft’s fuel tank\"[0] —I'm guessing that is a silicone rubber. I didn't know that rubber can decompose into sand. [0] https://science.nasa.gov/missions/voyager-program/voyager-1/... reply tecleandor 7 hours agoparentSeems like silicone rubber is extracted from SiO2, so maybe it's impurities? Or maybe the Si and O are doing something to leave the polymer? (I have no idea if that last thing is even possible) https://en.wikipedia.org/wiki/Silicone_rubber reply japanuspus 12 hours agoprevThe Voyager mission is such a wild achievement. Both the sublime design and craftsmanship that must have gone into the hardware, and the deep institutional knowledge required to keep it running is awe-inspiring. reply NaOH 11 hours agoprevProbably a better link: https://science.nasa.gov/missions/voyager-program/voyager-1/... reply Twisol 6 hours agoprevI had the extreme pleasure of seeing Bruce Waggoner of the Voyager team give a keynote at !!Con just last month. The recording landed on YouTube just a couple days ago, so this is great timing: https://www.youtube.com/watch?v=dF_9YcehCZo reply qingcharles 12 hours agoprevGreat to see they bought her some more time. I watched this excellent Voyager documentary recently: https://www.itsquieterfilm.com/ reply litoE 11 hours agoprevIt's been operating for 47 years and it still has fuel left to make attitude corrections. I wonder how they managed that feat. reply magicalhippo 6 hours agoparentNot to downplay how impressive the Voyager probes are, but it seems they packed a fair bit of hydrazine. From \"Engineering the Voyager Uranus mission\": While it was not a design requirement, the option for an extended mission past Saturn was always protected, unless it meant compromising a major mission objective at Jupiter or Saturn. Even though the probability of Voyager 2 lasting another five years was calculated to be in the range of 60 to 70 percent -- well below NASA's usual acceptable probability-of-success threshold -- the decision was made to send Voyager on to Uranus. After its Uranus encounter, Voyager 2 still carried 48% of the hydrazine initially loaded into its tanks, eight-and-a-half years before. [1]: doi:10.1016/0094-5765(87)90096-8 (can be found on the hub of science) reply Dennip 9 hours agoparentprevIIRC They have some type of nuclear power. Not like a reactor but something much simpler that generates heat from radioactive decay. reply robbiep 9 hours agorootparentthe flow of electrons to allow systems to work is not the same as the expulsion of gasses to provide thrust reply reacweb 9 hours agorootparentprevhttps://xkcd.com/2115/ The RTGs generated about 470 W of electric power at the time of launch. In 2023, it was 260W. reply turblety 11 hours agoparentprevI presume solar power? reply Tor3 11 hours agorootparentNo, way too little sun out here. The sun is just a dot far away. The Voyagers both use RTGs, radioisotope thermoelectric generators. Decaying plutonium, essentially. That's for the electrically powered systems. The thrusters use liquid hydrazine, which is common for those kind of thrusters. Edit: There's more about that in the NASA link in a sibling comment. reply krige 11 hours agorootparentprevSolar has nothing to do with it. Voyager uses hydrazine, of which over 80% has been used up over the years. They simply use not that much of it as it's not for thrust, but for aiming at Earth more or less. reply gpderetta 10 hours agorootparentprevEven with something like an ion engine (which I'm not sure were available when Voyager was launched), you would need leftover reaction mass. reply meindnoch 11 hours agorootparentprevLol. How would that work exactly? reply JumpCrisscross 7 hours agorootparent> How would that work exactly? Light sails. Laser propulsion. Photon rockets. None of which apply to Voyager. But none of which are laughably dismissible. reply Buttons840 6 hours agoprevI've heard good things about some Voyager documentaries, and I've wanted to watch one with my daughter, but NASA keeps making the documentaries out-of-date and incomplete. How many amazing stories will there be to tell by the time Voyager is truly beyond our knowledge? reply ChrisArchitect 4 hours agoprevOfficial release: https://science.nasa.gov/missions/voyager-program/voyager-1/... (https://news.ycombinator.com/item?id=41505008) reply onewheeltom 8 hours agoprev [–] Just another day at the office for the Voyager 1 team. Wow. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "NASA engineers successfully replaced clogged thrusters on the 47-year-old Voyager 1 spacecraft, ensuring its continued operation in interstellar space.",
      "The spacecraft faced issues due to aging hardware and silicon dioxide buildup in fuel tubes, but engineers managed to activate an older set of thrusters by carefully managing its limited power and heating systems.",
      "Voyager 1, launched in 1977 and currently 15.14 billion miles from Earth, continues to provide valuable data from beyond the solar system."
    ],
    "commentSummary": [
      "NASA successfully performed a delicate thruster swap to maintain the Voyager 1 mission, showcasing the spacecraft's robust design and remote control capabilities.",
      "The mission's success is due to the foresight in design, preservation of organizational knowledge, and the dedication of the team, including contributions from retired staff.",
      "Voyager 1's longevity and adaptability underscore the remarkable engineering and management efforts behind the mission."
    ],
    "points": 223,
    "commentCount": 61,
    "retryCount": 0,
    "time": 1726111354
  },
  {
    "id": 41516476,
    "title": "My business card runs Linux and Ultrix (2022)",
    "originLink": "http://dmitry.gr/?r=05.Projects&proj=33.%20LinuxCard",
    "originBody": "Toggle navigation Dmitry.GR Myself Projects Thoughts Dmitry.GR/ Projects/ LinuxCard My business card runs Linux (and Ultrix), yours can too UPDATES:: See \"Version 2\" Table of Contents Why? Parts selection What to emulate A MIPS primer What system? Let's emulate! The CPU The FPU The MMU MMU basics The MIPS MMU Emulating the MMU efficiently Communication Hypercalls Bring on the hardware! The honeymoon period How not to design a DMA unit Clocks again SD card support Coolness enhancement How it works How a normal DECstation boots How uMIPS boots How uMIPS runs Linux changes Improving performance Instruction cache Improving CPU speed Improving RAM bandwidth Dirty hacks specifically for Linux How to build and use one Building Building from source If you are lazy Using Version 2 Booting Ultrix About Ultrix First time booting Ultrix SCSI LANCE ESAR Memory probing & proper PROM API Ultrix Loader Making Ultrix work Framebuffer Mouse, Keyboard, ... and Tablet Patches Improvements in the emulator USB improvements More perf improvements Removing the TLB refill fast path Cache geometry changes Serial improvements More Floating Point Unit work A bootloader Hardware improvements v1.3 hardware And old hardware too Building from source (updated) The emulator The loader Further Updates Firmware v2.1.1 Firmware v2.2.0 In conclusion Acknowledgements Downloads Comments... Why? A long long time ago (in 2012) I ran Linux on an 8-bit AVR. It was kind of a cool record at the time. I do not think anyone has beaten it - nobody's managed to run Linux on a lower-end device than that 8-bit AVR. The main problem was that is was too slow to be practical. The effective speed was 10KHz, the boot time was 6 hours. Cool, but I doubt that any one of those people who built one of those devices based on my design ever waited for the device to boot more than once. It was time to improve it! So what could I improve? A number of things. First, I wanted the new design to be speedy enough to boot in a few minutes and reply to commands in seconds. This would make using the device practical and not a test of patience. Second, I wanted it to be easy to assemble for anyone. This meant no components with tight spacing, no components with too many pins, and no components with contacts hidden underneath them. A part of this wish was also that someone could actually assemble one, meaning that I had to select components that are actually buyable in the middle of the current ongoing shortage of, well, everything. Additionally, I wanted the device to be easy to interface with. The original project required a USB-to-serial adapter. This would not do. And, finally, I wanted the whole thing to be cheap and compact enough to serve as my business card. Parts selection Some things were pretty easy to decide on. For storage, for example, microSD is perfect - easy to interface with, widely available, cheap. I picked a simple microSD slot that is easy to solder and easy to buy: Amphenol 1140084168. Some choices were a litle harder, but not too much so. For example, I was surely not going to use DRAM again. It requires too many pins, necessitating more soldering than I would consider acceptable, given that I wanted this device to be easy to assemble. SRAM in megabyte sizes does not really exist. But there is a cool thing called PSRAM. It is basically DRAM, but in easy mode. It itself takes care of all the refreshing and externally acts just like SRAM. Ok, cool, but still that would usually be a lot of pins. Right? Enter \"AP Memory\" and \"ISSI\". They make QSPI PSRAM chips in nice SOIC-8 packages. AP Memory has models with 2MB and 8MB of RAM per chip, ISSI has them in 1MB, 2MB, and 4MB sizes. I decided to use these. They are available and my code supports them all! There were some miscellaneous choices, like which regulator to use. I chose MIC5317-3.3YM5TR due to having worked with it before and it being available in my \"random chips\" box. It is also easily available to buy. The USB connector was also a fun choice. I settled on: none. With the proper PCB thickness, one can lay out the board edge to fit into the end of a USB-C cable. I've seen this done before for micro-USB and figured it could be done for USB-C as well. At the end, though, I did not even need to do it, since someone else already saved me the 30 minutes it would have taken. I just had to remember that the board thickness needs to be 0.8mm for this to work. The last choice was the hardest - which microcontroller to use. The criteria were: built-in USB, no more than 32 pins with at least 0.65mm spacing, no pin-less packages, actually available to buy, QSPI support, as fast as possible. I did not get my last two wishes. After much searching and filtering for \"in stock\", I was forced to settle for an ATSAMD21 series chip, specifically the ATSAMDA1E16. It is not fast (specced to 48MHz, I clock it at 90MHz), it has many bugs (especially in its DMA engine), but it can be bought, it is easy to solder, and it'll have to do... UPDATE: another chip is now supported too, see later in this article. What to emulate I could have just taken my old ARM emulator (uARM) and used that. But what's the fun there? I decided to pick a new target. The ideal emulation target will: (1) be a RISC chip so that I have to spend fewer cycles on decoding instructions, (2) have no condition codes (like MIPS) or only set them on demand (like ARM), so that I am not wasting time calculating them every virtual cycle, (3) be 32-bit since 16-bit machines are all funky and 64 bit is a pain to emulate, (4) be known, and (5) have a workable set of GNU tools and Linux userspaces available. This set of requirements actually only leaves a few candidates: PowerPC, ARM, MIPS. I've done ARM, and I had no desire to mess with an endian-switchable CPU, so MIPS it was! This gives rise to the internal name of the project: uMIPS. A MIPS primer MIPS is old - one of the original RISC designs. If you are a RISC-V fanboy(/girl/being), MIPS will look familiar - it is where 99.9994% of the initial RISC-V spec was copied from. The original MIPS was a 32-bit design, optimized for ease-of-designing-it. It has (and does not hide) a delay slot, has a lot of registers, including a hard-wired zero register, and does not use condition codes. The original design was R2000, back in 1986, followed soon by the improved R3000 in 1988. These were the last chips implementing the MIPS-I instruction set. MIPS-II was short lived and only included the R6000, which barely saw the light of day. The real successors were the MIPS-III R4000-series chips, released in 1991. These were 64-bit already in 1991! Clearly, the easiest target would be the R2000/R3000 chips with their simple MIPS-I instruction set. MIPS-I is a rather simple instruction set. So much so that a complete emulator of just the instructions can be written in under 1000 lines of C code without any dirty tricks. The floating point unit is optional, so it can be skipped (for now). The MMU is weird. It is just a TLB that the software must fill manually. This may seem like a rather unusual choice, but in reality it is a clever one, if you're in 1986 and tring to minimize the number of transistors in your chip. Why have a hardware pagetable walker, when you can make the software do it? You may ask how it handles the situation where the code that would do the walking is itself not mapped? Well, a part of physical memory is always hard-mapped at a certain address, and all exception handlers live there. Even if this were not the case, since the software manages the TLB, it would not be hard to reserve an entry for this purpose. The hardware even has support for some \"wired\" entries that are meant to be permanent. More on all of this later. What system? MIPS R2000/R3000 is a processor. A processor does not a complete system make. What system to emulate? I searched around for a cool system and settled on DECstation2100 (or its big brother - DECstation3100). Why bother? It seemed like a simple system that Linux does support. Initially I was not planning to emulate the whole thing. Why? I had no plans to emulate the LANCE network adapter or the SII SCSI adapter. The last part might surprise you, since we will need a disk to use as our root fs. I did later add emulation of both of these parts, to make Ultrix happy. Let's emulate! The CPU MIPS is a rather old instruction set, which shows in a few places. The main one is that it attempts to prevent signed overflow. The normal instructions used for addition and subtraction will cause an actual exception if they cause an overflow. This does not map to how CPUs are used today, so nobody cares, but I still had to emulate it. There are \"unsigned\" versions of the instructions for addition and subtraction that do not do this, which is what all modern compilers will emit on MIPS. I wrote an emulator for the CPU in C first, to allow easy testing on my desktop, while the PCBs were being manufactured. It was not fast, nor meant to be, but it did allow for testing. You can see this emulator in cpu.c. Along the way here, I implemented some features of the R4000 CPU optionally. It turned out that to boot Linux compiled with modern compilers, this is necessary, as the compilers assume these instructions exist. Technically this is a bug. Realistically, I am likely the only person to ever notice. So, which features did I need to add? Likely branches (BxxL instructions), conditional traps (Tcc/TccI instructions), and atomics (LL/CC instructions). Of course, C is not the language one uses when one wants to go fast. I wrote an emulator in assembly too, targetting ARMv6-M (for the Cortex-M0 MCU I chose). I later added a sprinking of enhancements for ARMv7-M (in case I ever upgrade the project to a fancier CPU). This was tested on a Cortex-M7 and worked well too. The assembly emulator core is contained in cpuAsm.S and the ARMv6-M specific parts are in cpuM0.inc I mentioned delay slots earlier. What is a delay slot? Well, back in the day it was considered cool to expose your CPU's pipeline to the world. Just kiding, it was just a way to save some more transistors. Basically, the instruction after a jump will be executed even if the jump happens. This is called the delay slot. A naive way to avoid dealing with this is to place a NOP after each jump instruction. But with a good compiler, the delay slot can be put to a good use in almost all cases. Obviously one cannot place a jump instruction in the delay slot, since the CPU is already jumping somewhere. Doing this is illegal and undefined. An issue arises, however, if the instruction in the delay slot causes an exception of any sort. The CPU will record that the instruction was in the delay slot, and point the exception handler to the jump whose delay slot we're in. There is no way to return to this \"in delay slot\" state, so the exception handler is expected to take steps to somehow execute the delay-slot instruction and then complete the jump. The FPU The DECstation came with an FPU, so that floating point operations would be fast. Back then this was a separate chip, which was optional in a MIPS R2000/R3000 system. Linux, in fact, will more-or-less corectly emulate the FPU if it is not present, but this is slow. I used this mode initially, and even fixed a few bugs in Linux's emulation, but, in the end, I implemented an FPU emulator. This was necessary since it seems like a lot of MIPS binaries I could find all assume the FPU is available and use it freely. I never reimplemented the FPU emulator in assembly, instead calling out to the C FPU emulator when needed. I figure that squeezing a few cycles out of each instruction is meaningless when the actual FPU operation takes hundreds. The code for this is in fpu.c. I include Linux patches to remove FPU emulation support from the kernel. This saves some RAM. Later, I also added support for a \"minimal\" FPU - it supports the registers but no operations. This is allowed by the spec, since the FPU may refuse to execute any operation it is \"not sure it can do perfectly correctly\", so any compliant OS must implement a full FPU fallback anyways. Why? This saves 16K of code size in the binary, opening the possibility of running uMIPS on smaller devices yet. The MMU MMU basics (this is a very oversimplified summary, feel free to skip if you know this, and do not complain to me that it is not perfectly accurate!) Most CPUs access memory using virtual addresses (VA). The hardware works in terms of physical addresses (PA). Ability to map one to the other is the underpinning of memory safety in modern operating systems. The purpose of an MMU (Memory Management Unit) is to translate virtual addresses to physical addresses, to allow for this mapping. Normally this is done using a tree-like structure in RAM, called a pagetable. Most CPUs have a component whose job it is to walk that structure to resolve what physical address a given virtual address maps to. This component is a pagetable walker. In most cases the pagetable has 3 or 4 levels, which means that resolving a VA to a PA requires reading 3 or 4 words from main memory. Clearly you do not want to do 3 useless memory accesses for every useful one. So usually another component is included in an MMU - a TLB (Translation Lookaside Buffer). Basically you can think of a TLB as a cache of some of the current pagetable's contents. The idea is that before you go off doing those 3-4 memory reads into the pagetables, you can check and see if the TLB has a matching entry. If so, you can skip the pagetable walk. Clearly, like any cache, the TLB needs to stay in sync with the things it caches (the current pagetables). So, if the OS changes the pagetables, it needs to flush the TLB, since it might have stale entries. Usually, TLBs expose very little interface to the CPU, so there isn't a way to go read all the entries and remove only the newly-invalid ones. Additionally, this would be slow, so this is not usually done. However, invalidating the entire TLB also has costs - it needs to be re-filled, at the cost of 3-4 memory accesses per entry. This could hurt performance. A solution commonly used is called an ASID. What are the four main cases when pagetables might be modified? (1) Adding a new mapping over a virtual address that previously was not mapped to anything, (2) changing permissions on on existing mapping, (3) removing a mapping, and (4) entirely changing the memory map (for example to switch to a completely different process). In case 1, no TLB flush is necessary, since no stale TLB entry can exist. Cases 2 and 3 do indeed require flushing the TLB, but they aren't that common. Case 4 is quite common, though. It is done at every context switch. One might point out that since we're changing the entire memory map, the entire TLB would be invalid, and thus flushing it isn't a problem. This is wrong. Besides mapping userspace things, the MMU also maps various kernel structures, and there is no point penalizing them. If we could somehow tag which entries in the TLB go with which process, and temporarily disable them when another process runs, we could avoid a lot of context-swich flushing and the performance costs imposed by it. It would also be cool if we could tag entires that belong to the kernel and are valid in every process. Well, this exact technology exists in many MMUs. The idea here is that each pagetable entry will have a bit marking it as \"global\" (valid in all memory maps) or not. There should also be a register in the CPU setting the current ASID (Address Space ID). When a TLB entry is populated from the pagetables, the current ASID is recorded in it. When a lookup in the TLB is done, only entries matching the current ASID or those marked \"global\" will match. Cool! The MIPS MMU The idea at the time was to save transistors. Which of the above could be cut? Well, cutting out the TLB guarantees terrible performance in all cases. But that pagetable walker, do we really need it? What if we make the sotware do it? We can add a little bit of assistance, like ability to manage the TLB efficiently, but skip on the pagetable walker hardware. This is what MIPS did. Here is the MIPS virtual address space: Addresses Name Mapping 0x00000000..0x7fffffff kuseg mapped via MMU 0x80000000..0x9fffffff kseg0 mapped to physical 0x00000000..0x1fffffff, cached if there is a cache, only accessible in priviledged mode 0xa0000000..0xbfffffff kseg1 mapped to physical 0x00000000..0x1fffffff, not cached, only accessible in priviledged mode 0xc0000000..0xffffffff kseg2 mapped via MMU, only accessible in priviledged mode So, as you can see, some VAs do not map via the MMU at all. This means that code living there is able to run no matter the state of the MMU. Linux and Ultrix, predictably, put the kernel in kseg0. The kernel does, however, need to be able to dynamically map things in as well. kseg2 is one gigabyte of address space that is mappable via the MMU that the kernel can use. Memory-mapped devices will usually be accessed via kseg1. The 2 gigabytes at the bottom of the address range(kuseg) are for userspace tasks. What entry in a TLB should one replace when one needs to insert a new entry? An obvious answer might be \"the one least recently used\", but that would require tracking use, which costs transistors too. A simplification is \"the one least recently added\". This is easy, but it hides a fatal flaw. Imagine your TLB has N entries, and your workload sequentially uses N + 1 addresses, such that each would need a TLB entry. Now you'll always be replacing the entry you're about to need, guaranteeing that you NEVER hit the TLB and do a lot of pointless pagetable walks. How do we avoid this? The simplest method is replace a random entry. Sure, it might be the entry you're about to need, but for an N-entry TLB the chances are 1/N. Generating random numbers is slow in software, so MIPS R2000/R3000 provide some help. The CPU has a register called, literally RANDOM which is supposed to be constantly incrementing, every cycle. Since the \"when\" of \"when will you next need a new TLB entry\" is not predictable, this is as good as random, and requires very few transistors. The idea is that whenever you need to replace a TLB entry, you use a special instruction TLBWR to write to a random entry. I did not tell you about ASIDs by accident either. The MIPS R3000 MMU implements a 6-bit ASID. Emulating the MMU efficiently Emulating the R3000 MMU is a bit of a pain. Since any entry can be in any location, the proper way to do a lookup is to check each one. Doing a 64-cycle loop for every memory access is a non-starter speed-wise, of course. I use a hashtable indexed by the virtual address to keep all the TLB entries in buckets for faster checking. Using 128 buckets virtually guarantees that most buckets have zero or one entry in them, permitting much faster lookups. Initially this was a simple table of pointers, but this used too much RAM, so now it is a table of indices. Communication The DECstation had a few ways to communicate with the outside world. It had a built-in network card, which I do not emulate. It was optional, and I haven't found a use for it yet. Maybe I will later - it does not look complex. It also had a SCSI controller which one could attach hard disks and other SCSI peripherals to. Emulating this would be a fun challenge, and I'll probably get to it later, but I did not do it now - it was not necessary - I wrote a paravirtualized disk driver for Linux using hypercalls, more on this later. There was also an optional framebuffer card one could install that added support for a monochrome or a color display. Emulating these would also not be too hard, but my business card lacks a display, so I did not do it either - plus I am not even sure that Linux can make a use of it. The last method of communications that the DECstation had was DC7085 - a serial port controller that is basically a clone of a PDP11-era DZ-11. It supports four serial ports at a blistering 9,600bps speed (or any integer division thereof). Each serial port was allocated a purpose, and they were wired to different connectors indicating this purpose. #0 was for the keyboard, #1 for the mouse, #2 for modem, and #3 for printer. To the machine they are all the same, this was just the purpose DEC assigned to them. The stock PROM would use #3 as serial console instead if it did not detect a keyboard at #0, thus it is customary to use #3 as serial console for Linux on the DECstation. My PROM surrogate does not bother looking for or supporting external keyboard, and just defaults to serial console on #3. That being said, since it is cool to allow multiple login sessions, I also export #0 #2 as a second virtual serial port, so that you may login from two serial consoles at once, and do two things at once. How cool is that? So, how do I export these serial ports? When you connect the card to a computer, it'll show up as a USB composite device comprised of two CDC-ACM virtual serial ports. One of them is port #3, another is port #0 #2 on the virtual DZ-11. How will you know which is which? #3 has the boot console printing and will have the initial sh prompt. If you do not see this, try the other one, computers do not always number them in the order I export them. Hypercalls In the real world the PROM had to probe the real hardware to detect what was present where. As my PROM is running in an emulator, there is no need for such mess. We can simply request things from the emulator in an agreed-upon way. That way is a hypercall - a special invalid instruction that, if encounted in supervisor mode, the emulator will treat as a request for some kind of service. The instruction I chose is 0x4f646776, which is in the COP3 (coprocessor 3) decode space that was not allocated to any real purpose in these chips. The calling convention is close to the normal C calling convention on MIPS: parameters are passed in $a0, $a1, $a2, and $a3, return values are in $v0 and $v1. The $at register gets the \"hypercall number\" - the specific service we're requesting. A few hypercalls are implemented. #0 is used to get the memory map. The parameter is word index of the memory map to read. Word 0 is \"how many bits the memory map bitmap contains\", word 1 is \"how many bytes of RAM each bit represents\", words 2 and on are the bits of the map, up to the total specified in word 0. This can be used to build a memory map that the PROM can furnish to the running OS and allows me to have discontinuous RAM. Linux supports this and I tried it, but did not end up needing it. It is here in case I change my mind and need it again. Hypercall #1 outputs a single byte to the debug console (which is the same as DZ-11 port 3). This is used by the PROM and mbrboot to output debug strings without needing to have a complete DZ-11 driver in there. Hypercall #5 will terminate emulation. This can be used on the PC version of the emulator to quit peacefully. Hypercalls #2, #3, and #4 are used for SD card access. #2 will return card size in sectors, #3 will request a read of a given sector to a given physical RAM address and reply with a nonzero value if that worked. #4 will do the same for a card write. Bring on the hardware! The honeymoon period The first revision of this board came up well initially, after I sorted out the mess that is ATSAMD21's clocking system. I appreciate flexibility as much as the next guy, but this thing is TOO flexible. It took a lot longer than I'd care to admit to get this thing running at a sane speed and to enable some peripherals. The docs were too sparse to be of much use, too. Atmel, what happened to you? You used to have the best docs! The first revision of the board had two memory chips, each on their own SPI bus, an SD card on an SPI bus, and USB with the proper resistors. The USB was perfect. Unlike everyone and their grandmother (STMicro, I am glaring at you), Atmel did not license annoying Synopsis USB IP. They made their own. It is easy to use, elegant, and works well. Seriously, it just worked. In two days I got the hardware to work and wrote a USB device stack. I tip my hat to the team that worked on the USB controller. That being said, I have concerns. My main issue: USB descriptors aren't small. They are constant. I'd prefer to keep them in flash. I'd prefer to, but cannot. The USB unit uses a built-in DMA unit to read the data to send. This DMA unit CAN access flash, but if you have any flash wait states enabled, it sends garbage. I suspect that Atmel only tested it for reading from RAM, forgot that some memories have wait states, and did not account for that. Keeping all my descriptors in RAM is a colossal waste of RAM, which there is only 8KB of. Remember that tiped hat? I rescind it, Atmel. I had to work around the issue by sending the descriptors one piece at a time (rather than letting the hardware DMA it all automatically) just to save the valuable RAM. Using the SPI units directly worked well enough, until I tried to speed them up. Past about 18MHz The received data was garbled (missing a bit or two, all the following bits shifted). No amount of searching found an issue in my code, and all sample code did more or less the same things. My bus analyzer showed no issues. What gives? THIS GIVES (archived)! I was beyond furious when I found this forum post. Here I was, trying to build a fast device, and my SPI bus was going to be limited to the speed of a tired snail calmly strolling through peanut butter! With some more testing I found that the SPI units will work fine to about 16MHz, which I'll have to live with. The SPI units have no FIFOs, so code must manually feed them one byte at a time and read one byte at a time. This means that there is space between bytes on the bus as code wrangles bytes in and out of registers and memory. This is a waste of potential speed. The solution is DMA. Luckily this chip has DMA. Unluckily, it is fucked beyond belief, to a point where I am beginning to suspect that it was designed by a sleep-deprived stark raving lunatic. How not to design a DMA unit A normal garden-variety DMA unit has some minimal global configuration, and a few channels, each independent from the rest. Each channel will usually have a source address, a destination address, a length, and some configuration, to store things like transfer chunk size, trigger, interrupt enable bits, etc. Thus it is common in ARM MCUs to have each channel have precisely these 4 32-bit configuration registeres: SRC, DST, LEN, CFG. This is 16 bytes of SRAM per channel. ATSAMD21 has 12 DMA channels, so that would be 192 bytes of config data for the DMA unit as a whole. Not that much. Well, Atmel was having none of this! Instead, the unit itself only has a POINTER to where in the user RAM all this config data lives. For every transfer, the DMA unit will load its internal state for the active channel from this structure in RAM, and then operate on the channel. If another channel's data was already loaded, it will be written out to RAM first. Depending on your experience level, you may already be on your third or fourth \"oh, hell fucking no\" as you read this... Why is this bad? Let's imagine two SPI units being fed by DMA. Each one will have two DMA channels, one for receive, one for transmit. Four channels are active in total. Now what happens as both the SPI units are enabled? Two DMA channels (the transmit ones) will go active and attempt to send a byte. One will go first, then the second. This will generate 14(!!) bus transactions to the RAM! Four to read config data for one channel, one to read the byte to send, four to write back this config data, four to read the config data for the next channel, and one more to read the byte to send. So in order to send 2 bytes, the DMA unit did 14 RAM accesses. Not great. But wait...there's more. Let's take a look what happens next, as the SPI units finish sending this byte and clocking in the received byte, but are also ready for the next byte to send! At this point in time, logically only four bytes need to be moved (two from the units into the receive buffers, two from the transmit buffers into the units). Let's see how this plays out. Remember the DMA unit's internal config data is currently loaded to the second transmit channel's. First, it'll have to do 4 writes to write that data out, then 4 reads to load the first receive channel's structures, one write to memory to write the received byte to RAM, 4 writes to write out this channel's structures out, 4 reads to load structures for receive channel number 2, one write of the received byte to RAM, 4 bytes to write out the config structure for this channel back out to RAM, and then the 14 we already discussed to send the next two bytes. That adds up to 36 RAM accesses to simply read two bytes and write two bytes. All this pain, simply to save the transistors on the 192 bytes of SRAM it would have taken for the DMA unit to store all the config data internally. So, why is this bad? Let's say our MCU is running at its designed speed of 48MHz, its SPI units running at their designed max speed of 12MHz. At the point the second bytes need to be sent and first received bytes need to be received, we'll need to perform 36 accesses to RAM, but also 4 accesses to the SPI unit. The SPI unit is on an APB bus, which means that any access to it takes at least 4 cycles. This means that in between each sent and received byte we'll need 36 + 4 * 4 = 52 cycles. If the SPI unit runs at 1/4 the CPU speed, then it will send/receive a byte every 8 * 4 = 32 cycles. So every 32 cycles we'll need to do 52 cycles' worth of work. When they do not get enough cycles, the DMA channels give up and stop working... Oops... So, what can be done? I worked out a hybrid method where I send data using CPU writes and receive using DMA. This worked for two channels, but would not work for more. Once I got rev2 boards that had 4 RAM chips, even this failed, as just the 4 receive DMA units starved each other of bandwidth and got cancelled. Why was Atmel so damn stingy with internal SRAM? We'll probably never know. But they could have solved this exact issue simpler than with 192 bytes of SRAM in the DMA unit. Just adding a 4-byte FIFOs into the SPI units would do as well, then each DMA transaction could transfer more than a single byte, alleviating this traffic jam. Sadly, apparently nobody at Atmel has even tried to actually use their chip for anything. Atmel, what happened to you? Clocks again My clocking woes were not over yet. This chip has a number of internal oscillators, one of which is supposed to be a rather precise 32KHz oscillator called OSC32K. I wanted to use that as a source clock for a timer to implement my virtual real time clock. Well, despite much pain and many tears, the damn clock would not start... ever. The code should be simple: SYSCTRL->OSC32K.bit.EN32K = 1; SYSCTRL->OSC32K.bit.ENABLE = 1; while (!SYSCTRL->PCLKSR.bit.OSC32KRDY); Yeah... that did not happen. At the end, I decided that I can use a less-precise OSC32KULP to clock my timer. That one did start and I was able to use it. By this point in the project I was worn out, desensitized to this chip's many faults, and completely out of WTFs, so I resigned myself to a slightly imprecise real-time clock and trudged on. SD card support Not really much to say about SD card support. Been there, done that, got the t-shirt. My initial code for the prototype used multi-block reads and writes for better card access speed, but in the final prototype I was forced to abandon it since one of the RAM chips on the b2 boards shared the SPI bus with the SD card, so leaving the card selected was not an option. This was not that big a deal since SD access is rarely, if ever, a bottleneck here. Any card up to 2TB is supported. In the v2 revision of the board I wired up card detect pin to the MCU. It was not used, but I thought that I might find a use for it. I did not, so in v3 boards it was removed. I also added a card \"activity\" LED which lights up when card is accessed. It is simply a LED between the card's chip select line and Vcc. Whenever the card is selected, it is on. This LED also surves a second purpose. If at boot time the SD card or SPI SRAMs fail to initialize, it'll blink out an error code to help identify the problem. Coolness enhancement Now that the prototype worked and I was doing the layout for the final version, I decided to do some things to make it look cool. I buried all the traces in layers 2 and 3, leaving layers 1 and 4 uninterrupted copper. It loooks super cool! Of course the top layer copper is interrupted for the actual SMT pads, but other than that, it is all perfectly smooth and looks amazing! How it works How a normal DECstation boots Normally there is a built in 256KB ROM (called PROM by DEC) at physical address 0x1fc00000 that contains enough code to show messages onscreen and accept keyboard input, talk to SCSI devices, load files from disk to RAM and jump to them. This PROM also provides a lot of services to the loaded operating system via an array of callbacks. This includes things like console logging, EEPROM-backed environment variables, memory mapping info, etc. This is rather similar to UEFI. Normally this PROM would read the environment variables from EEPROM that would tell it which device to boot, and then load a kernel and boot from that device if all goes well. This emulator does not boot this way How uMIPS boots I had no desire to include a large ROM in the emulator, as the flash space in the microcontroller is limited. I also do not have a graphical console or a keyboard per se. That being said, I had to implement a sizeable subset of the PROM somehow, since MIPS Linux uses it. What to do? I decided to come up with my own boot process, which can still work just as well. There is indeed a ROM at 0x1fc00000. This is necessary for rebooting to work from Linux. That rom is tiny - 32 bytes. Its source code is found in the \"romboot\" directory. It merely loads the first sector of the SD card to the start of RAM at 0x80000000 and jumps to it. The first sector of the SD card contains a standard MBR partition table and up to 446 bytes of code. The code that lives here can found in the \"mbrboot\" directory. It is also rather simple. It looks through the partition table for a partition with type byte of 0xBB. If not found, an error is shown. Else, the partition in its entirety is read into RAM at 0x80001000, and then jumped to. This partition can be arbitrarily large, and this is where my implementation of the \"PROM\" lives. The actual size limit on it is placed by the fact that MIPS Linux expects to be loaded at 0x80040000. This is no accident - the first 192K of RAM is reserved for the PROM to use as long as the operating system expects to use PROM's services. Thus the limit on the loader's size is 188K. My PROM implementation's code can be found in the \"loader\" directory. It will search the SD card for a partition marked as active, attempt to mount it as FAT12/16/32, and look for a file called \"VMLINUX\" in the root directory. If found, it will be parsed as an ELF file, properly loaded, and run. Else an error will be shown. As this code has no serious size limits, it implements a proper ability to log to console, printf, and all sort of such creature comforts. As far as PROM services go, it provides console logging, memory mapping info, and reading environment variables, at least enough to make Linux happy. I have not tried to boot other operating systems on uMIPS (yet?). The kernel commandline I pass is rather simple: earlyprintk=prom0 console=ttyS3 root=/dev/pvd3 rootfstype=ext4 rw init=/bin/sh. The first parameter provides for early boot logging via the PROM console, which is useful to see. After the kernel is up, it'll use the third serial port for console. Originally for the DECstation that was the printer serial port, but Linux users on DECstation use that for serial console due to that being the easiest port to convert into a simple serial port. The rest just tells the kernel how to boot. I prefer to boot into sh, and then issue exec init myself, thus the init=/bin/sh How uMIPS runs After all the optimizations (which I'll detail in a bit) the effective speed of my virtual MIPS R2000/R3000 on this infernal ATSAMD21 chip is around 900KHz 1.2MHz. The CPU spends around 8% of its time handling timer interrupts, and thus around 0.83MIPS 1.06MIPS of CPU cycles are left for useful work. With this, the kernel takes around 2 minutes to boot and run sh. Executing busybox's init and getting to the login prompt takes another minute. Overall not too bad. Commands reply instantly, or in a few seconds. It takes gcc around 2 minutes to compile a hello world C program, and I estimate that in a few days' time, one could rebuild the kernel on the device itself, copy it to /boot, and reboot into it. Yes, I do intend to try this and time it did do this and it worked! The emulated real time clock is actually real time, plus or minus the inaccuracies of ATSAMD21's ultra-low-power 32KHz timer. It is ok enough that you will not notice. Try the uptime command. There is just one thing I did not yet address concerning running Linux on uMIPS. The storage. I said that it is an SD card, but surely DECstation had no SD card slot. However, Linux is open source. I simply created my own very simple paravirtualized disk driver which uses a hypercall to talk directly to the emulator and request sectors to be read or written directly into the virtual RAM. To Linux, this looks just like DMA, except instant. The whole implementation of the driver is under 200 lines of code and can be seen in pvd.patch Linux changes I made some changes to the kernel to make life easier. They are provided as patches against the 4.4.292 kernel, and as is a working kernel image. Why that version? Because when I started that project, it was an LTS version of the kernel, and since RAM is short, I wanted the smallest possible kernel, so this was preferable to a later version. The config I am using is available in kernel_4.4.292.config. A config for an even smaller kernel (that requires uMIPS to emulate the full FPU) is available in kernel_4.4.292.config_nofpu I did a lot of work making the kernel as small as possible. Since Linux does not support paging out pieces of the kernel, every byte of kernel code is one byte fewer available to use for user space. I ruthlessly removed options that were not needed. In the end I got the kernel down to just under 4MB, which is pretty damn good, considering that MIPS instructions are not very dense. As part of this work, I made a few code patches. For various reasons (cough..delay slots..cough) the kernel can find itself needing to interpret userspace code, or parse userspace instructions. No matter what kernel configs I gave, the code to handle microMIPS (a future MIPS expansion not known in the days of R2000/R3000) was present. It was wasting space and time trying to handle things that would never happen. The patch useless_exc_code.patch removes this code if the target CPU does not support microMIPS Before I implemented my FPU emulator, I was using the kernel's FPU emulation code that traps and executes FPU instructions. It had a bug. If compiled for a 32-bit MIPS processor it did not properly emulate some FPU instructions that operate on the double type. I believe this is wrong. It was causing crashes in code compiled for R3000. The patch fpu.patch modifies the kernel's MIPS FPU emulator by adding a config option to enable the full FPU emulation even on MIPS-I chips. Due to the differences between the R2000/R3000 and the R4000 the kernel needs to know at build time which CPU it is being built for. If you attempt to run the wrong kind of kernel on the wrong kind of CPU, it only gets far enough to panic about it. Fine, OK, but then why does this flag not affect a lot of TLB-handling code. Both kinds are always compiled in, despite us knowing at build time with 100% certainty that at least half of it will not ever be of any use? The patch tlbex_shrinkify.patch wraps the useless code in checks for the compile-time-selected CPU type and thus removes some kernel code, saving valuable bytes. As uMIPS runs with a real real-time clock, I did not want Linux to spend too much time handling timer interrupts. Normally, a 128Hz timer is used on DECstations by Linux. I added options for 64Hz, 32Hz, and 16Hz timer ticks as well. This reduces effective timer resolution, but effectively unloads the virtual CPU from having to spend most of its time handling timer interrupts. The patch clocksrc.patch does this, and the one called kill_clocksrc_warning.patch silences a pointless warning about timer resolution. If you do build uMIPS with full FPU emulation, there is aso a patch to remove all of the FPU emulation code from the kernel to save a few KB of RAM: fpu.patch. Improving performance Instruction cache One thing the processor will surely do every cycle is fetch an instruction. This means that every cycle begins with a memory access. For us that is a painful subject thanks to Atmel's errata-ridden SPI unit. And not just that, memory translation also needs to happen, and that also takes time. A good way to avoid both of these problems is a VIVT instruction cache. It'll read instructions 32 bytes at a time, and allow us to hopefully often not need to translate addresses or reach for main memory. I allocated 2KB of RAM to this cache. It is 32 sets of 2 ways of 32 byte lines. Whenever memory mappings change, it needs to be invalidated. I do this automatically and thus the running code on the virtual MIPS CPU does not need to know about it. The measured hit rate while booting Linux is around 95%, which is pretty nice for such a small cache. The geometry was determined experimentally by profiling how long a boot takes with various cache geometries. This one was found to be the best. Improving CPU speed ATSAMD21 series is specified to run at 48MHz. In my testing they run perfectly well up to 96MHz, with some specific chips able to hit 110MHz. I found no chip unstable at 96MHz, so I decided to just run at 90MHz, for some safety margin. This immediately got me a pretty serious performance uplift. No, it is not really 100%, since (1) SPI RAM is still limited by the SPI speed limit, and (2) flash memory has wait states which had to increase for the larger speed. But this did give me an honest 65% improvement. Still a good start. Now RAM SPI runs at CPU / 6 = 15MHz. Improving RAM bandwidth Since I could not make the RAM SPI units go faster due to Atmel's incompetence, I decided to go wider! I can drive four units at once. Given, there is overhead to each read and write command, but still this is faster than one or two. My code initially supported one, two, or four RAM chips, but for simplification I dropped that support and now only support four-channel RAM. Quite the statement eh? This microcontroller has four-channel RAM! The emulator accesses RAM in increments of 32 bytes. The RAM read/write commands themselves are 4 bytes each. This means that for a single-RAM chip situation, reading 32 bytes takes (4 + 32) * 8 = 288 SPI bits. In dual-channel configuration it'll take (4 + 16) * 8 = 160 SPI bits, since the command is still 4 bytes long, but we only read 16 bytes from each RAM , for a total of 32. For quad-channel RAM, we thus have (4 + 8) * 8 = 96 SPI bits to read 32 bytes. This is a 66% improvement from the single-channel case! In reality the improvement is less, since quad-channel mode cannot use DMA at all, so it is a bit slower. Real-life measurement shows that quad-channel mode is a 50% improvement over the single-channel case. But still, given this damn chip, any improvement is an improvement I'll take. But, why are all the RAM acceses 32 bytes in size? Well, as you see RAM accesses are slow. A typical 32-byte access takes 140-ish SPI cycles, which is around 12 microseconds. If every access took that long, my emulated CPU would be limited to no more than 85,000 memory accesses per second. That is too slow to be practical. Something had to be done. I decided on a cache. Sadly, my microcontroller has a very limited amount of RAM, so the cache had to be small. I evaluated various cache geometries, and found that a 20-set 2-way cache with 32-byte lines produced the best performance uplift for the emulator. It gets a 91% hit rate while booting the kernel, which is a pretty good payoff for 1.25KB of RAM. With a hit taking around half a microsecond and a miss taking around 12 microseconds, adding this cache improved the average memory access by 87%! Yes, this is effectively an L2 cache. Now, how many emulators do you know that have an L2 cache to paper over the terrible performance of their chosen host hardware, eh? The cache allocates on reads and writes, except for reads and writes of precisely 32 bytes in size. Those are passed through directly because they are either SD card access DMA or icache fetches that do not need to also be cached in this cache. After some more profiling, I rewrote the \"hot\" part of the memory access code in assembly for some more speed gain. GCC may have come a long way since a decade ago, but it still does not hold a candle up to hand-written assembly. I removed support I had for one and two-channel RAM to simplify the hot path as well. So now you need to populate all four RAM slots for the card to boot. If you populate different RAM sizes, the smallest one will dictate the final usable RAM size. The usable RAM size will always be four times the size of the smallest RAM chip. This isn't a big deal, the DECstation came with 4MB of RAM, and could be outfitted with a maximum of 24MB. This card can be outfitted with 32MB, so you'll be living like a king! That being said, due to the size of the Linux kernel, you're not going to get a successfull Linux boot unless you have at least 6MB of RAM, and uMIPS will refuse to boot if that is the case (eg: if you populate 4x 1MB chip). Dirty hacks specifically for Linux Remember how on MIPS the operating system must do its own pagetable walking and filling of the TLB? As you can imagine this happens often. Very often. How could I speed this up without causing any correctness issues? On taking the TLB refill exception, I verify the handler has not changed and matches the expected bytes, if so, I do what it would have done, but in native code, not emulated MIPS. This helps this particular code run quite a bit faster. Correctness is not compromised since this is only done if the handler matches what is expected, byte for byte. I also mentioned that due to how delay slots work, if a CPU takes an exception on an instruction in the delay slot, the kernel must be able to completely emulate that instruction, or in other way execute it and then jump to the right place? Linux uses the fact that MIPS has no PC-relative instructions, except jumps, and it is illegal to place a jump in the delay slot. How? Instead of emulating the delay-slot instruction, Linux copies it out to a special page in memory, where it is followed by a trap. Linux then jumps there in user mode to let it execute, catches the trap, and then re-directs execution where it should go. Now, if this sounds like a giant hassle to you, you are right. What can we do? Well, if an instruction in a delay slot causes an actual exception (like an illegal access, or a TLB refill exception, or some such thing), not much can be done. But what we CAN do is not make things worse. uMIPS will not deliver IRQs before executing an instruction in the delay slot of a branch. At worst, this will delay an IRQ by a cycle, which makes no difference to correctness. The benefit is that this sort of instruction copying and juggling can be done less. How to build and use one Building Now, why you really came here. How do you get one? Well, you could try knowing me personally and asking for my business card, I have a few to give out, but other than that, here is how to do it. You'll need to order the board from a board fabrication place. I am a fan of JLPCB and recommend them. The gerber files I provide come in two flavours. One as you see my card exactly, and one without my name and contact info :). This is a four-layer board, the board house will ask you for layer order, it is: GTL, G1, G2, GBL. At least JLPCB has options to also cover the edge connector in gold for better contact, called \"gold fingers\" and to grind the board edge to 45° for easier insertion. I suggest selecting both of these options - they are free. Remember to set the board thickness to 0.8mm. While you wait for the board to arrive, you'll want to order the parts. You'll need four of the same memory chip (I have the links above), an ATSAMDA1E16, an AMPHENOL 11400841 SD card slot, and a MIC5317-3.3YM5TR regulator. You'll also want to (optionally) order an 0603 sized blue or white LED for SD activity light. If you choose to have that LED, you'll also need a 430 ohm resistor in 0603 or 0805 size. Besides that, you'll need in 0603 or 0805 sizes: 2x 5.1Kohm resistor, 1x 1Kohm resistor, 3x 0.1uF capacitor, and 7x 1.0uF capacitor. You will also need an SD card and any SWD programmer capable of programming the ATSAMD chip. There are many out there. Pick your favourite. You'll need an SD card as well. 128MB is the bare minimum here if you want to fit the busybox-based rootfs in. To fit the debian or hybrid image I am providing, you'll want at least 512MB. You can write the image to the card using your favourite tool for that. On Linux and MacOS that is probably dd, on windows, Win32DiskImager. Once you've assembled the board, program the MCU with the provided binary software/emu/uMIPS.bin and you're done! Building from source You'll want to build a few things. You'll need both an ARM (CodeSourcery) and a MIPS GCC toolchain (I used mips-mti-linux I found online). First, build \"romboot\", \"mbrboot\", and \"loader\". Then, build the kernel. I provided the config, patches, etc. Then you'll want to build the emulator. To build for the MCU, use make CPU=atsamd21(UPDATE: proper target name changed, see updates later in the article). To build for PC, try make CPU=pc. Then you can build the SD card image. You'll want to copy the MBR from one of mine and modify it, then use mkdisk.sh to embed your kernel, mbrboot, and loader. Use a loopback mount to copy in your rootfs. If you want to run the emulator on PC, there are a few things to note. First of all, Ctrl^C will kill it :). Second, unlike the MCU version, the PC version does not incorporate the rom loader in the binary, so you'll need to provide a pointer to it on the command line. A typical command line is ./uMIPS ../romboot/loader.bin ../disk.wheezy If you are lazy For the lazy ones I am trialing selling all the parts and the board together as a kit on tindie. I'll see how this goes. My suspicion is that it'll end up being a giant pain in my ass and not worth the time, but I am giving it a fair shot. EDIT: Apparently not, and not even with a good reason. I quote: Please resubmit for admin approval once you have addressed: Other Reason.. LOL, how about NO? As a sidenote, if anyone knows companies that do this sort of thing for me (sell a kit I designed), please drop me a line by email. If you are really really lazy, I might consider having a batch of these factory-assembled by JLPCB as well. If you are interested, click here and let me know. No promises yet. Using I provide a few disk images. The smallest is the busybox-based one (disk.busybox) - it is small, fast, and cool. I built the busybox from source for MIPS-I with as many applets enabled as I could imagine being needed. The second image is a full debian wheezy (last version to support MIPS-I) rootfs. I should warn you that debian's \"init\" starts around 3000 processes while it boots, so that takes a long time. If you are using the debian disk image (disk.wheezy), I strongly suggest to just mount proc and sys, and do your things in \"sh\" without running \"init\", but it will work if you do ... eventually. I also provide a hybrid image (disk.hybrid). It has a busybox shell and init, but has all of the debian binaries, so things not provided by busybox are still there and work, like gcc and vim. This is the \"hybrid\" image. Using the LinuxCard is easy, insert the SD card, connect USB-C to a computer, and open your favourite serial console app (minicom, PuTTY, etc), if you do not see the boot log, try the other virtual serial port (two exist). In case of a boot error, the SD card LED will blink in an infinite pattern, you can see the code for details on what various numbers of blinks mean. Once you see the shell prompt, you can play around, or continue boot to login by typing exec init. After this you'll be able to login as \"root\" with the password of \"mips\" \"mipsmips\". There will also be a login prompt on the second serial port as well. So cool! Version 2 Booting Ultrix About Ultrix Ultrix is the period-correct UNIX for the DECstation2100/3100. The latest version is 4.5 and with some google-fu you can find ISOs of the install media. It supports the DECstation2100/3100 perfectly, and even has an X11-based UI! The goal of the v2 firmware was to properly run Ultrix on the card. This ended up requiring a lot of work. I had to improve emulation accuracy and implement more hardware. But it did work! First time booting Ultrix My first attempts were simple - copy the kernel to my \"boot\" partition and attempt to load it. It would, of course, not find its root filesystem and panic, but I wanted to see how far I would get at all. The first roadblock was an obvious one - the kernel is not in the ELF format that the linux kernel uses and my loader expects. It is in an older format called COFF. I dug up docs and started working on a parser for COFF. After a little work, I was able to load the kernel and let it run, just to see how far it would go. To my susprise, it got far enough to log some messages to the console! It crashed soon after, when it asked my PROM code for an env variable that I did not know about \"scsiid0\". Not a bad start. At this point I figured that in a week or so I would have Ultrix booting. It took a little longer... Ultrix was designed for this machine, and it was designed to support all parts of it. It does not probe for hardware since it knows that a DECstation2100/3100 should have. It assumes that the requisite hardware is there and starts initializing it. This was a problem for me - I still was not emulating the graphics, SCSI, or the network card. Linux has no support for them so I had not bothered. SCSI As this was my first time attempting to emulate SCSI, it took a while. SCSI is so over-engineered, the very word \"overengineered\" does not do justice to just how much so it it. There are messages, commands, statusses, selects and reselects, and oh so very much more. The SCSI chip in the DECstation2100/3100 is a very strange one that DEC designed just for this device. It is called SII or SMII and I found no docs for it other than the official summary in the DECstation3100 specification. It was helpful, as it listed the register bits and values. It was a start. Watching the Ultrix kernel try to access it before it gave up and paniced provided some more help, and reading the SCSI-I and SCSI-II specs filled in the rest. After much work it seemed like the kernel was happy enough to try to enumerate the bus. It would try to select each device in order. Progress! From there, the next step was to write a virtual SCSI disk. If you haven't dealt with SCSI before, it is rather unlike most sane designs. A sane design would have a host controller be a heavy/expensive/complex machine that talks to cheap simple devices. This makes sense because typically one would have more devices than host controllers. Not here. A SCSI device drives the bus and determines what it does and when. The only thing the host can do is reqest attention from the device. This took a little while to wrap my head around as it is rather backwards. It is actually even more complex since the target device can disconnect from the bus to do things and later reconnect and continue a transaction. It really is quite complex. Luckily, some of that is optional. A device can also reply without disconnecting, and my virtual disk does that. With a lot of work, I was able to figure out the proper state machinery to make Ultrix indeed identify and talk to my virtual SCSI disk. I split the code into two layers. The bottom handles the basics of just being a SCSI device and the top handles actual disk-specific things. The code later got expanded to support emulating a CDROM too, to allow me to do an Ultrix install from a virtual CDROM. While working on this, I noticed that the bus enumeration is slowing down the boot a lot. The issue is that there is no way to detect that \"no device with this ID exists on the bus\". One must attempt a select, and then wait for a timeout. This was taking a while since Ultrix implemented a timeout using a loop with a counter (not using the RTC), and at my virtal CPU speed it was taking seconds. The solution was a dummy SCSI device that does reply to some commands enough to be identified and tell the host that it has no media and is of an unknown type. This device is the \"SCSI nothing\". The SII controller has 128KB of SRAM for DMA-ing data to/from devices. The idea is that one schedules the transfer and it goes on at its pace, when done, an interrupt occurs and data can be copied in/out of this memory. On the PC, this is simple - i can allocate 128KB of RAM and be done with it. On the microcontroller, I do not have that much SRAM, so I steal some memory from my external memory for this, and present less than the full amount to the virtual OS. This works fine for Ultrix as it probes the memory amount page-by-page. Linux probes in 4MB increments, but I have a patch allow_64K_memory_multiples.patch that changes it to probe in smaller increments so that this memory stealing does not cost 4MB of usable RAM. Linux has no support for SII SCSI controller, so it continues to use the pvd device. LANCE The network card in the DECstation2100/3100 is LANCE. It is somewhat documented in the DECstation2100/3100 specification sheet and I implemented it enought to please Ultrix. It never sends or receives any packets (I can add that later), but it does initialize and interrupt as needed. LANCE has a 64KB SRAM buffer for packets. The PC build of uMIPS fully supports this, the \"micro\" build of uMIPS will just ignore writes and produce zero reads of this area to avoid wasting 64KB of memory. This works well enough to please Ultrix. Linux has no support for LANCE, so I have no idea if it would be ok with this setup. ESAR The MAC address for the network card is stored in a on-board EPROM called the \"ESAR\" (Ethernet Station AddRess). It lives at the same address as the real time clock, except it is wired to the upper byte of every word, while the DS1287 is wired to the bottom byte. This is a weird thing to do but it works. It does mean that some weird things are possible, like reading both the ESAR and the real time clock registers at once with one read. Luckily this is not usualy done. The ESAR data has some checksums and redundancy (so that its correctness is easy to verify). I implemented an ESAR for uMIPS, assigned the ethernet address 66:44:22:44:66:22 to the device, and provided for all the required redundancy and checksums. Ultrix is satisfied with this. Memory probing & proper PROM API While booting Ultrix I notied that it directly probed the amount of RAM in the system. This is strange since Linux simply queried the memory amount from a PROM API that conveniently exists for this. This was actually my mistake since I was emulating a much newer PROM iterface than the real DECstation2100/3100 had, and Linux was happy to use it. The newer standard (called REX) provides the OS a function pointer table with a lot of API. To signal REX support, a magic value is also passed. DECstation2100/3100 predate the REX API and used a different method of providing API to the OS - a table of jumps is placed at known offsets from the start of the PROM in the 0xbfcXXXXX address space. This API is also more primitive, and lacks, for example, the ability to tell the OS how much RAM there is. The pieces now fall into place... My only problem is that I do not have an ability to have a huge PROM, as I wrote earlier. I needed another method to offer this API. I decided to indeed have this jump table, but redirect all the jumps to an address in the RAM area reserved for the PROM 0x80001000..0x8002ffff. You'll recall that my OS loader loads there. Now it can provide this PROM API, just like it did the REX API. Cool! Testing Linux also shows that it happily uses this API properly as well. It is, of course, now also forced to probe the RAM amount. No big deal. I did find a bona fide bug in the kernel here! While it means (as per comments) to probe for a maximum of 480MB of RAM, but actually only probes for up to 30. The fix is in fix_mem_limit.patch. Ultrix Loader At this point, the kernel was loading far enough to panic about not finding the root filesystem, so it was time to figure out a good way to make this work. The problem is that Ultrix uses a completely different partitioning system than the well-familiar MBR I had been using. The Ultrix \"disklabel\" allows for 8 \"partitions\" but with some assumptions, like that the first (caled \"a\") is always the rootfs, the second (called \"b\") is always swap, the third (\"c\") always covers the entire disk (yes it does and is expected to overlap others), and another one (\"g\") is /usr. Now, if this was not fun enough yet, the partition table itself is expected to be inside the rootfs partition, and a whole lot of tools (including the installer) assume that this all starts at sector zero. Fun, eh? I spent a lot of time trying to figure out how to make the installer be happy to not start the rootfs at the 0th sector, but this was a lost cause. A large number of scripts involved assume that both the \"a\" and the \"c\" partition start at zero. The kernel also has similar assumptions. With some patching, I got it to work with an offset, but this was not a good approach. I decided to see if I could live with how Ultrix does things, instead of trying to force it to do things my way. Even though the rootfs and the partition table both start at the 0th sector, they both reserve some space up front for \"boot code\". Specifically, the first 16 sectors (8KB) are always free. I decided to simply place my loader there and teach it how to understand the Ultrix disklabel. As part of this work, I refactored the loader into a few pieces. One part was a partition table handler. There is an option for MBR, one for Ultrix, and one for NetBSD disk labels. One of these (build-time determined) is linked in to the loader, as needed. Another module was a binary loader. Two exist: ELF for Linux and NetBSD, and COFF for Ultrix. Same as before, only one is linked into the loader, as needed. The third modue is the filesystem driver. There is one for FAT12/16/32 (used for my Linux boot sequence), one for old UFS (for Ultrix), and one for modern UFS (for NetBSD). Again, just one is linked in, as needed. The cool part now is that I can mix and match these pieces as needed to create a loader for the OS I want to boot. The Linux loader is thus FAT + ELF + MBR, for Ultrix, the loader is UFS.old + COFF + Ultrix disklabel, and for NetBSD, it is UFS.new + ELF + NetBSD disklabel. I was too lazy to implement proper CD-booting, so installing Ultrix is a bit weird. I make a disk image with just the installer kernel (extracted from the CD), in a FAT partition, attach the CDROM to the emulator, and then boot. The installer will then re-partition the disk. For this, yet another loader combination is used: FAT + COFF + MBR. The modularity pays for itself! Making Ultrix work Framebuffer Once I had the Ultrix kernel booting properly, at least in the PC build of uMIPS, I really wanted to get the GUI working. Who wouldn't‽ The framebuffer came in two varieties for this machine. There was a monochrome one and a 8-bit color one. They both supported hardware cursor as well. I implemented most of the normally-used modes in the cursor hardware, but not any test modes. I emulated both the framebuffer types and they both work! The 8-bit framebuffer can display up to 259 colors onscreen at a time, out of a 24-bit palette. That is not a typo. The display itself can display 256 colors, and the cursor has its own 3-entry palette, which need not use any of the same colors. The resolution is 1024x1024 in memory, and 1024x864 onscreen. The remaining memory is free for the OS to use however it wishes. I steal memory from the main RAM, same as for the SII buffer. 128KB is used for the mono framebuffer, and a whole megabyte for the color one. The palette is also stored in stolen ram (just about a kilobyte). Mouse, Keyboard, ... and Tablet Of course, to make this work, I also had to make the keyboard and mouse work. They talk to the DECstation via serial, and the protocol is somewhat known, from various shreds available online. I was able to put together a passable keyboard emulator rather quickly. It is not a dumb keyboard. It has regions of keys, a bell, some lights, and can support differing autorepeat settings per key group. It is actually pretty cool. The mouse is a pretty basic one, with three buttons. I got that working rather quickly. The problem with emulating mice is a well known one - they are relative device, and most OSs apply acceleration to the mouse as you keep moving it to allow for better reach. Now, if you are running another OS, and passing these accelerated movements to it, it will re-accelerate them even more. This ends up being a mess. This is why most virtualization solutions prefer to load an absolute-pointing-device driver into the guest. I was not prepared to hack up Ultrix or find a way to load a different mouse driver in it. But then I noticed that DEC wrote about a \"graphical tablet\" that they were selling, that hooked up to the mouse port. Could it be that Ultrix supports this? Yup... Ultrix does. I wrote an emulator for the tablet and it worked wondefully - no more over-accelerated mouse for me! Sweet! Patches Ultrix assumes that it is booting on a real DECstation2100/3100, and that includes expecting the CPU to have caches. My virtual CPU does not expose caches to the guest OS, and while Linux handles that fine, Ultrix does not. It correctly probes the cache and finds its size as zero. But there is a logic bug in r3_kn01flush_cache, where if the cache size is zero, it gets into an almost-infinite loop. As uMIPS exposes no cache, it makes sense to patch the function away into just a return. There is another function of interest: kn01delay. It is used for short busy-wait delays when dealing with hardware. All of our virtual hardware is instant-fast, and thus no delays are needed. As long as I am patching a kernel, might as well make it faster. There is also a third area of interest - the periodic timer. In Linux, I was able to change the tick to 16Hz, but I cannot build Ultrix from source, so I cannot modify it easily. Ultrix uses a 256Hz tick. At that rate, on uMIPS hardware we'd never get any useful work done while only handling interrupts. I attempted to patch Ultrix to use a 16Hz timer and account for it correctly. This does not work - there are mathematical errors that happen. 64Hz works, but that is still too freqent for the uMIPS hardware to be usefully fast. I ended up patching the init code to set the timer to 16Hz, but accounting code to act like it is 64Hz. This means that \"realtime\" in Ultrix runs 4x slower than actual real time, but this is not really a big deal. Just keep in mind that a sleep 1 will delay 4 seconds and not 1. So how does one even apply such patches? How does one find the proper places to patch? I spent a LOT of time learning about the barely-documented symbol format used in the Ultrix kernel. It worked! I made a working parser for it and was able to properly identify the symbols I needed and to patch the places that needed patching. This was good until I realized that while the installer kernel does ship with symbols, the kernel installed for first boot does not (after first boot, the kernel is recompiled again, with options you choose, and that version DOES have symbols). No symbols means that I cannot use them to find the proper locations to patch. I decided on a different method - binary matching. Look for the proper set of bytes in a row, it should be unique in the kernel. If you find just one case - it's the right one. To save space in the loader (as it is limited to 8KB), I compress the \"pattern to look for\" cleverly. Cool. This is the final approach I used and you can see it in loadUltrix.c. Improvements in the emulator USB improvements After a lot of googling, I learned about interface association descriptors. Turns out that without them, windows will not load the USB CDC-ACM drivers for a device. After adding them, Windows would properly load the driver and it would show up as a COM port. I also learned about the peculiar ways that Windows enumerates devices. Sometimes it'll ask for a descriptor, stating that it'll accept 64 bytes, but after receiving just one 8-byte packet it will reset the bus. This was breaking my USB code, and this is now fixed. Windows now properly supports uMIPS and shows it as two COM ports. Sweet! More perf improvements At the end of emulating every instruction, the emulator jumps \"to the top\", fetching a new instruction to execute. In most cases before this a check is done for whether there is an interrupt pending. This jump was done using a BL - the only long-distance branch available on the Cortex-M0. It takes 3 cycles. The check involved loading a byte from memory (2 cycles), checking if it is zero (1 cycle), and jumping to the interrupt exception creation code if so (1 cycle if not - the common case). That means that the entire \"jump and begin handling the next instruction\" step took 6 cycles. I wanted to make it faster somehow. I decided that if I could free up a register, I could. Some reworking freed r11. There is a parameter you can pass to gcc to tell it to not use a given register in any C code it compiles: --ffixed-r11. Now that this register is not being used by anyone ever, we can do the clever thing. We keep the address of the \"load next instruction and execute it\" label in it. Now we can jump to it using just bx r11. This takes just 2 cycles - 4 cycles saved per virtual instruction - a significant speed up. But what if we do have a virtual interrupt to report? Whenever we have one to deliver, we just set r11 to point to the \"report a virtual interrupt\" label, and whenever the current virtual instruction is done being emulated, the interrupt will be reported and r11 will be reset. There is a bit more machinery needed to make this work, but this is it in general terms, and it does work! I also changed how the TLB hash works (from a table of 32-bit pointers to a table of 8-bit indices) to make the table and each entry smaller (from 24 bytes to 16). This saved a bit under a kilobyte of RAM, which I was able to allocate to the L2 cache. It has now grown from 1.25KB to a full 2KB for a measurable perf improvement! Removing the TLB refill fast path For Linux, I had implemented a fast-path for the TLB refill code - it executed in native code what he TLB refill handler would do. In my measurements it slightly improved performance. With all the other performance improvements I had implemented, it no longer offered a measurable improvement. Plus, it did not help Ultrix at all, by definition. Removing it saved flash space and removed complexity. Less complexity is always better. It is gone. Cache geometry changes Previously, when profiling to find the best L1i geometry, I used the Linux boot process. I decided to try harder. Now I profiled that, gcc compiling some code, a few other Linux binaries, Ultrix boot, and some Ultrix userspace utilities. The result of this investigation was that a direct-mapped L1i is slightly faster than a 2-way L1i cache. The hit rate goes down slightly, but checking only one cache line instead of two speeds up the checking enough to make up for it. I thus reconfigured the cache as a direct-mapped cache. Serial improvements Previously, the emulator would wait a fixed 20ms to send a character to the PC before giving up. I changed this to a permanent wait for the main console. This allows the user to not miss any output if they close their terminal. The emulator also shows its version up front, since it will definitely not be missed now. As of firmware v2.1.1, uMIPS also shows the RAM configuration in terms of the number of chips, each chip's size, and the bit width of the per-chip interface. More Floating Point Unit work I had already implemented a full virtual FPU, but now I wanted to see how necessary it really was. I knew that Linux would run if I emulated no FPU at all and would emulate it. I wanted to see if Ultrix would. It did not - it crashed with an invalid instruction trap in the kernel. This was not all that surprising. Once again, it was compiled for a particular machine - a machine that had an FPU. Its assumption that an FPU exists was sane. But there was still more to investigate. The MIPS spec says that the FPU may refuse to execute any instruction if it is not sure that it can perform it perfectly accurately. Since the spec is not clear on what that really means, basically any OS running on such a MIPS chip must implement a complete FPU fallback, capable of emulating any FPU instruction. But then why am I hitting an exception? The trick is that the FPU must still exist, it must refuse to do math. This is strictly different from not existing at all. I thus implemented a \"minimal\" FPU. It implements the instructions to identify itself, move data in and out of the floating point registers, and load and store floating point registers to memory. Any attempts to do actual floating point math report a \"coprocessor usage exception\" which is the proper way for the FPU to refuse to do math. This worked correctly for Ultrix - it now will not crash at boot, all applications that do floating point math still run, with the kernel emulating the math. I checked and Linux also supports this setup. Thus uMIPS now has three FPU configs that it can be built with: full, minimal, and none. A bootloader As I handed out more and more of these cards, the update story needed to be improved. Not everyone has a CortexProg lying around to reflash the firmare. I decided to make it simple and require as little user interaction as possible. The bootloader is just under 3K, I allocated 4K of flash to it, and relocated the main firmware to start 4K into the flash. So, how does it work? At boot, the bootloader will minimally initialize the SD card, attempt to find a FAT16 partition on it, see if it contains a properly-sized file called FIRMWARE.BIN on it, and if so, the firmware will be flashed from this file. On error, the error number will be blinked out on the LED, repeatedly. On success, a varying-frequency pattern of the LED will be repeated forever. If the card fails to be initialized, if it fails to mount, if the update file does not exist, or if it is not correctly sized, the bootloader will continue to boot the existing firmware, if any exists (some sanity checking is peformed). This means that when you insert a card with my Linux image or the Ultrix image, all will work as expected. Only FAT16 is supported, so some partitioning may be required on larger cards. I can live with that. Hardware improvements v1.3 hardware After reading my original article, a few people wrote in (including in the comments section here, on twitter, and in email) to suggest that maybe I should entirely abandon the shitty SPI units in this chip. Initially I was worried that the SPI unit speed issue was really an IO port speed issue, but a quick test showed that I could toggle a pin at half my CPU clock reliably and get nice square edges. I prototyped bit-banging SPI on the existing board to see what speeds I could attain and it was promising. I then laid out a new board, with different wiring, to allow me to actually use QSPI mode. The images for the new schematics and the layouts are the ones you see here! The ATSAMD21 series features a single-cycle IO port. This optional Cortex-M0+ feature is pretty useful for bit-banging. It really is single-cycle-fast. Normal loads and stores take two cycles minimum on a Cortex-M0+, but ones targetting this kind of a unit take just one. That is how I could toggle a pin at half the cpu speed for my test that I had just mentioned. With big-banging, the trick is to do as few operations per cycle as possible. Given this, it would be ideal to do minimal bit-twiddling. It would be super-awesome if I could wire up the four QSPI chips to GPIOS numbered 0..15, allowing me to just read/write the bottom 16 bits of the GPIO port for simple access. Alas, this was not meant to be. This chip has no contiguous 16 GPIO pins wired to the physical pins, so I settled for wiring RAM0 to GPIO0..3, RAM1 to GPIO4..7, RAM2 to GPIO8..11, and RAM3 to GPIO14..17. Since I will be driving them all together, the clock and chip select lines are all wired together. After all was said and done, after the assembly was coded, and the dust settled, I was able to get around a 9MHz clock speed on average. Since the command and address are also sent 4-bits-wide, the speed increase is nice. Previously (using hardware SPI) it took around 8 microseconds to read/write 32 bytes, now it took just under 4 microseconds. A nice speedup. An astute reader might notice that the first three RAMS ARE on consecutive GPIO pins. Three is not of much use to us, as it is not a power of two, but two... Yes indeed using only two RAMs i can attain faster speeds (but at half the width). The actual time to read/write 32 bytes is around 5 microseconds. Given this, I decided to re-add the previously-removed support for using less than 4 RAMs on the board. And I did. The newest firmware now supports 1, 2, or 4 RAMs populated on the new boards. I then went futher, and re-added this support for the old boards. That is not as well optimized - it is in C, not ASM, but good enough to play with. This will allow assembling these boards cheaper. Plus, Ultrix happily boots and runs in 4MB (it does need 5MB to start the GUI though). And old hardware too I did not want to maintain two separate-but-almost-equal branches of code for the older v1.2 hardware and the new v1.3 hardware. There was also no easy way to tell them apart in software from first glance. But a bit more investigation does provide an idea. The wiring for the RAMs is different enough that we can try each way and see if we detect a plausible RAM chip. It helps that not having RAM0 populated is never supported. This is precisely what I did, in fact. I tried both configs and see which produces a valid-looking ID from RAM0. From there, all four RAMs are probed, identified, and a configuration is picked. Support for less than 4 populated RAMs raises a few interesting questions. For speed, all RAMs are treated as if they are the same size, so the size of the smallest RAM determines the total amount of available RAM. This is because I stripe the data across them, of course. So, what if RAM0 is populated with 8MB, and RAM1 with 2MB? We could use just RAM0 and get 8MB of RAM or we could use both and get just 4MB, but faster, since more RAMs in parallel is always faster. I decided that more RAM is better than faster RAM, so in case of such conflicts, more RAM is always chosen. When there is a tie, the faster configuration is used, eg: 4MB, 1MB, 1MB, 1MB RAMs populated add up to 4MB in both the x1 and x4 configs. In this case the x4 config will be chosen and all the RAMs will be used. Building from source (updated) The emulator A new parameter called FPU is now passed to uMIPS build to specify the FPU type desired. Options are: none - no FPU at all, Ultrix will not like this but it makes the smallest image; minimal - an FPU that can store values but refuses to do math - Ultrix and Linux will support this, it is slightly larger; and full - a full FPU that does all the math - the fastest option that bloats the Cortex-M0 image by 17KB or so. The loader To build the proper loader, pass the BUILD parameter to make. The options are linux, ultrix, ultrix_install, or netbsd. The install loader is just for clean installs, which you have no reason to do since I already did it for you. The netbsd one is to attempt boots of NetBSD on this machine, as it is supported by NetBSD. The proper loader needs to be built and integrated into a disk image for a working system. The integration step also changed, mkdisk.sh is gone, replaced by a number of different tools, depending on the intended system. They are: mkdisk-linux.sh, mkdisk-netbsd.sh, mkdisk-unix.sh, and mkdisk-unixinstall.sh. Unix here refers to Ultrix, of course. The scripts are small and self explanatory. Open them for more details. They all operate on a disk image called \"disk\" To enable GUI in Ultrix, the env variable \"console\" needs to be properly set. In loader.c, find it and set it to \"0,0\" for text mode or \"1,0\" for console mode. Further Updates Firmware v2.1.1 In this version, BBQSPI memory access sped up by 11% for 4-chip case, 6% for others. Ram config shown on boot. Firmware v2.2.0 In this version, the bootloader was updated to better support other ATSAMD21 parts, including those with more flash & RAM. It now also exposes a version byte at offset 0x08. The previous bootloader was version 0x10, making this one version 0x11. The version will be shows on the serial console at boot now. Also, as ATSAMDA1E16 is now apparently out of stock everywhere, I added support for ATSAMD21E17A-AU/ATSAMD21E17A-AUT. The sad news is that this non-automotive part does not overclock nearly as well. It gets unstable much past 76MHz, so I decided to clock it at 72MHz. It does have more RAM (16KB), which allowed me to allocate a lot more memory to L1i and L2 caches. In most measurements, the performance loss due to lower speed is papered over by the gains of larger caches. On the topic of performance, I also rewrote the L2 cache code in assembly for speed and size gains. The speed gains are significant. For extra speed, there is now an option to move the actual access functions to RAM (which is faster than flash). This gains an extra 8% speed, but at the cost of using RAM. On the old 8KB-of-RAM parts this is not always worth it, since it necessitates shrinking the L2 from 2KB to 1.625KB to make space. On the new 16KB-of-RAM parts, though, it is we;ll worth it. It should be noted that there are 6 variants of RAM access low level functions as there are 2 possible access types (SERCOM or bit-banged) and 3 possible chip counts (1, 2, or 4). Only the ones you plan to use need to be moved to RAM. Others will still work from flash, if you want to make a universal firmware. The firmware I provide now moves the 4-chip bit-banged functions to RAM for ATSAMD21E17. See RAM_FUNCS_IN_RAM in the Makefile and the contents of spiRamAtsamd21.c While moving functions to RAM, it is easy to accidentally use too much RAM and end up with random crashes as stack collides with data. These are a pain to debug, so I decided to improve this experience. As an option in the Makefile, there is now ability to enable STACKGUARD. What does this do? As the very last word in the pre-allocated RAM (and thus the very first one that stack would overflow over) the code will keep a magic cookie, whose value depends on the current ticks.hi value. This value is checked and updated in the SysTick interrupt which happens every 16 million cycles. If the check fails, an error will be blinked out the LED and the execution will be halted. Starting with this version, the proper make incantations are now make CPU=atsamda1e16 and make CPU=atsamd21e17 The downloads have been updated with the new code and binaries for both chip types. They can be updated using the bootloader and an SD card In conclusion Acknowledgements I send a great many thanks to my cats for cutely lying under my table to keep me company during the many hours spent on this project. I send a giant, Mount Rushmore-sized middle finger to Atmel for this sorry excuse of a chip. UPDATE: I even gave up on using the SPI units here, so at this point, I send two of those fingers. Thanks for nothing, Atmel. Downloads The source code, gerbers, schematics, and all else except the disk images can be downloaded [here]. The license on my code is simple: free for all non-commercial use, including using as your own business card. For commercial use (like if you wish to sell kits of this project), contact me. The disk images (updated) are large so I have no desire to host them on my site, so: [Google Drive] or [MEGA]. © 2012-2024",
    "commentLink": "https://news.ycombinator.com/item?id=41516476",
    "commentBody": "My business card runs Linux and Ultrix (2022) (dmitry.gr)190 points by caned 18 hours agohidepastfavorite49 comments schoen 17 hours agoIn about 2000 we had a \"bootable business card\" which was a CD-ROM on which we loaded a bootable ISO image that we made with various tools that might be useful to a system administrator. The original version was made to promote Linuxcare. Most CD-ROM drives in PCs had a small inner ring that could hold \"mini CDs\" with a smaller diameter than a full CD, and a truncated version of this could be a business card CD (which we did not invent; it was commercially available from CD duplicating companies). So, if you had a machine that was broken in some way or you just wanted an ephemeral Linux system, you could take our live CD out of your wallet and boot it in the machine's CD-ROM drive. (Since optical drives are no longer common, nowadays people would use a \"live USB\" instead of \"live CD\" for this.) It's so cool that about 20 years of technological progress has taken us from \"your business card can be a tiny optical storage medium containing a usable OS image\" to \"your business card can be a tiny computer containing a usable OS image\" (giving a totally new meaning to \"bootable business card\"). reply cbanek 16 hours agoparentI remember these! And they weren't exactly circle CD-ROMs either, they had round sides but then straight top and bottom, so you could fit them in a business card holder / wallet. I always thought these were pretty cool. reply from-nibly 16 hours agorootparentI had pokemon TCG on one of those disks. Those were the days. reply 65j56j56 16 hours agorootparentprevAny drive that wasn't one where you insert the CD could read those, including portable music players of the time. You could buy mini CD'R's and burn all kinds of mini distros to it. reply tstrimple 16 hours agorootparentprevI remember seeing a few of these too. For reference, this is what they typically looked like from my experience. https://mediaduplication.wordpress.com/wp-content/uploads/20... reply _joel 13 hours agoparentprevThey were cool, until you loaded them into a 4 cd autoloader and they got stuck. Not that I ever did that... :) reply qingcharles 17 hours agoprevIt's a 32bit 48Mhz CPU, clocked to 90Mhz. I used to run a major EFnet server on Linux on a 25Mhz 386 in about 1996. If you'd given me this business card in the mid-90s, it would have been useable as my primary desktop. reply mmoskal 14 hours agoparentIt's software emulated though. The actual speed is equivalent to around 1MHz. Intrestingly the tiny Cortex M0+ is has about 4x the coremark per MHz of 386 (and twice 486). See https://www.eembc.org/coremark/scores.php and https://en.wikichip.org/wiki/coremark-mhz reply RF_Savage 13 hours agorootparentHere's one with 533MHz ARM926EJ core, 32 megs of RAM and 8 megs of Flash. No emulation here. https://www.thirtythreeforty.net/posts/2019/12/my-business-c... reply qingcharles 13 hours agorootparentAh, that's the one I remember. That's wild. People mention they bought that CPU for 60 cents. That was five years ago. I think I'd maxed my 386 server out with 8MB of RAM. That chip is the stuff of a madman's dreams in 1996. reply qup 17 hours agoparentprevIf I could make my eMachine run Linux, this thing would have been a breeze! reply joezydeco 17 hours agoprevThe PCB edge connector to USB-C is a nice trick. I need to steal that one for my next project. reply rahimnathwani 13 hours agoparentI'm confused about that part. He said he used a 0.8mm thick board. But the repo he linked said the board should be 0.6mm. https://github.com/Pinuct/Eagle_PCB_USB_connectors reply dmitrygr 5 hours agorootparent0.6 is too thin and makes poor contact. 0.7 is best but no board house makes it. 0.8 works. reply zaps 16 hours agoprevImpressive, but does it come in eggshell with Romalian type? reply BLKNSLVR 15 hours agoparentBut it's the watermark that makes it. reply smfjaw 17 hours agoprevsubtle off green PCB, my god, that's really nice reply codetrotter 14 hours agoparentLet’s see Paul Allen’s PCB! reply mjevans 15 hours agoprevThat Atmel chip sounds like quite a heck to work with... the DMA section alone. Also related, 'Improving RAM bandwidth' section. The author's final conclusion about the chip... \"Thanks for nothing, Atmel.\" I wonder if there are better options available today, but I fear anything useful might involve BGA or similar surface mount techniques. reply dmitrygr 15 hours agoparent> I wonder if there are better options available today Rp2350. That’s what I’m basing my next revision of this project on reply mjevans 15 hours agorootparenthttps://www.raspberrypi.com/products/rp2350/ 0.5MB RAM and 4MB of flash might not sound like a lot, but that's plenty for many micro computer system environments. reply dmitrygr 15 hours agorootparentIt supports QSPI PSRAM natively. So it’ll be a lot faster than doing in manually on ATSAMD21 reply mmoskal 14 hours agorootparentAlso M33 vs M0 and higher clock - should be 2-3x faster. And maybe you can do something with the second core. Im super impressed you got this to work! The memories of the SAMD clocks... Shudders. reply cperciva 18 hours agoprevMaybe I missed something, but what does this cost? reply dmitrygr 18 hours agoparentunder $15 ea to build in small quantities, less for more reply clay-dreidels 18 hours agorootparentThat’s cheaper than I thought. reply darby_nine 17 hours agoprevI nominate \"ultrix\" as the most ironic name ever for a piece of software. Not only was it not the last, it turned out to be one of the first! reply starspangled 15 hours agoparentDid it derive from ultimate, or ultra (beyond unix)? I assumed the latter. reply darby_nine 14 hours agorootparentRegardless, it seems like bad branding in retrospect. reply HackerQED 16 hours agoprevCool, imagine after a collapse of civilization, last survivors dig out this, as the cornerstone to rebuild the old-time industry. This card has such a sci-fi taste. reply sweeter 16 hours agoprevthis is amazing! the one sin you've committed is not running neofetch / fetch lol. I want to learn a lot more about booting in this aspect. It seems pretty neat that all it really does is load a couple of addresses that then look for an ELF binary vmlinux that is parsed and ran. Seems fairly simple all things considered. reply khqc 16 hours agoprevJust from the schematic, it took me longer than necessary to figure out the leftmost symbol was for USB-C reply dextrous 16 hours agoprevI must say, I love the tagline “creating order out of chaos (or reverse if needed)” reply jmclnx 5 hours agoprevVery Cool! reply WolfCop 16 hours agoprevDoes it run Doom? reply dmitrygr 16 hours agoparentIt can reply unixhero 16 hours agoprevThe tenacity if this guy is insane. reply taway1874 17 hours agoprevVery impressive! Thanks for sharing. reply deisteve 17 hours agoprevvery cool reply monocasa 14 hours agoprevNo PalmOS, Dmitry? Lol, I kid, this is awesome. reply sys_64738 17 hours agoprevnext [6 more] [flagged] dgfitz 17 hours agoparentIt takes a very uneducated mind to make a comment like this. Giants stood on the shoulders of giants for this to be possible. reply Brian_K_White 17 hours agorootparentIf you didn't read anything but the title, then it's not very remarkable today. Read any random single paragraph from the story and you can no longer say that. Especially the v2 with Ultrix. I never wrote a coff parser & loader from scratch. Or a reimplimentation of scsi itself just to then write some virtual scsi hardware good enough to satisfy a kernel that was hard coded for exactly that hardware down to practically cycle counting and who knows what mystery quirks. (He does, he knows what mystery quirks, now.) reply xandrius 17 hours agorootparentprevMeh. I've heard that now even clouds now run UNIX and WINDOWS so not very interesting.reply malux85 17 hours agoparentprevThe result is largely who you became while you build it, not the physical end product. reply BLKNSLVR 15 hours agorootparentThis is a beautiful comment. Universally applicable. Well said friend. reply dools 17 hours agoprevnext [4 more] [flagged] biztos 16 hours agoparentStill not sure what this is, but here it is: https://youtu.be/4YBxeDN4tbk?si=CqumD2CHaNMZ650g reply knodi123 16 hours agorootparentIt's Joel Bauer! https://joelbauer.com/ reply komali2 16 hours agorootparentprevWell one thing's for sure, the card does accurately represent that man as obnoxious. reply yieldcrv 17 hours agoprev [–] everyone acting like they can’t just reach out to a recruiter for a quarter million dollar job and need these standout things reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Dmitry.GR has developed a business card that runs Linux and Ultrix, showcasing a unique and practical application of microcontroller technology.",
      "The project involves emulating a DECstation2100/3100 using a MIPS architecture, with significant optimizations for performance, including overclocking and custom boot processes.",
      "Key updates include improved firmware versions, enhanced USB support, and hardware redesigns for better speed and compatibility."
    ],
    "commentSummary": [
      "In 2000, \"bootable business cards\" using mini CD-ROMs were popular among system administrators for promoting Linuxcare, allowing a Linux system to boot from a PC's CD-ROM drive.",
      "Today, \"live USBs\" have replaced these mini CDs due to the decline of optical drives, showcasing the evolution from optical storage to tiny computers with usable OS images.",
      "The article also covers technical advancements over the years, including the use of different CPUs and the challenges encountered in this technological transition."
    ],
    "points": 190,
    "commentCount": 49,
    "retryCount": 0,
    "time": 1726100467
  },
  {
    "id": 41519623,
    "title": "SpaceX Astronauts Begin Spacewalk, Putting New Spacesuits to Test",
    "originLink": "https://www.wsj.com/science/space-astronomy/spacex-launch-polaris-dawn-space-walk-bfed7f84",
    "originBody": "wsj.com#cmsg{animation: A 1.5s;}@keyframes A{0%{opacity:0;}99%{opacity:0;}100%{opacity:1;}}Please enable JS and disable any ad blockervar dd={'rt':'c','cid':'AHrlqAAAAAMAYhRlftPmI8QAFFEunA==','hsh':'D428D51E28968797BC27FB9153435D','t':'bv','s':47192,'e':'648ce4cbd230321ba1b6dfb3f32561283cad3620f7e8536f77d439fa1ae10e97','host':'geo.captcha-delivery.com'}",
    "commentLink": "https://news.ycombinator.com/item?id=41519623",
    "commentBody": "SpaceX Astronauts Begin Spacewalk, Putting New Spacesuits to Test (wsj.com)188 points by bookofjoe 7 hours agohidepastfavorite198 comments noneeeed 5 hours agoThe interesting thing for me is how this mission underscore the difference between SpaceX and other space companies. SpaceX have become an entire commercial space agency, able to supply everything from the rockets and capsules, to the ground operations and even the space suites and to do that in a complete package. If you have enough money you can ring them up and say \"I want to go into space\" and they can make that happen. That is a pretty big deal. reply stef25 5 hours agoparent> If you have enough money you can ring them up and say \"I want to go into space\" and they can make that happen. To whoever wants to be the first one to drop acid in space, the doors are open reply delichon 4 hours agorootparentWhen you throw open Huxley's doors of perception you get an extraordinary experience in the most quotidian of conditions. Like watching a river run by or just watching the washing machine run. Non psychonauts have to do extraordinary things, like become astronauts, to get to a similar level of awe-struck. A trip on a trip is a tragic waste of a trip. reply onemoresoop 3 hours agorootparentWhen you're uber-rich I guess you want something no commoner can do. reply pyrale 3 hours agorootparentConspicuous consumption at its finest. reply forgot-im-old 3 hours agorootparentprevThey also tested SpaceX's laser links, which are critical for Star Wars / Brilliant Pebbles goals, https://www.reddit.com/r/KamalaHarris/comments/1eunob4/elon_... reply aaronharnly 3 hours agorootparentprevOne might suspect the CEO has dibs on this one. reply jMyles 5 hours agorootparentprev> the first one to drop acid in space Seems unlikely we'll ever definitely now, but I suspect that whomever does it next won't be the first (or second, or third). reply brandall10 5 hours agorootparentEsp. on longer ISS trips. I can't believe this hasn't already been a sanctioned experiment. reply baseballdork 4 hours agorootparentI can believe it. Dunno how reasonable it would be to risk having someone in an altered state of mind on the ISS. What do you do if someone has a bad trip or decides to make poor decisions? reply KineticLensman 4 hours agorootparent> What do you do if someone has a bad trip or decides to make poor decisions The space shuttle had a hatch that could be opened to vacuum and was actually padlocked shut on some flights where the mission specialists weren't trusted by the commander [0]. [0] https://arstechnica.com/space/2024/01/solving-a-nasa-mystery... reply schiffern 1 hour agorootparentIf you mean STS-51-B, the commander duct taped the hatch. After that flight a padlock was added for a more permanent solution. > “I remember waking up at the beginning of a shift and seeing duct tape on the hatch,\" Gregory told Ars. \"I did not know what the origin of it was, and I didn’t pay any attention to it. I may have, but I don’t recall asking Overmyer about it.” Also this detail gives me pause: > The Space Shuttle has been retired for 13 years, but the padlock remains in the fabric of US spaceflight with Crew Dragon. A commander's lock is an __option__ for NASA's crews flying to the International Space Station on Crew Dragon, as well as private missions. Why even make it optional? Seems better to officially take responsibility out of the commander's hands and make the lock mandatory. This would greatly reduce risk to onboard trust dynamics, since the commander is just following orders instead of signaling mistrust toward the crew. This seems like one of those \"who needs all those lifeboats??\" situations. Why not avert the Titanic now? reply brandall10 4 hours agorootparentprevSimilar to what's been done on earth during such experiments. Just have to do it in a controlled way in a prepared environment, and have both an administrator and a person likely versed in taking the substance to minimize risk. Of course with just a sample of one it won't yield much scientific benefit. reply BurningFrog 4 hours agorootparentprevI'm not denying that intoxication leads to more bad decisions. But sober people also make many bad decisions. A space ship needs to be resilient to bad decisions, and I assume/hope the ISS already is well fortified in that respect. reply llamaimperative 4 hours agorootparentTypical bad decisions, especially by a professional crew of trained flight engineers, are quite different from \"bad trip\" bad decisions. Totally different risk vector and one they should have spent precisely zero seconds thinking about in the ISS's design. reply macksd 4 hours agorootparentIt took only 17 Space Shuttle flights before a crew-member threatened to kill themselves causing them to add additional locks to the hatches. I think a few seconds considering what a malicious actor could do would be worth it. reply llamaimperative 3 hours agorootparentWe're not discussing a malicious actor, we're discussing someone having a bad trip on LSD. It is a completely different thing. reply cjbgkagh 3 hours agorootparentThe unpredictability means they might as well be a malicious actor. reply llamaimperative 1 hour agorootparentNot really. Someone panicking on acid would do different things than a sober person trying to knock the ISS out of orbit. Obviously. reply cjbgkagh 1 hour agorootparent'Not really' - best not bet the lives and a huge amount of money on 'Not really' reply llamaimperative 33 minutes agorootparentI don’t know what argument you’re trying to make. The question is: should the ISS be hardened against someone on LSD. My answer is: that is fucking absurd, and generic hardening against malicious actors is tangential at best. jMyles 3 hours agorootparentprev...we're not talking about force-feeding someone a fifth of whiskey here. A stable, thoughtful, trained scientist, in the company of supportive and emotionally intelligent colleagues, does not present a significant risk when fed a normal dose of LSD. reply llamaimperative 1 hour agorootparentThey absolutely can present a significant risk when fed a normal dose of LSD. And I'm no prude when it comes to this stuff. Very standard advice for psychedelics is set & setting, and \"don't be physically trapped in an environment you might find uncomfortable\" is a reasonable derivation. reply mrguyorama 25 minutes agorootparentDon't be in the endless void of space which is already emotionally and psychologically taxing on human beings who are verifiably in peak physical and mental condition Seems like a pretty fucking bad set and setting honestly. Regular astronauts already deal with powerful feelings in their normal job. Adding a drug to that is a bad plan. reply roywiggins 4 hours agorootparentprevEven airplanes aren't resilient to truly bad decisions: https://www.nytimes.com/2023/10/24/us/alaska-airlines-off-du... reply dylan604 3 hours agorootparentprev> What do you do if someone has a bad trip or decides to make poor decisions? The same thing you do on terra firma. You do your best to keep them calm, and talk them down. I've never had to physically restrain anyone, but seems like that would actually be easier to do on the ISS. Just wrap them up and let 'em float around I'd also expect bad trips to not be as big of an issue in a NASA sanctioned experiment with a much more closely controlled dose vs some tab you got off the guy in a tye dyed t-shirt at the concert type of situation. reply diggan 3 hours agorootparent> I've never had to physically restrain anyone, but seems like that would actually be easier to do on the ISS. Just wrap them up and let 'em float around A great way of restraining someone is pushing them against an unmovable object. On Earth, that is either the floor or a wall, then you can pin them somewhat against that, at least restricting them somewhat. Walls and floors are the same in space, and the person can literally float away with six degrees of freedom, at any time. I feel like physically restraining people would be harder in zero gravity than non-zero. reply robertlagrant 3 hours agorootparentprevWhat's nice about the ISS is there's no way for someone to do something accidentally dangerous in there. reply xur17 2 hours agorootparentAre you being sarcastic? reply jandrese 4 hours agorootparentprevThat seems crazy. There are like 10 people in space at any given time. The idea that we might have sent someone mentally unstable given that we can be that selective is mind blowing. How in the world can you qualify for this if you aren't the best of the best of the best? reply cjbgkagh 3 hours agorootparentI don’t see mental instability as an independent defect but as a byproduct of some of the long tail aspects needed to be the best of the best. To put another way, many people are motivated to fill a void, that same void can cause mental instability. reply mycodendral 4 hours agorootparentprevI agree to not do it casually But it could probably be arranged under supervision with an anesthetizer option. They probably already have some protocol for antisocial or psychotic behavior. reply mycodendral 4 hours agorootparentprevI agree to not do it casually But it could reasonably be arranged under supervision with an anesthetizer option. They probably already have some protocol for erratic behavior. reply latchkey 4 hours agorootparentprevPeople make poor decisions regardless of drugs. reply ionwake 4 hours agorootparentprevI feel you don’t know enough about acid trips. If my spaceship commander just told me he had taken 2 tabs, I would start writing my obituary. reply andsoitis 5 hours agoparentprevI think a large part of it is because of Elon Musk's obsession about controlling (if not owning) as much of stack as possible. That mentality goes into even the things you do not own, but lease. In the Isaacson biography there's a theme across his ventures, sparked by early misfortune, about being in control of things end-to-end because then you have more flexibility to optimize, make more leaps that can come from orchestrating creative adn bespoke integration options. reply afiodorov 5 hours agorootparentNo, in this case it is much more pragmatic. Elon has commented on this aspect himself. When he started he thought of just buying rockets and related equipment from Russia on the cheap but quickly found the exuberant prices states want to charge. He then made a first order approximation of how much it’d cost to just manufacture things he needs and it was an order of magnitude lower - thus he bit the bullet and started doing just that. Had the states let him buy things at a fair price may be the controlling of the stack would not have happened. reply dingaling 17 minutes agorootparentAs a benchmark, Aerojet will charge NASA between $70 and $110 million for each new-build RS-25 engine for Artemis. And that's a design that dates back to the 1970s and which flew as the SSME on the Shuttle. SpaceX builds all 33 engines on a Starship for that amount. reply schiffern 5 hours agorootparentprevHere's story as retold by (now retired) SpaceX engine chief Tom Mueller: https://www.smithsonianmag.com/air-space-magazine/is-spacex-... > Significantly, the Merlin engines—like roughly 80 percent of the components for Falcon and Dragon, including even the flight computers—are made in-house. That’s something SpaceX didn’t originally set out to do, but was driven to by suppliers’ high prices. Mueller recalls asking a vendor for an estimate on a particular engine valve. “They came back [requesting] like a year and a half in development and hundreds of thousands of dollars. Just way out of whack. And we’re like, ‘No, we need it by this summer, for much, much less money.’ They go, ‘Good luck with that,’ and kind of smirked and left.” Mueller’s people made the valve themselves, and by summer they had qualified it for use with cryogenic propellants. > “That vendor, they iced us for a couple of months,” Mueller says, “and then they called us back: ‘Hey, we’re willing to do that valve. You guys want to talk about it?’ And we’re like, ‘No, we’re done.’ He goes, ‘What do you mean you’re done?’ ‘We qualified it. We’re done.’ And there was just silence at the end of the line. They were in shock.” That scenario has been repeated to the point where, Mueller says, “we passionately avoid space vendors.” reply heisgone 4 hours agorootparentI suspect one of the reason behind that is that the supplier is asked to provide a part that cost only a few thousands bucks, but could potentially blow a multi-million rocket. So, each part end up having a sort of implied insurance and over-the-top quality control, to avoid liability. When SpaceX build something itself, it can evaluate the proper risk of each part and doesn't need to add unnecessary markup. reply eastbound 2 hours agorootparentThat’s s problem in classic microeconomics: The right price is anything between the marginal cost of manufacturing and the total price of the rocket minus $1. Every parts manufacturer tries to extract as much as possible from the leeway in the middle. reply rqtwteye 3 hours agorootparentprevI work in medical devices and when I read such stories I often wonder how quickly we could develop things if we reexamined the company processes that have been developed over the last 30 years. Maybe everything we do is fine, but I suspect we do a lot of stuff very inefficiently. reply SoftTalker 3 hours agorootparentLife-critical devices are generally subject to a lot of regulations. Regulations that were in many cases written in blood. However I suspect you're correct that there isn't much periodic, retrospective review of these regulations and whether they still make sense or can be simplified. Especially when there are overlapping or possibly contradictory regulations. We know what wide-open unregulated markets result in when lives are at stake. It would be nice to think that engineers and executives would never sacrifice human life for profit, but we know that some of them will. I also think about all the advice I read on this forum to \"raise your rates\" and \"charge based on the value you deliver, not what your costs are.\" But when physical engineering companies take this approach, it's somehow just greed or complacency. Of course any potential client can just decide to hire their own staff and do the job in-house if your quote is viewed as unreasonable. reply robertlagrant 3 hours agorootparent> We know what wide-open unregulated markets result in when lives are at stake. We know much more about what happens with regulation: prohibitively expensive nuclear power and unbearably slow progress in simple medical devices. Opportunity cost is enormous here. > It would be nice to think that engineers and executives would never sacrifice human life for profit, but we know that some of them will. This is really simplistic. We wouldn't use anything if it cost what it cost to make it 100% safe. Everything has risks and prices. reply rqtwteye 37 minutes agorootparentWe also know what happens without any regulation: milk with lead added to make it look white, cars that kill their passengers when crashing, polluted rivers, polluted air and much more. We certainly need to constantly revisit things but without regulations our society wouldn’t be fun to live in. reply EricE 2 hours agorootparentprev>We know what wide-open unregulated markets result in when lives are at stake. Death is bad for business? I dunno why this myth that without government life would be a deadly hellscape seems to persist. reply schiffern 2 hours agorootparentIf we're talking wide-open unregulated markets, that's pretty obvious. It just comes from experience. https://en.wikipedia.org/wiki/Triangle_Shirtwaist_Factory_fi... https://en.wikipedia.org/wiki/Bhopal_disaster https://www.theguardian.com/environment/2011/jan/06/bp-oil-s... etc etc reply lainga 2 hours agorootparentprevSunk-tax fallacy. reply bell-cot 43 minutes agorootparentprev> I dunno why this myth that without government ... /s? (Look at the quality of life in failing/failed states, where there's effectively no government. Libertarian paradises are extremely rare; deadly hellscapes are all too common.) reply eastbound 3 hours agorootparentprevYes but certification!!! More seriously, I wonder as a human species if we’ll evolve beyond certification being the only way to prove a work was done properly. I suspect the civilization who does that will save an order of magnitude more human workers. reply jp42 4 hours agorootparentprevhere more such things from Dan Rasky (nasa scientist) about his experience with spacex: https://www.youtube.com/watch?v=SMLDAgDNOhk reply patwolf 4 hours agorootparentprevWhat made SpaceX capable of being able to design and build the valve themselves faster and cheaper than the vendor? It makes a good story, but I worry about other companies trying to emulate it when the economics of space vendors might be far different from other industries. reply alright2565 3 hours agorootparentI'm not sure if this applies to smaller parts like this, but traditional aerospace suppliers are allergic to iteration. Everything must work the first time. You can see this in how many rockets SpaceX has blown up vs. how many ULA has blown up. I imagine it's similar to what we see in software, where we see mega-waterfall projects delivered for the government cost way too much and still fail, and where tech companies & literal hobbyists can deliver higher quality software more quickly by avoiding an overbearing planning process. reply ploxiln 3 hours agorootparentprevhttps://danluu.com/nothing-works/ reply andsoitis 4 hours agorootparentprevI think you and I are saying the same thing. reply nomdep 3 hours agorootparentIt doesn't look like that. You are saying it like it was a symptom of a mental disorder, while he is stating that is the most practical and logical decision. reply treflop 3 hours agorootparentprevIn my opinion, it's not that he wants to control so much as that his goal is going to Mars. Someone that goes to work to do the tasks assigned to them (which applies to both managers and those on the line) vs. someone that goes to work because they want something (e.g. maybe a certain product made with certain features) thinks very differently. The latter has a whole plan and the plan has certain sub-requirements that all feed back into the greater goal. As part of going to Mars, you will likely have to send many supporting missions, which drives the requirement to lower costs. When designing bespoke systems, contracting it out to third party vendors can be expensive. Usually you contract out to third party vendors because they can utilize economies of scale because they have many clients who are asking for the same thing, but there are no economies of scale here because no one has reusable rockets in their catalog. reply HarHarVeryFunny 5 hours agorootparentprevIt's really because SpaceX is Musk's hobby company with the singular goal of putting a colony on Mars. The commercial launches and Starlink are just a way to leverage all the infrastructure they've had to build to make it pay for itself. reply paulryanrogers 5 hours agoparentprevExcept for the whole launch facilities thing they lease from the government. reply caconym_ 4 hours agorootparentGiven the incredibly hostile regulatory environment they've faced in Texas trying to operate their own private launch site, I think there are very clear benefits in using an established public facility that isn't under threat by constant attempts (often in outright bad faith) to curtail its activity or even get it shut down. It's not a question of money or capability. reply mrguyorama 4 minutes agorootparent>private launch site But you know it ISN'T private, because every rocket ever designed will end up in public space, either literally in outer space, or dropping parts in the ocean, or a tiny tiny tiny tiny chance of exploding somewhere less convenient. This idea that you can just make your own little closed off launch pad and therefore ignore society is stupid. Some of the more onerous pushback comes from people who have been ignored before and turned out to be right. For example, when Dupont and friends first started producing PFAS, someone out there was crying foul about \"hey maybe we shouldn't just dump this everywhere\", and they were ignored, it was dumped everywhere, and now we get to deal with the consequences because nobody listened to those people. This has happened hundreds of times, including serious things like lead paint and leaded gasoline, which we KNEW caused direct and measurable harm to people, and yet was completely ignored. A portion of the public has obviously lost ALL trust for the private sector claiming any sort of \"this couldn't possibly cause problems\" and are willing and able to take action. If you are a \"big\" enough entity, there is no such thing as \"private\". Anything you do, affects a lot of people, and you should be treated that way. reply rogerrogerr 5 hours agorootparentprevIt’s clear they’d be fine without that; they could probably have Texas ready for manned launches fairly quick. More likely they lease launch facilities from the government to minimize regulatory interference from… the government. reply jdross 5 hours agorootparentprevwhich they have also massively refurbished and could have handled themselves if not for regulations around creating a launch site. there aren’t many technical issues to pouring concrete in a good lat-lon reply michaelt 5 hours agorootparent> there aren’t many technical issues to pouring concrete in a good lat-lon Other than when a powerful and explosion-prone rocket destroyed its launchpad, hurling chunks of steel-reinforced concrete thousands of feet. But it's almost 18 months since that happened. reply gertrunde 4 hours agorootparentThat incident probably underscores the parent poster's point quite effectively. That launch was on 20th April 2023, and the next prototype test launch was only 212 days later on 18th November 2023, although I think the pad redesign/rebuild/repair work was complete by the end of July 2023. So only 3-4 months to redesign/rebuild/repair the pad (although it's probably reasonable to assume some design work had already occurred). reply jandrese 4 hours agorootparentAt the end of the day it wasn't a major problem. SpaceX tried something new, it didn't work out, so they had to fix it. Nobody was hurt. Property damage was minimal. The repair didn't take very long. All in all a good example of learning from your mistakes without spending too much time or money overthinking it. reply michaelt 3 hours agorootparentprevIn civil engineering terms, if falling apart with debris thrown thousands of feet doesn't count as a \"technical issue\" what does? I'm no rocketry expert, but I'm old enough to remember the Space Shuttle Columbia disaster; I'm pretty sure if you've got falling chunks of steel-reinforced concrete hitting your space vehicle during launch, that's an issue. reply jjk166 2 hours agorootparentThe launch pad didn't explode, the rocket on the launch pad exploded. A fully fueled falcon 9 has about as much energy as a small nuclear bomb, it's not a technical issue if your structure is unable to survive that at point blank range. reply gertrunde 2 hours agorootparentprevOh, definitely, of course it wasn't a good or desirable outcome. But it was also a prototype being deliberately tested to destruction, so the context as compared to Columbia was quite different. (And it wasn't just the rocket itself that was a prototype, the pad and tower were at least a little as well). And this has always been SpaceX's approach, rapidly iterating their design by building, testing, destroying, rinse/repeat - so it sometimes feels a little difficult to compare to a more NASA-style design process where a (usually) small number of items are produced with a significantly lower tolerance for failure. (Edit: And how much better is it to learn these design lessons before the cargo is more fragile/delicate/squishy?) reply iknowstuff 4 hours agorootparentprevWhats your point reply michaelt 3 hours agorootparentThat building a rocket launch pad is in fact more difficult than pouring a slab for a garage, and there some nontrivial technical issues involved. Having chunks of the launchpad go flying isn't just an inconvenience - flying debris during launch can damage critical rocket systems, as the Space Shuttle Columbia disaster demonstrated. reply wonderwonder 2 hours agorootparentand they figured out how to prevent this happening by the next launch just a few months later. So it turned out to be fairly trivial after all. SpaceX is able to iterate in months what it takes Nasa and its contractors years to do. reply taneq 4 hours agorootparentprevHaha well that’s gonna happen from time to time when you try new things. reply dmix 3 hours agorootparentCan't be taking risks. You might actually accomplish something. reply HideousKojima 5 hours agorootparentprevSoon to change with Starship launching from Boca Chica. And planned to be able to launch from essentially anywhere, eventually (in Musk Time so give it a decade). reply elif 2 hours agorootparentI have seen no plans for a 4th launch/catch tower.. has he even hinted at more sites? Or are you referring to the historical BFR design when musk planned on landing them on the ground like falcon? reply panick21_ 5 hours agorootparentprevThe government took all the good land for this in the 60s. So the government mostly just has the land in the right places, that's a really important part. Building most of the ground infrastructure, is still custom to every rocket company. reply api 5 hours agoparentprevThat's because SpaceX is run with the goal of building actual space transit capabilities, not milking the taxpayer and distributing pork to as many politically influential districts as possible. Until SpaceX and post-Apollo most of the space program was really a pork distribution system and a way to keep certain high-skill labor areas 'hot' in case we need them again. It wasn't a serious effort to learn about the universe or build a real presence in space. Some of that did happen but it happened almost as a side effect. Edit: Honestly I think SpaceX is just a decent aerospace company. They've appeared superhuman by contrast with companies like Boeing that have done nothing to innovate in the sector since the 1970s. When the comparison is with people completely asleep at the wheel, mere competence looks striking. reply xattt 5 hours agorootparentI don’t see downsides of keeping skilled labour “hot”. One indirect benefit of domestic manufacturing is that it gives blue collar workers a sense of productivity and involvement in society. Idle hands are the Devil’s playthings, and a fairly compensated workforce is less likely to feel stagnant in their stage of life, and less likely to be radicalized. Edit: it seems there is a distinction between how you keep the labour hot - meaningful long-term missions; or a half-assed approach. reply panick21_ 5 hours agorootparentThe downside to keeping skilled labour 'hot' are many. First of all, if you have skilled labor, instead of of keeping them 'hot' actually 'eat' them. Ok, the analogy breaks down. But why keep them 'hot' just have them build actually useful stuff instead. Its not like there is a lack of useful things they could be doing. And second giving people bullshit useless projects like SLS, does not actually keep them 'hot'. Its more like warmed up Pizza. The people and more importantly organization lose the actual ability to do things quickly at reasonable prices. Somehow it seems acceptable that SLS will cost more then 30 billion $ in development cost and 2-4 more billion $ per launch. Not to mention the time. Don't misunderstand political pork as actually strategic workforce development or long term government policy. That just what you personally wish it to be, not what it actually is. reply skummetmaelk 4 hours agorootparentWhich other useful things can they do? Spend their lives building advertising services or playing zero-sum games in financial markets? reply thmsths 2 hours agorootparentBuilding rockets and probes for a reasonable price and in a reasonable amount of time. SLS was initially billed as \"a simple project to reuse a bunch of parts from the Space Shuttle\" and yet it's over a decade old, 20 billions in cost and has not launched even once. So we get no real innovation (as I said previously it's just reusing fairly old technologies), it's not cheap and it's not fast. As for keeping the specialists hot; surely there are better ways to do it. It feels like saying let's keep digging ditches and filling them back up to keep our ditch diggers in shape. reply adolph 4 hours agorootparentprevWhile “keeping skill labor “hot”’ sounds great in theory, the practical example of US space industry pre-SpaceX leads me to grant greater weight to “not actually great.” reply fooker 4 hours agorootparentSkilled labor is somewhat transferable. Who do you think are making the shells being shipped out to Ukraine ? reply elif 2 hours agorootparentPalmer lucky (oculus) is mostly sending drones with grenades. I think those are probably most of the kills tbh, and (according to my friend who works there) it is a very young, fresh, startup approach and not at all like the traditional defense industry. reply thrance 5 hours agorootparentprevNASA is a scientific institution, it will lose money. But in the end, it enables private companies like SpaceX to be successful. More generally, no private company would be able to fund fundamental research and turn a profit. Apple used DARPA tech to make the iphone, Pfizer used decades of public research on mRNA to make the covid vaccine and SpaceX uses NASA's engineering expertise to get its rockets to orbit. Also, SpaceX gets billions of dollars from the US government annually. reply adventured 4 hours agorootparent> More generally, no private company would be able to fund fundamental research and turn a profit. I have to imagine you mean turn a profit directly on the research itself (isolated down to the cost per specific item of research). Because obviously big tech can quite trivially fund fundamental research at an enormous scale as they see fit while printing their traditional gigantic profits. Apple + Microsoft + Facebook + Amazon + Nvidia + Google = over half a trillion dollars in annual operating income (around $126 billion in op income last quarter). They could fund NASA (~$25 billion) and would never really notice it. It'd take about four to six months of further op income growth by those six companies and NASA's budget would be covered in perpetuity. reply thrance 3 minutes agorootparentThey could, but they won't. The private sector is allergic to such long-term investments with no clear path to profitability. And why would they fund research if their competitors could just as easily monetize its potential results ? They rely on publicly funded labs to make the real innovations and do the last (but nonetheless important) mile of bringing it to the public. reply z3phyr 3 hours agorootparentprevAdvances in science requires enormous resources. Take space travel. These companies will never try to innovate in deep space travel, because it brings nothing to them. Low Earth Orbit is where the profit is (but less science) They could fund current NASA, but they won't fund what NASA should be. reply dotnet00 3 hours agorootparentprevPlus, big tech do fund fundamental research, a bunch of them are working on quantum computing for instance. None of that is likely to turn a profit any time soon and the research is expensive. But they do it anyway. reply z3phyr 3 hours agorootparentQuantum Computing, Protein folding, Fusion etc are usually PR topics. If the intention is to advance these, then small amount of funding is useless. We need dedicated Manhattan project style endeavors, a population dedicated to the cause. reply delfinom 5 hours agorootparentprevNASA is scientific yes. But it's just as impacted by the defense industry's corruption of the US government where politicians go out of their way to interfere with procurement to funnel it to their donors and the cost of massively bloated projects which further harms the science they can do and harms competition. reply api 4 hours agorootparentprevI'm referring to things like the SLS and to some extent the Shuttle. NASA does decent scientific work. reply panick21_ 4 hours agorootparentprev> NASA is a scientific institution Why are they designing rockets then? Has very little to do with science. Nobody denies that NASA should 'lose' money you are literally arguing a straw man. The question is 'what should NASA spend its money on'. And wasting 80+ billion $ on a completely broken architecture that makes no sense what so ever seem to most people like a bad idea. People actually WANT NASA to actually do more science, and actually advanced tech development. Personally I think NASA should be building things like nuclear reactors for space, or mars, moon missions. They should work on super-advanced engines. On chemical systems to make water on Mars. There are lots of things NASA could do that there simply isn't another way to do it. What NASA should be doing is reinvesting in 70s technology that is completely useless and out-dated for purely political reasons. I don't fucking understand why everybody who criticizes NASA is attacked with 'you hate science' when this is not actually what people are complaining about. Its just kneejerk defense of all that is wrong with NASA and congress. reply croes 5 hours agoparentprevSo space will become more expensive. reply cwillu 5 hours agorootparentGo look up the cost of a soyuz launch, let alone a shuttle launch, and then tell us more about how this is making space more expensive. reply croes 5 hours agorootparentThat's why I wrote will become. SpaceX doesn't have real competition after Boeing failed. What happens in such a monopoly? The prices rise. reply ben_w 4 hours agorootparentNot necessarily. First, the competition is international, and some is from governments who need a non-US supplier. Second, the goal in most corporations isn't maximum profit per item but maximum profit per year, and if they can indeed deliver the prices Musk is speculating about of getting a million people to pay 100-200 thousand USD each to go to Mars, that allows the overall market to be much larger than if he can only charge 150 million (or even 1.5 billion) for 4-seat rides to a low orbit space station every six months. reply mhandley 4 hours agorootparentprevRocket Lab is growing into being a real competitor. They've been very innovative and successful in the small satellite market and, given what we've seen so far, I have confidence they will also succeed with Neutron which is their upcoming fully reusable medium-lift launcher aimed at supporting launch rates needed for megaconstellations. https://www.rocketlabusa.com/launch/neutron/ reply croes 3 hours agorootparentThat's the same kind of will be as mine. One of us will be correct. reply indoordin0saur 3 hours agorootparentprevWith the Starship the price to get a 100 ton object to orbit or one that is greater than 5 meters in diameter will suddenly become possible. Something previously cost infinity dollars will then be whatever price SpaceX charges. This is a decrease. reply croes 3 hours agorootparentThings you don't do, no matter why, cost $0. My Rolls Royce which I can't afford is a lot cheaper than the car I can afford which got more expensive than my previous one. reply elif 2 hours agorootparentThat's not true because your lack of a rolls Royce has forced you into buying a Kia, and not only suffering the cost of that vehicle, you are paying the lost utility and comfort of the rolls Royce. Similarly, space operators who can't launch goldie-locks efficiency payloads are paying for multiple inefficient small loads instead. reply indoordin0saur 2 hours agorootparentprevThis is a very strange way to think. You could apply it to anything that could not exist in the past that people happily pay for today. \"Medical care was much more affordable in middle ages!\" reply jandrese 4 hours agorootparentprevTesla is another company that had a near monopoly in their market space for a couple of years, but that didn't stop competition from showing up. It is notable how their margins have dropped as competition heated up however. reply croes 3 hours agorootparentCompetitors from China. I doubt that will happen for US satellites reply dmix 3 hours agorootparentprevThere's plenty of competition, especially from China. reply adventured 4 hours agorootparentprevDespite their slow pace, it's quite clear Bezos and Blue Origin + ULA will continue to plod forward. They're real competition with plenty of money behind them. reply tootie 4 hours agorootparentprevBoeing hasn't completely failed. Their ship actually worked just not with enough confidence to put people in it. That doesn't mean they're out of the game. There's still Blue Origin and also a bunch of smaller competitors working on different segments besides human transport. Astroforge seems ready to start mining asteroids and got up and running with shockingly little money due in no small part to the growing supply chain of components for commercial space travel. https://www.forbes.com/sites/alexknapp/2023/10/18/this-aster... reply adolph 4 hours agorootparentprevSpaceX is ahead for now and may continue to be ahead but does have very real competition domestically and abroad. Tesla : Rivian/Lucid :: SpaceX : Blue Origin/ULA Tesla : BYD :: SpaceX : LandSpace [0]/Galactic Energy [1] 0. https://x.com/AJ_FI/status/1833761435362447760 1. https://www.space.com/galactic-energy-ceres-1-sea-launch-vid... reply brigadier132 5 hours agorootparentprevWill become more expensive? Are you willfully ignoring the actual data? reply croes 5 hours agorootparentThe actual data doesn't show the future, does it? What happens if a commercial companies beats its competitors? Prices rise. reply brigadier132 3 hours agorootparentNo, not necessarily. Trivially, in a monopoly, raising prices infinitely for a product does not maximize profit. SpaceX has effectively had a monopoly on commercial space launch since it was first able to actually use reusable rockets. Because the innovation of reusing rockets has brought the marginal cost of launching so far down prices will continue to go down. Right now SpaceX is supply constrained, not demand constrained. All of their launches are booked. As SpaceX's supply increases they will be able to bring launch costs down even further until they hit an equilibrium of monopoly pricing that does maximize profit. reply api 5 hours agorootparentprevIt would be hard to make things worse than big classical aerospace. reply croes 4 hours agorootparentBoeing reply panick21_ 5 hours agorootparentprevYeah its gone be more expensive then literally not able to buy something. Practically infinity expensive. Because this literally something you couldn't buy before. Maybe if you went to Russia and gave them a lot of money. SpaceX has made everything cheaper in Space cheaper and many things possible that literally weren't a commercial thing before. reply croes 4 hours agorootparentLike cloud computing made things cheaper. But now the companies need money for AI and prices rise. Without real competition what stop SpaceX from rising its prices? reply sbuttgereit 2 hours agorootparentYou do realize one of the best ways to attract competition is to raise prices and increase profit margins to the point where it's worthwhile for competitors to enter the market to take some of that profit? If you look back historically, the idea that monopolies were broken up because of their ability to raise prices without the check of competition just isn't really telling the full story. Consider this from the Congressional Record of the House (1890) by a proponent of the then under debate anti-trust act (https://www.congress.gov/bound-congressional-record/1890/05/..., page 4100): \"Some say that the trusts have made products cheaper, have reduced prices; but if the price of oil, for instance, were reduced to 1 cent a barrel it would not right the wrong done to the people of this country by the \"trusts\" which have destroyed legitimate competition and driven honest men from legitimate business enterprises.\" The argument wasn't that the \"trusts make products -cheaper-\" idea was wrong, but that it didn't matter. The only way to maintain a natural monopoly is the ensure that the barriers to entry for competition are sufficient to make new entrants unviable. One way to do that is to leverage economies of scale to lower prices to the point where a new entrant simply can't compete on price. reply chaostheory 5 hours agorootparentprevDid you miss the fact that reusable rockets have made it cheaper? Have you kept up with the economics of space travel? reply croes 5 hours agorootparentThat why I wrote will become. Remember the time when cloud services made things cheaper? Guess what happened next? reply adventured 4 hours agorootparentCloud services have a lot of competition. I can choose Hetzner, which is a high quality low cost provider of basic cloud services. I can deploy open source options onto their cloud and push costs quite low. The cost of cloud services has not soared. And companies like Cloudflare have continued to undercut AWS at every opportunity. Oracle, Microsoft and Google all have strong incentive to hold AWS in check on what they can charge, and they do exactly that through rampant competition. reply TiredOfLife 2 hours agorootparentprev> Remember the time when cloud services made things cheaper No. When was that? Cloud made things easier and faster not cheaper. You trade money for convenience. reply teekert 5 hours agorootparentprevYeah, the way capitalism made goods more expensive. /s reply croes 5 hours agorootparentYou mean like Insulin in the US? Or healthcare or housing or education etc.? reply adventured 4 hours agorootparentYou're referring to extreme government regulation making it prohibitively expensive to start up insulin providers that compete with the established government protected monopolies. Which is anything but Capitalism in action. The same goes for healthcare in general. It's one of the most regulated sectors of the US economy and the government places an extreme cost on doing most anything in that sector through hyper regulation and very epic scale barriers to entry. The US is very far from being a Capitalist healthcare system, it's in fact the worst of both systems: it's a hyper regulated corporatist system, intentionally protected by the government from competition, captured by corporations. reply hylaride 3 hours agorootparentMany states even have Certificate of Need (known as CON in the industry) laws where incumbent medical providers, including whole hospitals, can effectively veto new entrants. Add in limited licensing for doctors (the number of medical spots in schools as not kept up with population) among other things and you've got a problem. https://en.wikipedia.org/wiki/Certificate_of_need reply biomcgary 2 hours agorootparentThis is part of the reason why I distinguish between free-market economics and capitalism, where the people with capital make the rules (including social capital, like current doctors). At the level of persuasion, I don't understand why people use and defend \"capitalism\" rather than \"free-market economics\". reply teekert 4 hours agorootparentprevIs lobbying and a broken IP system really capitalism? reply sham1 4 hours agorootparentYes. Those sorts of things are exactly the kind of skullduggery and shenanigans that the term capitalism was invented to be a critique of in the 19th century. The kinds of stuff pulled by the owning classes to secure their position and capital, up to and including influencing, and in some cases usurping, governments. I guess this would need a new term since the word has been seemingly reclaimed by the capitalists as a good thing. reply simonh 3 hours agorootparentYou are implicitly claiming that corruption, special interests and broken rights protections are unique problems for capitalism, unknown in non-capitalist systems. A clear eyed look at history makes it evident that if you take away private enterprise what you are left with is pretty much nothing but corruption, special interests and broken rights protections. Just look at the Soviet system, or Chinese state sector, or pretty much any thorough going socialist system ever. In non-capitalist industrial economies corruption isn't just a problem with the system, it is the system. The trick is to guarantee not just private rights to capital, but private rights full stop. Rights of ownership and economic freedoms for private citizens, sure, but also political rights, legal rights, labour rights. The full package. Capitalism on it's own isn't enough, and in fact without individual legal and political rights it's not possible to sustain meaningful individual rights to capital anyway. A good case in point in the Oligarchic system in Russia. In theory that's a capitalist system, but actually it's not. In practice individual citizens in Russia don't have meaningful capital and economic rights, if they actually try to fund and start a company that genuinely challenges the incumbents they get torn apart by the incumbents. Real capitalism means full individual rights over capital, but as with any other right that can only exist in a broadly free society. reply teekert 4 hours agorootparentprevHmm yeah I use it in the spirit of Ayn Rand, free markets, free minds shouldn’t be forced etc. reply krapp 2 hours agorootparentA system is what that system does, not what it should ideally do in theory, but constantly fails to do in practice. reply croes 3 hours agorootparentprevBy lobbying you mean buying politicians? Sounds like capitalism to me. reply rbanffy 5 hours agorootparentprevThe way monopolies do. reply seneca 5 hours agorootparentThere is no monopoly bigger, or more expensive, than government, which was previously the only player in the space. reply dialup_sounds 5 hours agorootparentprevI don't think you can monopolize going up. reply croes 4 hours agorootparentIf it costs a shitload of money you can. reply deskamess 6 hours agoprevThe first person (EV1) was awesome as the Earth was visible (day-side). And it slowly went night-side. I was surprised to not see any lights at that point and that was possibly because it was over the Pacific - or the actual vehicle was pointed away from Earth. That image of a human profile with earth in the background was kinda nice. The 40-50s lead up to the profile image: https://youtu.be/ABQBEdOzrV0?t=6868 . And as others have pointed out, the suit looks rigid. reply ceejayoz 5 hours agoparentThe lighted spacecraft in the foreground probably drowns out any lights on Earth. Same reason you don't see stars in pictures on the Moon. reply spuz 5 hours agoparentprevDoes anyone know if the reason they performed the EVA during the night was to prevent overheating? The crew reported temperatures (presumably of the suit) of 33.8C during the EVAs. reply dotnet00 3 hours agorootparentI think the main factor might've been crew wakefulness. Since they launched around the same time, I think their body clocks would've been tuned to that time too. They'd want all crew to be at the peak of their alertness in case of any emergency (particularly if they ended up having to deorbit in a hurry), so it might've been a significant factor in the timing. reply spuz 11 minutes agorootparentWe're talking a wait of just 45 minutes to do the EVA in daylight so that doesn't sound plausible as a significant reason to do it at night. reply pipe01 3 hours agorootparentprevThey oriented the capsule so that the hatch would face away from the sun, so presumably they were already accounting for heating from the sun reply ryzvonusef 4 hours agoprevAlso tested, Starlink Laser links! Apparently the crew videochatted with their families, via Crew Dragon > Laser Link > Starlink > Earth. I was surprised, i though laser link was a few years away! reply ryzvonusef 1 hour agoparentConfirmation, indeed laser links! https://x.com/PolarisProgram/status/1834286327891984474 reply bookofjoe 7 hours agoprevhttps://www.youtube.com/live/ABQBEdOzrV0?si=0MovnxWS3vBVQuad... reply alecco 4 hours agoprevFull stream recording https://x.com/SpaceX/status/1834154037606056327 Exiting hatch at 2:00:00 reply pmontra 5 hours agoprevhttps://archive.is/HuW2P reply bookofjoe 7 hours agoprevhttps://www.wsj.com/science/space-astronomy/spacex-launch-po... reply matthewfelgate 6 hours agoprevVery cool. Imagine what you could do with a ship the size of Starship in the future! reply elif 6 hours agoparentnext [3 more] [flagged] Culonavirus 4 hours agorootparentImagine being inspired by something. Who even does that. Pffft. Fucking tech bro startups. Also it's not like humans were ever nomads and explorers or anything... reply elif 2 hours agorootparentDont shoot the messenger, just remember taking away my karma when you see it happen sooner than you think reply wonderwonder 3 hours agoprevSpaceX is going to be worth more than most countries. They are going to literally own a large portion of mars and other planetary bodies as well as the transportation lanes. Just an incredibly exciting company that in time will become a true science fiction mega corp. reply limit499karma 54 minutes agoparent> They are going to literally own a large portion of mars and ... (sic) https://www.unoosa.org/oosa/en/ourwork/spacelaw/treaties/out... Article II Outer space, including the moon and other celestial bodies, is not subject to national appropriation by claim of sovereignty, by means of use or occupation, or by any other means. ... Article VI States Parties to the Treaty shall bear international responsibility for national activities in outer space, including the moon and other celestial bodies, whether such activities are carried on by governmental agencies or by non-governmental entities, and for assuring that national activities are carried out in conformity with the provisions set forth in the present Treaty. The activities of non-governmental entities in outer space, including the moon and other celestial bodies, shall require authorization and continuing supervision by the appropriate State Party to the Treaty. When activities are carried on in outer space, including the moon and other celestial bodies, by an international organization, responsibility for compliance with this Treaty shall be borne both by the international organization and by the States Parties to the Treaty participating in such organization. reply grecy 32 minutes agorootparentIf you are the only one that can get there and back, you effectively own a place reply Dig1t 4 hours agoprevThese space suits are very cool looking, I'm glad there was some consideration put towards aesthetics. The old marshmallow suits were iconic but definitely did not evoke thoughts of a cool sci-fi future like these ones do. reply 93po 3 hours agoparentthe traditional NASA suits are also absolutely massive and would make fitting people into the crew capsule very difficult reply rosmax_1337 6 hours agoprevI think the spacesuit looks kinda bad. reply andyjohnson0 5 hours agoparentBad as in good? I think they look pretty cool. I wouldn't want to survive alone on Mars with one, but they look like they've had some brand design input. reply rosmax_1337 5 hours agorootparentBad as in bad. A lot of space technology is purely functional, but these clearly have poor design taste put into them. The old suits that the Apollo era used were probably almost only functional, yet even they look better. reply ZiiS 4 hours agorootparentNot sure how it is possible to consider fashion vs getting to do a space walk. reply rosmax_1337 4 hours agorootparentI think they did consider fashion for these suits, they just did so poorly. reply snek_case 3 hours agorootparentI think it's hard to make the suits look sleek and cool if only because you need to fit many layers of insulation and rotator joints, tubing, etc. Makes it very difficult to have something tailored and fitted, which seems to be what SpaceX is trying to do. There could be a mobility advantage in having a suit that is closer to your body though, as in less risk of having your suit tangled or damaged by objects around you. reply rosmax_1337 3 hours agorootparentI'm not really making a judgement about their competence, the results just didn't turn out well. I think these things are indeed very difficult. To hazard a guess about the competency behind this, they had some people filling double roles doing the fashion of the suit. That they essentially asked an engineer with a side gig as a designer to make it. reply TeaBrain 1 hour agorootparentprevThey look poor compared to what? They're more modern looking than the ISS suits currently in use. reply rbanffy 5 hours agoparentprevIt seems very rigid, but it's also much less bulky than the current ISS EVA suit batch. The requirements are very different though - ISS suits are modular and designed to stay in space for as long as possible before needing to get back down for a refurbish. reply everfrustrated 3 hours agorootparentRigid means higher air pressure which means less time in the airlock meaning longer missions. reply JKCalhoun 4 hours agoparentprevRobosapien vibes… reply globalnode 4 hours agoprevim not sure i agree with this definition of spacewalk. is this like how the definition of space was changed too? overhype and marketing. reply patrick0d 4 hours agoparentWhat's your definition? reply BeefySwain 4 hours agoparentprevPlease explain what you mean by this. reply drooby 3 hours agorootparentI suspect OP wanted a human floating (\"walking\") in space. It appears that Isaacman popped his head out of a hole while holding onto a ladder. reply indoordin0saur 3 hours agorootparentprevI personally think this mission is amazing for many reasons. But the astronauts aren't free floating in space, they're just poking their heads out the hatch. reply dmix 3 hours agorootparentIt looks a lot like the Gemini spacewalks which is the main analogy to this. Just not floating on a tether I guess. reply uberman 5 hours agoprev [–] The average US resident produces about 16 tons of greenhouse gasses a year. A Falcon launch (like this one) produces about 28,000 tons of greenhouse gas. This guy paid to take a ride to space producing 1,700 person-years worth of greenhouse gas emissions or alternatively 6,000 car-years. I sure hope there was real science going on in the background as I'm kind of done with billionaire space tourism. Note: Several people have correctly pointed out that my emissions number is wrong. The actual 28,000 figure is in CO2 equivalence not directly in gasses emitted. Clearly some emissions are much worse than CO2. see: https://www.space.com/spacex-starship-rocket-launches-enviro... \"Starship launch produces 76,000 metric tons of carbon dioxide equivalent (a measure combining different types of greenhouse gases in one unit). That's 2.72 times more emissions than those produced by a single SpaceX Falcon 9 launch\" 78,000 metric tons of carbon dioxide equivalent / 2.72 is 27,941 metric tons of carbon dioxide equivalent. While I recognize that some celebrities also produce a lot of pollution, I struggle to justify space tourism that produces in 3 days what Taylor Swift produces in 3 years. reply coldpie 4 hours agoparentIt's good to keep an eye on the carbon impacts of space tourism, but if you're genuinely concerned about climate change, your efforts would be far better spent on promoting the passenger vehicle EV conversion and nuclear/wind/solar buildouts to replace coal plants. I don't think space tourism will become a significant carbon emissions source for at least a few more decades. reply uberman 3 hours agorootparentI happen to support electrification and renewables. It cost almost nothing (other than some negative rep) to raise a flag about the environmental costs of space tourism. I'm sure early coal fired plants said the same thing about their emissions as well. That surely one more plant or private jet could not hurt the environment. My view is if we normalize and celebrate space tourism now, then there will be no curtailing it in the future. reply indoordin0saur 3 hours agorootparentprevPlus the Starship uses Methalox which is the most efficient carbon based fuel in terms of H2O to CO2 ratio. It puts out twice as much water vapor as it does CO2. reply Culonavirus 4 hours agoparentprev> 28,000 tons of greenhouse gas I did the math and it's around 3 in Taylor Swift Year units. Outrageous! EDIT: For the people who actually eat this BS (28000 tons of a thing from another thing that has a mass of just 500 tons should raise your eyebrows), the actual number is around 120 tons of CO2 for one launch of one F9 booster. reply uberman 4 hours agorootparentI did mis-speak there. it is 28k tons of CO2 equivalent, my bad. see: https://www.space.com/spacex-starship-rocket-launches-enviro... \"Starship launch produces 76,000 metric tons of carbon dioxide equivalent (a measure combining different types of greenhouse gases in one unit). That's 2.72 times more emissions than those produced by a single SpaceX Falcon 9 launch\" 78,000 metric tons of carbon dioxide equivalent / 2.72 is 27,941 metric tons of carbon dioxide equivalent. reply rnrn 3 hours agorootparentDraft Environmental Assessment for the SpaceX Starship and Super Heavy Launch Vehicle at Kennedy Space Center (KSC) https://netspublic.grc.nasa.gov/main/20190801_Final_DRAFT_EA... Table 3-7 shows an estimated 83794 metric tons CO2e for 24 starship+superheavy launches, much lower than the figure in that article ( 1872000 tons CO2e for 24 launches ). reply rnrn 2 hours agorootparentI found a 2022 paper by Andrew Wilson, the cited source for the figure in the space.com article. The estimates for carbon footprint for space tourism in that paper are generally consistent with estimate in the Starship + Super Heavy EA and are also much lower than the per-launch numbers of 78000 tons for starship / 27941 tons for falcon 9. Loïs Miraux, Andrew Ross Wilson, Guillermo J. Dominguez Calabuig, Environmental sustainability of future proposed space activities, Acta Astronautica, Volume 200, 2022, Pages 329-346, ISSN 0094-5765, https://doi.org/10.1016/j.actaastro.2022.07.034. https://pure.strath.ac.uk/ws/portalfiles/portal/139539863/Mi... > In terms of orbital space tourism, two missions (one past, one planned) assumed to be typical have been analysed: a flight in Earth orbit based on the Inspiration4 mission (Falcon 9, 4 passengers), and a flight around the Moon based on the dearMoon mission (Starship Super Heavy, 10 passengers). Based on this, although different launch vehicles are used, it can be estimated that the carbon footprint of a passenger in an orbital tourism flight is about 660tCO2eq 4 passengers * 660 tCO2eq = 2640 tCO2eq for a launch, ~10x lower reply olex 4 hours agoparentprevDo you have a source for that number? The whole rocket weighs 550 metric tons fully fueled on the pad, I am having a hard time coming up with a way for that to create 50x as much greenhouse gas. According to Everyday Astronaut's article (https://everydayastronaut.com/rocket-pollution/), a Falcon 9 emits a total of about 600 tons of various gases during a launch. The difference vs the rocket's actual mass seems to be accounted for by water vapor, which comes from ground support equipment and not the rocket itself. reply geertj 4 hours agorootparent> The whole rocket weighs 550 metric tons fully fueled on the pad, I am having a hard time coming up with a way for that to create 50x as much greenhouse gas. It is in fact impossible. The rocket carries all of its fuel and oxidizer, so basic conservation of mass implies that the total amount of CO2 cannot exceed said 550 metric tons. reply garblegarble 3 hours agorootparentThe total emissions a launch is responsible for is not limited to the weight of the fully fuelled rocket. You've also got to include the emissions needed to extract/purify/cool/pump all that fuel/oxidiser (and any other consumables like nitrogen), a percentage of the construction emissions cost for the 1st stage, pre-flight refurb emissions, the whole of the 2nd stage, and some percentage of the Dragon capsule (including alterations made to it). Then you add all of the emissions of the ground operations needed for this flight including 1st stage recovery (and also any flight/boat diversions required from 3rd parties because of the exclusion zone). I'm also assuming that the altitude you emit certain things at (e.g. aluminium vapourising in the atmosphere) changes their impact vs at ground level. Personally, I think this mission is more worthwhile space tourism than Bezos who's pretending a high-altitude rocket flight that doesn't reach orbit is 'going to space'... at least they're testing new spacesuits on this mission. reply dotnet00 3 hours agorootparentIf we're looking at its emissions with all those downstream requirements involved too, the point of comparison would be all the emissions involved in everything that the average American relies on, which is certainly not just 16 tons. reply garblegarble 3 hours agorootparentYes, that seems worthwhile to do - I was just replying to the notion that the direct emissions of the rocket during launch is the limit of emissions. SpaceX is also highly integrated, so I do think the construction and ground operations cost (for everything SpaceX did that would not have happened had this mission not launched) are worthwhile including. My feeling is that the scale of rocketry isn't large enough that 28k tons of emissions makes a difference vs, say, the much greater number of private helicopter & jet flights (or single-use plastics, or growing & watering crops in a desert, or any of the crazier things we do as a species) reply rosmax_1337 4 hours agorootparentprevExcept you might be calculating the material, personnel, property costs also. The launch itself doesn't release that much, but everything leading up to the launch might. I still disagree with the general sentiment though. Space is more important than most other pursuits. reply geertj 3 hours agorootparent> Except you might be calculating the material, personnel, property costs also. The launch itself doesn't release that much, but everything leading up to the launch might. Yup, and the supply chain for all of those. At scale, that should go down to a small fraction above the direct emissions (certainly not an order of magnitude more). And if this never becomes at-scale, then we don't have a problem anyway. reply uberman 3 hours agorootparentprevI don't disagree that space science is important. Space tourism on the other hand is a different thing in my opinion. reply geertj 3 hours agorootparent> Space tourism on the other hand is a different thing in my opinion. Space tourism is funding part of the R&D required through for new space to be able to do exploration. reply uberman 3 hours agorootparentprevsee: https://www.space.com/spacex-starship-rocket-launches-enviro... \"Starship launch produces 76,000 metric tons of carbon dioxide equivalent (a measure combining different types of greenhouse gases in one unit). That's 2.72 times more emissions than those produced by a single SpaceX Falcon 9 launch\" Note, I did mis-quote when I said 28k tons of greenhouse gas, it is 28k tons of CO2 equivalent. reply uberman 3 hours agorootparentprevsee: https://www.space.com/spacex-starship-rocket-launches-enviro... \"Starship launch produces 76,000 metric tons of carbon dioxide equivalent (a measure combining different types of greenhouse gases in one unit). That's 2.72 times more emissions than those produced by a single SpaceX Falcon 9 launch\" Note, I did mis-quote when I said 28k tons of greenhouse gas, it is 28k tons of CO2 equivalent. reply ec109685 4 hours agoparentprevIn other words, he increased the carbon output of the US by .0005%, a pretty small relative increase. I am sure scientific knowledge increased more than that. reply forgot-im-old 3 hours agorootparentThat's a reasonable calculation I suppose. The 0.0005% increase is in carbon emissions per year by the U.S. -- that same 0.0005% increase in publications would be about 15. reply bangaladore 4 hours agoparentprevAccording to this source [1], you are off by two orders of magnitude. Starship also uses way less than you claimed. [1] https://www.theguardian.com/science/2021/jul/19/billionaires... reply uberman 3 hours agorootparenthttps://www.space.com/spacex-starship-rocket-launches-enviro... reply malfist 1 hour agoparentprev [–] > I'm kind of done with billionaire space tourism. Not to be rude, but you were never \"doing\" billionaire space tourism. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "SpaceX astronauts have begun a spacewalk to test new spacesuits, highlighting SpaceX's comprehensive capabilities in the commercial space sector.",
      "This mission underscores SpaceX's ability to offer end-to-end space services, from rockets and capsules to ground operations and spacesuits, making space travel more accessible for those with sufficient funds.",
      "The testing of SpaceX's new spacesuits is a significant step, showcasing their innovation and commitment to advancing space technology and safety."
    ],
    "points": 188,
    "commentCount": 198,
    "retryCount": 0,
    "time": 1726140617
  },
  {
    "id": 41519240,
    "title": "Kolmogorov-Arnold networks may make neural networks more understandable",
    "originLink": "https://www.quantamagazine.org/novel-architecture-makes-neural-networks-more-understandable-20240911/",
    "originBody": "Home Novel Architecture Makes Neural Networks More Understandable Read Later Share Copied! (opens a new tab) Comments Read Later Read Later machine learning Novel Architecture Makes Neural Networks More Understandable By Steve Nadis September 11, 2024 By tapping into a decades-old mathematical principle, researchers are hoping that Kolmogorov-Arnold networks will facilitate scientific discovery. Read Later Nico Roper for Quanta Magazine Introduction By Steve Nadis Contributing Writer September 11, 2024 View PDF/Print Mode artificial intelligence computer science deep learning interpretability machine learning neural networks All topics “Neural networks are currently the most powerful tools in artificial intelligence,” said Sebastian Wetzel (opens a new tab), a researcher at the Perimeter Institute for Theoretical Physics. “When we scale them up to larger data sets, nothing can compete.” And yet, all this time, neural networks have had a disadvantage. The basic building block of many of today’s successful networks is known as a multilayer perceptron, or MLP. But despite a string of successes, humans just can’t understand how networks built on these MLPs arrive at their conclusions, or whether there may be some underlying principle that explains those results. The amazing feats that neural networks perform, like those of a magician, are kept secret, hidden behind what’s commonly called a black box. AI researchers have long wondered if it’s possible for a different kind of network to deliver similarly reliable results in a more transparent way. An April 2024 study (opens a new tab) introduced an alternative neural network design, called a Kolmogorov-Arnold network (KAN), that is more transparent yet can also do almost everything a regular neural network can for a certain class of problems. It’s based on a mathematical idea from the mid-20th century that has been rediscovered and reconfigured for deployment in the deep learning era. Although this innovation is just a few months old, the new design has already attracted widespread interest within research and coding communities. “KANs are more interpretable and may be particularly useful for scientific applications where they can extract scientific rules from data,” said Alan Yuille (opens a new tab), a computer scientist at Johns Hopkins University. “[They’re] an exciting, novel alternative to the ubiquitous MLPs.” And researchers are already learning to make the most of their newfound powers. Fitting the Impossible A typical neural network works like this: Layers of artificial neurons (or nodes) connect to each other using artificial synapses (or edges). Information passes through each layer, where it is processed and transmitted to the next layer, until it eventually becomes an output. The edges are weighted, so that those with greater weights have more influence than others. During a period known as training, these weights are continually tweaked to get the network’s output closer and closer to the right answer. What Is Machine Learning? artificial intelligence What Is Machine Learning? July 8, 2024 Read Later A common objective for neural networks is to find a mathematical function, or curve, that best connects certain data points. The closer the network can get to that function, the better its predictions and the more accurate its results. If your neural network models some physical process, the output function will ideally represent an equation describing the physics — the equivalent of a physical law. For MLPs, there’s a mathematical theorem that tells you how close a network can get to the best possible function. One consequence of this theorem is that an MLP cannot perfectly represent that function. But KANs, in the right circumstances, can. KANs go about function fitting — connecting the dots of the network’s output — in a fundamentally different way than MLPs. Instead of relying on edges with numerical weights, KANs use functions. These edge functions are nonlinear, meaning they can represent more complicated curves. They’re also learnable, so they can be tweaked with far greater sensitivity than the simple numerical weights of MLPs. Yet for the past 35 years, KANs were thought to be fundamentally impractical. A 1989 paper (opens a new tab) co-authored by Tomaso Poggio, a physicist turned computational neuroscientist at the Massachusetts Institute of Technology, explicitly stated that the mathematical idea at the heart of a KAN is “irrelevant in the context of networks for learning.” One of Poggio’s concerns goes back to the mathematical concept at the heart of a KAN. In 1957, the mathematicians Andrey Kolmogorov (opens a new tab) and Vladimir Arnold (opens a new tab) showed — in separate though complementary papers — that if you have a single mathematical function that uses many variables, you can transform it into a combination of many functions that each have a single variable. Andrey Kolmogorov (top) and Vladimir Arnold proved in 1957 that it’s possible to rewrite a complicated mathematical function as a combination of simpler ones. From top: Alexander Makarov/RIA Novosti; Sputnik/Science Photo Library There’s an important catch, however. The single-variable functions the theorem spits out might not be “smooth,” meaning they can have sharp edges like the vertex of a V. That’s a problem for any network that tries to use the theorem to re-create the multivariable function. The simpler, single-variable pieces need to be smooth so that they can learn to bend the right way during training, in order to match the target values. So KANs looked like a dim prospect — until a cold day this past January, when Ziming Liu (opens a new tab), a physics graduate student at MIT, decided to revisit the subject. He and his adviser, the MIT physicist Max Tegmark (opens a new tab), had been working on making neural networks more understandable for scientific applications — hoping to offer a peek inside the black box — but things weren’t panning out. In an act of desperation, Liu decided to look into the Kolmogorov-Arnold theorem. “Why not just try it and see how it works, even if people hadn’t given it much attention it in the past?” he asked. Share this article Copied! (opens a new tab) Newsletter Get Quanta Magazine delivered to your inbox Subscribe now Recent newsletters (opens a new tab) Ziming Liu used the Kolmogorov-Arnold theorem to build a new kind of neural network. Wenting Gong Tegmark was familiar with Poggio’s paper and thought the effort would lead to another dead end. But Liu was undeterred, and Tegmark soon came around. They recognized that even if the single-value functions generated by the theorem were not smooth, the network could still approximate them with smooth functions. They further understood that most of the functions we come across in science are smooth, which would make perfect (rather than approximate) representations potentially attainable. Liu didn’t want to abandon the idea without first giving it a try, knowing that software and hardware had advanced dramatically since Poggio’s paper came out 35 years ago. Many things are possible in 2024, computationally speaking, that were not even conceivable in 1989. Liu worked on the idea for about a week, during which he developed some prototype KAN systems, all with two layers — the simplest possible networks, and the type researchers had focused on over the decades. Two-layer KANs seemed like the obvious choice because the Kolmogorov-Arnold theorem essentially provides a blueprint for such a structure. The theorem specifically breaks down the multivariable function into distinct sets of inner functions and outer functions. (These stand in for the activation functions along the edges that substitute for the weights in MLPs.) That arrangement lends itself naturally to a KAN structure with an inner and outer layer of neurons — a common arrangement for simple neural networks. Max Tegmark, Liu’s adviser, made the key suggestion that led to functioning Kolmogorov-Arnold networks: Why not build a KAN with more than two layers? Courtesy of Max Tegmark But to Liu’s dismay, none of his prototypes performed well on the science-related chores he had in mind. Tegmark then made a key suggestion: Why not try a KAN with more than two layers, which might be able to handle more sophisticated tasks? That outside-the-box idea was the breakthrough they needed. Liu’s fledgling networks started showing promise, so the pair soon reached out to colleagues at MIT, the California Institute of Technology and Northeastern University. They wanted mathematicians on their team, plus experts in the areas they planned to have their KAN analyze. In their April paper (opens a new tab), the group showed that KANs with three layers were indeed possible, providing an example of a three-layer KAN that could exactly represent a function (whereas a two-layer KAN could not). And they didn’t stop there. The group has since experimented with up to six layers, and with each one, the network is able to align with a more complicated output function. “We found that we could stack as many layers as we want, essentially,” said Yixuan Wang (opens a new tab), one of the co-authors. Proven Improvements The authors also turned their networks loose on two real-world problems. The first relates to a branch of mathematics called knot theory. In 2021, a team from DeepMind announced they’d built an MLP that could predict a certain topological property for a given knot after being fed enough of the knot’s other properties. Three years later, the new KAN duplicated that feat. Then it went further and showed how the predicted property was related to all the others — something, Liu said, that “MLPs can’t do at all.” The second problem involves a phenomenon in condensed matter physics called Anderson localization. The goal was to predict the boundary at which a particular phase transition will occur, and then to determine the mathematical formula that describes that process. No MLP has ever been able to do this. Their KAN did. But the biggest advantage that KANs hold over other forms of neural networks, and the principal motivation behind their recent development, Tegmark said, lies in their interpretability. In both of those examples, the KAN didn’t just spit out an answer; it provided an explanation. “What does it mean for something to be interpretable?” he asked. “If you give me some data, I will give you a formula you can write down on a T-shirt.” The ability of KANs to do this, limited though it’s been so far, suggests that these networks could theoretically teach us something new about the world, said Brice Ménard (opens a new tab), a physicist at Johns Hopkins who studies machine learning. “If the problem is actually described by a simple equation, the KAN network is pretty good at finding it,” he said. But he cautioned that the domain in which KANs work best is likely to be restricted to problems — such as those found in physics — where the equations tend to have very few variables. Liu and Tegmark agree, but don’t see it as a drawback. “Almost all of the famous scientific formulas” — such as E = mc2 — “can be written in terms of functions of one or two variables,” Tegmark said. “The vast majority of calculations we do depend on one or two variables. KANs exploit that fact and look for solutions of that form.” The Ultimate Equations Liu and Tegmark’s KAN paper quickly caused a stir, garnering 75 citations within about three months. Soon other groups were working on their own KANs. A paper (opens a new tab) by Yizheng Wang of Tsinghua University and others that appeared online in June showed that their Kolmogorov-Arnold-informed neural network (KINN) “significantly outperforms” MLPs for solving partial differential equations (PDEs). That’s no small matter, Wang said: “PDEs are everywhere in science.” A July paper (opens a new tab) by researchers at the National University of Singapore was more mixed. They concluded that KANs outperformed MLPs in tasks related to interpretability, but found that MLPs did better with computer vision and audio processing. The two networks were roughly equal at natural language processing and other machine learning tasks. For Liu, those results were not surprising, given that the original KAN group’s focus has always been on “science-related tasks,” where interpretability is the top priority. Related: A New Link to an Old Model Could Crack the Mystery of Deep Learning Researchers Discover a More Flexible Approach to Machine Learning Latest Neural Nets Solve World’s Hardest Equations Faster Than Ever Before Meanwhile, Liu is striving to make KANs more practical and easier to use. In August, he and his collaborators posted a new paper (opens a new tab) called “KAN 2.0,” which he described as “more like a user manual than a conventional paper.” This version is more user-friendly, Liu said, offering a tool for multiplication, among other features, that was lacking in the original model. This type of network, he and his co-authors maintain, represents more than just a means to an end. KANs foster what the group calls “curiosity-driven science,” which complements the “application-driven science” that has long dominated machine learning. When observing the motion of celestial bodies, for example, application-driven researchers focus on predicting their future states, whereas curiosity-driven researchers hope to uncover the physics behind the motion. Through KANs, Liu hopes, researchers could get more out of neural networks than just help on an otherwise daunting computational problem. They might focus instead on simply gaining understanding for its own sake. By Steve Nadis Contributing Writer September 11, 2024 View PDF/Print Mode artificial intelligence computer science deep learning interpretability machine learning neural networks All topics Share this article Copied! (opens a new tab) Newsletter Get Quanta Magazine delivered to your inbox Subscribe now Recent newsletters (opens a new tab) The Quanta Newsletter Get highlights of the most important news delivered to your email inbox Email Subscribe Recent newsletters (opens a new tab) Also in Computer Science Computer Scientists Prove That Heat Destroys Quantum Entanglement quantum computing Computer Scientists Prove That Heat Destroys Quantum Entanglement By Ben Brubaker August 28, 2024 Read Later Are Robots About to Level Up? The Joy of Why Are Robots About to Level Up? By Steven Strogatz August 14, 2024 Read Later How Base 3 Computing Beats Binary explainers How Base 3 Computing Beats Binary By Stephen Ornes August 9, 2024 Read Later Comment on this article Quanta Magazine moderates comments to facilitate an informed, substantive, civil conversation. Abusive, profane, self-promotional, misleading, incoherent or off-topic comments will be rejected. Moderators are staffed during regular business hours (New York time) and can only accept comments written in English. Show comments Next article The Cellular Secret to Resisting the Pressure of the Deep Sea",
    "commentLink": "https://news.ycombinator.com/item?id=41519240",
    "commentBody": "Kolmogorov-Arnold networks may make neural networks more understandable (quantamagazine.org)184 points by isaacfrond 8 hours agohidepastfavorite52 comments stefanpie 6 hours agoThe main author of KANs did a tutorial session yesterday at MLCAD, an academic conference focused on the intersection of hardware / semiconductor design and ML / deep learning. It was super fascinating and seems really good for what they advertise it for, gaining insight and interpret for physical systems (symbolic expressions, conserved quantities , symmetries). For science and mathematics this can be useful but for engineering this might not be the main priority of an ML / deep learning (to some extent). There are still unknowns for leaning hard tasks and learning capacity over harder problems. Even choices in for things like the chosen basis function used for the KAN “activations” and what other architectures these layers can be plugged into with some gain is still unexplored. I think as people mess around with KANs we’ll get better answers to these questions. reply notpublic 5 hours agoparentPresentation by the same author made 2 months back: https://www.youtube.com/watch?v=FYYZZVV5vlY reply triclops200 3 hours agoprevThe (semi) automatic simplification algorithm provided in the paper for KANs seem, to me, like they're solving a similar problem to https://arxiv.org/pdf/2112.04035, but with the additional constraint of forward functional interpretability as the goal instead of just a generalized abstraction compressor. reply light_hue_1 6 hours agoprevThey cannot. Just because one internal operation is understandable, doesn't imply that the whole network is understandable. Take even something much simpler: decision trees. Textbooks give these as an example of understandable systems. A tree where you make one decision based on one feature at a time then at the leaves you output something. Like a bunch of if statements. And in the 90s when computers were slow and trees were small this was true. Today massive decision trees and approaches like random forests can create trees with millions of nodes. Nothing is interpretable about them. We have a basic math gap when it comes to understanding complex systems. Yet another network type solves nothing. reply t_mann 5 hours agoparentI think of it as \"Could Newton have used this to find the expressions for the forces he was analyzing (eg gravitational force = g m_1 m_2 / d^2)?\". I once asked a physics prof whether that was conceivable in principle, and he said yes. It seems to me like KANs should be able to find expressions like these given experimental data. If that was true, then I don't see how that wouldn't deserve being called interpretability. reply fjkdlsjflkds 4 hours agorootparent> It seems to me like KANs should be able to find expressions like these given experimental data. Perhaps, but this is not something unique to KANs: any symbolic regression method can (at least in theory) find such simple expressions. Here is an example of such type of work (using non-KAN neural networks): https://www.science.org/doi/10.1126/sciadv.aay2631 Rephrasing: just because you can reach simple expressions with symbolic regression methods based on neural networks (or KANs) does not necessarily imply that neural networks (or KANs) are inherently interpretable (particularly once you start stacking multiple layers). reply nathan_compton 3 hours agorootparentprevJust giving the force law hardly counts as interpret-ability. You probably know that the 1/r^2 in the force law comes from the dimensionality of space. That is the interpretation. reply empath75 5 hours agoparentprevEven extremely complicated decision trees are interpretable to some extent because you can just walk through the tree and answer questions like: \"If this had not been true, would the result have been different?\". It may not be possible to hold the entire tree in your head at once, but it's certainly possible to investigate the tree as needed to understand the path that was taken through it. reply svboese 5 hours agorootparentBut couldn‘t the same be said about standard MLPs or NNs in general? reply ljosifov 2 hours agorootparentYou are right and IDK why you are downvoted. Few units of perceptrons, few nodes in a decision tree, few of anything - they are \"interpretable\". Billions of the sames - are not interpretable any more. This b/c our understanding of \"interpretable\" is \"an array of symbols that can fit a page or a white board\". But there is no reason to think that all the rules of our world would be such that they can be expressed that way. Some maybe, others maybe not. Interpretable is another platitudinous term that seems appealing at 1st sight, only to be found to not be that great after all. We humans are not interpretable, we can't explain how we come up with the actions we take, yet we don't say \"now don't move, do nothing, until you are interpretable\". So - much ado about little. reply Scene_Cast2 5 hours agorootparentprevLIME (local linear approximation basically) is one popular technique to do so. Still has flaws (such as not being close to a decision boundary). reply empath75 5 hours agorootparentprev_Sometimes_, and people do find features in neural networks by tweaking stuff and seeing how the neurons activate, but in general, no. Any given weight or layer or perceptron or whatever can be reused for multiple purposes and it's extremely difficult to say \"this is responsible for that\", and if you do find parts of the network responsible for a particular task, you don't know if it's _also_ responsible for something else. Whereas with a decision tree it's pretty simple to trace causality and tweak things without changing unrelated parts of the tree. Changing weights in a neural network leads to unpredictable results. reply tomhallett 3 hours agorootparentIf a KAN has multiple layers, would tweaking the equations of a KAN be more similar to tweaking the weights in a MLP/NN, or more similar to tweaking a decision tree? EDIT: I gave the above thread (light_hue_1 > empath75 > svboese > empath75) to chatgpt and had it write a question to learn more, and it gave me \"How do KAN networks compare to decision trees or neural networks when it comes to tracing causality and making interpretability more accessible, especially in large, complex models?\". Either shows me and ai are on the right track, or i'm as dumb as a statistical token guessing machine.... https://imgur.com/3dSNZrG reply baq 4 hours agoparentprevyeah. you can run SHAP[0] on your xgboosted trees, results are kinda interesting, but it doesn't actually explain anything IME. [0] https://shap.readthedocs.io/en/latest/index.html reply cubefox 4 hours agorootparentNo wonder. \"Shapley values\" have the problem that they assume all necessary conditions are equally important. Say a successful surgery needs both a surgeon and a nurse, otherwise the patient dies. Shapley values will then assume that both have contributed equally to the successful surgery. Which isn't true, because surgeons are much less available (less replaceable) than nurses. If the nurse gets ill, a different nurse could probably do the task, while if the surgeon gets ill, the surgery may well have to be postponed. So the surgeons are more important for (contribute more to) a successful surgery. reply ImHereToVote 5 hours agoparentprevA formula or equation that enables you to reason about complex systems might simply not exists. It could very well be that to reason about complexity forces you to actually do the complexity. reply mansoor_ 6 hours agoprevNot really. For a trivial function fitting problem, a KAN will allow you to visualise the contribution of each base function into the next layer of your network. Still, these trivial shallow networks are the ones nobody needs to introspect. A deep NN will not be explainable using this approach. reply Taikonerd 4 hours agoparentYeah. I'm not sure if anything with millions or billions of parameters will ever be \"explainable\" in the way we want. I mean, imagine a regular multivariable function with billions of terms, written out on a (very big) whiteboard. Are we ever really going to understand why it produces the numbers it does? KANs may have an order of magnitude fewer parameters, but the basic problem is still the same. reply etiam 4 hours agorootparentGood points. Personally I'm still basically with Geoff Hinton's early conjecture that people will have to choose whether they want a model that's easy to explain or one that actually works as well as it could. I'd imagine the really big whiteboard would often be understandable in principle, but most people wouldn't be very satisfied at having the model go \"Jolly good. Set aside the next 25 years in your calendar then, and tell me when you're ready to start on practicing the prerequisites!\". On the other hand, one might question how often we really understand something complex ostensibly \"explained\" to us, rather than just gloss over real understanding. A lot of the time people seem to act as if they don't care about really knowing it, and just (hopefully!) want to get an inkling what's involved and make sure that the process could be demonstrated not to be seriously flawed. The models are being held to standards that are typically not applied to people nor to most traditional software. But sure, there are also some real issues about reliability, trust and bureaucratic certifications. reply crazygringo 3 hours agorootparentprev> Are we ever really going to understand why it produces the numbers it does? I would expect so, because we can categorize things hierarchically. A medium-sized library contains many billions of words, but even with just a Dewey decimal system and a card catalog you could find information relatively quickly. There's no inherent difficulty in understanding what a billion terms do, if you're able to just drill down using some basic hierarchies. It's just about finding the right algorithms to identify and describe the best set of hierarchies. Which is difficult, but there's no reason to think it won't be solvable in the near term. reply scarmig 3 hours agorootparentprevI came across \"Learning XOR: exploring the space of a classic problem\" other day: https://www.maths.stir.ac.uk/~kjt/techreps/pdf/TR148.pdf Even something with three units and two inputs is nontrivial to understand on a deep level. reply afiori 4 hours agorootparentprevI found these articles very interesting in the context of future ways to understand LLM/AIs https://www.astralcodexten.com/p/the-road-to-honest-ai https://www.astralcodexten.com/p/god-help-us-lets-try-to-und... reply throwaway2562 3 hours agoprevThe point on interpretability is scientific applications is in symbolic regression - MLPs cannot always spit out an equation for some data set: KANs can. reply buildbot 2 hours agoparentI thought that MLPs are universal function approximators? https://en.wikipedia.org/wiki/Universal_approximation_theore... reply esafak 4 hours agoprevRecently discussed in https://news.ycombinator.com/item?id=40219205 reply RustySpottedCat 6 hours agoprevCan someone explain exactly what is the \"unknown\" of neural networks? We built them, we know what they comprise of and how they work. Yes, we can't map out every single connection between nodes in this \"multilayer perceptron\" but don't we know how these connections are formed? reply og_kalu 5 hours agoparentSota LLMs like GPT-4o can natively understand b64 encoded text. Now we have algorithms that can decode and encode b64 text. Is that what GPT-4o is doing ? Did training learn that algorithm ? Clearly not or at least not completely because typos in b64 that would destroy any chance of extracting meaning in the original text for our algorithms are barely an inconvenience for 4o. So how is it decoding b64 then ? We have no idea. We don't built Neural Networks. Not really. We build architectures and then train them. Whatever they learn is outside the scope of human action beyond supplying the training data. What they learn is largely unknown beyond trivial toy examples. We know connections form, we can see the weights, we can even see the matrices multiplying. We don't know what any of those calculations are doing. We don't know what they mean. Would an alien understand C Code just because he could see it executing ? reply mapt 3 hours agorootparentOur DNA didn't build our brain. Not really. Our DNA coded for a loose trainable architecture with a lot of features that result from emergent design, constraints of congenital development, et cetera. Even if you include our full exome, a bunch of environmental factors in your simulation, and are examining a human with obscenely detailed tools at autopsy, you're never going to be able to tell me with any authenticity whether a given subject possesses the skill 'skateboarding'. reply drdeca 1 hour agorootparentI find this analogy kind of confusing? Wouldn’t the analogous thing be to say that our DNA doesn’t understand, uh, how we are able to skateboard? But like, we generally don’t regard DNA as understanding anything, so that not unexpected. Where does “we can’t tell whether a person possesses the skill of ‘skateboarding’?” fit in with, DNA not encoding anything specific to skateboarding? It isn’t as if we designed our genome and therefore if our genome did hard-code skateboarding skill that we would therefore (as designers of our genome) have full understanding of how skateboarding skill works at the neuron level. I recognize that a metaphor/analogy/whatever does not have to extend to all parts of something, and indeed most metaphors/analogies/whatever fail at some point if pushed too far. But, I don’t understand how the commonalities you are pointing to between [NN architecture : full NN network with the specific weights] and [human genome : the whole behavior of a person’s brain including all the facts, behaviors, etc. that they’ve learned throughout their life] is supposed to apply to the example of _knowing_that_ a person knows how to skateboard? It is quite possible that I’m being dense. Could you please elaborate on the analogy / the point you are making with the analogy? reply HarHarVeryFunny 4 hours agorootparentprevBase64 encoding is very simple - it's just taking each 6-bits of the input and encoding (replacing) it as one of the 64 (2^6) characters A-Za-z0-9+/. If the input is 8-bit ASCII text, then each 3 input characters will be encoded as 4 Base64 characters (3 * 8 = 24 bits = 4 * 6-bit Base64 chunks). So, this is very similar to an LLM having to deal with tokenized input, but instead of sequences of tokens representing words you've got sequences of Base64 characters representing words. reply og_kalu 3 hours agorootparentIt's not about how simple B64 is or isn't. In fact i chose a simple problem we've already solved algorithmically on purpose. It's that all you've just said, reasonable as it may sound is entirely speculation. Maybe \"no idea\" was a bit much for this example but any idea certainly didn't come from seeing the matrices themselves fly. reply kevindamm 2 hours agorootparentThat's not entirely true in the case of base64 because of how statistical patterns within natural languages work. For example, you can use frequency analysis to decrypt a monoalphabetic substitution cipher on pretty much any language if you have a frequency table for character n-grams of the language, even with small numbers for n. This is a much more shallow statistical processing than what's going on within an LLM so I don't think many were surprised that a transformer stack and attention heads could decode base64. Especially if there were also examples of base64-encoding in the training data (even without parallel corpora for their encodings). It doesn't explain higher level generalizations like being a transpiler between different programming languages that didn't have any side-by-side examples in the training data. Or giving an answer in the voice of some celebrity. Or being able to find entire rhyming word sequences across languages. These are probably more like the kind of unexplainable generalizations that you were referring to. I think it may be better to frame it in terms of accuracy vs precision. Many people can explain accurately what an LLM is doing under all those matrix multiplies, both during training and inference. But, precisely why an input leads to the resulting output is not explainable. Being able to do that would involve \"seeing\" the shape of the hypersurface of the entire language model, which as sibling commenters have mentioned is quite difficult even when aided by probing tools. reply HarHarVeryFunny 3 hours agorootparentprevHuh? I just pointed out what Base64 encoding actually is - not some complex algorithm, but effectively just a tokenization scheme. This isn't speculation - I've implemented Base64 decode/encode myself, and you can google for the definition if you don't believe I've accurately described it! reply og_kalu 3 hours agorootparentThe speculation here is not about what b64 text is. It's about how the LLM has learnt to process it. Edit: Basically, For all anyone knows, it treats b64 as another language entirely and decoding it is akin in the network to translating French rather than the very simple swapping you've just described. reply HarHarVeryFunny 1 hour agorootparentLLMs, just like all modern neural nets, are trained via gradient descent which means following the most direct path (steepest gradient on the error surface) to reduce the error, with no more changes to weights once the error gradient is zero. Complexity builds upon simplicity, and the LLM will begin by noticing the direct (and repeated without variation) predictive relationship between Base64 encoded text and corresponding plain text in the training set. Having learnt this simple way to predict Base64 decoding/encoding, there is simply no mechanism whereby it could change to a more complex \"like translating French\" way of doing it. Once the training process has discovered that Base64 text decoding can be PERFECTLY predicted by a simple mapping, then the training error will be zero and no more changes (unnecessary complexification) will take place. reply Lerc 4 hours agoparentprevWe know how they are formed(and how to form them), we don't know why forming in that particular way solves the problem at hand. Even this characterization is not strictly valid anymore, there is a great deal of research into what's going on inside the black box. The problem was never that it was a black box(we can look inside at any time), but that it was hard to understand. KANs help some of that be placed into mathematical formulation. Generating mappings of activations over data similarly grants insight. reply mjburgess 4 hours agoparentprev* Given the training data, and the architecture of the network, why does SGD with backprop find the given f? vs. any other of an infinite set. * Why are there are a set of f each with 0-loss that work? * Given the weight space, and an f within it, why/when is a task/skill defined as a subset of that space covered by f? I think a major reasons why these are hard to answer is that it's assumed that NNs are operating within an inferential statistical context (ie., reversing some latent structure in the data). But they're really bad at that. In my view, they are just representation-builders that find proxy representations in a proxy \"task\" space (def, aprox, proxy = \"shadow of some real structure, as captured in an unrelated space\"). reply spencerchubb 4 hours agoparentprevWe know the process to train a model, but when a model makes a prediction we don't know exactly \"how\" it predicts the way it does. We can use the economy as an analogy. No single person really understands the whole supply chain. But we know that each person in the supply chain is trying to maximize their own profit, and that ultimately delivers goods and services to a consumer. reply lupire 6 hours agoparentprevWe don't know what each connection means, what information is encoded in each weight. We don't know how it would behave differently if each of the million or trillion weights was changed. Compare this to dictionaey, where it's obvious what information is on each page and each line. reply wslh 6 hours agoparentprevThe brain serves as a useful analogy, even though LLMs are not brains. Just as we can’t fully understand how we think by merely examining all of our neurons, understanding LLMs requires more than analyzing their individual components, though decoding LLMs is most likely easier, which doesn't mean easy. reply taneq 6 hours agoparentprevThere’s a ton of research going into analysing and reverse engineering NNs, this “they’re mysterious black boxes and forever inscrutable” narrative is outdated. reply IWeldMelons 3 hours agoprevFad. reply empath75 5 hours agoprevI have a question, which might not even be related to this -- one of the keys to the power of neural networks is exploiting the massive parallelism enabled by GPUs, but are we leaving some compute on the table by using just scalar weights? What if instead of a matrices of weights, what if they were matrices of functions? reply dahart 4 hours agoparentThey way to think about NNs is that they are already made of functions; groups of layered nodes become complex nonlinear functions. For example a small 3-layer network can learn to model a cubic spline function. The internals of the function are learned at every step of the way; every addition and multiplication. You can assume the number of functions in a network is a fraction of the number of weights. This makes the NN theoretically more flexible and powerful than modeling it using more complex functions, because it learns and adapts each and every function during training. I would assume its possible using certain functions to, say, model a small fixed-function MLP could perhaps result in more efficient training, if we know the right functions to use. But you could end up losing perf too if not careful. I’d guess the main problems are we don’t know what functions to use, and adding nonlinear functions might come with added difficultly wrt performance and precision and new modes of initialization and normalization. Linear math is easy and powerful and already capable of modeling complex functions, but nonlinear math might be useful I’d guess… needs more study! ;) reply ocular-rockular 1 hour agoparentprevWhat you're describing is very similar to deep Gaussian processes. reply mglz 4 hours agoparentprevGPUs are optimized for matrices of floating point values, so current neural networks use this as a basis (with matrices containing the scalar weights). reply immibis 4 hours agoparentprevEach row/column (I always forget which way around matrices go) of weights followed by a nonlinearity is a learnable function. reply itsthecourier 7 hours agoprevTL;DR: they are talking about KAN (Kolmogorov-Arnold networks) reply weberer 5 hours agoparentYeah. Thankfully, HN updated the title to be more descriptive. (Old title was \"Novel Architecture Makes Neural Networks More Understandable\") reply xiaodai 6 hours agoprevIt doesn't that's the problem reply js8 3 hours agoprev [–] I don't what KANs are, but from the informal description in the article \"turn function on many variables into many functions of single variable\", it sounds reminiscent of lambda calculus. reply samus 2 hours agoparent [–] Nope, that's just currying and/or partial application. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Researchers are investigating Kolmogorov-Arnold networks (KANs) to enhance the transparency and interpretability of neural networks.",
      "KANs differ from traditional multilayer perceptrons (MLPs) by using nonlinear functions instead of numerical weights, allowing for more nuanced adjustments and explanations of their outputs.",
      "Recent studies have shown KANs' effectiveness in scientific applications, such as knot theory and condensed matter physics, highlighting their potential to derive scientific rules from data."
    ],
    "commentSummary": [
      "Kolmogorov-Arnold networks (KANs) could improve neural network interpretability by deriving symbolic expressions and conserved quantities from data.",
      "A tutorial at MLCAD showcased KANs' potential for understanding physical systems, but challenges persist in learning complex tasks and integrating KANs with other architectures.",
      "The debate continues on whether large models, including KANs, can ever be fully understood, despite their ability to produce simpler expressions compared to deep neural networks."
    ],
    "points": 184,
    "commentCount": 52,
    "retryCount": 0,
    "time": 1726136045
  },
  {
    "id": 41515447,
    "title": "Why Oxide Chose Illumos",
    "originLink": "https://rfd.shared.oxide.computer/rfd/0026",
    "originBody": "committed RFD 26 RFD 26 Host Operating System & Hypervisor This RFD can be accessed by the following groups:[public] State committed RFD 26 Updated Servers in the Oxide Rack will form the backbone of both the compute and storage services available to customers. The rack will be composed of a variety of hardware and software components, some of which represent resource-constrained or inflexible environments; e.g., the Service Processor (SP) or Hardware Root Of Trust (ROT). In contrast, the server CPU is a cornucopia: software update delivery is easy when compared to device firmware; rich postmortem debugging and live tracing or instrumentation facilities are feasible; the full power of modern programming platforms like Rust is available. This RFD will explore options for the software stack that runs atop the host CPU: the Operating System (OS) and the Virtual Machine Monitor (VMM). Hypervisor Choices When choosing an open source VMM, there are two primary choices: KVM (on GNU/Linux) bhyve (on illumos) Not under consideration: Xen: Large and complicated (by dom0) codebase, discarded for KVM by AMZN VirtualBox: C++ codebase makes it unpalatable to work with Emerging VMMs (OpenBSD’s vmm, etc): Haven’t been proven in production In addition to the choice of a kernel VMM (the component which handles privileged VM context state, nested paging, etc) and its accompanying OS, attention should also be paid to the userspace portion of providing a VMM implementation. This component handles much of the device emulation state as well as tasks like live migration. KVM on GNU/Linux Of the existing open source VMMs, KVM is the most feature rich. It includes support for features such as: nested virtualisation processor feature masking and emulation VMM paravirt device emulation (Hyper-V time facilities, for example) PCI-passthru live migration The most popular userspace component to complement KVM is QEMU. Built as a general-purpose emulator supporting a vast array of platforms and devices, QEMU is able to rely on KVM for the \"hard\" parts of x86 virtualisation while exposing emulated devices to the running guest. The wide collection of supported devices comes at a cost: QEMU is complex and often the subject of bugs affecting its reliability and security. For that reason, QEMU is outside our scope of consideration for a VMM userspace. Google developed crosvm, a Rust-based userspace component using KVM, to allow for the use of other operating systems in the Chromebook environment. As part of their Firecracker project, Amazon adapted crosvm to produce a small and focused VMM userspace in Rust. With much of that now contained in rust-vmm, a series of Rust crates designed to perform emulation for a limited set of devices, several userspace VMMs have sprung up from that code lineage, including Cloud Hypervisor from Intel. While not all of the features we require are present in the rust-vmm crates, Firecracker, or Cloud Hypervisor, it could serve as a good base to build what we need, while preserving the desired level of safety and performance. bhyve on illumos bhyve began as a de novo VMM developed on FreeBSD. Unlike some of its predecessors, it was designed to require the HVM-accelerating features (VT-x, EPT) present in modern CPUs. This freed it from needing to support older and more onerous emulation logic such as shadow page tables. Between this and its lack of nested virtualisation support, the codebase is presently much smaller and more manageable than KVM. Simpler codebase (due to requiring modern hardware features to function) Most of its testing and production use has been on Intel CPUs Limited PCI-passthru support (reportedly iffy on AMD) Presently, bhyve utilizes a custom C-based userspace. While it is much smaller in scope than QEMU, the complexity of device emulation have caused it to be a source of bugs (security and otherwise) in the past. The rust-vmm crates do not presently have direct support for the bhyve kernel VMM , but its structure is not too different from KVM (for which the crates were designed), which makes porting them onto bhyve a plausible option if we were to want a Rust-based userspace on that platform as well. Note: While FreeBSD is where bhyve originated, and it remains the upstream repository of record for it, we are not considering it as a potential OS choice. There is not a significant difference in functionality between the illumos and FreeBSD implementations, since pulling patches downstream has not been a significant burden. Conversely, the more advanced OS primitives in illumos have resulted in certain bugs being fixed only there, having been difficult to upstream to FreeBSD. Goals and Exploration It may be possible to consider the OS and the VMM in isolation, but in practice these decisions are at least somewhat related. As we are exploring the specific bodies of software at hand, we should consider in depth the goals that we have for our OS and VMM. Rust as a First Class Citizen Most popular operating systems today are written chiefly or entirely in the C programming language. While it is certainly possible to write safe C at some cost, there is mounting evidence to suggest that from time to time, some expense has been spared. While we are not setting out to proactively rewrite the operating system in Rust, we are interested in exploring new venues for the use of safe and powerful languages like Rust in systems programming. Rust is seeing increasing use in the embedded space, and we at Oxide are working on our own microcontroller operating environment in the form of Hubris. It seems likely that Rust would be a good language for implementing system daemons and tools in a larger scale UNIX system, as well as system libraries that need to expose a C ABI. In a modular kernel, one can easily imagine writing a new NIC driver or packet filtering policy engine in Rust. A little under fifty years ago, UNIX made the transition from being mostly assembly language to mostly C: a demonstrably higher level language with a commensurate increase in robustness, portability, and engineering productivity. The future is unwritten, but Rust presents an exciting prospect for the basis of the next fifty years of UNIX development. Whichever platform we choose to build on, we are committing ourselves to some degree of maintenance work. That said, all choices we would consider here have an active upstream project with which we should try to maintain a relationship. As we explore the implementation of new and increasingly critical subsystems in a language other than C, we should consider our path to getting that work included in the upstream project. There are several key points in favour of illumos on the Rust front: no opposition to kernel modules created and maintained outside of the core source tree or with a different licence; indeed, extending so far as to provide a stable API/ABI for many core facilities required for device drivers unlike with Linux, there are few if any instances of conditional compilation used to change the shape of the kernel for different use cases; use of more complex C interaction tools like bindgen would not be required to maintain a set of crates that interact with kernel primitives smaller community with a set of values relatively tightly aligned with those we have seen in the Rust community; should be an easy sell to produce new core OS components (e.g., new drivers, programs, and libraries) in Rust within the base system There have also been early positive developments for Rust in the Linux kernel. In early July 2020, a Linux kernel mailing list thread on in-tree Rust support arranged a discussion at the upcoming Linux Plumbers Conference. That discussion occured in August 2020, and was reportedly positive in nature. As of March 2021, work on a prototype for writing Linux drivers in Rust is happening in the linux-next tree. Guest Facilities What do we need to support in guests? Operating Systems We expect that customers will chiefly expect to use Linux and Windows guests in an Oxide Rack. At least initially, only operating systems that support UEFI booting will be supported. Emulated Devices Guests workloads will expect some minimum set of emulated deviced to be available in order to function properly. Beyond the normal x86 PC architecture devices (APIC, PIT, etc), we’ll need interfaces to block storage and network connectivity. The virtio device model has been popular and is (mostly) well supported by modern OSes. There are some questions around the quality of drivers available for Windows on that front. For block storage, it would be advantageous to support API-driven hot insertion and hot removal of volumes from the guest. The PCI-centric device models (pci-virtio-block and NVMe) make this more challenging than something like virtio-scsi. It’s not clear to what extent such functionality is a requirement. Most OSes utilize the x86 TSC for timekeeping, when it’s available. Windows is a notable standout here, in that it prefers the HPET. We may need to implement some portion of the Hyper-V paravirtualized device interface, so that Windows guests can use it (and the TSC) for time. Under certain conditions, Linux guests are known to boot slowly as they wait for a slowly-filling entropy pool. It may be advantageous to provide a virtio-rng device for them to alleviate the dearth of boot time entropy. Out-of-band Management Most access to the guest operating system will likely be through idiomatic remote access systems provided by the OS itself; e.g., SSH for Linux, and Remote Desktop Protocol (RDP) for Windows. As with a physical server, in the event that a guest is malfunctioning the user may want out-of-band access to the (virtual) console of the guest. Console access may also be important when using operating system install media (e.g., ISO images) to create a virtual machine template image from scratch. In addition to console operation, the ability to \"insert\" an ISO file into an emulated CD-ROM drive or a raw disk image into an emulated USB drive may also be needed for this use case. To make this work we’d need to emulate a frame buffer, and a workstation keyboard and mouse. Our guest firmware package will need support for providing those things to the guest OS. The emulated frame buffer would need to be able to be connected from the server to the remote user via some protocol like VNC or SPICE. Nested Virtualisation Nested virtualisation is not considered an essential feature at this time. While it may be useful for CI/CD workloads, and can be convenient for hypervisor development, it is challenging to emulate the underlying interfaces with flawless fidelity. That fact, combined with its dreadful performance for more production-facing workloads make it less attractive as a focus for the first product. Live Migration Live migration of virtual machines (commonly known as vMotion in VMware), where the state of a running VM is moved between physical machines without downtime or any other significant perceived impact, is considered a necessary feature of our product. It allows operators to migrate workload within the greater system to balance capacity and mitigate the impact of planned maintenance. The cost of software updates or hardware maintenance on a node are greatly reduced if the instances can be drained from it first. Security Over the last 2+ years, a great deal of mistrust has been fostered over the ability (or lack thereof) of modern microprocessors to adequately isolate shared resources in the face of speculative execution and, more broadly, microarchitecture-focused attacks. Faced with these challenges, the OS must be flexible and provide the tools required (core-aware scheduling, vertical threads) in order to make a genuine attempt to mitigate those attacks. Control Plane Support Facilities Virtual machines are the most prominent customer-visible resource that the Oxide Rack will provide, but there are a number of critical internal software components that will form the basis for delivery of that service; e.g., highly available distributed block storage ([rfd29]), the user facing API ([rfd4], [rfd21]), and the control plane as a whole ([rfd48]). This additional software will almost certainly run on the same servers as the hypervisor, and thus facilities we might use to build it merit some consideration here. Facilities for support and debugging of that software are discussed in a later section. Isolation and Sandboxing Modern programming environments like Rust provide a high degree of memory safety, at least when used in their \"safe\" mode. As a result, a variety of common bugs in older software environments are generally avoided by construction. That said, bugs in the compiler or core libraries, or in \"unsafe\" code, can still result in exploitable defects. In addition, memory safety and even an advanced type system cannot completely preclude logic errors that exist in otherwise well-formed programs; e.g., a network service might incorrectly treat its input and allow a request to effect some unforeseen privilege escalation. As a result, we would like strong sandboxing facilities to allow us to balance the need for co-location of unrelated processes with the need for isolation between them. As a minimum unit of isolation, the process model shared by all modern UNIX platforms affords us an isolated address space for a body of related code. Security critical components could conceivably be split into multiple separate processes which communicate through IPC mechanisms; e.g., a network server might handle incoming requests and forward instructions to a separate process for managing a virtual machine. Emulated guest requests for HTTP Metadata or DNS may be processed in yet another separate process. These processes should be able to opt out of certain privileged operations; e.g., the guest metadata service need not be able to fork new processes or read local files, while the virtual machine manager might not need to make any network requests at all. To make privilege separation between processes work well, we would need observable, robust IPC mechanisms with a reasonable performance profile. Consider two different different classes of workload: first, a customer virtual machine; and second, a storage server program that forms part of our network block storage system. Each of these represent a different, but well-defined domain of security, functionality, and life cycle. These domains would benefit from being executed within an isolated partition where visibility of other domains is limited or completely obscured. From a life cycle perspective, the customer domain might be ephemeral, brought into existence by the control plane only while the VM is running on a particular server. In contrast, the network storage software is responsible for owning some amount of data, and will persist between server reboots. In addition to limiting cross-domain visibility, we will likely want to use quotas or caps to limit the blast radius of a run away software component (e.g., one leaking memory) and provide more predictable performance for customer workloads. Classes of limit we may wish to consider include: customer virtual machines pinned to a specific set of host CPUs internal services limited to a specific subset of CPU cores, which likely does not overlap with those used for customer workloads limits on memory resource usage limits on file system space usage in any shared pool of storage (the network storage service may deal more directly with entire NVMe disks) Service Supervision One level up from the process itself, the more abstract notion of a \"service\" may be composed of several related processes. A fault in any process may require a restart of all related processes. The supervisor will be responsible for restarting a software component that has failed, and may need to track and alarm on additional degraded states for software that appears to be failing persistently. The supervisor should be able to track related processes, such as if a program creates a child process to handle some helper task or some set of requests. Child processes that form part of a unit should not be able to \"escape\" their relationship with other processes in a unit, even through acts like double forking. The service supervisor will be another source of telemetry for the broader stack. Ideally it will report software faults with a similar set of structured information and through the same channels as faults in hardware devices. Comparison: Service Management Facililty (SMF) & systemd The Service Management Facility (SMF) is responsible for the supervision of services under illumos. It was introduced with Solaris 10 in 2005. SMF provides a number of facilities for managed services: Two levels of service object: the service (like a class), and the instance (a specific instantiation of a particular service, called default for the trivial case of a single-instance service). A typed key-value store with transactions, allowing properties to be attached at the service or instance level; utilises a basic inheritance structure so that common properties can be defined at the service level. Parallel start of services once dependent services are running, as well as configurable event-driven restart of related services as required. A set of ergonomic commands for management and monitoring of services, svcadm(1M) and svcs(1), and to ease access to values from the property store in service startup scripts, svcprop(1). A stable library interface, libscf(3LIB), for the creation, configuration, monitoring, and control of services. Automatic capture of stdin and stdout output into rotated log files. Integration with the Fault Management Architecture for reporting on software level faults. SMF is built in large part upon process contracts from the contract subsystem ( contract(4), libcontract(3LIB), uts/common/os/contract.c, etc). The classical UNIX process model presents a number of challenges for service management; e.g., the reparenting of processes when they outlive their parent, a behaviour often utilised as part of the startup sequence for long-running service processes. Process contracts provide a separate hierarchy for tracking the relationship between related processes in the system, providing a robust way to track whether a service is still running on a system. The subsystem also provides a variety of mechanisms for reporting on faults (e.g., fatal signals, core files, etc) and other life cycle events. SMF integrates well with other parts of the system, including the privileges(5) subsystem. Service instances can be configured to start not only with specific credentials (users and groups) but with a specific subset of privileges. For example, a process could be allowed to bind a listen socket with a port number lower than 1024 — the classical \"secure\" port range reserved for the super user — without being granted any other elevations. Conversely, a process might be denied the privilege to create new files in the file system, without hampering its ability to report status into the SMF-managed log files. Systemd was introduced in 2010, and in the ensuing decade has seen almost ubiquitous adoption by economically relevant Linux distributions. Ten years in, with a broadening sprawl of different programs under the one banner, it’s not clear that the systemd project has a single goal in mind — but the most critical functionality it provides is ostensibly similar to SMF: the management of services as a first class facility in the system. Though systemd has seen broad adoption, this process has not been without controversy. The project leadership has at times handled criticism or egregious bugs with perhaps less empathy than might have been ideal. One source of consternation has been the extent to which the project provides (at times quite minimal) implementations of such a wide variety of functionality, without considering the potential downsides of their new and often heavily opiniated approach. Instead of relying on an existing logging mechanism, or providing managed access to regular files on disk, systemd provides the journal subsystem. The journal is a binary log format that has at times been a bottleneck in production software. Journal files have been reported to become corrupt under some conditions. The journal daemon itself has been a source of relatively recent security issues, a problem that is less commonly seen with simpler direct-to-file logging. There have been similar simplistic expansions into other parts of the system, such as time synchronisation, where a variety of what may either be mistakes or deliberate design choices have often resulted in a patently inflexible and somewhat inaccurate source of time. The project has also moved to supplant existing name resolution caching with a local DNS proxy that until relatively recently could not resolve A records with sufficient entries to require EDNS support, and the official project position appears to be that if you want anything but the absolute basics to work you should probably use something else. Realistically, it seems a robust infrastructure product would likely end up using few if any of the components provided by the systemd project, despite there now being something like a hundred of them. Instead, more traditional components would need to be revived, or thoroughly bespoke software would need to be developed, in order to avoid the technological and political issues with this increasingly dominant force in the Linux ecosystem. Software Deployment It would help to have facilities in the operating system that enable robust deployment of the software artefacts that we layer on top. Some programs end up as a single broadly static binary, but others end up as a tree of files that must be deployed as a unit. A classical solution to the problem of deployment has been to use a software packaging system like dpkg/apt or rpm/yum. Management of those systems, especially at scale and with unattended operation, has proven challenging. Recent emerging approaches instead choose to build atomically deployable artefacts that comprise a prebuilt (and sometimes even immutable) file system image, containing all required and related files for a given service. These modern approaches often make use of operating system facilities like containers that can serve both as a unit of isolation and as a unit of deployment. The deployable artefact is often much like an entire file system; e.g., we have heard that Google uses ext4 file system images mounted over an iSCSI transport from a central highly available storage service. Another previously implemented solution is to use ZFS send streams to reconstitute a child file system into an existing system pool. As described earlier, at least some core internal workloads will need to live persistently on a particular server, and be started automatically as part of server boot. Such core services will need to exist in order to solve the chicken-and-egg problem of cold start of the rack. Server Management Facilities In order to make the system robust, and do so with a high level of automation, we will need visibility into the underlying hardware and its drivers. This should include both status information in addition to errors. We intend to collect that information in a programmatic and structured manner for use elsewhere in the system. As such its producers will ideally have an accessible and consistent interface to emit such status and error information. It is likely that there will be consumers both in the OS kernel itself (for example, to retire a bad page of memory) as well as in user mode. Component Inventory It would be good to have a rich topology map within the server that covers all of the enumerable hardware, including: sensors (temperature, etc) indicators (LEDs) serial numbers firmware versions physical locations (e.g., \"Front Disk Slot 5\") logical attachment points (e.g., nodes in a SAS or NVMe fabric) Ideally, this topology information would be concentrated in one consistent interface, rather than a collection of disjoint ones. Dependable resource IDs could be derived from that for use on the local system and to properly annotate information that is collected at the cluster level. Fault Management It would be good to have a system that can track telemetry that may itself not rise to the level of an actionable fault, but where emerging trends may (in aggregate) represent a fault; e.g., memory correctable errors (perhaps 1 per day is OK, but 10 per day is not) memory uncorrectable errors PCI-e errors Machine Check Architecture (MCA) As noted above, while some of these actions will be immediately actionable for the host OS, it is important that they also be made available to systems higher up in the stack so that reporting and decision-making at the cluster level will be possible. Comparison: illumos & Linux The Fault Management Architecture (FMA) was introduced in Solaris 10 in 2005, and was subsequently inherited by illumos. It is a comprehensive system with several components, both in the kernel and in user mode, supporting the detection and reporting of both software and hardware faults. Events are correlated by diagnosis engines to produce a whole system view of fault conditions, with the potential for corrective action; e.g., activation of hot spare disks in a degraded ZFS pool, retirement of memory and CPUs, etc. FMA provides kernel facilities for the management of faults in hardware devices; e.g., ddi_fm_acc_err_get(9F) and ddi_fm_dma_err_get(9F) for detecting errors reported by underlying hardware access (PCI, DMA, etc). ddi_fm_service_impact(9F) and ddi_fm_ereport_post(9F) for reporting device errors to the fault management system for recording and potential diagnosis. FMA stores informational telemetry (e.g., Ethernet link state changes) and hardware faults (e.g., correctable or uncorrectable DIMM errors) in a structured log. These log records are then processed by modules loaded by the fault management daemon (fmd(1M), FMDPRM). Some modules are diagnosis engines, and some serve other purposes such as forwarding telemetry records to remote hosts or fetching telemetry from subservient devices such as a service processor. Coherent management of the health of the system requires a complete picture of the hardware topology. Modern computing systems are often composed of a complex tree or graph of buses and switches, with many interconnected devices cooperating to provide service. NVMe disks may be connected through a variety of PCI bridges, just as SAS and SATA disks may be attached via a complex topology of expanders or port multipliers. FMA provides a view of the hardware through libtopo, the topology library (a somewhat dated description appears in FMDPRM). The system is represented in different ways (e.g., logical topology and physical topology) via different trees or graphs of objects with a common property-based interface. Properties are exposed with serial and model numbers, and any other notable device-specific information such as link speed or disk capacity. Each device has a URL-like construct, the Fault Management Resource Identifier (FMRI) that uniquely identifies it, and is used in other parts of the stack as a way to concretely nominate particular devices. FMA also provides a basis for the promotion of persistent soft faults into a hard fault using Soft Error Rate Discrimination (SERD). Broadly, one correctable memory error per week might be expected, but three per hour may be predictive of a fault. In addition to the general FM module interface, the Eversholt module and associated fault tree description language allow for the definition of various measurement engines and transformation of telemetry into diagnoses in a relatively declarative fashion. The Linux ecosystem has a variety of point solutions to particular subsets of what FMA provides. Because the kernel project does not generally ship any associated daemons or libraries, they can only provide the portions of fault management that can or must be kernel-resident. One such subsystem is Error Detection And Correction (EDAC), a framework chiefly tasked with reporting memory errors and retiring pages. As a kernel-only component, this module does not seek to perform more in-depth diagnosis that might require, say, a persistent long term store of error telemetry, or the kind of classification that is best performed in a daemon. The focus of EDAC appears relatively narrow, and it is not clear that there is a broader fault management effort underway. Instead, the general state of the art outside DIMM faults appears to be liberal use of dmesg and the occasional non-fatal (!) oops for reporting serious errors. Beyond the kernel, there are other projects like mcelog that seem to provide some user mode recording of more complex error conditions. The project is capable of decoding and logging the machine check exceptions from which it gets its name, as well as some set of memory and PCI errors. It even claims to support a level of predictive failure analysis. Platform support is not extremely broad; e.g., the project FAQ suggests that users of AMD Zen-era hardware should just stick to EDAC at least for now. The author of the project gave a talk in 2009 about a laudible goal of unified error reporting, but it seems that EDAC may perhaps have emerged as the plan of record and stalled other efforts. On the topology front, there are a mixture of sources that one would need to tie together to provide a complete picture of the system. Some data is present in textual files within /proc and /sys, some of which appear to be documented as stable and some of which are treated as private implementation details that could presumably change with any new kernel release. Others are commands with textual (sometimes parseable) output; e.g., lshw, hwinfo, lspci from pciutils, lsusb from usbutils, dmidecode, lsblk from util-linux, hdparm, mii-tool from net-tools, ethtool, etc. The illumos approach to fault management is by no means perfect. It no doubt has bugs, and will require work to adapt to the continuing evolution of computer hardware and software over time. With that in mind, it appears substantially more coherent and complete than the Linux ecosystem. The benefit of co-designed kernel and user mode components in one coherent whole is perhaps thrown into its sharpest relief with fault management, an area that requires participation right up and down the software stack. Software Engineering Facilities The Oxide Rack will represent a core layer of the infrastructure of a variety of customers with diverse needs and expectations. The product will provide facilities in several broad categories: General purpose virtual machines in which we will have little control over, and at times little ability to accurately predict, the exact nature of the workload in advance. Storage services for guests, where reliability and consistent performance are critical. A control plane and API service that users and operators use to interact with the product, with an expectation of predictable request latency and reliable operation under increasing load. Our interactions with customers will include the need to fix critical issues promptly, and to make measured improvements to the performance of the system. In order to make this possible, we will need various facilities from the operating system and the hypervisor. Debugging Metadata Many software defects are difficult to reproduce, and inspection of the production system is required in order to determine the cause of the issue. In order to aid that exploration, debugging metadata must not be limited to special debug builds of the software. The production software must include enough metadata to enable runtime inspection of the state and execution of the system. Useful metadata for debugging can include: Symbols for all functions and data objects in shared libraries, executable programs, and kernel modules; where inlining is prominent, as in Rust, we may need additional metadata beyond classical C/ELF symbols. Robust stack walking capability sufficient for runtime sampling/profiling as well as inspection of the state of a hung or crashed process. This may be provided via a number of approaches; e.g., ensuring all binaries are compiled with correct use of the frame pointer register, or include some additional unwinding metadata. It might seem like this is chiefly a concern at the application level, but our past experience has demonstrated how critical it is to extend this stance on complete visibility all the way down the stack. An application stack trace that stops at libc or some other system-provided library leaves us with a critical blind-spot in our exploration. Static Instrumentation During the design of the system, there will be some set of metrics that we would like to collect that we can determine in advance. Examples of this class of metrics include: counts of VM exit reasons per guest instance accumulated on-CPU time for each guest CPU histogram with broad buckets for slow I/O operations, so that we can determine after the fact that a guest or a disk has seen unusually slow I/O operations physical device statistics like packet, error, or interrupt counters per-facility kernel memory allocator statistics details about any processor power or clock scaling events For high-frequency events, and especially in modern vastly multi-core systems, efficient collection and aggregation of standing metrics like these is a critical part of making the system both high-performing and observable. It would be convenient for the system to present as much static instrumentation as possible in a consistent, discoverable framework. When static metrics are presented in a self-describing structure, it can help with two broad use cases: discovery during interactive exploration by support engineers without needing to learn new tools for each new class of metric library-based access by agents responsible for collecting and storing metrics over time, where the agent may have discover and work against different system versions that do not include the latest set of possible metrics Comparison: kstat & procfs/sysfs Sun introduced a uniform kernel statistics interface with Solaris 8 in 2000, known as kstat. This subsystem provides a stable kernel interface, kstat(9S); a stable library interface, kstat(3KSTAT) and libkstat(3LIB); and a command line tool, kstat(1M). The library can be used from Rust with relative ease. The statistics themselves may or may not be stable, depending on the source of the data. Some statistics are common between drivers of the same device type class; e.g., block devices provide a common base set of statistics that are used by iostat(1M) to summarise I/O activity. The Linux approach is somewhat less ruthlessly uniform. While much information is available, it has accumulated in a variety of forms and venues depending on subsystem age. Linux has had a basic /proc virtual file system since the 0.99 release in 1992, and appears to have been modeled at least in part on Plan 9. Rather than structured data, the kernel constructs string representations (except when it doesn’t; e.g., /proc/$pid/cmdline) that any consumer other than cat will need to parse to reconstitute the information. Over time, procfs has accumulated other non-process related files that convey other information about the system, often in a new textual format per file; e.g., see /proc/cpuinfo or /proc/meminfo. At some point, this file system became a writeable control interface in addition to a statistics interface, though still requiring the consumer to produce a textual representation of control messages that the kernel will then parse. Further, under the /proc/sys tree are an array of files with a different structure that is roughly analogous to the BSD sysctl(8) interface. Around Linux version 2.4 in 2001, sysfs was introduced at /sys and a partial migration of some of the non-process related files from /proc ensued. Some areas of this newer tree are at least documented as stable, though as of release candidates for version 5.8 in 2020 it continues to represent a mixture of statistics presentation and mutable control plane facilities. The virtual files in this new tree still generally require a round trip through a textual representation. It is not clear if the consolidation of statistics into a uniform structure can or will ever be completed, presumably at least in part due to use of the old /proc names in user tools that are developed and shipped apart from the kernel itself. Dynamic Instrumentation Static metrics are valuable when engineers can determine interesting values to collect and store in advance of a problem occurring. Unfortunately, many of the most pernicious software defects are not found or fixed by inspecting existing metrics. Complex defects are often not easily reproducible; their presentation may depend on ambient and difficult to catalogue background activity that is unique to a specific customer site at a specific time. In order to chase these defects to root cause and make a fix, we must be able to ask questions about the dynamic state of the system and iteratively refine those questions as we hone in on the problem. New instrumentation should be able to be enabled on a running production system without updating or even restarting the software. Human intervention in systems can lead to unforced errors, and even the most cautious engineers engaging in debugging exercises are not immune. Our instrumentation must be constructed to be safe to engage on a production system without fear of a crash, or data corruption, or serious and widely felt performance degradation. When not engaged, the machinery for instrumentation should have no impact on system operation or performance. In the limit, it should be safe to accidentally request the enabling of all possible system instrumentation without prolonged deleterious effect; it is better to report an error to the debugging engineer than to make the system unsafe. Modern systems generally consist of a lot of software, performing operations at a dizzying rate. A naive firehose approach to system observability where post-processing is used to locate the subset of interesting events is unlikely to broadly useful. In order to have minimal impact on system performance, and to answer very specific questions, the instrumentation system should be able to efficiently and safely filter and aggregate relevant events in-situ; that is, to make the decision to record the event, and which data to record, at the time of the event. Our hypervisor and other critical services, are likely to be implemented in a split between user processes and kernel modules. We will need instrumentation that can cleanly measure events that occur across more than one process, and which extend into the kernel. In general, we expect to able to gather at least the following information: scheduling events; e.g., when a thread or process goes to sleep and why, or what caused a thread or process to wake up timer-based profiling; e.g., to allow us to sample system state such as the kernel or user stack at regular intervals, possibly with other constraints such as limiting to a particular process system call entry and return, both per-process and globally kernel function entry and return, with guard rails that prevent the tracing of functions that are not safe to trace user process function entry and return, and breakpoint-style tracing of specific instructions or specific offsets within a function creation of new processes and threads, and termination (planned or unplanned) of existing processes and threads latency distributions of operations, some of which may be defined as the composite of multiple system-level operations by the engineer In some cases it can also be desirable to allow instrumentation to take limited mutating actions against the system, such as invoke a data collector program once a particular sequence of trace events has been detected. In order to aid in the reproduction of races in multi-threaded software, it can be helpful to be able to inject small delays (hundreds of microseconds) at specific moments in order to widen a suspected race window. Facilities that perturb the system may present a trade-off in safety, and it’s possible we might want to be able to restrict these separately from general instrumentation in customer environments. Comparison: DTrace & eBPF DTrace, as described in the 2004 paper, is a system for the dynamic instrumentation of production systems. A prominent goal in its construction is to be perfectly safe, and over more than a decade of use on a wide variety of production workloads it has proven to be sufficiently robust that engineers and operators need not worry when enabling it on a critical system. Many key design decisions stem from safety as a core goal; e.g., the instruction set for the tracing virtual machine allows no backwards branches, so infinite loops are impossible by construction. Joyent hosted arbitrary customer workloads within zones, an isolation and virtualisation technology similar in principle to FreeBSD jails or Docker containers. DTrace was sufficiently safe that access could be granted to customers to instrument software running within their container, with only restricted visibility into global system behaviour. In addition to raw DTrace access, part of the Cloud Analytics product was built on top of DTrace instrumentation. This product was able to collect statistics both from probes that fired in user applications, and from the underlying operating system, aggregating them in a live graphical view. Finally, countless real production problems were solved by long-running DTrace enablings distributed throughout the fleet, waiting to log data about the events leading up to some critical fault, but without otherwise affecting the regular operation of the system. In the more distant past, DTrace was a critical underpinning of the Analytics feature of the Fishworks appliance at Sun. Analytics enabled customers to drill down into fine detail while analysing the performance of the system, providing more abstract control over DTrace enablings and presenting an interactive graphical view of the resultant telemetry. The Berkeley (née BSD) Packet Filter (BPF) was introduced in 1992, to provide a safe virtual machine that could be included in the kernel for selective packet capture. By allowing the filtering engine to run safely in the kernel, the performance overhead of copying every packet into a user address space for filtering could be avoided. It followed from similar approaches taken in earlier systems. In 2014, an extended BPF (eBPF) was introduced to the mainline Linux kernel for a variety of uses. In contrast to many prior approaches, the eBPF virtual machine makes use of a just-in-time (JIT) compiler to convert eBPF instructions into native program text as they are loaded into the kernel. This choice appears to be the result of an attempt to build one system for two masters: Adding new behaviours to the system, even in the data path, where performance is of paramount performance and programs must run to completion for system correctness even if they have an outsized impact on the rest of the system; e.g., filtering, configuring, or redirecting socket connections classifying and shaping network traffic system call security policy, resource, and quota management in cgroups network encapsulation protocol implementation Tracing and performance measurement of the system; e.g., by allowing eBPF programs to hook various trace points and events from the perf subsystem The first use case, extending the data path, requires high performance at all costs. Without low latency operations, eBPF would not be an attractive target when implementing new network filtering or encapsulation facilities. The robustness and security of eBPF appear to depend fundamentally on a component called the \"verifier\", which scans the eBPF program upon load into the kernel. The verifier attempts to determine (before execution) whether an eBPF program will do anything unsafe, and seeks to ensure that it will terminate. There have been some serious vulnerabilities found in the verifier, and it is not clear the extent to which it has been proven to work. Indeed, kernel/bpf/verifier.c is (according to cloc) eight thousand lines of non-comment C code running in the kernel. CVE-2020-8835 from earlier this year is one such example of a security issue in the verifier. By contrast, DTrace has a more constrained programming model which has allowed a more readily verified implementation. A byte code interpreter is used, with security checks directly at the site of operations like loads or stores that allow a D program to impact or observe the rest of the system. The instruction format does not allow for backwards branches, so constraining the program length has a concomitant impact on execution time and thus impact on the broader system. Each action is limited in the amount of work it can perform; e.g., by caps on the number of bytes of string data that will be read or copied, and by the overall principal buffer size. Explicit choices have been made to favour limiting system impact — one could not implement a reliable auditing or accounting system in DTrace, as the system makes no guarantee that an enabling won’t be thrown overboard to preserve the correct execution of the system. In addition to issues of implementation complexity and verifier tractability, there is the small matter of binary size. The bpftrace tool, analogous on some level to dtrace, depends directly on the library form of BCC, Clang, and LLVM. This puts the directly linked text size (as visible via ldd) at around 160MB, which was more than half of the size of the entire SmartOS RAM disk. This doesn’t account for other parts of those toolchains that generally come along for the ride, or the debugging information that is often stripped from binaries in desktop Linux distributions. By contrast, dtrace and supporting libraries run to around 11MB total including CTF. In 2020, disks, memory, and network bandwidth, are relatively cheap. That said, in contexts within the system where we choose to execute the OS from a pinned RAM image, program text size may still be an issue. Lighter debugging infrastructure is easier to include in more contexts without other trade-offs. Finally, the tools in the eBPF ecosystem continue to present a number of opportunities for improvement with respect to user experience. A relatively easy task with DTrace is to trace all system calls being made on the system or by a particular process, and to then aggregate them by system call type, or obtain a distribution of the latency of each call, or some combination of those and other things. By contrast, on a modern Ubuntu system, repeated attempts to do the same basic thing resulted in hitting any number of walls; e.g., Probe names have not been selected to be terribly ergonomic; e.g., what would in DTrace be syscall::read:entry, where each of the colon-separated portions of the tuple are available in separate variables, the analogous probe available to bpftrace appears to be tracepoint:syscalls:sys_enter_read. There is only one variable, probe, which then contains the whole string. Useful output appears likely to require post-processing for even simple tracing activities. When trying to enable all tracepoint:syscalls:sys_* probes and count the distinct probe values, it becomes apparent that enabling probes at all results in sufficient risk to the system that you may enable at most 500 of them; following the instructions in the scary warning to increase the probe count results instead in tens of seconds of the system hanging and then pages of viscous and apparently diagnostic output from the eBPF verifier. Upon further inspection, there is instead a probe site that fires for the entry of every different kind of system call, tracepoint:raw_syscalls:sys_enter, though it is difficult to actually use this effectively: the only way to identify the system call is then by its number, which though stable is also different per architecture (e.g., even 32- or 64-bit processes on x86 have different numbers). Conversely, it is possible to cheaply enable tens of thousands of probe sites with DTrace. On a current system, there are 236 system calls and something like 34000 kernel function probe sites, not including a number of other probes. A simple enabling that counts the firings of all probes over a ten second run is responsive and easy: # time dtrace -n '::: { @ = count(); }' -c 'sleep 10' dtrace: description '::: ' matched 80872 probes dtrace: pid 28326 has exited 39354882 real 0m18.393s user 0m0.564s sys 0m8.115s Postmortem Debugging Defects are an inevitable part of shipping a software product. We should seek to delight our customers with a speedy resolution to reported issues, with as few repeated incidents as possible for each defect. As much as possible, we should aim to be able to fully debug an issue from the first reported incident. To enable us to do this, our system must record as much information as possible about the state of the system when the fault occurred. To fully enable postmortem debugging, the system should provide rich facilities for the creation of core files in the face of serious faults. A core file will generally include the complete contents of process memory at the time of the fault, but it needs to include other information as well. A process has a number of open file descriptors that can represent a diverse set of resources beyond the process itself, and thus important context for debugging; e.g., Facility Examples of Context local domain sockets, pipes which process or processes are on the remote end when the pipe opened or connection started send and receive buffer state network sockets local and remote address and port when the connection started send and receive buffer state semaphores and other IPC state of the IPC object open files and directories path at open time file system and inode number if we believe the file has been unlinked kernel event ports, eventfd, timerfd information about the state of the resource; e.g., associated object count, state of timers, etc debuggers and other tracing tools was the process under the control of a debugger when terminated? The creation of a core file may have been discretionary; e.g., using a tool like gcore to create it on demand. If not discretionary, the system must record details about the cause of process termination; e.g., memory address and program counter for segmentation violations, or the source of a fatal signal if delivered from outside the process. We should also note the time, both in terms of observed wall clock time and monotonic seconds since boot. The more context we record, the more likely it is we will be able to debug the problem the first time it happens. In addition to core files, we would like the system to be able to dump the entire contents of kernel memory, and optionally the memory of a specific selection of user processes, on panic of the system. Crash dumps provide a wealth of information and have been instrumental in debugging issues with kernel facilities such as device drivers and hypervisors. Crashes of an entire server are extremely disruptive, and are a class of fault that even advanced facilities like live migration are unable to paper over. We should seek to minimise the number of repeat incidents that take out an entire server. While customers enjoy speedy resolution to the issues they report, they are presumably even happier with resolution of issues they did not have to discover themselves. For customers who are amenable to remote data collection by support organisations, we can use rich crash data collection facilities to provide an automated reporting service: core files and crash dumps can be reported to Oxide on creation for timely investigation. As we engage with more customers and more deployed systems, managing the relationship between a specific core file and the software artefacts that produced it can be a challenge. It is important that core files include sufficient program text and metadata so that a debugger can make sense of them even if it is subsequently difficult to locate the exact input source code. Metadata may be in a format like DWARF format, or some other summarised format such as CTF, depending on binary size constraints and the toolchain that produced the binary. Comparison: illumos and Linux Postmortem debugging is a practice that requires many facilities in the system to work together, and highlights the need for a coherent approach. On illumos systems, the base operating system has a number of properties that help here: Binaries are never stripped of symbols for shipping, and frame pointers are always used to mark the current stack frame. Compact C Type Format (CTF) debugging information appears in every kernel module and many user libraries and commands; this information is then embedded in any core files or crash dumps, so the debugger can always find the correct information without the need to correllate a core file with a separate catalogue of debugging information packages for shipped binaries. The Modular Debugger (MDB) is an integral part of the operating system. It is available for live use on user programs, the kernel from user mode, and on the console while the system is paused (KMDB). The same tools can then be used on kernel crash dumps and user core files. New kernel facilities are encouraged to include first class support for MDB modules that aid in debugging; e.g., for zfs or i40e. Making software debuggable in this way needs to be baked in as part of the design; e.g., critical data structures must be reachable from the debugger, and ancilliary information like event timestamps may be stored that are unused except under the debugger at a later time. Core files for user software are generated using the same algorithms and format, whether as a result of a fault (SIGABRT, SIGSEGV, etc) or at the discretion of the operator with gcore(1). On Linux systems, gcore comes from GDB, an external package with different maintainers. On illumos it is maintained alongside the same core file support in the base operating system. Comprehensive use of note entries in core files allows them to contain information about the state of the process that is not visible from the process memory or register state; e.g., pfiles(1) (similar to lsof -p) works on core files by reading information about open files sockets, pipes, etc, that were saved in NT_FDINFO notes in the core file. Linux by contrast has relatively modest use of ancilliary data here, MDB first shipped with Solaris 8, in the year 2000, replacing the older ADB. It is the latest in a long line of postmortem debugging systems dating back to at least crash in System III. By contrast, the Linux postmortem debugging landscape is comparatively fragmented, and Linus himself is somewhat infamously opposed to debuggers. KDB was integrated into Linux in late 2010 and appears to deal only with debugging of the live system (a la KMDB). It is somewhat less flexible in general: it provides only a basic set of commands for examining the state of loaded modules, the kprintf() buffer, the process list, etc. Though the ability to register additional KDB commands is present, it is used in just one place in the rest of the kernel to date. There does not appear to be a mechanism for composition or complex interactive use, such as piping the result of one KDB command into another. For postmortem debugging, one is expected to use kdump and GDB. Reports from the field, such as from Dave Jones in 2014, suggest that this facility is perhaps not especially robust. To use kdump, another Linux kernel is loaded into memory with kexec(2), using a special reserved memory area that must be sized appropriately for the particular system. At least based on the kernel documentation, moving parts are somewhat abundant. A more classical dumping approach, diskdump, was floated in 2004 but did not appear to make it to integration. The kdump documentation (admittedly mostly unchanged since 2005) suggests GDB will have trouble with 64-bit dumps from x86 systems; mailing list posts from 2014 suggest that this may still be the case. Red Hat maintains the crash utility as a body of software separate from the kernel, which at least offers a debugging module system. The kernel tree also contains various Python scripts for use with GDB, which appear to make use of whatever type metadata is available to GDB itself. Co-development naturally leads to tight integration, and features like Postmortem DTrace have been enabled by treating the whole operating system as a cohesive whole. DTrace works by assembling telemetry from instrumentation in a per-CPU log-structured buffer, which is periodically switched out for the user mode tracing tools to copy out. If the system panics, the last active buffers are included (with all other kernel memory) in the resultant dump. The DTrace MDB module is able to lift those buffers out of the dump and use the DTrace libraries to produce output for any probe firings that occured in the lead up to the crash. When a bug is difficult to reproduce, one can easily introduce additional instrumentation without needing to deploy new software or restart anything; if and when the fault recurs, that extra telemetry is readily available like a black box flight recorder. Third-Party Software Oxide will produce a variety of software in-house, such as the control plane ([rfd48], [rfd61], et al) and the storage service ([rfd29], [rfd60], et al). Alongside the software we create and control ourselves, there will be software components that we merely adopt and integrate within our stack; e.g., we wish to ship CockroachDB ([rfd110]) rather than build our own fault tolerant database system. In addition to application-level software, new hardware devices will often require a driver implemented at least partially within the operating system kernel. We are by definition adopting and taking responsibility for any body of software that we ship in the product. Funding for development and maintenance of software can and does disappear, corporate stewards can merge or be acquired, and project structures can change significantly without much warning; e.g., RethinkDB, Riak, X.Org, MySQL, etc. We will hit bugs that others do not hit, and will likely need to fix them on our own timetable. Nonetheless, it is important to consider carefully how much work we are signing up for in choosing a platform. Comparison: illumos and Linux Linux is demonstrably popular, and as such many of the open source components that we may choose for integration into the product are developed and tested first against Linux interfaces. Other platforms are often an after-thought for project maintainers, if they are even considered at all. Little needs to be said here: Linux distributions generally ship with a package set that contains sufficiently current versions of relevant open source software, and other software is generally easy to build; manufacturers are often directly involved in providing in-tree drivers for their hardware. Building the product on top of illumos will mean we at Oxide are committing to some amount of engineering and advocacy work in the future to ensure continued support from relevant upstream projects. While the illumos community is smaller than the Linux community, we would by no means be the only participants. Our early control plane database prototyping efforts have been underway using software packaged by the OmniOS project, along with software we have built ourselves. These bodies of software were relatively easy to build with only minor patches, including: CockroachDB Prometheus Grafana Chrony (NTP) ClickHouse In addition to OmniOS, the pkgsrc project maintains cross-platform build recipes for a vast storehouse of open source software; some 20,000 packages are available for illumos systems. This project is a joint effort spear-headed by the NetBSD project. The cornerstone of ensuring that modern software is easy to build is the continued availability of the runtimes and standard libraries that underpin that software. Support for illumos is in upstream stable versions, with regular CI/CD builds, of both the Rust and Go toolchains. Python, OpenJDK, and Node are also available. If a critical piece of software is not available natively and we cannot port it, it can likely be made to work within an emulation environment such as LX-branded zones or even bhyve. With respect to drivers for hardware, it is difficult to predict how much work will be required before we have completed part selection. Some components will require custom driver work, such as the PCI-e interface for managing the switch. One notable example of a component where the illumos driver is actually maintained by the manufacturer themselves is Chelsio, who have had productive engagement with the illumos community in the past. Measurement Of Success For a foundational choice like the one this document describes, we should also consider the shape of a successful outcome. To succeed, most critically we need to ship a product on a specific time frame. That means applying the engineering talent we have amassed to work on a product for the market we believe exists. It means picking a stable foundation that allows us to build the product features we want, but also to support them once they are deployed in the field. It also requires us, where possible, to choose software that is already close to being what we need — where any improvements required to produce something we can ship are tractable and sustainable. The API-driven virtual machine interface we have chosen to expose to the customer does not generally require them to comport with any internal technology choice we make, provided the product runs their workloads with an acceptable level of performance, reliability, and operability. This applies equally to components like the Service Processor ([rfd8]), where we intend to eschew IPMI and other industry standards; and to the Host Bootstrap Software ([rfd20]), where we intend to discard the UEFI stack in favour of something that more directly meets our needs. It applies as well to our choice of hypervisor and operating system. With a relatively small team and a product that will form a critical infrastructure layer, it is important to focus on selecting and building tools that act as force multipliers for us; that allow us to find and correct software defects completely and promptly from as few observed incidents as possible. Determinations As of June 2021, considerable effort has been spent on exploring and reducing the risk the use of illumos and bhyve in the product. We have continued to work with the Rust community on illumos-specific toolchain issues with positive results. Helios development environments are available to Oxide staff and are in use for various development efforts, in local and cloud virtual machines, and on physical lab hardware. We have been able to boot and use Helios in some form on various AMD Ethanol systems, as well as on the Tofino Barefoot switch sample system. Our May 2021 milestone demonstration day featured several relevant components: Helios, our illumos distribution, running on an AMD EPYC 7402P system in our lab environment Propolis, our Rust-based userspace daemon for bhyve, was featured in our May milestone demonstration day booting an Alpine Linux guest image Omicron, our Rust-based suite of control plane software, running natively on Helios The CockroachDB and Clickhouse databases, running natively on Helios When selecting an operating system in particular, there are literally thousands of dimensions on which choices can be evaluated; each dimension a new question that can be asked. Because we can only choose a single operating system, the options are effectively mutually exclusive — all of those thousands of questions ultimately have to go the same way. This document is not, and likely cannot be, an exhaustive survey of all facets of all of the choices in front of us. Other subsequent RFDs and project plans will cover specific work items in more detail as they arise. With this in mind, and based on our experience thus far, we are moving forward with: Helios, our illumos distribution, as the operating system for the host CPU in our servers Propolis, our Rust-based userspace, and bhyve in the illumos kernel for guest workloads Postscript This RFD was originally written in 2021. While Propolis was open source during its development, Helios was open sourced in January 2024. The open sourcing of Helios prompted a discussion in [oxf-s4e4] that expanded on its history and rationale, along with reflections on its implementation in our shipping rack. External References [rfd4] Oxide Computer Co. RFD 4 User Facing API. [rfd8] Oxide Computer Co. RFD 8 Service Processor (SP). [rfd20] Oxide Computer Co. RFD 20 Host Bootstrap Software: Objectives. [rfd21] Oxide Computer Co. RFD 21 User Networking API. [rfd29] Oxide Computer Co. RFD 29 Storage Requirements. [rfd48] Oxide Computer Co. RFD 48 Control Plane Requirements. [rfd60] Oxide Computer Co. RFD 60 Storage Architecture Considerations. [rfd61] Oxide Computer Co. RFD 61 Control Plane Architecture and Design. [rfd110] Oxide Computer Co. RFD 110 CockroachDB for the control plane database. [oxf-s4e4] Oxide Computer Co. Oxide and Friends, Season 4, Episode 4, Helios Hypervisor Choices KVM on GNU/Linux bhyve on illumos Goals and Exploration Rust as a First Class Citizen Guest Facilities Operating Systems Emulated Devices Out-of-band Management Nested Virtualisation Live Migration Security Control Plane Support Facilities Isolation and Sandboxing Service Supervision Comparison: Service Management Facililty (SMF) & systemd Software Deployment Server Management Facilities Component Inventory Fault Management Comparison: illumos & Linux Software Engineering Facilities Debugging Metadata Static Instrumentation Comparison: kstat & procfs/sysfs Dynamic Instrumentation Comparison: DTrace & eBPF Postmortem Debugging Comparison: illumos and Linux Third-Party Software Comparison: illumos and Linux Measurement Of Success Determinations Postscript External References Table of Contents",
    "commentLink": "https://news.ycombinator.com/item?id=41515447",
    "commentBody": "Why Oxide Chose Illumos (oxide.computer)181 points by kblissett 21 hours agohidepastfavorite116 comments bonzini 12 hours ago> QEMU is often the subject of bugs affecting its reliability and security. {{citation needed}}? When I ran the numbers in 2019, there hadn't been guest exploitable vulnerabilities that affected devices normally used for IaaS for 3 years. Pretty much every cloud outside the big three (AWS, GCE, Azure) runs on QEMU. Here's a talk I gave about it that includes that analysis: slides - https://kvm-forum.qemu.org/2019/kvmforum19-bloat.pdf video - https://youtu.be/5TY7m1AneRY?si=Sj0DFpRav7PAzQ0Y reply hinkley 6 minutes agoparentIf they are being precise, then “reliability and security” means something different than “security and reliability”. How many reliability bugs has QEMU experienced in this time? The man power to go on site and deal with in the field problems could be crippling. You often pick the boring problems for this reason. High touch is super expensive. Just look at Ferrari. reply TimTheTinker 6 hours agoparentprev> When I ran the numbers in 2019, there hadn't been guest exploitable vulnerabilities that affected devices normally used for IaaS for 3 years. So there existed known guest-exploitable vulnerabilities as recently as 8 years ago. Maybe that, combined with the fact that QEMU is not written in Rust, is what is causing Oxide to decide against QEMU. I think it's fair to say that any sufficiently large codebase originally written in C or C++ has memory safety bugs. Yes, the Oxide RFD author may be phrasing this using weasel words; and memory safety bugs may not be exploitable at a given point in a codebase's history. But I don't think that makes Oxide's decision invalid. reply bonzini 6 hours agorootparentThat would be a damn good record though, isn't it? (I am fairly sure that more were found since, but the point is that these are pretty rare). Firecracker, which is written in Rust, had one in 2019: https://www.cve.org/CVERecord?id=CVE-2019-18960 Also QEMU's fuzzing is very sophisticated. Most recent vulnerabilities were found that way rather than by security researchers, which I don't think it's the case for \"competitors\". reply TimTheTinker 6 hours agorootparentYou're not wrong, and that is very impressive. There's nothing like well-applied fuzzing to improve security. But I still don't think that makes Oxide's decision or my comment necessarily invalid, if only because of an a priori decision to stick with Rust system-wide -- it raises the floor on software quality. reply akira2501 49 minutes agorootparent> it raises the floor on software quality. Languages cannot possibly do this. reply TimTheTinker 33 minutes agorootparentI believe TypeScript and Rust are both strong examples of languages that do this (for different reasons and in different ways). It's also possible for a language to raise the ceiling of software quality, and Zig is an excellent example. I'm thinking of \"floors\" and \"ceilings\" as the outer bounds of what happens in real, everyday life within particular software ecosystems in terms of software quality. By \"quality\" I mean all of capabilities, performance, and absence of problems. It takes a team of great engineers (and management willing to take a risk) to benefit from a raised ceiling. TigerBeetle[0] is an example of what happens when you pair a great team, great research, and a high-ceiling language. [0] https://tigerbeetle.com/ reply otabdeveloper4 3 hours agorootparentprevHeresy! Software written in Rust never has security vulnerabilities or bugs. The borrow checker means you don't have to worry about security, Rust handles it for you automatically so you can go shopping. reply ameliaquining 2 hours agorootparentI do think that only having one CVE in six years is a pretty decent record, especially since that vulnerability probably didn't grant arbitrary code execution in practice. Rust is an important part of how Firecracker pulls this off, but it's not the only part. Another important part is that it's a much smaller codebase than QEMU, so there are fewer places for bugs to hide. (This, in turn, is possible in part because Firecracker deliberately doesn't implement any features that aren't necessary for its core use case of server-side workload isolation, whereas QEMU aims to be usable for anything that you might want to use a VM for.) reply anonfordays 3 hours agoparentprev>Pretty much every cloud outside the big three (AWS, GCE, Azure) runs on QEMU. QEMU typically uses KVM for the hypervisor, so the vulnerabilities will be KVM anyway. The big three all use KVM now. Oxide decided to go with bhyve instead of KVM. reply bonzini 2 hours agorootparentNo, QEMU is a huge C program which can have its own vulnerabilities. Usually QEMU runs heavily confined, but remote code execution in QEMU (remote = \"from the guest\") can be a first step towards exploiting a more serious local escalation via a kernel vulnerability. This second vulnerability can be in KVM or in any other part of the kernel. reply _rs 6 hours agoparentprevI thought AWS uses KVM, which is the same VM that QEMU would use? Or am I mistaken? reply bonzini 6 hours agorootparentAWS uses KVM in the kernel but they have a different, non-open source userspace stack for EC2; plus Firecracker which is open source but is only used for Lambda, and runs on EC2 bare metal instances. Google also uses KVM with a variety of userspace stacks: a proprietary one (tied to a lot of internal Google infrastructure but overall a lot more similar to QEMU than Amazon's) for GCE, gVisor for AppEngine or whatever it is called these days, crosvm for ChromeOS, and QEMU for Android Emulator. reply 9front 2 hours agorootparentEC2 instances are using the Xen hypervisor. At least that's what reported by hostnamectl. reply wmf 2 hours agorootparentEC2 migrated off Xen around ten years ago. Only really old instances should be using Xen or Xen emulation. reply 9front 35 minutes agorootparentI'm puzzled by your comment. On an EC2 instance of AL2023 deployed on us-east-1 region this is the output of hostnamectl: [ec2-user][~]$ hostnamectl Static hostname: ip-x-x-x-x.ec2.internal Icon name: computer-vm Chassis: vm Machine ID: ec2d54f27fc534ea74980638ccc33d96 Boot ID: 6caf18b7ed3647819c1985c11f128142 Virtualization: xen Operating System: Amazon Linux 2023.5.20240903 CPE OS Name: cpe:2.3:o:amazon:amazon_linux:2023 Kernel: Linux 6.1.106-116.188.amzn2023.x86_64 Architecture: x86-64 Hardware Vendor: Xen Hardware Model: HVM domU Firmware Version: 4.11.amazon reply bonzini 0 minutes agorootparentKVM can emulate the Xen hypercall interface. Amazon is not using Xen anymore. wmf 21 minutes agorootparentprevWhat instance type is it? reply tptacek 5 hours agorootparentprevLambda and Fargate. reply my123 4 hours agorootparentIt was true for Fargate some time ago, but is not true anymore since quite a while. All Fargate tasks run on EC2 instances today. reply easton 3 hours agorootparent…which is probably the reason why task launches take 3-5 business weeks reply tptacek 1 hour agorootparentprevAh, interesting. Thanks for the correction! reply dastbe 4 hours agorootparentprevunless something has changed in the past year, fargate still runs each task in a single use ec2 vm with no further isolation around containers in a task. reply daneel_w 2 hours agorootparentprevQEMU can use a number of different hypervisors, KVM and Xen being the two most common ones. Additionally it can also emulate any architecture if one would want/need that. reply dvdbloc 3 hours agoparentprevWhat do the big three use? reply paxys 3 hours agorootparentAWS – Nitro (based on KVM) Google – \"KVM-based hypervisor\" Azure – Hyper-V You can of course assume that all of them heavily customize the underlying implemenation for their own needs and for their own hardware. And then they have stuff like Firecracker, GVisor etc. layered on top depending on the product line. reply daneel_w 3 hours agorootparentprevSome more data: Oracle Cloud - QEMU/KVM Scaleway - QEMU/KVM reply bonzini 2 hours agorootparentIBM cloud, DigitalOcean, Linode, OVH, Hetzner,... reply ReleaseCandidat 10 hours agoprevInstead of stating more or less irrelevant reasons, I'd prefer to read something like \"I am (have been?) one of the core maintainers and know Illumos and Bhyve, so even if there would be 'objectively' better choices, our familiarity with the OS and hypervisor trump that\". A \"I like $A, always use $A and have experience using $A\" is almost always a better argument than \"$A is better than $B because $BLA\", because that doesn't tell me anything about the depth of knowledge of using $A and $B or the knowledge of the subject of decision - there is a reason half of Google's results is some kind of \"comparison\" spam. reply actionfromafar 10 hours agoparentBut everyone at Oxide already knows that back story. At least if you list some other reasons list you can have a discussion about technical merits if you want to. reply ReleaseCandidat 10 hours agorootparentBut that doesn't make sense if you have specialists for $A that also like to work with $A. Why should I as a customer trust Illumos/Bhye developers that are using Linux/KVM instead of \"real\" Linux/KVM developers? The only thing that such a decision would tell me is to not even think about using Illumos or Bhyve. The difference between \"Buy our Illumos/Bhye solution! Why? I have been an Illumos/Bhyve Maintainer!\" and \"Buy our Linux/KVM solution! Why? I have been an Illumos/Bhyve Maintainer!\" should make my point a bit clearer reply panick21_ 10 hours agorootparentThose are not the only options. You can have KVM on Illumos, or Bhye on FreeBSD. And finding people to heir that know Linux/KVM wouldn't be a problem for them. This evaluation was done years ago and they added like 50 people since then. Saying 'We have a great KVM Team but our CEO was once an Illumos developer' is perfectly reasonable. And as I point out in my other comment, the former Joyant people like know more about KVM then anything else anyway. So it would be: \"Buy our KVM Solution, we have KVM experts\" But they evaluated that Bhyve was better then KVM despite that. reply ReleaseCandidat 10 hours agorootparent> \"Buy our KVM Solution, we have KVM experts\" Of course, but that is less of unique selling point. > But they evaluated that Bhyve was better then KVM despite that. If you are selling Bhyve you better say that whether it's true or not. So why should I, as a reader or employee or customer, trust them? reply panick21_ 10 hours agoparentprevBut Bryan also ported KVM to Illumos. And Joyand used KVM and they supported KVM there for years, I assume Bryan knows more about KVM then Bhyve as he seemed very hands on in the implementation (there is nice talk on youtube). So the idea that he isn't familiar with KVM isn't the case. So based on that KVM or Bhyve on Illumos, KVM would suggest itself. In the long term if $A is actually better then $B, then it makes sense to start with $A even if you don't know $A. Because if you are trying to building a company that is hopefully making billions in revenue in the future, then long term matters a great deal. Now the question is can you objectively figure out if $A or $B is better. And how much time does it take to figure out. Familiarity of the team is one consideration but not the most important one. Trying to be objective about this, instead of just saying 'I know $A' seems quite like a smart thing to do. And writing it down also seems smart. In a few years you can look back and actually say, was our analysis correct, if no what did we misjudge. And then you can learn from that. If you just go with familiarity you are basically saying 'our failure was predetermined so we did nothing wrong', when you clearly did go wrong. reply jclulow 8 hours agorootparentFor what it's worth, we at _Joyent_ were seriously investing in bhyve as our next generation of hypervisor for quite a while. We had been diverging from upstream KVM, and most especially upstream QEMU, for a long time, and bhyve was a better fit for us for a variety of reasons. We adopted a port that had begun at Pluribus, another company that was doing things with OpenSolaris and eventually illumos, and Bryan lead us through that period as well. reply ComputerGuru 2 hours agorootparentAre you/will you be upstreaming fixes and/or improvements to Bhyve? reply jclulow 55 minutes agorootparentYes, my personal goal is to ensure that basically everything we do in the Oxide \"stlouis\" branch of illumos eventually goes upstream to illumos-gate where it filters down to everyone else! reply specialist 6 hours agorootparentprev> Trying to be objective about this... And writing it down also seems smart. Mosdef. IIRC, these RFDs are part of Oxide's commitment to FOSS and radical openness. Whatever decision is ultimately made, for better or worse, having that written record allows the future team(s) to pick up the discussion where it previously left off. Working on a team that didn't have sacred cows, an inscrutible backstory (\"hmmm, I dunno why, that's just how it is. if it ain't broke, don't fix it.\"), and gatekeepers would be so great. reply tcdent 3 hours agoprevLinux has a rich ecosystem, but the toolkit is haphazard and a little shakey. Sure, everyone uses it, because when we last evaluated our options (in like 2009) it was still the most robust solution. That may no longer be the case. Given all of that, and taking into account building a product on top of it, and thus needing to support it and stand behind it, Linux wasn't the best choice. Looking ahead (in terms of decades) and not just shipping a product now, it was found that an alternate ecosystem existed to support that. Culture of the community, design principles, maintainability are all things to consider beyond just \"is it popular\". Exciting times in computing once again! reply sausagefeet 11 hours agoprevWhile it's fair to say this does describe why Illumos was chosen, the actual RFD title is not presented and it is about Host OS + Virtualization software choice. Even if you think it's a foregone conclusion given the history of bcantrill and other founders of Oxide, there absolutely is value in putting decision to paper and trying to provide a rational because then it can be challenged. The company I co-founded does an RFD process as well and even if there is 99% chance that we're going to use the thing we've always used, if you're a serious person, the act of expressing it is useful and sometimes you even change your own mind thanks to the process. reply taspeotis 12 hours agoprevI kagi’d Illumos and apparently Bryan Cantrill was a maintainer. Bryan Cantrill is CTO of Oxide [1]. I assume that has no bearing on the choice, otherwise it would be mentioned in the discussion. [1] https://bcantrill.dtrace.org/2019/12/02/the-soul-of-a-new-co... reply sausagefeet 12 hours agoparentEarly Oxide founders came from Joyent which was an illumos shop and Cantrill is quite vocal about the history of Solaris, OpenSolaris, and illumos. reply codetrotter 10 hours agorootparent> Joyent which was an illumos shop And before that, they used to run FreeBSD. Mentioned for example in this comment by Bryan Cantrill a decade ago: https://news.ycombinator.com/item?id=6254092 > […] Speaking only for us (I work for Joyent), we have deployed hundreds of thousands of zones into production over the years -- and Joyent was running with FreeBSD jails before that […] And I’ve seen some other primary sources (people who worked at Joyent) write that online too. And Bryan Cantrill, and several other people, came from Sun Microsystems to Joyent. Though I’ve never seen it mentioned which order that happened in; was it people from Sun that joined Joyent and then Joyent switched from FreeBSD to Illumos and creating SmartOS? Or had Joyent already switched to Illumos before the people that came from Sun joined? I would actually really enjoy a long documentary or talk from some people that worked at Joyent about the history of the company, how they were using FreeBSD and when they switched to Illumos and so on. reply selykg 3 hours agorootparentJoyent also merged with TextDrive, which is where the FreeBSD part came from. TextDrive was an early Rails host, and could even do it in a shared hosting environment, which is where I think a lot of the original user base came from (also TextPattern) As I recall they were also the original host of Twitter, which if I recall was Rails back in the day. reply panick21_ 10 hours agorootparentprevJoyent was using Solaris before Bryan worked there. Listen to the this podcast with Bryan and his co-founder about their origin story: https://www.youtube.com/watch?v=eVkIKm9pkPY This is about as good as you are gone get on the topic of Joyant history. reply codetrotter 10 hours agorootparentThank you, I will watch that right away :) reply panick21_ 10 hours agoparentprevBryan Cantrill also ported KVM to Illumos. At Joyent they had plenty of experience with KVM. See: https://www.youtube.com/watch?v=cwAfJywzk8o As far as I know, Bryan didn't personally work on the porting of bhyve (this might be wrong). So if anything, that would point to KVM as the 'familiar' thing given how many former Joyant people were there. reply bonzini 9 hours agorootparentKVM got more and more integrated with the rest of Linux as more virtualization features became general system features (e.g. posted interrupts). Also Google and Amazon are working more upstream and the pace of development increased a lot. Keeping a KVM port up to date is a huge effort compared to bhyve, and they probably had learnt that in the years between the porting of KVM and the founding of Oxide. reply elijahwright 7 hours agorootparentprevWhere is Max Bruning these days? reply mzi 11 hours agoparentprev@bcantrill is the CTO of Oxide. reply taspeotis 11 hours agorootparentYup, thanks reply gyre007 12 hours agoparentprevYeah I came here to say that Bryan worked at Sun so why do they even need to write this post (yes, I appreciate the techinical reasons, just wanted to highlight the fact via a subtle dig :-)) reply sausagefeet 12 hours agorootparentThis isn't a blog post from an Oxide, it's a link to their internal RFD which they use to make decisions. reply gyre007 5 hours agorootparentI never said it was a post by Oxide. reply rtpg 12 hours agoprev> There is not a significant difference in functionality between the illumos and FreeBSD implementations, since pulling patches downstream has not been a significant burden. Conversely, the more advanced OS primitives in illumos have resulted in certain bugs being fixed only there, having been difficult to upstream to FreeBSD. curious about what bugs are being thought of there. Sounds like a very interesting situation to be in reply transpute 11 hours agoprev> Xen: Large and complicated (by dom0) codebase, discarded for KVM by AMZN 1. Xen Type-1 hypervisor is smaller than KVM/QEMU. 2. Xen \"dom0\" = Linux/FreeBSD/OpenSolaris. KVM/bhyve also need host OS. 3. AMZN KVM-subset: x86 cpu/mem virt, blk/net via Arm Nitro hardware. 4. bhyve is Type-2. 5. Xen has Type-2 (uXen). 6. Xen dom0/host can be disaggregated (Hyperlaunch), unlike KVM. 7. pKVM (Arm/Android) is smaller than KVM/Xen. > The Service Management Facility (SMF) is responsible for the supervision of services under illumos.. a [Linux] robust infrastructure product would likely end up using few if any of the components provided by the systemd project, despite there now being something like a hundred of them. Instead, more traditional components would need to be revived, or thoroughly bespoke software would need to be developed, in order to avoid the technological and political issues with this increasingly dominant force in the Linux ecosystem. Is this an argument for Illumos over Linux, or for translating SMF to Linux? reply bonzini 11 hours agoparentTalking about \"technological and political issues\" without mentioning any, or without mentioning which components would need to be revived, sounds a lot like FUD unfortunately. Mixing and matching traditional and systemd components is super common, for example Fedora and RHEL use chrony instead of timesyncd, and NetworkManager instead of networkd. reply netbsdusers 3 hours agorootparent> Talking about \"technological and political issues\" without mentioning any I don't know why you think none were mentioned - to name one, they link a GitHub issue created against the systemd repository by a Googler complaining that systemd is inappropriately using Google's NTP servers, which at the time were not a public service, and kindly asking for systemd to stop using them. This request was refused and the issue was closed and locked. Behaviour like this from the systemd maintainers can only appear bizarre, childish, and unreasonable to any unprejudiced observer, putting their character and integrity into question and casting doubt on whether they should be trusted with the maintenance of software so integral to at least a reasonably large minority of modern Linux systems. reply packetlost 4 hours agorootparentprevThe Oxide folks are rather vocal about their distaste for the Linux Foundation. FWIW I think they went with the right choice for them considering they'd rather sign up for maintaining the entire thing themselves than saddling themselves with the baggage of a Linux fork or upstreaming reply actionfromafar 10 hours agorootparentprevI read it as \"we can sit in this more quiet room where people don't rave about systemd all day long\". reply bonzini 10 hours agorootparentBut do they? Oxide targets the enterprise, and people there don't care that much about how the underlying OS works. It's been ten years since a RHEL release started using systemd and there has been no exodus to either Windows or Illumos. I don't mean FUD in a disparaging sense, more like literal fear of the unknown causing people to be excessively cautious. I wouldn't have any problem with Oxide saying \"we went for what we know best\", there's no need to fake that so much more research went into a decision. reply panick21_ 10 hours agorootparentThe underlying hyperwiser on oxide isn't exposed to the consumers of the API. Just like on Amazon. I think arguably the bhyve over KVM was the more fundamental reason, and bhyve doesn't run on linux anyway. reply bonzini 9 hours agorootparentExactly, then why would they be dragged into systemd-or-not-systemd discussion? If you want to use Linux, use either Debian or the CentOS hyperscaler spin (the one that Meta uses) and call it a day. I am obviously biased as I am a KVM (and QEMU) developer myself, but I don't see any other plausible reason other than \"we know the Illumos userspace best\". Founder mode and all that. As to their choice of hypervisor, to be honest KVM on Illumos was probably not a great idea to begin with, therefore they used bhyve. reply jclulow 7 hours agorootparentFWIW, founder mode didn't exist five years ago when we were getting started! More seriously, though, this document (which I helped write) is an attempt specifically to avoid classic FUD tropes. It's not perfect, but it reflects certainly aspects of my lived experience in trying to get pieces of the Linux ecosystem to work in production settings. While it's true that I'm a dyed in the wool illumos person, being in the core team and so on, I have Linux desktops, and the occasional Linux system in lab environments. I have been supporting customers with all sorts of environments that I don't get to choose for most of my career, including Linux and Windows systems. At Joyent most of our customers were running hardware virtualised Linux and Windows guests, so it's not like I haven't had a fair amount of exposure. I've even spent several days getting SCO OpenServer to run under our KVM, for a customer, because I apparently make bad life choices! As for not discussing the social and political stuff in any depth, I felt at the time (and still do today) that so much ink had been split by all manner of folks talking about LKML or systemd project behaviour over the last decade that it was probably a distraction to do anything other than mention it in passing. As I believe I said in the podcast we did about this RFD recently: I'm not sure if this decision would be right for anybody else or not, but I believe it was and is right for us. I'm not trying to sell you, or anybody else, on making the same calls. This is just how we made our decision. reply bonzini 7 hours agorootparentFounder mode existed, it just didn't have a catchy name. And I absolutely believe that it was the right choice for your team, exactly for \"founder mode\" reasons. In other words, I don't think that the social or technological reasons in the document were that strong, and that's fine. Rather, my external armchair impression is simply that OS and hypervisor were not something where you were willing to spend precious \"risk points\", and that's the right thing to do given that you had a lot more places that were an absolute jump in the dark. reply InvaderFizz 6 hours agorootparentI would agree with that. Given the history of the Oxide team, they chose what they viewed was the best technology for THEM, as maintainers. The rest is mostly justification of that. That's just fine, as long as they're not choosing a clearly inferior long term option. The technically superior solution is not always the right solution for your organization given the priorities and capabilities of your team, and that's just fine! (I have no opinion on KVM vs bhyve, I don't know either deep enough to form one. I'm talking in general.) reply wmf 51 minutes agorootparentprevInstead people rave about Solaris. reply craftkiller 3 hours agoprevI wonder if CockroachDB abandoning the open source license[0] will have an impact on their choice to use it. It looks like the RFD was posted 1 day before the license switch[1], and the RFD has a section on licenses stating they intended to stick to the OSS build: > To mitigate all this, we’re intending to stick with the OSS build, which includes no CCL code. [0] https://news.ycombinator.com/item?id=41256222 [1] https://rfd.shared.oxide.computer/rfd/0110 reply implr 3 hours agoparentThey already have another RFD for this: https://rfd.shared.oxide.computer/rfd/0508 and on HN: https://news.ycombinator.com/item?id=41268043 reply alberth 6 hours agoprevIsn’t it simply Oxide founders are old Sun engineers, and Illumos is the open source spinoff of their old work. reply sophacles 5 hours agoparentAccording to the founders and early engineers on their podcast - no, they tried to fairly evaluate all the oses and were willing to go with other options. Practically speaking, its hard to do it completely objectively and the in-house expertise probably colored the decision. reply pclmulqdq 2 hours agorootparentTried to, sure, but when you evaluate other products strictly against the criteria under which you built your own version, you know what the conclusion will be. Never mind that you are carrying your blind spots with you. I would say that there was an attempt to evaluate other products, but not so much an attempt to be objective in that evaluation. In general, being on your own private tech island is a tough thing to do, but many engineers would rather do that than swallow their pride. reply daneel_w 8 hours agoprev\"• Emerging VMMs (OpenBSD’s vmm, etc): Haven’t been proven in production\" It's a small operation, but https://openbsd.amsterdam/ have absolutely proven that OpenBSD's hypervisor is production-capable in terms of stability - but there are indeed other problems that rule against it on scale. For those who are unfamiliar with OpenBSD: the primary caveat is that its hypervisor can so far only provide guests with a single CPU core. reply jclulow 7 hours agoparentYes, to be clear this is not meant to be a criticism of software quality at OpenBSD! Though I don't necessarily always agree with the leadership style I have big respect for their engineering efforts and obviously as another relatively niche UNIX I feel a certain kinship! That part of the document was also written some years ago, much closer to 2018 when that service got started than now, so it's conceivable that we wouldn't have said the same thing today. I will say, though, that single VCPU guests would not have met our immediate needs in the Oxide product! reply notaplumber1 1 hour agorootparent> I will say, though, that single VCPU guests would not have met our immediate needs in the Oxide product! Could Oxide not have helped push multi-vcpu guests out the door by sponsoring one of the main developers working on it, or contributing to development? From a secure design perspective, OpenBSD's vmd is a lot more appealing than bhyve is today. I saw recently that AMD SEV (Secure Encrypted Virtualization) was added, which seems compelling for Oxide's AMD based platform. Has Oxide added support for that to their bhyve fork yet? reply tonyg 8 hours agoprev> Nested virtualisation [...] challenging to emulate the underlying interfaces with flawless fidelity [...] dreadful performance It is so sad that we've ended up with designs where this is the case. There is no intrinsic reason why nested virtualization should be hard to implement or should perform poorly. Path dependence strikes again. reply bonzini 8 hours agoparentIt doesn't perform poorly in fact. It can be tuned at 90% of non-nested virtualization, and for workloads where it doesn't, that's more than anything else a testimony to how close virtualized performance is to bare-metal. That said, it does add a lot of complexity. reply mechanicker 2 hours agoprevWonder if this is more due to Bhyve being developed on FreeBSD and Illumos derives from a common ancestor BSD? I know NetApp (stack based on FreeBSD) contributed significantly to Bhyve when they were exploring options to virtualize Data ONTAP (C mode) https://forums.freebsd.org/threads/bhyve-the-freebsd-hypervi... reply jclulow 50 minutes agoparentWhile we have a common ancestor in the original UNIX, so much of illumos is really more from our SVR4 heritage -- but then also so much of that has been substantially reworked since then anyway. reply computersuck 2 hours agoprevBecause CTO Bryan Cantrill, who was a core contributor to illumos reply Aissen 5 hours agoprevPoint 1.1 about QEMU seems even less relevant today, with QEMU adding support for the microvm machines, hence greatly reducing the amount of exposed code. And as bonzini said in the thread, the recent vulnerability track record is not so bad. reply Rendello 21 hours agoprevI like the RFDs. Oxide just did a podcast episode on the process: https://oxide.computer/podcasts/oxide-and-friends/2065190 reply yellowapple 3 hours agoprevI'm surprised that KVM on Illumos wasn't in the running, especially with SmartOS setting that as precedent (even if bhyve is preferred nowadays). reply anonnon 11 minutes agoprevCtrl+f Cantrill >Phrase not found Bryan Cantrill, ex-Sun dev, ex-Joyent CTO, now CTO of Oxide, is the reason they chose Illumos. Oxide is primarily an attempt to give Solaris (albeit Rustified) a second life, similar to Joyent before. The company even cites Sun co-founder Scott McNealy for its principles: https://oxide.computer/principles >\"Kick butt, have fun, don't cheat, love our customers and change computing forever.\" >If this sounds familiar, it's because it's essentially Scott McNealy's coda for Sun Microsystems. reply kayo_20211030 6 hours agoprevIs the date on this piece correct? The section about Rust as a first class citizen seems to contain references to its potential use in Linux that are a few years out of date; with nothing more current than 2021. > As of March 2021, work on a prototype for writing Linux drivers in Rust is happening in the linux-next tree. reply kayo_20211030 6 hours agoparentnm, I read the postscript. The RFD was from 2021. I wonder how correct it was, and whether decisions made, based on it, were good ones or bad ones. reply blinkingled 9 hours agoprevI guess that's one way of keeping Solaris alive :) reply magicalhippo 4 hours agoprevBeen running Bhyve on FreeBSD (technically FreeNAS). Found PCIe pass-through of NMVe drives was fairly straight forward once the correct incantations were found, but network speed to host has been fairly abysmal. On my admittedly aging Threadripper 1920X, I can only get ~2-3 Gbps peak from a Linux guest. That's with virtio, the virtual intel \"card\" is even slower. They went with Illumos though, so curious if the poor performance is a FreeBSD-specific thing. reply ComputerGuru 1 hour agoparentI just spun up a VNET jail (so it should be essentially using the same network stack and networking isolation level as a bhyve guest would) and tested with iperf3 and without any tweaking or optimization and without even using jumbo frames I'm able to get 24+ Gbps with iperf3 (32k window size, tcp, single stream) between host/guest over the bridged and virtualized network interface. My test hardware is older than yours, it's a Xeon E5-1650 v3 and this is even with nested virtualization since the \"host\" is actually an ESXi guest running pf! But I think you might be right about something because, playing with it some more, I'm seeing an asymmetry in network I/O speeds; when I use `iperf3 -R` from the VNET jail to make the host connect to the guest and send data instead of the other way around, I get very inconsistent results with bursts of 2 Gbps traffic and then entire seconds without any data transferred (regardless of buffer size). I'd need to do a packet capture to figure out what is happening but it doesn't look like the default configuration performs very well at all! reply bitfilped 4 hours agoparentprevIt's been a minute since I messed with bhyve on FreeBSD, but I'm pretty sure you have to switch out the networking stack to something like Netgraph if you intend to use fast networking. reply craftkiller 3 hours agorootparentHmmm I'm not the OP, but I run my personal site on a kubernetes cluster hosted in bhyve VMs running Debian on a FreeBSD machine using netgraph for the networking. I just tested by launching iperf3 on the FreeBSD host and launching an alpine linux pod in the cluster, and I only got ~4Gbit/s. This is surprising to me since netgraph is supposed to be capable of much faster networking but I guess this is going through multiple additional layers that may have slowed it down (off the top of my head: kubernetes with flannel, iptables in the VM, bhyve, and pf on the FreeBSD host). reply BirAdam 7 hours agoprevI think a bigger reason for Oxide using Illumos is that many of the people over there are former Sun folks. reply saagarjha 10 hours agoprevUnrelated, but is this a homegrown publishing platform? reply crb 10 hours agoparentYes; it's referred to in Oxide's RFD about RFDs [1] https://rfd.shared.oxide.computer/rfd/0001 but the referenced URL is 404 unless you're an Oxide employee. [1] https://rfd.shared.oxide.computer/rfd/0001#_shared_rfd_rende... [2] https://github.com/oxidecomputer/rfd/blob/master/src reply dcre 5 hours agorootparentThat link is out of date. The site and backend are now open source. Only the repo containing the RFD contents is private. https://github.com/oxidecomputer/rfd-site https://github.com/oxidecomputer/rfd-api reply benjaminleonard 4 hours agoparentprevYep, you can see a little more on the [blog](https://oxide.computer/blog/a-tool-for-discussion) or the most recent [podcast](https://oxide.computer/podcasts/oxide-and-friends/2065190). The API and site repos are also public. reply jonstewart 6 hours agoprevIllumos makes sense as a host OS—it’s capable, they know it, they can make sure it works well on their hardware, and virtualization means users don’t need that much familiarity with it. If I were Oxide, though, I’d be sprinting to seamless VMWare support. Broadcom has turned into a modern-day Oracle (but dumber??) and many customers will migrate in the next two years. Even if those legacy VMs aren’t “hyperscale”, there’s going to be lots of budget devoted to moving off VMWare. reply parasubvert 4 hours agoparentOracle is a $53 billion company, and never had a mass exodus, just less greenfield deployments. Broadcom also isn't all that dumb, VMware was fat and lazy and customers were coddled for a very long time. They've made a bet that it's sticky. The competition isn't as weak as they thought, that's true, but it will take 5+ years to catch up, not 2 years, in general. Broadcom was betting on it taking 10 years: plenty of time to squeeze out margins. Customers have been trying and failing to eliminate the vTax since OpenStack. Red Hat and Microsoft are the main viable alternatives. reply throw0101b 7 hours agoprevSomewhat related, they discussed why they chose to use ZFS for their storage backend as opposed to (say) Ceph in a podcast episode: * https://www.youtube.com/watch?v=UvEKSqBBcZw Certainly they already had experience with ZFS (as it is built into Illumos/Solaris), but as it was told to them by someone they trusted who ran a lot of Ceph: \"Ceph is operated, not shipped [like ZFS]\". There's more care-and-feeding required for it, and they probably don't want that as they want to treat product in a more appliance/toaster-like fashion. reply pclmulqdq 3 hours agoparentCeph is sadly not very good at what it does. The big clouds have internal versions of object store that are far better (no single point of failure, much better error recovery story, etc.). ZFS solves a different problem, though. ZFS is a full-featured filesystem. Like Ceph it is also vulnerable to single points of failure. reply throw0101b 30 minutes agorootparent> The big clouds have internal versions of object store that are far better (no single point of failure, much better error recovery story, etc.). There are different levels of scalability needs. CERN has over a dozen (CEph) clusters with over 100PB of total data as of 2023: * https://www.youtube.com/watch?v=bl6H888k51w Certainly there are some number of folks that need more than that, but I don't there are many. > Like Ceph it is also vulnerable to single points of failure. The SPOF for ZFS is the host (unless you replicate, e.g., zfs send). What is SPOF of Ceph? You can have multiple monitors, managers, and MDSes. reply anonfordays 2 hours agoparentprevZFS and Ceph is apples to oranges. ZFS is scoped to a single host, Ceph can span data centers. reply ComputerGuru 1 hour agorootparentIt’s very possible to run a light/small layer on top of ZFS (either userspace daemon or via FUSE) to get you most of the way to scaling ZFS-backed object storage within or across data centers depending on what specific availability metrics you need. reply anonfordays 40 minutes agorootparentThat's true for any filesystem, not specific to ZFS. ZFS is not a clustered or multi-host filesystem. reply seabrookmx 36 minutes agorootparentprevWhat does this light/small layer look like? In my experience you need something like GlusterFS which I wouldn't call \"light\". reply throw0101b 29 minutes agorootparentprev> ZFS and Ceph is apples to oranges. Oxide is shipping an on-prem 'cloud appliance'. From the customer's/user's perspective of calling an API asking for storage, it does not matter what the backend is—apple or orange—as long as \"fruit\" (i.e., a logical bag of a certain size to hold bits) is the result that they get back. reply anonfordays 10 minutes agorootparentYes, it could be NTFS behind the scenes, but this is still an apples to oranges comparison because the storage service Oxide created is Crucible[0], not ZFS. Crucible is more of an apples to apples comparison with Ceph. [0] https://github.com/oxidecomputer/crucible reply wmf 1 hour agoparentprevYou mean they use Crucible instead of Ceph? reply leoh 7 hours agoprevI’d love to use Illumos, but a lack of arm64 support is a non-starter reply ComputerGuru 1 hour agoparentI don’t mean to downplay the importance for you personally but I do want to clarify that while it might be a non-starter for you, all of arm64 is so new that it’s hardly a non-starter for anyone considering putting it into (traditional) production. reply jclulow 7 hours agoparentprevFolks are working on it! I believe it boots on some small systems and under QEMU, but it's still relatively early days. I'm excited for the port to eventually make it into the gate, though! reply geerlingguy 2 hours agorootparentIn before someone asks about riscv64 ;) reply fefe23 8 hours agoprev [–] These sound like reason you retconned so it sounds like you didn't choose Illumos because your founder used to work at Sun and Joyent before. :-) Frankly I don't understand why they blogged that at all. It reeks of desperation, like they feel they need to defend their choice. They don't. It also should not matter to their customers. They get exposed APIs and don't have to care about the implementation details. reply jclulow 7 hours agoparent [–] It's not a blog post, it's an RFD. We have a strong focus on writing as part of thinking and making decisions, and when we can, we like to publish our decision making documents in the spirit of open source. This is not a defence of our position so much as a record of the process through which we arrived at it. This is true of our other RFDs as well, which you can see on the site there. > It also should not matter to their customers. They get exposed APIs and don't have to care about the implementation details. Yes, the whole product is definitely designed that way intentionally. Customers get abstracted control of compute and storage resources through cloud style APIs. From their perspective it's a cloud appliance. It's only from our perspective as the people building it that it's a UNIX system. reply stonogo 3 hours agorootparent [–] So at no point did anyone even suspect that Illumos was under consideration because it's been corporate leadership's pet project for decades? That seems like a wild thing to omit from the \"RFD\" process. Or were some topics not open to the \"RFD\" process? reply jclulow 38 minutes agorootparent [–] We are trying to build a business here. The goal is to sell racks and racks of computers to people, not build a menagerie of curiosities and fund personal projects. Everything we've written here is real, at least from our point of view. If we didn't think it would work, why would we throw our own business, and equity, and so on, away? The reason I continue to invest myself, if nothing else, in illumos, is because I genuinely believe it represents a better aggregate trade off for production work than the available alternatives. This document is an attempt to distill why that is, not an attempt to cover up a personal preference. I do have a personal preference, and I'm not shy about it -- but that preference is based on tangible experiences over twenty years! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The RFD 26 document explores the software stack for the host CPU in Oxide Rack servers, focusing on the operating system (OS) and virtual machine monitor (VMM).",
      "Key hypervisor choices include KVM on GNU/Linux and bhyve on illumos, with a preference for Rust-based userspace for safety and performance.",
      "The chosen platform is Helios (an illumos distribution) and Propolis (Rust-based userspace) with bhyve for guest workloads, emphasizing open-source development and robust server management facilities."
    ],
    "commentSummary": [
      "Oxide selected Illumos for their host OS and virtualization software due to its advanced OS features, familiarity, and a preference for the Rust programming language.",
      "Despite QEMU's reliability, Oxide chose bhyve over KVM, citing better fit and maintainability for their needs.",
      "The decision was driven by the team's extensive experience with Illumos and a desire to avoid the complexities associated with Linux and systemd, aiming for long-term sustainability and quality."
    ],
    "points": 181,
    "commentCount": 116,
    "retryCount": 0,
    "time": 1726089725
  },
  {
    "id": 41515730,
    "title": "Reader-LM: Small Language Models for Cleaning and Converting HTML to Markdown",
    "originLink": "https://jina.ai/news/reader-lm-small-language-models-for-cleaning-and-converting-html-to-markdown/?nocache=1",
    "originBody": "In April 2024, we released Jina Reader, a simple API that converts any URL into LLM-friendly markdown with just a simple prefix: r.jina.ai. Despite the sophisticated network programming behind the scenes, the core \"reading\" part is quite straightforward. First, we use a headless Chrome browser to fetch the source of the webpage. Then, we leverage Mozilla’s Readability package to extract the main content, removing elements like headers, footers, navigation bars, and sidebars. Finally, we convert the cleaned-up HTML into markdown using regex and the Turndown library. The result is a well-structured markdown file, ready to be used by LLMs for grounding, summarizing, and reasoning. In the first few weeks after the release of Jina Reader, we received a lot of feedback, particularly regarding the quality of the content. Some users found it too detailed, while others felt it wasn’t detailed enough. There were also reports that the Readability filter removed the wrong content or that Turndown struggled to convert certain parts of the HTML into markdown. Fortunately, many of these issues were successfully resolved by patching the existing pipeline with new regex patterns or heuristics. Since then, we’ve been pondering one question: instead of patching it with more heuristics and regex (which becomes increasingly difficult to maintain and isn’t multilingual friendly), can we solve this problem end-to-end with a language model? Illustration of reader-lm, replacing the pipeline of readability+turndown+regex heuristics using a small language model. At first glance, using LLMs for data cleaning might seem excessive due to their low cost-efficiency and slower speeds. But what if we're considering a small language model (SLM) — one with fewer than 1 billion parameters that can run efficiently on the edge? That sounds much more appealing, right? But is this truly feasible or just wishful thinking? According to the scaling law, fewer parameters generally lead to reduced reasoning and summarizing capabilities. So an SLM might even struggle to generate any meaningful content if its parameter size is too small. To explore this further, let’s take a closer look at the HTML-to-Markdown task: First, the task we’re considering isn’t as creative or complex as typical LLM tasks. In the case of converting HTML to markdown, the model primarily needs to selectively copy from the input to the output (i.e., skipping over HTML markup, sidebars, headers, footers), with minimal effort spent on generating new content (mostly inserting markdown syntax). This contrasts sharply with the broader tasks LLMs handle, such as generating poems or writing code, where the output involves much more creativity and is not a direct copy-paste from the input. This observation suggests that an SLM might work, as the task seems simpler than more general text generation. Second, we need to prioritize the long-context support. Modern HTML often contains much more noise than simplemarkup. Inline CSS and scripts can easily balloon the code to hundreds of thousands of tokens. For an SLM to be practical in this scenario, the context length must be sufficiently large. Token limits like 8K or 16K may not be useful at all. It seems that what we need is a shallow-but-wide SLM. \"Shallow\" in the sense that the task is primarily \"copy-paste\" and relatively straightforward, and \"wide\" in the sense that it requires long context support to be practical. However, previous research has shown that context length and reasoning ability are closely intertwined. For an SLM, it’s extremely challenging to optimize both dimensions while keeping the parameter size small. Today, we’re excited to announce the release of reader-lm-0.5b and reader-lm-1.5b, two SLMs specifically trained to generate clean markdown directly from noisy raw HTML. Both models are multilingual and support a context length of up to 256K tokens. Despite their compact size, these models achieve state-of-the-art performance on this task, outperforming larger LLM counterparts while being only 1/50th of their size. Below are the two models' specifications:reader-lm-0.5b reader-lm-1.5b # Parameters 494M 1.54B Context length 256K 256K Hidden Size 896 1536 # Layers 24 28 # Query Heads 14 12 # KV Heads 2 2 Head Size 64 128 Intermediate Size 4864 8960 Multilingual Yes Yes HuggingFace Repo Link Link Get Started with Reader-LM On Google Colab The easiest way to experience reader-lm is by running our Colab notebook, where we demonstrate how to use reader-lm-1.5b to convert the Hacker News website into markdown. The notebook is optimized to run smoothly on Google Colab’s free T4 GPU tier. You can also load reader-lm-0.5b or change the URL to any website and explore the output. Note that the input (i.e., the prompt) to the model is the raw HTML—no prefix instruction is required. Google Colab Please be aware that the free-tier T4 GPU comes with limitations that might prevent the use of advanced optimizations during model execution. Features such as bfloat16 and flash attention are not available on the T4, which could result in higher VRAM usage and slower performance for longer inputs. For production environments, we recommend using a higher-end GPU like the RTX 3090/4090 for significantly better performance. In Production: Available on Azure & AWS Soon Reader-LM will be available on Azure Marketplace and AWS SageMaker. If you need to use these models beyond those platforms or on-premises within your company, note that both models are licensed under CC BY-NC 4.0. For commercial usage inquiries, feel free to contact us. Quantitative Evaluation To quantitatively evaluate the performance of Reader-LM, we compared it with several large language models, including: GPT-4o, Gemini-1.5-Flash, Gemini-1.5-Pro, LLaMA-3.1-70B, Qwen2-7B-Instruct. The models were assessed using the following metrics: ROUGE-L (higher is better): This metric, widely used for summarization and question-answering tasks, measures the overlap between the predicted output and the reference at the n-gram level. Token Error Rate (TER, lower is better): This metric calculates the rate at which the generated markdown tokens do not appear in the original HTML content. We designed this metric to assess the model's hallucination rate, helping us identify cases where the model produces content that isn’t grounded in the HTML. Further improvements will be made based on case studies. Word Error Rate (WER, lower is better): Commonly used in OCR and ASR tasks, WER considers the word sequence and calculates errors such as insertions (ADD), substitutions (SUB), and deletions (DEL). This metric provides a detailed assessment of mismatches between the generated markdown and the expected output. To leverage LLMs for this task, we used the following uniform instruction as the prefix prompt: Your task is to convert the content of the provided HTML file into the corresponding markdown file. You need to convert the structure, elements, and attributes of the HTML into equivalent representations in markdown format, ensuring that no important information is lost. The output should strictly be in markdown format, without any additional explanations. The results can be found in the table below.ROUGE-L WER TER reader-lm-0.5b 0.56 3.28 0.34 reader-lm-1.5b 0.72 1.87 0.19 gpt-4o 0.43 5.88 0.50 gemini-1.5-flash 0.40 21.70 0.55 gemini-1.5-pro 0.42 3.16 0.48 llama-3.1-70b 0.40 9.87 0.50 Qwen2-7B-Instruct 0.23 2.45 0.70 Qualitative Study We conducted a qualitative study to visually evaluate the models on this HTML2Markdown task. We selected 22 HTML sources including news articles, blog posts, landing pages, e-commerce pages, and forum posts in multiple languages: English, German, Japanese, and Chinese. We also included the Jina Reader API as a baseline, which relies on regex, heuristics, and predefined rules. The evaluation focused on four key dimensions of the output, with each model rated on a scale from 1 (lowest) to 5 (highest): Header Extraction: Assessed how well each model identified and formatted the document’s h1,h2,..., h6 headers using correct markdown syntax. Main Content Extraction: Evaluated the models' ability to accurately convert body text, preserving paragraphs, formatting lists, and maintaining consistency in presentation. Rich Structure Preservation: Analyzed how effectively each model maintained the overall structure of the document, including headings, subheadings, bullet points, and ordered lists. Markdown Syntax Usage: Evaluated each model’s ability to correctly convert HTML elements such as(links),(bold text), and(italics) into their appropriate markdown equivalents. The results can be found below. Reader-LM-1.5B consistently performs well across all dimensions, particularly excelling in structure preservation and markdown syntax usage. While it doesn't always outperform Jina Reader API, its performance is competitive with larger models like Gemini 1.5 Pro, making it a highly efficient alternative to larger LLMs. Reader-LM-0.5B, though smaller, still offers solid performance, particularly in structure preservation. How We Trained Reader-LM Data Preparation We used the Jina Reader API to generate training pairs of raw HTML and their corresponding markdown. During the experiment, we found that SLMs are particularly sensitive to the quality of the training data. So we built a data pipeline that ensures only high-quality markdown entries are included in the training set. Additionally, we added some synthetic HTML and their markdown counterparts, generated by GPT-4o. Compared to real-world HTML, synthetic data tends to be much shorter, with simpler and more predictable structures, and a significantly lower noise level. Finally, we concatenated the HTML and markdown using a chat template. The final training data is formatted as follows: system You are a helpful assistant. user {{RAW_HTML}} assistant {{MARKDOWN}} The full training data amounts to 2.5 billion tokens. Two-Stage Training We experimented with various model sizes, starting from 65M and 135M, up to 3B parameters. The specifications for each model can be found in the table below.reader-lm-65m reader-lm-135m reader-lm-360m reader-lm-0.5b reader-lm-1.5b reader-lm-1.7b reader-lm-3b Hidden Size 512 576 960 896 1536 2048 3072 # Layers 8 30 32 24 28 24 32 # Query Heads 16 9 15 14 12 32 32 # KV Heads 8 3 5 2 2 32 32 Head Size 32 64 64 64 128 64 96 Intermediate Size 2048 1536 2560 4864 8960 8192 8192 Attention Bias False False False True True False False Embedding Tying False True True True True True False Vocabulary Size 32768 49152 49152 151646 151646 49152 32064 Base Model Lite-Oute-1-65M-Instruct SmolLM-135M SmolLM-360M-Instruct Qwen2-0.5B-Instruct Qwen2-1.5B-Instruct SmolLM-1.7B Phi-3-mini-128k-instruct The model training was conducted in two stages: Short-and-simple HTML: In this stage, the maximum sequence length (HTML + markdown) was set to 32K tokens, with a total of 1.5 billion training tokens. Long-and-hard HTML: the sequence length was extended to 128K tokens, with 1.2 billion training tokens. We implemented the zigzag-ring-attention mechanism from Zilin Zhu's \"Ring Flash Attention\" (2024) for this stage. Since the training data included sequences of up to 128K tokens, we believe that the model can support up to 256K tokens without issue. However, handling 512K tokens may be challenging, as extending RoPE positional embeddings to four times the training sequence length could result in performance degradation. For the 65M and 135M parameter models, we observed that they could achieve reasonable \"copy\" behavior, but only with short sequences (fewer than 1K tokens). As the input length increased, these models struggled to produce any reasonable output. Given that modern HTML source code can easily exceed 100K tokens, a 1K token limit is far from sufficient. Degeneration and Dull Loops One of the major challenges we encountered was degeneration, particularly in the form of repetition and looping. After generating some tokens, the model would begin to generate the same token repeatedly or get stuck in a loop, continuously repeating a short sequence of tokens until reaching the maximum allowed output length. Dull loops pointed by the red arrows represent serious degeneration of SLM. To address this issue: We applied contrastive search as a decoding method and incorporate contrastive loss during training. From our experiments, this method effectively reduced repetitive generation in practice. We implemented a simple repetition stop criterion within the transformer pipeline. This criterion automatically detects when the model begins to repeat tokens and stops decoding early to avoid dull loops. This idea was inspired by this HuggingFace discussion. Training Efficiency on Long Inputs To mitigate the risk of out-of-memory (OOM) errors when handling long input, we implemented chunk-wise model forwarding. This approach encodes the long input with smaller chunks, reducing VRAM usage. We improved the data packing implementation in our training framework, which is based on the Transformers Trainer. To optimize training efficiency, multiple short texts (e.g., 2K tokens) are concatenated into a single long sequence (e.g., 30K tokens), enabling padding-free training. However, in the original implementation, some short examples were split into two sub-texts and included in different long training sequences. In such cases, the second sub-text would lose its context (e.g., raw HTML content in our case), leading to corrupted training data. This forces the model to rely on its parameters rather than the input context, which we believe is a major source of hallucination. In the end, we selected the 0.5B and 1.5B models for publication. The 0.5B model is the smallest one capable of achieving the desired \"selective-copy\" behavior on long-context inputs, while the 1.5B model is the smallest larger model that significantly improves performance without hitting diminishing returns in relation to parameter size. Alternative Architecture: Encoder-Only Model In the early stages of this project, we explored using an encoder-only architecture to tackle this task. As mentioned earlier, the HTML-to-Markdown conversion task appears to be primarily a \"selective-copy\" task. Given a training pair (raw HTML and markdown), we can label tokens that exist in both the input and output as 1, and the rest as 0. This converts the problem into a token classification task, similar to what is used in Named Entity Recognition (NER). While this approach seemed logical, it presented significant challenges in practice. First, raw HTML from real-world sources is extremely noisy and long, making the 1 labels extremely sparse hence difficult for the model to learn. Second, encoding special markdown syntax in a 0-1 schema proved problematic, as symbols like ## title, *bold*, andtabledo not exist in the raw HTML input. Third, the output tokens do not always strictly follow the order of the input. Minor reordering often occurs, particularly with tables and links, making it difficult to represent such reordering behaviors in a simple 0-1 schema. Short-distance reordering could potentially be handled with dynamic programming or alignment-warping algorithms by introducing labels like -1, -2, +1, +2 to represent distance offsets, transforming the binary classification problem into a multi-class token classification task. Using dynamic programming to align the raw HTML (X-axis) and the markdown (Y-axis) for creating token-level training labels. In summary, solving the problem with an encoder-only architecture and treating it as a token classification task has its charm, especially since the training sequences are much shorter compared to a decoder-only model, making it more VRAM-friendly. However, the major challenge lies in constructing the training data. When we realized that the time and effort spent preparing the training data—using dynamic programming and heuristics to create perfect token-level labeling sequences—was significant, we decided to discontinue this approach. Conclusion Reader-LM is a novel small language model (SLM) designed for data extraction and cleaning on the open web. Inspired by Jina Reader, our goal was to create an end-to-end language model solution capable of converting raw, noisy HTML into clean markdown. At the same time, we focused on cost-efficiency, keeping the model size small to ensure Reader-LM remains practical and usable. It is also the first decoder-only long-context model trained at Jina AI. Although the task may initially appear to be a simple \"selective-copy\" problem, converting and cleaning HTML to markdown is far from easy. Specifically, it requires the model to excel at position-aware, context-based reasoning, which demands a larger parameter size, particularly in the hidden layers. In comparison, learning markdown syntax is relatively straightforward. During our experiments, we also found that training an SLM from scratch is particularly challenging. Starting with a pretrained model and continuing with task-specific training significantly improved training efficiency. There's still much room for improvement in terms of both efficiency and quality: expanding the context length, speeding up decoding, and adding support for instructions in the input, which would allow Reader-LM to extract specific parts of a webpage into markdown.",
    "commentLink": "https://news.ycombinator.com/item?id=41515730",
    "commentBody": "Reader-LM: Small Language Models for Cleaning and Converting HTML to Markdown (jina.ai)174 points by matteogauthier 20 hours agohidepastfavorite39 comments MantisShrimp90 1 hour agoI never really understand this reasoning of \"regex is hard to reason about, so we just use an LLM we custom made instead!\" I get it's trendy but reasoning about LLMs is impossible for many devs the idea that this makes it more maintainable is pretty hilarious. reply nickpsecurity 1 hour agoparentRegex’s require you to understand what the obscure-looking patterns do character by character in a pile of text. Then, across different piles of text. Then, juggling different regex’s. For a LLM, you can just tune it to produce the right output using examples. Your brain doesn’t have to understand the tedious things it’s doing. This also replaces a boring, tedious job with one (LLM’s) that’s more interesting. Programmers enjoy those opportunities. reply generalizations 53 minutes agorootparentIn either case you end up with an inscrutable black box into which you pass your html...honestly I'd prefer the black box that runs more efficiently and is intelligible to at least some people (or most, with the help of a big LLM). reply sippeangelo 8 hours agoprevFor as much as I would love for this to work, I'm not getting great results trying out the 1.5b model in their example notebook on Colab. It is impressively fast, but testing it on an arxiv.org page (specifically https://arxiv.org/abs/2306.03872) only gives me a short markdown file containing the abstract, the \"View PDF\" link and the submission history. It completely leaves out the title (!), authors and other links, which are definitely present in the HTML in multiple places! I'd argue that Arxiv.org is a reasonable example in the age of webapps, so what gives? reply faangguyindia 6 hours agoparentQuestion is why even use these small models? When you've Google Flash which is lightening fast and cheap. My brother implemented it in option-k : https://github.com/zerocorebeta/Option-K It's near instant. So why waste time on small models? It's going to cost more than Google flash. reply oezi 5 hours agorootparentWhat is Google Flash? Do you mean Gemini Flash? If so, then the article talks about that general purpose LLMs are worse than this specialized LLM for Markdown conversion. reply sippeangelo 4 hours agorootparentIn this case it is not, though. As much as I'd like a self-hostable, cheap and lean model for this specific task, instead we have a completely inflexible model that I can't just prompt tweak to behave better in even not-so-special cases like above. I'm sure there are good examples of specialised LLMs that do work well (like ones that are trained on specific sciences), but here the model doesn't have enough language comprehension to understand plain English instructions. How do I tweak it without fine-tuning? With a traditional approach to scraping this is trivial, but here it's unfeasible to the end user. reply dartos 5 hours agorootparentprevSometimes you don’t want to share all your data with the largest corporations on the planet. reply randomdata 4 hours agorootparentprevSmall models often do a much better job when you have a well-defined task. reply FL33TW00D 5 hours agorootparentprevPrivacy, Cost, Latency, Connectivity. reply choeger 14 hours agoprevMaybe I am missing something here, but why would you run \"AI\" on that task when you go from formal language to formal language? I don't get the usage of \"regex/heuristics\" either. Why can that task not be completely handled by a classical algorithm? Is it about the removal of non-content parts? reply baq 12 hours agoparentThere’s html and then there’s… html. A nicely formatted subset of html is very different from a dom tag soup that is more or less the default nowadays. reply JimDabell 10 hours agorootparentTag soup hasn’t been a problem for years. The HTML 5 specification goes into a lot more detail than previous specifications when it comes to parsing malformed markup and browsers follow it. So no matter the quality of the markup, if you throw it at any HTML 5 implementation, you will get the same consistent, unambiguous DOM structure. reply mithametacs 4 hours agorootparentyeah, you could just pull the parser out of any open source browser and voila a parser not only battle-tested, but probably the one the page was developed against reply faangguyindia 6 hours agorootparentprevThat's why the best strategy is to feed the whole page into LLM. (After removing html tags) and just ask LLM to give you the date you need in the format you need. If there is lots of javascript dom manipulation happening after pageload. Then just render in webdriver and screenshot, ocr and feed the result into LLM and ask it the right questions. reply mithametacs 4 hours agorootparentMy intuition is that you’d get better results emptying the tags or replacing them with some other delimiter. Keep the structural hint, remove the noise. reply nickpsecurity 13 hours agoparentprevIt’s informal language that has formal language mixed in. The informal parts determine how the final document should look. So, a simple formal-to-formal translation won’t meet their needs. reply vladde 6 hours agoprevUnfortunately not getting any good results for RFC 3339 (https://www.rfc-editor.org/rfc/rfc3339), such a page where I think it would be great to convert text into readable Markdown. The end result is just like the original site but with without any headings and the a lot of whitespace still remaining (but with some non-working links inserted) :/ Using thei API link, this is what it looks like: https://r.jina.ai/https://www.rfc-editor.org/rfc/rfc3339 reply bberenberg 5 hours agoparentTested it using the model in Google Colab and it did ok, but the output is truncated at the following line: > [Appendix B](#appendix-B). Day So not sure if it's the length of the page, or something else, but in the end, it doesn't really work? reply lelandfe 5 hours agoparentprevThat's their existing API (which I also tried, with... less than desirable results). This post is about a new model, `reader-lm`, which isn't in production yet. reply rwl4 2 hours agoprevNot sure about the quality of the model's output. But I really appreciate this little mini-paper they produced. It gives a nice concise description of their goals, benchmarks, dataset preparation, model sizes, challenges and conclusion. And the whole thing is about a 5-10 minute read. reply tatsuya4 8 hours agoprevIn real-world use cases, it seems more appropriate to use advanced models to generate suitable rule trees or regular expressions for processing HTML → Markdown, rather than directly using a smaller model to handle each HTML instance. The reasons for this approach include: 1. The quality of HTML → Markdown conversion results is easier to evaluate. 2. The HTML → Markdown process is essentially a more sophisticated form of copy-and-paste, where AI generates specific symbols (such as ##, *) rather than content. 3. Rule-based systems are significantly more cost-effective and faster than running an LLM, making them applicable to a wider range of scenarios. These are just my assumptions and judgments. If you have practical experience, I'd welcome your insights. reply smusamashah 7 hours agoprevAs per reddit their API that converts html to markdown can be used by appending url to https://r.jina.ai like https://r.jina.ai/https://news.ycombinator.com/item?id=41515... I don't know if its using their new model or their engine reply faangguyindia 10 hours agoprevMy brother made: https://github.com/zerocorebeta/Option-K Basically, it's utility which completes commandline for you While playing with it, we thought about creating a custom small model for this. But it was really limiting! If we use small model trained on MAN pages, bash scripts, stack overflow and forums etc... We miss the key component, using a larger model like flash is more effective as this model knows lot more about other things. For example, I can ask this model to simply generate a command that lets me download audio from a youtube url. reply fsndz 12 hours agoprevI can say that enough. Small Language Models are the future. https://www.lycee.ai/blog/why-small-language-models-are-the-... reply Diti 10 hours agoparentAn aligned future, for sure. Current commercial LLMs refuse to talk about “keeping secrets” (protection of identity) or pornographic topics (which, in the communities I frequent – made of individuals who have been oppressed partly because of their sexuality –, is an important subject). And uncensored AIs are not really a solution either. reply igorzij 8 hours agoprevWhy Claude 3.5 Sonnet is missing from the benchmark? Even if the real reason is different and completely legitimate, or perhaps purely random, it comes across as \"claude does better than our new model so we omitted it because we wanted the tallest bars on the chart to be ours\". And as soon as the reader thinks that, they may start to question everything else in your work, which is genuinely awesome! reply faangguyindia 6 hours agoparentIt's damn slow and overkill for such task. reply valstu 12 hours agoprevSo regex version still beats the LLM solution. There's also the risk of hallucinations. I wonder if they tried to make SML which would rewrite or update the existing regex solution instead of generating the whole content again? This would mean less output tokens, faster inference and output wouldn't contain hallucinations. Although, not sure if small language models are capabable to write regex reply rockstarflo 8 hours agoparentI think regex can beat SLM for a specific use case. But for the general case, there is no chance you come up with a pattern that works for all sites. reply siscia 10 hours agoprevThe more I think about the less I am completely against this approach. Instead of applying an obscure set of heuristic by hand, let the LM figure out the best way starting from a lot of data. The model is bound to be less debuggable and much more difficult to update, for experts. But in the general case it will work well enough. reply denidoman 2 hours agoprevnext step: websites add irrelevant text and prompt injections into hidden dom nodes, tags attributes, etc. to prevent llm-based scraping. reply alexdoesstuff 11 hours agoprevFeels surprising that there isn't a modern best-in-class non-LLM alternative for this task. Even in the post, they described that they used a hodgepodge of headless Chrome, readability, lots of regex to create content-only HTML. Best I can tell, everyone is doing something similar, only differing in the amount of custom situation regex being used. reply monacobolid 10 hours agoparentHow could it possibly be (a better solution) when there are X different ways to do any single thing in html(/css/js)? If you have a website that uses a canvas to showcase the content (think presentation or something like that), where would you even start? People are still discussing whether the semantic web is important; not every page is utf8 encoded, etc. IMHO small LLMS (trained specifically for this) combined with some other (more predictable) techniques are the best solution we are going to get. reply alexdoesstuff 7 hours agorootparentFully agree on the premise: there are X different ways to do anything on the web. But - prior to this - the solution seemed to be: everyone starts from scratch with some ad-hoc Regex, and plays a game of whackamole to cover the first n of the x different ways to do things. Best of my knowledge there isn't anything more modern than Mozilla's readability and that's essentially a tool from the early 2010s. reply foul 11 hours agoprevWhen does this SML perform better than hxdelete (or xmlstarlet or whatever) + rdrview + pandoc? reply fsiefken 10 hours agoparentThe answer is in the OP's Reader-LM report: About their readability-markdown pipeline: \"Some users found it too detailed, while others felt it wasn’t detailed enough. There were also reports that the Readability filter removed the wrong content or that Turndown struggled to convert certain parts of the HTML into markdown. Fortunately, many of these issues were successfully resolved by patching the existing pipeline with new regex patterns or heuristics.\" To answer their question about the potention of a SML doing this, they see 'room for improvement' - but as their benchmark shows, it's not up to their classic pipeline. You echo their research question: \"instead of patching it with more heuristics and regex (which becomes increasingly difficult to maintain and isn’t multilingual friendly), can we solve this problem end-to-end with a language model?\" reply Dowwie 5 hours agoprevI'm curious about the dataset. What scenarios need to be covered during training? reply WesolyKubeczek 2 hours agoprev [–] What ever happened to parsing HTML with regexes that you need a beefy GPU/CPU/NPU now to convert HTML to Markdown? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "In April 2024, Jina Reader was launched as an API to convert URLs into LLM-friendly markdown using the prefix r.jina.ai, leveraging a headless Chrome browser, Mozilla’s Readability, and Turndown.",
      "New small language models, reader-lm-0.5b and reader-lm-1.5b, were released, supporting up to 256K tokens and multilingual capabilities, outperforming larger LLMs in HTML-to-markdown conversion.",
      "Reader-LM models demonstrated superior performance in evaluations using metrics like ROUGE-L, Token Error Rate (TER), and Word Error Rate (WER), and excelled in structure preservation and markdown syntax usage."
    ],
    "commentSummary": [
      "Reader-LM is a new small language model designed for converting HTML to Markdown, developed by Jina AI.",
      "The model has received mixed reviews, with some users noting it missed key elements on certain web pages and others questioning its efficiency compared to traditional methods like regex.",
      "The model is not yet in production, and there are ongoing discussions about its performance, privacy considerations, and the potential benefits of using specialized LLMs for specific tasks."
    ],
    "points": 174,
    "commentCount": 39,
    "retryCount": 0,
    "time": 1726092456
  },
  {
    "id": 41514727,
    "title": "Mistral releases Pixtral 12B, its first multimodal model",
    "originLink": "https://techcrunch.com/2024/09/11/mistral-releases-pixtral-its-first-multimodal-model/",
    "originBody": "Login Search Search Startups Venture Apple Security AI Apps Events Startup Battlefield More Close Submenu Fintech Cloud Computing Layoffs Hardware Google Microsoft Transportation EVs Meta Instagram Amazon TikTok Newsletters Podcasts Partner Content Crunchboard Jobs Contact Us AI Mistral releases Pixtral 12B, its first multimodal model Kyle Wiggers 4:40 AM PDT • September 11, 2024 Comment Image Credits: Rafael Henrique/SOPA Images/LightRocket / Getty Images French AI startup Mistral has released its first model that can process images as well as text. Called Pixtral 12B, the 12-billion-parameter model is about 24GB in size. Parameters roughly correspond to a model’s problem-solving skills, and models with more parameters generally perform better than those with fewer parameters. Built on one of Mistral’s text models, Nemo 12B, the new model can answer questions about an arbitrary number of images of an arbitrary size given either URLs or images encoded using base64, the binary-to-text encoding scheme. Similar to other multimodal models such as Anthropic’s Claude family and OpenAI’s GPT-4o, Pixtral 12B should — at least in theory — be able to perform tasks like captioning images and counting the number of objects in a photo. Available via a torrent link on GitHub and AI and machine learning development platform Hugging Face, Pixtral 12B can be downloaded, fine-tuned and used under an Apache 2.0 license without restrictions. (A Mistral spokesperson confirmed the license being applied to Pixtral 12B via email.) This writer wasn’t able to take Pixtral 12B for a spin, unfortunately — there weren’t any working web demos at the time of publication. In a post on X, Sophia Yang, head of Mistral developer relations, said Pixtral 12B will be available for testing on Mistral’s chatbot and API-serving platforms, Le Chat and Le Plateforme, soon. It’s unclear which image data Mistral might have used to develop Pixtral 12B. Most generative AI models, including Mistral’s other models, are trained on vast quantities of public data from around the web, which is often copyrighted. Some model vendors argue that “fair use” rights entitle them to scrape any public data, but many copyright holders disagree, and have filed lawsuits against larger vendors like OpenAI and Midjourney to put a stop to the practice. Pixtral 12B comes in the wake of Mistral closing a $645 million funding round led by General Catalyst that valued the company at $6 billion. Just over a year old, Mistral — minority owned by Microsoft — is seen by many in the AI community as Europe’s answer to OpenAI. The younger company’s strategy thus far has involved releasing free “open” models, charging for managed versions of those models, and providing consulting services to corporate customers. Updated 9/11 at 8:11 a.m. Pacific: Clarified that Pixtral 12B is being made available under an Apache 2.0 license, not Mistral’s standard dev license that carries with it certain restrictions on commercial usage. More TechCrunch Get the industry’s biggest tech news Explore all newsletters TechCrunch Daily News Every weekday and Sunday, you can get the best of TechCrunch’s coverage. Add TechCrunch Daily News to your subscription choices Startups Weekly Startups are the core of TechCrunch, so get our best coverage delivered weekly. Add Startups Weekly to your subscription choices TechCrunch Fintech The latest Fintech news and analysis, delivered every Tuesday. Add TechCrunch Fintech to your subscription choices TechCrunch Mobility TechCrunch Mobility is your destination for transportation news and insight. Add TechCrunch Mobility to your subscription choices No newsletters selected No newsletters Email address (required) Subscribe By submitting your email, you agree to our Terms and Privacy Notice. Tags AI, AI, Generative AI, mistral, multimodal, open, Pixtral, Pixtral 12B Climate This startup is making manure out of other biogas power plants, and now has $62M to play with Mike Butcher 45 seconds ago Working away on his PhD in Munich only a few years ago, Stephan Herrmann (now a Dr.) couldn’t have conceived of a time when his idea for a carbon-negative power… Transportation Faraday Future gives CEO and founder raises and bonuses after delivering 13 cars Sean O'Kane 25 mins ago Faraday Future is doling out big raises and bonuses to its CEO as well its founder, despite having delivered just 13 cars in its 10-year history, and recently laying off… TechCrunch Disrupt 2024 Announcing the final agenda for the Space Stage at TechCrunch Disrupt 2024 Aria Alamalhodaei Devin Coldewey Richard Smith 26 mins ago We’re out-of-this-world excited to announce that we’ve finalized our dedicated Space Stage at TechCrunch Disrupt 2024. It joins Fintech, SaaS and AI as the other industry-focused stages — all under… Fintech Bolt has quietly settled its lawsuit with Fanatics amid ongoing boardroom drama Julie Bort Mary Ann Azevedo 41 mins ago Online sports apparel retailer Fanatics has agreed to settle and drop a lawsuit that it filed against troubled one-click payments provider Bolt in March, according to court documents obtained by… Startups Why Y Combinator companies are flocking to banking and HR startup Every Julie Bort 1 hour ago Rajeev Behera’s new all-on-one HR startup, dubbed Every, is either brilliant or crazy. Social Threads makes it easier to evangelize the open social web with a new direct link feature Sarah Perez 2 hours ago It’s a small advance, but one that speaks to Meta’s enginerring team paying attention to how the fediverse community is trying to educate Threads users about the possibilities. Transportation Autonomous delivery startup Nuro pivots and another Indian EV scooter startup takes the IPO road Kirsten Korosec 2 hours ago Welcome back to TechCrunch Mobility — your central hub for news and insights on the future of transportation. Sign up here for free — just click TechCrunch Mobility! The transportation… AI OpenAI unveils o1, a model that can fact-check itself Kyle Wiggers 2 hours ago ChatGPT maker OpenAI has announced its next major product release: A generative AI model code-named Strawberry, officially called OpenAI o1. To be more precise, o1 is actually a family of… Government & Policy Australian plan for misinformation law riles Elon Musk Natasha Lomas 3 hours ago The Australian government wants to fine social media platforms up to 5% of their global revenue if they fail to stop the spread of misinformation under a revised legislative plan… Commerce Amazon and Flipkart violated competition laws in India, report says Manish Singh 3 hours ago An Indian antitrust regulator has found that Amazon and Flipkart, owned by Walmart, violated local competition laws, according to a report from Reuters. The finding presents a new challenge for… Media & Entertainment Tune.FM wants to take on Spotify by using crypto to pay artists up to 100x more per stream Rebecca Szkutak 3 hours ago Tune.FM is a decentralized music streaming service where users pay for each song they stream using Tune.FM’s crypto token JAM. Robotics Google DeepMind teaches a robot to autonomously tie its shoes and fix fellow robots Brian Heater 3 hours ago DeepMind employed a new learning platform, ALOHA Unleashed, paired with its simulation program, DemoStart, to teach robots by watching humans. Hardware Apple AirPods Pro granted FDA approval to serve as hearing aids Brian Heater 3 hours ago The FDA on Thursday announced that it has granted what it calls “the first over-the-counter (OTC) hearing aid software device, Hearing Aid Feature.” Apps Google Wallet to test a feature that turns your US passport into a digital ID Aisha Malik 4 hours ago Google announced on Thursday that it’s introducing new Wallet updates for travelers and commuters. Most notably, Google Wallet will soon start beta testing the ability to create a Digital ID… AI White House extracts voluntary commitments from AI vendors to combat deepfake nudes Kyle Wiggers 4 hours ago The White House says several major AI vendors have committed to taking steps to combat nonconsensual deepfakes and child sexual abuse material. Adobe, Cohere, Microsoft, Anthropic OpenAI, and data provider… Gaming Microsoft lays off another 650 from gaming division Aisha Malik 5 hours ago Microsoft is laying off around 650 employees from its gaming division, according to an internal memo shared online by IGN. The latest cuts come eight months after the company laid… Featured Article Hacker tricks ChatGPT into giving out detailed instructions for making homemade bombs An explosives expert told TechCrunch that the ChatGPT output could be used to make a detonatable product and was too sensitive to be released. Lorenzo Franceschi-Bicchierai 5 hours ago Social Meta, TikTok, and Snap pledge to participate in program to combat suicide and self-harm content Kyle Wiggers 5 hours ago In an attempt to prevent suicide and self-harm content from spreading online, the nonprofit Mental Health Coalition (MHC) today announced a new program, Thrive, aimed at encouraging online platforms to… Crypto Bitcoin and NFTs may get greater legal protections as ‘personal property’ under proposed UK law Paul Sawers 6 hours ago The U.K. government has introduced a new bill to Parliament which proposes new legal protections for digital assets such as Bitcoin. Social X is working on a new way for people to block DMs Ivan Mehta 6 hours ago Elon Musk’s social network X is exploring a new feature that would allow users to block others from direct messaging them, but in a way that’s separate from the account… Robotics Face to face with Figure’s new humanoid robot Brian Heater 6 hours ago Figure is testing Figure 02’s efficacy for helping out in the kitchen and picking up around the house. Fundraising After using a business coach to shift careers, AceUp founder wants to drive coaching based on data Kyle Wiggers 6 hours ago Is a business coach really worth the investment? Execs often seek coaches to bolster aspects of their work, like communication skills and their productivity. At least anecdotally, these skills do… Startups Humanz brings its influencer marketing platform to the US Lauren Forristal 6 hours ago Humanz, a marketing platform for content creators and brands, has entered the U.S. market, the company announced on Thursday. Having launched in Israel in 2017, Humanz has gained strong traction… Hardware iFixit marks iPhone 16 arrival with battery-powered soldering iron launch Brian Heater 6 hours ago iFixit, everyone’s favorite gadget repair gadfly, is launching a portable soldering iron. The gadget is designed to make component repair more accessible for home users. The timing of the announcement… Government & Policy Irish Big Tech watchdog digs into platforms’ content reporting mechanisms after DSA complaints Natasha Lomas 7 hours ago Ireland’s media regulator said it is reviewing how major platforms let users report illegal content, following a high number of complaints. Fundraising Novatus nabs $40M to help financial institutions quell their regtech nightmares Ingrid Lunden 7 hours ago Novatus helps financial companies manage their data for risk and compliance data, and it has now raised $40M to expand into new markets. AI OffDeal wants to help small businesses find big exits with AI agents Maxwell Zeff 7 hours ago Small businesses are the unsung heroes of the American economy, employing nearly half of America’s workforce and making up 44% of the country’s GDP. But when it’s time for small… Privacy Google’s GenAI facing privacy risk assessment scrutiny in Europe Natasha Lomas 10 hours ago Google’s lead privacy regulator in the European Union has opened an investigation into whether it has complied with the bloc’s data protection laws in relation to use of people’s information… Gaming Consumer group sues to ban purchases inside games like Fortnite and Minecraft in Europe Ingrid Lunden 10 hours ago Video games are some of the most lucrative apps in the world, in part because of how they lure people into spending money on credits to buy digital goodies, to… Apps WhatsApp brings Meta Verified, customized messages to small businesses in India Jagmeet Singh Ivan Mehta 11 hours ago WhatsApp is now letting small businesses in India sign up for a Meta Verified badge and giving them the ability to send customized messages to customers. About TechCrunch Staff Contact Us Advertise Crunchboard Jobs Site Map Legal Terms of Service Privacy Policy RSS Terms of Use Privacy Placeholder 1 Privacy Placeholder 2 Privacy Placeholder 3 Privacy Placeholder 4 Code of Conduct About Our Ads Trending Tech Topics OpenAI o1 Apple AirPods Pro Figure Robot Mark Zuckerberg Y Combinator Tech Layoffs ChatGPT Facebook X YouTube Instagram LinkedIn Mastodon Threads © 2024 Yahoo. All rights reserved. Powered by WordPress VIP",
    "commentLink": "https://news.ycombinator.com/item?id=41514727",
    "commentBody": "Mistral releases Pixtral 12B, its first multimodal model (techcrunch.com)162 points by jerbear4328 23 hours agohidepastfavorite39 comments buran77 21 hours agoThe \"Mistral Pixtral multimodal model\" really rolls off the tongue. > It’s unclear which image data Mistral might have used to develop Pixtral 12B. The days of free web scraping especially for the richer sources of material are almost gone, with anything between technical (API restrictions) and legal (copyright) measures building deep moats. I also wonder what they trained it on. They're not Meta or Google with endless supplies of user content, or exclusive contracts with the Reddits of the internet. reply simonw 20 hours agoparentWhat do you mean by copyright measures? Has anything changed on that front in the last two years? My hunch is that most AI labs are already sitting on a pretty sizable collection of scraped image data - and that data from two years ago will be almost as effective as data scraped today, at least as far as image training goes. reply dartos 20 hours agorootparentThe issue with image models is that their style becomes identifiable and stale quite quickly, so you’ll need a fresh intake of different, newer, styles every so often and that’s going to be harder and harder to get. reply GaggiX 19 hours agorootparentThe style becoming identifiable and stale has mostly to do with CFG and almost nothing with the dataset, the heavy use of CFG by most models trades diversity with coherency. You don't need a costant intake of new images and styles, it's like saying that an image created two years ago is stale because it doesn't follow a new style or something. Also Pixtral is not a text-to-image model. reply p0rkbelly 6 hours agorootparentThere is the problem of literal style though. The aesthetics of say clothes do evolve overtime, not year to year big changes, but every 3-5? Sure. Just laughing at the thought of the model where any image generated is say stuck in 1990s grunge attire. reply esafak 19 hours agorootparentprevCFG for Classifier-Free Guidance? reply GaggiX 19 hours agorootparentExactly, https://arxiv.org/abs/2207.12598 Jonathan Ho, one of the authors of the CFG paper, now works for Ideogram, and Ideogram 2 is one of the very few models (or perhaps the only one) where I don't see the artifacts caused by the CFG, maybe he has achieved a breakthrough. reply namlem 13 hours agorootparentprevTrain LoRas for models that can take them reply dartos 5 hours agorootparentThe issue is getting the data on newer aesthetic styles. The more and more platforms lock down access to their data, the harder it’ll be for models to stay up to date on art trends. We just haven’t had image gen around long enough to witness a major style change like the skeuomorphic iPhone icons of old to the new modern flat ones. reply Eisenstein 20 hours agorootparentprev> Built on one of Mistral’s text models, Nemo 12B, the new model can answer questions about an arbitrary number of images of an arbitrary size given either URLs or images encoded using base64, the binary-to-text encoding scheme. Similar to other multimodal models such as Anthropic’s Claude family and OpenAI’s GPT-4o, Pixtral 12B should — at least in theory — be able to perform tasks like captioning images and counting the number of objects in a photo. This is a not a diffusion model -- it doesn't create images, it answers questions. reply whimsicalism 19 hours agorootparentprevsolvable without additional images reply dartos 5 hours agorootparentIt’s literally not. If an artist born today develops their own style that takes the world by storm in 20years, the image generators of the time (for this thought experiment, imagine we’re using the same image gen techniques as today) would not know about it. They wouldn’t be able to replicate it until they get enough training data on that style. reply bronco21016 19 hours agoparentprevAt what point does an agent sitting at a browser collecting information differ from a human? I have multiple ad-blockers running, how am I different from a bot scouring the “free” web? I get the idea of copyright and creators wanting to be paid for their content. However, I think there are plenty of human users out there not “paying” for “free” content either. Which one is a greater loss of revenue? A collection of over a million humans? Or 100 or so corporate bots? reply a2128 16 hours agorootparentHumans use Google Chrome from their home IP address that isn't on any blacklists, and they're always happy to make an account and download an app instead of accessing a website. Or at least that's what companies think humans are reply jazzyjackson 16 hours agoparentprevYou don't need a contract with reddit to scrape it, you can just add `.json` to any url and you'll get the entire thread as one object. reply 8n4vidtmkvmk 14 hours agorootparentThey have very heavy rate limits on their 1st party api now. I can't even delete my own content, nevermind scrape. reply jazzyjackson 1 hour agorootparentwell, it's called \"reddit\" not \"modify-via-API-it\" :-) reply htrp 17 hours agoparentprevthere are torrents all over the internet of AI training data for images and video.... img2dataset also exists reply GaggiX 19 hours agoparentprev>The days of free web scraping especially for the richer sources of material are almost gone I would say the opposite, it has never been easier to collect a huge amount of data, in particular if you have a target, also you don't even need to write a line of code if you are good at explaining Claude 3.5 Sonnet what you want to achieve and the details. reply reissbaker 15 hours agoprevCouple notes for newcomers: 1. This is a VLM, not a text-to-image model. You can give it images, and it can understand them. It doesn't generate images back. 2. It seems like Pixtral 12B benchmarks significantly below Qwen2-VL-7B [1], so if you want the best local model for understanding images, probably use Qwen2. If you want a large open-source model, Qwen2-VL-72B is most likely the best option. 1: https://qwenlm.github.io/blog/qwen2-vl/ reply Jackson__ 15 hours agoparent>If you want a large open-source model, Qwen2-VL-72B is most likely the best option. Only the 2&7B have been \"open sourced\". From your link: >We opensource Qwen2-VL-2B and Qwen2-VL-7B with Apache 2.0 license, and we release the API of Qwen2-VL-72B! reply aucisson_masque 19 hours agoprevMistral being more open than 'openai' is kind of a meme. How can a company call itself open while it refuses to openly distribute it's product and when competitor are actually doing it. reply seydor 13 hours agoparentMeta too. Openai is an ironic name now reply ChrisArchitect 21 hours agoprevRelated earlier: New Mistral AI Weights https://news.ycombinator.com/item?id=41508695 reply azinman2 22 hours agoprevI’d love to know how much money Mistral is taking in versus spending. I’m very happy for all these open weights models, but they don’t have Instagram to help pay for it. These models are expensive to build. reply candiddevmike 21 hours agoparentNo license with this one yet, though you can probably assume it's Apache like the others. reply mdasen 21 hours agorootparentThe article says they confirmed it's Apache via email reply wruza 12 hours agoprevA question for sd lora trainers, is this usable for making captions and what are you using, apart from BLIP? Also, can your model of choice understand your requests to include/omit particular nuances of an image? reply AuryGlenz 3 hours agoparentI’m no expert but Florence2 has been my go-to. It’s pretty great at picking up art styles and IP stuff - “The image depicts Goku from the anime series Dragonball Z…” I don’t believe you can really prompt it though, but the other models where I could also didn’t work well on that front anyways. TagGui is an easy way to try out a bunch of models. reply wruza 3 hours agorootparentYeah, blip mostly ignores prompt too. I tried to disassemble it and feed my prompts, to no avail. Although I found that default kohya gui arguments are not even remotely the best. Here's my args: finetune/make_captions.py ... \\ --num_beams=12 \\ --top_p=0.9 \\ --max_length=75 \\ --min_length=24 \\ --beam_search \\ ... With this, it's very often that I just take its caption as is, or add little. TagGui Oh, interesting, thanks! reply Flockster 20 hours agoprevCould this be used for a selfhosted handwritten text recognition instance? Like writing on an ePaper tablet, exporting the PDF and feed this into this model to extract todos from notes for example. Or what would be the SotA for this application? reply tonygiorgio 19 hours agoparent> the 12-billion-parameter model is about 24GB in size Probably not on the device itself but I would love that use case as well. At least going to my own server. I’d want to protect notes in particular, which is why I don’t do any cloud backup on my RM2. But some self hosted, AI assisted OCR workflows could be really nice. reply jhgg 18 hours agoparentprevTry out https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct reply whimsicalism 19 hours agoparentprevif you have a 3090, you could self host reply edude03 22 hours agoprev [–] 12B is pretty small, so I’m doubting it’ll be anywhere close to internvl2 however mistral does great work and likely this model is still useful for on device tasks reply Jackson__ 20 hours agoparentIt appears to be slightly worse than Qwen2VL 7B, a model almost half it's size, if you look at the Qwen's official benchmarks instead of Mistral's. https://xcancel.com/_philschmid/status/1833954941624615151 reply kaoD 19 hours agorootparentBut Qwen is not multimodal, or is it? reply Jackson__ 19 hours agorootparenthttps://qwen2.org/vl/ >Qwen2-VL is the latest addition to the vision-language models in the Qwen series, building upon the capabilities of Qwen-VL. Compared to its predecessor, Qwen2-VL offers: >State-of-the-Art Image Understanding >Extended Video Comprehension Besides, it'd have been pretty silly for them to mention it on their slides if it wasn't. reply jazzyjackson 16 hours agoparentprev [–] I've found llama 3.1 8B to be effective at transforming unstructured text into structured data, now that LM Studio accepts a json schema parameter. For a general knowledge chatbot it doesn't know much of course, but its a good worker bee. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "French AI startup Mistral has launched Pixtral 12B, a multimodal model that processes both images and text, featuring 12 billion parameters and a size of approximately 24GB.",
      "Pixtral 12B can answer questions about images using URLs or base64-encoded images and is available on GitHub and Hugging Face under an Apache 2.0 license, allowing unrestricted download and fine-tuning.",
      "Mistral, valued at $6 billion after a $645 million funding round, aims to rival OpenAI by providing free models, managed versions, and consulting services."
    ],
    "commentSummary": [
      "Mistral has launched Pixtral 12B, its first multimodal model capable of understanding but not generating images.",
      "Concerns have been raised about the data used for training, especially with increasing restrictions on web scraping, impacting the model's ability to stay updated with new styles.",
      "Pixtral 12B, based on Mistral’s Nemo 12B text model, can answer questions about images but performs below Qwen2-VL-7B in benchmarks."
    ],
    "points": 162,
    "commentCount": 39,
    "retryCount": 0,
    "time": 1726084030
  }
]
