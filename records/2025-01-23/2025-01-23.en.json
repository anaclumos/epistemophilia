[
  {
    "id": 42797260,
    "title": "I made an open-source laptop from scratch",
    "originLink": "https://www.byran.ee/posts/creation/",
    "originBody": "Hello! I&#x27;m Byran. I spent the past ~6 months engineering a laptop from scratch. It&#x27;s fully open-source on GH at: https:&#x2F;&#x2F;github.com&#x2F;Hello9999901&#x2F;laptop",
    "commentLink": "https://news.ycombinator.com/item?id=42797260",
    "commentBody": "I made an open-source laptop from scratch (byran.ee)2282 points by Hello9999901 22 hours agohidepastfavorite273 comments Hello! I'm Byran. I spent the past ~6 months engineering a laptop from scratch. It's fully open-source on GH at: https://github.com/Hello9999901/laptop redbell 2 hours agoI hardly know where to begin! This project is exceptional in every sense—a true masterpiece. Remarkably, its creator is still in high school, yet he’s already demonstrated brilliance beyond his years. The endorsements he’s received, the connections he’s begun to forge, and the incredible opportunities now within his reach are nothing short of extraordinary. As he himself put it, accomplishments like these are only possible when you believe deeply in your vision and persist relentlessly until the finish line. None of this would have been possible if he had given up before completing this remarkable work of art. It’s posts like this, fueled by incredible community support, that make Hacker News not just great but unmatched. With 2,000 points (and counting), this Show HN is currently ranked as the 4th-best Show HN of all time. If we exclude the #1 post (this upvotes itself)—which isn’t a true project—this post would be the 3rd-best of all time. Who knows? By tomorrow, it may surpass 2,741 points and claim the #1 spot outright. Outstanding work, Bryan. All the best. reply Hello9999901 2 hours agoparentThank you redbell! It truly means a lot. I'm incredibly grateful of the reception and the support from everyone. HNUnless they are forced to learn things that are uninteresting to them. This really resonates with me. I love math now, but absolutely loathed it in high school. The curriculum lacked any sort of way to apply math to real problems. I simply cannot learn things in the abstract like that. It's like learning a programming language without ever building a program. reply hylaride 4 hours agorootparentSame. I stopped \"caring\" about math when we started to learn polynomials. Binomials..ok. Trinomials...ok. But then it just became repetitive when the class was just adding more terms to the functions that over the semester I ended up spending most of the class daydreaming. reply DaSHacka 4 hours agorootparentprevYou hit it right on the head, I think. Even at my own university, I struggle to maintain a 3.0 GPA while at the same time actively tutoring students for the very courses I'm failing. The issue isn't knowledge or competency, it's a mix of work ethic and tolerance for menial busywork. I think some of us just aren't made for the academia grind... reply meristohm 47 minutes agorootparentIt's true, and okay, that the academia grind is only for a subset of us. It is not the only meaningful path! I went on to gradschool by rote, and I do not push it on my high-school students or anyone else. It took me about 40 years to find a sense of purpose (having a child was the catalyst). Sadly, the push for STEM seems motivated by capitalists wanting further control of valuable labor, so I'm really chuffed by Bryan's Show HN post even though open-source can be leveraged by capital, it doesn't have to be. It is a non-walled-garden model, and an example of what we can do collectively. Even if the Linux kernel is largely funded by corporations, it doesn't have to be. A concern is that a laptop is still not something my community can make with the local resources, and thus the exploitation of land, labor, and money continues. What would a fair-trade laptop cost? reply lobsterthief 6 hours agorootparentprevI disagree; I did similar projects like this in high school (not exactly like this; his is a true achievement). I did very well grade-wise and had a high GPA but I bombed the SAT because I didn’t understand that you didn’t lose the same number of points for questions you skipped. So the ones I didn’t have time to answer I just randomly selected, which resulted in a poor score. I found out later: 1. How SAT scoring works 2. That you shouldn’t take the last SAT of the year since then you cannot retake it 3. I probably should’ve taken the ACT instead I wish they’d prepared us in school for this, but they were too busy training us for standardized state testing since that determined their own budget. Could I have gotten into MIT? Unsure; back at 18 I didn’t know MIT existed and this was early Internet times. It would have been nice if my high school mentioned it as an option. In my case at least, doing projects like this and getting good grades didn’t automatically turn into attending any college I wanted. Either way, I ended up with a great career. Anyways, kudos to the person who made this project! reply blharr 5 hours agorootparentThankfully, the SAT no longer deducts points for wrong answers. But I agree, there's a big difference between testing and doing really great work. I'm somewhat on the other end of this, where I excelled in school, graduated valedictorian, but didn't gain any meaningful experience with projects and such and had poor leadership skills all around. reply 0xbadcafebee 3 hours agorootparentprevUnless you aren't fit for traditional academic learning models. I spent most of my young adulthood working on projects (not nearly as insanely technical as this! but) similar to this. But I dropped out of high school, didn't go to college, because none of them would teach me in a way, or a pace, that fit my learning disability or mental models. Luckily I had the drive to teach myself, and built a successful two-decade career, despite my parents and teachers telling me I'd fail and become homeless. High school kids have insane potential, and can achieve truly amazing things. But often people disregard them and don't set them up for success. So many companies could hire really great engineers, even from high school, if they could just find the motivated ones and put them in a mentorship/apprenticeship program that aligned with their interests and ways of learning. reply f1shy 4 hours agorootparentprevI’ve known few exemplars like this one. But at least 2. One made a flight simulator for 737 in the backyard that was used regularly by airline pilots to train. The other made a complete discrete FM stereo transmitter, mounted his own radio later. He was 16, and it was the early 90. So all from books. Both guys brutally failed in the first year in the University. They dis not like theory, they wanted to make. So… i dunno. 2 reference points there. reply boesboes 6 hours agorootparentprevGoing from how many gifted children end up underperforming because they are made to do stupid things & then getting labeled as difficult or slow: a lot more then you'd think. Being talented and gifted is generally not appreciated, not even in academia. Many of the most talented people never finish their education because academia is more about playing the game & having the grit (or lack of backbone?) to deal with the bullshit and do what you are told. And tbf, the best engineers I know are not necessarily the most talented ones, but those that developed the grit to push through the bs. reply helboi4 8 hours agorootparentprevI dunno. I only succeeded as a kid academically because of literally my IQ not because I had grit learnt from my projects. I pathologically hated being told what to do so the determination to do my own projects did not translate into anything assigned to me. reply makerdiety 1 hour agorootparentprevHonestly? All of them. All talented men in the universe are being rejected and not attending college or even participating in the work force. If you believe that the civilization project has within it intelligent or smart beings, like men or entire institutions or economies, then, honestly, you have been brainwashed by humanist propaganda and adhered to, authoritarian social convention to overvalue a bunch of idiots. The \"human\" species is not even remotely significant, never mind the forerunners of some grand existence like a Dyson sphere ring achievement or fantastic knowledge bearers who know about the meaning of life. The only intelligent guy will be, reliably, trying to break away from all civilizational objects and other arrogant shit. That evolution is happening, right now, since yesterday, if you will, without a doubt. Intelligence exits the mediocrity of standard implementations. Always and by definition. By definition of the difference between the intelligent and the human. reply leoedin 2 hours agoparentprevThis is an incredible achievement! I've been working in hardware design for 10 years. I've touched on most of what was covered here across various projects in my career, but never all at once. To have the discipline and motivation to carry a project like this through to completion is seriously impressive. reply toobulkeh 5 hours agoparentprevNot just any high school. https://exeter.edu/ reply rozularen 3 hours agorootparentYeah... Just checked and disclaimer I'm not trying to diminish OP's achievement which is huge but ... https://exeter.edu/admissions/financial-aid/tuition-costs/ reply jmb99 1 hour agorootparentHilarious that there’s a separate sub-$1000 line item for books and supplies. If it’s that expensive, you can’t just throw in some pens and pencils with the tuition? reply larodi 4 hours agorootparentprev...back in the day we had this partner who emigrated from undisclosed_balkan_country to the USA in the 90s. 20 years later his daugther in her teens gets $20k funding from the school principal, for her pet fashion project (not even STEM!). Her school is not even a top one, just a private school somewhere in major USA city. On the contrary, even though we've had top marks in the top math school in the same country, we'd never ever hoped to get even $200 for a project. Were we good enough to build a computer of course, it's not that hard once you get the basics, and once you've done x86 assembly in your early teens. But it was just impossible to even think about spending the money, or loosing them per se. Exeter Philips school has quote \"700 acres, 147 buildings, the world's largest high school library\". And I'm sure also lots of engineering development facilities where you can actually get your hands dirty. I can imagine the progress had I found myself there by some miracle. This kid is absolute winner to be in it, but I bet his parents must have also won the lottery ticket, one way or another, cause UK education is crazy expensive. Now, in order to not make this story super sad, let's admit that, even though we as schoolkids didn't have access to such campus and funding, by last year in school I could track music with FT2; build Linux kernel and write ASM/C/C++/Perl; operated a BBS; debugged the IE9 source with VXDs and all; took part in writing two demoscene productions, that we still proud of; and finally, but not lest being a bunch of smart kinds in their 20s we started a hosting company in 1998(or '99) which soon handled the amount of traffic which equaled that of the whole country. This all with no GPTs, no Google searches, not even forums that much back then. So of course, it matters, that you are not a dumbass, after all. But nobody ever gave us the security to pursue dreams the way this kid does. And I'm absolutely convinced we could've put together a laptop or something along the line. I say put together, because a lot of these parts are easily available now, one click away, nothing like what it was back in the day. He's not producing the chips, neither the screen, neither putting elements together, but the chassis and kbd, and does some wiring. Of course fascinating for a teenager to do, but you see, teenagers are not so stupid, and never were. And those in top schools are particularly bright and outpace many adults in many areas. From the images I can tell this is a school projects, so perhaps it took also a little mentoring to do it. This always make me think about two things it absolutely matters which school you are lucky to have gone to; and very likely all talent is lost soon after high-school, because... reasons. reply k8sToGo 6 hours agoparentprevThis guy is in high school yet has been CEO and what not according to his LinkedIn? What makes you think he is in high school reply lexicality 6 hours agorootparentBear in mind he's \"CEO\" of the underwater MATE ROV team at Phillips Exeter Academy, and if you scroll down a bit, it says that Phillips Exeter Academy is giving him a high school diploma. reply k8sToGo 6 hours agorootparentThanks. That makes more sense! reply spicysev 6 hours agorootparentprevHe is a high school senior right now. He is one of my closes classmates and really devoted to such projects. It's mind blowing reply t4TLLLSZ185x 6 hours agorootparentprevThe words \"senior project\". Which, fair enough, might mean different things in different parts of the world. reply jwr 16 hours agoprevCongratulations! From someone who does mixed electronics+mechanical design: this is hard. There are moments of desperation where you realize that everything depends on everything else, and there is no way to achieve all of your design goals. You then have to realize that engineering is all about compromises, and move on, compromising — but this is very difficult. It's easy to get bogged down in details and dependencies and never finish the project. It's very impressive work and it makes me so happy to see real hacker news on HN. This is real hacking. reply Hello9999901 15 hours agoparentThank you so, so much! You phrased it so well. The moments of desperation really hit you hard. I have uncountably many loose ends, but oh well, bad engineering :(. Honored that HN thinks I'm a hacker :) reply snake_doc 21 hours agoprevOkay, I'll help him humble brag: Bryan is in his last year of high school.Keep building! reply chuckwfinley 20 hours agoparentThis is incredible work for anyone, let alone a high schooler. Seriously impressive! I hope this turns into something I can buy (maybe a diy kit), in the future! reply Hello9999901 19 hours agorootparentThanks! I've been considering it (or enough detailed instructions to build one) since starting the project. I need to get a working model first though ;) reply ricardonunez 16 hours agorootparentprevWe are going full circle, Woz will be proud. reply GardenLetter27 20 hours agoparentprevYou study quantum mechanics in High School in the USA? reply Hello9999901 20 hours agorootparentWe discussed wave functions, probability, fermions/bosons, did calculations for particle in a box, the Schrödinger model, and went just up to deriving the hydrogen atom. Nothing super fancy, but it was one heck of an experience! reply stackghost 13 hours agorootparentnext [5 more] But did you win the Putnam? reply dang 12 hours agorootparentFor those who don't know, stackghost is referencing this classic moment on HN: https://news.ycombinator.com/item?id=35079 reply throwup238 11 hours agorootparentI wonder how @sanj feels about their moment of fame (they’re still active on HN). reply dang 11 hours agorootparentAs I've pointed out before, his concession at https://news.ycombinator.com/item?id=35350 was both witty and graceful. It's great that he's still active here! and anyway he's done a ton of things that are a lot more important than that bit. reply throwup238 11 hours agorootparentI hope that’s in the highlights :) reply GardenLetter27 9 hours agorootparentprevIt's really interesting, in the UK I don't think we did (but I later studied Physics at university) but we did have Further Maths which covered more advanced mathematics. Also your project is incredible btw, maybe look into robotics too. reply alias_neo 51 minutes agorootparent> in the UK I don't think we did Perhaps you didn't go to a high school quite like this one: https://exeter.edu/admissions/financial-aid/tuition-costs/ reply mattnewton 20 hours agorootparentprevSome do He thanks Phillips Exeter at the bottom of the project page, which is a very fancy private highschool, probably the best in the US. reply macNchz 19 hours agorootparentI went to a peer school that had at least a couple of math teachers with PhDs—my friends at the time who took their classes were, if I recall, nationally competitive in math olympiads. reply rafram 18 hours agorootparentprevIt's more possible than you'd think! The options are basically: Go to a fancy private school like Phillips Exeter Really luck out and get into a great public STEM magnet school Homeschool and take private classes / have very smart parents reply cbmamolo 17 hours agorootparentAll I did was provide him the space and time to work on the project ... his parents funded the entire project, but will get reimbursement soon. It's the great minds, and the desire to have meaningful projects that make Exeter such an awesome place. Byran is one of a kind!!!! reply rafram 18 hours agorootparentprevOh, or: Concurrently enroll at a community college (a really great option that I think every country should have) reply jogu 16 hours agorootparentI tested out of high school and went to community college instead, one of the best decisions of my life. reply notnaut 13 hours agorootparentprevSome public schools in very wealthy counties will teach some basic quantum mechanics in honors/AP classes, too. All you have to do is acquire parents that can afford the shittiest neighborhood in those districts! reply anonzzzies 12 hours agorootparentThey did in mine in the Netherlands. Also electronics and programming (this was a long time ago so it was all pretty new); it was a special class to prep for university more than the regular curriculum does, but it was a public school and not even a very good one; just a few really good and switched on teachers (physics, math and chem). reply 2muchcoffeeman 17 hours agorootparentprevCan't tell if this is sarcasm. reply rafram 16 hours agorootparentThe community college option is available to anyone who’s willing to spend a couple evenings a week taking classes, so I don’t think it’s really that out of reach. Most countries don’t offer their high school students any opportunity to study material that advanced. reply 2muchcoffeeman 8 hours agorootparentYour first 3 options are mostly “be born to the right parents”. So I couldn’t tell if your remark of “it’s more possible than you’d think!” Was serious or not. Hell I went to a really selective school. But even then, within that the top students, whom I was not one, got to do some extra stuff that would have greatly interested me and I would have been able to do. But my grades in humanities weren’t good enough to be one of the best. reply ericjmorey 6 hours agorootparentprevCommunity college course options often won't include quantum mechanics. reply TeMPOraL 17 hours agorootparentprevIn the high school in Poland I attended, I lucked into being in a class with a university TA assigned as physics teacher, and he did manage to sneak in QM more-less the same stuff as 'Hello9999901 listed in their reply. (He also taught us differentiation in the first semester, and basic integrals in the second, because as he said, you cannot learn physics properly without those tools. This annoyed the heck out of our math teacher; she ended up deciding that, if we're learning this anyway, we might as well learn it properly and gave us a much heavier intro to calculus in the last months of the last year.) reply apricot 5 hours agorootparentprevThe USA has some great schools. OP goes to Phillips Exeter Academy, which is an exclusive private school that ranks among the best high schools in the country. reply govg 19 hours agorootparentprevNot all high schools but the US has some schools which allow you to take very advanced material / even get a head start on your college credits. reply volemo 13 hours agorootparentprevYou don’t study basics of QM in your high schools? reply mschuster91 19 hours agorootparentprevWe had a cursory introduction at least about 15 years ago in Germany, it's not that far off. reply d3rockk 17 hours agoparentprevHOF HN post. reply gerdesj 17 hours agorootparentCare to explain? reply belden 15 hours agorootparentI think the message means that this post is worthy of a Hall of Fame on HackerNews. reply d3rockk 1 hour agorootparent^ reply ValdikSS 6 hours agoprevBy the way, you can tune boot times further. My print server board boots in 8 seconds to Debian 12 (bootloader + kernel + userspace). 1. Make sure the bootloader (u-boot) loads the kernel as fast as possible. Disable automatic Ethernet/USB/other subsystems initialization (you can keep them enabled, just don't activate unless requested in the shell manually by the user) Tune `distro_bootcmd` command Make sure that MicroSD/eMMC/SSD works full-speed (with proper clocks and speed protocol) 2. Use fast decompression algorithm for the kernel and initramfs It's either zstd or gzip 3. Collect boot file access data and sort the files on the filesystem The benefit in near-linear access & read-ahead I'm pretty sure that the current 20 seconds could be shrunk down to 14 or so. reply dataflow 15 hours agoprevThis is crazy. Hats off to you. My guess is you'll have recruiters knocking on your door yesterday, trying to grab you before the next one does. Whatever you do, don't let your talents go to waste (corporations can do that), and think about your long term success, not whatever they dangle in front of you for the short term. You're going places. reply Hello9999901 15 hours agoparentThank you so much for the heartfelt advice! I'll keep that close to heart :) reply bboygravity 9 hours agorootparentTip from an experienced EE: try to avoid middle-men recruiters at all cost. Go straight to the company you want to work for. Or better yet in your case: start your own company :p reply frognumber 7 hours agoprevImpressive! Suggestion: It would be nice to include a price list on the article. This project is impressive as heck, but aside from being intellectually out-of-reach for most kids, it would be financially challenging as well. Last I looked, CNC aluminum blocks were well out of the reach of 99.9% of kids (but that was decades ago; perhaps prices went down). For people wanting to follow in those footsteps, it'd be nice to know which things cost $5, which $50, $500, or $5000. Just that kind of intuition is helpful. reply lxe 19 hours agoprevThis is one of those special HN posts that demonstrates outsized excellence on the author's behalf. Watched the video and I'm very impressed. reply Hello9999901 19 hours agoparentTruly appreciate it thank you so much!! I poured my life and soul into this haha. reply cbmamolo 17 hours agorootparentYou sure did! reply petsfed 20 hours agoprevThis is really cool! There are some obvious next steps for improving the polish on this, would you say you were more resource constrained, time constrained, or skill constrained? For instance, did you put any thought into making flex PCBs to make the cable routing easier? I also think the concept of a laptop with a removable wireless keyboard is brilliant, and I think your implementation is a lot cleaner than e.g. the Surface or the iPad's case-keyboards. If I had a laptop that did that, it would be my go-to travel machine. One less thing to cart around. reply Hello9999901 20 hours agoparentHey! Thank you for the question. For sure, it's not a polished product and I don't mean for it to be. It works surprisingly well. (I've used it as my daily driver for school) With college apps and school work, the time was tight. I'd say that was the most limiting. Of course, resource and skill played its role. I did consider flex PCBs, but I didn't have the time to follow through with all the ambitions (i also wanted an FOC input sigh). I'm honored that you think my keyboard implementation is nice! I put a lot of thought into it — truly. Oh btw the keyboard works just as well as a solo device. I've used the keyboard more than the computer in some ways. Thanks! reply petsfed 46 minutes agorootparentIf you keep this idea alive (and I hope you do!), you might consider shrinking the keyboard battery and designing its docking configuration so that it automatically charges from the main battery when stored in the laptop. A 3 month keyboard battery capacity seems sort of excessive when its mechanically part of a machine that charges daily. I think one of the limitations to the keyboard concept you have is that it complicates using the laptop base as a stand for the screen in a tablet configuration. Outside of tablets with fully detachable keyboards (e.g. the Surface or the iPad pro), I don't think anybody has a good design for that. Was a touchscreen ever a consideration for stretch goals or design for expansion? reply vidarh 9 hours agoparentprevAlso loved the detachable keyboard (which has me fantasize about a detachable screen as well + external hdmi/displayport, as I hate the working positions I end up in with a laptop, so it'd be nice to be able to get a more comfortable setup in a hotel room etc. that still packages up to a laptop. reply daquisu 13 hours agoparentprevRegarding a laptop with a removable wireless keyboard, ZenBook Duo has that, although the touchpad is removed with the keyboard. It also has two screens and its own stand, I use it as my travel machine. reply petsfed 1 hour agorootparentHmmm. That's also an interesting solution to the same problem. Although honestly, the scenario where I want to have a removable keyboard, I'm likely using an external display (probably a hotel TV), and a small wireless mouse is a lot easier to transport than a keyboard (and more ergonomic than a keyboard small enough for transport) so the extra screen and trackpad are sort of lost on me. reply guywithahat 16 hours agoprevSometimes I wonder why I didn’t get into MIT, and then I see people like this exist reply mkesper 5 hours agoprevThanks to the work of the community the RK3588 is also on the right track regarding mainline support, severly reducing the fear of turning into unmaintained kernel hell. https://gitlab.collabora.com/hardware-enablement/rockchip-35... reply gsuuon 59 minutes agoprevI was impressed this is open source, then impressed it was done by one person, then impressed it only took 6 months, and eventually somehow impressed again that it was a _high schooler_. My mind is blown. Kudos for managing to do something insane like this. Very inspirational. reply LeFantome 1 hour agoprevBuilding the laptop is impressive given his age. I would be hard pressed to duplicate this feat even with the time and money to allocate to it. Honestly though, I think the maturity shown in his write-up impressed me even more. Inspirational. reply geerlingguy 15 hours agoprevAlways fun to read an article like this, for humility's sake. Wow! And I'm guessing if he attempts a 2nd edition, it'll probably be even thinner, lighter, and faster! reply Hello9999901 15 hours agoparentOh my god! Jeff — huge fan and subscriber!! Thank you for your words of encouragement :). Your videos have been a huge source of inspiration for me. reply itsmemattchung 13 hours agoprevJust skimmed the YouTube video and I'm blown away as well ... anytime my ego needs to get checked, I just scroll through HN posts. Truly impressive reply CYR1X 2 hours agoprevObviously super cool and kudos like everyone else in here. Feel like you could make a pared down version of this with commodity parts outside of the chassis if you aren't going for a flagship competitor. I guess you could also just buy a $20 chromebook, too. Maybe...you could fit a nice rockchip SOM inside a chromebook?? reply eadmund 14 hours agoprevThis may be the coolest thing I’ve seen this year. Wait, it’s January? This may be the coolest thing I’ve seen this year and last. And possibly the year before. Well, well done. Good luck to you! reply Hello9999901 14 hours agoparentThank you so much for your kind words! reply mllev 12 hours agoprevSo how long is the trip to Earth from your home planet? And do you plan on staying a while or are you just here for 6 months to humiliate us with your superintelligence? reply triyambakam 17 hours agoprevHey Bryan, great work and very inspiring. This has me meta curious about how a project like this is possible. Besides the support from your school, I imagine that your parents have been a big part of your success? reply stevelacy 3 hours agoprevThis is amazing, love the ESP32 watchdog controller. Had a question about the keyboard would it make sense for the keyboard to be hardwired to the laptop via USB-C and detachable to have one battery source? reply numpad0 4 hours agoprevMy vocabulary hasn't got appropriate compliments so I'd just say congratulations to the author, you've got serious talent, hard earned skills, and great mentors. Just commenting so I can't come back later and claim I wasn't stealing lots of ideas from the author for my own project; the hinge problem, keycaps, the mainboard designed on KiCAD, are all interesting. reply chironjit 20 hours agoprevI actually spent quite some time trying to build a custom driver for a custom screen for my Framework 13, only to burn the screen driver. Very impressed by what you have done here. Kudos to you on achieving designing and building a whole laptop! reply Hello9999901 19 hours agoparentThank you so much! If you'd like to discuss further, please let me know! My email is in the website. I have a Framework 16 and have tons of ideas. Never got around to it though. (I also burned a few screens, and had 3 as backup haha). reply NooneAtAll3 2 hours agoprev> runs +7B LLMs +7B means \"additional 7B\" if you want to say \"more than\" or \"at least\", you say \"7B+\" reply justmarc 12 hours agoprevA huge congratulations to Bryan, a wonderful achievement and a remarkable result! Keep it up Bryan! It's lovely to see HN so nice and friendly, keep it up guys! reply nashashmi 19 hours agoprevLooks good. Could be a small step to my vision for a dock dependent palm sized pc with high powered cpu connected by a single USB C with no other ports except for micro sd. And backed up by a mini battery for power stability on low watt chargers. reply Hello9999901 17 hours agoparentThanks! Have you taken a look at the Khadas Mind [1]? Super similar to what you're talking about with the handheld PC. [1]: https://www.khadas.com/product-page/mind reply nashashmi 7 hours agorootparentWow. Precisely what I have been looking for. This is great. And mind 2 is even better. reply mlepath 3 hours agoprevThis is an awesome project! Thanks for taking time to document this. What's next on your plate? How do we follow you? reply Vekz 13 hours agoprevBryan, thanks for publishing, this is great work. I'm curious if this this build could fit and swap a 13.3 E-ink screen with display board. Some open source hardware synergy with https://github.com/Modos-Labs/Glider/tree/main reply forinti 5 hours agoprevThat's impressive. I'd be really happy with myself if I just built a case and put off-the-shelf components in it. reply Havoc 3 hours agoprevAlso the 3588 chips can run LLMs on the NPU. Not quite llama.cpp level easy but definitely doable. For 7B class models the speed is usable reply kuon 13 hours agoprevCongratulations, this is awesome. I worked on medical devices where I did both hardware and software and it is really hard. I wish you the best and I really hope you'll continue to use your skills for good and open products. reply eddywebs 15 hours agoprevThis is really cool ! Kudos for getting this started. I wonder if initiatives such as one laptop per child could have been effective with this kind of approach. Eitherway I hope this project goes along way as I could see its application not only at home but also in developing nations. reply ValdikSS 6 hours agoprevHow is the idle power consumption of RK3588? I bet it's pretty high, I'd expect more than 1W. I have a board with old MT6572, it idles at 270mW with working CPU, even less when in semi-sleep (turns off CPU and wakes up every half a second). reply op00to 14 hours agoprevHey you didn’t mine the rare earth minerals! This ain’t really from scratch! Just joking, incredibly impressive! reply amelius 19 hours agoprevI'm curious how the USB-C connectors are made to the outside of the enclosure. What I've found is that it's a bad idea to use USB extension cables; these can introduce bit errors if e.g. you copy large amounts of data (order of terabytes). It's much better to insert a USB drive directly into a carrier board, but this is not always physically possible. reply Hello9999901 19 hours agoparentIt's almost standard to have the USB-C have extra wiggle-room (around 1mm or so). Then, the housing is 1mm past the USB-C connector. That's how the casings are made so that when you stick the connector in, it's flush or nearly so. I agree with USB extension cables concerns too! The error would increase depending on the quality (impedance, power, etc.) reply amelius 19 hours agorootparent> Then, the housing is 1mm past the USB-C connector. Yes, this is often the case but sometimes the USB-C connectors are on the same side of the board where you also need to plug in some cables that you need internally (maybe even other USB devices). Thus the option of letting an USB-C port stick out on one side of the enclosure is not always available. > I agree with USB extension cables concerns too! The error would increase depending on the quality (impedance, power, etc.) Yes, and the user of your device (who doesn't see the internal cable) will assume that they can plug in their own cable, so you'll have two cables. reply lr4444lr 4 hours agoprevFantastic work. How did you learn all of the EE and low level programming needed to pull this off? reply Atreiden 5 hours agoprevUnbelievably impressive. Such a wide breadth of skills and expertise needed to pull this off. And the final product looks great! Kudos to you! reply 0x38B 7 hours agoprevThis is one of the coolest, most inspiring projects I've seen anywhere wow! Seeing you and nrp connect here in the comments was so cool; just the start, I'm sure. It was neat to read through the progress log (1), which begins, > \"It was around 1AM. I wrote up the mission goal (2) and went to sleep at 2AM. The start\", and ends: > \"With the YouTube video and blog post almost done, I hope this isn’t the last of anyon_e. But rather, the start of a trailblazing journey.\" This project is the epitome of MUREX Electrical's mission statement, \"attempt the impossible\": It's \"impossible\", a non-MUREX Robotics Electrical member might say. However, we accept it as the process. In the end, we will have achieved something others might have called \"impossible\". But the achievement only comes through endless, motivated attempts at the impossible. (3) 1: https://www.byran.ee/progress 2: https://www.byran.ee/posts/mission, which links to (3) 3: https://github.com/murexrobotics/electrical?tab=readme-ov-fi... reply jagermo 1 hour agoprevwow, this is way more awesome than I could have thought. Very well done. reply swiftcoder 10 hours agoprevMad respect for this build. That's well beyond what many professionals in this field are willing to attempt reply MrDrMcCoy 20 hours agoprevThis is fantastic! I hope to follow in your footsteps as soon as a decent RISC-V board can supplant that RK3588. reply Hello9999901 20 hours agoparentI'd love to see one; hope to see that day come!! reply amatecha 15 hours agoprevI clicked through dreading that it's got a Raspberry Pi at its core, but no, RK3588 (same as MNT is using now)! Very nice. Ultra kudos for making it truly open source. Great work!!I ran out of time And then realized this was a ... high school project!? Way to go, amazing work! reply itzami 9 hours agoprevThe project is outstanding but the fact that you've documented everything AND did a video about it speaks volumes about what you'll achieve if you keep at it reply andrewmcwatters 21 hours agoprevSick! Finally someone posting something that puts the “hacker” in HN. Love the parts research you did. reply Hello9999901 20 hours agoparentThank you so much! reply xarope 15 hours agoprevWow this is fantastic, great job! I hope this heralds a new era of HW engineering. P.S. @Hello9999901 any relation to \"Bunnie\" Huang? reply Hello9999901 15 hours agoparentHaha I wish, but no. Just surname coincidence. reply vhiremath4 11 hours agoprevA seriously impressive piece of work, especially only in 6 months. Bravo! :) reply jokoon 9 hours agoprevSo if you want to install windows on this, do you have to add some secure bios feature? Is it possible to have access to that without big license fees? reply wizzwizz4 9 hours agoparentYou can just patch Windows to remove the check. It's not like any critical components of Windows are encrypted. reply apricot 5 hours agoprevHeck of a high school senior project, my hat's off to you. reply KeplerBoy 8 hours agoprevSuper cool work! One question: How much does JLC want for such a low volume cnc part of reasonable complexity like the laptop shell? reply rcarmo 8 hours agoprevPretty awesome. As someone who deals with Rockchip stuff a lot, I am going to take a look at the software part for sure. reply shahzaibmushtaq 12 hours agoprevAn amazing challenge you set for yourself and pulled it off in 7 months is admirable, commendable and exceptional. How much did it cost to make this open-source laptop? My wild guess is it's around $500-750. reply ritonlajoie 10 hours agoprevOP you are incredibly talented. I believe very few people on earth could do something like that at your age and deliver. Congratulations ! reply ysofunny 2 hours agoprevI only would regard this as from scratch if they smelt they own foundry reply doubleorseven 10 hours agoprevDear @Lenovo Please hire this guy to help you make the thinkpad's keyboard to do wireless magic with the trackpoint and the mouse buttons include. Thanks reply tuktuktuk 19 hours agoprevAmazin! what's the total cost for you? reply Hello9999901 19 hours agoparentHanging around 5 grand. Unfortunately the R&D process was rough! The R&D BOM is linked below, feel free to take a look. If you were to build it, I'd estimate it costing around 1500 dollars (or less). reply someothherguyy 17 hours agorootparentBack in my day, I thought spending $50 in wood shop was rough. reply tuktuktuk 13 hours agorootparentprevThank you for sharing! reply mschuster91 18 hours agoprevHoly. That's an achievement very few people can claim. Wonder if HN has a \"hall of fame\", a worthy entry. You did the smart thing there with the SoM (for the uninitiated: power sequencing to individual parts of an SoC and its external components is an epic hassle to get right and that's assuming you actually have proper documentation without it it's an utter pain), but how in hell did you get the high frequency stuff working out on what was likely your first or second try? This is IMHO where your work really shines. USB-C, DisplayPort (at 4K to boot) and PCIe at modern speeds are all but black magic to most, this isn't digital any more, this is good old analog circuitry and physics at work that most people don't even learn in university any more. reply Hello9999901 18 hours agoparentThank you so much — yes, that was the hardest part of this entire project! I spent 2 months getting eDP working (second PCB thankfully). I had the honor of learning high speed signaling from the best. I met some super cool people from Silicon Valley and research universities (from past work, like the MUREX Ethernet Switch). The ZMK Firmware community too! reply mschuster91 18 hours agorootparent> from past work, like the MUREX Ethernet Switch Just looked it up... https://news.ycombinator.com/item?id=40694254 for those who want a direct link. Jesus. Wish I had had even a fraction your talent at that age. Most impressive. reply Hello9999901 18 hours agorootparentI truly appreciate your encouragement. I can only imagine how successful you are! Thank you! reply j3s 21 hours agoprevVERY impressive. the laptop looks great. wish you could manufacture and sell the thing, i'd consider one :) reply mwcampbell 16 hours agoparentNot to take anything away from this amazing achievement, but if you want a similar open-source laptop, also based on an RK3588, that will actually be manufactured and sold, check out the MNT Reform Next: https://www.crowdsupply.com/mnt/mnt-reform-next reply Hello9999901 21 hours agoparentprevMaybe, depending on reception! I geared it so it could be manufactured at a semi-small scale. Unfortunately, I don't have the capacity to make them myself :(. Thank you for the interest! reply umrashrf 16 hours agoprevI’d like to follow up to see how it handles heat or excessive heat if any reply Hello9999901 15 hours agoparentI played Minecraft (epic gaming) with friends for a few hours, no problem! The massive heat capacity of the copper + heatpipe + active fan is good enough. reply Justta 11 hours agoprevMost of the older LED display have standard 30 pin or 40 pin connection. Lacking standard is keyboard connectors. Most standard are battery and fans. Maybe laptop should have two layers or parts.One for Motherboard and memory and another for connectors, fans, power supply, battery etc. Then we can have more standard even if a little thicker. reply myheartisinohio 4 hours agoprevCool project. Keep building! reply AnthonyMouse 14 hours agoprevI've long been disappointed that we've never really gotten standard laptops, in the way that there are ATX standard desktops. The laptop form factor hasn't really changed in decades. It's a rectangle with a screen in the lid and a keyboard in the base. Below the keyboard is a battery and a system board. The battery has to be replaced when it wears out and the system board when it becomes obsolete, but then why aren't they both fungible parts? If you take any arbitrary ATX PC from many years ago, you can replace the system board/CPU/memory/storage with modern ones and carry on using the same chassis, screen, power supply and keyboard provided they meet the required specs for the new parts (and they often do). So why can't I do this with the average laptop, instead of having to replace $200-$300 worth of perfectly good parts or more each time I want an upgrade? reply coryrc 12 hours agoparentI think my laptops have been split between mechanical and hardware EOL. I don't think the mechanicals would ever go for two lifetimes. But I actually take mine around places and am a little clumsy. reply AnthonyMouse 11 hours agorootparentMechanical what? The keys? There are people who have had the same keyboard for 30 years. You don't have to design things to be disposable junk. That's even part of the advantage. If you want a higher quality chassis made of rubber-padded metal that can survive being dropped off a table then you'd only have to buy it once. reply numpad0 10 hours agorootparentNo one personally buys a Toughbook new for home use. reply AnthonyMouse 10 hours agorootparentBecause you don't pay that much for something to be durable if the durability will just be defeated by obsolescence. Whereas if you could upgrade it to later generation processors, clumsy people would save themselves a lot of trouble to buy the chassis once. reply numpad0 4 hours agorootparentI guess we're talking from a completely different range within spectrum of laptop computing. Hinges, shells, flex cables, power ports, etc. all tend not last longer than hardware end of relevance for me unless it's one of rugged ones, but my use case is to throw into a bag and strategically yeet onto another luggage kinds. reply marssaxman 12 hours agoprevThis is one of the coolest projects I've seen here in a long time. Kudos! Your dedication to completion is admirable. reply engineer_22 5 hours agoprevAwesome work, the future is bright reply intelVISA 7 hours agoprevNicely done, huge amount of grit and craftsmanship. reply ekunazanu 6 hours agoprevThis is some seriously impressive stuff. reply bflesch 21 hours agoprevwell done, thanks for documenting and congratulations on completing the project! reply Hello9999901 20 hours agoparentTruly appreciate it. I spent many weeks afterwards documenting the steps as thoroughly as I could. My email's on the site if anyone needs to reach out, as well. :) reply junon 20 hours agoprevThis is so, so cool. Reminds me of Clockwork Pi stuff. Thanks for sharing :) reply martin293 8 hours agoprevI might have missed it but how much did this cost in total? reply rothos 7 hours agoprevAmazing work. How much did it end up costing? reply jballer 14 hours agoprevIncredible work reply eviks 15 hours agoprevHave you thought about finally adding a split ergonomic keyboard to a laptop instead of the standard slab? reply Hello9999901 15 hours agoparentThat's a sick idea too! Wouldn't be too hard. I'll keep that in mind never thought of it before even though I've used my fair share. reply eviks 15 hours agorootparentThere was a very old ThinkPad design that allowed the keyboard to take more space than the width of the laptop, but that was also bad old non-split layout But at least not wasting existing left/ right side space and having a gap in the middle instead would be a nice start Or maybe even get to the best of the portable/non-portable worlds: since the keyboard is wireless, you could detach 2 halves of the keyboard and place them on a desk at ergonomic shoulder distance And this would allow you to also ergonomically position the laptop itself for a better screen position vertical, just like your desktop monitor ThinkPad: https://youtu.be/RRHFi_l9UR0 reply xarope 11 hours agorootparentThe 701 \"butterfly\"? It was cool for its time. https://en.wikipedia.org/wiki/IBM_ThinkPad_701 reply ornornor 12 hours agoprevI wish I had your talent, that’s impressive! And it took you 6 months only. reply juhanakristian 1 hour agoprevWow I didn’t even know this was possible.. some people are just on another level. reply phlipski 4 hours agoprevSuper impressive! reply dc3k 10 hours agoprevi'm playing around with your onshape document and learning a lot of things for my own projects. thanks! (also, amazing work of course) reply aunver 22 hours agoprevCongratulations Byran, this is really impressive work! reply laidoffamazon 21 hours agoprevVery nice. Wish there were faster SOMs than the 3588 but maybe in a year or two. Looks like an MIT admissions portfolio project. Don’t know if it fits the uniqueness category for it but I guess the quality of the end product makes it good enough. Admittedly this isn’t fully open source like the Novena or the Reform but I doubt adcomms care. I just wish I was rich enough and skilled enough to be able to spend $4.5k on a neat project like this. reply Hello9999901 21 hours agoparentThanks for your suggestions and criticism! Much appreciated. Which aspect of it (aside from the SoM, which I admittedly do not have the R&D to make in this timescale) isn't open-source? I'd love to hear your thoughts. The Novena and Reform are amazing pieces of engineering, but I believe they sacrifice the portability and looks for repairability which some people certainly prefer. I wanted to aim for something that a non-technical consumer might look and say \"hmm, nice laptop!\" and not think it came out of the matrix or built it myself. In terms of college, still waiting :) reply teleforce 10 hours agoprevAnyone know the amount of RAM available for the laptop? Personally I'm a bit disappointed that it's based on Rockchip. If someone can come up with low cost open source laptop with RPi compute module 5 with 16GB RAM I think it will selling like hot cakes given the software and hardware eco-system that exist round RPi [1]. It just that the compute module has yet to come with 16GB RAM unlike the normal RPi 5 but it will probably just around the corner [2]. [1] Compute Module 5: https://www.raspberrypi.com/products/compute-module-5/ [2] New 16GB Raspberry Pi 5 on sale now at $120 (191 comments): https://news.ycombinator.com/item?id=42642873 reply numpad0 10 hours agoparentNo hardware sell like hot cakes relative to the pain and cost incurred. reply teleforce 9 hours agorootparentCare to explain? reply actionfromafar 10 hours agoparentprevI think 32 is the limit for the motherboard. reply paines 9 hours agoparentprev> Personally I'm a bit disappointed that it's based on Rockchip. Why would you? You mentioned Raspberry where, compared to the competition, you pay more for the name while they deliver even the same capabilities or more bang for the buck... Don't get me wrong. Huge Raspi fan here. I have 3 models laying around here, because they are the easiest to purchase. But the competition is not to be overlooked. Also, aren't the compute modules strandarized or compatible? So it should be interchangeable, no? reply teleforce 6 hours agorootparentBecause it's the eco-system for software, hardware, firmware, drivers, books, documentation, blogs, papers, training modules, etc. It's the same reasons Nvidia thriving for ML/AI while the rest are playing catch up. reply aio2 18 hours agoprevmy guess is when doing college applications, you figured you had to do something special to get into a good college, so you decided to do this lol Doesn't matter why, pretty sick. I'm studying physics myself, so its pretty inspiring to see you do this reply Hello9999901 18 hours agoparentThank you so much! The story behind the laptop was quite interesting — my friends and I were going to an athletics event far away, and he brought up the idea that I should make a laptop for my senior project as a joke (our school offers 1 free class for a \"project\", graciously funded by the school). I said \"hell yeah.\" That's pretty much how this came to be, college didn't play much of a role imho. And best of luck studying physics! reply camtarn 20 hours agoprevGenuinely incredible work. Looking forward to seeing what other cool projects you do in the future. reply spicysev 21 hours agoprevHoly hell. This is so cool ~ an admirer reply vim-guru 11 hours agoprevCongratulations on a beautiful build! reply Palomides 21 hours agoprevnice work! how much was it to get the case milled? reply Hello9999901 21 hours agoparentThanks! Around $300 total from JLCCNC with 6061 aluminum, bead blasting, and matte black anodization (top, midplate, bottom). reply Palomides 4 hours agorootparenthuh, actually not that bad reply cjbgkagh 4 hours agoprevI'm honestly rather envious. I guess my 'sour grape' is that the lack of funds and opportunity for me to do this is what lead me to go into software and then on into Machine Learning which I do think turned out for the best. Making electronics like this, while still difficult, is far easier than it used to be and I do enjoy it as a hobby in a way that I probably would not have as a career. It is no doubt an incredible achievement. I don't like the 'anyone can do this' when that clearly isn't true it comes across as a humble brag and seems to be a strong part of hustle culture. I would much prefer 'anyone with a decent amount of money and a high enough intelligence can do this', or 'this is now far easier to do than it has ever been'. I do like the idea of MIT being a beacon to the best and brightest and I do think that the lack of a level playing field means that many otherwise talented people miss out on that opportunity. Perhaps what I would really like is for the world to have more MITs but I don't know if that is possible and I worry that attempts to do this would undermine the quality of MIT. So perhaps I should be content that MIT exists as is and that some people get to go there even if I did not we all benefit from the fruits of their labor. My university was a top tier university renowned for harsh grading and I was still rather disappointed by the quality of my peers and I worry that the quality at universities in general has since declined further. Cheap and high quality small batch electronics and hardware fabrication is rapidly changing the world in a way that I think few people understand. It used to be that you had to have a decent size company to do this kind of stuff and that company needed capital investment, layers of management etc. So the cost of bringing a widget into the world was really expensive, risky, and took a long time. The only way to make that money back was to do things in bulk and sell a lot of them which meant you had to be sure there was a sufficient target market. These days a single person can design and fabricate a single item for comparatively very little. And if they want to make it accessible to the rest of the world there is no need to build a factory, just upload the plans. If it's a popular design in all likelihood someone in China will produce it in bulk at commodity prices. The speed of commodification has become so fast that it's practically instant. There is a bit of a phenomena going on at the moment with 'high tech overproduction' where it is claimed that China is intentionally over producing high tech goods to undermine Western markets it's my view that they're ahead of us on the commodification curve. As manufacturing also manufactures the manufacturing tools the commodification process is a self reinforcing cycle. reply _joel 9 hours agoprevAstounding, well done. reply KolmogorovComp 19 hours agoprevVery impressive work, and also nice video editing. Congrats. reply webprofusion 15 hours agoprevThis is what the internet was invented for. reply Hello9999901 15 hours agoparentThank you! reply teddy__d 22 hours agoprevamazing job!! reply miunau 6 hours agoprevExtremely cool project and congratulations on having the mental wherewithal to see it through, and in such short order! reply 6510 13 hours agoprevBesides the enormous effort, what did the part cost? reply handfuloflight 21 hours agoprevWhat's the BOM? reply Hello9999901 21 hours agoparentThanks for the question. I'm working on compiling the BOM in these few days. A preliminary R&D BOM is here (apologies, it's in Google Sheets): https://docs.google.com/spreadsheets/d/17arbJvPqW6koqEJwAzne... reply imcritic 8 hours agorootparentThis seems to list some components multiple times in a versioned way, did I get it right that this is rather a whole list of components you've bought while working on this project rather than the final list of the components needed to assemble the notebook you've built? reply madsmith 19 hours agoprevAmazing project. reply chrismorgan 12 hours agoprev> A highly integrated, high end, open source laptop. Not sure what’s meant by “high end” here. Performance is a rather important aspect, and the RK3588 this uses will make it slower than almost every laptop on the market. Practically all are twice as fast (both single and multi-core), most are 3–5× multi-core, and the best approach 7× (paired with 2.5× single-core). Looking at Lenovo India, they sell three laptops that are slower multi-core and maybe slower single-core (running Celeron N4020 or Athlon Silver 7120U); after that, they’re all at least twice as fast, in both single and multi-core benchmarks. (I’m simplifying to PassMark’s single-/multi-core scores, usingand such.) From : “In many aspects, the Rockchip RK3588 is the fastest consumer-procurable chip on the market.” As someone not involved in these spaces, this was my vague impression, but it still ends up disappointing if you simply can’t get good performance for a project like this because only bigger companies can buy the better-performance things. It’s an extremely impressive project, but unfortunately will be rendered not viable for many—probably most—people for this one reason. That makes me sad. I wish they’d sell us the good stuff. reply utopcell 11 hours agoparentI suspect that utmost performance was not the main focus of the project. Nevertheless, the complexity of the project would not be different if a different CoM was to be adopted. Even creating a custom CoM doesn't seem that much more complicated, as the daughter board is as complicated since it already needs to handle high-frequency traces as well. Super impressive project! reply barrkel 6 hours agoparentprevThere's a way to phrase this comment better, which reflects sadness on the facts of the market, and isn't a kind of attack on the author. reply wickedsight 9 hours agoprevJust opened Youtube and your video was on top of my home page! I will watch it later today. Congrats on the awesome project! Actually, I think 'well done' is more fitting since this must have taken a ton of work and willpower! reply system2 10 hours agoprevFYI, none of my engineer friends in the U.S. can pull this off. This is truly impressive. reply lemper 12 hours agoprevaight, mate. that's definitely impressive. no, not only impressive, i believe it can help you land a great job somewhere. reply baritodespa1 22 hours agoprevgg byran well played reply honeybadger1 6 hours agoprevsome people just have what it takes and all you can do is watch and appreciate. really awesome! reply unethical_ban 14 hours agoprevIf you can do something like this, then you'd be great at Factorio! :) On a less joking note, I wonder if I'm decent at Factorio, I could learn this. reply fifticon 10 hours agoparentnot only that, you can use this laptop to _play_ factorio! reply felipelalli 15 hours agoprevBrabo. reply ge96 21 hours agoprevdamn it looks clean reply jiveturkey 15 hours agoprevcame here to shit on this project, that there was no way it was open source down to the ME, like a raptor or framework computer. absolutely required IMO to be considered open source. i didn't find any firwmware in the repo (didn't look exhaustively) but I did find that the SoC this is based on is supported by https://github.com/edk2-porting/edk2-rk3588 . AFAICT the azoteq trackpad has proprietary firmware, so if that's true then i won't call this laptop fully open source. but from a practical perspective, i am much less worried about that then the boot path. love the keyboard, wish i could test drive it! so instead, i was left very, very impressed! reply stuckkeys 15 hours agoprevHolly crap. This man is the messiah of tech. Keep going my guy. That is so impressive. I look forward to what you do next. reply _fw 21 hours agoprevHoly fuck People like Byran live amongst us Making their own laptops but from SCRATCH Imagine how good this man’s pasta carbonara tastes reply Hello9999901 21 hours agoparentThank you so much! reply gerdesj 16 hours agoprev [–] At which point was the mental map created within Obsidion and did you really need it? You are clearly a very clever person and you do not need a web app wiggly graph thingie to throw ideas together. There's no need to gild a lily! Please keep the faith I love that you are focussed on being altruistic and sharing your skills to the benefit of everyone. Thank you. reply Consider applying for YC's Spring batch! Applications are open till Feb 11. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "A high school student successfully built an open-source laptop from scratch in six months, and the project is fully available on GitHub. The project gained significant attention, ranking as a top post on Show HN, highlighting discussions on education, talent, and the potential of young engineers. Although not a polished product, the laptop functions as a daily driver, demonstrating the possibilities of open-source hardware development."
    ],
    "points": 2282,
    "commentCount": 273,
    "retryCount": 0,
    "time": 1737578512
  },
  {
    "id": 42796950,
    "title": "How to improve your WFH lighting to reduce eye strain",
    "originLink": "https://rustle.ca/posts/articles/work-from-home-lighting",
    "originBody": "November 17, 2024 How to improve your WFH lighting to reduce eye strain I work from home everyday, I am susceptible to eye strain, eye pain, and dizziness. Having a working environment that’s as easy on my eyes as possible is of critical importance. I hope that by sharing what I've learned, it can be helpful to you if you work from home, and like many, have experienced WFH eye strain. Disclaimer: I'm not an ergonomist or optometrist. These tips are intended to help improve your work from home comfort based on my own personal experience. TLDR An even, diffused lighting environment is best for the eyes When it comes to light brightness, too much is just as problematic as too little Use natural light wherever possible Quality of artificial light matters The best lighting for camera, is not necessarily the best lighting for ergonomics Even the perfect lighting environment will fatigue you — take breaks, and take care of yourself What is it about computer screens that makes them fatiguing? Monitor flicker Most monitors have some level of flicker. This flickering is generally not visually noticeable, but can still be fatiguing. Monitors at lower brightness levels exhibit more flicker as they use PWM dimming (Pulse Width Modulation). In some monitors this is done by changing the ON/OFF intervals, increasing OFF timing, and lowering ON timing — while maintaining the same brightness! This creates an awful strobe-effect, and can be very exhausting — especially when working in an environment where the monitor is the sole light source. High contrast lighting High contrast lighting can exist in a few different formats. It can exist as glare — that is for example light coming in from a window, into a dark room, illuminating a part of your desk. This glare is fatiguing on your eyes because they are accommodating for two levels of brightness simultaneously, while also trying to conduct tasks on a computer monitor — such as reading. High contrast can also exist in a room with bright light on the left, and darkness on the right. This is fatiguing for your eyes to compensate for. Fixed focal distance When working at a computer, most of the time we, and the monitor don't move significantly. We focus at the same distances for long periods of time. This causes our eye muscles to get tight and overworked. This can cause our eyes to get tired, spasm, or sometimes even make it very hard to relax our eyes. Although improved office lighting can help with this, the main way to solve for this problem is to take breaks practicing the 20/20/20 rule: Every 20 minutes, look up from your screen and focus on something approximately 20 feet away for at least 20 seconds. My setup before Daytime Light in the office primarily comes in from the sun. The direct light causes glare, and because of the lack of diffusion, makes for dark corners in the room. Even with a light overhead, it's not enough to equal the amount from outside. This results in high contrast, dark spots behind the monitor, and harsh shadows. Blackout curtains are of little help because they only serve to reduce the size of the light coming in, as opposed to aiding with diffusion. Anytime it's dark outside, it's like working at night. Not fun. Daylight is harsh and direct, creating glare or harsh shadows. The lack of diffused light makes for dark spots in the office. Nighttime Light primarily comes from above. Because of this single light source, it makes for harsh shadows, dark spots, and high contrast. The contrast issue is particularly apparent with the monitor, which although can be dimmed, creates a bright monitor, which also self-shades the wall behind resulting in a dark background. A single light source from above creates harsh shadows, and high contrast. In addition, the light from above is blocked by me sitting in the desk chair. My setup after Daytime Light is much more evenly distributed, diffused, and balanced. Shadows are soft, and the lighting from left to right, and top to bottom is much more even. Nighttime Light levels are now much more balanced, with less blown out light, and less harsh shadows. In addition, corners of the room are illuminated and counter-balance the light from above and from the monitor. Backlighting and desk-lighting help to create a relaxing, low contrast task environment. What's changed Window light sources, and filtering light Windows now have light-filtering curtains, which helps to not only reduce the amount of light on bright days, but also to heavily diffuse the light coming in, which makes for a beautiful glow in the room. In addition, I've installed wood-blinds (not shown) behind the curtains which helps to further control the amount of light entering the office on very bright days. Products used: Sheer curtains • Wood Blinds Wall & ceiling lighting Each corner of the room now has a torchiere lamp, which helps to create an even lighting environment. These lamps point directly up and light the room indirectly through reflections off of the walls and ceiling. In the center of the ceiling, a fabric wrapped light fixture provides diffused lighting. This light is dimmable and can be adjusted to match the amount of light from the standing lamps. Products used: IKEA NOT Lamp (Discontinued but TAGARP is comparable) • Philipps Ultra Definition LED Soft White (Glass) Desk lighting The desk now has even lighting. The brightness of the monitor is now matched by a dimmable flicker-free backlight, which makes working at the display less fatiguing. In addition, a monitor lightbar (not shown), provides indirect light to the desk, reflected off the surface which is completely dimmable and flicker-free. These two lights together help to make for an even lighting environment during the day, and a calm and cozy environment at night. In addition to the two lights, the Apple Studio display has an ambient light sensor, which adjusts the brightness to match the ambient light, which is a game changer for light sensitive people. Products used: BenQ ScreenBar Plus • Warm White LED Strips • WaveForm Flicker-Free LED Dimmer Takeaways An even, diffused lighting environment is best for the eyes. Try to balance out dark/light spots with additional lighting, and diffusing strong light sources — like the sun. When it comes to light brightness, too much is just as problematic as too little. When dealing with too little light, add additional light sources which provide indirect, and reflective lights — such as up-lights, or torchieres, or lights that have a shade. When dealing with too much light — use blinds, sheer curtains, or light filtering curtains. If you have too much light from an artificial source, use lower wattage bulbs, or consider adding a dimmer switch. Use natural light wherever possible. If you've got natural light, use it to your advantage. Natural light is healthier than artificial light — there's no flicker, and it can provide us with much needed vitamin D. For rooms with small windows, use mirrors to help reflect natural light and fill the room. Quality of Artificial Light Matters. Wherever possible seek out high quality lighting that is dimmable and flicker-free. Spending a few extra dollars for these products is worth it, and it will result in less fatigue. For regular bulbs, I recommend Philipps Ultra Definition LED Soft White bulbs which have low flicker rates, glass enclosures, high color rendering index, and a relatively inexpensive pricepoint. The best lighting for camera, is not necessarily the best lighting for ergonomics. When working from home, we spend a lot of time on video calls, and it's natural to want to look our best. Studio lights that attach to our desks, or facing a window outdoor will result in great lighting on camera, but can also cause fatigue. Consider reserving this lighting for calls, or see if generally better room lighting can result in a similar light quality on camera. Even the perfect lighting environment will fatigue you — take breaks, and think beyond lighting! Having a great lighting environment will help you with stamina, but having good work habits like the 20/20/20 rule, and changing up posture and working environments are important in order to take the best care of your eyes, and your body. Think beyond lighting when considering ergonomics, such as room humidity (to limit dry eye), posture, chair position, and stay moving to keep blood flowing. It may seem daunting, but I believe that with some small changes it can make a big difference. I hope this gives you new ideas to try, and I'd love to hear what you are doing in your own office as well! Cheers!",
    "commentLink": "https://news.ycombinator.com/item?id=42796950",
    "commentBody": "How to improve your WFH lighting to reduce eye strain (rustle.ca)570 points by jahfer 22 hours agohidepastfavorite265 comments Karrot_Kream 21 hours agoA lot of the guidelines that are used to light a scene for a camera are also quite useful for lighting a room for yourself, just with less light needed as the human eye has a much higher dynamic range than a camera sensor: * Use diffuse light. This usually means multiple light sources bouncing and diffusing light off surfaces (ceilings, walls, etc) or diffusers. * Minimize shadows. Shadows lead to contrast which can lead to eye strain. Use multiple, maybe directional, light sources to illuminate shadows. * Minimize highlights. Windows without blinds let in lots of light which leads to contrast and can lead to eye strain. Curtains and blinds are great ways to diffuse light. * Uniform color temperature. Try to make sure all your lights have the same color temperature. Small variations are okay but large color temperature variations lead to color contrast which also tends to be hard on eye strain. * Select your color temperature based on needs and feeling. A lot of people prefer warmer color temperature lights and cool temperature lights are known to be more stressful for folks with anxiety-related conditions, but if your work requires accurate color representation, or you find yourself mentally trying to compensate for color temperature, then change the temperature to what you find most productive yet relaxing. * Wall color. Remember that \"white\" light that reflects off colored surfaces will take on a hue similar to the reflected surfaces. Walls of different colors can cause challenges for uniform color temperature, and warm colored walls can take cold lights and turn them warm. A side effect, of course, is that your room will become a lot more photogenic. It's no coincidence since photogenic rooms are often just easiest on the eye to look at. \"Golden Hour\" is considered a great time for photogenic events, photographs, and videos. \"Golden Hour\" lighting tends to be diffuse, not too strong, and warm toned. Humans tend to really like this style of lighting and if you do too, you might want to recreate some of these properties in your office. reply toddmorey 21 hours agoparentIn the history of LED lighting, it took quite a bit of work (hybrid phosphor technology, etc) to get them to emit warm color temperatures. The exact color temp can be a personal preference, but I think a lot of people aren't privy to the difference the color temperature of \"white\" light can make! I want to go on a guerrilla campaign around my neighborhood and replace porch light bulbs with warm equivalents. reply gehwartzen 18 hours agorootparentI just went through this with my car. All the OEM overhead interior lights were horrible 6000k+ type bulbs that not only made the interior feel like a hospital room but completely ruined my night vision turning me into the classic “Jesus!! You’re going to kill someone!” dad whenever my kids would fiddle with them at night. I cannot overstate how much of a difference it made switching them all to 3000k warm whites. Of course instead of being replaceable bulbs they were all SMDs directly soldered onto little PCBs making it a bit of a project … but so worth it! reply davidmurdoch 4 hours agorootparentThis is the level of obsession I aspire to reach one day. I love it! I own a 90s BMW and the interior lighting looks phenomenal in that car. Not great for passenger night reading or applying makeup though. Haha reply ssl-3 2 hours agorootparentprevI've become somewhat expert at identifying cheap prefabulated automotive LED modules on AliExpress that seem unlikely to be problematic, and modifying them to be something other than awful and stupid. It seems like such needless work. All I want are ~3000k LED widgets in various standard-automotive sizes that aren't stupid. What I get are ~3000k LED widgets that are overdriven and painful to be around at night, which also have extra \"CANBUS\" resistors that only serve as heaters. It's all pretty repulsive. Some recentl examples I got consumed 2.6 Watts, and about half of that was deliberately wasted as nothing but heat. (2.6 Watts is quite a lot for a little LED board to dissipate, which results in early failures and flickering and other nonsense.) But with the stupid heater-resistors removed, that dropped to only an eye-burning 1.3 Watts. With the current-limiting resistor swapped to a higher value, power consumption was reduced to a few hundred mW and the light output approximated the relatively-unintrusive incandescent lamp that was originally fitted. And finally, after all of that work of finding and modding the things, I can open the car door at night without becoming blind just like it was the 1990s again and also preserve the battery for things like starting the engine. reply walterbell 15 hours agorootparentprevWell done! How did you figure out 3000K replacement part numbers for each SMD? reply gehwartzen 14 hours agorootparentI happened to find someone on a German forum (cars a VW) that had taken his overheads apart to do the same thing who posted a p/n or rather an industry size. I had never dealt with replacing SMD LEDs before but it seems there is an industry standard sizing that most adhere to. Just ordered a few in that size from Amazon and they worked. From what I remember most in that size were rated for similar voltages so seemed pretty universal. I imagine as long as you can see the led and measure its dimensions finding a replacement should be easy. Desoldering/soldering was a pain as they were tiny; like 2mmx2mm. One cool thing I learned though is that you can use a multimeter in its ‘continuity’ setting to provide enough current to light up these small LEDs which is very convenient when testing if you soldered them correctly. https://www.zgsm-china.com/blog/something-you-should-know-ab... https://www.reddit.com/r/Tiguan/comments/1hq2hae/changed_ove... reply pkolaczk 10 hours agorootparentDesoldering / resoldering is not pain if you have proper tools to do it and know the correct technique. Trying to do it with an iron is making things very hard or even impossible (especially for smds with contacts below not at the sides). Using hot air rework station + preheater and low melt solder paste is the way to go and it’s a piece of cake then. And you can do the whole board at once in a few minutes. I modded most LEDs in my house because 2700-3000K is too warm for me, but 4000K is too cool. So I mix them ;) reply gehwartzen 3 hours agorootparentOh that’s good to know! Yeah, I definitely didn’t do myself any favors trying it with just a cheap unadjustable iron. I didn’t so much desolder the existing LEDs but instead just melted/scraped them off and cleaned the contacts. Was ugly for sure. Do you by chance have any resources (videos or tutorials) to recommend that show how this is best done? reply tecleandor 8 hours agorootparentprevI'm about to order a bunch of PCBs to substitute overhead lights on my car and some of my friends for something around 3000K and high CRI. Not only the color temperature is so cold, but usually the spectrum is terrible, and the quality of the light is awful. I just have to finish choosing the led driver :P reply walterbell 6 hours agorootparent> some of my friends Lucky friends! Maybe repair shops could offer customized interior lighting as an aftermarket service. reply tecleandor 3 hours agorootparentThing is, when you order a small PCB, the price difference between 1 and 20units is usually negligible, so I'd try to find the light sizes for all my friends cars :D Truth is, aftermarket lights for cars (typical AliExpress or Amazon stuff) always focus on brightness, but never on quality. It's usual for taxi drivers to have aftermarket lights in the vehicle, so you can find your money or credit card more easily, but the light quality is always terrible. :P reply teslabox 15 hours agorootparentprevThe bad LEDs can also be improved with yellow or amber film, which is more accessible to most people than desoldering. I found a German company that sent me a free sample of their amber film. As I recall their market was semiconductor manufacturing, but I can’t find them again. Amazon has a variety of films that should work. reply gehwartzen 14 hours agorootparentAbsolutely! That was my initial idea as well. Glad to know it’s an option reply gsck 3 hours agorootparentIf you look towards companies like Rosco or Lee they have hundreds of options for just this application. A sheet of Rosco 3408 takes you from about 5500K to 3800K so a much warmer colour. A small sheet which should be enough goes for about £7 https://shopwl.com/rosco-roscolux-3408-roscosun-1-2-cto/ you can probably find it cheaper just by searching Rosco 3408 reply imp0cat 13 hours agorootparentprevHow do you feel about LED headlights (assuming your car has them)? Great for safety or too bright and tiring? reply close04 8 hours agorootparentNot OP but I think it can be either. Great for the safety of one driver who can see the road perfectly can be awful for the safety of another who is blinded. LED bulbs exacerbate an already existing problem caused by vehicle noses being too tall such that the headlight is exactly eye-height for other drivers. Sometimes they're poorly calibrated too, aiming too high. Many drivers also retrofit super bright LED lights on older cars that do a very bad job at properly focusing the beam on the ground (same thing was happening with drivers modding their old mirror reflector headlights with xenon lamps and giving everyone on the road an artists impression of a supernova up-close). I've seen cars with lights that were obviously very bright but very well calibrated to point as much as possible to the road instead of my eyes. Those are maybe marginally more tiring than the warmer and much dimmer lights used in the past, but this is more than compensated in safety so probably a worthwhile trade. Proper calibration and sensible placing make all the difference. reply ropejumper 4 hours agorootparentThe cold LED headlights also strain your eyes more as the driver. It's much less comfortable to look at IMO. reply close04 1 hour agorootparentUnfortunately I can't compare between 2 modern cars equipped with similar lights save for the color temperature. I haven't seen modern LED headlights with warm color, so when I think of warm color headlights my brain immediately goes to my experience with cars as old as the mid '70s, with filament bulbs. This evokes memories of almost blindly searching for the road right in front of me, compared to today's \"I can see 200m ahead\". So by this measure, not having to squint to see anything ahead, my (aging) eyes feel much more rested and reassured today than decades ago. Are there new cars with warm LED lights? reply RHSeeger 1 hour agorootparentprev> I think a lot of people aren't privy to the difference the color temperature of \"white\" light can make Go out and buy cheap Christmas lights and expensive ones, and put them up next to each there, and show them that. The difference is staggering. To the point where they look dumb if you hang them up together. reply Karrot_Kream 21 hours agorootparentprevIn general I think the population at large, and especially programmer types, know way too little about lighting given how ubiquitous both lighting and photography/videography is in our everyday life. Good on you for fighting the warm light fight! reply Der_Einzige 20 hours agorootparentThey claim to know a lot but then none of them build proper Cf Lumen or F.Lux style tooling (i.e. super easy default instead of requiring tons of setup) for turning ALL blue light off (red shifting) IN THE DEFAULT OS as the primary way to use your device at night. None of that wussy shifting the color temperature brown shit that doesn't meaningfully reduce blue light hazard. I can literally preserve my whole night vision and have zero eye strain in pitch black conditions by red shifting my computer screen. No one supports this because we are stupid. https://en.wikipedia.org/wiki/Biological_effects_of_high-ene... The laser pointer community of all groups understands this stuff well. They recommend green lasers because they use the lowest power for the highest visibility at night. Blue lasers are easy to get super powerful, but unlike a lower powered green one, which usually will only temporarily blind you if you make a mistake a powerful blue laser will straight up destroy your eyes forever. Permanent blindness. reply onli 18 hours agorootparentI don't really get that impression. To the contrary: How dangerous blue light is supposed to be is commonly repeated, with many products for filtering it out, like glasses, and software in all devices that does the red shifting. Android has it by default, Linux does, macOS is Windows missing? Or is there something in the implementation of those measures that you object to? But note that the wikipedia article you link states clearly that the eye strain attributed to blue light has no scientific evidence. That's also what I found out last time I looked into it all those very strong statements about how dangerous blue light emission is supposed to be has no equivalent scientific studies that prove them. I think there was also a prominent article about that here on HN. I think the effect might be completely made up, as it is something that is easy to believe and to subjectively feel that it works, by a placebo effect basically. reply Der_Einzige 57 minutes agorootparentIt's insanity to be told that your claim is basically psudoscience or fake when night vision preservation, the mechanism of prevented eye-strain, is a well confirmed and relied upon part of our biology. Go talk to astronomers. I'm aware of Wikipedia claiming what you say it does. That's an incitement of Wikipedia and the big blue light crowd. Further, actual studies of filteirng blue light needed the right display tech. Before OLED, I couldn't actually \"turn off the non red pixels\", and thus never fully eliminated blue light from my display. Most of the \"science\" around blue light is trash. Yes, I say this as someone who actively published at top AI conferences and is intimately familiar with peer review. I'm going to keep on preserving my night vision and browsing the internet for hours with zero issues at night while being told that my red shifted phone is placebo. You try turning your OLED phone on at full brightness in the middle of the night with and without the red-shift and tell me which hurts your eyes more... reply teslabox 15 hours agorootparentprevThere are old studies on the hazard of blue light. I found a few of them on https://scholar.google.com I suspect they’re hard to find if you don’t know about specific keywords to include in your query. The blue light problem has to do with how high energy blue photons interact with specific substances found in minute quantities in our eyes, leading to inflammation. reply c0balt 20 hours agorootparentprev> They claim to know a lot but then none of them build proper Cf Lumen or F.Lux style tooling (i.e. super easy default instead of requiring tons of setup) for turning ALL blue light off (red shifting) IN THE DEFAULT OS as the primary way to use your device at night. Probably not what you are aiming for but for Linux this is a somewhat solved problems. The default DM, Gnome, has built-in color shift with a time schedule AFAIK. For Wayland and xorg there are also numerous other solutions that do a display-level redshift, I can recommend wl-sunset for Wayland. reply rubicks 16 hours agorootparentCan confirm https://github.com/jonls/redshift works great; been using it for years. reply scns 19 hours agorootparentprevKDE does too reply Karrot_Kream 20 hours agorootparentprev> The laser pointer community of all groups understands this stuff well This has been my major frustration also. Different enthusiast communities all discover these things independently and talk about them in their own domain language. Night photography and astral photography talks about red shifting as well. reply vijucat 9 hours agorootparentprevI literally feel sad when I turn off the blue light (using f.lux) or turn on eye saver mode on my monitor. No idea how to fix this. reply Dylan16807 12 hours agorootparentprevWhat do I need to preserve night vision for? Unless I'm using my computer to help stargaze, I would rather have color graphics. reply amarcheschi 19 hours agorootparentprevAlong light temperature there's light cri as well, which measures how faithful to real life a light is. New standards such as Tm30 came out recently as well reply gsck 2 hours agorootparentCRI is usually only reported on professional grade lamps (Theatre, Film/TV) you'll almost never find it reported on anything below that. reply AuryGlenz 1 hour agorootparentSome consumer bulbs at least used to have it listed on their packaging. reply esafak 5 hours agorootparentprevIt's hard enough to get the CRI out of manufacturers. I've never seen any other measure of light quality reporter ever. reply davidmurdoch 4 hours agorootparentWaveform lighting publishes their reports. reply wyager 21 hours agorootparentprevWhile the color temperature situation has improved, the actual spectral quality of most consumer LED lighting still leaves a lot to be desired. CRI makes an effort to measure this, although it's a low-granularity measurement. I personally buy surplus cinema lighting equipment and use it to light my house. I have a bunch of fancy cinematic LEDs with high CRIs that produce decent light, although they still can't fully compete with tungsten bulbs (e.g. Arrilite series) Ideally you want to be looking for something with a color temperature in the 3500K (mid-morning) to 6500K (clear blue noon day) range and a CRI of 95+. Also bug manufacturers to start using better color quality metrics like TM-30. reply wonger_ 20 hours agorootparentWhere do you buy the surplus cinema lighting equipment? And how do you mount it inside your house? I went down a high CRI office lighting rabbit hole last year and I could only find expensive, new photography lights. reply nicoburns 17 hours agorootparentThese guys do very high quality lighting aimed at homes and offices: https://www.waveformlighting.com/ reply namibj 15 hours agorootparentprevJust get modules or strip from Yuju; I have zero bad experience over many years with them. Sure, it's not cheap, but IMO ~1$/W plus simple PSU (either some standard voltage, or dimmable current, depending on what you got) for good and 1.5~2$/W for very good CRI is quite fine. reply Xcelerate 7 hours agorootparentYep. I’ve been replacing all the bulbs in my house with Yuji SunWave, and the difference is astonishing. I just put in four of their PAR30 4000K bulbs above my kitchen island, and the first time I turned the lights on at night I gasped at how much it looked like sunlight. (Then again, my wife said she couldn’t tell much of a difference and suggested my reaction was possibly confirmation bias, so YMMV.) They’re quite expensive compared to normal LED bulbs, but it’s hard to argue against the quality of life improvement, particularly in the winter where I work remotely from a home office all day. I was actually considering setting up an high brightness array of their full spectrum lighting above my desk. I know the article mentions diffuse lighting is best, but for some odd reason I prefer “spotlight” style. I don’t know why—it just feels cozier to me. One thing I’ve noticed as I’ve learned more about lighting is that the brighter the light, the better cool color temperatures start to look. For a long time I hated anything above 3000K (too “office-like”) but 4000K actually starts looking pretty good around ~7,000 lumens directly overhead, and I imagine 5000K would look alright above 15,000 lumens or so (perceived brightness is a logarithmic function of luminance). IMO 5000K looks downright ghastly in a home setting for the typical range of 500-800 lumens that most bulbs sell at nowadays. reply wyager 2 hours agorootparentprevYuji looks quite good, thanks. I need to see if I can integrate this with my DMX/Zigbee lighting control. reply wyager 2 hours agorootparentprevCraigslist. I just have a bunch of alerts for various cinema lighting brands (among other industrial goods I like using in my house, like pelican cases). For mounting, it depends. Some things, like Pavotubes, come with screw-in mounting solutions. Other stuff may require dedicated photo mounting hardware. Manfrotto Superclamps and similar are a great way to mount lights to random pipes, beams, etc. Also, most cinema lighting supports DMX control, which is nice for automation. It's harder to set up than Zigbee, but it works a lot better once you get it set up. That bit is definitely not consumer-friendly though. reply imp0cat 13 hours agorootparentprevAlso, MediaLight provide great high CRI options bias lighting for your TV or monitor, further reducing your eyestrain when watching TV at night: https://www.biaslighting.com/ reply toddmorey 18 hours agorootparentprevusing surplus cinema lighting equipment is genius. reply wyager 2 hours agorootparentIf you're patient, it's crazy how much money you can save off of new pricing. I frequently see discounts north of 75% off new price for stuff in pretty good condition. reply BenoitEssiambre 16 hours agoparentprevI'm in a basement and had fairly good success with certain \"led shop lights\" to make the space feel more like it has natural light (though I could improve things further). I do find however, that using diffuse lighting and minimizing shadows is what makes lighting feel artificial. Natural sunlight has very parallel rays that create very evenly lit surfaces with very sharp shadows. Whereas most artificial light has radial rays around the light source that diminish in intensity with the cube of the distance and thus create fading gradients on surfaces and weak shadows especially if you have many of them. I find I miss the feel of parallel rays of natural light so I try to find lights that have for example, parabolic reflectors to make the rays more parallel and position a few of them farther away from me pointed towards my visual field to try to replicate natural sunlight coming through a window. One challenge however, is that it's surprisingly difficult to get good parallel rays from artificial light sources because of conservation of etendue ( https://en.wikipedia.org/wiki/Etendue ). I wish there were more options on the market. reply Aerroon 6 hours agorootparentI've been wondering for a while now if our indoor lighting is too dim. Even in the shade during midday you're looking at 20,000 lux. Overcast days look dark and dreary and they're on the 1000-2000 lux range. Meanwhile home lighting is well below the 500 lux range, if not even in the 100-200. I even suspect that the current near-sightedness epidemic is caused by people spending too much time in dim lighting. Maybe if our indoor lighting was brighter our eyesight would not adapt to become near-sighted as much. reply BenoitEssiambre 16 hours agorootparentprevWhat I'm trying to achieve is lighting a bit like in these \"Artificial Skylight\" concepts: https://architizer.com/blog/practice/materials/let-there-be-... They claim their technology is about \"Rayleigh scattering\" but I think that's mostly marketing mumbo jumbo. Sure that might add a bit of diffuse blue but their main secret, I would bet, is parallel rays which explains why their fixtures require so much space above the ceiling to get around conservation of etendue. The effect of parallel rays and sharp shadows are definitely represented in their example pictures. reply bmelton 5 hours agorootparentI'd wager they're pretty good. If you can leverage a parabolic reflector at a light's focal length, you can get them to collimate in a way that approximates the sun by appearing to be pinned near-infinitely far away. The reflector is generally what takes up the space, but it's necessary(ish) you could approximate the effect by using a fresnel lens at the light's focal length, but it yields a smaller light with rainbowing effects. [This video](https://www.youtube.com/watch?v=6bqBsHSwPgw&t=843s) does a good job of showing how to make your own, as well as illustrating how good the effect is, while also demonstrating why space-hog implementations are likely to yield good results and slimmer ones not. reply nothrabannosir 1 hour agorootparentprevReminds me of this old dyi video about using decommissioned led monitors to create light boxes with parallel rays to emulate sunlight: https://www.youtube.com/watch?v=8JrqH2oOTK4 reply vvladymyrov 15 hours agorootparentprevI’ve installed 4 similar skylight lamps to the basement. Bought them on Amazon for 700$ each (had to replace 1 because it had uneven light). They make basement light much closer to ourdoor light and made basement very cozy and comfortable. I had depth space in the ceiling, but width of the lamp didn’t fit between joists so could not push lamp all the way at the ceiling level. End up building nice boxing around lamp. So it was medium complexity project with great result. reply sumea 20 hours agoparentprevI am quite sensitive to glare. I have tried many setups in my windowless office with low ceiling height and have found linear up-down pendant lights the best option. Up-light is more important as it bounces soft light from ceiling. When I want to work in dimmer environment in the evenings, I switch off the down-light. I also try to buy lightning fixtures that are designed anti-glare although they are more expensive. You can also make pendant lights yourself with led strips and aluminium profiles. Your eyesight and glasses also matter a lot. My glasses are quite worn with lots of scratches. They definitely make issues worse. reply walterbell 15 hours agorootparentGlasses are extremely affordable online (e.g. eyebuydirect, owned by the same multi-billion dollar company that sells lenses and frames to local opticians) if you have the prescription, PD (pupil distance) and physical frame measurements of your current glasses. reply anilakar 8 hours agoparentprevDiffused bright light from a white wall behind you is not any better. Learned this the hard way when I started WFH in Anno Covidi 1. reply bongodongobob 18 hours agoparentprevI agree up until golden hour. It's a very specific style of lighting and isn't any better or worse. It's not the best time to take a picture, it doesn't have the best light. It's a specific kind of light. reply AuryGlenz 1 hour agorootparentEh, I can tell you I definitely had more sales of portrait prints at golden hour for my high school senior sessions. You can, of course, make great photos in any light. Golden hour and blue hour are easy because the light is inherently nice. reply Dalewyn 16 hours agoparentprev>* Select your color temperature based on needs and feeling. A lot of people prefer warmer color temperature lights and cool temperature lights are known to be more stressful for folks with anxiety-related conditions, but if your work requires accurate color representation, or you find yourself mentally trying to compensate for color temperature, then change the temperature to what you find most productive yet relaxing. This is huge. I personally can not stand warm color temperatures in my office or bedroom, I know all the colors are off and it aggravates me to no end. On the other hand, I can't stand cool color temperatures in the dining room because it doesn't lend well to eating comfortably. And in either case, I can't stand warm color temperatures on any monitors/screens or televisions because of the same reason as when I'm in my office or bedroom. Use color temperatures that suit your preferences, going against them just because someone says it's better for you (eg: red shift at night, aka blue light fearmongering) is patented bullshit. reply camhart 22 hours agoprevA big thing not often spoken about with eye strain is dry eye caused by the lack of blinking due to focusing on screens too close to our face. This is an evolutionary phenomenon--close dangers cause extreme focus without blinking. Extreme focus on close items reduces our blinks. Our eye lids have glands in them that release oils on your eye with each blink. These oils help prevent the watery part of your tears from evaporating. When it evaporates your eyes dry out causing discomfort and potentially pain. If you don't blink enough, the oil doesnt get on your eyes and eventually, in extreme cases, the glands can even die. A lack of oil in tears can cause extreme eye fatigue and even pain. This is why dry eyes is on the rise. Remember to blink! I actually built a little web app to count my blinks. See https://dryeyestuff.com/. Not perfect, just a prototype. 100% free. reply Joeri 10 hours agoparentI used to have this dry eye problem a lot, but turning down the brightness of the display really helped with that. The eyes can adapt to very low settings, almost at the bottom of the range on macs at night for example. I find it is also important that whatever is behind the screen is lit indirectly equally to the brightness of the display. A bright screen in front of a dark wall is a perfect recipe for dry eyes for me. reply mh 2 hours agoparentprevThis is a great idea, and it seems surprisingly accurate. I know it's a prototype, but in case you're interested in feature requests: if I have multiple webcams, it seems to just choose the first one without a way to select another. reply humblepie 18 hours agoparentprevI, too, experienced dry eyes and found it challenging to consciously blink regularly. A few years ago, someone gifted me one of these \"3-D puzzles\" (similar to this: https://www.amazon.ca/Bookend-Miniature-Bookshelf-Birthday-B...). I kept it on my desk and it helped me somewhat regulate my constant focus on the screen by prompting me to glance at it occasionally. That's just something that worked for me. reply freefaler 20 hours agoparentprevbtw, if you have 2 different monitors on different lengths from your eye you change focus more frequently and your eyes feel better. A laptop + big monitor is less irritating for the eyes as long as they aren't put exactly on the same line. reply mjcohen 17 hours agorootparentWhen you get old enough, your eyes stop focussing. reply amadeusw 19 hours agoparentprevOptometrist recommended I take daily fish oil and give it a month to see result. Sure enough, roughly a month later, I stopped having dry eyes. My eyes feel good even now during winter, when both outside and inside air is quite dry. reply dmd 4 hours agoparentprevCool web app but doesn't work unless I take my glasses off, sadly. reply amelius 21 hours agoparentprevInteresting. Could an external stimulus trigger a subconscious blink? reply chasd00 21 hours agorootparent> Could an external stimulus trigger a subconscious blink? you could set something up where a water gun squirts you on a random interval between 2-5 minutes. heh i think i would kill someone if they did that to me. reply camhart 21 hours agorootparentprevI'm not aware of any triggers to cause subconscious blinking. That'd be fantastic if there was an option though. The web app can trigger a notification if your blinks / minute drop too low. Only challenge is modern browsers throttle websites that aren't visible, so the blink counting gets messed up. reply amelius 21 hours agorootparentMaybe you could set up the conditions for a Pavlovian response. E.g. let your app give a signal (e.g. beep or buzz) every 30 seconds if you don't blink. Then train yourself to blink if you hear the signal. Edit: Yes, it can be done: https://en.wikipedia.org/wiki/Eyeblink_conditioning reply DonHopkins 18 hours agorootparentYou could also lie less often, because you blink less frequently when you lie. ;) https://ris.utwente.nl/ws/files/23166678/HICSS47_Do_Liars_Bl... reply dpig_ 17 hours agorootparentprevSomeone just shared an extension that inserts random jump scares onto social media sites that you want to avoid. Maybe adding jump scares to work applications can help people blink more often, too. reply Nevermark 19 hours agoprevI once used shop lights aimed at my ceiling to get through a winter, while avoiding depression. Four pairs on stands around the room, behind the furniture aimed at a vaulted ceiling. It worked very well. Every day felt like summer. I quickly learned to turn off the sun and go back to regular lighting around 5pm. In my next house, where I am now, I have large cove molding rectangles with recessed bands of LED lighting, all bouncing off the ceiling. It’s great, because it’s really bright, but so even. Like a good day outside. You feel very awake, alert, & energized, but it is very relaxing too. They dim, but perhaps for the same reasons as the article mentioned, that isn’t always as relaxing. So I have different accent lighting & lamps in each room to create different evening moods. For working at home, for many years, the combination has been great. reply michaelteter 22 hours agoprevThis is another +1 for WFH. Many office environments have terrible lighting, and there's very little you can do about it. reply toddmorey 21 hours agoparentI remember standing up on office chairs to twist the florescent tube bulbs above my desk just enough to turn them off. That light was just awful. It helped a lot until maintenance would come in at night and \"fix\" the light and I'd have to do it all over. reply y-c-o-m-b 1 hour agorootparentI did the same thing and it was good for a long time, then one day I came in to find the maintenance people replacing it thinking it was broken instead of just twisting it back. I guess they weren't too bright (lol). reply xdennis 2 hours agorootparentprevCould be worse. I worked in an old building in London which I think used to be a warehouse. It was renovated but the lighting was very low and they said they couldn't install stronger diffused lighting because of the local code. They eventually added hanging spotlights. Very bright and very narrow. But they would always end up shining right into someone's eyes. Every once in a while someone got annoyed by one and would rotate it in some other direction, only to piss someone else off. reply therealdrag0 14 hours agorootparentprevSame. Though mine got left off for a long time which was great. reply bunderbunder 21 hours agoparentprevThe over-bright lighting, monitor glare, and eye strain are a favorite conversation topic at my workplace since RTO. reply mjcohen 17 hours agoparentprevMy eyes are sensitive to glare, so in my last job I took to wearing the visor I wore outside also inside. Got some weird looks but it was a lot easier on my eyes. reply matsemann 6 hours agoparentprevHad a coworker that could notice the flickering from the bulbs (fluorescent?). I can't see it, but for those who can it's apparently really jarring. reply benoliver999 11 hours agoparentprevOh god they just replaced our horrible fluorescent tubes with dazzling white LED. From bad to worse. reply thanatos519 20 hours agoparentprevAmen. I wear a ballcap at the office. reply Twirrim 20 hours agoparentprevOurs are absolutely atrocious. I forgot just how bad it got until they did a big office move a couple of months ago. Previous to that I'd been way out on the edge with large windows behind me (with some shading film on them). My move now put me in the centre of the floor with barely a window in visible range, stuck under these godawful, far too bright lights. The first day in that space reminded me just how much I'd hated that aspect of things before the pandemic. reply bensandcastle 21 hours agoprevThe natural light and diffuse light are good tips. Next is to get a big screen eg. 85\" 4K and put it 1.5m away. That should be your main display. I don't have that all the time, but then I get some variety, 85\" @ 1.5m a lot of the time, laptop some of the time, driving/walking etc. for longer range. 1.5m is the midpoint of focus for the muscles in your eyes. I built augmented reality displays and this was the focal plane we selected for to minimize eye strain and the felt sense of vergence/accommodation conflict. We could then throw graphics as close as ~30cm, or at infinity using vergence adjustments, even though the accommodation was at a fixed 1.5m. Graphics felt best at that distance, but they also felt ok in the range 0.5-10m, which suited nearly all productivity scenarios. reply frabert 21 hours agoparent85\" @ 1.5m is insanely big for me, do you not get sore having to dart your eyes about to read the corners? reply JumpCrisscross 21 hours agorootparent> do you not get sore having to dart your eyes about to read the corners? I wouldn't be surprised if this is what makes it healthier. Not only are you exercising your eyes, you're also giving them a chance to let you know when they need a break. reply syndicatedjelly 16 hours agorootparentYep, I set my mouse sensitivity to extremely low as well so that I’m basically “wiping the windshield” of my desk while using my computer reply stevenAthompson 20 hours agorootparentprevI do almost exactly this, but instead use a cheapish 65\" 4k/60hz TV instead. I can see bits of my surroundings with my peripheral vision, but only with the parts of my vision that are already blurry. I suspect that 85\" was chosen to maximize immersion for gamers (cover the entire field of view), rather than to minimize eyestrain. For me doing development work on a 65\" from about 1.5m is close to ideal. reply gehwartzen 17 hours agorootparentI have a 4k projector setup which I use from time to time instead of my regular 27” monitor. It makes a pretty big difference especially as it is reflected light. I can sit in front of it for hours without any noticeable eye strain. It’s fine for doing graphical work or web browsing but really not ideal for things like code or excel as I lose my place too often with my eyes having to dart around further. Might just be sitting too close though. reply neves 21 hours agoparentprevI made a glasses with the focus optimized for 1.5m. A lot easier on my eyes for working than my progressive lenses. reply stevenAthompson 20 hours agorootparentHow did you go about that? Can you just take your prescription someplace and have them made? reply zhengyi13 19 hours agorootparentTell your optometrist you want that, and they'll write the prescription. Last I went, they wrote me two different prescriptions: one with an up-close focal point for books/screen work, and a second one with the focal point further out for driving. reply neves 4 hours agorootparentprevYes. You can have reading lenses or lenses for medium distance. Measure the distance from your seat to the screen. Get a max, minimum and most common distance in your different seating positions. reply DavidVoid 20 hours agorootparentprevNot OP but look into \"computer glasses\". Usually they're optimized for distances a bit closer than 1.5 meters, but I'm sure that can be changed. reply mattclarkdotnet 16 hours agorootparentThe magic words I think are “occupational multifocals”, which have a small area at the top for distance vision and the remaining 75% set for whatever your occupation is. You need to be very specific on the distance setting reply neves 4 hours agorootparentYes. This is the better solution. Some expensive lenses like Zeiss, Varilux, and Hoya make you an extra pair when you by one. I tought that the extra pair must be equal the first one, but it isn't necessary. I could have asked for a general one and one with a greater area for medium distance. reply bongodongobob 18 hours agorootparentprevI used to work in the optical industry, that's not what computer glasses are, they are just blue blocker lenses. reply walterbell 17 hours agorootparentAny lens can be customized for desired focal length, including distance, intermediate (e.g. computer) or closeup/reading. reply bongodongobob 4 hours agorootparentRight. It's an industry term though and doesn't have anything to do with focal length. reply jwr 15 hours agoparentprevI would disagree with 1.5m, but I do recommend checking how close you are to your monitor. Let's assume you have a reasonably sized office monitor (27\" or so). Extend your arm with your hand as a fist, forward. If your screen is closer to you than your knuckles, it's too close. Now, for the height: all monitor stands are too low. If you keep your head straight and look at the monitor, you should be looking at the upper third. VESA mounting arms or monitor stands solve this problem. reply qweiopqweiop123 10 hours agorootparentI've always heard you should be in line with the top of your monitor when looking straight ahead. A quick google has says the said. Either way we're on the same lines too many people are looking up too much when looking at the monitor. It takes more effort than looking down. reply Izkata 1 hour agorootparentGP said too low, not too high. I'm thinking a typo though because that doesn't make much sense to me and I agree with you I always have to adjust monitors as low as they can go in the office. reply imp0cat 13 hours agorootparentprevall monitor stands are too low. If you keep your head straight and look at the monitor, you should be looking at the upper third. If the stand were too low and were to replace it with a higher one, you would then be looking at the lower part of the monitor and that is better? reply nluken 21 hours agoprevMany people overlook how spaces are lit from an aesthetic perspective as well as a from the functional perspective this article is written from. Lamps and other eye-level lighting sources do more than just help eye strain as the article suggests; they also work wonders from an interior design perspective, and make spaces feel way more livable. I always find homes overly reliant on overhead lighting struggle to shake the more sterile feel of offices, where overhead dominates. reply empressplay 18 hours agoparentYou can have both though, overhead white lighting for the day and lower temperature lamps for the evening. It's not a binary choice. reply jedberg 1 hour agoprev> and it can provide us with much needed vitamin D Sunlight through your windows won't have any effect on your Vitamin D. The Vitamin D is made by your body in response to UVB radiation, which you get when exposed to the sun. But most windows made in the last 50 years block UVB. reply animal531 10 hours agoprevI have always had great eyes with no problems, I didn't have to pay attention to any articles such as these because my eyes just did their job and everything was fine. BUT then I started working for myself and discovered that I had a pretty severe case of ADD for which I started taking medication. That caused quite a bit of dry eye, some because of the medication and some because I would just blink less. Shortly after that I also hit that age close to 50 where your eyes just degrade rapidly and stop focusing on close objects causing even more eye strain. In about the span of a week I went from \"normal\" eyes to extreme eye strain, pain and light sensitivity. I've had to make some big changes since then. I went for an eye checkup and got some glasses to help with the close vision. I also changed all my code, tools and web sites to dark mode where I can and turned down the brightness of all my displays a bit. I'm also taking a lot of eye drops and edited my work timer app to just beep every defined amount of time (around 20 minutes) so I could take a quick break to stare into the distance. Also changing viewing distance helps, for example if I'm looking at something on Youtube then getting up and standing 2-3 meters away from my desk helps your eyes rest a bit. I still need to look into lighting since I'm also sitting next to a big sliding glass door, so this article should come in handy there. reply mmcconnell1618 5 hours agoprevIf you do a lot of video calls, you can also consider how to light yourself so that you show up well on camera. The traditional three-point lighting setup is a good place to start. A key light which is the brightest, main light source, a fill light which is from the opposite side to soften shadows and a backlight to help you stand out from backgrounds. https://en.wikipedia.org/wiki/Three-point_lighting reply qwery 8 hours agoprev0) Yes, an evenly lit workspace is essential for any work. 0.1) Buy more lamps nothing fancy, ideally second hand, always with the common affordable household light socket. By the time you have the lighting arranged how you want, you may have too many lamps. At that point you can use your army of lamps to inform bigger (& more permanent) spending decisions. 1) If you have a regular occurrence of eyestrain or itchy or sore eyes particularly if you don't already wear glasses and don't think you're just spending too much time working (not sleeping) go to an optometrist and get your eyes/vision checked. Your eye muscles \"expect\" your eyes to be within spec and will work harder and harder to focus even if your eyes are wobbly and largely unfocusable like mine. 2) If your monitor's backlight flickers at a low enough frequency[0] that it's a problem get a new monitor[1]. If you spend enough time using your monitor that the eyestrain is real, upgrading your monitor is a no-brainer. 2.1) Spend time calibrating and adjusting your display/s, whatever it is. 3) Pay attention to how text is being rendered, be picky about it and change settings and fonts. Using all the anti-aliasing and hinting features is not always better. 3.1) Prefer light backgrounds with dark text. Your eyes have an easier time focusing with this configuration. If a light background is too bright to look at, you need to add light to the room. Understand that I put this point last because it is less significant than the others. [0] flicker, PWM: would love to see some research on that, by the way. Does the switching frequency matter? (it certainly does for my hearing) [1] FWIW, my general recommendation for serious, but not too serious, quality-cost sweet spot monitors is: IPS, 2560x1440, 27\", high refresh rate (i.e. ~120 Hz) this comes with some risk of gamer-knife gun mount greeblies, of course. reply hirvi74 20 hours agoprevIs no lighting considered too little? I like to sit in the dark with nothing but a computer monitor for light. I have noticed this greatly reduces eye-strain and distractions, but perhaps that is just me. reply Sohcahtoa82 20 hours agoparentYou're an exception. For most people, sitting in the dark with only their monitor giving off light massively increases eye-strain. reply hirvi74 3 hours agorootparentI do not wear eye glasses nor contacts, so I wonder if this is a difference maker? I have noticed people who wear either tend to complain about eye-strain more in general. reply devenson 2 hours agorootparentWhen I was younger, dark room, dark screen was fine. Gets harder to focus as you age. Bright screen and room cause smaller pupils and therefore easier focusing like a pinhole camera. reply dekhn 19 hours agoparentprevFor me it really depends on the situation. When I'm gaming or watching a movie, I prefer effectively no light. In fact, I built an enclosed space in my garage that is light tight. Then I have an overhead LED strip with variable intensity control. I find that programming and video calls both work best if I have a reasonable level (about 50% of what a typical office overhead would provide). I absolutely love complete darkness but after a while I start to feel like a cave troll. reply Kiro 20 hours agoparentprevSame here. Sounds like people are describing their yoga studios in this thread. Not for me. reply DoingIsLearning 10 hours agorootparentA lot of it is learned behaviour. You get used to working and being productive at night because it's often when you have free time and quiet time. Your brain then starts associating that light setting/environment with 'in the zone' mental states. As an example I used to be a midnight owl but now with kids I have to make the most of my daytime hours and squeeze as much sleep as I can in the night hours. I totally get the warmth of working the night hours but I am also fine with being in the zone during the daytime. You can shape your brain to get used to anything. reply rusty-ux 19 hours agoparentprevDo what works for you. If it’s comfortable and you don’t notice lingering issues it’s probably fine. reply smrtinsert 20 hours agoparentprevThis used to work for me as well, but one day my eyes just gave out. I now need a setup similar to the article. I use two diffuse light sources at corners in the room, with BenQ light bar in front, and (almost most importantly) a warm lamp backlighting the monitor. I might replace the backlight with an RGB bias light of some sort, but it has to be there or everything gets painful fast. reply freefaler 20 hours agorootparentYeah, light behind the monitor is a great idea. Reduces contrast between the bright monitor and dark background. I've used this for years and it makes working during the evening so much better. reply notsydonia 10 hours agoprevI appreciated this article and the intention behind it but I don't think your body makes vitamin D from being in a sunny room you need to be outdoors with the Sun's rays beaming onto your bare skin. I also cannot believe it's 2025, we have a lot of excessive features stuffed into practically every platform or tech product but nobody has come up with a genuinely healthier monitor/screen solution. LED back lighting or ceiling kills me, I hate it. reply qwery 7 hours agoparent> I don't think your body makes vitamin D from being in a sunny room This is correct, assuming the sunlight is being filtered through windows. The glass (or other materials) filters out the wavelengths (UV-B) that does the thing. reply esafak 5 hours agoprevUser-tested high CRI bulbs: https://docs.google.com/spreadsheets/d/12jj1A6PNjHmWbFNu0FSi... EPA Energy Star qualified bulbs: https://downloads.energystar.gov/bi/qplist/Lamps_Qualified_P... reply jjcm 20 hours agoprevOne slightly different approach I haven't seen mentioned here I use grow lights in my office. They're way brighter, so simulate sunlight better, are full spectrum, and as a bonus my plants are super happy. I use 5 of these with a diffuser: https://www.amazon.com/SANSI-Daylight-Spectrum-Sunlight-Gree... reply stevenAthompson 20 hours agoparentWhat do you use as the diffuser? I also use these, but they're a bit too bright and I find myself shutting them off because the light isn't diffuse enough. reply sebmellen 13 hours agoparentprevDo you wear sunscreen? reply marcyb5st 22 hours agoprevFor me the revolution was to get a rather expensive monitor [1] with a great reader mode that lessen the flicker considerably. When I game with it and I turn off reader mode my eyes really feel the difference even though I spend way less time gaming than working. I wonder if OLEDs will be even better since they shouldn't flicker during productivity (from what I read at least). [1] was one of the LG ultrawide 34\" but it was a few years ago and can't find it anymore. reply homebrewer 21 hours agoparentThere shouldn't be any flicker in any mode if you avoid monitors that use PWM for backlight control (which anyone who's sensitive to flicker probably should do). I was stupid enough to buy one with low frequency PWM (didn't have money for anything else at that moment) and \"solved\" the problem by setting brightness to 100% (which sets duty cycle to 100%), but it destroyed image quality. reply wvenable 22 hours agoparentprevIt might be time for me to upgrade my monitors (they are very old). I never thought too much about flicker, etc. Does anyone have any good recommendations for just decent 24\" monitors for coding? reply zonkerdonker 21 hours agorootparentIf you've got the budget for it, e-ink monitors are becoming more available recently, and I've heard that they make a huge difference for eye strain. There are still only a few companies manufacturing panels that large, and I think the framerate can be pretty jarring for anything other than text, but I have been keeping an eye out for when the prices drop. reply lazerwalker 19 hours agorootparentFor most people, using e-ink for general-purpose computing tasks is going to be so jarring and unpleasant that it's extremely difficult to recommend to anyone who doesn't have severe eyestrain issues and has tried and failed more typical accommodations. I adore e-ink for reading, and own several e-ink readers in various form factors, but the tradeoffs just don't make sense for a desktop computer for most people unless staring at your monitor for eight hours a day is causing you physical harm. reply skirmish 17 hours agorootparentprevMaybe ask on Monitors Reddit [1], people there are usually helpful (although often very gaming-oriented, so you should make it very clear it is for coding/office work). [1] https://old.reddit.com/r/Monitors/ reply energy123 17 hours agorootparentprevOne regret I have is buying a monitor that lacks a very high brightness setting. Not good if you have a sun-lit room. reply marcyb5st 22 hours agorootparentprevNot me, sorry. At the time I did watch a bunch of reviews on YouTube, but since the purchase I haven't kept up reply lawn 21 hours agorootparentprevI've been considering the \"programmer\" monitors from BenQ, but I don't know if they truly help. [0]: https://www.benq.com/en-us/monitor/programming/rd280u.html reply Dylan16807 12 hours agoparentprevLots of OLEDs use PWM, some at high frequencies and some not. reply hammock 20 hours agoparentprevWhat is reader mode and how do I activate it? Is reader mode a low refresh rate? reply zyxin 19 hours agorootparentMight just be normal mode? I know that some gaming monitors strobe the backlight in order to reduce motion blur, maybe reader mode just turns that off. reply waffletower 2 hours agoprevDark mode FTW. Would be nice if Hacker News had one. reply derac 2 hours agoparentdark reader works well for it reply wzyoi 19 hours agoprevI was wondering how practical it is to create 3D renders of your room instead of photos for the blog post. I think I found an answer: in this case, it's insanely practical. The reason is it's a blog of a senior designer at Shopify. He has the skills to make this easy for him, and showcasing them is smart. reply t0bia_s 10 hours agoprevVery important number about LED lights is CRI. If this number missing, I would not buy it. Typically >80 is ok, ideally >90. https://en.m.wikipedia.org/wiki/Color_rendering_index reply Winblows11 7 hours agoparentFor those in UK you can get 98 CRI lighting from this company: https://www.biaslighting.co.uk/collections/medialight-mk2 Not cheap compared to the usual Amazon Chinese stuff though reply sebmellen 13 hours agoprevBuy a Godox LA 200D light, a tall light stand on a tripod, set it up in the corner of your room, and point it up at your ceiling at an angle. It’s like having indoor daylight. reply lo_zamoyski 32 minutes agoprevI find that having a desk up against the wall (as in the diagram) makes lighting odd, and creates a claustrophobic feeling (like you're being punished by being made to face the wall or a corner). Facing away from doors is also psychologically unsettling, because people can sneak up on you without warning. This can cause distraction, as part of your attention to be pulled away from the task at hand by the vigilance of your subconscious. reply jen729w 21 hours agoprevThis is worth considering in the context of driving at night. I used to get terrible eye strain, causing fatigue and sleepiness. Obviously not good. Then I bought a Saab 900. It had a 'night mode' that disabled all dials except the speedo. The lights went off and the dial went down. (It came back on if it needed to alert you of something, say low fuel.) [0] This made a radical difference, and led me to the dash-brightness dial that nobody ever touches. Turns out if you turn that way down, reducing the contrast between you and the road, it's actually enough to get you 90% there. Because you probably don't drive a 1992 Saab (more's the pity). [0]: https://www.youtube.com/watch?v=KIfzUqYEkiw reply iforgot22 20 hours agoparentMy Crown Vic can be pretty dark on the inside. The problem with this is the outside light sources like other headlights, especially with certain cars like Teslas that are super bright and aim high. If anything, I feel less eye strain with more lights inside. But either way, I get a headache any time I drive for >2hr at night, haven't found any solution. reply Sohcahtoa82 20 hours agoparentprevAre there any cars that DON'T have a way to adjust the brightness of the dash lighting? Every car I've ever owned (86 Chrysler, 2000 Suzuki, 2016 Subaru, 2019 Tesla) had a dial to adjust the brightness of the dashboard lighting, or in the case of the Tesla, adjust the brightness of the screen. Sure, I couldn't completely shut anything off like your Saab, but I could easily turn the brightness down pretty dim. reply smileysteve 20 hours agoparentprevRelated, the color of your dash matters, the modern backlit screens (both dash and entertainment) emit much white and blue light. Mazda and Bmw (traditionally, less the last 5 years) seem to consider human ux more than others hence orange lighting. Modern ambient (orange or green) can be nice. reply imp0cat 13 hours agorootparentYou can usually turn the infotainment screen off completely at night. However, for some manufacturers, this means the screen displays black, but is still powered, ie. the backlight is on, which can be annoing. reply smileysteve 54 minutes agorootparentThere is a big difference between \"can turn off\" and \"doesn't emit white or blue light in the cabin at night by default\" when it comes to infotainment. Maybe the oleds will allow manufacturers become more conscious of this in the future, but as far as I can see, BMW, Ford (Cadillac), Mb group, and Kia Hyundai, Honda are not consciously limiting their lighting colors. reply damnever 9 hours agoprevI don't wear any glasses, but I still suffer from eye strain, especially from when I started my software engineering work years ago. I have also tried a lot of devices and medicines, but working long hours is just not good, and taking breaks is very important. So, I have even developed a macOS app to remind myself to take breaks. It is available on the Mac App Store now https://apps.apple.com/us/app/totalpause/id6482185943?mt=12, I hope it helps. reply thebrain 14 hours agoprevThe room of my house that I use as an office did not receive much sunlight from the window that faces a brick wall. I recently had a skylight installed and wow, it's been life changing. I don't even need a light on anymore during the day. I highly recommend installing a skylight to add more natural light to a room if it's possible. reply philjohn 9 hours agoprevMy biggest boons was adding a tuneable LED strip along the back side of my desk, which softly lights up the wall behind my monitors, and getting a Colorimeter. The colorimeter in particular has been great most monitors come eye-searingly bright out of the box, and with a white point nowhere near optimal I calibrate all three of my displays (MBP + 2 LG 4K 27\" monitors) to 6500k, 120cd/m2 which for me is tolerable and leaves my eyes not feeling so tired at the end of the day. reply DoingIsLearning 10 hours agoprevOn the topic of WFH eye strain but less on lighting. There was a post a while back where the author proposed using a laser projector as a means of reducing eye strain. She referred to lighting as a factor but I assume the major benefit is the increasing distance to the screen such that your eyes are focused on a point much further away than conventional screens (and I assume eye strain is not linear with distance). Is there any evidence on this topic? Assuming space is not a problem why is a projector arrangement not more popular? (for office work where people don't care about colour accuracy) reply Mali 7 hours agoparentOn your question about popularity I would put it down to expense and noise. The fans go like mad (especially on the cheaper projectors) and to get parity with a decent monitor, you have to spend thousands. reply chronogram 9 hours agoprevI use similar standing lights with diffusers in every non-door corner, and I agree that the quality of light matters, though that doesn't mean it has to be expensive. I used E27 bulbs with RA 95, only 1521 lumen each but the light is much better than the 2500 lumen bulbs I had before. They were very cheap at a discount store (Action in the Netherlands). I use a remote to turn the whole group in a room on. reply notjoemama 16 hours agoprevI want to add something I noticed about coloring diffuse light. In an older home, the \"white\" walls may have yellowed a bit and bouncing light off them will color what is diffused. This sameness has a backrooms kind of feel to me. But, repainting with fresh and more-white white paint, I can bounce yellow or warm colored light and it ends up being pleasing. reply randomtoast 8 hours agoprevOne aspect of eye strain is often linked to sore eye muscles, as these muscles must focus on the display for hours without relaxation. This is similar to other muscle-related pain that arises from constant tension in the same position. The eye muscles can be trained, there are exercises that can help reduce eye strain if performed regularly. reply sitzkrieg 22 hours agoprevswitching to light editor themes that more closely matches ambient lighting will reduce eye fatigue a ton too reply sappler 16 hours agoprevI had a lot of problems with eye strain and was always careful having good lightning around me. What solved it for me was going dark mode everywhere. I tried everything from terminal glasses to f.lux for reducing blue light. Since then I never had any issues. reply pretoriusdre 11 hours agoprevDimmable, adjustable color-temperature bulbs are one of those things which seem a bit unnecessary at first, but they really do make a big difference for quality of life. I use Wifi-enabled globes. On the plus side they have beautiful aesthetics but on the downside my lightglobes are probably going to get hacked and join a botnet. reply nunodonato 9 hours agoparentI do the same, got a few IKEA ones and placed them in all rooms and table/floor lamps. Now it gives me a lot of flexibility on how I want my lights to look and feel reply neilv 15 hours agoprevIf the room in the illustrations is what you have to work with, consider sometimes putting the desk in front of the window, facing out. When the sun is not line-of-sight, and you don't have any bright flares off other objects (like car windows or chrome). That gives you natural light, it should be even left/right, and it happens to be awesome fill light for looking good on videoconf. You can also glance around your monitor to refocus at a distance, frequently. reply evanjrowley 30 minutes agoparentThis is an underrated comment. A desk positioned so that you can view things beyond it at a distance has been the best remedy for my eyestrain, and I say that as someone using cheap oversized 1080p screens. An expensive high DPI monitor might be more convenient than rearranging a whole room to reorient an office desk, but in my experience, the vale it provides is fantastic. No matter the quality of the monitor, if my ability to adjust my focus is impeded by a room's layout, then my eyes will feel the same strain they'd have if I'd been staring at a wall all day. Interestingly, the photo search results I get for \"desk ceo\" all show the desk oriented in the ideal way (i.e., not facing a wall): https://kagi.com/images?q=desk+ceo reply xela79 8 hours agoprevput your monitor in front of a nice bland white wall, put a led strip at the back of your monitor that lights up the wall; this is reduces the contrast between the lightening from your monitor vs what's around, reducing eye strain a lot no lights shining into your monitor , those just create reflections and make the screen harder to see use the same for your TV, Philips Hue TVs are a good example on how to make the screen bigger and reduce eye strain, similmar things can be accomplished with that led strip reply pyaamb 19 hours agoprevAre there any solutions that involve reflecting mirrors that allow you to add variation to the apparent viewing distance so your eye muscles are exercised more vs staring at the same distance for too long? reply rusty-ux 19 hours agoparentCool idea. Would love to see prototypes of this. This is essentially what happens at an optometrists office when you go for an eye test. reply iforgot22 20 hours agoprevFrom my experience, if you're going to do just one thing in this list, it should be taking breaks. It's also the only thing you can reliably control even in the office. reply C-Loftus 22 hours agoprevAre there any communities of others online dealing with general eye strain? Or other blogs / videos that have helped others? I have had chronic eye pain for a while now and could really benefit from hearing what has helped others. I have not found doctors to be helpful I have a pretty normal Dell office monitor but not sure if I would benefit from an upgrade. I have relatively normal overhead lighting and try to take breaks or use a screen reader as much as I can, but haven't had much luck reducing pain. reply marcinreal 21 hours agoparentI know of such a community: https://ledstrain.org/ While it's informative, I would proceed with caution. Many users there indulge in a level of obsession that is not helpful. The basics of reducing eye strain are actually simple: 1. Don't use a display at unnecessarily high brightness. 2. Make sure there's plenty of natural light around you (avoid LEDs if possible). 3. Take frequent breaks and look off in the distance. (If you're in a social setting, assume an air of mystery with your ponderous gaze.) 4. Reduce your level of stress. Stress makes nothing better and everything worse. Enjoy life! Stretch regularly to reduce muscle tension in the body. 5. Probably diet helps, but that's a whole can of worms. Don't obsess over it, but try to reduce inflammatory foods. reply plun9 9 hours agoparentprevUsing a projector and/or TV as a monitor, at a distance, may help. I've been doing this for a while now, and it works for me. reply camhart 21 hours agoparentprevThere's a good chance its due to dry eye. If so, you need to blink more. Get an eye compress (heat it up in microwave, toss on eyes for 10 minutes). That can help release oils from glands onto your eyes. Artificial tears can help with comfort but wont solve the underlying problem--we don't blink (enough) when we focus on screens that are close to our face. reply C-Loftus 21 hours agorootparentI don't think that is the case but I could be wrong. My eyes do not feel dry at all and drops or hot washclothes haven't made much difference. Maybe that compress you speak of is better though reply camhart 21 hours agorootparentHot wash clothes don't maintain the heat long enough to release the oils. Decent eye compresses are $20. Here's a decent one. Certainly others work too. https://www.amazon.com/Bruder-Activated-Recommended-Professi... I got a fancier one from Tear Restore that has little cut outs, so I can see while using it (instead of keeping eyes closed). It may not work quite as good as the bruder, but it lets me get things done while using it. reply nozonozon 16 hours agoparentprevMany monitors allow adjustment of the individual R G and B components. This has been the single biggest help for me. I typically use R 45 G 35 B 15 or at night R 25 G 15 B 0 and that has helped me stay productive for longer without eye strain. reply kanbankaren 22 hours agoparentprevThe standard 60Hz refresh rate of monitors is unlikely to produce any eyestrain. The refresh rate of the backlight could produce eyestrain and headaches. Unfortunately, the exact frequency of the PWM used for backlight isn't often mentioned in the specs. In general, anything above 500 Hz is better as some people get headaches even for 250 Hz. reply JoshuaEN 19 hours agorootparentRtings.com measures PWM frequency[1] as part of their reviews. 1. https://www.rtings.com/monitor/tests/motion/image-flicker reply kanbankaren 22 hours agorootparentprevP. S. The linked website is poor on details and not worth reading. reply lucg 21 hours agoparentprevhttps://ledstrain.org/ reply saadel 7 hours agoprevI'm surprised there is no mention of circadian rythms or the effects of artificial lights on us reply dinkblam 21 hours agoprev> An even, diffused lighting environment is best for the eyes indeed. i am unsure where the love for those \"spotlights\" embedded into the ceiling comes from. every time i enter a room that has them i want to crouch like gollum. the only thing they seem to do is blind you. indirect lights can be expensive and hard to find though. i built 12 hanging lamps that illuminate the ceiling instead of the floor myself, which saves a few thousand bucks but was unfortunately more work than expected. reply amluto 21 hours agoparentCeiling downlights can me (for most people and most purposes) quite nice, but they need to actually be spotlights. If you have a light that is close to being a point source (very bright per unit solid angle), which includes most lights aimed at people, you want that light to avoid emitting light at an angle that you're likely to be able to see. So a downlight on a ceiling should emit very little light past an angle of, say, 45 degrees or even less from vertical. After all, for most purposes (but not bedrooms when in bed!), you are not looking straight up, and a light that's shining on the top of your head or even on the tops of your eyebrows is not irritating your eyes. But a bright light, shining at you, that's in your field of view when looking horizontally, can be extremely annoying. The office-style solution is big diffuse ceiling lights. The easy but rather inefficient solution is indirect lighting. The expensive solution is to use high-end architectural lights that have a trim or lens design that makes the light source almost invisible from shallow angles. An excellent and cheap solution is to use highly recessed lights with standard, inexpensive designs. A PAR30S lamp in an ordinary (not \"shallow\") 5\" or 6\" ceiling can, with a trim that allows it to be installed at a respectable recess, can work very nicely. (That \"S\" is important. The whole point is that it's a \"short neck\" light, so the bottom surface is farther above the ceiling. And the PAR part is important, too PARs are reflectors, not floods, and they emit over a narrower angle.) Here's a decent article about it: https://www.agcled.com/blog/glare-and-ugr.html As far as I can tell, this is almost completely ignored for residential and small business lighting, especially with LED lights from places like Costco and Home Depot. You do not want a bright light that emits over 180 degrees installed in plane with your ceiling. reply the_pwner224 11 hours agoparentprevMind sharing how you made the hanging lamps? I'm currently designing a new house and was thinking of doing a similar setup. Any guidance would be appreciated. reply dinkblam 6 hours agorootparent1.) get an appropriate bowl of a form/color you like. the required size differs whether you want to use 1 or 3 led-bulbs. i used this ( https://edelrostshop.de/de-at/products/rost-metall-schale-o-... ) and painted it white (got them to deliver 20 pieces without rust). the bowl should be less than 2 kg. the point here is to face the light exclusively to the ceiling while preventing you ever seeing the light sources directly. 2.) mount a lamp suspension on the ceiling, something like this ( https://www.amazon.de/dp/B094HX42PN/ ). 3.) if you want 3 lightbulbs, just screw in a a 3-way splitter ( e.g. https://www.amazon.de/dp/B07VH7KKCH/ ). the benefit here is more and more upwards-facing light 4.) the final part is connecting the bowl to the lamp suspension. i just made 4 screws in the suspension and 4 holes in the bowls and connected with basically-invisible nylon strings. getting the nylon strings to be all the same size was tricky but doable. reply rusty-ux 18 hours agoparentprevDoing it with hanging lamps is a clever solution. Doing it indirectly is the goal. There's one company out of the UK making super high end LED diffusers that can be precisely controlled both the amount of light and the direction. Can't remember the company but it was similar to this: https://www.acalbfi.com/technologies/photonics/optical-compo... reply williamjinq 11 hours agoprevI use screenbars (BenQ, Mi, etc.) and found them extremely helpful, in terms of reducing eye strain and leaving spaces for other desktop setups (I currently have no lamp on my desk). reply stusmall 3 hours agoprevThis was extremely useful. Thank you. reply andrewfromx 20 hours agoprevI started leaving most warm nightshift on MacOS and iOS on 24/7. Made a huge difference. Now when I see a normal not warm screen it hurts. reply darkwater 20 hours agoprevSooooo, I always saw offices with neon bulbs, so, very very cold light (well, actually \"hotter\" in Kelvin degrees...) and very bright. So I copied this in my home office, which is a 12 square meter (130 square foot) room with 6x 7000K bulbs, each emitting about 800 lumen. Am I doing totally wrong, according to TFA? Should I replace them with warm bulbs? I'usong GU10 bulbs. reply Karrot_Kream 19 hours agoparentColor temperature preferences tend to be very dependent on the person. I personally find warm color to be the most relaxing but find myself straining when viewing a screen in too warm of an environment. Warm tones will tend to slightly desaturate a picture and my eyes and brain strain to \"compensate\" for the desaturation. I find it best to keep my office environments a bit warm, but still on the cold side. reply rusty-ux 19 hours agoparentprevThis article is just what worked for me. Some people are more light color sensitive, if you like your cool lighting and a lot of light and it works go for it. reply darkwater 11 hours agorootparentI wasn't judging anyone, sorry if it felt judgemental. I was genuinely asking, because cold, bright light is all I experienced in a workplace and when I replicated it at home, I didn't feel any particular issue with my vision. But I also tend to act a bit like a mule or workhorse in these cases, carrying on anyway until I realize it's actually harmful. That's why I was asking. reply irunmyownemail 20 hours agoprevLight on my desk for light under the monitor, light from behind me which bounces off the angled ceiling and only light themes no dark themes. reply 01100011 19 hours agoprevI am sensitive to bad CRI and color temp(3100K FTW). I really like the cheap \"torchiere\", floor lamp/uplights on Amazon but am finally giving up on them due to issues with poor CRI. I find standard screw-in LEDs from Philips to be the best, easily available source of light. reply rusty-ux 19 hours agoparentAmen. Honestly the IKEA lamps are the best super cheap and you can swap out the bulbs with high quality dimmable philips. What torchiers are you swapping to? reply 01100011 2 hours agorootparentI hate that ikea uses the smaller sockets for so many of their lamps. I'm still shopping for floor lamps. Too many of them come with a reading light which I don't want . reply sberens 16 hours agoprevI'm working on something like the lamp depicted in the images[0]. It's bright, diffuse, dimmable, flicker free, and high CRI. [0]https://getbrighter.com reply sgt 5 hours agoparentIt's so expensive.. And it uses a whopping 500W. Why not then just use a regular halogen standing floor lamp? reply sberens 1 hour agorootparentYou get 6x less lumens/watt with a halogen lamp reply sebmellen 13 hours agoparentprevHow would this compare to something like the Godox LA 200D, which claims an output of 100,000 lumens? I use two of these lights on tall stands pointed at my ceiling, which seems to work very well reply sberens 1 hour agorootparentThey claim 101,000 lux, not lumens. Lux is light per square meter (roughly), lumens is total light output. The closer you get to the source, the higher the lux will be, so it's hard to compare lux equally. Based on their 230W power draw and typical COB efficiency, I'd guess it's 20,000-35,000 lumens. Tldr, we're brighter, fanless, have adjustable color temperature, smart home compatible, and more aesthetic. reply NeckBeardPrince 9 hours agoparentprevPlease help me understand a $1500 lamp... reply bevan 13 hours agoprevStrategically increasing the amount of light I'm exposed to was a game changer. Turns out we're bad at gauging how much light is in our environment. Few would suspect that the amount of lux we're exposed to indoors is often far less than 1% of the lux outside. A typical corporate office has under 500 lux, vs 50k+ midday-if you're at home and haven't put much thought into the lighting situation, the lux could be in the low double digits. You can get a free app \"Lux meter\" to measure yours now. We evolved to be outside a lot, and light regulates aspects of our biology. We probably shouldn't stay two orders of magnitude beneath the factory recommended exposure levels for months at a time. Hence clinical or sub-clinical SAD,sleep and mood disturbances etc [see references]. One solution is high wattage LEDs: https://www.benkuhn.net/lux/ I've had several ~250w LEDs over the years (these are huge and actually draw 250w, they aren't 250w equivalent). 250w might be overkill, and 80w is fine for me and a lot more practical. If you get one , be warned that depending on the wattage you'll probably have to build your own stand for it as most fixtures aren't rated for that high wattage. Related to light amount is of course light timing. This may be more important than getting a steadily high amount of lux during the day. Get lots of light in the morning, and not a lot at night (just low intensity bulbs, maybe just red ones, starting 1-2h after sunset is nice). That helped my sleep a ton. Check out Huberman LAb, he talks about light amount and timing ad nauseam. [1] Office workers sleep better and are more active with more lighting: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4031400/ [2] \"Day and night light exposure are associated with psychiatric disorders: an objective light study in >85,000 people\" https://www.nature.com/articles/s44220-023-00135-8 [3] \"Time spent in outdoor light is associated with mood sleep circadian rhythm related outcomes\" https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8892387/ ... reply taylorbuley 20 hours agoprevEnvironmental modification is one of my favorite emotional coping strategies. It.. feels like you're actually doing something! Cleaning up/tidying, \"sacred space creation,\" light and color therapy all work way more effectively than you might believe. reply hammock 20 hours agoparentTalk to me about color therapy reply taylorbuley 19 hours agorootparentNature bathing is great. But it turns out that people in hospital beds facing windows recover faster when they face nature. And, in fact, it's not the nature at all! It can be fake and just as effective. In the end, basically, it's the green. Color therapy or chromotherapy is incorporating specific colors into your environment to evoke desired emotional states. It blows my mind how well this works, and that's why you see peach colored walls in offices. Light works similarly. Warm vs. cool, etc. You can use light exposure to regulate mood, particularly for conditions like seasonal affective disorder (SAD). All of this is culturally-dependent but it works way better than you'd think. Here are some of the common color associations: 1. Red Emotions: Energy, passion, excitement, strength, urgency, love, anger, aggression. Therapeutic Use: Stimulates energy, increases heart rate, and can evoke strong emotions. Often used to combat fatigue or lethargy, but excessive use may lead to overstimulation or agitation. 2. Orange Emotions: Enthusiasm, creativity, warmth, optimism, sociability, joy. Therapeutic Use: Encourages social interaction, boosts creativity, and uplifts mood. Often used to combat depression or feelings of loneliness. 3. Yellow Emotions: Happiness, clarity, intellect, optimism, caution, anxiety (in excess). Therapeutic Use: Promotes mental clarity, stimulates the nervous system, and enhances focus. However, too much yellow can lead to feelings of anxiety or frustration. 4. Green Emotions: Balance, harmony, growth, renewal, calmness, peace, envy (in some contexts). Therapeutic Use: Known for its calming and balancing effects, green is often used to reduce stress and promote relaxation. It is also associated with nature and healing. 5. Blue Emotions: Calmness, serenity, trust, stability, sadness, coldness. Therapeutic Use: Reduces stress, lowers blood pressure, and promotes relaxation. Often used in spaces meant for rest or introspection. Darker shades can evoke feelings of sadness or detachment. 6. Purple Emotions: Spirituality, luxury, creativity, mystery, introspection, wisdom. Therapeutic Use: Encourages deep thinking, meditation, and spiritual connection. Often used to inspire creativity and self-reflection. 7. Pink Emotions: Love, compassion, nurturing, calmness, playfulness. Therapeutic Use: Promotes feelings of warmth and comfort. Often used to reduce aggression and create a soothing environment. 8. White Emotions: Purity, clarity, simplicity, peace, emptiness, sterility. Therapeutic Use: Creates a sense of space and cleanliness. Often used to promote mental clarity and a fresh start. 9. Black Emotions: Power, sophistication, mystery, fear, sadness, protection. Therapeutic Use: Can evoke feelings of protection and strength but may also lead to feelings of heaviness or depression if overused. 10. Brown Emotions: Stability, reliability, warmth, comfort, dullness. Therapeutic Use: Grounding and stabilizing, often used to create a sense of security and connection to the earth. 11. Turquoise Emotions: Calmness, clarity, communication, emotional balance. Therapeutic Use: Combines the calming effects of blue with the rejuvenating qualities of green. Often used to promote emotional balance and clear communication. 12. Gold Emotions: Success, wealth, luxury, wisdom, optimism. Therapeutic Use: Inspires confidence, abundance, and positivity. Often used to elevate mood and encourage a sense of achievement. 13. Silver Emotions: Modernity, sophistication, intuition, reflection, coldness. Therapeutic Use: Encourages introspection and clarity of thought. Often used to promote a futuristic or innovative mindset. 14. Gray Emotions: Neutrality, balance, sophistication, boredom, sadness. Therapeutic Use: Creates a sense of calm and balance but can also evoke feelings of dullness or detachment if overused. reply DonHopkins 18 hours agorootparentHow do you accomplish silver colored lighting? reply taylorbuley 3 hours agorootparentAlthough they are both forms of environmental modification, light therapy is different than color therapy. Light therapy uses e.g. daylight to boost affect. Color therapy uses e.g. silver paint to prime cognition. reply kali_00 17 hours agorootparentprevI would guess by reflecting light off something silvery, eg, a wall painted in metallic silver paint. reply DonHopkins 16 hours agorootparentThat's not how colored light actually works. (That's the joke!) There isn't a silver \"color\" of light, it's a visual phenomenon that warps and reflects the color of the environment around it. So unfortunately there are no silver colored light bulbs (except for using a silver coating to block or reflect the light). Maybe it's that shiny reflective things like disco balls are therapeutic, imparting modernity, sophistication, intuition, reflection, coldness, or disco fever. reply adhoc_slime 22 hours agoprevI recently picked up a monitor light and the change is huge, I wish I did it sooner. My rule of thumb is that I obviously don't point lights in my eyes, only on camera. no overhead light, but lamps are great because they direct light away from you and bounce it around the surfaces of the room. reply wvenable 22 hours agoparentI also found a monitor light to be a great addition. reply Carrok 22 hours agoprev> When it comes to light brightness, too much is just as problematic as too little Recently moved into a house with on/off light switches. Having the ceiling lights on full brightness was downright oppressive. I installed dimmer switches, and it's so much nicer it's hard to really convey. reply joelfried 22 hours agoparentHow do you source lightbulbs? I have yet to find a reliable LED bulb that doesn't hum or flicker on a dimmer despite being advertised as \"dimmable\" . . . reply boomskats 21 hours agorootparentLutron test a lot of LED lights for compatibility with their dimmers you can find the index here[0]. I can't imagine the performance of the LEDs listed is specific to Lutron dimmers. [0]: https://www.lutron.com/europe/Service-Support/Pages/Technica... reply GuB-42 21 hours agorootparentprevI actually \"solved\" the problem by using remote controlled light bulbs. I can still use the switch to turn them on and off, but I have to use the remote for dimming. Not the most elegant solution, but the dimming works flawlessly, and on my model I can also change the color temperature, which is nice, and it was actually cheaper than most \"dimmable\" light bulbs. reply jerlam 19 hours agorootparentI did this too with IKEA bulbs and their remote control that you can directly pair to multiple bulbs. It works out great, and I don't have to deal with another phone app or buy a hub in my house to manage. reply adhoc_slime 22 hours agorootparentprevthat's probably a problem with your dimmer switch, not the bulbs. reply gruez 22 hours agorootparentExplain. Isn't a dimmer switch just a variable resistor? Do you need fancy dimmer switches if you want them to work with LED bulbs? reply myself248 21 hours agorootparentOld-school dimmer switches were rheostats, and they got hot. Pretty much all dimmers now are TRIAC-based, which is a semiconductor that turns on partway through the AC wave, then turns off at the next zero-crossing, repeat. It chops the waveform so the light only gets power for a fraction of the time. An incandescent bulb works largely the same with either type. (You may hear the filament \"sing\" on a TRIAC dimmer since the fast-rising waveform edge has a lot of harmonic content, but this is usually very faint.) LED bulbs are non-dimmable by default. The typical job of a power supply is to ignore variations in the source and deliver uniform power to the load, and that's just what they do, driving the emitters at a constant brightness regardless of what the dimmer does, until it's letting through so little power that the poor thing just shuts off. Or flickers madly. Dimmable LED bulbs are actually super tricky, because the power supply has to measure the distortions in the incoming waveform, interpret that as a dimming command, and use that to control the output to the emitters. Any jitter in the measurement sampling means the resulting brightness will bounce around. Any jitter in the waveform, which an incandescent might've ignored as long as the area-under-the-curve was equal, might be picked up by the LED power supply and misinterpreted as a changing dimming level. It all sucks and we should abandon it immediately. LEDs should be driven with DC. But there's an awful lot of installed fixtures to keep us from that utopia. reply quickthrowman 19 hours agorootparent> LEDs should be driven with DC. But there's an awful lot of installed fixtures to keep us from that utopia. Every LED luminaire or lamp already has a DC inverter inside of it. Also, you can get (158) 28w 2x4 LED fixtures on a single 277V 20A circuit with #12 wire, DC lighting branch is never going to happen. For reference, that will light about 12,000 square feet of space assuming 9’ AFF for ceiling height. reply aidos 21 hours agorootparentprevNot 100% sure of the details but I think it’s more like a digital pwm system. I had dimmers installed when we rewired our house and I thought they were rubbish. Replaced those with Varilight v-pro and they were noisy. Discovered that I could switch between leading edge and trailing edge modes (or something like that) and the sound went away. Love them now. reply rb2k_ 21 hours agorootparentprevThere's differences. e.g. MOSFET vs TRIAC Zooz (popular zwave manufacturer) has some interesting tidbits on their homepage. https://www.support.getzooz.com/kb/article/1103-zen72-vs-zen... \"The ZEN77 Dimmer is recommended for 3-way and 4-way installations since you won't need to rewire your other switches in the set-up, you can simply replace the main switch with direct connection to power with the ZEN77 dimmer. This model can control up to 100 Watts of LED bulbs but we don't recommend using it in installations with chandeliers or large groups of lights over 6 bulbs. Version 1.0 and 2.0 of the ZEN77 (700 series Z-Wave chip) were MOSFET dimmers so if your bulbs work better with trailing-edge (or reverse-phase) drivers, those versions of the model worked best. Version 3.0 of the ZEN77 (700 series Z-Wave chip) is now a TRIAC dimmer so if your bulbs work better with leading-edge drivers, this model will work better. The 800 series version of the ZEN77 is also a TRIAC dimmer. Why We Changed to all TRIAC: We found that newer LED bulbs dimmed better with TRIAC dimmers, and considering limited availability for MOSFETS, we decided to transfer the ZEN77 model to TRIAC as well.\" reply InitialLastName 21 hours agorootparentprevIf a dimmer switch were a resistor, it wouldn't work at all with LED lights (the AC->DC converters in them don't just lower the current they provide when the AC input gets lower). In incandescent lights, a variable resistor would burn a lot of power unnecessarily, so instead they use phase cut dimming (with a triac switch) where the dimmer cuts out a variable portion of the AC cycle. That way you reduce the effective duty cycle of the power without burning the energy. This works well for incandescents because the filament glow scales nicely with the with the power being delivered to the bulb. It works poorly with (some) LED bulbs because the turn on/off time is slow relative to the power cycle, and the LED brightness itself doesn't just scale nicely with the current from the rectifier. reply amluto 21 hours agorootparentprevA dimmer is either a \"phase-cut\" device, either (commonly) forward-phase/leading-edge or (less commonly) reverse-phase/trailing-edge. At all times, it's either on or off, and it cycles between on and off once per half wave, so it produces 120 pulses per second. A good-quality light fixture will smooth out that waveform and produce approximately constant output. The worst choice is a no-neutral-required forward-phase dimmer. Neutral-required forward-phase dimmers are usually better. Reverse-phase dimmers can be excellent for LEDs (but disastrous for magnetic transformers) and always require neutrals. Some dimmers can operate in both modes. reply airblade 22 hours agorootparentprevA dimmer switch for an LED light is different from a dimmer switch for a non-LED light. If you try to use an LED light with a “normal” dimmer, it won’t work well. reply amluto 21 hours agorootparentprevYou buy a bunch and try them. Philips is generally reliable, but their online catalog is often out of date or incomplete, and they change their SKUs all the time. reply rusty-ux 18 hours agorootparentprevPhilips Ultra-Definition 60w equivalent bulbs are amazing and really reasonably priced. They also work really well with standard dimmers with extremely low flicker. reply slavik81 13 hours agorootparentI've heard good reviews, but half of my Philips UD bulbs have died since I installed them roughly six months ago. All the bulbs in enclosed fixtures have died (despite the bulbs stating they were suitable for enclosed fixtures), and maybe 1/6th of the bulbs in open fixtures have died. reply quickthrowman 19 hours agorootparentprevBuy Lutron Diva dimmers and dimmable LED lamps (you probably call them ‘bulbs’), problem solved. For the Lutron Diva, you probably want this one: https://www.homedepot.com/p/Lutron-Diva-LED-Dimmer-Switch-fo... Philips sells dimmable LED lamps that work well. I sell electrical work for a living and this is what I use on my own home. If you want higher grade, commercial LED fixtures have built in drivers with heat sinks and are generally rated for 50,000 hours. Commercial dimming typically uses separate dimming conductors that carry a 0-10VDC signal. reply MisterTea 4 hours agorootparentI use Shelly dimmers as they offer you the ability to use their cloud app thing or completely roll your own including flashing new firmware into the ESP controlling the thing. reply scotty79 7 hours agoparentprevI felt the light in my 5x4 meter room was to dim so I installed 20x 1500lm lightbulbs, about 200W of LED. I've put them in two sections 6+14. I pretty much used all 20 all the time. Almost as bright as daylight in sunny day. Perceptually 6 were about half as bright as 20. reply divan 21 hours agoprevHaving worked from home for many years, I was surprised that I had never come across the acronym “WFH” before. It took me a moment to decipher its meaning, especially since the article introduced another acronym, “PWM” (pulse-width modulation), right at the start. reply hammock 20 hours agoparentHave you heard of COVID? Is your home the ISS or something? reply khnov 18 hours agorootparentNow you need to tell him also what ISS is reply divan 9 hours agorootparentI know what ISS is. I just never had anyone in my circle reducing phrase \"work from home\" to a 3-letter acronym. reply bongodongobob 17 hours agoparentprevWhat planet do you live on? Most of us here are from Earth. reply zombiwoof 17 hours agoprevI love we have a society where we are approaching AGI and a popular story is humans who figure out how to light their home office reply OccamsMirror 17 hours agoparent> approaching AGI I don't see how we're approaching AGI? I think an argument can now be made that it seems possible. But I would need a convincing argument that LLMs are anything more than an early stepping stone. reply winrid 18 hours agoprevNote that most IPS displays have 0 PWM flicker. I use a couple IPS gaming monitors. IPS displays look better for code vs OLED anyway, IMO. reply layer8 21 hours agoprevThe environment and the screen should have similar brightness. In the pictures in the article, the screen seems much too bright in comparison. reply rusty-ux 20 hours agoparentYou are spot on! was a limitation of my 3d rendering abilities but great point. reply scotty79 7 hours agoprevJust pointing your desk lamp onto the wall behind the screen goes 90% of the way and can be done everywhere if only you have your desk against a wall that's not too dark. reply aunagar 21 hours agoprevWould love to know how did you generate those renderings? Is it blender or something else? reply rusty-ux 20 hours agoparentIt’s adobe substance stager 3d reply DonHopkins 18 hours agorootparentOh, it's CGI? I wanted to know who your housecleaner was and if I could borrow them. ;) So much cat hair... reply imp0cat 13 hours agorootparentAn army of robot vacuums working tirelessly 24/7 can help. That is, until they encounter a piece of cat excrement. :) reply thepratt 5 hours agoprevGreat set of suggestions. I experience a fair amount of over lighting on some calls due to my monitor (a 42\" Dell thing, maybe sitting 50-60cm away). Most things I use have dark mode, but when someone shares their screen and it's pure white I end up looking like an apparition. Does anyone have any additional suggestions where the monitor itself is the cause of the excess light? Is distance a helping factor (someone else mentioned 1.5m away from their 85\" TV)? reply rusty-ux 15 minutes agoparentThat’s really tough. For me I use a display that has an ambient light sensor so it matches my surroundings, that might help a bit. Ideally your environment is balancing the light level of your monitor (maybe 50% black level) so you can have something that works if it’s a white display window or dark. reply vault 20 hours agoprev@jahfer what about dark mode/night mode? are you using it? reply __mharrison__ 19 hours agoprevGreat article. Personally, I would prefer real images than the renders. reply marcodena 19 hours agoprevwhat suggestions do you have for the led strips behind the monitor? What if I wanna do the same for the back of my TV? reply rusty-ux 19 hours agoparentBest thing you can do for LED strips is to connect it to a flicker free dimmer control, such as this one made by waveform lighting. Requires a little DIY but it will mean 0 flicker. https://store.waveformlighting.com/products/filmgrade-flicke... reply echoangle 10 hours agorootparentTo be pedantic, they advertise 0 flicker when capturing with a camera at up to 240 fps. It’s probably still using PWM with maybe a few kHz, so physically, it’s still flickering. reply empressplay 19 hours agoprevI get these LED shop light tubes off of Amazon and put them _everywhere_. It's brighter inside than outside most days! It doesn't just help with eye strain but also mood and productivity https://www.amazon.com/Barrina-Integrated-Fixture-Utility-El... reply gunian 9 hours agoprevhere i am no home no money no job with 8 weeks to live looking at the lord's ladder reply bongodongobob 17 hours agoprevI think something that gets kind of buried in these discussions is that the majority of this lighting stuff is psychological. 60hz lighting doesn't strain anything. Sharp shadows don't strain anything. Colors don't strain anything. Too bright is bad. Too close is bad. Beyond that, it's preference. reply binary_slinger 21 hours agoprevAnother tool, although I have only anecdotal evidence, is f.lux and similar software. reply tonymet 20 hours agoprevdoes anyone know if there is a phase-shifter for led lights? a way to shift each led 60hz cycle to create more-uniform light? reply Havoc 18 hours agoparentYou’re probably better off using a high quality converter to DC and DC lights reply tonymet 13 hours agorootparentin theory all the switching dc supplies will be operating on the same AC cycle thus the same led frequency reply Havoc 7 hours agorootparentNo the output differs dramatically depending on quality of the supply. Simple example to illustrate: https://www.youtube.com/watch?v=r715nLvwWvE reply tonymet 2 hours agorootparentyes but the frequency is in the same phase . e.g. if you hook up 6 lights to a high quality supply, they will oscillate in sync reply 12 hours agoprev [2 more] [dead] ornornor 12 hours agoparent [–] What does this have to do with anything? reply Consider applying for YC's Spring batch! Applications are open till Feb 11. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "To minimize eye strain while working from home, create a balanced lighting environment using natural light, sheer curtains, and flicker-free artificial lights. Implement the 20/20/20 rule: every 20 minutes, look at something 20 feet away for 20 seconds to relax your eyes. Complement your workspace with light-filtering curtains and desk lighting that matches monitor brightness, while also taking regular breaks to maintain overall health."
    ],
    "commentSummary": [
      "To enhance work-from-home (WFH) lighting and reduce eye strain, use diffuse light from multiple sources and maintain a consistent color temperature. Consider the impact of wall color on lighting and aim for a \"Golden Hour\" effect—diffuse, warm, and gentle lighting for a photogenic and comfortable environment. Explore LED lighting options, such as high Color Rendering Index (CRI) bulbs, for improved light quality."
    ],
    "points": 570,
    "commentCount": 265,
    "retryCount": 0,
    "time": 1737576537
  },
  {
    "id": 42799136,
    "title": "Tailwind CSS v4.0",
    "originLink": "https://tailwindcss.com/blog/tailwindcss-v4",
    "originBody": "Holy shit it's actually done — we just tagged Tailwind CSS v4.0. Tailwind CSS v4.0 is an all-new version of the framework optimized for performance and flexibility, with a reimagined configuration and customization experience, and taking full advantage of the latest advancements the web platform has to offer. New high-performance engine — where full builds are up to 5x faster, and incremental builds are over 100x faster — and measured in microseconds. Designed for the modern web — built on cutting-edge CSS features like cascade layers, registered custom properties with @property, and color-mix(). Simplified installation — fewer dependencies, zero configuration, and just a single line of code in your CSS file. First-party Vite plugin — tight integration for maximum performance and minimum configuration. Automatic content detection — all of your template files are discovered automatically, with no configuration required. Built-in import support — no additional tooling necessary to bundle multiple CSS files. CSS-first configuration — a reimagined developer experience where you customize and extend the framework directly in CSS instead of a JavaScript configuration file. CSS theme variables — all of your design tokens exposed as native CSS variables so you can access them anywhere. Dynamic utility values and variants — stop guessing what values exist in your spacing scale, or extending your configuration for things like basic data attributes. Modernized P3 color palette — a redesigned, more vivid color palette that takes full advantage of modern display technology. Container queries — first-class APIs for styling elements based on their container size, no plugins required. New 3D transform utilities — transform elements in 3D space directly in your HTML. Expanded gradient APIs — radial and conic gradients, interpolation modes, and more. @starting-style support — a new variant you can use to create enter and exit transitions, without the need for JavaScript. not-* variant — style an element only when it doesn't match another variant, custom selector, or media or feature query. Even more new utilities and variants — including support for color-scheme, field-sizing, complex shadows, inert, and more. Start using Tailwind CSS v4.0 today by installing it in a new project, or playing with it directly in the browser on Tailwind Play. For existing projects, we've published a comprehensive upgrade guide and built an automated upgrade tool to get you on the latest version as quickly and painlessly as possible. New high-performance engine Tailwind CSS v4.0 is a ground-up rewrite of the framework, taking everything we've learned about the architecture over the years and optimizing it to be as fast as possible. When benchmarking it on our own projects, we've found full rebuilds to be over 3.5x faster, and incremental builds to be over 8x faster. Here are the median build times we saw when we benchmarked Tailwind CSS v4.0 against Catalyst:v3.4 v4.0 Improvement Full build 378ms 100ms 3.78x Incremental rebuild with new CSS 44ms 5ms 8.8x Incremental rebuild with no new CSS 35ms 192µs 182x The most impressive improvement is on incremental builds that don't actually need to compile any new CSS — these builds are over 100x faster and complete in microseconds. And the longer you work on a project, the more of these builds you run into because you're just using classes you've already used before, like flex, col-span-2, or font-bold. Designed for the modern web The platform has evolved a lot since we released Tailwind CSS v3.0, and v4.0 takes full advantage of many of these improvements. CSS @layer theme, base, components, utilities;@layer utilities { .mx-6 { margin-inline: calc(var(--spacing) * 6); } .bg-blue-500\\/50 { background-color: color-mix(in oklab, var(--color-blue-500) 50%, transparent); }}@property tw-gradient-from { syntax: \"\"; inherits: false; initial-value: #0000;} We're leveraging modern CSS features like: Native cascade layers — giving us more control than ever over how different style rules interact with each other. Registered custom properties — making it possible to do things like animate gradients, and significantly improving performance on large pages. color-mix() — which lets us adjust the opacity of any color value, including CSS variables and currentColor. Logical properties — simplifying RTL support and reducing the size of your generated CSS. Many of these features have even simplified Tailwind internally, reducing the surface area for bugs and making the framework easier for us to maintain. Simplified installation We've streamlined the setup process a ton in v4.0, reducing the number of steps and removing a lot of boilerplate. 1. Install Tailwind CSS npm i tailwindcss @tailwindcss/postcss; 2. Add the PostCSS plugin export default { plugins: [\"@tailwindcss/postcss\"],}; 3. Import Tailwind in your CSS @import \"tailwindcss\"; With the improvements we've made to this process for v4.0, Tailwind feels more light-weight than ever: Just one-line of CSS — no more @tailwind directives, just add @import \"tailwindcss\" and start building. Zero configuration — you can start using the framework without configuring anything, not even the paths to your template files. No external plugins required — we bundle @import rules for you out of the box, and use Lightning CSS under the hood for vendor prefixing and modern syntax transforms. Sure you only go through this once per project, but it adds up when you're starting and abandoning a different side-project every weekend. First-party Vite plugin If you're a Vite user, you can now integrate Tailwind using @tailwindcss/vite instead of PostCSS: vite.config.ts import { defineConfig } from \"vite\";import tailwindcss from \"@tailwindcss/vite\";export default defineConfig({ plugins: [ tailwindcss(), ],}); Tailwind CSS v4.0 is incredibly fast when used as a PostCSS plugin, but you'll get even better performance using the Vite plugin. Automatic content detection You know how you always had to configure that annoying content array in Tailwind CSS v3? In v4.0, we came up with a bunch of heuristics for detecting all of that stuff automatically so you don’t have to configure it at all. For example, we automatically ignore anything in your .gitignore file to avoid scanning dependencies or generated files that aren’t under version control: .gitignore /node_modules/coverage/.next//build We also automatically ignore all binary extensions like images, videos, .zip files, and more. And if you ever need to explicitly add a source that's excluded by default, you can always add it with the @source directive, right in your CSS file: CSS @import \"tailwindcss\";@source \"../node_modules/@my-company/ui-lib\"; The @source directive uses the same heuristics under the hood, so it will exclude binary file types for example as well, without you having to specify all of the extensions to scan explicitly. Learn more about in our new documentation on detecting classes in source files. Built-in import support Before v4.0, if you wanted to inline other CSS files using @import you'd have to configure another plugin like postcss-import to handle it for you. Now we handle this out of the box, so you don't need any other tools: postcss.config.js export default { plugins: [ \"postcss-import\", \"@tailwindcss/postcss\", ],}; Our import system is purpose-built for Tailwind CSS, so we've also been able to make it even faster by tightly integrating it with our engine. CSS-first configuration One of the biggest changes in Tailwind CSS v4.0 is the shift from configuring your project in JavaScript to configuring it in CSS. Instead of a tailwind.config.js file, you can configure all of your customizations directly in the CSS file where you import Tailwind, giving you one less file to worry about in your project: CSS @import \"tailwindcss\";@theme { font-display: \"Satoshi\", \"sans-serif\"; breakpoint-3xl: 1920px; color-avocado-100: oklch(0.99 0 0); color-avocado-200: oklch(0.98 0.04 113.22); color-avocado-300: oklch(0.94 0.11 115.03); color-avocado-400: oklch(0.92 0.19 114.08); color-avocado-500: oklch(0.84 0.18 117.33); color-avocado-600: oklch(0.53 0.12 118.34); ease-fluid: cubic-bezier(0.3, 0, 0, 1); ease-snappy: cubic-bezier(0.2, 0, 0, 1); /* ... */} The new CSS-first configuration lets you do just about everything you could do in your tailwind.config.js file, including configuring your design tokens, defining custom utilities and variants, and more. To learn more about how it all works, read the new theme variables documentation. CSS theme variables Tailwind CSS v4.0 takes all of your design tokens and makes them available as CSS variables by default, so you can reference any value you need at run-time using just CSS. Using the example @theme from earlier, all of these values will be added to your CSS to as regular custom properties: Generated CSS :root { font-display: \"Satoshi\", \"sans-serif\"; breakpoint-3xl: 1920px; color-avocado-100: oklch(0.99 0 0); color-avocado-200: oklch(0.98 0.04 113.22); color-avocado-300: oklch(0.94 0.11 115.03); color-avocado-400: oklch(0.92 0.19 114.08); color-avocado-500: oklch(0.84 0.18 117.33); color-avocado-600: oklch(0.53 0.12 118.34); ease-fluid: cubic-bezier(0.3, 0, 0, 1); ease-snappy: cubic-bezier(0.2, 0, 0, 1); /* ... */} This makes it easy to reuse these values as inline styles or pass them to libraries like Motion to animate them. Dynamic utility values and variants We've simplified the way many utilities and variants work in v4.0 by effectively allowing them to accept certain types of arbitrary values, without the need for any configuration or dropping down to the arbitrary value syntax. For example, in Tailwind CSS v4.0 you can create grids of any size out of the box: HTML You can also target custom boolean data attributes without needing to define them: HTML Even spacing utilities like px-*, mt-*, w-*, h-*, and more are now dynamically derived from a single spacing scale variable and accept any value out of the box: Generated CSS @layer theme { :root { spacing: 0.25rem; }}@layer utilities { .mt-8 { margin-top: calc(var(--spacing) * 8); } .w-17 { width: calc(var(--spacing) * 17); } .pr-29 { padding-right: calc(var(--spacing) * 29); }} The upgrade tool we released alongside v4.0 will even simplify most of these utilities for you automatically if it notices you using an arbitrary value that's no longer needed. Modernized P3 color palette We've upgraded the entire default color palette from rgb to oklch, taking advantage of the wider gamut to make the colors more vivid in places where we were previously limited by the sRGB color space. We've tried to keep the balance between all the colors the same as it was in v3, so even though we've refreshed things across the board, it shouldn't feel like a breaking change when upgrading your existing projects. Container queries We've brought container query support into core for v4.0, so you don't need the @tailwindcss/container-queries plugin anymore: HTML We've also added support for max-width container queries using the new @max-* variant: HTML Like our regular breakpoint variants, you can also stack @min-* and @max-* variants to define container query ranges: HTML Learn more in our all-new container queries documentation. New 3D transform utilities We've finally added APIs for doing 3D transforms, like rotate-x-*, rotate-y-*, scale-z-*, translate-z-*, and tons more. Mar 16, 2020 Michael Foster Boost your conversion rate Check out the updated transform-style, rotate, perspective, and perspective-origin documentation to get started. Expanded gradient APIs We've added a ton of new gradient features in v4.0, so you can pull off even fancier effects without having to write any custom CSS. Linear gradient angles Linear gradients now support angles as values, so you can use utilities like bg-linear-45 to create a gradient on a 45 degree angle:You may notice we've renamed bg-gradient-* to bg-linear-* too — you'll see why shortly! Gradient interpolation modifiers We've added the ability to control the color interpolation mode for gradients using a modifier, so a class like bg-linear-to-r/srgb interpolates using sRGB, and bg-linear-to-r/oklch interpolates using OKLCH: ...... Using polar color spaces like OKLCH or HSL can lead to much more vivid gradients when the from-* and to-* colors are far apart on the color wheel. We're using OKLAB by default in v4.0 but you can always interpolate using a different color space by adding one of these modifiers. Conic and radial gradients We've added new bg-conic-* and bg-radial-* utilities for creating conic and radial gradients:These new utilities work alongside the existing from-*, via-*, and to-* utilities to let you create conic and radial gradients the same way you create linear gradients, and include modifiers for setting the color interpolation method and arbitrary value support for controlling details like the gradient position. @starting-style support The new starting variant adds support for the new CSS @starting-style feature, making it possible to transition element properties when an element is first displayed:Check for updatesWith @starting-style, you can finally animate elements as they appear on the page without the need for any JavaScript at all. Browser support probably isn't quite there yet for most teams, but we're getting close! not-* variant We've added a new not-* variant which finally adds support for the CSS :not() pseudo-class: HTML CSS .not-hover\\:opacity-75:not(*:hover) { opacity: 75%;}@media not (hover: hover) { .not-hover\\:opacity-75 { opacity: 75%; }} It does double duty and also lets you negate media queries and @supports queries: HTML CSS .not-supports-hanging-punctation\\:px-4 { @supports not (hanging-punctation: var(--tw)) { padding-inline: calc(var(--spacing) * 4); }} Check out the new not-* documentation to learn more. Even more new utilities and variants We've added a ton of other new utilities and variants to v4.0 too, including: New inset-shadow-* and inset-ring-* utilities — making it possible to stack up to four layers of box shadows on a single element. New field-sizing utilities — for auto-resizing textareas without writing a single line of JavaScript. New color-scheme utilities — so you can finally get rid of those ugly light scrollbars in dark mode. New font-stretch utilities — for carefully tweaking variable fonts that support different widths. New inert variant — for styling non-interactive elements marked with the inert attribute. New nth-* variants — for doing really clever things you'll eventually regret. New in-* variant — which is a lot like group-*, but without the need for the group class. Support for :popover-open — using the existing open variant to also target open popovers. New descendant variant — for styling all descendant elements, for better or for worse. Check out the relevant documentation for all of these features to learn more. And that's it — that's Tailwind CSS v4.0. It's been years of work to get to this point, but we're all extremely proud of this release and we can't wait to see what you build with it. Check it out, play with it, maybe even break it, and definitely let us know what you think. Just no bug reports until tomorrow please — let us at least enjoy one celebratory team dinner and maybe relax in the hot tub at this hotel for a bit believing that somehow we really did ship flawless software.",
    "commentLink": "https://news.ycombinator.com/item?id=42799136",
    "commentBody": "Tailwind CSS v4.0 (tailwindcss.com)395 points by georg-stone 18 hours agohidepastfavorite227 comments jaredcwhite 15 hours agoAs a once-strident critic of Tailwind for its many failings and incompatibilities with the state of the actually-modern \"vanilla\" web art, I am very pleased to see the huge strides they've made with v4. Being able to access the Tailwind theme through native CSS variables (they even have an example in the docs of a button component written in native CSS in an external stylesheet using native variables! Oh happy day!) is absolutely massive, and being able to use a CSS-only config is equally amazing. Finally, Tailwind feels more like it's a utility (ironic, since its claim to fame is utility classes!) which you can add to any project, rather than some bizarro \"viral\" framework that wants to eat sensible architectures and lives in its own JavaScript-y silo. Perhaps all the criticism over the years actually had an effect. Or maybe they finally arrived at good conclusions on their own. Either way, I suspect many of the pro/anti-Tailwind arguments are no longer relevant, and that's a Very Good Thing. Now we can get down to business and ship product. reply sibeliuss 13 hours agoparentI'm still a critic in terms of actually using it, but what I find amazing about tailwind is how amenable it is to AI-generated workflows. It seriously works _so good_. Anything can be expressed, and systematically. There's something amazingly useful there. reply esperent 10 hours agorootparent> what I find amazing about tailwind is how amenable it is to AI-generated workflows It's highly amenable to human workflows as well. Of course, humans are more disparate than AIs so it doesn't suit all humans no tool can. But it sure does seem like a majority of humans web developers like it. reply ebiester 2 hours agorootparentprevSo, I wonder how AI is going to work with the new upgrade. Are teams going to delay upgrading until there is proper Claude support, for example? reply thangngoc89 2 hours agorootparentShould work well since there aren’t any removal of old features and classes. reply tipiirai 14 hours agoparentprev> I suspect many of the pro/anti-Tailwind arguments are no longer relevant I feel there are two issues with Tailwind for me as a designer / design engineer. * First, JavaScript/Tailwind engineers have hijacked the conversation on design. Instead of \"utility-first,\" \"dead code elimination,\" and \"type-safe CSS\" I focus more on desing systems. Whether to use Perfect Fifth or Perfect Fourth in typography for example. * Second, Tailwind makes it impossible for me to participate in the actual craft. Design decisions get buried in React components with cryptic expressions like `flex items-center shadow-lg p-6 hover:bg-gray-50 dark:bg-gray-800 py-[calc(theme(spacing[2.5])-1px)]`. This might make sense for JavaScript engineers, but blocks/makes it hard for systematic design. Instead of expressing precise mathematical relationships through CSS, we're essentially writing inline styles with better ergonomics. reply Foreignborn 13 hours agorootparentTailwind has solved a lot of problems. And this is coming from someone who doesn't use it half the time. Some thoughts about this topic, not necessarily in disagreement: Designers seldom think in the way you describe (explicit computer-readable rules, systems of design). A lot of designers are coming from crafts that are more implicit. It doesn't surprise me design business logic is not the topic of conversation. You need to update your tailwind classes to reflect your design business logic. Don't do \"bg-gray\", do \"on-surface-emphasis\" or whatever business logic. This is just a simple config change. You probably brought up modular scale as just one example, but modular type has become near myopic dogma of frontend engineers who haven't had any typesetting experience. A scale of 1.25 vs 1.5 is so in the weeds as to be useless. There are so many alternative ways to set type (e.g. take lessons from CJK typesetting) or innovate typography on the web (e.g. build your own leading-trim implementation!). reply tipiirai 13 hours agorootparentI'm a designer, so I absolutely think in terms of design systems where typography is just one part of the puzzle. There are, of course, many ways to set type — but the point was: no Tailwind engineer talks about typographic scales in the first place. Systematic thinking is absent, which is naturally built in CSS. reply brightball 6 hours agorootparentTailwind itself is a design system. The authors actually put together a short book explaining their point of view on design systems (without referencing Tailwind) and I found it very difficult to argue with any of their points. Then you look at Tailwind and realize it’s almost entirely that book packaged for others to use. https://www.refactoringui.com/ reply threatofrain 13 hours agorootparentprevI'm kind of surprised why this is even relevant to you. The designers I know work in Figma. The designer and the engineer collaborate to make sure the design isn't too hard to implement, but other than that the designer shouldn't care about whether developers are using Tailwind, StyleX, Sass, vanilla, or whatever. I hardly expect a designer to take responsibility for something as transient and idiosyncratic as \"oh, but this is hard in Tailwind.\" reply skydhash 14 hours agorootparentprevMaybe Tailwind is useful for bigger projects, but I tend to prefer component based class instead. The biggest DX for CSS is using the web inspector and directly modifying the DOM, then copying the adjusted values to the text editor. For a much serious project, I go with the balsamiq > figma route to create an actual design system. reply tambourine_man 6 hours agorootparentYou can allow the inspector to access the filesystem and modify the files without having to copy/paste for even better DX. reply d1sxeyes 12 hours agorootparentprevNo one cares what your typography ratio is if you can’t get it in front of a user quickly enough (dead code elimination), reliably enough (type safe), or cost effectively enough (i.e. if you have to pay for more developer time to implement the same work). Tailwind is a tool for craftspeople, not artists. It democratises design by making it simpler. For folks that are really good designers, they should be happy that they can up their prices knowing that customers are paying for their expert eye, rather than just the basics again and again. reply threatofrain 1 hour agorootparentTailwind does not democratize design. Tailwind is generally 1:1 to CSS, sometimes 1:2 or 1:3. It's such a thin layer over CSS that you have to remember all the CSS. That does not make design easy or hard. You don't give people Tailwind and end up with design as beautiful as ShadCN, even though ShadCN in some perspective is just another wave of stereotypical startup design. The ability to go from blank button to beautiful Tailwind button is all on the author, not the fact that you're writing in a new lightweight inline syntax. When I read the Tailwind book, my takeaway was their design philosophy was about the speed of iteration and how that interacts with designs whose parameters are too interdependent. Tailwind follows that philosophy by encouraging ad-hoc repeated edits over DRY. In that sense StyleX also fits the Tailwind philosophy very well even though StyleX is an even thinner layer over CSS. reply d1sxeyes 1 hour agorootparentI don’t really agree. You have curated colour palettes, so if you want a red, you’ve got red-50 to red-950, rather than an entire RGB colour space. Similarly for font sizes, you don’t have to decide on a pixel size, just whether it’s small, large, extra large, etc. Border radii. Border widths. Padding. There are lots of “sensible” defaults so you don’t have to pick even the units (should I use px or pt or em or rem or vw or ch or…) before we even talk about the numbers associated with those units. reply tipiirai 10 hours agorootparentprev> Type safe typography ratio Please explain how this is better than expressing typographic scales with CSS variables and calc()? reply d1sxeyes 8 hours agorootparentYou're using a right angle bracket there like I said that. I didn't. I would find it difficult to explain, especially given that this is exactly what Tailwind does: https://tailwindcss.com/docs/font-size. reply Ghoelian 3 hours agorootparent> No one cares what your typography ratio is if you can’t get it in front of a user [...] reliably enough (type safe) i mean it's not exactly what you said, but c'mon, that's obviously what they meant. reply d1sxeyes 1 hour agorootparentOk fair enough. I was actually talking about the whole of Tailwind rather than just the typography ratios, but I can see how I could be understood in that way. My mistake. Either way, Tailwind works in exactly the way OP describes as their counter example. reply 4m1rk 13 hours agoparentprevI just want the color palette of the Tailwind CSS. reply arielcostas 10 hours agorootparentTheir palette is really neat. I always check it out when I need colour inspiration, especially for \"cold greys\" and other \"state\" colours (green for success, red for error, yellow for warning...). reply jgalt212 6 hours agorootparentprevYes, someone should make a tailwind inspired bootstrap theme. reply gedy 5 hours agorootparentI actually did this for a former company that had a Bootstrap-based app the designers started bugging that \"we need to use Tailwind\", and I couldn't figure out why they cared what tech we used. In talking with them, realized they just were bored with our theme, ha. Not publicly accessible though, sorry. I should probably recreate this. reply agos 8 hours agorootparentprevyeah, the palette is neat. My main gripe with Tailwind is that it blends a set of design tokens (which I like) with a technology to apply them (which I don't like) reply dbbk 5 hours agorootparentJust copy and paste them out? reply vim-guru 11 hours agoprevCSS has become significantly more user-friendly than in the past, with most browsers now behaving consistently. It's worth learning as there is no build step involved, and it avoids cluttering your markup with excessive code. You could opt to use style attributes directly within your HTML. Historically, we avoided this to maintain a separation of concerns, but it's puzzling why some prefer reintroducing similar methods. Is it just to save a few keystrokes? Using style attributes even seems more straightforward since it doesn't require translating code in your head. I simply don't see the appeal. reply morbicer 11 hours agoparentThere's a bunch of things you can't do with style attributes, just to pick a few: Pseudo-classes (e.g., :hover, :focus) Pseudo-elements (e.g., ::before, ::after) Media queries Keyframes and animations And the DX in a larger project isn't great either. On the other hand, this upcoming standard is a great addition for collocating styles https://developer.mozilla.org/en-US/docs/Web/CSS/@scope reply nilslindemann 1 hour agorootparenthttps://developer.mozilla.org/en-US/docs/Web/CSS/@layer also solves a lot of problems. reply jitl 3 hours agoparentprevThese days tailwind is often paired with a component abstraction system that takes place of css classes as an abstraction system. Component abstraction achieves an even more powerful separation of semantics from styling, compare: Cool page vs: Cool page In both cases you need to go find the “hero” abstraction, but in component world we are also abstracting over the HTML tag name in addition to whatever styling stuff. When it comes to defining the “stylistic detail” in either case, it has been separated from the “content” which is our page source file. I am happy to pay a build step to get better separation of concerns content from presentation. reply aembleton 9 hours agoparentprev> I simply don't see the appeal. Not having to name CSS classes Easier to read as it's inline with the rest of your HTML Standardised way of naming so that everywhere uses the same conventions Not having to manage a separate file and remove unused components Being able to use media queries, which I couldn't do in a style tag No risk of changing a class that is used elsewhere. CSS has global scoping which has its benefits but is risky. reply agos 8 hours agorootparentno, a single html attribute containing \"relative before:absolute before:top-0 before:h-px before:w-[200vw] before:bg-gray-950/5 dark:before:bg-white/10 before:-left-[100vw] after:absolute after:bottom-0 after:h-px after:w-[200vw] after:bg-gray-950/5 dark:after:bg-white/10 after:-left-[100vw]\" is not \"easier to read\" (example taken at random from the Tailwind homepage) reply oneeyedpigeon 5 hours agorootparentJust viewing the source of the Tailwind homepage breaks me out in a cold sweat. And then there's: > console.log(document.querySelectorAll(\"body > script\").length) 27 reply aembleton 6 hours agorootparentprevEasier to figure out what to change without affecting other HTML components, though. reply superq 2 hours agorootparentIndividually applied styles kinda defeats the purpose of cascading style sheets though. As someone who's quite conversant with regular CSS, I really did like the ease of applying whatever style I wanted right in the code without needing to \"worry\" about whether this div is a \"panel\" or a \"hero\" or whatever, but it really does tend to make a huge mess of my HTML in very short order. reply skeletal88 9 hours agorootparentprevThe problem is that with bootstrap I have card, table, etc. With tailwind I have an unreadable alpabet soup when I am not a designer but a developer and am not familiar with tailwind. reply Ralfp 8 hours agorootparentAnd then you build on that, and you have card-alert, card-message card-message-small, card-body-alert, card-body-message, card-form, card-login-form together with extra sass variables for them. And then you ask \"why not just have variable for each component, not each component's style?\" and this is where tailwind steps in. reply mablopoule 5 hours agorootparentprevThis is the big one for me. In any big enough or unfamiliar frontend project, my go-to way to explore the codebase is to launch the front-end, and use the inspector to check the elements, and then search-back in the codebase the relevant classes to see what page/component display that, and how does it fetches the data. If there is only an alphabet soup, I no longer have a simple set of classes I can grep the codebase with to quickly find the relevant components. reply gedy 8 hours agorootparentprevI greatly agree with you, but should mention that there is DaisyUI which is a Tailwind plugin that adds these for you. It seems inspired by Bootstrap and others: https://daisyui.com/ reply 65 3 hours agorootparentprevBig one: Way smaller CSS bundle size. reply lawn 8 hours agorootparentprev> Easier to read as it's inline with the rest of your HTML I really don't think a big blob of text with a bunch of unnamed divs is easier to read. The styles night be easier to find but they're absolutely not easier to read. reply tipiirai 9 hours agoparentprevIt's because we've normalized React and tight coupling. Styling belongs to components, not in design systems. I feel the JavaScript-first engineering ecosystem needs brave new design-led companies, who see the power of modern CSS and systematic design. reply dbbk 5 hours agoparentprevYou're seriously advocating for inline styles in the year 2025, it does not surprise me you are confused. reply itzami 9 hours agoparentprev> Historically, we avoided this to maintain a separation of concerns, but it's puzzling why some prefer reintroducing similar methods. Is it just to save a few keystrokes? In bigger projects, if we start looking at the amount of files one has to deal with, Tailwind becomes very appealing. We've went through the regular `.css` route but then you have weird names, and, potentially, duplications or even conflicts. `css modules` is an option but you've now essentially duplicating the number of files that you have for each component / page. `sass` or `less` essentially bring the problems from `css modules` and regular `.css` into one. I don't inherently like or dislike Tailwind (although I very much started by absolutely disliking it) but you feel its value in a project with 200+ files composed of components and pages reply asimpletune 9 hours agoprevI used tailwind for my site and ended up liking it a lot, but maybe for reasons that are not often mentioned? The real value of tailwind is being able to read all the styles that affect a an element in one place. Sure the classnames can get long, but it’s still a lot faster to read that long line, than open the browser tools each time or scroll up and down one (or many) stylesheets. Then when you come back to that code later, you can confidently edit it without worrying about causing changes to other elements. The cascading nature of CSS is still helpful, but I find that I usually want to limit my styling to just one element. For all the rest I go back to css but I wrote very little css because I usually just want to get the whitespace right. reply twelve40 1 hour agoparent> real value of tailwind is being able to read all the styles that affect a an element in one place there are helpers for that in the IDE (e.g., \"Show Applied Styles for Tag\" in Intellij) or even in chrome dev tools that show all this in a readable way. Tailwind snippets mentioned in this HN topic look kind of scary in comparison. reply jitl 3 hours agoparentprevI think that is exactly why tailwind is good and nice. Although I also like that it makes “how do I do X?” a quick doc search instead of a 30 minute research project reply topicseed 8 hours agoparentprevI kind of agree but the same problem exists. If you add text-xl to a div and have nested divs and spans, they'll all be inheriting text-xl so for these nested elements, you also have to look up the element's lineage to find what's causing the text-xl. But overall it's a bit easier yes. reply javier123454321 3 hours agorootparentThat's the css cascade, not a tailwind issue. reply nakovet 16 hours agoprevI love tailwind, used in 3 projects in the past 4 years, it’s intuitive, well documented, simple. I don’t miss the days of emotion and styled components where I would have to think of a name for every styled div in the project, with tailwind a container is just a div and a few classes nothing else. Less bike shedding discussion, less brain cycles spent naming things, less time wasted in reviews. reply dbbk 8 hours agoparent> I don’t miss the days of emotion and styled components where I would have to think of a name for every styled div in the project I see people mention this issue fairly frequently, and it puzzles me a little. I have never once spent probably more than 1 second thinking about what to name something. Is it actually a blocker for some people? Are you really paralysed by this? reply gausswho 6 hours agorootparentThere was a time when BEM ruled (and scarred) the world. But names aren't a problem if you abandon the C in CSS and scope all styles to component. reply Kerrick 2 hours agorootparentWhat I don't understand is why BEM is scarring. If you're talking about the reason or timing something looks different, you must have named it in that conversation. Just use that name. \"When the menu is wide, the items' icons and text should be visible. When it's skinny, only the icons should be visible. This button toggles between them.\" .menu {} .menu--skinny {} .menu--wide {} .menu__icon {} .menu__text {} reply werdnapk 3 hours agorootparentprevI'd say BEM was a passing fad, but didn't rule the world. I also think tailwind will suffer the same fate, but we'll see how it plays out. reply agos 8 hours agorootparentprevespecially since with any decent CSS solution (any css in js, or CSS modules) you can reuse names reply dbbk 5 hours agorootparentLike really, I use Styled Components, just call it a Wrapper or InnerWrapper or Row or whatever and move on reply notjoemama 16 hours agoparentprev> a few classes You must be working with very good product owners then. The ones I've worked with love to specify the hell out of every possible detail. Like a web form is their personal HGTV renovation. I tried tailwind once and the classes ended up being an order of magnitude more than the markup. It got hard to read, quickly. reply lowercased 15 hours agorootparentyou can use @apply to merge the various utility classes together in to something resembling a name, like .btn-primary { @apply py-2 px-5 bg-violet-500 text-white font-semibold rounded-full shadow-md hover:bg-violet-700 focus:outline-none focus:ring focus:ring-violet-400 focus:ring-opacity-75; } But their v3 docs seem to be very against this. \"Whatever you do, don’t use @apply just to make things look “cleaner”. Yes, HTML templates littered with Tailwind classes are kind of ugly. Making changes in a project that has tons of custom CSS is worse.\" Reasons: * You have to think up class names all the time — nothing will slow you down or drain your energy like coming up with a class name for something that doesn’t deserve to be named. * You have to jump between multiple files to make changes — which is a way bigger workflow killer than you’d think before co-locating everything together. * Changing styles is scarier — CSS is global, are you sure you can change the min-width value in that class without breaking something in another part of the site? Yet... my experience using projects that use tailwind is that every button everywhere is styled the same way, but it's repeated in multiple areas. The 'kinda ugly' part, but also... it's repeated in multiple places. Trying to change the universal focus behaviour of buttons in a project like this is hard, because... I can't search focus:outline-none without finding everything that has 'focus:outline-none' on it. I can't just search/replace, because it'll impact other stuff. So I end up spending way too much time trawling through way too many scattered specific styles all over a codebase, vs having a defined 'btn' style someplace. \"well, that's just the project's fault\"... possibly, but it seems to be the promoted/preferred/evangelized way of using tailwind, judging by the projects I've had to get involved with the past few years. IME, this approach may have good short term benefits, but poorer longer term maintenance, doubly so when the original people are no longer involved in the project, and outsiders have to come in to deal with it. reply msoad 15 hours agorootparentI agree with Tailwind's stance on this. You really don't need @apply if you're breaking things down to smaller components. I often see people have things like text1text2.... This is where I think we need a linter to warn against things like that. Make those 's a component! reply omnimus 9 hours agorootparentI have bern heavy Tailwind user from before v1. It actually doesnt matter where you make the class grouping. You can make it in your templating language or you can make it in @apply. I prefer @apply because the same grouping can be reused if done correctly. @apply is great. The reason they are discouraging @appply is because its hard feature to implement and many people dont understand how it works so they get to problems and create issues. It bothers Authors so much they have always been considering taking @apply out but i think they know big chunk of the userbase would leave to different similar project. reply tuzemec 9 hours agorootparentprevWait, what? You want to make every li/dt/etc. component? That's insane. reply davedx 10 hours agorootparentprevFor buttons, I think the 'blessed' way to do it is to use (react) components and have the tailwind classes encapsulated there. It makes sense; you have a layer of abstraction where the implementation of your theme is in the \"lower level building blocks\" of your components. (Call it a component library if you want) I generally only use @apply when there are some heavier external interactive libraries that are hard to style. Like for example if you're embedding markup in your application, and you need to apply a set of styles to the markup it generates, and you can't do that directly in react because you don't control that part. reply djhn 9 hours agorootparentprev> I can't search focus:outline-none without finding everything that has 'focus:outline-none' Would a regex not work? Something like ]*?focus:outline-none reply Aeolun 12 hours agorootparentprevI generally use tailwind in combination with daisyui, and I don’t think about styling a lot any more beyond choosing the proper theme. reply fintechie 8 hours agoparentprevGave it a go for several projects, but didn't like it... for big projects it gets messy, fast. It also feels like it has become the new bootstrap. I'm very happy with my current CSS-in-JS workflow. Crafting good old css with LLM help. You just show the LLM a pic, ask for the components.... boom, done (with proper naming, etc) reply _heimdall 7 hours agoparentprevThis is actually one of the main reasons I avoid react when I have the option. Styling in react is terrible, I'll use tailwind with it but only as the least bad option. If I am using a frontend framework and a build step, svelte and astro are nice depending on the use case. I can style with plain old CSS and rarely have to reach for class names, if you keep components small you can get away with element selectors and let the framework scope styles at build time. reply bern4444 13 hours agoprevMy initial reaction to Tailwind was: what a pain. I already know CSS, now I have to learn CSS again... I imagine its how parents might feel when they go to help their children with math homework only to find the math is now totally different from when they were in school and their methods (while still valid) for solving problems are no longer accepted by the school and their child is annoyed by having to reconcile the two systems... I get the, tailwind works well in a team and scales nicely and you just have to use it, mentality. I've experienced it. I still prefer a plain style sheets that targets elements by a class or tag (especially since CSS supports nested selectors!). There is an issue in managing stylesheets, and for that I really like how Remix/React Router manages CSS where CSS flies are defined and applied at the route level. Where that isn't enough or dynamic styles are too complex for a .css file: style={{...}} is always available. CSS is so powerful, flexible, and extensible, tailwind feels like a limitation rather than an enhancement. I don't understand the continued appeal, but clearly many others do. I'm not sure why, but I am confident we'll all have moved on to something else in another 3 or 4 years too. reply jitl 13 hours agoparentI agree it’s frustrating initially to re-learn CSS… I used to make the same arguments. But after contributing to an open-source project using Tailwind I got up to speed, and now I feel differently. Tailwind is essentially just stenography for style={{…}} a single utility takes 15 characters that would take multiple lines in a .css file or style object. It greatly reduces the amount of time I need to spend googling stuff like “visual text replacement css” or perusing various css cheat sheets or skimming through css-tricks blog posts from 8 years ago. Instead I always go to tailwind docs, and quickly learn the utility class that just does the thing I want. A surprising amount of it fits in my brain cache, much more than trying to cache the css for all the tasks I might need to do in a layout. reply Aeolun 12 hours agoparentprevI think it’s because now your entire component is contained in a single file. No more separately messing around with CSS. The styles are right there on the element they apply to without having to cross reference anything. You could do this with just the css attribute, but that has the issue that everyone has always been taught it is wrong to do that, and a list of strings that contain a bunch of utility is easier than writing those plain CSS objects. reply arkh 11 hours agorootparent> You could do this with just the css attribute, but that has the issue that everyone has always been taught it is wrong to do that But why is it wrong to use the style attribute? What makes using tailwind to do the same thing \"not wrong\"? Also this https://tailwindcss.com/docs/hover-focus-and-other-states#us... and https://tailwindcss.com/docs/hover-focus-and-other-states#st... look like reinventing the Cascading of CSS with an awful syntax. reply atonse 2 hours agorootparentI think it's also about automatically purging and slimming down your CSS, which is easier to do when you're just comparing tokens. Although nowadays I'm sure they could technically do the same thing with the built in parsers without having to rely on just simple tokens. reply sebmellen 13 hours agoparentprevLimitation reduces available complexity and that’s why tailwind wins. It’s a composable, standardized, and modularized wrapper around a very massive and complex system. That’s a big advancement. reply nedt 9 hours agoparentprevI'm learning math with my kid and it's not different to what I learned. Can still solve everything quickly. On the other hand there are those posts on social media mixing multiplication and division and telling you there is only one true answer to those. Tailwind isn't that bad, but it's more into this direction for me as an old fart ;) reply mrits 3 hours agorootparentIn the US you can get all the right answers on a test and still fail reply xutopia 34 minutes agoprevAnyone who has ever had to deal with CSS over time knows that the biggest issue is having leftover classes. Utility first approach means this never happens. reply davidw 15 hours agoprevQuestion for people who are good at CSS stuff. I am not. I'm upgrading a personal Phoenix project to 1.7, and Phoenix now uses Tailwind by default. So I thought I'd try and update my one page thing to use it instead of Bootstrap. So far, it looks like crap whereas the Bootstrap one looked 'good enough'. What's the easiest way to get something that looks kinda sorta decent, with some nice defaults, without trying to become a designer? Or should I just reinstall Bootstrap and be done with this. reply jmull 14 hours agoparent> Or should I just reinstall Bootstrap and be done with this. That one. Tailwind gives you tools when you want to control the design, but you’ve got to control the design. When you just want something that looks good and don’t mind that bootstrap-y feel, bootstrap is the way to go. (There are projects that essentially recreate bootstrap on top of tailwind, which, in theory, might give you something like bootstrap as a starting point with the power of tailwind, but I haven’t seen it work better in practice than just using bootstrap.) reply ricw 12 hours agorootparentDisagree. Just get one of the tailwind UX kits and use their classes as your standard style guide. I personally use flowise as it’s free with paid more complex components if needed, but there are a bunch of others out there that fill the same space. reply _heimdall 7 hours agorootparentOn a new project, sure maybe going with tailwind + a theme/kit is a bit more up to date than using bootstrap. But on an existing page that already looked fine with bootstrap, why bother with the extra work? reply addandsubtract 8 hours agorootparentprevThe same can be said about Bootstrap, though. Just get a theme to make it look less bootstrapy. I think both viable ways to go, and it just boils down to personal preference (especially for hobby projects). reply jmull 5 hours agorootparentprevThat's fine (I've done it). It's just that bootstrap is better at it. reply skeletal88 8 hours agorootparentprevCan you suggest which of these kits are good? reply KolmogorovComp 6 hours agorootparentDaisyUI reply floydnoel 15 hours agoparentprevI use DaisyUI to accomplish that goal, here is my demo site for an example: https://nwk-landing-kit.netlify.app and here are the docs for DasiyUI: https://daisyui.com reply davidw 14 hours agorootparentThe DaisyUI description of the Tailwind button made me laugh: \"bg-indigo-600 px-4 py-3 text-center text-sm font-semibold inline-block text-white cursor-pointer uppercase transition duration-200 ease-in-out rounded-md hover:bg-indigo-700 focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-indigo-600 focus-visible:ring-offset-2 active:scale-95\" Yeah, that's not making my life easier is it. Also some of those class names are super cryptic. reply loxs 13 hours agorootparentThey are cryptic but the learning curve is not that steep. Once you get the idea, it becomes much easier (and quite pleasant for a CSS hater like me). For example px-3 means \"padding on the x axis 3 (spacing values)\" and gives you padding on both left and right. There is a (rather simple) language that you need to learn and at least for me it made things much, much better. You can probably get the feel for it in several hours of building a basic website with Tailwind (and DaisyUI I would recommend). reply homarp 13 hours agorootparentia that really easier than learning css directly? since you have to understand 'paddding'and 'px' ? see https://developer.mozilla.org/en-US/docs/Web/CSS/padding vs https://tailwindcss.com/docs/padding reply cloverich 2 hours agorootparentAs someone who used and knew CSS very well, and reluctantly used tailwind, yes. I don't strongly prefer one or the other. But I will say now that I know Tailwind, when I see those large class name strings, my brain parses them shockingly fast, and they kind of melt into the background similar to comments. Also, common linter rules will order the classnames, and that they are always the same between disparate tailwind projects, all together its not nearly the annoying mess I thought it would be on first sight. reply throwup238 12 hours agorootparentprevOne of the reasons Tailwind became so popular even more so than preceding frameworks like Bootstrap is that a lot of people who normally liked working with CSS directly fell in love with it. It removes the extra step of editing a separate CSS file without straying far from a 1:1 mapping and eliminates most of the cascading rules that have plagued frontend for decades (`group:` and friends notwithstanding). The latter is important when working on large teams because it eliminates the name bikeshedding and conflicts. reply agos 8 hours agorootparentsorry, but this is just not true. The cascading issue is still there, and it gets even worse because if you have an element with class \"p-1 p-0\" you cannot know in advance which will take precedence. Of course you shouldn't do that, but it happens to be the case that Phoenix does exactly that reply isleyaardvark 1 hour agorootparentIf the alternative is: .foo { padding: 0; padding: 1rem; } then I can see the precedence but I don’t see how it’s an advantage. You can write bad code in any methodology. That seems like an issue with Phoenix and not Tailwind or the Tailwind approach. reply ahofmann 7 hours agorootparentprevThis is not a cascading issue. This is a \"You're holding it wrong\" issue. And when Phoenix does write 'p-0 p-1', Phoenix and Tailwind are not a good match. reply Lockal 6 hours agorootparentprevYou suddenly reminded me about another good Tailwind thing: linter will highlight \"p-1 p-0\" as a mistake. I've never seen linters that warn on \"your-custom-p1 your-custom-p0\", but Tailwind makes it possible. reply floydnoel 3 hours agorootparentprevit's quicker to write, though. and many of these class names are the same in bootstrap, so I've been seeing stuff like 'p-0 m-1' in frontend projects for decades. another huge benefit these days is with the styling living alongside the markup, it is very easy to use chatGPT or other AI tools to copy/paste and have changes made, etc. it is much less work with one file vs multiple. you can give the context super easily and take the results very quickly. reply loxs 13 hours agorootparentprevI am not going to argue whether that's the case, as I never managed to learn CSS enough to be productive in a convenient (for me) way. Despite of trying many times. Whereas tailwindcss \"clicked\" right away and gave me the awesome superpower to build whole products without having to depend on other people. Before that I hated myself for the horrible, unmaintainable mess that I created, and double hated myself later when I had to revisit the project. Since I am using tailwindcss, I never had bad experience. Of course I still have to spend time wondering how to center some element and why it doesn't work, but that's not what I am complaining about. It might be just a personal defficiency, but it is how it is (for me). reply moojacob 15 hours agoparentprevIf your looking for components like bootstrap try https://flowbite.com/ for free. Tailwind also has paid components. You just copy and paste. The cool part is once you get more comfortable you can start to tweak the flowbite components. reply darkhorse13 10 hours agoparentprevYou can check out Halfmoon too instead of Bootstrap: https://www.gethalfmoon.com/ reply bitshaker 15 hours agoparentprevPut your HEEx into a LLM. They understand both Bootstrap and Tailwind very well. It will probably not be too hard. reply bushido 15 hours agoparentprevTailwind and Bootstrap and are kinda apple vs apple pie. Tailwind provides classes that still need to be stacked to build components. It'd be easier if you were using some source for UI components, I've used TailwindUI in the past, but that's paid. I'm sure there are good free alternatives. reply newusertoday 15 hours agoparentprevreinstall bootstrap and be done with it. You can look at daisyui it is closest to bootstrap. There is also shadcn ui, ui looks good but it would take you effort to port styles to your phoenix project. reply kyleee 15 hours agoparentprevI’d say it depends on who is paying you. If you have job security and want to tinker, then play with tailwind if you want (most trendy / loud people seem to like tailwind). If however you are working on your own project / are self funded / or have a deadline just stick with bootstrap that you know, and focus on getting your work done instead of bike shedding on the UI reply rglullis 18 hours agoprev\"Build times reduced\", \"no more @tailwind directives\", \"No more js configuration\", \"Designed for the modern web\". To me, this screams \"CSS now can do everything we used to do ourselves, but let's keep pretending that we are still needed somehow\" reply crowcroft 17 hours agoparentIf you going to hate on it, at least try understand what it is and why people use it. reply rglullis 17 hours agorootparentIt's not \"hate\". It's just that I don't see the value that it brings today. reply ervine 16 hours agorootparentWhether you enjoy utility classes isn't really related at all to what css supports. reply rglullis 16 hours agorootparentMaybe you are making my point? If tailwind was \"just utility classes\", fine. But what business would it be there if they just published a bunch of CSS files with their styles? With modern CSS, what problem are they really solving now? reply kyleyeats 16 hours agorootparentAs someone who made a competing product along these lines (that got no attention or traction): Tailwind gives CSS a \"place\" in the codebase. It benefits orgs, not necessarily apps. I didn't get it at first either. But it's very useful to the people to whom it's useful. reply rglullis 16 hours agorootparent> Tailwind gives CSS a \"place\" in the codebase. It benefits orgs, not necessarily apps. That is a better argument. But couldn't we be able to achieve that by, e.g: Create one standard HTML document with a predefined structure and including all the web components needed by your product. Having all designers and frontend developers developing their CSS (or SCSS) against this single base document This would be basically the CSS Zen Garden approach. It would still keep separation of content and styling and it would create a \"place\" for styling code. reply jitl 13 hours agorootparentThis works for products with a very limited scope that are mostly content focused, with a very small design team so everyone can agree to use the very limited toolkit in standards.html. The approach of saying “All ui shall only use these 29 components, anything else is forbidden” is not going to fly in a company with 10s of designers and 100s of engineers. We tried this style at Airbnb and it turns out forcing all UI changes from either designers or engineers to acquire a single exclusive lock on standards.html leads to a fuckton of contention and frustration, and soon people are just going to yolo their own thing totally ignoring the pristine blessed system because the system doesn’t work. The art of design systems isn’t a single technical approach it’s finding an optimal workflow so your design engineers can build a UI toolkit that your product teams will actually adopt and contribute to, within the constraints of your existing tech stack & organization. reply csomar 15 hours agorootparentprevThere are more \"modern\" ways to do this for large organizations like Storybook. Tailwind is a proposition for a small business that has a very small budget, want to buy a ready-theme and then make slight modifications. In a sense, no, it doesn't make any sense beyond that to use it. And once you use a good React/web-components framework, you realize there isn't really much value there as you shouldn't be really changing the CSS from page to another. The process for large orgs is tedious and too lengthy/expensive for the small ones. Imagine having to go through planning, visualization, creating the component (or adding props for customization, writing e2e tests, publishing it to storybook with docs, and then finally adding it to your page and get it pipelined in the merge CI/CD process. A small org with tailwind just open the page in question and add a class to the html element. reply _heimdall 6 hours agorootparentprevIn my experience, the main problem it solves is the fact that styling in react is still a pain in the butt. reply crowcroft 16 hours agorootparentprevWhat value does Rails provide to Ruby? reply rglullis 16 hours agorootparentDoes Ruby have an ORM/HTTP request handler/URL router in the standard library? I am not sure I follow the analogy. If all you are taking from tailwind is the utility classes, fine. This is the part that provides value. But tailwind is not just that, is it? reply crowcroft 15 hours agorootparentRails is a fully formed opinion on how you should write a web app in Ruby and all the various technical components you need to fully realize that opinion. Tailwind is a fully formed opinion on how to use CSS (utility classes etc.) and all the various technical components you need to fully realize that opinion. Thinking of Tailwind as just a bunch of regular CSS utility components is misunderstanding the scope of the project. That alone has a lot of tradeoffs and compromises the file size is absurd and you can't bundle things into components, you also can't have arbitrary values in the utility classes. It wouldn't be a complete opinion on 'the best way to use utility classes', or however you want to frame Tailwind. reply crooked-v 17 hours agoparentprev\"Basically just inline CSS, but less fiddly and much more optimizable than the style attribute\" was always the selling point of Tailwind in the first place. reply Sateeshm 15 hours agorootparentBut it's not just inline css though. The length of inline css string would almost always be 3 times longer than tailwind classes string. Also inline css doesn't allow media queries, pseudo classes, etc., which is a big deal. reply rglullis 16 hours agorootparentprevThen, yes. But nowadays CSS is a lot more powerful and has caught up. So why bother with Tailwind? reply jitl 12 hours agorootparentCSS has become more confusing over time, not less confusing, so the upside of a utility class approach like Tailwind is also improving over time rather than decreasing. Tailwind solves css confusion by presenting a “paved road” for many tasks. Want to do “thing”? Just look up “thing” in the tailwind docs, add “thing-2” to your class, done in 2 minutes. Rather than look up “thing” on google, skim several articles, then write 4-10 lines of CSS, done in 30 minutes. As CSS has gotten more complexity from new capabilities, solving various tasks with it feels more and more like a research project. Like centering a div vertically and horizontally there’s now 3-4 generations of solutions for this task: 1. Hacky stuff with position: absolute or floats 2. Flexbox approach. Hope you remember the 6 different attributes you need to set! Oh, there’s a new shorthand for flex now? Cool! More stuff to learn. 3. Grid approach. There are a bunch of different ways to structure a grid centering approach. Hope you remember how grid templates work! With tailwind, just add 2 classes, done. reply rglullis 6 hours agorootparent> With tailwind, just add 2 classes, done. But you can do that with a library of SASS mixins as well. Going through some of the answers I'm noticing that another \"issue\" I have with the tailwind approach is that it works by providing a post-processor when in reality most people just need a simple preprocessor and macro system. reply jitl 4 hours agorootparentSure? I don’t see how that’s an argument for CSS “being advanced enough” SASS ain’t CSS. I like SASS (I wrote a lisp in SASS in 2011 lol) and that’s actually how I think about Tailwind utilities it’s just using SASS mixins directly in your HTML. I would rather use Tailwind because it will take 1/3 the levels of abstraction, 1/4 the keystrokes and 1/2 the files to do the same job to: 1. Pick a class name (myclass) if people are using BEM or something this requires some algebraic thinking. 2. Write the class in file.scss to define the class and include mixins A, B, C 3. Add class=myclass in my html. Vs tailwind: 1. Add class=“A B C” to my html. I don’t really get pre processor vs post processor, it’s a build step to run at some point before I deploy with either tool that will produce some css files. ¯\\_(ツ)_/¯ reply rglullis 3 hours agorootparent> I don’t really get pre processor vs post processor, it’s a build step to run at some point before I deploy with either tool that will produce some css files. Not quite the same. If it's only a preprocessor, the tool is only dependent on your styling code. A post-processor needs the whole application. It's a lot easier to add or remove a pre-processor from your development process, and for those that want to consume your styles it is a lot nicer when you don't have to adopt their tooling as well. E.g, I can create a \"utility class library\" in SASS, generate the CSS and let people just import that directly into their web pages. Is that possible to do with tailwind? reply rglullis 3 hours agorootparentprev> I don’t see how that’s an argument for CSS “being advanced enough” That's a separate argument. I hear mostly two justifications for tailwind: 1. It is (or used to be) hard to do things in CSS like variables, calculated properties, themes, scoped rules / namespacing. (Pre/Post) Processors are going to be required anyway, so we might as well use Tailwind 2. It gives frontend developers and designers a common language (the utility classes) which makes it easier to establish workflows. My argument for (1) is that CSS already supports a lot of things that used to require processors, and (2) could be achieved with a simple \"library\" that could just be imported directly. reply BeetleB 15 hours agorootparentprevDo you have any links describing what you mean? I'm not a CSS guru, and perhaps I know only the \"old\" CSS. I did recently learn Tailwind and it's way nicer than the \"old\" CSS. If modern CSS makes styling as easy as Tailwind does, I'd love to read about it. reply Sateeshm 15 hours agorootparentprevBecause of multitude of reasons. One of the big ones being never having to search through a style tag or finding a CSS file. 100% of the element styling is described in the element markup. Tailwind was never about filling gaps in CSS. reply someothherguyy 15 hours agorootparentThat is atomic CSS, and it predates tailwind by quite a bit. There is a huge marketing push behind tailwind. If you look into who made it, you'd see it wasn't their first attempt. reply listenallyall 11 hours agorootparentprevA concrete example that seems like a no-brainer to me: let's add a little padding to the top and bottom of an element. \"py-2\" Done. In CSS, specifying padding (or margins) is 4 values... except I forget which is the first one. And what units should I use... px, pt, rem, em. Inline, same-page style, or an external .css file? If not inline, what should I name the class? Vanilla CSS literally requires 10x more time and mental attention. reply megaman821 1 hour agorootparentYou can using padding-inline and padding-block with var(--spacing-2). Tailwind is styling utility classes and design tokens in one. There are premade design tokens you can use the CSS styling though. reply rglullis 6 hours agorootparentprev> \"py-2\" Done. In CSS, specifying padding (or margins) is 4 values... except I forget which is the first one. False dichotomy. You are not stuck between tailwind and \"Vanilla CSS\", and one can solve the issue you are presenting with any reasonable set of SASS mixins. > If not inline, what should I name the class? Also solvable with good old SASS. reply buzzerbetrayed 15 hours agorootparentprevWhy do you keep saying CSS has “caught up”? Tailwind has always just mapped to CSS, so you’ve always been able to do anything in CSS that you could do in tailwind. What has CSS done that makes you see no value in Tailwind today where you used to see value? reply crummy 17 hours agoparentprevWhat did you need Tailwind for before that you couldn't do in normal CSS? Doesn't Tailwind map pretty much 1:1 to CSS? reply rayrrr 13 hours agorootparentEvery programming language maps 1:1 to CPU instruction sets, but very few humans want to speak CPU language directly…hope that helps. reply dankobgd 5 hours agorootparentprevIt's a horrible way to do what css can do but in a dumber way with added tooling. It is used by people that don't know css and this is where the problems start reply jitl 3 hours agorootparentI’ve been writing css/scss/stylus/linaria/whatever styling stuff for 18 years and I think tailwind improves life much more for css knowers than for css noobs. I prefer it over raw css for sure I’ve written the same shit enough times that I’m happy to take some shortcuts and more than willing to pay for some tooling to do so. Tailwind hands out less abstraction rope compared to vanilla css and sass and all the rest, so much lower chance I open up a file and find some convoluted special little kingdom of abstractions that need to be handled carefully. Reduced cognitive load in the long run for a small up front learning/setup cost. reply BeetleB 15 hours agorootparentprev> Doesn't Tailwind map pretty much 1:1 to CSS? Much of it does, but not all of it. reply _heimdall 6 hours agorootparentDo you have an example that Tailwind can do that can't be done in CSS? reply BeetleB 2 hours agorootparentI'm going off a claim from the Tailwind site :-) The original question wasn't whether Tailwind can do something CSS can't, but whether it was a 1:1 mapping. Can regular CSS do media queries all within the element's style attribute? I believe Tailwind also utilizes JS to get some convenience features to work. reply ervine 16 hours agorootparentprevYes. reply bitpush 17 hours agoparentprevReally!? Is that really what you got out of it? I understand you're not a fan of Tailwind, so perhaps say what you dont like about Tailwind instead of creating a strawman and attacking it. Also, shame on you for poo-pooing on someone else's open source project and people find useful. reply rglullis 16 hours agorootparent\"What I don't like about Tailwind\" has less to do with Tailwind itself and more about the violation of separation of content and presentation that they push so hard. > poo-pooing on someone else's open source project This is not some young kid doing free software out of kindness. This is a company making millions of dollars in revenue in a closed source product (tailwind UI), which is built on a foundation (tailwindCSS) that is becoming less and less needed. reply BeetleB 15 hours agorootparent> and more about the violation of separation of content and presentation that they push so hard. As someone who's done it both ways with web sites, that separation sucked. Having to figure out which style in my CSS file was messing things up was always a pain. And Tailwind tends to be \"local\" by default, limiting the damage done to other elements when I style an element. In my larger experience (not just with web sites), whether separating content and presentation is a good idea varies widely from use case to use case. reply yCombLinks 16 hours agorootparentprevOften content and presentation are part of the same package. In these cases, separating them makes maintenance of sites harder. reply mc3301 17 hours agorootparentprevYeah, I tried (not so hard) to like and use tailwind, but I've been using bootstrap for so long, and for my very simple needs, bootstrap was a better choice. But I wouldn't go around saying, \"tailwind is unnecessary for everyone.\" reply notjoemama 16 hours agorootparentI'm with you. Tailwind would be great if it could be SASS'd but Tailwind is a post processor and it would be silly to build a post-post processor just to make tailwind reusable. It simply isn't a good tool for large enterprise applications. Toys, home projects, sure. But then, why learn CSS twice for occasional use? Until they solve the post processor problem, I'll stick with bootstrap. But if they do, I would definitely switch. That would be the best of both worlds. reply pupppet 16 hours agoprevI appreciate there are people who find Tailwind useful and are productive with it, but it has this clean yet generic look that I now see everywhere, because Tailwind is now everywhere. reply gkoberger 15 hours agoparentThis is conflating Tailwind and TailwindUI/shadcn/etc. For the most part, Tailwind offers no more influence over style than normal CSS (okay, there's some exceptions... things like shadows are a bit more standardized, and indigo has gotten outside influence on color palettes as of late). reply cloverich 2 hours agoparentprevMy take on this problem is, good UI frameworks (i.e. tailwind + components) are still rare. So the vast majority of projects use a very small number of them. If 4-5 new quality UI projects popped up, especially if they departed more in their styling, we'd see more diversity. I've come to realize with design, like much creative work, the trends really do seem to be set by a shockingly small number of people. I wish more people realized this, because I feel there are talented individuals out there not making their own, thinking they'll never be heard. But I suspect some of those people would be unlocking new UI's for a large chunk of the web if they would give it a go. reply Sateeshm 15 hours agoparentprevTailwind gives you sensible defaults, but doesn't really influence your design too much. People just tend to design the sites a certain way. Especially, with ShadCn. reply sadmanca 16 hours agoparentprevI feel like that's a good thing, because Tailwind's options are well-thought and work well. I like to think of it in the same way as Inter being the new Helvetica when it comes to choosing a font for a website: lots of people do it, and it looks great, so why not? reply harrall 12 hours agoparentprevI heard this comment about Bootstrap back this in the day but none of my Bootstrap sites looked like each other. Bootstrap saved me a lot of time because it had a lot of utility classes (like Tailwind way before Tailwind) and it never enforced a certain look. You just loaded the classes that you wanted (unless you didn’t download the source Sass files, but that would have been silly). reply Zanfa 10 hours agoparentprevThe same way everything looked like Bootstrap a decade ago. reply gr__or 8 hours agoprevTailwind is a testament to the greatness of inline styles (styling a thing without naming it), the universality of strings and also the limits of strings. I am excited for what comes after it, until then I'll use Tailwind (reluctantly) reply mythz 14 hours agoprevWas it really necessary to break all existing apps using `npx @tailwindcss`? It's an easy enough change to `npx @tailwindcss/cli` but we now have to go back and update all Apps and templates and since it uses v4 we now have to test every App to see if anything's broken with a v4 major release. Given its massive install base, surprised they wouldn't maintain backward compatibility with v3 and have an explicit opt-in upgrade path to v4. Edit: v4 did actually break all our Tailwind Apps which doesn't support `@import \"tailwindcss\"`, likely because we use `npx tailwindcss` without any local npm dependencies. Trying to use the existing v3 `@tailwind components` renders a useless app.css. Only solution atm is to explicitly use v3 and change all our build scripts to use `npx tailwindcss@v3`. reply jitl 13 hours agoparentYou’re doing something silly living without a lock file to use a tool as part of your build process. Why don’t you actually include the version of the tool you want in your dev dependencies + lock file? That way, you don’t get broken. We’ve had the solution to this problem lock files for 15ish years, since Ruby’s `bundle` introduced it. reply mythz 13 hours agorootparentNot everything is a node App. We don't have a lock file because we don't have any local npm dependencies (as already mentioned). We're using `npx tailwindcss` as-is. reply nejsjsjsbsb 11 hours agorootparentShove an @ version in it reply mythz 11 hours agorootparentYep `npx tailwindcss@v3` is the solution we ended up with. Just had to change this in all our Apps and project templates. reply nejsjsjsbsb 11 hours agorootparentI wish @-less wasn't be allowed. Like you can do @latest but then human confirm is needed. That way you don't get this problem in scripts. reply jitl 13 hours agorootparentprevYou clearly have a dependency on tailwindcss (if it can break you, you depend on it), and are using npm to fetch it (npx uses npm internally, they are installed together) so you can easily solve your problem by adding a couple json files with some version numbers in them. npx will pick up and use the version from your package.json, so no workflow change other than running `npm install …` once to generate package.json for your preferred semver specifier, and package-lock.json to guarantee your builds use a known-good version from now on. reply arcanemachiner 12 hours agorootparentFWIW Tailwind can be used as a standalone executable. This is how Phoenix/Elixir bundles Tailwind without requiring npm. reply jitl 12 hours agorootparentI don’t care how you do it, just make getting your dependencies deterministic based on files committed to your repo! Happily the Tailwind hex package encourages you configure a fixed version number, so you’ll never get surprise breakage as long as you’re committing your mix.lock file as well. I’m only suggesting package.json/package-lock.json because their projects are already using the npm ecosystem to fetch the tailwind dependency. A Makefile that does `curl $TAILWIND_GITHUB_RELEASE_URL o ./departing/tailwind` is fine too (this is what Tailwind.hex is doing under the hood). reply preisschild 10 hours agorootparentprevAFAIK using the tailwindcss-cli without npm makes it impossible to use Tailwind plugins, such as DaisyUI. reply krnsi 5 hours agorootparentThis is not true, here is a demo projects which uses the Hex Tailwind and DaisyUI: https://github.com/naymspace/backpex/tree/develop/demo reply arcanemachiner 8 hours agorootparentprevI believe that is true, although there is a project which bundles Tailwinds and DaisyUI into a single standalone executable: https://github.com/dobicinaitis/tailwind-cli-extra I have not tried this out yet, personally. reply mythz 13 hours agorootparentprevFFS we don't have local dependencies. Which means we don't have any local node_modules and have never ran npm install (there are no dependencies to install!). There is no lock file or dependencies. Our Apps don't have any npm dependencies nor needs to run npm install by design, we're not going to start now, as already stated we're switching to `tailwindcss@v3` instead. The point is this was a documented supported use-case which all our non node.js Apps used, which have now broken as a result of this release. reply thiht 10 hours agorootparentYOU DO have a local dependency. Just add a mostly empty package.json like this: { \"devDependencies\": { \"tailwindcss\": \"^3.4.17\" } } Run npm install once, and be done with it. You could even commit your node_modules if it helps. reply mythz 10 hours agorootparentWe don't, we use npx to avoid needing any local dependencies. `npx tailwind@v3` is the solution, definitely wont be committing node_modules to git, ever. reply Aeolun 12 hours agorootparentprevI find it really hard to not immediately be reminded of this: https://xkcd.com/1172/ reply mythz 11 hours agorootparentWe only use npm dependencies and bloated node_modules folders [1] if we absolutely have to and for our non node.js Apps we explicitly don't. [1] https://www.reddit.com/r/ProgrammerHumor/comments/6s0wov/hea... reply preisschild 10 hours agorootparentBut why not though I also use tailwindcss in a non-node project, but use npm package / lock files to easily lock javascript / css dependencies and make renovate able to update them for me. reply mythz 10 hours agorootparentI've linked to why. We prefer #NoBuild solutions where we'd only use local npm deps/node_modules if we absolutely have to, and for non JS Apps we don't. reply jitl 4 hours agorootparentThe node_modules folder is still somewhere on your system (~/.npm/_npx probably?) containing the same tailwindcss dependency files, but if it helps you sleep easier at night it’s okay to pretend it doesn’t exist. reply mythz 2 hours agorootparentNo kidding, I thought global tools worked with fairy cloud dust, now I wont be able to sleep! But knowing there's not a going to be a bloated node_modules folder and local dependencies running different versions unnecessarily maintained in every project will definitely help. reply aembleton 6 hours agoparentprev> Given its massive install base, surprised they wouldn't maintain backward compatibility with v3 and have an explicit opt-in upgrade path to v4. Given it's a major version change, I'd expect it to create breaking changes, otherwise it would be a minor version change. reply cadamsdotcom 16 hours agoprevThanks Tailwind devs! It’s a testament to the success of the concept and the quality and consistency of execution over a seriously long time, that Tailwind became how LLMs write code for the web. Congrats on the release. reply harlanlewis 15 hours agoparentYes, I’m a huge fan of how easy it is to whip up quick isolated prototypes in Claude artifacts. There’s a risk of breaking changes in libs causing frustration in larger codebases, though. I’ve been working with LLMs in a Nextjs App Router codebase for about a year, and regularly struggle with models trained primarily on the older Pages Router. LLMs often produce incompatible or even mixed compatibility code. It really doesn’t matter which side of the fence your code is on, both are polluted by the other. More recent and more powerful models are getting better, but even SOTA reasoning models don’t totally solve this. Lately I’ve taken to regularly including a text file that spells out various dependency versions and why they matter in LLM context, but there’s only so much it can do currently to overcome the weight of training on dated material. I imagine tools like Cursor will get better at doing that for us silently in the future. There’s an interesting tension brewing between keeping dependencies up to date, especially in the volatile and brittle front end world, vs writing code the LLMs are trained on. reply iansinnott 15 hours agoprevMigration guide, if anyone is wondering: https://tailwindcss.com/docs/upgrade-guide Some breaking changes in there. May want to hold off on upgrading until LLMs come around to writing v4 code with ease. reply koito17 15 hours agoparentRenaming utilities like flex-shrink-* makes existing LLMs emit deprecated code today and broken code tomorrow. I wonder what the rationale is behind the renaming of various utilities (e.g. shadow-sm > shadow-xs, flex-shrink > shrink, decoration-slice > box-decoration-slice, ...) In new projects, I will probably use Tailwind v4 and constantly provide the upgrade guide as context to an LLM. In existing projects, I will continue to use Tailwind v3 until I am certain that it works alongside the tools used by my framework (React Router / Remix). reply NiloCK 15 hours agoparentprevThis reservation has taken firm hold on me and has made me a slower adopter of shiny new things. I feel alright about it from a local perspective (I'm a lot more productive now than I was before), but I do wonder what it does to the overall dynamic and incentive to write shiny new things or generally update the ecosystem. The LLMs will get more powerful, but to what extent will their work be dominated by existing tools (with lots of existent human-generated exemplar)? reply hipadev23 15 hours agorootparentLLMs have already plateaued in knowledge imo. There’s far less reason for humans to contribute code examples, answer questions, work on open-source projects, or even produce content knowing it’ll immediately be slurped up and resold. Web dev will be stuck with React and Tailwind circa 2021 for a very long time. reply verdverm 14 hours agorootparentprevWe shouldn't really have to retrain models for them to be able to work with new versions of libraries or frameworks. That seems like a flaw in the LLM (only?) setup. One should probably be using RAG at a minimum to pull in the correct documentation and references. Something like Kapa, but not limited to a single project https://www.kapa.ai/ reply tlrobinson 13 hours agoprevTailwind’s abuse of CSS classes as a DSL has always felt like a hack to me. You shouldn’t need a special editor extension to get highlighting/autocomplete/etc, just use TypeScript and a CSS-in-TS solution. reply darkhorse13 10 hours agoparent> just use TypeScript and a CSS-in-TS solution THIS is the definition of a hack to me, so yeah. reply dbbk 8 hours agorootparentMay be a hack but it just works. reply lelanthran 11 hours agoprevQuestion for those developers who, like me, have no f/end build step: How would I use tailwind without a f/end build step? All the examples of using tailwind 4.0 on the linked site assume a f/end build step. My project does not use (and I have no plans to include) npm, or PostCSS, etc. reply rahkiin 11 hours agoparentFor a static site I have, I manually run the Tailwind tool when I include new classes. It generates style css and I commit this file to git. reply lelanthran 11 hours agorootparentThanks; I actually did not know there was one until you and the sibling poster told me. To anyone else with the same question: See https://tailwindcss.com/blog/standalone-cli reply burgerrito 11 hours agoparentprevI'm pretty sure the point of using TailwindCSS is to use build step. There is another way, it uses Tailwind's CDN. But I think even the developer don't recommend it outside of development phase If you don't want to use NPM, I can recommend Tailwind's standalone CLI reply lelanthran 11 hours agorootparent> If you don't want to use NPM, I can recommend Tailwind's standalone CLI Didn't know there was one. Will look into it. reply stevoski 11 hours agoparentprevThis is what keeps me from using Tailwind too. I simply do not want to add the maintenance headaches of an npm-based build process to my project. reply lelanthran 11 hours agorootparent> I simply do not want to add the maintenance headaches of an npm-based build process to my project. Apparently I was just too blind to find it here is how you can (presumably) use tailwind without pulling in npm and node.js. https://tailwindcss.com/blog/standalone-cli reply pier25 14 hours agoprevPersonally I'm not interested in using Tailwind but my main criticism is that it didn't go far enough in its approach. Instead of using classes (which are extremely limiting) they should have created their own language like Imba did (inspired by TW). https://imba.io/ reply ptsd_dalmatian 5 hours agoprevTailwind and Sveltekit are the reason why I still love doing FE after all these years. Initially I was so sceptical of TW. I simply can't work without it now. Huge thank you to authors for their hard work. reply kaeshiwaza 12 hours agoprevLocality of behavior can be done with few lines of js. https://github.com/gnat/css-scope-inline reply vim-guru 11 hours agoparentI assume that you don't even need javascript after a few releases of Firefox. All other major players support scoped styles. https://developer.mozilla.org/en-US/docs/Web/CSS/@scope https://caniuse.com/?search=%40scope reply KronisLV 6 hours agoprevI'm not the biggest fan of Tailwind or anything, but good job on the release! Fewer dependencies and better performance are always welcome! reply todd3834 15 hours agoprevI’m a recent convert to tailwind. I’m very comfortable with css and I was initially turned off on the huge horizontal lines I saw in tailwind projects. However, more and more component libraries are based on tailwind so I decided to try some immersion therapy. Here are the top things I enjoy that was not obvious to me: 1. The class names are css shortcuts. Using them save you a lot of time. This is probably obvious to anyone who’s seriously looked at tailwind but I didn’t see that browsing the docs. I just saw nightmarishly long lines. 2. The lines look longer when you are not familiar with the class names. I initially pulled all of my class names out into a string outside my markup and included similar to how I’m used to using emotioncss. This made tailwind tolerable for me at first. However after several days I started to feel less turned off by those lines. I think it’s because I could recognize them. I will still break down a line with something like clsx. 3. clsx helps so much vs trying to entirely rely on tailwind syntax. The docs don’t discourage this at all but for some reason I thought it wasn’t idiomatic tailwind at first. 4. My app has to support a very large number of themes. Tailwind has proven to be a very attractive way of solving this problem. CSS variables are cool but the long syntax of using them is helped a lot in tailwind. 5. Adding my own custom variants is so easy and made me feel like a power user with such a small learning curve. All of this is just my two cents to guide anyone who is like me watching from the sideline and wondering why? Why would anyone ever want to tolerate those “disgusting long lines mixed into the html”. Neo, all I see is the lady in the red dress now reply Sateeshm 13 hours agoparentIt's sort of like javascript devs ignoring typescript because they never felt like they needed it. But once you start using it, it's difficult to go back. I was in never ts/tailwind gang for a few years until I tried them. reply ChrisArchitect 16 hours agoprevTitle is: Tailwind CSS v4.0 reply croisillon 12 hours agoprevText of TFA is cropped on my Safari (iPhone 7), a bit ironic init? reply dang 14 hours agoprevRecent and related: Tailwind CSS v4.0 Beta 1 https://news.ycombinator.com/item?id=42210553 Nov 2024 (125 comments) reply atonse 16 hours agoprevLooking forward to integrating this into my elixir projects! reply robertwt7 15 hours agoprevso much love for tailwind! I've used it since version 1 and have been always my default for starting new projects. Congrats to tailwind team! reply amai 10 hours agoprevSee also https://daisyui.com/ reply notracks 3 hours agoparentI heard that Daisyui v5 is ditching Tailwind so now it's just pure CSS. reply amai 3 hours agorootparentThe Daisy UI 5 beta release is still based on Tailwind 4 \"First Install Tailwind CSS 4 beta\" https://v5.daisyui.com/docs/v5-beta/ reply bitbasher 15 hours agoprevLast time I tried v4 the automatic content detection was pretty bad. It would falsely detect tailwind classes if it seen a similar name in a string in a code file. It seemed pretty dumb. reply dcre 12 hours agoparentThat is more or less how it worked in v3 already, because any string could be used as a CSS class. \"The way Tailwind scans your source code for classes is intentionally very simple — we don’t actually parse or execute any of your code in the language it’s written in, we just use regular expressions to extract every string that could possibly be a class name.\" https://v3.tailwindcss.com/docs/content-configuration#class-... The only difference in v4 is that scanning is so fast, they just scan roughly everything by default. You can still opt into manually specifying which files to scan. https://tailwindcss.com/docs/detecting-classes-in-source-fil... reply silverwind 10 hours agorootparentIn v3, one can use `blocklist` in the config to eliminate false-positives, but with the config file gone, this seems no longer possible, so I won't be upgrading. In one project, roughly 75% of the detections were false-positives, unnecessarily bloating the CSS. reply dcre 5 hours agorootparentYou can disable automatic detection and manually configure which files to look at: https://tailwindcss.com/docs/detecting-classes-in-source-fil... reply silverwind 10 hours agoprevDid they really nuke the entire js config file? I have a number of advanced use cases that depend on it, like eliminating false-positive content matches. reply thijsvandien 6 hours agoparentYou can still use it if you want, but most of the time it’s not needed anymore. reply sadmanca 16 hours agoprevHeck yeah, cool to see Tailwind add support for transitions without Javascript. As a burgeoning web developer, dealing with animation libraries seems like it's not worth the effort (or at least not for something as simple as a blog), so this is much appreciated. reply hit8run 12 hours agoprevI like tailwind but think it’s not needed anymore as CSS got better. Utility frameworks don’t necessarily need a build step. I forked and finished Tachyons successor Tachyons 5: It has container queries built in and p3 colors. https://github.com/gobijan/tachyons5/blob/main/tachyons5.css If you need very sophisticated layouts write them in plain css. For quick styling utilities are awesome. Unpopular opinion. Now downvote me to the ground. reply andrewmcwatters 15 hours agoprevTailwind is mind-boggling to me. Like something that would be written by someone who doesn’t actually write a lot of HTML or CSS. Bootstrap is bad enough, where you end up using 4, 5, 6 classes for display, margin, padding alterations to existing components. It’s just unreadable crap. No wonder it’s popular though. Most people suck at designing. The ones that don’t aren’t using Tailwind. reply intrasight 12 hours agoparent>The ones that don’t aren’t using Tailwind. True. And they're not using HTML or CSS either. reply Fred27 7 hours agoprevTailwind suits the kind of developers who hack things together and build up technical debt and spaghetti code rather thinking about things first and designing things properly. reply whstl 7 hours agoparentI don't know. In 20+ years I have seen way more abominations and spaghetti CSS using other every CSS technique. With Tailwind at least I know what to expect, and the code in practice rarely deviates from what it's supposed to look like. I'm all for \"not building up technical debt\" and \"thinking about things first\" but in my personal experience the anti-Tailwind crowd doesn't have much to show here in this regard. Sure: it's theoretically possible to build a perfect CSS ivory tower, I just haven't seen even a passable one in a non-trivial project, it always devolves into a mess for various reasons... often breaking the rules of the paradigms like BEM and OOCSS, often because of cutting corners here and there. Addendum: I'm all for criticising techniques, but it's interesting that the anti-Tailwind crowd always resorts to attacking the character of the developers that use it. reply ozim 5 hours agorootparentI feel like it is the same with ORMs argument is always \"they don't know proper SQL so they use ORM instead\". Which usually from people I know is they do know SQL and ORM and \"no ORM crowd\" doesn't have much to show. Exactly the same with Tailwind, I see coworkers doing Tailwind and knowing CSS well and not being \"pure vegan CSS developers\". But on the other hand we don't hire people who would utter such things like \"no ORM\" or \"no framework\" because those were also as in experience people who would create technical debt for others to deal with. reply christophilus 6 hours agorootparentprevSame. Been doing this for 20+ years. I’ve worked at companies whose names are recognized and respected. My current Tailwind project is by far the most maintainable one I’ve worked on when it comes to CSS. reply whstl 3 hours agorootparentExactly. I don't know why, and perhaps in theory it should suck indeed! But in the end it just works. reply robjn 5 hours agoparentprevHard disagree. Vanilla BEM + CSS produces a parallel component system that I find very unwelcome since React (or Vue/Svelte/etc.) already accomplishes this. Tailwind removes this burden and helps you focus on writing actual React components when needed. reply rayrrr 13 hours agoprev [–] To some extent, reading between the lines of many of these comments, I think “pure CSS” designers are feeling threatened by the quality level of Tailwind sites designed by the rest of us. reply tipiirai 13 hours agoparent [–] It's worse. React killed the craft of design engineering because everything is JavaScript. CSS develoeprs are gone. reply jitl 13 hours agorootparent [–] Design engineers still exist, they just write more JS these days. They are still building UI and design systems and teaching craft & supporting full stack teams. The craft is there, but the tools are different. reply tipiirai 12 hours agorootparent [–] True — they just need to be more engineers than designers. reply jitl 12 hours agorootparent [–] I don’t think putting html in one file and css in another file makes you more or less of a designer than putting both html and css into a single file. reply yoz-y 8 hours agorootparentI think the approach is different. Component style applications have to thing about all component presentation separately, web apps ressemble apps more than documents (d’uh) so it makes sense that they are designed as such. Documents are designed holistically, you don’t care that much about how a thing (say, an image that is an aside) would look on the next page because on the next page there would be something completely different. I think much of the debate and contention comes from the fact that many developers only worked on either web apps, or documents (e.g.: static blog, cms) and each try to convince the other group that their approach is the best. reply tipiirai 12 hours agorootparentprev [–] Switching from CSS based design systems to large TypeScript monoliths requires you to learn one or two things about engineering reply Consider applying for YC's Spring batch! Applications are open till Feb 11. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Tailwind CSS v4.0 introduces a high-performance engine, achieving builds up to 5x faster and incremental builds over 100x faster, enhancing performance and flexibility for developers. The update includes modern CSS features such as cascade layers, color-mix(), and built-in container queries, along with a first-party Vite plugin for improved integration and simplified installation. New features like 3D transform utilities, expanded gradient APIs, and CSS-first configuration with design tokens as CSS variables make it easier and faster to build modern web projects, with an upgrade guide available for existing projects."
    ],
    "commentSummary": [
      "Tailwind CSS v4.0 introduces significant enhancements, such as native CSS variable access and a CSS-only configuration, reducing dependency on JavaScript and increasing versatility. The update is noted for its compatibility with AI workflows and utility in design systems, although some designers find it challenging for systematic design. The release has sparked discussions due to breaking changes, affecting integration with existing projects and tools, while maintaining popularity for its ease of use and styling efficiency."
    ],
    "points": 395,
    "commentCount": 227,
    "retryCount": 0,
    "time": 1737592382
  },
  {
    "id": 42797756,
    "title": "Federal Court (Finally) Rules Backdoor Searches of Data Unconstitutional",
    "originLink": "https://www.eff.org/deeplinks/2025/01/victory-federal-court-finally-rules-backdoor-searches-702-data-unconstitutional",
    "originBody": "Better late than never: last night a federal district court held that backdoor searches of databases full of Americans’ private communications collected under Section 702 ordinarily require a warrant. The landmark ruling comes in a criminal case, United States v. Hasbajrami, after more than a decade of litigation, and over four years since the Second Circuit Court of Appeals found that backdoor searches constitute “separate Fourth Amendment events” and directed the district court to determine a warrant was required. Now, that has been officially decreed. In the intervening years, Congress has reauthorized Section 702 multiple times, each time ignoring overwhelming evidence that the FBI and the intelligence community abuse their access to databases of warrantlessly collected messages and other data. The Foreign Intelligence Surveillance Court (FISC), which Congress assigned with the primary role of judicial oversight of Section 702, has also repeatedly dismissed arguments that the backdoor searches violate the Fourth Amendment, giving the intelligence community endless do-overs despite its repeated transgressions of even lax safeguards on these searches. This decision sheds light on the government’s liberal use of what is essential a “finders keepers” rule regarding your communication data. As a legal authority, FISA Section 702 allows the intelligence community to collect a massive amount of communications data from overseas in the name of “national security.” But, in cases where one side of that conversation is a person on US soil, that data is still collected and retained in large databases searchable by federal law enforcement. Because the US-side of these communications is already collected and just sitting there, the government has claimed that law enforcement agencies do not need a warrant to sift through them. EFF argued for over a decade that this is unconstitutional, and now a federal court agrees with us. EFF argued for over a decade that this is unconstitutional, and now a federal court agrees with us. Hasbajrami involves a U.S. resident who was arrested at New York JFK airport in 2011 on his way to Pakistan and charged with providing material support to terrorists. Only after his original conviction did the government explain that its case was premised in part on emails between Mr. Hasbajrami and an unnamed foreigner associated with terrorist groups, emails collected warrantless using Section 702 programs, placed in a database, then searched, again without a warrant, using terms related to Mr. Hasbajrami himself. The district court found that regardless of whether the government can lawfully warrantlessly collect communications between foreigners and Americans using Section 702, it cannot ordinarily rely on a “foreign intelligence exception” to the Fourth Amendment’s warrant clause when searching these communications, as is the FBI’s routine practice. And, even if such an exception did apply, the court found that the intrusion on privacy caused by reading our most sensitive communications rendered these searches “unreasonable” under the meaning of the Fourth Amendment. In 2021 alone, the FBI conducted 3.4 million warrantless searches of US person’s 702 data. In light of this ruling, we ask Congress to uphold its responsibility to protect civil rights and civil liberties by refusing to renew Section 702 absent a number of necessary reforms, including an official warrant requirement for querying US persons data and increased transparency. On April 15, 2026, Section 702 is set to expire. We expect any lawmaker worthy of that title to listen to what this federal court is saying and create a legislative warrant requirement so that the intelligence community does not continue to trample on the constitutionally protected rights to private communications. More immediately, the FISC should amend its rules for backdoor searches and require the FBI to seek a warrant before conducting them.",
    "commentLink": "https://news.ycombinator.com/item?id=42797756",
    "commentBody": "Federal Court (Finally) Rules Backdoor Searches of Data Unconstitutional (eff.org)313 points by janandonly 21 hours agohidepastfavorite61 comments SpliffnCola 7 minutes agoIs this because they have full access now to the front door? reply idrathernot 19 hours agoprevEven if congress ends their blatantly unconstitutional endorsement of Section 702 spying, I still don’t see why anyone would believe that the government is going to do anything other than massively expand their ability surveil every living moment of our lives. I don’t see the point in them trying to play it off like the system has any integrity whatsoever. reply duxup 17 hours agoparentCongress has been woefully short of curiosity or oversight that doesn't involve partisan politics ... let alone leadership. reply impossiblefork 11 hours agoparentprevWhy do you think section 702 seems unconstitutional? It looks pretty legal to me at least. reply superkuh 19 hours agoprevIt's funny that even though the court ruled it is against the law to do so the federal agencies will continue to commit this crime (FBI) and pass illegal legislation (congress) until each specific agency changes it's internal regulations. If they do. The EFF seems to imply continued pressure is needed to convince them to do so. reply jmclnx 21 hours agoprevWell I guess of the the US Supreme Court. They seem to get every case these days :( reply perihelions 20 hours agoparentNo, just a district court (Eastern District of New York). reply ARandomerDude 17 hours agoprev> We expect any lawmaker worthy of that title to listen to what this federal court is saying and create a legislative warrant requirement so that the intelligence community does not continue to trample on the constitutionally protected rights to private communications. Sad to say it, but I find it laughable that the intel agencies would suddenly stop if it were illegal (though of course it should be illegal). They operate in secret and anyone in the government who opposes them will commit suicide or suddenly be in possession of child pornography. As New York Sen. Chuck Schumer once told Rachel Maddow on air, “Let me tell you, you take on the intelligence community, they have six ways from Sunday of getting back at you.” [1] 1. https://youtu.be/-gZidZfUoMU reply npvrite 18 hours agoprevCan we all stop pretending that they don't abuse of their power and hold your deepest darkest secrets indefinitely? Even though most of us are law abiding citizens. Can we please start making open hardware without Apple/google backdoors and stop pretending our systems are \"secure\". Can we please write all software in Rust and stop using languages that weren't designed for security. Yes C is beautiful. Yes it also lets you shoot yourself in the foot. Can we please use distributed systems to avoid censorship or holding our private information in the hands of the rich? reply oneplane 17 hours agoparentNo, we apparently can't, because every time someone attempts to do that, we don't end up with a usable end state or product that people actually want to use or participate in. Perhaps we just haven't had success yet, and it's not impossible. But such desired outcomes tend to also require everyone to \"be the same\" (knowledge, skills, capabilities) or \"want the same\" (desire to spend time and attention on this sort of thing etc.) and that's not how people work. reply ethin 18 hours agoparentprevI can't tell if your being sarcastic or actually serious, because nobody is rewriting everything in Rust. reply dmz73 17 hours agoparentprev>Can we all stop pretending that they don't abuse of their power and hold your deepest darkest secrets indefinitely? Even though most of us are law abiding citizens. I don't anyone know who thinks powerful don't abuse their power. It is the nature of the beast. And it seems none of us are law abiding citizens: https://www.saponelaw.com/blog/2019/10/professor-says-that-e... >Can we please start making open hardware without Apple/google backdoors and stop pretending our systems are \"secure\". Few try...and either fail or languish in obscurity. You comment in itself is the proof that open hw cannot compete since you don't know of these open hw platforms and don't use them even tough you seem to advocate their creation here. >Can we please write all software in Rust... Rust only eliminates memory safety issues of C/C++. There are large number of languages, some decades older than Rust, that provide various aspects of Rust memory safety without imposing the same limits...and some are being used but people always flock to either new and flashy or the most widely used. Besides, Rust still provides ample foot guns and pushes reliance on 3rd party libraries which replaces memory safety issues with supply chain issues. Not to mention the the very poor ergonomics of the language that purposefully shies away from a lot of syntax sugar that makes writing and reading (understanding) code easier. >Can we please use distributed systems to avoid censorship or holding our private information in the hands of the rich? Even if you managed to persuade a lot of people to use these, some nodes will become popular/trusted and be targeted for censorship and propaganda and that will achieve the same result as the current model. Again, it is the nature of the beast. What can be done? I don't know, probably nothing...things have to get to the point where most people are compelled to act because the alternative is death or worse, until such time there will just not be enough support for action to matter. Just how people are. reply zxvkhkxvdvbdxz 15 hours agorootparent> Rust only eliminates memory safety issues of C/C++ According to Microsoft, about 70% of all security bugs in their products are memory safety issues. These could be all be eliminated with a language that doesn't allow it in the first place. https://www.zdnet.com/article/microsoft-70-percent-of-all-se... reply johnnyanmac 18 hours agoparentprevAndroid, sure. There's still AOSP and there are a few niche devices dedicated to being as close to Open Hardware as we could be. >Can we please write all software in Rust and stop using languages that weren't designed for security. I'm all for it. But very few people want to pay for talent that can properly rewrite that legacy C/++ codebase into proper Rust. reply RajT88 15 hours agoparentprevYou missed dissing Microsoft in your post. I almost had a HackerNews Bingo! reply josefritzishere 21 hours agoprevThis should not have been so hard. reply treetalker 20 hours agoprevDon't worry: SCOTUS will determine that the Framers did not have a history and tradition of protecting metadata, so the Fourth Amendment has no application here; and, furthermore, the Court's recent jurisprudence regarding the Executive (dieu et mon droit) necessarily implies that the Government has an extremely compelling interest sufficient to overcome the Fourth Amendment and permit warrantless searches of everyone in the United States and elsewhere. reply rayiner 19 hours agoparentYou mean like in Riley, which was authored by Chief Justice Roberts and was a 9-0 decision? https://en.wikipedia.org/wiki/Riley_v._California > Riley has been widely praised as “a sweeping victory for privacy rights”[5] with legal scholars describing the decision as \"the privacy gift that keeps on giving.\" Since then the Court has picked up another privacy hawk (Justice Gorsuch), and another Justice (Barrett) that's also pretty strong on privacy: https://www.protectprivacynow.org/news/how-will-a-justice-am.... reply KennyBlanken 12 hours agorootparentThat Court you're so impressed with repeatedly refused to review lower court rulings that police can physically force a suspect to press their finger against their phone or force them to look at it to unlock it with thumbprint or face ID. The most recent ruling, which they refused to hear an appeal on, was v Payne. The opinion from the 9th circuit was that it was oke-dokeily because, and I quote, \"required no cognitive exertion, placing it firmly in the same category as a blood draw or fingerprint taken at booking\", and also because the cop could have done it while Payne was unconscious or asleep. The court you're so impressed with looked at that and in refusing to hear the appeal, upheld the decision. That court you're so impressed with also: ..ruled that silence does not indicate someone is exercising their right to remain silent (!) in Berghuis v. Thompkins. I mean really, you can't make this shit up. ...made Miranda rights almost worthless in Vega v. Tekoh, where a cop cornered a nurse in a storage room, threatened the nurse by putting his hand on his gun, threatened him and his family with deportation, etc...and then his statements were then used against them in court. The ruling removed the right to sue for having one's Miranda rights violated, only that they can suppress the statements in court. ...ruled that imprisoning someone did not count as \"custodial\". Again, you cannot make this shit up. ...threw a hissy fit when people dared to start protesting outside their homes. reply rpmisms 11 hours agorootparent> The opinion from the 9th circuit was that it was oke-dokeily because, and I quote, \"required no cognitive exertion, placing it firmly in the same category as a blood draw or fingerprint taken at booking\" You might dislike it, but legally, these are equivalent, with the requirement of a finger being less onerous. Biometrics are public information, like it or now. It's a well-reasoned decision. reply roenxi 10 hours agorootparentprevJust dealing with the one you seem to put as most important the 9th circuit's logic is watertight and police have to be able to investigate or a lot of murders will go unsolved. You'd really need to provide a reason why the search was unreasonable to make a point with that one. It makes a lot of sense to have protections against the police searching phones, homes, businesses, etc. But assuming that those protections aren't at play then it is entirely reasonable that a policeperson can force someone to put their thumb somewhere. The police already have the power to manhandle and imprison people, forced thumblocation is nothing compared to that. What do you want the Supreme court to do here, officially rule that police can coerce not just entire bodies at once, but also thumbs separately and individually? That seems like an unnecessary call for them to make. reply cherry_tree 16 hours agorootparentprevTo be fair the current court in 2025 is a different court than 2018, and in 2024 they overruled a long standing precedent in the chevron doctrine. It wouldn’t be out of the realm of possibility that this court would disregard Riley. I do agree that it currently like the court is broadly pro-privacy but I also think it entirely depends on the case and its specifics. reply rayiner 15 hours agorootparentFive of the Riley justices are still on the court. Of the four that aren’t, there’s two that seem stronger on privacy than his or her predecessor (Gorsuch, KBJ), one that seems like a wash (Kavanaugh), and maybe one that’s less strong on privacy (Barrett). You can point to Roe and Chevron, but those didn’t come out of nowhere. The first, conservatives vowed to overturn 50 years ago—and its expansive notion of judicially declaring rights is not embraced even among liberals on the court today. The second was a judge created interpretive doctrine anyway, and proved unworkable over time. But it also didn’t come out of nowhere. Gorsuch has been writing about it since before I was in law school, and that was 15 years ago. reply hedora 15 hours agorootparentRoe v Wade was based on the right to privacy. The Supreme Court ruling that overturned it eliminated our right to privacy. Then they invalidated Roe v Wade as a corollary. Since the current court eliminated the right to privacy in the US, I doubt they’ll spontaneously decide to restore it. reply rayiner 15 hours agorootparentRoe wasn’t about a right to “privacy.” The right to privacy comes from the fourth amendment: > The right of the people to be secure in their persons, houses, papers, and effects, against unreasonable searches and seizures, shall not be violated… If Roe was about “privacy”—i.e. laws against abortion are a “search” of your “person,” then by the same logic—laws against drowning your infant in the bathtub constitute a “search” of your “house.” Dobbs didn’t get rid of the right to privacy. It’s still there in the fourth amendment. It simply dropped the pretense that abortion was about privacy (which Casey had already done). reply FilosofumRex 14 hours agorootparentWrong on your first point Privacy right is not based on the 4th amendment but rather on the 9th amendment (unenumerated rights are persevered to the people). Correct on your second point Dobbs didn't take any rights away, it merely delegated to the States, where historically, such rights such rights are granted. reply rayiner 7 hours agorootparentA right can’t be “based” on the 9th amendment. The 9th amendment just says the bill of rights isn’t an exhaustive enumeration. It can’t be the source of rights—the rights must come from somewhere else. reply CamperBob2 1 hour agorootparentIt can’t be the source of rights—the rights must come from somewhere else. From God, who is great, especially at dodging process servers. When you build a legal system on bullshit, you can't be too surprised when irreparable cracks start to show after a couple hundred years. reply alwa 13 hours agorootparentprevI don’t think this is right. It reads backwards to me. Didn’t Roe, controversially, hold that abortion was protected as a function of constitutional protections related to privacy (in the due process clause of the 14th amendment—specifically as one form of the “liberty” you can’t be “deprived of” without “due process of law”)? Then attempted to explain in medically specific terms when exactly your pregnancy crosses over from being a matter of liberty/personal privacy into a matter that can be regulated or prohibited? Then didn’t Dobbs basically work by undoing the idea that you can locate abortion rights in the due process clause, rather than somehow wrecking the constitutional amendment itself? https://www.britannica.com/event/Roe-v-Wade reply rayiner 6 hours agorootparentRoe purports to be about privacy. But it’s really about the second point you list, which is purporting to define when a fetus becomes sufficiently developed to warrant protection from the state. After all, nothing changes between conception and the day before birth from the point of view of privacy. Or heck, even the day after birth—you have a privacy right in your home, not only your person. So the “privacy” right isn’t holding up any weight. All the work is being done by the moral determination that a fetus in the first two trimesters isn’t sufficiently developed to warrant legal protection. Once you assert that killing an 8 week fetus is no different than having a body part removed, the work being done by privacy is trivial. That latter question has nothing to do with privacy or the constitution. It’s a general moral judgment based on underlying biological facts. In that respect, Roe simply is an articulation of Harry Blackmun’s Methodist religious beliefs. The United Methodist Church came out strongly in favor of abortion legality in the 1960s. But there’s nothing in the constitution supporting the determination Roe reached and Roe didn’t even pretend there was. reply FilosofumRex 12 hours agorootparentprevThere is a legal subtlety here that's often misunderstood (or misrepresented) by most pro-abortionists: The right to privacy was curved out of the 9th Amendment in the district court decision (not the 4th or the 14th). The Supreme court upheld the district court decision and went further by adding the protection of the 14th amendment as to how and when this privacy right can be abridged by the states (not without due process of law). States must show a compelling interest to intervene in women's decision to abort, and this compelling interest ripens only after the first trimester of pregnancy. reply 1oooqooq 16 hours agorootparentprevthe 2001 upgrade to FISA made the suppreme court irrelevant in matters of citizen privacy. what they are deciding now is how to dress the mechanics of it. reply zdragnar 20 hours agoparentprevMy understanding of this case has nothing to do with metadata. The communications were captured in full because one party was not a citizen in the US (under national security reasons). Those communications were then stored and made available in full via keyword based search interfaces, and those later searches were made without first securing a warrant. I'm not going to bother reading the tea leaves too closely on this one, but I'd put it at least at even odds the supreme court would say the 4th amendment does apply here. reply idrathernot 19 hours agorootparentI think “metadata” is meant as an example of Barnum statement in the context of the original comment. It is very common for courts to reinterpret language as a means of getting to a specific end. Same reason that “Interstate Commerce” actually means all commerce in the 10th amendment. reply rayiner 15 hours agorootparentExcept in cases involving “metadata,” there’s typically a highly relevant difference in terms of who owns the data. The fourth amendment says: > The right of the people to be secure in their persons, houses, papers, and effects, against unreasonable searches and seizures, shall not be violated… You have a right as to your data, but not other people’s data about you. E.g. you have no right as to a pharmacy’s business records about you. (That’s their data, not your data.) To my knowledge, no court has ever distinguished between say image and the file metadata on someone’s computer. But the way “metadata” becomes relevant is often because someone else owns “metadata” about your files, such as server logs. reply Retric 15 hours agorootparentprevNo, a background of interstate commerce is considered to be meaningful. It’s why the Texas power grid being so isolating has meaning. However the court views a commodity as influencing and being influenced by interstate trade even if that specific gallon of oil never crossed state lines. From a purely economic standpoint it’s a reasonable take. Even Europe’s use of oil influences US oil prices let alone what happens in another state. It’s both an upside and downside of the judicial system that they take things in context. reply AnthonyMouse 14 hours agorootparentThere are two problems with that. The first is that the purpose of giving \"interstate commerce\" to the federal government was to address the problems incident to interstate commerce, e.g. someone in New York buys something from someone in Florida and there is a dispute, but New York doesn't have jurisdiction over the seller and Florida isn't interested in protecting New York buyers from Florida sellers. The sort of general purpose economic regulations at issue in Wickard weren't intended to be in scope to begin with. And the second is that as soon as you let go of that limiting principle, you don't have a limiting principle. \"Texas has its own power grid\" but if Texas has lower or higher power costs then customers might be more or less inclined to locate in Texas rather than some other state and affect demand for power in some other state, so the distinction is lost and there is a practical erasure of any line at all. reply Retric 9 hours agorootparent> a practical erasure of any line at all Not really the Texas power grid is hardly the only example. Banning products like say Fireworks is a much broader exception. If there’s no marker to regulate then the feds can’t get involved. This applies to more than just total bans, it’s also why California can have such influence on automobiles and Texas influences textbooks etc. You may feel there’s not enough things that fall into the exclusion, but that’s not the same thing as the exclusion not existing. > address the problems incident to interstate commerce Trademark infringement on trademarks held by an out of state entity breaks the principle you’re talking about without any sale directly crossing state lines. Same deal if some state decides to subsidize growing broccoli, and that’s ultimately what decided the issue. reply ndriscoll 1 hour agorootparentprevExcept in the ruling that established that idea, Filburn did not participate in any market at all with the grain in question. There was no commerce with anyone, which the court ruled affected commerce, and therefore subject to federal regulation. The idea that not participating in something is a form of participation is absurd and can be argued about literally everything. Federal government wants to regulate what's allowed for parks, public or private? As entertainment, they're a substitute good for copyrighted movies, so if you go to a park, you're not watching a movie. Interstate commerce. Federal government wants to regulate how you have sex with your spouse? They can regulate a market for traveling prostitution services. Don't use one because you're exclusive with your spouse? Interstate commerce. These aren't even that far from the actual ruling (actually the park one is probably less extreme), where he couldn't feed his chickens with food he grew. Literally they argued they can regulate how you feed yourself from the fruits of working your land with no trade. Sometimes it's fair to say SCOTUS deserves no respect and are either extremely disingenuous and corrupt, or profoundly mentally challenged and incompetent. Wickard is such a ruling. It's one of those things that makes you completely lose faith in the legitimacy of our government when you learn about it. reply Retric 52 minutes agorootparent> Filburn did not participate in any market at all You missed the nuance here. Growing alone didn’t make him a participant, his use made him part of the demand for a commodity. That [Filburn’s] own contribution to the demand for wheat may be trivial by itself is not enough to remove him from the scope of federal regulation Thus if he had been growing wheat as practice or for the artistic value without using it then he would not have been considered as influencing the market. reply matthewdgreen 2 hours agoparentprevThe new administration just ordered the resignation of every Democratic-appointed member on the Privacy and Civil Liberties Oversight this is the body that oversees the intelligence agencies and makes sure they don't abuse their power to spy on Americans. So much for dismantling the deep state. reply landryraccoon 20 hours agoparentprevCan you explain why? That doesn’t seem like sound reasoning to me. If you believe the reason is corruption, what personal incentive would the courts have to rule this way? Judges can easily be the victim of government overreach as well. reply idrathernot 19 hours agorootparentAnd having a log of every conversation and keystroke that any outspoken judge has ever made gives you all sorts of ways to align their opinions with the above all importance of “National Security” reply notjoemama 16 hours agorootparentprevThey were being sarcastic with no reasoning or explanation and there's no logical reason it ought to be the top comment in this thread. reply svachalek 19 hours agorootparentprevData is wealth and power, and today's Supreme Court will always side with wealth and power. reply Aloisius 18 hours agorootparentI'm not sure what this means. The Supreme Court will side with data because it is wealth and power? What side is an inanimate object on? reply RiverCrochet 18 hours agorootparentData in this case is on the side of wealth and power. You're also wrong about it being an inanimate object for two reasons. Reason A: Data is not an object but a pattern of objects or attributes of object(s) wherein the arrangement forms symbols according to a standard or protocol. Reason B: Data is quite animated when moving e.g. in response to searches or otherwise (re)-transmitted. reply Aloisius 17 hours agorootparentWe have more than one wealthy and powerful person. They're rarely all on the same side. So regardless of how courts rule on most issues, one could almost always argue they're taking the side of wealth and power and against the side of wealth and power. And data is inanimate in the dictionary definition sense of it not being alive. That other things can move it doesn't make it animate. reply Terr_ 13 hours agoparentprev> that the Framers Relevant, even though he's not a \"framer\": \"Using Metadata to find Paul Revere\" https://kieranhealy.org/blog/archives/2013/06/09/using-metad... > But I say again, if a mere scribe such as I—one who knows nearly nothing—can use the very simplest of these methods to pick the name of a traitor like Paul Revere from those of two hundred and fifty four other men, using nothing but a list of memberships and a portable calculating engine, then just think what weapons we might wield in the defense of liberty one or two centuries from now. reply reverendsteveii 20 hours agoparentprevnext [5 more] [flagged] rayiner 19 hours agorootparentIn a democracy, what method would you propose judges use to impose restrictions on the actions of the democratically elected government based on a written constitution? reply AyyEye 18 hours agorootparent> what method would you propose judges use to impose restrictions Judges are appointed (or voted in) by the very people you think they \"impose restriction\" on. Any ruling against the ruling class in our favor is nothing more than a happy accident. reply rayiner 16 hours agorootparentOkay, but, in your view, how should they go about determining when to impose restrictions on the government? reply tptacek 18 hours agorootparentprevDo you have an argument that would be persuasive to someone who doesn't believe that representative democracy is fundamentally flawed or somehow inoperative in the US? reply kittikitti 19 hours agoprev [–] We should go even further and abolish the FBI. They all believe they're above the law and have consistently been the enemy of Americans. What success can they even point to? reply FilosofumRex 13 hours agoparentDon't forget about it's little sibling, the NSA, and its reputation for openly and overtly mocking our Constitution and its social compact. Snowden is still in political exile in Moscow for the heinous crime of whistleblowing. https://www.reuters.com/article/world/exclusive-nsa-infiltra... reply cyanydeez 19 hours agoparentprevYes, its totally the FBI ans not every large tech firm invading and trading youe privacy. reply idrathernot 19 hours agorootparentPlausible deniability is key pillar of ensuring National Security reply cyanydeez 30 minutes agorootparentOr, its just an oligarchy established for plausibly deniability of.minority slavery reply datavirtue 19 hours agoparentprev [–] The FBI hunts down and removes extremely violent criminals from society every day. They certainly operate right on the edge of the law, as they should. Furthermore, they never stop and never quit and no distance is too far--they will prevail no matter what. Extremely important to the stability of society. reply parineum 14 hours agorootparentThe FBI has a pretty consistent record of operating beyond the law. They can't seem to go more than a decade (if that) without a major controversy. reply yownie 15 hours agorootparentprev [–] what world do you live in? reply Consider applying for YC's Spring batch! Applications are open till Feb 11. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A federal district court has ruled that backdoor searches of databases containing Americans' private communications under Section 702 require a warrant, marking a significant legal decision in United States v. Hasbajrami. Despite Congress reauthorizing Section 702, evidence shows misuse by the FBI and intelligence community, with 3.4 million warrantless searches conducted in 2021, prompting calls for reform to ensure constitutional protections. The court's decision highlights the unconstitutional nature of warrantless searches under Section 702, urging Congress to mandate warrants for searches involving U.S. persons' data before its expiration in 2026."
    ],
    "commentSummary": [
      "A federal court has declared backdoor searches of data unconstitutional, igniting debates on government surveillance and privacy rights.",
      "The ruling challenges the constitutionality of Section 702 spying, highlighting the roles of Congress and the Supreme Court in safeguarding privacy.",
      "Despite the ruling, skepticism persists about its impact, as critics believe federal agencies like the FBI may continue such practices until internal regulations are revised."
    ],
    "points": 313,
    "commentCount": 61,
    "retryCount": 0,
    "time": 1737581523
  },
  {
    "id": 42799103,
    "title": "Most Influential Papers in Computer Science History",
    "originLink": "https://terriblesoftware.org/2025/01/22/the-7-most-influential-papers-in-computer-science-history/",
    "originBody": "The 7 Most Influential Papers in Computer Science History Before we begin, let me be clear: yes, this is a subjective list. It’s not meant to end the debate — but to start it. These seven papers (sorted by date) stand out to me mostly because of their impact in today’s world. Honestly, each one deserves a blog post (or even a book!) of its own — but let’s keep it short for now. If your favorite doesn’t show up here, don’t worry, stick around for the bonus section at the end, where I’ll call out a few more that came this close to making the main list. So let’s dive in! 1. “On Computable Numbers, with an Application to the Entscheidungsproblem” (1936) Author: Alan Turing It’s the 1930s, and a “programmable machine” sounds like something out of a sci-fi novel. Then along comes Alan Turing, laying the groundwork for what computers can theoretically do. He sketches out a hypothetical “Turing Machine,” proving that, if something is computable at all, a machine (in principle) can handle it. The big idea Turing’s simple model — just a tape, a head for reading/writing, and a finite set of states, turned into the granddaddy of all modern computation. It defined what’s solvable (and what’s not) in a purely mechanical sense, basically giving us the “rules of the game” for digital problem-solving. Why it matters today Every single programming language, every single piece of code out there, is playing by Turing’s rules. Even when we talk about quantum computing, we’re still referencing the boundaries Turing described. That’s a huge testament to the power of one paper published in the mid-1930s. Learn more https://www.cs.virginia.edu/~robins/Turing_Paper_1936.pdf https://en.wikipedia.org/wiki/Turing%27s_proof https://www.youtube.com/watch?v=dNRDvLACg5Q 2. “A Mathematical Theory of Communication” (1948) Author: Claude Shannon Now that Turing showed us what machines can (and can’t) do, how do we actually move information around? Enter Claude Shannon, who basically invented information theory so we could talk about bits, entropy, and noisy channels in a rigorous way. The big idea Shannon took the abstract notion of “information” and turned it into something a little bit (pun intended) more measurable. This helped us figure out how to pack data more efficiently (compression) and how to protect it from errors (error-correcting codes), whether we’re sending signals into space or streaming Netflix on a Friday night. Why it matters today Every single time you send a text, stream a video, or call your mom on FaceTime, you’re using Shannon’s ideas. Without them, you’d be dealing with a lot more scrambled audio and jumbled data, trust me. Learn more https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf https://en.wikipedia.org/wiki/A_Mathematical_Theory_of_Communication https://www.youtube.com/watch?v=b6VdGHSV6qg https://www.youtube.com/watch?v=kP0zi5lX-Fo 3. “A Relational Model of Data for Large Shared Data Banks” (1970) Author: Edgar F. Codd So, we can compute and communicate — awesome. But eventually, we’re buried under mountains of data. Edgar F. Codd saw this coming and introduced the relational model, which is basically the reason we’re able to store and query data. The big idea Codd said, “Let’s store data in tables and manipulate it with logical operations.” This might sound obvious now, but at the time it was revolutionary. His blueprint led to SQL and the huge family of relational databases that power, oh, basically every bank, retail website, and enterprise system you can imagine. Why it matters today Even in the NoSQL era, the underlying concepts of how we organize data (tables, schemas, consistency) trace right back to Codd. If you ever wrote a SQL query in your life — it’s all thanks to him. Learn more https://www.seas.upenn.edu/~zives/03f/cis550/codd.pdf https://en.wikipedia.org/wiki/Codd%27s_12_rules 4. “The Complexity of Theorem-Proving Procedures” (1971) Author: Stephen A. Cook Now that we’re storing data efficiently, what about the computation itself? Turns out some problems are just…painfully hard. Stephen Cook’s paper introduced NP-completeness, a concept that basically says, “Yep, some tasks are so difficult that even supercomputers sweat.” The big idea Cook showed that the Boolean satisfiability problem (SAT) is NP-complete, meaning if you magically solve SAT quickly, you’ve instantly cracked a whole bunch of other seemingly impossible problems. This created a universal language for talking about problem difficulty. Why it matters today Whenever you see “NP-hard” in a problem description, or wonder why route optimization kills your CPU, that’s Cook’s legacy. It led to huge developments in algorithms, cryptography, and the hunt for efficient solutions (or at least decent approximations). Learn more https://www.inf.unibz.it/~calvanese/teaching/14-15-tc/material/cook-1971-NP-completeness-of-SAT.pdf https://en.wikipedia.org/wiki/P_versus_NP_problem https://www.youtube.com/watch?v=dJUEkjxylBw 5. “A Protocol for Packet Network Intercommunication” (1974) Authors: Vinton G. Cerf and Robert E. Kahn Great, we have tough problems to solve and data to store — but how do we hook all these computers together? Cerf and Kahn’s TCP turned isolated networks into an interconnected web, letting data hop around the planet in tiny packets. The big idea They created a universal language for different networks to talk. Packets get split up, zipped through various routes, and reassembled on the other side. This flexibility opened the door for global connectivity — no single monolithic network required. Why it matters today Short answer? Pretty much the entire internet. Whenever you browse the web, send an email, or securely log in to your bank’s website, you’re leaning on TCP/IP to move those bits around reliably. Sure, some real-time applications might use UDP, but the core idea of IP-based networking — laid out by Cerf and Kahn — still unites all our devices under one global network. Learn more https://www.cs.princeton.edu/courses/archive/fall06/cos561/papers/cerf74.pdf https://www.youtube.com/watch?v=PG9oKZdFb7w 6. “Information Management: A Proposal” (1989) Author: Tim Berners-Lee And speaking of TCP/IP — once machines could talk to each other easily, Tim Berners-Lee asked, “How about we make this thing friendly for everyone?” That’s where the World Wide Web was born. The big idea Berners-Lee pitched a global hypertext system, complete with hyperlinks, URLs, and HTTP. Suddenly, documents around the world were no longer isolated; they were “webbed” together, turning the internet into something normal people (not just scientists) could navigate. Why it matters today We live on the web. Whether it’s social media, online shopping, or reading obscure blog posts at 3 a.m., the seeds of it all came from this straightforward proposal. It changed how we share knowledge forever. Learn more https://cds.cern.ch/record/369245/files/dd-89-001.pdf https://time.com/21039/tim-berners-lee-web-proposal-at-25/ https://www.youtube.com/watch?v=qJNrvVv7SdU 7. “The Anatomy of a Large-Scale Hypertextual Web Search Engine” (1998) Authors: Sergey Brin and Larry Page Once Berners-Lee’s web blew up, it became a jungle of links, pages, and cat memes. Sergey Brin and Larry Page decided to tame that jungle. Their approach, based on link analysis, evolved into the search engine we now call “Google.” The big idea They introduced PageRank, which viewed links as votes of confidence rather than just a new dimension to count keywords. The result was a seismic jump in relevant search results, making the web feel, well…searchable. Why it matters today Type a question into Google and get an instant answer? That’s PageRank (and a lot of subsequent innovation) at work. It redefined how we navigate information online and kicked off a new era of data-driven tech—ads, analytics, machine learning, you name it. Learn more https://snap.stanford.edu/class/cs224w-readings/Brin98Anatomy.pdf https://en.wikipedia.org/wiki/PageRank https://www.youtube.com/watch?v=v7n7wZhHJj8&t=184s Bonuses (5 That Almost Made the List) 1. “Recursive Functions of Symbolic Expressions and Their Computation by Machine” (1960) – John McCarthy Introduced Lisp and the functional programming style that still sneaks into modern languages and frameworks. 2. “Go To Statement Considered Harmful” (1968) – Edsger Dijkstra A short but fiery editorial that argued goto leads to messy, unstructured code, sparking the structured programming revolution. 3. “Time, Clocks, and the Ordering of Events in a Distributed System” (1978) – Leslie Lamport You can’t sync real clocks perfectly in distributed systems, so you need logical ones. This is a must-read if you’re into distributed computing. 4. “No Silver Bullet—Essence and Accident in Software Engineering” (1986) – Fred Brooks Brooks argued that there’s no single magical fix for the inherent complexity of software development. Decades later, as we chase the “next big thing” in frameworks or methodologies, his message remains a sobering reminder that some problems are just hard. 5. “Attention Is All You Need” (2017) – Vaswani et al. The transformer architecture behind GPT and other big-name AI models. If you’re impressed by large language models, here’s your blueprint. Conclusion These days, we’re flooded with new stuff: fresh languages, mind-blowing AI breakthroughs, quantum leaps, and the JavaScript framework of the week. It’s all super exciting, but here’s the thing: foundations matter. Without them, we’re just piling on new toys without fully understanding the ground we’re building on. The papers in this post are a reminder of where our core concepts — data structures, algorithms, the very web — came from. Share this: Facebook LinkedIn Threads Bluesky Mastodon X Like Loading… computer science information theory, papers, sql, turing Discover more from Terrible Software Subscribe to get the latest posts sent to your email. Type your email… Subscribe More Like This We’ve Been Here Before Date December 14, 2024",
    "commentLink": "https://news.ycombinator.com/item?id=42799103",
    "commentBody": "Most Influential Papers in Computer Science History (terriblesoftware.org)310 points by jonbaer 18 hours agohidepastfavorite106 comments sovietswag 13 hours agoCommunicating Sequential Processes (Hoare), The Next 700 Programming Languages (Landin), As We May Think (Bush), Can Programming Be Liberated from the von Neumann Style (Backus) And this seems to be a cool course: https://canvas.harvard.edu/courses/34992/assignments/syllabu... > This course examines papers every computer scientist should have read, from the 1930s to the present. It is meant to be a synthesizing experience for advanced students in computer science: a way for them to see the field as a whole, not through a survey, but by reliving the experience of its creation. The idea is to create a unified view of the field of computer science, for students who already know something about it, by replaying its entire evolution at an accelerated frame rate. reply timmg 5 hours agoparentSeems like you need to have a Harvard account to see the lectures(?) reply serviceberry 15 hours agoprevI actually found this to be an odd mix. Are we selecting papers that had an influence on computer science (as in, the theory of computation), or that had an impact on technology? Or are we just using CS as a catch-all for \"all things computer\"? The Turing paper is foundational for CS, but without it, would the technology have evolved differently? Probably not. Most software engineers have not read it. Conversely, the IP standard is a technological cornerstone, but there's hardly any science in it. It's just a specification of a fairly simple protocol that you need to know when doing almost anything network-adjacent. reply freetime2 11 hours agoparentSomething doesn't feel quite right to me seeing the PageRank paper in a short list alongside Turing and Shannon's foundational work on computability and information theory. “On Computable Numbers, with an Application to the Entscheidungsproblem” is almost 90 years old at this point and just as true and relevant now as it was then. Is PageRank even as relevant today as it was in 1998, let alone another 50 years from now? reply fooker 10 hours agorootparentWe can’t estimate what will be relevant 50 years from now. If quantum or biological computing find some success, none of these are going to be relevant. That said, pagerank is important to understand the modern tech world. Search is something we take for granted, and it’s great there’s a deeply mathematical (and somewhat counterintuitive) theory behind it. reply bovermyer 7 hours agorootparentprevAn important part of historiography is considering documents and artifacts in the context of their time. We don't use cuneiform these days, but back in its day, it was as close to a standard writing system as it was possible to get. reply froh 11 hours agorootparentprevindeed this was itching me, too. I wonder how pagerank was influential to CS as a field? even mapreduce is more a rally clever technique than a boundary pushing or boundary identifying extension of the field. unlike, say, CSP which is missing in the list. still unlike the conciseness and structure of the list. it could evolve into a nice book :-D reply fooker 10 hours agorootparentIf you haven’t read the pagerank paper, you should. It’s not an obvious thing. Agreed about map reduce though. reply froh 9 hours agorootparentthanks for the nudge :-) My understanding of the term \"history of CS\" would be \"how CS evolved\", as a field. How do we think about \"processing 'data' with computers\". what can we compute? what are limitations in how fast we can compute? how to we talk about and present algorithms, prescriptions for these computations? how do we talk about data and how we structure it (leading to SQL, and sgml/xml/JSON)? Pagerank in contrast is a very specific type of breakthrough, a breakthrough for it's application domain. But it's not a breakthrough for how we compute or how we think about computation. does that make sense? reply tucnak 5 hours agorootparentprevHoare's paper is in the list, no? reply froh 2 hours agorootparentsurprisingly it's not, indeed. reply tucnak 1 hour agorootparentThis is why I never read the links, and always go comment first. What joke of a list :-) reply tuyguntn 8 hours agorootparentprevWill it be directly influential in 50 years from now on? Maybe no. But, indirectly looking, it influenced establishment of Google, which influenced many thousands of innovations in this field. So yes, PageRank is hugely influential in my opinion. reply bjourne 8 hours agorootparentprevThe big insight of the PageRank paper is that you can use MC integration to approximate PR, not PR itself. Hence making the problem much easier to distribute. reply cs702 17 hours agoprevGreat list of papers. I've read five of of the seven papers on the list. The two I haven't read are Cerf and Kahn's, and Berner-Lee's. Turing's paper on computability was particularly hard to follow, for me, because he used these gothic-font upper-chase characters to name all sorts of objects, and all those characters looked kinda the same to me! I had to use auxiliary materials to be able to make my way through the paper. Today, I would recommend reading it with Charles Petzold's easy-to-follow book on the paper: https://www.amazon.com/Annotated-Turing-Through-Historic-Com... Cook's paper on NP-completeness was also hard to follow (again, for me). As with Turing's paper, I had to use auxiliary materials to make my way. Today, I would recommend reading instead an introductory book on computational complexity that works through Cook's proof. Shannon's paper is a work of art, clearly articulated and beautifully written. It's just not casual reading, to put it mildly. Brin and Page's paper, and Codd's paper, are not hard to follow, at least as I remember them, but understanding Brin and Page's work requires some knowledge of Linear Algebra. Thank you for sharing this on HN. reply generationP 13 hours agoparentWhere does the Brin-and-Page paper require linear algebra? It mentions \"eigenvector\" once, in a tangential remark. The \"simple iterative algorithm\" is how you find the fixed point of any contraction mapping, linear or not. Knowing that it is also an eigenvector is just a distraction you aren't going to use Gaussian elimination, not if you know what is good for you. reply The_suffocated 8 hours agorootparentIt doesn't require linear algebra to understand the paper or how the algorithm works, but it does require linear algebra to understand why the algorithm works. In general, since the induced 1-norm of a stochastic matrix S is exactly equal to 1 but not smaller than 1, the mapping x↦Sx is NOT a contraction. Neither convergence of the power method nor uniqueness of fixed point are guaranteed. (If there are multiple fixed points, there are multiple inconsistent rankings.) In the paper, the significance of the so-called \"damping factor\" is not clear. However, with linear algebra, we know that the damping factor makes the stochastic matrix positive rather than merely nonnegative. Hence the Perron eigenvalue is \"simple\" (i.e. of multiplicity one), the Perron vector is unique and the power iteration must converge to it. reply generationP 5 hours agorootparentThat's easier explained using fixed-point theory: The damping factor makes the mapping x↦Sx into an actual contraction (on the space of probability distributions). Not to mention that it has a simple common-sense justification (you don't want to get stuck in a subnetwork that only links to itself, or drown out a subnetwork that has more outlinks than inlinks). There is probably some gain from understanding the algorithm specifically as a Markov chain iteration (if nothing else, it provides a great example for Markov chain iteration), but I think it's perfectly possible and easier to understand it as a fixed-point iteration on a compact space. And I am someone who does algebra for a living and normally explains everything algebraically if ever possible... reply mrkeen 10 hours agorootparentprevYes, but the paper blathers on and on in prose about history and related work and goals and future work. It might only mention eigenvector once in a tangential remark, but that remark is like 80% of the algorithm content of the paper. reply yuppiemephisto 15 hours agoparentprevIf you can get .tex source files, ask GPT to rename the variables to something nicer, including more than one letter long names. Helps. reply cs702 2 hours agorootparentLaTex didn't exist when Turing wrote that paper in the 1930's. I don't known if anyone has gone through the trouble to re-typeset the original paper in LaTex. reply vitus 15 hours agoprevOh man, if you think Shannon's A Mathematical Theory of Communication is his most foundational contribution to computer science, you haven't seen his master's thesis from a decade prior. https://en.wikipedia.org/wiki/A_Symbolic_Analysis_of_Relay_a... He outlined how you could use switching elements in circuits (read: transistors) to define boolean logic. (That's not to downplay the importance of, well, establishing the foundations of the entire field of information theory.) reply Tainnor 14 hours agoprev> He sketches out a hypothetical “Turing Machine,” proving that, if something is computable at all, a machine (in principle) can handle it. That's not what Turing proved. Instead, what he proved in his paper was that there are some problems which aren't solvable by Turing Machines (and therefore presumably by any machine). That's the Entscheidungsproblem (decision problem) referenced in the title. What TFA references is the so-called Church-Turing-Thesis, which is exactly that, a thesis. It can't really be proven although we have very strong reason to believe it given that in almost 100 years nobody has found a system of computation more powerful than Turing Machines. reply aorloff 13 hours agoparentAnd in 100 years we have gone completely the opposite direction in terms of where we think artificial intelligence lies. Rather than constructing a machine with all the known truths, modern searching for artificial intelligence using machines is mostly sifting through human commentary to create an organized feuilleton machine versus Leibniz's axiomatic approach reply psychoslave 9 hours agorootparentNo, Turing had precisely that approach of feeding the machine with learning material in mind[1], but you have to build a machine apt to consume generic knowledge body before you throw at it anything. https://philsci-archive.pitt.edu/9085/1/SterrettBringingUpTu... reply aorloff 2 hours agorootparentThere have also been attempts to canonicalize knowledge on the web (semantic web) and lots of things inside of computers are in fact deterministic. But that is not the direction that AI seems to be taking, it is \"smart\" because it does a good job of parroting humans that sound \"smart\", not because it \"knows\" truths. reply honungsburk 11 hours agoparentprevI beleive you are wrong that \"nobody has found a system of computation more powerful than Turing Machines\". A turing machine can not perform indeterminacy, however, the actor model can. reply fooker 10 hours agorootparentThis is a misconception. The actor model is a way to describe that a program exists, not if it’s possible to ‘compute’ that program. You’re right that it can describe more things than a Turing machine, but doesn’t provide a constructive way to compute them. reply tromp 9 hours agorootparentprevNon-deterministic Turing machines [1] are the standard way to define Non-deterministic complexity classes like NP or NEXP, so there are definitely Turing machines with indeterminacy. [1] https://en.wikipedia.org/wiki/Nondeterministic_Turing_machin... reply mrkeen 10 hours agorootparentprevI read that sentiment here a few years ago but couldn't get anything more out of it than actors can race, but a turing machine is determistic. I could very well have it wrong. If you were computing with actors, and you also had a sufficiently-detailed spec about the actor model, is there some particular algorithm you could not compute by just executing a TLA+ spec of your actor algorithm using Turing-ish software? reply udev4096 7 hours agoprevWhere's the infamous \"Evolution of Unix time-sharing systems\" by Dennis Ritchie? https://www.bell-labs.com/usr/dmr/www/cacm.pdf reply berbec 2 hours agoparentHow about \"SEQUEL: A Structured English Query Language\" Don Chamberlin and Raymond Boyce? https://web.archive.org/web/20070926212100/http://www.almade... reply mwn 7 hours agoprevIt's not papers but I would give special mention to Why Software Is Eating the World by Marc Andreessen and Amazon's original 1997 letter to shareholders. \"Companies in every industry need to assume that a software revolution is coming. This includes even industries that are software-based today.\" https://a16z.com/why-software-is-eating-the-world/ \"But this is Day 1 for the Internet and, if we execute well, for Amazon.com.\" https://www.aboutamazon.com/news/company-news/amazons-origin... reply kleiba 10 hours agoprevSince everyone likes chiming in with their own additions to the list, here's mine: While Cook was the first to introduce NP-completeness, Karp's paper presenting 21 problems that could be reduced polynomially to 3SAT was also an enormeous cornerstone that helped kick off a more general interest in Cook's theory. https://en.wikipedia.org/wiki/Karp%27s_21_NP-complete_proble... reply gundegy_man 14 hours agoprevNice work! I was actually doing something similar on my own, so I might recommend some papers RSA: A Method for Obtaining Digital Signatures and Public-Key Cryptosystems (1978) PageRank: The PageRank Citation Ranking: Bringing Order to the Web (1999) MapReduce: MapReduce: simplified data processing on large clusters (2008) Bitcoin: Bitcoin: A Peer-to-Peer Electronic Cash System (2008) BackProp: Learning representations by back-propagating errors (1986) Hoare Logic: An Axiomatic Basis for Computer Programming (1969) reply twothreeone 15 hours agoprevDiffie, Whitfield; Hellman, Martin E. (November 1976). \"New Directions in Cryptography\" ??? reply high_na_euv 9 hours agoprev>Go To Statement Considered Harmful” (1968) – Edsger Dijkstra This is outdated and does not apply to modern goto. It is often misunderstood which causes people to avoid goto even when it is very valid, even better than alternatives solution reply chris12321 9 hours agoparentHow does a modern goto differ from a traditional goto? reply Sharlin 9 hours agorootparent\"Modern\" goto (well, there aren't many modern languages with goto, but anyway) is semi-structured. Most importantly, it's local: you cannot jump into or out of subroutines, which was Dijkstra's major peeve. Using goto for backwards jumps is also usually discouraged post-Dijkstra. reply psychoslave 9 hours agoparentprevTo my understanding, the underlying issue is the way to structure code in a maintenance friendly way. It’s just very easy to go awry with unrestricted wild goto. There are more often than not some alternatives control flows which are easier to mentally follow. And things like label in Java[1] already capture most of relevant cases in which a generic goto statements might feel like a valid approach. This doesn’t mean that there is absolutely no case where a goto might be the most elegant easiest way to implement something, but that few cases are exceptional. I mean, no one feels like using a laser is a proper way to cut butter, but using lasers is sometime the best cutting accurate option. [1] https://www.geeksforgeeks.org/g-fact-64/ reply AdieuToLogic 15 hours agoprevHere's another foundational paper: Hewitt, Carl; Bishop, Peter; Steiger, Richard (1973). \"A Universal Modular Actor Formalism for Artificial Intelligence\". IJCAI. https://www.ijcai.org/Proceedings/73/Papers/027B.pdf reply kriro 10 hours agoprevSolid list. Two that influenced me personally were: Wolpert, D. H., & Macready, W. G. (1997). No free lunch theorems for optimization. IEEE transactions on evolutionary computation, 1(1), 67-82. And the corresponding search paper. Got me started in search and optimization (and Prolog). Licklider, J. C. (1960). Man-computer symbiosis. IRE transactions on human factors in electronics, (1), 4-11. More of a philosophical outlook but the thought of man-computer symbiosis instead of \"computer solves it\" has stuck with me (and is quite relevant in this day and age). reply 0x7f1 10 hours agoprevJust wrote a blog about the explosion of papers titled after Attention Is All You Need [1]. Also figured out the name probably didn’t originate from one of the authors. [1]https://open.substack.com/pub/0x7f1/p/is-all-you-need-is-all... reply lvl155 6 hours agoprevI read Shannon’s paper every year and it gives me new insights every single time. It transcends computer science. And Claude was one of the coolest “nerds” I’ve met. reply amirhirsch 16 hours agoprevDefinitely missing from this list: J. Cooley and J. Tukey, “An Algorithm for the Machine Calculation of Complex Fourier Series,” 1965 reply loph 4 hours agoprevKen Thompson's talk (later a paper) \"reflections on trusting trust.\" https://dl.acm.org/doi/pdf/10.1145/358198.358210 reply selcuka 15 hours agoprevI would also add J. Ziv and A. Lempel, \"A Universal Algorithm for Sequential Data Compression\", 1977 [1]. LZ (Lempel-Ziv) is the foundation of many data compression algorithms that are still in use today. [1] https://courses.cs.duke.edu/spring03/cps296.5/papers/ziv_lem... reply Upvoter33 15 hours agoprevLove this, agree w/ all. Probably just needs to be a bigger list. Unix paper. Hinton on deep learning (pick one). Map Reduce + GFS from Google. Paxos from dist systems. PGP paper; RSA paper reply generationP 13 hours agoparentDoes anyone actually use Paxos in real life? And let me ask the same question for all other academic distributed algorithms right away. I recall seeing a study some 10 years ago checking the major cloud providers for Byzantine fault tolerance and finding none of them to exhibit the qualities that the known algorithms would guarantee; apparently they would just rely on timing and hoping things don't get too twisted. reply vitus 6 hours agorootparent> Does anyone actually use Paxos in real life? Yes, it's very widely used at Google through Chubby, which underpins many core pieces of infrastructure, including name resolution. (It used to be common practice to depend more directly on Chubby for synchronization of state via shared global files, but that fell out of favor about 6 years ago due to reliability risks associated with just blasting out changes globally without any canarying.) https://en.wikipedia.org/wiki/Paxos_(computer_science)#Produ... lists a bunch of other use cases (notably, Google's Spanner and Amazon's DynamoDB). > And let me ask the same question for all other academic distributed algorithms right away. Raft (designed as a more understandable alternative to Paxos) is more commonly used, as I understand it. https://en.wikipedia.org/wiki/Raft_(algorithm)#Production_us... > I recall seeing a study some 10 years ago checking the major cloud providers for Byzantine fault tolerance and finding none of them to exhibit the qualities that the known algorithms would guarantee. I'm curious which study you're referring to. I could believe that while Paxos might be used as a building block, it might not be used in a consistent manner throughout. Also, note that not all of the variations of Paxos handle Byzantine faults. reply generationP 5 hours agorootparentAh, so Raft is what everyone uses, and Paxos is used by the very big providers nowadays. Good to know! I can't find the study any more, though I'm pretty sure I saw it on HN... reply baq 12 hours agorootparentprevYes, if you’re using any sort of multi master db, you’re using a variant of paxos, raft or something which you shouldn’t really trust until aphyr blasts it into the low earth orbit. The paper itself is very approachable and worth spending an hour or so on. reply generationP 5 hours agorootparentI remember trying to read the paper and giving up somewhere early in the proof. I certainly don't think it gives a great intuition why the algorithm holds, so \"approachability\" is a matter of definition (does it tell a fun story? sure yeah). reply bsingerzero 15 hours agoparentprev+1 on map reduce, thats a classic systems paper. Surprised no one has mentioned The Unreasonable Effectiveness of Data. Data is more important than complex domain specific algorithms. reply anthk 8 hours agoparentprevLisp from McArthy. Plan9+CSP. Both, maybe, polar opposites, but complementary. reply FranchuFranchu 16 hours agoprevIt would be interesting to see the opposite of this; which papers are really interesting and look useful, but did not end up having a significant impact? reply khazhoux 15 hours agoparentI could mail you my dissertation if you're interested reply gundegy_man 14 hours agoparentprevProbably the papers about VLIW design reply Upvoter33 5 hours agoparentprevPlan9 reply ahartmetz 8 hours agoprevEspecially the Codd paper (what if Codd had died in World War 2?) makes me wonder: What are the non-obvious (kind of arbitrary even) yet extremely helpful concepts that no one has thought of or that didn't find the resonance that they should have? reply Clubber 6 hours agoparentBinary search and efficient sorting algorithms is pretty important. reply rcpt 13 hours agoprevI think AlexNet was more influential than Attention is All You Need reply RamblingCTO 11 hours agoparentI'd argue both as well as McCulloch & Pitts. Maybe Boltzmann or Rummelhart (Backprop) as well. Honestly, I wouldn't know where to stop, there are so many cool papers. reply rcpt 22 minutes agorootparentYeah. But before AlexNet GPUs were only for graphics and esoteric papers in scientific computing. The realization that conv layers map well to cuda cores has led to GPU production being a national security issue. reply lccerina 6 hours agoprevNot a single woman? No Barbara Liskov Programming with abstract data types? No Grace Hopper The education of a computer? No Frances Allen Program optimization? reply peanutcrisis 6 hours agoparentShould the work of women be on that list for the sole reason that they are women? There are many more men who have written papers far influential than the ones you've mentioned yet they didn't make the list. If you believe in equality, then you have to believe that the work of people who happen to be women can compete on their own merit. The absence of women in that list isn't necessarily evidence of bias as implied in your remark. reply EdwardCoffin 6 hours agorootparent> papers far influential than the ones you've mentioned Citation needed reply peanutcrisis 6 hours agorootparentDon't act in bad faith, the entirety of this thread is filled with examples. reply EdwardCoffin 6 hours agorootparentI'd put Liskov's Programming with abstract data types up against any of them. Fran Allen's work was so fundamental it's hard to find compiler stuff that doesn't build on her work. > Don't act in bad faith This sounds like projection to me reply peanutcrisis 6 hours agorootparentYou asked for \"citations\", the thread is literally filled with references to them. How is it not bad faith to have to prove to you things that you can easily check for yourself? reply EdwardCoffin 6 hours agorootparentYou misunderstood the request. Your original comment was claiming that there were many papers far more influential than any of the papers named that were by women. I was requesting evidence of this influence. In response you say that what, all of the references filling this thread are more influential than say Liskov or Allen? If not all, which ones? The original comment you were responding to was pointing out that none of the papers listed were by women, and suggested several that were that are undeniably influential. Perhaps you think they aren't because you haven't read them, or presumably even heard of them? reply lvl155 6 hours agoparentprevI don’t think representation needs to be a thing for a personal list on a blog. Government? Absolutely. Corporate? maybe. That said, of course there have been many critical female contributions in the field. However, it’s also a numbers game since CS academia has been very gender/sex lopsided to this day. So production would represent that (sad) reality. reply akimbostrawman 5 hours agoparentprevDid you just assume their gender? reply baq 12 hours agoprevThe Part-Time Parliament by Leslie Lamport, written in such a style it is its own complementary material. (This is the Paxos paper.) reply zipy124 7 hours agoparentI think any list like this has to include: \"Time, clocks, and the ordering of events in a distributed system\" By Lamport also, in almost any networked system having strict ordering is fantastically useful. And of course how could we also forget \"The Byzantine generals problem\". reply ripe 13 hours agoprevIvan Sutherland's 1963 PhD thesis really was seminal for the graphical user interface. \"Sketchpad: A Man-machine Graphical Communication System.\" reply generationP 13 hours agoprevFor pratical purposes, perhaps this: Andrew Tridgell and Paul Mackerras, The rsync algorithm, June 1996, https://www.andrew.cmu.edu/course/15-749/READINGS/required/c... reply jldugger 15 hours agoprevIf this is up your alley, Ideas That Created the Future[1] has like 40 more, decent introductions to their ideas, and provides a sort of thematic throughline from Aristotle to the modern computer science. [1]: https://mitpress.mit.edu/9780262045308/ideas-that-created-th... reply jmgimeno 10 hours agoprev\"The UNIX TimeSharing System\", by Dennis M. Ritchie and Ken Thompson. reply 65 16 hours agoprevSurprised the Bitcoin paper isn't on here. reply udev4096 7 hours agoparentBTC is just combining all the past research into an application, which has it's own place but sadly not here. You might wanna read this [0] for all the past ideas that satoshi took [0] https://queue.acm.org/detail.cfm?id=3136559 reply sho_hn 15 hours agoparentprevIt doesn't really add anything to computer science, but then again the Sergey-Brin paper probably doesn't match that rigidity either. reply mrkeen 10 hours agorootparentI'm not so sure. In my (under-read) mental model, blockchain takes you from fail-stop fault-tolerance (a la Paxos) to Byzantine fault-tolerance, i.e. how do you compute in a massively distributed system when no node has any reason to trust any other node. reply ggm 14 hours agorootparentprevMerkles work probably more important. And progenitor papers on representation of branch streams implementing reversible editors reply dtaht 15 hours agoprevI´m pretty fond of ¨Ending the anomaly¨: https://www.usenix.org/system/files/conference/atc17/atc17-h... reply sho_hn 15 hours agoprevNice list! I would add \"On the Criteria to Be Used in Decomposing Systems into Modules\" (1972, ~7800 citations) by Parnas, though more software engineering than computer science in a strict sense. reply turingbook 11 hours agoprevSee also the book or the course by Harry R. Lewis at Harvard: Ideas That Created the Future https://www.amazon.com/Ideas-That-Created-Future-Computer/dp... \"Classics of CS\" https://canvas.harvard.edu/courses/34992/assignments/syllabu... reply mianos 17 hours agoprevI went with some cynicism. But completely wrong. These are it. Most every one recognisable as the root of many things we do. reply khazhoux 17 hours agoparentHa, me too! When the first title included Entscheidungsproblem I thought this would be intellectual edgelording. But, this is a legit list. Of course, you can't do justice to the entire field with such a short list. Two papers on web but none on graphics, e.g. But it's a fine reading list for sure. reply ggm 14 hours agoprevLouis Pouzin's work underpins/predates much of Cerf/Kahn. I think the also ran list should be in the main list. reply daitangio 11 hours agoprevI suggest to add at least Hoare CSP and something about quick sort reply anthk 9 hours agoparent9front/plan9 sites have that paper and it's magical and even an HSer would understand it. reply jdougan 15 hours agoprevJ.C.R. Licklider, \"Man-Computer Symbiosis\" from 1960. reply wslh 16 hours agoprevI wonder how someone could make such a list and completely ignore cryptography. No, it is not enough to mention NP-completeness. reply yazantapuz 4 hours agoparentdiffie-hellman should be on that list... maybe instead of he pagerank one imho. reply gorby91 15 hours agoparentprevIs there a particular paper to point to? reply Tainnor 13 hours agorootparentWe have Shannon's \"Communication Theory of Secrecy Systems\" as arguably the beginning of modern cryptography and then Diffie & Hellman's \"New Directions in Cryptography\" which first introduced public-key cryptography. reply wslh 7 hours agorootparentAnd FHE, MPC, ZK, among breakthroughs. Easy to check on the Wikipedia Turing Awards page [1]. Use Gödel prize as a \"helper\" [2]. [1] https://en.wikipedia.org/wiki/Turing_Award [2] https://en.wikipedia.org/wiki/G%C3%B6del_Prize reply anthk 8 hours agoprevhttp://www-formal.stanford.edu/jmc/recursive.html https://paulgraham.com/rootsoflisp.html This is the arch-rival of Unix+C, and also one of his best CS friends (GNU Emacs it's still there, and it was widely used on Unixen, among reusing GNU (Unix clone) tools). reply anthk 9 hours agoprevLambda calculus and Lisp are missing. reply igtztorrero 15 hours agoprevPlease add Satoshi Nakamoto, for Bitcoin and the money revolution reply high_na_euv 9 hours agoparentIdk why it is downvoted tbf Bitcoin paper is really interesting from CS perspective And also had HUGE influence on the world reply anthk 8 hours agorootparentNo, sorry, you are deluded. Compared to the ones from Shannon, Knuth, Ritchie, John McArthy... BTC barely maked a dent. reply high_na_euv 4 hours agorootparentIt literally creates trilion dollar \"industry\", calling it barely a dent is weirsld as hell And no, I dont own btc reply badgersnake 10 hours agoprev [–] I thought we were going to get through without any LLM bullshit, but no, they snuck it in at the end. reply Al-Khwarizmi 4 hours agoparent [–] Come on, we now have systems that can believably answer arbitrary questions in human language. This is literally what I dreamed of when I got into computing like 25 years ago, and would be considered science fiction only 5 years ago. As a side effect, entire tasks as important as machine translation and summarization have pretty much been solved for major languages. Regardless of whether you buy the full hype or you think they're just stochastic parrots, I think it more than qualifies to make the second list (and probably the first, but I get that there's no perspective to be so sure about that). The paper itself (as a paper, i.e. an explanation of the underlying results) is quite bad, by the way. It's better to learn about Transformers from one of the many good blog posts. But that doesn't detract from its influence. reply Consider applying for YC's Spring batch! Applications are open till Feb 11. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The list highlights seven seminal papers in computer science history, each contributing foundational concepts that underpin modern technology, such as the Turing Machine, information theory, and the relational database model.",
      "These papers introduced groundbreaking ideas like NP-completeness, TCP/IP, the World Wide Web, and PageRank, which have significantly influenced computation, communication, and internet development.",
      "Bonus mentions include influential works on Lisp, distributed systems, and the \"Attention Is All You Need\" paper, showcasing the evolution and diversity of computer science research."
    ],
    "commentSummary": [
      "The discussion centers on influential computer science papers, including classics like Hoare's \"Communicating Sequential Processes\" and Turing's work on computability.",
      "Participants debate the inclusion of more recent works, such as the PageRank paper, and suggest other impactful papers in cryptography and distributed systems.",
      "A Harvard course is mentioned, which examines these foundational papers to offer a comprehensive understanding of the evolution of computer science."
    ],
    "points": 310,
    "commentCount": 106,
    "retryCount": 0,
    "time": 1737592149
  },
  {
    "id": 42798108,
    "title": "F-Droid's Progress and What's Coming in 2025",
    "originLink": "https://f-droid.org/2025/01/21/a-look-back-at-2024-f-droids-progress-and-whats-coming-in-2025.html",
    "originBody": "A Look Back at 2024: F-Droid's Progress and What’s Coming in 2025 Posted on Jan 21, 2025 by HStill With 2024 now behind us, we wanted to take a moment to reflect on the growth and achievements we accomplished as a community last year, and celebrate the incredible support we received from the FOSS community throughout the journey. This year has been a milestone for us, with significant strides in decentralizing app distribution, expanding the F-Droid ecosystem, and solidifying our infrastructure. All of these advancements were made possible thanks to donations, grants, our volunteers and regular contributors. So thank you again to everyone who helped make 2024 another great year for F-Droid. Now let’s take a closer look at what we accomplished. A Review of Key Accomplishments in 2024 Decentralizing App Distribution: A Core Focus One of the most important initiatives we worked on in 2024 was the continued development of our app decentralization efforts. Our aim is to make F-Droid a more robust and accessible platform, making further in-roads into the hold Big Tech currently has on app distribution. Building on the work we started in 2022, as a part of the Filecoin Foundation for the Decentralized Web grant, we continued to make substantial progress this year in providing developers and end-users with more options to distribute their apps through a decentralized, equitable and privacy-oriented process. The goal for this project is to enable individuals and organizations to mirror and distribute F-Droid apps in a community-driven fashion, reducing reliance on centralized services. This work ties into a larger vision of creating a truly open-source ecosystem for Android apps that is not governed by proprietary companies. In 2024 we completed the following infrastructure upgrades: Broke out and overhauled core client logic around publishing and consuming repositories. Made client logic into libraries to make it easy to embed repositories in any app that needs it. Added support for mirroring repositories onto both IPFS and Filecoin. Added support in F-Droid client to use mirrors and repositories hosted on IPFS and Filecoin. Improved F-Droid client “whitelabel builds”. Enhanced F-Droid client’s existing “nearby” and “app swap” capabilities. Updated F-Droid’s Repomaker tool (for easy “point and click” curation and publishing of app repos) and add support for IPFS publishing. Supported iOS apps and progressive web apps (PWA) as packages that can be shipped via repositories. 2024 marked the end of this grant period, however the tools, features and policies established within the scope of the grant, will continue to be developed thanks to donations and other funders who are committed to further decentralizing app distribution. Expanding the F-Droid Ecosystem: Repomaker and Mobifree Another key project we started in 2024 was the further exploration and expansion of tools within the F-Droid ecosystem. Thanks to a major grant from EU Horizon Europe, we were able to deepen our focus on tools like Repomaker, which helps developers create their own F-Droid-compatible repositories. This tool is crucial for maintaining and growing the diverse range of apps available on F-Droid, and its expansion will support more developers who wish to contribute to the platform. Mobifree is an initiative that aims to provide a free, open-source, and decentralized alternative to traditional mobile app stores. It focuses on the freedom of choice, privacy, and user empowerment. Our contributions to this project will help strengthen the ties between F-Droid and other decentralized app distribution systems, ensuring that we remain a key player in the future of open-source mobile software. Do you have your own project ideas for Mobifree? You can apply for up to 50,000€ from NLnet NGI Mobifree. We can help you apply, just reach out via the regular F-Droid channels. Key Contributions in 2024: User research to understand app developer distribution workflows, compensation models, index preferences and APK metadata. Built on existing Fastlane tooling to further automate the app package and upload the process to F-Droid. Repomaker feature development made the repo creation and distribution process easier for non-tech users, non-profit and humanitarian organizations. Created a central registry of all DAPPER compatible repos and API for app stores like Murena’s App Lounge to pull in compatible repos and further decentralize app distribution process. In 2025, we continue to contribute to the Mobifree project, getting our tools, apps and software ready for pilot testing this spring. From there, we will continue to improve the software based on user-testing feedback, ultimately making the tools available for the general public to use and enjoy. F-Droid Community Engagement None of this would be possible without the incredible contributions from the FOSS community. In 2024, we saw a substantial increase in the number of contributions to F-Droid. From bug fixes and app updates to new apps being added to the store, our community of developers, testers, and contributors have been pivotal in keeping F-Droid running smoothly. Here are some statistics that highlight the community’s impact this year: App Updates: Over 7205 app updates were made, keeping the app catalog fresh and secure. New Apps Added: We welcomed over 402 new apps to the F-Droid repository, further expanding the variety of open-source apps available to users. Archived Apps: 939 Apps were successfully archived. Packages per Hour: Approximately 2-3 packages were created every hour, without interruption. Between updated and new apps, F-Droid built apps over 7600 times. Since most modern apps are split per device architecture, this means that each app version required multiple packages to be built, sometimes up to 4 packages for each app version. This increases the real number of builds by 2.5-3x. Putting it all together, that would equal between 2-3 packages created per hour, without interruption in 2024. These numbers are a reflection of the dedication and passion of the F-Droid community. We are immensely grateful for each and every contributor who made these achievements possible. Looking Ahead in 2025 New Team Members and Big Plans for 2025 As we look forward to 2025, we are excited to announce two key additions to the F-Droid team who will help us scale our efforts in the coming year. Hailey Still has joined us as a new project manager and UX designer. Hailey brings a diverse background of experience in managing complex projects and designing user-friendly and intuitive interfaces. Her expertise will continue to be instrumental in helping us secure new grant opportunities, streamline our operations and improve the user experience tools we work on. We are also pleased to welcome Nzambi Kakusu as our new grant administrator. Nzambi will play a crucial role in helping us secure and manage funding for F-Droid’s continued development. With her experience in grant management, Nzambi will help ensure that we can continue to sustain and grow the project in a way that aligns with our mission and values. OTF Grant and Infrastructure Work In 2025 we are thrilled to begin working on a grant funded by the Open Technology Fund. This grant will help us maintain F-Droid and focus on critical infrastructure work that was often overlooked, due to lack of consistent funding in the past. We’ll be working on improving the resilience and security of our systems, ensuring that F-Droid continues to serve as a reliable, open-source app distribution platform for years to come. There will be an official announcement article coming soon. To Wrap it Up As we continue to grow and evolve, we are committed to our mission of decentralizing app distribution, expanding the F-Droid ecosystem, and empowering users with open-source alternatives to proprietary software. With the support of our community, our volunteers, and our new team members, we are excited for what 2025 will bring. Thank you for being part of the F-Droid journey! We look forward to an exciting year ahead, and we invite you to continue supporting F-Droid through contributions, donations, and spreading the word about our mission. Let’s make 2025 another year of progress for the open-source community!",
    "commentLink": "https://news.ycombinator.com/item?id=42798108",
    "commentBody": "F-Droid's Progress and What's Coming in 2025 (f-droid.org)302 points by mappu 20 hours agohidepastfavorite88 comments crote 9 hours agoI wish they would add some kind of download counter. A decent bunch of the apps on F-Droid are woefully inadequate or still early in development. At the moment F-Droid allows sorting apps either by time of most recent update, or alphabetically. Neither of those is actually useful to me as user. If there are multiple apps doing the same thing, it is currently incredibly hard to see which one is the best. I do not want to download and install multiple apps only to find out they are essentially unusable, and that of the 5 options only a single one is actually functional. I understand that F-Droid might not want to go all-in with the rating and review system, but a barebones download counter would at least show some indication whether people install it once and remove it after trying it out, or leave it installed long enough to survive multiple updates. reply jron 4 hours agoparentF-Droid is littered with dead-end pet projects and no easy way to filter them out. Instead of wasting time on Filecoin pipe dreams, they should just fork Modrinth https://github.com/modrinth to handle Android apps instead of Minecraft mods. The difference in usability is night and day: https://modrinth.com/mods reply vrighter 5 hours agoparentprevI fully agree with this. It's so hard to actually find good apps on f-droid because of this. I've been bit by woefully incomplete prototypes on f-droid multiple times. reply spookie 3 hours agoparentprevUnfortunately that would lead to many great apps not getting proper attention. Things such as a downloads counter would simply promote the highest rated ones, preventing you from even considering newer alternatives. I would prefer to have more options to filter by time of release. Such as past month, past 6 months, and past year. That would filter out unmaintained apps altogether, even though there are many that still work well years after release. Another one would be search operators, or simply a better string matching algorithm, as I keep searching for apps that I know the name of, but make some typo, and they don't appear. Wonder what that is about. reply Almondsetat 9 hours agoparentprevHow does a download counter tell you if people immediately uninstall the app because it sucks? reply Jarwain 8 hours agorootparentDepends on implementation. One way, which is less private, show active installs by having the phone reports back to the store if it's unstalled A more secure way, show downloads for each version/build. Graph it. If an app is releasing/being updated regularly but more users are uninstalling, you'd expect the downloads per version to decrease or stay level reply irjustin 8 hours agorootparentprevDon't know about the parent, but for me VS Code's download count + star rating mental combo does make not download some extensions. reply pmontra 3 hours agoparentprevI usually visit the source code page (almost every time on GitHub) and check the timestamp of the commits, how many issues are open, how they are handled, etc. The usual indicator of a project being alive. The number of downloads is less important. Often there are plenty of equivalent proprietary apps on the Play Store with 100'000+ downloads and I'm going to F-Droid because I trust open source apps more. 10 downloads would be enough. reply DuncanCoffee 4 hours agoparentprevI sort by last updated and check the date. Since most of the projects are on Github I then open it and check last commit/stars. Maybe they could add Github stars? I use droidify btw reply mfld 5 hours agoparentprevAgreed. Or number of installations, if that is tracked. reply gunian 8 hours agoparentprevwhat if download counts lead to app dysmorphia? reply redox99 12 hours agoprevAfter more than a decade, the UI for updating apps in F Droid is still very buggy. One of its most basic and fundamental features. I don't get it. reply pmontra 12 hours agoparentStill not perfect but it improved a lot in the last year. When it says update, it really updates now, and when it says updated it's updated. Running the package installer works, etc. I believe that all the process is not very well integrated because it must run external components, whilst Play does it internally, but I might be wrong. reply a-french-anon 10 hours agoparentprevAgree, did a big update yesterday and the entries in the Update tab still get stuck in \"Downloading\" state even when they finished installing. But oh well, it does work in fine. Still a bit sad they destroyed the UI usability years ago on the altar of material design. reply emaro 12 hours agoparentprevThese days even automatic updates are working. But I agree the UI of the official app (there are others) isn't amazing. reply yjftsjthsd-h 11 hours agorootparent> But I agree the UI of the official app (there are others) isn't amazing. I think it does help that there are others. The official app should be good, but there's a decent collection of others that can do things better. If nothing else, they can make different tradeoffs, like only supporting new Android versions. reply Lammy 10 hours agorootparentprev> there are others Relevant: Droidify https://f-droid.org/en/packages/com.looker.droidify/ (meta lol) reply vrighter 4 hours agorootparentThis is the one I use and has worked pretty much flawlessly since day 1. I highly recommend it. I don't even have the f-droid app itself installed anymore. reply internet_points 10 hours agorootparentprevat least it's less annoying than Google Play :-) reply crabbone 5 hours agorootparentprevNot here, no. Updates don't work (a year-old Nokia). I used F-Droid to install Hacker's Keyboard and some FTP server. Both programs still work, but when F-Droid tries to upgrade anything it hangs for a while and then displays some error message. Well... I don't really care, as long as the programs I need sort of work. reply emaro 1 hour agorootparentI checked the requirements and automatic updates are supported from F-Droid 1.19 and Android 12. However it sounds like you may have a different issue. reply jorvi 9 hours agoparentprevJust use NeoStore. Uses F-Droid on the back-end but is somewhat less buggy, has a few more features, and is a lot more modern. reply rekoil 10 hours agoparentprevIt is also completely unusable on Android TV devices. reply Nux 9 hours agorootparentNot \"completely\", but it is a pain. reply openrisk 10 hours agoprev> progressive web apps (PWA) as packages that can be shipped via repositories Aah, wasn't aware this is possible now, thats cool. But raises the question of how exactly to check and communicate to users what data are being collected. With a PWA almost all of the code runs on the server. Even if that server is based on an open source project, there is no guarantee that there has been no modification (eg a plugin) that changes behavior. Not sure there is a way to reconcile these demands in an automated way that doesnt ultimately fall back on trusting the developer. reply black_puppydog 10 hours agoparentI thought that the whole point of PWAs is that they can nearly all the logic and storage on-device? reply openrisk 9 hours agorootparentIts very flexible, both ends of the spectrum are within the specification. One could force the \"on-device only\" by requiring no data exchange with the outside world but thats quite limiting too (and not trivial to check at upload time). reply idle_zealot 7 hours agorootparentSo... like a normal app? Nothing stops native apps from mostly running on the server. In fact, many do. Social media, banking, heck, do maps apps even do offline routing, or do they just query a server for that usually? reply openrisk 6 hours agorootparentyeah, its a very blurry line (like the overall separation and balance between client and server roles anyway). reply zo1 9 hours agorootparentprevThere is way too much marketing and \"evangelism\" around PWAs that it's hard to pin down exactly what they are or what they want the term to encompass. reply daniel-s 8 hours agoparentprevIs that as big a problem for PWAs? These apps are still running in the browser so never have access to many parts of a phone a native app would. reply tasn 14 hours agoprevBeen a happy f-droid user for as long as I can remember (and even published some packages there). Thanks and kudos to the f-droid team! reply Lerc 10 hours agoparentI too like f-droid but have found it too difficult to figure out how to publish packages for it. Last time I looked at it all the documentation covered tool chains I don't use. I see that there are Godot games on f-droid now so I hope that means there is an easy path to go from a Godot project to f-droid package. Also is there any support for PWAs on F-droid? reply tasn 6 hours agorootparentI had to write (heavily improve? I don't remember) the React Native guide when I release my first RN app there (non RN apps were easy). Though I assume/hope things are better now? reply epse 9 hours agorootparentprevPWA's just got a callout in the OP, freshly supported since this year reply donatj 10 hours agoprevAnyone have any strong recommendations for apps available on F-Droid I should try? I think I have maybe two apps installed via it currently. If something is available on the Google App Store I'm generally using that instead. reply camel-cdr 9 hours agoparentI'm the other way arround, 2-3 things from playstore and the rest from f-droid. Here is my list: Acode (text editor) Aegist (2FA) AntennaPod (podcast) Aurora Store (playstore) FairEmail (E-Mail) Feeder (RSS reader) Fennec (firefox with extension support) KeePassDX (password manager) KOReader (ebook reader) LocalSend (send data between devices in local network) Mastodon mobile client MuPDF viewer (PDF viewer) Oddysey (music player) NewPipe (youtube frontend) Offi (public transport) OsmAnd (map) QR Scanner (PFA) Red Moon (lower screen brightness lower than minimum for reading before bed) ScreenStream (stream screen to website in local network) Stealth (reddit client) Termux (linux shell eviroment) VLC reply szszrk 9 hours agorootparentF-Droid Basic is cool. Same thing, just without some sharing options etc., you probably don't use anyway. It was the first one to get actual background updates on modern android. Barcode Scanner is decent, it even searches for products online, if you scan something that looks like a food label etc. Casio G-Shock Smart Sync if you are tired of official Casio app in a casio bluetooth watch. DuckDuckGo is there... FadCam if you ever need a recording without tracking attention (while phone is locked). Could get you out of some legal trouble... Futo Keyboard is interesting project. GadgetBridge saves some old devices. I use it for some Xiaomi eink devices that overwise require... to be connected to chinese datacenter to be discovered and sync. Which is insane, as some versions discover and can be synced on EU servers... Grayjay for videos on many platforms. So many HackerNews clients I'm on Hacki currently. Home assistant, Jellyfin, Joplin, Mullvad, Telegram, Nextcloud, Organic Maps, Osmand~, Tailscale or Yubico authenticator are all there. I like them better from f-droid. Street Complete is an awesome way to contribute to OpenStreetMap. Thunderbird (!) which is a fork of K9 (still active), and looks and runs great. Highly recommend. reply ubertaco 5 hours agorootparentprevI recently swapped from the F-Droid version of AntennaPod to the Google Play Store version so that I could use Chromecast, which they strip from their F-Droid builds because the underlying library isn't open-source and is deemed \"impure\" by F-Droid (it gets you a \"This app has features you might not like\" banner, when honestly it's a feature I specifically want). A similar thing is true of Tempo (a Subsonic-client music player), where the F-Droid builds have Chromecast support stripped out, but the GitHub-published builds have it (so I also have to install Obtanium to get those updated). \"I want to listen to my audio on my devices in my house\" is a weird thing to exclude in the name of open-source purity. Otherwise, I love F-Droid. I just wish they had a bit more nuance to recognize that Chromecast support isn't the same as \"constantly reports your location in the background to corporate servers\", and so those shouldn't have the same severity of warning banners applied. reply nosrepa 1 hour agorootparentWeird, I can can stream antennapod's audio to any of my Google home devices and I assumed it'd work the same should I want to use my Chromecast. reply nosrepa 1 hour agorootparentprevAlso another odd thing to note, the droid version of antennapod has android auto support and the play version doesn't! reply bramhaag 6 hours agorootparentprevGreat list, some of the apps I commonly use you did not mention: Aves Libre (Gallery) Breezy Weather (Weather overview) Fossify Calendar Immich (Self hosted Google Photos alternative) K-9 Mail (E-Mail, to be rebranded to Thunderbird Mobile at some point in the future) NetGuard (Firewall and logging) Nextcloud (Self-hosted cloud storage) Unexpected Keyboard (Keyboard that works nicely with Termux) reply philipwhiuk 6 hours agorootparent> K-9 Mail (E-Mail, to be rebranded to Thunderbird Mobile at some point in the future) They decided not to rebrand but instead to publish two versions of the same app. See https://github.com/thunderbird/thunderbird-android for details The Thunderbird download is: https://f-droid.org/en/packages/net.thunderbird.android/ reply bramhaag 5 hours agorootparentI must've missed this somehow, thank you! reply tetris11 6 hours agorootparentprevFrom me: WifiAnalyse (hidden camera scanner) AdAway (block adds) HomeAssistant (automation) LocalSend (easy LAN send) OrganicMaps (nice OSM) reply never_inline 6 hours agorootparentprevFennec is still maintained? Also KDE connect can be alternative to sending files if you use linux on desktop. reply camel-cdr 2 hours agorootparentYes, although there were problems arround patching the the recent vulnerable version 129.0.2, because the build script broke: https://forum.f-droid.org/t/fennec-vulnerability-recommended... The fennec app was last updated 10 days ago, so it's now on a very recent version. (134.0.0 vs the latest 134.0.2) reply dizhn 7 hours agorootparentprevLibrera is an awesome ebook reader. The version on fdroid is the same as the paid app on the play store. reply Coolbeanstoo 6 hours agorootparentI hadnt checked it out in a while, but it seems to support OPDS now which is nice, I can browse my ebook library remotely now reply easyKL 2 hours agorootparentLibrera FD (F-Droid) has OPDS stripped out. https://github.com/foobnix/LibreraReader/issues/1335 But you can get the Play Store version through Obtainium. It's the PRO version (also available with Obtainium) the one with OPDS support. Kore reader also can open OPDS collections. reply easyKL 2 hours agorootparent*KOReader reply nanoxide 7 hours agorootparentprevIt's super annoying though, that installing an app from F-Droid sometimes appears as installed from Play Store as well. For example, I have VLC installed from F-Droid, but it also appears in the Play Store update list and _can't_ be updated from there. reply idle_zealot 3 hours agorootparentDoes someone know why this is? I assume it's Google just assuming it's the only app source and not bothering to check the installation source associated with apps when pulling them into its update check list. Or is there no way on Android to check whether you're the one that installed an app? reply crtasm 4 hours agorootparentprevYes, I wish Google would fix that. reply Gormo 5 hours agorootparentprevI thought I'd just point out that KeePassDX can itself do 2FA (as can KeePassXC on the desktop, so you can keep your TOTP keys synced too). reply Freak_NL 10 hours agoparentprevOsmAnd~, the premium version of OsmAnd (OpenStreetMap map application) is available for free on F-Droid (as permitted by the source licence). reply user070223 10 hours agoparentprevAnkiDroid (Anki Spaced repetition app) AntennaPod (Podcasts app) Coffee (quicktile alowing adjusting how long will the display will stay awak) NetGuard (VPN to control app internet access & domain based ad blocking) Newpipe (alternative youtube & other media sites frontend) OsmAnd (Navigation based on Open Street Map) Sky Map (google's sky map) Telegram FOSS (Telegram client) VLC (media player) WiFiAnalyzer reply vharuck 5 hours agoparentprevFor games: Forkyz (collects crossword puzzles from different sources, like the New York Times) PySolFc (mahjong, solitaire, other classic single-player games) Shattered Pixel Dungeon Dungeon Crawl Stone Soup MTG Familiar (database of Magic the Gathering cards and rules) Learning Japanese: AnkiDroid Kakugo (for kanji writing practice) Miscellaneous: NewPipe (buggy every so often, but it makes YouTube bearable for me) OsmAnd~ (offline map and directions app) SecScanQR (basic QR reader) Vanilla Music LibreOffice Viewer MuPDF viewer KeePassDX Termux reply eclecticfrank 6 hours agoparentprevI'm using: FreeOTP (OTP tool) Simple Chess Clock (once a year) Binary Eye (QR code scanner) Etar (Calendar) Simple Notes Pro (Very nice and simple note taking app) OsmAnd (Maps with great path details for hiking) reply merryocha 5 hours agoparentprevOthers have mentioned many good apps, but I'll add this one: Forkyz: Crossword puzzle app, a fork of the Shortyz app. If you like crossword puzzles, this pulls from a bunch of daily crosswords. reply alisonatwork 8 hours agoparentprevIn addition to all the other recommendations, Trail Sense is pretty great, and BRouter is quite useful for finding alternative routes if you use OsmAnd and cycle a lot. I also use Etar and DAVx⁵ for calendar and contacts sync. Rethink is useful if you don't already have another VPN setup for ad blocking etc. reply npteljes 7 hours agoparentprevI regularly use the following: Osmand and Organic maps for navigation (I like both, I just always keep two of essential apps, for redundancy) Fennec browser Cell and SatStat to monitor my phone connectivity RedReader for reddit UVC_Camera for my usb cam FairEmail, Etar calendar, KOReader, Nextcloud, Joplin, Deck, Catima, Davx5, jtxboard, FreeOTP for productivity / life organization Gauguin, AnutoTD, Apple flinger, 1010 klooni, SolitaireCG are my go-to games. reply t0bia_s 10 hours agoparentprevAurora Store to replace Play Store completely. reply ap-andersson 10 hours agoparentprevI myself use the following that are on F-Droid: Hacki for Hacker News Immich Organic Maps Synchting-Fork VLC Voice Audiobook Player reply dizhn 10 hours agorootparentWhy fork of Syncthing? reply Eavolution 9 hours agorootparentThe official Syncthing has been discontinued on android, I believe due to the difficulty of keeping up with the changing APIs. reply dizhn 8 hours agorootparentThank you. reply Aissen 3 hours agoprevSharing a small tip I discovered recently: Android 12 introduced the ability for third party apps to do background updates of apps they installed (or last updated). This should be usable by F-Droid, but is not yet to update itself(!) because it targets an older Android version (targetSdk 25). F-Droid Basic (by F-Droid developers) removes some features, but targets Android 14 instead (targetSdk 34): https://f-droid.org/packages/org.fdroid.basic/ ; my understanding is that it should be merged at some point, but isn't yet. Note that targetSdk is different from minSdk: https://developer.android.com/guide/topics/manifest/uses-sdk... F-Droid article on this: https://f-droid.org/2024/02/29/twif.html#f-droid-core reply ap-andersson 11 hours agoprevIn general F-Droid works well for me even if I have only maybe 2-3 apps that I get from there. I think it's good that it exists. There could be some improvements on the process of adding an app though. I recently made a very small app for myself that I thought \"maybe a couple of people would find this useful, I should add it to F-Droid\". I started by reading their docs on how to add a new app. I created an account on gitlab, forked their repo and added my app. Their pipeline failed without telling me why. After reading and re-reading everything a bunch of times I had to give up and look for help. The instructions said to fix all pipeline problems before creating a merge request. So I go to the contact page where there are a bunch of options. I chose IRC and join. Ask for help. No answer or any message sent in the channel for the next 24 hours. At this point I am getting a bit irritated and are thinking about truly giving up. Then eventually someone answers in the channel and says \"create a merge request and someone will help you\". Which is what the docs said NOT to do. Fine. Ok. I will create it. I go to Gitlab and chose the right template for the merge request. And now I get a whole bunch of new instructions and questions that I have to answer that the docs never mentioned. And it even mentions in that template that I should TRY to fix pipeline problems, or someone will help me (still going against official docs on their site). Since there was a bunch of questions I would have to look up to properly answer I did not have the time to do it right away... And I still have not done it. Its on a todo-list. tl;dr: F-Droid works okey but the whole process of submitting a new app for someone that has not gone through it before could be made way better with some updated and unified instructions. reply iggldiggl 9 hours agoparent> I started by reading their docs on how to add a new app. Did you stumble across https://f-droid.org/docs/Inclusion_How-To/, or did you miss that page somehow? Because there it mentions the Submission Queue as the simpler, if somewhat slower route to adding an app. For the submission queue, you just need to fill out the pertinent data in an issue ticket and then follow along with any further instructions you might subsequently receive. reply ap-andersson 6 hours agorootparentTo be honest I had forgot that it was an option. I zeroed in on the other alternative from the get go since I was the developer of the app and its very limited in its usefulness. Did not want to create work for others when I thought I could easily do the work :) reply greazy 7 hours agoparentprevThis is my experience with community driven open source projects and while its frustrating, what I like is that I can improve the process. Did you ever consider submitting an issue regarding the docs? reply ap-andersson 6 hours agorootparentTo be honest, not really. I wanted to first successfully get my app published before even thinking about that. I would need to explain WHAT to improve and HOW to improve it for it to be useful and that I cannot do until I am more familiar with the project. But it is a good point. reply daghamm 10 hours agoparentprevYou can run their pipeline locally with better observability. reply ggm 10 hours agoprevhttps://f-droid.org/2023/03/20/f-droid-board.html pleases me. Google store? Governance be damned. reply rekoil 10 hours agoprevI don't tinker much in the Android ecosystem, is there an F-Droid client available for Android TV devices? I've managed to install the smartphone version on my NVIDIA Shield TV, but it is more or less impossible to navigate using the remote control... reply nosrepa 1 hour agoparentCan you plug in a mouse and keyboard, if only temporarily? reply BizarroLand 22 minutes agorootparentIf you can't, you can probably still get a bluetooth mouse to work and navigate that way. There are lots of sub $10 bluetooth mini keyboard/mouse combos out there that can do the job even if they are generally crappy at it. reply oigursh 1 hour agoprevAnyone preferring a privacy respecting store should key an eye on https://accrescent.app/ reply allan_s 5 hours agoprevPeople have asked what app other are using and I was wondering \"which apps do you think are missing\" ? (it can be games or even very specific needs) as I think lot of people are like me here, we would like to hack on some \"weekend project\" but we lack creativity. reply aledue 4 hours agoparentI personally searched for and couldn't find an app that gathered various random generation tools, like the ones on random.org (integer sequences and sets, dice, list shuffle...) Other than that banking, transport, government apps are (almost) never on fdroid, but that's not something a weekend project can fix :) reply colordrops 12 hours agoprevJust wanna say that F-Droid is one of the most important projects out there for retaining some sanity and freedom on mobile devices. reply aszantu 8 hours agoprevI think it used to be able to download other people's cards. Last time I tried there were only language sets to download. reply Spunkie 13 hours agoprev> Supported iOS apps and progressive web apps (PWA) as packages that can be shipped via repositories. iOS apps on f-droid, huh? reply greazy 12 hours agoparentEuropeans can install alternative app stores https://support.apple.com/en-us/118110 reply rnaarten 6 hours agorootparentTrue, but only when you login with your Apple account. Which kind of defeats the purpose for some parts reply ubertaco 5 hours agoprev [3 more] [flagged] greenavocado 4 hours agoparentYou can bypass fdroid ideology by using custom repos reply bunabhucan 4 hours agoparentprev [–] Do chromecasts phone home with data on all nearby wifi devices? If they do then is that not reporting location data for those devices, albeit static monitoring? \"mehtacos phone was on ubertacos wlan\" is location data. reply Consider applying for YC's Spring batch! Applications are open till Feb 11. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "In 2024, F-Droid made notable advancements in decentralizing app distribution, enhancing its infrastructure, and expanding its ecosystem, largely due to community support and funding. Significant achievements included infrastructure upgrades and the expansion of tools like Repomaker and Mobifree, with over 7205 app updates and 402 new apps added. Looking forward to 2025, F-Droid plans to utilize an Open Technology Fund grant to further improve infrastructure and continue focusing on decentralizing app distribution and promoting open-source alternatives."
    ],
    "commentSummary": [
      "F-Droid users are advocating for a download counter to identify popular and reliable apps, as the platform lacks a rating system.",
      "Challenges include navigation difficulties due to incomplete or outdated apps, prompting suggestions for better search filters and UI updates.",
      "Despite these challenges, F-Droid is valued for its open-source apps, with discussions on app submission processes and the potential integration of progressive web apps (PWAs)."
    ],
    "points": 302,
    "commentCount": 88,
    "retryCount": 0,
    "time": 1737584287
  },
  {
    "id": 42799245,
    "title": "Understanding gRPC, OpenAPI and REST and when to use them in API design (2020)",
    "originLink": "https://cloud.google.com/blog/products/api-management/understanding-grpc-openapi-and-rest-and-when-to-use-them",
    "originBody": "API Management gRPC vs REST: Understanding gRPC, OpenAPI and REST and when to use them in API design April 10, 2020 Martin Nally Software Developer and API designer, Apigee As most software developers no doubt know, there are two primary models for API design: RPC and REST. Regardless of model, most modern APIs are implemented by mapping them in one way or another to the same HTTP protocol. It has also become common for RPC API designs to adopt one or two ideas from HTTP while staying within the RPC model, which has increased the range of choices that an API designer faces. This post tries to explain the choices, and give guidance on how to choose between them. gRPC is a technology for implementing RPC APIs that uses HTTP 2.0 as its underlying transport protocol. You might expect that gRPC and HTTP would be mutually exclusive, since they are based on opposite conceptual models. gRPC is based on the Remote Procedure Call (RPC) model, in which the addressable entities are procedures, and the data is hidden behind the procedures. HTTP works the inverse way. In HTTP, the addressable entities are “data entities” (called “resources” in the HTTP specifications), and the behaviors are hidden behind the data—the behavior of the system results from creating, modifying, and deleting resources. In fact, many of the APIs created here at Google and elsewhere combine RPC with a few ideas from HTTP in an interesting way. These APIs adopt an entity-oriented model, as does HTTP, but are defined and implemented using gRPC, and the resulting APIs can be invoked using standard HTTP technologies. We will try to describe how this works, why it might be good for you, and where it might not. Let's first take a closer look at how HTTP is commonly used for APIs. The three primary ways to use HTTP for APIs Most public APIs and many private distributed APIs use HTTP as the transport, at least in part because organizations are accustomed to dealing with the security issues of allowing HTTP traffic on ports 80 and 443. In my opinion there are three significant and distinct approaches for building APIs that use HTTP. They are: REST gRPC (and Apache Thrift and others) OpenAPI (and its competitors) REST The least-commonly used API model is REST—only a small minority of APIs are designed this way, even though the word REST is used (or abused) more broadly. A signature characteristic of this style of API is that clients do not construct URLs from other information—they just use the URLs that are passed out by the server as-is. This is how the browser works—it does not construct the URLs it uses from piece parts, and it does not understand the website-specific formats of the URLs it uses; it just blindly follows the URLs that it finds in the current page received from the server, or that were bookmarked from previous pages or are entered by the user. The only parsing of a URL that a browser does is to extract the information required to send an HTTP request, and the only construction of URLs that a browser does is to form an absolute URL from relative and base URLs. If your API is a REST API, then your clients never have to understand the format of your URLs and those formats are not part of the API specification given to clients1. REST APIs can be very simple. Lots of additional technologies have been invented for use with REST APIs—for example JSON API, ODATA, HAL, Siren or JSON Hyper-Schema and others—but you don't need any of those to do REST well. gRPC A second model for using HTTP for APIs is illustrated by gRPC. gRPC uses HTTP/2 under the covers, but HTTP is not exposed to the API designer. gRPC-generated stubs and skeletons hide HTTP from the client and server too, so nobody has to worry how the RPC concepts are mapped to HTTP—they just have to learn gRPC. The way a client uses a gRPC API is by following these three steps: Decide which procedure to call Calculate the parameter values to use (if any) Use a code-generated stub to make the call, passing the parameter values OpenAPI Probably the most popular way of designing RPC APIs that use HTTP is to use specification languages like OpenAPI (formerly known as the Swagger specification). A signature characteristic of the OpenAPI style of API is that clients use the API by constructing URLs from other information. The way a client uses an OpenAPI API is by following these three steps: Decide which OpenAPI URL path template to use Calculate the parameter values to use (if any) Plug the parameter values into the URL path template and send an HTTP request. It should be immediately obvious that an API that works this way is not a REST API. The OpenAPI method of using HTTP requires clients to have detailed knowledge of the format of the URLs they use in requests and to construct URLs that conform to that format from other information. This is the opposite of the way a REST API works, where clients are completely blind to the formats of the URLs they use, and never have to construct them. The model supported by OpenAPI is very popular and successful and is one of the most important options available to API designers—the fact that the OpenAPI model is not REST does not diminish its usefulness or importance. API Management APIs 101: Everything you need to know about API design API design best practices maximize value and efficiency. By Chris Latimer • 4-minute read The second observation you will probably make is that the client model for using an OpenAPI API is very similar to the client model for using a gRPC API. Where a gRPC client chooses a procedure to call, an OpenAPI client chooses a URL path template to use. gRPC and OpenAPI clients both calculate parameter values. Where a gRPC client uses a stub procedure to combine the parameters with the procedure signature and make the call, an OpenAPI client inserts the parameter values into the URL path template and issues an HTTP request. The detail is different but the overall model is very similar. OpenAPI also includes tools that will optionally generate a client stub procedure in the client programming language that hides these details, making the client experience of the two even more similar. One way to explain the close similarity between the client models of gRPC and OpenAPI is to consider OpenAPI to be a language for specifying a classic RPC API with a custom mapping to HTTP requests. If you accept that idea, then gRPC and OpenAPI are both RPC interface definition languages (IDLs), with the essential difference between them being that OpenAPI exposes the details of the underlying HTTP transport to the client and allows the API designer to control the mapping, while gRPC hides all the HTTP details using a predefined mapping. Even if you don't accept the idea that the fundamental API model used by OpenAPI is just good old-fashioned RPC, it is hard to deny that there are some obvious parallels between the two, and they are both distinct from REST. Either way, I think the parallels help motivate the more detailed comparison that follows. Each of these approaches has some benefits and drawbacks—we'll explore all three and leave you with some thoughts on how to decide which one is best for your application. Look how easy and obvious RPC is! Here is an example from a popular blog post that extols the virtues of RPC (we'll come back to this blog post later): Loading... The blogger says that many people find it easy to define an RPC API for this problem, but struggle to figure out how to solve the same problem using HTTP, wasting a lot of time and energy without realizing any benefit to their project. I agree. One reason is that designing an API on top of HTTP is a skill that has to be learned, and there are many options. Using the REST model is also easy and obvious Since we have designed many APIs using REST, it seems just as obvious to us how to express this example in REST. Here is what I would do: Loading... The username, contact_email, password, account_URL, and other bits of data supplied by the client are just simple JSON name/value pairs in the request body. I left out the details of what is in the headers, and how the results are returned, because it's all explained in the HTTP specifications—there aren't really choices or decisions to make. All the identifiers passed between the client and the server in both directions are URLs—there are no identifiers in the API that are not also URLs. Whenever one resource includes a reference to another, that reference is expressed using the other resource's URL. This technique is called hypertext, or hypermedia—if your API does not use URLs this way, it is not using the REST model, since hypertext linking is a signature feature that distinguishes REST from other models2. RPC APIs also express relationships between entities by including the identifiers of one entity in another entity, but those identifiers are not URLs that can be used directly without requiring additional information. Advantages of REST The claimed advantages of REST are basically those of the world wide web itself, like stability, uniformity, and universality. They are documented elsewhere, and REST is anyway a minority interest, so we won't dwell on them too much here. An exception is the entity-orientation inherent in the HTTP/REST model. This feature is of special interest because it has been widely discussed and adopted by proponents of non-REST models like gRPC and OpenAPI. In my experience, entity-oriented models are simpler, more regular, easier to understand, and more stable over time than simple RPC models. RPC APIs tend to grow organically as one procedure after another is added, each one implementing an action that the system can perform. An entity-oriented model provides an overall organization for the system's behaviors. For example, we are all familiar with the entity model of online shopping, with its products, carts, orders, accounts, and so on. If that capability were expressed using only RPC procedures, it would result in a long, unstructured list of procedures for browsing catalogs of products, adding them to carts, checking out, tracking deliveries, and returning products. The list quickly becomes overwhelming, and it's difficult to achieve coherence between the procedure definitions. One way to bring structure and order to the list is to model all the behaviors using a standard set of procedures for each entity type. HTTP is inherently entity-oriented, but you can also add entity-orientation to RPC, as discussed later. Grouping procedures by entity type is also one of the key ideas of object-oriented languages. How you use OpenAPI In OpenAPI, you define things called paths. An OpenAPI path looks like this in YAML: Loading... APIs that define paths like these expose the values of {petId} to the client in various places in the API, and require the client to use an appropriate path definition in order to convert the {petId} value (and other values) into a URL that can be used in HTTP requests. Expressing and using IDs this way is an alternative to the hypertext link that is a signature idea of REST. OpenAPI calls the variables in these paths \"parameters\" and the combination of a path and an HTTP method is called an operation—similar terminology to RPC systems. OpenAPI's use of URL templates with parameters can be viewed as a way to express RPC-like concepts with custom mappings to HTTP. OpenAPI advantages and disadvantages In my opinion, OpenAPI has two fundamental characteristics that account for its success. The first is that the OpenAPI model is similar to the traditional RPC model with which most programmers are familiar and comfortable. The model also fits well with the concepts of the programming languages they use. The second reason is that it allows programmers to define a custom mapping of those RPC concepts to HTTP requests. This second characteristic brings with it both benefits and problems. The primary benefit is that clients can access the API using only standard HTTP technologies. This is especially important for public APIs because it means that the API is accessible from almost all programming languages and environments without requiring the client to adopt any additional technology. A disadvantage is that it can require significant effort to design the HTTP details—witness all the guidance on the web on what you should and shouldn't do, much of it contradictory—and further effort by the consumer to learn it. When might gRPC be a better option than OpenAPI? Your design challenge for APIs described with OpenAPI is to define a combination of URL paths and HTTP methods to represent your \"operations\" and their \"parameters.\" This can be tricky work, because there are lots of options. It is not clear that it is a good use of time and energy for most projects. Frustrations resulting from this approach are described with passion in the blog post comparing SOAP to REST by Pascal Chambon that we mentioned before, which also supplied the RPC example we opened with. Chambon's post contains some misinformation and misunderstanding, and most of the reaction to his post focused on correcting that, but Chambon's mistakes actually add support to his main point, which is that designing your own mapping of RPC-like concepts onto HTTP is fairly complicated and difficult. Most of the advice that was offered in response to Chambon's blog post promoted REST as an alternative to the RPC-like model that Chambon and most other people are familiar with. This is certainly an option—the simple REST example that we described at the beginning of this post is a minimalist's take on how exactly to do that. Another option for Chambon is to keep his basic RPC model, but use gRPC instead of OpenAPI to express it. This avoids the complexity of defining a custom mapping of the API to HTTP. The RPC model has shown much more enduring popularity than any alternative, and if API designers are going to use an RPC-like model anyway, then they should weigh all the available technologies for doing that. gRPC benefits gRPC expresses an RPC API in an interface description language (IDL) that benefits from a long tradition of RPC IDLs that includes DCE IDL, Corba IDL, and many others. gRPC's IDL provides a simpler and more direct way of defining remote procedures than OpenAPI's approach of using URL paths, their parameters, and the HTTP methods that are used with them. gRPC uses HTTP/2 under the covers, but gRPC does not expose any of HTTP/2 to the API designer or API user. gRPC has already made all the decisions on how to layer the RPC model on top of HTTP so you don't have to—those decisions are built into the gRPC software and generated code. This makes life simpler for API designers and clients. By contrast, OpenAPI requires API designers to specify the details of how the RPC model is expressed on top of HTTP for their specific API, and the client of the API has to learn that detail. An important advantage of the OpenAPI approach is that it lets API clients use standard HTTP tools and technologies, which for many API designers justifies the effort. Regardless of how your API uses HTTP, it is likely that you will want to create client-side programming libraries in various languages for programmers to use. These programming libraries will take the form of procedures (possibly called functions or methods, depending on the programming language). One of gRPC's most attractive characteristics is that it is very good at generating client-side programming libraries that are intuitive for programmers to use and execute efficiently. OpenAPI can also generate client-side programming libraries, but I find the gRPC version simpler and more obvious, probably because its IDL only has to express RPC concepts and does not have to simultaneously describe a mapping of those concepts to HTTP. APIs specified in gRPC are also simple to implement on the server side. Because of the frameworks, libraries, and code-generation that gRPC provides, it may be simpler to create the server implementation of a gRPC method than to write a standard HTTP request handler that parses incoming requests and calls the right implementation functions, despite the many frameworks that aim to help with that. Another characteristic of gRPC is good performance. gRPC uses a binary payload that is efficient to create and to parse, and it exploits HTTP/2 for efficient management of connections. Of course, you can also use binary payloads and HTTP/2 directly without using gRPC, but this requires you and your clients to master more technology. gRPC also avoids the problem that even the best HTTP-based APIs don't implement the whole HTTP protocol, which requires API providers and clients to figure out how to specify and learn which subset of HTTP is supported by a particular API. This is a problem for both REST and OpenAPI APIs. gRPC avoids this problem by requiring the client and the server to both adopt special software that implements the complete gRPC protocol. We hope gRPC succeeds in keeping that protocol stable for at least 25 years as HTTP has done, so that clients don't break when servers are upgraded and vice versa. How do you combine the entity-oriented model with RPC? Regardless of whether you are using gRPC or OpenAPI, the trick to using RPC in an entity-oriented way is to constrain the RPC method definitions to only those that map easily to the standard entity operations (Create, Retrieve, Update and Delete, often called CRUD3, plus List) for each resource type. To use RPC in an entity-oriented style, you reverse the usual RPC thought process—instead of starting with procedure definitions, you start by defining your resource types, and then make RPC method definitions corresponding to the common entity operations on those types plus any additional operations you find necessary. Using RPC in an entity-oriented style depends on teaching people a constrained usage pattern. In practice we see that APIs that are designed this way are sometimes a blend of entity-oriented and procedure-oriented concepts, which undermines some of the benefits. So what are the downsides of gRPC? Every technology has downsides and limitations. We’ve already discussed some of OpenAPI's. A popular feature of HTTP APIs is that clients can use them and servers can implement them using only general-purpose and widely available technologies. API calls can easily be made by simply typing URLs into a browser, or issuing cURL commands in a terminal window or in a bash script. Programmers can access or implement an HTTP API using no more technology than a basic HTTP library. In contrast, gRPC requires special software on both the client and the server. gRPC-generated code has to be incorporated into client and server build processes—this may be onerous to some, especially those who are used to working in dynamic languages like Javascript or Python where the build process, at least on development machines, may be non-existent. The Google Cloud Endpoints product enables gRPC APIs to be accessed via HTTP and JSON without special software, which restores many options for clients, but not everyone wants to or is able to use Cloud Endpoints or find or build an equivalent. It’s simple to write a bot that crawls the entirety of a REST API without metadata4, similarly to the way a browser or a web bot can crawl the entire HTML web. You can’t do this with an RPC-style API, regardless of whether it’s described using gRPC or OpenAPI, because RPC gives each entity type a different API that requires custom software or metadata to use it. In practice it usually isn't critical to be able to write general-purpose API clients, although it can be useful. HTTP APIs are often proxied to add security features, perform input validation, map data formats, and solve many other problems. This typically requires adding, removing or modifying headers, and parsing and even modifying the body. Proxies use a combination of standard and custom headers to achieve this. These features are commonly implemented using products like Apigee Edge that do not require traditional programming skills or the kind of software development environments that can easily integrate gRPC. I think it would be much harder to do this sort of proxying for gRPC, and I am not aware of it being commonly done. Web API Design: The Missing Link Learn about API design patterns, principles, and best practices used by some of the world’s leading API teams in this ebook. Download Using an entity-oriented approach with gRPC is mostly useful for new-builds—you won't find it easy to retrofit to an existing RPC API. gRPC does not define a standard mechanism to prevent loss of data when two clients try to update the same resource at the same time, so if you use gRPC, you will likely have to invent your own. HTTP defines standard Etag and If-Match headers for this purpose—most of the HTTP APIs we design use these headers. Nor does gRPC define a mechanism for making partial updates, so you will likely have to invent your own. HTTP defines a method—PATCH—for partial updates but does not say what a patch should look like or how to apply it. There are two additional IETF standards that fill this gap for JSON: JSON merge patch and JSON patch. The first is simpler to use but does not handle all cases, particularly updates of arrays; the second handles more cases but is more complex to use. Most of the recent HTTP APIs I have built implement both standards and let the client choose; the Kubernetes API works this way too. Conclusion There are a few APIs that use the same REST hypertext model used by the HTML web. They aim to inherit the core qualities of the HTML web, like stability, uniformity, and universality. If you already know how to design APIs this way, or are motivated to learn, then this is a fine approach. This is my own preference. APIs that are described with OpenAPI are based on concepts analogous to those of RPC, but with a custom mapping to HTTP. This approach allows clients to access the resulting API using only commonly-available HTTP technologies, but it also adds additional design choices to these APIs, which can make them more difficult to design and build and more difficult to learn. If you are considering using OpenAPI for an API, you should also consider the option of designing and implementing it using gRPC. The fundamental API models of the two are comparable and gRPC avoids the need to invent your own mapping onto HTTP. Regardless of whether you use gRPC or OpenAPI for your API, you can obtain some, but not all, of the benefits of a REST API if you organize the API in an entity-oriented style, standardize the names of your procedures (for example by sticking to the verbs create, retrieve, update, delete and list), and impose other naming conventions. gRPC will bring some other benefits of its own. Using gRPC is especially attractive if one of the following is true: You can use a product like Cloud Endpoints so that your clients are not forced to adopt gRPC technologies just because you did. The API is internal, where you control the technology choices of all the clients as well as the server. If you adopt gRPC in place of OpenAPI or REST, you should at least be aware of the much more limited opportunity to augment or remediate the API's behaviors in proxies, especially those implemented using API management tools like Apigee Edge or its competitors. Depending on how and where you intend to use gRPC, this may or may not be a problem. As with most design challenges, there are many factors to consider and tradeoffs to be made. Hopefully this discussion has helped explain some of the ways in which HTTP and RPC-style APIs match up against one another. Special thanks to Nandan Sridhar and Marsh Gardiner for their contributions to this post. 1. Some REST APIs allow clients to append a query to a base URL, in which case clients need to understand the query syntax supported by the server in the query portion of the URL, although they don't need to know the format of the rest of the URL. Some REST API designers allow queries to be encoded in URL paths, in which case their query URLs start to look like the OpenAPI-style URLs discussed below. 2. Some REST commentators say that in order to claim compliance with the REST model you have to also implement something akin to HTML forms in JSON, but almost all would agree that if hypertext links are not a prominent feature of the API then it is not REST. 3. Even in strictly entity-oriented APIs we sometimes come across the need for a fifth operation that we think of as \"translate\" or \"convert\". Translate takes in one entity and produces another without creating a persistent resource. HTTP doesn't have a special method for this operation, so we have to use POST for both \"create\" and \"translate\". Sometimes we also use POST for retrieve to get around limitations on URL length, usually for URLs that include queries. 4. With a small amount of extra work on the server you can let the browser itself crawl a REST API. API Management API design 101: Links to our most popular posts Find our most requested blog posts on API design in one location to read now or bookmark for later. By The Google Cloud content marketing team • 1-minute read Posted in API Management Application Development Apigee Related articles API Management Google Cloud Apigee named a Leader in the 2024 Gartner® Magic Quadrant™ for API Management By Roderick Griner • 4-minute read API Management From gRPC to RESTful APIs: Expose your gRPC services to the REST of the world By Daniel Strebel • 6-minute read API Management Apigee and the Interoperability Model (ModI) for the Italian Public Administration By Filippo Lambiente • 8-minute read Application Development Faster API development with the Cloud Code plugin for API management By Emanuel Burgess • 4-minute read",
    "commentLink": "https://news.ycombinator.com/item?id=42799245",
    "commentBody": "Understanding gRPC, OpenAPI and REST and when to use them in API design (2020) (cloud.google.com)291 points by hui-zheng 18 hours agohidepastfavorite236 comments jdwyah 15 hours agoIf I could go back in time I would stop myself from ever learning about gRPC. I was so into the dream, but years later way too many headaches. Don’t do it to yourself. Saying gRPC hides the internals is a joke. You’ll get internals all right, when you’re blasting debug logging trying to figure out what the f is going on causing 1/10 requests to fail and fine tuning 10-20 different poorly named and timeout / retry settings. Hours lost fighting with maven plugins. Hours lost debugging weird deadline exceeded. Hours lost with LBs that don’t like the esoteric http2. Firewall pain meaning we had to use Standard api anyway. Crappy docs. Hours lost trying to get error messages that don’t suck into observability. I wish I’d never heard of it. reply stickfigure 14 hours agoparentIMO the problem with gRPC isn't the protocol or the protobufs, but the terrible tooling at least on the Java end. It generates shit code with awful developer ergonomics. When you run the protobuf builder... * The client stub is a concrete final class. It can't be mocked in tests. * When implementing a server, you have to extend a concrete class (not an interface). * The server method has an async method signature. Screws up AOP-oriented behavior like `@Transactional` * No support for exceptions. * Immutable value classes yes, but you have to construct them with builders. The net result is that if you want to use gRPC in your SOA, you have to write a lot of plumbing to hide the gRPC noise and get clean, testable code. There's no reason it has to be this way, but it is that way, and I don't want to write my own protobuf compiler. Thrift's rpc compiler has many of the same problems, plus some others. Sigh. reply bjackman 9 hours agorootparent> The client stub is a concrete final class. It can't be mocked in tests. I believe this is deliberate, you are supposed to substitute a fake server. This is superior in theory since you have much less scope to get error reporting wrong (since errors actually go across a gRPC transport during the test). Of course.. at least with C++, there is no well-lit-path for actually _doing_ that, which seems bonkers. In my case I had to write a bunch of undocumented boilerplate to make this happen. IIUC for Stubby (Google's internal precursor to gRPC) those kinda bizarre ergonomic issues are solved. reply Degorath 7 hours agorootparentStubby calls (at least in Java) just use something called a GenericServiceMocker which is akin to a more specialised mockito. reply tbarbugli 2 hours agorootparentprevIn my experience, only Swift has a generator that produces good-quality code. Ironically, it’s developed by Apple. reply rkagerer 10 hours agorootparentprevAny alternatives that take a similar philosophy but get the tooling right? reply stickfigure 4 hours agorootparentDepends what you mean by \"similar philosophy\". We (largeish household name though not thought of as a tech company) went through a pretty extensive review of the options late last year and standardized on this for our internal serviceservice communication: https://github.com/stickfigure/trivet It's the dumbest RPC protocol you can imagine, less than 400 lines of code. You publish a vanilla Java interface in a jar; you annotate the implementation with `@Remote` and make sure it's in the spring context. Other than a tiny bit of setup, that's pretty much it. The main downside is that it's based on Java serialization. For us this is fine, we already use serialization heavily and it's a known quantity for our team. Performance is \"good enough\". But you can't use this to expose public services or talk to nonjava services. For that we use plain old REST endpoints. The main upsides are developer ergonomics, easy testability, spring metrics/spans pass through remote calls transparently, and exceptions (with complete stacktraces) propagate to clients (even through multiple layers of remote calls). I wrote it some time ago. It's not for everyone. But when our team (well, the team making this decision for the company) looked at the proof-of-concepts, this is what everyone preferred. reply crabbone 5 hours agorootparentprevProtobuf is an atrocious protocol. Whatever other problems gRPC has may be worse, but Protobuf doesn't make anything better that's for sure. The reason to use it may be that you are required to by the side you cannot control, or this is the only thing you know. Otherwise it's a disaster. It's really upsetting that a lot of things used in this domain are the first attempt by the author to make something of sorts. So many easily preventable disasters exist in this protocol for no reason. reply morganherlocker 35 minutes agorootparentAgree. As an example, this proto generates 584 lines of C++, links to 173k lines of dependencies, and generates a 21Kb object file, even before adding grpc: syntax = \"proto3\"; message LonLat { float lon = 1; float lat = 2; } Looking through the generated headers, they are full of autogenerated slop with loads of dependencies, all to read a struct with 2 primitive fields. For a real monorepo, this adds up quickly. reply cyberax 20 minutes agorootparentThis is because protobuf supports full run-time reflection and compact serialization (protobuf binary objects are not self-describing), and this requires a bit of infrastructure. This is a large chunk of code, but it is a one-time tax. The incremental size from this particular message is insignficant. reply bellgrove 3 hours agorootparentprevCan you elaborate? reply dtquad 14 hours agoparentprevYour problems has more to do with some implementations than the grpc/protobuf specs themselves. The modern .NET and C# experience with gRPC is so good that Microsoft has sunset its legacy RPC tech like WCF and gone all in on gRPC. reply zigzag312 9 hours agorootparentI would really like if proto to C# compiler would create nullable members. Hasers IMO give poor DX and are error prone. reply junto 13 hours agorootparentprevAgreed. The newest versions of .NET are now chef’s kiss and so damn fast. reply bborud 10 hours agoparentprevSince you mention Maven I'm going to make the assumption that you are using Java. I haven't used Java in quite a while. The last 8 years or so I've been programming Go. Your experience of gRPC seems to be very different from mine. How much of the difference in experience do you think might be down to Java and how much is down to gRPC as a technology? reply piva00 8 hours agorootparentIt's not Java itself, it's design decisions on the tooling that Google provides for Java, mostly the protobuf-gen plugin. At my company we found some workarounds to the issues brought up on GP but it's annoying the tooling is a bit subpar. reply bborud 6 hours agorootparentHave you tried the buf.build tools? Especially the remote code generation and package generation may make life easier for you. a couple of links https://buf.build/protocolbuffers/java?version=v29.3 https://buf.build/docs/bsr/generated-sdks/maven reply hedora 14 hours agoparentprevThe biggest project I’ve used it with was in Java. Validating the output of the bindings protoc generated was more verbose and error prone than hand serializing data would have been. The wire protocol is not type safe. It has type tags, but they reuse the same tags for multiple datatypes. Also, zig-zag integer encoding is slow. Anyway, it’s a terrible RPC library. Flatbuffer is the only one that I’ve encountered that is worse. reply TeeWEE 14 hours agorootparentWhat do you mean with validating the bindings? GRPC is type safe. You don’t have to think about that part anymore. But as the article mentions OpenAPI is also an RPC library with stub generation. Manual parsing of the json is imho really Oldskool. But it depends on your use case. That’s the whole point: it depends. reply matrix87 13 hours agorootparentprev> The wire protocol is not type safe. It has type tags, but they reuse the same tags for multiple datatypes. When is this ever an issue in practice? Why would the client read int32 but then all of a sudden decide to read uint32? reply divan 11 hours agoparentprevI use gRPC with Go+Dart stack for years and never experienced these issues. Is it something specific to Java+gRPC? reply robertlagrant 8 hours agorootparentGo and Dart are probably the languages most likely to work well with gRPC, given their provenance. reply throwaway127482 3 hours agorootparentGoogle has massive amounts of code written in Java so one would think the Java tooling would be excellent as well. reply drtse4 9 hours agoparentprevAs someone that used it for years with the same problems he describes... spot on analysis, the library does too much for you (e.g. reconnection handling) and handling even basic recovery is a bit a nuisance for newbies. And yes, when you get random failures good luck figuring out that maybe is just a router in the middle of the path dropping packets because their http2 filtering is full of bugs. I like a lot of things about it and used it extensively instead of the inferior REST alternative, but I recommend to be aware of the limitations/nuisances. Not all issues will be simply solved looking at stackoverflow. reply azemetre 14 hours agoparentprevWhat would you recommend doing instead? reply doctorpangloss 13 hours agorootparentDo you need bidirectional streams? If so, you should write a bespoke protocol, on top of UDP, TCP or websockets. If you don't, use GraphQL. reply nithril 11 hours agorootparent\"Write a protocol and GraphQL\", god damn it escalates quickly. Fortunately, there are intermediate steps. reply grumbelbart2 7 hours agorootparentAny suggestions for a good RPC library? reply masterj 3 hours agorootparentI have had a really good experience with https://connectrpc.com/ so far. Buf is doing some interesting things in this space https://buf.build/docs/ecosystem/ reply galangalalgol 5 hours agorootparentprevWhat about songle directional streams? Graphql streams aren't widely supported yet are they? Graphql also strikes me as a weird alternative to protobufs as the latter works so hard for performance with binary payloads, and graphql is typically human readable bloaty text. And they aren't really queries, you can just choose to ignore parts of the return for a rpc. reply oppositelock 14 hours agoprevI've been building API's for a long time, using gRPC, and HTTP/REST (we'll not go into CORBA or DCOM, because I'll cry). To that end, I've open sourced a Go library for generating your clients and servers from OpenAPI specs (https://github.com/oapi-codegen/oapi-codegen). I disagree with the way this article breaks down the options. There is no difference between OpenAPI and REST, it's a strange distinction. OpenAPI is a way of documenting the behavior of your HTTP API. You can express a RESTful API using OpenAPI, or something completely random, it's up to you. The purpose of OpenAPI is to have a schema language to describe your API for tooling to interpret, so in concept, it's similar to Protocol Buffer files that are used to specify gRPC protocols. gRPC is an RPC mechanism for sending protos back and forth. When Google open sourced protobufs, they didn't opensource the RPC layer, called \"stubby\" at Google, which made protos really great. gRPC is not stubby, and it's not as awesome, but it's still very efficient at transport, and fairly easy too extend and hook into. The problem is, it's a self-contained ecosystem that isn't as robust as mainstream HTTP libraries, which give you all kinds of useful middleware like logging or auth. You'll be implementing lots of these yourself with gRPC, particularly if you are making RPC calls across services implemented in different languages. To me, the problem with gRPC is proto files. Every client must be built against .proto files compatible with the server; it's not a discoverable protocol. With an HTTP API, you can make calls to it via curl or your own code without having the OpenAPI description, so it's a \"softer\" binding. This fact alone makes it easier to work with and debug. reply mandevil 14 hours agoparentThere is a distinction between (proper) REST and what this blog calls \"OpenAPI\". But the thing is, almost no one builds a true, proper REST API. In practice, everyone uses the OpenAPI approach. The way that REST was defined by Roy Fielding in his 2000 Ph.D dissertation (\"Architectural Styles and the Design of Network-based Software Architectures\") it was supposed to allow a web-like exploring of all available resources. You would GET the root URL, and the 200 OK Response would provide a set of links that would allow you to traverse all available resources provided by the API (it was allowed to be hierarchical but everything had to be accessible somewhere in the link tree). This was supposed to allow discoverability. In practice, everywhere I've ever worked over the past two decades has just used POST resource_name/resource_id/sub_resource/sub_resource_id/mutatation_type or PUT resource_name/resource_id/sub_resource/sub_resource_id depending on how that company handled the idempotency issues that PUT creates with all of those being magic URL's assembled by the client with knowledge of the structure (often defined in something like Swagger/OpenAPI), lacking the link-traversal from root that was a hallmark of Fielding's original work. Pedants (which let's face it, most of us are) will often describe what is done in practice as \"RESTful\" rather than \"REST\" just to acknowledge that they are not implementing Fielding's definition of REST. reply nicholasjarnold 1 hour agorootparent> Pedants (which let's face it, most of us are) will often describe what is done in practice as \"RESTful\" rather than \"REST\" just to acknowledge that they are not implementing Fielding's definition of REST. Yes, exactly. I've never actually worked with any group whom had actually implemented full REST. When working with teams on public interface definitions I've personally tended to use the so-called Richardson's Maturity Model[0] and advocated for what it calls 'Level 2', which is what I think most of us find rather canonical and principal of least surprise regarding a RESTful interface. [0] https://en.wikipedia.org/wiki/Richardson_Maturity_Model reply bborud 9 hours agorootparentprevI tend to prefer RESTish rather than RESTful since RESTful almost suggests attempting to implement Fielding's ideas but not quite getting there. I think the subset of approaches that try and fail to implement Fielding's ideas is an order of magnitude (or two) smaller than those who go for something that is superficially similar, but has nothing to do with HATEOAS :-). REST is an interesting idea, but I don't think it is a practical idea. It is too hard to design tools and libraries that helps/encourages/forces the user implement HATEOAS sensibly, easily and consistently. reply physicles 14 hours agoparentprev> There is no difference between OpenAPI and REST, it's a strange distinction. That threw me off too. What the article calls REST, I understand to be closer to HATEOAS. > I've open sourced a Go library for generating your clients and servers from OpenAPI specs As a maintainer of a couple pretty substantial APIs with internal and external clients, I'm really struggling to understand the workflow that starts with generating code from OpenAPI specs. Once you've filled in all those generated stubs, how can you then iterate on the API spec? The tooling will just give you more stubs that you have to manually merge in, and it'll get harder and harder to find the relevant updates as the API grows. This is why I created an abomination that uses go/ast and friends to generate the OpenAPI spec from the code. It's not perfect, but it's a 95% solution that works with both Echo and Gin. So when we need to stand up a new endpoint and allow the front end to start coding against it ASAP, the workflow looks like this: 1. In a feature branch, define the request and response structs, and write an empty handler that parses parameters and returns an empty response. 2. Generate the docs and send them to the front end dev. Now, most devs never have to think about how to express their API in OpenAPI. And the docs will always be perfectly in sync with the code. reply plorkyeran 14 hours agorootparentHATEOAS is just REST as originally envisioned but accepting that the REST name has come to be attached to something different. reply Cthulhu_ 7 hours agorootparentprev> Once you've filled in all those generated stubs, how can you then iterate on the API spec? The tooling will just give you more stubs that you have to manually merge in, and it'll get harder and harder to find the relevant updates as the API grows. This is why I have never used generators to generate the API clients, only the models. Consuming a HTTP based API is just a single line function nowadays in web world, if you use e.g. react / tanstack query or write some simple utilities. The generaged clients are almost never good enough. That said, replacing the generator templates is an option in some of the generators, I've used the official openapi generator for a while which has many different generators, but I don't know if I'd recommend it because the generation is split between Java code and templates. reply ak217 13 hours agorootparentprevIn my part of the industry, a rite of passage is coming up with one's own homegrown data pipeline workflow manager/DAG execution engine. In the OpenAPI world, the equivalent must be writing one's own OpenAPI spec generator that scans an annotated server codebase, probably bundled with a client codegen tool as well. I know I've written one (mine too was a proper abomination) and it sounds like so have a few others in this thread. reply foobarian 1 hour agorootparent> In the OpenAPI world, the equivalent must be writing one's own OpenAPI spec generator Close, it's writing custom client and server codegen that actually have working support for oneOf polymorphism and whatever other weird home-grown extensions there are. reply jpc0 11 hours agorootparentprev> This is why I created an abomination that uses go/ast and friends to generate the OpenAPI spec from the code OpenAPI is a spec not documentation. Write the spec first then generate the code from the spec. You are doing it backwards, at least in my opinion. reply mdaniel 3 hours agorootparentThat's conceptually true, and yet if the hundreds of code generators don't support Your Favorite OAPI Feature ™ then you're stuck, whereas the opposite is that unless your framework is braindead it's going to at least support some mapping from your host language down to the OAPI spec. I doubt very seriously it's pretty, and my life experience is that it will definitely not be bright enough to have #/component reuse, but it's also probably closer to 30 seconds to run $(go generate something) than to launch an OAPI editor and now you have a 2nd job I'd love an OAPI compliance badge (actually what I'm probably complaining about is the tooling's support for JSON Schema) so one could readily know which tools to avoid because they were conceived in a hackathon and worked for that purpose but that I should avoid them for real work reply talideon 6 hours agorootparentprevI'm scratching my head here. HATEOAS is the core of REST. Without it and the uniform interface principle, you're not doing REST. \"REST\" without it is charitably described as \"RESTish\", though I prefer the term \"HTTP API\". OpenAPI only exists because it turns out that developers have a very weak grasp on hypertext and indirection, but if you reframe things in a more familiar RPC-ish manner, they can understand it better as they can latch onto something they already understand: procedure calls. But it's not REST. reply mkleczek 13 hours agorootparentprev> This is why I created an abomination that uses go/ast and friends to generate the OpenAPI spec from the code. This is against \"interface first\" principle and couples clients of your API to its implementation. That might be OK if the only consumer of the API is your own application as in that case API is really just an internal implementation detail. But even then once you have to support multiple versions of your own client it becomes difficult not to break them. reply physicles 12 hours agorootparentI don't see why it couples clients to the implementation. Effectively, there's no difference between writing the code first and updating the OpenAPI spec, and updating the spec first and then doing some sort of code gen to update the implementation. The end state of the world is the same. In either case, modifications to the spec will be scrutinized to make sure there are no breaking changes. reply Cthulhu_ 6 hours agorootparentYeah this is the way, I mean if the spec already exists it makes sense to go spec-first. I went spec-first last time I built an API because I find most generators to be imperfect or lacking features; going spec-first ensured that the spec was correct at least, and the implementations could do the workarounds (e.g. type conversions in Go) where necessary. That is, generate spec from code and your spec is limited to what can be expressed by the code, its annotations, and the support that the generator has. Most generators (to or from openapi) are imperfect and have to compromise on some features, which can lead to miscommunication between clients/servers. reply jitl 11 hours agorootparentprevOpenAPI spec being authored by a human or a machine, it can still be the same YAML at the end of the day, so why would one approach be more brittle / breaks your clients than the other? reply XorNot 12 hours agorootparentprevThe oapi-codegen tool the OP was put out (which I use) solves this by emitting an interface though. OpenAPI has the concept of operation names (which also have a standard pattern), so your generated code is simply implementing operation names. You can happily rewrite the entire spec and provided operation names are the same, everything will still map correctly which solves the coupling problem. reply arccy 6 hours agoparentprevThese days there's gprc reflection for discovery https://grpc.io/docs/guides/reflection/ reply cpursley 5 hours agoparentprevI'm piggybacking on the OpenAPI spec as well to generate a SQL-like query syntax along with generated types which makes working with any 3rd party API feel the same. What if you could query any ole' API like this?: Apipe.new(GitHun) |> from(\"search/repositories\") |> eq(:language, \"elixir\") |> order_by(:updated) |> limit(1) |> execute() This way, you don't have to know about all the available gRPC functions or the 3rd party API's RESTful quirks while retaining built-in documenting and having access to types. https://github.com/cpursley/apipe I'm considering building a TS adapter layer so that you can just drop this into your JS/TS project like you would with Supabase: const { data, error } = await apipe.from('search/repositories').eq('language', 'elixir').order_by('updated').limit(1) Where this would run through the Elixir proxy which would do the heavy lifting like async, handle rate limits, etc. reply cyberax 17 minutes agoparentprev> To me, the problem with gRPC is proto files. Every client must be built against .proto files compatible with the server; it's not a discoverable protocol. That's not quite true. You can build an OpenAPI description based on JSON serialization of Protobufs and serve it via Swagger. The gRPC itself also offers built-in reflection (and a nice grpcurl utility that uses it!). reply Pooge 2 hours agoparentprev> https://github.com/oapi-codegen/oapi-codegen I'm using it for a small personal project! Works very well. Thank you! reply TheGoodBarn 13 hours agoparentprevJust chiming in to say we use oapi-codegen everyday and it’s phenomenal. Migrated away from Swaggo > oapi during a large migration to be interface first for separating out large vertical slices and it’s been a godsend. reply happyweasel 8 hours agoparentprevBuggy/incomplete Openapi codegen for rust was a huge disappointment for me. At least with grpc some languages are first class citizens. Of course generated code has some uglyness. Kinda sad http2 traffic can be flaky due to bugs in network hardware. reply toprerules 16 hours agoprevAs someone who has worked at a few of the FAANGs, having thrift/grpc is a godsend for internal service routing, but a lot of the complexity is managed by teams building the libraries, creating the service discovery layers, doing the routing etc. But using an RPC protocol enables those things to happen on a much greater scale and speed than you could ever do with your typical JSON/REST service. I've also never seen a REST API that didn't leak verbs. If I need to build a backend service mesh or wire two local services together via an networked stream, I will always reach for grpc. That said, I absolutely would not use grpc for anything customer or web facing. RPC is powerful because it locks you into a lot of decisions and gives you \"the one way\". REST is far superior when you have many different clients with different technology stacks trying to use your service. reply jitl 11 hours agoparentFor a public API I wouldn’t do this, but for private APIs we just do POST /api/doThingy with a JSON body, easy peasy RPC anyone can participate in with the most basic HTTP client. Works great on every OS and in every browser, no fucking around with “what goes in the URL path” vs “what goes in query params” vs “what goes in the body”. You can even do this with gRPC if you’re using Buf or Connect one of the server thingies that try not to suck; they will accept JSON via HTTP happily. reply ryathal 3 hours agorootparentI'd argue just making everything POST is the correct way to do a public Api too. REST tricks you into endpoints no one really wants, or you break it anyway to support functionality needed. SOAP was heavy with it's request/respone, but it was absolutely correct that just sending everything as POST across the wire is easier to work with. reply porridgeraisin 33 minutes agorootparentYeah, I like doing this as well. And all the data goes in the request body. No query parameters. Especially when the primary intended client is an SPA, where the URL shown is decoupled with the API URL. Little bit of a memory jolt: I once built a (not for prod) backend in python as follows: write a list of functions, one for each RPC, in a file `functions.py` then write this generic function for flask: import server.functions as functions @server.post(\"/\") def api(method: str): data: Any = request.json if request.is_json else {} fn = lookup(functions, method) if fn is None: return {\"error\": \"Method not found.\"} return fn(data) And `lookup()` looks like: def lookup(module: ModuleType, method: str): md = module.__dict__ mn = module.__name__ is_present = method in md is_not_imported = md[method].__module__ == mn is_a_function = inspect.isfunction(md[method]) if is_present and is_not_imported and is_a_function: return md[method] return None So writing a new RPC is just writing a new function, and it all gets automatically wired up to `/api/function_name`. Quite nice. The other nice feature there was automatic \"docs\" generation, from the python docstring of the function. You see, in python you can dynamically read the docstring of an object. So, I wrote this: def get_docs(module: ModuleType): md = module.__dict__ mn = module.__name__ docs = \"\" for name in md: if not inspect.isfunction(md[name]) or md[name].__module__ != mn: continue docs += md[name].__doc__ + \"\" return docs[:-6] Gives a simple text documentation which I served at an endpoint. Of course you could also write the docstring in openapi yaml format and serve it that way too. Quite cursed overall, but hey, its python. One of the worst footguns here is that you could accidentally expose helper functions, so you have to be sure to not write those in the functions file :P reply pandemic_region 11 hours agorootparentprevThis. The amount of time lost debating correct rest semantics for a use case is staggering. reply porridgeraisin 31 minutes agorootparentYeah, when it matters in close to 0% of cases. Everyone reads the docs for everything anyways, any shared knowledge granting implicit meaning to things is very close to useless in practice with REST APIs. reply spelunker 3 hours agorootparentprevArguing the Right Way To Do REST was a favorite passtime amongst people at one of my previous jobs. Huge waste of time. reply Cthulhu_ 6 hours agoparentprevWhat about non-web client/server applications though? I'm thinking online games / MMOs that require much more realtime communications than REST does. I have no idea what is used now, socket connections with something on the line I suppose. reply rfw300 11 hours agoparentprevWhat do you mean by “leak verbs”? reply jon_richards 8 hours agorootparentNot OP, but https://cloud.google.com/blog/products/api-management/restfu... The problem is that clients generally have a bunch of verbs they need to do. You have to design your objects and permissions just right such that clients can do all their verbs without an attacker being able to PATCH \"payment_status\" from \"Requires Payment\" to \"Payment Confirmed\". RPC uses verbs, so that could just be the SubmitPayment RPC's job. In REST, the correct design would be to give permission to POST a \"Payment\" object and base \"payment_status\" on whether that has been done. reply robertlagrant 6 hours agorootparentThis is the most painful bit of REST for sure. reply bitzun 16 hours agoprevUnless you are doing bidirectional streaming (for which it seems pretty well suited, but I haven't used it, so it might be a fucking mess), grpc is usually a waste of time. Runtime transitive dependency hell, toolchain hell, and the teams inside Google that manage various implementations philosophically disagree on how basic features should work. Try exposing a grpc api to a team that doesn't use your language (particularly if they're using a language that isn't go, python or java, or is an old version of those.) Try exposing a grpc api to integrate with a cots product. Try exposing a grpc api to a browser. All will require a middleware layer. reply lordofgibbons 16 hours agoparentI've used grpc at multiple companies and teams within these companies, all of them 100-500ish engineering team size, and never had these dependency and tool chain issues. It was smooth sailing with grpc. reply hamandcheese 15 hours agorootparentI have worked full time at now two companies of that size making the dependency and tool chain problems not be a problem for all the normies. reply 9rx 20 minutes agoparentprev> Try exposing a grpc api to a team that doesn't use your language Because of poor HTTP/2 support in those languages? Otherwise, it's not much more than just a run of the mill \"Web API\", albeit with some standardization around things like routing and headers instead of the randomly made up ones you will find in a bespoke \"Look ma, I can send JSON with a web server\" API. That standardization should only make implementing a client easier. If HTTP/2 support is poor, then yeah, you will be in for a world of hurt. Which is also the browser problem with no major browser (and maybe no browser in existence) ever ending up supporting HTTP/2 in full. reply drtse4 9 hours agoparentprevIn my opinion, you shouldn't expose it to a browser, it's not what is good at, build something custom that converts to json. Like using REST to talk between backend services, makes no sense using a human readable protocol/api especially if there are performance requirements (not a call every now and then with a small amount of data returned). reply 9rx 9 minutes agorootparentTo be fair, it was intended to be for browsers. But it was designed alongside the HTTP/2 spec, before browsers added HTTP/2 support, and they didn't anticipate that browsers wouldn't end up following the spec. So now it only works where you can rely on a spec-compliant HTTP/2 implementation. reply robertlagrant 6 hours agorootparentprevThe article seems to be an advert for this, with its plug of that hosted gRPCJSON service. reply txdv 10 hours agoparentprev> Try exposing a grpc api to a browser I remember being grilled for not creating \"jsony\" interfaces: message Response { string id = 1; oneof sub { SubTypeOne sub_type_one = 2; SubTypeTwo sub_type_two = 3; } } message SubTypeOne { string field = 1; } message SubTypeTwo { } In your current model you just don't have any fields in this subtype, but the response looked like this with our auto translator: { \"id\": \"id\", \"sub_type_two\": { } } Functionally, it works, and code written for this will work if new fields appear. However, returning empty objects to signify the type of response is strange in the web world. But when you write the protobuf you might not notice reply crabbone 5 hours agoparentprevNothing in Protobuf is suited for streaming. It's anti-streaming compared to almost any binary protocol you can imagine (unless you want to stream VHD, which would be a sad joke... for another time). reply cyberax 12 minutes agorootparent> Nothing in Protobuf is suited for streaming. Uhh... Why? Protobuf supports streaming replies and requests. Do you mean that you need to know the message size in advance? reply aaomidi 16 hours agoparentprevBidirectional streaming is generally a bad idea for anything you’re going to want to run “at scale” for what it’s worth. reply mvdtnz 15 hours agorootparentWhy do you say that? I'm involved in the planning for bidi streaming for a product that supports over 200M monthly active users. I am genuinely curious what landmines we're about to step on. reply joatmon-snoo 10 hours agorootparentbidi streaming screws with a whole bunch of assumptions you rely on in usual fault-tolerant software: there are multiple ways to retry you can retry establishing the connection (e.g. say DNS resolution fails for a 30s window) _or_ you can retry establishing the stream your load-balancer needs to persist the stream to the backend; it can't just re-route per single HTTP request/response how long are your timeouts? if you don't receive a message for 1s, OK, the client can probably keep the stream open, but what if you don't receive a message for 30s? this percolates through the entire request path, generally in the form of \"how do I detect when a service in the request path has failed\" reply cyberax 10 minutes agorootparent> there are multiple ways to retry you can retry establishing the connection (e.g. say DNS resolution fails for a 30s window) _or_ you can retry establishing the stream That's not how protobuf works? If a connection fails, you simply get an IO error instead of the next message. There is no machinery in gRPC that re-establishes connections. You do need to handle timeouts and blocked connections, but that's a generic issue for any protocol. reply jpc0 11 hours agorootparentprevNot going to give you any proper advice but rather a question to have an answer for. It's not unsolvable or even difficult but needs an answer at scale. How do you scale horizontally? User A connects to server A. User A's connection drops. User A reconnects to your endpoint. Did you have anything stateful you had to remember? Did they loadbalancer need to remember to reconnect user A to server A? What happens if the server dropped, how do you reconnect the user? Now if your streaming is server to server over gRPC on your own internal backend then sure, build actors with message passing, you will probably need an orchestration layer (not k8s, that's for ifra, you need an orchestrator for your services probably written by you), for the same reason as above. What happens if Server A goes down but instead of User A it was Server B. The orchestrator acts as your load balancer would have but it just remembers who exists and who they need to speak to. reply jon_richards 15 hours agoprevI've been having fun with connectrpc https://connectrpc.com/ It fixes a lot of the problematic stuff with grpc and I'm excited for webtransport to finally be accepted by safari so connectrpc can develop better streaming. I initially thought https://buf.build was overkill, but the killer feature was being able to import 3rd party proto files without having to download them individually: deps: buf.build/landeed/protopatch buf.build/googleapis/googleapis The automatic SDK creation is also huge. I was going to grab a screenshot praising it auto-generating SDKs for ~9 languages, but it looks like they updated in the past day or two and now I count 16 languages, plus OpenAPI and some other new stuff. Edit: I too was swayed by false promises of gRPC streaming. This document exactly mirrored my experiences https://connectrpc.com/docs/go/streaming/ reply thayne 11 hours agoparentIt still uses protocol buffers though, which is where many of the problems I have with gRPC comes from reply jon_richards 8 hours agorootparentThe auto-generated SDKs are very useful here. An API customer doesn't have to learn protobuf or install any tooling. Plus they can fall back to JSON without any fuss. Connectrpc is much better at that than my envoy transcoder was. If you're thinking from the API author's point of view, I might agree with you if there was a ubiquitous JSON annotation standard for marking optional/nullable values, but I am sick of working with APIs that document endpoints with a single JSON example and I don't want to inflict that on anyone else. reply masterj 2 hours agorootparentprevYou can also choose to use JSON instead. Works great with curl and browser dev tools. reply NAHWheatCracker 17 hours agoprevMy only work experience with gRPC was on a project where another senior dev pushed for it because we \"needed the performance\". We ended up creating a JSON API anyways. Mostly because that's what the frontend could consume. No one except for that developer had experience with gRPC. He didn't go any deeper than the gRPC Python Quick start guide and wouldn't help fix bugs. The project was a mess for a hundred reasons and never got any sort of scale to justify gRPC. That said, I've used gRPC in bits outside of work and I like it. It requires lot more work and thought. That's mostly because I've worked on so many more JSON APIs. reply lordofgibbons 16 hours agoparentThat sounds more like a critique of the \"senior\" developer who didn't know grpc isn't compatible with browsers before adopting it than grpc itself. reply NAHWheatCracker 16 hours agorootparentCorrect, I wasn't critiquing gRPC. I was critiquing a type of person who might push for gRPC. That developer probably thought of it as a novelty and made up reasons to use it. It was a big hassle that added to that teams workload with no upside. reply reactordev 15 hours agorootparentWhen all you have is a hammer… gRPC is fantastic for its use case. Contract first services with built in auth. I can make a call to a service using an API that’s statically typed due to code generation and I don’t have to write it. That said, it’s not for browsers so Mr gRPC dev probably had no experience in browser technologies. A company I worked for about 10 years ago was heavy gRPC but only as a service bridge that would call the REST handler (if you came in over REST, it would just invoke this handler anyway). Everything was great and dtos (messages) were automatically generated! Downside was the serialization hit. reply whoevercares 14 hours agorootparentgRPC is indeed for backend service to service calls with strong contract/model first approach. It’s important for company in serious API and SDK vending business. reply awinter-py 15 hours agorootparentprevyes who would imagine that the homegrown rpc of the internet and browser company would work on the internet and in a browser very fair critique reply rednafi 17 hours agoprevGoogle somehow psyoped the entire industry to use gRPC for internal service communications. The devex of gRPC is considerably worse than REST. You can’t just give someone a simple command to call an endpoint—it requires additional tooling that isn’t standardized. Plus, the generated client-side code is some of the ugliest gunk you’ll find in any language. reply lmm 16 hours agoparent> You can’t just give someone a simple command to call an endpoint—it requires additional tooling that isn’t standardized. GRPC is a standard in all the ways that matter. It (or Thrift) is a breath of fresh air compared to doing it all by hand write down your data types and function signatures, get something that you can actually call like a function (clearly separated from an actual function function as it should be, it behaves differently but usable like one). Get on with your business logic instead of writing serialisation/deserialisation boilerplate. GraphQL is even better. reply coolhand2120 1 hour agorootparent> GraphQL is even better. Letting clients introduce load into the system without understanding the big O impact of the SOA upstream is a foot gun. This does not scale and results in a massive waste of money on unnecessary CPU cycles on O(log n) FK joins and O(n^2) aggregators. Precomputed data in the shape of the client's data access pattern is the way to go. Frontload your CPU cycles with CQRS. Running all your compute at runtime is a terrible experience for users (slow, uncachable, geo origin slow too) and creates total chaos for backend service scaling (Who's going to use what resource next? Nobody knows!). reply nsonha 16 hours agorootparentprev> GraphQL is even better just a casual sentence at the end? How about no. It's in the name, a query-oriented API, useless if you don't need flexible queries. Why don't you address the problem they talked about, what is the cli tool I can use to test grpc, what about gui client? reply mjr00 15 hours agorootparentFor GUI, I've been very happy with grpcui-web[0]. It really highlights the strengths of GRPC: you get a full list of available operations (either from the server directly if it exposes metadata, or by pointing to the .proto file if not), since everything is strongly typed you get client-side field validation and custom controls e.g. a date picker for timestamp types or drop-down for enums. The experience is a lot better than copy & pasting from docs for trying out JSON-HTTP APIs. In general though I agree devex for gRPC is poor. I primarily work with the Python and Go APIs and they can be very frustrating. Basic operations like \"turn pbtypes.Timestamp into a Python datetime or Go time.Time\" are poorly documented and not obvious. proto3 removing `optional` was a flub and then adding it back was an even bigger flub; I have a bunch of protos which rely on the `google.protobuf.Int64Value` wrapper types which can never be changed (without a massive migration which I'm not doing). And even figuring out how to build the stuff consistently is a challenge! I had to build out a centralized protobuf build server that could use consistent versions of protoc plus the appropriate proto-gen plugins. I think buf.build basically does this now but they didn't exist then. [0] https://github.com/fullstorydev/grpcui reply ewhauser421 14 hours agorootparenttimestamppb.New(time) is hard to figure out? reply mjr00 12 hours agorootparent> timestamppb.New(time) is hard to figure out? No need to be snarky; that API did not exist when I started using protobuf. The method was called `TimestampProto` which is not intuitive, especially given the poor documentation available. And it required error handling which is unergonomic. Given that they switched it to timestamppb.New, they must've agreed with me. https://github.com/golang/protobuf/blame/master/ptypes/times...a query-oriented API, useless if you don't need flexible queries Right but, the typical web service at the typical startup does need flexible queries. I feel people both overestimate its implications and under estimate its value. Standard \"I need everything\" in the model call Simplified \"I need two properties call\", like id + display name for a dropdown I need everything + a few related fields, which maybe require elevated permissions GraphQL makes that very easy to support, test, and monitor in a very standard way. You can build something similar with REST, its just very ergonomic and natural in GraphQL. And its especially valuable as your startup grows, and some of your services become \"Key\" services used by a wider variety of use cases. Its not perfect or something everyone should use sure, but I believe a _lot_ of startup developers would be more efficient and satisfied using GraphQL. reply mplanchard 31 minutes agorootparentGraphQL is fine until you have enough data to care about performance, at which point you have to go through and figure out where some insane SQL is coming from, which ultimately is some stitched together hodgepodge of various GraphQL query types, which maybe you can build some special indexes to support or maybe you have to adjust what's being queried. Either way, you patch that hole, and then a month later you have a new page that's failing to load because it's generating a query that is causing your DB CPU to jump to 90%. I'm convinced at this point that GraphQL only works effectively at a small scale, where inefficient queries aren't disastrously slow/heavy, OR at a large enough scale where you can dedicate at least an entire team of engineers to constantly tackle performance issues, caching, etc. To me it also makes no sense at startups, which don't generally have such a high wall between frontend and backend engineering. I've seen it used at two startups, and both spent way more time on dealing with GraphQL BS than it would have taken to either ask another team to do query updates or just learn to write SQL. Indeed, at $CURRENT_JOB the engineering team for a product using GraphQL actively pushed for moving away from it and to server-side rendering with Svelte and normal knex-based SQL queries, despite the fact that none of them were backend engineers by trade. The GraphQL was just too difficult to reason about from a performance perspective. reply apayan 15 hours agorootparentprevgrpcurl is what I use to inspect gRPC apis. https://github.com/fullstorydev/grpcurl reply reactordev 15 hours agorootparentprevTake the protobuf and generate a client… gRPC makes no assumptions on your topography, only that there’s a server, there’s a client, and it’s up to you to fill the logic. Or use grpcurl, or bloomrpc, or kreya. The client is the easy part if you just want to test calls. reply lmm 14 hours agorootparentprev> It's in the name, a query-oriented API, useless if you don't need flexible queries. It's actually still nice even if you don't use the flexibility. Throw up GraphiQL and you've got the testing tool you were worried about. (Sure, it's not a command line tool, but people don't expect that for e.g. SQL databases). reply sitzkrieg 16 hours agoparentprevi agree, was forced to use it at several companies and it was 99% not needed tech debt investment garbage even in go its a pain in the ass to have to regen and figure out versioning shared protos and it only gets worse w each additional language but every startup thinks they need 100 microservices and grpc so whatever reply hamandcheese 15 hours agorootparent> even in go its a pain in the ass to have to regen and figure out versioning shared protos and it only gets worse w each additional language The secret is: don't worry about it. There is no need to regenerate your proto bindings for every change to the protos defs. Only do it when you need to access something new in your application (which only happens when you will be making changes to the application anyway). Don't try and automate it. That is, assuming you don't make breaking changes to your protos (or if you do, you do so under a differently named proto). reply echelon 17 hours agoparentprev> The devex of gRPC is considerably worse than REST. Hard disagree from the backend world. From one protocol change you can statically determine which of your downstream consumers needs to be updated and redeployed. That can turn weeks of work into a hour long change. You know that the messages you accept and emit are immediately validated. You can also store them cheaply for later rehydration. You get incredibly readable API documentation with protos that isn't muddled with code and business logic. You get baked in versioning and deprecation semantics. You have support for richer data structures (caveat: except for maps). In comparison, JSON feels bloated and dated. At least on the backend. reply danpalmer 16 hours agorootparentI also disagree, at Google everything is RPCs in a similar way to gRPC internally, and I barely need to think about the mechanics of them most of the time, whereas with REST/raw HTTP, you need to think about so much of the process – connection lifecycle, keepalive, error handling at more layers, connection pools, etc. However, I used to work in a company that used HTTP internally, and moving to gRPC would have sucked. If you're the one adding gRPC to a new service, that's more of a pain than `import requests; requests.get(...)`. There is no quick and hacky solution for gRPC, you need a fully baked, well integrated solution, rolled out across everyone who will need it. reply pianoben 15 hours agorootparentThe flexibility of HTTP has advantages, too; it's simple to whip up a `curl` command to try things out. How does Google meet that need for gRPC APIs? reply danpalmer 14 hours agorootparentThere's a curl for RPCs internally. It helps too that RPC servers are self describing, so you can actually list the services and methods exposed by a server. I'd say it's much simpler than curl, although again that's in large part because there's a lot of shared infra and understanding, and starting from scratch on that would be hard. reply allset_ 15 hours agorootparentprevServer reflection exists (https://grpc.io/docs/guides/reflection/), but you don't really need to whip out curl when you have the RPC service's definition. It tells you everything you need to know about what to send and what you will receive, so you can just start writing type-safe code. reply bootsmann 10 hours agorootparent>you don't really need to whip out curl when you have the RPC service's definition Following up a \"how do I experiment with this in my workflow\" with \"oh you don't need to\" is not the greatest look. There is a vast portion of programming bugs that stem from someone misunderstanding what a given API does, so the ability to quickly self-verify that one is doing things right is essential. reply rednafi 16 hours agorootparentprevMy perspective stems from working with it in backend services as well. The type safety and the declarative nature of protobufs are nice, but writing clients and servers isn’t. The tooling is rough, and the documentation is sparse. Not saying REST doesn’t have its fair share of faults, but gRPC feels like a weird niche thing that’s hard to use for anything public-facing. No wonder none of the LLM vendors offer gRPC as an alternative to REST. reply spockz 14 hours agorootparentprevThe benefits you mention stem from having a total view on all services and which protos they are using. The same is achievable with a registry of OpenAPI documents. The only thing you need to ensure is that teams share schema definitions. This holds for gRPC as well. If teams create new types just copying some of the fields they need your analysis will be lost as well. reply matrix87 13 hours agorootparentprev> You get incredibly readable API documentation with protos that isn't muddled with code and business logic. I mean, ideally (hopefully) in the JSON case there's some class defined in code that they can document in the comments If it's a shitty shop that's sometimes less likely. Nice thing about protos is that the schemas are somewhere reply recursivedoubts 16 hours agoprev> If your API is a REST API, then your clients never have to understand the format of your URLs and those formats are not part of the API specification given to clients. Roy Fielding, who coined the term REST: \"A REST API should be entered with no prior knowledge beyond the initial URI (bookmark) and set of standardized media types that are appropriate for the intended audience (i.e., expected to be understood by any client that might use the API). From that point on, all application state transitions must be driven by client selection of server-provided choices that are present in the received representations or implied by the user’s manipulation of those representations.\" https://roy.gbiv.com/untangled/2008/rest-apis-must-be-hypert... I know it's a dead horse, but it's so funny: the \"API specification\" given to clients, in a truly RESTful system, should only be the initial entry point URI/URL. reply jahewson 15 hours agoparentThis idea of self-describing REST is now better known as HATEOAS. Personally I think it’s bloated and doesn’t solve a real problem. https://en.m.wikipedia.org/wiki/HATEOAS reply crabmusket 5 hours agorootparentHATEOAS is fantastic when your clients are humans. Not so much when they're code. reply curt15 16 hours agoparentprevHow does one even write an API client against a REST API that only publishes the initial entry point? in particular, how should the client discover the resources that can be manipulated by the API or the request/response models? reply deathanatos 14 hours agorootparentThe responses from prior requests give you URLs which form subsequent requests. For example, if I, GETthat might return the details of my account, which might include a list of links (URLs) to all subscriptions (or perhaps a URL to the entire collection) in the account. (Obviously you have to get the account URL in this example somewhere too, and usually you just keep tugging on the objects in whatever data model you're working with and there are a few natural, easy top-level URLs that might end up in a directory of sorts, if there's >1.) See ACME for an example; it's one of the few APIs I'd class as actually RESTful. https://datatracker.ietf.org/doc/html/rfc8555#section-7.1.1. Needing a single URL is beautiful, IMO, both configuration-wise and easily lets one put in alternate implementations, mocks, etc., and you're not guessing at URLs which I've had to do a few times with non-RESTful HTTP APIs. (Most recently being Google Cloud's…) reply AdieuToLogic 15 hours agorootparentprev> How does one even write an API client against a REST API that only publishes the initial entry point? in particular, how should the client discover the resources that can be manipulated by the API or the request/response models? HAL[0] is very useful for this requirement IMHO. That in conjunction with defining contracts via RAML[1] I have found to be highly effective. 0 https://datatracker.ietf.org/doc/html/draft-kelly-json-hal 1 https://github.com/raml-org/raml-spec/blob/master/versions/r... reply pests 13 hours agorootparentprevLook up HATEOS. The initial endpoint will you give you the next set of resources maybe the user list and then the post list. Then as you navigate to say, the post list, it will have embedded pagination links. Once you have resource urls from this list you can post/put/delete as usual. reply recursivedoubts 4 hours agorootparentprevyour browser is a client that works against RESTful entries points that only publish an initial entry point, such as https://news.ycombinator.com from that point forward the client discovers resources (articles, etc) that can be manipulated (e.g. comments posted and updated) via hypermedia responses from the server in responses reply wstrange 4 hours agorootparentThe browser is also driven by an advanced wetware AI system that knows which links to click on and how to interpret the results. reply loudgas 15 hours agorootparentprevYour Web browser is probably the best example. When you visit a Web site, your browser discovers resources and understands how it can interact with them. reply deathanatos 14 hours agoparentprev> the \"API specification\" given to clients, in a truly RESTful system, should only be the initial entry point URI/URL I don't know that I fully agree? The configuration, perhaps, but I think the API specification will be far more than just a URL. It'll need to detail whatever media types the system the API is for uses. (I.e., you'll need to spend a lot of words on the HTTP request/response bodies, essentially.) From your link: > A REST API should spend almost all of its descriptive effort in defining the media type(s) used for representing resources and driving application state That. I.e., you're not just returning `application/json` to your application, you're returning `+json`. (Unless you truly are working with JSON generically, but I don't think most are; the JSON is holding business specific data that the application needs to understand & work with.) That is, \"and [the] set of standardized media types that are appropriate for the intended audience\" is also crucial. (And I think this point gets lost in the popular discourse: it focuses on that initial entry URL, but the \"describe the media types\", as Fielding says, should be the bulk of the work — sort of the \"rest of the owl\" of the spec. There's a lot of work there, and I think sometimes people hearing \"all you need is one URL\" are right to wonder \"but where's the rest of the specification?\") reply recursivedoubts 4 hours agorootparentthatshould not be API specific, otherwise you are just smuggling an API specification into a second-order aspect of your system and violating the uniform interface. reply eadmund 16 hours agoparentprevYou both agree: when he writes ‘format of your URLs,’ he literally means the format of the URLs, not the format of the resources. Like you, I clicked on the article expecting yet another blogger who doesn’t understand REST but it appears this author has at least some basic knowledge of the concepts. Good for him! I like gRPC too, and honestly for a commercial project it is pretty compelling. But for a personal or idealistic project I think that REST is preferable. reply resonious 16 hours agoparentprevClassic case of a good idea going viral, followed by people misunderstanding the idea but continuing to spread it anyway. reply est 15 hours agoparentprevI think the original REST is only suitable for \"file\" resources, so there's WebDAV and nobody bothers to use it these days. reply gghoop 16 hours agoprevI dislike the use of gRPC within the data center. People reach for it citing performance, but gRPC is not high performance and the quality of the available open source clients is very poor, particularly outside of the core C++/Java implementations like the nodejs implementation. I am not against the use of protobuf as an API spec but it should be possible to use it with a framing protocol over TCP, there just isn't a clear dominant choice for that way of doing RPC. When it comes to web based APIs I am more in favour of readable payloads, but there are issues here since we tend to use JSON but the type specificity is loose, which leads to interop problems between backend languages, particularly in nodejs where JSON parse is used to implement a schema mapping. In order to do this properly, encoders and decoders need to be generated explicitly from schemas, which somewhat diminishes the use of JSON within the context of JS. reply jahewson 15 hours agoparentI agree, though Zod greatly helps with the JS schema issue. I’m keeping an eye on Microsoft’s TypeSpec project too: typespec.io for interoperable schema generation. reply kyrra 13 hours agoparentprevThe main benefit of protos is interop between various languages. If you are a single language tech stack, it matters less. Also, if you use languages outside of Google's primary languages, you're likely not going to get as good of an experience. reply mvdtnz 15 hours agoparentprevIn what situation is performance enough of a concern that you would consider gRPC but not enough of a concern that you would let nodeJS anywhere near your stack? reply gghoop 14 hours agorootparentNo one is picking Nodejs for high performance, but when it is chosen for other reasons it's still expected to perform well. The Nodejs gRPC library performs poorly relatively to the overall performance characteristics of Nodejs, and this is a problem because most of the work performed by typical Nodejs services is API-related work (sending data, encoding and decoding payloads, managing sockets etc). That's not even touching on the bugs in the http2 implementation in node core or the grpc library itself, but much of the selling point of gRPC is supposedly the language interop, and this seems like false advertising to me. reply MobiusHorizons 13 hours agorootparentI would imagine the reason is really that Google internally doesn't allow NodeJS in production, so the tooling for gRPC for NodeJS does not benefit from the same level of scrutiny as languages Google uses internally. reply whoevercares 14 hours agoparentprevThere was a talk in 2023 of a non-TCP based protocol, Homa in RPC for data center use-case https://youtu.be/xQQT8YUvWg8?si=g3u5TogBe0_QpPpj. reply abalaji 15 hours agoprevEveryone is hating on gRPC in this thread, but I thought I'd chime in as to where it shines. Because of the generated message definition stubs (which require additional tooling), clients almost never send malformed requests and the servers send a well understood response. This makes stable APIs so much easier to integrate with. reply inetknght 15 hours agoparent> Because of the generated message definition stubs (which require additional tooling), clients almost never send malformed requests and the servers send a well understood response. Sure. Until you need some fields to be optional. > This makes stable APIs so much easier to integrate with. Only on your first iteration. After a year or two of iterating you're back to JSON, checking if fields exist, and re-validating your data. Also there's a half dozen bugs that you can't reproduce and you don't know why they happen, so you just work around them with retries. reply hedora 14 hours agorootparentThere’s also a gaping security hole in its design. They don’t have sane support for protocol versioning or required fields, so every field of every type ends up being optional in practice. So, if a message has N fields, there are 2^N combinations of fields that the generated stubs will accept and pass to you, and its up to business logic to decide which combinations are valid. It’s actually worse than that, since the other side of the connection could be too new for you to understand. In that case, the bindings just silently accept messages with unknown fields, and it’s up to you to decide how to handle them. All of this means that, in practice, the endpoints and clients will accumulate validation bugs over time. At that point maliciously crafted messages can bypass validation checks, and exploit unexpected behavior of code that assumes validated messages are well-formed. I’ve never met a gRPC proponent that understands these issues, and all the gRPC applications I’ve worked with has had these problems. reply bluGill 2 hours agorootparentI have yet to see a good way to do backward compatibility in anything. The only thing I've found that really works is sometimes you can add an argument with a default value. Removing an argument only works if everyone is using the same value of it anyway otherwise they are expecting the behavior that other value causes and so you can't remove it. Thus all arguments should be required in my opinion. If you make a change add a whole new function with the new arguments. If allowed the new function can have the same time (if overloading should be done this way is somewhat controversial I'm coming out in favor but the arguments against do make good points which may be compelling to you). That way the complexity is managed since there is only a limited subset of the combinatorial explosion possible. reply kybernetikos 10 hours agorootparentprev> every field of every type ends up being optional in practice. This also means that you cant write a client without loads of branches, harming performance. I find it odd that grpc had a reputation for high performance. Its at best good performance given a bunch of assumptions about how schemas will be maintained and evolved. reply abalaji 13 hours agorootparentprevHence, the qualification of stable API. You can mark fields as unused and fields as optional (recently): https://stackoverflow.com/a/62566052 When your API changes that dramatically, you should use a new message definition on the client and server and deprecate the old RPC. reply matrix87 13 hours agorootparentprev> After a year or two of iterating you're back to JSON, checking if fields exist, and re-validating your data. Every time this has happened to me, it's because of one-sided contract negotiation and dealing with teams where their incentives are not aligned i.e. they can send whatever shit they want, and we have to interpret it and make it work reply swyx 17 hours agoprevalways felt like grpc was unnecessarily inaccessible to the rest of us outside google land. the grpc js client unnecessarily heavy and kinda opaque. good idea but poorly executed compared to people who are familiar with the \"simplicity\" of REST reply rgbrgb 17 hours agoparentyes! REST is kind of like HTML... source available by default, human-readable, easy to inspect GRPC is for machines efficiently talking to other machines... slightly inconvenient for any human in the loop (whether that's coding or inspecting requests and responses) The different affordances make sense given the contexts and goals they were developed in, even if they are functionally very similar. reply kyrra 16 hours agoparentprevThe official grpc JavaScript implementation is sort of bad. The one by buf.build is good from what I've seen. https://buf.build/blog/protobuf-es-the-protocol-buffers-type... reply tempest_ 17 hours agoparentprevGRPC is a nice idea weighed down by the fact that it is full of solutions to google type problems I dont have. It seems like a lot of things have chosen it because a \"binary\" like rpc protocol with a contract is a nice thing to have but the further away from GoLang you get the worse it is. reply limaoscarjuliet 17 hours agoparentprevThere are uses where gRPC shines. Streaming is one of them you can transparently send a stream of messages in one \"connection\". For simple CRUD service, REST is more than enough indeed. reply masterj 16 hours agoparentprevYou should check out https://connectrpc.com/ It's based on grpc but works a lot better with web tooling reply dlahoda 17 hours agoparentprevafaik grpc did callbacks before we got sse/ws/webrtc/webtransport. so grpc was needed kind of. and also canonical content streaming was in grpc. in http there was no common accepted solution at old times. reply coder543 16 hours agorootparentYour memory appears to be incorrect. SSE was first built into a web browser back in 2006. By 2011, it was supported in all major browsers except IE. SSE is really just an enhanced, more efficient version of long polling, which I believe was possible much earlier. Websocket support was added by all major browsers (including IE) between 2010 and 2012. gRPC wasn't open source until 2015. reply dilyevsky 16 hours agorootparentprevIm old enough to have worked with asn.1 and its various proprietary “improvements” as well as SOAP/wsdl and compared to that working with protobuf/stubby (internal google predecessor to grpc) was the best thing since sliced bread reply kybernetikos 10 hours agorootparentprevEven in 2025 grpc is still awful for streaming to browsers. I was doing Browser streaming via a variety of different methods back in 2006, and it wasn't like we were the only ones doing it back then. reply echelon 17 hours agoparentprevThe frontend / backend split is where you have the REST and JSON camps fighting with the RPC / protobuf / gRPC factions. RPCs have more maintainable semantics than REST as a virtue of not trying to shoehorn your data model (cardinality, relationships, etc.) into a one-size-fits-all prescriptive pattern. Very few entities ever organically evolve to fit cleanly within RESTful semantics unless you design everything upfront with perfect foresight. In a world of rapidly evolving APIs, you're never going to hit upon beautiful RESTful entities. In bigger teams with changing requirements and ownership, it's better to design around services. The frontend folks don't maintain your backend systems. They want easy to reason about APIs, and so they want entities they can abstract into REST. They're the ultimate beneficiaries of such designs. The effort required for REST has a place in companies that sell APIs and where third party developers are your primary customers. Protobufs and binary wire encodings are easier for backend development. You can define your API and share it across services in a statically typed way, and your services spend less time encoding and decoding messages. JSON isn't semantic or typed, and it requires a lot of overhead. The frontend folks natively deal with text and JSON. They don't want to download protobuf definitions or handle binary data as second class citizens. It doesn't work as cleanly with their tools, and JSON is perfectly elegant for them. gRPC includes excellent routing, retry, side channel, streaming, and protocol deprecation semantics. None of this is ever apparent to the frontend. It's all for backend consumers. This is 100% a frontend / backend tooling divide. There's an interface and ergonomic mismatch. reply eadmund 16 hours agorootparentProtobufs vs. JSON are orthogonal to REST vs. RPC: you can have REST where the representations are protobufs or JSON objects; you can have RPC where the requests and responses are protobufs or JSON objects. reply dlahoda 17 hours agoparentprevthere are well working (official) generators of openapi/json schemas for grpc. so once you wrote grpc, you get open api rpc for free. reply pphysch 17 hours agoparentprevHow could gRPC be simpler without sacrificing performance? reply jeeyoungk 17 hours agorootparentThere's two parts to gRPC's performance 1. multiplexing protocol implemented on top of HTTP/2 2. serialization format via protobuf For most companies, neither 1 or 2 is needed, but the side effect of 2 (of having structured schema) is good enough. This was the idea behind twrip https://github.com/twitchtv/twirp not sure whether this is still actively used / maintained, but it's protobuf as json over HTTP. reply liontwist 17 hours agorootparentprevWhat kind of performance? Read? Write? Bandwidth? reply dlahoda 17 hours agorootparentgrpc \"urls\" and data are binary. binary with schema separation. 3x smaller payload. reply taeric 2 hours agoprevBy far the worst part of OpenAPI is how aspirational the documentation seems to remain. It seems it is always leveraging things that almost worked in the previous version with advice on how it should be done. But if you do the new way, expect that about half of the tooling you find won't work. It really is WSDL all over again. Where if you buy in to a specific vendor's tooling, and don't leave it, things actually do mostly work as advertised. You want to piecemeal anything, and holy crap at the unexpected pitches. reply tyre 16 hours agoprev> The least-commonly used API model is REST—only a small minority of APIs are designed this way brother. reply mjr00 16 hours agoparentTechnically they're right, though; the textbook definition of REST is rare to nonexistent in my experience. What people do instead is create JSON-RPCs-over-HTTP APIs, sometimes following a REST-like URL scheme, and sometimes using different HTTP verbs on the same URL to perform different actions as one would in REST... but the API isn't really REST. The creator of REST has gone on the record multiple times about how you shouldn't call these APIs REST[0]. But in practice when most people say REST they just mean \"JSON RPC over HTTP\". I avoid calling things REST now and just use \"JSON HTTP API\" to avoid the \"well, actually...\" responses. (and yes, these APIs are by far the most common.) [0] https://roy.gbiv.com/untangled/2008/rest-apis-must-be-hypert... reply eadmund 16 hours agoparentprevReal REST is a very, very small minority. Fake REST (i.e., JSON RPC) is really ridiculously common. reply mplanchard 15 hours agorootparentI’ve never liked the no true scotsman nature of REST (which is exacerbated by the fact that its canonical “specification” is a broad PhD dissertation with a lot of other concepts thrown in), so I have adopted a fairly lax definition: if your URLs are subjects and you use HTTP verbs for the verbs, I feel like it qualifies. reply eadmund 15 hours agorootparentLanguage is a means of communication, and we have to have some sort of agreement on terms. REST had an original meaning; that is a useful thing to be able to discuss. JSON-RPC is also a useful thing to discuss. But the two things are different. It’s confusing to use the one word or phrase to mean two different things (like ‘inflammable’!). Granted, language is to some extent defined by usage: if enough people use a term incorrectly, and few enough people use it correctly, then the incorrect usage becomes correct and the correct incorrect. Fine, we can use ‘REST’ to mean ‘RPC over HTTP with a JSON encoding.’ But could the advocates of that usage propose a term we can all use for what Fielding described in his thesis? Potrzebie? The thing that worries me, is if we start using ‘REST’ to mean JSON-RPC, and ‘Potrzebie’ to mean ‘the style formerly known as REST’: will people start using ‘Potrzebie’ to mean JSON-RPC? Perhaps worse, maybe they will start using it to mean ‘gRPC with JSON encoding.’ I propose that it’s best to use words and phrases for what they originally mean, for as long as one can, and to fight strenuously against changing them. Otherways wun May nyet wit was kood hap. reply mplanchard 15 hours agorootparentREST’s original meaning is pretty ambiguous and poorly specified. The dissertation was written to describe the design and architecture of the HTTP protocol itself, which was largely designed with and alongside this concept of REST. It predates a lot of modern Internet usage and thus doesn’t map perfectly onto current paradigms. I’d argue that even saying a “REST API” means you’re already in the land of impurity. “True REST” is expounded upon by Fielding in a variety of places, and is essentially HATEOS (hypermedia as the engine of application state). But no one, and I mean no one, actually does that. Thus, in order to communicate effectively and “have an agreement on terms,” as you say, we need a less strict definition. I provided my suggestion. If your suggestion is to go back to the primary sources, I have. Multiple times. It does not provide a particularly concrete or useful definition (because its point was not to define REST). If it had, trying to define it would be much less of a no true scotsman game. Notice that we’re not sitting here debating the meaning of HTTP. reply kelseyfrog 13 hours agorootparentIt is. One of the biggest points of tension is that we've more or less settled on JSON as an interchange format which is not exactly hypermedia put of the box. That contradiction has severe implications in the application of HATEOS as it exists re JSON APIs. reply physicles 14 hours agorootparentprevMaybe I've been educated in a strange part of the internet, but I assume that this ship already sailed ~10 years ago: when most people (90%+) hear REST, they imagine something vaguely like JSON-RPC. (and this is how ChatGPT, a sort of average of all opinions on the Internet, understands it) So if you say REST and mean something other than that, then you're committing to being misunderstood by most people. reply deathanatos 13 hours agorootparent> So if you say REST and mean something other than that, then you're committing to being misunderstood by most people. Perhaps, but TFA is clearly written in that it is using the actual, real meaning of REST, not the value-drift corruption the laity have wrought. (The upthread comment snips out the surrounding context that brings that clarity.) Which brings us right back to the problem at hand: Potrzebie. reply physicles 9 hours agorootparentThis is true. And the comments are full of people confused about how the article is using the term REST. reply awinter-py 16 hours agoprevunless you want to be locked into a cursed ecosystem where you spend all your time reimplementing libraries that have existed for decades in rest land, fighting code generators that produce hideous classes that will randomly break compatibility, and debugging random edge-casey things in your hosting stack bc nobody truly supports h2, steer clear of grpc 'rest' isn't anything (complementary) reply bmilleare 1 hour agoprevI'm not sure why the article picks these 3 options as if that's it. An RPC API can happily exist over plain old HTTP/1 (no protobuf required) and it also doesn't mention the primary benefit of RPC over REST/RESTish (IMO) and that's the ability to stack multiple RPC calls into a single request. reply bigmutant 14 hours agoprevNever really understood the folks pushing for RPC-over-HTTP. RPC is for systems that are close together (ie intra-DC). These simple rules work well: 1. JSON-over-HTTP for over the web 2. RPC (pick your flavor) for internal service-to-service I will say that Amazon's flavor (Coral-RPC) works well and doesn't come with a ton of headache, its mostly \"add ${ServiceName}Client to build\" and incorporate into the code. Never mind its really odd config files Related note, I've never understood why Avro didn't take off over GRPC, I've used Avro for one project and it seems much easier to use (no weird id/enumerations required for fields) while maintaining all the code-gen/byte-shaving reply 9rx 4 hours agoparent> 1. JSON-over-HTTP for over the web So literally gRPC[1]? You make it sound like there is a difference. There isn't, really. What gRPC tried to bring to the table was establishing conventions around the details neither HTTP or JSON define, where otherwise people just make things up haphazardly with no consistency from service to service. What gRPC failed on in particular was in trying to establish those conventions on HTTP/2. It was designed beside HTTP/2 with a misguided sense of optimism that browsers would offer support for HTTP/2 once finalized. Of course, that never happened (we only got half-assed support), rendering those conventions effectively unusable there. [1] I'll grant you that protobufs are more popular in that context, but it is payload agnostic. You can use JSON if you wish. gRPC doesn't care. That is outside of its concern. reply lpapez 7 hours agoprevI've come to the conclusion that whatever the question is, gRPC isn't the answer unless you are working on Google backend. The performance benefit they mention comes at the cost of (un)debugability of the binary protocol, and the fact that the interface definition language requires client code generation just further deepens the existing moats between teams because of diverging tooling and explicit boundaries drawn up by said contract. IMO gRPC mostly ends up used as a band-aid for poor cross-team collaboration, and long-term worsens the symptoms instead of fixing the core issue. The fact that it's PITA to use is secondary, but significant too. reply Glyptodon 17 hours agoprevAccording to this, what is GraphQL? This article seems like something written with limited or unusual experience. reply eadmund 7 hours agoparent> According to this, what is GraphQL? GraphQL is akin to gRPC: a non-HTTP protocol tunnelled over HTTP. Unlike gRPC, I’m unconvinced that GraphQL is ever really a great answer. I think what the latter does can be done natively in HTTP. reply mdaniel 3 hours agorootparentFor all the people singing the praises of how efficient gRPC is, I enjoy countering that the most efficient response is one which doesn't include 99% of data that the client doesn't care about in the slightest GCP (and I believe Azure, too) offer `GET /thing?$fields=alpha,beta.charlie` style field selection but now there's a half-baked DSL in a queryparam and it almost certainly doesn't allow me to actually express what I want so I just give up and ask for the top-level key because the frustration budget is real I for sure think that GraphQL suffers from the same language binding problem as gRPC mentioned elsewhere: if you're stack isn't nodejs, pound sand. And the field-level security problem is horrific to fix for real reply mplanchard 20 minutes agorootparentEfficient in terms of wire transfer sure, but GraphQL tends to wind up generating queries that are quite difficult to optimize at the DB layer, so you wind up spending way more computer and time than you would otherwise need. If you're in an organization where folks with no database knowledge are writing the GraphQL queries, this winds up being a never-ending game of whack-a-mole. For anything performance sensitive, I'd much rather have a nice, optimized endpoint that returns more data than the client needs rather than have the client be able to issue any query they want. reply rswail 11 hours agoprevThe problem with gRPC is the \"R\". It's been the same with JMI, Corba, ONC-RPC and all the others. Making \"procedure calls\" remote and hiding them underneath client libraries means that programmers do not consider the inherent problems of a networked environment. Problems like service discovery, authentication, etc are hidden beneath something that \"looks like\" a local procedure call. That's one problem, the other is that procedure calls are focusing on the verbs, not the nouns (called \"entities\" or \"resources\" in the article). If you can't express an FSM about a noun and what causes its state to change, then how the hell do you know what it does or how changes to its environment affect it? If you don't know whether some procedure call is idempotent, how the hell can you write code that handles the various network failure modes that you have to deal with? reply divan 11 hours agoparenthttps://scholar.harvard.edu/files/waldo/files/waldo-94.pdf reply thayne 11 hours agoparentprevThat is a problem, certainly, but not the only one. reply rollulus 10 hours agoprevLot of gRPC hate here. I like gRPC in terms of an API specification, because one only needs to define the “what”, whereas OpenAPI specs are about the “how”: parameter in path, query, body? I don’t care. Etc. Plus the tooling: we ran into cases where we could only use the lowest common denominator of OpenAPI constructs to let different tech stacks communicate because of orthogonal limitations across OpenAPI codegenerators. Plus, Buf’s gRPC linter that guarantees backwards compatibility. Plus fewer silly discussions with REST-ish purists: “if an HTTP endpoint is idempotent should deleting the same resource twice give a 404 twice?” dude, how’s that helping the company to make money? Plus, easier communication of ideas and concepts between human readers of the (proto) spec. reply robertlagrant 8 hours agoparent> Plus fewer silly discussions with REST-ish purists: “if an HTTP endpoint is idempotent should deleting the same resource twice give a 404 twice?” dude, how’s that helping the company to make money? It helps by trying to map standard metaphors to your company's concepts instead of inventing bespoke return types for your company's concepts. You still need to decide whether or not to indicate that the resource is either not there, or was never there. reply afiodorov 5 hours agoprevI’ve generally regarded gRPC as a high-performance protocol mainly suited for connecting microservices—something you’d keep internal rather than expose publicly. But it shines in use cases like live captioning, where a transcription service has to stay in sync with a video feed and can’t afford dropped messages. In my experience, using plain WebSockets for high-throughput internal communication was a mistake because while WebSockets use TCP underneath, they don’t inherently handle reconnection or message acknowledgments. With gRPC, those features come built-in, saving you from implementing them yourself. reply TeeWEE 14 hours agoprevIt depends. That’s the whole point. I see a lot of people here saying one is better than the other. But it depends on your use case and company size. GRPC is a lot more complex to start using and hides internals. However it has some major advantages too like speed, streaming, type safety, stub generation. Once you have it in place adding a function is super easy. The same can be said of OpenAPI. It’s easier to understand. Builds upon basic REST tech. However JSON parsing is slow, no streaming and has immature stub generation. From my experience a lot of users who use OpenAPI only use it to generate a spec from the handwritten endpoints and do manual serialization. This is the worst of the two worlds. manual code in mapping json to your objects manual code mapping function parameters to get params or json often type mapping errors in clients Those engineers often don’t understand that OpenAPI is capable of stub generation. Let alone understand GRPC. GRPC saves a lot of work once in place. And is technical superior. However it comes at a cost. I’ve seen OpenAPI generated from routes, with generated clients libs work really well. This requires some time to setup because you can hardly use OpenAPIGenerator out of the box. But once setup I think it hits a sweet spot: simple: http and json can be gradually introduced from hardcoded manual json serialization endpoint (client and server) can be used as an external api allows for client lib generation But it really depends on your use case. But to dismiss GRPC so easily mainly shows you have never encountered a use case where you need it. Once you have it in place it is such a time saver. But the same holds for proper OpenAPI RPC use. However my inner engineer hates how bad the tooling around OpenAPI is, the hardcoded endpoints often done instead of server stubs, and the amount of grunt work you still need todo to have proper client libs. reply msoad 15 hours agoprevI think everyone who worked at Google in the past has PTSD from having to migrate gRPC schemas. What a mess! Type safety doesn't have to be this costly reply Octoth0rpe 17 hours agoprevOof, I strongly disagree with this article's description of how REST apis are used, and the distinction between openAPI and rest. If I design a REST api in 2023, and in 2024 produce an openapi yaml or json file for that API with no other changes, is it somehow no longer a REST api? of course not. The article seems to be predicated on this distinction. > The least-commonly used API model is REST Is that true? I don't think it is frankly, though I suppose if any API that would be a REST api _if it didn't have an openapi spec_ is somehow no longer a REST api, then maybe? But as previously stated, I just don't think that's true. > A signature characteristic of [REST APIs] is that clients do not construct URLs from other information I don't think this is true in practice. Let us consider the case of a webapp that uses a REST api to fetch/mutate data. The client is a browser, and is almost certainly using javascript to make requests. Javascript doesn't just magically know how to access resources, your app code is written to construct urls (example: getting an ID from the url, and then constructing a new url using that extracted ID to make an api call to fetch that resource). In fact, the only situation where I think this description of how a REST api is used is _defensibly_ true (and this is hella weak), is where the REST api in question has provided an openapi spec, and from that spec, you've converted that into a client library (example: https://openapi-ts.dev). In such a situation, the client has a nice set of functions to call that abstract away the construction of the URL. But somewhere in the client, _urls are still being constructed_. And going back to my first complaint about this article, this contrived situation combines what the article states are two entirely distinct methods for designing apis (rest vs openapi). Re: the article's description of rpc, I actually don't have any major complaints. reply golly_ned 16 hours agoparentThis stood out to me as well. The author must have a particular understanding of REST that differs from the usual sense in which it’s used. He might be technically correct — I haven’t read the primary sources related to REST — but it distracted from the meat and potatoes of the article, which is really a comparison of gRPC and OpenAPI. It seemed very strange for this reason. reply jijji 16 hours agorootparentor he works for Google (author of gRPC) and is being paid to extol the virtues, albeit short sighted, of gRPC reply silisili 17 hours agoparentprevYou're being way too polite. The article is garbage and completely incorrect about what REST and OpenAPI even are. reply mvdtnz 15 hours agorootparentYou're wrong. The author is using \"REST\" to mean an API at Level 3 on the Richardson Maturity Model[0] this was the original conception of what it meant to be a \"REST API\" before the wider internet decided \"REST\" meant \"nice looking URLs\". What he refers to as \"OpenAPI APIs\" could be called Level 2 Web APIs on the same model. He uses \"REST\" correctly. He uses \"OpenAPI\" as a shorthand for the class of web APIs that are resource-based and use HTTP verbs to interact with these resources. [0] https://en.wikipedia.org/wiki/Richardson_Maturity_Model reply silisili 14 hours agorootparentI could concede perhaps he wasn't necessarily wrong on REST, though I personally think it's pedantic and incorrect, regardless of what the creator of the term says. Things evolve, and returning a list of objects instead of a list of links was an obvious progression, since spamming 1000s of GET requests doesn't scale well in the post 90s. If the industry at large generally agrees on what makes an API restful, it feels like we should accept such evolution. OpenAPI is a description language and has little to do with an API itself. It's documentation. People were using 'unpure' REST long before it or Swagger even existed. And as the parent pointed out, you can add an openapi spec later, and it doesn't magically change the API itself. Further, it creates a weird circular logic that doesn't work. From https://swagger.io/docs/specification/v3_0/about/ \"OpenAPI Specification (formerly Swagger Specification) is an API description format for REST APIs\" reply curt15 15 hours agoparentprevHATEOAS is crucial to what [Roy Fielding](https://roy.gbiv.com/untangled/2008/rest-apis-must-be-hypert...) calls REST APIs. >A REST API must not define fixed resource names or hierarchies (an obvious coupling of client and server). Servers must have the freedom to control their own namespace. Instead, allow servers to instruct clients on how to construct appropriate URIs, such as is done in HTML forms and URI templates, by defining those instructions within media types and link relations. Most APIs that people call \"RESTful\" regardless of whether they come with an OpenAPI spec don't obey HATEOAS. A typical OpenAPI spec describes the possible request paths and verbs. However, you probably wouldn't be able to discover all that information just by starting from the entry point and parsing the `hrefs` in the response bodies. reply robertlagrant 7 hours agorootparentRoy Fielding would also say that you probably don't need that definition of REST. The main advantages most people get from REST are in the standardised tooling, faster browser/library parsing of JSON, HTTP makes firewalls easy, and it looked so much nicer than the incumbent, SOAP[0]. [0] https://www.w3.org/TR/2000/NOTE-SOAP-20000508 reply eadmund 15 hours agoparentprev> > A signature characteristic of [REST APIs] is that clients do not construct URLs from other information > I don't think this is true in practice. 'recursivedoubts: https://news.ycombinator.com/item?id=42799917 The blogger is completely correct. In a true REST (i.e., not JSON-RPC) API, the client has a single entry URL, then calls the appropriate HTTP verb on it, then parses the response, and proceeds to follow URLs; it does not produce its own URLs. Hypertext as the engine of application state. For example, there might be a URL http://foocorp.example/. My OrderMaker client might GET http://foo.example/, Accepting type application/offerings. It gets back a 200 response of type application/offerings listing all the widgets FooCorp offers. The offerings document might include a URL with an order-creation relationship. That URL could be http://foocorp.example/orders, or it could be http://foocorp.example/82347327462, or it could be https://barcorp.example/cats/dog-attack/boston-dysentery — it seriously doesn’t matter. My client could POST to that URL and then get back a 401 Unauthorized response with a WWW-Authenticate header with the value ‘SuperAuthMechanism system=\"baz\"’, and then my client could prompt me for the right credentials and retry the POST with an Authorization header with the value ‘SuperAuthMechanism opensesame’ and receive a 201 response with a Location header containing a URL for the new empty order. That could be http://foocorp.example/orders/1234, or it could be https://grabthar.example/hammer — what matters is that my client knows how to interact with it using HTTP verbs, headers and content types, not what the URL’s characters. Then my client might POST a resource with content type application/order-item describing a widget to that order URL, and get back 202 Accepted. Then it might POST another resource describing a gadget, and get back 202 Accepted. Then it might GET the original order URL, and get back a 200 OK of type application/order which shows the order in an unconfirmed state. That resource might include a particular confirm URL to PUT to, or perhaps my client might POST a resource with content type application/order-confirmation — all that would be up to the order protocol definition (along with particulars like 202, or 200, or 201, or whatever). Eventually my client non-idempotently PUTs or POSTs or whatever, and from then on can poll the order URL and see it change as FooCorp fulfills it. That’s a RESTful API. The World Wide Web itself is a RESTful API for dealing with documents and also complete multimedia applications lying about being documents, but the RESTful model can be applied to other things. You can even build a RESTful application using the same backend code in the example, but which talks HTML to human beings whose browsers ask for text/html instead of application/whatever. Or you might build a client which asks for ‘text/html; custom=orderML’ and knows how to parse the expected HTML to extract the right information, and everything shares common backend code. Or you might use htmx and make all this reasonably easy and straightforward. That’s what REST is. What REST is not, is GETting http://api.foocorp.example/v1/order/$ORDERID and getting back a JSON blob, then parsing out an item ID from the JSON blob, then GETting http://api.foocorp.example/v1/item/$ITEMID and so forth. reply Octoth0rpe 5 hours agorootparentI think there's the REST that Fielding intended, and there's the REST that everyone has spent almost 20 years implementing. At some point we should acknowledge that the reality of REST-like API design is a valid thing to point to and say \"that's REST!\" even if it doesn't implement all of Fielding's intentions. To me the critical part of REST is the use of http semantics in API design, which makes it very un-RPC like. The idea of a naive api client crawling through an API to get at the data that it needs seems so disconnected from the reality of how _every api client I've ever implemented_ works in a practical sense that it's unfathomable to me that someone thinks that this is a good idea. I mean, as a client, I _know_ that I want to fetch a specific `order` object, and I read the documentation from the API provider (which may in fact be me as well, at least me as an organization). I know the URL to load an order is GET /orders/:id, and I know the url to logout is DELETE /loginSession. It would never make sense to me to crawl an API that I understand from the docs to figure out if somehow the url for fetching orders has changed. I do think we need some kind of description of REST 2.0 that makes sense in today's world. It certainly does not involve clients crawling through entry urls and relationships to discover paths that are clearly documented. It probably does involve concepts of resources and collections of resources, it certainly mandates specific uses for each http method. It should be based on the de facto uses of REST in the wild. And this thing would _definitely_ not look like an rpc-oriented api (eg soap, grpc). reply ctxc 14 hours agorootparentprevThank you for the summary! :) reply turnsout 16 hours agoparentprevYeah, the author has an extremely idiosyncratic take on the definition of REST which is either based on a misunderstanding, or a fundamentalist view of \"pure\" REST. reply robertlagrant 6 hours agoprevThis article seems to make the mistake of thinking that things are either full Roy Fielding REST or it's RPC. OpenAPI is not similar to gRPC because it's noun-oriented, not verb-oriented. gRPC is more like SOAP: ignore HTTP semantics and write method calls and we'll sort it out. OpenAPI is somewhere on the path to full REST: few verbs; lots of nouns. reply bborud 10 hours agoprevOne thing that plagues almost all API solutions where you have to generate code is that the vast majority of code generators are bad, and often the code they generate is ugly. I've never understood why so many code generators are so fiddly. They are supposed to parse text and produce text as output. You would think that it would be possible to do this without involving all manner of junk dependencies. It reminds me of what I refer to as \"the most important slide in software engineering\". It was a slide by Sterling Hughes (PHP, MyNewt etc) from a presentation I can no longer remember the details of. But the slide simply said \"nobody cares if it works for you\". In the sense that if you write code that other people are supposed to use, do make an effort to put yourself in their place. Sterling was probably 16-17 at the time, and worked as a paid intern where I worked. But despite his young age, he managed to express something that most of us will never fully take on board. Whenever I get an OpenAPI yaml file instead of a client library for some system I know things are going to be frustrating. reply bilekas 5 hours agoprev> The problem is that MVPs don’t actually establish whether the team /could/ get to a finished product, and in practice many can’t. Isn't that WHY you go to investors? To get the funding to hire to get it to market? reply larusso 12 hours agoprevI generally like the article. I wished the REST concept would have been explained with some code / payload examples though. Other the that it managed to steer me away from gRPG. All the cons he mentioned are huge deal breakers in my opinion. I would only consider if I can control both server and client and its implementation details (tech stack in this case). But he addressed some issues with OpenAPI I constantly struggle with. And the fact that seemingly none is able to say what the standard is for certain patterns. And don’t get me started with OData … reply matrix87 13 hours agoprevI feel like this article jus",
    "originSummary": [
      "API design primarily uses two models: gRPC (Remote Procedure Call) and REST (Representational State Transfer), both often mapped to HTTP, with gRPC using HTTP/2 for efficient communication.",
      "REST APIs are known for their simplicity and stability, while gRPC offers efficient client libraries and server implementations, making it suitable for internal APIs or when using specific tools like Cloud Endpoints.",
      "OpenAPI, similar to RPC, requires constructing URLs from templates, exposing HTTP details, and may involve complex custom HTTP mappings, offering broader HTTP tool compatibility compared to gRPC."
    ],
    "commentSummary": [
      "Developers often find gRPC challenging due to tooling issues, particularly in Java, and difficulties with debugging and configuration, despite its advantages like bidirectional streaming and efficient transport.",
      "OpenAPI is a documentation standard for APIs, facilitating easier client generation and testing, and is often confused with REST, which is rarely implemented in its pure form.",
      "The decision to use gRPC, OpenAPI, or REST depends on specific use cases, team expertise, and project requirements, highlighting the importance of understanding each technology's strengths and limitations."
    ],
    "points": 291,
    "commentCount": 236,
    "retryCount": 0,
    "time": 1737593262
  },
  {
    "id": 42798960,
    "title": "NIH hit with freezes on meetings, travel, communications, and hiring",
    "originLink": "https://www.science.org/content/article/trump-hits-nih-devastating-freezes-meetings-travel-communications-and-hiring",
    "originBody": "www.science.org Verifying you are human. This may take a few seconds. www.science.org 906a099f4a82171a",
    "commentLink": "https://news.ycombinator.com/item?id=42798960",
    "commentBody": "NIH hit with freezes on meetings, travel, communications, and hiring (science.org)260 points by carbocation 18 hours agohidepastfavorite388 comments dekhn 18 hours agoSome of these changes, if continued and expanded, will likely have long-term negative effects on the US's position in science. I have my issues with NIH but to fix NIH requires subtlety. This seems more designed to \"punish those liberal researchers\" https://en.wikipedia.org/wiki/Cutting_off_one%27s_nose_to_sp... reply whyenot 18 hours agoparentThis is surgery with a butter knife. It's going to cause a lot of unnecessary disruption and pain which could have been avoided with a more nuanced approach. What we are seeing with this and some of the new administration's other initiatives, is the abandonment of US soft power in the world. reply dekhn 18 hours agorootparentHuh, I like the way you phrased that: \"abandonment of US soft power in the world\". But I think there is also punitive aspect: they are intentionally \"punishing\" the people at NIH they perceive as being liberal and favoring other countries. reply tired_and_awake 17 hours agorootparentIt's a good point. There is a target on the back of scientists who do research that is perceived as social science but is infact biology. Seems like they then construct myths of waste/corruption (just read this thread) and punish everyone. reply gunian 17 hours agorootparentprevthe purge is real :) reply duxup 17 hours agorootparentprevI sometimes like to imagine if I was president ... how hard would it be to find someone in a given department who could lead it and know better than the communication restrictions as we have here. Granted, I fear the folks in charge now DO know better and the side effects are intentional ... but I wonder if it would be possible to pick someone and get it reasonably right. reply grahamj 17 hours agorootparentprevEven if these idiots wanted subtlety they're too stupid to make it happen. They just want to do big, bold things but have no idea how to actually make anything better. reply JohnBooty 5 hours agorootparentThey certainly don't know how to do this stuff with any sort of grace or subtlety but perhaps more importantly... they don't care to. They perceive no value from \"soft power\" stuff like the NIH and to a large extent consider it to be (part of) the enemy. And most of their voting base really doesn't understand a single thing about the NIH. I mean, truly: the average American doesn't know what that program even is. Other than occasionally getting roasted for funding some ridiculous-sounding project like studying depression in chickens or something.[1] So the only real \"win\" here for the new ruling party is to burn it down and then brag that they saved \"the American taxpayers\" $XYZ billion dollars. (Which will wind up directly in the hands of the rich via tax breaks, and by definition can't really benefit poor people because they already pay little tax) [1] Most of the time these \"ridiculous sounding\" programs are actually pretty relevant and/or useful if you look into them. reply cam_l 10 hours agorootparentprev>have no idea how to actually make anything better True, but that is not their intention. reply dekhn 16 hours agorootparentprevI knew a guy who was the president of a co-op house at UC Berkeley (basically, a student-run housing system decoupled from the university). They had regular meetings and he described one of the challenges: while most people who came to the meetings just wanted to vote on measures to buy food and change policies, there was a subgroup that \"just wanted to fuck shit up\". You know, like during a protest there are always some people who go around doing unncessary damage to unrelated/innocent businesses. They just enjoy breaking things and enjoy making people mad. It's not a perfect analogy but I see Trump and his cronies as a \"fuck shit up\" contingent they seem to genuinely enjoy making their enemies unhappy by breaking things, regardless of the societal cost of their actions. reply monkeycantype 13 hours agorootparentI was at some of those meetings, though unlikely to be the exact same ones :) At the meetings I attended the furious debate was whether we allowed a volunteer group cook food in our kitchen to feed homeless people. The interesting thing about this debate was that to make a decision, you had to have a position on what the coop was, Was it cheap housing for students, or was the cheap housing something that emerged from committing to a practice of cooperative behaviour? Supporting the volunteers cooking in our kitchen came at a cost, many volunteers were homeless, some had mental health issues. I felt it was morally right to let them use our kitchen, And I know that for this I did get labeled by some as pursuing chaos as performative social justice points and to upset opponents, But from my perspective I sincerely believe it was the right thing to do. When parallels like this are revealed, it really makes me wonder how the orange cult people and I ended up on such different paths, both as a response to dissatisfaction with the status quo, why have the stories they’ve been told resonated with them when they seem so inauthentic to me, and why is the more progressive ideology that seems reasonable to me so distasteful to them reply e40 6 hours agorootparentprevI like this analogy. I've had the thought many, many times over the last 8 years that many people seem to like breaking things because their perceived enemies will be unhappy. Was this co-op perhaps Barrington Hall? I had good friends that lived there while at UCB. To my young eyes it was absolute anarchy. reply dekhn 2 hours agorootparentCloyne. I think barrington had been shut down by then? reply speakfreely 13 hours agorootparentprevI doubt anyone would deny the policy actions themselves are clumsy but it's important to remember that a majority of the country believes they are directionally correct. People were desperate to escape the rot of the status quo and chose the high variance option. Michael Moore described it best when he noted that Trump was the human hand grenade that was being tossed in frustration, with a faint hope that things would settle better after the carnage. reply UltraSane 15 hours agoparentprevThis is what happens when you let a horse in the hospital. It wrecks stuff reply ashoeafoot 11 hours agorootparentWhich the voters in majority voted for as they considered stuff already wrecked for them reply UltraSane 6 hours agorootparentNo, because they are gullible, sexist, and racist idiots who can't learn. Things can already get MUCH more wrecked. Just wait until Trump deports all the illegal immigrants who are the backbone of the agriculture sector and food prices explode. And prices for imports explode due to completely unnecessary tariffs. reply krapp 5 hours agorootparentTo be fair, people were saying this about voting for Trump back in 2016, that he was their \"bull in a China shop\" and they didn't care what he did as long as he wrecked the establishment. It's basic accelerationism destabilize the system and force radical social change through the chaos. And it seems to be working. reply UltraSane 5 hours agorootparenta small percentage of Trump voters are accelerationists. reply krapp 4 hours agorootparentA much larger percentage would seem to sympathize with the accelerationists' goals, even if they're ignorant of their methods. reply Jerrrry 4 hours agorootparentprev\"Everyone who disagrees with me is bad.\" Please, not another 4 years of this. Sincerely, reddit is not HN. reply UltraSane 4 hours agorootparentSome things are too fundamental to be simple \"disagreements\" in a society. Disagreements over slavery led to the Civil War. reply narrator 9 hours agoparentprevAs the token Trump supporter on HN, let me give you what I think's going on. I think they are doing an ideological purge. They want to get rid of anyone promoting transgender science. I think RJK JR. wants to get rid of anyone he thinks is against his MAHA agenda. They want to get rid of any pandemic scaremongering too. Peter Hoetz said that Bird Flu will start once the trump administration takes office for example.[1]. Hoetz who is a major figure in the vaccine research industry said that \"starting January 21st we've got some big stuff coming down the pike starting with H5N1...\" and after the Fauci pardon anything is possible. The bird flu outbreak had been behind a lot of food inflation. I wouldn't put it past people on the radical left who want to hurt the Trump administration to hype the bird flu pandemic to drive up food prices through mass culling of livestock. [1] https://x.com/TaraBull808/status/1865026704504426860?t=hZnEk... reply pseudalopex 7 hours agorootparent> As the token Trump supporter on HN In a minority most likely. Not even close to alone. > Peter Hoetz said that Bird Flu will start once the trump administration takes office for example.[1]. No. He said we have big picture stuff coming down the pike starting January 21. Like a flu wide spread in birds already, that jumped species already, that could develop human to human transmission. He framed it clearly as a present danger the incoming administration would inherit. Even without the removed context about Kennedy's claims no vaccines are safe and effective and vaccines cause autism. > The bird flu outbreak had been behind a lot of food inflation. I wouldn't put it past people on the radical left who want to hurt the Trump administration to hype the bird flu pandemic to drive up food prices through mass culling of livestock. Whoever the radical left are, they don't decide about culling livestock. What agenda do you think was behind the culls driving up food prices before the election? And do you expect tariffs and deportations to lower food prices? reply Jerrrry 4 hours agorootparentMost people support Trump overtly, even if they lie to themselves in the meantime. Cowardice kills, still too many people in polical limbo lala delusional land, tuning between TV channels to be told who to blame next. Trump is Clinton with balls. reply mike_hearn 3 hours agoparentprevNIH staff openly conspired against both the public and the President last time Trump was in charge in many different ways. What Fauci and his grantees did wasn't subtle, so the fixes aren't going to be subtle either: the NIH needs either to be abandoned entirely or it needs a massive purge and culture change. Fundamentally a civilized society cannot tolerate bureaucracies that act like they did during COVID. Fauci is gone now but unfortunately the NIH as an institution is deserving of any and all damage Trump does to them. Frankly if I were in his shoes I'd be going much further than mere freezes. reply rUsHeYaFuBu 18 hours agoprev> Even more troubling to many researchers is a pause on study sections that many received word of today. Without such meetings, NIH cannot make research awards. Cancer and many other topics of research will be hurt by this. reply PaulKeeble 17 hours agoparentCancer, infectious diseases, brain disorders, HIV and the RECOVER programme for Long Covid. All have significant NIH funding and will be halted by this. Lots of smaller funded conditions as well, the NIH does a lot both from a clinical research and a public health perspective. reply peppertree 18 hours agoparentprevThe first pro-cancer administration in history. reply jcgrillo 18 hours agorootparentSecond. They were just as anti-life the first time around just mercifully less competent. reply duderific 18 hours agorootparentI don't know if they are more competent now. There are just fewer people willing to push back, and few to no guardrails in place for when the leader doesn't adhere to the usual governmental and ethical norms. reply Jerrrry 4 hours agorootparentRewriting your comment with less overt political bias: The vocal minority have become reminded of their privileged status, and normality is returning to the social order. reply rUsHeYaFuBu 4 hours agorootparentNonplussed here. In what way is normality returning to the social order? Or are you pegging the 'normal' social order to some particular time in the past? reply Jerrrry 3 hours agorootparentWell yes, just as easily as the divisiness of the previous administration was deemed \"normal\". Turns out blue haired Kindergarden teachers discussing sexual matters with pupils isn't normal. BDSM publically heralded as \"bravery\" in the Whitehouse. The Democrats think rappers twerking was an acceptable fundraiser. At some point telling black Americans to shut up and vote for the black person is insulting. But, easier mentally to categorically define 65% of Americans as essentially retarded racists. So, the pendulum swang, again. reply rUsHeYaFuBu 3 hours agorootparentI'm pretty sure the current administration has already shown themselves willing to abandon many previous established norms formed over the history of the US so I am not entirely convinced this is returning to \"normalcy\". Well maybe authoritarianism is is the 'natural' place that the social order tends to. reply Jerrrry 1 hour agorootparentAbandoning weaponized political norms to salvage social norms is what both sides have done. Well maybe authoritarianism is is the 'natural' place that the social order tends to. It is. People tend to regress to tradition when sudden change upsends common order. Liberal terrorism (that we have seen) doesn't really change what people think just what they are allowed to express openly without fear of recorse. Reducing to atomics, society is composed of families. Families start with a father. It's that simple. reply krapp 3 hours agorootparentprevIt's weird that people like you use dyeing hair as some kind of shorthand for radical leftist ideology as if you stepped out of a time portal from the 1950s. Zoe Quinn really did a number on you guys, didn't she? reply Jerrrry 1 hour agorootparentBasic pattern recognition. The weirder question is, why the need to proclaim ideological association in a such an orthodox medium? Hint: it relates to sexual signaling. Which fits into the anti-normative anti-natalist sentiment those people are so infatuated with. Humans don't have blue hair naturally, just to make that clear. reply yongjik 10 hours agorootparentprevDemocrats need to put these messages out hard. If they win the 2026 election we might even have a chance at putting a brake on Trump's idiocy. (I know, I know, trust Democrats to shoot their own foot in the most crucial moments, but one can hope...) reply nradov 12 hours agorootparentprevThe position they've taken is that they want to shift cancer funding from treatment to prevention. I have no idea whether this makes any sense or if it will actually happen but that is the current messaging. reply greatpatton 10 hours agorootparentThere is no prevention strategy that will prevent all cancers. Even people with the most healthy lifestyle can get a cancer. That's why you have to do both. reply lII1lIlI11ll 10 hours agorootparentprevWouldn't that require... conducting research on cancer prevention? reply tired_and_awake 18 hours agoprev174 scientists either at the NIH or funded by the NIH have won the Nobel prize. https://www.nih.gov/about-nih/what-we-do/nih-almanac/nobel-l... Odds are if you or someone you know has been treated with ... Any kind of modern medicine ... You have personally been impacted by NIH. That ignores the epidemiological knock on effects that we all benefit from oh and the whole \"understanding of biological systems\". But screw it, they need to get in line with the party. reply dekhn 17 hours agoparentNIH also played a big role in the creation of biotech industry (funding much of the basic research that set the foundation for amazing medical treatments). I guess we'll have to depend more on the largesse of billionaires. reply chii 16 hours agorootparent> funding much of the basic research it's unfortunate that these funding don't actually bring in profits with which to maintain and continue future funding. It's why i somewhat dislike the publicly funded research model, because the commercialization of basic research is what leads to profit in the future, and this part is poorly done by gov'ts (or very well done by private parties looking to profit off public research). I say that to change the system, these basic research should have IP associated with it, by which if companies use it, they pay a royalty after they achieve profit of $X (where X can be decided based on the research itself). It's obvious that taxes aren't sufficient. reply kmos17 14 hours agorootparentNot everything needs to be profit generating, government funded research is how we get breakthroughs with very wide ranging benefits, especially when we are talking about advances in medicine. Profit making will often have the opposite effect by incentivizing the protection of existing monopolies, fake innovations to protect patentable revenue streams, and anti competitive regulatory capture. reply robertlagrant 8 hours agorootparentAlmost all of that is fixable by government. It doesn't have to be captured. It's paid out of taxes to not be captured. Monopoly prevention, same. Fake innovations the way to protect against them is to make sure there's an environment in which competitors can arise. reply kmos17 4 hours agorootparentIn theory yes, in practice in the US today it’s becoming more and more of a fantasy. I hope we can get back there one day, we have the constitutional framework to do so, but the current system is running fast in the opposite direction, one party is rabidly embracing deregulation which will exacerbate these negative points, and the other one failed to live up to its ideals because the entire political class depends on lobbying dollars and was too busy focusing on performative agendas. reply dekhn 16 hours agorootparentprevNIH-funded research generates IP owned by the university that contracted with the NIH to do the work. This is due to Bayh-Dole (https://en.wikipedia.org/wiki/Bayh%E2%80%93Dole_Act). reply Jerrrry 4 hours agorootparentprevWhat else did they play a big role in creating? Go ahead, pretend this isn't related to the bio weapon this agency allowed to be created and leaked. Trump loves cancer, nothing to do with the politically weaponized pandemic. reply grahamj 17 hours agoparentprevRemember when Idiocracy was just a movie? reply petesergeant 16 hours agorootparentI had a person on the other end of tech support tell me they’d _love_ to hear how my day was going as part of a script recently. Wasn’t Costco but still reply bbarnett 16 hours agoparentprevBut screw it, they need to get in line with the party. To be fair to both sides, I hear your right saying hiring has becone political. With DEI pledges for hiring. Hiring should be merit baed, and not based upon politics either. reply serf 16 hours agoparentprevwould you expect double the nobels if you doubled their available resources? another loaded question : do you believe the nih is the single government ran example of a perfectly lean and well managed agency without excess expenditure? reply notjoemama 16 hours agoparentprev> $47.4 billion U.S. National Institutes of Health (NIH) $47.4B is a significant amount of money. I don't know whether their expenditures are appropriate. I don't see that in the article either. Unless someone does know that can comment, the $47.4B is unaccounted for in coloquial dialog, is it not? > The hiring freeze is governmentwide > pause on communications and travel ... Such pauses are not unprecedented when a new administration comes in. Hmm, seem like the author is fomenting malice by using ‘devastating’ in the title. Perhaps building a character judgement that might not actually be there, helping to draw anger and hate from people already opposed to the new administration? > NIH travel chief Glenda Conroy sent an email to senior agency officials early today notifying them of an “immediate and indefinite” suspension of all travel throughout HHS with few exceptions, such as currently traveling employees returning home. Researchers who planned to present their work at meetings must cancel their trips, as must NIH officials promoting agency programs off site or visiting distant branches of the agency. “Future travel requests for any reason are not authorized and should not be approved,” the memo said. I guess someone needs to ask the question, how exactly is the NIH going to prevent people from \"going home\"? Does that mean simply that they will not be paying for their travel? Or for that matter, researchers who want to present their work must do it at their own cost or from approved unpaid time off? I feel like someone is forgetting how hard the MAJORITY of US citizens have it. Inflation has hit non-wealthy people the most. They don't have jobs where they get paid travel or paid time off. While I don't mean to inject some form of class into the discussion, I do wonder what exactly are the things to be fearful of in this scenario. I'm just not seeing a worrying concern here given reality. Unless, there's a more rampant amount of fragility in the well paid health community? I'm sorry. I just don't get it. reply UniverseHacker 16 hours agorootparent> the $47.4B is unaccounted for in coloquial dialog, is it not? Almost all of it is research grants for biomedical research, with priorities set by congress, e.g. they set what specific diseases, etc. should be worked on. This represents more than half of all funding for academic scientific research in the USA. > Unless, there's a more rampant amount of fragility in the well paid health community? Most of this work is done by graduate students and postdocs, which are paid very little. Traveling to conferences is part of doing their job, but they generally couldn't afford to pay for it themselves. They are already required to keep travel expenses down to the point where the hotels are often dirty and unsafe usually whatever is cheapest in town. Grad students in the USA currently make about $34k/year and postdocs (with PhDs) about $60k/year in the USA. They're usually in high cost of living urban areas, and in practice, they're often expected to work 60-80 hour weeks if they want to produce enough to remain in academia. This works out to less than minimum wage in the places they are generally located. When I was a graduate student, I made just enough to rent a single bedroom in a large group house full of strangers, and as a postdoc I had a 4 hour round trip daily commute to get to someplace I could afford to live. reply notjoemama 15 hours agorootparentThanks for the information. It helps. People like me just don't know. reply AlotOfReading 16 hours agorootparentprevHave you ever worked in a government funded environment? Travel frugality is already the rule and has been for decades. This is going from \"be frugal\" to \"don't\". reply dekhn 16 hours agorootparentprevNIH pays for work travel. When a person is paid for NIH travel, it's bare bones (cheaper hotel, cheaper tickets). It's for work you go, you work many long hours drinking shitty coffee. Maybe at the end of the day there is some fun at a restaurant/bar but it's not paid for in the per diem. It was brutal typically, if I travelled on NIH or NSF or DOE dime, it was a red eye from California to Washington DC, take cheapest possible transport to the NIH offices, then turn around and return the same night, so that I could go to work the next day (to be productive with little sleep so I could keep my job). As for \"non-wealthy\": most scientists are not well-compensated. They spent their 20s and 30s working for very little pay (for example, in grad school my pay was $25-33K/year in San Francisco, and even as a Staff Scientist at a national lab, there's no way I could afford to buy a house in the area). They work punishingly hard jobs competing with super-ambitious people for fairly small amounts of money. I don't really see what your point is; breaking the NIH is not going to fix wealth disparity in the US. reply notjoemama 15 hours agorootparentThanks for the information. It helps. People like me just don't know. > breaking the NIH is not going to fix wealth disparity in the US I didn't say or imply that. I'm sorry if that's how it sounded. I've traveled for work both paid and not, both with implicit and explicit frugality. I'm not seeing how a pause on paying for travel, as the norm quoted within the article, is an egregious practice, nor how it could break the NIH. Especially given it appears to be a norm when administrations turn over. reply dekhn 15 hours agorootparentThe propaganda of Trump's people has been very effective. They managed to paint a bunch of people who work hard for the US and aren't particularly well-compensated (compared to what tthey could get in industry) are some sort of \"elites\" who are actively working to keep conservsatives down. The way to look at it is like this: even if previous administrations froze travel for a bit (I don't recall that ever happening; a bigger impact was when the state or the federal government failed to pass a budget. That would break everything for weeks to months), the intent of the Trump administration is to harm the people they don't like. This freeze is only an opening move; we can expect to see far more dire and serious attempts at damaging our valuable public institutions (which fairly serve liberals and conservatives alike). I appreciate you saying \"people like me just don't know\". It's rare that people will actually listen to the other side and admit they lacked perspective. reply tptacek 18 hours agoprevHere's a really excellent piece on the guts of how NIH's processes work: https://theinfinitesimal.substack.com/p/distinguishing-real-... reply duxup 18 hours agoprev> Another consequence of the communications pause, according to an NIH staffer involved with clinical trials at NIH's Clinical Center, is that agency staff cannot meet with patient groups or release newsletters or other information to recruit patients into trials. Another unknown is whether NIH researchers will still be allowed to submit papers to peer-reviewed journals. That seems unnecessary at best. reply arcticbull 18 hours agoparentNone of this is necessary, that's the point. reply johnnyanmac 17 hours agorootparent>Officials have also ordered a communications pause, a freeze on hiring, and an indefinite ban on travel. I thought trump was bringing American jobs back? So far he's impacted thousands, maybe even hundreds of thousands of jobs in 2 days, and hasn't even made a plan for how to get americans hired. (yes, a miniscule piece of me is enjoying how fast the Schedenfreude came. But still, I do mostly want to focus on fixing the country over pointing fingers). reply chii 16 hours agorootparentpeople who voted for trump is not going to change their mind based on evidence. They're essentially voting on faith, it's almost like a religion. reply tayo42 14 hours agorootparentI guess there's hope for the tens of thousands that decided to sit out this time too. reply sebazzz 13 hours agorootparentprevIf that is the case, is there any way back from Trumpism to the \"normal\" Republican party from the era of Bush? reply vkou 12 hours agorootparentShort of civil war, nuclear war, or a twenty-point election blow-out, no. reply Dalewyn 16 hours agorootparentprevTrump just mandated a legally binding and fundamental shift from equity-based hiring (eg: affirmative action, DEI, et al.) back to merit-based. This by its very nature necessitates a temporary freeze on all human resources activites pending reviews to see if any as they currently stand violate equal opportunity guarantees and other requirements based on objective factors. Personally, and I say this as a minority (Japanese-American male), I 300% approve of all this. People must be hired based on their character and capability, not because they happened to be born with the right skin color or sexual organs. reply nullocator 12 hours agorootparentAs someone with a fair amount of experience on hiring review boards in the public sector (local and federal), I felt what many of these programs/approaches did was ensure that minorities were given a better opportunity to be evaluated based on their merits. My expectation now is that if someone is a minority they're going to have a much harder time getting a shot to demonstrate that character and capability at all, maybe not though, time will tell. Perhaps for some it's viewed as a positive if these groups are underrepresented in terms of percentage of federal employees as long as their roles are filled by someone perceived to be more capable with stronger character (weird how frequently that's a white guy these days). reply Eextra953 13 hours agorootparentprevThe war against DEI is clearly over so not that it matters anymore but I have to push back every time I see this sort of comment. DEI, as it relates to hiring, does not and has never meant that people are hired because of their sex or skin color. What it means in practice is that organizations cast a wider net in the search for, and I can't stress this enough, qualified applicants. This wider net has proven to bring forth better candidates to organizations. Also, casting a wider net opens the door for people who are white/asian but come from less traditional backgrounds and/or less prestigious schools to apply as well. Please read up on what DEI truly is before applauding its demise. As scientist, engineers, programmers, and technologists (I assume you are one of those if you are in this forum) we can't blindly believe the propaganda of either side of the political spectrum. In this case, a coordinated effort has deliberately mischaracterized DEI and merit-based hiring as opposites. They are not opposites. reply Dalewyn 13 hours agorootparent>qualified applicants. The problem is what \"qualified\" means under Affirmative Action, DEI, et al.. As a Japanese-American, I've both first and second-hand witnessed racism against Asians (and Whites) in favor of Blacks and Hispanics in the name of Affirmative Action. That is racism and discrimination in both ways and there have been court rulings prohibiting Affirmative Action, I in absolute terms cannot in good faith accept Affirmative Action. The \"D\" in DEI stands for \"Diversity\", which in the majority of cases meant hiring people with stronger weighting placed on their race or nationality and sex. This again is very literally racism, sexism, and discrimination and completely unacceptable. The \"E\" in DEI stands for \"Equity\", which in clearer terms means Equal Conclusions, not Equal Opportunities as indirectly declared in our Declaration of Independence (all men created equal with an unalienable right to pursuit of happiness). This is an affront to one of the very cornerstones of our country and the American Dream, which is completely unacceptable. Lastly the \"I\" in DEI stands for \"Inclusion\", which would be fine were it not for the fact that the actual implementation has involved excluding peoples who do not subscribe to certain narratives. Nowhere has this been more obvious than in politics and specifically identity politics, where the Center and Right simply call me an American but the Left call me various labels. I very sincerely don't have time for that divisive bullshit. As someone who loves and appreciates science and technology and all the wonders we can accomplish, I reiterate my 300% support for the absolute rescinding of equity-based hiring and other equity-based human resources programmes. As Martin Luther King once said, I too dream of a day when everyone will be judged by the quality of their character. I dream of a day when everyone will be judged by what they are capable of, not what they are born as. reply btreecat 6 hours agorootparentYou experienced racism and are calling it DEI reply zo1 5 hours agorootparentDEI and other such affirmative action concepts are racism. reply zo1 5 hours agorootparentprevI too have to push back, because I have been the victim of these laws and profiling, and I'm not an \"ignorant\" person falling for propaganda as you seem to imply is the only way for anti-DEI sentiments to occur. DEI is ideology and hidden threats of retaliation under the guise of \"equality\". And because nothing can be explicit, or it's all hidden under feel-good language (exactly the kind you used in your post). It adds a layer of redirection about what's really going on, and it all gets put forth with everyone having the understanding (because of examples) that if they don't find \"diverse enough qualified\" candidates, then they are racist and will be punished. Meanwhile, the language provides cover for blatant anti-white, anti-asian and anti-male discrimination: https://www.cnbc.com/2021/06/06/tech-industry-2020-anti-raci... They even have specialized software that uses AI to determine race (profiling?): https://www.piratewires.com/p/gem-hr-software-dei Or what about ESG that has a racial-quota point system that allows investors and people to discriminate against companies that don't conform? https://corpgov.law.harvard.edu/2021/10/30/racial-equity-aud... This kind of stuff is just the tip of the iceberg, there is more: https://www.bloomberg.com/company/press/bloomberg-2023-gei/ What about the fact that several countries (Western) have explicit gender-based quotas enacted as law. E.g. Norway, Germany, France, India, and others. Or what about countries that have actual race-based points systems to discriminate against minority (white) groups: https://en.wikipedia.org/wiki/Black_Economic_Empowerment And no, just because it's some obscure and obfuscated \"points system\" doesn't mean it isn't racial discrimination, profiling and retaliation against companies that don't conform. How some places in the US have race-quotas for boards: https://pacificlegal.org/discriminatory-mandates-prevent-qua... And there are so many more, it's right infront of our eyes: https://www.msn.com/en-us/money/companies/red-state-sues-tec... So why don't we drop the pretense, ordinary people see what's really going on, and no amount of sugar-coating and \"feel good\" words will make the very real discrimination go away, and they won't be gaslight and have their concerns waved away because of \"falling for right wing propaganda\" and \"please read up on what DEI truly is\". reply ashoeafoot 11 hours agorootparentprevits a race based caste quota system in reality with a priesthood that accidentally spread racism. It produces the same results reliably as real world socialism. No need to read up on idealistic texts which produce the same outcome with the given real humans. reply DemocracyFTW2 10 hours agorootparentprevExcept of course what you will now get is jobs preferentially for those who are white, male Trump loyalists. You're 300% delusional when you think there's any chance that from now on \"character and capability\" will be the relevant hiring metrics. Just have a look at the assortment of clowns, creeps, conmen and criminals that is Trump's cabinet. reply UltraSane 15 hours agorootparentprevIt is a exercise of power for the sake of power. The pointlessness of it is the point. reply zzzeek 17 hours agorootparentprevThe point is to create chaos and panic so that , after a sufficient period of general suffering, the government can come back with a \"solution to the problem\" which is typically some vastly inferior approach with heavy privatization and corruption built in, but as the stakeholders are desperate for relief they have no choice but to accept. This is how you tear the government down and replace it with corrupt oligarchic interests reply CuriouslyC 17 hours agorootparentJoke's on them, they're just going to hasten a revolution. reply Tostino 16 hours agorootparentI hear Mario Party has been pretty popular recently. reply JeremyNT 14 hours agorootparentprevExcept that this is the kind of governance that American voters chose. They can't claim to have been misled. They explicitly voted for oligarchy and this is what they asked for. If you're waiting for American voters to wake up and demand good governance you'll be waiting a really long time. reply CuriouslyC 6 hours agorootparentI don't expect voters to wake up, I do expect the assassins of the rich and powerful to become folk heroes along with riots and vandalism of corporate property, and when the government tries to crack down on it, against the government as well. reply idle_zealot 16 hours agorootparentprevThat's why they're partnering with Oracle and AI companies to build datacenters to pull in and automatically analyze all available private and public video feeds and communications to monitor for potential violent action. China got there first, the US will rapidly roll out the same playbook but painted with liberal terminology. It's not about harmony here, it's about freedom and protecting ourselves from terrorists and the radical left. reply pseudalopex 12 hours agorootparent> That's why they're partnering with Oracle and AI companies to build datacenters to pull in and automatically analyze all available private and public video feeds and communications to monitor for potential violent action. Larry Ellison said it on record if anyone doubts it. \"Citizens will be on their best behavior because we are constantly recording and reporting everything that's going on.\"[1] [1] https://arstechnica.com/information-technology/2024/09/omnip... reply Tostino 4 hours agorootparentI feel like his yacht shouldn't be that hard to track... What a dystopian hellscape we are approaching. reply CuriouslyC 3 hours agorootparentprevThe oligarchs are playing a losing game in the long term, they'll always have to rely on people to help them, and it's too easy to kill someone if you're willing to sacrifice your life to do it. reply philipov 17 hours agorootparentprevNow let's go break some windows! reply ashoeafoot 11 hours agorootparentprevShocktheraphy as seen in Iraq reply crooked-v 18 hours agorootparentprevnext [11 more] [flagged] khazhoux 18 hours agorootparentPeople love that line, but is it true? The premise of this freeze agree or not is that the U.S. Government spends too much. Or are you suggesting that this is to get back at NIH because of Fauci? Or, are you saying that Republicans hate cancer research, or...? reply doctoboggan 17 hours agorootparent> Or are you suggesting that this is to get back at NIH because of Fauci? Yes, I believe thats a main driver for why this administration does not like the NIH reply moshun 17 hours agorootparentprevConsidering we had over a hundred million Americans fall ill to a global pandemic that ended barely two years ago, one might conclude that the only reason to hobble the groups that are intended to fight such outbreaks (among many other tasks) would be a poor decision. In fact, one could conclude based on the literal rhetoric from the President and members of his staff, that it’s out of sheer spite. Which itself is amazing, considering that he was in charge while it happened. reply Ar-Curunir 17 hours agorootparentprevIf you still believe the republican have principles that they actually believe in, I have a bridge to sell you reply doomroot13 18 hours agorootparentprevI think arcticbull is saying NIH is not necessary, essentially in favor of the freezes. reply arcticbull 18 hours agorootparentI am not in favor of shuttering one of the agencies responsible for public health, although I can see why my comment could be read either way. If you believe in reform there are compassionate ways to do it. reply tptacek 18 hours agorootparentCDC probably does more public health? The work NIH funds through its grant process is not broadly \"public health\"; it's virtually all American biotechnology research and bio/biochem basic research. Public health deals with the interface of health knowledge and public policy and communication; people credentialed in public health are not, generally, practicing scientists as we'd think about them. NIH grant researchers, on the other hand, are wearing white lab coats and working with fume hoods. reply dekhn 18 hours agorootparentNo, HHS does the vast majority of public health research in the US, which includes both CDC and NIH. The CDC budget is roughly a quarter of the NIH budget; most public health research is NIH-funded. The CDC is more operational they take what is learned and apply it to prevention and treatment. reply tptacek 17 hours agorootparentIt may in fact be the case that more HHS-funded public health work is downstream of NIH than of CDC, but NIH is obviously not primarily a public health organization; here's the most recent RePORT data: https://tinyurl.com/nih-grant-report reply pseudalopex 12 hours agorootparent> NIH is obviously not primarily a public health organization They didn't say it was. Or did they edit? reply monero-xmr 18 hours agorootparentprevThe scientific establishment has had 2 major crises recently the replication crisis, and the totally insane unscientific and politicized handling of COVID. Additionally there is undeniably a lot of waste, nepotism / back scratching, and completely ludicrous funded social “science”. While what’s happening now may wind up bad, I have seen very little push from the establishment itself to do anything to restore public faith in the current system. Trump is the executioner, but the conviction was handed down before he came around. @dang why is this flagged? reply tptacek 18 hours agorootparentThe replication crisis stuff is mostly a message board fixation. The problem we have is that people unfamiliar with the field don't have a sense of what the denominator is, only the numerator. There is a truly gargantuan amount of NIH-funded research happening; NIH funds over 30,000 PIs per year, and those grants cover years worth of research, most of which involve teams of 5-10 people. I'm not saying research fraud doesn't matter or isn't worth the stories written about it. I'm saying that people commenting on it generally don't have any sense of its scale, and fill in some very weird blanks about it. reply llm_trw 17 hours agorootparentIn ml basically every paper from a Western university is either useless to industry or not replicable. It is very much a crisis and one that academia has no interest to solve. In a sane world we'd start with figuring out what the mystery meat in the version of pytorch used for training is and work up from there. There is zero funding for building up the required infrastructure for doing actual science instead of whatever the fuck you call the current mess. Basically anything on a more complex dataset than cifar is probably a statistical fluke because of hyper parameter tuning. To find this out you will need to spend three months building up all the bit rotten code that doesn't work from the paper. To draw the conclusion that everyone is full of shit you'll need to waste years of your life. reply light_hue_1 17 hours agorootparentI'm an AI/ML researcher. If this was true, then we would never have made progress in the first place. Instead, we're making astounding progress at a rate that no one really would have expected. Learning to replicate papers is a skill. It's just a matter of practice. You'll get it if you try. reply llm_trw 17 hours agorootparentThe progress isn't happening at Western universities. reply 331c8c71 13 hours agorootparentWhy you keep emphasihing the \"Western\" part? Non-western are any better? reply llm_trw 13 hours agorootparentYeah, I can replicate work out of a paper quite easily if half or more of the contributors have Chinese, Indian or Russian email addresses. A lot of small to medium sized companies fighting for mind share also release one script deploy pipelines for their work, which are better than the academic releases. It's western universities and big tech that releases at best bit rotten code and at worse no code what so ever. reply light_hue_1 8 hours agorootparentprevTo a first order approximation all of the progress is happening at Western universities and the companies they founded. Everyone else is playing catch-up. It's hard to even name mediocre advances that come from anywhere else. As well as the entire hardware and software stack that we have. You clearly have some sort of political agenda here and not a technical argument. reply jghn 17 hours agorootparentprevNot directly tied to GP's complaints but another thing that often comes up are people complaining about the frivolity of the research without understanding how basic science works. \"Can you believe we're paying money to understand fruit fly mating behavior!?!\", well yes, because fruit flies are model organisms and developing an understanding of these very mundane sounding things are important before being able to make broader hypotheses reply ashoeafoot 11 hours agorootparentprevIf you cant replicate foundational papers your faculty is a house of cards built on lies. reply monero-xmr 18 hours agorootparentprevI think something like a very public, televised shaming of researchers found to be fraudulent. Furthermore far, far more done to catch fraud before it is published, like bounties paid out, and instant career destruction to the fraudsters. There was decades of “but everyone is doing it!” in the system, so much so that fraudsters became presidents of Ivy League institutions. The reckoning has not come. reply tptacek 18 hours agorootparentThis is what I mean by how messageboardy this issue is. reply monero-xmr 17 hours agorootparentWhat is your solution to prevent continued fraud and abuse in our research process, other than random people going through published papers? reply tptacek 17 hours agorootparentThe optimal amount of fraud in any large system is almost never \"zero\". reply monero-xmr 17 hours agorootparentYes I will stick by my opinion we should have scientific fraudsters go on a televised panel to be shamed publicly and mercilessly reply tptacek 17 hours agorootparentSounds awesome, maybe MrBeast can host. reply dullcrisp 17 hours agorootparentMrBeast has important things to do. reply lamename 17 hours agorootparentprevSure. As long as all the other fraudsters and science deniers that were just elected or waiting to be appointed go too reply fzeroracer 17 hours agorootparentprevI hope the deep irony of someone whose named after and advertises a cryptocoin talking about fraudsters being shamed publicly on televised panels isn't lost on you. reply totallynothoney 1 hour agorootparentprevMaoist struggle sessions, now with shitcoins! reply renewiltord 17 hours agorootparentprevHave you read this post about p-value suspiciousness as a means of testing for widespread non-replicable results? https://www.cremieux.xyz/p/ranking-fields-by-p-value-suspici... And that's not to speak of things like biotech which is full of photoshopped images of fluorescence and whatnot. reply tptacek 17 hours agorootparentI've been on a messageboard for more than a couple years, I am familiar with the concept of p-hacking, and it has nothing to do with what I just wrote. reply renewiltord 17 hours agorootparentThe claim in the post is that the replication crisis is real. Of course, it's among those you consider messageboard folk so you can dismiss it on those grounds, but I think you are wrong to say it has nothing to do with what you wrote since it attempts to deal with both the numerator and the denominator by taking a sample across fields. reply tptacek 17 hours agorootparentI don't know what point you're trying to make here. Is it that there are fraudulent research papers in journals? Everybody knows that's true. Is it that there are tens of thousands of them produced every year? That's made up. People are counting on readers not to know or take the time to learn how much research is happening, so that individual instances of the problem (in the United States) seem systemic. reply renewiltord 17 hours agorootparentThe post makes the point that a large amount of science produced in the US is not trustworthy. Fraudulence implies intentional deception, but it is more likely that it is simply a response to incentives. I don't see why you can't understand what I'm saying, but in the hope that it's easier I'll put it in bullet points and try to be direct * The post is a sample of papers across many fields * It describes a method to determine whether widespread p-hacking is occurring in a field * Widespread p-hacking in a field will lead to unreproducible results (because the results are an artifact of the data rather than reflecting any underlying cause) * You said: The replication crisis stuff is mostly a message board fixation. The problem we have is that people unfamiliar with the field don't have a sense of what the denominator is, only the numerator. * The approach in the post uses a statistical method across many papers across many fields. It does not rely on the existence of some number of non-replicable results (in fact, one should expect some percentage of results to be non-replicable). * Therefore, it doesn't rely on \"the numerator\" so to speak, but a sample of all papers in a field to characterize the field's replicability. You're a smart guy. I follow your posts here and on Twitter. So I can only assume that either I've flipped some bozo bit or something in my communication style is abstruse. I don't understand why you're saying that p-hacking has nothing to do with the replication crisis (most would expect the two to be quite related), or that saying \"the replication crisis is real\" means I am claiming that individual papers exist that are non-replicable (which is fine, but not a replication crisis). In either case, we're clearly getting nothing out of talking to each other so I might as well leave off here. reply tptacek 16 hours agorootparentI think if the claims that article makes are both (i) true and (ii) meaningful (as opposed to an annoying artifact of the norms of how these fields operate) we'd be seeing downstream effects of it that we do not see†. I think this is a particularly weird time in the history of biomedical science to be trying to further the argument that everybody is adding cards to a giant house of cards, but you do you. Either way: people are responding to specific stories about fraudulent research, and my point is that the numerator of those stories is infinitesimal compared to the denominator of how many papers are published in these fields annually, by how many separately-funded R1 research teams. I'm really not interested in being boxed into a debate about whether p-hacking matters, or scientific fraud exists. Fraud is bad! I'll just keep reiterating that the optimal level of fraud in a large system is almost never zero. † That article is about a bunch of different fields, and I'm commenting only on a very specific subset of them; fuck do I know how well social science is operating right now reply ziddoap 16 hours agorootparent>I'll just keep reiterating that the optimal level of fraud in a large system is almost never zero. I've been trying to think about what you meant since you wrote this earlier in another comment, and I'm having some trouble wrapping my head around it so I think I'll just ask (if you'd be so kind to explain). What makes some level of fraud more optimal than zero fraud? reply tptacek 16 hours agorootparentPatrick McKenzie popularized that wording, but I think he may have taken inspiration from Dan Davies, and the idea is older still. It's simply that fraud is endemic to large systems, and past a certain threshold eliminating it makes people worse-off on balance as the countermeasures retard the efficient operation of the system. You can get to zero fraud in lending easily: just don't make loans. Grinding all scientific research to a halt to \"true up\" the statistical reporting used for otherwise valid and important research results would be a very good example of an intervention that would (a) \"reduce fraud\" and (b) leave us all significantly worse off. reply defrost 16 hours agorootparentI'm in agreement with tptacek and P.McKenzie and feel it's something more generally true; perfection is the enemy of throughput. Whether it's mine processing, agriculture, bank lending, { insert domain } there are always a number of drags on profit ( fraud, equipment wear and tear, petty theft, holes in fences ) and generally always a threshold past which the cost of pursuit of perfection exceeds the decreasing marginal benefit. reply ziddoap 16 hours agorootparentprevThat makes a lot more sense than the literal interpretation. Thanks. reply madhadron 18 hours agorootparentprevHere speaks someone with no scientific training or knowledge of the NIH, public health, or how any of it operates. reply monero-xmr 18 hours agorootparentnext [3 more] [flagged] redcobra762 17 hours agorootparentCan you elaborate on \"years of failure\"? reply llamaimperative 17 hours agorootparentprevwhat? I cannot even parse this argument. reply _DeadFred_ 17 hours agorootparentprevWait until you learn about the first unscientific politicized traitor to Freedom in the USA. General George Washington, having survived smallpox himself, ordered the mass variolation (via inoculation, much more dangerous than a vaccine) of the Continental Army to protect troops from the disease, marking one of the first large-scale public health initiatives in American history. reply BurningFrog 17 hours agorootparentPretty sure that would bee illegal in several ways today. reply Jtsummers 17 hours agorootparentMembers of the US military actually get more vaccines and, until recently, had fewer protections with respect to declining them than the typical US citizen (specifically with COVID, Trump has indicated, not sure if he's done it yet, that he'll reinstate people who refused a lawful order to be vaccinated for it; so now I guess they can just refuse any vaccines even if it impacts readiness). During the run up to the wars in Afghanistan and Iraq, US service members were vaccinated for smallpox (appropriate for this thread). That was this century, 30+ years after smallpox was declared eradicated. They were also vaccinated for other diseases. Join the military and you sign away a lot of your rights (arguably for some good benefits, but we'll see how the new VA head and others treat the veterans and current troops). reply refurb 17 hours agorootparentprevInoculating the military isn’t public health. Nobody would call today’s military vaccines a “public health program”, its military readiness. The word “public” is key. You know, the overall population. reply light_hue_1 17 hours agorootparentprevCOVID was not a scientific crisis. It was an amazing success. We built and rolled out a vaccine in record time. Science works. Amazingly well. We're making progress on cancer, on many disorders that used to kill or limit many people's lives. Instead of actually looking at what science does for you constantly, every day, you prefer to attack people. The result will be tragic. Realistically, your loved ones will die of heart disease or cancer. Sounds like a really smart move to not fund science that will save their lives. Much better to support Trump and save up for their funerals! reply cruffle_duffle 18 hours agorootparentprevnext [8 more] [flagged] LeafItAlone 18 hours agorootparent>Every single person responsible for how the government handled COVID needs to be fired. Where do you stop? Because the administration overseeing the start of it just got re-elected. reply peterashford 18 hours agorootparentprevWhich part of the response? Vaccines and masking were effective. Or was there something else you were referring to? reply arcticbull 17 hours agorootparentWhat's really cool is seeing the giant drop-off in flu rates during the lockdowns. Flu infections fell 64% and at least one strain went extinct. I'd link to the NIH publication but I'm not sure it'll stay up, lol. [1] [1] https://hsph.harvard.edu/news/a-sharp-drop-in-flu-cases-duri... reply avs733 18 hours agorootparentprevWe just rehired the guy in charge…minor observation I’m sure. reply fzeroracer 17 hours agorootparentprevSo Trump belongs in jail then? Or do you have a special carve out for the president that oversaw this? reply icameron 17 hours agorootparentprevI’m confused by this sentiment. How was science at fault for the collective government response to a global pandemic when the hospitals were full, millions died, and countries closed their borders in attempts to slow the spread? With hindsight you may find some poor choices but at the time, what could a scientist have done differently in your opinion? reply dekhn 17 hours agorootparentThere is an entire litany of complaints that I've seen. It typically centers around \"we shouldn't have funded this virology research in the first place\" and \"the government should have been totally transparent about every single piece of knowledge it had, and clearer about things that were ambiguous\" and \"it was a mistake to lie about masks not working to save masks for health workers\" and freedom of choice around vaccination. Doing public health at scale is incredibly hard. Doing so in a politicized environment with active disinformation is even harder. I'm not going to fault the government response. By and large it did what I expected it to do. Ideally, there would be an honest postmortem and we'd improve our techniques for the future, but I worry that won't every happen again. reply kurthr 17 hours agorootparentprevYeah, sounds right. Let's list the unnecessary drug treatments funded directly by NIH. The treatment of HIV with AZT and other HAART drugs. AIDS is caused by \"poppers\" not HIV, duh! Treatment of breast cancer and Myeloid Leukemia and with Herceptin and Imatinib. Fake cancers! Vaccines for HPV and Corona viruses are useless, because horse de-wormers treat both! Also, they're not real because COVID and cervical cancer are harmless. It's time to stop the insanity and realize that jade eggs in your bum are the real solution. Other useless drugs funded by NIH: Depo-Provera, Taxol, L-DOPA, Propranolol, Tagamet, Embrel, Tamoxifen, Cyclosporin, Warfarin, Methotrexate, Hydrocortisone. reply travisporter 17 hours agorootparentTruly sad that I believed you were being genuine for a second reply groovy2shoes 16 hours agorootparentPoe's Law strikes again. reply kurthr 16 hours agorootparentprevSee, there's my mistake. You could tell. I'll try harder to appear genuine. reply tptacek 17 hours agorootparentprevThey weren't saying the NIH was unnecessary, but since that thread got flagged, it's not easy to see their clarification. reply schmookeeg 17 hours agoprevI assume the NIH was singled out as some sort of vengeance for the CV19 response? Hopefully they'll restore some semblance of order after the pound of flesh is theatrically exacted. reply ericd 16 hours agoparentSounds like it's happening in at least some other parts of the government, too. This order seems to have paused disbursement of even some already-committed funding: https://www.whitehouse.gov/presidential-actions/2025/01/unle... \"Sec. 7. Terminating the Green New Deal. (a) All agencies shall immediately pause the disbursement of funds appropriated through the Inflation Reduction Act of 2022 (Public Law 117-169) or the Infrastructure Investment and Jobs Act (Public Law 117-58), including but not limited to funds for electric vehicle charging stations made available through the National Electric Vehicle Infrastructure Formula Program and the Charging and Fueling Infrastructure Discretionary Grant Program, and shall review their processes, policies, and programs for issuing grants, loans, contracts, or any other financial disbursements of such appropriated funds for consistency with the law and the policy outlined in section 2 of this order. Within 90 days of the date of this order, all agency heads shall submit a report to the Director of the NEC and Director of OMB that details the findings of this review, including recommendations to enhance their alignment with the policy set forth in section 2. No funds identified in this subsection (a) shall be disbursed by a given agency until the Director of OMB and Assistant to the President for Economic Policy have determined that such disbursements are consistent with any review recommendations they have chosen to adopt.\" reply travisporter 17 hours agoprevMy issue is more with the CDC. Are there state institutions that can take up the slack locally? Asking for a friend with kids who wants to move to a place with a lower risk of getting measles reply WesternWind 18 hours agoprevPretty sure a bunch of statistics that might be used to argue against Republican talking points are going to disappear or not be updated. Maybe they won't even get to be collected. It's a lot easier to lie if you prevent scientists and health care professionals from undercutting you with inconvenient truths. As someone who has worked in public health and epidemiology, this kind of open ended restriction is extremely concerning. Also appears to undercut the whole free speech thing that President Trump supposedly supports, and that the constitution provides for in the first amendment. reply linsomniac 18 hours agoparent>statistics that might be used to argue against Republican talking points are going to disappear So, you're saying the WHOLE map will be drawn with Sharpie? ;-) reply UncleOxidant 18 hours agoparentprev> Also appears to undercut the whole free speech thing that President Trump supposedly supports Saying he supports \"free speech\" and actually doing it are two different things. In reality he supports speech that supports his preferred narrative of reality and opposes speech that doesn't. reply duxup 18 hours agoparentprevFree speech, states rights, government accountability, any serious sense of libertarianism and so on are all with the caveat of “if I like it” as far as the current Republican Party is concerned. reply 762236 18 hours agoparentprevIf you've worked as a scientist, why not gather evidence before jumping to conclusions? For example, you could inquire of the administration why they did this. They're actually putting in quite an effort at transparency. reply WesternWind 17 hours agorootparentMy concerns about the future are based on what President Trump did in 2017 with the NIH budget (attempting to cut it 22%)¹ thus preventing important scientific research funding, as well as removal of covid-19 health statistics from the CDC in 2020² in the middle of a major pandemic. Unfortunately, given that he is now holding the highest office in the US, it's a well known fact that Donald Trump lies constantly both when in office³ and when campaigning⁴, and I lack all faith in his administration's supposed transparency. 1.https://www.science.org/content/article/nih-plan-reduce-over... 2.https://www.washingtonpost.com/health/2020/07/16/coronavirus... 3. https://www.cnn.com/2021/01/16/politics/fact-check-dale-top-... 4. https://www.salon.com/2024/11/09/six-big-lies-that-won-the-e... reply ayakang31415 18 hours agorootparentprevBecause for some people, this is not a scientific peer-reviewed discussion, but rather typical conversation normal people have over dinner table with gut feelings and limited information. Just assume that \"I believe\", \"in my opinion\", \"AFAIK\" are implied for statements like above. reply sega_sai 16 hours agoprevI am waiting to see what will happen with NASA, NSF... Obviously it's less important than health, but still thousands of people, lives. reply 1oooqooq 16 hours agoparentif musk et al cannot fill all the positions they have open with people leaving NIH, be sure NASA is certainly next. reply kazinator 17 hours agoprevI can fix it: s/NIH/DOD/ :) reply readthenotes1 14 hours agoprevKinda telling the NIH to mask up HHS suspended their association with the Eco health Alliance, so I'm not sure what vector is of worry. https://oversight.house.gov/release/breaking-hhs-formally-de... https://www.science.org/content/article/nih-says-grantee-fai... reply trhway 16 hours agoprevMy guess is what happening is influenced by and patterned after the Musk's Twitter initial period getting rid of what Musk didn't like from the start, review and cuts/layoffs of what didn't pass the review. With all the due respect to the science having been done at/with NIH, one can suspect that the bureaucracy there is out of control similar to what we see at academia. The huge sign that the things got really rotten is that NIH couldn't own its work in Wuhan on the coronavirus, and that Fauci needed preemptive pardon. So some dead tissue debriding seems to be in order. reply nojvek 3 hours agoprevLikely many folks who depend on funding from NIH voted for Trump. Feel sorry for those who didn’t vote for him. No sympathy for those who did. reply sandinmyjoints 1 hour agoparentCan you be more specific who are you thinking of? reply fungiblecog 16 hours agoprevThe USSR wasn't doomed by communism per se. it was doomed by prioritising ideology and politics over any other considerations. The US is now in the same doom-spiral. reply aksss 16 hours agoprev> \"The hiring freeze is governmentwide, whereas a pause on communications and travel appears to be limited to the Department of Health and Human Services (HHS), NIH’s parent agency. Such pauses are not unprecedented when a new administration comes in.\" reply guelo 18 hours agoprevIt's scary to think that Trump seems to actually believe right wing media's black and white propaganda. The end point of this kind of anti-intellectual, anti-urban movement is something like Pol Pot's killing fields. reply A4ET8a8uTh0_v2 18 hours agoparentDoesn't it seem a tiny wee bit like a hyperbole? reply UniverseHacker 17 hours agorootparentWhat I think we're re-discovering is that it's not so straightforward to tell while it's happening. I'm willing to bet during the rise of people like Mussolini, Pol Pot, Stalin, Hitler, etc. many people were concerned, and many others were saying it was nothing and they were just being hyperbolic. However, it was the same with times when it didn't turn out like that. What concerns me most about this, is the general attitude of entirely dehumanizing specific groups of people. That, I expect, is the crux that makes atrocities possible, and it is happening now. reply pseudalopex 11 hours agorootparent> I'm willing to bet during the rise of people like Mussolini, Pol Pot, Stalin, Hitler, etc. many people were concerned, and many others were saying it was nothing and they were just being hyperbolic. Yes. A German Jewish newspaper published this.[1] \"We do not subscribe to the view that Herr Hitler and his friends, now finally in possession of the power they have desired for so long, will enact the proposals circulating in the Angriff or the Völkischer Beobachter newspapers; they will not suddenly divest German Jews of their constitutional rights, lock them away in race ghettos, or subject them to the avaricious and murderous impulses of the mob. They not only cannot do this because many other crucial factors hold their powers in check, ranging from the Reich president to some of the political parties affiliated with them, but they also clearly do not want to go this route, for when one acts as a European world power, the whole atmosphere is more conducive to ethical reflection upon one’s better self than to revisiting one’s earlier oppositional role: operating as a European world power means that one seeks an enduring place in the harmonious exchange of peoples of culture. And beyond that, it is clear that the powers at Wilhelmstrasse no longer see demagogic appeals designed to heat up mass gatherings of the Volk as strictly necessary. The new Prussian Minister of the Interior [Hermann Göring] can perform a far greater service to the old comrades in arms and party friends by rejuvenating the huge, state civil service along National Socialist lines than by making open concessions to the brutal manifesta tions of hatred of Jews.\" [1] https://www.ushmm.org/m/pdfs/20091110-Matthaus-Ch-1.pdf reply UniverseHacker 5 hours agorootparentWow that is really something else, and closely echoes what people are saying nowadays. reply krapp 5 hours agorootparentprevI wonder if a term like \"Hitler Derangement Syndrome\" was popular at the time. reply int_19h 16 hours agorootparentprevIt was exactly like that with Putin, as well. To remind, the guy was elected fair and square in a landslide originally. And there were people back then who screamed from the rooftops that if you elect a former KGB operative as a \"strong hand\", that'll be the last free elections that you'll have. And yes, there were many in the middle who laughed at them. I wonder how many of those have left the country since. reply shigawire 16 hours agorootparentprev>What concerns me most about this, is the general attitude of entirely dehumanizing specific groups of people And they are armed to the teeth. reply silverquiet 2 hours agorootparentprevProbably twentyish years ago, I read some comment or other that when reading right-wing propaganda, just mentally replace \"immigrant\" with \"jew\" and see how it sounds to you. Since then I can never not see that, and I think it really is the key to understanding the nature of these movements; they thrive on dehumanization of groups of humans. There's an LBJ quote that springs to mind immediately: > “If you can convince the lowest white man he's better than the best colored man, he won't notice you're picking his pocket. Hell, give him somebody to look down on, and he'll empty his pockets for you.” reply twodave 16 hours agorootparentprevWhat does America or even Trump have to gain by installing a dictatorship? Assuming he even could, which is probably worth arguing. He is already the most powerful executive of the most powerful (by far, it is not close) and future-proof (again, by far) nation on Earth. His actions in my view are more aligned with getting America the best deal ahead of whatever unfavorable population and trade collapses await the world in the next few decades. As an American, I only half expect democracy to survive putting my cohort (millennials) through retirement. Industrialization has left the world on a pretty bleak course, with America most equipped to weather it. Most of the moves I see the current administration readying itself for are ones to strengthen our already dominant geopolitical advantages worldwide. Hard to do that if you have a civil war to manage on top of it. reply UniverseHacker 3 hours agorootparentWhat did past demagogues and dictators have to gain? It always turned out awful in the end, but concerns over that didn't stop them or their supporters. Trump appears to have no stable ideals, he seems to be only interested in attention and appearing successful. He's previously tried unsuccessfully to launch into politics from the left, right, and center but eventually found traction with this new movement, so he doubled down and played the demagogue after a lifetime of being the epitome of the elite. Ultimately there is a narrative that regular people are doing bad economically because of a few specific groups of people, and that this can be fixed by dehumanizing, terrorizing, and persecuting these people. They will need an all powerful leader not constrained by laws, opposing politics, or human rights to \"solve the problem.\" Of course people think his actions are \"aligned with getting America the best deal\" or else they wouldn't want to help install him as dictator. Trump seems to have gotten pretty far along this path already. He appointed his own sycophant supreme court that ruled him above the law and unprosecutable. He failed at trying to overthrow an election, but managed to get away scott free by intimidating people out of doing their jobs, and eventually pardoned everyone criminally convicted of participating. He was convicted of 34 felonies in a trial by jury, but received no sentence. Now he's experimented with simply announcing the 14th amendment of the US constitution invalid by executive order, and will see if he gets away with it if so, I think that qualifies as being successfully installed as a dictator. reply tclancy 17 hours agorootparentprevDunno. Wasn’t there for the ramp up. reply guelo 17 hours agorootparentprevProbably. But this political movement won't stop with this term. 2016 Trump was more moderate than today's Trump. The next time the movement is stopped it will come back even more ferociously, especially since the movement is fed constant propaganda about being aggrieved and revenge fantasies. reply frob 18 hours agoprevnext [9 more] [flagged] Johanx64 18 hours agoparentnext [7 more] [flagged] dylan604 17 hours agorootparentI reject the premise of your entire comment. Of course they can hide actual scientific data and replace it with their own version. They will tell anyone and everyone over and over that their version is the truth and people will believe it as such. They will also believe that anyone trying to dispute it is wrong and vilify them. This has already happened in 1.0, and plenty of other examples of it from history. But one would have to crack open a history book to know about that, and the populace is trending to banning books rather than opening them. reply p_ing 17 hours agorootparent> Of course they can hide actual scientific data and replace it with their own version. They'll just break out the magic markers on all the dis-favorable weather reports. reply voganmother42 17 hours agorootparentAfter a few sips of bleach things tend to get blurry reply Johanx64 16 hours agorootparentprevWell, you see laws of physics and periodic table doesn't swap out every time there's a change in administration. Neither are field effect transistors swapped out for vacuum-tubes with a different ruling party. That makes a pretty good first order approximation of what is and isn't scientific, and worth spending taxpayer dollar on. Science isn't prescriptive (doesn't tell you what to do), it's descriptive. Prescriptions are policy and have nothing to do with science. reply dylan604 16 hours agorootparentThere you go trying to apply logic to where it is not welcomed. The people that try to squash science and promote whatever notions they prefer do not care about descriptive vs prescriptive. They will push their agenda regardless of facts. It just happens the periodic table doesn't threaten their agenda. These are the same types that threaten to jail those that disagree with them. reply Johanx64 15 hours agorootparentWell, they do deal mostly with prescriptive policy things given the global context. Namely, do the \"green\" policies make sense given that: 1. China will only increase it's total energy and resource consumption 2. And so will India and rest of the developing nations (unless you want them to be relegated to being perpetual backwater shitholes?) All these will result in ever increasing CO2 output and pollution as these and other countries develop and transition. Many \"green\" products solar panels, inverters, etc are made in China. And they are gearing up to be a very strong competitor in electric vehicles too. Which will futher increase the trade deficit and directly subsidize the competitors. Most sensible discussions as far as I can tell deal with these realpolitik considerations first and foremost. reply xdennis 16 hours agoparentprevI don't know what role NIH played in the Covid response, but if they carry some responsibility for the previous administration's decision to force/coerce people to take the vaccine against their will then it seems fair that the new administration should hold them to account. reply fuzzer371 15 hours agorootparent> previous administration's decision to force/coerce people to take the vaccine against their will I'm glad they did. Because people are idiots and would listen to their social media influencer's tell them that vaccines cause autism then chug down some horse de-wormer. reply nothrowaways 18 hours agoprevThis seems more like a coup in developing countries as time goes by. reply andrewstuart2 18 hours agoparentIf you trust the election results, and I generally do because I have no proof otherwise, then the American people voted for this. But it certainly looks like an authoritarian power transfer. reply scarecrowbob 18 hours agorootparentI am affected by this and I didn't vote for this. It's easy to say that \"the American People\" voted for this, but it's not correct to say that I am personally being affected by my bad choices. I think the elections were \"fair\", for what its worth I just don't agree that \"fairness\" makes me blame-worthy when I face the consequences of other folks actions. reply freedomben 18 hours agorootparentI agree with you completely, but this gets at a debate that's been going on since the ancient days about whether democracy is actually good or not. Whether a tyrant is elected by a fair majority or inherits the throne at best (or Divine Right of Kings, whatever) makes little difference IMHO. That said, while I have criticisms for democracy and do enjoy pointing out it's imperfections when people talk about democracy as some self-evident ideal, I do think it's probably the best system of government in a sea of less than ideal choices. reply peterashford 17 hours agorootparentI'd argue the problem lies less with democracy, more with the media. Democracy requires an informed populace. The success of populist politicians shows you don't have that reply rcMgD2BwE72F 17 hours agorootparentI'd the cause is advertising-based business model of the media industry, then. Too few people pay for news these days. So the news aren't the product and we ended mis and ill-informed. reply from-nibly 17 hours agorootparentprevIf it worked it would work. If having an informed populace is required for democracy to work then it doesn't work. Especially when you ask the question, \"informed about what\"? reply generalizations 16 hours agorootparentprevThis is why early democracies limited voters to a subset which was perceived as better informed or more responsible rule of any person passing thresholds (like e.g. land ownership) that proved a minimum level of capacity. reply int_19h 16 hours agorootparentIt would be more accurate to say that it was intended to (and did) limit voting to a subset that was perceived as more loyal to the social order, such as property laws and other arrangements that define who's on top of the social pyramid. reply generalizations 14 hours agorootparentConsidering it was a similar subset which in the US was largely responsible for the US revolution, I can't say that interpretation is persuasive. reply swatcoder 17 hours agorootparentprevEarnestly: do you have a sense of when and where such an informed populace existed? reply sangnoir 16 hours agorootparentprevIs it the media or unfettered capitalism? Traditional media (newspapers, and local news) is dying slowly, they cannot successfully compete with centralized juggernauts with global footprints sucking up all the ad dollars. Was it Thiel who remarked that capitalism may not be compatible with democracy? One guess on which he'll choose to keep. reply pseudalopex 11 hours agorootparentIt was worse than you remembered. Thiel said freedom and democracy were not compatible. reply Paradigma11 7 hours agorootparentprevI do think a big part of the problem is the two party system. It makes negative campaigning far to effective. That way you get pathological choices, like Biden vs Trump when you wouldnt entrust any of them with running a small business you own. reply light_hue_1 17 hours agorootparentprevI'm tired of hearing that people aren't informed. They were informed. Trump told them that science is crap, that tariffs are good, that we should punish immigrants, that he would pardon the jan 6th people, etc. This is what people voted for and what they wanted. Hate and retribution. When Democrats finally stop with this false narrative that if only people knew better, they'd vote for Democrats, maybe we'll start actually winning again. reply lesuorac 16 hours agorootparentSure, they heard from Trump that tariffs would fix inflation. But they still aren't informed because they didn't actually know what tariffs are [1]. > When Democrats finally stop with this false narrative that if only people knew better, they'd vote for Democrats, maybe we'll start actually winning again. Really do not hate to point this out. Democrats are really responsible for a lot of this mess. Both Republics and Democrats really love to blame the other one but they're really just the Father and Mother and when the child is having problems, it's both of their faults. Personally, I think it's gotten this way because of the whole first-past-the-post so if you have a belief like \"far right\" or \"far left\" then your best bet politically is to run underneath Republican or Democrat and push out the \"moderates\" in a primary as opposed to making a new political party that actually has your beliefs. [1]: https://thenightly.com.au/politics/us-politics/what-is-a-tar... reply int_19h 15 hours agorootparentI don't think it's just FPTP, given that other countries that have it are nowhere nearly as polarized. I think it's actually the combination of FPTP and open (or at least broadly accessible) primaries. In Canada and UK, parties generally have much more control over their primaries, and party elites generally try to exercise that power to ensure that candidates don't upset the existing arrangements too much. Not even necessarily as a deliberate strategy, but when you have to work your way through the \"smoke-filled rooms\" as a candidate, that filters out the purists and favors those willing to compromise. US was also like that for a long time, and notably we didn't have this degree of polarization then. But once primaries are wide open and the party no longer controls the candidates, it seems inevitable that more extreme candidates will win. They appeal to the more ideologically motivated voters who are generally more likely to show up and vote (especially so in the primaries), and so any candidate who wants to win there has to appeal to them first. reply pseudalopex 8 hours agorootparentUS had cycles of polarization from the earliest years. When in your model did parties lose control to more ideologically motivated voters? reply krapp 15 hours agorootparentprevThe problem is the Democrats aren't far left, they aren't even really left. There is no viable political counterbalance to the right wing in US politics anymore, the so-called left is moving right, and the right is moving further right. Kamala Harris' whole campaign (such as it was) was an attempt to court \"moderate\" Republicans rather than the base who was never enamored with Biden to begin with. I agree that first past the post is a problem and this election was more lost by the Democrats than won by the Republicans, but I think it's a myth that any of it has to do with the Democrats going \"too far left.\" Call me when any Democratic Presidential candidate openly calls for dismantling American imperialism and scaling back the military, or criticizes capitalism and endorses nationalized healthcare, education and UBI, or doesn't kiss the ass of police or curry favor with Evangelical Christians, or has immigration policies that actually materially differ from those of Republicans. reply Tostino 3 hours agorootparentTo be fair, we almost had Bernie which supported all of those policies and positions (other than UBI if I remember right). reply krapp 1 hour agorootparentAnd what happened to him? The party purged him so fast there's nothing left of him but memes. reply ch33zer 18 hours agorootparentprevI 100% agree with what you said, but no one looks back and remembers the people who voted against Hitler, or says 'there were some good people in Nazi Germany'. reply aaomidi 17 hours agorootparentWhat is possible though is a form of actual resistance. There is nothing forcing us to just share a union so to speak. reply generalizations 16 hours agorootparentStates rights has been too closely associated in the popular mind with slavery and racism (via arguments over what the civil war was really about). It'll be tough to disassociate those enough to make a persuasive argument for secession. reply XorNot 16 hours agorootparentAt the point you actually want a secession movement people aren't worried about the optics of it. It's not happening because the actual cost of secession, civil war and everything else is staggering. These are one way, permanent changes where a large number of the agitators will not live to see the better world. It's compromise, carnage and collateral damage. Doing anything else is a better option 100% of the time. Nobody's going to hear \"we're seceding because we're sick of dying from lack of healthcare\" and think \"ooh, I wonder if it's secretly about slavery?\" reply refurb 15 hours agorootparentprevYou’re over complicating things. You are a willing participant in the political system. You agree to live by the results of the election, even if you don’t personally agree with them. You’re not personally responsible for the results, but you are a willing participant in the system that caused them. reply scarecrowbob 11 hours agorootparent\"Willing\" in what sense? What's my realistic alternative? Lay it on me. I didn't vote, for what it's worth. I don't think it's ethical to participate in that kind of thing. The US is 2 crimes and a real estate scam in a trench coat. When I was younger, I indeed thought that maybe it had some legitimacy, but after having read quite a lot of the history of the millions that the government here has enslaved and murdered I no longer thing it's legitimate. I understand that they have the guns and the power, but realistically I am not a willing participant in this system. reply refurb 7 hours agorootparent> \"Willing\" in what sense? What's my realistic alternative? Lay it on me. Leave the country. I did it. > I didn't vote, for what it's worth. I don't think it's ethical to participate in that kind of thing. That's your choice. You have plenty of options. Get involved politically. Organize. Fund raise. Plenty of opportunity to affect the outcome. But you'd rather just complain about it online. reply grahamj 17 hours agorootparentprevAll eligible voters are collectively responsible for the outcome of the election. You can't be proud of democracy if you won't shoulder the blame when it renders poor outcomes. reply llamaimperative 16 hours agorootparentThere's a difference between shouldering the burden of the outcome and the outcome indicating that you made bad choices. reply klipt 16 hours agorootparentprevElon Musk found that if you throw billions at getting our low propensity rural voters you can sway the election by enough percents. reply Tostino 3 hours agorootparentThat's true, but man am I mad at the Democrats for entirely abandoning that demo for decades before. It lead to this situation. reply nickff 18 hours agorootparentprevAuthoritarian power transfers usually involve (immediate) executions, or at least the imprisonment/exile of important people from the previous government/administration/regime, to stabilize the new regime: https://en.wikipedia.org/wiki/The_Logic_of_Political_Surviva... reply HumblyTossed 17 hours agorootparentThe Heritage Foundation (who is behind all this and has been at it since 1973), says that the transfer of power will be bloodless if we allow it to remain bloodless. https://www.nytimes.com/2024/07/03/us/politics/heritage-foun... They've been telling us exactly what they're going to do ... and we just let it happen. reply tshaddox 18 hours agorootparentprevPresumably authoritarian regimes only execute/imprison/exile dissidents if they think it's necessary to eliminate opposition. An authoritarian regime which does not think there's much of a threat to opposition probably wouldn't bother. reply aaomidi 17 hours agorootparentThere is a huge cost in executions. Especially in places where they have not been super common place and super public. reply jojobas 16 hours agorootparentprevServes to prove it's not an authoritarian regime? They know they won by not so much and are likely to lose next time, especially given Trump can't run anymore. reply groby_b 18 hours agorootparentprevTBF, we're only on day 2. (I am 90% certain it won't come to that, but there's a 10% queasiness left that hasn't been ameliorated by the haphazard approach untethered from legality that we've seen so far) reply jayknight 17 hours agorootparent\"This guy is running around giving everyone pardons. The funny thing, maybe the sad thing, is he didn’t give himself a pardon. And if you look at it, it all had to do with him.\" https://www.independent.co.uk/news/world/americas/us-politic... I fully expect Trump to ruin Biden's final years with investigations and court cases. Maybe that's not out right execution, but Trump definitely wants revenge. reply johnnyanmac 17 hours agorootparentI don't think even Trump wants to call in the constitutional landmark case of a president pardoning himself. That'd be true chaos. >I fully expect Trump to ruin Biden's final years with investigations and court cases I'm still 50/50 on if Trump makes it that far, physically. He's older than when Biden ran his administration and not exactly in the best health. But I guess they did get Regean through his last years. Nothing's impossible. reply pstuart 16 hours agorootparent> That'd be true chaos. We've crossed the Rubicon. reply groby_b 17 hours agorootparentprevSure. But that is, for better or worse, perfectly legal. (No, I'm not saying it's great, but I'm saying this is still within non-dictatorship realms) Whereas executions have a small issue in that area. reply refurb 15 hours agorootparentprevHe ran on “lock her up” in 2016 and never followed through. I find it hilarious that everyone is scared of Trump when there was a concerted effort by the other side to use the justice system to stop him from ever running again. Need I remind everyone of multi-year Russiagate investigation that was all made up? The misdemeanor charges that becomes felonies with massive fines? The media collusion to silence the Hunter Biden laptop story when the FBI knew at the time it was real? It’s the pot calling the kettle black. It’s the thieves accusing everyone else of stealing. reply krapp 14 hours agorootparentAll made up? You're thinking of the Benghazi hearings, which were admitted to be nothing but a smear campaign by the Republicans[0]. \"Russiagate\" meanwhile found plenty of stuff[1]. [0]https://www.vox.com/2015/9/30/9423339/kevin-mccarthy-benghaz... [1]https://www.acslaw.org/projects/the-presidential-investigati... reply refurb 7 hours agorootparentYes, the Steele Dossier which the entire Russiagate investigation was based on was fake and funded by Hillary Clinton. https://apnews.com/article/russia-ukraine-2022-midterm-elect... Four years and immense government resources were wasted investigating the President calling him a \"tool of the Russian government\". And the \"plenty of stuff\" found? \"Undisclosed contacts\" and \"obstruction of the investigation\". What a massive flop. reply ropetin 14 hours agorootparentprev> I find it hilarious that everyone is scared of Trump when there was a concerted effort by the other side to use the justice system to stop him from ever running again. I honestly don't get what you mean by this. Lets say all your comments about Biden, the FBI, laptops and whatever else are 100% true. How does that change if I should be worried about stuff Trump might do? To use a totally extreme example, if John Wayne Gacy went around talking all the shit in the world about Jeffrey Dahmer, does that somehow make Dahmer not a serial killer? > He ran on “lock her up” in 2016 and never followed through. So that proves he lies and makes promises he can't/won't/doesn't keep? Is that a positive trait in a politician? How are we supposed to determine when he is 'just joshing, bro' and actually is being truthful? And yes, I know Biden and the dems also make promises they don't keep, but again that doesn't excuse Trump from doing the same. reply HideousKojima 16 hours agorootparentprevI mean if Biden actually was making corrupt deals trading influence/access for money then he absolutely should be investigated and prosecuted for it. And there's already hard proof that Hunter was selling access to his father, and that the foreigners he was selling said access to believed he could actually provide it. If we didn't have the DOJ and other institutions protecting Biden for the last several years it would be pretty easy to find out if he actually did sell influence (vs. Hunter deceiving the people he was making deals with) with just a couple of warrants and subpeonas. reply archagon 16 hours agorootparenthttps://www.pbs.org/newshour/amp/politics/former-fbi-informa... reply HideousKojima 16 hours agorootparentWhich has nothing to do with the hard evidence on Hunter's laptop reply mschuster91 5 hours agorootparentprevAn established bureaucracy used to \"following orders\" will just soldier on assuming the authoritarian comes in legally. The most well known example is Germany. The Führer came in elected, got voted to dictator after the Reichstagsbrand and then never left power and barely anyone in the executive resisted. And this is likely also where the US are heading. The putsch will not be loud and with a bang like Syria, it will be slow as molasses but about as difficult to impossible to stop like a broken dam worth of water moving downstream. reply dekhn 18 hours agorootparentprevI mean, it's pretty clear they want to sue Fauci and more. Attacking dedicated public servants working for the health of the country is going to have serious negative effects. reply blackeyeblitzar 17 hours agorootparentnext [9 more] [flagged] dekhn 17 hours agorootparentIt seems unlikely that any court case would \"uncover the truth\" about the decision making behind that. Unfortunately, this has become so politicized that determining the steps that lead to NIH funding research in China, determining if a law was broken, and what the intent was, are unlikely to happen in the future. It seems very unlikely to me that a knowledgeable head of an NIH institute would break the law in a discoverable way. My guess is that when this occurred, the decision-makers did not believe they were doing anything illegal. (I think NIH funding virology research in China is dumb. That should not have happened.) reply tripletao 12 hours agorootparent> (I think NIH funding virology research in China is dumb. That should not have happened.) I'm not sure why \"in China\" is your point of concern here? Lots of virology research presents minimal danger, and seems fine to me to fund even in geopolitical adversaries (after calculation of the diplomatic and scientific costs and benefits). My concern with the WIV's work is that they were searching for deadlier and faster-spreading human viruses. So if their research succeeds, then they're deliberately just a containment failure away from a novel human pandemic. China's laxer safety standards compounded that risk, but I'd oppose such work in Baric's lab at UNC too. reply Enginerrrd 17 hours agorootparentprev>I think NIH funding virology research in China is dumb. That should not have happened. This gets really subtle and tricky. This type of thing is closely linked to biological weapons research and we want collaboration and transparency to an extent to maintain access to information. reply blackeyeblitzar 16 hours agorootparentprevA couple things make me think they knew they were doing something illegal. One, they changed the definition of GoF when questioned about it, in a way that makes no sense. Two, they tried to avoid transparency laws like FOIA as well: https://www.thenation.com/article/archive/nih-foia-covid-ori... reply dang 12 hours agorootparentIf you keep using HN primarily for political and ideological battle, we're going to have to ban you. It's not what this site is for, and destroys what it is for, and we've asked you more than once already. https://news.ycombinator.com/newsguidelines.html https://hn.algolia.com/?sort=byDate&dateRange=all&type=comme... (This is not about NIH or GOF it's about your account's commenting history. I just put it here because it's your most recent post.) reply dekhn 16 hours agorootparentprevUnderstood. Gain-of-function is one of those terms that only really makes sense with a lot of biological context and knowledge. Trying to explain that to the general public or politicians (especially in an adversarial context like a congressional hearing) is just not going to work. Trying to hide things in their chat messages and texts and emails was dumb. I still can't believe they did that (one guy even used the exact scenario that we learned at Google: don't say anything in email that would get printed in a negative light on the front page of the NY Times; I had a manager there who actually did have one of his emails, as part of the Oracle Java court case, in a prominent article in the Times). Public employees of the government should assume that literally everything they produce as part of their job (including on personal devices) will eventually be seen externally by people without the necessary context to understand. From what I can see, Fauci already admitted he made a collection of bad statements and decisions. For me, that's the end of the matter. Suing him over this is just going to damage the country. For example, next time there is a crisis, all those hardworking public servants are going to look at what happened and conclude \"no, I will not be the public communicator that helps the country understand the situation we are in and how we are going to get out of it\". reply theGnuMe 16 hours agorootparentprevOk but that is where the bats are. And funding gives you access. Even with funding China blocked a lot of investigation. Just wait until we get Ebola 2 or what not. reply joyeuse6701 17 hours agorootparentprevWhy? Trump didn’t face any in the end, did he? reply gunian 17 hours agorootparentprevthat's the beauty of it all outright murders usually create resistance this way the opposition is cleansed in a mini genocide as well as algorithmically in a decentralized manner so resistance can't form the fourth reich is here :) reply UncleOxidant 18 hours agorootparentprevUnfortunately, a good chunk of people who voted for him really didn't pay much attention beyond him promising to lower prices. But this is the electorate we have a lot of them just can't be bothered with much detail as attention spans are short these days. reply stevenwoo 14 hours agorootparentI listened to and watched a lot of interviews with people who voted for Trump. They just said he didn’t mean it whenever they were confronted with quotes from Trump that obviously disagreed with their views as evidenced by earlier questions in the interviews. Or take the many Muslims in Dearborn who still claim Trump is better than Harris would be on Israel/Palestine they refuse to believe the evidence despite action days in,(or from trumps last term) it’s not possible to argue rationally with these people. Not only are they barely paying attention, they only see and hear what they want to. reply travisporter 17 hours agorootparentprevNo they listened about TikTok reply johnnyanmac 17 hours agorootparentfor 13 hours? I suppose we never specificied how long it had to be banned for. reply swatcoder 18 hours agorootparentprev> If you trust the election results, and I generally do because I have no proof otherwise, then the American people voted for this. No, if you trust the election results, it's a legitimate electi",
    "originSummary": [],
    "commentSummary": [
      "The NIH (National Institutes of Health) is experiencing restrictions on meetings, travel, communications, and hiring, potentially affecting the US's scientific reputation. Critics claim these measures are punitive and politically driven, targeting researchers perceived as liberal, which could disrupt public health research, including cancer and infectious diseases. This situation highlights broader issues regarding the politicization of science and concerns about the decline of US soft power."
    ],
    "points": 260,
    "commentCount": 388,
    "retryCount": 0,
    "time": 1737590795
  },
  {
    "id": 42801370,
    "title": "Bun 1.2 Is Released",
    "originLink": "https://bun.sh/blog/bun-v1.2",
    "originBody": "Bun is complete toolkit for building and testing full-stack JavaScript and TypeScript applications. If you're new to Bun, you can learn more from the Bun 1.0 blog post. Bun 1.2 Bun 1.2 is a huge update, and we're excited to share it with you. Here's the tl;dr of what changed in Bun 1.2: There's a major update on Bun's progress towards Node.js compatibility Bun now has a built-in S3 object storage API: Bun.s3 Bun now has a built-in Postgres client: Bun.sql (with MySQL coming soon) bun install now uses a text-based lockfile: bun.lock We also made Express 3x faster in Bun. Node.js compatibility Bun is designed as a drop-in replacement for Node.js. In Bun 1.2, we started to run the Node.js test suite for every change we make to Bun. Since then, we've fixed thousands of bugs and the following Node.js modules now pass over 90% of their tests with Bun. For each of these Node modules, Bun passes over 90% of the Node.js test suite. Here's how we did it. How do you measure compatibility? In Bun 1.2, we changed how we test and improve Bun's compatibility with Node.js. Previously, we prioritized and fixed Node.js bugs as they were reported, usually from GitHub issues where someone tried to use an npm package that didn't work in Bun. While this fixed actual bugs real users ran into, it was too much of a \"wack-a-mole\" approach. It discouraged doing the large refactors necessary for us to have a shot at 100% Node.js compatibility. That's when we thought: what if we just run the Node.js test suite? There are so many tests in the Node.js repository, that the files can't all be listed on GitHub. Running Node.js tests in Bun Node.js has thousands of test files in its repository, with most of them in the test/parallel directory. While it might seem simple enough to \"just run\" their tests, it's more involved than you might think. Internal APIs For example, many tests rely on the internal implementation details of Node.js. In the following test, getnameinfo is stubbed to always error, to test the error handling of dns.lookupService(). test/parallel/test-dns-lookupService.js const { internalBinding } = require(\"internal/test/binding\"); const cares = internalBinding(\"cares_wrap\"); const { UV_ENOENT } = internalBinding(\"uv\"); cares.getnameinfo = () => UV_ENOENT; To run this test in Bun, we had to replace the internal bindings with our own stubs. test/parallel/test-dns-lookupService.js Bun.dns.lookupService = (addr, port) => { const error = new Error(`getnameinfo ENOENT ${addr}`); error.code = \"ENOENT\"; error.syscall = \"getnameinfo\"; throw error; }; Error messages There are also Node.js tests that check the exact string of error messages. And while Node.js usually doesn't change error messages, they don't guarantee it won't change between releases. const common = require(\"../common\"); const assert = require(\"assert\"); const cp = require(\"child_process\"); assert.throws( () => { cp.spawnSync(process.execPath, [__filename, \"child\"], { argv0: [] }); }, { code: \"ERR_INVALID_ARG_TYPE\", name: \"TypeError\", message: 'The \"options.argv0\" property must be of type string.' + common.invalidArgTypeHelper([]), }, ); To work around this, we had to change the assertion logic in some tests to check the name and code, instead of the message. This is also the standard practice for checking error types in Node.js. { code: \"ERR_INVALID_ARG_TYPE\", name: \"TypeError\", message: 'The \"options.argv0\" property must be of type string.' + common.invalidArgTypeHelper([]), }, While we do try to match the error messages of Node.js as much as possible, there are times where we want to provide a more helpful error message, as long as the name and code are the same. Progress so far We've ported thousands of files from the Node.js test suite to Bun. That means for every commit we make to Bun, we run the Node.js test suite to ensure compatibility. A screenshot of Bun's CI where we run the Node.js test suite for every commit. Every day, we are adding more and more passing Node.js tests to Bun, and we're excited to share more progress on Node.js compatibility very soon. In addition to fixing existing Node.js APIs, we've also added support for the following Node.js modules. node:http2 server You can now use node:http2 to create HTTP/2 servers. HTTP/2 is also necessary for gRPC servers, which are also now supported in Bun. Previously, there was only support for the HTTP/2 client. import { createSecureServer } from \"node:http2\"; import { readFileSync } from \"node:fs\"; const server = createSecureServer({ key: readFileSync(\"key.pem\"), cert: readFileSync(\"cert.pem\"), }); server.on(\"stream\", (stream, headers) => { stream.respond({ \":status\": 200, \"content-type\": \"text/html; charset=utf-8\", }); stream.end(\"Hello from Bun!\"); }); server.listen(3000); In Bun 1.2, the HTTP/2 server is 2x faster than in Node.js. When we support new APIs to Bun, we spend a lot of time tuning performance to ensure that it not only works, but it's also faster. Benchmark of a \"hello world\" node:http2 server running in Bun 1.2 and Node.js 22.13. node:dgram You can now bind and connect to UDP sockets using node:dgram. UDP is a low-level unreliable messaging protocol, often used by telemetry providers and game engines. import { createSocket } from \"node:dgram\"; const server = createSocket(\"udp4\"); const client = createSocket(\"udp4\"); server.on(\"listening\", () => { const { port, address } = server.address(); for (let i = 0; i{ console.log(`Received: data=${data} source=${address}:${port}`); client.unref(); }); server.bind(); This allows packages like DataDog's dd-trace and @clickhouse/client to work in Bun 1.2. node:cluster You can use node:cluster to spawn multiple instances of Bun. This is often used to enable higher throughput by running tasks across multiple CPU cores. Here's an example of how you can create a multi-threaded HTTP server using cluster: The primary worker spawns n child workers (usually equal to the number of CPU cores) Each child worker listens on the same port (using reusePort) Incoming HTTP requests are load balanced across the child workers import cluster from \"node:cluster\"; import { createServer } from \"node:http\"; import { cpus } from \"node:os\"; if (cluster.isPrimary) { console.log(`Primary ${process.pid} is running`); // Start N workers for the number of CPUs for (let i = 0; i{ console.log(`Worker ${worker.process.pid} exited`); }); } else { // Incoming requests are handled by the pool of workers // instead of the primary worker. createServer((req, res) => { res.writeHead(200); res.end(`Hello from worker ${process.pid}`); }).listen(3000); console.log(`Worker ${process.pid} started`); } Note that reusePort is only effective on Linux. On Windows and macOS, the operating system does not load balance HTTP connections as one would expect. node:zlib In Bun 1.2, we rewrote the entire node:zlib module from JavaScript to native code. This not only fixed a bunch of bugs, but it made it 2x faster than Bun 1.1. Benchmark of inflateSync using node:zlib in Bun and Node.js. We also added support for Brotli in node:zlib, which was missing in Bun 1.1. import { brotliCompressSync, brotliDecompressSync } from \"node:zlib\"; const compressed = brotliCompressSync(\"Hello, world!\"); compressed.toString(\"hex\"); // \"0b068048656c6c6f2c20776f726c642103\" const decompressed = brotliDecompressSync(compressed); decompressed.toString(\"utf8\"); // \"Hello, world!\" C++ addons using V8 APIs If you want to use C++ addons alongside your JavaScript code, the easiest way is to use N-API. However, before N-API existed, some packages used the internal V8 C++ APIs in Node.js. What makes this complicated is that Node.js and Bun use different JavaScript engines: Node.js uses V8 (used by Chrome), and Bun uses JavaScriptCore (used by Safari). Previously, npm packages like cpu-features, which rely on these V8 APIs, would not work in Bun. require(\"cpu-features\")(); dyld[94465]: missing symbol called fish: Job 1, 'bun index.ts' terminated by signal SIGABRT (Abort) To fix this, we undertook the unprecedented engineering effort of implementing V8's public C++ API in JavaScriptCore, so these packages can \"just work\" in Bun. It's so complicated and nerdy to explain, we wrote a 3-part blog series on how we supported the V8 APIs... without using V8. In Bun 1.2, packages like cpu-features can be imported and just work. $ bun index.ts { arch: \"aarch64\", flags: { fp: true, asimd: true, // ... }, } The V8 C++ APIs are very complicated to support, so most packages will still have missing features. We're continuing to improve support, so packages like node-canvas@v2 and node-sqlite3 can work in the future. node:v8 In addition to the V8 C++ APIs, we've also added support for heap snapshots using node:v8. import { writeHeapSnapshot } from \"node:v8\"; // Writes a heap snapshot to the current working directory in the form: // `Heap-{date}-{pid}.heapsnapshot` writeHeapSnapshot(); In Bun 1.2, you can use getHeapSnapshot and writeHeapSnapshot to read and write V8 heap snapshots. This allows you to use Chrome DevTools to inspect the heap of Bun. You can view a heap snapshot of Bun using Chrome DevTools. Express is 3x faster While compatibility is important for fixing bugs, it also helps us fix performance issues in Bun. In Bun 1.2, the popular express framework can serve HTTP requests up to 3x faster than in Node.js. This was made possible by improving compatibility with node:http, and optimizing Bun's HTTP server. S3 support with Bun.s3 Bun aims to be a cloud-first JavaScript runtime. That means supporting all the tools and services you need to run a production application in the cloud. Modern applications store files in object storage, instead of the local POSIX file system. When end-users upload a file attachment to a website, it's not being stored on the server's local disk, it's being stored in a S3 bucket. Decoupling storage from compute prevents an entire class of reliability issues: low disk space, high p95 response times from busy I/O, and security issues with shared file storage. S3 is the defacto-standard for object storage in the cloud. The S3 APIs are implemented by a variety of cloud services, including Amazon S3, Google Cloud Storage, Cloudflare R2, and dozens more. That's why Bun 1.2 adds built-in support for S3. You can read, write, and delete files from an S3 bucket using APIs that are compatible with Web standards like Blob. Reading files from S3 You can use the new Bun.s3 API to access the default S3Client. The client provides a file() method that returns a lazy-reference to an S3 file, which is the same API as Bun's File. import { s3 } from \"bun\"; const file = s3.file(\"folder/my-file.txt\"); // file instanceof Blob const content = await file.text(); // or: // file.json() // file.arrayBuffer() // file.stream() 5x faster than Node.js Bun's S3 client is written in native code, instead of JavaScript. When you compare it to using packages like @aws-sdk/client-s3 with Node.js, it's 5x faster at downloading files from a S3 bucket. Left: Bun 1.2 with Bun.s3. Right: Node.js with @aws-sdk/client-s3. Writing files to S3 You can use the write() method to upload a file to S3. It's that simple: import { s3 } from \"bun\"; const file = s3.file(\"folder/my-file.txt\"); await file.write(\"hello s3!\"); // or: // file.write(new Uint8Array([1, 2, 3])); // file.write(new Blob([\"hello s3!\"])); // file.write(new Response(\"hello s3!\")); For larger files, you can use the writer() method to obtain a file writer that does a multi-part upload, so you don't have to worry about the details. import { s3 } from \"bun\"; const file = s3.file(\"folder/my-file.txt\"); const writer = file.writer(); for (let i = 0; i .r2.cloudflarestorage.com\", bucket: \"my-bucket\", }); // Sets the default client to be your custom client Bun.s3 = client; Postgres support with Bun.sql Just like object storage, another datastore that production applications often need is a SQL database. Since the beginning, Bun has had a built-in SQLite client. SQLite is great for smaller applications and quick scripts, where you don't want to worry about the hastle of setting up a production database. In Bun 1.2, we're expanding Bun's support for SQL databases by introducing Bun.sql, a built-in SQL client with Postgres support. We also have a pull request to add MySQL support very soon. Using Bun.sql You can use Bun.sql to run SQL queries using tagged-template literals. This allows you to pass JavaScript values as parameters to your SQL queries. Most importantly, it escapes strings and uses prepared statements for you to prevent SQL injection. import { sql } from \"bun\"; const users = [ { name: \"Alice\", age: 25 }, { name: \"Bob\", age: 65 }, ]; await sql` INSERT INTO users (name, age) VALUES ${sql(users)} `; Reading rows is just as easy. Results are returned as an array of objects, with the column name as the key. import { sql } from \"bun\"; const seniorAge = 65; const seniorUsers = await sql` SELECT name, age FROM users WHERE age >= ${seniorAge} `; console.log(seniorUsers); // [{ name: \"Bob\", age: 65 }] 50% faster than other clients Bun.sql is written in native code with optimizations like: Automatic prepared statements Query pipelining Binary wire protocol support Connection pooling Structure caching Optimizations stack like buffs in World of Warcraft. The result is that Bun.sql is up to 50% faster at reading rows than using the most popular Postgres clients with Node.js. Migrate from postgres.js to Bun.sql The Bun.sql APIs are inspired by the popular postgres.js package. This makes it easy to migrate your existing code to using Bun's built-in SQL client. import { postgres } from \"postgres\"; import { postgres } from \"bun\"; const sql = postgres({ host: \"localhost\", port: 5432, database: \"mydb\", user: \"...\", password: \"...\", }); const users = await sql`SELECT name, age FROM users LIMIT 1`; console.log(users); // [{ name: \"Alice\", age: 25 }] Bun is a package manager Bun is a npm-compatible package manager that makes it easy to install and update your node modules. You can use bun install to install dependencies, even if you're using Node.js as a runtime. Replace npm install with bun install $ npm install $ bun install In Bun 1.2, we've made the biggest change yet to the package manager. Problems with bun.lockb Since the beginning, Bun has used a binary lockfile: bun.lockb. Unlike other package managers that use text-based lockfiles, like JSON or YAML, a binary lockfile allowed us to make bun install almost 30x faster than npm. However, we found that there were a lot of paper cuts when using a binary lockfile. First, you couldn't view the contents of the lockfile on GitHub and other platforms. This sucked. What happens if you receive a pull request from an external contributor that changes the bun.lockb file? Do you trust it? Probably not. That's also assuming there isn't a merge conflict! Which for a binary lockfile, is almost impossible to resolve, aside from manually deleting the lockfiles and running bun install again. This also made it hard for tools to read the lockfile. For example, dependency management tools like Dependabot would need an API to parse the lockfile, and we didn't offer one. Bun will continue to support bun.lockb for a long time. However, for all these reasons, we've decided to switch to a text-based lockfile as the default in Bun 1.2. Introducing bun.lock In Bun 1.2, we're introducing a new, text-based lockfile: bun.lock. You can migrate to the new lockfile by using the save-text-lockfile flag. bun install save-text-lockfile bun.lock is a JSONC file, which is JSON with added support for comments and trailing commas. bun.lock // bun.lock { \"lockfileVersion\": 0, \"packages\": [ [\"express@4.21.2\", /* ... */, \"sha512-...\"], [\"body-parser@1.20.3\", /* ... */], /* ... and more */ ], \"workspaces\": { /* ... */ }, } This makes it much easier to view diffs in pull requests, and trailing commas make it much less likely to cause merge conflicts. For new projects without a lockfile, Bun will generate a new bun.lock file. For existing projects with a bun.lockb file, Bun will continue to support the binary lockfile, without migration to the new lockfile. We will continue to support the binary lockfile for a long time, so you can continue to use commands, like bun add and bun update, and it will update your bun.lockb file. bun install gets 30% faster You might think that after we migrated to a text-based lockfile, bun install would be slower. Wrong! Most software projects get slower as more features are added, Bun is not one of those projects. We spent a lot of time tuning and optimizing Bun, so we could make bun install even faster. That's why in Bun 1.2, bun install is 30% faster than Bun 1.1 JSONC support in package.json Have you ever added something to your package.json and forgot why months later? Or wanted to explain to your teammates why a dependency needs a specific version? Or have you ever had a merge conflict in a package.json file due to a comma? Often these problems are due to the fact that package.json is a JSON file, and that means you can't use comments or trailing commas in it. package.json { \"dependencies\": { // this would cause a syntax error \"express\": \"4.21.2\" } } This is a bad experience. Modern tools like TypeScript allow for comments and trailing commas in their configuration files, tsconfig.json for example, and it's great. We also asked the community on your thoughts, and it seemed that the status-quo needed to change. What JS ecosystem upgrade path would you prefer to permit comments in package.json? — Rob Palmer (@robpalmer2) April 17, 2024 In Bun 1.2, you can use comments and trailing commas in your package.json. It just works. package.json { \"name\": \"app\", \"dependencies\": { // We need 0.30.8 because of a bug in 0.30.9 \"drizzle-orm\": \"0.30.8\", /*to patch a package. Edit the files in the node_modules/ directory. Run bun patch committo save your changes. That's it! Bun generates a .patch file with your changes in the patches/ directory, which is automatically applied on bun install. You can then commit the patch file to your repository, and share it with your team. For example, you could create a patch to replace a dependency with your own code. ./patches/is-even@1.0.0.patch diff git a/index.js b/index.js index 832d92223a9ec491364ee10dcbe3ad495446ab80..2a61f0dd2f476a4a30631c570e6c8d2d148d419a 100644 a/index.js +++ b/index.js @@ 1,14 +1 @@ 'use strict'; var isOdd = require('is-odd'); module.exports = function isEven(i) { return !isOdd(i); }; + module.exports = (i) => (i % 2 === 0) Bun clones the package from the node_modules directory with a fresh copy of itself. This allows you to safely make edits to files in the package's directory without impacting shared file caches. Easier to use We've also made a bunch of small improvements to make bun install easier to use. CA certificates You can now configure CA certificates for bun install. This is useful when you need to install packages from your company's private registry, or if you want to use self-signed certificate. bunfig.toml [install] # The CA certificate as a string ca = \"-----BEGIN CERTIFICATE-----... --END CERTIFICATE-----\" # A path to a CA certificate file. The file can contain multiple certificates. cafile = \"path/to/cafile\" If you don't want to change your bunfig.toml file, you can also use the ca and cafile flags. bun install cafile=/path/to/cafile bun install ca=\"...\" If you are using an existing .npmrc file, you can also configure CA certificates there. .npmrc cafile=/path/to/cafile ca=\"...\" bundleDependencies support You can now use bundleDependencies in your package.json. package.json { \"bundleDependencies\": [\"is-even\"] } These are dependencies that you expect to already exist in your node_modules folder, and are not installed like other dependencies. bun add respects package.json indentation We fixed a bug where bun add would not respect the spacing and indentation in your package.json. Bun will now preserve the indentation of your package.json, no matter how wacky it is. bun add is-odd package.json // an intentionally wacky package.json { \"dependencies\": { \"is-even\": \"1.0.0\", \"is-odd\": \"1.0.0\" } } omit=dev|optional|peer support Bun now supports the omit flag with bun install, which allows you to omit dev, optional, or peer dependencies. bun install omit=dev # omit dev dependencies bun install omit=optional # omit optional dependencies bun install omit=peer # omit peer dependencies bun install omit=dev omit=optional # omit dev and optional dependencies Bun is a test runner Bun has a built-in test runner that makes it easy to write and run tests in JavaScript, TypeScript, and JSX. It supports many of the same APIs as Jest and Vitest, which includes the expect()-style APIs. In Bun 1.2, we've made a lot of improvements to bun test. JUnit support To use bun test with CI/CD tools like Jenkins, CircleCI, and GitLab CI, you can use the reporter option to output test results to a JUnit XML file. bun test reporter=junit reporter-outfile=junit.xml junit.xml You can also enable JUnit reporting by adding the following to your bunfig.toml file. bunfig.toml [test.reporter] junit = \"junit.xml\" LCOV support You can use bun test coverage to generate a text-based coverage report of your tests. In Bun 1.2, we added support for LCOV coverage reporting. LCOV is a standard format for code coverage reports, and is used by many tools like Codecov. bun test coverage coverage-reporter=lcov By default, this outputs a lcov.info coverage report file in the coverage directory. You can change the coverage directory with coverage-dir. If you want to always enable coverage reporting, you can add the following to your bunfig.toml file. bunfig.toml [test] coverage = true coverageReporter = [\"lcov\"] # default [\"text\"] coverageDir = \"./path/to/folder\" # default \"./coverage\" Inline snapshots You can now use inline snapshots using expect().toMatchInlineSnapshot(). Unlike toMatchSnapshot(), which stores the snapshot in a separate file, toMatchInlineSnapshot() stores snapshots directly in the test file. This makes it easier see, and even change your snapshots. First, write a test that uses toMatchInlineSnapshot(). snapshot.test.ts import { expect, test } from \"bun:test\"; test(\"toMatchInlineSnapshot()\", () => { expect(new Date()).toMatchInlineSnapshot(); }); Next, update the snapshot with bun test u, which is short for update-snapshots. bun test u Then, voilà! Bun has updated the test file with your snapshot. snapshot.test.ts import { expect, test } from \"bun:test\"; test(\"toMatchInlineSnapshot()\", () => { expect(new Date()).toMatchInlineSnapshot(); expect(new Date()).toMatchInlineSnapshot(`2025-01-18T02:35:53.332Z`); }); You can also use these matchers, which do a similar thing: toThrowErrorMatchingSnapshot() toThrowErrorMatchingInlineSnapshot() test.only() You can use test.only() to run a single test, excluding all other tests. This is useful when you're debugging a specific test, and don't want to run the entire test suite. import { test } from \"bun:test\"; test.only(\"test a\", () => { /* Only run this test */ }); test(\"test b\", () => { /* Don't run this test */ }); Previously, for this to work in Bun, you had to use the only flag. bun test only This was annoying, you'd usually forget to do it, and test runners like Jest don't need it! In Bun 1.2, we've made this \"just work\", without the need for flags. bun test New expect() matchers In Bun 1.2, we added a bunch of matchers to the expect() API. These are the same matchers that are implemented by Jest, Vitest, or the jest-extended library. You can use toContainValue() and derivatives to check if an object contains a value. const object = new Set([\"bun\", \"node\", \"npm\"]); expect(object).toContainValue(\"bun\"); expect(object).toContainValues([\"bun\", \"node\"]); expect(object).toContainAllValues([\"bun\", \"node\", \"npm\"]); expect(object).not.toContainAnyValues([\"done\"]); Or, use toContainKey() and derivatives to check if an object contains a key. const object = new Map([ [\"bun\", \"1.2.0\"], [\"node\", \"22.13.0\"], [\"npm\", \"9.1.2\"], ]); expect(object).toContainKey(\"bun\"); expect(object).toContainKeys([\"bun\", \"node\"]); expect(object).toContainAllKeys([\"bun\", \"node\", \"npm\"]); expect(object).not.toContainAnyKeys([\"done\"]); You can also use toHaveReturned() and derivatives to check if a mocked function has returned a value. import { jest, test, expect } from \"bun:test\"; test(\"toHaveReturned()\", () => { const mock = jest.fn(() => \"foo\"); mock(); expect(mock).toHaveReturned(); mock(); expect(mock).toHaveReturnedTimes(2); }); Custom error messages We've also added support for custom error messages using expect(). You can now pass a string as the second argument to expect(), which will be used as the error message. This is useful when you want to document what the assertion is checking. example.test.ts import { test, expect } from 'bun:test'; test(\"custom error message\", () => { expect(0.1 + 0.2).toBe(0.3); expect(0.1 + 0.2, \"Floating point has precision error\").toBe(0.3); }); 1import { test, expect } from 'bun:test'; 23test(\"custom error message\", () => { 4expect(0.1 + 0.2, \"Floating point has precision error\").toBe(0.3);^ error: expect(received).toBe(expected) error: Floating point has precision error Expected: 0.3 Received: 0.30000000000000004 jest.setTimeout() You can now use Jest's setTimeout() API to change the default timeout for tests in the current scope or module, instead of setting the timeout for each test. jest.setTimeout(60 * 1000); // 1 minute test(\"do something that takes a long time\", async () => { await Bun.sleep(Infinity); }); You can also import setDefaultTimeout() from Bun's test APIs, which does the same thing. We chose a different name to avoid confusion with the global setTimeout() function. import { setDefaultTimeout } from \"bun:test\"; setDefaultTimeout(60 * 1000); // 1 minute Bun is a JavaScript bundler Bun is a JavaScript and TypeScript bundler, transpiler, and minifier that can be used to bundle code for the browser, Node.js, and other platforms. HTML imports In Bun 1.2, we've added support for HTML imports. This allows you to replace your entire frontend toolchain with a single import statement. To get started, pass an HTML import to the static option in Bun.serve: import homepage from \"./index.html\"; Bun.serve({ static: { \"/\": homepage, }, async fetch(req) { // ... api requests }, }); When you make a request to /, Bun automatically bundles the andtags in the HTML files, exposes them as static routes, and serves the result. An index.html file like this: index.htmlHomeBecomes something like this: index.htmlHomeTo read more about HTML imports and how they're implemented, check out the HTML imports documentation. Standalone executables You can use bun build compile to compile your application, and Bun, into a standalone executable. In Bun 1.2, we've added support for cross-compilation. This allows you to build a Windows or macOS binary on a Linux machine, and vice versa. You can run the following command on a macOS or Linux machine, and it will compile a Windows binary. bun build compile target=bun-windows-x64 app.ts [8ms] bundle 1 modules [1485ms] compile app.exe bun-windows-x64-v1.2.0 For Windows specific builds, you can customize the icon and hide the console window. bun build compile windows-icon=./icon.ico windows-hide-console app.ts Bytecode caching You can also use bun build bytecode flag to generate a bytecode cache. This improves the startup time of applications like eslint to be 2x faster. bun build bytecode compile app.ts ./app Hello, world! You can also use the bytecode cache without compile. bun build bytecode outdir=dist app.ts ls dist app.js app.jsc When Bun generates output files, it will also generate .jsc files, which contain the bytecode cache of its respective .js file. Both files are necessary to run, as the bytecode compilation doesn't currently compile async functions, generators, or eval. The bytecode cache can be 8x larger than the source code, so this makes startup faster at a cost of increased disk space. CommonJS output format You can now set the output format to CommonJS with bun build. Previously, only ESM was supported. bun build format=cjs app.ts This makes it easier to create libraries and applications meant for older versions of Node.js. app.ts app.js // app.ts export default \"Hello, world!\"; Better CommonJS detection Some packages really want to trick bundlers and get the current module's file path, do a runtime require, or check if the current module is the main module. They try all kinds of things to make it work, such as: \"use strict\"; if (eval(\"require.main\") === eval(\"module.main\")) { // ... } Bun supports both CommonJS and ESM; in fact, you can use require() and import in the same file. However, one of the challenges of supporting both is that there's a lot of ambiguity. Consider the following code, is it CommonJS or ESM? console.log(\"123\"); There's no way to tell. Then, how about this? console.log(module.require(\"path\")); CommonJS, because it's using module.require() to get the path module. And this? import path from \"path\"; console.log(path); ESM, because it's using import. But, what about this? import path from \"path\"; const fs = require(\"fs\"); console.log(fs.readFileSync(path.resolve(\"package.json\"), \"utf8\")); ESM, because it's using import. If we said it was CommonJS due to the require, then the import would break the code. We want to simplify building stuff in JavaScript, so let's just say it's ESM and not be fussy. Finally, what about this? \"use strict\"; console.log(eval(\"module.require('path')\")); Previously, Bun would have said ESM, because it's the default when there's no way to tell (including when the file extension is ambiguous, there's no \"type\" field in package.json, no export, no import, etc). In Bun 1.2, Bun will say CommonJS, because of the \"use strict\" directive at the top of the file. ESM is always in strict mode, so an explicit \"use strict\" would be redundant. Also, most build tools that output CommonJS include \"use strict\" at the top of the file. So we can now use this as a last-chance heuristic when it's completely ambiguous whether the file is CommonJS or ESM. Plugin API Bun has a universal plugin API for extending the bundler and the runtime. You can use plugins to intercept import() statements, add custom loaders for extensions like .yaml, and implement frameworks for Bun. onBeforeParse() In Bun 1.2, we're introducing a new lifecycle hook for plugins, onBeforeParse(). Unlike the existing lifecycle hooks that run JavaScript code, this hook must be a N-API addon, which can be implemented in a compiled language like Rust, C/C++, or Zig. The hook is called immediately before parsing, without cloning the source code, without undergoing string conversion, and with practically zero overhead. For example, you can create a Rust plugin that replaces all occurrences of foo with bar. bun add g @napi-rs/cli napi new cargo add bun-native-plugin From there, you can implement the onBeforeParse() hook. These are advanced APIs, primarily designed for plugin and framework authors who want to use native code to make their plugins really fast. lib.rs build.ts use bun_native_plugin::{define_bun_plugin, OnBeforeParse, bun, Result, anyhow, BunLoader}; use napi_derive::napi; define_bun_plugin!(\"foo-bar-plugin\"); #[bun] pub fn replace_foo_with_bar(handle: &mut OnBeforeParse) > Result { let input_source_code = handle.input_source_code()?; let output_source_code = input_source_code.replace(\"foo\", \"bar\"); handle.set_output_source_code(output_source_code, BunLoader::BUN_LOADER_JSX); Ok(()) } Other changes We also made a lot of other improvements to bun build and the Bun.build() APIs. Inject environment variables You can now inject environment variables from your system environment into your bundle. CLI JavaScript bun build env=\"PUBLIC_*\" app.tsx bun build drop You can use drop to remove function calls from your JavaScript bundle. For example, if you pass drop=console, all calls to console.log() will be removed from your code. JavaScript CLI import { build } from \"bun\"; await build({ entrypoints: [\"./index.tsx\"], outdir: \"./out\", drop: [\"console\", \"anyIdentifier.or.propertyAccess\"], }); Banner and footer You can now use the banner and footer options in bun build to add content above or below the bundle. CLI JavaScript bun build banner \"/* Banner! */\" footer \"/* Footer! */\" app.ts This is useful for appending content above or below the bundle, such as a license or copyright notice. /** * Banner! */ export default \"Hello, world!\"; /** * Footer! */ Bun.embeddedFiles() You can use the new Bun.embeddedFiles() API to see a list of all embedded files in a standalone executable, compiled with bun build compile. import { embeddedFiles } from \"bun\"; for (const file of embeddedFiles) { console.log(file.name); // \"logo.png\" console.log(file.size); // 1234 console.log(await file.bytes()); // Uint8Array(1234) [...] } require.main === module Previously, using require.main === module would mark the module as CommonJS. Now, Bun rewrites this into import.meta.main, meaning you can use this pattern alongside import statements. import * as fs from \"fs\"; if (typeof require !== \"undefined\" && require.main === module) { console.log(\"main!\", fs); } ignore-dce-annotations Some JavaScript tools support special annotations that can influence behavior during dead-code elimination. For example, the @__PURE__ annotation tells bundlers that a function call is pure (regardless of whether it actually is), and that the call can be removed if it is not used. let button = /* @__PURE__ */ React.createElement(Button, null); Sometimes, a library may include incorrect annotations, which can cause Bun to remove side effects which were needed. To workaround these issue, you can use the ignore-dce-annotations flag when running bun build to ignore all annotations. This should only be used if dead-code elimination breaks bundles, and fixing the annotations should be preferred to leaving this flag on. packages=external You can now control if package dependencies are included in your bundle or not. If the import does not start with ., .. or /, then it is considered a package. CLI JavaScript bun build ./index.ts packages external This is useful when bundling libraries. It lets you reduce the number of files your users have to download, while continuing to support peer or external dependencies. Built-in CSS parser In Bun 1.2, we implemented a new CSS parser and bundler in Bun. It's derived from the great work of LightningCSS, and re-written from Rust to Zig so it can be integrated with Bun's custom JavaScript and TypeScript parser, bundler, and runtime. Bun is an complete toolkit for running and building JavaScript and TypeScript. One of the missing pieces of Bun's built-in JavaScript bundler, bun build, is support for bundling and minifying CSS. How it works CSS bundlers combine multiple CSS files and assets referenced using directives like url, @import, @font-face, into a single CSS file you can send to browsers, avoiding a waterfall of network requests. index.css foo.css bar.css @import \"foo.css\"; @import \"bar.css\"; To see how it works, you can try it using bun build. bun build ./index.css You'll see how the CSS files are combined into a single CSS file. dist.css /** foo.css */ .foo { background: red; } /** bar.css */ .bar { background: blue; } Import .css files from JavaScript We've also made it possible to import .css files in your JavaScript and TypeScript code. This will create an additional CSS entrypoint that combines all the CSS files imported from a JavaScript module graph, along with @import rules. index.ts import \"./style.css\"; import MyComponent from \"./MyComponent.tsx\"; // ... rest of your app In this example, if MyComponent.tsx imports another CSS file, instead of adding extra .css files to the bundle, all the CSS imported per entrypoint is flattened into a single CSS file. shell bun build ./index.ts outdir=dist index.js 0.10 KB index.css 0.10 KB [5ms] bundle 4 modules Using Bun.build() You can also bundle CSS using the programmatic Bun.build() API. This allows you to bundle both CSS and JavaScript in the same build, with the same API. api.ts import { build } from \"bun\"; const results = await build({ entrypoints: [\"./index.css\"], outdir: \"./dist\", }); console.log(results); Bun APIs In addition to supporting Node.js and Web APIs, Bun also has a growing standard library that makes it easy to do common tasks, without adding more external dependencies. Static routes in Bun.serve() Bun has a built-in HTTP server that makes it easy to respond to HTTP requests using standard APIs like Request and Response. In Bun 1.2, we added support for static routes using the new static property. To define a static route, pass the request path as the key and a Response object as the value. import { serve } from \"bun\"; serve({ static: { \"/health-check\": new Response(\"Ok!\"), \"/old-link\": Response.redirect(\"/new-link\", 301), \"/api/version\": Response.json( { app: require(\"./package.json\").version, bun: Bun.version, }, { headers: { \"X-Powered-By\": \"bun\" }, }, ), }, async fetch(request) { return new Response(\"Dynamic!\"); }, }); Static routes are up to 40% faster than doing it yourself in the fetch() handler. The response body, headers, and status code are cached in memory, so there's no JavaScript allocation or garbage collection. If you want to reload the static routes, you can use the reload() method. This is useful if you want to update the static routes on a schedule, or when a file changes. import { serve } from \"bun\"; const server = serve({ static: { \"/\": new Response(\"Static!\"), }, async fetch(request) { return new Response(\"Dynamic!\"); }, }); setInterval(() => { const date = new Date().toISOString(); server.reload({ static: { \"/\": new Response(`Static! Updated at ${date}`), }, }); }, 1000); Bun.udpSocket() While we added support for node:dgram in Bun 1.2, we also introduced UDP socket support in Bun's APIs. Bun.udpSocket() is a faster, modern alternative and is similar to the existing Bun.listen() API. import { udpSocket } from \"bun\"; const server = await udpSocket({ socket: { data(socket, data, port, addr) { console.log(`Received data from ${addr}:${port}:`, data.toString()); }, }, }); const client = await udpSocket({ port: 0 }); client.send(\"Hello!\", server.port, \"127.0.0.1\"); Bun's UDP socket API is built for performance. Unlike Node.js, it can send multiple UDP datagrams with a single syscall, and supports responding to backpressure from the operating system. const socket = await Bun.udpSocket({ port: 0, socket: { drain(socket) { // Socket is no longer under backpressure }, }, }); // Send multiple UDP datagrams with a single syscall: // [ , ,][] socket.sendMany([ [\"Hello\", 12345, \"127.0.0.1\"], [\"from\", 12346, \"127.0.0.1\"], [\"Bun 1.2\", 12347, \"127.0.0.1\"], ]); This is great for building game servers that need to broadcast game state updates to every peer. Bun.file() Bun has a built-in Bun.file() API that makes it easy to read and write files. It extends the Web-standard Blob API, and makes it easier to work with files in a server environment. In Bun 1.2, we've added support for even more Bun.file() APIs. delete() You can now delete files using the delete() method. An alias of unlink() is also supported. import { file } from \"bun\"; await file(\"./package.json\").delete(); await file(\"./node_modules\").unlink(); stat() You can now use the stat() method to get a file's metadata. This returns the same Stats object as fs.stat() in Node.js. import { file } from \"bun\"; const stat = await file(\"./package.json\").stat(); console.log(stat.size); // => 1024 console.log(stat.mode); // => 33206 console.log(stat.isFile()); // => true console.log(stat.isDirectory()); // => false console.log(stat.ctime); // => 2025-01-21T16:00:00+00:00 Support for S3 files With newly added built-in support for S3, you can use the same Bun.file() APIs with a S3 file. import { s3 } from \"bun\"; const stat = await s3(\"s3://folder/my-file.txt\").stat(); console.log(stat.size); // => 1024 console.log(stat.type); // => \"text/plain;charset=utf-8\" await s3(\"s3://folder/\").unlink(); Bun.color() To support CSS with bun build, we implemented our own CSS parser in Bun 1.2. In doing this work, we decided to expose some useful APIs for working with colors. You can use Bun.color() to parse, normalize, and convert colors into a variety of formats. It supports CSS, ANSI color codes, RGB, HSL, and more. import { color } from \"bun\"; color(\"#ff0000\", \"css\"); // => \"red\" color(\"rgb(255, 0, 0)\", \"css\"); // => \"red\" color(\"red\", \"ansi\"); // => \"\\x1b[31m\" color(\"#f00\", \"ansi-16m\"); // => \"\\x1b[38;2;255;0;0m\" color(0xff0000, \"ansi-256\"); // => \"\\u001b[38;5;196m\" color({ r: 255, g: 0, b: 0 }, \"number\"); // => 16711680 color(\"hsl(0, 0%, 50%)\", \"{rgba}\"); // => { r: 128, g: 128, b: 128, a: 1 } dns.prefetch() You can use the new dns.prefetch() API to prefetch DNS records before they are needed. This is useful if you want to pre-warm the DNS cache on startup. import { dns } from \"bun\"; // ...on startup dns.prefetch(\"example.com\"); // ...later on await fetch(\"https://example.com/\"); This will prefetch the DNS record for example.com and make it available for use in fetch() requests. You can also use the dns.getCacheStats() API to observe the DNS cache. import { dns } from \"bun\"; await fetch(\"https://example.com/\"); console.log(dns.getCacheStats()); // { // cacheHitsCompleted: 0, // cacheHitsInflight: 0, // cacheMisses: 1, // size: 1, // errors: 0, // totalCount: 1, // } Helpful utilities We also added a few random utilities to Bun's APIs. Bun.inspect.table() You can now use Bun.inspect.table() to format tabular data into a string. It's similar to console.table, except it returns a string rather than printing to the console. console.log( Bun.inspect.table([ { a: 1, b: 2, c: 3 }, { a: 4, b: 5, c: 6 }, { a: 7, b: 8, c: 9 }, ]), ); // ┌───┬───┬───┬───┐ // │ │ a │ b │ c │ // ├───┼───┼───┼───┤ // │ 0 │ 1 │ 2 │ 3 │ // │ 1 │ 4 │ 5 │ 6 │ // │ 2 │ 7 │ 8 │ 9 │ // └───┴───┴───┴───┘ Bun.randomUUIDv7() You can use Bun.randomUUIDv7() to generate a UUID v7, a monotonic UUID suitable for sorting and databases. index.ts import { randomUUIDv7 } from \"bun\"; const uuid = randomUUIDv7(); // => \"0192ce11-26d5-7dc3-9305-1426de888c5a\" New in Bun's built-in SQLite client Bun has a built-in SQLite client that makes it easy to query SQLite databases. In Bun 1.2, we've added a few new features to make it even easier to use. ORM-less object mapping When you query a SQL database, you often want to map your query results to a JavaScript object. That's why there's so many popular ORM (Object-Relational Mapping) packages like Prisma and TypeORM. You can now use query.as(Class) to map query results to instances of a class. This lets you attach methods, getters, and setters without using an ORM. import { Database } from \"bun:sqlite\"; class Tweet { id: number; text: string; username: string; get isMe() { return this.username === \"jarredsumner\"; } } const db = new Database(\"tweets.db\"); const tweets = db.query(\"SELECT * FROM tweets\").as(Tweet); for (const tweet of tweets.all()) { if (!tweet.isMe) { console.log(`${tweet.username}: ${tweet.text}`); } } For performance reasons, class constructors, default initializers, and private fields are not supported. Instead, it uses the equivalent of Object.create() to create a new object with the class's prototype and assigns the values of the row to it. It's also important to note that this is not an ORM. It doesn't manage relationships, generate SQL queries, or anything like that. However, it does remove a lot of boilerplate to get JavaScript objects from SQLite! Iterable queries You can now use query.iterate() to get an iterator that yields rows as they are returned from the database. This is useful when you want to process rows at a time, without loading them all into memory. import { Database } from \"bun:sqlite\"; class User { id: number; email: string; } const db = new Database(\"users.db\"); const rows = db.query(\"SELECT * FROM users\").as(User).iterate(); for (const row of rows) { console.log(row); } You can also iterate over the query using a for loop, without calling iterate(). for (const row of db.query(\"SELECT * FROM users\")) { console.log(row); // { id: 1, email: \"hello@bun.sh\" } } Strict query parameters You can now omit the $, @, or : prefix when passing JavaScript values as query parameters. import { Database } from \"bun:sqlite\"; const db = new Database(\":memory:\", { strict: false, strict: true, }); const query = db.query(`select $message;`); query.all({ $message: \"Hello world\" message: \"Hello world\" }); To use this behavior, enable the strict option. This will allow you to omit the $, @, or : prefixes, and will throw an error if a parameter is missing. Tracking changed rows You can now access the number of rows changed and the last inserted row ID when running queries. import { Database } from \"bun:sqlite\"; const db = new Database(\":memory:\"); db.run(`CREATE TABLE users (id INTEGER, username TEXT)`); const { changes, lastInsertRowid } = db.run( `INSERT INTO users VALUES (1, 'jarredsumner')`, ); console.log({ changes, // => 1 lastInsertRowid, // => 1 }); BigInt support If you want to use 64-bit integers, you can enable the safeIntegers option. This will return integers as as a BigInt, instead of a truncated number. import { Database } from \"bun:sqlite\"; const db = new Database(\":memory:\", { safeIntegers: true }); const query = db.query( `SELECT ${BigInt(Number.MAX_SAFE_INTEGER) + 1n} as maxInteger`, ); const { maxInteger } = query.get(); console.log(maxInteger); // => 9007199254740992n You can also enable this on a per-query basis using the safeIntegers() method. import { Database } from \"bun:sqlite\"; const db = new Database(\":memory:\", { strict: true }); const query = db.query(\"SELECT $value as value\").safeIntegers(true); const { value } = query.get({ value: BigInt(Number.MAX_SAFE_INTEGER) + 1n, }); console.log(value); // => 9007199254740992n Reliable cleanup with using With JavaScript's using syntax, you can automatically close statements and databases when their variables go out of scope. This allows you to clean up database resources, even if there's a thrown error. Read on for more details on Bun's support for this new JavaScript feature. import { Database } from \"bun:sqlite\"; { using db = new Database(\"file.db\"); using query = db.query(\"SELECT * FROM users\"); for (const row of query.all()) { throw new Error(\"Oops!\"); // no try/catch block needed! } } // scope ends here, so `db` and `query` are automatically closed Compile and run C from JavaScript We've added experimental support for compiling and running C from JavaScript. This is a simple way to use C system libraries from JavaScript without a build step. random.c random.ts #include#includeint random() { return rand() + 42; } Why is this useful? For advanced use-cases or where performance is really important, you sometimes need to use system libraries from JavaScript. Today, the most common way to do this is by compiling a N-API addon using node-gyp. You might notice if a package uses this, because it runs a postinstall script when you install it. However, this isn't a great experience. Your system needs a modern version of Python and a C compiler, which is usually installed using a command like apt install build-essential. And hopefully you don't run into a compiler or node-gyp error, which can be quite frustrating. gyp ERR! command \"/usr/bin/node\" \"/tmp/node-gyp@latest--bunx/node_modules/.bin/node-gyp\" \"configure\" \"build\" gyp ERR! cwd /bun/test/node_modules/bktree-fast gyp ERR! node v v12.22.9 gyp ERR! node-gyp v v9.4.0 gyp ERR! Node-gyp failed to build your package. gyp ERR! Try to update npm and/or node-gyp and if it does not help file an issue with the package author. error: \"node-gyp\" exited with code 7 (SIGBUS) How does it work? In case you didn't know, Bun embeds a built-in C compiler called tinycc. Surprise! Unlike traditional C compilers, like gcc or clang, that can take seconds to compile a simple program, tinycc compiles simple C code in milliseconds. This makes it possible for Bun to compile your C code on-demand, without a build step. Using the bun:ffi APIs, you can compile and run C code from JavaScript. Here's an example project that uses the N-API to return a JavaScript string from C code. hello-napi.c hello-napi.js #includenapi_value hello_napi(napi_env env) { napi_value result; napi_create_string_utf8(env, \"Hello, N-API!\", NAPI_AUTO_LENGTH, &result); return result; } Instead of requiring a build step with node-gyp, as long as you have Bun, this just works. musl support In Bun 1.2, we've introduced a new build of Bun that works on Linux distros that use the musl libc instead of glibc, like Alpine Linux. This is supported on both Linux x64 and aarch64. You can also use the alpine version of Bun in Docker. docker run rm it oven/bun:alpine bun print 'Bun.file(\"/etc/alpine-release\").text()' 3.20.5 While musl enables smaller container images, it tends to perform slightly slower than the glibc version of Bun. We recommend using glibc unless you have a specific reason to use musl. JavaScript features JavaScript is a language that is constantly evolving. In Bun 1.2, even more JavaScript features are available thanks to the collaboration of the TC39 committees, and the hard work of the WebKit team. Import attributes You can now specify an import attribute when importing a file. This is useful when you want to import a file that isn't JavaScript code, like a JSON object or a text file. import json from \"./package.json\" with { type: \"json\" }; typeof json; // \"object\" import html from \"./index.html\" with { type: \"text\" }; typeof html; // \"string\" import toml from \"./bunfig.toml\" with { type: \"toml\" }; typeof toml; // \"object\" You can also specify import attributes using import(). const { default: json } = await import(\"./package.json\", { with: { type: \"json\" }, }); typeof json; // \"object\" Resource management with using With the newly introduced using syntax in JavaScript, you can automatically close resources when a variable goes out of scope. Instead of defining a variable with let or const, you can now define a variable with using. import { serve } from \"bun\"; { using server = serve({ port: 0, fetch(request) { return new Response(\"Hello, world!\"); }, }); doStuff(server); } function doStuff(server) { // ... } In this example, the server is automatically closed when the server variable goes out of scope, even if an exception is thrown. This is useful for ensuring that resources are properly cleaned up, especially in tests. To support this, an object's prototype must define a [Symbol.dispose] method, or [Symbol.asyncDispose] method if it's an async resource. class Resource { [Symbol.dispose]() { /* ... */ } } using resource = new Resource(); class AsyncResource { async [Symbol.asyncDispose]() { /* ... */ } } await using asyncResource = new AsyncResource(); We've also added support for using in dozens of Bun APIs, including Bun.spawn(), Bun.serve(), Bun.connect(), Bun.listen(), and bun:sqlite. import { spawn } from \"bun\"; import { test, expect } from \"bun:test\"; test(\"able to spawn a process\", async () => { using subprocess = spawn({ cmd: [process.execPath, \"-e\", \"console.log('Hello, world!')\"], stdout: \"pipe\", }); // Even if this expectation fails, the subprocess will still be closed. const stdout = new Response(subprocess.stdout).text(); await expect(stdout).resolves.toBe(\"Hello, world!\"); }); Promise.withResolvers() You can use Promise.withResolvers() to create a promise that resolves or rejects when you call the resolve or reject functions. const { promise, resolve, reject } = Promise.withResolvers(); setTimeout(() => resolve(), 1000); await promise; This is a useful alternative to new Promise(), since you don't need to create a new scope. const promise = new Promise((resolve, reject) => { setTimeout(() => resolve(), 1000); }); await promise; Promise.try() You can use Promise.try() to create a promise that wraps a synchronous or asynchronous function. const syncFn = () => 1 + 1; const asyncFn = async (a, b) => 1 + a + b; await Promise.try(syncFn); // => 2 await Promise.try(asyncFn, 2, 3); // => 6 This is useful if you don't know if a function is synchronous or asynchronous. Previously, you would have to do something like this: await new Promise((resolve) => resolve(syncFn())); await new Promise((resolve) => resolve(asyncFn(2, 3))); Error.isError() You can now check if an object is an Error instance using Error.isError(). Error.isError(new Error()); // => true Error.isError({}); // => false Error.isError(new (class Error {})()); // => false Error.isError({ [Symbol.toStringTag]: \"Error\" }); // => false This is more correct than using instanceof because the prototype chain can be tampered with, and instanceof can return false-negatives when using node:vm. import { runInNewContext } from \"node:vm\"; const crossRealmError = runInNewContext(\"new Error()\"); crossRealmError instanceof Error; // => false Error.isError(crossRealmError); // => true Uint8Array.toBase64() You can now encode and decode base64 strings using Uint8Array. toBase64() converts a Uint8Array to a base64 string fromBase64() converts a base64 string to a Uint8Array new Uint8Array([1, 2, 3, 4, 5]).toBase64(); // \"AQIDBA==\" Unit8Array.fromBase64(\"AQIDBA==\"); // [1, 2, 3, 4, 5] These APIs are standard alternatives to the usage of Buffer.toString(\"base64\") in Node.js. Uint8Array.toHex() You can also convert Uint8Array to and from hex strings. toHex() converts a Uint8Array to a hex string fromHex() converts a hex string to a Uint8Array new Uint8Array([1, 2, 3, 4, 5]).toHex(); // \"0102030405\" Unit8Array.fromHex(\"0102030405\"); // [1, 2, 3, 4, 5] These APIs are standard alternatives to the usage of Buffer.toString(\"hex\") in Node.js. Iterator helpers There are new APIs that make it easier to work with JavaScript iterators and generators. iterator.map(fn) Returns an iterator that yields the results of the fn function applied to each value of the original iterator, similar to Array.prototype.map. function* range(start: number, end: number): Generator { for (let i = start; ix * 2); result.next(); // { value: 6, done: false } iterator.flatMap(fn) Returns an iterator that yields the values of the original iterator, but flattens the results of the fn function, similar to Array.prototype.flatMap. function* randomThoughts(): Generator { yield \"Bun is written in Zig\"; yield \"Bun runs JavaScript and TypeScript\"; } const result = randomThoughts().flatMap((x) => x.split(\" \")); result.next(); // { value: \"Bun\", done: false } result.next(); // { value: \"is\", done: false } // ... result.next(); // { value: \"TypeScript\", done: false } iterator.filter(fn) Returns an iterator that only yields values that pass the fn predicate, similar to Array.prototype.filter. function* range(start: number, end: number): Generator { for (let i = start; ix % 2 === 0); result.next(); // { value: 4, done: false } iterator.take(n) Returns an iterator that yields the first n values of the original iterator. function* odds(): Generator { let i = 1; while (true) { yield i; i += 2; } } const result = odds().take(1); result.next(); // { value: 1, done: false } result.next(); // { done: true } iterator.drop(n) Returns an iterator that yields all values of the original iterator, except the first n values. function* evens(): Generator { let i = 0; while (true) { yield i; i += 2; } } const result = evens().drop(2); result.next(); // { value: 4, done: false } result.next(); // { value: 6, done: false } iterator.reduce(fn, initialValue) Reduces the values of an iterator with a function, similar to Array.prototype.reduce. function* powersOfTwo(): Generator { let i = 1; while (true) { yield i; i *= 2; } } const result = powersOfTwo() .take(5) .reduce((acc, x) => acc + x, 0); console.log(result); // 15 iterator.toArray() Returns an array that contains all the values of the original iterator. Make sure that the iterator is finite, otherwise this will cause an infinite loop. function* range(start: number, end: number): Generator { for (let i = start; i{ yield \"Bun is written in Zig\"; yield \"Bun runs JavaScript and TypeScript\"; } const result = randomThoughts().forEach((x) => console.log(x)); // Bun is written in Zig // Bun runs JavaScript and TypeScript iterator.find(fn) Returns the first value of the original iterator that passes the fn predicate, similar to Array.prototype.find. If no such value exists, it returns undefined. function* range(start: number, end: number): Generator { for (let i = start; ix % 100 === 0); console.log(result); // undefined Float16Array There's now support for 16-bit floating point arrays using Float16Array. While 16-bit floating point numbers are less precise than 32-bit floating point numbers, they are much more memory efficient. const float16 = new Float16Array(3); const float32 = new Float32Array(3); for (let i = 0; i{ console.log(\"Received data:\", data); }; AbortSignal.any() You can use AbortSignal.any() to combine multiple instances of AbortSignal. If one of the child signals is aborted, the parent signal is also aborted. const { signal: firstSignal } = new AbortController(); fetch(\"https://example.com/\", { signal: firstSignal }); const { signal: secondSignal } = new AbortController(); fetch(\"https://example.com/\", { signal: secondSignal }); // Cancels if either `firstSignal` or `secondSignal` is aborted const signal = AbortSignal.any([firstSignal, secondSignal]); await fetch(\"https://example.com/slow\", { signal }); Behaviour changes Bun 1.2 contains a few behaviour tweaks to that you should be aware of, but we think is unlikely to break your code. We avoid making these changes unless we think the status-quo is so broken that it's worth it. bun run uses the correct directory Previously, when you ran a package.json script using bun run, the working directory of the script was the same as the current working directory of your shell. In most cases, you don't notice a difference, because your shell's working directory is usually the same as the parent directory of your package.json file. cd /path/to/project ls package.json bun run pwd /path/to/project However, if you cd into a different directory, you'll notice the difference. cd dist bun run pwd /path/to/project/dist This does not match what other package managers do, like npm or yarn, and more-often-than-not causes unexpected behaviour. In Bun 1.2, the working directory of the script is now the parent directory of the package.json file, instead of the current working directory of your shell. cd /path/to/project/dist bun run pwd /path/to/project/dist /path/to/project Uncaught errors in bun test Previously, bun test would not fail when there was an uncaught error or rejection between test cases. import { test, expect } from \"bun:test\"; test(\"should have failed, but didn't\", () => { setTimeout(() => { throw new Error(\"Oops!\"); }, 1); }); In Bun 1.2, this has now been fixed, and bun test will report the failure. # Unhandled error between tests --------------------------- 1import { test, expect } from \"bun:test\"; 23test(\"should have failed, but didn't\", () => { 4setTimeout(() => { 5throw new Error(\"Oops!\"); ^ error: Oops! at foo.test.ts:5:11 --------------------------- server.stop() returns a Promise Previously, there was no way to gracefully wait for connections to close from Bun's HTTP server. To make this possible, we made stop() return a promise, which resolves when in-flight HTTP connections are closed. interface Server { stop(): void; stop(): Promise; } Bun.build() rejects when it fails Previously, when Bun.build() would fail, it would report the error in the logs array. This was often confusing, because the promise would resolve successfully. import { build } from \"bun\"; const result = await build({ entrypoints: [\"./bad.ts\"], }); console.log(result.logs[0]); // error: ModuleNotFound resolving \"./bad.ts\" (entry point) In Bun 1.2, Bun.build() will now reject when it fails, instead of returning errors in the logs array. const result = build({ entrypoints: [\"./bad.ts\"], }); await result; // error: ModuleNotFound resolving \"./bad.ts\" (entry point) If you want to restore to the old behaviour, you can set the throw: false option. const result = await build({ entrypoints: [\"./bad.ts\"], throw: false, }); bun p is an alias for bun print Previously, bun p was an alias for bun port, which was used to change the port of Bun.serve(). The alias was added before Bun supported bun print. To match Node.js, we've changed bun p to be an alias for bun print. bun p 'new Date()' 2025-01-17T22:55:27.659Z bun build sourcemap Previously, using bun build sourcemap would default to inlined source maps. bun build sourcemap ./index.ts outfile ./index.js index.js console.log(\"Hello Bun!\"); //# sourceMappingURL=data:application/json;base64,... This was confusing, because it is the opposite of what other tools do, like esbuild. In Bun 1.2, bun build sourcemap now defaults to linked source maps. index.js index.js.map console.log(\"Hello Bun!\"); If you want to restore to the old behaviour, you can use sourcemap=inline. Bun is even faster We spend a lot of time improving performance in Bun. We post almost daily updates of \"In the next version of Bun\" which you can follow on @bunjavascript. Here's a preview of some of the performance improvements we made in Bun 1.2. node:http2 is 2x faster node:http is 5x faster at uploading to S3 path.resolve() is 30x faster fetch() is 2x faster at DNS resolution bun hot uses 2x less memory fs.readdirSync() is 5% faster on macOS String.at() is 44% faster atob() is 8x faster fetch() decompresses 30% faster Buffer.from(String, \"base64\") is 30x faster JSON.parse() is up to 4x faster Bun.serve() has 2x more throughput Error.captureStackTrace() is 9x faster fs.readFile() is 10% faster console.log(String) is 50% faster JavaScript is faster on Windows Getting started That's it — that's Bun 1.2, and it's still just the beginning for Bun. We've added a ton of new features and APIs that make it easier than ever to build full-stack JavaScript and TypeScript applications. Install Bun To get started, run any of the following commands in your terminal. curl powershell npm brew docker curl fsSL https://bun.sh/installbash Upgrade Bun If you already installed Bun, you can upgrade with the following command. bun upgrade We're hiring We're hiring engineers, designers, and contributors to JavaScript engines like V8, WebKit, Hermes, and SpiderMonkey to join our team in-person in San Francisco to build the future of JavaScript. You can check out our careers page or send us an email. Thank you! Bun is free, open source, and MIT-licensed. We receive a lot of open source contributions from the community. So, we'd like to thank everyone who has fixed a bug or contributed a feature. We appreciate your help! @nektro @dylan-conway @pfgithub @heimskr @cirospaciari @Devanand-Sharma @lgarron @zackradisic @DonIsaac @RiskyMH @paperclover @rgarcia @fel1x-developer @190n @lcrespom @versecafe @dtinth @yooneskh @sroussey @ArnaudBarre @kjjd84 @cainba @robertshuford @Gobd @citkane @marcosrjjunior @thecrypticace @metonym @ianzone @chawyehsu @komiya-atsushi @jbergstroem @sirmews @laesse @WingLim @martinamps @brainkim @Electroid @01101sam @eventualbuddha @snoglobe @nattui @swen128 @hex2f @imide @cdfzo @alii @Kapsonfire-DE @NReilingh @SunsetTechuila @luavixen @rtzll @advaith1 @gvilums @Nanome203 @ippsav @guest271314 @yamalight @ceymard @adhamu @gjungb @kaioduarte @BjornTheProgrammer @arthurvanl @lirantal @Eckhardt-D @CanadaHonk @sourcegr @alexlamsl @refi64 @huseeiin @FaSe22 @deiga @Skywalker13 @KiwiZ0 @lewismiddleton @matubu @mjomble @wpaulino @Xmarmalade @bakkot @stilt0n @levabala @DannyJJK @Marukome0743 @sacsbrainz @mohit-s96 @fmorency @jakeboone02 @17hz @jakebailey @oddyamill @MARCROCK22 @vktrl @mroyme @inad9300 @billywhizz @pythonmcpi @m1212e @dariushalipour @eval @davidstevens37 @zpix1 @HibanaSama @mangs @victor-homyakov @silverwind @ghoshArnab @Ptitet @ThatOneBro @Imgodmaoyouknow @lmmfranco @farcaller @ryuujo1573 @otecd @bjon @rista404 @trcio @kdrag0n @speelbarrow @0livare @exoego @vadzim @umarfchy @jmho @panva @vitch @perkrlsn @ibanks42 @erik-dunteman @nmarks413 @forcefieldsovereign @bomberstudios @mohiwalla @surprisedpika @ShrootBuck @oscarfsbs @diogomdp @LudvigHz @nacmartin @nithinkjoy-tech @Sushants-Git @tobycm @creator318 @janos-r @AbhiPrasad @JonnyBurger @HUMORCE @zawodskoj @eigilsagafos @jess-render @gaurishhs @ridiculousfish @jakeg @ananis25 @DaleSeo @ahaoboy @lafkpages @henrikstorck @rcaselles @yhdgms1 @e3dio @jrmccannon @anchan828 @ghost @fzn0x @windwiny @RanolP @dsernst @yus-ham @jlucaso1 @KilianB @josephjclark @Uziniii @erikbrinkman @boyer-victor @welfuture @jwigert @Deckluhm @liudonghua123 @tuttarealstep @LukasKastern @jdfwarrior @evanshortiss @jdalton @jprinaldi @yoavbls @tomerh2001 @Bellisario @sitiom",
    "commentLink": "https://news.ycombinator.com/item?id=42801370",
    "commentBody": "Bun 1.2 Is Released (bun.sh)237 points by ksec 12 hours agohidepastfavorite150 comments iainmerrick 3 hours agoLots of good stuff here, but I do wonder if some of the default behaviour is getting a little too magical: When you use new Response(s3.file(...)), instead of downloading the S3 file to your server and sending it back to the user, Bun redirects the user to the presigned URL for the S3 file. That's a rather surprising choice for the default, and it's not at all obvious how you'd disable it if you don't want to expose your S3 bucket directly. reply hombre_fatal 2 hours agoparentPass a stream into the response. Response(file.stream()) reply mort96 1 hour agorootparentI mean thanks but that doesn't change the fact that > That's a rather surprising choice for the default, and it's not at all obvious how you'd disable it if you don't want to expose your S3 bucket directly. reply iainmerrick 8 minutes agorootparentTo be fair, it does answer my “not at all obvious” bit just calling stream() is pretty clear! I should have thought a little longer. reply Cthulhu_ 10 hours agoprevWhy are they putting 3rd party (databases) and external (s3) librares into their core / std lib? Wouldn't that be better as an optional library? I think a runtime like this should be very, very careful with what they put into the std lib, adding these already makes it feel like a kitchen sink project. reply jbergstroem 9 hours agoparentI think this position is also one of the reasons it gained attention. Batteries included is a popular and chosen route for many. Setting up typescript can be hard. Same goes for webpack, s3, postgres, jest and more. I also find the simplified file and stream access quite interesting. Lets wait and see how a distributed deployment provider turns out. reply homebrewer 9 hours agorootparentWhich means they did not learn from Python's mistakes. You need to look further than the next couple of years. For some reason during the past 10 years or so it has become fashionable to throw away everything the industry has learned in decades prior and start anew, repeating the same mistakes. We'll never turn into proper engineering with such attitude. https://lwn.net/Articles/776239 https://lwn.net/Articles/888043 https://lwn.net/Articles/790677 reply flohofwoe 8 hours agorootparentThe opinionated batteries-included stdlib was exactly what made Python popular in the first place though (and even despite its 'weird' syntax). And tbh, most of the current problems were also only added in the Python 3.x era (e.g. 3.x looks very much like a 'second system' https://en.wikipedia.org/wiki/Second-system_effect) reply libwithttl 8 hours agorootparentprevThis doesn't seem like a mistake other than how it is the obvious lead to the scope creep, maintenance creep pipeline. Python having an highly capable standard lib is exactly why some people love it, specially \"10 years ago\" when it was very slow to rev up your machines that need to be very much isolated from other machines, and you just had to have a bright enough person on systems making sure everything was included from the get go. Python saved the ass of more than 0 people reading this discussion right now on that merit alone. I think there's a simple solution to all this. Libraries targeting third party protocols get an expiration date and have to forcefully be replaced by name after a given number of versions. Even if they keep the same underlying code, still change the name to force developers to look up its usage and legacy. How many versions? However many equates to the threshold you use to call most systems \"legacy\". I don't mind some job security and some timebomb punishment aimed at dinosaurs. I have bigger and more consistent issues with that than with weter or not C++ let's me crack a .rar without extra libs. reply spiffytech 5 hours agorootparentprev> during the past 10 years or so it has become fashionable to throw away everything the industry has learned in decades prior and start anew This is a curious take to me. I've spent the last 10 years seeing people claim again and again that if JS just had common stuff built in like , we wouldn't have all this library churn, node_modules bloat, and left-pad silliness. That the mistake was not including a standard library. reply pjmlp 2 hours agorootparentprevBatteries included mean wherever there is a fully compliant implementation, there is something available, even if it isn't the best solution in the galaxy. Whereas the best solution in the galaxy might only work in a few selected planets, in other ecosystems without batteries. I prefer batteries included, and not having a culture with a function per package. reply nindalf 1 hour agorootparentprevI think their move away from a binary lock file to a text based lock file in this release makes this pretty clear they shoot first and ask questions later. Any of those problems they've identified with the binary lock file are kinda obvious if you think about it for a bit. A strong indicator that you should think about it is that the popular languages with package managers (npm, ruby, rust) have text based lock files. The fact that the bun team didn't think about it and thought that binary was better because it was faster and no one had thought of this idea feels like hubris to me. It's cool that they're doing the mainstream thing now, but it's something for them to think about. reply Rucadi 8 hours agorootparentprevPython's \"mistakes\" are my weapons in my restricted corporate production system. reply poulpy123 8 hours agorootparentprevBatterie-included is one of the main things that made python great and useful before it became big enough to not need it (don't know for bun). reply ubercore 8 hours agorootparentprevAs most things do, it cuts both ways. Rust suffers from their very slow adoption of libraries into a standard library, imo. reply recursivedoubts 4 hours agorootparentprev\"mistakes\" https://www.tiobe.com/tiobe-index/ reply synergy20 2 hours agorootparentprevgolang is also battery included which seems great. i hope lua can have something similar though smaller reply e3bc54b2 9 hours agorootparentprevI despise Python's tooling and never touch it willingly. That said, Python's 'mistake' also made it one of the most used languages ever. For nearly 2 decades, you could just type `python` in terminal and get rolling, and that was invaluable. The only real 'mistake' that Python did was breaking backwards compatibility so spectacularly that single greatest feature was rendered useless. reply Yajirobe 7 hours agorootparent> single greatest feature was rendered useless. Which feature are you referring to? reply e3bc54b2 6 hours agorootparentbeing able to type `python` and start writing a program that would work nearly everywhere. With compatibility break there was a decade of confusion, even the simplest print statement wouldn't work. I understand there were real reasons to do all that, but it did cause damage. Steve Yegge put it better than I can[0]: > the thing is, every single developer has choices. And if you make them rewrite their code enough times, some of those other choices are going to start looking mighty appealing. They’re not your hostages, as much as you’d like them to be. They are your guests. Python is still a very popular programming language, to be sure — but golly did Python 3(000) create a huge mess for themselves, their communities, and the users of their communities’ software — one that has been a train-wreck in progress for fifteen years and is still kicking. > How much Python software was rewritten in Go (or Ruby, or some other alternative) because of that backwards incompatibility? How much new software was written in something other than Python, which might have been written in Python if Guido hadn’t burned everyone’s house down? It’s hard to say, but I can tell you, it hasn’t been good for Python. It’s a huge mess and everyone is miserable. [0] https://steve-yegge.medium.com/dear-google-cloud-your-deprec... reply otabdeveloper4 1 hour agorootparent> there were real reasons to do all that No there weren't. It's just pure idiocy and incompetence. reply e3bc54b2 50 minutes agorootparentWell, I would agree with you. But I'm no language designer nor maintainer. It could all be bollocks, but since I'm the ignorant one, they get benefit of doubt. reply viraptor 9 hours agorootparentprev> Setting up typescript can be hard. Node just enabled it by default. You still need the dev dependency for manual compilation and checks, but at runtime it should \"just work\". https://nodejs.org/en/blog/release/v23.6.0 reply Shacklz 8 hours agorootparentAs enthusiastic as I am about node's typescript-support, calling it \"just works\" is a bit of a stretch. Not entirely sure if it's still the case anymore on the latest versions but last I checked it was required to use `.ts`-suffixes for all the imports, something a standard typescript project will hardly ever have and needs to be specifically configured to be considered valid syntax (allowImportingTsExtensions:true). But yeah, there's progress, and once this gets solid traction (which I'm sure it will) it might finally be the last drop in the bucket to convince TC39 to stop being so antagonistic to having some notion of type-support directly in Javascript. reply re-thc 9 hours agorootparentprev> but at runtime it should \"just work\" Maybe when it doesn't use WASM and there's proper integration. Otherwise it's just like npm and people still need to look for alternatives. reply nsonha 9 hours agorootparentprevnothing wrong with that, but why putting it on a global object instead of a built-in module. Better yet, that module may be publishable on jsr/npm reply skwee357 8 hours agoparentprevAFAIK bun is VC backed, so they need to make money at some point, therefore a speculation: They want to make bun an all in one runner in order to vendor lock you in somehow. But I might be wrong. It indeed does not make sense to put such dependencies in the core/std lib reply jeswin 8 hours agoparentprev> Wouldn't that be better as an optional library? Totally agree. In their words, \"Bun aims to be a cloud-first JavaScript runtime. That means supporting all the tools and services you need to run a production application in the cloud\". This doesn't give me a lot of confidence. This particular design choice seems even worse than Node. reply diggan 5 hours agorootparent> This particular design choice seems even worse than Node. We could argue that it's worse/better, but in the end it's just different. NodeJS when it appeared had the vibe and \"marketing\" to be something lightweight, fast and event-driven (compared to the alternatives at the time at least), where the 3rd party ecosystem provided the tooling for what Bun now tries to bundle into their \"all-in-one\" tool. We've seen the same cycle multiple times. Developers need flexibility to configure something so a flexible solution appears, everyone gets excited and starts migrating. Eventually, more developers are tired of the flexibility and don't understand why there are so many configuration-options, so eventually a \"all-in-one\" solution appears, everyone gets excited and starts migrating. Eventually, people need to be able configure more things so.... reply jatins 2 hours agoparentprevI think it makes it easy to provide paid hosted services later if needed. Hypothetical example: S3 client built in, enable a flag and now get a dashboard seeing analytics around file downloads, download latencies etc Just pure hypothesis on my end given they have to make money somehow at some point reply shubhamjain 9 hours agoparentprevQuite a lot of people have told Jared (Bun’s Author) the same thing, but his opinion is that Bun should have everything a basic project might need. Keeping it in core he can make it more optimized than it would be as third-party library. It’s a misguided approach according to me. And I feel Jared has become way too ambitious. But what can I say, it’s his passion project. reply animuchan 8 hours agorootparentI can understand this vision: it's neat to be able to just open a file and start making a program, without having to choose one of the 20 relatively popular Postgres client libraries available on npm. From the engineering standpoint, sure, it's a disaster. But it's also the lost magic of TurboPascal and friends, where you could just be immediately productive, with no dependencies, no external tooling, on an old computer gathering dust in the school library. reply Raed667 5 hours agorootparent> without having to choose one of the 20 relatively popular Postgres client libraries They can easily provide official extensions/packages clearly namespaced and avoid all this mess. But I fear that they're more focused on a \"headline-driven-development\" approach, the more different from the status quo, the better reply animuchan 4 hours agorootparentAnd it's successful! I've unironically learned about Bun on YouTube, and now am a happy user. :) A set of \"blessed\" known good packages would be wonderful to have in any language, I wonder why it's not a thing. reply Raed667 2 hours agorootparentI'm wondering what made you switch ? I find the ubiquity of node very comforting, maybe I'm not seeing the argument that will convert me over ? reply morcus 2 hours agorootparentNot the original commenter It has just been a pretty low effort drop in replacement for me. It's definitely not a complete game changer, but quick iteration is just that bit more convenient since it's faster and I don't need to remember all the flags I normally have for my setup (Typescript, .env file, etc...) reply mythz 2 hours agorootparentprevIt's one of the things I love about Bun, I now write any non-trivial stand-alone scripts with TypeScript which is capable of a lot of functionality without needing any config or to install any external packages. reply bilater 1 hour agorootparentprevI like the batteries included option and would likely use it a lot. But not sure why they simply did not make a \"batteries\" package you could install on top of core and avoid the inevitable push back they knew they would get from this. reply meiraleal 7 hours agorootparentprevBeing misguided or not, it is good to have different approaches, including the ones that doesn't work. reply lioeters 6 hours agorootparentBravo, let a hundred flowers bloom. May the best ones win, or at least prove themselves to be \"worse is better\". reply gardenhedge 7 hours agorootparentprevI think it's great. There are already alternatives so makes sense for Bun to do its own thing reply h1fra 2 hours agoparentprevI'm sure in the short term people will be happy, but maintaining that in the long run is a footgun. I can't see a team big enough to achieve that correctly, especially the bug confusion (e.g: is it bun or the 3rd party issue) that will creep on the main repo. reply supriyo-biswas 9 hours agoparentprevThis is for their managed offerings. The problem of VC backed software was always that there would be these integrations that'd try to provide something unique to it, or IOW, lock-in. reply zaphirplane 8 hours agorootparentWhat’s the monetisation plan reply timeon 8 hours agorootparentSince at least web2.0 I've been burned many times. So my default position on VC backed projects without clear monetization plan is that it is probably bait-and-switch. reply sod 8 hours agoparentprevThose packages exist already though. Pretty sure the bun maintainers (or Ciro Spaciari in this case) asked the question \"how fast could it be if written in zig?\". reply izietto 6 hours agorootparentIsn't it possible to write it in Zig as a separate extension? Every mature language I'm aware of supports this AFAICT reply Aeolun 9 hours agoparentprevAs long as it works and follows a consistent API, why would that be an issue? I kinda like the idea of not having to import potentially very slow JS code to do things that I need in basically all my projects. reply flohofwoe 8 hours agoparentprevThe interesting question is: Where do you draw the line? Should a HTTP server/client be in the stdlib? File access? String templating? And why not a window system, 3D API and UI wrapper? IMHO a stdlib should mainly provide standardized interface types, but not necessarily the implementations behind those interfaces. But that's probably not a very popular opinion since it falls between the two existing options of having a very bare bones and a batteries-included stdlib ;) reply kyriakos 8 hours agoparentprevit is a very bad idea. you start your project using the built in client libs, you are locked in to bun as js runtime. if the license changes you are stuck. reply SwiftyBug 8 hours agorootparentNot only that, but when the new hot JS runtime inevitably drops, it's going to be a pain to move to that. reply ksec 6 hours agoparentprev>Wouldn't that be better as an optional library? It would be better if the libraries were not the most optimal or good enough. In bun's case it is not just the minimal they are basically making everything as good as it can be. reply rakatata 6 hours agoparentprevs3 is pretty much a de-facto standard, just like json like or not. Postgres also makes sense, it is the most widespread and community-liked db. What is the point of \"optional libraries\" tbh? It was a PITA in PHP back in the days and very inconvenient, prob most devs would prefer this way. I feel like HN is on cognitive dissonance, they complain JS projects having too many dependencies and they also complain now when things are more integrated into the runtime because it increases vendor-lock and few extra megabytes (actually kilobytes according to the devs) to the binaries :/. Lastly, big companies also prefer less dependencies, it is not just devs. reply chearon 1 hour agorootparentIt probably looks very wrong to people who still think lots of \"modularity\" and small packages is a good thing. I'm all for it, and lots of Bun APIs are purely practical. Bun.stringWidth, for example, exposes code Bun already has internally. Nodejs probably has the same thing, but instead of us being able to use it, it gets reimplemented in 10 different versions in node_modules. How is that better? I doubt the Bun team will have to change the S3 code very much over the years. The test runner, bundler, Postgres client, sure, I can see those being harder to maintain. But I'm also tired of everyone assuming everything needs to change all of the time. DX aside, my team is still on Webpack and we've only needed one new feature from it in the last ~5 years. Why can't Bun's bundler reach maturity and only receive a few updates? reply sergiotapia 3 hours agoparentprevthat's old school javascript mentality. new school is bake it all in and let people get to work on the interesting bits. reply Karupan 11 hours agoprevUsing bun has been a great experience so far. I used to dread setting up typescript/jest/react/webpack for a new project with breaking changes all over the place. With bun, it’s been self contained and painless and it just works for my use. Can’t comment on the 3rd party libraries they are integrating like s3, sql etc but at least it looks like they are focused on most common/asked for ones. Thanks for the great work and bringing some much needed sanity in the node.js tooling space! reply herpdyderp 9 hours agoparentHow does bun make a difference in the frontend tech stack that you mentioned? reply DanielHB 9 hours agorootparentLast I tried (several months ago) it didn't, the built-in frontend bundler was not very useful so everybody just used 3rd party bundlers so (for most people) it would not have any meaningful differences compared to nodejs. It seems they are putting more effort in the bundler now, so it seems like it can replace plain SPA applications just fine (no SSR). The bundler is inspired by esbuild so you can expect similar capabilities. IMO the main benefit of using their bundler is that things (imports/ES-modules, typescript, unit tests, etc) just behave the same way across build scripts, frontend code, unit tests, etc. You don't get weird errors like \"oh the ?. syntax is not supported in the unit test because I didn't add the right transform to jest configuration. But works fine in the frontend where I am using babel\". But if you want to use vercel/nextjs/astro you still are not using their bundler so no better or worse there. reply nsonha 9 hours agorootparentprevnot up to this point, but with this release, bun is now a bundler. That means potentially no webpack, vite and their jungle of dependencies. It's possible to have bun as a sole dependency for your front and back end. Tbh I'll likely add React and co, but it's possible do do vanilla front end with plain web components. reply cal85 9 hours agorootparentBun has always been a bundler (and package manager, and Node runtime). This release adds \"HTML imports\" as a way to use the bundler. reply matt_kantor 5 hours agorootparentDoesn't the name \"bun\" come from the fact that it's a bundler? reply Raed667 10 hours agoparentprevi have been setting up these react/ts/etc project with vite or next.js, just fine , i think you're underestimating how much progress happened in other tooling as well reply nsonha 9 hours agorootparentIdk about next 15 but you can literally bootsrap next 13 using a single index.tsx with typescript & next being the only 2 dependencies in package.json. No typescript is fine too. It's not new, has been the case for a few years, so honestly I don't get people complaining about next's complexity. reply hu3 3 hours agoprevA recent client uses bun in production. I'm told their dev experience with Bun is out of this world bonkers good because of speed and simplicity. Dev experience can play a big role long term. If your codebase and/or process sucks, you'll lose good people unless you pay FAANG tier compensation. reply atoko 2 hours agoprevFrom what I can tell, this change was merged and released without a passing build. Indicating that the project’s quality assurance process is little more than lip service. I’m not sure how you would track regressions if your tests are flakey to begin with. reply martpie 2 hours agoparentTBH, all of what’s in this release came from previous 1.1.x patches. It seems they just drafted a new release to communicate the groups of change from the previous releases. reply tmikaeld 8 hours agoprevBatteries included makes a huge difference, this is why i love that Web APIs (Fetch API, Service Workers, Web Components, and ES6+, WebRTC etc) are now native on both V8 and Webkit runtimes. But it has to be to a certain degree, maybe S3 is too far, but SQL drivers makes sense but again, to which degree? There are _many_ databases out there, should there be drivers for half of them? Even at that level it's a lot of added code which means slower executable. Also, I think Bun is missing out on security by adding such sensitive APIs to Bun, imagine bun taking all your source files and uploading it to your private S3 due to some script or path issue that allowed eval to run! It's game over right there. reply jjice 2 hours agoparent> Even at that level it's a lot of added code which means slower executable. Does it? Legitimate question. I would've assume that this could be almost entirely negligible depending on how the code is loaded into the runtime. If the code being loaded is only triggered when an import statement is seen, wouldn't that lead to essentially no speed overhead? Even if it was statically linked in, I don't see why having the code would slow down the executable by any amount that we'd want to consider. Maybe literally more of an executable to load into memory, but I don't see that being a tangible slow down. Would love to know if I'm missing a big piece here though. reply chrisandchris 4 hours agoparentprevThere was a discussion here on HN a while sgo, why browser will not support SQLite 1st hand. Maybe that point applies to Bun too: Point is, who is responsible for maintaining the Lib and how do you change the Lib when SQLite changes. There might be a bug in SQLite. How do you fix it in Bun? Which versions receive a fix? How do you handle that a parch of your runtime (Bub) now might change behaviour of code running on it (because users worked around it)? These are solvable issues, to some degree and with some downsides. However, at some point you stop being a runtime and start being a platform, which will bring other resposibilities and issues with it. reply chipgap98 3 hours agoparentprevI think the argument in favor of S3 is that there are many object storage services that implement an S3-compatible API. I know its not truly a web standard, but it is also something that a lot of people have standardized around. reply msoad 10 hours agoprevI have not tried Bun yet but the long list of features makes me skeptical that it's all solid and bug-free. I'm wishing to be wrong. I'll give it a spin in a future project. From a project management perspective I'm a little confused why would you spend time on S3 support while you're still not 100% Node.js compatible. Next.js is a very big ecosystem and if you can get Next.js customers onboard you'll grow much more than supporting S3. reply bmacho 8 hours agoparent> I have not tried Bun yet but the long list of features makes me skeptical that it's all solid and bug-free. Especially that it is written in Zig, which is very memory unsafe. I mean if you refer a variable that is not alive anymore, it just accesses some random unrelated memory instead of segfaulting (in debug and safe mode too)[0]. How hard would it be to bolt a memory liveness system above it, that flags a variable name dead and blocks access to it, if it is dead? No, \"just don't write UB\"[1]. Anyway I'd certainly not put a Zig made anything facing the internet, especially not a webserver. [0] : https://news.ycombinator.com/item?id=41720995 [1] : https://github.com/ziglang/zig/issues/16467#issuecomment-164... reply zamalek 2 hours agorootparentIn the early days of the project it was segfaulting during performance tests. That was a pretty hair-raising bit of information for me. Deno it is. That being said, all of these run times use a JS JIT that are written in a memory unsafe language, that emit and execute raw machine code. They frequently have vulnerabilities. reply swiftcoder 9 hours agoparentprev> while you're still not 100% Node.js compatible 100% compatibility is a nice marketing win, but the long tail of compatibility may not make much difference to the average user. What percentage of the total Node.js API surface area do you actually use in your day-to-day? How many weird edge-cases therein are you actually depending on? reply homebrewer 9 hours agorootparentDo you inspect 100% of the code of each library you use to make sure it does not rely on missing or incompatible functionality? reply swiftcoder 8 hours agorootparentI mean, you are either auditing your supply chain or you are not. And if you are not, then minor node incompatibilities are the least of your worries. reply drewbitt 2 hours agoparentprevIt is not very solid nor bug-free. We tried it last year and crashed all the time. reply re-thc 10 hours agoparentprev> From a project management perspective This assumes you know what the project(s) is/are. Also the people working on it aren't robots. Maybe certain things take time to figure out and meanwhile you can do something else? It's also not just 1 person on the task. > if you can get Next.js customers onboard you'll grow much more than supporting S3 Towards what? That doesn't make $$$. This is VC-backed. The goal isn't to provide Bun for free and gain all the users in the world. reply msoad 10 hours agorootparentThis is a very uniformed response IMO. S3 seems very niche compared to Node.js compatibility. Not sure why you're attacking me for saying this? reply yoavm 10 hours agorootparentYou mentioned Next.js and then Node.js. As for Next.js, it is supported by Bun (https://bun.sh/guides/ecosystem/nextjs). I don't think it's safe to assume that every single Node API is more commonly used than S3, which really is the standard cloud-based object storage API. reply elchangri 9 hours agorootparentprevS3 and niche don't fit in same sentence reply nsonha 9 hours agoparentprev> a bug where bun add would not respect the spacing and indentation in your package.json. Bun will now preserve the indentation of your package.json, no matter how wacky it is I find this entry pretty funny. Who even asks for this and what makes they think it's worth writing code for. reply yieldcrv 9 hours agoparentprevI’ve used it on big existing projects with tons of dependencies and small projects I’m impressed The dumbest thing I saw was Amazon’s CDK library looking for specific package manager lockfiles and was therefore semi-incompatible with bun But if you use SST it doesnt matter reply Imustaskforhelp 10 hours agoprevHTML imports In Bun 1.2, we've added support for HTML imports. This allows you to replace your entire frontend toolchain with a single import statement. To get started, pass an HTML import to the static option in Bun.serve: import homepage from \"./index.html\"; Bun.serve({ static: { \"/\": homepage, }, async fetch(req) { // ... api requests }, }); this is amazing and so cool thanks a lot !! reply sampullman 9 hours agoparentHow does this allow me to replace e.g. Vite? Is there a way to do hot module reloading, css pre-processing, or load framework specific plugins (like the Vue SFC compiler)? Serving a static file isn't exactly new, so I feel like I must be missing something. reply Imustaskforhelp 39 minutes agorootparentbun has its own bundler as well. So I suppose bun already replaces vite & I also think it does css pre processing but just not framework specific I liked this approach because I actually wanted to create very simple / static serving in bun and I had to actually do a lot of hoops like reading it from Bun.file() and then some things more Its just nice that its now solidified into the standard library / behaviour I suppose reply mythz 9 hours agorootparentprevFrom its documentation [1] it looks a lot like a parceljs replacement [2], i.e. a zero config bundler which processes and bundles the dependencies in .html pages. So great for simple websites, not for replacing an entire Vite stack. [1] https://bun.sh/docs/bundler/fullstack [2] https://parceljs.org reply sampullman 9 hours agorootparentThanks for the links! I should have researched a little more before replying. It actually mentions HMR at the bottom of the docs, and I see plugins are already available. So while it can't currently replace my Vite stack for most projects, it seems like it eventually could. I'm not sure how I feel about this sort of coupling in general, but for small projects it could be very convenient (as you mention). reply nsonha 9 hours agorootparentprev> framework specific plugins (like the Vue SFC compiler)? They have a plugin api, but honestly I don't like the sound of \"framework-specific plugins\". It is because of this, all front-end frameworks are now a mini compiler (inside a bundler/compiler) and being too comfortable now to come up with new wacky syntaxes. I prefer frameworks to just be frameworks and being able to write normal typescript. reply sampullman 8 hours agorootparentIf that's what you prefer, there are plenty of options. I'm happy writing Typescript with no framework (or even plain es6), as long as there isn't UI complexity. But for a large project, I'll happily trade a compilation step for the tools modern frameworks provide for managing complexity. reply cdmckay 2 hours agoprevI just tried Bun to make a script to copy files from a service we were moving off of to S3 and it's pretty great. Instead of having to tinker with my package.json, tsconfig.json, etc. to get everything just right, it works right of the box the way you expected with `bun init`. Then it's just `bun run index.ts`. And it's fast! Node is great but there's just too many options to configure. I appreciate that Bun went ahead and made a bunch of assumptions and pulled commonly used stuff directly into it. reply XCSme 1 hour agoparent> Node is great but there's just too many options to configure What exactly do you have to configure in Node? reply mythz 9 hours agoprevCouldn't think of a project that was more doomed to fail than a competing alternative to Node.js, but glad that I gave it a shot when I needed to create lots of stand-alone scripts to process text files and SQLite DB updates which I was able to create with TypeScript, bun:sqlite [1] and bun $ Shell [2] working OOB without needing to manage any configuration files or local npm dependencies. I've since tried it with new JS/TypeScript Projects which also makes use of its built-in Bundler [3] and testing support [4], installing deps is also instant. Having everything work OOB, fast, are real quality of life improvements where Bun has now become my first choice for any new JS project. [1] https://bun.sh/docs/runtime/shell [2] https://bun.sh/docs/api/sqlite [3] https://bun.sh/docs/bundler [4] https://bun.sh/docs/cli/test reply joshstrange 5 hours agoprevI actually used Bun for the first time the other day and it was an amazing experience. All my projects have Webpack or Vite configured to let me write Typescript and once setup they work almost flawlessly but it’s a pain to set it all and not worth it for small scripts. On the other hand Bun worked right out of the box. I had spent 10-30 minutes futzing around with node-ts or whatever the tool is to run TS “directly” on the CLI and I was dealing with the all the dreaded messages “not a module”, “can’t use import/require”, “ESM/CJS” and trying all the normal fixes (changing package.json module type, changing tsconfig, changing the way import/require) all to get a ~200 line script to run. I switched to Bun as a Hail Mary and it worked wonderfully. reply 0xferruccio 9 hours agoprevI love the direction, especially including s3 and Postgres support natively it makes a ton of sense for this to exist as an alternative to the “build your own framework” status quo This is the standard in every web framework like Rails and Laravel, and the JS ecosystem will really benefit from this. The next steps are migration and schema management and a better out of the box testing story (w/ nice way to setup factories) reply culi 10 hours agoprevI maybe chose the wrong horse and hopped onto deno early on. Bun's success has really surprised me. It had some really misrepresented benchmarks that were oft-repeated early on that seemed to contribute to it but they've been able to really capitalize on its hype since then. I guess choosing a small up-and-coming language like zig has the added benefit of making that entire community rally behind you reply CharlesW 1 hour agoparent> I maybe chose the wrong horse and hopped onto deno early on. After reading through this thread, I'm personally leaning toward Deno for my first non-Node project. Demo seems to be more thoughtfully managed, more pragmatic (e.g. Node/NPM compatibility), more secure, with better technology choices (e.g. Rust vs. Zig) overall. reply porridgeraisin 58 minutes agorootparent> Node/NPM compatibility Bun is miles better in this regard. Deno initially did not even want to focus on Node/NPM compatibility, and then backtracked once they understood the importance of it. Bun OTOH runs the entire Node.js test suite on every commit, and the mentality of breaking less existing node/npm code is clear. e.g, just in this release, `bun publish` has the exact same CLI as `npm publish`, and bun also works out of the box with .npmrc. About the Node.js test suite.. many modules are at 100% compatibility, and many are at 90%. You can track it here: https://bun.sh/docs/runtime/nodejs-apis Also, they reimplement the V8 public C++ API in JavaScriptCore (!) so that packages like npmjs.com/cpu-features work [1] Every new feature has this aspect to it, e.g the postgres client inbuilt is a drop-in replacement for the `postgres` package. As far as Node/NPM compatibility, and thought given to compatibility in general, is concerned, there is absolutely no contest. And if I'm allowed a little snarky slight... Deno couldn't even maintain compatibility with their own API for reading and writing files during the Deno 1->2 update. [1]: https://bun.sh/blog/how-bun-supports-v8-apis-without-using-v... reply culi 1 hour agorootparentprevI'm still a fan of Deno and hoping it wins out but > more pragmatic (e.g. Node/NPM compatibility) Both projects are good on this front. Deno actually originally explicitly promised NOT to work on node compatibility to \"move the industry forward\". They realized this was a failing move and backtracked (which has been a little controversial amongst the core base) > with better technology choices (e.g. Rust vs. Zig) I think it's a little silly to take the choice of language as a \"technology choice\". Both are new languages, both still have a lot to prove, and both have pros and cons the other lacks reply diggan 5 hours agoparentprev> I maybe chose the wrong horse and hopped onto deno early on In the end, it's all JavaScript (or TypeScript if you like Kool-Aid), as long as you know the language you can pretty much effortlessly jump between node, bun and deno, they're more similar than they are different :) Migrating projects on the other hand, well... reply culi 51 minutes agorootparentIf you're writing production code and not using TypeScript then you're using JsDocs. Which is just TypeScript again reply int_19h 9 hours agoparentprevOTOH it's strange to me that they're shipping v1.2, when Zig itself is at v0.13 (and generally considered a moving target). reply culi 8 hours agorootparentZig is following a different versioning philosophy. It's not semver, it's https://0ver.org/ reply sampullman 9 hours agorootparentprevA bit ambitious maybe, but I don't necessarily see why breaking changes in Zig would need to be exposed to end users. reply crakhamster01 3 hours agoprevThese improvements look amazing! I'm always blown away by the performance benchmarks of Bun. Can anyone speak to how much adoption it's getting professionally? Even anecdotal points are useful. When v1.0 was released I briefly tried testing it out in my employer's monorepo. It unfortunately wasn't a drop-in replacement and we hit compatibility issues/couldn't invest time debugging. While we could have migrated smaller projects to it, we decided not to split our tooling and just stuck with pnpm. Curious if others have had a different experience! reply atonse 3 hours agoparentI've been using it for a greenfield NextJS 14 project for a client for about 8 months. It's been very smooth. No issues whatsoever. (edit: one issue was that I wasn't getting stack traces for error pages but they fixed that a few months ago) The only issue I hit is that Vercel only runs node, so builds that may have passed on bun, sometimes fail on Vercel. So just before I deploy, I run `npm run build` to catch any node-specific build issues and fix them before Vercel finds them. I hope Vercel will add support for bun in their edge runtime. reply mirkodrummer 2 hours agoprevaws sdk apis embedded in a runtime? aside from lockin it looks a terrible thing to do, to me it seems just a driver to increase adoption, I'd actually expect free performance gains by just importing s3 from aws-sdk in bun. this way the path forward is clear: a bloated runtime full of 3rd party integrations reply moooo99 2 hours agoparentApparently their S3 is running entirely based on a native implementation and offers substantial performance improvements over the AWS SDK. Bun seems to be a little inspired by the Go language design. It is batteries included and it has a substantial stdlib. Integrating something like an S3 SDK what I would consider to be a high level feature seems like an interesting choice, but makes a lot of sense considering Bun specifically targets cloud environments reply monroewalker 1 hour agoprevIf building a desktop app with the intention of having JavaScript plugins, would it not be viable to include bun in the app bundle to use to run the plugins? Curious if anyone has played around with this idea reply Jarred 8 hours agoprevI work on Bun. Happy to answer any questions. reply Y-bar 7 hours agoparentAre there plans for a stripped-down version without S3 and SQL and things like that for those like us who just want a fast runtime to build our frontend resources to static files? reply Jarred 7 hours agorootparentNo plans to do that. If you're worried about binary size from features you don't use: the binary size cost of Bun.sql is less than 50 KB (you can check this yourself via the .linker-map file in the *-profile.zip builds of Bun or via https://bloaty-csv-reader.vercel.app which was a tool I wrote a little while ago to see how much space various features of Bun use) If you're worried about runtime overhead, practically everything in Bun is lazily loaded. So if you never access Bun.sql or Bun.S3Client, it will not load it. And, even if it does load it, because we implement it in native code and worry about this a lot, it doesn't really cost much to load. reply Y-bar 3 hours agorootparentNot worried about size on disk. That said, it's not just runtime overhead I am worried about, it is a handful of smaller risks that compound: Those include security risks, bugs in one section affecting something else, scope creep affecting time-to-fix-bugs, not everything is lazy-loaded so it will affect performance. And like diggan said here, I like tools with focus, for example I choose to use one note taking app, one separate app to write code in, another app to chat with, and yet more apps for things like email, and SSH even though they are all text-centric apps and could be bundled into one and the same. reply ksec 6 hours agorootparentprevThank You Jarred. I seriously dont understand why there is such a huge resistance in including those. Especially when they are faster and more memory efficient. reply diggan 5 hours agorootparent> why there is such a huge resistance in including those I don't know about all the reasons, but personally I stay away from projects who try to bundle in as much functionality as possible into one \"all-in-one\" thing. I prefer approaches where you yourself chose the right library for the problem at hand, as it tends to be that different libraries have different tradeoffs, and I want to chose those tradeoffs myself. Summarized as The Unix Philosophy or \"do one thing and do it well\" I suppose. I'm not saying it's wrong of Bun to include those things, their value proposition is the \"all-in-one\" approach, which a large group of people seem to like, so seems they're doing right by their audience. But again, personally I don't like the tradeoffs involved with that approach, but I wouldn't try to convince Bun either to go against their explicit goals. reply ibejoeb 4 hours agoparentprevI didn't think this would be quite so contentious. It's been great for me. I manage very distributed teams across multiple time zones, multiple language barriers, and heterogeneous platforms. It's difficult. Bun makes it easier because the features in the distribution are virtually guaranteed to work together and obviate the tooling version hell. I have fewer 1:1 setup meetings. I haven't had cross-platform issues. Finally, the speed is actually important on lower-tier hardware. It's for the same reasons that I switched to biome. It's faster and reduces total dependencies. I very happy with this combo. Is there more code tooling (linting, formatting, etc.) on the roadmap for bun, or are you focusing on the runtime features? reply Etheryte 7 hours agoparentprevI see Bun pop up every now and then and it looks amazing. How is the development funded? With projects such as Node, I don't have to worry about it disappearing from underneath my feet, with Bun I'm not so sure. reply XCSme 1 hour agoprevHow's the Windows support? I check every few months, and it always fails to even run bun install on my projects on Windows. reply mort96 1 hour agoparentWhy does it matter reply XCSme 1 hour agorootparentBecause I develop on Windows? reply lvncelot 10 hours agoprevReally liking the text-based lockfile. I know there's a way to get diffing locally via a `[diff \"lockb\"]`, `textconv = bun` git setting, but that's still 1. manual setup, and 2. not really webui friendly. Other stuff, like the C interop and psql client sounds amazing as well. I'm currently only using Bun for smaller sideprojects where I'm also trying to use some of the more out-there features, and it has been a blast so far. Though the most important question for me (as always with these announcement videos): where can I get a bun plush? reply sausagefeet 10 hours agoprevCan anyone speak to the performance numbers on the Bun page? It represents itself as significantly faster than the existing options, but why is that the case? Is it related to Bun itself? It must be the JavaScript engine they chose right? My understanding is it uses the Safari one rather than V8. Is JSC really that much faster? If so, what are the trade-offs of choosing that over V8 or any other option? reply nicoburns 9 hours agoparent> Is JSC really that much faster I think it is a bit faster. But a lot of Bun's speed comes from implementing API's in Zig rather than JS (which node does a lot) reply pansa2 9 hours agoparentprev> My understanding is it uses the Safari [JavaScript engine] rather than V8 Looks like you’re right. Thanks for clearing up my misunderstanding: I’m not sure why, but I thought Bun was using a custom JS implementation. reply DanielHB 9 hours agorootparentFrom their docs it is \"written in zig\" which implies \"all of it is Zig\". They don't hide that they use JSC but they don't like to advertise either. It is a bit manipulative advertising to draw hype people in I would say. You know those tech influencers who like to peddle stuff to get views. edit: I was a bit unfair, it feels like those \"hype tech influencers\" are the ones who downplay JSC in favor of promoting Zig, not the project itself. The frontpage of Bun mentions JSC twice and Zig once. reply intelVISA 7 hours agorootparentprevWell in their defense it would hard to get funding for JavaScriptCoreWrapper.sh so it's a useful misunderstanding to curate. reply rob74 9 hours agoparentprevIt's surprisingly hard to find an answer to this (an FAQ on the Bun site would be a good idea?), but I suspect it has to do with reimplementing some stuff (bundler, package manager, test runner etc.) in a compiled language (Zig) rather than using the JS runtime for everything (this is probably also the reason why Bun integrates so many functions into one \"kitchen-sink\" executable), and using a JS runtime which is faster than V8 at least in some regards. reply hamandcheese 7 hours agoprevRe: bun patch, it would be great if it were possible to fetch remote patches (with a sha specified). I aim to have an \"upstream first\" policy when it comes to patching/forking dependencies. And fun fact about GitHub, you can append .patch to a PR url or commit URL to get a patch file. This makes patches self-documenting (they literally are a link to the upstream PR) if the tool can fetch remote patches. Nix is the only tool I'm aware of that makes this easy. reply voiper1 11 hours agoprev>To work around this, we had to change the assertion logic in some tests to check the name and code, instead of the message. This is also the standard practice for checking error types in Node.js. Sounds like something they should try upstreaming? Now they'll need to track all the tests to manually modify/import... reply Rizu 3 hours agoprevi am not a JavaScript programmer, but can someone explain exactly what bun does ? does it compile JavaScript and target a backend like LLVM equivalent for the browser ? reply sureIy 3 hours agoparent> what bun does ? It installs npm packages, it executes JS and TS code, it bundles code for front end, it runs tests. Plus it has a lot of random pieces related to this development, like integrated database access. Compare it to node, npm, webpack, and jest, all in one, and whatever the guy dreams of. It's certainly fast and offers great DX, but I wouldn't bet on it to stay around for, say, 5 years. reply javier123454321 3 hours agoparentprevIt doesn't really do anything for the browser. It provides a really fast runtime for JS to run in a server, a stdlib for http, crypto, and filesystem (among others) that JS lacks, a bundler for JS projects to go to a browser and a compiler for typescript to JS, among a few other things. reply ch4s3 3 hours agoparentprevBun is a build tool and code runner primarily. It can run a tsx or js file, and it can do bundling or the compilation of TypeScript into regular JS. It also manages packages and can run tests. It aims (I think) to be a node compatible replacement for node. reply rcarmo 11 hours agoprevurl and dgram are still not all there yet, but I’m going to see if Node-RED runs now. reply culi 10 hours agoprevlooking forward to updating more .gitignores with 3 of these 4 package-lock.json yarn.lock deno.lock bun.lock reply hamandcheese 6 hours agoparentThis is an underrated and misunderstood comment. Let me explain: projects usually support only one package manager. In a world of N competing JS package managers, you need to ban lock files from N-1 of them. reply sampullman 9 hours agoparentprevCan't forget pnpm-lock.yaml ;) reply culi 8 hours agorootparentI use pnpm the most and I missed this :O reply conradludgate 10 hours agoparentprevWhy would you gitignore those? Adding lockfiles to git repositories is considered good practice reply culi 8 hours agorootparentI said 3 out of 4 of these. Committing multiple lockfiles is not good practice but I see people that struggle with the idea that package managers are not interchangeable all the time reply 5Qn8mNbc2FNCiVV 1 hour agorootparentIf someone commits a wrong lockfile they are fired where I work at lol (exaggerating, but only slightly) reply Etheryte 7 hours agorootparentprevSurely you would enforce this at pull request time, no? Ignoring the file works from a functional perspective, yes, but does nothing to solve the actual problem. reply culi 1 hour agorootparentthe \"actual problem\" is often management trying to find cheap labor or even using AI to \"do it themselves\" reply Cthulhu_ 10 hours agorootparentprevAnd why would they use multiple competing package managers and runtimes? It isn't a good faith comment. reply diggan 5 hours agorootparent> And why would they use multiple competing package managers and runtimes? Some of us work across multiple projects and aren't up in our arms about what package manager the current project use. Some days you touch 3-4 projects that happen to all use different package managers. reply phinnaeus 9 hours agorootparentprevIt’s not just good practice it’s the whole point reply danr4 8 hours agoprevnpmrc support is huge reply fithisux 11 hours agoprev [–] Congratulations!!! reply revskill 11 hours agoparent [–] Why ? reply Cthulhu_ 10 hours agorootparent [–] What? Why would you ask that? reply Consider applying for YC's Spring batch! Applications are open till Feb 11. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Bun 1.2 is a significant update for JavaScript and TypeScript development, enhancing Node.js compatibility and adding built-in support for S3 and Postgres. The update introduces Bun.sql for SQL databases, a faster package manager, and a built-in test runner, alongside improved performance and new JavaScript features. Bun 1.2 also enhances bundling with HTML imports, standalone executables, and a new CSS parser, while remaining free and open-source with ongoing community contributions."
    ],
    "commentSummary": [
      "Bun 1.2 has been released, introducing features such as HTML imports and built-in Amazon S3 support, which enhance its \"batteries-included\" approach.",
      "The release aims to simplify JavaScript development by integrating tools like TypeScript, Jest, and Webpack, appealing to developers seeking a streamlined setup.",
      "While Bun's speed and ease of use are praised, concerns about potential lock-in, third-party library inclusion, and long-term compatibility with Node.js persist among users."
    ],
    "points": 237,
    "commentCount": 150,
    "retryCount": 0,
    "time": 1737615028
  },
  {
    "id": 42800557,
    "title": "Tech takes the Pareto principle too far",
    "originLink": "https://bobbylox.com/blog/tech-takes-the-pareto-principle-too-far/",
    "originBody": "Bobby Lockhart Game Designer and Silly Person About Me Games Prototypes Talks Contact Me Site Menu About Me Games Prototypes Talks Contact Me Tech Takes the Pareto Principle Too Far Posted on December 6, 2024 by bobbylox • 0 comment There’s a reason video games build what’s called a ‘vertical slice’. If you’re not familiar, a vertical slice is a single playable area, with all mechanics, final art, vfx, sfx, music, etc. Basically, a little piece of exactly how the finished game will look and feel and play. The vertical slice is what game developers show publishers and investors to demonstrate not only that the game itself is going to be good, but also that the game development team has all the skills necessary to deliver the game to the level of polish the market demands. Contrast this with the tech industry, which submits for approval an ‘MVP’. A minimum viable product is the absolute least one can create that someone will pay for. It seems tech investors have gotten very used to evaluating MVPs, and rightly so — they need to be able to assess the potential of these prototypes so they can decide which are worth their investment. The problem is that MVPs don’t actually establish whether the team /could/ get to a finished product, and in practice many can’t. Recently, getting VC funding has become such an end in itself that engineers in the tech industry, centered around Silicon Valley, have optimized their skillset for prototyping. There’s an oft quoted idea called the Pareto Principle, which states that 20% of the effort produces 80% of the results. So, if you can just prioritize the right 20%, you can get most of the way towards the desired outcome. The whole sector has become great at this, nearly to the exclusion of all else. And who can blame them? Look at the inverse — doing 80% of the work to complete only 20% of the job doesn’t sound like much fun. What the Pareto Principle doesn’t capture, and what its adherents seem to forget, is that you still HAVE TO DO that last 20%. End users usually don’t enjoy using 80% of a website, or driving 80% of a car. Unfortunately, with so many digital products abandoned at the funding stage or forced to release early, engineers and designers often don’t have any practice with the last 80% of the effort required to finish something. I’ve worked with many such engineers, and it can be really sad knowing that you can never get to the level of polish the concept deserves. Because there is such a culture around early adoption of new technology, there’s a big population willing to overlook that the things they use are unfinished. It’s good that there are folks willing to try out nascent products, but we don’t need those products lionized, nor the people who use them. Because of their approval, the broader population has begun to defend the truncated. This applies to games as well. Day 1 patches are the norm, as are DLC which feels like it ought have been part of the core game. When products remain incomplete it’s often because all potential customers have already paid and there’s no financial incentive to finish. How many of the products you use every day feel like they needed a few more iterations to really work correctly? However, there is another, more frustrating reason why a product might remain unfinished. Maybe it’s literally impossible to complete. I think that’s the situation we find ourselves in for certain applications of AI, like self-driving cars, image generation, and text generation. Even people who advocate for these technologies rarely assert that the results are useable as-is, especially in a world where people are accustomed to a much higher, human-level quality. At best they are useful as a starting point for a human to then finish the image, or the cover letter, or to take the wheel. The problem is I don’t think that the current methodology is capable of taking us the other 80/20% of the way. I’ll break from my main point briefly to justify that assertion. It’s funny to think about now, but in the 70s, AI researchers believed they were most of the way towards achieving AGI (artificial general intelligence, aka the AI from the movies). They thought that if an expert system, or a perceptron, or a set of predicates was just developed far enough they would eventually reach sentience, or at least eliminate tedious work. Many believed that the hardware explosion Moore’s law promised was enough to create AI, and the software would take care of itself. Some of that was true — expert systems handle things like WebMD, and constraint solvers manage the incredible logistics of modern freight. The limits of the techniques of the time weren’t felt until much later. That’s where we are with Generative AI, too. I think that the Pareto Principle is technically true in a lot of fields, but I also feel our society would be a lot better off if we didn’t know about it. As I said, doing the last 80% of the work to produce only the final 20% of the result is hard on morale. It’s no wonder that work is so often abandoned, or outsourced. Perhaps more investors should demand to see a vertical slice, instead. If we took a more craftsmanlike view of software, we would realize that a chair is not 80% done when you can sit on it. It’s the details and the polish that make something worthy of use. So while from a utilitarian standpoint something may have most of the features a person might ask for, from a humanist point of view 20% of the work still only produces 20% of the results. Bobby Lockhart is an award-winning designer of learning games. Keep in touch on Bluesky or on LinkedIn Category: Uncategorized Leave a Reply Your email address will not be published. Required fields are marked * Comment * Name * Email * Website Save my name, email, and website in this browser for the next time I comment. ← Previous Recent Posts Tech Takes the Pareto Principle Too Far All AAA Games Should Also Be Edu-Games My Thoughts on Convergence Station What Isn’t Game Design 5 Radical Ideas for Dialogue Systems Pages About Me Contact Me GAM 341 – Intro to Level Design – Fall 2015 Games Hi-5 Heroes Prototypes Student Interviews Talks Tags Uncategorized (35) Meta Log in Entries feed Comments feed WordPress.org About Me Games Prototypes Talks Contact Me Site Menu About Me Games Prototypes Talks Contact Me © 2024 Bobby Lockhart Rundown Proudly powered by WordPress Scroll to top",
    "commentLink": "https://news.ycombinator.com/item?id=42800557",
    "commentBody": "Tech takes the Pareto principle too far (bobbylox.com)235 points by bobbylox 15 hours agohidepastfavorite95 comments blululu 13 hours agoWhile I appreciate that the author put in the time and effort to write this, I have to say that I disagree with pretty much all of this. Beyond quibbling about specific points, the MVP and the Vertical Slice are functionally similar they totally different in their purpose. Video Games are generally competing for a slice of a large preexisting market. The test is whether it can compete against existing products. A new to the world software start up is trying to serve a need that is currently unserved. The test is whether there is any market at all for this product (PMF). The 80/20 rule is about getting some validation that the thing you are building is worth building in the first place not that it can be done, but that it should be done. There aren't a ton of specific examples listed, but the images might insinuate some products that the author has in mind. I would just point out that Magic Leap, Humane were both hardware products that spent >5 years in development. They cam out as completed fully finished products that were complete by any standard. The problem wasn't that these products shipped with missing features, it was that nobody wanted what they were selling (see also the Apple Vision Pro, which is technically phenomenal in terms of design/engineering/manufacturing, but not really very useful). These products did the opposite of the Lean Methodology and show the risk of trying something brand new and not validating the assumptions as quickly as possible. reply dartos 4 hours agoparentTo quibble a bit, I don’t think that magic leap one released as complete products. Magic leap had like a 30 degree FOV and no software. It was a dev kit, literally. The consumer version was never released IIRC Also, I don’t think you argued against the main thesis which is something like “startup land is too good at the first 80% of a product, but not the latter 20%” That rings true for me. Our industry is not known for robustness and quality. There is robust and high quality software out there, but most of it is not. reply PaulHoule 2 hours agorootparentMagic leap is an atypical example. It wasn't a \"lean startup\", it spent billions of dollars and many years developing an El Dorado [1] [2] [3] technology. They would have liked to deliver the system they had promised but investors were right to pressure them to deliver something even a broadly available dev kit is an important milestone for a technology like that. [1] For example, the liquid metal fast breeder reaxctor https://www.iaea.org/sites/default/files/publications/magazi... [2] https://kguttag.com/ tells you just how hard it is [3] Apple Vision Pro is a refined if overly expensive product that takes a different approach to the same end and consumers were indifferent reply cma 3 hours agorootparentprev> Magic leap had like a 30 degree FOV So they released both a vertical slice and a horizontal slice then. reply bruce511 11 hours agoparentprev>> A new to the world software start up is trying to serve a need that is currently unserved. Well, yes in some cases. In lots of others it's just another implementation of existing software. Oh, you're building another food delivery app, because the current one is missing feature xxx? This doesn't make them bad, the world is full of different contexts, and providing a solution tailored to a specific context is valuable. But most startups are not \"novel\". Even in a novel space (like self driving cars) there are a bunch of companies in that space (basically competing for VC money. ) I'm very much in the MVP camp if anything I'm more extreme I'm in the \"show me a market and how to reach them before coding anything\" camp. reply cjblomqvist 10 hours agorootparentThere are other parts than the product that can the innovation for a startup. Very few things are novel product wise when they hit it big. Innovation can be bringing existing business model to a new market for example. Then you should most likely 80/20 that new thing. reply Xelbair 7 hours agoparentprev>A new to the world software start up is trying to serve a need that is currently unserved. man, really? 99% of software is basically glorified CRUD view for some DB. it's nothing novel. If we venture outside of corporate software: most startups do not create anything novel they just monetize existing businesses in a different(usually more predatory) way, or do X but digital. both usually go for 'virtual monopoly' by: offering service(never a product!) for free > capture the market > enshittify > new startups repeats the cycle. The actual novelty where you find a need that is unserved is sub 0.1% of them. In reality you aim for product that fits current buzzword meta for funding. I do 100% agree that product-market-fit is probably the thing you should try to get ASAP though. reply Gravityloss 7 hours agorootparentThat's the point. It's not supposed to be technologically novel, at least at start. It's a new business. The innovation is in the business part. When we were in Rome and exited the Colosseum, it started raining. Some random dude walked to us and sold us an umbrella. Great business, both of us were better off. reply Xelbair 7 hours agorootparentIt's more akin to someone giving you an umbrella subscription(if it rains more than twice this year I'll save money!), killing our all the competition with dumping prices by running on loss for years, and then hiking up the prices, while also tracking you and telling every food vendor your favorite food so they can prepare a greeter for you. if you think that's a sustainable model, and is good for society then you do you. reply Gravityloss 4 hours agorootparentI don't like those predatory businesses. I think many games are like that nowadays. But not all businesses are like that. I think a game like Factorio is honest and respectable. It doesn't have any subscription models, you buy it once and own it forever. Technologically, AFAIK, it's using old fashioned technology (Allegro was started in 1990). The innovation is in the gameplay part. reply Xelbair 1 hour agorootparentthe issue is that Factorio sells you a product. outright. and they are an outlier. reply HPsquared 7 hours agorootparentprevSounds like the restaurant business. reply passwordoops 6 hours agoparentprevThough I appreciate the effort you put into your response, I think you're quibbling and missed the point. His qualm is with companies developing what amounts to a good demo (whether you call it MVP, prototype, Beta product, etc), validate their hypothesis but then call it a finished product. Validation is supposed to be just that, validation. Instead he's arguing companies call that initial validation market success, then relabel the demo a working product, warts and all. And, you're correct in saying the Vision Pro didn't flop because it wasn't developed properly. It flopped because no one wants what it has to offer, and that's a different problem from what the author describes reply ljm 31 minutes agorootparentI'm still not convinced on this though. These MVPs are usually a lot less minimal than you would expect for a prototype far more than just wireframes and a basic POC that you expect to chuck out and rewrite. Might even have a handful of paying users as well. Since the comparison was made to games development, I think the closer equivalent is an early access release where you're generally paying for a WIP game at a lower price. Money changes hands, you get access to product in return, but there's no guarantee that it would ever be 'finished' or even what 'finished' might mean. reply globular-toast 10 hours agoparentprev> Video Games are generally competing for a slice of a large preexisting market. The test is whether it can compete against existing products. A new to the world software start up is trying to serve a need that is currently unserved. I feel like it's completely the opposite. A new game doesn't need to \"replace\" old games. People have played those games. It only needs to be new and good enough to get people's attention. But tools usually are replacing something existing. Tech that actually creates new possibilities is few and far between. The internet is one example of that. Can you think of another? I think most new tech is aiming to replace older tech that is currently used for those problems. reply evv555 6 hours agorootparentI don't really disagree but if you look at the most played games on steam many of them are now old. There's a growing inventory of games that are turning into classics. reply philipwhiuk 4 hours agorootparentNot really. Prior classics age out at roughly the same rate. reply awesome_dude 13 hours agoparentprevnext [11 more] [flagged] dang 10 hours agorootparentLet's not bother with that please. We want curious conversation here. Posting interesting and curious things matters infinitely more than spelling. Of course it's always fine to ask for clarification when necessary. reply awesome_dude 10 hours agorootparentI want to be clear on what the idea being conveyed is, I'm guessing one thing, but the poster says another. That's a bad thing? reply dang 2 minutes agorootparentOh no, like I said, it's fine to ask for clarification. You should just do it in a way that is clearly distinct from chastising someone for typos. That wasn't clear in your post. blululu 11 hours agorootparentprevFair point. I could have and should have done better. Sadly the edit window has passed otherwise I would clean up the original. In a sense, this more or less represents the core purpose of an MVP in action. The author might take the same lesson and revisit this essay in light of the feedback here (or just abandon it as a misstep that is not worth the effort to refine). I was sincere in saying that I appreciate the effort to get something out and start a conversation. While I disagree with most of the essay, there is certainly something to be said for striving to produce high quality work in everything we do. An MVP mindset can be an excuse for not holding ourselves to the highest standard (case in point, me posting a thing with half a dozen typos). reply awesome_dude 10 hours agorootparentThanks, I didn't mean it as a dig, I just wanted to be sure that what I was guessing was being said was what you actually meant. reply isoprophlex 12 hours agorootparentprevWho cares. The meaning is apparent and the structure is coherent. A well written comment, so these things seem too minor to call out IMO. What's more, these days if you include some strategic typos, you help convince me that you're not a lazy slop-poster, or an outright bot. reply awesome_dude 10 hours agorootparentChatGPT can you write me a response that includes enough typos to look human, but not enough that I look like a prince from Nigeria tracking down relatives to give them money? reply isoprophlex 8 hours agorootparentI don't disagree but as long as 95% of people forget to do this, it's still a useful heuristic, i guess... reply throwup238 12 hours agorootparentprevnext [3 more] [flagged] mvdtnz 11 hours agorootparentDon't post ChatGPT output in HN comments. reply throwup238 11 hours agorootparentIt wasn’t ChatGPT, I just deleted a “p” and some apostrophes and spaces. It was just faster than swiping to the app. reply pflenker 12 hours agoparentprevI think it’s also worth mentioning that the author fundamentally misunderstands a MVP. A vertical slice video game presentation _is_ a kind of MVP, and there are tons of anecdotes e.g. from the E3 that tell us just how minimal these can be. MVPs take different shapes and forms, a polished but limited video game level is no different than a polished landing page with functionality limited to e.g. sign up. reply caseyy 10 hours agorootparentThe most equivalent milestone to MVP in games is first playable (FP) or prototype. This is done at first pass/L1/greybox quality. Vertical slice (VS) is a type of beautiful corner — this is done at production quality. The purpose of FP is to prove the game loop and that a game is worth producing — that it is viable, or you could say that it has reached the minimum viable state. The purpose of VS is to try out the entire production process and test burndowns, etc. I can confirm, as someone who has worked in games for decades, that the author understands it correctly. FP in automotive would be a prototype car, VS in automotive would be the first factory produced car. VS in games often marks the end of pre-production and a shift of priorities from iterating and experimenting to producing bulk content. MVP would be much earlier. Then in another sense, MVP is already marketable and commercially viable. But a game is that neither at VS nor FP. So if you look at MVP from that perspective, it is not even close to either VS or FP. It would be like somewhere beyond around alpha. In any case, MVP != VS :) The MVP concept doesn’t work with game production that well because it’s a hit driven industry where most of the costs go into producing the hit. Like in movies, music, TV and book publishing — there are many stages of green-lighting before a product is first made available to the market as going from zero to market is where the bulk of the costs are. Going zero to market MVP as the first green-light check would be quite expensive ($50M for market leading VR/handheld, $100M for market leading console and Windows games minimum spent by the time a game is shown to the players) and risky. So instead, we start green-lighting and reviewing the prototype whenthe last 80% give us something much more that isn’t quantified: The feeling of having completed something of value, and having done it properly, carries an inherent value that surpasses the last 20% output. It is unquantifiable and priceless. This is when work or products become timeless and truly valuable. Not to mention that feeling of satisfaction and completeness of taking an accomplishment to that level. This is why software development _as a job_ sucks, and sucks deeply: how often do you get to put the icing on the cake, and put a ribbon on it, and get a final effort that matches what you were able to envision ? \"Job\" satisfaction is for _hobbyist_ software development. Capitalism generates crap software. reply _heimdall 6 hours agorootparentI've always looked at this a bit differently. For me the last 20% is fulfilling, but it's also a grind. In a software job I rarely have to do that to get paid, I can spend most of my days on the easy stuff that gets far enough. The pay is good enough that I can spend my time outside of work doing what I want and put in the effort to grind through the last 20% and really feel proud if the end result. This may be why so many software developers gravitate to wood working. If you have the time to put in the effort for that last 20% its very noticeable and satisfying. reply LawrenceKerr 6 hours agorootparentprev> Capitalism generates crap software. As opposed to state-funded software development, which is renowed for its high quality and innovation. reply hnthrow90348765 5 hours agorootparentYou'd see a lot more from both sides if the motivations were there. The motivation in safety critical stuff is not killing people. It won't matter if I get a boring CRUD app feature to near perfection, I make the same regardless, and that's true in private companies and government. I think the safety critical developers maybe have some deep itch to scratch and compensation is way less important to them (otherwise they should be making millions in salary given the stakes), but we don't need to use that bar for every developer or product. But maybe AI will commoditize all of the old boring CRUD apps and those kinds developers are only worth $20-30/hr. reply palata 2 hours agorootparent> and compensation is way less important to them (otherwise they should be making millions in salary given the stakes) I think I disagree. Doing safety critical does not mean you work 1000x more. Just that you put more care into what you do (you focus on safety vs productivity), have audits and actual processes to ensure quality. reply isleyaardvark 1 hour agorootparentprevNASA is a counterpoint. If they actually have resources, the government is capable of good work. But when it is done on the cheap you don’t get the best work. Whether it’s from underfunding the particular agency, or when they have to outsource to private contractors (often by law the lowest bidder). Don’t know how that fits into the capitalist/state-funded matrix. reply palata 2 hours agorootparentprevBecause it is funded by a capitalist government does not mean it's not living in a capitalist world... didn't you mix it up with the difference between private and public projects? reply latency-guy2 13 hours agoparentprevFor yourself, sure. But likely not for your users. In fact, I would bet Pareto is not extreme enough in this scenario. E.g. Excel or git, or their potential eventual successors. Former and latter has largely being the same commands and feature set used by 99% of users since V1. They are now old, storied projects with enhancements and features/improvements that go decades long, and even inspired or spun out new products/projects out of the ideas built within. For the article itself, Pareto exists as a reminder that work expended is rarely if ever equal to results produced. There are instances where it pays off. But you always pay a price. Make sure you're willing to pay that price. Sometimes a chair with 3 legs is all you need or care for. That 4th leg might give you more balance in an uneven plane, but I work in a decently flat garage and I'm not paying the premium for that 4th leg. reply niemandhier 11 hours agoprevSoftware that is critical is not build like that. Medical device control software is not build like that, drone flight control is not build like that, power plant safety is not build like that. The problem that I noticed in the recent years is: People see the fast dev cycles for non critical software and think they can replicate it in areas where it really does not fit. I guess that’s how we ended up with Teslas self driving. I am a bit worried that ai seems to be build like that, using development cycles fit for a convenience appliance for what could be used as a weapon. reply mrkeen 11 hours agoparentI always wonder where the devs come from who end up doing important work. Seniority doesn't mean anything if a dev's 20 years has been spent flinging crap over the wall and then wondering how to keep up with all the support tickets being filed. How does one get onto the \"software is suppose to work\" career track. reply Cthulhu_ 10 hours agorootparent> How does one get onto the \"software is suppose to work\" career track. Boring companies that have had IT for a long time; industries like government, taxes, energy, administration, CRM, insurance, pensions, banking, etc. You won't get recruiters knocking on your doorstep to come and work for those though, and you'll possibly be working with 10+ year old tech and development practices. reply WJW 5 hours agorootparent10+ year old development means practices like scrum and agile? Or do you mean 10+ year old tech like Golang and Rust? /s of course but I think you need to calibrate your level of \"old\" :) reply meiraleal 1 hour agorootparentReact included reply Animats 10 hours agorootparentprevGo into database internals, or flight control, or hard real time operating systems. Those are areas where it has to work. reply potatoman22 3 hours agorootparentprevIn my experience: all over. It's the company that teaches them not to screw up the important work. reply palata 2 hours agoparentprev> Medical device control software is not build like that > power plant safety is not build like that. Agreed. > drone flight control is not build like that You obviously haven't been working in the drone industry, have you? Just a guess :-). reply bigmattystyles 13 hours agoprevI feel like every concept can be taken too far or is expected to perfectly encapsulate every situation. The few principles I live by are vague to avoid that predicament 1. Don't let the perfect be the enemy of the good 2. Under promise, over deliver 3. Graveyards are full of indispensable men 1 took me a long time to really learn 2 In my case, where I've sucked at estimations, it's really not over deliver, but deliver my under promise. 3 is a De Gaulle quote and my favorite when I think I can't be replaced because I've had an ego boost from a recent accomplishment. Alternatively, it can also be interpreted as 'the world goes on'. reply froddd 12 hours agoparentThat quote is Georges Clemenceau, not Charles De Gaulle. He was a French politician, but long before De Gaulle. reply goldfishgold 4 hours agorootparentNot Clemenceau either. https://quoteinvestigator.com/2011/11/21/graveyards-full/?am... reply froddd 1 hour agorootparentThat’s a rabbit hole level of interesting! Thanks for sharing! reply bigmattystyles 52 minutes agorootparentIt's why I come to HN honestly reply lsy 11 hours agoprevThere’s a conflation going on here. Pareto can be good engineering—a product that solves 80% of use cases at 20% of the cost is a great efficiency: tax prep software for simple tax returns only; a minimalist photo sharing site with few social features; a phone with a great UI and no user-installable apps. This gets conflated with products that are 80% reliable across all their tasks (LLMs, brittle software). That makes it difficult for users to rely on the product, because occasionally a failure will happen, and the user can’t build a mental model of what works and doesn’t. reply MisterKent 10 hours agoparentThat's not pareto. That's just finding a niche of people who would prefer more focused products. Tax prep software for simple returns only is an entire product. Adding support for the other 20% would lose your initial base's interest. Tax software that aims to solve all problems whose MVP is it handles 80% of people's tax returns is the pareto the author is talking about. But the real complexity is the other 20%. Pareto as a minimalism process for focused product development is not engineering (good or bad). Forgetting pareto and believing (or lying) that you are truly 80% of the way there is a big problem in engineering and funding. The author is correct in that. reply Puts 7 hours agoprevWhat if we come to a point where even the investors don't care if you can finish a product at all? If the pitch is good enough and they think you can bring in more investors down the road, they will get back their money at a higher evaluation anyway. This would actually explain why so many startups fail – nobody cared for them to succeed anyway, because the initial investors got rich even without there being a product. reply LeFantome 54 minutes agoparentThat just means the investors are your real customer. reply matt_s 4 hours agoprevFor building software that is your basic web application/SaaS an important part of agile processes, including an MVP, is you build the minimum to get the product out there with lower priority features in a backlog. The idea behind vertical slices in traditional software development (not games) is you have a testable slice of software that can be shipped. A major benefit of iterative development is you may have features sitting in a backlog that keep getting pushed aside for higher priority features that you aren't expending software development and testing resources on those features. Contrast that with a waterfall approach where an entire product is designed up front, requirements documented and then built. The product likely ends up with features that are not important and rarely used. Iterative development, or agile, processes are not the best process for every project. You definitely want high risk projects like nuclear power plant control software to use a waterfall approach to ensure safety. reply Clubber 4 hours agoparent>Contrast that with a waterfall approach where an entire product is designed up front, requirements documented and then built. The product likely ends up with features that are not important and rarely used. In software development, waterfall projects were often iterative, though in longer cycles. I'm sure there were some that weren't but each version is essentially a waterfall iteration. For example, way back when, we would do 2 major releases and 2 patch releases every year, so our iterations were 3 months. Keep in mind this was software that we cut onto CD's and shipped out. The benefit of waterfall is that biz is required to think about the project as a whole instead of a wishlist. Sometimes with agile, you end up with a Homermobile because the biz isn't forced to think of everything at once. Both have plusses and minuses. reply Justta 12 hours agoprevFirst 20% of effort will finish 80% of the work. Second 20% effort will finish 16% of the 20% left.Totally 96% will be finished. reply abhorrence 3 hours agoparentI once had a PM who loved the Pareto principle a little too much, and would constantly push us to \"apply it\" even after we already had. I got frustrated by this and drew the graph that goes along with your sentence, showing that miraculously about 99% of the work can be done with 60% of the effort! My PM did not take the correct lesson away from the encounter. reply knowhy 11 hours agoprev> I think that the Pareto Principle is technically true in a lot of fields, but I also feel our society would be a lot better off if we didn’t know about it. I doubt that. From what I understand Vilfredo Pareto introduced it to describe the existing allocation of wealth in Italy on the brink of fascism. He claimed that the crops in his garden followed this principle. I highly doubt that that can be replicated. Ever since people refer to the Pareto principle when they observe a 80/20 distribution. Like it is some kind of natural law. But it is not. At least I have yet to see a scientific explanation why a 80/20 distribution would have any kind of special meaning. Just because some distributions are 80/20 doesn't mean there is anything special about it, a lot of distributions are not 80/20. So I think society would be better off if people would stop acting like it is a natural law and there is nothing to change about it. Paereto distribution is dangerous since it is applied to justify hierarchies in society. And it is just not a good justification. reply marcosdumay 5 hours agoparentHum? There's pretty much a mathematical rule that states that variables free to have any values tend to distribute themselves like that, just like the one that states that variables that are bounded tend to distribute themselves normally. Or are you talking about the specific numbers? Because yes, the specific numbers are almost never correct. reply knowhy 3 hours agorootparentPareto distribution can be expressed in mathematical terms. So what? That does not explain why a specific distribution should follow this rule. reply prmph 4 hours agoparentprevhttps://en.wikipedia.org/wiki/Pareto_principle#Mathematical_... reply Archelaos 3 hours agoprevFollowing the discussions here, I found that it could benefit from a better understanding what the Pareto principle is generally good for and what it is not. So here are my thoughts. The Pareto principle is useless if considering just a specific predetermined goal in isolation. If I want to climb to the top of a mountain, I have to climb 100 % of the height. Then it makes no practical difference, when I know that I can reach 80% of the height in 20% of the time, for example. However, the Pareto principle encourages us to (re)evaluate a goal in the light of limited resources. Is it better to be satisfied with only climbing 80% of the height of the mountain and use the time saved for other activities that I would otherwise miss out on? The answer to this question depends on other more general goals that I am pursuing. Applied to software development, this tells us that we should consider for example implementing certain features or striving for a certain level of quality not as goals in themselves, but as framed by more fundamental goals. It is therefore no wonder that given the same code base, the immediate goals what to do next can differ greatly depending on what the fundamental goals are, such as earning money vs. having fun vs. taking pride in, etc. (they may align by coincidence, though). The Pareto principle helps us to (re)evaluate and compare immediate goals in the light of such fundamental goals: Is it better to implement feature A completely and dispense with feature B, or is it better to implement a simplified feature A´ and have room for a simplified feature B´ in the same timeframe (in this example the limiting resource)? Here, the fundamental goal is implicit in the utility function indicated by the word \"better\", in our example better according to earning money vs. better according to having fun vs. better according to taking pride in, etc. Of course, considering the Pareto principle when (re)evaluating immediate goals does not gurantee to arrive at the best conclusion. And there are additional considerations outside the Pareto principle, such as short-term goals competing with long-term goals under the same fundamental goals, or legal obligations that are non-negotiable. Here, we enter the sphere of policies, where the policy makers decide upon regulations beyond the individual fundamental goals. In practice we have a hierarchy of multiple goals. On each hierarchy level the Pareto principle is still worth considering as long as there are conflicting goals and limited resources. To recapitulate: The Pareto principle can only be applied meaningfully when evaluating certain alternative goals according to a given utility function for one or more specific limited resources. reply emsal 13 hours agoprevDoesn't this fundamentally misunderstand the Pareto principle? The 80% and 20% of causes in standard examples don't refer to portions of a sequential effort, but rather slices of competing agents/producers/customers in an economic system. Like, 20% of clients account for 80% of sales, that kind of thing. reply minitoar 13 hours agoparentSeems analogous to me. “20% of the functionality (in a product) is 80% of the work (to implement it)” reply openrisk 10 hours agoprevGood post even if only for making the term \"vertical slice\" more broadly known. Clearly the applicability of this alternative in other domains might not be as direct as in gaming software but its an interesting way to think about how to structure deliverables on the way to the \"1.0\" release. In other words thinking of horizontal and vertical slice chunks of work, where horizontal means perfecting one functionality that applies across all product components (may not be visible to end-user), while vertical is perfecting all functionalities of one component that is visible to the end user. reply ryandvm 3 hours agoprevAt first I thought this was about the \"Peter Principle\" and I couldn't agree more. reply scarab92 12 hours agoprevIf the MVP finds product market fit and the market is large enough, then the economic incentive to finish the remaining 20% will exist. If the market isn’t large enough, then the customer still got 80% of the value whereas in the authors idealised world, they likely wouldn’t have gotten anything at all, since the minimum cost to develop it was 5x higher (assuming 80/20 holds). Overall it seems we’re better off with startups following the Pareto principal than not following it, and the authors real issue is just with bad product management decisions afterwards. reply jpease 3 hours agoprevScanning the comments, it looks like 80% of you agree with 20% of what the author is saying. reply gunian 10 hours agoprevIf I had around 8-12 weeks to live and have a project that is 5% complete what would the right allocation of resources be? reply drawkward 3 hours agoparentAllocate 0% of resources of the project, and spend your remaining time with loved ones, see some wonders, give them experiences to remember you by, make things right, etc. reply spencerflem 6 hours agoparentprevGenuinely, if I had 8-12 weeks to live, the project would not be my priority, unless it was a welcome distraction. Prioritize whichever parts bring you joy, and do something about the 8-12 weeks if you can. reply unsigner 9 hours agoprevvideo games are guilty of this too, and the proliferation of Unity and Unreal is partly due to an overoptimization for prototypes reply Cumpiler69 7 hours agoparentA lot of Unity and Unreal games seem to have one thing in common: unoptimized slop with garbage performance with smeary TAA and needing a 4090 GPU to get decent performance. Game studios don't care about optimizations anymore, they care about shoveling something out the door as quickly and as cheaply as possible. reply Finnucane 5 hours agoprevVideo games are not tech? reply awesome_dude 13 hours agoprevMy thought on the article is by and large most people don't care if the product that have is perfect, they (myself included) only really care that it does whatever job it's supposed to do to a level that's passable. I was, and still am, prepared to use a large number of things that aren't perfect housing, transport, furniture, computers, clothing, FOOD, and more. edit: Added Food, I don't get the best chef on the planet to prepare my meals every day, or any chef for the most part, not only because I don't have the money, but also because I don't value that sort of thing enough I'm more than happy with my own cooking for the most part. reply LeFantome 1 hour agoprevYou may or may not have to finish the last 20%. Since the whole article is based on the premise that you have to, it fails for me. There are attributes of your project that are table stakes (absolutely required to compete) and there are regulatory or safety requirements. You need all those to ship. However, everything else is “value” or even just “perceived value” and it is up to your customer which ones “you have to do””. Let’s put the 80/20 rule another way. If I can get 8 out of 10 features in 20% of the time, it makes sense to do those 8 ( assuming they all have at least some value ). But what do I do with the 80% of the effort that it would take to get the last two features? The answer is opportunity cost. What could I do with that time instead? Put another way, what am I NOT going to be able to accomplish because I chose to add those last two features? If the answer is that I have other features that customers value more, I should do those instead. If I have a backlog of features that all take the same effort as the first 8. I can do 32 of them in the time it would take to deliver the original 2!! The statement was made that “customers do not like to use 80% of a website”. If I read this article, maybe I implement the full 10 original features. If I embrace 80/20, maybe I implement 40 instead. Hey look, the “just do 100%” approach resulted in a website with 25% as many features as 80/20. If “customers do not like 80 percent of a website”, they are probably even less happy with 25%. Right? Now, not all features have the same value. So, the math is not as simple as above. But, in my view, this is the right way to think about 80/20. The perfect is the enemy of the good. So that tue perfectionists can hate me even more, the same is true “within” features (or whatever other axis you are evaluating). Sometimes customers “expect” or even “demand” features they do not really use. Compliance is a an example. Or stuff that used to matter in a product category (and is still used as a filter) not really does not anymore. You can get a lot of value in your product by adding this stuff, but it is a waste of resources to “do it right” or match every competitor like for like. You may find again that you get essentially all the market success “value” from doing some fraction of the work. Note, I am not saying to ship stuff that is buggy or stuff that does not really work. If that is what you think I am saying, you misunderstand. A shorter version may to say “build what your customers will actually use and not much more”. What you really want to spend your time on is the stuff that excites people, that differentiates your offering, and takes “relatively” little effort to execute. That is probably not “the last 20%”, most of the time. reply renewiltord 10 hours agoprevI actually like this stuff. I get to use a lot of software. One of my favorites is half baked open source. Pretty good to read and get ideas from. reply wickedsight 10 hours agoprevAbout Pareto applied to AI, I find this interesting: > Even people who advocate for these technologies rarely assert that the results are useable as-is, especially in a world where people are accustomed to a much higher, human-level quality. At best they are useful as a starting point for a human to then finish the image, or the cover letter... I think this might be a bit of a thought trap. Because the people working on and advocating for these technologies are definitely in the top 20% of intelligence/capabilities/whatever you want to call it. For that 20%, the (arguably achieved) 80% might not be good enough. But there are a lot of people who are already vastly outperformed by many of the currently available AI tools in many tasks. There are many people who are just awful at writing and can already benefit greatly from these tools, for example to write readable cover letters. Or for explaining things in basic language, something I often use it for when trying to understand complicated texts from another field. The other side of Pareto is that perfect is the enemy of good. Sometimes 80% adds so much value for the majority, that the other 20% isn't necessary to label something good enough. The article contains an image of a Cyber Truck, which is fitting, but I truly loved my early Model 3 which was arguably also only 80% done. reply eecc 8 hours agoprevYeah, let me drop one example: JIRA. reply monero-xmr 13 hours agoprev [4 more] [flagged] dang 13 hours agoparent [–] Please don't post like this. I actually banned you until I double-checked and saw that you're a legit user. Your comments have veered too far in the direction of ideological battle and flamewar. That's not what this site is for, and destroys what it is for. Please veer back. https://news.ycombinator.com/newsguidelines.html reply monero-xmr 13 hours agorootparent [–] Uhhh why is my comment a flamewar? reply dang 10 hours agorootparent [–] I was referring to your comments in general. Your GP comment wasn't flamewar. But it was quite offtopic and we got complaints about it being unintelligible. reply Consider applying for YC's Spring batch! Applications are open till Feb 11. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Bobby Lockhart critiques the tech industry's reliance on the Pareto Principle and MVPs, which often result in unfinished products due to insufficient incentives to complete them.",
      "He contrasts this with the gaming industry's 'vertical slice' approach, which focuses on creating a polished product from the start.",
      "Lockhart advocates for a craftsmanlike approach to software development, emphasizing the importance of completing and refining products for a polished final outcome."
    ],
    "commentSummary": [
      "The Pareto principle, or the 80/20 rule, is being discussed in the context of tech, particularly in software and product development, suggesting that 80% of effects come from 20% of causes.",
      "Critics argue that this principle results in products being marketed as complete despite lacking the final 20% polish, while supporters claim it enables efficient resource use and rapid market entry.",
      "The debate spans multiple industries, including gaming and AI, with differing views on whether the principle fosters innovation or leads to mediocrity."
    ],
    "points": 235,
    "commentCount": 95,
    "retryCount": 0,
    "time": 1737604388
  },
  {
    "id": 42796906,
    "title": "I made a multiplayer shooter game in Lisp, here is my journey",
    "originLink": "https://ertu.dev/posts/i-made-an-online-shooter-game-in-lisp/",
    "originBody": "Home » Posts I made a multiplayer shooter game in Lisp January 22, 2025 Developing a multiplayer third-person shooter game as a solo developer is a journey filled with challenges and rewards. I embarked on this adventure to create Wizard Masters, a web-based multiplayer game where players battle as mages wielding elemental spells. Built using Clojure, a Lisp dialect, this project pushed the boundaries of web game development and my own skills as a programmer. Here’s how it went. In Wizard Masters, players can choose from six elemental spells—fire, toxic, ice, lightning, and earth—and compete in two modes: solo and team deathmatch. I published the game on CrazyGames to reach a broader audience. However, its multiplayer nature demanded a large player base, which was a constant challenge. Game Link: https://wizardmasters.io Why Clojure? Clojure is my go-to programming language, both professionally and personally. It’s a full-stack language: Clojure runs on the JVM for backend development. ClojureScript compiles to JavaScript, enabling smooth browser-based applications. This one language to rule them all approach made it an obvious choice for me. Additionally, Clojure’s REPL ( Read-Eval-Print Loop) system is a game-changer. Unlike typical REPLs, Clojure’s REPL is highly interactive and organic, allowing live updates to the game without refreshing the browser. This significantly sped up my development process, enabling me to create and test mechanics in real time. I created a YouTube video showing how to use the REPL with jMonkeyEngine, a Java-based game framework. You can watch it here. Graphics Library After experimenting with several graphics libraries, including Three.js and PlayCanvas, I chose Babylon.js. Here’s why: Feature-rich: Babylon.js offers a robust set of tools for 3D development. Great documentation: Compared to other libraries, Babylon’s documentation stands out for its clarity and comprehensiveness. Supportive community: The community’s assistance proved invaluable. ClojureScript’s npm integration through shadow-cljs made it easy to incorporate Babylon.js, allowing me to focus on building the game. Code The following code snippet showcases the implementation of a :player/jump rule, which handles the player’s ability to jump in the game. This rule uses a :what block, essentially a hashmap, to define the conditions under which the rule is triggered. In Wizard Masters, all game data resides in a global game database—a single large hashmap. The fields referenced in the :what block (e.g., :pointer-locked?, :player/ground?) are keys in this global hashmap. To execute the rule, several conditions must be met: Each key in the :what block must have a non-null value. The :when block must evaluate to true. The :then block is executed when the above conditions are satisfied. The triggering of this rule occurs through the fire-rules function, which activates the rule whenever a :what field is updated, regardless of whether the value changes. If a field has a :then false attribute, the rule will not trigger, avoiding infinite loops and unnecessary executions. The rule system was influenced by the Clojure library O’Doyle Rules by Zach Oakes. Here’s the complete implementation for the :player/jump rule: (reg-rule :player/jump {:locals {:jump-vec (v3)} :what {:keys-pressed {} :player/mobile-jump-click? {} :pointer-locked? {:then false} :player/ground? {:then false} :player/capsule {:then false} :player/anim-groups {:then false} :keys-was-pressed {:then false} :player/jump-up? {:then false}} :when (fn [{{:keys [player/game-started? player/mobile-jump-click? player/ground? player/dash? player/jump-up? player/current-running-anim-group player/current-health]} :session}] (and game-started? (nil? current-running-anim-group) (or (re/key-is-pressed? \"Space\") mobile-jump-click?) ground? (not (freezing?)) (not (wind-stunned?)) (not jump-up?) (not dash?) (> current-health 0))) :then (fn [{{player-capsule :player/capsule player-jump-force :player/jump-force} :session {:keys [jump-vec]} :locals}] (j/assoc! jump-vec :y player-jump-force) (when-not (casting-spell?) (re/fire-rules {:player/jump-up? true})) (let [player-pos (api.core/get-pos player-capsule)] (api.physics/apply-impulse player-capsule jump-vec player-pos)))}) How It Works :what block defines dependencies, such as key presses and the character’s physical capsule etc. :when block ensures the jump can only occur under certain conditions (e.g., the player isn’t frozen or stunned). :then block applies an upward force to the player’s capsule and triggers additional rules if needed. The next example demonstrates a rule for updating the player’s rotation every frame. This ensures the character always faces the forward direction of the camera. Here, the dt field in the :what block represents the time elapsed between the current frame and the previous one, causing the rule to trigger on every frame. (reg-rule :player/rotation {:locals {:forward-temp (v3) :result-temp (v3)} :what {:dt {} :camera {} :player/model {}} :when (fn [{{:keys [player/game-started?]} :session}] game-started?) :then (fn [{{camera :camera player-model :player/model} :session {:keys [forward-temp result-temp]} :locals}] (let [[yaw offset] (api.camera/get-char-forward-dir {:camera camera :forward-temp forward-temp :result-temp result-temp})] (m/assoc! player-model :rotation.y (+ yaw offset))))}) Network Writing network code for a fast-paced multiplayer game was a monumental task. I initially chose to handle all the networking myself, thinking it would be a rewarding experience. It was indeed rewarding—but also hellish. I used several Clojure async libraries, including Aleph, Manifold, and core.async, to manage the complexity of real-time communication. When a player joins a game, a WebSocket connection is established. The backend continuously sends world snapshots to all connected players at a tick rate of 20 (one update every 50 milliseconds). Additionally, specific processes handle actions like spellcasting, damage calculation, and player deaths. The following snippet registers a process called :super-nova, which handles the logic for a player casting a Super Nova spell. When triggered, the backend processes the event asynchronously, computes the area of effect, and notifies relevant players. (reg-pro :super-nova (fn [{:keys [id data]}] (try (add-super-nova id (:pos data)) ;; Add the visual effect (apply-range-damage {:current-player-id id :pos (:pos data) :radius super-nova-range :diameter super-nova-diameter :max-damage 600 :damage-pro-id :got-super-nova-hit}) ;; Calculate damage (catch Exception e (log/error e \"Super nova hit error!\"))))) Area of Effect Damage Calculation The core of this process is the apply-range-damage function. This function determines which players are affected by a spell, calculates the damage based on proximity, and updates the game state accordingly. (defn apply-range-damage [{:keys [current-player-idposradiusheightdiametermax-damagedamage-pro-idshape-typedamage-over-timedamage-params] :or {shape-type :sphere}}] (when-let [room-id (get-room-id-by-player-id current-player-id)] (let [current-player-ids (set (keys (get-players-with-same-room-id current-player-id))) my-team (get-player-team current-player-id) players-within-range (->> (get-world-by-player-id current-player-id)(keep(fn [[player-id player-data]](let [[x y z] [(:px player-data) (:py player-data) (:pz player-data)][x1 y1 z1] posplayer-distance (distance x x1 y y1 z z1)within-range? (if (= shape-type :cylinder)(within-cylinder? pos [x y z] radius height)( (:health player-data) 0)(:focus? player-data)within-range?)[player-id player-distance]))))) damage-and-positions (for [[player-id distance] players-within-range:let [damage (generate-damage {:distance distance :max-damage max-damage :area-of-affect-diameter diameter :shape-type shape-type})damage (get-damage-for-player damage current-player-id player-id)]:when (> damage 0)](let [world (swap! world (fn [world](let [health (max 0 ( (get-in world [room-id player-id :health]) damage)) died? (= 0 health) world (assoc-in world [room-id player-id :health] health)](if died? (assoc-in world [room-id player-id :st] \"die\") world))))died? (= 0 (get-in world [room-id player-id :health]))](when died?(update-stats-after-death current-player-id player-id))(send! player-id damage-pro-id(merge {:player-id current-player-id:damage damage:died? died?}damage-params))(update-last-damage-time player-id)(add-damage-effect player-id (if (= damage-pro-id :got-ice-tornado-hit) :ice :fire))[player-id damage died?]))] (when damage-over-time (register-enemies-for-damage-over-time (now)current-player-idroom-iddamage-over-time(map first players-within-range))) {:damage-and-positions damage-and-positions}))) The process begins by filtering players to identify those in the same room, excluding teammates, and checking if they are within the spell’s radius or cylindrical area. Once the affected players are identified, the damage is calculated, decreasing with distance from the spell’s origin while ensuring it is positive and does not exceed a player’s current health. Finally, the game state is updated by deducting the damage from each player’s health, marking players as “dead” if their health reaches zero, notifying affected players of the damage, and updating the attacker’s stats accordingly. The backend handles real-time complexity by continuously updating multiple players and ensuring synchronization across all devices. Events such as spellcasting and deaths are processed asynchronously through queues to maintain performance. Developing custom networking code from scratch was both challenging and educational, offering valuable insights into the trade-offs between control and complexity. The Journey At the beginning of my game development journey, everything felt fantastic. I was quickly iterating through features, mechanics, and UI development, making substantial progress in short bursts. However, as the project grew, it became increasingly challenging to manage. The difficulty wasn’t directly tied to Clojure’s nature—there were many other contributing factors. Game development is fundamentally an art of state management. States are everywhere, and managing numerous unrelated systems in harmony is a challenging task. While Clojure’s immutability by default offers many advantages, it also introduces complexity. To handle the intricate state management required for game development, I had to create my own abstractions. Writing a custom DSL (domain-specific language) became a necessity, but it wasn’t easy. Adding to the challenge was the lack of a strong Clojure game development community. While Clojure excels in domains like SaaS products and finance, its adoption in game development is virtually nonexistent. The absence of shared tools, libraries, and best practices in this space made the journey even more isolating. I couldn’t help but feel envious of the tooling ecosystems surrounding major game engines like Unity and Unreal Engine. While Babylon.js is a great library, it lacks the robust plugin ecosystems, frameworks, and tools that mainstream engines offer. Developing with Babylon.js often meant building tools from scratch. While this taught me a lot about graphics programming, it came at the cost of time—a critical resource in modern game development. Good tooling is a cornerstone of game development today. The lack of it left me feeling perpetually one step behind. Developing 3D games for the web comes with inherent limitations. While WebGL has improved significantly, and WebGPU is on the horizon, web-based games are still far behind native games in terms of performance and graphical fidelity. These limitations force developers to make compromises. Resource constraints on the web are another hurdle. Users expect quick load times and minimal downloads. Asking players to wait for a 500MB download before they can play is unrealistic. This restricts web games to being small, free, and often simplistic. The result? Web games rarely rival the scale or polish of PC or console games. Financially, web game development doesn’t make much sense. Monetization options are limited, with ads being the primary choice. But ads disrupt immersion and often force you to design your game around them just to earn a modest income. The web game market is tiny compared to PC and console markets. While there are a few success stories like Agar.io and Wordle. Only a couple of major web game publishers, like CrazyGames and Poki, exist. Rejection from these platforms can make it nearly impossible to reach a large audience. To sustain yourself financially, you’d need to create a high volume of games in a short period, which is neither practical nor creatively satisfying. The one advantage web games have is ease of distribution. Sharing a link is all it takes for anyone to jump in and start playing. But this strength alone doesn’t compensate for the many weaknesses. Making a game is hard; making a successful game is even harder. Creating a good game is like cooking a great meal—you can do wonders with a handful of ingredients or fail miserably with a dozen. There’s no single formula for success. Everything needs to align: timing, optimization, art, sound, trends, and theme. A few years ago, the battle royale trend exploded, leading to a flood of games trying to capitalize on the craze. Even great games like Spellbreak couldn’t sustain themselves. But following trends isn’t inherently bad; it’s about execution. If done well, it can be a smart business move. Final Thoughts Game development is a deeply rewarding yet demanding journey. Whether you’re building games with unconventional tools like Clojure or navigating the limitations of web platforms, the challenges are immense. I’ve learned several key lessons. First, web games are fantastic for prototyping thanks to their ease of distribution, but resource constraints make them unsuitable for larger projects. Second, Clojure’s REPL and functional paradigm enabled rapid iteration, but its niche nature and lack of game development resources added unnecessary difficulty. Lastly, tooling is crucial—while building custom tools taught me a lot, the absence of robust, ready-made tools significantly slowed progress. Moving forward, transitioning to a mainstream engine like Unity or Unreal could streamline development and allow me to focus more on the creative aspects of game design. Ultimately, the act of creating games—bringing ideas to life—remains an unparalleled experience, with each challenge offering invaluable lessons. It’s not just about the end result but the lessons learned along the way. Lisp Clojure Game Development Babylonjs Web Games",
    "commentLink": "https://news.ycombinator.com/item?id=42796906",
    "commentBody": "I made a multiplayer shooter game in Lisp, here is my journey (ertu.dev)212 points by ertucetin 22 hours agohidepastfavorite42 comments emmanueloga_ 17 hours agoQuick summary: ClojureScript + Babylon.JS on the frontend and Clojure for the backend, coordination via Websockets. The point about lack of tooling for Babylon.JS is interesting: a lot of times I see people wanting to release games but spending most of the time building tools... sometimes all of the time. It is a delicate balance! Congratulations on releasing! Very cool project. reply irjustin 17 hours agoparent> The point about lack of tooling for Babylon.JS is interesting: a lot of times I see people wanting to release games but spending most of the time building tools... sometimes all of the time. It is a delicate balance! Ah we all know what we really want to do build tools! A game just makes the journey a tad more interesting. reply TeMPOraL 7 hours agorootparentOr, we want to make a game, but we also want to make the base a reusable game engine, so we can later make another game with it, and this way we can also make simpler tech demos on the way to impress our friends (real or on social media), and because of that we need to do this properly, and wait I remember the Virtual File System in StarCraft was fascinating to me as a kid, let's do something like it, and wow did you see this new ambient occluding path traced cube marched screen space voxel protrusion demo, I absolutely need to implement this for my game, and... ... this is how you end up making three skeletons of a game engine and never making that game itself, before running out of childhood and having to switch to doing webshit for a living to pay for mortgage. Ask me how I know. reply stephen_cagle 11 minutes agorootparentI love the saying \"running out of childhood\". Going to look for places to use that. reply ryandrake 53 minutes agorootparentprevThis post was physically painful to me, and also makes me paranoid that someone else has access to my /mnt/old_projects filesystem. reply casenmgreen 1 hour agoprevPLEASE PLEASE PLEASE option to invert mouse Y-axis. It looks fantastic, but I can only play if down on mouse is up on view. reply kamranjon 19 hours agoprevThis is bonkers and so cool that you did this solo. Loaded right up on my iPhone 13, prompted me to turn my phone to landscape and I was running around in a 3d world shooting spells at other players really great work and surprisingly polished for being a solo project. reply barrenko 6 hours agoparentI'd say relatively unsurprising for a Clojure project, props to the OP. reply ertucetin 10 hours agoprevAuthor here, thank you all for taking the time to read about my journey and, of course, for playing my game. I’m very glad that you liked it! reply protosevn 10 hours agoparentThanks for taking the leap by making a game with Clojure! I’m a aspiring gamedev and my focus so far has been working bottom up with C and friends. But I do love Lisp and especially Clojure, I’m really hopeful now that Jank is gonna be a full time project this year. reply tines 19 hours agoprevPlease include a little animated GIF or something that gives us a taste of what it looks like! reply mplanchard 19 hours agoparentGame's linked at the top of the blog[0], and loads in the browser, but agree a gif would be nice. [0]: https://wizardmasters.io/ reply prmoustache 3 hours agoprevhmmm seems buggy, player is aiming at the sky continuously and I can't get it to make it aim at a normal level more than a microseconds with the mouse. reply dao 2 hours agoparent> player is aiming at the sky continuously Same here :( I'm guessing this wasn't tested (at all?) in Firefox? That's unfortunate. reply Bootvis 1 hour agorootparentWorks fine for me on Firefox (Windows). reply casenmgreen 1 hour agorootparentDitto, on Linux. reply andreamonaco 2 hours agoprevCool! Even though my computer is seemingly too underpowered to play it. Anyway, do you plan to make it a big game? reply ertucetin 2 hours agoparentI guess no, if it performs really well I might consider it. reply andreamonaco 2 hours agorootparentI really hope that you will succeed! Maybe you'll want to find a way to earn money with it though reply xeromal 2 hours agoprevIncredible job! Hopped in a bit and got some great kills. Reminds me of quake reply tetris11 18 hours agoprevHoly cow, I've been playing for 20 minutes without realising. This is amazing! reply crummy 9 hours agoprevHi Ertu! Great to see your work again! Keep it up! reply cactusplant7374 1 hour agoprevHow did you learn how to make games? Any books you recommend? Or github projects you learned from? reply simplify 13 hours agoprevGame is actually really good! Was this inspired by Spellbreak per chance? Edit: I see you mention it in your blog post. I enjoyed it just as much! reply chrisvalleybay 11 hours agoprevThis was so much fun! Great work! Really brought me back to Quake 3! reply winrid 17 hours agoprevPretty cool, but it seems I crashed it to a grey screen. Even game counter stopped. I was battling someone, was fun though. reply koinedad 19 hours agoprevWas surprised that it loaded on my phone, nice work! reply snozolli 3 hours agoprevThe settings don't appear to allow y-axis inversion, but maybe I missed it. reply pjmlp 5 hours agoprevLooks quite nice. reply victorbjorklund 10 hours agoprevDamn this is a good game. reply bitwize 11 hours agoprevI found some old gaming instincts kicking in when I asked myself: did I get a frag? This is a fun little romp. Very Quake-esque in terms of mechanics (though the floaty jumps have more of a Tribes feel). Excellent job putting it all together in a Lisp... even if that Lisp is Clojure! I like to think of Lisp as a mind-tool for creatives, a way to turn thoughts into code so quickly it can keep up with the highly iterative creative process for other endeavors and enhance the reach of people working in other digital media besides code. It's probably less relevant in that capacity today than it was in the past, though. reply mentos 1 hour agoparentI played a ton of Quake and Tribes 2 I felt very at home not sure if it was intentional but definitely an awesome result. reply HenryBemis 19 hours agoprevInteresting to see that I already had an account at CrazyGames. It wasn't loading from your URL, but I saw that it's CG so when I logged on with my account my Firefox played ball (I got ABP, Ublock, NoScript, PrivacyBadger, LARGE hosts file, so 'some' websites are broken ;) Cool game, fast. Someone dominated me for 4 mins and then I decided to switch to Fortnite :) reply airstrike 17 hours agoprevwhaaaa this is too good! reply beretguy 18 hours agoprev [–] How many closing parenthesis do you have on a last line? reply ARandomerDude 1 hour agoparentI earn my paycheck in Clojure and have for about 10 years. In my experience if your code ends with ))))))))))) you're doing it wrong. This is a code smell for me. Instead of (qux (baz (bar (foo x)))) use other tools: Big picture: is this code overly procedural? If so, is it possible to make it declarative and functional? Line-level picture: one of the thread macros, functional composition, etc. will make this more readable. For example: (-> x foo bar baz qux). reply dreamcompiler 13 hours agoparentprevThings that terrify ICE car drivers about electric vehicles: Range. Things actual EV drivers rarely worry about: Range. Things that terrify non-Lisp programmers about Lisp: Parentheses. Things actual Lisp programmers rarely worry about: Parentheses. reply Jach 1 hour agorootparentI don't think that's a great example, as I never heard of \"range anxiety\" until EVs started to become popular, and it was applied to and admitted by exclusively owners of EVs and also E-bikes. But it's 2025, it was enough to downvote and ignore the GP comment... Even responding with a dismissive https://www.thejach.com/imgs/lisp_parens.png is too much in current year. Another comment mentions paredit/parinfer, I'm not exactly masochistic but I hate tools that automatically type more than indentation for me so I don't use them, but I also basically never think about counting parens. reply jazzyjackson 10 hours agorootparentprevBit of a selection bias there, people with range as a concern don’t become EV drivers, same goes for people terrified of parentheses. reply bre1010 6 hours agoparentprevIn college Intro to CS was taught with Racket (a lisp) and we even had to write code with pen and paper during our exams. Got really good at quickly visually matching parentheses which is still helpful to me today in non-lisps. (But given that the handwritten code would never be run, you could also just fudge it and write a bunch of )))))'s at the end and hope the TA grading it wouldn't count them either) reply KingMob 13 hours agoparentprevAs many as needed for the level of nesting? ...you do know no s-expression editor makes you count or type those out manually, right? It's all handled by paredit or parinfer, unless you're a masochist. It's just not something any lisper thinks about. reply HenryBemis 2 hours agoparentprev [–] Exactly as many as opening! (I mean what else did you expect?) :) reply Consider applying for YC's Spring batch! Applications are open till Feb 11. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The game \"Wizard Masters,\" a multiplayer third-person shooter, was developed using Clojure, a Lisp dialect, and is available on CrazyGames. Clojure was chosen for its full-stack capabilities and interactive REPL system, while Babylon.js was used for graphics due to its features and community support. Challenges included complex networking with Clojure async libraries, state management with Clojure's immutability, and the lack of a strong Clojure game development community, highlighting the potential benefits of transitioning to mainstream engines like Unity or Unreal for future projects."
    ],
    "commentSummary": [
      "A multiplayer shooter game was developed using Lisp, with ClojureScript and Babylon.JS for the frontend, and Clojure for the backend, utilizing Websockets for coordination. Despite challenges with Babylon.JS tooling, the game was successfully released and is compatible with multiple devices, including iPhones, receiving positive feedback for gameplay and polish. Some users experienced control issues in Firefox, and the developer is contemplating expanding the game based on its performance, highlighting the project's solo development and community support."
    ],
    "points": 212,
    "commentCount": 42,
    "retryCount": 0,
    "time": 1737576222
  },
  {
    "id": 42799540,
    "title": "Trae: An AI-powered IDE by ByteDance",
    "originLink": "https://www.trae.ai/home",
    "originBody": "Docs Community Download Ship Faster with Trae Trae is an adaptive AI IDE that transforms how you work, collaborating with you to run faster. Download",
    "commentLink": "https://news.ycombinator.com/item?id=42799540",
    "commentBody": "Trae: An AI-powered IDE by ByteDance (trae.ai)205 points by Lermatroid 17 hours agohidepastfavorite341 comments softirq 15 hours agoSide note, but I hate that we're moving to a world where coding costs a subscription. I fell in love with coding because I could take my dad's old Thinkpad, install Linux for free fire up Emacs and start hacking without an internet connection. We're truly building walls everywhere. reply _Algernon_ 11 hours agoparentIt doesn't though. You can still code the old fashioned way, and you are even likely to become a better programmer for it. Personally, I tried copilot when I got it for free as a student and it didnt make a difference. The reason I know is that I was coding on two devices, one which had copilot installed the other didnt, and I didnt care enough to install it on the latter through an entire semester. Its just slightly better autocomplete, by a questionable standard of \"better\". reply redviperpt 8 hours agorootparentI agree with your overall point, but Cursor's autocomplete is significantly betters than copilots. reply mostlysimilar 2 hours agorootparentThen don't use autocomplete. reply dartos 15 hours agoparentprevDon’t worry. There’s literally nothing an llm can write or tell you that you can’t write yourself or find in a manual somewhere. reply JKolios 7 hours agorootparentEven if you absolutely have to use an LLM for some reason, there are already perfectly good LLMs for code generation that you can comfortably run on commodity hardware. reply dartos 4 hours agorootparentYeah the deepseek models are actually pretty solid. I use that with avante.vim for tedious refactors. All local. reply ryang2718 11 hours agorootparentprevAlthough, tbf, some libraries are documented better than others. Also, local llms with an agentic tool can be a lot of fun to quickly prototype things. Quality can be hit or miss. Hopefully the work trickles down to local models long-term. reply guappa 10 hours agorootparentAnd you think an llm can generate code to use an undocumented library? :D reply 3D30497420 9 hours agorootparentEven documented libraries can be a struggle, especially if they are not particularly popular. I'm doing a project with WiFi/LoRa/MQTT on an ESP32. The WiFi code was fairly decent, but the MQTT and especially LoRa library code was nearly useless. reply TeMPOraL 7 hours agorootparentprevOf course, why wouldn't it? It's a generative model, not a lookup table. Show it the library headers, and it'll give you decent results. Obviously, if the library or code using it weren't part of the training data, and you don't supply either in the context of your request, then it won't generate valid code for it. But that's not LLM's fault. reply dartos 4 hours agorootparent> not a lookup table You can imagine the classic attention mechanism as a lookup table, actually. Transformers are layers and layers and layers of lookup tables. reply andsoitis 10 hours agorootparentprevIf there are open source projects that use said library, then probably yes. reply guappa 10 hours agorootparentUnless they are not hosted on github, then no :D reply TeMPOraL 7 hours agorootparentprev> There’s literally nothing an llm can write or tell you that you can’t write yourself or find in a manual somewhere. That's like saying, there's literally nothing a service business can do for you that you can't do yourself. It's only true in a theoretical sense, if neither time nor resources are a constraint. In such hypothetical universe, you don't need a dentist you only need to spend 5+ years in medical school + whatever extra it takes to become proficient with tools dentists use + whatever money it takes to buy that equipment. You also don't need accountants, lawyers, hairdressers, or construction companies. You can always learn this stuff and do it yourself better! Truth is, time and attention is finite. Meanwhile, SOTA LLMs are cheap as dirt, they can do pretty much anything that involves text, and do it at the level of a mediocre specialist i.e. they're literally better than you at anything except the few things you happen to be experienced in. Not perfect, by no means error-free just better than you. I feel this still hasn't sunk in for most people. reply dartos 4 hours agorootparent> That's like saying, there's literally nothing a service business can do for you that you can't do yourself. No, it’s not. You’re making my statement abstract for the sake of arguing. I’m not a cook, doctor, or a lawyer. I can’t prepare meals for a party of more than 2. I can’t perform surgery. I can’t effectively defend myself in a court of law. I (and I assume OP) have programming expertise. I can write exactly all code an llm could write. For simple scripts, demos and other easily Googleable tasks, LLMs will be faster, but it’s nothing out of reach for me. These tools won’t force you to pay a subscription to code. You don’t need them if you already have experience. reply ben_w 2 hours agorootparent> No, it’s not. You’re making my statement abstract for the sake of arguing. > I’m not a cook, doctor, or a lawyer. I can’t prepare meals for a party of more than 2. They are demonstrating how over-broad your own statement was with an *equivalent* statement to show how it only passes on an unhelpful technicality. Immediately after your quotation is this: > you only need to spend 5+ years in medical school + whatever extra it takes to become proficient LLMs pass the bar exam and the medical exam. These are things which I assume I would be able to do myself if only I were willing to dedicate 5 years of my life to each. > I can write exactly all code an llm could write. I can often see many errors in the code that ChatGPT produces. Within my domain, it's just a speed-up, a first draft I have to fix. Outside my domain, it knows what I *can't* Google because I've never heard the keyword that would allow me to. On legal questions, ChatGPT (despite passing the bar exam) seems to make up cases. I belive this because I can google the cases and fail to find them. Is this because they don't exist, or because they're not indexed on Google? I don't have the legal background necessary to know — and it would take me years to get the knowledge necessary to differentiate \"it's worse than first glance\" from \"it's better than second glance\". reply tarkonaut 7 hours agorootparentprev> they're literally better than you at anything except the few things you happen to be experienced in Like speaking english and coding? reply falcor84 6 hours agorootparentI don't know how you compare, but ChatGPT's English grammar and vocabulary are significantly better than mine. And when I prompt it appropriately, it also seems to be a better creative writer than I am, at least for short pieces. reply dartos 5 hours agorootparent> it also seems to be a better creative writer than I am, at least for short pieces Don’t be so hard on yourself. Chatgpt (and other LLMs) are awful at creative prose. reply ben_w 2 hours agorootparent> Chatgpt (and other LLMs) are awful at creative prose. As are most humans. Don't get me wrong, what I've seen from even the better LLMs have a certain voice and tropes and sacherine worldview that isn't dark enough where it needs to be for the story to work; but on the other hand, what I see on some fiction writing subreddits… the AI is often a genuine improvement over amateur writers, even in cases where the AI contradicts itself about plot elements. Which is frustrating, because I have the feeling the novel I've been trying to finish writing for the last decade may be usurped by AI before I get my final draft. reply codr7 1 hour agorootparentprevExcept it has no freaking idea what it's doing, that's the difference. reply knowitnone 48 minutes agoparentprev\"install Linux for free fire up Emacs and start hacking without an internet connection.\" that still works you know. Nobody is forcing you down this subscription path (except Microsoft) reply wrsh07 15 hours agoparentprevYou can run it all locally: https://github.com/ggml-org/llama.vscode reply wilg 15 hours agoparentprevThis is a free tool (though I wouldn't use it since its from Bytedance). Also you could have an AI powered IDE locally without a subscription. reply d1sxeyes 10 hours agorootparentFor now. It’s not clear what the monetisation strategy is, but probably it will be paid in future (alongside whatever other strategies they may have, like selling data, etc) reply ltadeut 4 hours agoparentprevThat's not going away at all though. But I am glad we now have more paid options available. Tooling is important and people that do good work should be able to charge for high quality tools. I would be much happier in a world full of tools licensed like Sublime Text, where I can purchase a license and just run it without the need to constantly phone home though. reply sdesol 11 hours agoparentprev> world where coding costs a subscription I think you are approaching this with the wrong mindset. I see it as I'm paying somebody to type and document for me. If you treat LLMs like a power tool, it is very easy to do a cost benefit analysis. reply pjmlp 11 hours agoparentprevThat is what happens when developers want to be paid for their work, but refuse to pay for the tools they use, regardless of little they may cost. So we're going back to the last century, but given we are in a different computing context, only the stuff that can be gated via digital stores, or Web Services, gets to have a way to force people to pay. reply lazycog512 4 hours agoparentprevThis is an age where you can write your own LLM extension. There's no moat, all the clever prompting tricks Cursor et al. are just that there is no secret sauce besides the model at the other end. Complexity isn't an issue either, have the model write the interface to itself. reply guappa 10 hours agoparentprevYou can still do like that. The problem is that coding was a passion, but turned out to be very lucrative profession so loads of people who can't do it want to do it. This is why we have languages like Go, and AI tools: allow people who don't want to learn how to be developers, to get a job as developers. reply Yasuraka 6 hours agorootparentAnd here I thought those are all using Python, JS and Ruby. reply guappa 2 hours agorootparentI don't think anyone's using ruby. reply eikenberry 15 hours agoparentprevYou are not alone. The only future for these sorts of AI coding helpers is for them to use 100% free software AIs. On the bright side good progress is being made in that area and the main sticking point seems to be the expensive hardware to run them on (and integration). Costs on that hardware will hopefully drop over time so they won't still be mostly limited to 1st world (like the subscriptions). reply codr7 1 hour agorootparentIt will fail epically as always with these morons, let's hope some of us still feel like helping out once the coin drops. reply zild3d 5 hours agoparentprev> I fell in love with coding because I could take my dad's old Thinkpad, install Linux for free fire up Emacs and start hacking without an internet connection. I'm not understanding what it is about a private company launching a product that changes that? reply handfuloflight 49 minutes agorootparentIt changes that others aren't going to be learning coding in the same, purist way. reply tonyhart7 5 hours agoparentprev>>\"coding costs a subscription.\" You are free to coding without spend a dime, these AI dev tool cost money because these LLM cost money to run You can get the same experience with open source tools that you can run your own model on your pc reply segmondy 15 hours agoparentprevNo one is forcing you to subscribe. You can code the old fashion way, if you wish to use AI, you can run your own local model. reply twasold 3 hours agoparentprevVs code is free. And copilot has a free tier. reply einrealist 7 hours agoparentprevAnd a world, where you cannot be sure who has access to your source code (or even to your systems). reply wruza 15 hours agoparentprevJust get a graphics card and run a prompt-compatible llm yourself. Recent models like phi-4 show decent results (relative to your general amazement baseline) even on medium quantization. I’m running q4_k_m (8gb) with custom “just print and stfu” characters and rarely reach Claude anymore. reply barrenko 6 hours agoparentprevIt will be free soon enough. reply cynicalsecurity 8 hours agoparentprevIt's much worse, I doubt they created and published this IDE for profit, they want people's data. reply maxehmookau 8 hours agoparentprevI think that world exists already. I've been paying for JetBrains licences for years because their value is greater than their cost. You can do it without IDEs, nothing is stopping you. I don't think this is a new phenomenon though. reply nicman23 11 hours agoparentprevAnd you still can. What are you on about. reply Barrin92 15 hours agoparentprev>but I hate that we're moving to a world where coding costs a subscription I mean you don't need to if you don't want to. I am gainfully employed as a software developer and what I do everyday is literally just fire up Emacs on my Linux machine and write code. To this day I haven't figured out what llms are supposed to do that a bunch of yasnippets don't. Just like five years ago most of my day is reading and debugging code, I'm not limited by how fast I can type. reply handfuloflight 14 hours agorootparentYou're definitely limited by how fast you can read and understand. reply svantana 11 hours agorootparentTrue, but the jury is still out whether text generators help out with either of those. reply handfuloflight 9 hours agorootparentThe jury which only has you as a member, obviously. reply Gormo 5 hours agorootparentprevIs there some alternative, though? Using an LLM might speed up writing code, but obviously can't speed up reading and interpreting it. reply thomasfromcdnjs 15 hours agoparentprevI've been coding for around 25 years, I have 3 or so subscriptions to different AI products that I use for coding. It is kind of terrifying that I probably would stop coding for the day if those subscriptions end. (I get far too much convenience out of them) I have tried to rationalize it by the fact that I do pay for internet, and version control, and my peripherals etc reply muixoozie 15 hours agorootparentYou should check out openrouter if you haven't already. reply ioulaum 11 hours agorootparentIt's been a little janky for me usually get better results from directly using a provider's API. reply ioulaum 11 hours agoparentprev$20/mo isn't a lot... Especially if you make money with coding. reply guappa 10 hours agorootparentIf you pay out of pocket it means it's not an approved tool by your company, which means you can be fired and sued for leaking their intellectual property. Also 20$ per month is way less than what it costs them to run it. Eventually they will need to charge way more to cover their costs, and the people who can't code without an AI assistant will need to pony up :) reply ioulaum 5 hours agorootparentThe coding IDEs at $20/mo do not result in your IP being shared with the AI provider for training. That only applies to regular ChatGPT use. And developers actively using AI for coding can easily spend more than $20/mo for the API. There are people spending $10-15/day in OpenAI API usage working through Cline. reply kstrauser 5 hours agorootparent> The coding IDEs at $20/mo do not result in your IP being shared with the AI provider for training. If you’re not running the model locally, you’re sending your code to them for analysis. Now ByteDance has it. reply przmk 9 hours agorootparentprevIt is a lot outside of the US. Even with an ok developer salary in Belgium, I'd have to really have a use for something to pay $20/mo. reply ioulaum 5 hours agorootparentIf it improves your performance by 1%, then for a salary of $2000/mo, that $20/mo is breakeven. If it benefits you more than 1%, then you're in profit. Of course, if you're in a job that doesn't actually care about performance, and performing won't lead to better salary at some point, then it may not matter. reply poki89 4 minutes agoprevIt looks like a lot of Chinese companies have started open sourcing. reply verdverm 16 hours agoprevTrae and Cursor have both certainly embraced the Kelsey Hightower take on \"no code\" platforms https://github.com/kelseyhightower/nocode https://github.com/getcursor/cursor https://github.com/Trae-AI/Trae reply timdorr 16 hours agoparentEven better is nothing at all: https://github.com/timdorr/ Not even a commit: https://github.com/timdorr/-/commit/9e5a571abd3fc4f8714e8c40... reply verdverm 15 hours agorootparentIs that not a commit hash in the link? This is what a commit-less repo looks like on GH: https://github.com/verdverm/_ Unfortunately they don't show the description \"starting repository for any language and project\" (a play on the _ (any) token in CUE) reply doix 10 hours agoprevI see many comments about not using this because China/ByteDance stealing data, etc. I get it for for enterprise, you don't want your code leaking to the outside world, even if in my experience, the code is mostly garbage and not worth that much. But if it's for some personal project that you're putting on Github, does it matter? If my code is going on Github anyway, it's going to get slurped up regardless. I don't particularly care if it gets processed by Cursor or ByteDance before it gets scraped. reply diggan 4 hours agoparent> But if it's for some personal project that you're putting on Github, does it matter? I don't think the fear is that they'll steal code you'll end putting publicly on GitHub, but everything else. I guess there is some fear that it won't just analyze and process what you currently have open, but might scrape your computer for more data and so on. I personally don't believe ByteDance would be stupid enough to even attempt exfiltrating files from developers machines, which typically are better protected than the average user computer, just trying to see the perspective of others with a more charitable reading :) reply throwaind29k 3 hours agorootparentYou don't trust the OS do that kind of safeguarding? reply ryandrake 2 hours agorootparentDesktop OS's typically don't protect one application's data from another, or the system's data from an application. At least not out of the box without configuring some kind of sandboxing solution. The user-based permissions model is an outdated dinosaur from a time when we could trust the applications we run on our systems to act on our behalf. Applications now act on the developer's behalf, often against the user. An application \"running as me\" should not have access to every resource (file or peripheral) on the device that I have access to. That's a huge blast radius. Operating Systems really need to start treating developers as adversarial from a security/permissions point of view. reply littlestymaar 10 hours agoparentprevI'm glad people care about their personal data, I just wished they cared as much when talking about ChatGPT, because in reality if you're not a Chinese dissident living abroad, then OpenAI is much more dangerous to you than the CCP. reply knowitnone 54 minutes agorootparentOpenAI does not have a history of disappearing people and when they return, their attitude has completely changed (see Jack Ma). OpenAI does not have a history of aggression against other nations shipping vessels. OpenAI does not have a history of corporate espionage and IP theft. OpenAI does not have a history of setting up police stations in other nations. OpenAI does not have a history of operating an illegal bio-lab in foreign nations. That's pretty defamatory to compare OpenAI to the CCP. OpenAI does not have a history of spreading a virus that killed millions and pointing fingers at other nations. reply jay-barronville 6 hours agorootparentprev> […] in reality if you're not a Chinese dissident living abroad, then OpenAI is much more dangerous to you than the CCP. Correct. I laugh a little every time I hear some folks immediately push the “Big Bad China is just trying to steal our data” narrative. My immediate reaction is, “Grab some tea and let’s sit down real quick, so I can tell you what some of our companies right here in America and our government do with our data.” reply kstrauser 5 hours agorootparentIt’s legal to be opposed to both, you know. At least OpenAI is in my legal jurisdiction. reply descendant 3 hours agorootparentAgreed. I get that both might have the same level of access to my data but if I was staying in USA, I would be much more okay with my data getting in the hands of an organization which is within the jurisdiction of the democratic country I live in. Also, given the global political scenario, China is considered a competitor/enemy due to opposite ideologies (not my personal opinion) so the choice is obvious. However, whether USA is/will stay a democracy for long is a longer debate. reply knowitnone 1 hour agoparentprevso all your code goes on Github because mine certainly does not reply jofzar 16 hours agoprevWho would ever allow this in their organisation after the tiktok shutdown, just go for one of the other 17 ai IDE forks reply diggan 4 hours agoparentWho cares what political stunts the US is currently going through? Except for the ~4% of the world who happens to live there, the rest of us tend to choose tools based on either what others around us use, or what we've found to work best for us. If that happens to be an editor from China instead of US, I don't know what difference that would make? Both governments in those countries are crazy about spying on both their own citizens and everyone else, and have their corporations under surveillance. As long as the editor doesn't send opened files/files from my drive to some remote backend, I couldn't care less about the nationality of the developers. reply knowitnone 47 minutes agorootparentthen you should install everything that comes out of China, Russia, North Korea, Iran reply echoangle 3 hours agorootparentprev> As long as the editor doesn't send opened files/files from my drive to some remote backend, I couldn't care less about the nationality of the developers. Is this sarcasm? Isn’t that exactly what the editor has to do to do its job? How else would the AI stuff work? It’s not running on-device, right? reply nurumaik 11 hours agoparentprevChina itself? Russia that has been blocked from all western services? Any non-US/western company? reply wood_spirit 13 hours agoparentprevThis is being talked about in real life as we post on HN: the new administration has a different opinion on this https://www.bbc.com/news/live/cn0y51z7wedt (I am guessing that there will be a deal floated for Elon to buy TikTok in a few weeks) reply ioulaum 11 hours agorootparentIt's doubtful if he can pony up another $50 billion. Although, if TikTok's earnings can cover the payments on the loan, it could be possible. reply herbst 9 hours agoparentprevIts not shut down, your president made it available again without a single change. And it was only a geoblock to begin with. By that logic it can't be that bad? reply smrtinsert 15 hours agoparentprevThis is the real issue. reply cedws 15 hours agoparentprevTikTok was shut down because of clueless bureaucrats and political games, not because of any specific threat. If there was a real risk posed by China why are they idly allowing millions of people to migrate to Xiaohongshu, which is even more strongly controlled by China? reply Aloisius 15 hours agorootparentWhat bureaucrats? It was banned because of a law passed by Congress. Representatives aren't bureaucrats. reply ioulaum 11 hours agorootparentIt was always BS though. AOC did an interview where she explains (what should be obvious to anyone), that US Gov had no proof whatsoever beyond trying to point at that Chinese law (much like Section 702 in the US), that makes TikTok seem risky. Except that TikTok isn't legally a Chinese company, and isn't really under their jurisdiction. Chinese companies like to be under the control of the Chinese government no more than big American companies like to be overly exposed to the US... Thus Apple being based in Ireland and keeping most of their profit outside of the US. reply nikkwong 10 hours agorootparentAbsolutely and verifiably false, again! There are numerous data points that prove that the CCP exploits the TikTok algorithm to influence western views towards a pro-eastern bend. See my comment history for references. The close-doored national intelligence briefing that the senate got about the way the CCP manipulates the algorithm and on how TikTok is a national security threat resulted in a 100-0 vote in favor of banning TikTok. When do you get 100% of the senate to agree upon anything, ever? TikTok was a threat, and will remain a threat until it’s divested from or shut down. reply guappa 10 hours agorootparentAnd the USA wants only fb and google to be able to tell them what propaganda to push. It's all a propaganda war, it's not about propaganda-vs-real news. reply ioulaum 5 hours agorootparentThere's also the Section 702 angle... US can get all US social media to spy on non-US citizens without anything in the way of legal pushback. US citizens have rights. Non-US citizens, do not. I think the problem with TikTok under their audited, non-interference under their Project Texas is... That they might not easily hand over data, or suppress views that US Gov isn't a fan of. Which will change once TikTok is owned by a US company. reply gunian 10 hours agorootparentprevParable of the Talents seem very appropriate the fourth reich has created lil fiefdoms no difference whatsoever to each side reply ioulaum 5 hours agorootparentThe position of governments in society, and thus the kinds of solutions they are biased towards, tend to be similar. It's funny seeing things like US Gov trying to control free speech, while being restricted by the Constitution. They do still manage via back-channels. reply ioulaum 6 hours agorootparentprevThey have all the US data on Oracle servers, with Oracle having the responsibility to monitor all access and make sure that no Chinese entity can get access. All algorithms are monitored by Oracle such that only user actions can determine what they see. So, unless you're claiming that \"Oracle\" is actively colluding with China, you're letting your worldview be defined by government propaganda. The Pro-Palestine thing that TikTok refused to suppress at US Gov's command, is not \"China promoting Eastern causes\", it's \"Americans\" taking a position that the government wants suppressed. Thus Biden being forced to cave and at least pretend that he cared about Palestinians towards the end of his Presidency... Which he stopped caring about once Democrats had lost and it no longer mattered what Democrat voters think for the time being. reply Prbeek 9 hours agorootparentprevYou can call them Zionist financed representatives reply yard2010 6 hours agorootparentHahaha how do you like the weather they're controlling? reply meowface 15 hours agorootparentprevThey'll probably (justifiably) try to ban Xiaohongshu, too, if it gets anywhere near TikTok's reach in the US market. reply thekevan 12 hours agoprevThere's no way I am using a Chinese IDE with VSCode (w/ Coppilot) and Cursor existing after reading the privacy issues in the court documents with TikTok. Bytedance had some extremely overreaching expectations of data it wanted. That being said, I am using (Chinese) Deepseek r1 because there isn't currently a free LLM on par with it. I am careful with what I share though, a little more so than with any others that are not locally fun. reply guappa 10 hours agoparentTo me USA and China are both foreign nations, and USA is the one who is far more aggressive in terms of army and spies. So as a non USA citizen, I'm way more afraid of USA than I am of China. reply animuchan 8 hours agorootparentI'm thinking about this the same way, if a bit more selfishly: as a non-US citizen, I might want to move to the US in the future. Which is why I would like the US govt to have as little data as possible about me (who knows what the mainstream politics will look like, things that are very innocent today might be punishable by death in 10 years). Chinese govt, on the other hand, can have my data freely, since I don't think I'll ever move to the Mainland China. reply latexr 8 hours agorootparent> I might want to move to the US in the future. > (…) (who knows what the mainstream politics will look like, things that are very innocent today might be punishable by death in 10 years). Maybe it’s just me, but I don’t think I’d ever consider moving to a country where I believed politics to be so unstable that an action could go from “very innocent” to “punishable by death” in the span of a decade. reply echoangle 3 hours agorootparentI couldn’t name a single country on earth where I would be certain that this could not happen. 10 years is a long time, a lot can happen politically. reply latexr 1 hour agorootparentWe’re talking about going from fully innocent and legal action to full on death penalty in just over two terms (depending on country). That seems quite a stretch for many countries where the death penalty was abolished. https://en.wikipedia.org/wiki/Capital_punishment_by_country It doesn’t seem plausible that in countries where even euthanasia isn’t allowed things could get so out of hand that the death penalty would not only be reimplemented but it would devolve so fast that you’d start executing people for things which aren’t even crimes right now. reply CryptoBanker 20 minutes agorootparentYou are literally debating something that hasn't happened yet. The death penalty example was just that, an example reply animuchan 7 hours agorootparentprevYou're right of course. Unless my current place turns into a (more) active warzone, I won't consider relocating. (I also understand that my outsider view on the US political system isn't well-informed. Still, better safe than sorry.) reply cma 2 hours agorootparentprevLook at what almost just happened in South Korea, it can be very sudden and out of character. reply yard2010 7 hours agorootparentprevIn 1-2 decades Mainland China will move to you ;) reply redcobra762 8 hours agorootparentprevThen you know nothing of how China operates its military or its intelligence gathering apparatus. Ask any of the parts of the world that disagree with China about their sovereignty… reply guappa 7 hours agorootparentI don't see tanks in taiwan… What I see is this: https://en.wikipedia.org/wiki/List_of_countries_with_oversea... As I've already said, I never claimed china are the good guys. Just that they aren't the worse guys. reply redcobra762 5 hours agorootparent...you realize the US is invited to be in those countries, right? And why would you see enemy tanks on an island? What you do see are many many Chinese ships and planes, constantly violating Taiwanese air and sea boundaries. reply computerthings 8 hours agorootparentprevThat something else is even more toxic doesn't make poison not poison. It's not a choice between what poison you pick, but between accepting or rejecting poison. reply TeMPOraL 8 hours agorootparentA poison is not going to harm you unless it makes no contact with your body; what GP is saying is, regardless of which one is more potent, it's safer to pick the one you're least likely to ever find yourself near. reply computerthings 6 hours agorootparentIt's safer to not ingest any poison. You don't extinguish fire with gasoline because kerosene might be worse. You don't eat rat poison because fentanyl would be worse. It's a false dichotomy. reply knowitnone 45 minutes agorootparentprevgood for you. don't ever visit the US because they are watching you closely reply roenxi 8 hours agorootparentprevAnd, logically if we're worried about spying it actually makes the most sense for average Chinese people to use US software and vice versa. The biggest threat to a person is always the police & military of their own country. For example, China killed a lot of people. They were all Chinese. reply miningape 9 hours agorootparentprevI think you're unaware of just how aggressive China is. Look at their neighbours and their policies they don't have a single friend because everyone is afraid of their bullying behaviour backed up by their military. Look at conquered regions like Tibet and Xinjiang. Look at how they set up police forces in foreign countries to keep an eye on Chinese citizens living abroad. Even having kidnapped and illegally held Chinese citizens in England because they posted anti-CCP messages on WeChat. Look at countries like the Phillipines (not even a direct neighbour) who are trying to hold on to small fishing islands just off their coast because the CCP claimed those islands in the 1970s. Remember a few years ago when they ran a week long military exercise around Taiwan... because a US representative spoke to the Taiwanese president. Sure I agree with you that the US also overstepped a line here, but for your response to be shelling the waters around the island is excessive to say the least. Look at the aggressive nationalist and imperialist news they feed their own population, and the propaganda spread to make the Japanese seem like demons. Did you know there are several theme parks in China where children are encouraged to Bayonette a mannequin of Japanese Imperialist soldiers? Call the US as bad as you like, but I've never seen a theme park where children are actively taught how to kill and demonise the Taliban or Nazis. reply spencerflem 9 hours agorootparentYou could say all of that about the American empire. We renamed it to the Gulf of America, talk about annexing Canada, own Hawaii, fund the colony of Israel, invented Manifest Destony, destabilize democracy via the CIA, firebombed Vietnam, etc etc etc And don't talk to me abt Chinese spying when the NSA and Five Eyes hoovers up and processes the entire internet. Re: propaganda, read your own comment. The American propaganda machine got you to demonize China pretty well And to top it all off, despite having a much smaller population AMERICA HAS MORE PRISONERS THAN CHINA. Its insane! reply miningape 9 hours agorootparentIn general I do agree (except for the recent Canada stuff I believe it's more of a bluff than anything else) But I want to respond to this: > And to top it all off, despite having a much smaller population AMERICA HAS MORE PRISONERS THAN CHINA. Its insane 1. The US publically acknowledges who is in prison, China doesn't. You can be disappeared in China without anyone knowing. 2. The Chinese operate several Black prisons both within and without their borders. These are Prisons without actual sentences, laws or rights. The public doesn't even know of their existence. This is where you land up as a political prisoner in China. 3. The Chinese have placed thousands of Uigur muslims in \"re-education camps\" (not prisons) many of whom are only guilty by association (i.e. there's no direct crime the CCP arrested them for, other than being a blood relative of someone who did commit one of these \"crimes\") 4. Chinese police are already spread thin enough dealing with all the political prisoners meaning there are many dangerous criminals who are freely committing crimes in China. The CCP keeps this information under tight wraps so as not to cause a panic. As a result the Chinese are a lot less cautious than they should be so (unofficial) crime levels are much higher than they should be. It's not uncommon to hear of (or see) Children who were kidnapped and had their limbs chopped off so they could beg more effectively there has never been a widespread crackdown on this behaviour and many perpetrators are still walking the streets forcing children to beg for them. 5. Not to even talk about the prison organ harvesting claims made by several groups who have been persecuted by the CCP. From Falun Gong practitioners to Uigur muslims. Also don't be lazy and just call this propaganda I could easily do the same with other messages in this thread (\"CCP propaganda\") meaning it's not a strong argument to anyone on the fence / on the other side. Yes, it is very anti-China, but that's because I'm trying to bring it in contrast to the very anti-US message. I don't think either country is perfect and in fact I prefer to not live in (or near) either. reply guappa 9 hours agorootparentnext [2 more] [flagged] miningape 8 hours agorootparent1 & 2: Wikipedia keeps an active list. Currently there are 15 held in Guantanamo bay. Could you provide me with a similar list of prisons + prisoners from China? (no because they intentionally keep it secret). I think the fact that we can name and put pressure on the US about this ~prison~ torture-camp makes enough of the argument for me. https://en.wikipedia.org/wiki/List_of_current_detainees_at_G... 3. Yeah it's sad. You can find a list of police files which have been confirmed by 3rd parties. Based on 2018 data we have >5,000 in detention currently. https://www.icij.org/investigations/china-cables/xinjiang-po... 4. No, and trying to use official sources unfortunately gives you an overly positive impression on China. This is one of the reasons it's so dangerous for an average citizen to live under a system which hides information you are left without knowledge of how much danger you're in. If you think you're safe you're more likely to be taken advantage of. There was an interesting case recently where CCP propaganda showed that other countries feared the Chinese passport (trying to boost nationalism and pride in China). This has led to several Chinese citizens getting robbed abroad, they then hold their passport in the attackers face thinking it will scare them off. Only for the passport to also get stolen. 6. Absolutely, the direct danger from the US isn't from the US itself but the governments it backs up (indirect danger?). Whether it's because the government is incompetent or evil, history shows that whatever the US props up is likely to collapse as soon as they leave but the country is in a much worse position than if the US never involved itself. reply redcobra762 8 hours agorootparentprevThe US is largely talk, vs. the substantial action China takes and has taken against territories it considers its own. The US importantly has a robust and independent judiciary where a fair trial is infinitely more likely than in China. Even for foreigners, the US legal system is accessible, understandable, and weildable for protection. It’s far from perfect, but 100x what you’d get in China. reply lossolo 9 minutes agorootparentWas Iraq also talk when US invaded it in violation of international law? Not to mention overthrowing democratically elected governments in South America. Which countries did China invade in the last 3 decades? reply guappa 7 hours agorootparentprev> US importantly has a robust and independent judiciary where a fair trial is infinitely more likely than in China Except if you have an abortion. Then laws don't matter. reply descendant 3 hours agorootparentI would like to highlight the small bit of positive in the American system. I don't support the removal of abortion law, nor do I like mixing religion with politics. But, due to the system that exists in the USA, even though the government removed the right to abortion, the individual states can still support it and form their own laws around it. However, a similar situation in other countries would result in complete removal without any option. reply spencerflem 2 hours agorootparentGive it like a year, I promise you it will be banned federally reply redcobra762 5 hours agorootparentprev...this makes zero sense. reply yard2010 6 hours agorootparentprevWhy don't you ask the kids at Tiananmen Square? / Was fashion the reason why they were there? reply guappa 6 hours agorootparentnext [7 more] [flagged] kstrauser 5 hours agorootparentHow many social credits are you getting for these posts? You went from “eh, China isn’t so bad” to “Tiananmen Square protestors had it coming” within one thread. reply guappa 2 hours agorootparentI could ask you how much you get paid to write comments supporting the genocide of palestinians… You people with no morals try really really hard to twist words uh? But at the end of the day you're just a sad racist. reply kstrauser 2 hours agorootparentLink or didn’t happen, comrade. I’ll save you time: you won’t find one. reply spencerflem 2 hours agorootparentprevThey got u there tho, I'm with you that there's a ton of China hate deliberately stoked by US Govt and media, mostly as a distraction from out problems at home. But there's nothing to defend about Tiananmen Square reply spencerflem 4 hours agorootparentprevYeah , that's wild. The proper move would be to show the very similar US response to Students Protests https://en.m.wikipedia.org/wiki/Kent_State_shootings They sent in the goons to beat up and arrest students protesting genocide or climate change too. Hasn't gotten to tanks yet but truthfully it doesn't need to because we both know those protests aren't doing anything. If there was a credible student uprising it would happen here. reply guappa 2 hours agorootparentThanks, I only knew about the USA bombing workers on strike. reply animuchan 8 hours agorootparentprevTaliban is a terrorist group, painting them in a bad light is factual. Nazis, also factual. So I don't think I fully understand the last sentence. Do you mean it's not appropriate for specifically _minors_ to have a theme park about how Nazis are bad guys? Because there's a lot of western content to this effect, including movies and video games, which are also accessible to children. If anything, Chinese content guidelines usually prohibit graphical display of violence, so it's much more of a milquetoast thing than e.g. South Park. reply miningape 8 hours agorootparentThis is fair and I think reflects my personal opinions on keeping children out of politics. My point is more to do with seeing little kids being dressed up in CCP uniforms, handed a bayonette, and told to charge at the Japanese. I also agree we see this around the world \"including movies and video games, which are also accessible to children\" but I draw the line where it's being encouraged by the government to do these actions in person. Combine this with the anti-japanese rhetoric taught in primary education and it's a nasty combination*. [1] Another thing to consider is that children are generally not given access to games/movies/whatever that have such mature themes by their parents. I'd make this same argument if I saw US children being dressed up as US soldiers and asked to charge at Nazi \"soldiers\". Even though we can both (hopefully) agree that Nazi ideologies are/were disgusting and deserve to be bayonetted. > If anything, Chinese content guidelines usually prohibit graphical display of violence, so it's much more of a milquetoast thing than e.g. South Park. Not when the Japanese or another of China's enemies are involved. Then it's gloves off. * Also I do want to give some slack here too, the Japanese have never acknowledged or apologised for their attrocities in WW2. [1] https://asiatimes.com/2024/07/china-scrambling-to-unplug-ant... reply spencerflem 7 hours agorootparentThe house overwhelmingly passed https://www.congress.gov/118/bills/hr5349/BILLS-118hr5349rfs... , requiring all high schoolers \"(A) learn that communism has led to the deaths of over 100,000,000 victims worldwide; (B) understand the dangers of communism and similar political ideologies; and (C) understand that 1,500,000,000 people still suffer under communism.\" Including an oral history \"Portraits in Patriotism\" If this isn't propaganda against the CCP I don't know what is. Call it truthful or not, kids are being told China is The Enemy. They're dressed up in military uniforms as early as middle school if you're in JROTC. And funny you bring up them suppressing hate speech when here in America we've recently decided to do the opposite with X and Facebook. reply Iny0ka 7 hours agorootparentprevFact check: China has strong ties with Central Asian states, Russia, NK, Myanmar, Laos, Cambodia, Pakistan. Regional tensions mainly involve US-allied nations. reply guappa 9 hours agorootparentprevCan you compare the military spending of one vs the other and report back please? I never said china has never done anything bad. There is no such country on the planet. China doing bad stuff doesn't make the bad stuff the other countries do disappear, and my point remains valid on which one is more of a threat to nationals of other countries. reply miningape 9 hours agorootparentI'm not making the claim that the US is innocent because China did all this stuff. I'm making the claim that I would feel much safer living on the Mexican-American, Canadian-American border or even in Panama, than I would near any of China's current conflicts. And that China is just as aggressive (if not more so) with their neighbours. At least if the Americans win I don't have to be worried about becoming a political prisoner, forced into re-education camps, or having my organs harvested. Also there are many more conflicts currently where China is involved than the US. reply guappa 9 hours agorootparentI'm sure Canadians are feeling real safe with the threats of imminent invasion. I live in scandinavia and I can tell you people aren't feeling particularly safe about USA not going to invade. (I don't think they'll do it for real… but then again I also didn't think Putin would go into Ukraine for real so, don't take my work as gospel) reply sausagefeet 10 hours agorootparentprevIf you woke up tomorrow in either USA or China, which would you prefer to wake up in and why? reply bromuro 8 hours agorootparentLiving in these countries sounds to me like a nightmare (and I am from a 3rd world country). I’d prefer China because the ancient culture, history and thriving economy. reply rthrfrd 8 hours agorootparent3rd world country or not I hope you appreciate that you are incredibly fortunate for \"ancient culture\" and \"history\" to be 2 of your top 3 considerations for where to live and the quality of life that would give you. Perhaps revalidate the \"thriving economy\" assertion before taking the plunge though (and be mindful of how hyper-localised that is). reply guappa 9 hours agorootparentprevIf you want to pay me a trip, I'd gladly go to both, and leave my regular electronic devices at home while crossing both borders :D reply grardb 10 hours agorootparentprevI feel the need to point out that while many people might understand the point you're trying to make, the way this question is phrased doesn't do the best job of conveying it. If I was presented with two options: waking up tomorrow as the child of a poor farmer in a third-world country, or waking up as one of Donald Trump's children, I would definitely choose the latter. However, that doesn't mean that I trust Trump more than I would trust the farmer. In other words, quality of life (or a preferred way of living) are not inherently tied to trust, morality, or anything like that. reply rthrfrd 8 hours agorootparentWhy did you choose to wake up as the child of a peasant in one country but the child of the president in another? Quality of life is inherently tied to trust and morality both in terms of the effect of fear (a lack of trust) or isolation (a lack of moral consensus, or equality and sense of shared belonging). reply roca 7 hours agorootparentprevThat makes you an outlier. Despite everything, millions of people are still desperate to get into the USA. China, not so much. reply piker 10 hours agorootparentprevThis is probably true, but at least in principle one of those foreign nations stands for free speech and democracy and the other stands for censorship and authoritarianism. [Edit: hey Europeans commenting and downvoting below, note the words \"in principle\" in the above comment and evaluate which of the two countries do or do not purport to stand for these things despite whatever your hot take may be on the current moment.] reply herbst 9 hours agorootparentThat's 2 words non US people usually don't think when they think US. Democracy is literally the last thing that would come to my mind right now. Edit:// \"In principle\" chinas gov stands for stability, economic development and national unity... reply looofooo0 9 hours agorootparentLol, you can shitpost anything about any politician in the USA. Hang any flag on your car, house etc. and you will be fine. Try the same in China. reply guappa 9 hours agorootparentYou can shitpost but the algorithm will make sure it doesn't get read :) reply herbst 9 hours agorootparentprevHuh? Does that negate the missing democracy, the political distrust and the political censorship within your country? I am happy for you and your flags. However I didn't claim that china is more \"free\" just that the US definitely isn't seen as that from the outside anymore. It's not always a direct competition reply piker 7 hours agorootparentprev> Edit:// \"In principle\" chinas gov stands for stability, economic development and national unity... As did Stalin, Mussolini, Hirohito and the National Socialist Party. It's the \"how\" that matters. reply guappa 6 hours agorootparentNo, Mussolini openly despised democracy. Please don't be teaching history without having spent even 5 minutes learning it :) reply piker 3 hours agorootparentPerhaps you might spend 5 minutes reading the quoted comment again and note the absence of \"democracy\". reply yard2010 6 hours agorootparentprevExactly. What's the big idea to be strong and stable when you have no respect whatsoever for human lives? You can just kill the competition! Sacha Baron Cohen's The Dictator is a piece of art that demonstrates this absurdity in the best possible way. reply easyThrowaway 9 hours agorootparentprevGiven the last few days, As a EU citizen I'm not so sure such distinction is fully accurate anymore. And I'm pretty sure in 2 to 3 years from today the distinction will be even less noticeable. I wouldn't trust a US data company to not capitulate to... \"personal requests\" by Musk or Trump any more than I trust it not happening in an hungarian, russian, turkish or chinese one with their respective leadership (official or otherwise). reply littlestymaar 10 hours agorootparentprev> at least in principle one of those foreign nations stands for free speech and democracy Only of of them is hypocritical, that's it. reply guappa 10 hours agorootparentprevYou mean the one where the nazi is in charge? The one that bankrolled all the right wing terrorism and mass killing of farmers in my country? Is that the one I should be trusting more? reply chucklenorris 10 hours agorootparentprevThat's actually funny.. considering that the main reason TikTok ban was proposed was the lack of censorship of sensitive topics like a certain genocide reply riskyingo 11 hours agoparentprevI would rather let Chinese companies take my data than US/EU ones where I have bank accounts. Unless you are a super important, influential person, you shouldn't worry about China. reply littlestymaar 10 hours agorootparentYou forgot people of Chinese origin which the CCP monitors even abroad (scary stuff really) but yes. reply nikkwong 10 hours agorootparentprevYeah, you better hope the code that you’re writing is not important at all. reply guappa 10 hours agorootparentprevIt's just US, there's no EU getting your data. reply henearkr 10 hours agorootparentThat can be read in a funny way, which you intended maybe? \"It's just US, not EU.\" reply guappa 9 hours agorootparentBut I'm not american, so I'm not included in US :D reply henearkr 7 hours agorootparentBut then what did EU mean? ;) reply ryang2718 11 hours agoparentprevHow have you found R1? I've been meaning to try it with Aider's Architect mode. Have you tried the 7b? reply chvid 9 hours agoparentprevAs far as I can tell this is operated out of Singapore, which is not some communist backwater, according to the law there and on top of US big tech infrastructure. The models this editor work with are by Anthropic and OpenAI both US companies. reply truekonrads 8 hours agorootparentSingapore or not, these TOS grant you nada in terms of privacy. You'd be a fool to use this: We may use Your Content to provide the Services to you and to other users, including without limitation troubleshooting, diagnostics, security and safety reviews, and customer support requests. You hereby grant to us, our affiliates and our third party partners (“SPRING Parties”) an unconditional, irrevocable, non-exclusive, royalty-free, sublicensable, transferable, perpetual and worldwide license, to reproduce, use, and modify Your Content in connection with the provision and improvement of the Services and its underlying technologies, as well as for the SPRING Parties' respective business operations, in each case, to the extent permitted by applicable laws. reply blackeyeblitzar 2 hours agorootparentprevOne nuance to consider regarding Bytedance: TikTok’s CEO is also based in Singapore, but is a former Xiaomi executive who worked in Beijing for years. The firm’s employees in places like Singapore could be coerced by the government of China because of their friends, family, assets there. Employees of Bytedance (and other Chinese companies) have to deal with draconian employee rules and agreements that tie them back to the rules of their Chinese mainland parent corporation and the Chinese government itself. For example look at the details that came out in this lawsuit against TikTok, where employees have to agree to uphold Chinese national interests, uphold socialism, etc. https://dailycaller.com/2025/01/14/tiktok-forced-staff-oaths... reply samy_aik 11 hours agoparentprevI think you are contradicting yourself! Also, why add \"China\" to a product when you don't attach \"America\" to the other. If you are thinking politics, fine, else we are in for tech advancement! reply ioulaum 11 hours agoparentprevI doubt that this sends back tons of data. Early TikTok was basically a standard Chinese startup in terms of how they collected data to optimize the app... It's not because they cared about what \"you\" are doing. The Chinese tech ecosystem is just more competitive than the US one. Most social media companies have been careless with data until they got public backlash though... TikTok's systems are probably more secure today than anyone else's... Because of the backlash. reply nikkwong 10 hours agorootparentAbsolutely and verifiably false. WeChat became so important to the CCPs surveillance interests that it was essentially nationalized; and other apps that grow to its scale basically suffer the same fate. The only reason that hasn’t happened overtly to TikTok is because it’s an international product, so instead the nationalization happens discretely. reply ioulaum 5 hours agorootparentIt is verifiably not controlled by the CCP... Refer Project Texas... And all US data already being on Oracle's cloud so that all access can be monitored (same for auditing of all their algorithms for any kind of programmed bias). WeChat is an app that works inside China, so obviously it's subject to Chinese laws. All countries have some restrictions on what is and is not okay to have on online platforms. China just adds in \"social disruption\" alongside content that could promote violence (which would be blocked on Facebook, etc. also) You need to not just drink in propaganda... When something just \"can't be done\" (because it is being monitored), then it just doesn't matter even if you have ill intent. reply gunian 10 hours agorootparentprevYou don't hear about the other side because people like me usually get murdered in cold blood there is no difference but for you to believe that is a singularity like me being treated like a human :) reply aredox 7 hours agorootparentprevByteDance is ready to close TikTok rather than divest and earn billions of dollars. Does that sound like a business or an intelligence front? reply ioulaum 5 hours agorootparentNo one with even a pretension to dignity would just cave in the face of force. Although, they do seem willing to sell potentially. China restricted them from selling their recommendation algorithm, and it's hard to sell TikTok without. But, TikTok did immediately start work on an independent algorithm that has no ties to China, so that they can sell it if it really comes down to it. reply entropyneur 10 hours agoprevIt's hard enough to avoid Chinese electronics, I'm not about to start using Chinese software. reply jbverschoor 10 hours agoparentYou should be running any dev stuff in a VM / Container anyway because of all the supply chain attacks / typo malware reply DoctorOW 16 hours agoprevI feel like this would've been better as a VSCode extension. Copilot, Q, Gemini, all were able to take this approach. Also, VSCode isn't considered a full IDE and adding some AI features isn't enough to change that. It seems like they forked VS Code just for the ability to say they \"created an IDE\" in the same way other projects fork Chromium to \"build a browser\". reply ed 15 hours agoparentNah the extension API is pretty limited. Copilot uses proprietary API’s not available to extensions. If you really want an integrated experience, and not just a sidebar UI, you need to go the same route as Cursor and fork Code-OSS (the MIT-licensed part of VS Code, analogous to Chromium for Chrome) reply ThePyCoder 13 minutes agorootparentWhat about Continue? It's an open source, bring your own api AI integration for vscode. It does everything that copilot does, including the editing-your-code-in-front-of-you diff style editor. I don't think it has any special api access? reply tushgaurav 12 hours agoparentprevNo, I don't think that's possible, VSCode's extension API is pretty limited. This is the reason that we have cursor as a separate application. reply antoniuschan99 15 hours agoparentprevBytedance is signalling it wants to compete against vscode for ide marketshare. Wondering if it is a fork of vscode though because right now it will be competing against cursor. reply verdverm 14 hours agorootparentIt looks to be a fork, as other comments and the license list seem to indicate reply giancarlostoro 15 hours agoprevReminds me of JetBrains AI, I think I'll stick to JetBrains solution for the time being. I'm not super crazy on AI coding solutions, but the JetBrains flavor hits a sweet spot for me, built-in and does what I usually need it to do. reply krashidov 11 hours agoparentHas it gotten better? I tried it out early and it was unusable reply joshstrange 5 hours agorootparentSame here, I need to try it again but Copilot+Aider cover my needs right now. I would like LLM completion in my commit messages which Copilot doesn't have but JetBrains AI does IIRC. I really want JetBrains AI to get better, I think they are supposed to be adding Claude support this month? Or already did? reply surfingdino 11 hours agorootparentprevStill unusable. I had to disable it. reply dygd 9 hours agorootparentprevCompared to plugin like Sourcegraph Cody with Claude 3.5 Sonnet, Jetbrain's is still atrocious. reply codr7 1 hour agoprevThe last thing I need, truly. If I ever have a need for buggy crap code, any random boot camp dev can write it for no money at all. reply halamadrid 12 hours agoprevMy personal experience has been that very basic stuff like converting an object to another shape or creating a simple validation function or things like that has been super effective with AI. I've gotten so lazy that I try and prompt it to exactly what I want instead of even editing a bit. But the moment it gets a little less basic, its a different story. I absolutely hate the experience. So I love it and I hate it at the same time. Is it going to replace me? I don't feel like its there yet. And this has been the case for a while now. There is just too much money and people invested in this that its hard to say anything negative about it. Nearly every VC has rebranded their websites around an AI-driven future. And, to be fair, it’s not a fad—it genuinely works well for many things. But for now, I’m still skeptical about how far it can really go. reply ilikegreen 7 hours agoprevVery unrelated, but: I run Linux on an older Thinkpad and I appreciate fast websites. This is a very well-designed website! It loads fast and it scrolls quite quickly. Does anybody feel the same? I feel like nobody touched upon this, but it's always very nice to feel interfaces are responsive. reply matheusd 7 hours agoparentInteresting, I have the exact polar opposite perception. I accessed this through a Qubes AppVM (no GPU, limited memory and CPU budget) and the presence of videos makes this a very slow scrolling experience for me. In general, anything that involves JS/CSS animation/blur/effects makes sites pretty slow (up to unusable for me). The unlogged homepage for github.com for example, spins my cpu at 100%. reply RamblingCTO 11 hours agoprevI've been toying with roo-code (cline fork) and it's fun. Before you give out your data to china maybe consider that or continue. reply mring33621 16 hours agoprevWhat LLM provider(s) is it talking to? reply handfuloflight 15 hours agoparentAccording to the Discord, Claude 3.5 Sonnet and OpenAI GPT-4o. Also according to the Discord, \"SPRING (SG) PTE. LTD. or one of its affiliates\" is fully subsidizing the costs. This doesn't look like a bring your own keys type of deal. reply segmondy 15 hours agorootparentIf it's truly talking to OpenAI or Claude, then the traffic can be intercepted and redirected to your own AI router for other cloud AI providers or your local Ollama instance. reply handfuloflight 14 hours agorootparentAssuming there's no middleware transformations integral to how the IDE works. reply msoad 15 hours agoprevCan someone please do some security research on this to see what it sends when it calls home? I'm terrified to install a binary like that reply CitrusFruits 12 hours agoparentIf you want the same thing but not as sketchy you can check out Cursor. You pay them, so it's a little more transparent what they're getting out of it. reply guappa 10 hours agoparentprevVScode phones home… what's the difference? Use something like emacs/vim/kate/eclipse/qtcreator if you want to avoid \"phone home\" software. reply rubslopes 1 hour agorootparentIf an American company does something bad with my data, it's possible not guaranteed, but possible that it will be punished. If a Chinese company does the same, at most someone in China will shrug. reply blackoil 12 hours agoparentprevUnless you are using self hosted llm, you can assume all the code is being sent to the server. This is true for all similar services. reply mixmastamyk 14 hours agoparentprevThese things will never be completely safe, from version to version, to new management. As Darth Vader one said, \"I am altering the deal.\" reply throwup238 13 hours agorootparentPray I do not AIter it any further. reply gunian 10 hours agorootparentIn this world Darth Vader wins and we get hunted like dogs for thought crime :) reply throwup238 10 hours agorootparentSounds familiar! https://en.m.wikipedia.org/wiki/Metalhead_(Black_Mirror) reply gunian 9 hours agorootparentlol was hoping for ssuicide booths from futurama instead of black mirror :) reply throwup238 9 hours agorootparentJust wait until OpenAi releases their Santa. reply gunian 8 hours agorootparentmeet Strategic Autonomous Noel Tasking Android our fully autonomous robot ngl death by santa sounds a lot better than john doe at Super 8 lol reply ioulaum 11 hours agoprevApparently the target market for this is supposed to be \"Chinese coders living abroad\". Thus it supporting English and Chinese languages. And it may be a competitive step taken after Alibaba launched their new coding assistant. reply nurumaik 11 hours agoparentIsn't any modern llm inherently multilingual? reply ioulaum 5 hours agorootparentLLM aside (which is just GPT-4o and Claude 3.5), I imagine that the GUI for the IDE may have better Chinese support in some way. Although I haven't tried to find a Chinese mode myself. That it's targeted at Chinese abroad is just what I saw from Googling it and reading the SCMP (Singapore) article on it. The company that made it is also from Singapore (even if it's under Bytedance's umbrella). They seem to be separate from TikTok. The only thing they have in the app store for example, is some chatbot application. reply homebrewer 10 hours agorootparentprevNot really, there' a world of difference between how every model I've tried¹ handles English vs my own native language². It's especially noticeable with smaller models, which produce coherent English, but completely break down in other languages: spewing out incorrect grammar, mixing words from other languages where they make zero sense, etc. ¹: which is dozens of them at this point, open and not. ²: with lots of speakers and training data. reply gunian 10 hours agorootparentWould finetuning fix this? If they are statstical parrots it would seem like it wouldn't reply ioulaum 5 hours agorootparentProbably tough to fix small models with fine-tuning... You'd likely need to train them with enough data from languages (and maybe translations between them) that they need to support, so that they can connect ideas in different languages properly. Separately, the fact that they can translate between languages where they have never seen any translation data in their inputs shows that they have internal models of the world and language, that go beyond what one might expect from a statistical parrot. They do have world understanding perhaps limited some by the fact that their input data may not cover a lot of everyday things. reply pogue 9 hours agoprevThis post got political almost instantly. Unfortunate, but expected in today's client, I suppose. Regardless, what is this exactly? reply jm547ster 43 minutes agoparentSeems to have been heavily downvoted also, it's flown off the front page. Times have changes for HN. Also double standard when it comes to the like of Deepseek r1 earlier this week :shrug: reply _ink_ 9 hours agoprevSo how is it different from: https://www.marscode.com/ ? Also, anybody knows a decent IDE where AI is a first class citizen? So far my experience with plugins is, that they have too little context to be actually useful. reply epaga 9 hours agoparentCursor has been absolutely game-changing for me, especially its Composer Agent mode. reply _ink_ 9 hours agorootparentLooks great indeed. Unfortunately without local LLM support it's a nogo for my company. :( reply debugpro 10 hours agoprevI wrote a Tetris game with it, but there’s a bug where the blocks suddenly disappear while falling. I haven’t been able to fix the bug, and I’m not sure if it’s an issue with the IDE or Sonnet. The experience isn’t very good. reply powerapple 9 hours agoprevWhere does it say it is done by ByteDance? I would definitely use it it is really done by ByteDance... Couldn't find any information of ByteDance on the page though. reply kleiba 10 hours agoprevI've lost track: what are the current sota alternatives that I can run locally? What's the preferred IDE that integrates AI nicely? reply thejocker6 8 hours agoparentVSCode with the continue plugin. Might work with VSCodium as well, but haven't tried it yet. reply icar 8 hours agoprevPlease, please, stop the AI IDEs... reply plun9 7 hours agoparentWhy? reply spondyl 15 hours agoprevI know it's a marketing site so the speed is for the sake of the demo but the various animations showing a cursor scrolling past reams of text quicker than any human can scan, and just blindly hitting Accept makes for weird optics. reply userbinator 15 hours agoparentThe message is, unfortunately, clearly quantity-over-quality. reply chvid 9 hours agoprevI took it for a spin and it looks great a great way of playing with a LLM. Sure some of the elements are in IntelliJ, Copilot, or maybe Cursor (which I haven't tried) but it is all very well put together. Recommended. (But of course it would be nice to be able to run it against a local model which doesn't seem possible at the moment.) reply Sharon_Q 9 hours agoprevWhy do Chinese Internet companies tend to monopolize the market with free strategies? reply 1317 4 hours agoprevah, so that's what they were scraping my git host for! reply thomasfromcdnjs 16 hours agoprevAwesome work. Sidenote: That marketing video was nauseating, it was moving too fast and didn't show any features for long enough or for enough steps. reply pasc1878 9 hours agoprevInteresting that the macOS download is Intel and not Apple Silicon reply Aeolun 12 hours agoprevIsn’t this just Cursor but worse? reply tushgaurav 12 hours agoparentIt's surprisingly very good. The AI responses are very good, and it's free to use. They don't charge anything for claude-3.5-sonnet. I also love its UI even though they totally copied JetBrains Fleet's UI. reply gkbrk 11 hours agorootparent> They don't charge anything for claude-3.5-sonnet Makes you think about how they're extracting value from you to make up for this. I rack up non-significant amounts when using Claude 3.5 Sonnet for coding. Considering the data they siphon from their other \"free\" apps, I don't want to think about what this does on developer machines with code bases and production access. reply Alifatisk 8 hours agoprevWhen will it be released to homebrew? reply serverlessmania 3 hours agoprevclaude-sonnet for free! How? reply khantsithu 6 hours agoprevthis is nice, i love chinese free products haha reply noloblo 16 hours agoprevIs it electron or rust? reply rumblefrog 16 hours agoparentIt appears to be a VSCode fork, and supports VSC marketplace extensions, so I would guess Electron reply noloblo 16 hours agorootparentHard to look at slower editors after sublime speed and hyper speed from zed rust reply Alifatisk 8 hours agorootparentYou can't compare Zed with Vscode and it's cousins. I've used Zed myself and it is still very lacking, limited extensions. Zed only has syntax highlighting and an ai assistant, but that's it. Even though Vscode (and the rest) is still also considered a text editor and not a IDE, they have by far way more features. Sure, you get super duper high speed, but at what cost? reply kstrauser 4 hours agorootparentZed has good LSP support for lots of languages. It’s not only syntax highlighting and AI. reply noloblo 4 hours agorootparentprevFair point are there rust or cpp based ides in parity with vs code to compare? reply meowface 15 hours agorootparentprevI've tried Zed and Ghostty and I just don't really get the speed thing. Typing latency is extremely unnoticeable in VS Code/Cursor and other Electron apps for me. reply nurumaik 11 hours agorootparentFor me it's very noticeable. Typing, invoking lsp actions, opening terminal panel, switching between files. Everything feels much much faster. Switched to zed several months ago and never looked back reply noloblo 14 hours agorootparentprevWhat's Ghostty reply meowface 14 hours agorootparentA new terminal emulator popular on HN that advertises its minimal typing latency. reply jmccarthy 16 hours agoparentprev0:01.39 /Applications/Trae.app/Contents/MacOS/Electron reply BonoboIO 9 hours agoprevPair programming with your invisible Chinese coworkers, you don’t even know you have. reply darthrupert 13 hours agoprevIn an optimal world, what llms provide us is what vast libraries should have already 2 decades ago. But I guess instead of writing great libraries to avoid repeating ourselves, we drop all code to a lake monster which eats it and spits out answers. reply meiraleal 7 hours agoprevA developers tool not available for Linux? reply lostmsu 3 hours agoparentFrom the top 4 here https://survey.stackoverflow.co/2024/technology#1-integrated... only 2 are: vs code vs IntelliJ IDEA Notepad++ reply miah_ 3 hours agoprevNo reply echelon 16 hours agoprevChina is taking every venture-funded AI startup and shitting on them. Almost all of them. Tencent's Hunyuan 3D Tripo, etc. Tencent's Hunyuan Video Sora, Runway, Pika, etc. ByteDance's Trae Cursor, etc. DeepSeek R1 GPT, etc. Unitree Figure, Tesla, Boston Dynamics, etc. And a ton of this stuff is completely open source. All of the Tencent stuff is. This destroys the moat of so many companies that have spent hundreds of millions of dollars training and building their tech. It's just out there for free for anyone to build with. And has anyone been checking the volume of Chinese AI research papers? Almost every impactful paper I've read in the last month has mostly Chinese names. And a lot of those papers come with code. Usually permissively licensed. China is absolutely killing at the AI game. They've fast followed (or in many cases led) into a position of strength. I wouldn't want to be RunwayML, Pika, Luma, or Tripo right now. Any \"foundation model\" company with a single use case is getting cloned and commoditized. edit: Please don't downvote me because you don't like the message. I'm actually fine with you shooting the messenger, but this is absolutely worth talking about. It's pretty surreal to watch this all start to unfold over the last quarter or so. I want to read what others think about it. reply whatever1 16 hours agoparentSlightly relevant but they are also in the game of commercial mathematical optimization solvers and this year they reached parity with the sota American ones (ibm cplex, gurobi, Fico Xpress). It is that bad that all the American companies withrew from the public benchmark library. You can only see there Chinese commercial solvers and open source ones. https://plato.asu.edu/bench.html reply coliveira 15 hours agorootparentYes, I checked the recent solvers in the MIP space and it seems they're blowing out the existing American solvers. Huawei has invested a lot of money in this area. But in my opinion this was completely expected, most research in this area is now done in China, American universities have in comparison stopped in time. reply coalteddy 15 hours agorootparentprevAny blogs or other writing about this topic you can recommend? I worked with gurobi in the past but haven't been keeping up with the trends and performance gains. Love this field of CS! reply adamsiem 16 hours agorootparentprevWhat are the solvers? reply whatever1 16 hours agorootparentCOPT cardinal optimizer, Optverse from Huawei, MindOpt from Alibaba reply dr_kiszonka 14 hours agoparentprevIf these AI efforts are funded by the Chinese government, it makes perfect sense as a strategy. The more successful they are, the more risky it becomes to invest in US AI companies, leading to fewer investments (and less know-how in the long term), and China getting even more ahead. (I am not in AI, so please correct me if this is inaccurate.) reply whoevercares 45 minutes agorootparentGood news is the system in China frequently get in the way people with connections wins the government or state owned businesses contract, making it tougher for many innovators to strive with a shrinking private-owned/consumer-demand environment (Source: frustrated friends who lose the government contract to someone with inferior technology) reply segmondy 15 hours agoparentprevTrae doesn't look opensource, from their site looks like you download the binary (osx only for now) and install it. You can't configure it to use your own AI or another provider. You need to use their own API, all your prompts and code are go to bytedance. reply lsllc 16 hours agoparentprevInteresting theory maybe trying to diluting/confusing the technology market with alternatives that phone home [to the LLM] with your content. I just had to talk someone I know out of using RedNote as the TikTok shutdown loomed last week they had ZERO IDEA about its provenance. Unless you're paying attention, it's actually not clear what's what. reply Yeri 15 hours agorootparentWhile I definitely wouldn't trust this tech, we've also normalised phoning home with all western tech. How many products keep sending all kind of unknown telemetry, have a ton of trackers, etc? Can't really blame them doing the same. reply wruza 15 hours agorootparentprevMaybe they just be them, being a second economy and a 4x bigger population that is a few iq points smarter on average. reply fspeech 12 hours agoparentprevHow could more competition be bad? Sure some startups' business plans no longer work and have to pivot but many more startups now have access to better products at lower cost. reply whoevercares 14 hours agoparentprevComputer vision and deep learning conferences have been overwhelmed with Chinese authors since many years ago. No surprises. However the quality of the work skyrocketed recently reply pzo 15 hours agoparentprevYeah I have similar feeling: 1) They try similar like Zuck with LLama make sure GPT or Claude or Gemini is not the real winner in the West world like Android/iOS was for smartphone, Facebook/Whatsapp for social media/IM, Google Search for search, Windows for desktop OS. Majority people get used to to single product and they don't switch often. 2) Trying to kickstart community similar like LLama kickstarted big community around it that helped with tooling, testing, etc. 3) Slowing down development of ai companies in the west giving them less data for training (after all midjourney, elevenlabs, llama kickstarted training on copyrighted content, it's pretty sure they using user data at least from e.g. free version of GPTo-mini) and bleeding their budget kind of war of attrition. 4) Familiarise the west audience with their ai models after all still not many people using models like DeepkSeek since not many providers host them and majority of people don't have fast enough computer to run full models fast enough Trae helps with that reply suraci 13 hours agorootparentThey did what they did because everything they want to buy is banned or is to be banned by your government. Or are you being sarcastic? You must be being sarcastic. reply pzo 11 hours agorootparentI'm not being sarcastic. If that would be the only reason you said then they wouldn't open source it or didn't market for english speaking audience. reply coliveira 15 hours agoparentprevWell, this is apparent if you follow the releases, but Silicon Valley doesn't want you to know that. The strategy of making this open source has a clear goal: to make it easier for other Chinese startups to use it. reply xnx 16 hours agoparentprevByteDance's UI-TARS Skyvern, ec. reply add-sub-mul-div 16 hours agoparentprevMaybe they think it's in their national interest that a generation of Americans stops practicing their reading skills because of the tl;dr machine and stops building job skills that aren't reliant on the job-doing machine. reply DoctorOW 16 hours agorootparentI'm okay with the software being made open to the public. I think ultimately it's good for society as a whole. The US is welcome to do the same (I.E. actually opening OpenAI) but I don't think the drawbacks of AI are mitigated by making it more profitable to the American establishment. reply atulvi 16 hours agoprevAnother vscode wrapper reply Alifatisk 8 hours agoparentHow many do we have now? 3? Cursor, Windsurf and now Trae. reply TiredOfLife 11 hours agoprevNo Windows or Linux versions. The Chinese are Rich. reply csomar 9 hours agoprevA disappointment as this is yet another wrapper around VSCode. It doesn't change/re-invent the current paradigms of writing code. Effort would have been much better spent on getting AI auto-complete \"merged\" with LSP. But who is listening? Pretty much everyone these days think they can plug an LLM and have their life issues sorted out. reply ElectroNomad 16 hours agoprevHow is it different from Cursor? Just another AI wrapper around VS Code smh reply Alifatisk 8 hours agoparentIt's free. Cursor costs after 50 requests, even Windsurf is cheaper. reply humanlity 9 hours agoprevSo tired of such news reply toprerules 16 hours agoprevI feel like I'm taking crazy pills, because every time I try to use AI for any of my projects including the \"better\" Anthropic and OpenAI models, I get terribly written, buggy code that has nothing to do with the domain or question. Unless I'm feeding it actual softball, single function questions it adds almost no value to my dev cycle. Mind you, all of this terrible code also costs a ton in tokens. It's sad because I have seen some truly remarkable progress in LLMs, but I feel like we aren't allowed to be honest anymore that LLMs aren't going to replace programmers or moderate our expectations. reply nosefurhairdo 15 hours agoparentIt takes some practice to get good using them. I use it as a sophisticated autocomplete. For example, if I'm building an api client layer in a web frontend, I might have `fooApi.ts`, `mockFooApi.ts`, and `barApi.ts`. instead of writing `mockBarApi.ts`, something that might take 5 15 minutes, I can feed those 3 existing files into context, and a few seconds and pennies later I have a nearly perfect file ready for review. For any semi novel development I want to write it myself to establish the patterns I like and get an understanding of what I'm building. It's the boring stuff I hand over to Claude. reply toprerules 14 hours agorootparentThat's just it, I try not to write the boring parts at all. That's what libraries and design pattern are for avoiding writing boilerplate. I certainly can see it being useful for writing additional tests, although I am very worried based on what I've seen that it will introduce more bugs than it's worth, which is why it's still a tool I use for hobby projects and not for real work. reply guappa 10 hours agorootparent> I certainly can see it being useful for writing additional tests If your metric is number of tests, sure. If your metric is number of useful tests… eh reply raincole 14 hours agoparentprev> but I feel like we aren't allowed to be honest anymore that LLMs aren't going to replace programmers or moderate our expectations. But literally every single programmer I know has this opinion. reply gt0 13 hours agorootparentAgree, who is not allowing us to be honest? I have found programmers are freely talking about the limitations of AI. reply ithkuil 12 hours agorootparentIt looks like if the stuff you read online, written by people who need to optimize for various metrics of which objectivity is not among the top ones, shapes the perception of reality reply lencastre 12 hours agorootparentprevIt’s a matter of incentives and who benefits from a generalized belief that AI will wipe the floor with the whole programmer class. reply __loam 12 hours agorootparentprevThis opinion used to be a lot less popular especially on hackernews reply guappa 10 hours agorootparentprevI know developers that love it. Of course they are terrible at their job, so AI doesn't make their code much worse. reply shepherdjerred 14 hours agoparentprevCopilot Edits works incredibly well. It writes 80% of my code with the majority of my work being small fixes. https://code.visualstudio.com/docs/copilot/copilot-edits reply toprerules 14 hours agorootparentWhat kind of code/projects do you work on? reply shepherdjerred 14 hours agorootparentPython, TypeScript, Java, Go. I've done some stuff for a compiler in Java, backend web services in Python/Go/TS, frontend w/ React/TS. I was a huge skeptic of this stuff at first but started using it about a year ago. I could definitely live without it, but it also saves me a significant amount of time. I've been working on a feature in Python. With Copilot edits I just needed to find files the implemented the pattern, add it to the chat context. and write something like \"implement feature x following the same pattern\". It never gets it right the first time, but you can just keep the conversation going and have it iterate. Afterwards I can just write /tests and Copilot generates reasonable tests. If it missed cases I can ask it to write cover those tests. Often times I can also just write literally \"cover edge cases\" and it handles all reasonable scenarios. reply userbinator 15 hours agoparentprevYou're already above average if you can see that AI-generated code is bad. Those who are below average will think it's amazing and improving their abilities. reply thekevan 12 hours agoparentprevI feel like it's so hot or cold. People have been raving about Loveable \"one-shotting\" the creation of an app given one prompt. I tried to have it recreate a basic landing page from a screenshot and it wasn't even close. It invented some sections, reproduced others terribly and completely ignored others. I went through numerous prompts until I ran out for the day. With what I was left with at that point, it probably would have been a wash to fix that code or just start from scratch manually. I don't blame Loveable, I think it's a wonderful product bordering on magic, like v0 and Bolt.new. I just think they are amazing products that have been over hyped to god-like status. reply anon-3988 15 hours agoparentprevYou don't use LLMs to write code for you. This is like asking it what is 2+2, it may be able to answer that but that's not how it works. What it is just a summary of the internet. reply kristopolous 12 hours agoparentprevI've got a colleague at work and when he's wrong, I've got to get Claude to agree with me before he believes me. I used to think he was competent but now I think he's a moron reply guappa 10 hours agorootparentIf you formulate the questions right, AI will agree to anything. reply anonzzzies 12 hours agoparentprevI see this here often, but really, can someone please make a video or blog post with a complete code session so we can see what exactly happens? I am curious. reply throwaway2037 14 hours agoparentprevCan you provide a specific example of where an LLM failed? If you show us your prompt, your \"want/need\", and the result, we can better judge your situation. My guess: Your domain is weird/rare, so LLMs are terrible because their training data is very limited. reply thesz 13 hours agorootparent> Your domain is weird/rare, so LLMs are terrible because their training data is very limited. And this is how knowledge collapse [1] shows its' head. [1] https://arxiv.org/abs/2404.03502 I am not a big fan of LLMs so I try them once in a while, asking to \"implement blocked clause decomposition in Haskell.\" They can recite a paper (and several other papers, with references) almost verbatim, but they do not posess enough comprehension of what is going on in these papers. As time passes by, with each new LLM, the level of comprehension drops. Last time LLM tried to persuade me to write code instead of providing me with code. reply jcheng 10 hours agorootparentI gave Claude 3.5 Sonnet your prompt and it generated this: https://claude.site/artifacts/7aa41881-937e-4863-a407-c999ea... With this example usage: Example usage: let clause1 = Set.fromList [1, 2] represents (x1 ∨ x2) let clause2 = Set.fromList [-1, 3] represents (¬x1 ∨ x3) let formula = Set.fromList [clause1, clause2] Decompose the formula let (nonBlocked, blocked) = decompose formula How did it do? reply thesz 7 hours agorootparentThis is very good. Did it do just from the prompt or you had to nudge it? Can you share full chat history? reply guappa 10 hours agorootparentprev> My guess: Your domain is weird/rare, so LLMs are terrible because their training data is very limited. The training data is always limited, because if there was an existing software that already did what I'm doing, I'd be using it instead of writing it :) So by definition, unless I'm learning how to program and doing exercises that thousands have done before me, I'm doing something for which there is no training data. reply virgildotcodes 15 hours agoparentprevIt’s a tool with limited functionality. I think over time you vaguely learn the shape of its limits and work with what it can do, and it becomes genuinely useful. Doing basic stuff with a very limited scope that’s likely well documented all over the internet, great. Boilerplate, tedious yet simple things, works awesome as enhanced autocomplete. More complex stuff, give it a shot, maybe you’ll get lucky, otherwise if it starts fucking up I’ve had the most success just taking a step back and doing it myself, maybe chatting with it as a live docs substitute, or a realtime stack overflow/discord programming channel which are also sometimes of dubious quality but frequently useful. The misery really lies in getting stuck in that cycle of it just messing up over and over as you try to get it to make this thing work that is clearly beyond its scope and it’s just turning everything into a greater and greater mess of hallucinated bullshit. reply petesergeant 14 hours agorootparent> It’s a tool with limited functionality. I think over time you vaguely learn the shape of its limits and work with what it can do, and it becomes genuinely useful. Yeah, I think this is exactly right. I use it for adding new files to an existing codebase, but without giving it access to that codebase, by passing in the definitions I want it to work with. It gets stuff wrong a lot, and I need to keep anything I'm asking for _well_ within a scope that I can be eagle-eyed about. But if I want to do something pretty simple that would be slightly annoying to write by hand, and can be done in a single file, it makes a pleasant alternative to typing out the code by hand. I don't think I would be able to do this as a junior developer, I think this is only working because I can tell when it's full of shit, and I lasted about 90m of being willing to let IDE-integration happen, because it's too easy to be lazy and not scrutinize every little change, and that way lies total madness. This makes me beyond skeptical of non-developers writing anything significant with it: they'd be much, much better off learning Bubble or similar. reply insane_dreamer 12 hours agoparentprevSame here. I find it useful for: autocompletion (about half the time) questions on the syntax for certain commands (faster than looking up in the docs, usually pretty accurate) questions for how to solve a particular problem or gotcha (faster than looking up on SO, but also less accurate) It's a useful tool for certain cases, for sure. But it's not a \"game changer\" in terms of productivity. reply jkingsman 15 hours agoparentprevI think it's part art of prompting, and part problem scope they're not gonna get you a super polished final product without a LOT of support (yet), but they excel at rapid I-don't-care-what-the-code-looks-like-just-get-me-an-interactive-prototype work, and at solving specific problems (e.g. the function should do THIS, write it and write me a test suite). However, there are some agentic IDEs like Windsurf that are working to have LLMs orchestrate their own work. Well supported with a rock solid test suite, I've seen some people do pretty incredible things with them. The more a codebase provides tools for LLMs to live of the land in terms of tests and comments, the more aggressive you can be with what you ask them to do. I've built a couple pretty simple apps that have been basically no code beyond tweaks for me. I do think a big knowledge gap is the acceleration of prompting by knowing how to talk about code. LLMs show a lot of difference in response between \"make the top bit stay on the screen on small screens\" and \"make theelement sticky for viewportsI feel like we aren't allowed to be honest anymore that LLMs aren't going to replace programmers or moderate our expectations. I haven't heard anyone in tech say programmers will be replaced by this tech. I have heard persons outside of tech say it, but I think they lack sufficient context. I mean, if we get AGI, yes, we might be fully out of a job. But, so far I think many programmers agree this seems to be a ways off. But, I think you underestimate how many programmers used to copy paste code from StackOverflow and articles, etc. We even had/have a phrase for that: \"copy pasta\". Those programmers are getting more applicable templated code than they used to get via copy paste. When I'm coding in my preferred languages, I am faster without AI. But, when I'm writing yaml or something else for an unfamiliar tool or platform, I do get a productivity boost, even if I have to debug the code / configuration. reply swozey 16 hours agoprevI can't imagine getting an engineering job at an engineering company and asking them if I'd be allowed to create an opensource IDE that would never be a sold product. I can barely even get allowed time to write SRE tooling to improve our own engineers lives. Some C-levels newphew into computers and need a school project or something? reply markus_zhang 16 hours agoparentBytedance created all internal tools by themselves. Personally I'd definitely love to join such a company if I'm in that tooling team. Whether they are being used by outsiders is irrelevant because I already bagged the skills. reply swozey 16 hours agorootparentEh, I make tooling for a living. It sucks to have nobody using your tools even if they were fun to make. You spend months/years putting something out there that you think will really benefit a company/team/foss and it goes nowhere. It's like painting a painting you love and it getting thrown in an attic. reply markus_zhang 15 hours agorootparentI get what you said, but I'm not in a tooling team so I have a different perspective :) reply impulser_ 15 hours agoprev [–] Bro the West is getting cooked by China right now. We might be fucked. Everything that has been coming out of China is better than anything our mega trillion dollar corporation are producing and they are doing with with sanctions on the best GPUs. I don't work in big tech, why are they so bad with sigifically more money and more advanced technology? Google had a decade and a half head start and they can't even produce a model that is as good and as cheap as Deepseek building with no compute compared to Google. I have been using Trae and VS Code with Copilot is laughable compare too it despite using the same models. So we can't even prompt LLMs as good as they do and they don't even speak English as a first language LMFAO. We might be fucked and this is just the beginning. Maybe time to learn code should be time to learn Chinese. reply cedws 15 hours agoparentThis won’t be a popular opinion here on HN but I think it’s the beginning of the end for Western hegemony. Asia is powering full steam ahead while we sit idle. We have much lower social cohesion and it’s getting worse all the time. We’re becoming lower trust societies. We don’t incentivise building enough. Success is considered having enough capital to sit back and generate more capital while contributing nothing back to society. At the forefront of our politicians’ minds are how many genders there are. In the UK just building a new high speed railway has practically been the Manhattan project. Will we get back on track? I think the chances of that are rapidly diminishing. reply shigawire 11 hours agorootparent>At the forefront of our politicians’ minds are how many genders there are. This seems so irrelevant. Why do you think reactionary politics will lead to more innovation? reply guappa 10 hours agorootparentI don't think you understood his comment. reply wordofx 13 hours agorootparentprevWell the west is too busy trying to answer the question “what is a woman” and allowing mass migration into their countries. While Asia is like “there’s men and woman, and we make it hard for you to live in our country and never get citizenship while we out build you in every aspect” reply repeekad 13 hours agorootparentwhat's funny is debating about \"what is a woman\" vs having to work 6 days a week 6 to 9pm, I know which country I'd rather live in reply opwieurposiu 15 hours agoparentprevMy kids are in elementary school that Chinese language in the morning and English in the afternoon. Only problem is now I have no idea what they are talking about half the time. reply wood_spirit 13 hours agorootparentWhich Chinese language do they learn? reply opwieurposiu 3 hours agorootparentMandarin with the Simplified Characters (4 Tone Mainland Style) reply tnt128 15 hours agoparentprev [–] Haven’t tried Trae. Is it objectively better than cursor? I felt this statement might be premature. US is still leading in software and AI space. reply impulser_ 15 hours agorootparentThe chat UX is definitely better than Cursor. It less janky and I like that it wait until generation is done before asking you if you want to apply the suggested the changes instead of loading them into your file right after like cursor does. Also you can apply at cursor which is nice for snippets. The Builder tool is probably the best feature of Trae. It builds React UIs better than any tool even v0 which is built for building React UIs. It very good at recreating UI from image even better than using the model directly which I'm not too sure how... Code completion is probably closer to copilot than cursor so it depends on how hardcore you like your AI code completion. So Cursor might be better especially for repetitive refactoring. The overall design and feel of Trae is better the VS Code and Cursor, but I'm a big fan of JetBrains and it feel a lot like a JetBrains IDE. reply handfuloflight 14 hours agorootparentWould you choose this over JetBrains? reply impulser_ 14 hours agorootparentNo, because I like debuggers but if you don't care about debuggers than I would. It's basically a better Fleet. In fact, it's if Fleet and VS Code had a baby lol. reply verdverm 14 hours agorootparentDoes Jetbrains have a language agnostic IDE? Their page makes it look like you need to pick one specific to a language, but I have repos with multiple languages Like why does this page even exist? https://www.jetbrains.com/ides/#choose-your-ide reply impulser_ 14 hours agorootparentIt's Fleet. https://www.jetbrains.com/fleet/ reply verdverm 13 hours agorootparentmuch appreciated reply tnt128 15 hours agorootparentprevAh. Thx for the break down. reply scottyeager 13 hours agorootparentprev [–] DeepSeek is absolutely the leader in terms of cost value for high performance AI models. US firms might have the lead in terms of absolute best performance, true. But their prices are 10-20x more for a product that is marginally better at most. When it comes to coding DeepSeek V3 is very close to Claude 3.5 Sonnet and exceeds GPT-4o. reply Consider applying for YC's Spring batch! Applications are open till Feb 11. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Trae, an AI-powered Integrated Development Environment (IDE) by ByteDance, has initiated discussions about the trend towards subscription-based coding tools and associated costs.",
      "Concerns are raised about data privacy, particularly with Chinese companies like ByteDance, and the potential impact on traditional programming skills.",
      "The debate underscores differing views on AI's role in software development and the geopolitical implications of using tools from Chinese companies."
    ],
    "points": 205,
    "commentCount": 341,
    "retryCount": 0,
    "time": 1737595297
  },
  {
    "id": 42799629,
    "title": "Foundations of Large Language Models",
    "originLink": "https://arxiv.org/abs/2501.09223",
    "originBody": "Computer Science > Computation and Language arXiv:2501.09223 (cs) [Submitted on 16 Jan 2025] Title:Foundations of Large Language Models Authors:Tong Xiao, Jingbo Zhu View PDF Abstract:This is a book about large language models. As indicated by the title, it primarily focuses on foundational concepts rather than comprehensive coverage of all cutting-edge technologies. The book is structured into four main chapters, each exploring a key area: pre-training, generative models, prompting techniques, and alignment methods. It is intended for college students, professionals, and practitioners in natural language processing and related fields, and can serve as a reference for anyone interested in large language models. Subjects: Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG) Cite as: arXiv:2501.09223 [cs.CL](or arXiv:2501.09223v1 [cs.CL] for this version)https://doi.org/10.48550/arXiv.2501.09223 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Tong Xiao [view email] [v1] Thu, 16 Jan 2025 01:03:56 UTC (361 KB) Full-text links: Access Paper: View PDF TeX Source Other Formats view license Current browse context: cs.CLnewrecent2025-01 Change to browse by: cs cs.AI cs.LG References & Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer (What is the Explorer?) Connected Papers Toggle Connected Papers (What is Connected Papers?) Litmaps Toggle Litmaps (What is Litmaps?) scite.ai Toggle scite Smart Citations (What are Smart Citations?) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv (What is alphaXiv?) Links to Code Toggle CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub Toggle DagsHub (What is DagsHub?) GotitPub Toggle Gotit.pub (What is GotitPub?) Huggingface Toggle Hugging Face (What is Huggingface?) Links to Code Toggle Papers with Code (What is Papers with Code?) ScienceCast Toggle ScienceCast (What is ScienceCast?) Demos Demos Replicate Toggle Replicate (What is Replicate?) Spaces Toggle Hugging Face Spaces (What is Spaces?) Spaces Toggle TXYZ.AI (What is TXYZ.AI?) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower (What are Influence Flowers?) Core recommender toggle CORE Recommender (What is CORE?) About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs. Which authors of this paper are endorsers?Disable MathJax (What is MathJax?)",
    "commentLink": "https://news.ycombinator.com/item?id=42799629",
    "commentBody": "Foundations of Large Language Models (arxiv.org)199 points by pkoird 17 hours agohidepastfavorite18 comments MR4D 13 hours agoMe to ChatGPT: Assume you are a college instructor for a Freshman Computer Science course. Your job is to take a pdf file from the internet and teach the topics to you students. You will do this by writing paragraphs or bullet points about any and all key concepts in the PDF necessary to cover the topic in 2 hours of lectures The pdf file is at https://arxiv.org/pdf/2501.09223 Build the lecture for me. reply astrange 9 hours agoparentIt can't read PDFs. If you ask it to, it generates code to read the first X characters of the PDF and does a bad job. (Claude is much better at it.) reply helsinkiandrew 9 hours agorootparentYes it can both via websearches and uploaded (atleast I'm doing it daily). EDIT: This article says its only in ChatGPT Enterprise, but works for me on free plan: https://help.openai.com/en/articles/10416312-visual-retrieva... reply cdfuller 3 hours agorootparentThat article is referencing visuals embedded in PDFs. As a free user you wouldn't be able to ask ChatGPT to analyze a graph inside a PDF, only text. reply drmindle12358 2 hours agoprevAuthors are from Northeaster University, Shenyang, China, not the Northeastern U in Boston. Don't understand why the two Chinese professors write an LLM book in english, definitely not from experiences, probably under pressure to publish. reply crisissolution 10 hours agoprevDidn't know I could find it on arxiv, will definitely give it a read reply thatxliner 13 hours agoprevThese things can be on Arxiv?? reply bradly 13 hours agoparentI assumed Arxiv was peer-reviewed content only, but it looks like that is not the case. Submission guidelines: https://info.arxiv.org/help/submit/index.html Moderation process: https://info.arxiv.org/help/moderation/index.html reply niyyou 9 hours agorootparentOn the contrary, ArXiv is for pre-prints, i.e. not (yet) peer-reviewed. Off the top of the my head, it was initially used by physicists who often have huge collaborations and long reviewing time. Then the ML community invaded the space later on. This does not mean a peer-reviewed paper cannot go there of course. reply gus_massa 7 hours agorootparentMost of the times the peer review version has a copiright restriction, so the arxiv version is the finañ draft that may have small differences. reply williamstein 11 hours agorootparentprevAs an academic, I always thought of arxiv as where you put your papers first, before they are peer reviewed. Before that we used our webpages, but they kept breaking. reply htrp 15 hours agoprevat 231 pages this is definitely book territory reply ipython 15 hours agoparentThankfully the submission is self aware the first sentence of the article is literally: > This is a book about large language models. reply bradly 12 hours agorootparentThe book too it self aware, though you do have to make it to page ii. > In writing this book, we have gradually realized that it is more like a compilation of \"notes\" we have taken while learning about large language models. Through this note-taking writing style, we hope to offer readers a flexible learning path. Whether they wish to dive deep into a specific area or gain a comprehensive understanding of large language models, they will find the knowledge and insights they need within these \"notes\". reply TeMPOraL 8 hours agorootparentNow I wonder if the LLMs described in it are self-aware too, and whether by the time I reach the end of this book, I will become self-aware as well. reply hintymad 11 hours agoprev [–] Is it just me or this book looks rather like a Word doc than a Latex one? reply hexomancer 11 hours agoparentA Who cares? B The latex source of the book is available on the ArXiv page. reply jsvlrtmred 2 hours agoparentprev [–] It's just you reply Consider applying for YC's Spring batch! Applications are open till Feb 11. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The book \"Foundations of Large Language Models\" by Tong Xiao and Jingbo Zhu explores key concepts such as pre-training, generative models, prompting techniques, and alignment methods in large language models. It targets college students, professionals, and practitioners in the field of natural language processing, making it a valuable resource for those interested in Computation and Language, Artificial Intelligence, and Machine Learning. The book is available for further reading on arXiv under the identifier 2501.09223 [cs.CL]."
    ],
    "commentSummary": [
      "A book titled \"Foundations of Large Language Models\" by authors from Northeastern University, Shenyang, China, is available on Arxiv, noted for its comprehensive 231 pages.",
      "The discussion highlights ChatGPT's capabilities in reading PDFs, with some users pointing out its limitations in analyzing visual content.",
      "Arxiv is explained as a platform for pre-prints, which means the content is not necessarily peer-reviewed, and users express surprise at the book's availability there."
    ],
    "points": 199,
    "commentCount": 18,
    "retryCount": 0,
    "time": 1737596006
  },
  {
    "id": 42803279,
    "title": "Hacking Subaru: Tracking and Controlling Cars via the Starlink Admin Panel",
    "originLink": "https://samcurry.net/hacking-subaru",
    "originBody": "‹ Back Hacking Subaru: Tracking and Controlling Cars via the STARLINK Admin Panel Thu Jan 23 2025 Introduction On November 20, 2024, Shubham Shah and I discovered a security vulnerability in Subaru’s STARLINK connected vehicle service that gave us unrestricted targeted access to all vehicles and customer accounts in the United States, Canada, and Japan. Using the access provided by the vulnerability, an attacker who only knew the victim’s last name and ZIP code, email address, phone number, or license plate could have done the following: Remotely start, stop, lock, unlock, and retrieve the current location of any vehicle. Retrieve any vehicle’s complete location history from the past year, accurate to within 5 meters and updated each time the engine starts. Query and retrieve the personally identifiable information (PII) of any customer, including emergency contacts, authorized users, physical address, billing information (e.g., last 4 digits of credit card, excluding full card number), and vehicle PIN. Access miscellaneous user data including support call history, previous owners, odometer reading, sales history, and more. After reporting the vulnerability, the affected system was patched within 24 hours and never exploited maliciously. Proof of Concept Taking over a Subaru using only the license plate in about 10 seconds, retrieving over a years worth of location history from the vehicle Map displaying 1,600 leaked coordinates from a 2023 Subaru Impreza, similar data was retrievable for any internet-connected Subaru Vulnerability Writeup A little over a year ago, I bought my mom a 2023 Subaru Impreza with the promise that she would let me borrow it to try and hack it. I’d spent the last few years hunting for vulnerabilities in other automakers, but didn’t yet have the chance to look at Subaru. While visiting home for thanksgiving this year, I took my opportunity and asked for the account login to see if I could get anywhere. Auditing the MySubaru Mobile App The first thing I wanted to test was the MySubaru app. This app allowed users to send vehicle commands, so I proxied the app using Burp Suite and intercepted the telematic command HTTP requests, hoping to find a vulnerability to unlock cars without authorization. The below request was sent when unlocking a car via the app: POST /g2v30/service/g2/unlock/execute.json;jsessionid=AE6E4482F5C4493A79C8F3BD656F8BBA HTTP/1.1 Host: mobileapi.prod.subarucs.com Content-Type: application/json Connection: keep-alive Accept: */* User-Agent: MySubaru-PROD-SOA/2024110100 CFNetwork/1568.300.101 Darwin/24.2.0 Content-Length: 83 Accept-Language: en-US,en;q=0.9 Accept-Encoding: gzip, deflate, br { \"delay\": 0, \"unlockDoorType\": \"ALL_DOORS_CMD\", \"vin\": \"4S3GTAV64P3701234\", \"pin\": \"1234\" } After failing to bypass the authorization for in-app vehicle commands, I looked around the app a bit more but couldn’t find anything interesting to test. Everything seemed properly secured. There weren’t a lot of endpoints. The authorization worked really well. Maybe testing the MySubaru app was the wrong approach. From my past experience with car companies, I knew there could be publicly accessible employee-facing applications with broader permissions than the customer-facing apps. With that in mind, I decided to shift focus and started hunting for other Subaru-related websites to test. Finding the Subaru Admin Panel I sent my friend Shubs a message over Discord to see if he’d be interested in helping me find any potential Subaru employee applications. He said sure and then immediately sent me this message: shubs — 11/19/2024 have you seen this host before? subarucs.com He noticed that ‘my.subaru.com’ (a domain that the MySubaru app was using) was a CNAME for ‘mys.prod.subarucs.com’ (a domain that I hadn’t seen before). nslookup my.subaru.com Server: 127.0.0.53 Address: 127.0.0.53#53 Non-authoritative answer: my.subaru.com canonical name = www.mysubaru.com. www.mysubaru.com canonical name = mys.prod.subarucs.com. Name: mys.prod.subarucs.com We ran a scan to find other subdomains and checked the output: … STARLINK® Admin Portal https://portal.prod.subarucs.com/login.html … Well, that definitely looked like employee functionality. From a quick Google, it appeared that STARLINK was the name of Subaru’s in-vehicle infotainment system which provided all of the remote functionality for the vehicle. This appeared to be an admin panel related to it. The Subaru STARLINK admin panel. Arbitrary Account Takeover on Subaru STARLINK Admin Portal At first glance, it didn’t seem like there would be much here. It was just a login panel, and we didn’t have any credentials. I checked the source of the website hoping to see a bit more, and the following bit caught my eye:There were some interesting JavaScript files under the “/assets/_js/” folder that were loaded into the login page, so I went ahead and brute forced the directory in hopes of finding other JavaScript files. After a few minutes of running FFuF, we got a hit for a “login.js” file which the following very interesting code snippet: $('#new_password_submit').on('click', function(e) {e.preventDefault();if($('#forgot-password-step4-form').valid()) { disableBtns(); $.ajax({ url: \"/forgotPassword/resetPassword.json\",type: \"POST\", contentType: \"application/json\", data: JSON.stringify({ email: email, password: $('#new_password').val(), passwordConfirmation: $('#confirm_new_password').val() }),async: false }).done(function (response) { It appeared that there was a “resetPassword.json” endpoint that would reset employee’s accounts without a confirmation token! If this worked how it was written in the JavaScript, then an attacker could simply enter any valid employee email and take over their account. I sent the following POST request to confirm that the functionality was even accessible: HTTP Request POST /forgotPassword/resetPassword.json HTTP/1.1 Host: portal.prod.subarucs.com { \"email\": \"random@random.com\", \"password\": \"Example123!\", \"passwordConfirmation\": \"Example123!\" } HTTP Response HTTP/1.1 200 Content-type: application/json Content-length: 7 “error” It seemed to be working, we just needed to find an employee’s email address to test it on. Since this was a fairly large application, there were probably a bunch of different users, we just needed to find some way to enumerate them. I dug through the rest of the JS looking for an endpoint that might let us enumerate emails until I saw the following: HTTP Request GET /adminProfile/getSecurityQuestion.json?email=example@example.com HTTP/1.1 Host: portal.prod.subarucs.com HTTP Response HTTP/1.1 200 Content-type: application/json Content-length: 7 { \"error\": \"Invalid email\" } The above endpoint would return the user’s security questions if their email was valid. We could use this to enumerate user accounts until we found someone that was active on this platform. Enumerating Employee Emails Using LinkedIn, we did a quick search for “Subaru STARLINK” and found a few employees who appeared to be software engineers. After getting their names, we Googled and found that Subaru emails are in the following format: [first_initial][last]@subaru.com We tossed the few emails that we’d pieced together into the “getSecurityQuestion.json” endpoint and hit send. On the fourth attempt, we got a response back!What city were you born in?The jdoe@subaru.com (redacted) email was valid! We went back to the reset password endpoint and hit send. HTTP Request POST /forgotPassword/resetPassword.json HTTP/1.1 Host: portal.prod.subarucs.com { \"email\": \"jdoe@subaru.com\", \"password\": \"Example123!\", \"passwordConfirmation\": \"Example123!\" } HTTP Response HTTP/1.1 200 Date: Wed, 20 Nov 2024 03:02:31 GMT Content-Type: application/json Connection: close X-Frame-Options: SAMEORIGIN Content-Length: 9 \"success\" It worked! We tried logging in. We had successfully taken over an employee’s account, but there was now a 2FA prompt to actually use the website. It was custom, so we tried to see if there was anything to do to bypass it. Bypassing 2FA We tried the simplest thing that we could think of: removing the client-side overlay from the UI. Match $('#securityQuestionModal').modal('show'); Replace //$('#securityQuestionModal').modal('show'); After removing the client-side overlay, we clicked around and the whole app seemed to function normally. All of the buttons worked, and were returning server-side data. 2FA bypassed. Tracking My Mom for the Last Year The left navbar had a ton of different functionality, but the juiciest sounding one was “Last Known Location”. I went ahead and typed in my mom’s last name and ZIP code. Her car popped up in the search results. I clicked it and saw everywhere my mom had traveled the last year: Date Odometer Location 11/21/2024 6:18:56 PM 14472.6 41.30136,-96.161142 11/21/2024 4:59:51 AM 14472.6 41.301402,-96.161134 11/21/2024 4:49:02 AM 14472.6 41.301286,-96.161145 … … … 11/02/2023 1:44:24 PM 6440.6 41.256003,-96.080627 11/01/2023 9:52:47 PM 6432.5 41.301248,-96.159951 11/01/2023 12:16:02 PM 6425.2 41.259397,-96.078775 The “Last Known Location” endpoint was more than the last location, it gave me the exact coordinates of everywhere that she had started her engine or used a telematics command over the last year. I didn’t realize this data was being collected, but it seemed that we had agreed to the STARLINK enrollment when we purchased it. To better understand the data, I exported a year’s worth of location history from my mom’s 2023 Impreza and imported it into the Google Maps iframe below. She kindly gave her permission to share it, saying that her life is \"too boring\" for anyone to misuse the information. Visualizing a Year of Subaru Location History Map displaying 1,600 leaked coordinates from a 2023 Subaru Impreza, similar data was retrievable for any internet-connected Subaru Our STARLINK purchase agreement history, accessible from the admin panel. There were a ton of other endpoints. One of them was a vehicle search which let you query a customer’s last name and zip code, phone number, email address, or VIN number (retrievable via license plate) and grant/modify access to their vehicle. Retrieving street address, phone number, email, emergency contacts, authorized users, and billing information of any Subaru STARLINK customer. The STARLINK search functionality which allows you to search via zip code and last name, VIN, email address, and phone number. Unlocking a Friend’s Car After searching and finding my own vehicle in the dashboard, I confirmed that the STARLINK admin dashboard should have access to pretty much any Subaru in the United States, Canada, and Japan. We wanted to confirm that there was nothing we were missing, so we reached out to a friend and asked if we could hack her car to demonstrate that there was no pre-requisite or feature which would’ve actually prevented a full vehicle takeover. She sent us her license plate, we pulled up her vehicle in the admin panel, then finally we added ourselves to her car. Adding ourselves as an authorized user to our friend's Subaru to demonstrate that we could execute commands on their vehicle. We waited a few minutes, then we saw that our account had been created successfully. Now that we had access, I asked if they could peek outside and see if anything was happening with their car. I sent the “unlock” command. They then sent us this video. Success! Afterwards, she confirmed that she did not receive any notification, text message, or email after we added ourselves as an authorized user and unlocked her car. Timeline 11/20/24 11:54 PM CST: Initial report sent to SecOps email 11/21/24 7:40 AM CST: Initial response from Subaru team 11/21/24 4:00 PM CST: Vulnerability fixed, unable to reproduce 01/23/25 6:00 AM CST: Blog post released Addendum When writing this, I had a really hard time trying to do another blog post on car hacking. Most readers of this blog already work in security, so I really don’t think the actual password reset or 2FA bypass techniques are new to anyone. The part that I felt was worth sharing was the impact of the bug itself, and how the connected car systems actually work. The auto industry is unique in that an 18-year-old employee from Texas can query the billing information of a vehicle in California, and it won’t really set off any alarm bells. It’s part of their normal day-to-day job. The employees all have access to a ton of personal information, and the whole thing relies on trust. It seems really hard to really secure these systems when such broad access is built into the system by default. Thanks Happy (late) holidays, thanks for reading! Huge thanks to the following people for helping write and review this post: Gren (https://bsky.app/profile/nop.codes) Ian Carroll (https://x.com/iangcarroll) Justin Rhinehart (https://twitter.com/sshell_) Joseph Thacker (https://x.com/rez0__) Brett Buerhaus (https://x.com/bbuerhaus) Maik Robert (https://x.com/xehle_) Joel Margolis (https://x.com/0xteknogeek) Find me on:twitter: https://twitter.com/samwcyodiscord: zlz",
    "commentLink": "https://news.ycombinator.com/item?id=42803279",
    "commentBody": "Hacking Subaru: Tracking and Controlling Cars via the Starlink Admin Panel (samcurry.net)196 points by ramimac 6 hours agohidepastfavorite142 comments like_any_other 6 hours ago> After reporting the vulnerability, the affected system was patched within 24 hours and never exploited maliciously. So 'only' Subaru, Starlink, their business and advertising partners, and law enforcement, can remotely track (and disable don't think you can run from the law!) your car? > I didn’t realize this data was being collected, but it seemed that we had agreed to the STARLINK enrollment when we purchased it. Assuming it's possible to not agree to it does that completely disable the system, or is everyone with a Subaru just one warrant away from getting locked in their car until the police can come to arrest them? Does the car still store (I'm charitably assuming it doesn't transmit) location data, so all your friends can retroactively be identified and arrested as well, even if you never agreed to any tracking? (To get ahead of the usual retort haha yes, phones also track this data, therefore let's not fix any problems unless we can fix all of them at the same time. But actually let's use the other problems as an excuse to do nothing.) reply aftbit 4 hours agoparentI was lucky to have the DCM in my 2019 Outback (which is responsible for cellular communication and thus this whole STARLINK thing) replaced with a bypass box under the warranty program related to the end of 3G service. My car was trying to go online 30 times an hour or something like that, draining the battery enough that it needed to be replaced after just 4 years. They don't have enough new DCMs so they were willing to replace it with a bypass box instead, which seems even better to me. So at least my Subaru cannot connect to the cloud anymore. I'm sure it still stores location and telemetry data for insurance fraud reasons though. reply dylan604 3 hours agorootparent> So at least my Subaru cannot connect to the cloud anymore. Sounds like something you can use to justify a higher selling price when the time comes reply mmmlinux 3 hours agorootparentprevFWIW i never replaced the 3g box in my 2018 subaru, and never have a battery drain issue. the battery did fail about 4 years in to owning the car, but it was a cell failure not a drain issue. reply pfooti 2 hours agorootparentMy Subaru, a 2018 outback, would be completely battery dead if I left it parked for more than a week or so. Happened once at the airport, after which I started carrying a battery pack car starter. The guy at the airport who have us a jump said it happened all the time with Subarus in long term parking. reply reginald78 1 hour agorootparentprevMy local mechanic said that there is a firmware update that supposedly fixes the radio drain last time I replaced the battery but I haven't looked into it. He primarily works on Subarus and it sounded like he'd seen that as a root cause of dead battery a lot. reply bongodongobob 4 hours agorootparentprevThat's avg time to replace a battery. reply aftbit 4 hours agorootparentHmm really? My Camry made it nearly 10 years, and my Civic still had a good battery 6 years in when I sold it. Regardless, the battery and DCM were both tested by Subaru. The battery tested bad, and the DCM tested for a high parasitic draw. I drove the car daily, and the battery would die if I didn't drive it for 4 days. I didn't just make this up either, search \"DCM parasitic draw\" on Google for more. Subaru even sent me a letter outlining my options for repairs. reply vel0city 3 hours agorootparentLots of places will say an average car battery's life is somewhere around 3-5 years. It is highly dependent on weather. Here where there are regularly long spans of 100F+ days, a battery will have done pretty good to make it five years; many die in 3-4. Same in very cold climates. If you're in a place with good weather all the time they'll last considerably longer. reply thejazzman 2 hours agorootparentor maybe even more importantly, a garage reply tedmielczarek 3 hours agorootparentprevI've heard about this being a known issue with cars from other manufacturers, so I can believe it. It's interesting that nobody thought to include a way to let the vehicle know it should stop trying to communicate to handle a potential end-of-service situation like this. It's fairly common for people to keep cars for more than a decade, they're a really expensive necessity for many. reply anotherhue 3 hours agorootparentprevThat's a known issue. Reports all over the subaru forums. I tried to measure it myself with my little multimeter and now I don't have a working multimeter. One suggestion I tried that seemed to work was to not keep the key close to the car, since that'd not physically possible for me I wrap it in an ESD bag. Haven't had much issue since, but no promises. reply bongodongobob 3 hours agorootparentprevMaybe I'm used to harsher weather in the Midwest. reply MisterTea 1 hour agorootparentprevDepends on the battery and how it was (ab)used. I have seen batteries last only 3 years or as long as 7. reply mattmaroon 2 hours agoparentprevIt’s not the Starlink you’re thinking of, that’s Subaru’s name for their in-car infotaimnent system and predates the SpaceX Starlink. So it’s just Subaru. Also I doubt any ad partners can disable your car (outside of vulnerabilities like this). Subaru can but that’s the case with most modern vehicles. reply reaperducer 2 hours agoparentprevAssuming it's possible to not agree to it My new car came with something called \"Google Built-In,\" which seems to be the bastard sibling of Google Car Play. During the set up, if you'd like to read the privacy policy, you must scan a QR code on your phone, which opens a web page that does not display on mobile devices. If you'd like to opt-out of anything, you have to create a Google account, then log the car into that Google account, then log into that Google account on your phone, then go hunting for the settings on both the car and in the Google account online. Good luck finding them all. Also, it is not possible to uninstall certain \"essential\" Google apps from the car. Apparently, YouTube is now an \"essential\" part of driving. reply rurp 2 hours agorootparentYeesh, which care manufacturer is this from? reply reaperducer 2 hours agorootparentVolvo. reply onetokeoverthe 2 hours agorootparentThe real owner of Volvo is Chinese automaker Geely. The company acquired the Swedish automaker from Ford in 2010 for $1.8Bil. source. https://www.fool.com/investing/how-to-invest/stocks/who-owns...) reply dantillberg 2 hours agoprevLast year, I submitted a \"right to know\" request to Subaru, and they sent the following back. I've reformatted it for legibility. Basically asserts they'll do and sell whatever they want (except another car to me). > Subaru may collect the following personal information about a consumer: > Categories of personal information: > Identifiers: Consumer records, Commercial information, Internet or Other Electronic Network Activity, Audio recordings, Vehicle geolocation, Professional or employee-related information, Inferences, Sensitive personal information > Categories of sources from which the personal information is collected: Retailers, i.e. authorized Subaru dealerships , Provided by consumer or vehicle, Third parties > Business or commercial purpose for which Subaru collects or sells personal information: To provide services to the consumer, To market goods and services to consumers, To provide marketing by third parties for third party goods and/or services, To comply with legal obligation > Categories of third parties with whom the personal information is shared: Business service providers, Contractors, Retailers, Corporate parent and affiliates, Third party providers of goods and/or services, Entities required to comply with the law > Categories of personal information sold: Identifiers for third party marketing of goods or services., Consumer records for third party marketing of goods or services > Categories of personal information disclosed for business purpose: Identifiers are disclosed to service providers, contractors, and third parties., Consumer records are disclosed to service providers, contractors, and third parties., Commercial information is disclosed to service providers, contractors, and third parties., Internet or other electronic information is disclosed to service providers, contractors, and third parties., Vehicle geolocation is disclosed to service providers., Inferences are disclosed to service providers and contractors., Sensitive personal information is disclosed to service providers and contractors. reply mavdi 4 hours agoprevNot surprised. I've had a few interactions with Subaru connected services dev team as an external contractor from another car company, everyone was everyone else's cousin, friend, homeboy from India. Nepotism was rampant, no one wanted to listen to advice, a strong culture of corporate antibodies had formed. I'm surprised they even got it to work at this level. reply duxup 4 hours agoparentI love my Subaru as far as reliability, all wheel drive performance in snow and ice, and such. But OMG it's consumer tech was dated when I bough it, and it's just full of inexplicable issues and caveats and such. Even just the limitations and the UX issues are so obvious that it sends a message that if they tried to fix them they would introduce just as many new issues. I'm at the point where despite the car being good, I'm not interested in a new one from Subaru. I just want carplay or android auto whatever similar services a given mobile OS provides to do similar things. That's it, every time it's something else (even when offering car play) from a car maker it is so bad and so naively built that it makes me less confidant in that company. I know, they want my data and all and that's the motivation, but man it's just such a downer with every system.... and here I am with a good car in most respects and I'm not planning on buying from them again. reply numpad0 1 hour agorootparentI suspect it has to do with slow adoption of CarPlay/Android Auto in Japan everyone still options aftermarket infotainment at dealerships and happier about it than with phone-based experiences. From a random Google search result[1]: > More than three-fourths (79%) cite the built-in navigation system. However, this percentage has decreased from 81% in 2022 and 82% in 2021. Use of Android Auto/Apple CarPlay apps is increasingly the preferred system, with 7% of users citing this in 2023, compared with 5% in 2022 and 3% in 2021. That's like 80% CP/AA adoption by 2060. UI/UX and especially overall experience polish had always been a major challenge for Japanese engineering. Everything is committee designed in perpetual intra-company tug of war, and it shows as a \"family sized mega pack\" UI consists of bunch of snippet codes each with an attention grab dialog to prove its worth. That was clearly one of major causes that led to total collapse of domestic phone industry and iPhone dominance, but I suppose it hasn't affected car infotainment, or mass market cars in general. 1: https://japan.jdpower.com/sites/japan/files/file/2023-11/202... reply duxup 1 hour agorootparent>UI/UX and especially overall experience polish had always been a major challenge for Japanese engineering. I can believe it. The whole issue of \"Japanese video game companies don't understand the internet\" to some extent still feels like it is an issue at times. For a while it felt like we got late 1990s solutions in the mid 2010s... it's gotten better-ish in the land of video games, but man it's so bad at times still. reply scottbez1 1 hour agorootparentprevHmm this is really different than my experience with a 2018 Crosstrek, so maybe things have changed? When I bought it, Subaru was among the earlier CarPlay/Android Auto adopters (we specifically ruled out a new model year Prius because it lacked it and we couldn't wait a year to replace our totaled car), and other than a very rare issue where the head unit screen doesn't turn on, it's been pretty rock solid with both phone OSes. Environmental controls are all physical hardware, CarPlay/AA is integrated well, etc; I can't really complain about any UX in the car. The only UX gripe I can think of is that Apple doesn't let you use natural touch inputs to pan/zoom a map (instead forcing you to tap to bring up on-screen d-pad, then keep tapping the tiny button targets while trying to keep an eye on the road), but that's entirely on Apple; Android Auto allows normal 2 finger pan/zoom, so it's not a Subaru problem. reply blackeyeblitzar 3 hours agorootparentprevSubaru infotainment is also very controlling. Want to use the keypad while you’re taking a phone call on the go? No, it won’t let you if the car is moving. You’ll need to use your phone’s UI. Other CarPlay cars don’t do this. reply js2 3 hours agorootparentMy 2017 Mazda with CarPlay does something similar. It truncates any lists (songs, podcast episodes, contacts, etc) that CarPlay displays to 10 items. All it does is incentivize folks to use their phone. It's incredibly annoying because the Mazda command dial for interacting with CarPlay is otherwise excellent and I don't think that limiting the list size does anything to reduce distraction. reply eptcyka 1 hour agorootparentIt is an incentive to use Siri. You can’t actually use carplay if Siri is disabled. reply spelunker 3 hours agorootparentprevI have a car from another Japanese manufacturer (Mazda) their connected services app is weird and clunky and was down twice this month. And I'm expected to pay $10/month for this thing after the first year! Cmon. reply ajsnigrutin 4 hours agorootparentprevyep... there was a tv ad for subaru vehicles a couple of years ago (not that long!), and during the ad, they showed the infotainment system, where the user pans the map on the navigation touchscreen, and the map moves at maybe 1fps! in an ad! I kinda wish they standardized the car interface for tablets (like android auto, but more features), where you could just buy a tablet and insert it in (like din slots for radio, but tablet-sized), and the car would expose some non-critical interfaces to the tablet (AC,...), and you could just buy a replacement tablet if needed. Cars are made to last 10, 15, even more years, while the computers/entertainment devices move a lot faster, and that includes the connectivity (many cars on the streets today were made before 4g, and 3g is mostly dead). reply monitorlizard 3 hours agorootparentThis is a genuinely good idea for a business that I think you should explore further if you have the bandwidth. reply Cumpiler69 4 hours agoparentprevnext [15 more] [flagged] duxup 4 hours agorootparentWorking across cultures is so wild. I worked in situations where things were outsourced and yeah the Indian experience was horrific. But that also was influenced by the nature of the relationship.... they didn't work \"for us\" in any real way. I worked for a company where we (a Midwestern company) were acquired by a valley company and at HQ there was a clear divide between the Indian (US citizens, not H1B) folks and the local CA team. It wasn't bad, but you could see it socially and feel the vibe and such. I traveled there a few times and I was just friendly and ... man they were great. Very friendly, very professional, and highly capable. Sometimes I think the business relationships also creates the informal \"working culture\" too. reply UltraSane 3 hours agorootparentWe once hired an Indian programmer who absolutely didn't get along with his boss, who was also Indian. Turns out the boss was a Dalit and the programmer was a Brahmin. And this is how I learned about the Indian Caste system. reply kccqzy 4 hours agorootparentprevI've heard of this claim so often that I assume it to be true, though personally I've only had the fortune to work in better environments where my Indian colleagues aren't nepotistic. I suspect this might be related to the hiring bar: if a company only hires the top talent perhaps this would not be an issue. reply gedy 4 hours agorootparentUnfortunately I think it depends on the number and position of Indian folks. Small numbers where you deal with people individually, I've not seen issues and it's nice to work with Indian devs. In larger numbers or when in charge of hiring, there seems to be a prevalent issue of Indian cultural norms and favors kicking in and it can happen fast. reply tonyedgecombe 4 hours agorootparentMonocultures are almost always bad, if nothing else they are a signal that you aren't hiring on merit. reply Cumpiler69 3 hours agorootparent>Monocultures are almost always bad Depends on the culture, the size of the team, the product you're working on and the target demographic. reply fullofideas 4 hours agorootparentprevThe linked article is about a fraud conducted by a few bad apples. I can see people colluding with others that are similar to them for criminal activities gangs, drug smuggling, and probably other frauds too. I am not sure how you inferred caste based nepotism among *all* indians in tech from that article. reply UltraSane 3 hours agorootparentI can only say that every Indian boss I got started hiring only Indians. reply blackeyeblitzar 3 hours agorootparentYou’re making the same mistake others here are. People hire from their networks just like they hire referrals. If you worked in India or China or wherever, you probably know some talented people of those ethnicities just as a result of your experience. Using your network to hire those talented people is normal and not discriminatory. Everyone does it. Somehow Indians are singularly attacked for it on hacker news and all kinds of assumptions (like nepotism) are made with zero evidence. reply UltraSane 1 hour agorootparentOnly hiring Indians inside the US takes extra effort. reply Cumpiler69 4 hours agorootparentprevFor one, I didn't say \"all Indians\" are bad apples, just that the nepotism and cast issues are rampant enough that it's a know issue at this point in the tech industry where Indians are sometimes overrepresented. Secondly, do you expect people to post links to all cases of Indian nepotism/cast issues in the tech industry, when Google is at your fingertips with enough cases that it's not an isolated incident? That link was one an Indian friend shared right now when I sent him the Subaru link, when I asked him if the nepotism is real. reply fullofideas 3 hours agorootparentFair enough that you did not say all Indians. But, your statement was broad enough to say nepotism is widespread among Indians in tech. And the article you linked was about fraud which doesn’t imply widespread nepotism in tech. I am not asking for articles for all instances but something that is more relevant to the point that you are making. reply Cumpiler69 1 hour agorootparentSure, but if an group of Indian employees within Apple US are going out of their way to scam their employer for money, it's another black mark on the graph for that demographic being into unethical activities, since then you can't say anymore \"well it's just one rouge bad apple, not representative for the whole group\" when it's a coordinated effort of a whole group. If they're wiling to go that far to scam their employer it's not a far fetch the fact they're also into nepotism when hiring. reply blackeyeblitzar 3 hours agorootparentprev> Indians are some of the most racially nepotistic out there I’ve heard this claim made often here but never observed it in real life. I think you and others who repeat this claim are confusing nepotism with just using one’s network to accelerate hiring. If someone happens to have a social or professional network mostly of one race, that doesn’t mean they’re automatically nepotistic when they draw from that network. Somehow this label rarely arises when white managers end up with mostly white teams. Why is that? reply bilekas 4 hours agoprev> I didn’t realize this data was being collected, but it seemed that we had agreed to the STARLINK enrollment when we purchased it. This is mind blowing to me.. Number 1 why you need a car connected to the internet all the time ? And how you're not required to sign at least 10 forms to confirm you understand that ALL of your travel data will be recorded and distributed at will. reply jeroenhd 4 hours agoparent> Number 1 why you need a car connected to the internet all the time To open the car with an app (programming against Bluetooth is harder than calling a web API), or honk the horn if you lost it in a large parking lot. > And how you're not required to sign at least 10 forms to confirm you understand that ALL of your travel data will be recorded and distributed at will. Legally speaking, I believe that depends on your local privacy laws. Practically speaking, car makers (and government agencies) love these features for troubleshooting and tech support, or for flagging crashes before any authorities or local press have time to arrive (think Tesla). Don't ask them about finding your stolen car, though. Then the data may suddenly not be available. reply bilekas 4 hours agorootparent> To open the car with an app (programming against Bluetooth is harder than calling a web API), or honk the horn if you lost it in a large parking lot I really hope this was sarcastic. How did we ever manage to find our cars before IoT cars … reply rtkwe 3 hours agorootparentYou can also do other stuff like start the climate controls so the car is comfortable before you reach it on hot or cold days. reply bilekas 3 hours agorootparentThese things could certainly be convenient, de-icing while you're having breakfast etc but surely you don't need my location data recorded for those services ? Maybe if there was a list of options I could select from then I guess it's fine. reply dylan604 3 hours agorootparentprevI could do that with a key fob well before connected cars were a thing. People just choose to ignore the most basic of information reply willis936 2 hours agorootparentAnd car manufacturers are incentivized to remove such valuable and logical features. Enshittification abounds. reply reaperducer 2 hours agorootparentprevYou can also do other stuff like start the climate controls so the car is comfortable before you reach it on hot or cold days. Also a solved problem. This one since at least the 1970's. No internet required. reply rtkwe 1 hour agorootparentI'd only really heard of it as an aftermarket feature you had to mod onto the vehicle not as a base part of a package. At least not in the low market range you can get that feature now. Used EVs less than 30k come with that feature now. reply reginald78 1 hour agorootparentprevI can't tell sometimes if people are trolling on HN or just young. Many so called great innovations I see are just stuff that was available decades ago but didn't involve ad tech predators. You can tell this stuff isn't that interesting to people otherwise they'd upcharge you for it instead of using it as a pathetic piece of bait for a trap. reply beezle 4 hours agorootparentprevJust as an aside a friend had their car nicked in NYC this winter. He was able to tell the cops the car location from some Toyota find my car type thing. The cops said they saw nothing on the street so unless he could come and make the horn beep infront of a garage and then get a warrant there was nothing more to do. He now has a new vehicle. reply theonething 2 hours agorootparentIs your comment supporting the find my car feature or saying it was useless? reply bilekas 3 hours agorootparentprevI'm starting to come around to the idea in general actually with all the comments promoting the benefits. I still don't see why travel locations need to be recorded though. A pining service would suffice if it was always connected. reply selimthegrim 2 hours agorootparentprevThis might say more about the NYPD than Toyota reply reaperducer 2 hours agorootparentpreva friend had their car nicked in NYC this winter. He was able to tell the cops the car location from some Toyota find my car type thing. The cops said they saw nothing on the street so unless he could come and make the horn beep infront of a garage and then get a warrant there was nothing more to do. Here's the way it's done with Volvos (from the manual): If the vehicle has been stolen or otherwise used without permission, the vehicle's owner, police and Volvo Assistance can agree to track the vehicle. Note This applies even if the vehicle has been opened and stolen using the associated remote key. The following needs to be done: 1. Contact Volvo Assistance and say that you need help tracking the vehicle. Tracking begins. 2. File a police report. 3. Contact Volvo Assistance again and give them the police case number. 4. Volvo Assistance notifies the police of the vehicle's location. reply reaperducer 2 hours agorootparentprevhonk the horn if you lost it in a large parking lot. Solved problem since at least the late 1980's. No internet required. reply dv_dt 2 hours agoparentprevThere was a recent HN posting on the US banning Chinese car brands from being connected to the internet. https://news.ycombinator.com/item?id=42706212 If Chinese companies comply with the ban by providing car models without internet connectivity, it's hilarious to me that that the nationalist regulation could make Chinese branded vehicles more desirable from a security & privacy standpoint. reply insane_dreamer 4 hours agoparentprevRemote start, climate etc from the app all require and always on connection (how else?). Tesla has the same thing. I use the remote app often quite useful. reply dylan604 3 hours agorootparentRemote start does not require an app nor an always on connection. I was able to do this with a key fob in the early 00s reply redwall_hp 2 hours agorootparentNeither does climate control. If I remote start my Civic with its fob, it will heat or cool to the desired temperature I left it on. (And it will run the defroster if it's below a certain temperature outside.) Having to think about the climate control is an anti-feature in itself, when that's a basic thermostat feature... reply insane_dreamer 1 hour agorootparentprevI know how key fobs work, thank you. I mean the ability to control the car remotely without line of sight or being anywhere near it, such as turning on the climate to warm up the car while still in a building so it's not freezing cold when we get back to the car with the kids, or while the car is in the garage, locking the car remotely because you're blocks away and aren't 100% sure you locked it before you left, opening the windows slightly so the car doesn't overheat in hot weather (Subaru doesn't do this unfortunately, but the Tesla does). The one thing it doesn't do which I wish it did, is roll up the windows remotely. reply n_plus_1_acc 2 hours agoparentprevEuropean eCall directive requires a sim card reply netsharc 5 hours agoprevHah, them being able to bypass the 2FA by commenting-out the line: $('#securityQuestionModal').modal('show'); is... mind-boggingly stupid of whoever got the job to write that Starlink web-app. OTOH, the hacker hijacked a Starlink employee's account to get in, isn't that over the line in terms of \"ethical hacking\"/legality standpoint? reply bean-weevil 4 hours agoparentIt may well be over the line, but it sounds like subaru is grateful for the report, so nothing will come of it. Definitely not a risk I would take. reply Mountain_Skies 5 hours agoparentprevBack when I used to do AppSec, these types of issues were extremely common. Software developers and their managers would argue endlessly about them not being real vulnerabilities, which meant I had to put together a proof of exploitability. And since these were interdepartmental fights, office politics get involved. Just one of the dozen or so reasons why I stopped doing AppSec and went back to development. reply _joel 4 hours agorootparentThat seems like a culture issue rather than an appsec issue? reply preisschild 4 hours agoparentprevOr not requiring ANYTHING to authenticate in your forgetPassword endpoint, but being able to set a new password directly instead of sending a randomly generated per email / send a one time token to reset the password yourself via email reply insane_dreamer 1 hour agorootparentThis one was especially egregious. reply _joel 4 hours agoparentprevGlad they had unit and integration tests to make sure that an unauthenticated user couldn't reset passwo... oh, wait... reply ben7799 2 hours agoprevI have a 2013 Outback Limited that is basically right before all this stuff got really stupid and weird. It's a great car other than it's not very fast and it gets really bad gas mileage. Amazing in the snow. I have had it since December 2012, so I've had plenty of service visits where I got newer loaners. (I special ordered my car to basically load it but not have Starlink, not have the Sunroof, but have the leather seats and the HK upgraded stereo.) Every time I have gotten a newer Subaru as a loaner it strikes me that they are worse cars for all this new stuff. The user interface is horrible in the new ones. In a lot of cases they have a skeumorphic interface up on the touch screen that mimics the physical controls in my car! The actual physical controls are about 100x faster to operate and you quickly learn where the buttons are without looking. I had an Ascent Onyx loaner last summer.. the entire touch screen UI looked like it was barely operating above 10fps. Just gross. Lots of the UI is black and white as well, not even tasteful grayscale. The Onyx I had also had the upgraded HK stereo and that is not as good as the one in my car as well, it sounded noticeably worse. The electric steering on the new Subarus is terrible as well. My old Outback is not exactly a sports car but getting out of new one back into mine it feels like you're getting into a Porsche or something when you feel the hydraulic steering. Engine/Turbo lag on a lot of the new ones is gross as well. This is of course even worse! My car only has 120k miles on it, I plan to keep it for another 4 years and then maybe give it to my kid when he gets his license. Somehow I doubt Subaru will have a competitive vehicle by then. For me to consider another one they'd really need to have an EV Outback/Forester/Ascent or a Hybrid version that gets at least 40mpg. And they need to fix all this horrible infotainment stuff in a way that the car operates better than a kids toy and actually drives well like an older Subaru. Also they need to get off the whole stupid thing with giant rims. It's supposed to be a Subaru, it needs to have tires appropriate to going relatively fast on dirt roads. reply reginald78 1 hour agoparentWe actually own two Outbacks, a 2011 and a 2019. Both my wife and I hate the touchscreen system in the 2019, it is full of irritating bugs and even the physical climate controls (which IIRC were going away for the 2020 model year) have horrible indicators of their status compared to the older one. I'd say the backup camera is a welcome addition for the newer one but if the roads are even remotely dirty the camera almost immediately becomes totally obscured rendering it useless, which around here occurs at least half the year. Combined with the battery drain issue I will probably not buy another one. At the most I'll give them a test drive to see if the control system has been returned to some semblance of sanity. Unfortunately all new cars seem to be privacy nightmares so I'm not sure how I'll avoid that. reply DwnVoteHoneyPot 2 hours agoparentprevIn addition to your comments, I think Subaru's all-wheel drive system has been switched to electrical instead of mechanical, making it worse. There are roller tests on youtube which show Subaru AWD being outperformed by Ford AWD systems. reply ben7799 2 hours agorootparentThey have different AWD systems in different vehicles and for some vehicles there is more than one system depending on which transmission you purchased. (At least when there was a choice) Mine is electrically controlled (and many Subarus are) but it's still connected full time. IME driving other electrically controlled non-full time systems what you feel in those are the electrically controlled clutch packs completely disconnect the rear wheels and the AWD is 100% disabled until the traction control system kicks in. Then you get a brief moment where the car feels out of control until the clutch activates the AWD. The tradeoff is that system that completely disconnects the rear wheels results in those vehicles (E.x. Honda/Toyota) getting much better fuel economy than Subarus as they operate as front-wheel drive almost all the time. I have never been in any Subaru that behaved that way. And a roller test is not where it matters anyway. Roller tests are contrived. Where you feel the difference between permanent AWD and part-time AWD is medium and high speed situations where the vehicle starts to lose control. Most people will never put any family crossover/SUV into a situation anywhere close to the roller tests or hill ascent tests. All of this seems to become completely meaningless with EVs being the future. reply olyjohn 1 hour agorootparentprevThe CVT in combination with the terrible traction control also kills any chance you have of getting out of a stuck situation. Subaru's AWD system is now mostly just marketing. So it's basically on part with most AWD systems, because most of them really are a joke. reply _huayra_ 2 hours agoprevFYI for Subaru owners, you can opt out and have your data deleted anywhere in the US (not just California): https://www.subaru.com/support/consumer-privacy.html It'll take ~6 months or so, but they will send you a confirmation email. reply jcomis 2 hours agoparentfwiw I have done this and received no confirmation or anything after more than 6 months. I keep submitting, maybe its working, but it doesnt seem to actually result in a confirmable change. for sure my retailer, which are 3rd parties according to that page, still has 100% access to the data, as they were able to tell my car was in another state when I called recently. seems pretty troubling reply nyokodo 2 hours agorootparent> I have done this and received no confirmation or anything after more than 6 months Sue them, make it a class action. reply simonlc 2 hours agoprevQuestion, if you can remote start a subaru with starlink, does that mean I could start my car from the command line during winter??? I don't pay for starlink, never really looked into it, but it sounds cheaper than installing a remote start system lol. reply t1234s 1 hour agoprevHaving developed back end portals like this one for much smaller companies I find it hard to believe that there is an open endpoint to reset a password without any type of verification. What goes wrong in development that this type of crap makes it to production? reply duxup 1 hour agoparentIt's weird, is there no feedback of any kind in these companies? All outsourced maybe? I don't get it. That would be a huge red flag and a fairly easy to understand / sell red flag. reply t1234s 14 minutes agorootparentEven if it was outsourced is there zero quality control? reply anarticle 5 minutes agorootparentOutsourcing means you want to impose an upper limit on the cost of the feature/software. Quality control may cost more, so yes! The acceptance criteria are: service portal needs to work so employees can do xyz. The design is terrible but \"works\"! Having to rearc after these terrible designs would cost more, so you can see where this is going... reply rjmunro 4 hours agoprev> After reporting the vulnerability, the affected system was patched within 24 hours and never exploited maliciously. How did they verify the never exploited maliciously part? Did the person who's password they changed ever notice that their password didn't work any more and report the problem? reply arrowleaf 1 hour agoparentAssuming `samwcurry@gmail.com` belongs to the same person who owns samcurry.net, they changed their own password. reply ziddoap 4 hours agoprevIs anyone aware of a list of affected models posted anywhere? All 2015+ models? Obviously the ability to pull up account history, previous owners, etc. is applicable to anyone with a Subaru. But I'm curious if location history shows up for people that have Subarus and never registered Starlink/never used the app. The author says: >but it seemed that we had agreed to the STARLINK enrollment when we purchased it. But it's not clear to me whether \"it\" refers to purchasing Starlink or purchasing the vehicle. reply Cerium 4 hours agoparentPurchasing the vehicle. reply stuff4ben 2 hours agoprevAs a DevSecOps/SRE whatever, I just gotta give props to the Subaru team for getting it patched within 24 hours. While it's just a small internal admin dashboard without real customer usage, the fact they acknowledged and fixed the issue so quickly speaks well of at least that part of Subaru IT. reply yapyap 4 hours agoprevBuying a car that is connected to the internet sounds horrific. reply jeroenhd 4 hours agoparentYou'll have a hard time avoiding it if you buy a new car: https://foundation.mozilla.org/en/privacynotincluded/categor... reply quesera 3 hours agorootparentWow, thanks for that link! I've been wishing for something exactly like this. Mozilla Foundation's \"Privacy Not Included\" product review site https://foundation.mozilla.org/en/privacynotincluded/ reply t0bia_s 4 hours agorootparentprevDon't buy a new one. Buy old one and remove all tracking devices if necessary. reply niij 5 hours agoprevHow do you disable this in a Subaru? Can you tell Subaru to turn it off or is there a low-effort way to disable it (fuse pull)? reply willis936 5 hours agoparentI can't speak for Subaru, but I did this on my Toyota last weekend (cell radio, not space radio). The fuse pull on Toyota DCM affected other systems (GPS, microphone). I got a hold of the service manual, identified the cell antenna cables, and simply unplugged them. Now \"simply unplugging them\" required ripping apart quite a few things and dealing with some annoyingly deep bolts, but it wasn't too terrible. Pulling the antenna cable is the right move. Fun fact on car GPS: it actually feeds back through carplay so your navigation gets worse without it. What I have yet to figure out is if the network connection is given to the car through carplay. reply stn8188 4 hours agorootparentThis is absolutely my biggest concern for when I inevitably am forced to replace our aging vehicles. The unfortunate reality of living in the Northeast is that it's near impossible to find decent (not rusted) older cars for sale. Sure, I'm within the 99.9999% of Americans who have nothing to hide, but that unequivocally does not justify the physical capability of remote monitoring or control. reply UniverseHacker 4 hours agorootparentWhy not just take a weekend to fly to where used cars are unrusted, and drive one back? Also, there are quite a few well made older cars that are fully galvanized and don’t rust. I know a few people in the Northeast still driving 1980s Volvo 740s, still unrusted after nearly 40 years of salty roads. 1987 and later had the best rustproofing. I believe many older Audis also are galvanized. reply potato3732842 4 hours agorootparentThe best galvanization was in the 80s when the OEMs were scrambling in the \"surprise, lead just got banned\" era (lead acts like an anode, think of it like a lesser version of a high zinc primer). That said, it's not the thick hot dip you see on I beams or on bolts at the hardware store and a modern car with modern coatings and attention to corrosion is still better. Of the three continents that produce a lot of cars the Europeans still take corrosion more seriously than anyone else. reply UniverseHacker 14 minutes agorootparentJust as a warning one needs to be willing to dish out the cash for one of these high end 80s European cars: they can sometimes cost hundreds of dollars. Really nice always garaged examples occasionally even cost more than their value by weight as scrap steel. And, although most are still going strong, it's possible some only have another 30-40 years of usable life left before needing major repairs, possibly even sooner if they have over a million miles already. A good running used engine or transmission for one can easily cost over $100 so one should also budget for that down the road as well. I just didn't want to get anyones hopes up on here, thinking they could easily afford one on a software engineer salary. reply tedmielczarek 3 hours agorootparentprevSince Volvo was mentioned in the parent comment, did you know that if you buy a new Volvo you can get two free plane tickets to Gothenburg + one night's hotel stay to go pick it up at the Volvo factory? You can drive it around Europe for a while and then they will ship it back to the US for you. AFAIK it doesn't cost extra, it just adds some lead time as well as time waiting for your car to get to you after you fly home: https://www.volvocars.com/us/l/osd-tourist/ reply zhengyi13 2 hours agorootparentOur family did nearly exactly this when I was a child in the 80s, just subbing in Volkswagen and Dusseldorf IIRC. It was some months later when the car actually arrived on the west coast of the US, but my parents seemed to think it worthwhile. reply bityard 4 hours agorootparentprevWould you be willing to share the details of this either publicly or privately? (My email is in my profile.) I'm trying to do the exact same thing on my 2021 Highlander and have not been able to find any info. I found threads where people have asked how to do this on forums and reddit, but the responses are ALL along the lines of, \"first, get a sheet of tinfoil, then shape it into a hat...\" etc etc. reply UniverseHacker 4 hours agorootparentprevDoes it actually never connect with the antenna disconnected? Most wireless receiving devices I’ve used will generally still work a bit with no antenna. reply willis936 4 hours agorootparentThe screen shows a no signal symbol. I seriously doubt that a cell protocol could lock with just an exposed coax. The truly paranoid could stuff a piece of foil in the coax connector to short it to ground. reply connicpu 4 hours agorootparentprevIn the case of starlink, it's completely useless without the phased array antenna because the signals are highly directional. reply Meneth 4 hours agorootparentSubaru Starlink is not the same as SpaceX Starlink. reply connicpu 4 hours agorootparentWell that's highly confusing. I'm surprised there wasn't a trademark dispute. reply aftbit 4 hours agorootparentprevWrong Starlink. Subaru's is cellular based, not space. reply numpad0 4 hours agorootparentprev(note: Subaru trademark \"STARLINK\"(2012) predates and is unrelated to SpaceX trademark \"Starlink\"(2017)) reply aftbit 4 hours agoparentprevIt depends on the car, but yes, you can pull a fuse. The component is called the DCM. If you do that though, you'll lose the front speakers and the microphone, which are routed through the DCM to support STARLINK-based cellular calls (like OnStar). The better solution is to disassemble the dash and replace the DCM with a bypass box, or yoink the cellular antenna cables as others have said. That's a bit of work for sure. I was lucky to have it done by Subaru under warranty service, in response to the 3G cell shutdown causing the DCM to malfunction and drain the battery. Here's a chart of DCM fuse location by vehicle: https://www.subaruoutback.org/threads/dcm-fuse-location-2020... reply godber 4 hours agoprevThis claims to bypass the telematics functionality: https://www.autoharnesshouse.com/69018.html > Note for customers retaining OEM headunit: This adapter can also be used for those wishing to remove/disable the OEM Subaru Telematics functions. This is done to eliminate the tracking cabability that Subaru has built into these vehicles. If this is you, we will need to add an additional part to this adapter to re-enable the bluetooth microphone. Please purchase the option 2 adapter near the bottom of this page for this situation. reply bityard 4 hours agoparentIs there anything like this for Toyota DCM? We bought a second-hand 2021 Highlander and thus did not sign any contract allowing our family to be tracked by Toyota. I went on a hunt recently for information on neutering the DCM but have thus far only found speculation and contradictory info. reply willis936 2 hours agorootparentYeah it's bad out there. Don't do what the yahoos hacking up their harnesses have done. The Toyota DCM I'm familiar with has 3 coax antenna lines coming in. The outer 2 are cell and the inner is GPS. Pull the cell antenna cables out of the DCM and you should be good to go. Best to hunt down your vehicle's service manual and verify the procedure first. reply LandoCalrissian 2 hours agoparentprevAwesome, thank you. Seems pretty straight forward. reply godber 4 hours agoparentprevThe info on that page seems a bit contradictory, but maybe I need more coffee. reply panki27 3 hours agoprevThis is even worse than the VW data leak reported around a month ago. [1] [1] https://media.ccc.de/v/38c3-wir-wissen-wo-dein-auto-steht-vo... reply naitgacem 2 hours agoparentDid you perhaps forgot to edit and add the link? I'm interested in hearing about that reply TechDebtDevin 2 hours agorootparentI hope you speak German. reply jazzyjackson 1 hour agorootparentIf you download the .mp4 you will notice it is 'deu-eng-fra', there is an English overdub available as a separate audio track. reply t0bia_s 4 hours agoprevImagine that manufacturer can do that without any hacks and your knowledge about data collecting. Now imagine that you sell those cars to foreign countries that your government consider as enemy. I'm curious when there will be some ban of car brand, like TikTok. reply longtimelistnr 3 hours agoparentIsn't this already in place? I thought this is exactly why Chinese EV's haven't flooded the market yet. reply TechDebtDevin 2 hours agoparentprevChinese cars cannot connect to the cloud in the USA. reply sc68cal 2 hours agoprevThis really reinforces my aversion to internet connected cars. They used one to kill Michael Hastings. reply tadhgpearson 3 hours agoprevI love the variety of tooling and joining the dots to complete this attack: dns + scanning + human factors research + html bypass on the admin site itself... reply therealfiona 1 hour agoprevThat's it... I'm not buying a car with any internet connection unless I can rip it out. And every day that's going to get harder. Guess I'll stick with old Kai Vans... reply yellow_lead 58 minutes agoprevNo bounty for such a big vulnerability is unbelievable. reply plagiarist 5 hours agoprevI wish that keeping this much data was a liability. I want companies to be liable for damages in the millions of dollars if they share an entire year's worth of location data without express permission from the vehicle owner. HIPAA for \"just\" PII. reply neeeeeeal 5 hours agoparentThis is the way. What legitimate interest could Subaru have in maintaining this much data about their customers? reply genewitch 4 hours agorootparentThe most charitable guess I can make is that they use it to improve their driver assist, lane keeping, pacing, and that sort of thing. location and g force and direction when the automated system shuts off and returns control to the driver, that sort of thing. I don't agree with it, but that would be my guess. I own a Subaru that does this, so I'm not happy about it, but what can I do? That's rhetorical. reply TheCondor 3 hours agorootparentprevAre you new? That stuff is probably more valuable than many of us want to admit. There is the maybe more noble value: training data for maps, traffic analysis AI, engineering duty cycle data, things like that. Then there are the other uses, for example various surveys and studies are needed for new roads or signal changes, can this kind of data proxy for that? We would be talking about cutting millions of dollars out of some of these projects and months or even years off a timeline. Then the ad-tech, where do you put billboards and signage? Where do you build a shop? Probably other uses we aren’t even thinking about. reply ndileas 4 hours agorootparentprevUnfortunately, selling it to repo men is a widely accepted practice. reply mschuster91 4 hours agorootparentprevThe same thing all car manufacturers are after... AI. And I'm not joking this time. Cars have become a commodity, especially since China made their first vehicles that didn't get outright banned in Europe for being too unsafe to be roadworthy, and even some nominally \"entry level\" cars have more horsepower under the hood than a 1990s 7-series BMW (138 kW). Strict requirements on emissions, fuel consumption and crash safety have all but eliminated differences in optics (the amount of shapes is finite). So the only thing left to differentiate other than build quality (where China is rapidly catching up) is assistance systems... and there, AI is the hot craze, and AI only works when it has insane amounts of data to gobble up. reply inetknght 5 hours agoparentprev> I want companies to be liable for damages in the millions of dollars if they share an entire year's worth of location data without express permission from the vehicle owner. Moreover, not just millions of dollars in aggregate, but millions of dollars per individual customer whose privacy was violated. reply TheJoeMan 4 hours agorootparentIf I collected this much information about a single individual, I would go to jail for stalking. But with the wonders of technology, I can stalk \"at scale\"! reply chatmasta 5 hours agoprevFor those who might not read the article, note that “Starlink” is not the SpaceX service. It’s an internal name for a Subaru customer service web app. reply wolrah 5 hours agoparent> It’s an internal name for a Subaru customer service web app. It's actually the name of Subaru's connected infotainment system: https://www.subaru.com/vehicle-info/subaru-starlink.html reply blackeyeblitzar 3 hours agoprevA shocking thing about Subaru cars with Starlink (their infotainment system and connected service for things like remote start) is how deep the violation of privacy is. For example they share your location data with Sirius XM by default, unless you go deep in their menus to realize it’s even happening and opt out. They bury the consent in fine print that you fly through at the dealership. Truly a despicable company. reply oremolten 1 hour agoparentall car companies do this since around 2011.(in the us) they also immediately sell all this data to car insurance companies. many apps on your phone that use location data for anything are mostly using an APK that includes a couple car insurance companies code that also just directly shares that with them. (they made the location APK for app makers to have an easy to use location data tool) reply high_na_euv 3 hours agoprev [–] What a shitshow! reply Consider applying for YC's Spring batch! Applications are open till Feb 11. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A security vulnerability in Subaru's STARLINK service was discovered on November 20, 2024, allowing unauthorized access to vehicles and customer accounts in the US, Canada, and Japan. The flaw enabled attackers to remotely control vehicles and access personal information using minimal data, such as a last name and ZIP code, due to issues in the STARLINK admin panel. The vulnerability was reported and patched within 24 hours without any malicious exploitation, underscoring the challenges in securing connected car systems due to broad access permissions."
    ],
    "commentSummary": [
      "A vulnerability in Subaru's Starlink system was discovered, allowing hackers to track and control vehicles, but it was patched within 24 hours without any malicious exploitation.",
      "Concerns persist regarding data collection and remote access by Subaru and its partners, with some users reporting battery drain issues linked to the system.",
      "The incident underscores the risks associated with internet-connected cars, emphasizing the need for improved user privacy protections and clearer opt-out processes for data collection."
    ],
    "points": 196,
    "commentCount": 142,
    "retryCount": 0,
    "time": 1737634939
  },
  {
    "id": 42802498,
    "title": "Where is London's most central sheep?",
    "originLink": "https://diamondgeezer.blogspot.com/2025/01/londons-most-central-sheep.html",
    "originBody": "diamond geezer Wednesday, January 22, 2025 It's time to tackle one of London's great unanswered questions. Where is London's most central sheep? I'm only interested in live sheep, so not a cuddly toy in Hamleys nor lamb cutlets at The Ritz. I'm not interested in temporary sheep like those that get driven over Southwark Bridge in September or shorn at the Lambeth Country Show in June. Also by 'most central' I mean closest to the centre of London which is generally agreed to be Trafalgar Square, specifically the statue of Charles I at the top of Whitehall. Hopefully that's clear. I don't believe Charles III keeps sheep at Buckingham Palace, nor has anybody else nearby got a large enough back garden. London Zoo's website does not reveal the existence of any sheep at best llamas. Also none of the armed forces based in London have a regimental sheep, the UK's sole ovine mascot being a ram called Pte Derby XXXIII owned by the Mercian Regiment in Lichfield. So, city farms it is. Where is London's most central city farm? That's easy, it's Vauxhall City Farm which is just over a mile south of Trafalgar Square. It's been here on the edge of the Pleasure Gardens since 1976 so is one of London's oldest city farms and receives over 60,000 visitors a year. Some of its residents live out front in wooden pens but they're not sheep, they're goats as any self-respecting three year old could tell you. The entrance is off to the left past an outdoor desk staffed by cheery volunteers who'll grin, sell you feed and encourage you to make a donation. The City Farm is 50 next year so has an anniversary appeal underway, should you have part of £250,000 to spare. For the sheep turn right. Where is London's most central sheep? Here she is. She's in the sheep enclosure at Vauxhall City Farm, lapping away at a bowl of water resting on a spare tyre. She's a Shetland, a hardy breed with a good-natured temperament, so ideal for pottering around with toddlers in a confined space. There were many such underage visitors during my visit, all overexcited to be right up close to a sheep's head nuzzling through railings. Crossing the divide into the yard itself is more of a paid-for activity, or if you're a volunteer just part and parcel of your dung-sweeping duties. Alas I don't know what this sheep's name is, the City Farm isn't as keen as some in pinning biographical details to the railings, but there is no closer sheep to Trafalgar Square so she is London's most central sheep. Where is London's second most central sheep? You didn't think there was just one sheep did you? Here's another, this time a Herdwick, a larger shaggier breed. She's called Daffodil, or Daffy for short, and she's very much the poster girl for sheep at Vauxhall. I didn't see her initially, she was hiding inside a free-standing wooden structure as if trying to escape the hubbub, the yard also being home to three occasionally frisky alpacas called Rolo, Toffee and Cookie. I suspect sometimes Daffy hops up the steps to the top platform and surveys her domain like a woolly empress. She is thus not always the second most central sheep in the capital, sometimes she's first depending on the precise location of the other sheep. Where is London's third most central sheep? There are of course several sheep at Vauxhall, it would be silly and a tad cruel only to have two. Here are four of them crowding up against the railings by the pigpen because a small child has proffered a handful of scrummy animal feed. School parties tend to be taller and thus less conducive to low-level feeding, plus they may be about to be given a wheelbarrow each and asked to get stuck in. This particular corner of the farmyard by the entrance is actually the furthest north so I can state with some certainty that the back ewe was London's most central sheep at the time I took this photo, and next to her the second, third and fourth. Where is London's fifth most central sheep? Daffy didn't appear in that last photo so she was London's fifth most central sheep at the time. Where is London's sixth most central sheep? Also at Vauxhall City Farm. I counted seven sheep in the yard altogether, I just never managed to get them all in the same photo. I did try asking the keeper to confirm the precise number of sheep but unfortunately she didn't know. Whatever, given my count it must be the case that London's seven most central sheep are all here in SW8... and free to visit. Be sure to wash your hands thoroughly before exiting past the geese and buggy park, and yes the acrid whiff of animal excreta does eventually fade as you return to the outside world. Where is London's eighth most central sheep? Park that question for a second. Where is London's second most central city farm? That'll be in Spitalfields. It took some working out to confirm that this was the second closest to Trafalgar Square, I had to make myself a map using the extremely helpful list of London's city farms at londonfarmsandgardens.org.uk. They reckon there are twelve city farms in London but I reckon one of those is just over the border in Essex so it's eleven. The map's interesting because eight of the city farms form a near straight line running diagonally from Kentish Town through Hackney and Mudchute to the foot of Shooters Hill, but I think that's a coincidence. Spitalfields City Farm is on the site of a former railway depot and was also born in the 1970s, but is less cramped, easier to walk round and less pungent. Where is London's eighth most central sheep? That'll be Beatrix, another Herdwick ewe, here at Spitalfields City Farm. Their information game is strong so I know she used to graze on the North Downs in Surrey but lost an ear in a dog attack when she was young and moved here in August 2020. Her enclosure is a much better size, with scattered wood and the inevitable spare tyre, even room for gambolling. Don't expect to get close enough for feeding but that's fine because feeding's not permitted here anyway. Where is London's ninth most central sheep? You didn't think there was just one sheep at Spitalfields City Farm did you? There are several more hiding away in the byre, this time Castlemilk Moorits, a rare breed with brownish wool originally from Scotland. They're 37% Shetland, 28% Soay, 18% Manx and 17% Wiltshire Horn and all descended from a single ram on Sir Jock Buchanan-Jardine's estate, apparently. The information board also confirms there are nine of them here altogether with names like Twiglet, Lavender, Samphire and Rolo. Rolo is occasionally London's seventeenth most central sheep when he stands over by the polytunnels. London's most central donkeys are two pens away, one of whom is called Derek, but that's another story. Thursday update: Due to errors in this post, I've published an apology here. posted 07:00 : comments (27) > click to return to the main page ...or read more in my monthly archives Jan25 Jan24 Feb24 Mar24 Apr24 May24 Jun24 Jul24 Aug24 Sep24 Oct24 Nov24 Dec24 Jan23 Feb23 Mar23 Apr23 May23 Jun23 Jul23 Aug23 Sep23 Oct23 Nov23 Dec23 Jan22 Feb22 Mar22 Apr22 May22 Jun22 Jul22 Aug22 Sep22 Oct22 Nov22 Dec22 Jan21 Feb21 Mar21 Apr21 May21 Jun21 Jul21 Aug21 Sep21 Oct21 Nov21 Dec21 Jan20 Feb20 Mar20 Apr20 May20 Jun20 Jul20 Aug20 Sep20 Oct20 Nov20 Dec20 Jan19 Feb19 Mar19 Apr19 May19 Jun19 Jul19 Aug19 Sep19 Oct19 Nov19 Dec19 Jan18 Feb18 Mar18 Apr18 May18 Jun18 Jul18 Aug18 Sep18 Oct18 Nov18 Dec18 Jan17 Feb17 Mar17 Apr17 May17 Jun17 Jul17 Aug17 Sep17 Oct17 Nov17 Dec17 Jan16 Feb16 Mar16 Apr16 May16 Jun16 Jul16 Aug16 Sep16 Oct16 Nov16 Dec16 Jan15 Feb15 Mar15 Apr15 May15 Jun15 Jul15 Aug15 Sep15 Oct15 Nov15 Dec15 Jan14 Feb14 Mar14 Apr14 May14 Jun14 Jul14 Aug14 Sep14 Oct14 Nov14 Dec14 Jan13 Feb13 Mar13 Apr13 May13 Jun13 Jul13 Aug13 Sep13 Oct13 Nov13 Dec13 Jan12 Feb12 Mar12 Apr12 May12 Jun12 Jul12 Aug12 Sep12 Oct12 Nov12 Dec12 Jan11 Feb11 Mar11 Apr11 May11 Jun11 Jul11 Aug11 Sep11 Oct11 Nov11 Dec11 Jan10 Feb10 Mar10 Apr10 May10 Jun10 Jul10 Aug10 Sep10 Oct10 Nov10 Dec10 Jan09 Feb09 Mar09 Apr09 May09 Jun09 Jul09 Aug09 Sep09 Oct09 Nov09 Dec09 Jan08 Feb08 Mar08 Apr08 May08 Jun08 Jul08 Aug08 Sep08 Oct08 Nov08 Dec08 Jan07 Feb07 Mar07 Apr07 May07 Jun07 Jul07 Aug07 Sep07 Oct07 Nov07 Dec07 Jan06 Feb06 Mar06 Apr06 May06 Jun06 Jul06 Aug06 Sep06 Oct06 Nov06 Dec06 Jan05 Feb05 Mar05 Apr05 May05 Jun05 Jul05 Aug05 Sep05 Oct05 Nov05 Dec05 Jan04 Feb04 Mar04 Apr04 May04 Jun04 Jul04 Aug04 Sep04 Oct04 Nov04 Dec04 Jan03 Feb03 Mar03 Apr03 May03 Jun03 Jul03 Aug03 Sep03 Oct03 Nov03 Dec03 Jan02 Feb02 Mar02 Apr02 May02 Jun02 Jul02 Aug02 Sep02 Oct02 Nov02 Dec02 Life viewed from London E3 » email me » follow me on twitter » follow the blog on Twitter » follow the blog on RSS » my flickr photostream twenty blogs our bow arseblog ian visits londonist broken tv blue witch on london the great wen edith's streets spitalfields life linkmachinego round the island wanstead meteo christopher fowler the greenwich wire bus and train user ruth's coastal walk round the rails we go london reconnections from the murky depths quick reference features Things to do in Outer London Things to do outside London London's waymarked walks Inner London toilet map 20 years of blog series The DG Tour of Britain London's most... read the archive Jan25 Dec24 Nov24 Oct24 Sep24 Aug24 Jul24 Jun24 May24 Apr24 Mar24 Feb24 Jan24 Dec23 Nov23 Oct23 Sep23 Aug23 Jul23 Jun23 May23 Apr23 Mar23 Feb23 Jan23 Dec22 Nov22 Oct22 Sep22 Aug22 Jul22 Jun22 May22 Apr22 Mar22 Feb22 Jan22 Dec21 Nov21 Oct21 Sep21 Aug21 Jul21 Jun21 May21 Apr21 Mar21 Feb21 Jan21 Dec20 Nov20 Oct20 Sep20 Aug20 Jul20 Jun20 May20 Apr20 Mar20 Feb20 Jan20 Dec19 Nov19 Oct19 Sep19 Aug19 Jul19 Jun19 May19 Apr19 Mar19 Feb19 Jan19 Dec18 Nov18 Oct18 Sep18 Aug18 Jul18 Jun18 May18 Apr18 Mar18 Feb18 Jan18 Dec17 Nov17 Oct17 Sep17 Aug17 Jul17 Jun17 May17 Apr17 Mar17 Feb17 Jan17 Dec16 Nov16 Oct16 Sep16 Aug16 Jul16 Jun16 May16 Apr16 Mar16 Feb16 Jan16 Dec15 Nov15 Oct15 Sep15 Aug15 Jul15 Jun15 May15 Apr15 Mar15 Feb15 Jan15 Dec14 Nov14 Oct14 Sep14 Aug14 Jul14 Jun14 May14 Apr14 Mar14 Feb14 Jan14 Dec13 Nov13 Oct13 Sep13 Aug13 Jul13 Jun13 May13 Apr13 Mar13 Feb13 Jan13 Dec12 Nov12 Oct12 Sep12 Aug12 Jul12 Jun12 May12 Apr12 Mar12 Feb12 Jan12 Dec11 Nov11 Oct11 Sep11 Aug11 Jul11 Jun11 May11 Apr11 Mar11 Feb11 Jan11 Dec10 Nov10 Oct10 Sep10 Aug10 Jul10 Jun10 May10 Apr10 Mar10 Feb10 Jan10 Dec09 Nov09 Oct09 Sep09 Aug09 Jul09 Jun09 May09 Apr09 Mar09 Feb09 Jan09 Dec08 Nov08 Oct08 Sep08 Aug08 Jul08 Jun08 May08 Apr08 Mar08 Feb08 Jan08 Dec07 Nov07 Oct07 Sep07 Aug07 Jul07 Jun07 May07 Apr07 Mar07 Feb07 Jan07 Dec06 Nov06 Oct06 Sep06 Aug06 Jul06 Jun06 May06 Apr06 Mar06 Feb06 Jan06 Dec05 Nov05 Oct05 Sep05 Aug05 Jul05 Jun05 May05 Apr05 Mar05 Feb05 Jan05 Dec04 Nov04 Oct04 Sep04 Aug04 Jul04 Jun04 May04 Apr04 Mar04 Feb04 Jan04 Dec03 Nov03 Oct03 Sep03 Aug03 Jul03 Jun03 May03 Apr03 Mar03 Feb03 Jan03 Dec02 Nov02 Oct02 Sep02 back to main page the diamond geezer index 2024 2023 2022 2021 2020 2019 2018 2017 2016 2015 2014 2013 2012 2011 2010 2009 2008 2007 2006 2005 2004 2003 2002 my special London features a-z of london museums E3 local history month greenwich meridian (N) greenwich meridian (S) the real eastenders london's lost rivers olympic park 2007 great british roads oranges & lemons random boroughs bow road station high street 2012 river westbourne trafalgar square capital numbers east london line lea valley walk olympics 2005 regent's canal square routes silver jubilee unlost rivers cube routes Herbert Dip metro-land capital ring river fleet piccadilly bakerloo ten of my favourite posts the seven ages of blog my new Z470xi mobile five equations of blog the dome of doom chemical attraction quality & risk london 2102 single life boredom april fool ten sets of lovely photos my \"most interesting\" photos london 2012 olympic zone harris and the hebrides betjeman's metro-land marking the meridian tracing the river fleet london's lost rivers inside the gherkin seven sisters iceland just surfed in? here's where to find... diamond geezers flash mob #1 #2 #3 #4 ben schott's miscellany london underground watch with mother cigarette warnings digital time delay wheelie suitcases war of the worlds transit of venus top of the pops old buckenham ladybird books acorn antiques digital watches outer hebrides olympics 2012 school dinners pet shop boys west wycombe bletchley park george orwell big breakfast clapton pond san francisco thunderbirds routemaster children's tv east enders trunk roads amsterdam little britain credit cards jury service big brother jubilee line number 1s titan arum typewriters doctor who coronation comments blue peter matchgirls hurricanes buzzwords brookside monopoly peter pan starbucks feng shui leap year manbags bbc three vision on piccadilly meridian concorde wembley islington ID cards bedtime freeview beckton blogads eclipses letraset arsenal sitcoms gherkin calories everest muffins sudoku camilla london ceefax robbie becks dome BBC2 paris lotto 118 itv",
    "commentLink": "https://news.ycombinator.com/item?id=42802498",
    "commentBody": "Where is London's most central sheep? (diamondgeezer.blogspot.com)183 points by GeoAtreides 9 hours agohidepastfavorite106 comments steerpike 8 hours agoWhen my wife and I lived in Bristol we developed a metric designed to measure how enjoyable a city was to live in that we called \"time to sheep\". Basically it's a measure of how long you have to travel from the center of the city before you're in the English countryside surrounded by sheep and the best cities have a low (but not too low) \"time to sheep\" metric. It helped explain one of the reasons we loved living in Bristol so much when we had such a hard time living in London. Could also have been that Bristol is just a crazy beautiful city to live in, but where's the fun in that, right? reply barrkel 3 hours agoparentThis only makes sense if you enjoy the English countryside. I'm an Irishman. I grew up in the countryside, in the west, and spent 15 years living in London in my 20s and 30s. I can count on one hand the number of visits to the English countryside I made that weren't on the back of a motorcycle, and then, I didn't stop except for petrol. The city is what I enjoyed, the chaos, the diversity, ambition, variety. No smaller city would be as good. reply baxtr 3 hours agorootparentYour preferred metric is \"time to chaos\" I guess then? reply xenocratus 49 minutes agorootparent\"Time to something I haven't seen or experienced in the past 3 years\" Having spent more than 5 years in small towns, London has fixed my utter boredom. reply pyrale 1 hour agorootparentprevTime to cow dung, the higher the better obviously. reply thaumasiotes 29 minutes agorootparentThat's also a problem with sheep, you know. reply walthamstow 1 hour agorootparentprevtime to chicken shop reply j4coh 1 hour agorootparentprevTime to sidewalk puke reply xenocratus 51 minutes agorootparentYes, obviously no small town has sidewalk puke, surely not in England! reply dairylee 7 hours agoparentprevAlthough it's not quite sheep Newcastle has a Town Moor (Larger than Central Park) which has grazing cattle. There's also a farm not too far from the city centre which has grazing sheep. https://en.wikipedia.org/wiki/Town_Moor,_Newcastle_upon_Tyne https://www.bbc.co.uk/programmes/p03hm60d/p03hm2x8 https://maps.app.goo.gl/vojeS3eDTFznpYwMA reply gambiting 6 hours agorootparentI was going to say, I'm sure Desmond Dene has sheep there. reply rantallion 5 hours agorootparentIf you mean Jesmond Dene, there's a petting zoo with a few small beasts and birds. I know they have a couple of breeds of goat but I don't recall seeing any sheep on my last visit (within the last month). reply gambiting 3 hours agorootparentHa, yes thank you autocorrect. And they had sheep when I last visited couple years ago, maybe they got rid of them now. reply scott_w 2 hours agorootparentYou still don't need to go too far, there's a fair few farms just outside Ponteland that have grazing sheep and I'll regularly cycle past farmers with their collies on the quad bikes on a Sunday morning. reply noneeeed 5 hours agoparentprevI live in Bath, so quite a bit smaller than Bristol, but I really apprecaite the fact that we can be in the city centre in half an hour, or in the countryside in 15 minutes. If I didn't live in Bath I'd probably live in Bristol, it's a great city. And I absolutely agree that it's kind of the perfect size for a metropolitan area. I think a lot of London is saved by having so many parks, and so many large parks and commons. I know Paris has a lot less green space than London and when I visited I definitely felt that. reply tonyedgecombe 5 hours agorootparentI like Bristol but the traffic is so bad. It desperately needs a tram system. Bath is nice though, my son lives there and we love visiting. reply noneeeed 4 hours agorootparentHah, very true. I never drive there for a reason. reply fiftyacorn 7 hours agoparentprevEdinburgh used to have 2000 sheep on Arthurs seat right in the center of town until the early 80s. There were urban legends about student pranks of putting sheep into the halls of residence rooms reply xemdetia 23 minutes agoparentprevI find this fun because I always described this metric as 'time to cow.' I suppose a sheep is fine too. reply f4c39012 8 hours agoparentprevthere's also a measure of the minimum appropriate \"time to sheep\" https://en.wikipedia.org/wiki/List_of_humorous_units_of_meas... reply bryanlarsen 2 hours agoparentprevIf your metric is time or distance to large amounts of nature, I recommend Ottawa, Canada where the 140 square mile Gatineau Park starts 5 miles from downtown. reply marbs 4 hours agoparentprevI like that metric. If we instead consider \"time to cows\" then Cambridge does quite well. Midsummer Common, Stourbridge Common and The Backs have (seasonal) cows. https://www.bbc.co.uk/news/uk-england-cambridgeshire-5616806... https://www.hiddencambridge.uk/#summer reply oneeyedpigeon 6 hours agoparentprevThere needs to be a counterbalancing variable, though; presumably you want to live in a city, otherwise you'd just live in the countryside somewhere with a TTS of zero :) Maybe the other factor is \"time for pizza to arrive at door\"? reply riffraff 6 hours agorootparentthere's presumably pizza in the smallest towns tho, I'd suggest Time To Theatre. Not because of the Theatre per se, but because \"big enough to have a theatre\" is probably a good proxy of \"big enough to be appealing to people who enjoy something other than nature\". reply macintux 5 hours agorootparentIn Indiana, my favorite dinner theater is in a town of 500 people. For a while, someone was trying to start a theater in an even more remote spot, an unincorporated community about an hour from there with maybe a dozen homes nearby, but they finally moved it to a large town. reply bryanlarsen 1 hour agorootparentI think by \"theater\" the OP was implying professional theatre. Lots of small towns have theater, but professional theatre is a much higher bar. In my case, my old workplace in Ottawa, Canada had a \"time to moose\" of about 5 miles and a \"time to theatre\" of about 1/2 a mile. Sadly, the professional Opera company in Ottawa went bankrupt so we only have amateur Opera now, but we do get regular professional Broadway productions so it still counts. reply themaninthedark 1 hour agorootparent>\"time to moose\" of about 5 miles and a \"time to theatre\" of about 1/2 a mile. Unit of measure error: unit specified it time, unit supplied is distance :P reply mr_toad 4 hours agorootparentprevWalking distance from the Pub to home. More seriously, time taken to get to work. reply beeforpork 4 hours agoparentprevWhen I was in Bristol, the smell of burned weed was frequent. More frequent than other cities, I think. reply tonyedgecombe 4 hours agorootparentBristol has the highest traces of cocaine in its sewage. https://www.itv.com/news/2019-03-14/which-uk-city-tops-list-... reply alt227 2 hours agorootparent> Bristol was the only UK city participating in last year’s research. London’s wastewater, which has previously topped the cocaine chart, was not included. reply yapyap 4 hours agorootparentprevMight’ve also been a secret bonus in the TTS metric reply trgn 2 hours agoparentprev> Bristol is just crazy beautiful city to live in Curious, because of geography? architecture? reply reidrac 1 hour agorootparentI'd like to know as well. I live in Bristol and is alright, but it may depend on which part of Bristol are we talking about ;) reply skipants 3 hours agoparentprevThat's amazing. Now I want to see how relevant a \"time to cow\" metric is to Canadian cities. reply pomian 2 hours agorootparentThat's an easy one. We call it \"time to moose\"! reply Dilettante_ 1 hour agorootparentThat's definitely a metric you do *not* want to hit zero reply kabouseng 1 hour agoparentprevIn Africa I suppose we have time to lion... reply alt227 2 hours agoparentprev> a metric designed to measure how enjoyable a city was to live in Your metric of how enjoyable a city is to live in is based on how long it takes to leave that city? The logical endpoint of that is moving to the countryside where the TTS = 0, which is very easy to achieve. Begs the question, why are you even living in a city at all?! reply simmonmt 1 hour agorootparentYou can enjoy living in a city but also enjoy outdoorsy pursuits. TTS is a measure of your ability to do both. It can also be a measure of the maximum size of city you enjoy. There are people who like cities but still wouldn't want to live in NY/LA/London reply wheybags 8 hours agoparentprevMy metric for when you've left the city is \"have I passed a field of potatoes\" reply usrusr 6 hours agorootparentHere in Germany I run an inverse of that for \"am I in the wider halo of a larger city or am I in a truly rural environment\": when approaching a metropolitan area, the outer urban halo starts where there are still farms, but many of them have switched to housing horses. reply sevensor 2 hours agorootparentprevWhat I found striking about Seoul was that there would be three rows of potatoes in between a ten story apartment block and a busy highway. Not a square meter wasted on unproductive grass. reply jfk13 3 hours agorootparentprevHow large does the field have to be? I grow some in my back garden...does that count as zero, then? reply 369548684892826 7 hours agorootparentprevThis probably works best in Idaho reply vanderZwan 7 hours agorootparentPOTATO LAND! POTATO LAND! [0] https://www.youtube.com/watch?v=xWtLw83_jE0&t=426s [1] https://www.youtube.com/watch?v=dwF3e78j7pw (official African mirror) [2] https://www.youtube.com/watch?v=V8Jff85kMeU (official Asian mirror) (PS: curious how the video resolution gets lower with each channel) reply walrus01 7 hours agorootparentprevIn the broadest possible sense, Idaho can be divided into \"potato\" and \"non-potato\" Idaho. For instance if you drive US95 through here (the creatively named Idaho County, Idaho) it's almost entirely wheat farms, and what isn't a wheat farm is either forest, wilderness or cattle ranch. The potato part doesn't really start until you get down into the whole valley/flat land area occupied by Meridian, Boise, Nampa, etc. https://www.google.com/maps/place/Grangeville,+ID+83530,+USA... reply reaperducer 5 hours agorootparent(the creatively named Idaho County, Idaho) New York County, New York says hi. (You may know it as \"Manhattan.\") reply NoMoreNicksLeft 3 hours agoparentprevI like your metric. I aim to get my time-to-sheep number down below 60 seconds. But if you mean \"within a car driving down the road's distance to sheep\", then I aim to get it to 0. reply sandworm101 3 hours agoparentprevA better metric imho would be time to a wild animal. I'd go with distance to a wild bear, or anything else that could threaten a human. That is where wilderness starts imho. For London, that measurement is likely hundreds of miles. In much of north america, it is probably be less than one. I've been to the English countryside. It is more city park than open country. reply themaninthedark 1 hour agorootparentWe had a wild bear with cubs go into the dumpster for food at our university campus, this is North America of course. I don't know London at all but I would hazard that you have foxes and other wild animals living in the city, just well hidden. We have coyotes that have taken up residence in many American cities. reply wmanley 1 hour agorootparentprevRichmond park has Adders and Deer, both of which have the potential to kill you but in practice would be very unlikely to. To get to the nearest wild wolf you'd probably have to look as far as the Ardennes in Belgium, which is roughly 400km away. For bears you'd probably be looking at 1000km or so in the Pyrenees on the French/Spanish border. reply bryanlarsen 2 hours agorootparentprevI've had bears in the ravine in my back yard but I don't think that really counts, it's still urban. But ~200 bears do live in Gatineau Park, a 140 square mile piece of fairly untouched nature that starts 5 miles from downtown Ottawa, Canada. reply walrus01 8 hours agoparentprevPresumably this could be quantified through a call to something like the Google maps api for a specific lat/long starting point, for driving, walking or biking time in minutes, as an SLP (sheep latency protocol) reply mxfh 7 hours agoprevIf you're in Berlin, especially with kids, there more than a dozen of children's farmyards (not counting zoos and actual farms) all quite central in multi-centric Berlin. Mostly for (early) childhood educational purposes, so they are are prime spot for Kindergarten day trips. https://www.berlin.de/kultur-und-tickets/tipps/kinder/kinder... https://www.visitberlin.de/en/farms-children Since most of them are in former West-Berlin the reason they exist can likely be explained from a mixture of empty lots not rebuild after the bombing in WWII, historical farms in outer Groß-Berlin (Domäne Dahlem) and the impracticality of casual trips into the countryside with kids beyond the wall. reply Xophmeister 4 hours agoprevLooks like there's an errata, already :) https://diamondgeezer.blogspot.com/2025/01/a-sheepish-apolog... reply jrmg 4 hours agoparentSome good advice at the end of that: 1) Don't state something as fact when you haven't researched it fully. 2) Remember that when you do state something because you believe it's fact, it could be based on incomplete information. 3) If you're not 100% sure about something, best introduce at least some element of doubt. 4) Don't trust everything you read just because somebody you trust presented it as fact. reply asjir 1 hour agorootparentI think this was overly self-critical what would researching fully even mean? They can have however many sheep they want hidden within one-mile distance from Trafalgar square, no-one would expect the author to scour every possible location ensuring that there is no sheep hidden lol just to make a funny post 100% sure to be true reply nottorp 2 hours agorootparentprevIt also says \"But they might have been goats.\". reply throw0101c 5 hours agoprevIf anyone is in/visiting Toronto, Canada, there's a farm right downtown: * https://riverdalefarmtoronto.ca * https://en.wikipedia.org/wiki/Riverdale_Farm * https://www.blogto.com/city/2018/09/riverdale-farm-toronto/ reply mattkevan 1 hour agoprevWe used to live just a few minutes from the Oasis Waterloo farm and I can confirm there are indeed sheep. Pigs too. For being so central it was surprisingly rural there were horse stables and an apple orchard with all kinds of rare varieties just over the road from our flat. As an aside, a family member recently became a Freeman of the City of London which means they're officially allowed to drive sheep over London bridge. reply eitally 1 hour agoprevI can't speak for further up the peninsula, but the nearest sheep to me in San Jose is almost certainly at Emma Prusch Farm Park (http://www.pruschfarmpark.org/), which like the city farms mentioned in the blog, is a nonprofit farm in the middle-ish of the city. Depending where you live in SJ, though, you may be closer to Happy Hollow Park & Zoo[2], which also has sheep. From there you get into actual farms, and there are at least two within a few miles: Ray of Sunshine Farm in Almaden Valley (mostly focus on produce & goat cheese, but do have sheep): https://www.rayofsunshinefarm.com/ and Deer Hollow Farm in Cupertino (another non-profit): https://deerhollowfarmfriends.org/ From there, there are a bunch more sheep farms heading south toward Gilroy & Hollister, but that's getting a bit too far afield. Frankly, I'm actually surprised there were so many civic farms in the south bay. That said, for south & east bay folks, I think it would be a lot more interesting to track the roaming herds of goats used to keep dry grass & brush to a minimum in fire hazard areas. reply leprechaun1066 7 hours agoprev> they're goats, as any self-respecting three year old could tell you Sheep or goat? https://www.youtube.com/watch?v=dCzZN--4Its reply gadders 7 hours agoprevFYI he's published an update: https://diamondgeezer.blogspot.com/2025/01/a-sheepish-apolog... tl;dr; he missed a small zoo near Waterloo Station. reply walthamstow 8 hours agoprevWhile Trafalgar is known as the official centre of London, you’d have trouble convincing anyone that Vauxhall is more central than Spitalfields. reply gadders 8 hours agoparentIt's Charing Cross: https://en.wikipedia.org/wiki/Charing_Cross#:~:text=Since%20.... reply Pinus 7 hours agorootparentSome years ago, I was looking for London hotels on some booking site, and noticed that they were listed as being so-and-so many km (with .1 or even .01 precision) from London, which seemed amusing given that they were all in London. So I fired up QGIS and drew a circle (in some suitable projection!) with the indicated radius around each hotel, and found that they intersected on Nelson’s Column in Trafalgar Square. reply Quarrel 7 hours agorootparentAs the parent indicated, this stuff is surprisingly standardised in London. Distance is measured from Charing Cross. The Cross is near Nelson's Column, but I would be surprised if the column was actually the central point. As others have pointed out, this leads to some weirdness, as lots assume that the City (old London & older Roman London) would be the obvious place to measure things from. reply fredoralive 6 hours agorootparentThe original Charing Cross was at the south of the square, where the statue of Charles I is now. (The cross at the railways station nearby is a Victorian folly). reply anyonecancode 5 hours agorootparentprevIn a similar vein, for New York City the official highway distance to the city is measured from Columbus Circle [1]. [1] https://en.wikipedia.org/wiki/Columbus_Circle reply andrewaylett 3 hours agorootparentIn Edinburgh, it's the old post office building at the end of Princes Street. Which is also where the A1, A7, and A8 meet. reply cesaref 7 hours agorootparentprevThere's also the confusion as to whether people mean 'Greater London' or 'The City of London'. For the centre of the City, i've generally thought that Leadenhall Market is the centre, given it is built on the original Roman Forum. For non-Londoners, the city was originally a walled city, and lies to the east end of what is now considered greater london. It's these days synonymous with the financial industry. There are special laws for it, honours like Freedom of the City, it's quite an interesting place. Although the wall is long gone, there are place names which refer to it, and the gates which exited through it. So we have roads like 'London Wall' and locations like Bishopsgate, Aldgate etc. Newgate was added in the 12th century, so not exactly 'New' these days, so not a very future proof naming convention... https://leadenhallmarket.co.uk/history-of-leadenhall-market/ https://inspiringcity.com/2013/04/13/the-seven-gates-of-lond... reply gadders 7 hours agorootparent>>Although the wall is long gone... You can still see chunks of the wall: https://livinglondonhistory.com/londons-ancient-roman-and-me... reply gadders 7 hours agorootparentAlso, if you want see some cool Roman stuff, there is a Temple of Mithras under the new Bloomberg headquarters near Cannon Street station. https://www.londonmithraeum.com/ Free to enter, but pre-booking recommended. reply foldr 3 hours agorootparentprevThere aren’t any special laws for the City of London (except any local byelaws). The City is administered in its own unique way, but English law applies just like anywhere else in England. https://www.quora.com/Does-the-City-of-London-as-it-is-defin... reply walthamstow 1 hour agorootparentBy far my most hated misconception about London. If what people say were true, every company in the world would be based there. reply walrus01 8 hours agorootparentprevAs a non British person I would think the center of the city would be some point equidistant centrally between the boundaries of the original roman city wall, in the square mile, but the actual center of the city seems to have migrated since then. reply nicoburns 7 hours agorootparentModern London encompasses two historic settlements (the city of London, and westminster). What is now consider the centre is somewhere between the two. https://en.m.wikipedia.org/wiki/Westminster reply MrsPeaches 5 hours agorootparentprevAnything that is south of the River Thames is considered \"South\" (prounounced \"Souf\" in MLE [1]), no matter the actual distance. E.g. Waterloo Station would be considered South London but is actually at the same latitude as Buckingham Palace! Hence why a Londoner would never describe Vauxhall as \"Central\". Londoners would generally discount any part of this map south of the river from \"Central London\": https://en.wikipedia.org/wiki/Central_London#/media/File:Ope... reply dghf 7 hours agorootparentprev> I would think the center of the city would be some point equidistant centrally between the boundaries of the original roman city wall That would be the centre of the City, but not necessarily of the city. reply madeofpalk 6 hours agorootparentprevThe center of the city has just as much (or more) to do with vibes than physical geography. reply MrsPeaches 5 hours agoparentprevThe River Thames, confusing Londoners sense of space since times immemorial. reply Rochus 1 hour agoprevThese poor sheep have to live on a concrete floor and hardly ever see a real meadow. reply dhosek 5 hours agoprevHmm, not sure where the closest sheep is to me here in the inner suburbs of Chicago, but there is a goat farm a couple miles away in the Austin neighborhood. It’s pretty wild seeing them take the herd to their pasture a couple blocks away from the house where their shed is in the back yard. reply Havoc 7 hours agoprevPretty wild to blog about that topic in particular haha. Those all look closer to sheep singular? Mudchute has maybe 30ish reply dkdbejwi383 6 hours agoparentAs a long time DG reader, this is both really on brand and completely novel. reply captainbland 3 hours agoprevLast time I was there they had grazing sheep in Green Park. That's pretty central! reply barrettondricka 4 hours agoprevWouldn't that need to be the sheep with the closest distance to all the other sheeps in London? reply shermantanktop 3 hours agoparentIn order to measure distance between sheep, you really should start from the center of the sheep. So let’s make the math easier by assuming a spherical sheep in a vacuum… reply Theodores 4 hours agoprevNever mind sheep, what about cats? Larry, the Number 10 cat has to be a contender. Considering the wealth of the nation was based on the woollen trade, with London being the place where weavers from Flanders made that wealth, times have changed. The Speaker's Chair in the House of Lords is still a wool sack, although, a few years ago, it was found to be stuffed with horsehair. The Royal Navy grew to defend the cross channel trade in wool and that led to 'Britannia Rules The Waves' in a big way, up until about a century ago. The British weather and the plague made it so that wool was the winning product, with the customers being the armies of Europe and the slaves that needed to be clothed. Although mining was crucial to the Industrial Revolution, wool was the original cradle of innovation. We owe so much to wool, and sheep. Oh, Fuller's Earth was also crucial to the success story, needed for cleaning wool, along with urine, which peasants provided all by themselves, in abundance. Other wool was not with the long, tough fibres that British wool had, hence the desirability of the product. reply ageitgey 7 hours agoprev[Deleted as I missed the reference in the article] reply pseudonymcoward 7 hours agoparentThe author addresses this in the post. \"I'm not interested in temporary sheep like those that get driven over Southwark Bridge in September or shorn at the Lambeth Country Show in June.\" reply shawabawa3 7 hours agoparentprevyou have made the tragic mistake of not reading literally the second sentence of the article: > I'm not interested in temporary sheep like those that get driven over Southwark Bridge in September or shorn at the Lambeth Country Show in June reply walrus01 8 hours agoprevLooking at the satellite view of some grassy rooftops in the city of London (the square mile), it seems to me that a sufficiently motivated wealthy person could keep several sheep in a more central location. Some of those roofs look like they have more habitat space than the most central sheep lives in. reply gadders 7 hours agoparentA friend of mine at a city brokers with a rooftop garden told me that one of the traders released some rabbits there. You could sit on the trading floor and look up and see them stretched out on the glass atrium roof sunbathing. reply Quarrel 7 hours agorootparentFinding the most central rabbit would be quite the task! reply shermantanktop 3 hours agorootparentBy the time the survey is done the number will have changed. Instantaneous global knowledge is impossible, sadly. reply defrost 7 hours agoparentprevThere are literal multi floor spacious underground bunkers in London beneath some of the more exclusive and expensive properties. A good many are known by filed dimensions, some are suspected to be larger than declared. While a number may be urban bunkers, others ostentatious wealth displays for shoes, clothes, jewels, and rare collectables of the very wealthy perhaps one is home to a rabbit .. or a sheep. reply dghf 7 hours agorootparent> some are suspected to be larger than declared I can't find a link now, but allegedly the Crossrail project accidentally tunnelled into one of these. reply walrus01 7 hours agorootparentprevFor all we know, some wealthy sheep collector has sheep in a 3rd level sub basement in their London townhouse, strapped into an oculus headset and roaming around on a multi dimensional treadmill through endless grassy spring time fields. reply surfingdino 4 hours agoprevNo sheep in Canary Wharf? They have a farm there, don't they? reply Xophmeister 4 hours agoparentVauxhall and Spitalfields are much closer to Trafalgar Square. reply biomene 4 hours agoparentprevThat would be mudchute farm. But Canary Wharf is not considered part of central London, which is what the author is concerned with. reply te_chris 4 hours agoprevMy son loves the city farms. Mudchute one is huge and great. reply Pinus 8 hours agoprevIsn’t there a recurring publicity stunt with sheep on Savile Row? =) reply willvarfar 7 hours agoprev [–] I clicked on it expecting an article on how to trip up LLMs. Refreshing change that it wasn't! :D So, do any LLMs give any humorously nonsensical answers? reply Consider applying for YC's Spring batch! Applications are open till Feb 11. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Vauxhall City Farm, established in 1976, is the closest city farm to central London, located just over a mile from Trafalgar Square.",
      "The farm houses several sheep, including a Shetland and a Herdwick named Daffodil, showcasing the rural charm within the urban environment.",
      "Spitalfields City Farm, the second closest to central London, hosts a Herdwick ewe named Beatrix and several Castlemilk Moorits, emphasizing the unique appeal of city farms in London."
    ],
    "commentSummary": [
      "The \"time to sheep\" metric, which gauges the duration to reach the countryside from a city center, initiated a debate on urban versus rural living preferences.",
      "The conversation highlighted cities like London, Bristol, and Toronto, focusing on their accessibility to nature or farms.",
      "Humorous city metrics such as \"time to chaos\" and \"time to cow\" were also part of the discussion, adding a light-hearted perspective."
    ],
    "points": 183,
    "commentCount": 106,
    "retryCount": 0,
    "time": 1737626300
  }
]
